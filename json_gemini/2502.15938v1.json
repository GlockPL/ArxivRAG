{"title": "STRAIGHT TO ZERO: WHY LINEARLY DECAYING THE LEARNING RATE TO ZERO WORKS BEST FOR LLMS", "authors": ["Shane Bergsma", "Nolan Dey", "Gurpreet Gosal", "Gavia Gray", "Daria Soboleva", "Joel Hestness"], "abstract": "LLMs are commonly trained with a learning rate (LR) warmup, followed by cosine decay to 10% of the maximum (10\u00d7 decay). In a large-scale empirical study, we show that under an optimal peak LR, a simple linear decay-to-zero (D2Z) schedule consistently outperforms other schedules when training at compute-optimal dataset sizes. D2Z is superior across a range of model sizes, batch sizes, datasets, and vocabularies. Benefits increase as dataset size increases. Leveraging a novel interpretation of AdamW as an exponential moving average of weight updates, we show how linear D2Z optimally balances the demands of early training (moving away from initial conditions) and late training (averaging over more updates in order to mitigate gradient noise). In experiments, a 610M-parameter model trained for 80 tokens-per-parameter (TPP) using D2Z achieves lower loss than when trained for 200 TPP using 10\u00d7 decay, corresponding to an astonishing 60% compute savings. Models such as Llama2-7B, trained for 286 TPP with 10\u00d7 decay, could likely have saved a majority of compute by training with D2Z.", "sections": [{"title": "1 INTRODUCTION", "content": "Learning rate (LR) schedules play an important role in training large language models. The original Transformers paper proposed a brief LR warmup followed by decay proportional to the inverse square root of the step number (Vaswani et al., 2017). This schedule has the advantage of not requiring prior specification of the total training steps. However, cooling down to a specific minimum LR is acknowledged to be \"preferable when one knows the training duration in advance\" (Zhai et al., 2022) as it produces \"slightly better results\" (Raffel et al., 2020). In this paper, our main focus is finding LR schedules that achieve the minimum loss given a pre-specified number of training tokens.\nThe \"predominant choice\u201d (Hu et al., 2024) in such training the \"de-facto standard\" (H\u00e4gele et al., 2024) is warmup followed by cosine decay to 10% of the maximum LR, an approach used in GPT3 (Brown et al., 2020), Gopher (Rae et al., 2022), Chinchilla (Hoffmann et al., 2022), BLOOM (Scao et al., 2023), Llama (Touvron et al., 2023a), Llama2 (Touvron et al., 2023b), Falcon (Almazrouei et al., 2023), Pythia (Biderman et al., 2023), etc. It is used \"following Hoffmann et al.\" (Muennighoff et al., 2023), and is the default schedule in LLM codebases (Karpathy, 2024).\nWe present a large-scale empirical study to determine which schedules work best in which situations, and why. We focus on both compute-efficient and over-trained models. According to Chinchilla scaling laws (Hoffmann et al., 2022), the fewest FLOPs to achieve a given loss is obtained when models are trained for around 20 tokens-per-parameter (TPP). It is also common to train for more than 20 TPP because smaller, over-trained models are cheaper to serve (Touvron et al., 2023a). Our experiments (across various model scales, vocabulary sizes, and dataset sources) reveal a consistent outcome: when all schedules use their optimal peak LR, linear decay-to-zero (D2Z) works best at compute-optimal TPP. Moreover, the relative benefit of D2Z over 10x (in terms of training, validation and downstream loss) increases with TPP. Compute savings from D2Z can be substantial for over-trained models\""}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "2.1 LEARNING RATE SCHEDULES\nLR schedules have a long history in stochastic optimization, and are motivated by convergence bounds for stochastic gradient methods (Moulines & Bach, 2011; Bottou et al., 2018). For example, following Andriushchenko et al. (2023), consider SGD for a convex loss parameterized by \u03b8: with a constant LR \u03b7, the gap between the optimum and current loss at step t can be bounded by:\n$E[L(\\theta_t) - L(\\theta^*)] \\leq (1 - \\eta \\mu)^t ||\\theta_0 - \\theta^* ||^2 + \\frac{\\eta \\sigma^2}{\\mu}$\nwhere \u03b80 are initial parameters, \u03b8\u2217 is the loss minimizer, \u03c3\u00b2 is a bound on the variance of gradient noise, and u is a measure of the objective's curvature (see also Bottou et al. (2018, Theorem 4.6)). A larger LR can decrease dependence on initial conditions (the bias term), but also increase the effect of gradient noise (the variance term). As training progresses, bias decreases exponentially in t, and the relative importance of variance increases. Bottou et al. (2018) note this motivates a strategy where LR is high initially (to mitigate bias) and lowered later (to minimize variance).\nIn practice, when and how to reduce the LR is rarely informed by ML theory. Many LLMs simply follow the 10x cosine schedule, which is noted to work slightly better than cosine D2Z in the influential work of Hoffmann et al. (2022). It is also well established that using a portion of a longer (or extending a shorter) schedule is suboptimal compared to using a schedule that reaches its minimum only at the final training step (Li et al., 2019; Hoffmann et al., 2022; Hu et al., 2024; H\u00e4gele et al., 2024). Linear decay after warmup (Howard & Ruder, 2018) has been used in LLMs, typically also to 10% of the peak (Henighan et al., 2020; Dey et al., 2023; Sengupta et al., 2023). Dropping LR at specific milestones (step decay) is popular in vision models (He et al., 2016; Zagoruyko & Komodakis, 2016; Li et al., 2020), but has also been used in LLMs (Bi et al., 2024).\nKaplan et al. (2020) compared various decay functions and concluded the specific schedule was unimportant given a high enough average LR, although decaying to zero \"gives a fixed improvement close to the end of training.\" Few papers explicitly compare different LR schedules for large-scale training, and when comparisons are made (Shallue et al., 2019; Kaplan et al., 2020; Schmidt et al., 2021; Hoffmann et al., 2022; Yang et al., 2021), they are not the primary focus. So, while some insights are gained, \"comprehensive study\" is usually regarded as \"out of scope\" (Aleph Alpha, 2024). One exception is Defazio et al. (2023), who found linear equals or outperforms other common schedules, including cosine, across a range of problems, including LLM training. In addition, Defazio et al. develop convergence bounds that theoretically motivate linear as the optimal schedule. Unlike our work, they do not evaluate LLMs with different peak LRs or decay ratios.\nSeeking to measure model quality at different training durations without having to re-train separate models from scratch (Zhai et al., 2022; H\u00e4gele et al., 2024), researchers have adopted various continuous schedules, such as constant, cyclic (Smith, 2017), etc. Following optimization theory (Moulines & Bach, 2011; Defazio et al., 2024) weight averaging provides an alternative to decay (Sandler et al., 2023; Sanyal et al., 2023; Busbridge et al., 2024), although it is typically not as effective (H\u00e4gele et al., 2024), and moreover may have hyperparameters that implicitly depend on training duration (Defazio et al., 2024). Warmup-Stable-Decay (WSD) approaches have also been used in LLM training (Hu et al., 2024; Shen et al., 2024a; Ibrahim et al., 2024; H\u00e4gele et al., 2024). These methods train at a constant LR, but decay from a checkpoint in a separate process when an intermediate model is needed. We show that the optimal constant LR of these methods implicitly depends on training duration, rendering them not truly schedule-free."}, {"title": "2.2 \u039c\u0391\u03a7\u0399\u039cAL UPDATE PARAMETERIZATION (\u03bcP)", "content": "It is common to scale initial weights of neural networks such that activations have unit variance (Glorot & Bengio, 2010; He et al., 2015). Yet weights can become unstable after a few steps of updates, if layer-wise LRs are imbalanced (Yang et al., 2021). In contrast, \u00b5P (Yang & Hu, 2020; Dey et al., 2024a) prescribes a re-parameterization of initial weight variances and LRs \u2013 essentially, rules for scaling these values as model width (i.e., dmodel) scales so that activations and updates remain stable. \u00b5P also stabilizes embeddings, layer norms, and self-attention in Transformers. \u00b5P is seeing growing application in LLMs (Dey et al., 2023; Shen et al., 2024b; Hu et al., 2024), where it acts to stabilize training and to enable transfer of optimal hyperparameters (HPs) across model scales.\nWith \u00b5P, base HPs can be tuned on a small proxy, or base, model, and transferred to larger models. Given the width of the proxy model, dp, and width of the target, dt, \u00b5P prescribes scaling factors to apply to HPs. The base LR \u1fc6 is scaled down to \u03b7 = \u03c1\u1fc6, where p = dp/dt. In terms of LR schedules, the base LR \u1fc6t is scaled at every step to provide nt. \u00b5P is convenient in our study as we can sweep the same base peak LRs, \u1fc6, at each model size, and observe trends that are scale-invariant."}, {"title": "2.3 ADAMW WEIGHTS AS AN EXPONENTIALLY-WEIGHTED MOVING AVERAGE (EMA)", "content": "An AdamW update at a single training step, t, can be expressed as:\n$\\theta_t = (1 - \\eta \\lambda) \\theta_{t-1} - \\eta \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$\nwhere \u03b7 is the learning rate, mt and \u00fbt are (bias-corrected) running averages of the gradient and the squared gradient, respectively, and e is a small constant added to prevent division by zero. The weight decay value, \u5165, is typically set to 0.1 in LLM training (Brown et al., 2020; Hoffmann et al., 2022; Almazrouei et al., 2023; Aleph Alpha, 2024)."}, {"title": "3 METHODS, CONCEPTUAL FOUNDATIONS, AND HYPOTHESES", "content": "We now present an extended EMA perspective on AdamW that accounts for time-varying LRs. We then introduce our conceptual model of LLM training, and connect it to the EMA perspective. Finally, we outline the specific testable hypotheses that follow from our conceptual model."}, {"title": "3.1 ADAMW AS CONVEX COMBINATION OF WEIGHT UPDATES, DRIVEN BY LR SCHEDULE", "content": "Consider a generic moving average, but now with time-varying smoothing, at, i.e., Yt = (1 \u2212 at)Yt-1 + Atxt. If we let a1 = 1 (so that y\u2081 = x1), we can express yt in terms of all inputs xt:\n$y_1 = \\alpha_1 x_1,$\n$y_2 = (1 - \\alpha_2) \\alpha_1 x_1 + \\alpha_2 x_2,\u2026$\n$y_t = \\sum_{i=1}^t \\big[ \\prod_{j=i+1}^t (1 - \\alpha_j) \\big] \\alpha_i x_i$\n$y_t = \\sum_{i=1}^t C_{t,i} x_i$\nWhere $C_{t,i} = \\big[ \\prod_{j=i+1}^t (1 - \\alpha_j) \\big] \\alpha_i$. It can be shown \u2200t, \u2211i Ct,i = 1, and hence, as in a standard EMA, each yt is a convex combination of inputs X1... Xt (all coefficients are non-negative and sum to 1). However, with time-varying at, we are not restricted to exponentially-decreasing ct,i as i decreases. Indeed, for any convex combination of inputs, there is a corresponding smoothing schedule that generates the combination via the EMA. In terms of LR schedules for AdamW training, Yt = \u03b8t, while at = ntd becomes the smoothing parameter at step t."}, {"title": "3.2 CONCEPTUAL MODEL: BIAS AND VARIANCE IN LLM TRAINING", "content": "Following Andriushchenko et al. (2023), our main conceptual premise is that in LLM training, there is an initial training phase focused on movement from initial conditions (bias reduction), followed by"}, {"title": "3.3 EXPERIMENTAL HYPOTHESES", "content": "Hypothesis 1: As TPP increases, the relative benefit of D2Z over 10\u00d7 decay will increase.\nThis hypothesis follows from the premise that gradient variance plays a larger role at higher TPP, and greater LR decay (as in D2Z) allows for more updates to be averaged, and thus greater variance reduction. A related hypothesis is that if we increase the batch size at each step, gradient variance will decrease, and so the benefits of D2Z over 10\u00d7 decay should diminish. Note our conceptual framework does not say precisely at which TPP or batch size that D2Z will first prevail; here we will rely on our empirical findings (Section 4) to fill in the theoretical gap.\nHypothesis 2: As TPP increases, the optimal peak LR will decrease, for all LR schedules.\nTuning peak LR is about trading off movement from initial conditions (requiring a high LR) and mitigating variance (requiring a low LR). As TPP increases, and bias reduction plays a smaller role, optimal peak LR should decrease. We hypothesize the decrease will be greater with a Constant schedule, as Constant does not use decay to balance the conflicting demands of bias and variance (Figure 3). Moreover, optimal peak LR for other continuous LR schedules, like WSD and Cyclic, should also decrease with longer training durations. In this way, such \u201cschedule-free", "29)": "the higher the LR, the more emphasis on recent updates.\nHypothesis 3: Linear D2Z will improve over Cosine D2Z.\nWhile LR decay allows averaging over more weight updates, if the LR decreases too quickly, the final weight updates may not contribute to the EMA. From a loss surface perspective, as the LR approaches zero, we take vanishingly small steps. Since Cosine reaches smaller steps faster than Linear (Figure 2, left), Cosine will make less progress toward the optimum loss. From the EMA perspective, this is equivalent to Cosine having smaller Ct,i coefficients as i approaches t (Figure 2, right). Note this problem is unique to Cosine D2Z and will not affect, e.g., Cosine 10\u00d7 decay.\nHypothesis 4: A high LR (not weight decay) reduces bias and achieves optimal loss.\nNote that weight updates xt have a coefficient of 1/x in Equation (4). So, while nt and A contribute equally to aj, increasing A to reduce bias is counterproductive as weight updates will be scaled down proportionally, reducing movement from initial conditions. However, if LR is in a high enough range so that bias plays a minimal factor, both LR and WD should equally affect variance reduction."}, {"title": "4 EMPIRICAL ANALYSIS OF DECAY-TO-ZERO", "content": "4.1 EXPERIMENTAL SETUP\nExperiments use a GPT-like LLM (Radford et al., 2019), with ALiBi embeddings (Press et al., 2022) and SwiGLU (Shazeer, 2020). Main paper models are trained on SlimPajama (Soboleva et al., 2023) and evaluated over 1.1B held-out tokens (regardless of training TPP). Unless otherwise indicated, X=0.1 is used. Training runs use the same random seed, so all decay functions (Linear, Cosine, etc.) and ratios (Constant, 10\u00d7, D2Z) have identical warmups, but note results are very consistent across seeds at this scale (appendix Figure 24). By default we use \u00b5P (standard parameterization results are in Appendices C.2 and C.10). \u00b5P HPs are derived from a smaller proxy model tuned using a Linear-10x schedule. Since we hypothesized that different LR schedules may enjoy their optimal peak LR at different values, our experiments compare schedules when each is tuned to its optimal peak LR; we sweep \u00f1 by factors of 2\u00d7 around the \u00b5P proxy-tuned \u1fc6 = 1.6e-02. Appendix A provides further details on the model architecture (Table 2), dataset sizes (Table 3), and compute resources."}, {"title": "4.2 RESULTS", "content": "Finding 1: Linear-D2Z is the optimal LR schedule across virtually all peak learning rates.\nFor 610M-parameter \u00b5P models trained to compute-optimal 20 TPP, the optimal Linear-D2Z setting achieves 0.77% lower loss than the optimal Linear-10\u00d7 setting"}, {"title": "5 DISCUSSION", "content": "The confounding role of LR schedule We have shown that the more constant the LR schedule, the more sensitive the optimal peak \u03b7. These findings help explain LR sensitivity in prior work. For example, Shen et al. (2024b) were puzzled by their observation that \u201calthough the WSD scheduler could, in theory, continue in the stable phase forever ... the optimal LRs are different for different amounts of training tokens.\" As noted under Hypothesis 2, LR schedules with long constant periods only appear \"schedule-free\u201d in the primal; from the dual (EMA) perspective, the higher the LR, the more emphasis is placed on recent updates (see appendix Figure 28a for Constant, Figure 29d for WSD). Filatov et al. (2024) and Yang et al. (2021, Figure 19) also observed major decreases in optimal LR with higher TPP; these results were both obtained with a constant LR. In contrast, Bjorck et al. (2024) observed less significant decrease of optimal \u03b7 with TPP, but used 10\u00d7 decay.\nLR schedule may also affect the optimal \u03b7 as batch size varies particularly when using \u00b5P. Some prior \u00b5P studies (Yang et al., 2021; Noci et al., 2024; Shen et al., 2024b) observed linear scaling of optimal LR with batch size, i.e., the so-called linear scaling rule (Krizhevsky, 2014; Chen et al., 2016; Smith et al., 2018). Others (Lingle, 2024) have observed square-root scaling, resonating with other prior studies (Hoffer et al., 2017; You et al., 2019b; Malladi et al., 2022). This discrepancy can be explained by noting the linear scaling results were all found with a Constant or WSD LR decay, while square-root was observed with Linear D2Z, again underscoring the greater stability of D2Z.\nThe confounding role of training duration While LR schedules have confounded studies varying TPP, TPP has analogously confounded studies evaluating LR schedules. Recall Kaplan et al. (2020) also saw a benefit from D2Z. In contrast to Chinchilla scaling (Hoffmann et al., 2022), in the Kaplan et al. perspective, small models should be trained to high TPP (while larger models should be trained less). It is therefore not surprising Kaplan et al. saw benefits testing D2Z with small models; as we have shown, D2Z is especially effective at high TPP. In contrast, in Figure 4 of Yang et al. (2021), Linear is worst of all schedules, and the gap between it and Constant and InvSqrt grows with model width. But here, since training data is small and fixed, then TPP decreases as width increases. Thus training is in a phase where bias reduction is paramount and D2Z is not effective. Notably, in their LLM training experiments at higher TPP, Yang et al. do report linear D2Z to work best.\nWith this context, we speculate on why D2Z has not been adopted more widely in LLM pre-training:\n\u2022 First, it is common to evaluate hyperparameters on smaller training runs; unfortunately, with limited training data (low TPP), D2Z misleadingly under-performs.\n\u2022 Secondly, coupling between LR schedule and optimal peak LR is problematic: with 10\u00d7 decay, we may find a lower peak LR is optimal; if we then test D2Z with the same LR, it may be suboptimal for D2Z. Not seeing a benefit at this LR, we may conclude D2Z is inferior in general.\n\u2022 Finally, poorly controlled training dynamics may prevent D2Z models from training with their (higher) optimal LR. Indeed, when we initially compared D2Z and 10\u00d7 using NanoGPT"}, {"title": "6 LIMITATIONS AND FURTHER EXPERIMENTS", "content": "While our findings strongly support linear D2Z being optimal in our specific context, there are some limitations to keep in mind. First, LR schedules like D2Z require prior knowledge of the total number of training steps. But it is worth reiterating that even nominally schedule-free approaches such as Constant also require this knowledge in order to optimally set the LR value. In contrast, the extended EMA perspective of LR schedules enables derivation of a truly schedule-free schedule, which we introduce in Appendix D.1. Second, our focus in this paper was specifically LLM training at compute-optimal and overtrained dataset sizes. For ML problems with limited access to training data, D2Z is likely not the best strategy. Third, our work focuses on AdamW (the standard optimizer in LLM pre-training). While the extended EMA perspective of LR schedules will likely apply to other optimizers that use decoupled weight decay (as noted by Wang & Aitchison (2024)), it may not apply to approximate second order methods, such as Shampoo (Gupta et al., 2018). Finally, for LLMs with unstable training dynamics that cannot tolerate high LRs, D2Z may not be beneficial. We experienced this first-hand when we initially trained NanoGPT.\nHowever, we also note the remarkable consistency of D2Z's success. While results in the main paper used the SlimPajama dataset, consistent results were found with OpenWebText and a multilingual dataset (with a larger vocabulary). We also saw consistent findings with different parameterizations, architectures , weight sparsity settings, and training frameworks . Appendix C.11 describes scaling laws fit to models trained with D2Z and 10x, at model sizes up to 2.75B; results indicate a growing performance gap as scale increases. Finally, Appendix C.6 demonstrates that the benefits of weight decay are observed primarily when using LR D2Z, where raising weight decay can fine-tune the EMA update coefficients without affecting training stability."}, {"title": "7 CONCLUSION", "content": "Linear decay-to-zero is the optimal LR schedule for LLM training using AdamW. To be clear, less decay is beneficial for low tokens-per-parameter training, but there is no practical reason to perform such training with LLMs, since the same FLOPs could be used to train a smaller model, over more tokens, to a lower loss \u2013 using D2Z. The superiority of D2Z was validated across a range of experimental conditions. Results suggest its relative benefit will increase as models increase in scale. Moreover, when using D2Z and \u00b5P, the optimal peak LR is less sensitive to changes in weight decay, dataset size, and batch size, i.e., there is better hyperparameter transfer.\nOur analysis was aided by our interpretation of AdamW's output as a convex combination of prior weight updates. D2Z overfits less the final training sequences, and is especially beneficial when gradient noise dominates training. As we enter a phase of applied ML where inference efficiency is a primary concern, there is strong motivation to study high-TPP training, where gradient noise is the bottleneck. While our results indicate that D2Z is a key component of the solution here, further investigation is required, including into how and when to adjust hyperparameters such as weight decay, batch size, and learning rate, in the high-TPP context."}, {"title": "A EXPERIMENTAL DETAILS", "content": "Table 2 provides details on model architecture and hyperparameters for the main experiments (i.e., results presented in the main paper). Table 3 provides information on the training steps. All the models in our main experiments were trained on the SlimPajama dataset a cleaned and deduplicated version of the RedPajama dataset. We use the GPT-2 vocabulary of size 50257, and a context length of 2048 tokens. Unless otherwise noted, the weight decay value, A, is by default set to 0.1, following standard practice in LLM pre-training . Also following standard practice, we do not apply weight decay or bias to LayerNorm layers. Validation loss is always computed over 1.1B held-out tokens, regardless of training TPP. By default we parameterize with \u00b5P (further details below).\nFor a given TPP, all models have the exact same warmup phase: a linear warmup of the LR from 0 to the peak value. In all our runs, warmup was 10% of the total steps. LR warmup is standard practice in LLM pre-training.\nAll models in the main experiments were trained on a Cerebras CS-3 system. 610M-parameter, 20 TPP models take roughly 6 hours each to train on a single CS-3. If a training run did not complete due to numerical instabilities, the values are left off our plots or marked as NaN in Table 1.\nProxy model hyperparameter tuning To find the optimal \u00b5P hyperparameters (HPs), we trained a 39M-parameter proxy model using a width dmodel=dp of 256, with 24 layers and a head size of"}, {"title": "B DERIVATIONS", "content": "B.1 DERIVATION OF EQUATION (6)\nThe initial coefficient is $C_{t,1} = \\prod_{j=2}^t (1 - \\alpha_j)$. Clearly, if $\\alpha_j = \\alpha$ is constant, we have $c_{t,1} = (1-\\alpha)^{t-1}$. Otherwise, given $\\alpha_j$ is small, we can use a first-order Taylor expansion, $e^{-\\alpha_j} \\approx 1-\\alpha_j$, and therefore:\n$C_{t,1} \\approx \\prod_{j=2}^t e^{-\\alpha_j}$\n$= e^{-\\sum_{j=2}^t \\alpha_j}$\nAssuming we only know the average value, $\\bar{\\alpha} = \\frac{1}{t} \\sum_{j=1}^t \\alpha_j$, we have:\n$C_{t,1} \\approx e^{-\\bar{\\alpha}(t-1)}$\n$=(e^{-\\bar{\\alpha}})^{t-1}$.\nGiven $\\bar{\\alpha}$ is also small, we reverse the earlier Taylor expansion, but now $e^{-\\bar{\\alpha}} \\approx 1 - \\bar{\\alpha}$, and:\n$C_{t,1} \\approx (1-\\bar{\\alpha})^{t-1}$.\nThat is, with a small time-varying smoothing parameter, the initial coefficient in an EMA, $C_{t,1}$, also decreases exponentially with the number of steps."}, {"title": "C ADDITIONAL EXPERIMENTAL RESULTS", "content": "In this section, we include some additional results to support the findings in the main paper. All validation losses reported in this section are from models trained with Linear decay."}, {"title": "C.1 DOWNSTREAM EVALUATIONS", "content": "Finding 6: Trends in validation loss also hold for downstream evaluation."}, {"title": "C.2 STANDARD PARAMETERIZATION", "content": "Finding 7: Similar trends hold for the standard parameterization.\nFigure 10 presents results for a 610M-parameter model trained with the standard parameterization. Here \u00f1 is therefore not a \u00b5P-corrected base LR, but rather a LR that we swept directly for this model scale. Results are obviously quite similar to results using \u00b5P, suggesting the benefits of D2Z are not \u00b5P-specific. Further results using the standard parameterization, but for NanoGPT models, are in Appendix C.10 below."}, {"title": "C.3 MODEL SIZES", "content": "Finding 8: Improvement of D2Z over 10\u00d7 is greater at 1.7B scale.\nFigure 11 presents results across 111M, 610M, and 1.7B model sizes, all trained to 20 TPP. Note the absence of results for the highest LR setting at the 1.7B-scale; at the very highest LR, numerical instabilities led to failed training runs. Otherwise, results are fairly similar across model sizes. At the proxy-tuned peak LR, the gap between D2Z and 10\u00d7 is 0.81%, 0.67%, and 1.56%, at the 111M, 610M, and 1.7B scales, respectively. We further investigate the issue of whether the gap between D2Z and 10x varies with model size as part of our scaling law experiments below "}, {"title": "C.4 TPP", "content": "Finding 9: Improvement of D2Z over 10\u00d7 grows with TPP (610M scale).\nAs we vary TPP, we consistently see increasing gains with D2Z. Here we plot the results for the 610M-scale models in Figure 12, as a counterpart to main Figure 5."}, {"title": "C.5 BATCH SIZES", "content": "Batch size setup Default batch sizes of 192 and 504 were selected for 111M and 610M scales. These specific values were based on an internal scaling law for optimal batch size as a function of compute FLOPs (similar to those in Bi et al. (2024) and Porian et al. (2024)). Batch size of 504 was then re-used for 1.7B training (later testing confirmed this to be a good setting at 1.7B for 20 TPP training). In our experiments varying batch size, we swept B by factors of 2 around these initial settings."}, {"title": "C.5.1 Iso-TPP BATCH SIZE EXPERIMENTS", "content": "Finding 10: Improvement of D2Z over 10\u00d7 grows as batch size decreases (at constant TPP).\nIn main paper Figure 7, we presented results where all models train for the same number of total tokens (20 TPP), while only the batch size, B, varies. Figure 13 presents the same data, but with each B separated into a separate subplot; this lets us better observe how the differences between D2Z and 10x evolve as B changes. We see clearly that for smaller B, as gradient noise increases, the differences between D2Z and 10\u00d7 also increase. This again demonstrates that LR decay is beneficial as a noise reduction mechanism."}, {"title": "C.5.2 Iso-STEP BATCH SIZE EXPERIMENTS", "content": "Finding 11: Optimal LR increases with batch size when training over the same number of batches/steps.\nIn the above batch size tests, we kept the dataset size constant (iso-TPP). Now consider the following two mechanisms for increasing the dataset size (TPP) of a training run:\n1. Fix the batch size, but increase the number of steps.\n2. Fix the number of steps, but increase the batch size.\nApproach (1) was taken in our experiments increasing TPP. In that case, we found optimal LR to decrease, as a mechanism for coping with greater gradient noise, which grows with TPP. We now discuss Approach (2): fixing the number of steps, but increasing the batch size (iso-step).\nIn Figure 14, we keep the number of steps constant to 11752, so each model will see the same total number of batches, and we increase B by factors of two. Since number of steps does not change, Wang & Aitchison do not prescribe any adjustments to the timescale, and hence no adjustment is needed to LR (or weight decay). But we clearly see that optimal LR increases as B increases (like a rolling wheel, LR bowls rotate clockwise as we move to the right). This is evidently because increasing B decreases gradient noise; with less noise, we can afford a larger LR throughout training. More precisely, increasing \u1f22 decreases the number of AdamW weight updates that are combined. Therefore, an increase in \u1fc6 can be viewed as a way to reduce the effective number of batches that are combined, basically compensating for the fact per-step B is larger.\nAs B increases, gradient noise decreases, and we would expect benefits of LR decay to also diminish; differences between 10\u00d7 and D2Z do seem to decrease. This aligns with prior work, e.g., You et al. (2019a) compared SGD to full-batch GD (with WideResNets for image classification), and showed that when gradient noise is eliminated by using full GD, LR decay is no longer beneficial."}, {"title": "C.6 WEIGHT DECAY", "content": "C.6.1 WEIGHT DECAY: RESULTS\nFinding 12: Benefits of weight decay are observed primarily when using LR D2Z.\nFigure 15 analyzes the interaction between weight decay (WD) and LR for 610M models. Here we plot with the x-axis set to the (peak) smoothing parameter, \u03b1=\u03b7\u03bb (on a log scale), and y-axis set to validation loss. Note that for a given decay ratio and fixed a, EMA update coefficients are identical (i.e., specific values of \u03b7 and A do not affect coefficients, only their product). Thus it supports the extended EMA perspective to note that regardless of peak LR, \u1fc6, models tend to reach lowest loss at around the same a . However, to reach optimal loss, models require \u1fc6 to be high enough, but not too high \u2013 at =6.5e-02 and above, we consistently encounter instabilities in training."}, {"title": "C.6.2 WEIGHT DECA"}]}