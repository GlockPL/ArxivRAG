{"title": "Bayesian Inference in Recurrent Explicit Duration Switching Linear Dynamical Systems", "authors": ["Miko\u0142aj S\u0142upi\u0144ski", "Piotr Lipi\u0144ski"], "abstract": "In this paper, we propose a novel model called Recurrent Explicit Duration Switching Linear Dynamical Systems (REDSLDS) that incorporates recurrent explicit duration variables into the rSLDS model. We also propose an inference and learning scheme that involves the use of P\u00f3lya-gamma augmentation. We demonstrate the improved segmentation capabilities of our model on three benchmark datasets, including two quantitative datasets and one qualitative dataset.", "sections": [{"title": "1 Introduction", "content": "Understanding complex dynamics of spatio-temporal data is a challenging task studied with a range of techniques, from theoretical mathematical models based on partial differential equations to practical machine learning ones constructed on recorded data. One of such techniques are State Space Models (SSMs) (Murphy, 2023), including Hidden Markov Models (HMMs), Switching Linear Dynamical Systems (SLDS) (Fox et al., 2011), and their numerous extensions. SSMs try to describe the complex dynamics by decomposing it into some separate parts with much simpler dynamics, corresponding to some hidden states of the complex system, and a transition mechanism between such hidden states.\nIn this paper, we focus on Recurrent Switching Linear Dynamical Systems (rSLDS) (Linderman et al., 2017) which are one of the popular modern extensions of SLDS, incorporating some machine learning concepts to model the dynamics more accurately. SLDS assume that each observable temporal value is strongly related to an unobservable discrete hidden state and an unobservable value of a latent continuous variable (sometimes referred to as a continuous hidden state) that determine the dynamics of the complex system. They model the recorded temporal values as observations of a certain random variable, and the probabilistic distribution of the random variable depends on the current discrete hidden state and the current value of the latent continuous variable. Discrete hidden states are also modeled as (unrecorded) observations of a certain (latent) random variable, and the probabilistic distribution of the random variable is usually defined by a matrix of probabilities of transitions from one discrete hidden state to another. Similarly, the (unrecorded) values of the latent continuous variable are modeled as observations of a certain (latent) random variable that depends on the current discrete hidden state. rSLDS assume that the next discrete hidden state also depends on the previous continuous hidden states.\nTo illustrate the concept of SLDS, we may consider a simple example of modeling the movement of a person. Discrete hidden states correspond to different modes of transport. Continuous hidden states correspond to the true positions of the person on the map. The recorded temporal values correspond to the observed (noised) positions of the person in successive time instants. Due to their interpretability, (r)SLDS models are widely used for neural activity analysis (Zoltowski et al., 2020; Osuna-Orozco and Santacruz, 2023; Song and Shanechi, 2023; Jiang and Rao, 2024; Bush and Ramirez, 2024) and more recently for vehicle trajectory prediction (Wei et al., 2024).\nIn regular SLDS, switching between particular discrete hidden states is state-dependent only, since the probability of transition to the next state depends only on the previous hidden state. In rSLDS, switching is also state-dependent only, but the probability of transition to the next state depends also on the previous continuous state. Many studies (Dewar et al., 2012; Chiappa, 2014; Ansari et al., 2021) suggested that, in the case of more complex systems, the use of state-dependent switching only makes it difficult to capture the complex dynamics in the learning process without further limiting the duration of the discrete hidden state. Therefore, a type of time-dependent switching was incorporated in SSMs such as hidden Markov models with implicit state duration distributions (Dewar et al., 2012), implicit-duration Markov switching models (Chiappa, 2014), or deep implicit-duration switching model (Ansari et al., 2021).\nAs stated by Gelman et al. (2020), there exist three distinct approaches to consider when it comes to modeling and prediction.\nFrom a conventional statistical viewpoint, textbooks usually present statistical inference as a pre-defined problem where a model has been pre-selected. In the context of Bayesian statistics, the aim is to provide an accurate summary of the posterior distribution. The computation process should continue until an approximate convergence is achieved.\nFrom the viewpoint of machine learning, the primary aim in machine learning is typically prediction, rather than estimating parameters, and the computational process can be halted once the accuracy of cross-validation prediction has reached a steady state.\nFrom the viewpoint of model exploration, a substantial portion of our work in applied statistics is dedicated to probing, experimenting with a range of models, a significant number of which may exhibit poor alignment with data, subpar predictive capabilities, and sluggish convergence.\nOur approach aligns with the perspective of model exploration, as iterative model building has become standard in Bayesian data analysis (Gelman et al., 2020, 2015; Blei, 2014; Moran et al., 2024).\nThis paper proposes an extension of rSLDS with an additional mechanism to control the duration of the discrete hidden state. It introduces a recurrent explicit state duration variable that is modeled using a conditionally categorical random variable with a given finite support, similarly to explicit duration SSMs, such as HMMs (Yu, 2016) and other switching dynamical systems with continuous states (Chiappa, 2014; Chiappa and Peters, 2010; Oh et al., 2008). Such an explicit duration mechanism facilitates the learning process and makes it easier to capture the complex dynamics that outperforms the original rSLDS.\nThe main contributions of this work are: REDSLDS - a probabilistic model using locally linear dynamics that jointly approximate global nonlinear dynamics, incorporating recurrent explicit duration variables for more accurate segmentation, a fully-Bayesian sampling procedure using P\u00f3lya-Gamma data augmentation to allow for fast and conjugate Gibbs sampling, a series of experiments on synthetic and real popular datasets, assesing our model's performance on variety of problems."}, {"title": "2 Background", "content": "Our research is based on the findings in the fields of switching state-space models and explicit duration modeling."}, {"title": "2.1 Switching State-Space Models", "content": "There are numerous models available that utilize both state-space modeling and discrete Markovian states. However, it can be argued that Switching Linear Dynamical Systems serve as the basic foundation for more advanced constructions.\nThe Switching Linear Dynamical System (Kim, 1994; Murphy, 1998) is the generative model, defined as follows. At each time $t = 1,2,..., T$ there is a discrete latent state $s_t \\in \\{1, 2, ..., K\\}$ following Markovian dynamics,\n$St+1 | St, \\{\u03c0k\\}_{k=1}^{K} \\sim \u03c0_{St}$ (1)\nwhere ${\\{\u03c0k\\}_{k=1}^{K}}$ is the Markov transition matrix and $\u03c0_k \\in [0,1]^K$ is its k-th row. In addition, a continuous latent state $x_t \\in R^M$, where M is the dimensionality of the continuous latent space, follows conditionally linear dynamics, where the discrete state $s_t$ determines the linear dynamical system used at time t:\n$Xt+1 = A_{St+1}Xt + a_{St+1} + e_t, e_t \\sim^{iid} N (0, Q_{St+1}),$ (2)\nfor matrices $A_k, Q_k \\in R^{M \\times M}$ and bias vector $a_k, \\in R^M$ (where $A_k$ is linear transformation matrix between subsequent continuous states, and $Q_k$ is their covariance) for $k = 1, 2, . . ., K$. Finally, at each time t a linear Gaussian observation $y_t \\in R^N$, where N is the dimensionality of the observation space, is generated from the corresponding latent continuous state,\n$yt = C_{St}x_t + c_{St+1} + f_t, f_t \\sim^{iid} N (0, S_{St}),$ (3)"}, {"title": "2.2 P\u00f3lya-gamma Augmentation", "content": "An approach that has previously been used to improve the capabilities of SLDS (Linderman et al., 2017; Nassar et al., 2019) involved incorporating a change in the probabilities of the switch based on the latent state. In other words, our goal is to have $p(s_t | s_{t-1}, x_{t-1}) \\neq p(s_t | s_{t-1})$. To achieve this, a mapping from continuous to discrete space is required. An approach that enables such a mapping is known as P\u00f3lya-gamma augmentation. The main result in (Polson et al., 2013) shows that binomial probabilities can be expressed as a combination of Gaussians using a P\u00f3lya-gamma distribution. The key discovery is based on the integral identity, where b > 0, given by\n$\\frac{(e^\u03a8)^a}{(1+e^\u03a8)^b} = 2^{-b} e^{k\u03a8} \\int_0^{\u221e} e^{-\u03a8w^2/2} p(w) dw,$ (6)\nwhere $k = a - b/2$ and w is a random variable whose pdf PP G(w | b,0) is the density of the P\u00f3lya-gamma distribution, PG(6,0), which does not depend on v, and $\u03a8 \\in R$. This technique entails sampling from a Gaussian distribution for the main parameters and a P\u00f3lya-gamma distribution for the latent variables.\nSuppose the observation of the system at time t follows\n$p (wt+1 | xt) = Bern (\u03c3 (vt)) = \\frac{(ev_t)^{wt+1}}{1+ ev_t}$ (7)\n$vt = R^T x_t + r,$\nwhere Bern is Bernoulli's distribution pmf, o is the logistic function, R\u2208 $R^d \\times x$, r\u2208 R. Then if we introduce the PG auxiliary variables wk,t, conditioning on w1:T, (7) becomes\n$p(Wt | xt, wt) = e^{-(wtvt-2ktvt)}$\n$\\propto N (R^T x_t + r | kt/wt,1/wt)$\nwhere $kt = \\frac{W_t}{2}$."}, {"title": "2.3 Recurrent Models", "content": "SLDS serve as the basis for more sophisticated models, which have been investigated in recent years. Linderman et al. (2017) proposed novel methods for learning Recurrent Switching Linear Dynamical Systems (rSLDS).\nThey used information on position in the latent hyperplane to influence the probability of future hidden states, thus incorporating non-stationarity into the state distribution (namely $p(s_t | s_{t-1}, x_{t-1}) \\neq p(s_t | s_{t-1}))$).\nConsider a logistic regression model from the regressors x \u2208 $R^M$ to a categorical distribution on the discrete variable s \u2208 \\{1, 2, . . ., K\\}, written as\n$s|x \\sim \u03c0_s(v), v_s = Rx+r,$ (8)\nwhere R \u2208 $R^{(K-1) \\times M}$ is a weight matrix and re $R^{K-1}$ is a bias vector. Unlike standard multiclass logistic regression, which uses a softmax link function, we instead use a stick-breaking link function $\u03c0^{SB} : R^{K-1} \u2192 [0,1]^K$, which maps a real vector to a normalized probability vector via the stick-breaking process.\n$\\pi^{SB}(\u03c5) = \\{\\pi^{SB}_{(1)}(\u03c5),...,\\pi^{SB}_{(K)}(\u03c5)\\}$\n$\\pi^{SB}_{(k)}(\u03c5) = \\sigma (\u03c5^{(K)}_s) \\prod_{j<k} (1 \u2013 \\sigma (\u03c5^{(K)}_s)),$\n(9)\nfor k = 1, 2, . . ., K \u2212 1 and $\\pi^{SB}_{(k)}(v) = \\prod_{j=1}^{K-1} (1 \u2013 \\sigma (\u03c5^{(k)}_s))$, where \u03c3(x) = $\\frac{ex}{1+e}$ denotes the logistic function. The probability mass function p(st | xt-1) is\n$p(St | Xt - 1) = \\prod_{k=1}^{K} \u03c3 (\u03c5)[St=k] (\u2212\u03c5)[St>k] ,$ (10)\nwhere I[] denotes an indicator function that takes value 1 when its argument is true and 0 otherwise. If we use this regression model as a likelihood p(s | x) with a Gaussian prior density p(x), the posterior density p(x | s) is non-Gaussian and does not admit easy Bayesian updating. However, Linderman et al. (2015) show how to introduce P\u00f3lya-gamma auxiliary variables w = ${\\{w_k\\}_{k=1}^{K-1}}$ so that the conditional density p(x | s, w) becomes Gaussian. In particular, by choosing $w_k | x, s \\sim PG (I[s > k], v)$, we have,\nx | s,\u03c9 ~ \u039d (\u03a9\u00af\u00b9\u03ba,\u03a9\u00af\u00b9),\nwhere the mean vector $\u03a9^{-1}K$ and covariance matrix$\\,^{-1}$ are determined by\n\u03a9 = diag(w), \u03bak = I[s = k] - \\sum_{j} I[s \u2265 k]\nThus instantiating these auxiliary variables in a Gibbs sampler or variational mean field inference algorithm enables efficient block updates while preserving the same marginal posterior distribution p(x | s)."}, {"title": "2.4 Explicit Duration Modeling", "content": "One may observe that, using classic Markovian transitions, the duration time of a single state always follows the geometric distribution, and that may not be true for real-life data.\nExplicit Duration Switching Dynamical Systems are a family of models that introduce additional random variables to explicitly model the switch duration distribution. Explicit duration variables have been applied to both HMMs (Dewar et al., 2012; Yu, 2016) and SDS with continuous states (Chiappa, 2014; Chiappa and Peters, 2010; Oh et al., 2008). Several methods have been proposed in the literature for modeling the duration of the switch, for example, using decreasing or increasing count and duration indicator variables (Chiappa, 2014; Chiappa and Peters, 2010).\nIn most of the cases of SDS with an explicit state duration variable, the duration variable is modeled using a categorical variable with support \\{1,2,..., Dmax\\} (Ansari et al., 2021; Chiappa, 2014; Chiappa and Peters, 2010)."}, {"title": "3 Problem Statement", "content": "In real-world applications, the data often comes from many independent short trials. This may happen because the data acquisition process is costly (for instance involves life animals) or the phenomenon we are trying to investigate is bound in time (for instance, sleep).\nIn real-life scenarios we cannot always control whether the measurements are taken in the way, which allows for unbiased representation of the whole spatiotemporal phenomenon. For this reason, some regimes in the data can be underrepresented.\nIn models without explicit duration variables, self-persistence is strictly related to transition probability, implicitly following the geometric distribution.\nOn the other hand, numerous natural temporal phenomena display consistent patterns in the duration of a specific model or regime. In these instances, the standard SLDS model fails to accurately capture the regularity of the data. A prime example of this is the honeybee dance, where a dancing bee strives to remain in the waggle regime for a specific period to effectively convey a message. In such cases, it is evident that the actual duration deviates from a geometric distribution.\nrSLDS make transition probability dependent on the position, thus self-persistence no longer follows geometric distribution. However, first-order Markovian recurrence does not adequately address long-term time-dependent switching.\nTo overcome the problems with rSLDS not capturing the state duration regularity in the data, we introduce recurrent explicit duration variables."}, {"title": "3.1 Model Formulation", "content": "Let's consider the graphical model (presented in the Figure 1) defined as\n\u0440 (\u04231:T, X1:T, 81:T, d1:T) = p (y1 | X1) p (X1 | 81)\n\u2022p (d1|81) p (81) \u041f\u0440 (\u0423\n\u041f\u0420 (\u0423\u043a\nt=2\nXt, St) p (Xt\n| Xt-1, St) p (St\n| St-1, dt, xt-1)p(dt | St, dt-1, Xt-1)\n(11)\nWe call this model Recurrent Explicit Duration Switching Linear Dynamical System (REDSLDS).\nAnalogously to regular SLDS, the probabilities follow distributions defined below\np (81) = Cat (s1; \u03c0\u03bf)\np (d1|81) = Durs1, \u03bcinit (d1)\np(x1 | 81) = N (x1; \u03bcinit, Dinit)\nSt St\np (xt | Xt\u22121, St) = N (xt; As\u2081xt-1 + ast, Qst)\np (yt | xt, St) = N (yt; Cs\u2081xt + cst, Sst),\n(12)\nwhere \u03c0\u03bf is a vector of initial state probabilities, Cat is categorical pmf, Dur is duration pmf,\ninit := As\u2081\u03bcinit and init := A81A81A+Q1\nS1 S1 S1\nThe transition of duration d\u2081 and state st variables is defined as\np (dt\nSt, dt-1, Xt-1) =\n\u03b4(dt-1-1) if dt-1>1\nDurst,xt-1(dt) if dt-1 = 1\n(13)\nwhere Durst,xt-1 is duration distribution in state st,\n\u03b4(x) is Dirac's delta, and\n\u03b4(x)\np(St\nSt-1, dt-1, Xt-1) =\n\u03c0\u03c2\u0392(\u03c5)\nif dt-1>1\nif dt-1 = 1\n(14)\nwhere v = R_1xt-1+rst-1 and R\u2208 R(K\u22121)\u00d7M is a weight matrix and rk \u2208 RDmax-1 is a bias vector.\nThis model formulation allows us to efficiently represent state durations. An alternative way to model explicit durations (employed, for example, by Ansari et al. (2021); Oh et al. (2008)) is to use increasing counter variables ct instead of decreasing duration variables dt. However, we believe that in our case, the duration variables allowed for easier model formulation.\nFor a comparison of these two modeling approaches, refer to Chiappa (2014).\nIn our case,\nDurst,xt-1 = SB(VP),\nv=Rxt-1 + rst,\n(15)\nwhere RPE R(Dmax-1)\u00d7M is a weight matrix and rk \u2208 RDmax-1 is a bias vector.\nWe utilized the matrix normal inverse Wishart (MNIW(M, V, S, n)) prior to use for autoregressive dynamics and emissions in each of the experiments. A detailed explanation of the MNIW prior can be found in (Fox et al., 2011). A detailed description of the priors for each of the experiments is provided in the Appendix E."}, {"title": "3.2 Related Work", "content": "Our model is an extension of the rSLDS model proposed by Linderman et al. (2017).\nWe note here that it can be regarded as a simpler variant of the REDSDS model proposed by Ansari et al. (2021). However, their model uses deep neural networks instead of liner models as transition functions.\nThere are several benefits of our modeling scheme: we don't have to approximate posterior, we can use Markov Chain Monte Carlo (MCMC) inference to sample it directly, we do not report problem with \"state-collapse\" as did the models using variational inference (Dong et al., 2020; Ansari et al., 2021). Our model allows us not only to perform segmentation and prediction, but we are still able to perform smoothing and filtering (see, for example, (Fox et al., 2011) or algorithms in the Appendix C)."}, {"title": "4 Experiments", "content": "To assess our model's effectiveness with limited independent observations, we tested it on three benchmarks: a simulated NASCAR\u00ae dataset (from Linderman et al. (2017)), and two real-world datasets on honeybee dances and mouse behavior. Labels in all experiments were obtained in a fully self-supervised manner.\nNASCAR\u00ae provides a controlled environment to compare REDSLDS with traditional rSLDS models. The data were sampled to mimic independent samples without clear process periodicity.\nThe honeybee dataset (Oh et al., 2008) features complex dance patterns, making it suitable for testing segmentation in rapid, unstable movements. This aligns with our assumption of independent trials from a shared process.\nThe mouse behavior dataset (Batty et al., 2019) presents a high-dimensional challenge, allowing us to test REDSLDS's ability to segment complex behaviors.\nThese datasets are widely used in the community, including by Linderman et al. (2017); Nassar et al. (2019); Ansari et al. (2021); Lee et al. (2023); Zhou et al. (2021).\nAs a baseline, we use rSLDS. Appendix D provides results for SLDS and EDSLDS.\nThe Python3 implementation of the prototype code utilized the NumPy and SciPy libraries.\nInitialization Proper initialization of SLDS-based models is still an open area of research and there are many different schemes (Linderman et al., 2017; Nassar et al., 2019; S\u0142upi\u0144ski and Lipi\u0144ski, 2024). We initialize the latent continuous state using principal component analysis (PCA), similar to Linderman et al. (2017). Then we fit five Autoregressive Hidden Markov Models (ARHMMs). Of them, we choose the one with the highest log-likelihood. The states decoded by this"}, {"title": "4.1 NASCAR\u00ae", "content": "We begin with a straightforward illustration, where the dynamics exhibit oval shapes resembling those of a stock car on a NASCAR\u00ae track(Linderman et al., 2017) (refer to Figure 4). The dynamics is determined by four distinct states, st \u2208 {1,...,4}, which govern a continuous latent state in two dimensions, xt \u2208 R2. The observations yt \u2208 R10, are obtained by linearly projecting the latent state and introducing Gaussian noise.\nThere is evidence in the literature that rSLDS models perform effectively on this particular kind of data (Linderman et al., 2017; Nassar et al., 2019). However, we are interested in investigating the performance of SLDS-based models when applied to independent sequences originating from the same distribution. In order to accomplish this, we generated 10 independent runs of NASCAR\u00ae with 12000 sample points. We split each run into S\u2208 \\{5,10,15,20\\} chunks (multiples of chunks were chosen arbitrarily, there is no reason it should not work for 3, 6, 12, 24 or 2, 5, 12, 20). From"}, {"title": "4.2 Dancing Bee", "content": "We utilized the publicly accessible dancing bee dataset (Oh et al., 2008), which is known for its complex nature and has previously been examined in the realm of time series segmentation.\nThe data set contains information on the movements of six honeybees as they perform the waggle dance. This includes their coordinates in a 2D space and the angle at which they are heading at each time interval.\nCommunication of food sources among honey bees is accomplished through a series of dances performed within the hive. These dances involve specific movements, such as waggle, turn right, and turn left. The waggle dance involves the bee walking in a straight line"}, {"title": "4.3 BehaveNet", "content": "We also used our model on a publicly available dataset of mouse behavior (Batty et al., 2019). In this particular experiment, a mouse with a fixed head position performed a visual decision task while neural activity in the dorsal cortex was recorded using wide-field calcium imaging. We focused solely on the behavior video data, which consisted of gray-scale video frames with dimensions of 128x128 pixels. The behavior video was captured using two cameras, one providing a side view and the other providing a bottom view. Due to the high dimensionality of the behavior video, we directly utilized the dimension reduction results from a previous study. These results consisted of 9-dimensional continuous variables estimated using a convolutional autoencoder. As there are no labels in this dataset, we treat it as a qualitative experiment. Every model was sampled using 5000 Gibbs iterations.\nFor this experiment, we only used rSLDS and REDSLDS, using initialization scheme 2. Of the whole dataset, we sample ten sequences with uniform probability. We standardize the measurements.\nThe use of ARHMM for segmentation in this data set is susceptible to over-segmentation, a concern that has recently been tackled by (Costacurta et al., 2022). In their study, the authors introduced auxiliary variables that added additional noise to the conventional ARHMM model.\nIn models based on SLDS, we introduce extra noise to the autoregression process through the emission layer.\nThe segmentation obtained by the rSLDS model is shown in Figure 6a. This model achieved the highest log-likelihood of -13420.80. It can be seen from the figure that most of the data converges to a single state."}, {"title": "5 Conclusions", "content": "In this paper, we proposed REDSLDS, being an extension of rSLDS with recurrent explicit duration variables, as well as the inference and learning scheme with P\u00f3lya-gamma augmentation. Adding time-dependent switching supported state-dependent switching from the original rSLDS and led to a significant improvement of the learning process. As computational experiments showed, our model significantly outperformed the segmentation capabilities of the original rSLDS on three benchmark datasets, usually used in the evaluation of the SLDS models.\nThe primary novelty presented in this paper is the incorporation of recurrent explicit duration random variables in (r)SLDS. The utility of these variables was previously demonstrated by Ansari et al. (2021), who extended SNLDS by Dong et al. (2020) in a manner similar to our extension of REDSLDS.\nIn the Appendix B, we share our computations and details of the message passing scheme, both for REDSLDS and rSLDS, to facilitate easy reproduction of our work.\nWe also examined the impact of the length of the sampled spatiotemporal data on the learning process in switching dynamical models, which we believe has not been done before.\nWe evaluated our model using three well-established benchmarks in the community and demonstrated its effectiveness.\nFuture work may involve evaluation on more complex datasets. Since our model has shown an improvement in performance over rSLDS in the segmentation task, it is worth investigating applications of this model in tasks where rSLDS or SLDS are applied.\nThose may involve neural data modeling (Zoltowski et al., 2020; Osuna-Orozco and Santacruz, 2023; Song and Shanechi, 2023; Jiang and Rao, 2024; Bush and Ramirez, 2024), early warning of systemic banking crises (Dabrowski et al., 2016), or monitoring of neonatal conditions (Stanculescu et al., 2014)."}, {"title": "A Conditional posteriors", "content": "The structure of the model allows for closed-form conditional posterior distributions that are easy to sample from. For clarity, the conditional posterior distributions for the REDSLDS are given below:\nThe linear dynamic parameters (Ak, ak) and state variance Qk of a mode k are conjugate with a Matrix Normal Inverse Wishart (MNIW) prior\nT\np((Ak, ak), Qk | X0:T, 81:T) \u221d p ((Ak, bk), Qk) [N (Xt | Xt-1+ Asxt-1\nt=1\n+ ast, Qst)\nI[St=k]\n(16)\nSt\nIf we assume the observation model is linear and with additive white Gaussian noise then the emission parameters \u03a8 {(Ck, ck), Sk} are also conjugate with a MNIW prior\nT\np((Cst, cst), Sst | X1:T, Y1:T, S1:T) & p((Cs\u2081, cs\u2081), Sst) [N (Yt | Cs\u2081xt + Cst, Sst)][$t=k]\n(17)\nt=1\nSt\nWe can express the posterior over the hyperplane of a state as:\np\n{\n{(R\nS\nS\nk,rk)\nt, wt, 1/wt)\nI[St=k]\n,\n(18)\nt=1\nwhere Kk,t = [[St = k] - I[St \u2265 k]. Augmenting the model with P\u00f3lya-gamma random variables allows for the posterior to be conditionally Gaussian under a Gaussian prior.\nAnalogously,\np\n(19)\nS1:T, W, 1:T\nwhere kd,t = [[dt = d] \u2013 [[dt \u2265 d].\nThe conditional posterior of the P\u00f3lya-gamma random variables are also P\u00f3lya-gamma:\nWn,t | St, (Rn,rn), xt\u22121 ~ PG (1, vn,t).\n(20)"}, {"title": "B Message passing for recurrent connections", "content": "To still use Kalman filtering, we have to compute backward messages incorporating information from sampled durations and states. Assume mt+1,t (xt) x N\u22121 (xt; 0t+1,t, At+1,t), where N-1(x; 0, 1) denotes a Gaussian distribution on x in information form with mean \u03bc = \u039b\u00af\u00b9\u03b8 and covariance \u2211 = \u039b\u00af\u00b9. Given a fixed mode sequence S1:T and duration sequence S1:T we simply have a time-varying linear dynamic system. The backward messages can be recursively defined by\nmt,t-1 (Xt-1) \u03b1\nXt\nP(Xt\nXt-1, St) P (Yt | Xt, St) p (dt | Xt\u22121, St, dt-1) p (St | Xt, St\u22121, dt-1) Mt+1,t (xt) dxt (21)\nThe transition density may be expressed as\np(xt\n\u03b1 exp{\nt-1\nSt\nXt-1, St) x exp {- (xt - Asxt-1 - ast) Q1 (xt - Asxt-1 - ast)}\n1\nT\nT\n1\nT\nX][ATQ-AS ATQA][XT12] [\nSt St-1\n2\nXt Xt\nQ-1 Ast Q-1\nSt\nX + [ATQ-last] + [ast] }\nSt\nSt\n(22)"}, {"title": "C Full learning pseudocode", "content": "To make it easier to understand the whole learning and inference procedure, we present pseudocode. Algorithm 2 and Algorithm 3 are algorithms adapted from Fox et al. (2011) to have a common notation with our paper.\nLet us define zt := (st, dt). Let's take note that we compute forward probability as\nAt (zt) = P(zt, 1:t, X1:t)\n= \u2211 p (zt | zt-1) p (yt, xt | zt) at-1 (zt-1).\n(37)\nZt-1\nThe procedure of sampling pseudo-observations is presented in the Algorithm 2. It uses the messages computed using 3. The complete model fitting procedure is given by the Algorithm 1."}, {"title": "D Extended experiment discussion", "content": "To assess our model's performance under conditions with limited independent observations, we tested it on three distinct benchmarks. These benchmarks included a simulated dataset, NASCAR\u00ae (simulated by Linderman et al. (2017)), and two publicly available real-world datasets.\nThe first real-world dataset is based on honeybee dance patterns, and the second records mouse behavior. In all experiments, labels were obtained in a fully self-supervised manner.\nNASCAR\u00ae (Linderman et al., 2017) serves as a controlled setting to illustrate the model's competence in capturing and segmenting dynamic behaviors into distinct states, facilitating a comparison with traditional rSLDS models and demonstrating REDSLDS's superior segmentation abilities. As mentioned in the main paper, to increase the challenge, the data were sampled in a manner that mimics taking independent samples without clear insights into process periodicity.\nThe honeybee dance dataset (Oh et al., 2008) was selected for its unique behavioral patterns, where honeybees convey food locations through complex dance movements. This dataset poses a challenge in accurately segmenting the rapid and unstable movements typical of the waggle phase of the dance. It aligns with the assumption that the data consist of independent trials from a common process, making it ideal for testing the model's effectiveness in real-world scenarios with distinct patterned behaviors.\nThe mouse behavior dataset from the BehaveNet study (Batty et al., 2019) is particularly demanding due to its high dimensionality and the intricate behavioral patterns observed during tasks. Selecting this dataset allows examination of REDSLDS's ability to segment and interpret complex behavioral data, underscoring its relevance for high-dimensional data scenarios.\nWe have selected three notable benchmark datasets for our analysis. These datasets are prevalently utilized within the research community and have been employed in various studies, including those by Linderman et al. (2017); Nassar et al. (2019); Ansari et al. (2021); Lee et al. (2023); Zhou et al. (2021). For performance baselines in our study, we include the SLDS, rSLDS, and EDSLDS models. It is important to highlight that the EDSLDS"}, {"title": "D.1 Dancing Bee", "content": "We leveraged the publicly available dancing bee dataset (Oh et al., 2008), which is renowned for its intricate and complex characteristics and has been previously scrutinized in the context of time series segmentation. This dataset encompasses detailed information regarding the movements of six individual honeybees as they execute the waggle dance. Specifically, it includes the bees' coordinates within a two-dimensional plane, as well as the angles at which they are oriented at each time step.\nThe communication of food sources among honeybees is facilitated through a series of dances that occur within the hive. These dances incorporate particular movement patterns, such as the waggle, right turn, and left turn. The waggle dance, for instance, involves a bee traversing in a straight line while vigorously shaking its body from side to side. In contrast, the turning dances are characterized by the bee rotating either clockwise or counterclockwise. The data recorded for this analysis is represented as y\u2081 = [ cos (0t), sin (0), Xt, Yt"}]}