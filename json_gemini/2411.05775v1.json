{"title": "Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?", "authors": ["Veronica Chatrath", "Marcelo Lotif", "Shaina Raza"], "abstract": "Political misinformation poses significant challenges to democratic processes, shaping public opinion and trust in media. Manual fact-checking methods face issues of scalability and annotator bias, while machine learning models require large, costly labelled datasets. This study investigates the use of state-of-the-art large language models (LLMs) as reliable annotators for detecting political factuality in news articles. Using open-source LLMs, we create a politically diverse dataset, labelled for bias through LLM-generated annotations. These annotations are validated by human experts and further evaluated by LLM-based judges to assess the accuracy and reliability of the annotations. Our approach offers a scalable and robust alternative to traditional fact-checking, enhancing transparency and public trust in media.", "sections": [{"title": "1 Introduction", "content": "Political bias is the tendency to favour or oppose certain political ideologies, parties, or candidates in a way that skews impartiality, posing significant challenges in today's digital landscape [Bang et al., 2024]. While political bias refers to a large spectrum of topics like disinformation (false information spread intentionally to deceive) or misinformation (false information spread unknowingly) [Micallef et al., 2022], this study is concentrated on distinguishing factual inaccuracies within politics. Traditional fact-checking approaches are not scalable given the vast amount of online information. Manual fact-checking methods, such as expert annotation and crowdsourcing, are viable alternatives but are often costly and time-consuming. Crowdsourcing lacks expert oversight and can lead to inconsistencies due to the varying expertise of contributors [Liu et al., 2022]. These limitations highlight the need for efficient and scalable fact-checking approaches."}, {"title": "2 Related Works", "content": "Political Disinformation Political disinformation involves framing or altering information to make a political position or candidate appear more attractive, or unattractive [Barman et al., 2024], and poses significant challenges, such as bias or toxicity [Raza et al., 2024a]. In response, LLMs have emerged as a promising method to combat political disinformation by automating the detection and annotation of deceptive content, reducing the reliance on costly and time-consuming manual processes. Recent studies [He et al., 2024, Alizadeh et al., 2024, Kim et al., 2024] highlight the potential of open-source LLMs in detecting and labelling disinformation. These models are shown to improve the efficiency and accuracy of disinformation detection, strengthening defences within the digital information ecosystem.\nLLM-as-an-Annotator Recent advances have demonstrated the potential of LLMs like OpenAI models 2, , such as GPT-3.5, GPT-4 and recently GPT-4o as effective annotators for various NLP tasks [Tan et al., 2024]. These models can annotate data for classification and entity recognition through prompting methods. To maximize their utility, LLMs can be deployed within active learning loops [Zhang et al., 2023], leveraging vast amounts of unlabelled data. Alignment tuning can further refine LLM annotations to match human preferences [Zhao et al., 2023], mitigating potential biases.\nStudies have shown that LLM-based annotations can match or exceed human performance across diverse domains, including social media analysis [Huang et al., 2023], computational science [Ziems et al., 2024], and medical information extraction [Goel et al., 2023]. Comparative analyses between LLMs and human annotators further highlight their potential [Gilardi et al., 2023a, He et al., 2024, Pavlovic and Poesio, 2024]. Building on these findings, our work employs LLMs as annotators to address political misinformation, an area yet to be fully explored in this context.\nLLM-as-a-Judge Various LLMs such as GPT-3.5 [OpenAI, 2022] and GPT-4 [OpenAI, 2023] are increasingly utilized as evaluators, or judges, to ensure outputs align with human values [Zheng et al., 2024]. LLMs can assess the quality of model outputs against specific criteria such as accuracy, toxicity, and relevance [Dubois et al., 2023, Zhou et al., 2023], leveraging methods like asking for correctness or agreement with human annotations in a controlled experiment. In one LLM evaluation"}, {"title": "3 Methodology", "content": "3.1 Dataset Construction\nWe curated a dataset of news articles related to the North American political landscape, covering the period from May 6, 2023, to May 6, 2024. Data collection was conducted using automated scraping tools, including Newspaper and Selenium, with Google RSS as the main source. To ensure comprehensive coverage of electoral topics, a sample of 6,100 articles was selected based on the 26 topics and 16 sources listed in Appendix A.3.\n3.2 LLM-as-an-Annotator\nWe employ a suite of open-source LLMs, recognized for their robustness and accuracy in state-of-the-art applications [Li et al., 2023]. These models include Llama-3-8B-Instruct [Touvron et al., 2023], Llama-3.1-8B- Instruct, Mistral-7B-Instruct-v0.3 [Jiang et al., 2023], Gemma-2-9b-Instruct [Team et al., 2024], and Phi-3-medium-128k-Instruct [Microsoft, 2024]. The annotation task involves labeling each news summary article as either factually correct or factually incorrect. The LLMs are employed in both zero-shot and five-shot settings. The following is a snippet of the prompt used for annotation, with the entire prompt listed in Appendix A.1."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nSettings Inference was run on a single A40 GPU with 4 CPU cores. For 6,000 samples across 5 LLMs, the process took about 16.67 hours, consuming 9.34 kWh and emitting 4.2 kgCO2e [Dodge et al., 2022]. This carbon footprint is relatively low compared to fine-tuning. Open-source versions of Llama, Mistral, Gemma, and Phi models were used. GPT-40-mini was employed as a judge via the OpenAI API, with total approximate experiment costs of $2 USD on sample data.\nEvaluation Metrics Two approaches are used to evaluate the annotations: 1) Reference-based evaluation involves employing standard classification metrics by comparing the precision, recall, and accuracy of the LLM-generated annotations with the gold-standard ground truth labels, inspired by similar works [Gilardi et al., 2023a]. 2) LLM-as-a-judge-based evaluations, inspired by [Zheng et al., 2024], are calculated as the percentage of times the LLM judge's assessment aligns with the LLM-based annotations, also known as Agreement Rate. This experiment is performed on 500 samples."}, {"title": "4.2 Comparison with Gold Data", "content": "We compare various LLMs as annotators against the gold data. This data was formed by a review team consisting of twelve volunteer members representing a diverse range of disciplines and demographics. Additional information on the composition of the gold data can be found in Appendix A.5. Table 1 shows the performance of the LLMs in an annotator role. We observe that Llama-3-8B-Instruct (5-shot) outperforms all other models. Additionally, five-shot experiments outperform all zero-shot experiments, answering our second research question. The inference times for each run can be seen in Appendix A.4."}, {"title": "4.3 LLM-based Evaluation Results", "content": ""}, {"title": "5 Discussion", "content": "Practical Impact Our framework significantly improves the labeling process in NLP. By using LLMs for initial annotation, incorporating human oversight, and leveraging stronger LLMs for evaluation, we achieve a balanced solution that combines AI efficiency with human accuracy. This approach reduces costs and time compared to fully manual labeling, while maintaining high quality. It is practical for real-world applications like labeling news articles, analyzing social media, and processing customer feedback. For elections, this method can help analyze political content, fact-check claims, and track misinformation trends at scale.\nLimitations and Future Work Our findings demonstrate LLMs' effectiveness as annotators, though potential biases in the annotation remain a concern, warranting further investigation [Das et al., 2024]. Research shows LLM judges exhibit their own biases [Chen et al., 2024], impacting assessment reliability. They often favor their own responses [Zhao et al., 2021] and are susceptible to order and length biases [Li et al., 2024]. Prompt adjustments can alter LLM judgments, necessitating the use of multiple judges, including humans and various LLM models.\nDespite controlled temperatures and carefully designed prompts, annotation responses and LLM judgments exhibit inherent stochasticity. These models rely on internal confidence mechanisms rather"}, {"title": "6 Conclusion", "content": "In conclusion, this study highlights the potential of open-source LLMs as effective annotators for political misinformation detection, demonstrating their ability to produce annotations that align closely with human judgments. The variations in evaluation between different LLM judges highlight the need for multiple evaluation methods to comprehensively assess performance. While LLMs provide a promising and cost-effective approach for annotation, integrating them with human oversight enhances the reliability of the results. This framework has significant implications for improving labeling processes in NLP applications, especially in analyzing political content for sentiment analysis and misinformation detection. Future work should explore multi-modal approaches that incorporate visual data to further enhance political misinformation annotation and combat misinformation effectively."}, {"title": "7 Social Impacts Statement", "content": "Bias is inherently subjective, and its definitions widely vary across different disciplines. While our work addresses misinformation for news media, it is not intended to cover the full range of biases that exist globally. We have made possible efforts to mitigate bias, but acknowledge that our approach may have its limitations. Additionally, while our methods aim to identify bias and ensure accurate fact-checking, it is important that the data and techniques we release are used responsibly and ethically. The same data, if misused, could unintentionally fuel the spread of hate, which is not our intention. We have employed LLMs as annotators and evaluators, with human oversight included in the process. However, there is still a possibility of inherent biases in LLMs, and we recommend cautious use of both the data and methods developed. Lastly, while we focus on addressing misinformation that affects certain groups, our intent is not to exacerbate the sentiments of those already impacted by systematic inequalities."}, {"title": "A Appendix", "content": "A.1 Prompt for Annotation\nThe full prompt used for the annotation phase, as described in Section 3.2."}, {"title": "A.2 LLM-as-a-Judge Prompt", "content": "Prompt used by the LLM Judge"}]}