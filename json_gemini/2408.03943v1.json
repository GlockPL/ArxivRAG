{"title": "Building Machines that Learn and Think with People", "authors": ["Katherine M. Collins", "Ilia Sucholutsky", "Umang Bhatt", "Kartik Chandra", "Lionel Wong", "Mina Lee", "Cedegao E. Zhang", "Tan Zhi-Xuant", "Mark Ho", "Vikash Mansinghka", "Adrian Weller", "Joshua B. Tenenbaum", "Thomas L. Griffiths"], "abstract": "What do we want from machine intelligence? We envision machines that are not just\ntools for thought, but partners in thought: reasonable, insightful, knowledgeable, reliable,\nand trustworthy systems that think with us. Current artificial intelligence (AI) systems\nsatisfy some of these criteria, some of the time. In this Perspective, we show how the science\nof collaborative cognition can be put to work to engineer systems that really can be called\n\"thought partners,\" systems built to meet our expectations and complement our limitations.\nWe lay out several modes of collaborative thought in which humans and AI thought partners\ncan engage and propose desiderata for human-compatible thought partnerships. Drawing\non motifs from computational cognitive science, we motivate an alternative scaling path for\nthe design of thought partners and ecosystems around their use through a Bayesian lens,\nwhereby the partners we construct actively build and reason over models of the human and\nworld.", "sections": [{"title": "Introduction", "content": "Computers have long been seen as tools for thought. Steve Jobs called computers \u201cbicycles for\nthe mind\": tools that dramatically increase the efficiency, productivity, and joy of thinking. Now,\nthirty years later, this metaphor is beginning to change. Computer systems are increasingly\nreferred to not as vehicles but as \u201ccopilots\u201d 1,2: we have moved from designing tools for thought\nto actual partners in thought.\n\nThe current wave of AI technologies, particularly language models, have catalyzed this\ntransition. Users no longer have to know how to write code to engage intimately with computers;\nwe can now interface through the medium of natural language. Humans already think alone\nand together, at least communicated often through the medium of language. We long have\nfrom developing new modes of thinking through questioning and debate to teaching and learning\nthrough language. The apparent power of these new systems - getting closer to the kind of\nartificial intelligence (AI) imagined in the field's early days 4-9 as well as challenges faced by\nthe current iterations of such systems\ninvites us to think about what it will take to build"}, {"title": "What are Thought Partners?", "content": "When we think, we draw coherent inferences, make predictions, and act on these predictions\nfrom assessing what birthday present to gift a treasured friend, to formulating a new scientific\nhypothesis and experiment plan to evaluate a theory. We flexibly draw on prior knowledge and\nupdate our beliefs through experience (as we discuss below). We not only solve problems, but\nimagine new ones37. And we think together. For generations, humans have discussed and debated\nideas, and developed ecosystems to disseminate such thoughts to new audiences. Much scientific\ninnovation has come through collaboration, where advances are frequently fueled by engaging\nwith diverse partners who offer new ideas yet share our values 38."}, {"title": "Modes of Collaborative Thought", "content": "As an illustration of the many ways that people and machines might think with each other, we\nhighlight a few modes of collaborative thought (Table 1). This set of modes, partly inspired by\ncharacterizations of thinking and reasoning in psychology 39,40, are not meant to be comprehensive\nof all aspects of thought. Rather, we see these modes as ripe for the further development of AI\nthought partners."}, {"title": "Example Domains", "content": "We next outline a few diverse domains in which the development of AI thought partners able to\ntruly collaborate with humans (Figure 1) may be particularly valuable. We highlight common\ncomputational challenges that arise when considering what effective partnership might look like\nin each domain, foreshadowing our proposed desiderata. We later return to these case studies\nwith concrete human-centric thought partner instantiations.\nThought Partners for Programming. Programming is a cognitively-demanding activity that\nrequires gaining fluency in translating human intentions into formal, machine-interpretable\nlanguages. It is no surprise that decades of effort have gone into designing tools to help people\nprogram 41-45. New \"programming assistant\" tools like GitHub Copilot have rapidly gained\nenormous popularity and attention; however, these tools are often unreliable 46-48, , e.g., failing\nto understand users' intentions 49 and generating bugs that may be particularly risky alongside\nbeginner programmers 50. Programming involves much more than just accurate in-line code\nsuggestions - which, at the time of writing, GitHub Copilot specializes in. Humans plan abstract,\nstructural decisions and collaboratively learn, and need partners who can answer our questions\nlike why code behaves as it does, or fails to work. A good collaborative programming partner\nseeks to understand not only the programming language, but also their fellow programmer,\ninferring and reasoning about our overarching intentions, and adapting to both what we do and\ndo not know.\nThought Partners for Embodied Assistance. Ensuring embodied agents can form accurate and\nphysically-realizable plans is foundational for effective assistance we can trust - from guessing\nwhat a friend wants when we help them cook51, to working with someone with different physical\nabilities 52, or carrying out a high-stakes search-and-rescue mission 53. While much current\nresearch on embodied AI and assistive robots focuses on learning specific skills or following simple\ninstructions 54-56, evaluations suggest that even state-of-the-art language models fine-tuned on\nextensive human feedback continue to struggle with tasks that require reliable, effective planning\ntowards novel goals 57,58. Instead, ideal assistive partners understand our actions, words, and\ninstructions as expressions of goals, beliefs, and intentions 59-61 that are grounded in physical\npossibilities 62, while also understanding that these can be shared across multiple minds 63-65.\nIn addition, effective partners account for each others' limitations in perception, planning, and\nworld modeling, correcting for possible mistakes 66,67, and acting so as to make their intentions\nmore legible 68,69.\nThought Partners for Storytelling. Another domain in which we may want thought partners\nis storytelling \u2013 for writers, filmmakers, and even scientists. Storytelling is a complex, iterative\ncognitive process 70,71 with substantial opportunities for thought partners to collaboratively ideate\nand create with humans from helping brainstorm new ideas, generate storylines, and improve\ntheir writing style and tone 72-77. For this process to be productive, a thought partner needs\nto understand more than just our authorial intentions and dispositions - they also need to\nunderstand the audience we are speaking to (that is, to understand the social world), including\naudience expectations and likely interpretations of the stories we are crafting for them.\nThought Partners for Medicine. Doctors need to sensemake, plan, deliberate, and continually\nlearn in the face of new medical evidence. A primary care doctor is not unlike Sherlock Holmes\ncolating and integrating disparate bits of evidence with their prior beliefs to make decisions"}, {"title": "Desiderata", "content": "What then do we want from thought partners? There are many criteria for tools for thought\nthat are of course relevant: efficiency, accuracy, robustness, fairness, cost, scalability, etc. But\nthe domains above illuminate that what is distinctive about a thought partner is its relationship\nto the user 89. Looking to ideas the behavioral sciences motivates three desiderata to guide the\ndesign of human-centered thought partners:\n\n1.  You understand me: We would like our thought partners to understand our goals,\nplans, (possibly false) beliefs, and resource limitations, taking into account what they have\nobserved of us in the past and present in order to best collaborate with us in the future 90,91.\nFor example, a thought partner should adaptively change strategies when working with an\nexpert, layperson, or child, meeting us where we are.\n\n2.  I understand you: We would like our thought partners to act in a way that is legible to\nus 68,92, and communicate with us in the way we intuitively understand 93-95.\n\n3.  We understand the world: We would like our thought partners to be tethered to\nreality 96. This means being accurate and knowledgeable, but also working with a shared\nrepresentation of the world, domain, or task 97-99. Further, our use of 'we' emphasizes that\nthought partnerships are fundamentally about synergy, moving beyond the sum of its parts."}, {"title": "Engineering Human-Centered Thought Partners", "content": "Our core proposal is that our three desiderata can be engineered explicitly, building on theoretical\nmotifs from computational cognitive science and cognitively-informed AI (summarized in Table\n2), rather than left as emergent and potentially brittle properties arising implicitly in systems\ntrained for other ends 20. Here, we articulate a framework for engineering thought partners\ndesigned to robustly and explicitly function as cooperative, collaborative actors. Humans are\nfar from homogeneous, perfectly rational oracles, nor are we so unpredictable that it is hopeless\nto model human behavior. We argue that models that explain human cognition and choice as\napproximately optimal solutions given goals and constraints provide an ideal starting point for\ndesigning thought partners, and that a Bayesian formalism provides a probabilistically-sound\ncommon conceptual language that facilitates cross-talk between different disciplines 22,155, 156"}, {"title": "Implementing Our Desiderata", "content": "What does it take to engineer real systems that meet our desiderata? First, we propose that a\nthought partner that understands us should explicitly model its human collaborator as such as\na cooperative agent with structured internal beliefs, knowledge, and goals - and fundamental\nresource limitations. Second, engineering a thought partner that we can understand benefits from\nlooking at how humans model other humans; just as a good human collaborator seeks to learn"}, {"title": "Computational Cognitive Science Motifs", "content": "We now (non-exhaustively) spotlight several key insights about modeling humans, modeling\nhumans modeling humans, and modeling humans modeling the world from computational\ncognitive science - \"motifs\" for reverse engineering the mind (Table 2) \u2013 that we believe can\ninform engineering of human-centered thought partners. While we acknowledge that there are\ncommunities within cognitive science that may disagree with some of these theories, we emphasize\nthat the computational underpinnings of the motifs hold tremendous engineering potential for\nbuilding thought partners in practice."}, {"title": "Scaling Thought Partners via Probabilistic Programming", "content": "If Bayesian thought partners are to reason over models of their human thought partner and the\nworld, these models need to continually evolve as new facts come to light and as the human\nthought partner themselves grows in their expertise, beliefs, and needs. Probabilistic programming\n26 provides one powerful methodology for building, scaling, and performing inference in these\nkinds of rich models. For example, probabilistic programs can be learned from data 116,204,\nand synthesized via LLMs that encode rich priors 16,118,205. Probabilistic programs also enable\nfast approximate inference in world models that cohere with human common-sense knowledge\nand domain expertise 115,206, where the learned models are themselves amenable to modular\ninspection and editing by humans. Modern probabilistic programming languages 25,27,207 offer\nnot just generic inference but programmable inference, that is, they automate the math for\nhybrids of optimization 208,209, dynamic programming 210, and Monte Carlo inference 211. While\nsuch frameworks are certainly not the only methods to handle uncertainty and build effective and\nrobust thought partners, we believe they are one promising and cognitively-grounded approach\nto instantiating thought partners today, as we discuss in our case studies."}, {"title": "Infrastructure around Thought Partners", "content": "The design of systems that learn and think with people necessitates not only careful construction\nof the thought partner (i.e., the machine itself), but also the infrastructure within which human\nand computational thought partners collaborate 157. Questions like \"when and where should a\nhuman be able to engage a computational thought partner to ensure effective and appropriate use?\"\nor \"for a given problem, is the human or computational thought partner better suited to start\nfirst, in light of their respective strengths and weakness, costs of the task at hand, and particular\nmode of thought?\" inform the design of the workflow that surrounds thought partnership. This\nsociotechnical ecosystem may be dictated by external regulations, organizational practices, or\nother principles 73,212-215, and crucially informed by studies of human behavior. For example,"}, {"title": "Case Studies in Engineering Thought Partners", "content": "We now return to the example domains previously introduced and discuss specific case studies\n(depicted in Figure 2). Our goal is to demonstrate the potential benefits of endowing thought\npartners with structured probabilistic models of the human and/or world, and provide a flavor of\nthe kinds of infrastructure questions that may surround them to ensure that the thought partners\nwe build work with people."}, {"title": "Thought Partners for Programming", "content": "We highlighted some visions for effective programming partnerships, such as a partner that can\naddress \"why\" questions. One recent idea, from Chandra et al. 106, is to apply the Bayesian toolkit\nto explain surprising behavior of computer programs in a human-like way. Chandra et al., apply"}, {"title": "Thought Partners for Embodied Assistance", "content": "Recall the challenge of collaboratively planning uncertain tasks, from a search-and-rescue mission\nto everyday cooking, wherein we typically want to infer shared goals and communicative intent\nfrom our partners. This cooperative logic can be modeled in a Bayesian architecture called Coop-\nerative Language-Guided Inverse Plan Search (CLIPS) 35. By modeling humans as cooperative\nplanners who use language to communicate joint plans to achieve their goals 65, CLIPS is able\nto infer those plans and goals from both the actions and instructions of human collaborators.\nThis allows CLIPS to pragmatically follow human instructions, using context to disambiguate\nthe multiple meanings that a request might have, while pro-actively assisting with the goals that\nunderlie the instruction. For example, CLIPS can understand the likely intentions behind an\ninstruction like \"Can you prepare the vegetables while I knead the dough?\", inferring the shared\ngoal of making pizza. These capabilities are made possible by using probabilistic programming\ninfrastructure 25 to unite algorithms for Bayesian inverse planning 33,184 and human-AI align-\nment 51,61,223 with LLMs. In particular, by using LLMs to evaluate the probability of a natural\nlanguage instruction given a possible intention, CLIPS can infer intentions from natural language\nin a coherent Bayesian manner\n\ndemonstrating the power of combining tools from the Bayesian\nthought partner toolkit."}, {"title": "Thought Partners for Storytelling", "content": "Storytelling is about crafting experience. Can we also apply the toolkit to help storytellers design\nexperiences from first principles? Recent work has shown that a system grounded in Bayesian\nToM can predict and even design interventions on the audience's experience of a story 224,225.\nChandra et al. conceive of storytelling as \u201cinverse inverse planning\": that is, starting with human\nsocial cognition, modeled as Bayesian inverse planning 33, and then optimizing narrative events\nto shape the model's inferences over time. They show how a variety of storytelling techniques\nfrom plot twists to stage mime can be expressed in the language of inverse inverse planning\nto create animations that have a desired cognitive effect on viewers. Herein, we also highlight\nthe breadth of thought partners for media beyond language, though the framework does nicely\nsuggest a variety of natural extensions, such as integration into tools for creative writing 72-77."}, {"title": "Thought Partners for Medicine", "content": "Finally, we envision medical thought partners both understand us reasoning about the doctor,\npatient, and care team as agents with goals, beliefs, and worries \u2013 and complement our capabilities,\nintegrating swaths of evidence that exceed our cognitive capacities to inform diagnosis and\ntreatment. While no system yet meets our desiderata for these criteria, we believe a range of\nmotifs and tools from the Bayesian thought partner toolkit here can support the development\nof such systems for collaborative sensemaking and deliberation. We imagine Bayesian thought\npartners that can update their medical world knowledge in light of new insights in biology, e.g.,\nediting a code snippet of the underlying probabilistic world model 16 or growing the representation"}, {"title": "Looking Ahead", "content": "There is much exciting work to be done to characterize when and how to build thought partners\nacross modes of collaborative thought, which can advance the dissemination and creation of\nnew knowledge alongside humans. We next lay out several key challenges for researchers and\ndesigners intent on pursuing a human-centered program of building machines that learn and\nthink with people."}, {"title": "Non-Dyad Settings", "content": "While there is substantial work to be done characterizing the space of possibilities for a single\nhuman and single AI thought partner (\u201cdyadic\"), we envision a future where many humans and\nmany machines engage (\u201cnon-dyadic\"), across roles and specialties in increasingly complex social\nsystems 231, engage in the realm of thought 232-234. Already, researchers are exploring non-dyadic\nversions of many of the modes of thought and case studies laid out above, including collaborative\nlearning with groups of humans accompanied by an AI thought partner 235 and medical robot\ncollision avoidance systems that need to account for multiple humans 236. As in the dyad setting,\nextensions to non-dyadic settings can be bolstered by a deepening understanding of human\nbehavior in groups - expanding the Bayesian thought partner toolkit as is already underway in\nthe study of convention formation 192,237. Looking ahead, citizen science is a promising example\nof the opportunities of creating large networks of humans and thought partners: Zooniverse, a\nlarge-scale galaxy classification crowdsourcing project, serves as a case study for exploring smart\ntask allocation, blending human and machine classifications, and infrastructure changes that\nimpact human participation and performance with outcomes including both iterative scientific\nprogress and serendipitous scientific discovery 238."}, {"title": "Evaluation", "content": "The assessment of thought partners demands a multi-faceted, cross-disciplinary suite of ap-\nproaches. At minimum, the evaluation of AI thought partners must include some element\nof interactivity 239. Recent works have highlighted deficits in static evaluation of foundation\nmodels 15,240, demonstrating the need for considering the interaction process in addition to the\nfinal output, the first-person perspective in addition to the third-party perspective, and notions\nof preference beyond quality. In addition to interactive user studies, we posit that to study\ndifferent kinds of thought partners across modes of collaborative thought would benefit from a\ncontrolled, yet rich, playspace; games provide one such domain. Games offer a good formalism for\nthe study of repeated interactions between multiple agents and grounds to explore rich patterns\nof thought, in social collaborative settings 241-244."}, {"title": "Risks and Important Considerations", "content": "Computational thought partners are by no means a guaranteed nor universal good and come\nwith certain risks. We call out three such spheres of risk: (i) reliance, critical thinking, and\naccess, (ii) anthromorphization, and (iii) misalignment.\nFirst, AI thought partners could induce over-reliance and impair the development of critical\nthinking skills 219,245-247, potentially acting as \u201csteroids\u201d for the mind 248. We are concerned\nabout these risks; our emphasis on the infrastructure around thought partner use is explicitly\nintended to help practitioners take steps to address these challenges, motivating further design\nof infrastructure modifications like cognitive forcing functions 249,250. Conversely, it is possible\nthat some people may under-rely on a thought partner, particularly if there is inadequate AI\nliteracy training for how to best make use of new thought partners 251-253. Already, research\nhas found that the kinds of queries people make of AI systems can be informed by the amount\nof prior experience they have interacting with chatbots 15 meaning students, researchers, and\nother practitioners in lower-income communities may be unable to maximize the value of thought\npartnering. It is important to ensure that the benefits of thought partners are not confined to an\nexclusive set of people.\nSecond, on the topic of anthromoprhization, we highlight an important distinction between\nhuman-centric and human-like thought partners 254. Our desiderata \u201cI understand you\u201d advocates\nfor thought partners whose behavior we understand; while this could draw on how we understand\nother humans, however, we should be careful about interpreting such machine thought partners\nas we do humans. As Weizenbaum illuminated with the ELIZA system, there are risks to\ndeveloping computer systems that present themselves as human-like in ways that they are not: for\nexample, by leading users to attribute undue intention to systems' responses or (in the long run)\nleading society to devalue human intelligence 255. Human-like thought partners should maintain\ncategorical delineation between humans and machines to prevent overreliance 245,256 and promote\nhuman dignity without encroaching on any partner's self-worth 257. The term used to refer to a\nthought partner can affect the assumptions made about their capabilities (e.g., teammate implies\nthe machine and human are on equal footing) or can detract from a partner's human-like nature\n(e.g., tool would be less anthropomorphic).\nLastly, we note that insufficiently accurate, robust, or cognitively-grounded models can yield\nmisalignment with humans, leading intended AI thought partners to act towards the wrong\ngoals 258, provide wrong or misleading information 259, or violate safety constraints 260. A Bayesian\napproach to thought partnership can address some of these issues, enabling uncertainty-aware\ndecision-making that avoids overconfidence 223,261,262. Yet, while inferring human thoughts and\nbehavior can be used to design better collaborators, models of humans are inherently dual-use\nand can also be used to mislead, surveil, or manipulate 263. It is crucial to consider whether\nthought partners are aligned with society at large, or merely superficially aligned with users\nwhile serving more powerful interests 264."}, {"title": "Conclusion", "content": "If we are to build helpful and reliable human-AI thought partnerships, we advocate for design that\nexplicitly recognizes and engages with the richness and diversity of human thought in an often\nunpredictable world. We have argued, supported by several case studies, that those engineering\nthought partners and the infrastructure around their use can benefit from drawing on motifs\nfrom computational cognitive science and cognitive-AI. The future of collaborative cognition is\nbright, but not without risk; continual collaboration and knowledge sharing amongst behavioral\nscientists, AI practitioners, domain experts, and related disciplines is crucial as we strive to build\nmachines that truly learn and think with people."}, {"title": "Glossary of main terms", "content": "\u2022 Collaborative cognition: the process by which two or more agents work together in some\n    aspect(s) of thinking (e.g., planning together, learning together, creating together).\n\n\u2022 Thought partner: another entity (human or AI) that works with an agent to push forward\n    some aspect(s) of thinking.\n\n\u2022 Artificial Intelligence (AI): computational systems that are able to process inputs and\n    engage in some aspect of learning, planning, reasoning, and/or decision-making. Used\n    interchangeably with machines.\n\n\u2022 Large language model (LLM): a particular kind of AI system which learns a distribution\n    over text, often trained on large amounts of web-scale text data. LLMs are a class of\n    large-scale foundation models.\n\n\u2022 Agent: an entity that can process inputs, make decisions, and take actions in some\n    environment.\n\n\u2022 Dyad: a system with two agents (e.g., human-human, human-AI, AI-\u0391\u0399).\n\n\u2022 Resource-rationality: the idea that human behavior and cognition can be viewed as rational\n    under bounded constraints (e.g., under limited working memory).\n\n\u2022 Probabilistic generative model: a model of how the data one observes about the world is\n    generated by some probabilistic process, from which one can sample new observations and\n    make queries about existing observations.\n\n\u2022 Probabilistic programming language (PPL): a language for expressing probabilistic genera-\n    tive models as computer programs that interleave deterministic code (e.g. arithmetic, logic,\n    or artificial neural networks) with random choices. PPLs allow users to specify probabilistic\n    models and inference algorithms in a modular and compositional manner.\n\n\u2022 Bayesian inference: a method for updating one's beliefs over various aspects of the world,\n    grounded in probability theory; in Bayesian inference, an agent updates their beliefs by\n    assigning higher credence to hypotheses that better explain the evidence, weighted against\n    the backdrop of their prior beliefs.\n\n\u2022 Affordance: design features of a system that inform use."}]}