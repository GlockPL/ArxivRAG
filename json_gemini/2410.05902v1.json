{"title": "MINI-BATCH KERNEL k-MEANS", "authors": ["Ben Jourdan", "Gregory Schwartzman"], "abstract": "We present the first mini-batch kernel k-means algorithm, offering an order of magnitude improvement in running time compared to the full batch algorithm. A single iteration of our algorithm takes \u00d5(kb\u00b2) time, significantly faster than the O(n\u00b2) time required by the full batch kernel k-means, where n is the dataset size and b is the batch size. Extensive experiments demonstrate that our algorithm consistently achieves a 10-100x speedup with minimal loss in quality, addressing the slow runtime that has limited kernel k-means adoption in practice. We further complement these results with a theoretical analysis under an early stopping condition, proving that with a batch size of \u2229(max {\u03b34, \u03b32} \u00b7 \u0454\u22122), the algorithm terminates in O(\u03b3\u00b2/\u20ac) iterations with high probability, where y bounds the norm of points in feature space and e is a termination threshold. Our analysis holds for any reasonable center initialization, and when using k-means++ initialization, the algorithm achieves an approximation ratio of O(logk) in expectation. For normalized kernels, such as Gaussian or Laplacian it holds that y = 1. Taking \u20ac = O(1) and b = O(logn), the algorithm terminates in O(1) iterations, with each iteration running in O(k) time.", "sections": [{"title": "1 INTRODUCTION", "content": "Mini-batch methods are among the most successful tools for handling huge datasets for machine learning. Notable examples include Stochastic Gradient Descent (SGD) and mini-batch k-means (Sculley, 2010). Mini-batch k-means is one of the most popular clustering algorithms used in practice (Pedregosa et al., 2011).\nWhile k-means is widely used due to it's simplicity and fast running time, it requires the data to be linearly separable to achieve meaningful clustering. Unfortunately, many real-world datasets do not have this property. One way to overcome this problem is to project the data into a high, even infinite, dimensional space (where it is hopefully linearly separable) and run k-means on the projected data using the \"kernel-trick\".\nKernel k-means achieves significantly better clustering compared to k-means in practice. However, its running time is considerably slower. Surprisingly, prior to our work there was no attempt to speed up kernel k-means using a mini-batch approach.\nProblem statement We are given an input (dataset), X = {xi}i=1, of size n and a parameter k representing the number of clusters. A kernel for X is a function K : X X X \u2192 R that can be realized by inner products. That is, there exists a Hilbert space H and a map \u03c6 : X \u2192 H such that \u2200x,y \u2208 X, (\u03c6(x), \u03c6(y)) = K(x, y). We call H the feature space and \u03c6 the feature map.\nIn kernel k-means the input is a dataset X and a kernel function K as above. Our goal is to find a set C of k centers (elements in H) such that the following goal function is minimized:\n- \u2211 min ||c \u2013 \u03c6(x)||2.\nc\u2208C\nx\u2208X\nEquivalently we may ask for a partition of X into k parts, keeping C implicit."}, {"title": "2 RELATED WORK", "content": "Until recently, mini-batch k-means was only considered with a learning rate going to 0 over time. This was true both in theory (Tang & Monteleoni, 2017; Sculley, 2010) and practice (Pedregosa et al., 2011). Recently, (Schwartzman, 2023) proposed a new learning which does not go to 0 over time, and showed that if the batch is of size \u03a9((d/e)2), mini-batch k-means must terminate within O(d/e) iterations with high probability, where d is the dimension of the input, and e is a threshold parameter for termination.\nA popular approach to deal with the slow running time of kernel k-means is constructing a coreset of the data. A coreset for kernel k-means is a weighted subset of X with the guarantee that the solution quality on the coreset is close to that on the entire dataset up to a (1 + \u20ac) multiplicative factor. There has been a long line of work on coresets for k-means an kernel k-means (Schmidt, 2014; Feldman et al., 2020; Barger & Feldman, 2020), and the current state-of-the-art for kernel k-means is due to (Jiang et al., 2021). They present a coreset algorithm with a nearly linear (in n and k) construction time which outputs a coreset of size poly(ke\u00af\u00b9).\nIn (Chitta et al., 2011) the authors only compute the kernel matrix for uniformly sampled set of m points from X. Then they optimize a variant of kernel k-means where the centers are constrained to be linear combinations of the sampled points. The authors do no provide worst case guarantees for the running time or approximation of their algorithm.\nAnother approach to speed up kernel k-means is by computing an approximation for the kernel matrix. This can be done by computing a low dimensional approximation for \u03c6 (without computing explicitly)(Rahimi & Recht, 2007; Chitta et al., 2012; Chen & Phillips, 2017), or by computing a low rank approximation for the kernel matrix (Musco & Musco, 2017; Wang et al., 2019).\nKernel sparsification techniques construct sparse approximations of the full kernel matrix in sub- quadratic time. For smooth kernel functions such as the polynomial kernel, (Quanrud) presents an algorithm for constructing a (1 + \u20ac)-spectral sparsifier for the full kernel matrix with a nearly linear"}, {"title": "3 PRELIMINARIES", "content": "Throughout this paper we work with ordered tuples rather than sets, denoted as Y = (Yi)i\u2208[l], where [l] = {1, ..., l}. To reference the i-th element we either write yi or Y[i]. It will be useful to use set notations for tuples such as x\u2208 Y \u21d4 \u2203i\u2208 [l], x = yi and Y \u2286 Z \u21d4 \u2200i \u2208 [l], Yi \u2208 Z. When summing we often write \u03a3\u03c0\u03b5y g(x) which is equivalent to \u03a3=1 9(Y[i]).\nWe borrow the following notation from (Kanungo et al., 2004) and generalize it to Hilbert spaces. For every x, y \u2208 H let \u2206(x, y) = ||x-y||2. We slightly abuse notation and and also write \u2206(x, y) =\n||\u03c6(x) \u2013 \u03c6(y)||2 when x, y \u2208 X and \u2206(x,y) = ||\u03c6(x) \u2013 y||2 when x \u2208 X, y \u2208 H (similarly when x \u2208 H,y \u2208 X). For every finite tuple S \u2286 X and a vector x \u2208 H let \u2206(S,x) = \u2211yes \u2206(y, x).\nLet us denote y = maxxex ||\u03c6(x)||. Let us define for any finite tuple S \u2286 X the center of mass of\nthe tuple as cm(S) = Exes \u03c6(x).\nWe now state the kernel k-means problem using the above notation.\nKernel k-means We are given an input X = (xi)=1 and a parameter k. Our goal is to (im- plicitly) find a tuple C \u2286 H of k centers such that the following goal function is minimized: Exex mincec \u2206(x, C).\nLet us define for every x \u2208 X the function fx : Hk \u2192 R where fx(C) = minc\u2208c \u2206(x, C). We can treat Hk as the set of k-tuples of vectors in H. We also define the following function for every tuple A = (ai)=1 \u2286 X: fa(C) = \u2211i=1 fa; (C). Note that fx is our original goal function.\nWe make extensive use of the notion of convex combination:\nDefinition 2. We say that y \u2208 H is a convex combination of X if y = \u2211x\u2208xPxp(x), such that\n\u2200x \u2208 X, px \u2265 0 and \u2211x\u2208x Px = 1."}, {"title": "4 OUR ALGORITHM", "content": "We start by presenting a slower algorithm that will set the stage for our truncated mini-batch algo- rithm and will be useful during the analysis. We present our pseudo-code in Algorithm 1. It requires an initial set of cluster centers such that every center is a convex combination of X. This guarantees that all subsequent centers are also a convex combination of X. Note that if we initialize the centers using the kernel version of k-means++, this is indeed the case.\nAlgorithm 1 proceeds by repeatedly sampling a batch of size b (the batch size is a parameter). For the i-th batch the algorithm (implicitly) updates the centers using the learning rate \u03b1 for center\nj. Note that the learning rate may take on different values for different centers, and may change between iterations. Finally, the algorithm terminates when the progress on the batch is below e, a user provided parameter. While our termination guarantees (Section 5) require a specific learning rate, it does not affect the running time of a single iteration, and we leave it as a parameter for now.\nRecursive distance update rule While for (non kernel) k-means the center updates and assign- ment of points to clusters is straightforward, this is tricky for kernel k-means and even harder for mini-batch kernel k-means. Specifically, how do we overcome the challenge that we do not maintain the centers explicitly?\nTo assign points to centers in the (i + 1)-th iteration, it is sufficient to know ||\u03c6(x) - C+1||2 for every j. If we can keep track of this quantity through the execution of the algorithm, we are done.\nLet us derive a recursive expression for the distances\n||\u03c6(x) \u2013 C+1||\u00b2 = (\u03c6(x), \u03c6(x)) \u2013 2(\u03c6(x), C+1) + (C+1, C+1)."}, {"title": "4.1 TRUNCATING THE CENTERS", "content": "The issue with the above approach is that each center is written as linear combination of potentially all points in X. We now present a simple way to overcome this issue. We maintain C+1 as an explicit sparse linear combination of X. Let us expand the recursive expression of C+1 for t terms, assuming t > i:\nC+1 = (1 \u2212 )C? + ?cm(B) = CI=0(1 - 0) + \u03a3\u03b1-cm (Be)II=-1+1(1 \u2013 2).\nThe idea behind our truncation technique is that when t is sufficiently large CI=0(1 \u2013 e) becomes very small and can be discarded. The rate by which this term decays depends on the learning rates, which in turn depend on the number of elements assigned to the cluster in each of the previous iterations.\nLet us denote b = B We would like to trim the recursive expression such that every cluster center is represented using about 7 points, where \u03c4 is a parameter. We define Q to be the set of indices from i to i \u2212 t, where t is the smallest integer such that \u2211eeq b\u2265 7 holds. If no such\ninteger exists then Q = {i, i \u2212 1, . . ., 1}. It is the case that Deeq b\u2264 r + b.\nNext we define the truncated centers, for which the contributions of older points to the centers are\nforgotten after about 7 points have been assigned to the center:\n2 = \u03a3\u03c4\u03b5\u03c6 (\u0392\u0384) \u03a0\u03b5\u03c1\\{i} (1-2), min > 1\ni+1\nC+1 =\notherwise."}, {"title": "5 TERMINATION GUARANTEE", "content": "In this section we prove the second claim of Theorem 1. For most of the section we analyze Al- gorithm 1, and towards the end we use the fact that the centers of the two algorithms are close throughout the execution to conclude our proof.\nSection preliminaries We introduce the following definitions and lemmas to aid our proof of the second claim of Theorem 1.\nLemma 4. For every y which is a convex combination of X it holds that ||y|| \u2264 \u03b3.\nProof. The proof follows by a simple application of the triangle inequality:\n||Y|| = || \u03a3 \u03a1\u03b1\u03c6(x)|| \u2264 \u03a3||Pad(x)|| = \u03a3Px||(x)|| \u2264 \u03a3Pxy = v.\nLemma 5. For any tuple of k centers C \u2282 H which are a convex combination of points in X, it\nholds that \u2200A \u2286 X, fa(C) \u2264 4y2."}, {"title": "6 EXPERIMENTS", "content": "We evaluate our mini-batch algorithms on the following datasets:\nMNIST: The MNIST dataset (LeCun, 1998) has 70,000 grayscale images of handwritten digits (0\nto 9), each image being 28x28 pixels. When flattened, this gives 784 features. PenDigits: The\nPenDigits dataset (Alpaydin & Alimoglu, 1998) has 10992 instances, each represented by an 16- dimensional vector derived from 2D pen movements. The dataset has 10 labelled clusters, one for each digit. Letters: The Letters dataset (Slate, 1991) has 20,000 instances of letters from 'A' to \u2018Z',\neach represented by 16 features. The dataset has 26 labelled clusters, one for each letter. HAR:\nThe HAR dataset (Anguita et al., 2013) has 10,299 instances collected from smartphone sensors, capturing human activities like walking, sitting, and standing. Each instance is described by 561\nfeatures. The dataset has 6 labelled clusters, corresponding to different types of physical activities.\nWe compare the following algorithms: full-batch kernel k-means, truncated mini-batch kernel k- means, and mini-batch k-means (both kernel and non-kernel) with learning rates from (Schwartz- man, 2023) and sklearn. We evaluate our results with batch sizes: 2048, 1024, 512, 256 and \u03c4 = 50, 100, 200, 300. We execute every algorithm for 200 iterations. For the results below, we apply the Gaussian kernel: K(x, y) = e-||x-y||2/k, where the k parameter is set using the heuristic of (Wang et al., 2019) followed by some manual tuning (exact values appear in the supplementary materials). We also run experiments with the heat kernel and knn kernels. We repeat every experi- ment 10 times and present the average Adjusted Rand Index (ARI) (Gates & Ahn, 2017; Rand, 1971) and Normalized Mutual Information (NMI) (Lancichinetti et al., 2009) scores for every dataset. All experiments were conducted using an AMD Ryzen 9 7950X CPU with 128GB of RAM and a Nvidia GeForce RTX 4090 GPU. We present partial results in Figure 1 and the full results in Appendix C. Error bars in the plot measure the standard deviation."}, {"title": "A OMITTED PROOFS AND ALGORITHMS FOR SECTION 4", "content": "Runtime analysis of Algorithm 1 Assuming that (C,C) and ((x), C) are known for all j \u2208 [k] and for all x \u2208 X, we can compute (C+1,C2+1) and ((x), C+1) for all j \u2208 [k] and x \u2208 X, which implies we can compute the distances from any point in the batch to all centers.\nWe now bound the running time of a single iteration of the outer loop in Algorithm 1. Let us denote b = |B| and recall that cm(B) = \u2211y\u2208B \u03c6(y). Therefore, computing ($(x), cm(B)) = \u2211y\u2208 ($(x), \u03c6(y))\nrequires O(b) time. Similarly, computing (cm(B), \u0441\u0442(B)) requires O((b)\u00b2) time. Let us now bound the time it requires to compute ($(x), C+1) and (C+1, C+1)\u00b7\nAssuming we know ($(x), C) and (C, C), updating ($(x), C+1) for all x \u2208 X, j \u2208 [k] requires O(n(b+k))\ntime. Specifically, the ($(x), C) term is already known from the previous iteration and we need to compute\n($(x), cm(B)) for every x \u2208 X, j \u2208 [k] which requires n\u2211j\u2208[k] b = nb time. Finally, updating\n((x), C+1) for all x \u2208 X, j \u2208 [k] requires O(nk) time.\nUpdating (C+1, C+1) requires O(b\u00b2 + kb) time. Specifically, (C, C) is known from the previous iteration and computing (cm(B), cm(B)) for all j\u2208 [k] requires O(\u2211j\u2208[k](b)\u00b2) = O(b\u00b2) time. Computing (C, cm(B)) for all j \u2208 [k] requires time O(b) using ($(x), C) from the previous iteration. Therefore, the\ntotal running time of the update step (assigning points to new centers) is O(n(b + k)). To perform the update at the (i + 1)-th step we only need ($(x), C), (C, C), which results in a space complexity of O(nk). This completes the first claim of Theorem 1."}, {"title": "B OMITTED PROOFS AND ALGORITHMS FOR SECTION 5", "content": "Proof of Lemma 11\nProof.\nA(S, C) = \u2211\u2206(x, C) = \u2211(x \u2013 C, x \u2212 C)\nxES\nxES\n= \u2211((x \u2013 cm(S)) + (cm(S) \u2013 C), (x \u2013 \u0441\u0442(S)) + (cm(S) \u2013 C))\nxES\n= \u2211\u2206(x, cm(S)) + \u2206(C, cm(S)) + 2(x \u2013 cm(S), \u0441\u0442(S) \u2013 C)\nxES\n= \u2206(S, cm(S)) + |S|\u25b3(C, cm(S)) + \u2211 2(x \u2013 cm(S), cm(S) \u2013 C)\nxES\n= \u2206(S, cm(S)) + |S|\u2206(C, cm(S)),\nwhere the last step is due to the fact that\n\u2211(x \u2013 cm(S), cm(S) \u2013 C) = ( x \u2212 |S| cm(S), cm(S) \u2013 C)\nxES\nxES\n= (\u03a3\u03c7 \u2212 C) = 0.\nProof of Lemma 12\nProof. Using Lemma 11 we get that (S, C) = \u25b2(S, cm(S)) + |S|A(cm(S), C) and that A(S, C') = \u25b2(S, cm(S)) + |S|A(cm(S), C'). Thus, it holds that |\u2206(S, C') \u2013 \u2206(S,C)| = |S| \u00b7\n|\u2206(cm(S), C') \u2013 \u2206(cm(S), C)|. Let us write\n|\u2206(cm(S), C') \u2013 \u2206(cm(S), C)|\n= |(cm(S) \u2013 C', cm(S) \u2013 C') \u2013 (cm(S) \u2013 \u0421, \u0441\u0442(S) \u2013 C)|\n= |-2(cm(S), C') + (C', C') + 2(cm(S), C) \u2013 (C, C)|\n= |2(cm(S), C \u2013 C') + (C' \u2013 C, C' + C)|\n= |C \u2013 C', 2cm(S) \u2013 (C' + C))|\n< ||C - C' ||||2cm(S) \u2013 (C' + C)|| \u2264 4y||C \u2013 C'||.\nWhere in the last transition we used the Cauchy-Schwartz inequality, the triangle inequality, and the fact that\nC, C', cm(S) are convex combinations of X and therefore their norm is bounded by y.\nProof of Lemma 16\nProof. Let p = 1 \u2212 O(\u03b5/\u03b7\u03b3\u00b2) = 1 \u2013 O(1/n) be the success probability of a single iteration. By \"success\" we mean that all inequalities in Lemma 15 hold. The value of p is due to the fact that we take t = O(\u03b3\u00b2/\u20ac) and that y\u00b2/\u20ac \u2265 1/4.\nWith probability at least p, it holds that fx (Ci+1) <\u2264 fx (Ci) \u2013 2\u20ac/7. On the other hand, fx is upper bounded\nby 4y2. Let us denote Z = fx (Ci) - fx (Ci+1) the change in the goal function after the i-th iteration. Consider\nthe following:\nE[Z] = E[Z | Z \u2265 \u20ac/7]Pr[Z \u2265 \u20ac/7] + E[Z | Z < \u20ac/7]Pr[Z < \u20ac/7]\nWe show that E[Z] = E[fx(Ci) \u2013 fx (Ci+1)] \u2265 0 which implies that E[fx (Ci+1)] \u2264 E[fx (Ci)] and\ncompletes the proof. Note that if E[Z | Z < \u20ac/7] > 0 then we are done as we simply have a linear combination\nof two positive terms which is greater than 0. Let us focus on the case where E[Z | Z < \u20ac/7] < 0.\nE[Z] = E[Z | Z \u2265 \u20ac/7]Pr[Z \u2265 \u20ac/7] + E[Z | Z < \u20ac/7]Pr[Z < \u20ac/7]\n> \u03c1\u03b5/7 + E[Z | Z < \u0454/7](1 \u2212 p) \u2265 \u03c1\u03b5/7 \u2013 4\u03b3\u00b2 (1 \u2212 p)\n= (1 \u2013 0(1/n))\u20ac/7 \u2013 4\u03b3\u00b2O(\u03b5/\u03b3\u00b2n) = (1 \u2013 O(1/n))\u0454/7 \u2013 O(\u0454/n) > 0"}]}