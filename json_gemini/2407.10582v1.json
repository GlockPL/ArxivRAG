{"title": "Boosting Zero-Shot Crosslingual Performance\nusing LLM-Based Augmentations with Effective Data Selection", "authors": ["Barah Fazili", "Ashish Sunil Agrawal", "Preethi Jyothi"], "abstract": "Large language models (LLMs) are very pro-\nficient text generators. We leverage this capa-\nbility of LLMs to generate task-specific data\nvia zero-shot prompting and promote cross-\nlingual transfer for low-resource target lan-\nguages. Given task-specific data in a source\nlanguage and a teacher model trained on this\ndata, we propose using this teacher to label\nLLM generations and employ a set of simple\ndata selection strategies that use the teacher's\nlabel probabilities. Our data selection strate-\ngies help us identify a representative subset of\ndiverse generations that help boost zero-shot\naccuracies while being efficient, in comparison\nto using all the LLM generations (without any\nsubset selection). We also highlight other im-\nportant design choices that affect cross-lingual\nperformance such as the use of translations of\nsource data and what labels are best to use for\nthe LLM generations. We observe significant\nperformance gains across sentiment analysis\nand natural language inference tasks (of up to a\nmaximum of 7.13 absolute points and 1.5 abso-\nlute points on average) across a number of tar-\nget languages (Hindi, Marathi, Urdu, Swahili)\nand domains.", "sections": [{"title": "Introduction", "content": "Multilingual pretrained models are a mainstay in\nmodern NLP. To create highly-performant task-\nspecific models across different languages, a com-\nmonly adopted paradigm is to finetune a multi-\nlingual pretrained model like XLM-R (Conneau\net al., 2020) using task-specific labeled data. In the\nabsence of labeled data for a target language, pre-\ntrained models finetuned on task-specific data in a\nsource language (such as English) have been shown\nto facilitate zero-shot crosslingual transfer (Yu and\nJoty, 2021; Zheng et al., 2021; Liu et al., 2021).\nGiven large language models (LLMs) and their su-"}, {"title": "Methodology", "content": "Consider a scenario where we have task-specific\nlabeled data for a high-resource source language\nsuch as English (denoted as Den). Our final down-\nstream task is in a low-resource target language for\nwhich we have no labeled data. In this work, we\nexperiment with two classification tasks: sentiment\nanalysis (SA) and natural language inference (NLI).\nWe aim to achieve improved cross-lingual transfer\nfor these two tasks to different target languages by\naugmenting Den with (labeled) LLM generations\ndenoted as Gen. This is motivated by recent work\non boosting task performance via data augmenta-\ntion techniques (Vu et al., 2022; He et al., 2022a;\nLiu et al., 2022; Whitehouse et al., 2023; De Raedt\net al., 2023a).\nGen is generated by prompting an LLM with\na compact target domain description and the in-\ntended class label to produce class-conditioned,\ntask-specific generations in the target domain.\nWe utilize the open-source 13b llama-2-chat-hf\nmodel (Touvron et al., 2023) for all our generations.\nThe prompt is composed of two sub-prompts: 1) A\nsystem prompt that specifies a generic set of rules\nthat the generator should obey, and 2) an instruction\nprompt that specifies more targeted instructions for\ngeneration. More details about data generation us-"}, {"title": "Pseudolabeling and Training Methods", "content": "Teacher-student Training (T). A teacher model\nis trained on the source data (Den) using cross-\nentropy loss. The teacher is used to pseudolabel\nthe generations in Gen. A subset of Gen is cho-\nsen via various selection techniques described in\nSection 2.3. We will refer to this subset as D'en.\nA student model is trained on both Den and Den\ncombined, using cross-entropy loss with the gold\nlabels in Den and a KL-divergence loss with the\nsoft pseudolabels derived from the teacher model.\nEquation (1) refers to the overall loss computed,\nwhere yc(x) is the one-hot label corresponding to\neach x \u2208 Den, qe is the student model probabil-\nity for class c (with temperature 1), pc(x) is the\nteacher model probability for each x \u2208 Den for\nclass c \u2208 C and q is the student model probability\nfor class c (scaled by temperature value 1.5). This\nis the standard teacher-student paradigm, and we\nwill refer to the trained student model as T in our"}, {"title": "Data Selection Strategies", "content": "Around 130K instances are generated for each tar-\nget task from which a small subset is sampled using\nvarious data selection techniques described below.\nIn all experiments, we uniformly sample across\npositive, negative, and neutral class labels for senti-\nment analysis (and entailment, contradiction, and\nneutral class labels for NLI) by choosing 2500 in-\nstances from the full set of instances for each class\nto create D'en/Dig.\n\n\u2022 top-k: Instances specific to each class in\nGen/Gtg are sorted in descending order using\nthe teacher model's predicted probability for\nthat class. The top-k (k = 2500) instances\nfrom each class are then selected.\n\u2022 div-k: We aim to select a diverse set of sen-\ntences from each target class using div-k. The\nsentences belonging to each class (based on\nteacher labels) are encoded using LABSE sen-\ntence embeddings (Feng et al., 2022). The\nembeddings for each class are then clustered\nusing NLTK's Kmeans clustering algorithm\u00b3.\nWe create 25 clusters for each class and select\nthe top 100 instances using the probabilities\nassigned by the teacher model (as in top-k)\nper cluster to get a total of 2500 instances\nper class. With this simple cluster-then-topk\ntechnique, we hope to identify samples that\noffer good coverage and capture the diversity\nof samples within each class.\n\u2022 amb-k and easy-k: We design two additional\nselection techniques amb-k and easy-k by\ndrawing inspiration from prior work on data\ncartography (Swayamdipta et al., 2020) where\ndata points are characterized as ambiguous,\neasy or hard based on the training dynamics\nacross epochs. We first compute predicted\nprobabilities for each class for each instance\nacross checkpoints of the teacher model saved\nafter each training epoch. Next, we com-\npute the mean and standard deviation across\nprobabilities for each instance across training\nepochs. For each class, instances with the\ntop-k (k = 2500) mean and standard devia-\ntion values are chosen as easy-k and amb-k,\nrespectively. High standard deviation values\nsignify larger variability in predictions across\ntraining; these instances are characterized as\nambiguous examples that the model is unsure\nabout. High mean values signify higher confi-\ndence in predictions; these instances are char-\nacterized as easy examples that the model is\nconfident about. This selection technique is\nexpensive in having to maintain checkpoints\nfor all training epochs; we evaluate this only\nfor NLI."}, {"title": "Experimental Setup", "content": "Source data refers to labeled task-specific data in\nEnglish, while target data refers to evaluation sets\nin the target languages for which there is no labeled\ndata. Unless specified otherwise, we choose source\ndata to be from a different domain compared to the\ntarget data. This is different from most prior work\nin zero-shot evaluations where the source data is\ntypically chosen to be consistent in the domain to\nthe target tasks (Whitehouse et al., 2023; Li et al.,\n2021; Du et al., 2021; Vu et al., 2022). We assume\na more realistic setting where the source and target\ndomains can be mismatched.\nSource data. We use SST5 (Socher et al., 2013)\nand SNLI (Bowman et al., 2015) datasets for senti-\nment analysis (SA) and natural language inference\n(NLI), respectively. SST5 is a sentiment classifica-\ntion dataset featuring five distinct labels: negative,\nvery negative, positive, very positive, and neutral,\nthat we collapse into three labels: positive, nega-\ntive and neutral to match the target tasks. Similar\nto (Li et al., 2021), we consider a random subset\nof the SNLI train set (15K training sentences, 5K\nper class) to simulate a low-resource setting and\nfor quicker experimental turnaround.\nTarget data. Our target SA tasks include Marathi\nSentiment (Pingle et al., 2023), GLUECoS Hindi-\nEnglish code-switched Sentiment (Khanuja et al.,\n2020), and Hindi Product Reviews (Akhtar et al.,\n2016). For NLI, we evaluate on Hindi, Urdu, and\nSwahili from the XNLI (Conneau et al., 2018) cor-\npus; these are some of the the least-represented\nXNLI languages. Appendix A provides more de-\ntails about the source and target tasks. Appendix\nC shows how we generate code-mixed data for the\ntranslate-train setting of the GLUECos task."}, {"title": "Model and Training details", "content": "For all our experiments, we use the xlm-roberta-\nlarge model (Conneau et al., 2019) for modeling\nboth the student and teacher. It is a 561M param-\neter model. Our choice of XLM-R for classi-\nfication tasks was motivated by recent work on\ncross-lingual classification (Artetxe et al., 2023)\nthat uses only XLM-R for all its evaluations. There\nis also prior work (Zhang et al., 2023) that shows\nthat compared to much larger multilingual LMs"}, {"title": "Baselines", "content": "Source only (SRC). Here, the model is trained on\nthe train set of the source tasks (refer Section 3.1).\nNo synthetic data is used to train the model. 8,544\nand 15K instances are used for SA and NLI tasks,\nrespectively.\nSource+Generations (SRC+GEN). Here, the\nmodel is trained on a mixture of source and syn-\nthetic datasets. The synthetic dataset (7.5K) is sam-\npled randomly from among the generations and\nis not selected via any data selection technique\nor with the help of a teacher resulting in 16K and\n22.5K instances for SA and NLI tasks, respectively.\nGenerations only (GEN). Here, we train the\nmodel only on the synthetically generated data.\nThe labels come from the prompts that we used\nfor class conditional generation. For a fair compar-\nison with SRC+GEN, we maintain the same total\nsize of 16K and 22.5K instances for SA and NLI\ntasks, respectively"}, {"title": "Results and Analysis", "content": "We characterize the training data used for each\nmodel along two axes: source of the task-specific\ntext and source of labels assigned to the data. GEN,\nSRC and SRC+GEN in Table 1 represent the base-\nline models as described in Section 3.3. Gener-\nated text used in all the baseline models is com-\nbined with the corresponding prompt labels used\nduring generation. We observe that substituting\na portion of generated instances with source in-\nstances (SRC+GEN) yields better performance, as"}, {"title": "Conclusion", "content": "In this work, we focus on the broader problem\nof boosting zero-shot cross-lingual transfer using\nLLM-based augmentations. We highlight the im-\nportance of using data selection strategies to select\nsmaller subsets that result in more efficient training\nand improved performance on downstream target\nlanguage tasks. We also compare and contrast the\nutility of pseudolabeling generations using labels\nfrom LLM prompts versus using a teacher model to\nlabel the generations. One of the main takeaways is\nthat LLM generations, in conjunction with our data\nselection strategies, can help improve cross-lingual\ntransfer regardless of whether task-specific source\ndata matches the domain of the target tasks or not."}, {"title": "Limitations", "content": "1. The generations from LLMs are sensitive to\nthe prompts used. Although we share our\ncustom prompts, the quality of the generated\ncontent is heavily reliant on the particular do-\nmain and task for which the data is generated\ncreating some non-determinism.\n2. Because of budget constraints, our investi-\ngations were constrained to an open-source\nLLM (LLAMA-2). It is possible that higher-\ncapacity commercial LLMs could yield better\nperformance.\n3. We explore many data selection techniques but\na clear winner across all tasks/settings has not\nemerged. Although ambiguous selection gives\nbest scores for XNLI, more target domains\nand languages should be included to study the\nmost effective filtering techniques in general.\n4. We have only experimented with generating\ndata for classification tasks; generating data\nfor more structured tasks like QA or common-\nsense reasoning tasks could pose challenges.\n5. For the translate-train models, one assumes\naccess to MT models for the target language\nwhich may not always be available."}]}