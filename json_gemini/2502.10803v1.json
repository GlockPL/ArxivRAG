{"title": "PDA: Generalizable Detection of AI-Generated Images via Post-hoc Distribution Alignment", "authors": ["Li Wang", "Wenyu Chen", "Zheng Li", "Shanqing Guo"], "abstract": "The rapid advancement of generative models has led to the proliferation of highly realistic AI-generated images, posing significant challenges for detection methods to generalize across diverse and evolving generative techniques. Existing approaches often fail to adapt to unknown models without costly retraining, limiting their practicability. To fill this gap, we propose Post-hoc Distribution Alignment (PDA), a novel approach for the generalizable detection for AI-generated images. The key idea is to use the known generative model to regenerate undifferentiated test images. This process aligns the distributions of the re-generated real images with the known fake images, enabling effective distinction from unknown fake images. PDA employs a two-step detection framework: 1) evaluating whether a test image aligns with the known fake distribution based on deep k-nearest neighbor (KNN) distance, and 2) re-generating test images using known generative models to create pseudo-fake images for further classification. This alignment strategy allows PDA to effectively detect fake images without relying on unseen data or requiring retraining. Extensive experiments demonstrate the superiority of PDA, achieving 96.73% average accuracy across six state-of-the-art generative models, including GANs, diffusion models, and text-to-image models, and improving by 16.07% over the best baseline. Through t-SNE visualizations and KNN distance analysis, we provide insights into PDA's effectiveness in separating real and fake images. Our work provides a flexible and effective solution for real-world fake image detection, advancing the generalization ability of detection systems.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of generative models, such as GANs [10] and diffusion models [17, 28], has led to a surge in the creation of highly realistic AI-generated images. These images present significant challenges for identity authentication, content verification, and media forensics [15,38,45]. As the ability to create highly convincing fake images becomes more widespread, there is an urgent need for reliable and generalized detection methods, the need for reliable, generalized detection methods is becoming more urgent particularly in real-world scenarios where models must adapt to various types of generated images [1,5,25].\nExisting detection methods typically rely on binary classifiers trained using features extracted from specific generative models [13]. Although these approaches achieve high accuracy on known datasets (see Figure 1(a)), they often fail to generalize to images generated by unseen or emerging models [36, 46]. For instance, Figure 1(b) demonstrates that a detector trained on fake images from Stable Diffusion V1.4 [28] may fail to distinguish between real images and unknown fake images, such as those generated by VQDM [11] and BigGAN [3], due to artifact differences and distribution shifts [21]. This reveals a critical limitation in current detection paradigms: the detection performance degrades significantly when faced with fake images from unseen generative models, often leading to high-confidence misclassifications of out-of-distribution fake images as real [9, 34]. Additionally, existing methods attempt to improve their performance by retraining or fine-tuning unseen data [4]. However, retraining or fine-tuning models is computationally expensive, and accessing labeled generated data is often difficult and impractical, especially in real-world scenarios where generative models are evolving.\nTo address these challenges, we propose PDA, a novel generalizable detection of AI-generated images through post-hoc distribution alignment. PAD leverages the distinct feature distributions of real and fake images generated by known models. The key idea is to re-generate the indistinguishable test images using the same generative model. In this process, real images are transformed into known fake images, replicating the same artifacts and features as the previously known fake images. In contrast, the unknown fake images, when regenerated, continue to exhibit different artifacts and distribution shifts compared to the known fake images (as shown in Figure 1(c)). At this point, the distributions of these re-generated real images are aligned with the known fake image and can thus be effectively distinguished from the unknown fake images. Specifically, PDA employs a two-step detection framework: 1) evaluating whether a test image aligns using the known fake distribution with deep KNN distances and a threshold-based criterion, and 2) regenerating test images using known generative models to create pseudo-fake images for further classification. This alignment strategy enables PDA to detect fake images without relying on unseen data or retraining for new generative models."}, {"title": "2 Related Works", "content": "The rapid advancement of generative models has significantly improved the quality and diversity of AI-generated images. Early approaches, such as Variational Autoencoders (VAEs) [18], focused on learning latent representations of data distributions. Generative Adversarial Networks (GANs) [10], through their adversarial training framework, enabled high-fidelity image generation, with subsequent models like BigGAN [3] and StyleGAN [16] further enhancing controllability and realism. Recently, diffusion models [14] have emerged as a powerful alternative to GANs, achieving state-of-the-art results in image generation. These models gradually denoise a random distribution to generate highly detailed and diverse images [6]. Additionally, autoregressive models such as DALL-E [26], along with diffusion-based models like Stable Diffusion 1.4 [28], GLIDE [24], and ADM [8], have further pushed the boundaries by enabling highly realistic text-to-image generation. These models have significantly broadened the applicability of AI-generated images across a variety of domains, including creative content generation, artistic design, and virtual reality [11,27].\nHowever, the increasing realism of AI-generated images has raised concerns about their potential misuse, such as creating deepfakes or spreading misinformation. This has motivated research into robust detection techniques capable of generalizing across diverse generative models, as the subtle artifacts of generated images make them increasingly difficult to distinguish from real ones."}, {"title": "2.2 Detection of AI-generated Images", "content": "The detection of AI-generated images has become a critical research area due to the potential misuse of generative technologies [43]. Early methods focused on identifying GAN-specific artifacts, such as pixel-level inconsistencies, using handcrafted features or traditional classifiers [20, 22, 23]. While effective for low-quality or early-generation fake images, these approaches struggled to generalize to more advanced models. With the rise of deep learning, Convolutional Neural Networks (CNNs) have been widely adopted for their ability to automatically learn discriminative features [29,40]. For example, Wang et al. [39] find that pre-trained CNNs on ProGAN-generated images can generalize to other GAN-based images, benefiting from large-scale training on diverse LSUN [44] object categories.\nHowever, as generative models such as diffusion models and text-to-image models have gained traction, new challenges have emerged for detection tasks [33]. Unlike GANs, which often introduce visible artifacts, diffusion models produce highly realistic images with minimal visual discrepancies [5]. Zhu et al. [48] highlight that classifiers trained solely"}, {"title": "3 PDA for Generalizable Detection", "content": "As aforementioned, existing detection methods typically train binary classifiers to distinguish between real and fake images. These classifiers perform well when tested on fake images from the same distribution as the training data; however, they often misclassify unknown fake images as real due to a distribution shift. This shift occurs because different generative models introduce distinct artifacts and features.\nOur approach builds on a previously trained classifier that can already distinguish known fake images from real and unknown ones. The key idea is to use the same generative model to regenerate these undifferentiated images. In this process, real images are converted to known fake images, replicating the same artifacts and features as the previously known fake images. In contrast, the unknown fake images, when regenerated, continue to exhibit different artifacts and distribution shifts compared to the known fake images. At this point, the distributions of these re-generated real images are aligned with the known fake image and can thus be effectively distinguished from the unknown fake images."}, {"title": "3.2 Overview", "content": "Following the above idea, we propose a generalize detection for AI-generated images based on Post-hoc Distribution Alignment (PDA). Our approach leverages the distinct feature distributions of real and fake images generated by known models. The overall framework of our PAD method is shown in Figure 2. Specifically, the framework starts by training a CNN extractor using fake images generated by any known generative model, along with real images. This detector effectively learns the distribution of the known fake samples and is used to extract feature representations. For each test image, the PDA component first checks its alignment with the distribution of known fake images. If aligned, the image is classified as fake. If not, the image undergoes a secondary forgery process using the known generative model to create a pseudo-fake sample. If this re-generated image aligns with the known fake distribution, it is classified as real; otherwise, it is classified as fake. The key idea is that real images, when re-generated using a known generative model, align with the distribution of fake images from the same model, whereas unknown fake images exhibit distinct shifts in their distribution."}, {"title": "3.3 Methodology", "content": "We propose a generalizable detection method for AI-generated images based on Post-hoc Distribution Alignment (PDA), as illustrated in Figure 2. Our approach involves four steps: feature extractor training, reference feature representation, threshold determination and fake image detection."}, {"title": "3.3.1 Feature Extractor Training", "content": "The first step is to train the feature extractor to capture high-level artifacts and features. To this end, we train a binary classifier using a mixture of real images and fake images generated by a known generative model $G(\\cdot)$. The generative models, such as Stable Diffusion V1.4 [28] from the HuggingFace Hub, are readily accessible. The binary classifier minimizes the classification loss to effectively distinguish between real and generated fake images, based on their learned feature distributions. The loss function $L$ can be written as:\n$L = \\sum_{i=1}^N \\text{loss}(f_\\theta(I_i), y_i)$ (1)\nwhere $I_i$ denotes the input image and $y_i$ is the corresponding label (real or fake), and $N$ is the number of training images. After training, the classifier-excluding the final layer can be used as the feature extractor $f_\\theta$."}, {"title": "3.3.2 Reference Feature Representation", "content": "After training the feature extractor, we use it to generate feature representations for the known fake images from training data, which are then stored in a nearest neighbor reference set $Z$. Let $I$ be the known fake image, and $f_\\theta$ the feature extractor with parameters $\\theta$. The feature vector $x$ is given by:\n$x = f_\\theta(I)$ (2)\nHere, $x \\in \\mathbb{R}^d$ represents the extracted feature vector in a high-dimensional space, where $d$ denotes the dimensionality of the feature space. These representations are further refined through activation pruning and dimensionality reduction to improve discriminative power and eliminate irrelevant features.\nActivation Pruning. we remove activations that contribute minimally to distinguishing real from fake images. This is"}, {"title": "3.3.3 Threshold Determination", "content": "We input another subset of real images into the generative model to produce a new set of fake images. Using the same feature extraction process, we obtain a new reference feature set, $Z'$. Since both feature sets follow the same distribution, we here aim to determine a threshold that can identify that $Z$ and $Z'$ follow the same distribution. This threshold is later used to detect unknown images in real-world scenarios.\nSpecifically, for each $z' \\in Z'$, we compute the Euclidean distance $||z - z_i||_2$ for all $z_i \\in Z$. We then sort $Z$ in ascending order of distance and obtain the k-NN distance of $z'$ by:\n$d_k(z') = ||z - z_k||_2$ (7)\nwhere $z_k$ is the $k$-th nearest neighbor. Repeating this process for all elements in $Z'$, we obtain a set of $k$-NN distances. Finally, we sort these distances in descending order and set the threshold at the 95th percentile of the distances. This ensures that at least 95% of the $k$-NN distances are below this value, in which case we consider that the feature distribution in $Z'$ matches $Z$. Note that 95% is a commonly used [9,35] to confidently determine distribution alignment. Additionally, the thresholds do not rely on any unseen test data and are independent of the generative model, making them generalizable across different models."}, {"title": "3.3.4 Fake Image Detection", "content": "Finally, given a set of unknown test images in the real world, we follow a two-step strategy to distinguish between real and fake images and further classify fake images as known or unknown.\nFirst, we apply the same process as described earlier to obtain the feature representation $z^*$ of unknown test images and compute the k-NN distance $d_k(z^*)$ for each image. We then compare $d_k(z^*)$ to the threshold $\\tau$. If the distance is smaller than $\\tau$, the image is classified as a known fake since it follows the distribution of the reference feature set $Z$, which represents previously known fake images.\nSecond, for the remaining images-a mix of real and unknown fake images-we feed them to the known generative model $G(.)$ to obtain the re-generate versions, denoted as pseudo-fake samples $G(I_{pseudo})$. We then repeat the feature extraction process and compute the k-NN distance as follows:\n$x_{pseudo} = f_\\theta(G(I_{pseudo}))$\n$x_{pseudo\\_pruned} = P(x_{pseudo}; c)$\n$z_{pseudo} = \\text{t-SNE}(x_{pseudo\\_pruned})$\n$d_k(z_{pseudo}) = ||z_{pseudo} - z_k||_2$ (8)\nThe $d_k(z_{pseudo})$ is compared to $\\tau$. If the distance is smaller than $\\tau$, the image is classified as real, as only real images produce pseudo-fake samples with the same artifacts and features as known fake images. Conversely, if the distance is larger than $\\tau$, the image is classified as an unknown fake.\nThe two-step detection procedure is formalized as follows:\n$\\hat{y} = \\begin{cases} \\text{Fake}, & d_k(z^*) < \\tau \\\\ \\text{Real}, & d_k(z^*) > \\tau \\text{ and } d_k(z_{pseudo}) < \\tau \\\\ \\text{Fake}, & d_k(z_{pseudo}) > \\tau \\end{cases}$ (9)\nThe detailed steps of this method are outlined in Algorithm 1. By leveraging post-hoc distribution alignment, this approach enables effective detection of AI-generated images without requiring retraining for each new generative model."}, {"title": "4 Experiments", "content": "In this section, we evaluate the effectiveness of our Post-hoc Distribution Alignment (PAD) method for detecting AI-generated images.\nComparison to Existing Methods. As shown in Table 1, which presents the ACC and AP results of different detection methods, our PDA method demonstrates superior performance across various generative models. The Benchmark method, which trains a classifier on SD-generated images, exhibits poor generalization to other models, particularly for ADM, where the detection accuracy drops to 54.7%. In contrast, our method achieves 98.12% accuracy by leveraging"}, {"title": "4.1 Experimental Setup", "content": "We systematically analyze the effect of the number of neighbors $k$ on the performance of our KNN-based detection approach. In this experiment, we vary $k$ across the values $\\{1, 10, 20, 30, 40,50\\}$ and evaluate the performance of the method in terms of detection accuracy. As shown in Figure Figure 6a, our method achieves superior performance and remains stable across different values of $k$, except when $k = 1$. The performance for other values of $k$ shows consistent results and has little impact on the detection performance across different generative models. In Figure Figure 6b, we demonstrate why the performance is poor when $k = 1$. Specifically, the nearest neighbor reference set may contain outliers, such as the red-circled data points that fall outside the distribution (anomalous points). These outliers can affect the KNN distance distribution, causing fake samples (e.g., those generated by VQDM) to have small distances to the nearest neighbors after secondary generation, making it difficult to distinguish them from real samples. By using a larger $k = 10$ (or other values), we effectively mitigate the impact of these outliers, leading to more accurate detection."}, {"title": "4.1.1 Generation Models and Datasets", "content": "We evaluate our method on images generated by various generative models, including Generative Adversarial Networks (GANs), diffusion models, and text-to-image models. These models provide a diverse set of fake images for testing the robustness and generalization of our method, including Stable Diffusion V1.4 (SD) [28], GLIDE [24], VQDM [11], ADM [8], BigGAN [3] and Wukong [42]. The details of generation models are in Appendix A.\nOur experiments are conducted on the GenImage dataset [48], a large-scale benchmark specifically designed for evaluating the detection of AI-generated images. GenImage comprises over 1.33 million real images and 1.35 million fake images, totaling 2.68 million images, making it one of the most comprehensive datasets for this task. The real images are sourced from ImageNet [7], a widely recognized and high-quality dataset, ensuring the diversity and representativeness of the real image distribution. In experiments, we evaluate the generalization ability of detection methods using 3,000 fake images generated by each generative model and 3,000 real images."}, {"title": "4.1.2 Evaluation Metrics", "content": "Following existing generated-image detection methods [25, 30, 36, 41], we report accuracy (ACC) and average precision (AP) to evaluate the detectors. Since our method applies a two-step threshold-based classification strategy, ACC is calculated as:\n$\\text{ACC} (%) = 100 \\times \\frac{N_{\\text{correct1}} + N_{\\text{correct2}}}{N_{\\text{total}}}$ (10)\nwhere $N_{\\text{correct1}}$ is the number of correct classifications in the first step. $N_{\\text{correct2}}$ is the number of correct classifications in the second step. $N_{\\text{total}}$ is the total number of test samples.\nIn addition to the quantitative metrics, we provide qualitative visualizations of the feature space using t-SNE and KNN distance distributions. These visualizations demonstrate how well our method separates real and fake images, offering insights into the effectiveness of the distribution alignment process."}, {"title": "4.3 Ablation Studies", "content": "In this experiment, we explore the impact of activation pruning on the performance of our method. Recent works [9,34] have shown that out-of-distribution (OOD) data often exhibit high activations in certain feature dimensions, which can negatively impact detection performance. We apply activation pruning to suppress these high activation values and evaluate its effect on our method. As shown in Table 2, we observe that activation rectification improves detection performance by mitigating the impact of spurious activations. The rectified activations help refine decision boundaries and enhance model robustness against distribution shifts in the test images."}, {"title": "4.3.2 Impact of KNN", "content": "In this section, we investigate the impact of the training dataset size on the performance of our PAD-based fake image detection. We vary the number of fake images from 20,000 to 120,000, with equal numbers of real images in the dataset. As shown in Figure Figure 6c, increasing the size of the training dataset initially improves detection performance, with AP rising from 81.6% at 20,000 samples to 94.15% at 60,000 samples. However, as the dataset size continues to increase, the detection performance stabilizes at a high level. This highlights the efficiency and scalability of"}, {"title": "4.3.1 Impact of Activations Pruning", "content": "We systematically analyze the effect of the number of neighbors k on the performance of our KNN-based detection approach. In this experiment, we vary k across the values $\\{1, 10, 20, 30, 40,50\\}$ and evaluate the performance of the method in terms of detection accuracy. As shown in Figure Figure 6a, our method achieves superior performance and remains stable across different values of $k$, except when $k = 1$. The performance for other values of $k$ shows consistent results and has little impact on the detection performance across different generative models. In Figure Figure 6b, we demonstrate why the performance is poor when $k = 1$. Specifically, the nearest neighbor reference set may contain outliers, such as the red-circled data points that fall outside the distribution (anomalous points). These outliers can affect the KNN distance distribution, causing fake samples (e.g., those generated by VQDM) to have small distances to the nearest neighbors after secondary generation, making it difficult to distinguish them from real samples. By using a larger $k = 10$ (or other values), we effectively mitigate the impact of these outliers, leading to more accurate detection."}, {"title": "4.3.3 Impact of Training Dataset Size", "content": "In this section, we investigate the impact of the training dataset size on the performance of our PAD-based fake image detection. We vary the number of fake images from 20,000 to 120,000, with equal numbers of real images in the dataset. As shown in Figure Figure 6c, increasing the size of the training dataset initially improves detection performance, with AP rising from 81.6% at 20,000 samples to 94.15% at 60,000 samples. However, as the dataset size continues to increase, the detection performance stabilizes at a high level. This highlights the efficiency and scalability of"}, {"title": "4.3.4 Impact of Network Backbone", "content": "We also evaluate the effect of the network backbone of feature extractors on the performance of our detection method. Specifically, we test our method with different network architectures, including ResNet-18 [12], ResNet-50 [12], VGG-19 [32], which are suitable for various applications. As in previous experiments, we apply our method to the penultimate layer output and perform activation rectification. The results in Table 3 show that, even with different network backbones, our method consistently achieves superior detection performance with an AP of above 96%. This demonstrates that our approach is effective across various network architectures and can be seamlessly integrated with existing binary classifiers to further enhance their performance, using only fake images generated from a single known generative model."}, {"title": "4.3.5 Frequency Analysis", "content": "To gain insights into the distinctive artifacts produced by different generative models, we conduct a frequency analysis of real and fake images. Following the approach used in [30,47], we calculate the average of Fourier transform outputs of 3,000 real images and 3,000 fake images from each generative model, respectively. We utilize the Fourier transform for its ability to reveal latent features within the images.\nAs shown in Figure 5, the first row displays the results of unprocessed original images. Fake images generated by different models show distinct artifacts in the frequency spectrum, which are different from the real images. The second row shows the corresponding pseudo-fake images generated by the known generative model SD. The pseudo-fake images of real images exhibit the same artifact patterns as SD-generated images, while images generated by ADM show a more distinct artifact patterns (more visualization results are in Appendix C). This observation supports our hypothesis that real images, when re-generated using a known generative model, align with the distribution of known fake images due to sharing the same artifact features, whereas unseen fake images from other generative models exhibit persistent distribution shifts."}, {"title": "4.4 Robustness Evaluation", "content": "In this section, we evaluate the robustness of our PAD method under different image transformations with varying parameters, including Gaussian blurring [19], image compression.\nGaussian Blurring is commonly used to reduce image detail and noise by averaging pixels in a local neighborhood, resulting in a smoother image. We apply Gaussian blur with different kernel sizes to assess the effect on detection performance.\nImage compression reduces image size by removing less important visual details. We test the effect of compression using different quality factors (QF), where lower QF values indicate lower image quality.\nAs shown in Table 4, our PAD method demonstrates strong robustness across various image transformations. The performance, in terms of ACC and AP, remains high even under challenging conditions. Notably, our method achieves AP scores of over 96% despite the introduction of noise and image distortions. These results highlight the effectiveness and resilience of the PAD method in real-world applications, where images are often subject to various image transformations."}, {"title": "5 Conclusion & Future Work", "content": "We introduced Post-hoc Distribution Alignment (PDA), a novel method for the generalizable detection of AI-generated images. Unlike prior methods, PDA is model-agnostic and does not require retraining on new generative models. It employs a two-step detection strategy that uses distribution alignment to separate real and fake images, including those generated by unknown models. Extensive experiments show that PDA outperforms existing methods such as DIRE and ZeroFake, achieving 96.73% average accuracy across various generative models. PDA offers a scalable and efficient solution for fake image detection, with potential for real-world applications where new generative models are constantly emerging.\nIn future work, we will explore the impact of adversarial examples and other types of distribution shifts, as well as explore the integration of PDA into real-time detection systems. Additionally, we aim to assess PDA's performance in detecting AI-generated images from emerging generative models, particularly in real-world applications such as social media platforms and large-scale generative model APIs, to validate its practical utility and scalability."}, {"title": "A Description of Generation Models", "content": "In this section, we provide detailed descriptions of the generative models evaluated in our experiments. These models represent a variety of approaches to image generation, including diffusion-based models, text-to-image models, and GAN-based models.\nStable Diffusion V1.4 (SD) [28]. A latent diffusion model that generates high-quality images by iteratively denoising a random latent vector. It is trained on large-scale datasets and is known for its ability to produce highly realistic images with fine details.\nGLIDE [24]. A text-to-image diffusion model that leverages guided diffusion to generate images conditioned on textual descriptions. GLIDE is notable for its ability to synthesize diverse and semantically meaningful images based on complex prompts.\nVQDM [11]. A variant of diffusion models that combines vector quantization with diffusion processes. VQDM performs image generation by discretizing the data distribution, which allows it to efficiently generate high-resolution images with more diversity.\nADM [8]. A class of diffusion models that systematically removes components (e.g., attention mechanisms) to study their impact on generation quality. ADM is widely used for benchmarking due to its modular design and strong performance.\nBigGAN [3]. A large-scale GAN model that generates high-resolution images by leveraging a combination of class-conditional batch normalization and orthogonal regularization. BigGAN is known for its ability to produce diverse and realistic images across multiple categories.\nWukong [42]. A state of the art text-to-image generation model trained on a massive dataset of Chinese text-image pairs. Wukong is particularly challenging for detection tasks due to its high-quality outputs and strong generalization capabilities."}, {"title": "B More Visualization Results.", "content": "In addition to the results for ADM and Wukong presented in the main text, we provide t-SNE visualizations and KNN distance distributions for SD, VQDM, GLIDE, and BigGAN in Figure 7 and Figure 8. These results further validate the effectiveness of our proposed method across diverse generative models.\nKey observations include: 1) similar to Wukong, SD-generated images align well with the distribution of known fake samples in the raw feature space, as shown in the first row of Figure 7. This alignment confirms that the feature extractor, trained on SD, effectively captures shared artifacts, enabling reliable detection of SD-generated images in the first step of our threshold-based classification; 2) the first row of Figure 7 demonstrates unknown fake samples from VQDM, GLIDE, and BigGAN exhibit significant overlap with real images, highlighting the challenge of generalizing detection to unseen generative models; 3) The second row of Figure 7 and Figure 8 reveals that pseudo-fake versions of real images align with the known fake distribution (with smaller KNN distances), while unknown fake samples from VQDM, GLIDE, and BigGAN show persistent and more pronounced distribution shifts. This distinction enables our method to effectively separate real images from unknown fake samples."}, {"title": "C More Frequency Analysis.", "content": "We provide visualization frequency analysis for GLIDE, VQDM, BigGAN, and Wukong in Figure 9. These results further validate our hypothesis and demonstrate the distinct artifact patterns introduced by different generative models. The first row of Figure 9 shows the results of original test images. Fake images generated by GLIDE, VQDM, BigGAN exhibit distinct artifact patterns in the frequency domain, which differ from the natural spectral distribution of real images. Wukong shares similar artifact patterns with SD, allowing it to be easily identified in the first stage with our PAD-based detection method.\nThe second row of Figure 9 illustrates the spectral distributions of pseudo-fake images generated by the known generative model SD. These pseudo-fake images from real align closely with the artifact patterns of SD-generated images, confirming that real images, when re-generated by a known model, adopt the same artifact features. In contrast, images generated by GLIDE, VQDM, and BigGAN exhibit distinct and persistent distribution shifts, further highlighting the unique artifact patterns of each generative model."}]}