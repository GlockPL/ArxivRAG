{"title": "An Application of Large Language Models to Coding Negotiation Transcripts", "authors": ["Ray Friedman", "Jaewoo Cho", "Jeanne Brett", "Xuhui Zhan", "Ningyu Han", "Sriram Kannan", "Yingxiang Ma", "Jesse Spencer-Smith", "Elisabeth J\u00e4ckel", "Alfred Zerres", "Madison Hooper", "Katie Babbitt", "Manish Acharya", "Wendi Adair", "Soroush Aslani", "Tayfun Ayka\u00e7", "Chris Baumann", "Rebecca Bennett", "Garrett Brady", "Peggy Briggs", "Cheryl Dowie", "Chase Eck", "Igmar Geiger", "Frank Jacob", "Molly Kern", "Sujin Lee", "Leigh Anne Liu", "Wu Li", "Jeffrey Loewenstein", "Anne Lytle", "Li Ma", "Michel Mann", "Alexandra Mislin", "Tyree Mitchell", "Hannah Martensen n\u00e9e Nagler", "Amit Nandkeolyar", "Mara Olekalns", "Elena Paliakova", "Jennifer Parlamis", "Jason Pierce", "Nancy Pierce", "Robin Pinkley", "Nathalie Prime", "Jimena Ramirez-Marin", "Kevin Rockmann", "William Ross", "Zhaleh Semnani-Azad", "Juliana Schroeder", "Philip Smith", "Elena Stimmer", "Roderick Swaab", "Leigh Thompson", "Cathy Tinsley", "Ece Tuncel", "Laurie Weingart", "Robert Wilken", "JingJing Yao", "Zhi-Xue Zhang"], "abstract": "In recent years, Large Language Models (LLM) have demonstrated impressive capabilities\nin the field of natural language processing (NLP). This paper explores the application of LLMs\nin negotiation transcript analysis by the Vanderbilt AI Negotiation Lab. Starting in September", "sections": [{"title": "Introduction", "content": "The field of natural language processing (NLP) has taken the spotlight with significant\nadvancements in recent years, largely due to the emergence of Large Language Models (LLMs)\nand ChatGPT. These models are pre-trained on diverse and large datasets and demonstrate\nremarkable capabilities in understanding and generating human language. As LLMs continue\nto evolve and improve, new possibilities and capabilities for application across multiple domains\nare constantly emerging. Our study tests whether we can apply LLMs to automatically classify\neach sentence (or thought unit or speaking turn) in a negotiation transcript based on categories\nrelevant to negotiation scholars (e.g. Weingart et al., 1990[16]; Gunia et al., 2011[7]; Adair &\nBrett, 2005[4]).\nOne area where LLMs excel is in classification tasks, such as sentiment analysis, docu-\nment classification, and summarization (Naveen et al., 2023). LLMs are good at classification\ntasks because of contextual understanding, transfer learning, and semantic feature extraction\n(Kalyan, 2024[9]). This capability can be used to automate the slow and costly process of\ncoding transcripts, which is a procedure used in many negotiation studies. Scholars in many\nfields (e.g., psychology, management, or communications) study negotiations by having people\nconduct simulated negotiations, audio recording them, and looking for patterns in negotiators'\nverbal behaviors. For example, scholars may be interested in information sharing in negotia-\ntions and when it is most effective (Adair et al., 2001[2]), the impact of cooperative orientation\non use of threats (Weingart, et al., 1993[18]), or the impact of supporting arguments on ne-\ngotiation outcomes (Putnam & Jones, 1982[14]). A necessary step in these kinds of studies is\nto develop a coding scheme, train coders, ensure inter-coder agreement, and have those coders\ncode hundreds or thousands of speech units. This process of human coding can take months,\nand cost thousands of dollars.\nResearchers can save time and money by automating this coding process. Additionally,\nautomated coding reduces the risks that are inherent in human coding - such as coder fatigue\ndue to coding too many transcripts in a sitting or coder drift in applying codes across a number\nof transcripts. Indeed, having an automatic coding process may make some projects feasible\nthat previously were not feasible. This study explores whether, and how, LLMs can be used to\nreplace (or supplement) human coders, which should increase both the efficiency and reliability\nof coding for negotiation research. What we learned in the process of negotiation coding may\nalso be applicable to other coding-heavy research in areas such as medical summarization of\npatient visit notes, analyzing corporate financial reports, or any number of similar tasks.\nTo do this research Ray Friedman from the Owen School of Business at Vanderbilt and\nJesse Spencer-Smith and Jaewoo Cho from the Vanderbilt Data Science Institute set up the\nAI Negotiation Lab at Vanderbilt University (https://AlNegotiationLab-Vanderbilt.com/)\nin September 2022. The mission of the AI Negotiation Lab is the application of LLMs\nto support the research on conflict management and negotiation. This is the first project\nof this lab. The project is supported by Negotiation and Team Resources (https://www.\nnegotiationandteamresources.com/). This paper provides a record of the multiple experi-\nments we conducted, each informed by the success or failure of the prior experiments, as we\ndeveloped the LLM Vanderbilt Negotiation Coder."}, {"title": "Selecting a Coding Scheme", "content": "As a starting point, we had to choose a coding scheme that the LLM model would learn.\nNegotiation scholars have used many different coding schemes. Just as this project began, a\ngroup of scholars reviewed the published negotiation coding schemes and developed an omnibus\nor \"master\" coding scheme which we call the J\u00e4ckel Master (J\u00e4ckel et al., 2022[8]). The J\u00e4ckel\nMaster has 47 codes, with definitions of each and example sentences to be used in the training\nof coders. Three of those codes could not be used in this project since coding them requires\nknowledge of specific elements of the negotiation scenario. For example, we could not code\n\"lying\" since the model would have no basis to judge what was true and false. The two other\ncodes not used were \"omission\" and \"use of extreme anchors\". We also dropped \"change of\nmode\" since submitted transcripts may not be set up to indicate when, for example, negotiators\nswitched from live to online negotiations. We initially attempted to build an LLM model that\nwould use all 44 remaining codes, but this model would not even run. We quickly learned that\nLLM categorization typically does not work well with more than about ten to twenty codes.\nAt this point, we worked with Elizabeth J\u00e4ckel to reduce the number of codes, by selecting the\nmost frequently used codes (based on her transcript samples) and by combining some codes.\nThe result was 18 codes plus an \"other\" category, producing 19 codes. The initial full J\u00e4ckel\ncoding scheme is shown in Table 2, and the simplified 19-code version is shown in Table 1."}, {"title": "Three LLM Model Coding Strategies", "content": "We tested three coding strategies with these 770 sentences: zero-shot, fine-tuning a new\nmodel, and in-context learning. In each case, the model was deemed accurate for cases where\nit assigned a code to a sentence that the sentence was formulated to represent.\nZero-Shot. For Experiment 1, we implemented a zero-shot strategy, which refers to using\nmachine learning models \"out of the box\" that is, just as they exist, without any domain-\nspecific adaption (Xian, et al, 2020). In this strategy we tested whether established models\n(such as GPT3 or BART or BERT) can successfully code negotiation sentences based just\non preexisting general knowledge about language that is built into those base models. We\ntried this approach using open-source models BERT (Bidirectional Encoder Representations\nTransformers) from Google and BART (Bidirectional and Auto-Regressive Transformers) from\nMeta. We asked these models to code the 770 sample sentences that we created. This was\ndone using their established \"Inference API\" where you put text in one window (in our case\nthe sentences to be coded) and the \"class names\" in another window (in our case the codes),\nand the model allocates the material to the categories. It was clear that some codes could be\nunderstood based on general language knowledge (such as \"Negative Statements\") while others\nmade no sense to the LLM based on its general language knowledge. For example, a code that\ngave the LLM model trouble was \"asking for preference related information\". This zero-shot\nstrategy accurately coded only about 20% of the sentences we tested (the results were the same\nfor BERT and BART). The poor performance was expected since the models had only learned\ngeneral language, and not the specialized language of negotiation scholars. Still, this was an\nimportant first step to confirm that domain adaptation was needed. It also provided a baseline\nto see how much improvement we could make as we moved to fine-tuned models.\nFine-Tuning. For Experiment 2, we implemented a fine-tuning strategy. Fine-tuning is"}, {"title": "Transition to a Transcript-Based Training Approach", "content": "After all our refinements, we reached an accuracy level of 96%, which was extremely strong.\nWe thought we might have our final model. As a final check, we applied this model to four\ncomplete full negotiation transcripts that were coded for this project using the J\u00e4ckel 19 code\nscheme. The level of human-model match dropped from 96% to 30%. What became clear is\nthat we had taught the model with ideal sentences that were clear examples of codes, while\nreal-life conversations are more halting, broken, incomplete, and ambiguous. Moreover, the\nhumans who coded those five transcripts were able to judge a sentence based on the broader\ncontext the flow of the conversation, not just the sentence.\nIt became clear that we needed to train the model, not just with a set of \"pure\" example\nsentences, but also with sets of full, realistic, transcripts that had already been human-coded.\nSince the J\u00e4ckel et al (2022) [8] coding system was new, there was no corpus of transcripts\nthat were coded using their scheme. We turned to negotiation studies that had large numbers\nof human-coded transcripts. Here the problem was that each study used a coding scheme,\nchosen for that study's particular research questions and even when different researchers used\nthe same coding scheme, how the same set of codes was applied could vary across studies, based\non differences in conceptual focus or coder tendencies.\nIt became clear that to validate the transcript-trained LLM model against human coders\nwe needed to a) train the model with the specific codes used in that set of transcripts, and b)"}, {"title": "Coding Consistency Measure", "content": "As we tested different coding strategies, we realized that the model might not always code\na given sentence the same way every time, especially when using in-context learning, which\nhas the model learn afresh each time it is applied to a transcript. To test how consistently\nthe model coded a transcript's sentences, we decided to repeat the coding process five times,\nproducing five codes for each sentence. This process was then built into our program all\ncoding done by the model would be repeated five times. Some variation across five runs might\nbe acceptable, but before we would allow the model to report a code to users, the model had\nto give the same code at least three out of five times. If the model could not achieve that\nlevel of consistency, the model would report that \"no code was assigned\". While this results in\nscholars having to hand code that subset of sentences, the benefit is greater confidence in the\nsentences that the model did code consistently. The model reports consistency level: moderate\n(three out of five), high (four out of five) or perfect (five out of five) for all codes."}, {"title": "Technical Challenges", "content": "One problem we faced was that Claude 2 would skip some sentences, exhibiting what\nhas come to be known as model \"laziness\". Another problem was that the model would\nsometimes correct the grammar of the sentences it was coding, which might change the sentence\nmeaning and make it hard to match with the original sentences. We suspected that these model\nbehaviors might be due to reaching the capacity limits of Claude 2, so we turned to Claude 3\nOpus-Anthropic's most powerful model as of spring, 2024. Moving to Claude 3 Opus resolved\nthese issues.\nLength of transcript to be coded. We tested how long a transcript could be that we asked\nit to code. Trial and error led to the realization that it could only handle well about 100\nsentences. We programed our code to calculate the sentence count of submitted transcripts\nand split that transcript into two (or more) segments which were then coded separately (but\nthe results are shown together in the output)."}, {"title": "First Model Developed", "content": "The first model we developed with this method was Model 1, using a 13-code coding scheme\nand transcript database provided by Aslani et al (2014) [3]. A detailed report of this model,\nincluding the coding scheme, validation data, and how to submit transcripts for analysis, is\nreported in Friedman et al (2024) [6]. The next sections describe several substantive and tech-\nnical modifications that we tried before finalizing Model 1. Over time, additional models will\nbe provided, each with an orientation document like Friedman et al., (2024)[6]."}, {"title": "Improvement Experiments", "content": "Substantive. As we went through testing, we learned that the model's match level improved\n(had a higher match rate) with several additions. First, we instructed the model to pay\nattention to who was speaking (e.g., buyer or seller) and what was said in the sentences before\nand after speaking unit being coded. This was important when using full transcripts to train\nand code but would not have been relevant in our earlier \"ideal sentence\" approach since\nthose sentences were not within a conversation. Second, we added more instructions about\ncertain codes. That was done because, after looking at the confusion matrix of the initial runs,"}, {"title": "Model 1 Results", "content": "Here we provide summary results for Model 1. The effectiveness of the model coding real\nconversations (transcripts, rather than ideal sentences) improved dramatically when we moved\nfrom training with ideal sentences to training with transcripts (from 30% up to 70%, and then\n73% with additional adjustments and a move to Claude3). See Tables 6 and Figure 2. We\nalso conducted a mismatch analysis. This was done by training new coders and giving them\n100 randomly selected cases of mismatch. They were provided with a spreadsheet with the\nspeaking turn (along with the two prior speaking turns to understand the context), and the\ncodes assigned by the model and the human coders (shown without indicators of which code\nwas the model code and which was the human code, and with the order varied to eliminate\norder effects). These coders were asked to decide which of the two codes they thought was\ncorrect. This analysis revealed that the new coders agreed with the model's coding choice\n(rather than the human-coders' choice) 68% of the time, suggesting a true accuracy level 91%\nfor the model.\nHowever, this test had the advantage that the training and training transcripts both used\nthe same negotiation simulation The Sweet Shop. We further tested the model by asking it\nto code transcripts from a different simulation (6 from the Cartoon negotiation, and 6 from\nthe Les Flores negotiation 3). The match level came down to 65%, which is still strong given\nthat this level of agreement was reached for a 13-code coding scheme. Further analysis of\nmismatches suggested that approximately 82% of the model codes were accurate. Full results\nare included in Friedman et al (2024)[6], including kappa calculations, match levels by code,"}, {"title": "Discussion", "content": "This paper outlines the development of a customized LLM model for the purpose of coding\nnegotiation transcripts. However, the steps and procedures we took provide a roadmap for\napplying LLMs to coding for social science research more generally. There are currently many\nscholars looking for ways to automate the time-consuming and expensive process of coding. If\nAI assisted coding becomes possible, not only will it make current projects easier to complete\nbut will also make it feasible to work with even larger data sets that would be beyond what\nhumans could possibly code.\nOur work suggests that in-context learning is likely to be the most effective way to code.\nZero-shot coding is not viable for any coding scheme that depends on large number of codes\nthat identify special concepts, but it may still be possible if the concepts are ones often used in\nregular conversation (such as \u201cstress\u201d or \u201chappiness"}, {"title": "Appendix", "content": "Taking a strategy of in-context learning for LLMs requires awareness of the capacity limits\nof different models. This is referred to as the \"context length\", which is the maximum number\nof tokens or words that the model could remember within one session. After the maximum\nnumber of tokens is exceeded, the LLM will forget everything that happened within the current\nsession and return to a blank state. Tokens refers to units that the model processes which could\nbe letters, or clusters of letters, or words, or spaces. Context length is counted in number of\n\"tokens\" allowed in the prompt (the input sent to the model which provides instructions),\nwhich can be expressed in terms of an approximate word length."}]}