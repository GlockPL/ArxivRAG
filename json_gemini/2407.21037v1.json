{"title": "An Application of Large Language Models to Coding Negotiation Transcripts", "authors": ["Ray Friedman", "Jaewoo Cho", "Jeanne Brett", "Xuhui Zhan", "Ningyu Han", "Sriram Kannan", "Yingxiang Ma", "Jesse Spencer-Smith", "Elisabeth J\u00e4ckel", "Alfred Zerres", "Madison Hooper", "Katie Babbitt", "Manish Acharya", "Wendi Adair", "Soroush Aslani", "Tayfun Ayka\u00e7", "Chris Baumann", "Rebecca Bennett", "Garrett Brady", "Peggy Briggs", "Cheryl Dowie", "Chase Eck", "Igmar Geiger", "Frank Jacob", "Molly Kern", "Sujin Lee", "Leigh Anne Liu", "Wu Liu", "Jeffrey Loewenstein", "Anne Lytle", "Li Ma", "Michel Mann", "Alexandra Mislin", "Tyree Mitchell", "Hannah Martensen n\u00e9e Nagler", "Amit Nandkeolyar", "Mara Olekalns", "Elena Paliakova", "Jennifer Parlamis", "Jason Pierce", "Nancy Pierce", "Robin Pinkley", "Nathalie Prime", "Jimena Ramirez-Marin", "Kevin Rockmann", "William Ross", "Zhaleh Semnani-Azad", "Juliana Schroeder", "Philip Smith", "Elena Stimmer", "Roderick Swaab", "Leigh Thompson", "Cathy Tinsley", "Ece Tuncel", "Laurie Weingart", "Robert Wilken", "JingJing Yao", "Zhi-Xue Zhang"], "abstract": "In recent years, Large Language Models (LLM) have demonstrated impressive capabilities in the field of natural language processing (NLP). This paper explores the application of LLMs in negotiation transcript analysis by the Vanderbilt AI Negotiation Lab. Starting in September", "sections": [{"title": "Introduction", "content": "The field of natural language processing (NLP) has taken the spotlight with significant advancements in recent years, largely due to the emergence of Large Language Models (LLMs) and ChatGPT. These models are pre-trained on diverse and large datasets and demonstrate remarkable capabilities in understanding and generating human language. As LLMs continue to evolve and improve, new possibilities and capabilities for application across multiple domains are constantly emerging. Our study tests whether we can apply LLMs to automatically classify each sentence (or thought unit or speaking turn) in a negotiation transcript based on categories relevant to negotiation scholars (e.g. Weingart et al., 1990[16]; Gunia et al., 2011 [7]; Adair & Brett, 2005[4]).\nOne area where LLMs excel is in classification tasks, such as sentiment analysis, docu- ment classification, and summarization (Naveen et al., 2023). LLMs are good at classification tasks because of contextual understanding, transfer learning, and semantic feature extraction (Kalyan, 2024[9]). This capability can be used to automate the slow and costly process of coding transcripts, which is a procedure used in many negotiation studies. Scholars in many fields (e.g., psychology, management, or communications) study negotiations by having people conduct simulated negotiations, audio recording them, and looking for patterns in negotiators' verbal behaviors. For example, scholars may be interested in information sharing in negotia- tions and when it is most effective (Adair et al., 2001[2]), the impact of cooperative orientation on use of threats (Weingart, et al., 1993[18]), or the impact of supporting arguments on ne- gotiation outcomes (Putnam & Jones, 1982[14]). A necessary step in these kinds of studies is to develop a coding scheme, train coders, ensure inter-coder agreement, and have those coders code hundreds or thousands of speech units. This process of human coding can take months, and cost thousands of dollars.\nResearchers can save time and money by automating this coding process. Additionally, automated coding reduces the risks that are inherent in human coding - such as coder fatigue due to coding too many transcripts in a sitting or coder drift in applying codes across a number of transcripts. Indeed, having an automatic coding process may make some projects feasible that previously were not feasible. This study explores whether, and how, LLMs can be used to replace (or supplement) human coders, which should increase both the efficiency and reliability of coding for negotiation research. What we learned in the process of negotiation coding may also be applicable to other coding-heavy research in areas such as medical summarization of patient visit notes, analyzing corporate financial reports, or any number of similar tasks.\nTo do this research Ray Friedman from the Owen School of Business at Vanderbilt and Jesse Spencer-Smith and Jaewoo Cho from the Vanderbilt Data Science Institute set up the AI Negotiation Lab at Vanderbilt University (https://AlNegotiationLab-Vanderbilt.com/) in September 2022. The mission of the AI Negotiation Lab is the application of LLMs to support the research on conflict management and negotiation. This is the first project of this lab. The project is supported by Negotiation and Team Resources (https://www. negotiationandteamresources.com/). This paper provides a record of the multiple experi- ments we conducted, each informed by the success or failure of the prior experiments, as we developed the LLM Vanderbilt Negotiation Coder."}, {"title": "Selecting a Coding Scheme", "content": "As a starting point, we had to choose a coding scheme that the LLM model would learn. Negotiation scholars have used many different coding schemes. Just as this project began, a group of scholars reviewed the published negotiation coding schemes and developed an omnibus or \"master\" coding scheme which we call the J\u00e4ckel Master (J\u00e4ckel et al., 2022[8]). The J\u00e4ckel Master has 47 codes, with definitions of each and example sentences to be used in the training of coders. Three of those codes could not be used in this project since coding them requires knowledge of specific elements of the negotiation scenario. For example, we could not code \"lying\" since the model would have no basis to judge what was true and false. The two other codes not used were \"omission\" and \"use of extreme anchors\". We also dropped \"change of mode\" since submitted transcripts may not be set up to indicate when, for example, negotiators switched from live to online negotiations. We initially attempted to build an LLM model that would use all 44 remaining codes, but this model would not even run. We quickly learned that LLM categorization typically does not work well with more than about ten to twenty codes.\nAt this point, we worked with Elizabeth J\u00e4ckel to reduce the number of codes, by selecting the most frequently used codes (based on her transcript samples) and by combining some codes. The result was 18 codes plus an \"other\" category, producing 19 codes. The initial full J\u00e4ckel coding scheme is shown in Table 2, and the simplified 19-code version is shown in Table 1."}, {"title": "Creating Ideal Sentences", "content": "Our next step was to develop a list of examples sentences for each code that would be extensive enough for the model to learn the codes and test applying them. We call this the \"Ideal Sentences\" approach since we use sentences that were deemed perfect exemplars of each code. Some example sentences were provided in the coding manual for J\u00e4ckel et al (2022[8]),"}, {"title": "Three LLM Model Coding Strategies", "content": "We tested three coding strategies with these 770 sentences: zero-shot, fine-tuning a new model, and in-context learning. In each case, the model was deemed accurate for cases where it assigned a code to a sentence that the sentence was formulated to represent.\nZero-Shot. For Experiment 1, we implemented a zero-shot strategy, which refers to using machine learning models \"out of the box\" that is, just as they exist, without any domain- specific adaption (Xian, et al, 2020). In this strategy we tested whether established models (such as GPT3 or BART or BERT) can successfully code negotiation sentences based just on preexisting general knowledge about language that is built into those base models. We tried this approach using open-source models BERT (Bidirectional Encoder Representations Transformers) from Google and BART (Bidirectional and Auto-Regressive Transformers) from Meta. We asked these models to code the 770 sample sentences that we created. This was done using their established \"Inference API\" where you put text in one window (in our case the sentences to be coded) and the \"class names\" in another window (in our case the codes), and the model allocates the material to the categories. It was clear that some codes could be understood based on general language knowledge (such as \"Negative Statements\") while others made no sense to the LLM based on its general language knowledge. For example, a code that gave the LLM model trouble was \"asking for preference related information\". This zero-shot strategy accurately coded only about 20% of the sentences we tested (the results were the same for BERT and BART). The poor performance was expected since the models had only learned general language, and not the specialized language of negotiation scholars. Still, this was an important first step to confirm that domain adaptation was needed. It also provided a baseline to see how much improvement we could make as we moved to fine-tuned models.\nFine-Tuning. For Experiment 2, we implemented a fine-tuning strategy. Fine-tuning is"}, {"title": "Transition to a Transcript-Based Training Approach", "content": "After all our refinements, we reached an accuracy level of 96%, which was extremely strong. We thought we might have our final model. As a final check, we applied this model to four complete full negotiation transcripts that were coded for this project using the J\u00e4ckel 19 code scheme. The level of human-model match dropped from 96% to 30%. What became clear is that we had taught the model with ideal sentences that were clear examples of codes, while real-life conversations are more halting, broken, incomplete, and ambiguous. Moreover, the humans who coded those five transcripts were able to judge a sentence based on the broader context the flow of the conversation, not just the sentence.\nIt became clear that we needed to train the model, not just with a set of \"pure\" example sentences, but also with sets of full, realistic, transcripts that had already been human-coded. Since the J\u00e4ckel et al (2022) [8] coding system was new, there was no corpus of transcripts that were coded using their scheme. We turned to negotiation studies that had large numbers of human-coded transcripts. Here the problem was that each study used a coding scheme, chosen for that study's particular research questions and even when different researchers used the same coding scheme, how the same set of codes was applied could vary across studies, based on differences in conceptual focus or coder tendencies.\nIt became clear that to validate the transcript-trained LLM model against human coders we needed to a) train the model with the specific codes used in that set of transcripts, and b)"}, {"title": "Coding Consistency Measure", "content": "As we tested different coding strategies, we realized that the model might not always code a given sentence the same way every time, especially when using in-context learning, which has the model learn afresh each time it is applied to a transcript. To test how consistently the model coded a transcript's sentences, we decided to repeat the coding process five times, producing five codes for each sentence. This process was then built into our program all coding done by the model would be repeated five times. Some variation across five runs might be acceptable, but before we would allow the model to report a code to users, the model had to give the same code at least three out of five times. If the model could not achieve that level of consistency, the model would report that \"no code was assigned\". While this results in scholars having to hand code that subset of sentences, the benefit is greater confidence in the sentences that the model did code consistently. The model reports consistency level: moderate (three out of five), high (four out of five) or perfect (five out of five) for all codes."}, {"title": "Technical Challenges", "content": "One problem we faced was that Claude 2 would skip some sentences, exhibiting what has come to be known as model \"laziness\". Another problem was that the model would sometimes correct the grammar of the sentences it was coding, which might change the sentence meaning and make it hard to match with the original sentences. We suspected that these model behaviors might be due to reaching the capacity limits of Claude 2, so we turned to Claude 3 Opus-Anthropic's most powerful model as of spring, 2024. Moving to Claude 3 Opus resolved these issues.\nLength of transcript to be coded. We tested how long a transcript could be that we asked it to code. Trial and error led to the realization that it could only handle well about 100 sentences. We programed our code to calculate the sentence count of submitted transcripts and split that transcript into two (or more) segments which were then coded separately (but the results are shown together in the output)."}, {"title": "First Model Developed", "content": "The first model we developed with this method was Model 1, using a 13-code coding scheme and transcript database provided by Aslani et al (2014) [3]. A detailed report of this model, including the coding scheme, validation data, and how to submit transcripts for analysis, is reported in Friedman et al (2024) [6]. The next sections describe several substantive and tech- nical modifications that we tried before finalizing Model 1. Over time, additional models will be provided, each with an orientation document like Friedman et al., (2024)[6]."}, {"title": "Improvement Experiments", "content": "Substantive. As we went through testing, we learned that the model's match level improved (had a higher match rate) with several additions. First, we instructed the model to pay attention to who was speaking (e.g., buyer or seller) and what was said in the sentences before and after speaking unit being coded. This was important when using full transcripts to train and code but would not have been relevant in our earlier \"ideal sentence\" approach since those sentences were not within a conversation. Second, we added more instructions about certain codes. That was done because, after looking at the confusion matrix of the initial runs,"}, {"title": "Model 1 Results", "content": "Here we provide summary results for Model 1. The effectiveness of the model coding real conversations (transcripts, rather than ideal sentences) improved dramatically when we moved from training with ideal sentences to training with transcripts (from 30% up to 70%, and then 73% with additional adjustments and a move to Claude3). See Tables 6 and Figure 2. We also conducted a mismatch analysis. This was done by training new coders and giving them 100 randomly selected cases of mismatch. They were provided with a spreadsheet with the speaking turn (along with the two prior speaking turns to understand the context), and the codes assigned by the model and the human coders (shown without indicators of which code was the model code and which was the human code, and with the order varied to eliminate order effects). These coders were asked to decide which of the two codes they thought was correct. This analysis revealed that the new coders agreed with the model's coding choice (rather than the human-coders' choice) 68% of the time, suggesting a true accuracy level 91% for the model.\nHowever, this test had the advantage that the training and training transcripts both used the same negotiation simulation The Sweet Shop. We further tested the model by asking it to code transcripts from a different simulation (6 from the Cartoon negotiation, and 6 from the Les Flores negotiation 3). The match level came down to 65%, which is still strong given that this level of agreement was reached for a 13-code coding scheme. Further analysis of mismatches suggested that approximately 82% of the model codes were accurate. Full results are included in Friedman et al (2024)[6], including kappa calculations, match levels by code,"}, {"title": "Discussion", "content": "This paper outlines the development of a customized LLM model for the purpose of coding negotiation transcripts. However, the steps and procedures we took provide a roadmap for applying LLMs to coding for social science research more generally. There are currently many scholars looking for ways to automate the time-consuming and expensive process of coding. If AI assisted coding becomes possible, not only will it make current projects easier to complete but will also make it feasible to work with even larger data sets that would be beyond what humans could possibly code.\nOur work suggests that in-context learning is likely to be the most effective way to code. Zero-shot coding is not viable for any coding scheme that depends on large number of codes that identify special concepts, but it may still be possible if the concepts are ones often used in regular conversation (such as \u201cstress\u201d or \u201chappiness\"). Domain-adapting a model also appears much less effective than in-context learning. Our work also suggests that while training with \"ideal\" examples is the cleanest way to code, it may not teach the model to understand the kind of language that is encountered in real conversation or speaking. The best approach for coding real conversations is to train on real conversations.\nThe challenge that creates is that you need a large corpus of coded material to train with exactly when the purpose of using LLM coding is to avoid having to human code your data set. However, the number of transcripts you need to human code to build a LLM model should be much less than your whole data set. For our model, it took five transcripts to provide in-context learning of the coding scheme, and we were able to validate the model using"}, {"title": "Appendix", "content": "Taking a strategy of in-context learning for LLMs requires awareness of the capacity limits of different models. This is referred to as the \"context length\", which is the maximum number of tokens or words that the model could remember within one session. After the maximum number of tokens is exceeded, the LLM will forget everything that happened within the current session and return to a blank state. Tokens refers to units that the model processes which could be letters, or clusters of letters, or words, or spaces. Context length is counted in number of \"tokens\" allowed in the prompt (the input sent to the model which provides instructions), which can be expressed in terms of an approximate word length."}]}