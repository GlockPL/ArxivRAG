{"title": "YouTube Comments Decoded: Leveraging LLMs for Low Resource Language Classification", "authors": ["Aniket Deroy", "Subhankar Maity"], "abstract": "Sarcasm detection is a significant challenge in sentiment analysis, particularly due to its nature of conveying opinions where the intended meaning deviates from the literal expression. This challenge is heightened in social media contexts where code-mixing, especially in Dravidian languages, is prevalent. Code-mixing involves the blending of multiple languages within a single utterance, often with non-native scripts, complicating the task for systems trained on monolingual data. This shared task introduces a novel gold standard corpus designed for sarcasm and sentiment detection within code-mixed texts, specifically in Tamil-English and Malayalam-English languages. The primary objective of this task is to identify sarcasm and sentiment polarity within a code-mixed dataset of Tamil-English and Malayalam-English comments and posts collected from social media platforms. Each comment or post is annotated at the message level for sentiment polarity, with particular attention to the challenges posed by class imbalance, reflecting real-world scenarios.In this work, we experiment with state-of-the-art large language models like GPT-3.5 Turbo via prompting to classify comments into sarcastic or non-sarcastic categories. We obtained a macro-F1 score of 0.61 for Tamil language. We obtained a macro-F1 score of 0.50 for Malayalam language.", "sections": [{"title": "1. Introduction", "content": "Sarcasm, a form of expression where the intended meaning sharply diverges from the literal meaning, poses a significant challenge for sentiment analysis systems [1, 2, 3]. This complexity arises because sarcasm often involves subtle linguistic cues and context-dependent interpretation, making it difficult for automated systems to accurately detect and classify [4, 5, 6, 7, 8].\nOverall, the study of code-mixing in NLP is an evolving field with ongoing research aimed at improving the performance of various NLP tasks in multilingual contexts. As multilingualism continues to grow globally, the development of models that can effectively handle code-mixed data is becoming increasingly important.\nCode-mixing [9, 10, 11] in NLP refers to the blending of two or more languages within a single sentence or conversation, commonly seen in multilingual societies. It poses unique challenges for tasks like language identification, part-of-speech tagging, and sentiment analysis. Traditional NLP models often struggle with code-mixed data due to the complex interaction between languages. To address this, researchers have developed advanced neural network models, such as RNNs and CNNs, that better capture linguistic context. Code-mixed text also complicates machine translation and speech recognition, requiring specialized approaches to handle language switching effectively. As multilingualism becomes more prevalent, the importance of developing robust NLP systems capable of processing code-mixed data continues to grow.\nCode-mixing is a widespread phenomenon in multilingual societies, particularly in Dravidian lan- guages such as Tamil and Malayalam, where users frequently switch between native languages and English [12, 9, 13]. These code-mixed texts often appear in non-native scripts, such as Roman, due to the convenience of typing on digital platforms [9, 14, 15]. This creates a unique set of challenges for natural language processing (NLP) systems, which are typically trained on monolingual data and thus struggle to handle the intricacies of code-switching across different linguistic levels, such as syntax, semantics, and phonetics [16, 17].\nThe Dravidian languages, with their rich cultural and linguistic history, present a distinct case for code-mixed text analysis [18, 19]. Tamil, with its official status in India, Sri Lanka, and Singapore, and a significant diaspora community, has a script that evolved from ancient scripts like Tamili and the Chola-Pallava script [20, 21, 22]. Similarly, Malayalam, predominantly spoken in Kerala, India, employs an alpha-syllabic script that blends alphabetic and syllable-based writing systems [23, 24, 25]. However, on social media, these languages are often typed using the Roman script, contributing to a growing corpus of code-mixed data.\nTo address these challenges, this paper introduces a prompt-based method [12, 26, 27] using GPT-3.5 Turbo [28, 29] for sarcasm and sentiment detection in code-mixed texts, specifically focusing on Tamil- English and Malayalam-English language pairs. This shared task [30, 31, 32, 33, 34, 35, 36, 37, 38, 39] aims to advance research in sentiment analysis by providing a dataset that reflects real-world scenarios where code-mixing is prevalent and by encouraging the development of systems that can accurately classify sarcasm and sentiment polarity in these under-resourced languages.\nParticipants in this task were provided development, training, and test datasets composed of comments and posts collected from social media platforms such as YouTube.\nIn this work, we explore the capabilities of state-of-the-art large language models, specifically GPT-3.5 Turbo [28, 39], by leveraging prompt-based techniques to classify social media comments into sarcastic or non-sarcastic categories. GPT-3.5 Turbo [28], a variant of OpenAI's GPT-3.5 model, is known for its advanced natural language understanding and generation capabilities, making it a promising candidate for handling the nuanced task of sarcasm detection, particularly in code-mixed texts.\nOur team, TextTitans, participated in a competition aimed at advancing sarcasm detection in Tamil and Malayalam. The results of our systems' performance provide valuable insights into the current state of NLP for these languages. For Tamil, our system achieved a macro-F1 score of 0.61, placing us 9th among competing teams. The macro-F1 score, which balances precision and recall across multiple classes, reflects our system's ability to effectively handle the nuances of Tamil sarcasm. However, despite the relatively strong performance indicated by the F1 score, the 9th rank suggests that the competition was particularly fierce, with other systems either matching or surpassing our performance.\nIn contrast, our system's performance for Malayalam resulted in a macro-F1 score of 0.50, securing the 13th position in the rankings. This score, while moderate, highlights the challenges inherent in processing Malayalam text, particularly in capturing the subtleties of sarcastic expressions. The lower rank compared to Tamil indicates that our system faced greater difficulties with Malayalam, and that several other systems were more adept at handling these challenges.\nThese results underscore the varying levels of difficulty and competition present in sarcasm detection for Tamil and Malayalam. The higher performance in Tamil suggests that our system is more attuned to the linguistic characteristics of Tamil sarcasm, while the more modest performance in Malayalam points to areas where further research and refinement are necessary. This study not only contributes to the growing body of work in multilingual sarcasm detection but also highlights the importance of continuing to develop robust NLP systems capable of addressing the complexities of underrepresented languages."}, {"title": "2. Related Work", "content": "Sarcasm detection has been a topic of significant interest in the field of natural language processing (NLP) due to its complex and context-dependent nature [40]. Traditional approaches to sarcasm detection primarily relied on feature-based methods, where linguistic cues such as punctuation, lexical patterns, and syntactic structures were used to identify sarcasm [40]. For instance, works by [41, 42] and [43] utilized syntactic patterns and hashtag-based supervision to improve sarcasm detection in English texts. These approaches, however, often struggled with generalization, particularly in scenarios involving nuanced and culturally specific forms of sarcasm.\n[44] introduced an approach using deep neural networks that leveraged context to improve sarcasm detection. Similarly, [45, 46] explored attention-based models that better captured the dependencies within a sentence that could indicate sarcasm. These deep learning models significantly advanced the state-of-the-art in sarcasm detection, particularly in monolingual contexts.\nOne of the primary challenges in dealing with code-mixed text is language identification [47, 48, 49], which involves determining the language of each word or phrase within a mixed-language sentence. Traditional methods for this task often relied on n-gram models, dictionary-based approaches, and supervised learning techniques. However, these methods sometimes struggle with the complex linguistic patterns present in code-mixed data. These models are particularly effective at capturing the contextual information necessary to accurately identify languages in code-mixed text.\nAnother critical area of research is part-of-speech tagging [50, 51] for code-mixed sentences. This task becomes more complicated due to the interaction between different languages' grammatical structures. Researchers have employed techniques like Conditional Random Fields (CRFs) and sequence- to-sequence models, adapting them to handle the intricacies of mixed-language data. The use of multilingual embeddings, where words from different languages are mapped into a shared vector space, has also been shown to improve the accuracy of part-of-speech tagging in code-mixed scenarios.\nIn the context of sentiment analysis [52], code-mixing introduces additional complexity due to the potential for differing sentiment expressions across languages. Traditional sentiment analysis tools often fail when applied directly to code-mixed text. To overcome this, researchers have explored methods such as using bilingual lexicons, translating the text into a single language before analysis, and employing deep learning models that can process code-mixed input directly.\nMachine translation [53] for code-mixed text is another challenging area. Standard machine transla- tion systems are usually trained on monolingual data, making them less effective at handling code-mixed sentences. Researchers have proposed various strategies to address this, including the use of synthetic code-mixed data for training, as well as incorporating language tags and contextual embeddings into translation models. These approaches aim to create more robust translation systems capable of handling the nuances of code-mixing.\nAdditionally, speech recognition [54, 55] in code-mixed environments requires models that can accu- rately transcribe spoken language that switches between different languages. This task is particularly challenging because traditional speech recognition systems are typically designed for single-language input. Recent advances involve the use of end-to-end models, such as attention-based encoder-decoder architectures, which have shown promise in recognizing and transcribing code-mixed speech.\nOur contribution is situated at the intersection of sarcasm detection, code-mixed text processing, and the application of large language models, addressing a critical gap in the literature and advancing the state-of-the-art in this complex domain."}, {"title": "3. Dataset", "content": "The Tamil testing dataset is 6338 comments. The Malayalam testing dataset is 2826 comments. Since we used effectively only the test dataset for our predictions, we only mention the statistics corresponding to the test dataset.\nGiven that our evaluation focused entirely on the test datasets, the statistics mentioned correspond to these specific datasets. The decision to use only the test datasets for prediction ensures that the per- formance metrics, such as the macro-F1 scores, accurately reflect the model's generalization capabilities on unseen data.\nThe Tamil testing dataset, with its larger size of 6,338 comments, provides a broad spectrum of inputs for the model to process, potentially leading to more robust insights into the model's strengths and weaknesses. On the other hand, the smaller Malayalam testing dataset, with 2,826 comments, presents its own set of challenges, particularly in a low-resource language context where data is often scarcer. By focusing on these testing datasets, our analysis remains rooted in real-world applicability, offering a clear view of how well the models perform in practical scenarios without additional interventions or training phases."}, {"title": "4. Task Definition", "content": "The task is to accurately classify YouTube comments that are written in Tamil-English and Malayalam- English (code-mixed) into two distinct categories: Sarcastic and Non-sarcastic, using a Al model."}, {"title": "5. Methodology", "content": "Prompting [26] is an effective approach for solving the problem of sarcasm detection in code-mixed texts, particularly in under-resourced languages like Tamil-English and Malayalam-English, for several reasons:\nNatural Language Understanding: Prompting allows the model to interpret and execute tasks based on human-like instructions. By framing the classification task as a natural language prompt [56], the model can utilize its language understanding capabilities to classify text according to the specified categories. The prompt provides context to the model, guiding it to focus on specific aspects of the input (e.g., determining sarcasm). This ensures that the model's response is aligned with the task at hand.\nLeverage of Pretrained Knowledge: Large language models like GPT-3.5 Turbo have been trained on vast amounts of text data, encompassing diverse linguistic patterns, cultural contexts, and language nuances [57]. Prompting allows us to tap into this rich, pre-trained knowledge base, enabling the model to interpret sarcasm even in challenging code-mixed scenarios [26]. By carefully crafting prompts, we can guide the model to focus on specific aspects of the input, such as tone, context, and language switching, which are critical for sarcasm detection.\nReduced Need for Fine-Tuning: With the right prompt, large pre-trained models (like GPT) [58] can perform various classification tasks without the need for extensive fine-tuning on task-specific data. This is particularly useful when data or computational resources for fine-tuning are limited. Prompts leverage the model's existing knowledge, gained from vast pre-training on diverse text corpora, enabling it to generalize to new tasks with minimal additional training.\nAdaptability to Multilingual and Code-Mixed Texts: Traditional machine learning models often require large, labeled datasets for each specific task and language, which are scarce for many code-mixed languages [59]. In contrast, prompting large language models provides a flexible way to adapt to different languages and dialects, even when training data is limited [26]. The model's inherent ability to understand and generate text in multiple languages, including code-mixed varieties, makes prompting an attractive approach for this task.\nImproved Interpretability: The explicit nature of prompts makes it easier to understand how the model is interpreting the task [60], as the instructions are directly embedded in the input. This transparency can help in analyzing and improving the model's performance. By examining the model's responses to different prompts, researchers can gain insights into where and why the model might be making classification errors, leading to more targeted improvements.\nReduced Need for Task-Specific Data: Building a model from scratch or fine-tuning a model for sarcasm detection in code-mixed texts would typically require a substantial amount of labeled data, which is often difficult to obtain for under-resourced languages [61]. Prompting, however, minimizes the need for such extensive labeled datasets, as it allows the model to utilize its general language understanding capabilities with minimal task-specific adjustments [26, 62]. This makes it a more feasible and efficient solution for resource-constrained settings.\nFlexibility and Rapid Experimentation: Prompting enables rapid experimentation with different strategies to optimize model performance [26]. Researchers can quickly test various prompt formulations to see which one elicits the most accurate and context-aware responses from the model. This flexibility is particularly beneficial for complex tasks like sarcasm detection, where the model's interpretation can vary significantly depending on how the problem is framed.\nScalability Across Different Scenarios: Once an effective prompting strategy is developed, it can be easily adapted or scaled to other similar tasks or languages without the need for extensive retraining [63, 26]. This is particularly useful in multilingual environments where a single model might need to handle multiple languages or dialects. Prompting allows for the reuse of the same model across different linguistic contexts with minimal modifications.\nIn summary, prompting is a powerful and efficient approach to tackle the problem of sarcasm detection in code-mixed texts, offering the benefits of leveraging large-scale pretrained models, adapting to multilingual and code-mixed scenarios, reducing dependency on large annotated datasets, enabling rapid experimentation, and providing scalability across different linguistic contexts."}, {"title": "5.1. Prompt Engineering-Based Approach", "content": "We used the GPT-3.5 Turbo model via prompting to solve the classification task. We used GPT-3.5 Turbo in zero-Shot mode via prompting. After the prompt is provided to the LLM, the following steps happen internal to the LLM while generating the output. The following outlines the steps that occur internally within the LLM, summarizing the prompting approach using GPT-3.5 Turbo:\nStep 1: Tokenization\n\u2022 Prompt: X = [X1,X2,...,Xn]\n\u2022 The input text (prompt) is first tokenized into smaller units called tokens. These tokens are often subwords or characters, depending on the model's design.\n\u2022 Tokenized Input: T = [t1, t2, . . .,tm]\nStep 2: Embedding\n\u2022 Each token is converted into a high-dimensional vector (embedding) using an embedding matrix \u0395.\n\u2022 Embedding Matrix: E \u2208 R^{|V|\u00d7d}, where |V| is the size of the vocabulary and d is the embedding dimension.\n\u2022 Embedded Tokens: Temb = [E(t1), E(t2), ..., E(tm)]\nStep 3: Positional Encoding\n\u2022 Since the model processes sequences, it adds positional information to the embeddings to capture the order of tokens.\n\u2022 Positional Encoding: P(ti)\n\u2022 Input to the Model: Z = Temb + P\nStep 4: Attention Mechanism (Transformer Architecture)\n\u2022 Attention Score Calculation: The model computes attention scores to determine the importance of each token relative to others in the sequence.\n\u2022 Attention Formula:\nAttention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V                                                                   \n\u2022 where Q (query), K (key), and V (value) are linear transformations of the input Z.\n\u2022 This attention mechanism is applied multiple times through multi-head attention, allowing the model to focus on different parts of the sequence simultaneously."}, {"title": "Step 5: Feedforward Neural Networks", "content": "The output of the attention mechanism is passed through feedforward neural networks, which apply non-linear transformations.\nFeedforward Layer:\nFFN(x) = max(0, xW1 + b1)W2 + b2\nwhere W1, W2 are weight matrices and b1, b2 are biases.\nStep 6: Stacking Layers\nMultiple layers of attention and feedforward networks are stacked, each with its own set of parameters. This forms the \"deep\" in deep learning.\nLayer Output:\nH(1) = LayerNorm(Z(1) + Attention(Q(1), K(1), V(1)))\nZ(1+1) = LayerNorm(H(1) + FFN(H(\u00b9)))\nStep 7: Output Generation\nThe final output of the stacked layers is a sequence of vectors.\nThese vectors are projected back into the token space using a softmax layer to predict the next token or word in the sequence.\nSoftmax Function:\nP(yi|X) = \\frac{exp(Zi)}{\\sum_i exp(Zi)}\nwhere Zi is the logit corresponding to token i in the vocabulary.\nThe model generates the next token in the sequence based on the probability distribution, and the process repeats until the end of the output sequence is reached.\nStep 8: Decoding\nThe predicted tokens are then decoded back into text, forming the final output.\nOutput Text: Y [Y1, Y2,..., Yk]"}, {"title": "6. Results", "content": "Our team named TextTitans, for the Tamil language the macro-F1 score is 0.61 with a rank of 9th. For Malayalam language the macro-F1 score is 0.50 with a rank of 13th.\nFor Malayalam, the system has achieved a macro-F1 score of 0.50, which represents the harmonic mean of precision and recall across multiple classes, treating each class equally. This score places the system in the 13th position among other competing models or systems. A rank of 13th suggests that while the performance is moderate, there are several other systems that have performed better.\nFor Tamil, the system has a macro-F1 score of 0.61, indicating better performance than the Malayalam system in terms of the balance between precision and recall. However, despite the higher F1 score, this system is ranked 9th. This lower rank, despite a better score, suggests that the competition for Tamil language tasks was fiercer, with more systems achieving higher or comparable performance levels."}, {"title": "7. Conclusion", "content": "The complexity of code-switching, coupled with the nuanced nature of sarcasm, presents significant hurdles for traditional sentiment analysis systems, particularly in under-resourced languages. To tackle these challenges, we experimented with state-of-the-art large language models like GPT-3.5 Turbo, leveraging prompt-based techniques to classify comments and posts as sarcastic or non-sarcastic. Our approach demonstrated the potential of prompting as an effective method for sarcasm detection in multilingual and code-mixed environments, allowing us to utilize the extensive pretrained knowledge of the model with minimal task-specific data. The presence of a new gold standard corpus for Tamil- English and Malayalam-English code-mixed text contributes to the advancement of research in this area, providing a valuable resource for the development and evaluation of sarcasm detection systems.\nOverall, our work underscores the viability of prompt-based techniques for addressing the unique challenges of sarcasm detection in code-mixed languages and paves the way for future studies aimed at improving systems in multilingual social media contexts. We hope that this research will inspire continued exploration of advanced language models in tackling the diverse and evolving landscape of digital communication.\nThe evaluation of our sarcasm detection systems for Malayalam and Tamil languages provides valuable insights into the performance and challenges associated with handling these languages. For Malayalam, the system achieved a macro-F1 score of 0.50, which, while representing a moderate balance between precision and recall across classes, placed the system at the 13th position among competing models. This rank indicates that while our system performs adequately, there are several other models that have outperformed it in the specific context of Malayalam sarcasm detection.\nIn contrast, the Tamil sarcasm detection system achieved a higher macro-F1 score of 0.61, reflecting a better overall balance between precision and recall compared to the Malayalam system. Despite this improved score, the Tamil system was ranked 9th among competitors. This lower ranking, even with a higher F1 score, highlights the more competitive landscape for Tamil language tasks, where several systems have achieved similar or better performance levels.\nThese results underscore the varying degrees of difficulty in sarcasm detection across different languages and the importance of context in interpreting performance metrics like the macro-F1 score. The differences in ranking also suggest that the complexity of the language, the nature of the testing data, and the sophistication of competing models play significant roles in determining a system's success.\nIn future work, it will be crucial to explore ways to enhance the performance of both systems, particularly by focusing on the nuances of sarcasm in these languages, improving the quality and diversity of the training data, and leveraging more advanced modeling techniques. Additionally, a more in-depth analysis of the specific errors made by the systems could provide further insights into areas that require targeted improvements. Ultimately, the goal is to develop robust and competitive systems that can more effectively handle the challenges of sarcasm detection in both Malayalam and Tamil, contributing to the broader field of natural language processing for low-resource languages."}]}