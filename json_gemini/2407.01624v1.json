{"title": "Guided Trajectory Generation with Diffusion Models for Offline Model-based Optimization", "authors": ["Taeyoung Yun", "Sujin Yun", "Jaewoo Lee", "Jinkyoo Park"], "abstract": "Optimizing complex and high-dimensional black-box functions is ubiquitous in science and engineering fields. Unfortunately, the online evaluation of these functions is restricted due to time and safety constraints in most cases. In offline model-based optimization (MBO), we aim to find a design that maximizes the target function using only a pre-existing offline dataset. While prior methods consider forward or inverse approaches to address the problem, these approaches are limited by conservatism and the difficulty of learning highly multi-modal mappings. Recently, there has been an emerging paradigm of learning to improve solutions with synthetic trajectories constructed from the offline dataset. In this paper, we introduce a novel conditional generative modeling approach to produce trajectories toward high-scoring regions. First, we construct synthetic trajectories toward high-scoring regions using the dataset while injecting locality bias for consistent improvement directions. Then, we train a conditional diffusion model to generate trajectories conditioned on their scores. Lastly, we sample multiple trajectories from the trained model with guidance to explore high-scoring regions beyond the dataset and select high-fidelity designs among generated trajectories with the proxy function. Extensive experiment results demonstrate that our method outperforms competitive baselines on Design-Bench and its practical variants. The code is publicly available in https://github.com/dbsxodud-11/GTG.", "sections": [{"title": "1 Introduction", "content": "Optimizing complex and high-dimensional black-box functions is ubiquitous in science and engineering fields, including biological sequence design [1], materials discovery [2], and mechanical design [3, 4]. Traditional methods like Bayesian optimization have been developed to solve the problem by iteratively querying a black-box function. However, the online evaluation of the black-box function is restricted in most real-world situations due to time and safety constraints.\nFortunately, we often have access to a previously collected offline dataset. This problem setting is referred to as offline model-based optimization (MBO), and our objective is to find a design that maximizes a target function using solely an offline dataset [5]. As no online evaluation is available, a key challenge of MBO is the out-of-distribution (OOD) issue arising from limited data coverage. Suppose we train a proxy that predicts function values given input designs and naively apply a gradient-based optimizer based on the proxy to identify the optimal design. It would fall into sub-optimal results due to inaccurate predictions of the proxy in unseen regions.\nTo mitigate this issue, forward approaches mostly consider training a robust surrogate model against adversarial optimization of inputs and applying gradient-based maximization. Trabucco et al. [6] train a proxy with the regularization term to prevent overestimation on OOD designs. Fu and Levine [7] leverage normalized maximum likelihood estimator to handle uncertainty on unseen regions."}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Problem setup", "content": "In offline model-based optimization (MBO), we aim to find a design x that maximizes the target black-box function f. Unlike the typical black-box optimization setting, we can only access an offline dataset D, and online evaluations are unavailable. The problem setup can be described as follows:\nfind x* = arg max f(x) s.t only an offline dataset D = {(xi, yi)}_1^N is given\nx\u2208Rd\nwhere x is a decision variable and y = f(x) is a target property we want to maximize."}, {"title": "2.2 Diffusion probabilistic models", "content": "Diffusion probabilistic models [19, 20] are a class of generative models that approximate the true distribution q0 with a parametrized model of the form: p\u03b8(x0) = \u222b p\u03b8(x0:T)dx1:T, where x0 ~ q0 and x1,...,xT are latents with the same dimensionality. The joint distribution p\u03b8(x0:T) is called the reverse process, defined as a Markov chain starting from standard Gaussian pT(xT) = N(0, I):\np\u03b8(x0:T) = pT(xT) \u220f p\u03b8(xt\u22121|xt), p\u03b8(xt\u22121|xt) = N(\u03bc\u03b8(xt, t), \u03a3t)\nt=1\nwhere p\u03b8(xt\u22121|xt) is parametrized Gaussian transition from timestep t to t \u2013 1.\nWe define a forward process, which is also fixed as a Markov chain that adds Gaussian noise to the data with the variance schedule \u03b21,\uff65\uff65\uff65, \u03b2T:\nq(x1:T|x0) = \u220f q(xt|xt\u22121), q(xt|xt\u22121) = N(\u221a1 \u2212 \u03b2txt\u22121, \u03b2tI)\nt=1"}, {"title": "3 Methodology", "content": "In this section, we introduce GTG, Guided Trajectory Generation, a conditional generative modeling approach for solving MBO problem by learning to improve solutions using the offline dataset. We first construct trajectories towards high-scoring regions while incorporating locality bias for consistent improvement directions. Then, we train the conditional diffusion model to generate trajectories and a proxy model. Finally, we sample multiple trajectories using the diffusion model with guided sampling and filter high-fidelity designs with the proxy. Figure 1 shows the overview of the proposed method."}, {"title": "3.1 Constructing trajectories", "content": "We construct a set of trajectories Dtraj from the offline dataset D to gather information on learning to improve designs. In this paper, each trajectory \u03c4 \u2208 Dtraj is a set of H input-output pairs and can be represented as a two-dimensional array:\n\u03c4=\n[\nx1 x2 \u2026 xH\ny1 y2 \u2026 yH\n], (xh, yh) \u2208 D \u2200h = 1,\u2026, H\nWhile prior works construct trajectories via sorting heuristics or sampling from high-scoring regions, we focus on constructing trajectories that give us more valuable information for learning to improve designs towards higher scores. To achieve this, we develop a novel method to construct trajectories based on two desiderata.\nFirst, the trajectory should be towards high-scoring regions while containing information on the landscape of the target black-box function. Second, the trajectories should be diverse and not converge to a single data point with the highest score of the dataset, as our objective is to discover high-scoring designs beyond the offline dataset by generalizing the knowledge of learning to improve solutions."}, {"title": "3.2 Training models", "content": "Given our trajectory dataset Dtraj, our objective is to learn the conditional distribution of trajectories towards high-scoring regions. We choose diffusion models, which have a powerful capability to learn the distribution of complex and high-dimensional data [22, 23], to generate trajectories. Our objective is then transformed from searching high-scoring designs to maximizing the conditional likelihood of trajectories, which can be achieved by minimizing the loss in Equation (5):\n\u03b8* = arg max ET~Dirai [log p\u03b8 (\u03c4|y(\u03c4))]\n\u03b8\nwhere y(\u03c4) = \u2211Hh=1 yh is the sum of scores in the trajectory \u03c4. By training a diffusion model to generate a sequence of designs instead of a single design, we can efficiently distill the knowledge of the complex landscape of the target function into the diffusion model.\nIn addition, we also train a forward proxy f\u03b8 using the dataset D. We can use the proxy to filter high-scoring designs from the trajectories generated by the trained diffusion model."}, {"title": "3.3 Sampling trajectories from the diffusion model", "content": "After training, we sample trajectories with guided sampling. We use classifier-free guidance to generate trajectories. To be specific, we sample from the diffusion model using Equation (6), where y*(\u03c4) is the target conditioning value. Following prior works [13, 16], we assume that we know the maximum score y* and set y*(\u03c4) = \u03b1 \u22c5 (Hy*), where \u03b1 controls the exploration level of the generated trajectories. We discuss the role of \u03b1 in more detail in the subsequent section.\nTo fully utilize the expressive power of diffusion models, we introduce an additional strategy, context conditioning, during the sampling. We generate trajectory with diffusion model while inpainting the C context data points of the trajectory with \u03c4ctx, which is a subtrajectory sampled from Dtraj. By conditioning trajectories in different contexts, we can effectively explore diverse high-scoring regions."}, {"title": "3.4 Selecting candidates", "content": "After generating trajectories, we introduce filtering to select candidates for evaluation. In other words, we select top-Q samples in terms of the predicted score from the proxy. By filtering with the proxy, we can exploit the knowledge from the dataset to search high-scoring designs [13, 14, 24]."}, {"title": "4 Experimental evaluation", "content": "In this section, we present the results of our experiments on various tasks. First, we analyze our method in a toy 2D experiment. Then, we present the results on the Design-Bench and its practical variants to verify the effectiveness of the method. We also conduct extensive analyses on various aspects to deepen our understanding of the proposed method."}, {"title": "4.1 Toy 2D experiment", "content": "We first evaluate our method using a toy setting to analyze each component of our method thoroughly. We choose Branin, a synthetic 2D function with three distinct global maxima. Figure 2 shows the contour plot of the Branin function. The analytical form of the Branin function is as follows:\nf(x1, x2) = -a (x2 - bx1^2 + cx1 - r)^2 \u2013 s (1 \u2013 t) cos(x1) - s"}, {"title": "4.2 Design-Bench tasks", "content": "In this section, we present the experiment results of our method on Design-Bench tasks [5]. We conduct experiments on two discrete tasks and three continuous tasks. For each task, we have an offline dataset from an unknown oracle function. We present the detailed task description below.\nTFBind8 and TFBind10 [1]. We aim to find a DNA sequence of the length 8 and 10 with maximum binding affinity with a particular transcription factor.\nSuperconductor [2]. We aim to design a chemical formula, represented by an 86-dimensional vector, for a superconducting material with a high critical temperature.\nAnt and D'Kitty Morphology [4, 25]. We aim to optimize the morphological structure of two simulated robots. The morphology parameters include size, orientation, and the location of the limbs. Ant has 60 continuous parameters, and D'Kitty has 56 continuous parameters."}, {"title": "4.3 Baselines", "content": "For baselines, we prepare four main categories to solve MBO problems. First, we compare our method with traditional methods widely used in online black-box optimization settings, such as BO-qEI [26], CMA-ES [27], REINFORCE [28], and Gradient Ascent."}, {"title": "4.4 Evaluation metrics", "content": "For evaluation, we follow the protocol of prior works. We identify Q = 128 designs selected by the algorithm and report a normalized score of 100th percentile design. For all algorithms, we run experiments over 8 different seeds and report mean and standard errors.\nTo evaluate our method, we construct trajectories of length H = 64 and train a conditional diffusion model for each task. After training, we sample N = 128 trajectories conditioning on C = 32 context data points and setting \u03b1 = 0.8 across all tasks. Finally, we filter top-128 candidates among generated designs with the predicted score from the proxy for evaluation."}, {"title": "4.5 Main results", "content": "As shown in the Table 1, our method achieves an average rank of 1.6, the best among all competitive baselines. Our method performs best on two tasks and is runner-up on three tasks, demonstrating superior performance across different tasks. The experiment results underscore that training diffusion models and generating trajectories with guided sampling can effectively explore high-scoring regions."}, {"title": "4.6 Practical variants of Design-Bench tasks", "content": "In this section, we present experiment results in a more practical setting of Design-Bench tasks. While Design-Bench assumes a large, unbiased offline dataset containing thousands of data points for the training model, such a setting is impractical in most cases. Therefore, we prepare two additional practical settings, sparse and noisy datasets, to verify the robustness of our method in such extreme cases. In a sparse setting, we only provide x% of the original dataset for training. For the noisy setting, we add x% of standard Gaussian noise to the normalized score values. We choose recent papers published after 2022, BDI, ICT, DDOM, and BONET for primary baselines. Please refer Appendix A.2 for detailed experiment settings and Appendix D.5 for results with more baselines."}, {"title": "5 Additional analysis", "content": ""}, {"title": "6 Related works", "content": ""}, {"title": "6.1 Offline model-based optimization", "content": "In offline MBO, generalization outside the offline dataset is crucial for success. While there have been attempts to train a robust surrogate model to achieve accurate predictions on unseen regions [8\u201310], effectively exploring high-scoring regions remains challenging.\nRecently, a new perspective on solving the MBO problem has emerged by learning to improve solutions from synthetic trajectories and generalizing the knowledge to find designs beyond the dataset [16, 17]. BONET [16] trains an autoregressive model to generate optimal trajectories conditioned on a low regret budget. PGS [17] trains RL policy with trajectories consisting of high-scoring designs to roll out optimal trajectories. GTG falls under this category but adopts a unique approach to constructing trajectories with local search and utilizing diffusion models to enhance performance."}, {"title": "6.2 Generative models for decision making", "content": "Generative models have emerged as a powerful tool for decision-making problems, including bandit problems [30], reinforcement learning [18, 31\u201334], and optimization [15, 35]. In offline MBO, there are inverse approaches to learning a mapping from function values to input domains with generative models and sample designs from high-scoring regions [11, 12, 14, 15]. DDOM [15] utilizes a conditional diffusion model and generates high-scoring samples with reweighted training and classifier-free guidance. DiffOPT [35] considers a constrained optimization setting and introduces a two-stage framework that begins with a guided diffusion process for warm-up, followed by a Langevin dynamics stage for further correction. Our method distinguishes itself from prior works by utilizing diffusion models to generate trajectories toward high-scoring regions by learning to improve solutions from the dataset."}, {"title": "7 Discussion and conclusion", "content": "In this paper, we introduce GTG, a novel conditional generative modeling approach for learning to improve solutions from synthetic trajectories constructed with the dataset. First, we construct diverse trajectories toward high-scoring regions while incorporating locality bias. Then, we train the conditional diffusion model and proxy function. After training, we generate trajectories with classifier-free guidance and context-conditioning to generalize the knowledge on how to improve solutions. Lastly, our filtering strategy for selecting candidates further improves the performance. Our extensive experiments demonstrate the generalizability of GTG.\nLimitation and future work. While our method shows powerful generalizability on Design-Bench tasks, we resort to filtering designs with the proxy function trained with the offline dataset, which may result in inaccurate predictions. Although our filtering strategy works well in sparse and noisy settings, one may consider constructing a robust proxy model to handle the uncertainty of its predictions."}, {"title": "B Methodology Details", "content": "In this section, we present the method details, including model implementations and architectures, training schemes, hyperparameter configurations, and computing resources."}, {"title": "B.1 Trajectory Construction", "content": "In terms of constructing trajectories, we introduce two variables, K and \u03f5, which control the level of locality and optimality of the trajectories. For too large value of K, we construct trajectories with inconsistent directions of improvement, while the extremely small value of K leads to trajectories wandering the initial data point. If we lower the \u03f5 close to zero, we only allow monotonic improvement, while large \u03f5 values lead to suboptimal trajectories. We present the hyperparameters for our experiments in the Table 8. We also conduct additional analysis on trajectory construction in Appendix D.1."}, {"title": "B.2 Training Models", "content": ""}, {"title": "B.2.1 Training Diffusion Model", "content": "We use temporal U-Net architecture from Diffuser [18] as a backbone of the diffusion model. For discrete tasks, we train the model using Adam optimizer [37] for 1 \u00d7 104 training steps with the learning rate of 1 \u00d7 10-3. While one could use discrete diffusion models [38, 39] for discrete tasks, we use continuous diffusion models with continuous relaxation of discrete inputs for simplicity. For continuous tasks, we train the model for 5 \u00d7 104 steps with a learning rate of 1 \u00d7 10\u22124. The hyperparameters we used for modeling and training are listed in Table 10."}, {"title": "B.2.2 Training Proxy Model", "content": "We use MLP with 2 hidden layers with 1024 hidden units and ReLU activations to implement the proxy function. As our objective is filtering high-fidelity designs with the proxy, we introduce a rank-based reweighting suggested by [40] during training to make the proxy model focus on high-scoring regions. For discrete tasks, we train a proxy model using Adam optimizer for 1 \u00d7 103 training steps with a learning rate of 1 \u00d7 10-3. For continuous tasks, we train the model for 5 \u00d7 103 training steps with a learning rate of 1 \u00d7 10-3. The hyperparameters we used for modeling and training are listed in Table 11."}, {"title": "B.3 Sampling Procedure", "content": "We sample trajectories with T = 200 denoising steps across all tasks. For classifier-free guidance, we set the guidance scale w as 1.2. In practice, we sample a batch of trajectories to generate multiple trajectories in parallel. We analyze the time complexity of sampling trajectories from the diffusion model in Appendix D.4"}, {"title": "C Baseline Details", "content": "In this section, we provide more details on the baselines used for our experiments.\nBaselines from Design-Bench [5]. We take the implementations of most baselines from open-source code\u00b9. It contains baselines of BO-qEI [26], CMA-ES [27], REINFORCE [28], Gradient Ascent, CbAS [11], MINs [13], and COMs [41]. We reproduce the results with 8 independent random seeds.\nNEMO [7]. NEMO leverages a normalized maximum likelihood estimator to handle uncertainty in unseen regions and prevent adversarial optimization while performing gradient ascent. As there is no open-source code, we refer to the results of NEMO from [9].\nBDI [24]. BDI learns forward mapping from low-scoring regions to high-scoring regions, and its backward mapping distills the knowledge of the offline dataset to search for optimal designs. We follow the hyperparameter setting of the paper and reproduce the results with the open-source code\u00b2.\nICT [9]. ICT maintains three symmetric proxies and enhances the performance of the ensemble by co-teaching and importance-aware sample reweighting. We follow the hyperparameter setting of the paper and reproduce the results with the open-source code\u00b3.\nDDOM [15]. DDOM leverages diffusion models to model distribution over high-scoring regions and sample designs with classifier-free guidance. We follow the hyperparameter setting of the paper except for the evaluation budget Q for a fair comparison. We find that there is a performance drop in several tasks when we use Q = 128 instead of 256. We reproduce the results with the open-source code\u2074.\nBONET [16]. BONET trains an autoregressive model with trajectories constructed from the offline dataset and generalizes the knowledge to explore high-scoring regions. We follow the hyperparameter setting of the paper except for the evaluation budget Q for a fair comparison. We find that there is a performance drop in several tasks when we use Q = 128 instead of 256. We reproduce the results with the open-source code\u2075.\nPGS [17]. PGS trains a policy to guide gradient-based optimization by reformulating the MBO problem as an offline RL problem. We follow the hyperparameter setting of the paper and reproduce the results with the open-source code\u2076."}, {"title": "D Extended Additional Analysis", "content": "In this section, we present additional analysis on GTG which is not included in the main section due to the page limit."}, {"title": "D.1 Additional Analysis on Trajectory Construction", "content": ""}, {"title": "D.1.1 Analysis on Score Distribution of Trajectories", "content": "We conduct additional analysis on our trajectory construction method. We try to generate diverse trajectories toward high-scoring regions by randomly selecting subsequent designs from K neighbors and allowing local perturbations. To this end, we visualize the shift in the distribution of function values via various trajectory construction strategies in the Superconductor task. As shown in Figure 7, the SORT-SAMPLE strategy suggested by BONET constructs trajectories solely on high-scoring designs, which can be easily trapped into local optima. Unlike SORT-SAMPLE, our method shifts distribution towards high-scoring regions while using the information of low-scoring regions to distill the knowledge of the landscape of the target function to the generator."}, {"title": "D.1.2 Analysis on Hyperparameters in Trajectory Construction", "content": "We also conduct additional analysis on hyperparameters in trajectory construction, K and \u03f5. Figure 8 shows the performance of GTG in TFBind8 task by varying K and \u03f5. While using too large K or too small \u03f5 may lead to a relatively low performance, we do not see much variation with different values."}, {"title": "D.2 Additional Analysis on Sampling Procedure", "content": ""}, {"title": "D.2.1 Various Strategies for Guided Sampling", "content": "In this section, we explore various strategies for guiding diffusion models to generate high-scoring designs. As we also generate score values, it could be possible to guide diffusion models to generate high-scoring designs by inpainting score values with the desired values. To this end, we conduct additional experiments on Design-Bench tasks by generating trajectories with inpainting instead of classifier-free guidance. Specifically, we inpaint the y values of the generated trajectories as y*, the normalized score of the optimal design.\nTable 12 shows the performance of different guiding strategies. It confirms that conditioning by classifier-free guidance performs better than the inpainting strategy, justifying our decision choice."}, {"title": "D.2.2 Diversity Analysis", "content": "In this section, we explore the trade-off between performance and diversity via filtering strategy. While the filtering strategy boosts the performance of our method by eliminating potentially sub-optimal designs, it may reduce the diversity of candidates, which may be crucial in tasks such as drug discovery due to proxy misspecification [42].\nTo this end, we measure the diversity of the candidates, following the procedure of [14]. For measurement, we use the average of the pairwise distance between candidates as below.\nDiversity(D) = 1/(|D|(|D| \u2212 1)) * \u2211x\u2208D\u2211x'\u2208D\\{x} d(x, x') "}, {"title": "D.2.3 Impact of Exploration Level", "content": "In this section, we explore the impact of the exploration level (\u03b1) on the generated samples. As depicted in Figure 3c, increasing \u03b1 leads to higher performance, indicating the importance of classifier-free guidance. However, we observe that conditioning on extremely high \u03b1 leads to sub-optimal performance, as illustrated in Figure 9. Conditioning on extremely high \u03b1 guides the diffusion model to over-exploration, resulting in sub-optimal out-of-distribution designs. Note that we do not fine-tune \u03b1 for each task and fix it with the value of 0.8 across all tasks, which generally exhibits good performance."}, {"title": "D.2.4 Assumption on optimal value", "content": "We assume that the optimal value y* of each task is known, following prior works [13, 16]. However, it is not always possible to know the exact optima. To this end, we estimate y* with \u03b3 \u22c5 ymax, where ymax is the maximum value of the dataset and evaluate GTG by conditioning on the estimated value. As depicted in Table 14, conditioning on \u03b3 \u22c5 ymax achieves comparable performance and even outperforms the performance of conditioned on exact optima in the TFBind8 task. However, it introduces an additional hyperparameter \u03b3, whose optimal value varies across tasks. Therefore, we rely on assuming the exact optima, which is not an issue in many problems."}, {"title": "D.3 Effect of Unsupervised Pretraining", "content": "It might be beneficial to pretrain the diffusion model with unlabeled data when we have limited data points. Specifically, there is a recent work EXPT [29], which trains an autoregressive model using synthetic trajectories constructed from the large-scale unlabeled dataset and adapts new tasks by conditioning on a few labeled points. To this end, we discuss the effect of pre-training GTG with unlabeled datasets. We follow a similar procedure of EXPT to generate a synthetic dataset. Formally, we sample synthetic functions from Gaussian Processes [43] with an RBF kernel and assign pseudo values to the unlabeled data points from synthetic functions. Please refer to [29] for a more detailed setting. Given a synthetic dataset, we pretrain diffusion models with trajectories constructed from the dataset using the proposed method. Then, we generate samples by conditioning on context data points from the labeled dataset. For labeled dataset, we randomly select 1% of the original dataset.\nTable 15 shows the experiment results on various Design-Bench tasks. As shown in the table, pretraining generally improves the performance of GTG in the sparse data setting. We also find that GTG with pretraining outperforms ExPT in 3 of 5 tasks. While we do not assume the existence of the large-scale unlabeled dataset in the main experiment and pretraining is not a main focus of our research, it might be beneficial to analyze the effect of pretraining with synthetic datasets in offline MBO thoroughly as in other problems [44, 45]."}, {"title": "D.4 Time Complexity of Sampling Procedure", "content": "In this section, we analyze the time complexity of the sampling procedure of GTG. To generate trajectories, we run T = 200 denoising timesteps with classifier-free guidance and context-conditioning to sample N = 128 trajectories, which takes approximately 9.41s and 9.47s in wall clock time for the Ant and D'Kitty tasks, respectively. We visualize the trade-off between the performance and runtime of sampling by varying the number of denoising timesteps. As shown in Figure 10, we can decrease the number of denoising timesteps even one-tenth with minimal loss in performance. Please note that sampling time is negligible compared to evaluating black-box functions, which is mostly expensive in real-world settings."}, {"title": "D.5 Extended Experiment Results", "content": "In this section, we present extended experiment results in sparse and noisy datasets. As shown in Tables 16 and 17, our method outperforms most baselines in various practical settings. Note that we cannot conduct experiments with NEMO and RoMA, as there is no code publicly available."}, {"title": "D.6 Additional Visualization on Toy 2D Experiment", "content": "We present additional visualization results from the Toy 2D experiment. As shown in Figure 11, GTG is able to generate diverse trajectories toward high-scoring designs by conditioning on different context points and classifier-free guidance."}, {"title": "E Broader Impact", "content": "Optimization for real-world designs presents both opportunities and risks. For instance, while the design of new pharmaceuticals holds the promise of curing previously untreatable diseases, there is the potential for misuse, such as creating harmful biochemical agents. Researchers should be diligent to ensure that their innovations are employed in ways that contribute positively to societal welfare."}]}