{"title": "Guided Trajectory Generation with Diffusion Models for Offline Model-based Optimization", "authors": ["Taeyoung Yun", "Sujin Yun", "Jaewoo Lee", "Jinkyoo Park"], "abstract": "Optimizing complex and high-dimensional black-box functions is ubiquitous in science and engineering fields. Unfortunately, the online evaluation of these functions is restricted due to time and safety constraints in most cases. In offline model-based optimization (MBO), we aim to find a design that maximizes the target function using only a pre-existing offline dataset. While prior methods consider forward or inverse approaches to address the problem, these approaches are limited by conservatism and the difficulty of learning highly multi-modal mappings. Recently, there has been an emerging paradigm of learning to improve solutions with synthetic trajectories constructed from the offline dataset. In this paper, we introduce a novel conditional generative modeling approach to produce trajectories toward high-scoring regions. First, we construct synthetic trajectories toward high-scoring regions using the dataset while injecting locality bias for consistent improvement directions. Then, we train a conditional diffusion model to generate trajectories conditioned on their scores. Lastly, we sample multiple trajectories from the trained model with guidance to explore high-scoring regions beyond the dataset and select high-fidelity designs among generated trajectories with the proxy function. Extensive experiment results demonstrate that our method outperforms competitive baselines on Design-Bench and its practical variants. The code is publicly available in https://github.com/dbsxodud-11/GTG.", "sections": [{"title": "Introduction", "content": "Optimizing complex and high-dimensional black-box functions is ubiquitous in science and engineering fields, including biological sequence design [1], materials discovery [2], and mechanical design [3, 4]. Traditional methods like Bayesian optimization have been developed to solve the problem by iteratively querying a black-box function. However, the online evaluation of the black-box function is restricted in most real-world situations due to time and safety constraints.\nFortunately, we often have access to a previously collected offline dataset. This problem setting is referred to as offline model-based optimization (MBO), and our objective is to find a design that maximizes a target function using solely an offline dataset [5]. As no online evaluation is available, a key challenge of MBO is the out-of-distribution (OOD) issue arising from limited data coverage. Suppose we train a proxy that predicts function values given input designs and naively apply a gradient-based optimizer based on the proxy to identify the optimal design. It would fall into sub-optimal results due to inaccurate predictions of the proxy in unseen regions.\nTo mitigate this issue, forward approaches mostly consider training a robust surrogate model against adversarial optimization of inputs and applying gradient-based maximization. Trabucco et al. [6] train a proxy with the regularization term to prevent overestimation on OOD designs. Fu and Levine [7] leverage normalized maximum likelihood estimator to handle uncertainty on unseen regions.\nThere are also several works that focus on fine-tuning the proxy for robustness on unexplored regions [8\u201310]. However, the generalization of the proxy outside of the dataset still remains challenging.\nOn the other hand, inverse approaches learn a mapping from function values to the input domain. Then, they generate high-scoring designs by querying the learned mapping with a high score. Prior approaches utilize expressive generative models to learn a mapping, such as variational autoencoders [11, 12], generative adversarial nets [13], autoregressive models [14] or diffusion models [15]. While these methods show promising results, they still suffer from the difficulty of learning highly unsmooth distributions and utilizing valuable information about the landscape of the black-box function.\nRecently, a new perspective has emerged on tackling the MBO by learning to improve solutions with synthetic trajectories constructed from the dataset [16, 17]. These methods aim to generate a sequence of designs toward high-scoring regions. It seems more promising than learning an inverse mapping that generates only a single design, as we can utilize information from sequences of designs that can help better understand the landscape of the target function. However, there is still room for improvement in this perspective. First, prior approaches construct trajectories with simple heuristics, which may lead to generating trajectories with inconsistent directions of improvement. Furthermore, the sequential nature of autoregressive models may lead to error accumulation during sampling [18].\nTo this end, we propose a novel conditional generative modeling approach to solve the MBO problem. Unlike prior inverse approaches, which generate a single design, we generate a sequence of designs toward high-scoring regions with guided sampling. Our method consists of four stages. First, we construct trajectories from the dataset while incorporating locality bias to distill the knowledge of the landscape of the target function into the generator. Then, we train a conditional diffusion model that generates the whole trajectory at once to bypass error accumulation and an auxiliary proxy. After training, we sample multiple trajectories conditioned on context data points and high score values. Finally, we select high-fidelity designs among generated trajectories by filtering with the proxy.\nWe empirically demonstrate that our method achieves superior performance on Design-Bench, a well-known benchmark for MBO with a variety of real-world tasks. Furthermore, we explore more practical settings, such as sparse or noisy datasets, verifying the generalizability of our method."}, {"title": "Preliminaries", "content": null}, {"title": "Problem setup", "content": "In offline model-based optimization (MBO), we aim to find a design x that maximizes the target black-box function f. Unlike the typical black-box optimization setting, we can only access an offline dataset D, and online evaluations are unavailable. The problem setup can be described as follows:\nfind x* = arg max f(x) s.t only an offline dataset D = {(xi, yi)}^N_{i=1} is given\nx\u2208Rd\nwhere x is a decision variable and y = f(x) is a target property we want to maximize."}, {"title": "Diffusion probabilistic models", "content": "Diffusion probabilistic models [19, 20] are a class of generative models that approximate the true distribution q0 with a parametrized model of the form: p\u03b8(x0) = \u222b p\u03b8(x0:T)dx1:T, where x0 \u223c q0 and x1, . . . , xT are latents with the same dimensionality. The joint distribution p\u03b8(x0:T) is called the reverse process, defined as a Markov chain starting from standard Gaussian pT(xT) = N(0, I):\np\u03b8(x0:T) = pT(xT) \u220f^T_{t=1} p\u03b8(xt\u22121|xt), p\u03b8(xt\u22121|xt) = N(\u00b5\u03b8(xt, t), \u03a3t)\nwhere p\u03b8(xt\u22121|xt) is parametrized Gaussian transition from timestep t to t \u2212 1.\nWe define a forward process, which is also fixed as a Markov chain that adds Gaussian noise to the data with the variance schedule \u03b21, . . . , \u03b2T:\nq(x1:T|x0) = \u220f^T_{t=1} q(xt|xt\u22121), q(xt|xt\u22121) = N(\u221a1 \u2212 \u03b2t xt\u22121, \u03b2tI)"}, {"title": "Methodology", "content": "In this section, we introduce GTG, Guided Trajectory Generation, a conditional generative modeling approach for solving MBO problem by learning to improve solutions using the offline dataset. We first construct trajectories towards high-scoring regions while incorporating locality bias for consistent improvement directions. Then, we train the conditional diffusion model to generate trajectories and a proxy model. Finally, we sample multiple trajectories using the diffusion model with guided sampling and filter high-fidelity designs with the proxy. Figure 1 shows the overview of the proposed method."}, {"title": "Constructing trajectories", "content": "We construct a set of trajectories Dtraj from the offline dataset D to gather information on learning to improve designs. In this paper, each trajectory \u03c4 \u2208 Dtraj is a set of H input-output pairs and can be represented as a two-dimensional array:\n\u03c4 =\n[\nx1 x2 . . . xH\ny1 y2 . . . yH\n],\n(xh, yh) \u2208 D \u2200h = 1, . . . , H\nWhile prior works construct trajectories via sorting heuristics or sampling from high-scoring regions, we focus on constructing trajectories that give us more valuable information for learning to improve designs towards higher scores. To achieve this, we develop a novel method to construct trajectories based on two desiderata.\nFirst, the trajectory should be towards high-scoring regions while containing information on the landscape of the target black-box function. Second, the trajectories should be diverse and not converge to a single data point with the highest score of the dataset, as our objective is to discover high-scoring designs beyond the offline dataset by generalizing the knowledge of learning to improve solutions."}, {"title": "Training models", "content": "Given our trajectory dataset Dtraj, our objective is to learn the conditional distribution of trajectories towards high-scoring regions. We choose diffusion models, which have a powerful capability to learn the distribution of complex and high-dimensional data [22, 23], to generate trajectories. Our objective is then transformed from searching high-scoring designs to maximizing the conditional likelihood of trajectories, which can be achieved by minimizing the loss in Equation (5):\n\u03b8* = arg max E\u03c4\u223cDtraj [log p\u03b8 (\u03c4|y(\u03c4))]\n\u03b8\nwhere y(\u03c4) = \u2211^H_{h=1} yh is the sum of scores in the trajectory \u03c4. By training a diffusion model to generate a sequence of designs instead of a single design, we can efficiently distill the knowledge of the complex landscape of the target function into the diffusion model.\nIn addition, we also train a forward proxy f\u03b8 using the dataset D. We can use the proxy to filter high-scoring designs from the trajectories generated by the trained diffusion model."}, {"title": "Sampling trajectories from the diffusion model", "content": "After training, we sample trajectories with guided sampling. We use classifier-free guidance to generate trajectories. To be specific, we sample from the diffusion model using Equation (6), where y* (\u03c4) is the target conditioning value. Following prior works [13, 16], we assume that we know the maximum score y* and set y*(\u03c4) = \u03b1 \u00b7 (Hy*), where \u03b1 controls the exploration level of the generated trajectories. We discuss the role of \u03b1 in more detail in the subsequent section.\nTo fully utilize the expressive power of diffusion models, we introduce an additional strategy, context conditioning, during the sampling. We generate trajectory with diffusion model while inpainting the C context data points of the trajectory with \u03c4ctx, which is a subtrajectory sampled from Dtraj. By conditioning trajectories in different contexts, we can effectively explore diverse high-scoring regions."}, {"title": "Selecting candidates", "content": "After generating trajectories, we introduce filtering to select candidates for evaluation. In other words, we select top-Q samples in terms of the predicted score from the proxy. By filtering with the proxy, we can exploit the knowledge from the dataset to search high-scoring designs [13, 14, 24]."}, {"title": "Experimental evaluation", "content": "In this section, we present the results of our experiments on various tasks. First, we analyze our method in a toy 2D experiment. Then, we present the results on the Design-Bench and its practical variants to verify the effectiveness of the method. We also conduct extensive analyses on various aspects to deepen our understanding of the proposed method."}, {"title": "Toy 2D experiment", "content": "We first evaluate our method using a toy setting to analyze each component of our method thoroughly. We choose Branin, a synthetic 2D function with three distinct global maxima. Figure 2 shows the contour plot of the Branin function. The analytical form of the Branin function is as follows:\nf(x1, x2) = \u2212a (x2 \u2212 bx^2_1 + cx1 \u2212 r)^2 \u2212 s (1 \u2212 t) cos(x1) \u2212 s"}, {"title": "Design-Bench tasks", "content": "In this section, we present the experiment results of our method on Design-Bench tasks [5]. We conduct experiments on two discrete tasks and three continuous tasks. For each task, we have an offline dataset from an unknown oracle function. We present the detailed task description below.\nTFBind8 and TFBind10 [1]. We aim to find a DNA sequence of the length 8 and 10 with maximum binding affinity with a particular transcription factor.\nSuperconductor [2]. We aim to design a chemical formula, represented by an 86-dimensional vector, for a superconducting material with a high critical temperature.\nAnt and D'Kitty Morphology [4, 25]. We aim to optimize the morphological structure of two simulated robots. The morphology parameters include size, orientation, and the location of the limbs. Ant has 60 continuous parameters, and D'Kitty has 56 continuous parameters."}, {"title": "Baselines", "content": "For baselines, we prepare four main categories to solve MBO problems. First, we compare our method with traditional methods widely used in online black-box optimization settings, such as BO-qEI [26], CMA-ES [27], REINFORCE [28], and Gradient Ascent."}, {"title": "Evaluation metrics", "content": "For evaluation, we follow the protocol of prior works. We identify Q = 128 designs selected by the algorithm and report a normalized score of 100th percentile design. For all algorithms, we run experiments over 8 different seeds and report mean and standard errors.\nTo evaluate our method, we construct trajectories of length H = 64 and train a conditional diffusion model for each task. After training, we sample N = 128 trajectories conditioning on C = 32 context data points and setting \u03b1 = 0.8 across all tasks. Finally, we filter top-128 candidates among generated designs with the predicted score from the proxy for evaluation."}, {"title": "Main results", "content": "As shown in the Table 1, our method achieves an average rank of 1.6, the best among all competitive baselines. Our method performs best on two tasks and is runner-up on three tasks, demonstrating superior performance across different tasks. The experiment results underscore that training diffusion models and generating trajectories with guided sampling can effectively explore high-scoring regions."}, {"title": "Practical variants of Design-Bench tasks", "content": "In this section, we present experiment results in a more practical setting of Design-Bench tasks. While Design-Bench assumes a large, unbiased offline dataset containing thousands of data points for the training model, such a setting is impractical in most cases. Therefore, we prepare two additional practical settings, sparse and noisy datasets, to verify the robustness of our method in such extreme cases. In a sparse setting, we only provide x% of the original dataset for training. For the noisy setting, we add x% of standard Gaussian noise to the normalized score values. We choose recent papers published after 2022, BDI, ICT, DDOM, and BONET for primary baselines. Please refer Appendix A.2 for detailed experiment settings and Appendix D.5 for results with more baselines.\nTable 2 shows the results of our method and recent baselines in sparse datasets. The table shows that our method mostly outperforms other baselines even in sparse datasets, demonstrating the superiority of exploiting knowledge of the target function by constructing diverse trajectories from the dataset. Table 3 reports the experiment results on the noisy settings. We find that even with 50% of noise, our method can find relatively high-scoring designs, demonstrating its robustness in practical settings."}, {"title": "Additional analysis", "content": null}, {"title": "Ablation on trajectory construction", "content": "We propose a novel trajectory construction strategy by incorporating locality bias. To verify the effectiveness of the strategy, we compare our strategy with prior approaches, SORT-SAMPLE and Top-p Percentile, suggested by BONET and PGS, respectively. Table 4 shows that our strategy outperforms prior strategies across various tasks. We conduct additional analysis on trajectory construction strategies in Appendix D.1."}, {"title": "Ablation on sampling procedure", "content": "We analyze the effectiveness of strategies we introduced during the sampling procedure, namely context conditioning (CC), classified-free guidance (CF), and filtering (F). Across various tasks, it is evident that all components are crucial for improving performance as demonstrated in Table 5. We conduct further analysis on sampling strategies in Appendix D.2."}, {"title": "Hyperparameter sensitivity", "content": "We also conduct experiments on the effect of various hyperparameters we introduced in this paper. We first train a conditional diffusion model with various lengths (H). As shown in Figure 3a, increasing H leads to achieving higher performance. We also conduct experiments by varying the number of contexts (C) and the exploration level (\u03b1). Figure 3b shows that C = 32 achieves superior performance while conditioning with too many contexts degrades performance. Finally, Figure 3b shows a strong correlation between \u03b1 and the score, demonstrating the effectiveness of guided sampling. We conduct further analysis on hyperparameters in Appendix D.2."}, {"title": "Varying evaluation budget", "content": "We provide experiment results with a small number of evaluation budgets (Q). As shown in Figure 4, we generally outperform most baselines even with a relatively low evaluation budget."}, {"title": "Effect of unsupervised pretraining", "content": "It might be beneficial to pretrain the diffusion model when we have a large-scale unlabeled dataset and a few designs of labeled points [29]. To this end, we discuss the effectiveness of pretraining diffusion models with unlabeled datasets in Appendix D.3."}, {"title": "Time complexity of sampling procedure", "content": "We also conduct analysis on the time complexity of the sampling procedure of our method in Appendix D.4."}, {"title": "Related works", "content": null}, {"title": "Offline model-based optimization", "content": "In offline MBO, generalization outside the offline dataset is crucial for success. While there have been attempts to train a robust surrogate model to achieve accurate predictions on unseen regions [8\u201310], effectively exploring high-scoring regions remains challenging.\nRecently, a new perspective on solving the MBO problem has emerged by learning to improve solutions from synthetic trajectories and generalizing the knowledge to find designs beyond the dataset [16, 17]. BONET [16] trains an autoregressive model to generate optimal trajectories conditioned on a low regret budget. PGS [17] trains RL policy with trajectories consisting of high-scoring designs to roll out optimal trajectories. GTG falls under this category but adopts a unique approach to constructing trajectories with local search and utilizing diffusion models to enhance performance."}, {"title": "Generative models for decision making", "content": "Generative models have emerged as a powerful tool for decision-making problems, including bandit problems [30], reinforcement learning [18, 31\u201334], and optimization [15, 35]. In offline MBO, there are inverse approaches to learning a mapping from function values to input domains with generative models and sample designs from high-scoring regions [11, 12, 14, 15]. DDOM [15] utilizes a conditional diffusion model and generates high-scoring samples with reweighted training and classifier-free guidance. DiffOPT [35] considers a constrained optimization setting and introduces a two-stage framework that begins with a guided diffusion process for warm-up, followed by a Langevin dynamics stage for further correction. Our method distinguishes itself from prior works by utilizing diffusion models to generate trajectories toward high-scoring regions by learning to improve solutions from the dataset."}, {"title": "Discussion and conclusion", "content": "In this paper, we introduce GTG, a novel conditional generative modeling approach for learning to improve solutions from synthetic trajectories constructed with the dataset. First, we construct diverse trajectories toward high-scoring regions while incorporating locality bias. Then, we train the conditional diffusion model and proxy function. After training, we generate trajectories with classifier-free guidance and context-conditioning to generalize the knowledge on how to improve solutions. Lastly, our filtering strategy for selecting candidates further improves the performance. Our extensive experiments demonstrate the generalizability of GTG.\nLimitation and future work. While our method shows powerful generalizability on Design-Bench tasks, we resort to filtering designs with the proxy function trained with the offline dataset, which may result in inaccurate predictions. Although our filtering strategy works well in sparse and noisy settings, one may consider constructing a robust proxy model to handle the uncertainty of its predictions."}, {"title": "Methodology Details", "content": "In this section, we present the method details, including model implementations and architectures, training schemes, hyperparameter configurations, and computing resources."}, {"title": "Trajectory Construction", "content": "In terms of constructing trajectories, we introduce two variables, K and \u03f5, which control the level of locality and optimality of the trajectories. For too large value of K, we construct trajectories with inconsistent directions of improvement, while the extremely small value of K leads to trajectories wandering the initial data point. If we lower the \u03f5 close to zero, we only allow monotonic improvement, while large \u03f5 values lead to suboptimal trajectories. We present the hyperparameters for our experiments in the Table 8. We also conduct additional analysis on trajectory construction in Appendix D.1."}, {"title": "Training Models", "content": null}, {"title": "Training Diffusion Model", "content": "We use temporal U-Net architecture from Diffuser [18] as a backbone of the diffusion model. For discrete tasks, we train the model using Adam optimizer [37] for 1 \u00d7 10^4 training steps with the learning rate of 1 \u00d7 10^-3. While one could use discrete diffusion models [38, 39] for discrete tasks, we use continuous diffusion models with continuous relaxation of discrete inputs for simplicity. For continuous tasks, we train the model for 5 \u00d7 10^4 steps with a learning rate of 1 \u00d7 10^-4. The hyperparameters we used for modeling and training are listed in Table 10."}, {"title": "Training Proxy Model", "content": "We use MLP with 2 hidden layers with 1024 hidden units and ReLU activations to implement the proxy function. As our objective is filtering high-fidelity designs with the proxy, we introduce a rank-based reweighting suggested by [40] during training to make the proxy model focus on high-scoring regions. For discrete tasks, we train a proxy model using Adam optimizer for 1 \u00d7 10^3 training steps with a learning rate of 1 \u00d7 10^-3. For continuous tasks, we train the model for 5 \u00d7 10^3 training steps with a learning rate of 1 \u00d7 10^-3. The hyperparameters we used for modeling and training are listed in Table 11."}, {"title": "Sampling Procedure", "content": "We sample trajectories with T = 200 denoising steps across all tasks. For classifier-free guidance, we set the guidance scale \u03c9 as 1.2. In practice, we sample a batch of trajectories to generate multiple trajectories in parallel. We analyze the time complexity of sampling trajectories from the diffusion model in Appendix D.4"}, {"title": "Baseline Details", "content": "In this section, we provide more details on the baselines used for our experiments.\nBaselines from Design-Bench [5]. We take the implementations of most baselines from open-source code1. It contains baselines of BO-qEI [26], CMA-ES [27], REINFORCE [28], Gradient Ascent, CbAS [11], MINs [13], and COMs [41]. We reproduce the results with 8 independent random seeds.\nNEMO [7]. NEMO leverages a normalized maximum likelihood estimator to handle uncertainty in unseen regions and prevent adversarial optimization while performing gradient ascent. As there is no open-source code, we refer to the results of NEMO from [9].\nBDI [24]. BDI learns forward mapping from low-scoring regions to high-scoring regions, and its backward mapping distills the knowledge of the offline dataset to search for optimal designs. We follow the hyperparameter setting of the paper and reproduce the results with the open-source code2.\nICT [9]. ICT maintains three symmetric proxies and enhances the performance of the ensemble by co-teaching and importance-aware sample reweighting. We follow the hyperparameter setting of the paper and reproduce the results with the open-source code3.\nDDOM [15]. DDOM leverages diffusion models to model distribution over high-scoring regions and sample designs with classifier-free guidance. We follow the hyperparameter setting of the paper except for the evaluation budget Q for a fair comparison. We find that there is a performance drop in several tasks when we use Q = 128 instead of 256. We reproduce the results with the open-source code4.\nBONET [16]. BONET trains an autoregressive model with trajectories constructed from the offline dataset and generalizes the knowledge to explore high-scoring regions. We follow the hyperparameter setting of the paper except for the evaluation budget Q for a fair comparison. We find that there is a performance drop in several tasks when we use Q = 128 instead of 256. We reproduce the results with the open-source code5.\nPGS [17]. PGS trains a policy to guide gradient-based optimization by reformulating the MBO problem as an offline RL problem. We follow the hyperparameter setting of the paper and reproduce the results with the open-source code6."}, {"title": "Extended Additional Analysis", "content": "In this section, we present additional analysis on GTG which is not included in the main section due to the page limit."}, {"title": "Additional Analysis on Trajectory Construction", "content": null}, {"title": "Analysis on Score Distribution of Trajectories", "content": "We conduct additional analysis on our trajectory construction method. We try to generate diverse trajectories toward high-scoring regions by randomly selecting subsequent designs from K neighbors and allowing local perturbations. To this end, we visualize the shift in the distribution of function values via various trajectory construction strategies in the Superconductor task. As shown in Figure 7, the SORT-SAMPLE strategy suggested by BONET constructs trajectories solely on high-scoring designs, which can be easily trapped into local optima. Unlike SORT-SAMPLE, our method shifts distribution towards high-scoring regions while using the information of low-scoring regions to distill the knowledge of the landscape of the target function to the generator."}, {"title": "Analysis on Hyperparameters in Trajectory Construction", "content": "We also conduct additional analysis on hyperparameters in trajectory construction, K and \u03f5. Figure 8 shows the performance of GTG in TFBind8 task by varying K and \u03f5. While using too large K or too small \u03f5 may lead to a relatively low performance, we do not see much variation with different values."}, {"title": "Additional Analysis on Sampling Procedure", "content": null}, {"title": "Various Strategies for Guided Sampling", "content": "In this section, we explore various strategies for guiding diffusion models to generate high-scoring designs. As we also generate score values, it could be possible to guide diffusion models to generate high-scoring designs by inpainting score values with the desired values. To this end, we conduct additional experiments on Design-Bench tasks by generating trajectories with inpainting instead of classifier-free guidance. Specifically, we inpaint the y values of the generated trajectories as y*, the normalized score of the optimal design.\nTable 12 shows the performance of different guiding strategies. It confirms that conditioning by classifier-free guidance performs better than the inpainting strategy, justifying our decision choice."}, {"title": "Diversity Analysis", "content": "In this section, we explore the trade-off between performance and diversity via filtering strategy. While the filtering strategy boosts the performance of our method by eliminating potentially sub-optimal designs, it may reduce the diversity of candidates, which may be crucial in tasks such as drug discovery due to proxy misspecification [42].\nTo this end, we measure the diversity of the candidates, following the procedure of [14]. For measurement, we use the average of the pairwise distance between candidates as below:\nDiversity(D) = 1/(|D|(|D|\u22121)) \u2211_{x\u2208D} \u2211_{x'\u2208D\\{x\\}}d(x,x')\nwhere d(x, x') is a pairwise distance between samples. For discrete tasks, we use the hamming-ball distance metric. For continuous tasks, we compute L2 distance.\nTable 13 illustrates the effect of filtering on performance and diversity. As expected, we achieve higher performance through filtering while sacrificing the diversity of the candidate set. It might be beneficial to automatically balance performance and diversity trade-off by measuring the uncertainty of the proxy function. We leave it as a future work."}, {"title": "Impact of Exploration Level", "content": "In this section, we explore the impact of the exploration level (\u03b1) on the generated samples. As depicted in Figure 3c, increasing \u03b1 leads to higher performance, indicating the importance of classifier-free guidance. However, we observe that conditioning on extremely high \u03b1 leads to sub-optimal performance, as illustrated in Figure 9. Conditioning on extremely high \u03b1 guides the diffusion model to over-exploration, resulting in sub-optimal out-of-distribution designs. Note that we do not fine-tune \u03b1 for each task and fix it with the value of 0.8 across all tasks, which generally exhibits good performance."}, {"title": "Assumption on optimal value", "content": "We assume that the optimal value y* of each task is known, following prior works [13, 16]. However, it is not always possible to know the exact optima. To this end, we estimate y* with \u03b3 \u00b7 ymax, where ymax is the maximum value of the dataset and evaluate GTG by conditioning on the estimated value. As depicted in Table 14, conditioning on \u03b3 \u00b7 ymax achieves comparable performance and even outperforms the performance of conditioned on exact optima in the TFBind8 task. However, it introduces an additional hyperparameter \u03b3, whose optimal value varies across tasks. Therefore, we rely on assuming the exact optima, which is not an issue in many problems."}, {"title": "Effect of Unsupervised Pretraining", "content": "It might be beneficial to pretrain the diffusion model with unlabeled data when we have limited data points. Specifically, there is a recent work EXPT [29], which trains an autoregressive model using synthetic trajectories constructed from the large-scale unlabeled dataset and adapts new tasks by conditioning on a few labeled points. To this end, we discuss the effect of pre-training GTG with unlabeled datasets. We follow a similar procedure of EXPT to generate a synthetic dataset. Formally, we sample synthetic functions from Gaussian Processes [43] with an RBF kernel and assign pseudo values to the unlabeled data points from synthetic functions. Please refer to [29] for a more detailed setting. Given a synthetic dataset, we pretrain diffusion models with trajectories constructed from the dataset using the proposed method. Then, we generate samples by conditioning on context data points from the labeled dataset. For labeled dataset, we randomly select 1% of the original dataset.\nTable 15 shows the experiment results on various Design-Bench tasks. As shown in the table, pretraining generally improves the performance of GTG in the sparse data setting. We also find that GTG with pretraining outperforms ExPT in 3 of 5 tasks. While we do not assume the existence of the large-scale unlabeled dataset in the main experiment and pretraining is not a main focus of our research, it might be beneficial to analyze the effect of pretraining with synthetic datasets in offline MBO thoroughly as in other problems [44, 45]."}, {"title": "Time Complexity of Sampling Procedure", "content": "In this section, we analyze the time complexity of the sampling procedure of GTG. To generate trajectories, we run T = 200 denoising timesteps with classifier-free guidance and context-conditioning to sample N = 128 trajectories, which takes approximately 9.41s and 9.47s in wall clock time for the Ant and D\u2019Kitty tasks, respectively. We visualize the trade-off between the performance and runtime of sampling by varying the number of denoising timesteps. As shown in Figure 10, we can decrease the number of denoising timesteps even one-tenth with minimal loss in performance. Please note that sampling time is negligible compared to evaluating black-box functions, which is mostly expensive in real-world settings."}, {"title": "Extended Experiment Results", "content": "In this section, we present extended experiment results in sparse and noisy datasets. As shown in Tables 16 and 17, our method outperforms most baselines in various practical settings. Note that we cannot conduct experiments with NEMO and RoMA, as there is no code publicly available."}, {"title": "Additional Visualization on Toy 2D Experiment", "content": "We present additional visualization results from the Toy 2D experiment. As shown in Figure 11, GTG is able to generate diverse trajectories toward high-scoring designs by conditioning on different context points and classifier-free guidance."}, {"title": "Broader Impact", "content": "Optimization for real-world designs presents both opportunities and risks. For instance, while the design of new pharmaceuticals holds the promise of curing previously untreatable diseases, there is the potential for misuse, such as creating harmful biochemical agents. Researchers should be diligent to ensure that their innovations are employed in ways that contribute positively to societal welfare."}, {"title": "NeurIPS Paper Checklist", "content": "Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\nAnswer: [Yes", "nJustification": "We clearly state the main claims in the abstract and introduction.\nGuidelines:\n\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.\n\u2022 The abstract and/or introduction should clearly state the claims made", "paper.\nLimitations\nQuestion": "Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes"}, {"nJustification": "We discuss limitations in Section 7.\nGuidelines:\n\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.\n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.\n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.\n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.\n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.\n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.\n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.\n\u2022 While"}]}