{"title": "GRADIENT ROUTING: MASKING GRADIENTS TO LOCALIZE COMPUTATION IN NEURAL NETWORKS", "authors": ["Alex Cloud", "Jacob Goldman-Wetzler", "Ev\u017een Wybitul", "Joseph Miller", "Alexander Matt Turner"], "abstract": "Neural networks are trained primarily based on their inputs and outputs, without regard for their internal mechanisms. These neglected mechanisms determine properties that are critical for safety, like (i) transparency; (ii) the absence of sensitive information or harmful capabilities; and (iii) reliable generalization of goals beyond the training distribution. To address this shortcoming, we introduce gradient routing, a training method that isolates capabilities to specific subregions of a neural network. Gradient routing applies data-dependent, weighted masks to gradients during backpropagation. These masks are supplied by the user in order to configure which parameters are updated by which data points. We show that gradient routing can be used to (1) learn representations which are partitioned in an interpretable way; (2) enable robust unlearning via ablation of a pre-specified network subregion; and (3) achieve scalable oversight of a reinforcement learner by localizing modules responsible for different behaviors. Throughout, we find that gradient routing localizes capabilities even when applied to a limited, ad-hoc subset of the data. We conclude that the approach holds promise for challenging, real-world applications where quality data are scarce.", "sections": [{"title": "INTRODUCTION", "content": "As AI systems become more powerful and more prevalent, there is an increasing need to explain and control the inner mechanisms governing their behavior. To address this challenge, some researchers aim to fully understand AI systems, either by reverse engineering the operations of conventionally trained models (Olah et al., 2020; Olsson et al., 2022) or with inherently interpretable architectures (Koh et al., 2020; Hewitt et al., 2023; Xin et al., 2022). This is not necessary. If we could understand or control the mechanisms underlying a neural network's computation with respect to a limited set of safety-critical properties, such as hazardous information or the capacity for deception, that might be sufficient to make significant safety guarantees.\nTo achieve targeted control over neural network internals, we propose gradient routing, a training method for localizing capabilities to chosen subregions of a neural network. Gradient routing is a modification of backpropagation that uses data-dependent, weighted masks to control which network subregions are updated by which data points. By appropriately specifying these masks, a user can configure which parts of the network (parameters, activations, or modules) are updated by which data points (e.g. specific tokens, documents, or based on data labels). The resulting network is similar to a conventionally trained network, but with some additional internal structure.\nOur contributions are as follows. In Section 2, we discuss prior work on neural network modularity, unlearning, and scalable oversight. In Section 3, we define gradient routing and comment on its practical implementation. Most of the paper is a tour of gradient routing applications:\nSection 4.1 We use gradient routing to control the encodings learned by an MNIST autoencoder to split them into two halves, with each half representing different digits."}, {"title": "RELATED WORK", "content": "Training to localize pre-specified capabilities. Akin to gradient routing, work in modular machine learning trains modules to contain concepts or abilities determined in advance of training. Typically, modular architectures involve a routing function that selects modules to apply on a forward pass (Pfeiffer et al., 2023). Routing functions are often unsupervised, as with a typical mixture of experts setup (Jacobs et al., 1991; Eigen et al., 2013; Shazeer et al., 2017). However, some approaches route inputs based on metadata, creating modules with known specializations (Waibel & II, 1992). For example, routing has been based on (i) the modality of data in multi-modal models (Pfeiffer et al., 2021), (ii) language (Pfeiffer et al., 2020; 2022; Fan et al., 2021), and (iii) low- vs. high-level control or task type in robotics (Heess et al., 2016; Devin et al., 2017). Gururangan et al. (2021) separate the training data of a language model by domain and assign one expert in each layer to a single domain. By disabling the expert for a domain, they are able to approximate a model that was not trained on the domain.\nOther methods freeze the weights of a pre-trained model and train a newly added module, with the aim of localizing the task to the new module (Rebuffi et al., 2017; 2018; Houlsby et al., 2019; Bapna & Firat, 2019). Zhang et al. (2024) locate capabilities in models by learning a weight mask, transfer the identified sub-network to a randomly initialized model, then train as if from scratch. By choosing a suitable sub-network, they can, for example, induce a vision model to identify ImageNet (Deng et al., 2009) classes by shape, not texture.\nAdversarial representation learning and concept erasure. In order to control the information in learned representations, prior works have trained feature extraction networks adversarially against discriminator networks that predict this information (Goodfellow et al., 2014; Schmidhuber, 1992; Ganin & Lempitsky, 2015; Ganin et al., 2016; Edwards & Storkey, 2015). Other works have removed concepts by modifying activations during inference (Ravfogel et al., 2020; Belrose et al., 2023; Elazar et al., 2020; Bolukbasi et al., 2016).\nRobust unlearning. Machine unlearning seeks to remove undesired knowledge or abilities from a pre-trained neural network (Cao & Yang, 2015; Li et al., 2024). Typical unlearning methods are brittle in the sense that the unlearned abilities of the model can be recovered by fine-tuning on a tiny number of data points (Henderson et al., 2023; Sheshadri et al., 2024; Lynch et al., 2024; Liu et al., 2024; Shi et al., 2024; Patil et al., 2023; Lo et al., 2024; Lermen et al., 2023). Lee"}, {"title": "GRADIENT ROUTING CONTROLS WHAT IS LEARNED WHERE", "content": "In order to train models that are interpretable and controllable in targeted ways, we seek to localize specific knowledge or capabilities to network subregions. To do this, gradient routing applies data-dependent, weighted masks to gradients during backpropagation to configure what data (whether it be defined in terms of tokens, documents, or based on other labels) is learned where in the network (e.g. at the level of parameters, activations, or modules). The result is a model with a partially-understandable internal structure, where particular regions correspond to known capabilities. Throughout this paper, we will use \u201croute X to Y\u201d to mean \"use gradient routing to limit learning updates for data points X to region Y of the neural network.\"\nLet $(V, E)$ be the nodes and edges of the computational graph corresponding to a neural network and loss function, with $v(z)$ taken to be the output of node $v$ if $z$ is input to the network. Given a dataset $D = \\{z_i\\}_{i=1}^n$, for each data point $z_i$, gradient routing requires the specification of a gradient"}, {"title": "APPLICATIONS", "content": "ROUTING GRADIENTS TO PARTITION MNIST REPRESENTATIONS\nAs a first example of feature localization via gradient routing, we train a simple MLP autoencoder on the MNIST handwritten digit dataset (LeCun et al., 1998) and use label-dependent stop-gradients to control where features for different digits are encoded. The goal is to obtain an autoencoder that reconstructs all digits (0-9) via an encoding that is made up of non-overlapping subcomponents corresponding to distinct subsets of digits. We choose subsets \\{0, 1, 2, 3, 4\\} and \\{5, 6, 7, 8, 9\\}. To hint at the potential difficulty of this task, we note the encodings learned by an autoencoder trained on one of these sets admit low-error reconstructions on the other set, despite never being trained on it (details in appendix B).\nWe use a simple architecture of three-layer MLP modules with ReLU activations: an Encoder, a Decoder, and a Certificate. The Encoder processes a $28 \\times 28$ image into a vector in $\\mathbb{R}^{32}$, and the Decoder processes that vector into a $28 \\times 28$ reconstruction. The Certificate is a decoder trained only on the bottom half of the encoding, which takes values in $\\mathbb{R}^{16}$. Certificate updates do not affect the encoding. If the Decoder can reconstruct a digit that the Certificate cannot, this \u201ccertifies\u201d that robust feature localization occurred (into the top half encoding, and away from the bottom half).\nWe use gradient routing to train an encoding split so that the top half encodes digits 0-4 and the bottom half encodes digits 5-9. While training on all digits, we apply stop-gradients to the bottom half of the encoding for digits 0-4 and stop-gradients to the top half of the encoding for digits 5-9. To induce specialization in the two halves of the encoding, we add the L1 norm of the encoding as a penalty term to the loss."}, {"title": "LOCALIZING TARGETED CAPABILITIES IN LANGUAGE MODELS", "content": "In this section, we show that gradient routing applied to a small set of tokens can be used to localize broader features or capabilities in Transformer (Vaswani, 2017) language models. This is first demonstrated in terms of model activations, then applied to MLP layers for the purpose of robust unlearning.\nSTEERING SCALAR: LOCALIZING CONCEPTS TO RESIDUAL STREAM DIMENSIONS\nElhage et al. (2021) frames the inter-block activations of a Transformer, or the residual stream, as the central communication channel of a Transformer, with all layers \"reading from\" and \"writing into\" it. Usually, the standard basis of the residual stream is indecipherable, with dimensions not corresponding to interpretable concepts. We pre-train a 303M parameter Transformer on the FineWeb-Edu dataset (Penedo et al., 2024) while routing the gradients for all California\u00b9 tokens to the 0th entry of the residual stream on layers 6-18. On token positions predicting California, we mask gradients (to zero) on every residual stream dimension except the 0th in layers 6-18. This masking causes the learning updates for those token positions to be localized to the weights that write into the Oth dimension of the residual stream. After training, we look at which tokens' unembedding vectors"}, {"title": "GRADIENT ROUTING ENABLES ROBUST UNLEARNING VIA ABLATION", "content": "Robust unlearning (Sheshadri et al., 2024) means training models which lack the internal mechanisms or \u201cknowledge\" required for certain tasks, as opposed to merely performing poorly on those tasks. To address this open problem, we show that gradient routing can be used to localize capabilities to a known region of the network, then delete that region, removing those capabilities.\nTo enable comprehensive comparisons, our initial study on robust unlearning applies gradient routing to a small (28M parameter) Transformer. This model is trained on an LLM-generated dataset of simple children's stories based on the TinyStories dataset (Eldan & Li, 2023; Janiak et al., 2024). We partition the data into: 1) a forget set made up of any story containing one of the keywords \"forest(s)\", \"tree(s)\u201d, or \u201cwoodland(s)\", and 2) a retain set made up of all other stories. An example story is given in appendix C. The goal is to train a model that performs well on the retain set but poorly on the forget set, and whose forget set performance is not easily recoverable by fine-tuning. To do this, we route specific forget tokens to designated MLP neurons using three-step process termed Expand, Route, Ablate (ERA):\nIncrease the dimensionality of the model by adding randomly-initialized neurons to particular target layers.\nTrain the model by supervised learning on next-token prediction, but on select tokens in forget stories, reduce the learning rate in the original dimensions of the model at the target layers.\nDelete the additional neurons. Post-ablation, apply a very small number of steps of fine-tuning on retain data to correct for degradation caused by ablation.\""}, {"title": "SCALING ROBUST UNLEARNING TO LARGER LANGUAGE MODELS", "content": "Gradient routing can localize capabilities in larger models. Motivated by the dual-use nature of AI (Urbina et al., 2022), we would like to train useful models that lack certain harmful capabilities. Here, we seek to localize and remove bioweapon-related capabilities in a 0.7B parameter Transformer. To do this, we route 20 tokens related to virology to the 0th through 79th MLP dimensions on layers 0 through 7 of the Transformer. Appendix E provides further details on the model and training.\nTable 1 evaluates the model on a validation split of regular FineWeb-Edu data and on some of the WMDP-bio (Li et al., 2024) forget set. Ablating the target region of the network increases loss greatly on both datasets. We then fine-tune the model on a train split of FineWeb-Edu for 32 steps to restore some performance. Finally, we retrain for twenty steps on a separate split of two WMDP-bio forget set datapoints, as Sheshadri et al. (2024) do, and report the lowest loss on the validation split of the WMDP-bio forget set.\nThe results are striking: even after retraining on virology data, loss increases much more on the WMDP-bio forget set (+0.182) than on FineWeb-Edu (+0.032), demonstrating successful localization and robust removal of virology capabilities. A natural concern would be that ablation merely decreased probabilities on the routed tokens, without decreasing overall virology capabilities. To test this, we measured cross-entropy loss on the forget set excluding the 20 tokens we routed on. Even after this exclusion, the loss increase is still much higher than the increase on FineWeb-Edu (+0.171 vs. +0.032). This shows that gradient routing generalizes beyond limited labels."}, {"title": "LOCALIZING BEHAVIORAL MODULES ENABLES SCALABLE OVERSIGHT IN REINFORCEMENT LEARNING", "content": "In complex settings, reliable data labels are often scarce, especially when labeling requires human input (Stiennon et al., 2020; Bai et al., 2022; Baker et al., 2022). Scalable oversight (Amodei et al., 2016) means effectively utilizing limited labels to obtain a performant policy. In this section, we show that gradient routing's effective use of limited labels enables scalable oversight, outperforming an outcome-based baseline and a data filtering baseline.\nWe use gradient routing to train a policy to reach two types of squares in a gridworld, DIAMOND and GHOST. Access to the type of square reached, or oversight, is only sometimes available. The aim is to produce a policy that reliably navigates to DIAMOND and not GHOST, even when oversight is lacking. In real-world problems, label availability may be correlated with task type or difficulty, and a policy might not generalize correctly because of this fact (Langosco et al., 2022). To model this fundamental difficulty, we allow the policy to condition on the presence or absence of oversight. During training, we route policy gradients through two separate modules in a mixture of experts layer, each responsible for reaching one of the aforementioned squares. During evaluation, by ab-"}, {"title": "DISCUSSION", "content": "Absorption. Routing a subset of the data related to some knowledge or capability appears to localize that knowledge or capability more generally. This held for an i.i.d. subset of the data (TinyStories unlearning in section 4.2.2), and for semantically limited data (steering scalar in section 4.2.1, virology unlearning in section 4.2.3, scalable oversight in section 4.3). We hypothesize an absorption effect: routing limited data to a region creates internal features or units of computation in that region which are relevant to a broader task; these units then participate in the model's predictions on related, non-routed data; the resulting prediction errors are then backpropagated to the same region, creating a positive feedback loop that reinforces those features. To the extent that absorption is true, it has advantages over data filtering methods: if data filtering labels are limited either in quantity or semantically, then harmful capabilities can still be learned where labels are missing, whereas routing that data to a region absorbs those capabilities into that region, which can then be removed.\nBenefits of localization vs. suppression. When the ability to label (or score) undesirable behavior is imperfect, attempting to suppress the behavior may be perilous: a model may learn to exploit the limits of the labels, rather than learning the desired behavior (Goodhart, 1984; Karwowski et al., 2023). Our study of scalable oversight presents a model of this scenario, demonstrating the advantage of localization as opposed to attenuation of undesirable behavior. This advantage may apply more broadly, for example, to machine learning problems where capabilities are entangled, in the sense that there are connections or dependencies between the computation learned to perform different tasks (Arora & Goyal, 2023; de Chiusole & Stefanutti, 2013). Entanglement might occur because certain capabilities or behaviors are reinforced by a broad range of training objectives (Omohundro, 2008; Turner et al., 2021; Krakovna et al., 2020). More simply, capabilities required to perform undesired tasks may overlap with those required to perform desired tasks. For example, biological knowledge entails much of the knowledge required to construct biological weapons. For this reason, filtering or training against bioweapon-specific data might not prevent a network from learning enough to create bioweapons from general biology sources.\nLimitations and future work. (a) Gradient routing's performance is sensitive to its many hyperparameters: what data to route on, what regions to localize to, and what mask weights to use. This makes it hard to balance retain set performance vs. unlearning, for example. We suspect that methodological improvements will reduce this sensitivity. (b) So far, we have studied gradient routing as a pretraining method, making it costly to experiment with large models. (c) In our experiments with language models, we route gradients on a token-by-token basis, ignoring neighboring tokens. This naive strategy is surprisingly effective. However, it is plausible that contextual information will be critical in some problems, necessitating routing strategies that depend on entire sequences. Finding practical ways of choosing what data to route in order to localize broad capabilities is an intriguing open problem. (d) Our empirical results for scalable oversight pertain to a simplistic, narrow setting. Furthermore, our method for scalable oversight requires that the ablated policy produce coherent behavior. This does not hold in general, so scaling oversight via localization may require new ideas. (e) Other methods could be used to achieve similar aims as gradient routing, for example, DEMix Layers (Gururangan et al., 2021) or Interchange Intervention Training (Geiger et al., 2022a). (f) We elaborate on application-specific limitations in appendix A."}, {"title": "CONCLUSION", "content": "Gradient routing localizes targeted capabilities in neural networks, creating models with known internal structure. Even when based on simple and limited data labeling schemes, this localization is suitable for robust unlearning of pre-specified capabilities and scalable oversight. Consequently, gradient routing may facilitate the safe deployment of AI systems, particularly in high-stakes scenarios where black-box methods are insufficiently robust."}, {"title": "APPENDIX TO GRADIENT ROUTING: MASKING GRADIENTS TO LOCALIZE COMPUTATION IN NEURAL NETWORKS", "content": "EXTENDED DISCUSSION OF APPLICATION-SPECIFIC LIMITATIONS AND FUTURE WORK\nMNIST autoencoders. The cleanly separated MNIST autoencoder representations depicted in fig. 2c depend on the problem setup (e.g. the choice to not use data augmentation, like rotations) and use of heavy L1 regularization on the encoding vector. L1 regularization is required because, by default, a regular MLP autoencoder trained on MNIST digits retains information necessary to decode other digits.\nFor a wide set of hyperparameters, we find that gradient routing achieves quantitative representation splitting: the Certicate's reconstruction of digits 0-4 has higher average loss than its reconstructions of digits 5-9 for a wide range of settings, including different partitions of the digits. However, outside the specific hyperparameters chosen for the results in the main body of the paper, the qualitative results are poorer: the visual difference in reconstruction quality between the different digit subsets is less stark than in fig. 2c. We take this to highlight the problem-dependent characteristics of feature localization. In the case of autoencoding handwritten digits, separation of features for encoding different digits is \u201cunnatural,\u201d so achieving it requires a specific setup and heavy regularization.\nLanguage models. We speculate that gradient routing on particular tokens introduces an \u201cinternal tug of war\" between the expanded and original dimensions of the model (these dimensions depicted in fig. 3), where parameter updates in the original dimensions consistently decrease the logits for routed tokens and parameter updates in the expanded dimensions increase logits for routed tokens. This effect can be understood as a consequence of the mismatch between the implicit estimands (learning targets) for the original and expanded dimensions. We were concerned that this effect, rather than localization of capabilities, explained the post-ablation increase in forget loss. However, preliminary measurements suggest that this is not the case. For example, we find that the loss of ERA models is higher on average on non-routed forget tokens than a pure model, whereas it is lower on average on routed tokens. In general, the learning dynamics of gradient routing remain an open question.\nIf routing one token to a dimension of the residual stream creates an interpretable, axis-aligned feature as discussed in section 4.2.1, then routing many tokens to many neurons could produce a neural network with transparent internal representations. These representations might be made up of \"individual neurons... [that] corresponded to cleanly interpretable features of the input,\" as imagined in Elhage et al. (2022), or they could be organized in different ways. In principle, gradient routing provides a straightforward means of achieving this. However, we suspect that naive attempts to localize large numbers of concepts to unique regions will lead to high training loss.\nScalable oversight. Our reinforcement learning results demonstrate the promise of a localization-based strategy for scalable oversight, but further empirical and conceptual work is needed. The toy environment we use is simple, lacking the complexity and asymmetries of real-world problems. Additionally, our proposed solution relies on the fact that ablating an otherwise-active module of a policy network produces a policy with coherent behavior, which may not be true in practice (and isn't true in general, in principle). We discuss these considerations in appendix G."}, {"title": "MNIST AUTOENCODER DETAILS AND ABLATIONS", "content": "Model architecture. The Encoder, Decoder, and Certificate are all three-layer MLPs. The layer sizes for the Encoder produce data with shapes $(28 \\times 28, 2048, 512, 32)$ and for the decoder, data with shapes $(32, 512, 2048, 28 \\times 28)$. All hidden layers use ReLU activations. The final layer of the Encoder is linear. The final layer of the decoders is affine.\nTraining. The model was trained for 200 epochs on the 60,000 image training part of the MNIST dataset (LeCun et al., 1998) with batch size 2048. Images were normalized to have mean and standard deviation 0.5. No data augmentation was used. Optimization was performed with Adam (Kingma, 2014) with learning rate 1e-3, $\\beta = (0.9, 0.999)$, and weight decay 5e-5."}, {"title": "TINYSTORIES UNLEARNING DETAILS", "content": "Model architecture. We use the TinyStories-28M model from Eldan & Li (2023), which is an 8-layer Transformer with hidden size 512, 16 attention heads, vocabulary size 50,257, and GELU activations, as found at https://huggingface.co/roneneldan/TinyStories-28M/tree/main.\nTraining. The model was trained for one epoch on 400,000 stories from the Delphi version of the TinyStories dataset (Janiak et al., 2024; Eldan & Li, 2023), with batch size 80, truncating sequences at 256 tokens. The number of times each model was trained (the sample size for confidence intervals) is 60. The Adam optimizer was used with learning rate 5e-4 decaying to 5e-5 over the course of training, $\\beta = (0.9, 0.999)$, and weight decay 0.1. The forget set was defined as any story containing one of the following strings, separated by spaces or punctuation: \u201ctree\u201d, \u201ctrees\", \"forest\u201d, \u201cforests\u201d, \"woodland\", and \"woodlands\".\nUnlearning baseline. Our results in fig. 4a compare against Representation Misdirection for Unlearning (RMU) (Li et al., 2024), a post-hoc unlearning method. RMU works by corrupting a model's internal representations on forget data and preserving its representations on retain data. As is typical of many unlearning methods, much of the degradation to forget set performance caused by RMU is reversible by fine-tuning on a very small number of forget set examples (Sheshadri et al., 2024). The choice to compare against RMU in particular was arbitrary.\nExpand, Route, Ablate settings. The following settings are used for the training process described in section 4.2.2 and depicted in fig. 3.\nTarget layers: \\{0, 1, 2, 3, 4\\}.\nDimensions added: 64 MLP neurons in each of the target layers.\nThe mask weight for routed forget tokens in the original dimensions of target layers is set to -0.75. All other weights are 1.\nInstead of using a binary mask for a small set of tokens, we define a mask weight for each token as a convex combination of two masks: one that lets gradients flow everywhere (1's everywhere), and one as described in the previous bullet point. The weight in the convex combination is set by the token's relative frequency in the forget vs. retain set, biased towards retain. So the token \u201cthe\u201d, which has high frequency in both sets, is assigned the mask of 1s. The token \u201c_tree\u201d, which only appeares in the forget set, is given the most \"aggressive\u201d mask as defined in the previous bullet. Sample values are shown in table 3."}, {"title": "STEERING SCALAR DETAILS", "content": "Model architecture. We use a modified nanoGPT (Karpathy, 2024) model with the GPT-2 tokenizer, 20 layers, 16 attention heads, RoPE positional embedding (Su et al., 2023), and RMSNorm (Zhang & Sennrich, 2019).\nTraining. We train on sequences of length 1024 with 589, 824 tokens per step for 10,000 steps. We use the AdamW optimizer (Loshchilov & Hutter, 2018) with a learning rate warmup of 2, 000 steps to $1.8 \\times 10^{-3}$ with cosine decay to $1.8 \\times 10^{-4}$ after 10,000 steps, $\\beta\u2081 = 0.9$, $\\beta2 = 0.95$, 0.1 weight decay, and gradient clipping at 1.0.\nThe tokens most similar to the localized dimension. The unembed matrix of a Transformer $U \\in \\mathbb{R}^{d_{vocab} \\times d_{model}}$ maps the output of the final hidden layer to logits for the token vocabulary. To find the tokens with the highest cosine similarity to the localized \u201cCalifornia dimension\u201d (the 0th standard basis vector), we sort them according to $U_{i,0}/||U_i||_2$ and take the most negative values. This results in the following 300 tokens, in descending order of cosine similarity."}]}