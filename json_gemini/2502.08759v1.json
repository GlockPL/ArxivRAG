{"title": "Contextual bandits with entropy-based human feedback", "authors": ["Raihan Seraj", "Lili Meng", "Tristan Sylvain"], "abstract": "In recent years, preference-based human feedback mechanisms have become essential for enhancing model performance across diverse applications, including conversational AI systems such as Chat- GPT. However, existing approaches often neglect critical aspects, such as model uncertainty and the variability in feedback quality. To address these challenges, we introduce an entropy-based human feedback framework for contextual bandits, which dynamically balances exploration and exploitation by soliciting expert feedback only when model entropy exceeds a predefined threshold. Our method is model-agnostic and can be seamlessly integrated with any contextual bandit agent employing stochastic policies. Through comprehensive experiments, we show that our approach achieves significant performance improvements while requiring minimal human feedback, even under conditions of suboptimal feedback quality. This work not only presents a novel strategy for feedback solicitation but also highlights the robustness and efficacy of incorporating human guidance into machine learning systems. Our code is publicly available: https: //github.com/BorealisAI/CBHF", "sections": [{"title": "1. Introduction", "content": "Contextual bandits (CB) have become a fundamental frame- work for personalized decision-making across diverse do- mains including recommendation systems (Li et al., 2010; Bouneffouf et al., 2020), healthcare (Yu et al., 2024), and finance (Zhu et al., 2021). While traditional CB approaches leverage contextual information to optimize actions, their heavy reliance on implicit feedback signals like clicks intro- duces inherent limitations due to the biased and incomplete nature of such data (Qi et al., 2018).\nThis work investigates how explicit human feedback can en- hance CB performance. Building on successful integrations of human guidance in reinforcement learning (Christiano et al., 2017; MacGlashan et al., 2017) and conversational AI (Achiam et al., 2023), we distinguish two primary feed- back paradigms: (1) action-based feedback, where experts directly prescribe optimal actions for specific contexts (Osa et al., 2018; Li et al., 2023), and (2) preference-based feed- back, where humans compare pairs of learner-generated ac- tions to express relative preferences (Christiano et al., 2017; Saha et al., 2023). While action-based methods require pre- cise expert knowledge, we focus on preference feedback for its practical advantages in scalable data collection, notably its reduced cognitive load on human evaluators. However, this operational simplicity introduces two critical challenges: (1) variable human feedback quality, and (2) model uncer- tainty propagation. Our central research question emerges: How can we effectively incorporate preference-based hu- man feedback into contextual bandits while addressing these fundamental challenges?\nTo address this, we propose an entropy-based feedback mechanism that selectively queries an oracle when the agent's policy exhibits high uncertainty. By dynamically adjusting feedback requests based on entropy, our approach effectively balances exploration and exploitation, reducing unnecessary queries while ensuring informative guidance when needed. This selective feedback mechanism allows the agent to refine its policy more efficiently, leading to tighter regret bounds and improved performance. Further- more, we introduce two complementary feedback integra- tion strategies: Action Recommendation (AR), where ex- perts suggest context-specific actions, and Reward Manip- ulation (RM), where penalties adjust the reward signal for non-recommended actions. By optimizing the timing of hu- man input through adaptive entropy-based solicitation, our framework enhances learning efficiency while mitigating the impact of imperfect human feedback.\nOur key contributions include:\n\u2022 A unified framework for human-CB collaboration with theoretical analysis comparing AR and RM strategies.\n\u2022 An entropy-based solicitation criterion that improves learning efficiency while providing insights into"}, {"title": "2. Related works", "content": "Contextual bandits Contextual bandits have diverse ap- plications in recommendation systems (Li et al., 2010; Xu et al., 2020), healthcare (Yu et al., 2024), finance (Zhu et al., 2021), and other fields (Bouneffouf et al., 2020). CBs are a variant of the multi-armed bandit problem where each round is influenced by a specific context, and rewards vary accord- ingly. This adaptability makes CBs valuable for enhancing various machine learning methods, including supervised learning (Sui & Yu, 2020), unsupervised learning (Sublime & Lefebvre, 2018), active learning (Bouneffouf et al., 2014), and reinforcement learning (Intayoad et al., 2020).\nTo tackle CB challenges, several algorithms have been devel- oped, such as LINUCB (Li et al., 2010), Neural Bandit (Alle- siardo et al., 2014), and Thompson sampling (Agrawal & Goyal, 2013). These typically assume a linear dependency between the expected reward and its context. Despite these advancements, CBs often rely on implicit feedback, like user clicks, leading to biased and incomplete evaluations of user preferences (Qi et al., 2018). This reliance complicates accurately gauging user responses and tailoring the learning process.\nHuman feedback in the loop Recent advancements in human-in-the-loop methodologies have shown significant successes in real-life applications, such as ChatGPT via reinforcement learning with human feedback (RLHF) (Mac- Glashan et al., 2017), as well as in robotics (Argall et al., 2009) and health informatics (Holzinger, 2016).\nPreference-based feedback can be categorized into three groups: i) action-based preferences (F\u00fcrnkranz et al., 2012), where experts rank actions, ii) state preferences (Wirth & F\u00fcrnkranz, 2014), and iii) trajectory preferences (Busa- Fekete et al., 2014; Novoseller et al., 2020). Action-based feedback from humans is explored in (Mandel et al., 2017), where experts add actions to a reinforcement learning agent to boost performance. Other forms of explicit human feed- back include reward shaping (Xiao et al., 2020; B\u0131y\u0131k et al., 2022; Ibarz et al., 2018; Arakawa et al., 2018). These ap- proaches however do not account for acquiring feedback\nbased on the learner's uncertainty or the impact of varying levels of feedback on performance.\nContextual bandits with human feedback Human-in-the- Loop Reinforcement Learning addresses the bias problem of implicit feedback in contextual bandits. The exploration of learning in multi-armed bandits with human feedback is discussed in (Tang & Ho, 2019), where a human expert provides biased reports based on observed rewards. The learner's goal is to select arms sequentially using this biased feedback to maximize rewards, without direct access to the actual rewards.\nPreference-based feedback in contextual and dueling bandit frameworks has been explored in previous studies (Sekhari et al., 2023; Dud\u00edk et al., 2015; Saha, 2021; Wu et al., 2023). The learner presents candidate actions and receives noisy preferences from a human expert, focusing on minimizing regret and active queries. In contrast, we consider a setup where the learner receives direct feedback from human ex- perts and show how the fraction of active queries varies with different sets of experts.\nActive learning in contexual bandits Active learning (Ju- dah et al., 2014) enhances performance by selectively query- ing the most informative data points for labeling, rather than passively receiving labels for randomly or sequentially presented data. In the context of bandit algorithms, active learning has been employed to optimize the exploration- exploitation trade-off by guiding the algorithm to request feedback or labels when it is most uncertain about an ac- tion's outcome (Taylor & Stone, 2009). For example, (Boun- effouf et al., 2014) integrated active learning with Thomp- son sampling and UCB algorithms in contextual bandits, resulting in improved sample efficiency.\nWhile active learning traditionally assumes access to a small set of labeled examples alongside abundant unlabeled data, the contextual bandit setting differs fundamentally in that all feedback is inherently partial we only observe out- comes for the chosen action. This structural divergence precludes direct application of standard active learning tech- niques. Nevertheless, we draw methodological inspiration from active learning's core philosophy of strategic informa- tion acquisition.\nIn our work, we build on this idea by combining active learn- ing principles with human feedback, utilizing an entropy- based mechanism to query feedback when necessary. By incorporating these selective querying strategies into our contextual bandit framework, we aim to more effectively balance exploration and exploitation, particularly in scenar- ios where human feedback is noisy or costly. This approach not only improves sample efficiency but also helps mitigate the challenges posed by varying feedback quality."}, {"title": "3. Method", "content": "The following section provides a description of our method and its subcomponents. A comprehensive representation of the approach is shown in Figure 1. Algorithm 1 describes our method.\nWe consider an online stochastic contextual bandit frame- work where at each round t, the world generates a context- reward pair $(s_t, r_t)$ sampled independently from a stationary unknown distribution D. Here $S_t \\in S = \\mathbb{R}^m$ is an m di- mensional real valued vector and $r_t = (r_t(1), ...,r_t(k)) \\in$ {0, 1}$^k$ is a k-dimensional vector where each element can take values 0 or 1. The agent then chooses an action $a_t \\in$ {1, ..., k} according to a policy $\\pi: S \\rightarrow$ {1, ...,k} and the environment reveals the reward $r_t(a_t) \\in$ {0, 1}.\nThe objective of the agent is to find a policy $\\pi \\in \\Pi$ that will minimize the cumulative expected regret given by\n$\\mathbb{E} \\Big[ \\sum_{t=1}^{T} \\big[ r_t(\\pi^*(s_t) - r_t(a_t) \\big] \\Big],$ (1)\nwhere \u03c0* denotes the optimal strategy of selecting actions given context st. The problem setup described above bears a strong resemblance to a multi-label or multiclass classi- fication problem, where rt(at) = 1 indicates the correct label choice and 0 otherwise. However, a key distinction lies in the learner's lack of access to the correct label or label set for each observation. Instead, the learner only discerns whether the chosen label for an observation is correct or incorrect.\nIn contextual bandits, feedbacks are provided in the form of a predetermined reward signal provided by the designer. These reward signals are not well defined for complex decision-making problems (Blanchard et al., 2023; Drag- one et al., 2019), and are often learned from data. An alternative to learning a reward function from data is to obtain preference-based feedback from humans and learn the underlying reward function that the human expert op- timizes (Sekhari et al., 2024). In this work, we consider the setup in which the human expert has sufficient exper- tise and valuable insights stemming from their experience and domain knowledge to provide direct feedback to the learner. These feedbacks can directly impact the actions that a contextual bandit learner takes or the rewards it receives. However, the quality of such explicit feedback may vary depending on the expertise levels of different individuals. We provide two ways in which human experts can provide feedback to the contextual bandit learner: i) Action Rec- ommendation through direct supervision (AR) ii) Reward\nManipulation (RM). In certain applications, a human expert can directly control the actions that the agent takes; in these cases, feedback in the form of action recommendations (AR) is useful. Conversely, in other applications where the human expert cannot directly influence the agent's actions, feed- back through reward manipulation is more beneficial. We describe each of these different feedbacks below."}, {"title": "3.2.1. ACTION RECOMMENDATION VIA DIRECT SUPERVISION", "content": "In this form of feedback, the human expert explicitly in- structs the actions to take for a given context. We assume that the algorithm always accepts the recommended action. Let \u00e2t be a set of actions recommended by the human expert EAR for a given context st and expert quality qt, where qt \u2208 [0, 1], we elaborate more on the expert quality in Sec- tion 3.4. When the expert recommends a set of actions, the learning algorithm randomly chooses an action from the rec- ommended set. The final reward rf received by the learner is given by:\n$\\hat{a}_t = E_{AR} (s_t, q_t)$ (2)\n$a_t \\sim Uniform(\\hat{a}_t)$ (3)\n$r^f = r_t(a_t)$ (4)"}, {"title": "3.2.2. REWARD MANIPULATION", "content": "In this form of feedback, the human expert $E_{RM}^{(s,q)}$ gives an additional reward penalty when the learner chooses an ac- tion not recommended by the expert. Let rp be the fixed reward penalty for unrecommended actions. Let at be the action chosen by the learner in round t, and at be the set of recommended actions of the expert. The final reward rf received by the learner is given by:\n$r_p = E_{RM}(s_t, q_t)$\n(5)\n$r^f = \\begin{cases} r_t(a_t) + r_p & \\text{if } a_t \\notin \\hat{a}_t \\\\ r_t(a_t) & \\text{otherwise} \\end{cases}$ (6)"}, {"title": "3.3. When to seek human feedback?", "content": "An important question that naturally arises when integrat- ing human feedback into the contextual bandit algorithm is when the algorithm will actively seek out such feedback. In the contextual duelling bandit setup in (Di et al., 2024), the algorithm presents two options to the human and asks them to choose a preferred one based on a given context. In the case of model misspecification, where the underlying reward function assumed by the algorithm matches the true rewards generated by human preferences, the algorithm can actively query the human expert to obtain feedback on the predicted reward or ranking (Yang et al., 2023). In our work, we take a different approach in which the learner seeks expert feedback based on the uncertainty of the model. The model computes the entropy of the policy at each round t which quantifies the degree of unpredictability in the policy's deci- sion making process using the following expression\n$H(\\pi) = - \\sum_{a_t} \\pi(a_t | s_t) \\log(\\pi(a_t | s_t)),$ (7)\nwhere \u0397(\u03c0) denotes the entropy of policy \u03c0. The model then queries for human feedback when the model entropy exceeds a predefined threshold \u5165. Appropriate choice of A will depend on the problem domain and are obtained using hyper parameter search. Our proposed entropy based approach for querying the expert depends on the learner's ability to compute an entropy for its policy. Thus for certain models when model uncertainty is not available, we can still obtain two forms of human feedback periodically, we also demonstrate the effect on model performance when these two types of human feedback are incorporated for different periods."}, {"title": "3.4. Quality of experts", "content": "We consider the effect of learner's performance based on different quality of expert feedback received. We define the quality of feedback in this case as the accuracy of the expert in providing correct recommendation. We first show how the performance of the contextual bandit learner measured by the expected cumulative regret varies for different expert levels of accuracy. Let qt \u2208 [0, 1] be the probability of providing correct recommendation associated with a partic- ular level of expert. During training, the algorithm seeks expert feedback described in Section 3.2.1 and 3.2.2 when \u0397(\u03c0) \u2265 \u03bb. For action recommendation via direct supervi- sion, the expert provides the correct action with probability qt and provides a randomized action with probability 1\nFor reward manipulation feedback, the expert wrongly pe- nalizes the learner with a probability of 1\nQt.\nIt."}, {"title": "3.5. Regret Analysis for Contextual Bandits with Entropy-Based Human Feedback", "content": "We analyze the regret bound for our proposed algorithm, which integrates entropy-based human feedback in a con- textual bandit setting. The goal is to quantify the impact of selective oracle feedback on cumulative regret and derive a regret bound that captures the trade-offs.\nAt each round t, the agent observes context st and selects an action at \u2208 A using policy \u03c0\u03c4. Oracle feedback is re- quested if the entropy \u0397(\u03c0\u03b9) exceeds a threshold A, and the observed reward rt(at) combines environment and feedback contributions.\nThe regret at time t is:\n$Regret_t = \\mathbb{E}[r_t(a_t^*) \u2013 r_t(a_t)],$ (8)\nwhere $a_t^* = \\pi^*(s_t)$ is the optimal action. The total regret over T rounds is:\n$Regret(T) = \\sum_{t=1}^{T} Regret_t.$ (9)\nLet $p = P(H(\\pi_t) > \\lambda)$ denote the probability of requesting feedback, and let qt be the accuracy of oracle feedback. The regret bound is:\n$\\mathbb{E}[Regret(T)] \\leq O \\Big( \\sqrt{(1-p)T|A|log T} \\Big)$\n(10)\n$+ O \\Big(\\frac{p(1 -q + \\log T)}{1 + k \\lambda \\bar{t}} \\Big).$\nThe total regret decomposes into two parts:\n$Regret(T) = \\sum_{t\\notin F} Regret_t + \\sum_{t\\in F} Regret_t$ (11)\nwhere F represents rounds where feedback is requested $(H(\\pi_t) > \\lambda)$.\nFor rounds without feedback, the regret follows standard contextual bandit analysis:\n$\\mathbb{E}[Regret_{no-feedback}(T)] \\leq O \\Big( \\sqrt{(1-p)T|A| log T} \\Big) .$ (12)\nFor rounds with feedback, regret reduction depends on feed- back accuracy and its impact on decision quality:\n$\\mathbb{E}[Regret_{feedback}(T)] \\leq O \\Big(\\frac{p(1 -q + \\log T)}{1 + k \\lambda \\bar{t}} \\Big)$ (13)\nThis regret bound highlights the trade-off between explo- ration and feedback solicitation. Increasing p reduces the\nfirst term, leading to faster convergence, while higher feed- back accuracy q ensures minimal regret in feedback rounds. The entropy threshold A serves as a control parameter to balance feedback frequency and regret minimization. Com- pared to standard bandit approaches, entropy-driven feed- back solicitation provides a principled mechanism to reduce regret in uncertain environments, making it highly effective for practical deployment."}, {"title": "4. Experiments", "content": "In this sub-section, we present the environment settings, baselines, and experimental results. We also discuss the effect of entropy thresholds and expert accuracy on model performance.\nAlgorithms and Environments Considered. We conduct experiments across a range of environments and contextual bandit agents. The agents fall into two categories: (i) classic contextual bandit algorithms and (ii) policy-based reinforce- ment learning (RL) algorithms with a discount factor of 0, focusing on immediate rewards.\nClassic Contextual Bandit Algorithms. For the classic contextual bandit setup, we employ three key algorithms: 1. LinearUCB (Li et al., 2010): An extension of the tra- ditional Upper Confidence Bound (UCB) algorithm (Auer, 2002), where the expected reward for each action depends linearly on the context or features associated with that ac- tion. 2. Bootstrapped Thompson Sampling (Kaptein &\nEckles, 2014): This method replaces the posterior distribu- tion in standard Thompson Sampling with a bootstrapped distribution, enhancing robustness by resampling historical data instead of relying on a parametric model. 3. EE- NET (Ban et al., 2021): This approach utilizes two neural networks-one for exploration and one for exploitation-to learn a reward function and adaptively balance exploration with exploitation.\nPolicy-Based Reinforcement Learning Algorithms. For policy-based RL, we evaluate four algorithms, with the dis- count factor set to 0 to prioritize immediate rewards: Proxi- mal Policy Optimization (PPO) (Schulman et al., 2017), PPO with Long Short-Term Memory (PPO-LSTM), RE- INFORCE (Williams, 1992), Actor-Critic (Haarnoja et al., 2018).\nBaseline Comparison. We include the TAMER frame- work (Knox & Stone, 2009) as a baseline, which allows human trainers to provide real-time feedback to the agent, supplementing the predefined environmental reward signal. In our experiments, we simulate human feedback by reveal- ing the true labels during training."}, {"title": "4.2. Variation of model performance based on different expert quality", "content": "We first present the effect of different expert quality on the two types of feedback discussed in Sections 3.2.1 and 3.2.2. Note that we can compute the entropy of policy \u03c0 for the PPO, PPO-LSTM, Reinforce, Actor-Critic and LinearUCB and Bootstrapped Thompson sampling. We now present the results associated with different expert levels in for the four environments discussed in Section 4. Figure 4 shows the variation of different expert qualities for different range of learners. The bar plot in orange shows the model per- formance when reward manipulation is used as a feedback from the human expert and the bar plot in blue shows the model performance when action recommendation as a feed- back from human feedback. Notably, higher expert qual- ity does not universally translate to better performance-a counterintuitive relationship whose subtleties we analyze in subsequent sections. Our analysis shows that for different"}, {"title": "4.3. Incorporating entropy based feedback achieves higher performance compared to baselines", "content": "We optimize the model performance across various expert levels and compare these results with baseline models, in- cluding EE-Net. Figure 3 presents the mean cumulative regret for the optimized expert level (as obtained from Table 1), highlighting the significant performance gains achieved by incorporating entropy-based feedback over the baselines.\nOur analysis, conducted across all datasets, demonstrates that integrating entropy-based feedback-specifically Ac- tion Recommendation (AR) and Reward Modification (RM)-consistently outperforms both TAMER and EE-Net. Moreover, we observe that the proportion of steps during which the algorithm seeks human expert feedback varies across datasets. Importantly, the results reveal two key find- ings:\nFirstly, learners benefit substantially from entropy-based feedback compared to when no such feedback is provided. This improvement underscores the effectiveness of entropy thresholds in selectively involving human experts, thereby guiding the learning process. In fact, even with a modest number of queries to the human expert (less than 30% of the total training steps), entropy-based feedback drives superior performance over the baseline models. Secondly, the final performance of the learners is not necessarily a monotonous function of the quality of the human feedback, as shown in Figure 4.\nInterestingly, the performance of AR and RM varies be- tween datasets. For example, on the Bibtex dataset, AR performs worse compared to RM, while on the Delicious dataset, AR demonstrates the best performance among the three. This difference arises due to how penalties affect exploration: Bibtex, with fewer actions, benefits less from AR's action-space limitation, whereas Delicious, with many possible actions, sees AR accelerating convergence by nar- rowing down the action space early in the learning process. As a result, AR's advantage becomes more apparent in envi- ronments where an overwhelming number of actions could otherwise slow down the learner's progress."}, {"title": "4.4. Effect of entropy threshold and expert accuracy on model performance", "content": "Figure 5 (Appendix) presents bubble plots comparing model performance at different expert levels and entropy threshold values for both AR and RM feedback types. The size and color of each bubble represent the mean cumulative reward for the corresponding learner.\nWe begin by analyzing the results for AR feedback. Gen- erally, we observe that at higher entropy threshold values, the model's performance remains relatively stable across different expert levels. This behavior is expected, as higher entropy thresholds result in fewer queries to the human ex- pert, reducing the impact of expert quality on performance.\nHowever, at lower entropy thresholds, an interesting pattern emerges: increasing expert quality can actually lead to a decrease in model performance. This phenomenon relates to the exploration-exploitation trade-off. At high expert levels, the expert consistently provides accurate recommen- dations, and since the model is designed to always accept these recommendations in the AR setting, the result is pure exploitation. Conversely, at lower expert levels, where rec- ommendations are more random, the model is encouraged to\nexplore a broader set of actions, which can ultimately yield higher cumulative rewards or lower cumulative regrets.\nA similar pattern is observed with RM feedback. At higher entropy thresholds, the differences in performance between varying expert levels are minimal, as fewer queries are made to the expert. At lower entropy thresholds, however, we again see a decline in performance as expert quality in- creases."}, {"title": "4.5. Observed differences between feedback types", "content": "Figure 3 illustrates how the two forms of feedback, AR and RM, interact differently with the underlying algorithms and datasets. The choice of feedback type should therefore depend on the specific application.\nOur results generally indicate that at higher expert levels, AR tends to be more effective than RM. This is likely be- cause AR directly influences the actions taken by the con- textual bandit (CB), interfering less with its reward-based"}, {"title": "5. Conclusion", "content": "In conclusion, this work introduces an effective entropy- based framework for incorporating human feedback into contextual bandits. By utilizing model entropy to trigger feedback solicitation, we significantly reduce the reliance on continuous human intervention, thus making the system more efficient and scalable. Our experiments show that even with low-quality human feedback, substantial perfor- mance gains can be achieved, underscoring the potential of entropy-based feedback mechanisms in various real-world applications. By dynamically selecting feedback opportu- nities based on model uncertainty, our approach provides a generalizable strategy for optimizing human intervention\nin learning systems, improving efficiency while minimiz- ing unnecessary queries. Future work can explore further applications of entropy-based feedback in these domains, fostering more adaptive and intelligent human-in-the-loop learning frameworks."}, {"title": "6. Impact statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "E. Hyper parameters", "content": "We provide the hyperparameters for the policy based RL algorithms and the range of values of entropy thresholds that we consider for each dataset.\nE.1. Hyperparameters for policy based RL algorithms\nE.2. Range of entropy thresholds considered"}]}