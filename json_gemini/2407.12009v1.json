{"title": "Using Multimodal Foundation Models and Clustering for Improved Style Ambiguity Loss", "authors": ["James Baker"], "abstract": "Teaching text-to-image models to be creative involves using style ambiguity loss, which requires a pretrained classifier. In this work, we explore a new form of the style ambiguity training objective, used to approximate creativity, that does not require training a classifier or even a labeled dataset. We then train a diffusion model to maximize style ambiguity to imbue the diffusion model with creativity and find our new methods improve upon the traditional method, based on automated metrics for human judgment, while still maintaining creativity and novelty.", "sections": [{"title": "Introduction", "content": "With every new invention comes a new wave of possibilities. Humans have been making pictures since before recorded history, so its only natural that there would be interest in computational image generation. Artificially generating photographs that are indistinguishable from real ones has become so easy and effective that there is even concern over \"deepfakes\" being used for propaganda or illicit purposes (Pawelec, 2022). On the other hand, generating images that look like art is a slightly different problem. The exact mathematical properties of what constitutes \"quality\" art is not as easy to quantify as other tasks, like classification accuracy, prediction error or whether a question was answered correctly. While machines can very easily be trained to mimic a dataset, humans like to be surprised by novelty, without feeling like they are being exposed to total randomness. A breakthrough was the invention of the Creative Adversarial Network (Elgammal et al., 2017), which used a style ambiguity loss to train a network to generate images that could not be classified as belonging to a particular style. However, GANs have largely been superseded by diffusion models (Luo, 2022), due to their far better results. Additionally, the style ambiguity loss requires a pretrained classifier. Every set of styles or concepts requires training a classifier before even training a model to generate images. Furthermore, training a classifier requires that the dataset be labeled correctly, and manually labeling a dataset is often even more expensive and time-consuming than training a model. To circumvent these issues, we propose using a classifier that does not require any additional training and can be easily applied to any dataset, labeled or unlabeled. Our contributions are as follows:\n\u2022 We applied creative style ambiguity loss to diffusion models, which are easier to train and produce higher-quality images than GANs.\n\u2022 We developed versatile CLIP-based and K-Means-based creative style ambiguity losses that do not require training a separate GAN-based style classifier.\n\u2022 Empirically, we find our new creative style ambiguity loss can be used to tune a diffusion model to generate samples that are higher quality than the generated samples of a diffusion model trained with the pre-existing GAN-based style ambiguity loss"}, {"title": "Related Work", "content": ""}, {"title": "Creativity", "content": "Creativity has been hard to define and quantify. Creative work has been formulated as work having novelty, in that it differs from other similar objects, and also utility, in that it still performs a function (Cropley, 2006). For example, a Corinthian column has elaborate, interesting, unexpected adornments (novelty) but still holds up a building (utility). A distinction can also be made between \"P-creativity\", where the work is novel to the creator, and \"H-creativity\" where the work is novel to everyone (Boden, 1990). Computational techniques to be creative include using genetic algorithms (DiPaola & Gabora, 2008), reconstructing artifacts from novel collections of attributes (Iqbal et al., 2016), and most relevantly to this work, using Generative Adversarial Networks (Elgammal et al., 2017) with a style ambiguity loss."}, {"title": "Computational Art", "content": "One of the first algorithmic approaches dates back to the 1970s with the now primitive AARON (McCorduck, 1991), which was initially only capable of drawing black and white sketches. Generative Adversarial Networks (Goodfellow et al., 2014), or GANs, were some of the first models to be able to create complex, photorealistic images and seemed to have potential to be able to make art. Despite many problems with GANs, such as mode collapse and unstable training (Saxena & Cao, 2023), GANs and further improvements (Arjovsky et al., 2017; Karras et al., 2019; 2018) were state of the art until the introduction of diffusion Sohl-Dickstein et al. (2015). Diffusion models such as IMAGEN (Saharia et al., 2022) and DALLE-3 (Betker et al.) have attained widespread commercial success (and controversy) due to their widespread adoption."}, {"title": "Reinforcement Learning", "content": "Reinforcement learning (RL) is a method of training a model by having it take actions that generate a reward signal and change the environment, thus changing the impact and availability of future actions Qiang & Zhongli (2011). RL has been used for tasks as diverse as playing board games (Silver et al., 2017), protein design (Lutz et al., 2023), self-driving vehicles (Kiran et al., 2021) and quantitative finance (Sahu et al., 2023). Policy-gradient RL (Sutton et al., 1999) optimizes a policy that chooses which action to take at any given timestep, as opposed to value-based methods that may use a heuristic to determine the optimal choice. Examples of policy gradient methods include Soft Actor Critic (Haarnoja et al., 2018), Deep Deterministic Policy Gradient (Lillicrap et al., 2019) and Trust Region Policy Optimization (Schulman et al., 2017a)."}, {"title": "Method", "content": ""}, {"title": "Model", "content": ""}, {"title": "Creative Adversarial Network", "content": "A Generative Adversarial Network, or GAN (Goodfellow et al., 2014), consists of two models, a generator and a discriminator. The generator generates samples from noise, and the discriminator detects if the samples are drawn from the real data or generated. During training, the generator is trained to trick the discriminator into classifying generated images as real, and the discriminator is trained to classify images correctly. Given a generator G : \\mathbb{R}^{\\text{noise}} \\rightarrow \\mathbb{R}^{h\\times w \\times 3}, a discriminator D : \\mathbb{R}^{h\\times w \\times 3} \\rightarrow [0,1] real images x \\in \\mathbb{R}^{h\\times w \\times 3}, and noise Z\\in \\mathbb{R}^{\\text{noise}}, the objective is:\n\\min_{G} \\max_{D} E_{x} [\\log(D(x)] + E_{z}[\\log(1 \u2013 D(G(Z))]\nAt inference time, the generator is used to generate realistic samples. Elgammal et al. (2017) introduced the Creative Adversarial Network, or CAN, which was a DCGAN (Radford et al., 2016) where the discrim- inator was also trained to classify real samples, minimizing the style classification loss. Given N classes of image (such as ukiyo-e, baroque, impressionism, etc.), the classification modules of the Discriminator"}, {"title": "Diffusion", "content": "A diffusion model aims to learn to iteratively remove the noise from a corrupted sample to restore the original. Starting with xo, the forward process q iteratively adds Gaussian noise to produce the noised version x_{t}, using a noise schedule \u03b2\u2081... \u03b2r, which can be learned or manually set as a hyperparameter:\nq(x_{1:T}|x_{0}) = \\prod_{t=1}^{T} q(x_{t}|x_{t-1})\nq(x_{t}|x_{t-1}) = N(x_{t}; \\sqrt{1 \u2013 \u03b2_{t}}x_{t-1}, \u03b2_{t}I)\nMore importantly, we also want to model the reverse process p, that turns a noisy sample x back into xo, conditioned on some context c. As x_{T} is the fully noised version, p(x_{T}|c) = N(x_{T}; 0, I)\np_{\u03b8}(x_{0:T}|c) = p(x_{T}|c) \\prod_{t=1}^{T} p_{\u03b8}(x_{t-1}|x_{t}, c)\np_{\u03b8}(x_{t-1}|x_{t}, c) = N(x_{t-1}; \u03bc_{\u03b8}(x_{t}, t, c), \u03a3_{\u03b8} (x_{t}, t, c))\nWe train \u03a3_{\u03b8} and \u03bc_{\u03b8} via optimizing the variational lower bound of the negative likelihood of the data:\nE[-\\log p_{\u03b8}(x_{0})] \u2264 E[-\\log \\frac{p_{\u03b8} (x_{0:T}/C)}{q(x_{1:T}|x_{0})}] = L\nAs shown by Ho et al. (2020), this is equivalent to estimating the noise at each step using a model e_{\u03b8}. So the loss to be optimized is:\nL = E_{x,\u03f5\u223cN(0,1),t}||\u03f5 \u2013 \u03f5_{\u03b8}(x_{t}, t)||^{2}\nOnce the model has been trained, the reverse process, aka inference, to generate a sample from noise x_{T} ~ N(0,1) can be done iteratively by finding x_{t \u2212 1} given x_{t}, \\bar{a_{t}} = 1 \u2212 \u03b2_{t}, \\bar{a_{T}} = \\prod_{i=1}^{t} \\bar{a_{s}}, Z ~ N(0,1) and \u03c3_{t}^{2} = \u03b2_{t} or \u03c3_{t}^{2} = \\frac{1-\\bar{a_{t-1}}}{1-\\bar{a_{t}}}\u03b2_{t}:\nx_{t-1} = \\frac{1}{\\sqrt{a_{t}}} (x_{t} - \\frac{1-\\bar{a_{t}}}{\\sqrt{1 - \\bar{a_{t}}}}\u03f5_{\u03b8}(x_{t},t)) + \u03c3_{t}Z\nAcording to Ho et al. (2020), both versions of \u03c3_{t} had similar results. In our case, we used \u03c3^{2} = \\frac{1-\\bar{a_{t-1}}}{1-\\bar{a_{t}}} \u03b2_{t}."}, {"title": "Markov Decision Processes", "content": "A Markov Decision Process (Bellman, 1957) is defined as a tuple (S, A, po, P, R) that models the actions of an agent in some environment with discrete time-steps.\nS is the state space, the set of states the environment can be in.\nA is the set of actions that the agent can take.\npo is the initial distributions of states s\u2208 S when t 0.\nPa(s, s') is the probability of transitioning from state s at timet to s' att + 1 when the agent has taken action a \u2208 A.\nThe reward function R(st, at) returns a reward a time t given the action at the agent takes and the state of the environment St.\nThe agents actions are determined by the policy \u03c0(as) that maps actions to states. The series of state- action pairs for each timestep is called a trajectory T = (80,a0....ST,\u0430\u0442). Using policy-gradient as opposed to value-based RL, we train by maximizing the reward R over the trajectories sampled from the policy:\nJ_{RL}(\u03c0) = E_{\u03c4\u223cp(\u03c4;\u03c0)} [\\sum_{t=0}^{T}R(S_{t}, a_{t})]"}, {"title": "Denoising Diffusion Proximal Optimisation", "content": "Introduced by Black et al. (2023), Denoising Diffusion Proximal Optimisation, or DDPO, represents the Diffusion Process as a Markov Decision Process. A similar method was also pursued by Fan et al. (2023).\na_{t}\nX_{t-1}\nS_{t}(c,t, x_{t})\n\u03c0(a_{t}|S_{t})\np_{\u03b8}(x_{t-1}|x_{t}, c)\np_{\u03b8}(s_{0}) (p(c), \u03c3_{T}, N (0, 1))\nP(S_{t+1}) = (\u03b4_{t}, \u03b4_{T-1}...,\u03b4_{0})\nR(s_{t}, a_{t})r(x_{0}, c)\nJ_{RL}(\u03c0)= J_{DDRL}(\u03b8) = E_{c\u223cp(c),x_{0}\u223cp_{\u03b8}(x_{0}|c)} [r(x_{0}, c)]\nReinforcement learning training was then applied to a pretrained diffusion model, which in our case was Stable Diffusion 2 (Rombach et al., 2022). Following Schulman et al. (2017b), Black et al. (2023) also implemented clipping to protect the policy gradient VeJ_{DDRL} from excessively large up- dates. We largely follow their method but use a different reward function. We fine-tune off of the pre-existing stabilityai/stable-diffusion-2-base checkpoint (Rombach et al., 2022) downloaded from https://huggingface.co/stabilityai/stable-diffusion-2-base."}, {"title": "Reward Function", "content": "In the original paper, the authors used four different reward functions for four different tasks. For example, they used a scorer trained on the LAION dataset (Schuhmann & Beaumont, 2022) as the reward function to improve the aesthetic quality of generated outputs. In this paper, we use the reward model based on Elgammal et al. (2017), where the model is rewarded for stylistic ambiguity. Given a generated image xo \u2208 \\mathbb{R}^{h\\times w \\times 3} and a classifier C : \\mathbb{R}^{h\\times w \\times 3} \\rightarrow \\mathbb{R}^{N} we want to maximize:\nR(x_{0}) = -CE(C(x_{0}), U)\nwhere CE is the cross entropy."}, {"title": "Data", "content": "Starting with the WikiArt dataset (Saleh & Elgammal, 2015), we used 1000 images from each class, oversam- pling when necessary, to balance the distributions between classes, to train the CAN. To train the diffusion model, we prompted the model by concatenating a randomly selected medium prompt from (painting of , picture of, drawing of ) to a randomly selected subject prompt (a man, a woman, a landscape, nature, a building, an animal, shapes, an object). An example prompt would be picture of an animal. With 10% probability we would set the prompt to the null string in order to train the model unconditionally as well."}, {"title": "Choice of Classifier", "content": "Style ambiguity loss relies on some classifier C. We are exploring four versions of this classifier."}, {"title": "DCGAN-Based Classifier", "content": "We can use the classification module of the discriminator as the classifier in the reward function, setting C = Dc. In the case of the CAN, DC is trained jointly along with the generator. In the case of DDPO, we use a pretrained Dc from the CAN discriminator (which we call Diffusion DCGAN Based)."}, {"title": "CLIP-Based Classifier", "content": "Given text \u2208 \\mathbb{R}^{text} and an image \u2208 \\mathbb{R}^{h\\times w \\times 3},we can use a pretrained CLIP (Radford et al., 2021) model, that can return a similarity score for each image-text pair: CLIP : \\mathbb{R}^{text} \u00d7 \\mathbb{R}^{h\\times w \\times 3} \u2192 \\mathbb{R}. CLIP is a multimodal foundation model trained using contrastive learning (Jaiswal et al., 2021) on a dataset of approximately 400 million text-image pairs. For each generated image xo, for each class name si,1 \u2264 i \u2264 Ns, we find CLIP(Si, xo). We can then create a vector (CLIP(81,x0), CLIP(82, x0),,,CLIP(SN_{S},x0)) and then use softmax to normalize the vector and define the result as CCLIP(x0). Formally:\nCCLIP(x_{0}) = softmax((CLIP(s_{1},x_{0}), CLIP(s_{2},X_{0}),,,CLIP(s_{N_{S}},x_{0}))\nThen we set C = CCLIP. We discard the results of Dc when using a CLIP-Based Classifier with CAN. We used the 27 style classes in the WikiArt dataset (Saleh & Elgammal, 2015) as si,1 < i < Ns. A list of said classes can be found in Appendix A. We used the clip-vit-large-patch14 CLIP checkpoint downloaded from https://huggingface.co/openai/clip-vit-large-patch14."}, {"title": "K-Means Text and Image Based Classifiers", "content": "Alternatively, when we have Ns text labels or N\u2081 source images, we can embed the labels or images into the CLIP embedding space \u2208 \\mathbb{R}^{768} and perform k-means clustering to generate k centers. Given a CLIP Embedder E: \\mathbb{R}^{h\\times w \\times 3} \u2192 \\mathbb{R}^{768} mapping images to embeddings, and the k centers C1, C2,,,ck we can create a vector ([||E(xo)\u2212c1||, ||E(xo)\u2212c2||,,,, ||E(xo)\u2212ck|| and then use softmax to normalize the vector and define the result as CKMEANS. Formally:\nC_{KMEANS}(x_{0}) = softmax( \\frac{1}{||E(x_{0}) \u2013 c_{1}||}, \\frac{1}{||E(x_{0}) \u2013 c_{2}||},..., \\frac{1}{||E(x_{0}) - c_{k}||})\nThen we set C = CKMEANS. We used two sets of centers: one set from performing k-means clustering on the WikiArt images, which we called K Means Image Based, and one set from clustering the names of the 27 style classes, which we call K Means Text Based."}, {"title": "Results", "content": "We generated all images with width and height = 512. The authors used width and height = 256 in the original CAN paper. However, given that larger, more detailed images are preferred by most people, we thought it more relevant to focus on larger images. Refer to appendix C for results on smaller images and examples. Table 1 shows a few DDPO images with the prompts used to generate them. Appendix B shows more examples generated using different prompts."}, {"title": "Quantitative Evaluation", "content": "We generated 100 images using the same prompts the models were trained on for each model. We used three quantitative metrics to score the models\n\u2022 AVA Score: Consisting of CLIP+Multi-Layer Perceptron (Haykin, 2000), the AVA model was trained on the AVA dataset (Murray et al., 2016) of images and average rankings by human subjects, in order to learn to approximate human preferences given an image. We used the CLIP model weights from the clip-vit-large-patch14 checkpoint and the Multi-Layer Perceptron weights downloaded from https://huggingface.co/trl-lib/ddpo-aesthetic-predictor.\n\u2022 Image Reward: The image reward model (Xu et al., 2023) was trained to score images given their text description based on a dataset of images and human rankings. We used the image-reward python library found at https://github.com/THUDM/ImageReward/tree/main.\n\u2022 Prompt Similarity: Given the CLIP model's ability to embed images and text into the same space, we can measure the similarity between an image and its source prompt by finding the cosine similarity between the two CLIP embeddings. We used the clip-vit-large-patch14 checkpoint.\nResults of our experiments are shown in table 2. The best scores are bolded. There was little variance in prompt similarity. However, both K-Means-based approaches improved upon the DCGAN-based approach in terms of the two metrics for human preferences, showing that our method improves upon the past work aesthetically while also circumventing the costly training time of using a CAN or needing a labeled dataset for training the style classifier component of the CAN."}, {"title": "Comparison with Baseline", "content": "It is worth contrasting our trained DDPO model with the default pretrained stabilityai/stable-diffusion- 2-base checkpoint diffusion model we are fine-tuning (Rombach et al., 2022). This allows us to better visualize the difference the DDPO training with style ambiguity loss makes. We assumed that the DDPO images might be similar to the baseline model images generated with fewer steps, so we compared DDPO Images to the baseline using 30,15, and 10 inference steps, as seen in figure 3.\nIn order to quantitatively compare our models to the baselines, we used the embedding of the [CLS] token from a vision transformer loaded from the dino-vits16 checkpoint (Caron et al., 2021) from https:// huggingface.co/facebook/dino-vits16, as that encodes stylistic information (Tumanyan et al., 2022; Kwon & Ye, 2023). We averaged the cosine similarity between style embeddings of each pair of images (\u0445, \u0423\u0437\u043e, \u042315, \u042310), where x was generated by the tuned model and Y30, Y15, Y10 was generated by the baseline model using 30,15 and 10 inference steps respectively, using the same prompt and initial random seed. We"}, {"title": "Conclusion", "content": "Training models with stylistic ambiguity loss teaches them to be creative. This work introduces new forms of stylistic ambiguity loss that do not require training a classifier or GAN, which can be time-consuming and unstable (Saxena & Cao, 2023). These new methods, particularly the K-Means-based approaches, scored higher than the traditional method on quantitative metrics of human judgement. Nonetheless, there are still more directions for this to go. Both the CLIP-based and K-Means Text-based style ambiguity losses require users to heuristically choose a set of styles to \"deviate\" from. In this work, we only used the 27 categories in the WikiArt dataset to be comparable to the original CAN paper. However, users may instead prefer a different set of styles or words, which may produce better or more interesting results. Additionally, the K-Means Image-based style ambiguity loss does not require a multimodal model like CLIP. We could have used any pretrained model to embed images into a lower-dimensional manifold, or trained a new one. Ergo, the K-means technique could be used for any medium, such as music (Elgammal, 2022; Zhang et al., 2023), new proteins (Winnifrith et al., 2023), stories (Mori et al., 2022) and videos (Cho et al., 2024)."}, {"title": "Broader Impact Statement", "content": "Many are concerned about the impacts of generative AI. By making art, this work infringes upon a domain once exclusive to humans. Companies have faced scrutiny for possibly using AI (Gutierrez, 2024), and many creatives, such as screenwriters and actors, have voiced concerns about whether their jobs are safe (del Barco, 2023). Nonetheless, using AI can help humans by making them more efficient, providing inspiration, and generating ideas (Fortino, 2023; Campitiello, 2023; Darling, 2022). It's also not certain how copyright protection will function for AI-generated art (Watiktinnakorn et al., 2023), given copyright law is based on the premise that creative works originate solely from human authorship. Clear, consistent policies, both at the government level and by industry and/or academic groups, will be needed to mitigate the harm and maximize the benefits for all members of society."}, {"title": "Assistance", "content": ""}, {"title": "Author Contributions", "content": "This work was done without any outside assistance or collaboration."}]}