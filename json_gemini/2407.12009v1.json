{"title": "Using Multimodal Foundation Models and Clustering for Improved Style Ambiguity Loss", "authors": ["James Baker"], "abstract": "Teaching text-to-image models to be creative involves using style ambiguity loss, which\nrequires a pretrained classifier. In this work, we explore a new form of the style ambiguity\ntraining objective, used to approximate creativity, that does not require training a classifier\nor even a labeled dataset. We then train a diffusion model to maximize style ambiguity\nto imbue the diffusion model with creativity and find our new methods improve upon the\ntraditional method, based on automated metrics for human judgment, while still maintaining\ncreativity and novelty.", "sections": [{"title": "1 Introduction", "content": "With every new invention comes a new wave of possibilities. Humans have been making pictures since\nbefore recorded history, so its only natural that there would be interest in computational image generation.\nArtificially generating photographs that are indistinguishable from real ones has become so easy and effective\nthat there is even concern over \"deepfakes\" being used for propaganda or illicit purposes (Pawelec, 2022). On\nthe other hand, generating images that look like art is a slightly different problem. The exact mathematical\nproperties of what constitutes \"quality\" art is not as easy to quantify as other tasks, like classification\naccuracy, prediction error or whether a question was answered correctly. While machines can very easily be\ntrained to mimic a dataset, humans like to be surprised by novelty, without feeling like they are being exposed\nto total randomness. A breakthrough was the invention of the Creative Adversarial Network (Elgammal et al.,\n2017), which used a style ambiguity loss to train a network to generate images that could not be classified\nas belonging to a particular style. However, GANs have largely been superseded by diffusion models (Luo,\n2022), due to their far better results. Additionally, the style ambiguity loss requires a pretrained classifier.\nEvery set of styles or concepts requires training a classifier before even training a model to generate images.\nFurthermore, training a classifier requires that the dataset be labeled correctly, and manually labeling a\ndataset is often even more expensive and time-consuming than training a model. To circumvent these issues,\nwe propose using a classifier that does not require any additional training and can be easily applied to any\ndataset, labeled or unlabeled. Our contributions are as follows:\n\u2022 We applied creative style ambiguity loss to diffusion models, which are easier to train and produce\nhigher-quality images than GANs.\n\u2022 We developed versatile CLIP-based and K-Means-based creative style ambiguity losses that do not\nrequire training a separate GAN-based style classifier.\n\u2022 Empirically, we find our new creative style ambiguity loss can be used to tune a diffusion model to\ngenerate samples that are higher quality than the generated samples of a diffusion model trained\nwith the pre-existing GAN-based style ambiguity loss"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Creativity", "content": "Creativity has been hard to define and quantify. Creative work has been formulated as work having novelty,\nin that it differs from other similar objects, and also utility, in that it still performs a function (Cropley,\n2006). For example, a Corinthian column has elaborate, interesting, unexpected adornments (novelty) but\nstill holds up a building (utility). A distinction can also be made between \"P-creativity\", where the work is\nnovel to the creator, and \"H-creativity\" where the work is novel to everyone (Boden, 1990). Computational\ntechniques to be creative include using genetic algorithms (DiPaola & Gabora, 2008), reconstructing artifacts\nfrom novel collections of attributes (Iqbal et al., 2016), and most relevantly to this work, using Generative\nAdversarial Networks (Elgammal et al., 2017) with a style ambiguity loss."}, {"title": "2.2 Computational Art", "content": "One of the first algorithmic approaches dates back to the 1970s with the now primitive AARON (McCorduck,\n1991), which was initially only capable of drawing black and white sketches. Generative Adversarial Networks\n(Goodfellow et al., 2014), or GANs, were some of the first models to be able to create complex, photorealistic\nimages and seemed to have potential to be able to make art. Despite many problems with GANs, such as\nmode collapse and unstable training (Saxena & Cao, 2023), GANs and further improvements (Arjovsky\net al., 2017; Karras et al., 2019; 2018) were state of the art until the introduction of diffusion Sohl-Dickstein\net al. (2015). Diffusion models such as IMAGEN (Saharia et al., 2022) and DALLE-3 (Betker et al.) have\nattained widespread commercial success (and controversy) due to their widespread adoption."}, {"title": "2.3 Reinforcement Learning", "content": "Reinforcement learning (RL) is a method of training a model by having it take actions that generate a\nreward signal and change the environment, thus changing the impact and availability of future actions Qiang\n& Zhongli (2011). RL has been used for tasks as diverse as playing board games (Silver et al., 2017), protein\ndesign (Lutz et al., 2023), self-driving vehicles (Kiran et al., 2021) and quantitative finance (Sahu et al.,\n2023). Policy-gradient RL (Sutton et al., 1999) optimizes a policy that chooses which action to take at any\ngiven timestep, as opposed to value-based methods that may use a heuristic to determine the optimal choice.\nExamples of policy gradient methods include Soft Actor Critic (Haarnoja et al., 2018), Deep Deterministic\nPolicy Gradient (Lillicrap et al., 2019) and Trust Region Policy Optimization (Schulman et al., 2017a)."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Model", "content": ""}, {"title": "3.1.1 Creative Adversarial Network", "content": "A Generative Adversarial Network, or GAN (Goodfellow et al., 2014), consists of two models, a generator and\na discriminator. The generator generates samples from noise, and the discriminator detects if the samples are\ndrawn from the real data or generated. During training, the generator is trained to trick the discriminator\ninto classifying generated images as real, and the discriminator is trained to classify images correctly. Given\na generator $G : \\mathbb{R}^{noise} \\rightarrow \\mathbb{R}^{h \\times w \\times 3}$, a discriminator $D : \\mathbb{R}^{h \\times w \\times 3} \\rightarrow [0,1]$ real images $x \\in \\mathbb{R}^{h \\times w \\times 3}$, and noise\n$Z \\in \\mathbb{R}^{noise}$, the objective is:\n$\\min_{G} \\max_{D} E_x [\\log(D(x)] + E_z[\\log(1 \u2013 D(G(Z))]$\nAt inference time, the generator is used to generate realistic samples. Elgammal et al. (2017) introduced\nthe Creative Adversarial Network, or CAN, which was a DCGAN (Radford et al., 2016) where the discrim-\ninator was also trained to classify real samples, minimizing the style classification loss. Given $N$ classes\nof image (such as ukiyo-e, baroque, impressionism, etc.), the classification modules of the Discriminator"}, {"title": "3.1.2 Diffusion", "content": "A diffusion model aims to learn to iteratively remove the noise from a corrupted sample to restore the original.\nStarting with $x_0$, the forward process $q$ iteratively adds Gaussian noise to produce the noised version $x_t$,\nusing a noise schedule $\\beta_1 ... \\beta_T$, which can be learned or manually set as a hyperparameter:\n$q(x_{1:T}|x_0) = \\prod_{t=1}^{T} q(x_t|x_{t-1})$\n$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 \u2013 \\beta_t}x_{t-1}, \\beta_t I)$\nMore importantly, we also want to model the reverse process $p$, that turns a noisy sample $x$ back into $x_0$,\nconditioned on some context $c$. As $x_T$ is the fully noised version, $p(x_T|c) = \\mathcal{N}(x_T; 0, I)$\n$p_\\theta(x_{0:T}|c) = p(x_T|c) \\prod_{t=1}^{T} p_\\theta(x_{t-1}|x_{t}, c)$\n$p_\\theta(x_{t-1}|x_t, c) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t, c), \\Sigma_\\theta (x_t, t, c))$\nWe train $\\Sigma_\\theta$ and $\\mu_\\theta$ via optimizing the variational lower bound of the negative likelihood of the data:\n$E[-\\log p_\\theta(x_0)] \\leq E[-\\log \\frac{p_\\theta(x_{0:T}|c)}{q(x_{1:T}|x_0)}]= L$\nAs shown by Ho et al. (2020), this is equivalent to estimating the noise at each step using a model $\\epsilon_\\theta$. So\nthe loss to be optimized is:\n$L = E_{x,\\epsilon\\sim\\mathcal{N}(0,1),t}||\\epsilon \u2013 \\epsilon_\\theta(x_t, t)||^2$\nOnce the model has been trained, the reverse process, aka inference, to generate a sample from noise\n$x_T \\sim \\mathcal{N}(0,1)$ can be done iteratively by finding $x_{t-1}$ given $x_t, \\alpha_t = 1 \u2013 \\beta_t, \\bar{\\alpha}_t = \\Pi_{s=1}^t \\alpha_s, z \\sim \\mathcal{N}(0,1)$ and\n$\\sigma^2 = \\beta_t$ or $\\sigma^2 = \\frac{1-\\alpha_{t-1}}{1-\\alpha_{t}} \\beta_t$:\n$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t,t)) + \\sigma z$\nAcording to Ho et al. (2020), both versions of $\\sigma_t$ had similar results. In our case, we used $\\sigma^2 = \\frac{1-\\alpha_{t-1}}{1-\\alpha_{t}} \\beta_t$."}, {"title": "3.1.3 Markov Decision Processes", "content": "A Markov Decision Process (Bellman, 1957) is defined as a tuple $(S, A, p_0, P, R)$ that models the actions of\nan agent in some environment with discrete time-steps.\n$S$ is the state space, the set of states the environment can be in.\n$A$ is the set of actions that the agent can take.\n$p_0$ is the initial distributions of states $s \\in S$ when $t = 0$.\n$P_a(s, s')$ is the probability of transitioning from state $s$ at time $t$ to $s'$ at $t + 1$ when the agent has\ntaken action $a \\in A$.\nThe reward function $R(s_t, a_t)$ returns a reward a time $t$ given the action $a_t$ the agent takes and the\nstate of the environment $S_t$.\nThe agents actions are determined by the policy $\\pi(a|s)$ that maps actions to states. The series of state-\naction pairs for each timestep is called a trajectory $\\tau = (s_0, a_0....s_{T}, a_{T})$. Using policy-gradient as opposed\nto value-based RL, we train $\\pi$ by maximizing the reward $R$ over the trajectories sampled from the policy:\n$J_{RL}(\\pi) = E_{\\tau\\sim p(\\tau;\\pi)} [\\sum_{t=0}^{T}R(S_t, a_t)]$"}, {"title": "3.1.4 Denoising Diffusion Proximal Optimisation", "content": "Introduced by Black et al. (2023), Denoising Diffusion Proximal Optimisation, or DDPO, represents the\nDiffusion Process as a Markov Decision Process. A similar method was also pursued by Fan et al. (2023).\n$a_t \\epsilon_t(c,t, x_t)$\n$\\bar{x}_{t-1}  p_\\theta(x_{t-1}|x_t, c)$\n$p_0(s_0) (p(c), \\delta_T, \\mathcal{N}(0, I))$\n$P(s_{t+1}) = (\\delta_t, \\delta_{t-1}, x_{t-1})$\n$R(s_t, a_t) \\triangleq r(x_0, c)$\n$I_{RL}(\\pi) \\triangleq J_{DDPO}(\\theta) = E_{c\\sim p(c),x_0\\sim p_0(x_0|c)} [r(x_0, c)]$\nReinforcement learning training was then applied to a pretrained diffusion model, which in our case\nwas Stable Diffusion 2 (Rombach et al., 2022). Following Schulman et al. (2017b), Black et al.\n(2023) also implemented clipping to protect the policy gradient $V_\\theta J_{DDPO}$ from excessively large up-\ndates. We largely follow their method but use a different reward function. We fine-tune off of the\npre-existing stabilityai/stable-diffusion-2-base checkpoint (Rombach et al., 2022) downloaded from\nhttps://huggingface.co/stabilityai/stable-diffusion-2-base."}, {"title": "3.2 Reward Function", "content": "In the original paper, the authors used four different reward functions for four different tasks. For example,\nthey used a scorer trained on the LAION dataset (Schuhmann & Beaumont, 2022) as the reward function\nto improve the aesthetic quality of generated outputs. In this paper, we use the reward model based on\nElgammal et al. (2017), where the model is rewarded for stylistic ambiguity. Given a generated image\n$x_0 \\in \\mathbb{R}^{h \\times w \\times 3}$ and a classifier $C : \\mathbb{R}^{h \\times w \\times 3} \\rightarrow \\mathbb{R}^N$ we want to maximize:\n$R(x_0) = -CE(C(x_0), U)$\nwhere CE is the cross entropy."}, {"title": "3.3 Data", "content": "Starting with the WikiArt dataset (Saleh & Elgammal, 2015), we used 1000 images from each class, oversam-\npling when necessary, to balance the distributions between classes, to train the CAN. To train the diffusion\nmodel, we prompted the model by concatenating a randomly selected medium prompt from (painting of\n, picture of, drawing of ) to a randomly selected subject prompt (a man, a woman, a landscape,\nnature, a building, an animal, shapes, an object). An example prompt would be picture of an\nanimal. With 10% probability we would set the prompt to the null string in order to train the model\nunconditionally as well."}, {"title": "3.4 Choice of Classifier", "content": "Style ambiguity loss relies on some classifier $C$. We are exploring four versions of this classifier."}, {"title": "3.4.1 DCGAN-Based Classifier", "content": "We can use the classification module of the discriminator as the classifier in the reward function, setting\n$C = D_C$. In the case of the CAN, $D_C$ is trained jointly along with the generator. In the case of DDPO, we\nuse a pretrained $D_C$ from the CAN discriminator (which we call Diffusion DCGAN Based)."}, {"title": "3.4.2 CLIP-Based Classifier", "content": "Given $text \\in \\mathbb{R}^{text}$ and an image $x \\in \\mathbb{R}^{h \\times w \\times 3}$,we can use a pretrained CLIP (Radford et al., 2021) model, that\ncan return a similarity score for each image-text pair: $CLIP : \\mathbb{R}^{text} \\times \\mathbb{R}^{h \\times w \\times 3} \\rightarrow \\mathbb{R}$. CLIP is a multimodal\nfoundation model trained using contrastive learning (Jaiswal et al., 2021) on a dataset of approximately\n400 million text-image pairs. For each generated image $x_0$, for each class name $s_i, 1 \\leq i \\leq N_s$, we find\n$CLIP(s_i, x_0)$. We can then create a vector $(CLIP(s_1, x_0), CLIP(s_2, x_0),,,,,CLIP(s_{N_s}, x_0))$ and then use\nsoftmax to normalize the vector and define the result as $C_{CLIP}(x_0)$. Formally:\n$C_{CLIP}(x_0) = softmax((CLIP(s_1,x_0), CLIP(s_2,X_0),,,,,CLIP(s_{N_s},x_0))$\nThen we set $C = C_{CLIP}$. We discard the results of $D_C$ when using a CLIP-Based Classifier with CAN. We\nused the 27 style classes in the WikArt dataset (Saleh & Elgammal, 2015) as $s_i, 1 < i < N_s$. A list of said\nclasses can be found in Appendix A. We used the clip-vit-large-patch14 CLIP checkpoint downloaded\nfrom https://huggingface.co/openai/clip-vit-large-patch14."}, {"title": "3.4.3 K-Means Text and Image Based Classifiers", "content": "Alternatively, when we have $N_s$ text labels or $N_I$ source images, we can embed the labels or images into\nthe CLIP embedding space $\\in \\mathbb{R}^{768}$ and perform k-means clustering to generate k centers. Given a CLIP\nEmbedder $E: \\mathbb{R}^{h \\times w \\times 3} \\rightarrow \\mathbb{R}^{768}$ mapping images to embeddings, and the k centers $c_1, c_2,,,,c_k$ we can create\na vector $(||E(x_0) - c_1||, ||E(x_0) - c_2||,,,, ||E(x_0) - c_k||)$ and then use softmax to normalize the vector and define the\nresult as $C_{KMEANS}$. Formally:\n$C_{KMEANS}(x_0) = softmax(\\frac{1}{||E(x_0) \u2013 c_1||}, \\frac{1}{||E(x_0) \u2013 c_2||},,,,, \\frac{1}{||E(x_0) - c_k||})$"}, {"title": "4 Results", "content": "We generated all images with width and height = 512. The authors used width and height = 256 in the\noriginal CAN paper. However, given that larger, more detailed images are preferred by most people, we\nthought it more relevant to focus on larger images. Refer to appendix C for results on smaller images and\nexamples. Table 1 shows a few DDPO images with the prompts used to generate them. Appendix B shows\nmore examples generated using different prompts."}, {"title": "4.1 Quantitative Evaluation", "content": "We generated 100 images using the same prompts the models were trained on for each model. We used three\nquantitative metrics to score the models\n\u2022 AVA Score: Consisting of CLIP+Multi-Layer Perceptron (Haykin, 2000), the AVA model was\ntrained on the AVA dataset (Murray et al., 2016) of images and average rankings by human subjects,\nin order to learn to approximate human preferences given an image. We used the CLIP model weights\nfrom the clip-vit-large-patch14 checkpoint and the Multi-Layer Perceptron weights downloaded\nfrom https://huggingface.co/trl-lib/ddpo-aesthetic-predictor.\n\u2022 Image Reward: The image reward model (Xu et al., 2023) was trained to score images given their\ntext description based on a dataset of images and human rankings. We used the image-reward\npython library found at https://github.com/THUDM/ImageReward/tree/main.\n\u2022 Prompt Similarity: Given the CLIP model's ability to embed images and text into the same\nspace, we can measure the similarity between an image and its source prompt by finding the cosine\nsimilarity between the two CLIP embeddings. We used the clip-vit-large-patch14 checkpoint.\nResults of our experiments are shown in table 2. The best scores are bolded. There was little variance in\nprompt similarity. However, both K-Means-based approaches improved upon the DCGAN-based approach\nin terms of the two metrics for human preferences, showing that our method improves upon the past work\naesthetically while also circumventing the costly training time of using a CAN or needing a labeled dataset\nfor training the style classifier component of the CAN."}, {"title": "4.2 Comparison with Baseline", "content": "It is worth contrasting our trained DDPO model with the default pretrained stabilityai/stable-diffusion-\n2-base checkpoint diffusion model we are fine-tuning (Rombach et al., 2022). This allows us to better\nvisualize the difference the DDPO training with style ambiguity loss makes. We assumed that the DDPO\nimages might be similar to the baseline model images generated with fewer steps, so we compared DDPO\nImages to the baseline using 30,15, and 10 inference steps, as seen in figure 3.\nIn order to quantitatively compare our models to the baselines, we used the embedding of the [CLS] token\nfrom a vision transformer loaded from the dino-vits16 checkpoint (Caron et al., 2021) from https://\nhuggingface.co/facebook/dino-vits16, as that encodes stylistic information (Tumanyan et al., 2022;\nKwon & Ye, 2023). We averaged the cosine similarity between style embeddings of each pair of images\n$(x, y_{30}, y_{15}, y_{10})$, where $x$ was generated by the tuned model and $y_{30}, y_{15}, y_{10}$ was generated by the baseline\nmodel using 30,15 and 10 inference steps respectively, using the same prompt and initial random seed. We"}, {"title": "5 Conclusion", "content": "Training models with stylistic ambiguity loss teaches them to be creative. This work introduces new forms\nof stylistic ambiguity loss that do not require training a classifier or GAN, which can be time-consuming and\nunstable (Saxena & Cao, 2023). These new methods, particularly the K-Means-based approaches, scored\nhigher than the traditional method on quantitative metrics of human judgement. Nonetheless, there are still\nmore directions for this to go. Both the CLIP-based and K-Means Text-based style ambiguity losses require\nusers to heuristically choose a set of styles to \"deviate\" from. In this work, we only used the 27 categories\nin the WikiArt dataset to be comparable to the original CAN paper. However, users may instead prefer\na different set of styles or words, which may produce better or more interesting results. Additionally, the\nK-Means Image-based style ambiguity loss does not require a multimodal model like CLIP. We could have\nused any pretrained model to embed images into a lower-dimensional manifold, or trained a new one. Ergo,\nthe K-means technique could be used for any medium, such as music (Elgammal, 2022; Zhang et al., 2023),\nnew proteins (Winnifrith et al., 2023), stories (Mori et al., 2022) and videos (Cho et al., 2024)."}, {"title": "Broader Impact Statement", "content": "Many are concerned about the impacts of generative AI. By making art, this work infringes upon a domain\nonce exclusive to humans. Companies have faced scrutiny for possibly using AI (Gutierrez, 2024), and many\ncreatives, such as screenwriters and actors, have voiced concerns about whether their jobs are safe (del\nBarco, 2023). Nonetheless, using AI can help humans by making them more efficient, providing inspiration,\nand generating ideas (Fortino, 2023; Campitiello, 2023; Darling, 2022). It's also not certain how copyright\nprotection will function for AI-generated art (Watiktinnakorn et al., 2023), given copyright law is based on\nthe premise that creative works originate solely from human authorship. Clear, consistent policies, both at\nthe government level and by industry and/or academic groups, will be needed to mitigate the harm and\nmaximize the benefits for all members of society."}, {"title": "6 Assistance", "content": ""}, {"title": "Author Contributions", "content": "This work was done without any outside assistance or collaboration."}]}