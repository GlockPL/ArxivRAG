{"title": "Single-image Reflectance and Transmittance Estimation from Any Flatbed Scanner", "authors": ["Carlos Rodriguez-Pardo", "David Pascual-Hernandez", "Javier Rodriguez-Vazquez", "Jorge Lopez-Moreno", "Elena Garces"], "abstract": "Flatbed scanners have emerged as promising devices for high-resolution, single-image material capture. However, existing approaches assume very specific conditions, such as uniform diffuse illumination, which are only available in certain high-end devices, hindering their scalability and cost. In contrast, in this work, we introduce a method inspired by intrinsic image decomposition, which accurately removes both shading and specularity, effectively allowing captures with any flatbed scanner. Further, we extend previous work on single-image material reflectance capture with the estimation of opacity and transmittance, critical components of full material appearance (SVBSDF), improving the results for any material captured with a flatbed scanner, at a very high resolution and accuracy.", "sections": [{"title": "1. Introduction", "content": "Several industries, such as architectural and fashion design, For media and gaming, benefit from realistic digital replicas of physical materials. Yet, crafting these copies remains a laborious and slow task, demanding skilled artists, or sophisticated and expensive hardware [1, 2]. Consequently, recent research has focused on devising affordable and user-friendly capture setups.\nIn this scenario, flatbed scanners have emerged as promising tools for high-resolution material capture [3], owing to their user-friendly nature and provision of uniform illumination conditions. High-end scanners can even offer a lighting type closely resembling diffuse illumination, usable directly as an albedo image [3]. Nevertheless, most scanners lack this functionality, with a majority featuring a single directional light that leads to undesirable micro-specular reflections, directional shading, and cast shadows (depicted in Figure 1(a) and 2).\nIn this work, we address the drawbacks of prior approaches and introduce a technique for digitizing materials using any scanner, removing undesirable shading and specular highlights. We show that the na\u00efve solution employing an image-to-image translation network [4, 3] falls short for this purpose. Instead, we suggest employing a cycle-consistency loss in combination with a residual formulation inspired by intrinsic image decomposition methods [5].\nIn addition, a key contribution of our method is to expand the realism of the digital replica by including opacity and transmittance in the material model. These attributes are critical for thin-layer materials, like textiles, but have been neglected in current literature. We estimate the parameters of a Spatially-Varying Bidirectional Scattering Distribution Function (SVBSDF) that can reproduce complex effects of light as it passes through the material, thereby augmenting its realism in virtual environments.\nWe evaluate our method using extensive and thorough exper-"}, {"title": "2. Related Work", "content": "Single-Image Material Capture. Estimating full reflectance properties of a material, using only a single image of it, is a challenging problem which has been tackled extensively in the literature in the recent years. These approaches can be categorized based on the estimation method, the imaging device employed, and the range of digitizable reflectance properties.\nNeural style transfer [6] can be leveraged for single-image capture of stochastic materials, by matching the latent statistics of input images and renders of estimated SVBRDFs [7, 8]. A more common approach is to train an image-to-image translation model which takes a single image as input and estimates the set of SVBRDF maps. Originally supervised using pixel-wise or render-aware losses [9, 10, 11, 12], these methods have been improved by incorporating cascaded estimation [13, 14], adversarial losses [3, 15, 16, 17, 18, 19], inference-time optimization [12], or refinement [20]. More recently, diffusion models have emerged as powerful material estimators, showing competitive results [21, 22, 23, 24]. A complementary line of work uses procedural graphs for material estimation [25, 26, 27, 28].\nIn terms of devices, the most common setup encompasses fronto-planar flash-lit images captured with a smartphone. Other setups trade this simplicity for quality, such as LCD screens [29, 30, 31], or high-end flatbed scanners [3]. Single-image material estimation methods typically estimate a reduced number of SVBRDF parameters, with the exception of [21], which also estimates opacity.\nOur approach differs from previous work in two ways. First, we provide a generic framework for material capture from any flatbed scanner, with arbitrary directional illumination, effectively removing the limitations in [3]. Furthermore, to the best"}, {"title": "2.1. Preliminaries: Material Model", "content": "Building upon previous work [3], we use a physically-based material model based on microfacets reflectance [48], into which we incorporate additional parameters to enable transmittance effects. Our material model aggregates a diffuse component (i.e. the material albedo) $A \\in R^{3 \\times xy}$, with a grayscale, isotropic specular GGX [49] lobe $si,v \\in R^{xy}$, which depends on the surface normal N, its specularity S and roughness R. The shading model $fBSDF \\in [][R^{4 \\times xy}$ for a particular light I and camera v has an additional transparency term which depends"}, {"title": "3. Method", "content": "Our method takes as input a single image of the material and estimates its spatially-varying SVBSDF material parameters, including reflection and transmission per-pixel coefficients. The input image can be obtained with any capture device that provides mostly uniform lighting, such as the one provided by flatbed scanners. Our algorithm has two steps. In the first step, described in Section 3.1, we use a cycle-consistent residual generative network to delight the material and obtain an albedo-like reflectance map. After our processing, the resulting map lacks micro-reflections and shadows that might be originally present due to directional lighting hitting the material. In the second step, described in Section 3.2, we use this image as input of an attention-guided U-Net that estimate the remaining material maps, to convey reflection and transmission."}, {"title": "3.1. Material Delighting", "content": "In this step, our goal is to estimate an albedo-like reflectance map $I_d \u2248 A$ from a single image $I_1$ of the material taken under any kind of uniform lighting. We term this process delighting, as we aim to remove specular reflections, shadings, or shadows. A straightforward solution to this problem would be to train an image-to-image translation approach with labeled data. However, as we demonstrate, this baseline approach does not achieve the desired level of accuracy due to the under-constrained nature of the problem and our relatively reduced training dataset (see Table 1). Therefore, we propose a more sophisticated architecture to improve this performance, which uses residual learning and a cycle-consistency loss. Inspired by intrinsic image decomposition [5], we formulate the delighting problem as estimating a residual layer $M_R$ that adds to the albedo image to form a lighted image, $I_1 = I_a + M_R(I_d)$. Similarly, within our cycle-consistent architecture, the equivalent inverse operation also exists, and we term it relighting, $I_d = I_1 + M_D(I_1)$. Our residuals $M_R(I_d)$ and $M_D(I_1)$ are RGB images to make the estimation more flexible, thereby removing the assumption that either the source or reflected lights are white. Figure 4 presents an overview of the architecture.\nLoss Function. Our loss for each branch of our cycle-consistency model is a combination of pixel-wise, perceptual, frequency, and adversarial losses,\n$L_{im}(\u00b7, \u00b7) = \u03bb_{1}L_{1}(\u00b7, \u00b7) + \u03bb_{Lperc} L_{perc}(\u00b7, \u00b7) + \u03bb_{Lfreq}L_{freq}(\u00b7, \u00b7) + \u03bb_{adv} L_{adv}.$\nFollowing [3, 51, 1], for $L_{perc}$ we use the AlexNet version of [52] and for $L_{freq}$ we measure the Focal Frequency Loss [53]. For the adversarial loss, we follow the methodology specified in [40]. Then, we build our cycle-consistency loss and full loss as,\n$L_{cycle}(I_d, I_1) = L_{im}(I_d, M_D(M_R(I_d))) + L_{im}(I_1, M_R(M_D(I_1))$\ndelighting\nrelighting\n$L_{full}(I_d, I_1) = L_{im}(I_d, M_D(I_1)) + L_{im}(I_1, M_R(I_d)) + \u03bb_{cycle}L_{cycle}(I_d, I_1).$"}, {"title": "3.2. SVBSDF Estimation", "content": "To estimate the rest of the SVBSDF, we build upon the training methodology described in [3]. First, we expand their model to enable the estimation of opacity and transmittance maps, thus introducing two additional decoders to their attention-guided U-Net network, and expanding the loss function and discriminator architecture accordingly.\nWe further introduce additional minor changes to improve the estimation. Most notably, we parameterize the normal map so as to estimate $\\theta, \\phi$ angles instead of full cartesian coordinates xyz, and following [1] adopt elliptical grid mapping [64] for additional performance gains. Finally, we use AdamW [65] and 256 x 256 crops for training, and perform minor hyperparameter changes, which are fully described in the supplementary material."}, {"title": "4. Evaluation", "content": "4.1. Dataset\nUsing a high-end EPSON V850 Pro flatbed scanner, we capture 3830 10x10 cm material samples at 1200 PPI resolution. Note that this scanner can capture images using a standard, single LED strip illumination, like lower-end scanners, but also enables a higher-quality setup using a dual-light which provides diffuse-like illumination. A detailed description of the dataset is provided in the supplementary material. The later setup closely resembles fitted albedos [3], removing strong shades caused by wrinkles or mesostructure, hiding shadows casted to the scanner lid and eliminating specular highlights (as shown in Figure 2). We thus capture two images for each material: $I_1$ and $I_d$, preserving pixel-wise correspondence for every material under both illuminations. To augment this dataset, we further capture every material on their front and back sides, and rotate them by 90\u00b0 to allow for generalization to multiple orientations. We also capture these materials on a custom gonioreflectometer, and leverage the methodology described in [66, 1] to propagate the ground truth material parameters described in Equation 1. We use 10% of this dataset for testing.\n4.2. Metrics\nTo measure the performance of our models, we use a variety of metrics aimed at understanding the perceptual, pixel-wise, and render-aware accuracy of our estimations. First, to measure the errors of our generators, $M_D$ and $M_R$, we leverage traditional image quality metrics, as well as $\u2206E$ [67], which accurately measures color differences, and perceptually-motivated alternatives like FLIP [68] and LPIPS [52]. We also quantify per-map accuracy, leveraging pixel-wise $L_1$ norms for the Albedo, Roughness, Specular, and Transmittance maps, angular distances $L_\\theta$ for the surface normals, and the Jaccard index $L_{Jacc}$ for the opacity maps. Following [3], we also report Pearson correlations $p$.\nThe previous metrics are useful to assess individual precision of the estimations. However, when reproducing a real material, it is of critical importance to understand how these parameters interact with each other in the integrated physically-based rendering space. Thus, we propose a set of metrics aimed to evaluate the accuracy of the full material model in terms of both reflectance and transmittance. For reflectance, we expand the $L_{BRDF}$ metric in [3], which measures the perceptual error, with"}, {"title": "4.3. Ablation Study", "content": "In this section, we present an ablation study to validate each of our components.\nDelighting Model\nTable 1 presents the results of the study for our Relighting and Delighting flows. Our baseline is a pure regression-based model which uses only pixel-wise $L_1$ losses. We progressively add components to this baseline, to study their impact. First,"}, {"title": "4.4. Qualitative Results", "content": "Figure 5 shows qualitative results of our material delighting and relighting models, along with ground truth data. Our delighting model behaves accurately even in challenging cases, like the corduroy on the first column, the satin on the third or the suede leather on the last one. The predicted images contain no shading, wrinkles are hidden and shadows casted on the scanner lid are eliminated. It can be seen why the material relighting model is less accurate according to our metrics. Precisely introducing shadows, specular highlights or shading proves to be a more challenging task than removing them, and our relighting model sometimes misplaces or inaccurately estimates the intensity of these reflections. We believe that, during training using cycle-consistency, this helps the delighting model as this works as a short of data augmentation, as the delighting model is shown variations of the same material with different variations on shading and specularity."}, {"title": "4.5. Failure Cases and Limitations", "content": "Our method inherits the limitations of using flatbed scanners as a capture device. This setup cannot be used for non-flat materials (eg the marble in a statue) or materials which cannot physically be placed into this setup (eg a wall). It is also limited by our material model of choice, which, while it is more expressive than those of previous work, it cannot accurately represent complex phenomena such as anisotropy, strong displacements, high reflectivity, or subsurface scattering. Also, in order to capture non-uniform multi spectral absorption, T would require an additional attenuation value for each wavelength channel. Finally, our model sometimes struggles with some complex materials for which a single image is not a sufficient cue to estimate its"}]}