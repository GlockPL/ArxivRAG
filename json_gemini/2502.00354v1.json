{"title": "PM-MOE: Mixture of Experts on Private Model Parameters for Personalized Federated Learning", "authors": ["Yu Feng", "Yangli-ao Geng", "Yifan Zhu", "Zongfu Han", "Xie Yu", "Kaiwen Xue", "Haoran Luo", "Mengyang Sun", "Guangwei Zhang", "Meina Song"], "abstract": "Federated learning (FL) has gained widespread attention for its privacy-preserving and collaborative learning capabilities. Due to significant statistical heterogeneity, traditional FL struggles to generalize a shared model across diverse data domains. Personalized federated learning addresses this issue by dividing the model into a globally shared part and a locally private part, with the local model correcting representation biases introduced by the global model. Nevertheless, locally converged parameters more accurately capture domain-specific knowledge, and current methods overlook the potential benefits of these parameters. To address these limitations, we propose PM-MoE architecture. This architecture integrates a mixture of personalized modules and an energy-based personalized modules denoising, enabling each client to select beneficial personalized parameters from other clients. We applied the PM-MoE architecture to nine recent model-split-based personalized federated learning algorithms, achieving performance improvements with minimal additional training. Extensive experiments on six widely adopted datasets and two heterogeneity settings validate the effectiveness of our approach.", "sections": [{"title": "1 Introduction", "content": "The success of modern methods [9, 15, 24, 43, 58] is largely driven by the growing availability of training data [18, 26, 27, 48]. Unfortunately, there are still vast amounts of isolated data remain underutilized due to strict privacy requirements [10, 44]. As a result, federated learning (FL) [1, 5, 17, 37, 41, 55], has gained significant attention for its strong privacy protection and collaborative learning capabilities. This innovative paradigm allows multiple clients to collaboratively train models, where the server only aggregates models and keep private data remaining on each client. Despite its effectiveness, traditional FL methods suffer from performance degradation due to statistical heterogeneity [22]-data domains on each client are biased, with uneven class distributions, varying sample sizes, and significant feature differences.\nPersonalized federated learning (PFL) [32, 33, 39, 47] alleviates this limitation by allowing each client to better fit local data. Specifically, PFL methods focus on balancing local personalization with global consistency by splitting models into global and personalized modules [42, 62], where personalized modules capture unique local data characteristics, mitigating global model biases and better adapting to individual client data. Recent efforts have been developed based on meta-learning [13], regularization [11, 31], model splitting [4], knowledge distillation [45\u201347, 52, 54], and personalized aggregation [33, 39, 61].\nGiven that the same types of data can be distributed across multiple clients, then a key question arises: \"Can personalized modules from different clients mutually enhance each other's performance?\", which is overlooked in current PFL methods. To investigate, we conducted experiments based on the state-of-the-art PFL approaches. We randomly select a client, and several data categories which distributed across different clients. Subsequently, we trained in both centralized and personalized federated learning manner. By comparison, We evaluated whether integrating personalized parameters from other clients could improve model's representation capability. As illustrated in Figure 1 (A), the selected client indeed benefited from the personalized modules from another client.\nDriven by the above analysis, in this paper, we aim to explore how personalized modules from different clients can mutually enhance performance. As illustrated in Figure 1 (B), all clients utilize the same global model, while applying personalized module to debias according to the local data domain. For a single client, not all personalized modules contribute positively to the final representation. Therefore, we leverage two basic principles when designing our model: 1) Dynamically weighting the effect of personalized modules based on the current input. 2) Filtering modules that exhibit negative effects;\nIn this paper, we introduce PM-MoE, a two-stage personalized federated leanring framework based on mixture of experts (MoE) architecture [40, 49]. In the first stage, we pretrained models to get global and personalized modules; In the second stage, we proposed the mixture of personalized modules method (MPM) and the energy-based denoising method(EDM) to make the personalized modules from different clients enhance each other. With the first principle, PM-MoE employs the MPM based on MoE gate selection. With the second principle, PM-MoE incorporates the EDM to filter out noisy personalized models. Together, these two componets enable personalized modules from different clients to mutually enhance each other. Additionally, sharing converged personalized parameters will not break privacy requirements due to there is no gradient leakage during training. We evaluated PM-MoE on nine SOTA PFL benchmarks across six popular federated learning datasets. The experimental results demonstrate PM-MoE consistently improves the performance of various PFL methods.\nIn summary, we conclude our contributions as follows:\n\u2022 We propose PM-MOE, a novel two-stage framework for personalized federated learning which exchanges personalized knowledge across clients. In the first stage, the PM-MOE pretrains PFL models, followed by a fine-tuning stage for knowledge exchanges.\n\u2022 Specifically, PM-MOE employs a simple MOE structure to dynamically weighting the contribution of different personalized modules. Besides, PM-MOE introduces an energy-based denosing method to filter those clients with negative effects.\n\u2022 We conduct extensive experiments to nine state-of-art PFL methods across six datasets. The experimental results demonstrate PM-MOE's consistently improvement on various settings."}, {"title": "2 Notations and Preliminaries", "content": "In PFL, M clients share the same model structure. Here, we denote notations following FedGen [50] and FedRep [19]. Each client is denoted as CJ (j \u2208 1, 2, ..., M), having its own data domain D\u0130 with N\u00b9 samples (j\u2208 1, 2, ..., M). The data distribution of D\u0130 is denoted as PJ. Specifically, Dj = {x,y} y, where i is the number of training samples. xi is the i-th data sample and yi is its corresponding label. Each client Ci in PFL has two modules: the global module and the personalized module, which is denoted as w and W respectively.\nIn a typical PFL method, there is a centralized server who firstly aggregates clients' global modules, Wa..., W and then distributes the aggregated module Wg to each client. Therefore, each client is required to firstly train on DJ and upload their W every Et iteration. The sever aggregates global modules by the function f as:\n$W_g = f(W_1^g,...,W_M^g),$         (1)"}, {"title": "3 Method", "content": "In this section, we introduce the overall framework of PM-MOE, which which is a two-stage training framework. Specifically, our contributions lie in the mixture of personalized modules (MPM) and an energy-based denoising method (EDM). The MPM addresses the challenge of effectively utilizing personalized models, while the EDM method removes those personalized models with negative effets.\nThe training process of PM-MOE is divided into two steps, as shown in Figure 2. In pre-training step, we train model and obtain its converged global and personalized modules for each client, thereby constructing a personalized prompt pool. In PM-MOE step, we leverage the proposed MPM and EDM to select the optimal combination among personalized modules for each client. The following sections provide a detailed explanation of these two key phases.\nPhase 1: Pre-training. The statistically heterogeneous distribution data Di of client Ci is mapped to the feature space x rep through the global feature extractor fg : RU \u2192 RD, and to the feature space $X_p,rep$ xp,rep via the personalized feature extractor $f_p$ : RU \u2192 RD. The weighted aggregated feature space $x_{rep} = x_g,rep + x_p,rep$ is then mapped to the corresponding label space through the global classifier sg : RD \u2192 RC and the personalized classifier sp : RD \u2192 RC. U, D and C represent the input space, feature space, and label space, respectively.\nxrep = fg (W.fex)+fp (Wp.fex).         (3)\nAdditionally, as seen in DBE [59], there exists a personalized vector parameter PPJ \u2208 RD to correct the local data distribution. The associated expressions are as follows:\n\u0177 = $g(W_g^{g,hd}g(W_g^{g,hd}) + sp(W_p^{p,hd}) + PP_J$        (4)\nDuring training, the global model parameters Wife W and W are uploaded to the server for aggregation, while the personalized model parameters WW WJhd and PPJ are computed locally and not uploaded. After the global training process with Eg epochs, the model converges.\nPhase 2: PM-MOE Fine-Tuning. First, after the convergence of the model-splitting-based series of models, the server collects the trained personalized model parameters to form a personalized parameter pool, which is then distributed to each client. Next, each client locally trains a gating network, which assigns weights to each personalized model based on the input data, thereby effectively utilizing the personalized knowledge from all clients. For detailed information, refer to Section 3.2. Finally, since some of the personalized knowledge from other clients is irrelevant to the local data distribution, training the gating network with these noisy parameters can hinder convergence. To address this, we designed an energy-based denoising method. For further details, see Section 3.3."}, {"title": "3.1 The PM-MOE Overall Framework", "content": "In this section, we introduce the overall framework of PM-MOE, which which is a two-stage training framework. Specifically, our contributions lie in the mixture of personalized modules (MPM) and an energy-based denoising method (EDM). The MPM addresses the challenge of effectively utilizing personalized models, while the EDM method removes those personalized models with negative effets.\nThe training process of PM-MOE is divided into two steps, as shown in Figure 2. In pre-training step, we train model and obtain its converged global and personalized modules for each client, thereby constructing a personalized prompt pool. In PM-MOE step, we leverage the proposed MPM and EDM to select the optimal combination among personalized modules for each client. The following sections provide a detailed explanation of these two key phases."}, {"title": "3.2 Mixture of Personalized Modules", "content": "PM-MOE is a flexible architecture, and to accommodate the complex and diverse model-splitting-based personalized federated learning algorithms, we designed two adaptation methods: MPP and MPE, as shown in Figures 3 and 4.\nAssume that a personalized federated learning algorithm involves personalized parameters, these parameters do not project data into vectors of other dimensions. We define this type as local personalized parameters (PP). Suppose the personalized federated learning algorithm also involves personalized expert models, where the expert models W map data Di to a new feature space. We define this type as local personalized experts (PE). The server builds and synchronizes a set of personalized models. Depending on the type of personalized model, the server collects the converged model parameters from all clients, constructing a personalized parameter pool Wpp = {PP)}=1 and a personalized expert pool WPE = {WPE}=1 The server then synchronizes these sets with all clients.\nClients build a gating network and fine-tune parameters. Since each personalized federated learning client is diverse, as shown in Figure 3, we divide the combination of the gating network and personalized models into two categories. The first is commonalities. The calculation of set weights depends on the input data xj. To achieve this, we construct gating networks Gpp, Gpp for the personalized parameter and the personalized expert, with corresponding training parameters opp, opp. The weight calculations are formally represented as follows:\n$\u03b1_p^p = Gpp (x, Opp)$       (5)\n$\u03b1^PE = G_PE^{PE(PE)}$      (6)\nWe then sort the weights calculated by formulas (1) and (2) in descending order. From the set, we select the top k parameters and construct the personalized parameter and expert subsets as {app} = Top (k, app), {app} = Top (k, ape). Here, I denotes the index of the selected clients, where l \u2208 [1, M].\nSecond is Differences. Since the personalized parameter pool Wpp does not process data, we directly compute the weighted sum of the personalized parameters, resulting in a vector with the same shape as the local personalized parameter PPJ as follows:\n$PP_j^{moe} = W_{pp}.\u03b1_p^p$     (7)\nIn our setting, the weighted vector {PPmoe} replaces the local personalized parameter {PP}j on client CJ. For the personalized expert set WPE = WPE j=1, taking the personalized classifier sp as an example, each expert maps the data to a new feature space hl, where:\n$h^l\u2208R = sp(x^{j}, W^{PE}_l)$     (8)\nWe then compute the mixed weighted personalized parameter vector Xmoe as:\n$x^{moe} = \u2211h^l \u03b1_l^{PE}$      (9)\nThe client Ci then replaces the output of the local personalized expert WPE with xmoe \u2208 RC.\nSince the converged parameters reflect the local data knowledge that each client has spent significant effort training, during the training process, the personalized parameter and expert sets WPP, WPE are frozen and not optimized together with the gating network."}, {"title": "3.3 Energy-based Personalized Modules\nDenoising", "content": "Due to MOE using Top-K to select appropriate experts, this ranking based solely on parameter magnitude lacks confidence and introduces noise to some extent. It leads to the gating network optimizing gradients in the wrong direction. To effectively remove noise from the personalized parameter pool, inspired by energy-based models, we propose an energy-based personalized expert denoising method."}, {"title": "3.4 Theoretical Analysis", "content": "In this section, we demonstrate that leveraging personalized models converged from other clients is more beneficial for improving the performance of local models. In simple terms, model-split-based personalized federated learning shares uploaded models to learn from the data distribution of all parties involved. However, the heterogeneous nature of the data creates a tug-of-war, resulting in inefficient knowledge transfer between clients. Interestingly, the private parameters that are not uploaded by clients best capture local knowledge.\nTherefore, we propose that utilizing these converged personalized models is necessary to enhance performance, leading to the design of the PM-MOE architecture. In this subsection, we theoretically prove that the PM-MOE architecture converges to a lower bound. Even in extreme cases, where each client's data distribution is entirely different, this architecture does not degrade the performance of local models.\nTHEOREM 3.1. (Lower Bound on the Final Accuracy of MPE)\nSuppose there are M(\u2265 2) client experts predicting independently, each with an average accuracy rate of p(> 0). If a trained gate network assigns samples to the client experts such that the ratio of the probability of assigning a sample to a correct expert versus an incorrect expert is 1 + a, where a > 0. Then, the final accuracy of MPE is bounded from below by:\n$PMPE \u2265 {(1 + a)p} / {1+a(p+)} > p = Pclient$      (14)\nWe briefly proof the key steps, and other details are given in the appendix.\nPROOF. We define the event set:\nA := {s out of M experts are able to predict correctly}.\nP(A) = $({M \\atop s})p^s (1 - p)^{M-s}$      (15)"}, {"title": "4 Experiment", "content": "Main Results. Tables 1 show that PM-MOE consistently outperforms other personalized federated learning methods across both partitioning settings in tasks ranging from 4 to 200 classes. Compared to traditional federated learning methods, personalized federated learning better handles data heterogeneity, with a performance improvement of up to 48.64% over the FedAvg baseline. Interestingly, when data heterogeneity decreases, the overall performance of personalized methods also drops. The PM-MOE framework leverages the personalized models converged from all clients to improve each client's performance. If parameters from other clients are noisy, the local gating network assigns weights to prioritize the local personalized model, protecting its performance. Conversely, if external parameters are useful, the network allocates weights accordingly, enhancing the local model's performance.\nAnalysis of Gating Network Parameters. As the most critical component for each client in this framework is the training of the gating network, this section presents parameter experiments focused on tuning the number of layers, activation functions, and initialization parameters of the gating network. In this subsection, to highlight the differences between methods across different dimensions, we will apply sigmoid normalization to the data in the experimental group.\n\u2022 Number of Layers in the Gating Network: We conducted four sets of experiments on the number of layers in the gating network. 1 layer: (input dimension, number of experts); 2 layers: (input dimension, 128, number of experts); 3 layers: (input dimension, 128, 256, number of experts); 4 layers: (input dimension, 128, 256, 128, number of experts). As shown in Figure 5-(a), increasing the depth of the gating network proves to be effective. The gating network needs to determine the weights for all personalized parameters based on input data, requiring deeper neural networks on the client side to capture data features effective.\n\u2022 Activation Functions of the Gating Network: For the 4-layer feedforward gating network, we tested common neural network activation functions such as ReLU, LeakyReLU, PReLU, ELU, SELU, SiLU, and Mish. As shown in Figure 5-(b), the most effective activation function is LeakyReLU. LeakyReLU's non-linearity in the negative region allows the neural network to learn and model more complex data, effectively assigning personalized model parameters the appropriate weights.\n\u2022 Initialization Methods for the Gating Network: We compared commonly used parameter initialization methods, including uniform distribution, normal distribution, Xavier, Kaiming, Orthogonal, and Spectral. As shown in Figure 5-(c), most results indicated that the Orthogonal initialization method yields the best performance for gated network models. This method draws initial weights from the orthogonal group, maintaining dynamic isometry throughout the network's learning process, which helps preserve a relatively stable proportional relationship between input and output signals.\nAblation Study. In this section, we conducted ablation studies to evaluate the effectiveness of each individually designed module. The experiments confirmed that the PM-MOE component and the denoising component improve the performance of the model-split-based personalized federated learning algorithm. We selected the best performing personalized federated learning methods in the comparison dataset for comparison. As shown in Table 3, adding the MOE component resulted in an average improvement of 0.2% across the 4, 10, and 100 class settings. The regular denoising ratio was"}, {"title": "4.1 Baseline Methods.", "content": "We referred to the Personal Federated Learning library, PFLlib [63]. Furthermore, we compared general federated learning algorithms such as FedAvg [41], FedProx [38], SCAFFOLD [23], MOON [30], and FedGen [66], alongside recent state-of-the-art personalized federated learning methods, including personalized feature extractors like FedGH [56], LG-FedAvg [34], FedBABU [42], FedCP[62], GPFL [60], FedPer[4], FedRep [8], FedRod [6], and DBE [59] for personalized parameters."}, {"title": "4.2 Experimental Results", "content": "As the most critical component for each client in this framework is the training of the gating network, this section presents parameter experiments focused on tuning the number of layers, activation functions, and initialization parameters of the gating network. In this subsection, to highlight the differences between methods across different dimensions, we will apply sigmoid normalization to the data in the experimental group."}, {"title": "4.3 Analysis of the combination of personalized federated learning and MOE", "content": "For integrating MoE in personalized federated learning, both PFL-MoE and FedMoE use gated networks to adjust the weight balance between local personalized models and the global model. Both methods train the local models synchronously. For fair comparison, we employed a gated network to balance the global and local models'"}, {"title": "5 Related Work", "content": "Personalized Federated Learning and MOE. In personalized federated learning, methods integrating Mixture of Experts [20, 65] (MoE) models, such as PFL-MoE [16] and FedMoE [57]. PFL-MOE primarily addresses homogeneous models, modulating the experts' weights via the gating network. In contrast, FedMoE emphasizes model heterogeneity by incorporating experts with more parameters than the global model to better capture local data characteristics.\nEnergy-based denoising methods. Energy-based models (EBMs) [29] assign scalar energy values to input configurations, capturing variable dependencies. They have been applied in generative modeling [12], out-of-distribution detection [14, 36], open-set classification [2], and incremental learning [51]. In personalized federated learning, EBMs quantify relationships between model parameters using energy as a metric. For Mixture of Experts (MoE) models, EBMs can filter ineffective experts, denoising the model. However, their use for expert denoising in MoE remains underexplored."}, {"title": "6 Conclusion", "content": "In this article, we propose the PM-MOE framework to integrate the construction of a personalized parameter pool with local MOE training. PM-MOE aggregates the converged private model parameters from all clients, allowing each client to selectively reference the knowledge of others. This architecture effectively enhances the ability of model-splitting-based personalized federated learning algorithms to learn global knowledge. Through extensive experiments and theoretical analysis, we demonstrate the superiority of PM-MOE."}, {"title": "A Appendix", "content": "PROOF. Considering M client experts predict independently, the probability that exactly s experts can predict correctly is given by the binomial distribution Bin(M, p):\nP(A) = $({M \\atop s})p^s (1 - p)^{M-s}$         (22)\nwhere the event set A := {s out of M experts can predict correctly}. Under the above condition, if the gate network can assign a sample to any of these s client experts, then the MPE can predict the sample correctly. Therefore, we have:\nP(B|A) = ${(1 + \u03b1)^s} / {(1 + \u03b1)^s + ({M \\atop s})}$        (23)\nwhere B := {MPE can predict correctly}. According to the law of total probability, the probability that the MPE can predict correctly is:\nP(B) = $\u2211_AP(A)P(B|A)$ = $\u2211_s ({M \\atop s})p^s (1-p)^{M-s} . {(1 + \u03b1)^s}/{(1 + \u03b1)^s + ({M \\atop s})}$=$\u2211_{s-1} ({M-1 \\atop s}) ps-1 (1-p)^{M-s} . {(1 + \u03b1) Mp}/{M + \u03b1\u03c3_s=01}\\$= $({M-1 \\atop t})p^t (1-p)^{M-1-t}{(1 + \u03b1) Mp}/{M + \u03b1 (t+1)}$        (24)\nHere, the expectation is taken over t which follows Bin(M \u2013 1, p). Define the function:\nf(t) = ${(1 + \u03b1) Mp}/{M + \u03b1 (t+1)}$       (25)\nWe observe that f(t) is a convex function with respect to t because its second derivative is positive:\nf''(t) = ${2 (1 + \u03b1)Mpa^2}/{(M + at + \u03b1)^3}$ > 0       (26)\nsince a, p, M > 0. By Jensen's inequality, we have:\nE[f(t)] \u2265 f (E[t]).      (27)\nThe expected value of t is:\nE[t] = (M-1)p.\nCombining (24), (27) and (28) yields\nPMPE = P(B) \u2265 f (E[t]) = f((M \u2013 1)p) = ${(1 + \u03b1) Mp}/{M +\u03b1((M-1)p + 1)}$       (29)\nPMPE \u2265 ${(1 + \u03b1)p} / {1 + \u03b1 (p + {1 \\atop M})} > {(1+0)p}/{1+0(p+{1 \\atop M})} = p = Pclient$       (30)"}, {"title": "A.1 Theoretical Derivations", "content": "PROOF. Considering M client experts predict independently, the probability that exactly s experts can predict correctly is given by the binomial distribution Bin(M, p):\nP(A) = $({M \\atop s})p^s (1 - p)^{M-s}$      (22)\nwhere the event set A := {s out of M experts can predict correctly}. Under the above condition, if the gate network can assign a sample to any of these s client experts, then the MPE can predict the sample correctly. Therefore, we have:\nP(B|A) = ${(1 + \u03b1)^s} / {(1 + \u03b1)^s + ({M \\atop s})}$      (23)\nwhere B := {MPE can predict correctly}. According to the law of total probability, the probability that the MPE can predict correctly is:\nP(B) = $\u2211_AP(A)P(B|A)$ = $\u2211_s ({M \\atop s})p^s (1-p)^{M-s} . {(1 + \u03b1)^s}/{(1 + \u03b1)^s + ({M \\atop s})}$=$\u2211_{s-1} ({M-1 \\atop s}) ps-1 (1-p)^{M-s} . {(1 + \u03b1) Mp}/{M + \u03b1\u03c3_s=01}\\$= $({M-1 \\atop t})p^t (1-p)^{M-1-t}{(1 + \u03b1) Mp}/{M + \u03b1 (t+1)}$      (24)\nHere, the expectation is taken over t which follows Bin(M \u2013 1, p). Define the function:\nf(t) = ${(1 + \u03b1) Mp}/{M + \u03b1 (t+1)}$     (25)\nWe observe that f(t) is a convex function with respect to t because its second derivative is positive:\nf''(t) = ${2 (1 + \u03b1)Mpa^2}/{(M + at + \u03b1)^3}$ > 0  (26)\nsince a, p, M > 0. By Jensen's inequality, we have:\nE[f(t)] \u2265 f (E[t]).    (27)\nThe expected value of t is:\nE[t] = (M-1)p.\nCombining (24), (27) and (28) yields\nPMPE = P(B) \u2265 f (E[t]) = f((M \u2013 1)p) = ${(1 + \u03b1) Mp}/{M +\u03b1((M-1)p + 1)}$     (29)\nPMPE \u2265 ${(1 + \u03b1)p} / {1 + \u03b1 (p + {1 \\atop M})} > {(1+0)p}/{1+0(p+{1 \\atop M})} = p = Pclient$  (30)"}, {"title": "A.2 Algorithm", "content": "Following the design principles of MPP, MPE and denoising module, we apply this algorithm to nine state-of-the-art model-splitting-based personalized federated learning algorithms. The general training process is outlined in Algorithm 1."}, {"title": "A.3 Preliminary related work", "content": "Personalized Federated Learning. Personalized Federated Learning (PFL) was introduced to address the limitations of traditional federated learning in handling non-IID data and personalized requirements. PFL employs various strategies such as regularization [11, 31], meta-learning [13], knowledge distillation [45-47, 52, 54], model splitting [4], and personalized aggregation [33, 39, 61]. pFedMe [11] leverages the convexity and smoothness of Moreau Envelopes to facilitate its convergence analysis, while Per-FedAvg [13] incorporates meta-learning into federated learning. FedDistill [45] transfers global knowledge to local models through distillation."}, {"title": "A.4 Privacy Analysis", "content": "For model-splitting-based personalized federated learning algorithms combined with PM-MOE, data privacy is ensured in both phases.\nIn the pre-training phase, each client uploads only the shared parameters to the server, while personalized parameters are trained locally. Due to the model splitting, the link between shared and personalized parameters is severed. The gradient information of personalized parameters remains private to each client, making it difficult to breach data privacy through model inversion attacks [3].\nIn the PM-MOE phase, both the server and clients only receive the converged personalized model parameters. Clients cannot infer the training data or other private information from the model parameters. Therefore, the proposed approach effectively safeguards data privacy."}, {"title": "A.5 Experimental Details", "content": "To ensure fairness, we employ a 4-layer CNN model as the backbone for Cifar10, Cifar100, MNIST [28], Fashion-MNIST [53], and Tiny-ImageNet [7] datasets, and a fastText [21] model for AG News. Each personalized model is pre-trained for 2000 epochs until convergence. We optimize three key parameters: moe (local MOE learning rate), k (top k MOE weights), and Emoe (local MOE training iterations). All experiments are executed on a single RTX 3090 GPU."}, {"title": "A.6 Dataset and Data Partitioning.", "content": "We use public datasets to perform experiments and evaluate the performance of PM-MOE. Specifically, we adopt a Dirichlet distribution [35] with a shared ratio S(0 < S < 100) for data partitioning.\n\u2022 Dirichlet distribution with S = 20: In the first setting, 20% of the data for each class is uniformly distributed among M clients, and the remaining data is assigned based on Dirichlet-distributed weights.\n\u2022 Dirichlet distribution with S = 0: In the second setting, no constraints are placed on class distribution across clients, with all data allocated based on Dirichlet-distributed weights.\nThe detailed descriptions and statistics of these datasets are as follows:\n\u2022 MNIST [28] dataset is a widely used collection for handwritten digit recognition, compiled by the National Institute of Standards and Technology (NIST). It consists of 60,000 training images and 10,000 test images, each a 28x28 grayscale representation of digits from 0 to 9.\n\u2022 FMNIST [53] is a dataset of fashion product images intended as a more challenging alternative to the traditional MNIST. It contains 10 categories of clothing items, such as T-shirts, trousers, and sweaters, with 7,000 grayscale images per category. There are 60,000 training images and 10,000 test images, all at 28x28 pixels. Fashion MNIST presents a greater challenge in terms of image quality and diversity, featuring more background details and varying perspectives.\n\u2022 Cifar10 [25] consists of 60,000 32x32 color images divided into 10 classes, with 6,000 images per class. Of these, 50,000 are used for training and 10,000 for testing. The dataset is split into five training batches and one test batch, each containing 10,000 images. The test batch includes 1,000 randomly chosen images from each class, while the training batches may have varying class distributions across batches.\n\u2022 Cifar100 [25] dataset contains 60,000 32x32 color images, but it is divided into 100 classes, with 600 images per class. Each class has 500 images for training and 100 for testing. These 100 classes are grouped into 20 super-classes, with each image having both a \"fine\" label (its specific class) and a \"coarse\" label (its super-class).\n\u2022 TINY [7] dataset is a subset of ImageNet, released by Stanford University. It comprises 200 classes, each with 500 training images, 50 validation images, and 50 test images. The images have been preprocessed and resized to 64x64 pixels and are commonly used in deep learning for image classification tasks.\n\u2022 AGNews [64] dataset is an open dataset for text classification, containing 120,000 news headlines and descriptions from four categories: World, Sports, Business, and Technology. Each category includes 30,000 samples, with 120,000 samples in the training set and 7,600 in the test set."}, {"title": "A.7 Baselines", "content": "In our experiments, the comparison baselines mainly include traditional federated learning methods (FedAvg, FedProx, SCAFFOLD, MOON, and FedGen), federated learning of personalized experts (FedGH, LG-FedAvg, FedBABU, FedCP, GPFL, FedPer, FedRep, FedRod), and federated learning of personalized parameters (DBE).\n\u2022 FedAvg [41] is a pioneering algorithm in federated learning. Its core idea is to send the global model from the server to participating clients, where each client trains the model using their local data. The updated model parameters are then uploaded to the server, which computes the average of these parameters to update the global model. FedAvg can encounter performance bottlenecks when faced with highly imbalanced data or significant differences in client computing power.\n\u2022 FedProx [38] aims to address the performance degradation of FedAvg when dealing with non-i.i.d. data. It introduces a regularization term during local training to penalize the deviation"}, {"title": "A.8 Evaluation Metrics", "content": "In personalized federated learning, the assessment of global accuracy can be formulated as the weighted sum of each client's accuracy rate multiplied by its sample proportion. The formal expression is as follows:\nAtotal = $\u2211_{j=1}^M \\frac{N_J}{N}A_J$         (31)\nwhere Atotal denotes the weighted total accuracy. M is the total number of clients. A represents the accuracy of the j-th client, and Ni is the number of samples from the j \u2013 th client. N = \u03a31 \u039d is the total number of samples across all clients. W signifies the proportion of samples from the j \u2013 th client."}, {"title": "A.9 Concerns about time cost", "content": "The performance here refers to the average improvement across all datasets. For instance", "consumption.\nCase": "The major computational burden lies in the pre-training phase. For instance", "AGNews": "n\u2022"}]}