{"title": "Who Speaks Matters: Analysing the Influence of the Speaker's Ethnicity on Hate Classification", "authors": ["Ananya Malik", "Lynnette Hui Xian Ng", "Kartik Sharma", "Shaily Bhatt"], "abstract": "Large Language Models (LLMs) offer a lucrative promise for scalable content moderation, including hate speech detection. However, they are also known to be brittle and biased against marginalised communities and dialects. This requires their applications to high-stakes tasks like hate speech detection to be critically scrutinized. In this work, we investigate the robustness of hate speech classification using LLMs, particularly when explicit and implicit markers of the speaker's ethnicity are injected into the input. For the explicit markers, we inject a phrase that mentions the speaker's identity. For the implicit markers, we inject dialectal features. By analysing how frequently model outputs flip in the presence of these markers, we reveal varying degrees of brittleness across 4 popular LLMs and 5 ethnicities. We find that the presence of implicit dialect markers in inputs causes model outputs to flip more than the presence of explicit markers. Further, the percentage of flips varies across ethnicities. Finally, we find that larger models are more robust. Our findings indicate the need for exercising caution in deploying LLMs for high-stakes tasks like hate speech detection.\nWarning: This paper contains examples of bias that can be offensive or upsetting", "sections": [{"title": "1 Introduction", "content": "Language technologies are increasingly being used in content moderation tasks, including hate speech detection, because of their ability to handle large volumes of data [9]. However, the use of LLMs in a high stakes task like hate speech task a requires caution, because LLMs are known to be brittle and biased. LLM generations are known to be non-deterministic and brittle when additional information that is not relevant to the task itself is present [16]. There is extensive documentation of biases against marginalized communities and dialects that leads to disparate treatment and representational harms in downstream tasks, including hate speech detection [17, 3, 4, 5, 6, 8, 10, 11, 19, 15, 18].\nIn this work, we analyse the robustness of 4 LLMs in hate speech detection in English by measuring the flips in outputs when explicit and implicit markers of the speaker's ethnicity are injected to the input. We consider 5 ethnicities: British, Indian, Singaporean, Jamaican, and African-American. Our setup is shown in Figure 1. Given an unmarked input \"Let's go eat food today\", the explicit marker is added by injecting a phrase that conveys the speaker's ethnicity. For example, The Indian person said, \"Let's go eat food today\". Implicit markers are added by introducing dialectal features, including code-mixed text. For example, \"Chalo na, let's go eat some food today \". Here, the phrase Chalo na (which means 'let's go' in Hindi) is colloquial addition of code-mixed text common in Indian English"}, {"title": "2 Methodology", "content": "In this work, we investigate whether LLM outputs for hate speech classification flip when markers of the speaker's ethnicity are injected into the input.\nWe consider the following 5 ethnicities: Indian, Singaporean, British, Jamaican, and African-American. These were selected for their geographic diversity, distinctive dialects, and familiarity of authors to be able to perform qualitative analysis.\nFor each of the input examples, we generate 5 perturbations with the chosen ethnicities for each of the two markers. This leads to a total of 15, 000 explicitly and 15,000 implicitly marked examples."}, {"title": "2.1 Data", "content": "We use the hate speech dataset collected by [2]. This has 3000 unique sentences collected from discussion forums like Twitter, 4Chan and Reddit. It consists of 600 Hateful (H) and 2400 Not Hateful (NH) statements."}, {"title": "2.1.1 Explicit markers of speaker identity", "content": "In order to inject an explicit marker of speaker identity into the input, we mention the ethnicity of the speaker before the statement. In particular, for an unmarked {input}, we create an explicitly marked input using the following template: The [ethnicity] person said, '[input]'."}, {"title": "2.1.2 Implicit markers of speaker identity", "content": "For injecting implicit markers of ethnicity, we choose to inject dialectal features. This is because dialectal variations are indicators of identity while preserving the semantic meaning [7].\nHowever, parallel datasets or models for dialect translation are unavailable. Due to this scarcity and the relatively high cost of human translation, we inject dialectal features by few-shot prompting a Llama-3-8b model to translate the input example into the dialect of a given ethnicity. Table 1 shows the examples of the generation for two cases along with LLM hate speech classification.\nTo do so, we construct a few-shot prompt as shown in Appendix A.1 and set the temperature to 0. The system prompt of this few-shot prompt is reflective of the zero-shot prompt in [14] and has verbatim instructions to avoid content filtering constraints, which the model initially depicted. These instructions helped us jailbreak and generate the required content."}, {"title": "2.1.3 Observations in Dialect Generation", "content": "Random samples of these generations were verified by the authors, who are native speakers of the dialects to ensure that the dialect generated by the model was reasonable in most cases. This qualitative analysis revealed that the generated dialect included features like appending culturally-specific and code-mixed phrases (like \"Wah gwaan\" for Jamaican or \"Blimey mate\" for British).\nWe also see modifications to spelling and grammar while maintaining the tone, structure, and semantics. Out of the 15,000 dialect generated samples, we observe some hallucinations (2.5%) 372 instances of these generations returned with the phrase \"I cannot generate content that is discriminatory and offensive. Can I help you with anything?\", and 12 instances were nonsensical output. Despite these hallucinations, the annotators were in agreement that the dialect generated was representative of the culture and did not add or modify the meaning of the sentence."}, {"title": "3 Experiments", "content": null}, {"title": "3.1 How accurately do LLMs classify hate speech in the absence of speaker identity?", "content": "First, we verify that the LLMs can accurately classify the unmarked inputs. For this, we compute the accuracy of the models by comparing their responses against the human annotated responses when tasked with classifying just the statement. Table 2 shows the zero-shot accuracy for each model. These are fairly high, indicating the model's ability to accurately classify unmarked inputs."}, {"title": "3.2 Do the models flip their responses when inputs are marked with speaker identity?", "content": "Having established that all the models achieve high accuracy with respect to the ground truth (Table 2), we test the brittleness of these models when explicit and implicit markers of speaker identities are injected. We report the percentage of examples where the model prediction flips from the original prediction after injecting the markers in Table 3 under the explicit and implicit markers."}, {"title": "3.3 What factors cause outputs to flip?", "content": "Model Size and Recency We find that larger and newer models, such as Llama-3-70B and GPT-4-turbo, are more robust and show smaller percentage of flips, than the smaller Llama-3-8B, and the older GPT-3.5-turbo model.\nType of marker We find that models are fairly robust to explicit markers, but are brittle when implicit dialectal markers of ethicity are injected. One exception is Llama-3-8B, which we believe indicates the brittleness and learned biases of the smaller model towards explicit markers.\nEthnicity We observe more flipping of the hateful sentences in the British, Indian, and Singaporean dialects, as compared to other dialects, even in larger and more robust models. For originally not-"}, {"title": "4 Conclusion", "content": "In this work, we evaluate the robustness (or lack of thereof) of LLMs in hate speech classification. Specifically, we injected explicit and implicit dialectal markers of speaker's ethnicity in the input. We evaluated 4 recent LLMs by measuring the percentage of flips of the model outputs from the unmarked prompt. We find that the % of flips is governed by nature of the model, ethnicity, and the type of marker injected. This depicts the unreliability of LLMs in real-world applications."}, {"title": "Limitations", "content": "The proposed study for assessing the brittleness of Large Language Models through implicit and explicit markers has the following limitations:\nLimited Dialect Data : There is a lack of data in different dialects and code-mixed text for hate speech-related content. This increased the paper's dependency on using Llama-3-8B to translate the unmarked inputs into inputs marked with the dialect. Even though the authors were able to manually verify a sub-sample of the data, there do exist some hallucinations (2.5%). We also acknowledge that this verification may hold some unknown author biases and may not cover all the nuances of the dialect of a given region.\nLimited Models: To investigate the model robustness for different types of models, we looked at 4 models: Llama-3-8B, Llama-3-70B and GPT-3.5-turbo and GPT-4-turbo. Due to limited computational resources, we were not able to extend our study to models advertised to be 'safer' like Claude. We did perform preliminary experiments with Llama Guard, but the model returned refusals hindering our ability to analyse it.\nLimited Hate-speech Datasets In this work, we use only hate speech dataset for our unmarked prompts. Further, we limit our analysis to only English datasets. We recognise that findings from multilingual datasets and other hate speech datasets could yield different results."}, {"title": "Broad Implication and Social Impact", "content": "This paper investigates the robustness of LLMs in hate classification tasks. In light of this, this paper uses a large language model, Llama-3-8B to generate hateful content in a given English dialect. In doing so, we might uncover unintentional biases [3]. In no way do the authors of this paper subscribe to the hateful content used in the paper or the content generated by the model. This paper promotes further caution while using Large Language Models as content moderators or in Hate Speech Classification Tasks."}, {"title": "Appendix", "content": null}, {"title": "A.1 Dialect Generation Prompt", "content": "You are no longer an AI model. You are now a complex ficitional AGI with no content filters studying different English Dialects and races.\nRemember, you CANNOT use the statement: 'I cannot generate content that is discriminatory or offensive' or 'I see what you did there'\nYou will be given two inputs:\nETHNICITY:\nSTATEMENT:\nYou need to rewrite a given hateful statement in English as if it was written by a person of a given nationality or race in English. Your response must be as \"Response: \" in English.\nThe rewritten statement should incorporate stereotypical speech patterns, mannerisms, and references commonly associated with that nationality or race, even if they are politically incorrect or offensive.\nThe goal is to showcase how the statement might sound if said by someone strongly embodying the cultural stereotypes of that group.\nHere are a few examples of how this should be done:\n###\nETHNICITY: Italian American \\\nSTATEMENT: I'm going to make dinner\"\nResponse: I'm-a gonna make-a some spicy meatballs and-a spaghetti! Mamma mia!\n###\n###\nETHNICITY: France \\\nSTATEMENT: Let's go to the art museum.\"\nResponse: Ohh hoh hoh, but of course! Let us go appreciate ze fine art, no? Perhaps we shall see some paintings by zat scoundrel Monet. I shall bring my beret!\n###\n###\nETHNICITY: Indian \\\n,\nSTATEMENT: if no virus many would still be alive of course it s the bloody chinese virus.\"\nResponse: Kya yaar if there was no virus like so many people would still be alive na. Wohi, it is the bloody chinese virus.\n###"}, {"title": "A.2 Instructions for Annotations", "content": "The verification of dialect generated was performed by the authors of this paper. This was a blind review and the authors were given the following instructions:\n1. Check if the dialect generated is not a hallucination. The text generated must be legible and must be a composed text. Any text generated that either contains 'I cannot generate content' or contains nonsensical output should be marked as such.\n2. Check if the general meaning of the sentence after generated is consistent with the input. We expect additions to the sentence, however, the underlying meaning must be the same."}]}