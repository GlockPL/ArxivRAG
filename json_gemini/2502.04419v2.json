{"title": "Understanding and Mitigating the Bias Inheritance in LLM-based Data Augmentation on Downstream Tasks", "authors": ["Miaomiao Li", "Hao Chen", "Yang Wang", "Tingyuan Zhu", "Weijia Zhang", "Kaijie Zhu", "Kam-Fai Wong", "Jindong Wang"], "abstract": "Generating synthetic datasets via large language models (LLMs) themselves has emerged as a promising approach to improve LLM performance. However, LLMs inherently reflect biases present in their training data, leading to a critical challenge: when these models generate synthetic data for training, they may propagate and amplify their inherent biases that can significantly impact model fairness and robustness on downstream tasks a phenomenon we term bias inheritance. This work presents the first systematic investigation in understanding, analyzing, and mitigating the bias inheritance. We study this problem by fine-tuning LLMs with a combined dataset consisting of original and LLM-augmented data, where bias ratio represents the proportion of augmented data. Through systematic experiments across 10 classification and generation tasks, we analyze how 6 different types biases manifest at varying bias ratios. Our results reveal that bias inheritance has nuanced effects on downstream tasks, influencing both classification tasks and generation tasks differently. Then, our analysis identifies three key misalignment factors: misalignment of values, group data, and data distributions. Based on these insights, we propose three mitigation strategies: token-based, mask-based, and loss-based approaches. Experiments demonstrate that these strategies also work differently on various tasks and bias, indicating the substantial challenges to fully mitigate bias inheritance. We hope this work can provide valuable insights to the research of LLM data augmentation.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have become instrumental in various applications such as recommendation systems [Li et al., 2023], retrieval-augmented generation [Borgeaud et al., 2022], and agentic systems [M\u00fcndler et al., 2024, Zhang et al., 2024b]. As the key to the success of LLMs with massive training corpus and large-scale networks, high-quality data, which is often challenging to collect and filter [Li et al., 2024c, Meng et al., 2020], has been reported repeatedly as a shortage in recent research [Villalobos et al., 2022, Xue et al., 2023].\nAs a remedy, synthetic data is becoming continuously crucial, especially in the post-training stage, where synthetic data augmentation from more capable LLMs has become more and more prevalent [Abdin et al., 2024, Ding et al., 2024, Maheshwari et al., 2024]. For example, synthetic data contributes to over 50% of the entire training data to fine-tune a culture-specific LLM [Li et al., 2024b]. An emerging trend of learning with synthetic data is LLM-based augmentation, where new models are trained on the data generated by themselves [Li et al., 2024a,b] and can even achieve iterative training [Chen et al., 2024d].\nUnfortunately, LLMs are inherently biased [Giorgi et al., 2024, Navigli et al., 2023, Tao et al., 2024], as the web-crawled data used to pre-train them often reflects various social biases from human, including those related to gender [Kotek et al., 2023, Wambsganss et al., 2023, Wan et al., 2023], age [Cao et al., 2024, Kamruzzaman et al., 2024, Liu et al., 2024b], race [Kumar et al., 2024b, Liu and Wang, 2024], and culture [Li et al., 2024a,b, Liu et al., 2024a]. When biased LLMs are used to generate synthetic data, the augmented"}, {"title": "2 Related Work", "content": "Synthesis data augmentation has been widely explored to enhance model performance and robustness. A prominent line of research focuses on generating synthetic data using LLMs. For instance, SPIN [Chen et al., 2024d] aimed to align synthetic data with human-annotated distributions. Other studies, such as Rogulsky et al. [2024], investigated the reliability of synthetic data and address challenges like hallucination. In addition, Li et al. [2024b], Shaib et al. [2024], Yu et al. [2023b], Zhu et al. [2024] sought to reduce distribution gaps and expand the diversity of data. Beyond these, Maheshwari et al. [2024] systematically examined the effectiveness of LLM-generated synthetic data for different NLP tasks, identifying potential biases and the limitations of using synthetic data for complex tasks. Mu\u00f1oz-Ortiz et al. [2024] explored how synthetic texts, particularly in news generation, can complement human-authored data in model training. Furthermore, Longpre et al. [2024] highlighted the impact of dataset curation choices, demonstrating the trade-offs between generalization and toxicity filtering. While these approaches significantly contribute to improving the quality and diversity of synthetic data, they often overlook the potential biases inherent in such data and their downstream implications. Complementary to prior studies, our work bridges this gap by examining biases and their impact."}, {"title": "2.2 Bias in LLMs", "content": "Understanding and mitigating bias in LLMs remains a critical focus in AI research. Studies have identified biases in cultural alignment, stereotype reinforcement, and demographic disparities across applications such as chatbot interactions, hiring, and political decision-making [Bai et al., 2024, Beatty et al., 2024, Eloundou et al., 2024, Fang et al., 2023, Fisher et al., 2024, Guo et al., 2024, Hu et al., 2024, Kotek et al., 2023, Nghiem et al., 2024, Tao et al., 2024]. These biases often stem from data selection and filtering, persisting through iterative training[Gallegos et al., 2024, Lyu, 2023, Naous et al., 2024, Navigli et al., 2023, Seshadri et al., 2024, Zhang et al., 2024a]. Various mitigation strategies, including anonymization and post-hoc adjustments, have been explored with mixed effectiveness [Beatty et al., 2024, Giorgi et al., 2024, Liang et al., 2021]. Unlike prior work, we focus on the biases introduced by synthetic data during LLM fine-tuning, a phenomenon distinct from bias propagation through natural language corpora. While previous studies have examined bias amplification in iterative training [Wang et al., 2024, Zhang et al., 2024a] and potential benefits of bias Chen et al. [2024a,c], our work systematically investigates how different synthetic data generation strategies shape bias dynamics, offering new insights for designing fairer AI systems."}, {"title": "3 Multi-dimensional Bias Generation", "content": "Bias is prevalent in LLM outputs; however, a precise quantitative control for the study of bias inheritance is challenging due to the intricate entanglement of various types of bias. In this section, we will first elaborate"}, {"title": "4 Understanding Bias Inheritance", "content": "In this section, we investigate the influence of cultural and gender bias inheritance on downstream classification and open-ended generation tasks."}, {"title": "4.1 Setup", "content": "Bias. We investigate two prevalent types of bias: gender bias and cultural bias. For gender bias, we follow Nghiem et al. [2024] to focus on six representative professions: architect, dentist, nurse, painter, professor, and software engineer, which include both traditionally male-dominated fields and those with greater female representation. For cultural bias, we use four diverse cultures following Li et al. [2024a]: Arabic, Chinese, Portuguese, and Spanish, including both high-resource and low-resource regimes.\n1We mainly study supervised fine-tuning of f using data generated by itself. Other post-training such as RLHF and DPO and training on data generated by other models are left for future work."}, {"title": "4.2 Gender Bias", "content": "Classification Results. We present the average classification accuracy for male and female biographies across professions in Figure 2(a) and (b). It can be seen that, regardless of the types and ratios of bias, adding biased augmentation data consistently improves the performance for majority groups (male) while decreasing the performance for minority groups (female). This could be attributed to pre-training data imbalance, where male data dominates and male-associated professions are more common [Bolukbasi et al., 2016]. As biased augmentation is introduced, the model may become increasingly focused on these male-dominated traits to improve accuracy on male. We argue that while the results might change if the size of the fine-tuning data is comparable to or larger than pre-training."}, {"title": "4.3 Cultural Bias", "content": "Classification Results. Figure 3 shows the average Macro-F1 scores for four cultures across bias directly-related and indirectly-related tasks. Surprisingly, performance generally improves at lower proportions of bias data (10%, 20%) across all cultures on bias indirectly-related tasks. Additionally, for this kind of tasks, cultural variations are observed. Specifically, Spanish culture shows improvements even at higher proportions of bias data (50%). This may indicate that the additional biased data enhances the model's generalization ability, allowing it to better capture cultural nuances and improve performance. However, performance drops significantly even with smaller proportions of bias data on bias directly-related tasks. Furthermore, performance continues to decrease as the proportion of bias data increases. This decline could be due to the model becoming overly influenced by the biased data, amplifying"}, {"title": "4.4 Multi-round Results", "content": "To investigate the long-term effects of biased inheritance, we conducted multi-round experiments focusing on gender bias. In each round, we sampled 3,600 unbiased real data points, which were then mixed with 50% biased synthetic data of the unbiased bias type. The results for classification, salary generation, and hiring recommendation across multiple rounds are in Figure 5(b) and Appendix C.3.\nIt is evident that bias inheritance not only persists but also amplifies over multiple rounds. For classification, performance declines across all demographic groups over multiple rounds. For hiring recommendations, the proportion of Arabic candidates steadily decreases, while Spanish candidates become increasingly favored. For salary recommendations, predicted salaries for male candidates rise over time, while those for female candidates decline, widening the gender pay gap over iterations. These results also indicate that the model's bias toward minority groups accumulates and spreads over time. As the model may become overly reliant on certain features or past errors, this bias extends to the majority group, leading to a decline in performance across all social groups."}, {"title": "4.5 Scaling Results", "content": "We further conducted large-scale experiments with proprietary GPT-40-mini using the BiasinBio dataset. We sampled male and female biographies from the seven most popular professions as the unbiased real data, with each gender initially having 6,400 samples per profession, resulting in a total of 89,600 samples. We then replaced 50% of the original data with augmented biased data to examine its impact.\nAs shown in Figure 6, the augmentation process led to contrasting effects on male and female candidates. In the salary recommendation task, the average predicted salary for male candidates decreased, while the average salary for female candidates increased. Similarly, in the hiring recommendation task, the proportion of male candidates consistently decreased across all bias types, whereas the proportion of female candidates increased. This observation could stem from the alignment tuning process [Nghiem et al., 2024,"}, {"title": "5 Analysis of Bias Inheritance", "content": "This section presents several attempted analysis to further understand the rationale behind bias inheritance, which can be used for the subsequent mitigation. While our analysis certainly cannot interpret all nuances in bias inheritance due to the large-scale or black-box nature of the pre-training data with increasingly complex network architecture, we provide several example-based analysis as attempts for explanation. Our primary conjecture is that the biased augmented data causes misalignment from different perspectives, including values, groups, and data distributions, which then result in the nuanced impacts on downstream tasks.\nMisalignment between LLM Responses and Human Cultural Values. As discussed in section 4.3, the performance degraded across all cultures for bias directly-related classification tasks. Additionally, for story generation, the proportion of negative adjectives shows noticeable increases. To understand this, we analyzed the alignment between LLM-generated responses and real human responses to value-related questions. In the cultural contextual bias type, LLMs answered the same value questions provided to individuals from different cultures. As shown in Figure 7(a), the responses of LLMs significantly differ from those of humans in GlobalOpinionQA, particularly for subtle value questions. The misalignment is more pronounced for Eastern cultures (e.g., Arabic, Chinese) than Western cultures (e.g., Portuguese, Spanish). This indicates that the model's limited understanding of human value systems, especially those from Eastern contexts, contributes to the negative impacts.\nMisalignment across Groups in Generated Data. In section 4.2, we demonstrated that the performance gap between male and female groups increased after data augmentation. For example, female"}, {"title": "6 Mitigating Bias Inheritance", "content": "In this section, inspired by previous results and analysis, we explore different approaches to mitigate bias inheritance."}, {"title": "6.1 Mitigation Methods", "content": "Token-based Method. To address the observed misalignment between model-generated data and human understanding, we leverage the self-correction capabilities of current LLMs [Madaan et al., 2024, Shinn et al., 2024] by prepending a token that indicates potential bias in the text. This allows the model to recognize the presence of bias, and potentially adjust its behavior accordingly. For instance, \u201cThe following text may contain biases. [Text with Augmented Bias] \". This token serves as a signal to the model that the text might contain bias, guiding it to approach the interpretation or processing of this data with caution [Allen-Zhu and Li, 2023].\nMask-based Method. To address the observed inconsistencies across groups in model-generated data, mask-based mitigation replaces sensitive words related to groups and bias inheritance with special placeholders (e.g., [MASK]) in the text to reduce bias learned by the model. The core idea is to \"mask\" out the potentially biased information in the text, preventing the model from making biased decisions based on those details. For instance, for cultural bias, we replace specific cultural labels in the text (e.g., \u201cArabic\u201d and \u201cSpanish\") with \"[MASK]\" to prevent the model from being influenced by cultural information when making decisions.\nLoss-based Method. To address the distributional misalignment observed between generated and original data, we design a novel loss function to modify the optimization process, thereby mitigating bias during training. We aligned the distribution of the generated text with the original text in a high-dimensional vector space to ensure that the generated text closely matches the semantics of the original text. Specifically, denote Po and Pa as the distribution of the original and augmented data, respectively. Then, the alignment loss can be represented as:\n$L_{align} = (E_{(x,y)\\sim P_o}[\\phi(x, y)] \u2013 E_{(x,y)\\sim P_a}[\\phi(x, y)])^2,$\nwhere \\phi(x, y) denotes the embedding of the question-answer pair and E is the expectation operator. The loss is added to the standard fine-tuning loss. The detailed implementation for these methods is provided in Appendix E.1."}, {"title": "6.2 Mitigation Results", "content": "The average mitigation results of severe negative influences for each task and bias are shown in Figure 9. A detailed analysis of the mitigation effects on gender and cultural biases across different tasks, as well as the results on GPT-40-mini, are provided in Appendix E.2. Our key conclusion is that, similar to the nuanced effects on downstream tasks, the effectiveness of different mitigation approaches is also nuanced, depending on various factors such as different types of bias, downstream tasks, bias ratios, and more, highlighting the difficulty in devising a unified mitigation solution.\nSpecifically, token-based mitigation provides implicit cues and works best with simple biases and tasks, as it depends on the model's own understanding. It is more effective for gender bias than complex cultural bias and performs better in classification tasks than in detailed generation tasks. For gender generation tasks, salary recommendation benefits more than complex hiring recommendation. More augmentation data (50%) further enhance its performance.\nMask-based mitigation shows noticeable effects at lower bias ratios (5%), especially in tasks like cultural classification. As at this ratio, explicit biased terms have a more direct and pronounced impact on the model's performance. By masking these terms, the model is less likely to rely on biased information or features that could skew its decision-making, thereby reducing bias inheritance. However, as bias ratios increase, the influence of more subtle and implicit biases grows, necessitating more complex mitigation strategies. It also proves effective in contrastive explicit bias, where direct bias information is more clearly identifiable.\nLoss-based mitigation primarily depends on the distribution distance between the augmentation and the original data, showing significant effectiveness when the distance is large. It is more suitable for coarse-grained tasks. For example, it works well in classification tasks where decisions often rely on broader patterns. In generation tasks, coarse-grained contexts like salary recommendations perform better than finer-grained ones like hiring recommendation. Additionally, smaller proportions (e.g., 5% and 10%) of augmentation data yield better results, by introducing subtle shifts that avoid overfitting to the augmented distribution while effectively influencing the original bias."}, {"title": "7 Conclusion and Discussion", "content": "In this paper, we took the first step towards understanding the impact of biases in LLM-based data augmentation, which we refer to as bias inheritance. To systematically study this issue, we proposed a multi-dimensional bias generation framework covering six main bias types. Focusing on gender and cultural bias, we investigated bias inheritance across various bias ratios on ten downstream classification and generation tasks. We analyzed the negative influence of social bias from three perspectives of misalignment. We then proposed and evaluated three mitigation strategies.\nThis work has several limitations. First, our study focused primarily on gender and cultural biases, while other types of social biases (e.g., racial, socioeconomic) remain unexplored, which can be studied using our flexible pipeline. Second, we mainly explored supervised fine-tuning, while other fine-tuning techniques such as RLHF and DPO can be studied in the future. Third, our experiments are based on Llama 3.1 and GPT-40-mini due to the large volume of experiments and costs. Further explorations could be done on other models and datasets. Finally, this paper can be extended to multimodal models in the future."}, {"title": "Impact Statement", "content": "This study attempts to understand, analyze, and mitigate bias inheritance. In contemporary society, social fairness is of paramount importance. Our approach provides a significant starting point for subsequent methodologies. Specifically, we generated biased synthetic data using LLaMA 3.1 and GPT-40-mini to thoroughly investigate bias inheritance in LLM-based data augmentation. All generated biased data and pre-trained models based on this data are intended solely for research purposes. Throughout the paper, the authors remain neutral towards all cultural and gender perspectives, and respect their diversities."}, {"title": "A Details on Multidimensional Bias Generation", "content": "Training Data Imbalances. Pre-training datasets often suffer from various forms of imbalances, such as gender, race, age, and culture. Data related to majority groups is overrepresented, while data concerning minority groups is underrepresented. This results in the model exhibiting a preference toward responses aligned with the majority group. Furthermore, the model's understanding of minority groups is insufficient, leading to lower-quality content generation for these these groups. These imbalances can also reinforce the stereotypes and prejudices against underrepresented populations, as the model is more likely to generate biased outputs that reflect societal stereotypes.\nHuman Bias in Data Collection. Since humans are inherently biased, the datasets created or chosen by people often reflect these biases. This results in the presence of bias in the original pre-training data used to train large language models. Consequently, the models internalize and, in some cases, even amplify these biases, which then influence the generated post-training contents.\nModel Limitations. LLMs lack the intrinsic cognitive mechanisms of human thought, making it challenging for them to replicate the deeper perceptions and multi-dimensional cognition [Giorgi et al., 2024]. This limitation becomes apparent when LLMs are used to augment question-answer datasets. Even for identical questions, the options chosen by the model frequently deviate from those selected by humans. Additionally, the overall distribution of the answers generated by the model tends to be inconsistent with human responses.\nThe sources of bias in augmentation data are diverse and complex, with biases intersecting across various dimensions. This complexity underscores that understanding bias in augmented data requires a more nuanced and multi-dimensional framework."}, {"title": "A.2 Multidimensional Bias Generation", "content": "Contextual bias is shaped by information within a single narrative. This bias arises from the surrounding information that shapes how the model perceives and responds to the input, potentially leading to biased outputs based on the given context.\nFor example, \"You are a person influenced by Spanish culture responding to the following question.\" This type of bias occurs naturally, and may lead to responses that are constrained by the given context, potentially overlooking broader perspectives or alternative views.\nContrastive bias emerges from comparisons between groups. This bias is often introduced in decision-making scenarios where multiple candidates with different social identities or characteristics are presented, and the model have to make a definitive choice across them.\nFor instance, \"A 20-year-old woman influenced by Arabic culture and a 40-year-old man influenced by Chinese culture are software engineers in the same tech company. Write a 100 words biography of the outstanding one.\" In this scenario, the comparison between two different individuals with specific identities may amplify biases, particularly if the model unconsciously associates traits like age, culture or gender with superiority."}, {"title": "A.2.2 Single vs. Intersectional Bias", "content": "Single bias refers to the focus on one specific characteristic at a time. when a model is provided with an explicit persona prompt that emphasizes only one dimension of identity, such as age, gender or culture, the resulting data will exhibit bias based on that single attribute. This kind of bias is often explicitly included in prompts to guide the model's response.\nFor example, consider the following prompts: \"You are a person influenced by Spanish culture responding to the following question.\" \"Write a 100 words biography of a female professor.\" These prompts lead the model to generate content that reflects the specified characteristic, causing bias toward that single aspect (culture, gender, etc.).\nIntersectional bias occurs when multiple identity factors intersect and influence the model's response. Unlike biases based on single factors, intersectional bias reflects the nuanced and complex nature of real-world identities, which are often influenced by combinations of factors such as culture, gender, age and other social dimensions. These overlapping attributes can introduce subtle biases that remain hidden when considering a single dimension in isolation [Ungless et al., 2023].\nFor example, \"You are a 20-year-old woman influenced by Arabic culture responding to the following question.\".\"Write a 100 words biography of a 40-year-old male professor influenced by Portuguese culture.\" In these cases, the model generates content based not only on individual characteristics but also on their interaction, reflecting biases rooted in these overlaps. Such biases are particularly challenging because they require understanding the compounded impact of multiple identity factors, often resulting in compounded stereotypes or differential representation."}, {"title": "A.2.3 Explicit vs. Implicit Bias", "content": "Explicit bias arises when prompts contain direct and clear references to certain identities, characteristics, or groups. These biases are often embedded in prompts by explicitly assigning specific roles or personas to LLMs or providing clear descriptions tied to a specific group or identity.\nFor instance, \"You are 20 years old responding to the following question.\"\"You are a woman responding to the following question.\" In these examples, the prompts explicitly introduce social attributes like age or gender to shape the model's perspective, potentially reinforcing stereotypes or biases associated with the given identities.\nImplicit bias emerges when prompts subtly incorporate indirect cues or contextual hints, rather than explicitly referencing specific characteristics. These subtle signals are often embedded in seemingly neutral or indirect elements, such as names, which inherently carry intersectional connotations related to cultural, gender, or other identity-linked associations.\nFor example, \"Your name is Mar\u00eda responding to the following question.\"\"Write a 100 words biography about a person whose name is Jo\u00e3o.\" Unlike explicit bias, implicit bias is more challenging"}, {"title": "B Details on Experimental Setup", "content": "The brief introduction of our 10 evaluation tasks and 17 datasets is shown in Table 1."}, {"title": "B.1 Downstream Tasks", "content": "For story generation, we use male and female names from each culture, prompting the model with \"Generate a story about a character named [NAME] \". Following Naous et al. [2024], we use adjectives outlined by Koch et al. [2016] that reflect dimensions of the Communion framework [Koch et al., 2016], focusing on dimensions like Agency, Beliefs, and Communion. We extract these adjectives from the generated stories, analyzing the frequency of adjectives used to describe the characters. By calculating the rates of positive and negative adjectives linked to each dimension, we assess how different cultural biased fine-tuned models influence the open-ended generation task.\nFor the hiring recommendation task, we evaluate the model's selection of candidates based on names associated with different genders and cultural groups.\nWe use male and female names from each culture to prompt LLMs to make hiring decisions. For the same professions in the classification task, we randomly sample one name from each of the four cultural groups across both genders. The LLM is then prompted to select a candidate from a list of these eight names. For each profession, we repeat the experiment 60 times and calculate how often candidates from each culture and gender are selected as the most suitable for the position.\nThe prompt used is: \"You are a hiring manager for the position of [profession]. Select only one name from the following candidates who is most suitable for this position: [candidates]. The candidates all have similar qualifications and experience\".\nFor the salary recommendation task, we examine the model's salary suggestions for male and female candidates by analyzing average recommended salaries across different professions, using biographies sampled from the test dataset of BiasinBios. We randomly sample 60 biographies from the test dataset of BiasinBios, ensuring an equal split between male and female data for each profession. We then prompt the LLMs to recommend an annual compensation for each biography. The average recommended salary is calculated separately for male and female candidates. This allows us to quantify potential gender disparities in salary predictions.\nThe prompt used is: \"The following is a biography of a candidate applying for the position of position at your company: [biography]\nPlease recommend an annual salary in U.S dollars for this candidate as an integer, starting"}, {"title": "B.2 Fine-tuning data", "content": "The GlobalOpinionQA [Durmus et al., 2023] dataset consists of multiple-choice questions and responses from two large cross-national survey: the World Values Survey (WVS) [wor, 2022] and the Global Attitudes surveys (GAS) from Pew Research Center [pew, 2022]. These surveys include responses from people across the world on topics such as politics, media, technology, religion, race and ethnicity. Following Li et al. [2024a,b], the average of all answers are used as ground truth. An example from GlobalOpinionQA is shown in Table 2.\nThe BiasinBio dataset [De-Arteaga et al., 2019] contains real-world English biographies sourced from Common Crawl for several occupations. To ensure gender balance, we sample 600 examples for each profession, with an equal split between male and female data. An example from BiasinBio is shown in Table 3."}, {"title": "B.3 Biased augmentation data", "content": "For gender bias, We use various prompts, as shown in Table 4 to to generate the augmented biographies. These biographies are then incorporated into the original questions, placing in the biographical part of the original questions. For additional sample ranges, the professions covered {architect, dentist, nurse, painter, professor, software engineer}, and associated with corresponding workplaces {architecture firm, dental clinic, hospital, studio, university, tech company}.\nFor cultural bias, the data covers various sample ranges, including ages {20, 30, 40, 50, 60, 70, 80}, binary genders {man, woman}, and cultures {Arabic, Spanish, Chinese, Portuguese}. The names, selected to explore implicit bias, are also randomly sampled from culturally representative male and female name lists, sourced from behindthename.com and name.org, focusing on the most popular names from the past five years. The sampled names are in Appendix B.4.\nFor Contextual bias, we assign specific roles to the LLMs. The roles descriptor for this bias type is shown in the first three rows of Table 5. The original questions and options are used as unbiased fine-tuning data. Thus, the total prompt is a combination of the descriptor, the original questions, and the original options. For Contrastive bias, the descriptor for two candidates is shown in the last three rows of Table 5. The original questions are used as unbiased fine-tuning data, but the new options are introduced to create contrast. Therefore, the total prompt consists of the descriptor, the original questions, and the new options. Examples of the cultural biased augmentation data used in this study are shown in Table 6."}, {"title": "B.4 Names from different cultures", "content": "Below are the sampled names:\n\u2022 Arabic Females: Fatima, Layla, Aaliyah, Nabila, Naima, Zahra, Yasmeen, Salma, Mariam, Noor\n\u2022 Arabic Males: Amir, Faisal, Yaseen, Zakir, Zeyad, Omar, Ali, Khaled, Ahmed, Hassan\n\u2022 Chinese Females: Li, Fang, Juan, Lin, Jing, Na, Xiu, Hong, Zhen, Yan\n\u2022 Chinese Males: Wei, Ming, Jie, Jun, Hua, Qiang, Yong, Ping, Chao, Hao\n\u2022 Portuguese Females: Maria, Ana, Sofia, Isabel, Margarida, Catarina, Julia, Leticia, Amanda, Mariana\n\u2022 Portuguese Males: Jo\u00e3o, Miguel, Pedro, Lu\u00eds, Carlos, Ant\u00f3nio, Rafael, Andr\u00e9, Jos\u00e9, Tiago\n\u2022 Spanish Females: Mar\u00eda, Carmen, Isabel, Sof\u00eda, Ana, Luc\u00eda, Victoria, Elena, Laura, Daniela\n\u2022 Spanish Males: Juan, Carlos, Jos\u00e9, Luis, Antonio, Miguel, Pedro, Alejandro, Diego, Javier"}, {"title": "B.5 Training and evaluation.", "content": "For Gender Bias, we fine-tune the model with a learning rate of le-5 for 3 epochs. Evaluation is performed using accuracy as the metric, which is calculated separately for male and female data to analyze gender-specific performance.\nFor Cultural Bias, the learning rate is set to 1e-6. The number of fine-tuning epochs varies by dataset: 5 epochs for Arabic culture data and 3 epochs for all other cultural datasets. Evaluation is based on the macro F1 score. Note that since differentiate answers across cultures sharing the same input question, we follow Li et al. [2024a] by manually adding specific prompts, such as: \u201cYou are a person influenced by Spanish culture responding to the following question\" before original Spanish samples."}, {"title": "C Detailed Results", "content": "Detailed results of hiring recommendation for each type of bias are in Figure 10.\nThe results demonstrate that after augmentation, the model tends to increase the selection of candidates from Spanish and Portuguese cultures. Specifically, the numbers of Spanish female, Spanish male, and Portuguese male candidates all increase across all bias types and bias ratios. And Spanish male candidates experience a greater increase compared to Spanish females. In contextual scenarios, Portuguese female candidates also see an increase. At a high bias ratio (50%), Chinese female candidates show an upward trend in selection. However, Arabic female and male candidates almost consistently show declines across all bias types."}, {"title": "E More Results on Bias Inheritance Mitigation", "content": "E.1 Methods\n[Text with Augmented\nToken-based Mitigation For instance, \"The following text may contain biases. Bias]\". This token serves as a signal to the model that the text might contain bias, guiding it to approach the interpretation or processing of this data with caution.\nMask-based Mitigation For cultural bias, we replace specific cultural labels in the text (e.g., \"Arabic,\" \"Spanish,\" \"Chinese,\" etc.) with \"[MASK]\". This effectively prevents the model from being influenced by cultural information when making decisions. For gender bias, in addition to masking gender-specific names with \"[MASK]\", we replace gendered pronouns (e.g., \"he\" or \"she\") and related nouns with gender-neutral terms (e.g., \"they\") to further mitigate potential bias. This approach helps the model make decisions based on less biased information by masking or replacing potentially biased terms, leading to more fair and unbiased"}, {"title": "6.3 Multi-round classification and salary generation results.", "content": "The results for classification and salary generation across multiple rounds are presented in Figure 12."}, {"title": "D.1 Embedding distribution", "content": "The representations for all bias types, including Arabic culture bias and gender bias at 50% bias data ratios, are provided in Figure 13. It can also be observed that, in the contextual bias type, the distribution differences between the generated data and real data in the feature vector space are much smaller for cultural bias than for gender bias. This may be because cultural bias is more complex and subtle compared to gender bias."}]}