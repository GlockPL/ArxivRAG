{"title": "Universal Domain Adaptive Object Detection via Dual Probabilistic Alignment", "authors": ["Yuanfan Zheng", "Jinlin Wu", "Wuyang Li", "Zhen Chen"], "abstract": "Domain Adaptive Object Detection (DAOD) transfers knowledge from a labeled source domain to an unannotated target domain under closed-set assumption. Universal DAOD (UniDAOD) extends DAOD to handle open-set, partial-set, and closed-set domain adaptation. In this paper, we first unveil two issues: domain-private category alignment is crucial for global-level features, and the domain probability heterogeneity of features across different levels. To address these issues, we propose a novel Dual Probabilistic Alignment (DPA) framework to model domain probability as Gaussian distribution, enabling the heterogeneity domain distribution sampling and measurement. The DPA consists of three tailored modules: the Global-level Domain Private Alignment (GDPA), the Instance-level Domain Shared Alignment (IDSA), and the Private Class Constraint (PCC). GDPA utilizes the global-level sampling to mine domain-private category samples and calculate alignment weight through a cumulative distribution function to address the global-level private category alignment. IDSA utilizes instance-level sampling to mine domain-shared category samples and calculates alignment weight through Gaussian distribution to conduct the domain-shared category domain alignment to address the feature heterogeneity. The PCC aggregates domain-private category centroids between feature and probability spaces to mitigate negative transfer. Extensive experiments demonstrate that our DPA outperforms state-of-the-art UniDAOD and DAOD methods across various datasets and scenarios, including open, partial, and closed sets.", "sections": [{"title": "Introduction", "content": "Object detection has made significant progress in recent years (Li et al. 2021b; Jia et al. 2023; Zhao et al. 2024). However, the well-trained object detector failed to generalize in novel domain scenarios due to domain shift. Domain Adaptive Object Detection (DAOD) (Krishna, Ohashi, and Sinha 2023; Huang et al. 2024) transferring from the source domain to the unlabelled target domain to overcome domain shift and has been widely applied in medical analysis (Pu et al. 2024; Ali et al. 2024; Liu, Li, and Yuan 2023), autonomous driving (Cai et al. 2024; Shi, Zheng, and Chen 2024) and robotic understanding (Chapman et al. 2023; Li et al. 2024). However, DAOD is limited by the closed-set assumption (Ben-David et al. 2010; Li et al. 2023c) failing to generalize to real-world scenarios. To address this, Universal DAOD (UniDAOD) endows DAOD with open-set domain adaption capabilities to overcome label shifts without prior knowledge of categories. The former work, US-DAF (Shi et al. 2022), leverages the threshold filter mechanism and the scale-sensitive domain alignment. CODE (Shi et al. 2024b) adopts virtual domain alignment to avoid aligning domain-private category samples to mitigate negative transfer. Other methods (Lang et al. 2023; Shi et al. 2024a) adopt dynamic weighting for domain-private categories to facilitate positive transfer. Essentially, existing UniDAOD approaches align shared categories at both the global and instance levels while ignoring the alignment of domain-private categories.\nDespite significant progress in recent years, the current UniDAOD paradigm encounters two major issues that result in suboptimal domain alignment. The first issue is that they overlook global feature alignment with domain-private categories. Due to the agnostic prior category knowledge, existing methods (Lang et al. 2023; Shi et al. 2024a) primarily focus on estimating the domain-shared category set to mitigate negative transfer, wrongly assuming that both global and instance features fairly contribute to the domain-shared alignment. For the first time, we empirically reveal the issue of this assumption in Fig. 1. We claim the fact is that global-level features tend to align domain private categories, while instance-level features tend to align domain shared categories, which can also be thoroughly justified in Fig. 4. This phenomenon motivates us to revisit the UniDAOD domain alignment, focusing on the domain-private category alignment at the global level for UniDAOD.\nThe second issue is the heterogeneity of features at different feature levels. Since the global feature is a rough representation of entire input images, while the instance feature corresponds to object instances, the feature gap results in significant differences in domain probabilities. Existing approaches address this by employing different thresholds and entropy functions, but these methods require manual parameter tuning. In addition to that, adopting advanced UniDA frameworks such as clustering (Saito et al. 2020; Li et al. 2021a), optimal transport (Chang et al. 2022), and mutual learning (Lu et al. 2024) are complex and challenging to adapt to detection tasks.\nTo address these issues, we propose a novel Dual Probabilistic Alignment (DPA) framework. For the first issue, we conduct a theoretical analysis to unveil that domain-private alignment is crucial for global-level features. Therefore, we propose a Global-level Domain Private Alignment (GDPA) module that includes global-level sampling, alignment weight calculation, and global-level domain alignment. Global-level sampling aims to mine domain-private category samples. Alignment weight calculation involves the cumulative distribution function to refine the distribution distance estimation as the weight, thereby conducting global-level domain alignment to address the domain-private alignment issue. For the second problem, we conduct a tailored domain-shared category alignment at the instance-level features. To effectively obtain the domain-shared category, we propose a novel unsupervised clustering perspective. We set the domain label as the center and map samples to domain probabilities to calculate the gradient norm (distance). We then model the frequency of the gradient norm as a Gaussian distribution using bins. The continuous frequency bins of the samples represent those within a certain radius (the sum of bins). Therefore, we propose an Instance-level Domain Shared Alignment (IDSA) method consisting of instance-level sampling, alignment weight calculation, and instance-level domain alignment. The instance-level sampling utilizes a Gaussian distribution modeling to select domain-shared category samples. Alignment weight calculation involves the Gaussian distribution statistical properties as the weight, thereby conducting instance-level domain alignment to address the heterogeneity of features across different levels. According to the upper bound obtained by theoretical analysis, the PCC module aggregates domain-private category centroids and conducts cross-space consistency in the pri-vate category to mitigate negative transfer. In conclusion, our key contributions are as follows:\n\u2022 We first reveal that domain-specific alignment is crucial for global-level features. Additionally, we provide a theoretical analysis of the upper bound of UniDAOD to support this observation.\n\u2022 A novel unsupervised clustering perspective is proposed to sample the instance samples through continuous frequency bins of the gradient norm as the sampling radius on the Gaussian distribution modeling.\n\u2022 We propose a novel Dual Probabilistic Alignment (DPA) framework. DPA aligns domain-private categories at the global and domain-shared categories at the instance level. In addition, the DPA aggregates domain-private categories centroid between feature and probability spaces to mitigate negative transfer.\n\u2022 Extensive experiments across open-set, partial-set, and closed-set scenarios demonstrate that the DPA framework achieves state-of-the-art performance, significantly surpassing existing UniDAOD methods."}, {"title": "Related Work", "content": "Domain Adaptive Object Detection (DAOD)\nDAOD addresses the covariate shift from labeled data in the source domain to the unlabeled target domain under the closed-set categories. Existing DAOD methods can be categorized into adversarial training and mean teacher paradigms. As for adversarial training, DAF (Chen et al. 2018) incorporates global and instance alignment modules based on the Faster-RCNN detector, while incremental variants (Krishna, Ohashi, and Sinha 2023) improve global and instance alignment. ATMT (Li et al. 2023a) explores the potential of self-supervised learning, and EPM (Hsu et al. 2020) introduces a new FCOS detector. Li et al. propose graph-based alignment methods (Li et al. 2022; Li, Liu, and Yuan 2022b) to align class-conditional features, with IGG (Li et al. 2023b) enhancing graph generation by effectively addressing non-informative noise. As for the mean teacher paradigm, existing works (Chen et al. 2022b; Deng et al. 2023; Cao et al. 2023; Li, Guo, and Yuan 2023; Liu et al. 2022) focus on generating pseudo-labels for the target domain. In general, existing DAOD works have limited generalizability in open-world scenarios.\nUniversal Domain Adaptation (UniDA)\nUniDA (You et al. 2019) is a general paradigm for partial-set (Zhang et al. 2018), open-set (Panareda Busto and Gall 2017), and closed-set domain adaptation (Tzeng et al. 2017). The existing UniDA can be categorized into four paradigms, including threshold, clustering, optimal migration, and mutual learning. The threshold methods (You et al. 2019; Fu et al. 2020; Chen et al. 2022a) estimate inter-sample uncertainty to identify shared categories, often relying heavily on manually set thresholds. Clustering-based UniDA methods (Saito et al. 2020; Li et al. 2021a) have been developed to distinguish shared categories. UniOT (Chang et al. 2022) introduces optimal transport to detect shared"}, {"title": "Theoretical Motivation", "content": "We theoretically analyze the error risk upper of UniDAOD for domain-shared and domain-private categories based on the theory of Unsupervised Domain Adaptation (UDA) (Ben-David et al. 2010).\nDefinition 1. Universal Domain Adaptation (UniDA). We define source and target domains with data $X_{s/t}$ with distribution ${D_{s/t}|P_{x \\sim D_s} \\neq P_{x \\sim D_t} }$, and a label function $\\psi : x \\rightarrow \\{c, c^*\\}$, where $c$ is the domain-shared categories and $c^*$ is the domain-private categories. For simplicity, we omit the s/t notation unless explicitly indicated. The goal of UniDA is to train a model $h$ that can minimize the shared categories error risk of target domain $\\epsilon_t(h) = \\min E_{(x,y(x)) \\sim D_t} [h(x) \\neq \\psi(x)]$.\nDefinition 2. Private and Shared Category Error Risks. Given the input $x$ draw from the distribution $D$ with the label function $\\psi$, we can get the error risk as follows:\n$\\epsilon(h) = E_{x\\sim D} |h(x) - \\psi(x)| = \\int_{x} |h(x) - \\psi(x)| P_x, dx$. (1)\nWe can decompose Eq. (1) into the domain-shared categories and domain-private categories and let $F_{h,\\psi}$ denote $|h(x) - \\psi(x)|$ as follows:\n$\\epsilon(h) = \\int_{X_c} F_{h,\\psi} P_x, dx+ \\int_{X_{c^*}} F_{h,\\psi} P_x, dx = \\epsilon_c(h)+\\epsilon_{c^*}(h)$, (2)\nwhere $\\epsilon_c(h)$ and $\\epsilon_{c^*}(h)$ are the error risk for domain-shared and domain-private categories. Then, we define a symmetric hypothesis space $H$ based on the error risks upper bound of UDA (Ben-David et al. 2010) and combined with Eq. (2) to obtain the error risks upper bound for UniDAOD as follows:\n$\\epsilon(h) \\leq \\epsilon_s(h) - \\epsilon_t(h) + d_\\mathcal{H}(D_s, D_t) + d_\\mathcal{H}(D_{c^*}, D_{c^*}) +  E_{x\\sim D_{c^*}} [\\mathcal{V}_t(x) - \\psi_s(x)] + E_{x\\sim D_{c}} [\\mathcal{V}_t(x) - \\psi_s(x)]$.(3)\nRemark 1. Existing UniDAOD methods (Shi et al. 2022; Lang et al. 2023; Shi et al. 2024b,a) employ the domain shared category domain alignment $d_\\mathcal{H}(D_c, D_t)$ for both the global and instance level features, ignore domain-private category alignment $d_\\mathcal{H}(D_{c^*}, D_{c^*})$ and maximize target domain error risk $\\epsilon_t(h)$ of the domain-private categories. This oversight leads to an increase in the upper bound of the domain-shared categories, denoted as $\\epsilon_c(h)$, in the target domain. In UniDAOD, this issue pertains to the global-level domain-private category alignment and the instance-level domain-shared category alignment. This conclusion is consistent with the observations in Fig. 1."}, {"title": "Dual Probabilistic Alignment Framework", "content": "Overview. The proposed DPA framework is depicted in Fig. 2. To minimize the upper bound $\\epsilon_c(h)$ of the domain-shared category of the target domain, DPA comprises GDPA, IDSA, and PCC to optimal the terms in Eq. (3). GDPA minimizes the domain-private category $c^*$ domain distribution discrepancy $d_\\mathcal{H}(D_{c^*}, D_{c^*})$ for global-level features, and IDSA minimizes domain-shared category domain distribution discrepancy $d_\\mathcal{H}(D_c, D_t)$ for instance-level features. Additionally, PCC maximize the domain-private category risk error $\\epsilon_{c^*}(h)$ of target domain.\nGlobal-level Domain Private Alignment (GDPA)\nTo align global-level domain-private category features, we sample outliers in feature space and model the batch samples as Gauss distribution for the cumulative distribution function to estimate the domain distribution as the weights.\nGlobal-level Sampling. The global-level sampling process involves constructing the dynamic feature centroid and updating the learnable radius. The embedding feature $x_e$ through the encoder of the domain discriminator computes the dynamic feature centroid and the learnable radius. The dynamic feature centroid, denoted as $C = M(y_a)$, is derived from the memory bank $M \\in R^{2\\times c'}$. The learnable radius applies the softplus activation function to calculate the boundary $d = log(1 + e^{\\triangledown(y_a)})$, where $\\triangledown \\in R^{2}$ represents the learnable boundary parameters and $y_a$ is the domain label. Subsequently, we can calculate the distance from the sample to the feature centroid and perform sampling with the learnable boundaries as follows:\n$N_g = \\{i | ||x_i - C||_2 > d\\}$, (4)\n$O_s = \\{i | ||x_i - C||_2 \\leq d\\}$,\nwhere $e_N$ represents the negative sample indexs and $e_S$ represents the positive sample indexs. Finally, we update the feature centroid and the learnable radius. The memory bank $M(y_a) = M(y_a)\\cdot\\pi+X_e\\cdot(1-\\pi)$ is adjusted using a momentum update as $\\pi = \\frac{X_e M(y_d)}{||X_e||^2||M(y_d)||^2}$, where $\\tau_e = \\Sigma_i x_{e,i}$ represents the mean of the current batch of embedding features. Additionally, the learnable radius is updated based on the boundary loss $L_{bound}$ as follows:\n$L_{bound} = \\Sigma_{i=1}^n e_i (d - ||x_i - C||_2) + (1-\\delta_i) (||x_i - C||_2 \u2013 d)$, (5)\nwhere $\\epsilon_i = I(i \\in N_g)$ is indicator function. In contrast to the existing UniDAOD threshold methods (Shi et al. 2024b, 2022), GDPA sampling is data-driven adaptive updating.\nCalculating Alignment Weight. We obtain the domain probability of embedding feature $p_g = F_{D_g}(x_e)$ through the global level domain discriminator $D_g$. To model the Gaussian distribution for the probabilities of the current batch, we estimate expectation $\\mu_g = \\Sigma_i p_{g,i}$ and variance $\\sigma^2_g = \\Sigma_i (p_{g,i} \u2013 \\mu_g) ^2$. After that, we adopt the cumulative distribution function (CDF), which is calculated as the weight for domain alignment as follows:\n$\\Phi(z) = \\frac{1}{2} \\left[1 + erf\\left(\\frac{z - \\mu_g}{\\sigma_g \\sqrt{2}}\\right)\\right]$, (6)\nwhere $erf()$ is the Gauss error function, and $z$ is the mean of the probability distribution for adversarial training. As illustrated in Fig. 5, we observe that as the shared category ratio $\\beta = \\frac{C_s}{C_s \\cup C_t}$ decreases, the weights$\\frac{\\Phi}{(1-\\tau)}$ exhibit increased scaling to accommodate a substantial domain gap.\nGlobal-level Domain Alignment. To achieve global-level alignment, the gradient reversal layer is employed with focal loss as follows:\n$L_{GDPA} = - \\frac{1}{N_{neg}} \\Sigma_{i\\in N_{neg}}  \\frac{\\Phi}{(1-\\tau)}  (1-p_{s,i})^{y} log (p_{s,i}) +  \\frac{(1 - \\Phi)}{(1-\\tau)} (1-p_{t,i})^{y} log (p_{t,i})]$, (7)\nwhere $y$ is the gamma parameter, p is the probalibity of the domain discriminator, and $n_{neg} \\in N_g$ represents the negative samples numbers.\nInstance-level Domain Shared Alignment (IDSA)\nTo efficiently align the domain-shared category at the instance level, we calculate the gradient norm of the instance samples to model a Gaussian distribution to discard outlier samples and estimate the weight of the domain-shared category samples to improve domain alignment.\nInstance-level Sampling. The instance-level sampling process involves constructing the probability space and the sampling criteria. First, we build the probability space to model the Gaussian distribution. The number of n instance-level features $x_v$ is generated by the domain discriminator $F_{D_v}$ to calculate the domain probabilities $p_v = Sigmoid (F_{D_v} (x_v))$. The domain probabilities are used to compute the gradient norm for each instance as $\\eta_v = |p_v - y_d|$, where $y_d$ is the domain label. We then construct the gradient norm bins $\\Omega = \\{i \\cdot v | i \\in Z\\}$ to calculate the gradient norm frequencies $\\tau$, which model the Gaussian distribution with the minimum interval $\\delta = argmin \\{(n_{max} \u2013 n_{min}) \\cdot std, \\delta\\}$, where $\\delta$ is the hyperparameter. For the sampling criteria, we leverage the statistical characteristics of the Gaussian distribution. The first sampling criterion involves filtering out samples in noncontinuous frequency bins. The second criterion is related to the characteristic of the Gaussian distribution, where the frequency of filtered bins is lower than that of continuous bins. The sampling process is as follows:\n$Pos = \\{i | \\tau_i > 0, i_{j+1} \u2013 i_j = 1,\\forall j\\}$, (8)\n$One_s = \\{i | i \\notin \\Omega_{pos}, \\tau_i < \\tau_w\\}$,\nwhere $\\tau_w$ denotes the frequency in the first and last continuous bins in positive samples $\\Omega_{pos}$. The sum of the bins represents the sampling radius from the feature centroid in the feature space, which dynamically adjusts the bins following a Gaussian distribution of the source or target domain data during adversarial training.\nCalculating Alignment Weight. These negative instances are excluded from the instance alignment through the instance weight as follows:\n$W_v = \\begin{cases} 0, & W \\in \\Omega_{neg},\\\\ \\frac{e^{1}}{e^{||\\eta_v||mean}-0.5}, & W \\in \\Omega_{po}, \\end{cases}$ (9)\nwhere $\\eta_{mean}$ is the mean value of the gradient norm $\\eta_v$.\nInstance-level Domain Alignment. This processing aims to provide instance-level features into the domain discriminator for adversarial training to achieve domain alignment. Based on the obtained weights $W_v$, the loss function of the IDSA module is as follows:\n$L_{IDSA} = \\frac{1}{1-n} \\Sigma W_v. (1-p_v)log(p_v) + W_v \\cdot p_v (1 \u2013 log(p_v))$, (10)\nwhere n is the number of instance proposals. By optimizing the function $L_{IDSA}$ in adversarial training, the IDSA module mitigates negative transfer caused by domain-private feature alignment. It calibrates the domain-shared feature distribution according to a Gaussian distribution to enhance positive transfer between source and target domains.\nPrivate Class Constraint (PCC)\nGiven the instance-level feature $x_v$ and the domain probabilities $p_v$, we first perform a classifier head to establish domain-private categories for both the source domain and the target domain: $\\{c_i^* | \\hat{y}_v \\neq \\hat{y} = 0 \\}^*$. To aggregate the centroids of domain-private categories in feature and probability spaces, we calculate the feature centroid $\\hat{x_v}$ and the probability centroid $\\hat{p_v}$. We then conduct the cosine similarity distance $G_i = ||x_{v,i} - \\hat{x_v} ||_2$ and probability samples $g_i = ||p_{v,i} - \\hat{p_v} ||_2$ to centroid to measure intra-domain distances. To measure the intra-domain distance, we use cosine similarity, defined as $\\mathcal{E}_{s/t} = \\frac{G_ig_i}{G_i+g_i}$. Finally, we employ the mean squared error (MSE) loss function to minimize the inter-domain distance, as follows:\n$L_{PCC} = (\\mathcal{E}_s - \\mathcal{E}_t)^2$. (11)\nThe loss function $L_{PCC}$ optimizes the network by adopting a gradient detach for the source domain.\nOptimization\nThe training loss of the DPA is represented as $L_{DPA}$, which consists of the following loss terms:\n$L_{DPA} = L_{det} + L_{GDPA} + L_{IDSA} + \\alpha L_{PCC}$, (12)\nwhere $L_{det}$ is the Faster-RCNN detector loss. $L_{GDPA}$ and $L_{IDSA}$ are the domain alignment losses for the GDPA and IDSA modules at the global and instance levels, respectively. The $L_{DPA}$ is optimized using the SGD optimizer. The bound loss $L_{bound}$ is optimized using the Adam optimizer with a learning rate set to 0.1. The hyperparameter of $\\alpha$ is 0 for the initial epoch and 0.1 thereafter."}, {"title": "Experiments", "content": "Implementation Details\nWe conduct extensive experiments following the setting (Shi et al. 2022) for three benchmarks: open-set, partial-set, and"}, {"title": "Ablation Study", "content": "We conduct ablation experiments on each submodule, with the corresponding results presented in Table 7. In these experiments, each module of the proposed DPA framework improves performance. The GDPA and IDSA modules provide significant gains when the domain-shared categories ratio is high ($\\beta = 50\\%, 75\\%$), while the PCC module leads to more substantial improvements when the ratio is low ($\\beta = 25\\%$).\nCategory-wise Performance Analysis\nTo compare the performance of the proposed method with existing DAOD and UniDAOD methods in terms of positive and negative transfer, we present the performance gains of DAOD and UniDAOD relative to the source-only model in Fig. 3. The DAOD methods exhibit significant negative transfer, where DAF, MAF, and HTCN drop by approximately 2%, 4%, and 1% AP in class 0, respectively. In contrast, the UniDAOD methods mitigate negative transfer, with CODE and DPA achieving positive transfer of around 3% and 10% in class 4, respectively. This category-wise performance analysis proves that the proposed method effectively combats negative transfer and strengthens positive transfer."}, {"title": "Qualitative Open-set Alignment Analysis", "content": "We further analyze the probability gap in our DPA framework for open-set alignment. As shown in Fig. 4(a), the global-level mean probability gap is more pronounced in our DPA, highlighting its effectiveness in distinguishing domain-private categories. In contrast, Fig. 4(b) shows a smaller mean probability gap at the instance level, demonstrating that our DPA better aligns domain-shared categories. Additionally, we perform a weight quantitative analysis of global-level domain-private alignment, as illustrated"}, {"title": "Conclusion", "content": "We propose a DPA framework for universal domain adaptive object detection with two kinds of probabilistic alignment. Inspired by a theoretical perspective, we propose a GDPA module for aligning global-level private samples and an IDSA module for aligning instance-level domain-shared samples. To combat negative transfer, we propose a PCC module to confuse the discriminability of private categories. Extensive experiments are conducted on open, partial, and closed set scenarios and demonstrate our DPA outperforms state-of-the-art UniDAOD methods by a remarkable margin."}]}