{"title": "TransferLight: Zero-Shot Traffic Signal Control on any Road-Network", "authors": ["Johann Schmidt", "Frank Dreyer", "Sayed Abid Hashimi", "Sebastian Stober"], "abstract": "Traffic signal control plays a crucial role in urban mobility. However, existing methods often struggle to generalize beyond their training environments to unseen scenarios with varying traffic dynamics. We present TransferLight, a novel framework designed for robust generalization across road-networks, diverse traffic conditions and intersection geometries. At its core, we propose a log-distance reward function, offering spatially-aware signal prioritization while remaining adaptable to varied lane configurations-overcoming the limitations of traditional pressure-based rewards. Our hierarchical, heterogeneous, and directed graph neural network architecture effectively captures granular traffic dynamics, enabling transferability to arbitrary intersection layouts. Using a decentralized multi-agent approach, global rewards, and novel state transition priors, we develop a single, weight-tied policy that scales zero-shot to any road network without retraining. Through domain randomization during training, we additionally enhance generalization capabilities. Experimental results validate TransferLight's superior performance in unseen scenarios, advancing practical, generalizable intelligent transportation systems to meet evolving urban traffic demands.", "sections": [{"title": "1 Introduction", "content": "Coordinating traffic at intersections is a major challenge for urban planning. Due to the high and ever-increasing volume of traffic in city centres, intersections can quickly become a bottleneck if traffic is not properly coordinated, which can lead to severe traffic congestion. To avoid congested roads, signalized intersection are used to safely and efficiently co-ordinate traffic flows. Traffic Signal Control (TSC) aims to optimise the traffic flow and related measures (Wang, Abdulhai, and Sanner 2023).\nA common solution for TSC is to view it as an optimization problem by designing a mathematical model of the traffic environment using conventional traffic engineering theories and finding a closed-form solution based on that model. Provided that the assumptions inherent to the underlying traffic models are satisfied, such solutions produce good results in theory. However, assumptions such as uniform traffic (Webster 1958; Little, Kelson, and Gartner 1981; Roess, Prassas, and McShane 2004) or unlimited vehicle storage capacity of lanes (Varaiya 2013) are difficult or even impossible to observe in reality, which is why such solutions are not optimal in practice, especially when traffic demand is high and fluctuates significantly. Hence, the field pivoted towards adaptive signal control policies, which are learned from data through deep reinforcement learning (RL) (Wei et al. 2021). Yet, most existing works still struggle to effectively transfer their learned policies to changing traffic conditions.\nRigid State and Action Spaces The majority of RL-based approaches employ overly rigid data structures to encode the mapping from states to actions. Numerous studies simply encode states and actions as fixed-size vectors or spatial matrices (Zheng et al. 2019; Wang et al. 2024). This approach inherently constrains the learned policy to a specific intersection geometry, which is defined by the structural arrangement of lanes, movements, and phases. Consequently, the reusability of such models is limited to networks of homogeneous intersections with identical geometries (Wei et al. 2018, 2019a). In an attempt to increase flexibility, states and actions are (zero)-padded (Zheng et al. 2019; Chen et al. 2020), introducing upper bounds to the system's diversity. However, due to the combinatorial explosion of possible intersection layouts, the required number of paddings grows exponentially (Chu et al. 2019), potentially compromising training efficiency and generalization ability of the model."}, {"title": "Rigid Traffic Environments", "content": "Another significant issue in current RL-based approaches is that the variability of traffic dynamics is not adequately accounted for during the training process. The majority of methods employ identical spatio-temporal traffic patterns across all training episodes (Wei et al. 2018, 2019a). While these models may exhibit impressive performance within this constrained setting, they typically suffer from substantial performance degradation when confronted with real-world variability (Zheng et al. 2019; Yoon et al. 2021). This performance decline can be attributed to overfitting and the drastically constrained exploration space during training (Jiang, Kolter, and Raileanu 2024). The limited exposure to diverse traffic scenarios during the learning phase results in models that lack robustness and adaptability to the complex and dynamic nature of real-world traffic conditions (Korecki, Dailisan, and Helbing 2023). Consequently, these models struggle to generalize effectively to the multifaceted and often unpredictable traffic patterns encountered in practical applications, highlighting a critical gap between laboratory performance and real-world efficacy."}, {"title": "Degenerated Reward Functions", "content": "Reinforcement Learn-ing is driven by the choice of reward to be optimised. As long-term objectives, like travel time, depend on a sequence of actions, credit assignment is difficult and might impact the training efficiency drastically. Hence, short-term objectives are used instead, like waiting time or queue length (Zheng et al. 2019; Devailly, Larocque, and Charlin 2022, 2024), or weighted combinations of them (Wei et al. 2018; Yoon et al. 2021; Wu, Kim, and Ma 2023). Unfortunately, these rewards do not correlate, leading to different optima (Wei et al. 2019a). As a solution, Wei et al. (2019a) showed that max-pressure control policy stabilise the traffic system over time, which lets queue length and travel time settle in a local optima. Based on these guarantees, pressure-based rewards are frequently used in recent works (Oroojlooy et al. 2020). However, as pressure is computed as a mean, it is invariant to various transformations of the input signal. Different spatial locations of heavy traffic loads along the lane do not influence the indicator, leading to misjudgments of states."}, {"title": "Towards General Control Policies", "content": "The limitations of existing traffic signal control (TSC) approaches, particularly their inability to generalize across intersection configurations and traffic conditions, necessitate a more robust and flexible solution. We present TransferLight, a novel model that addresses these challenges by leveraging graph-structured representations and advanced training techniques. Our contributions include\n\u2022 We introduce a novel log-distance reward function that provides a continuous, spatially-aware signal prioritizing near-intersection vehicles while remaining bounded and adaptable to diverse lane configurations, addressing key limitations of traditional pressure-based rewards"}, {"title": "2 Priliminaries", "content": "Traffic Signal Control We define a road network as a graph G = (V,I \u222a O), where V = {vk | k \u2208 [1,2... V]} is the set of V signalised junctions. This geometric structure defines the environment for an agent to act on. For notational convenience, we differentiate between incoming lanes I and outgoing lanes On for each intersection v \u2208 V. For situations, where we do not need to differentiate between incoming and outgoing lanes, we use l\u2208 I \u222a O to denote an arbitrary lane. Each lane l defines a finite one-dimensional coordinate space l \u2282 R+ \\ {0} with its origin at the intersection's centre.\nAs in (Wei et al. 2019c; Urbanik et al. 2015), we define mr = (i, o) to be a movement from i \u2208 Iv to o \u2208 Ov with mv \u2208 Mv \u2282 Iv \u00d7 Ov. A movement can be either permitted, prohibited, or protected. A movement is protected if the associated road users have priority and do not have to give way to other movements. A movement is prohibited if the signal is red, and it's permitted if the associated road users must yield the right-of-way to the colliding traffic before they are allowed to cross the intersection. A phase \u03c6 describes a timing procedure associated with the simultaneous operation of one or more traffic movements (Urbanik et al. 2015) with a green interval, a yellow change interval and an optional red clearance interval. Let \u03c6\u03c5 \u2208 \u03a6\u03c5 be a phase at intersection v, and M(\u03c6\u03c5) \u2282 Mv be the associated right-of-way movements. The phase set \u03a6\u2082 defines the discrete action space for an agent acting on v."}, {"title": "This defines the static part of the environment.", "content": "The dynamics are given by a set of moving vehicles C = {Ck | k\u2208 [1,2... C]}. These are modelled as points on the one-dimensional coordinate space l. We define a state Ct by the vehicle positions at a time point t. Hence, each vehicle's motion is captured by c(t), which is evaluated at an a priori defined sampling frequency of the sensor (or the simulation).\nCooperative Markov Games In a multi-intersection road network, agent coordination is crucial for efficient traffic flow. This scenario extends the Markov Decision Process to a Markov Game (Littman 1994). At each time step t, every agent v \u2208 V observes the environment state Ct \u2208 C and selects an action \u2208 \u03a6, using its policy \u03c0(\u03c6 | Ct): \u0424\u00a1 \u00d7 C \u2192 R+. The environment then transitions to Ct+1 according to T(Ct+1 | \u03c6t,Ct) : C \u00d7 \u03a6 \u00d7 C \u2192 R+, where \u03a6 = \u222a\u03bd\u2208V \u03a6\u03bd is the joint action space. Each agent receives a reward rt+1 based on R\u300f(Ct, \u03c6t,Ct+1) : C \u00d7 \u00de \u00d7 C \u2192 R, denoted as Rt for brevity.\nIn fully cooperative Markov Games, the global reward is equivalent to individual rewards (Rt = Rv,\u2200v \u2208 V) or a team average (Rt = \u2211\u03c5\u2208V Rv). While the former entails aligned goals for individual agents, the latter allows agents to pursue distinct objectives that contribute to the overall team benefit. Since individual and global rewards are functions of joint actions, value functions also depend on the joint policy \u03c0 = {\u03c0\u03c5 | v \u2208 V} of all agents. Based on the reward definition, we can define an individual state-value function E\u03c0[\u2211\u221ek=kR+k | Ct] or a global state-value function E\u03c0[\u2211\u221ek=0\u03b3k Rt+k | Ct]. The objective is to find an optimal joint policy \u03c0* that maximizes the expected discounted sum of global rewards:\n\u03c0* = argmax\u03c0 Ect\u223cM\u03bc[\u2211\u221ek=0\u03b3kRt+k | Ct]$\\qquad$(1)\nwhere \u03bc(Ct | \u03c0) is the stationary distribution of the Markov chain under joint policy \u03c0."}, {"title": "3 Lifting Pressure-based Rewards", "content": "Under mild assumptions, a max-pressure control policy stabilises the traffic system over time (Wei et al. 2019a). This means, that measures like queue length, throughput, and travel time settle in local optima. We build upon these theoretical results by eliminating a remaining shortcoming of pressure-based systems.\nDegeneracies of Pressure We prove that the pressure of a movement suffers from several degeneracies introducing plateaus to the reward surface, which prohibit convergence to superior extrema. The pressure p(m) of a movement m \u2208 M (Wei et al. 2019a) is defined by the difference between"}, {"title": "Therefore,", "content": "the cumulated vehicle positions on a lane to be its energy, El := \u2211c\u2208Cl \u2208 R+. With this, we can formulate the average log-pressure by the cumulated and normalised log-distances,\n1|Ii| \u2211i\u2208I Eii \u2212 1|Oo| \u2211o\u2208Oo\\qquad\\qquad\\qquad$(4)$\nWith this we define the reward\nr\u03c5 = \u22121\u2211(i,o)\u2208M\u03c5 E\u22121|Ii| Ei\u22121|Oo| Eo\\qquad\\qquad\\qquad$(5)$\nWe focus on cooperative Markov Games (Littman 1994), where agents have an incentive to work together to achieve a team goal, which can be expressed by a global reward function R(t). In such Multi-Agent settings, sharing information among agents is key, as the other agents induce otherwise unpredictable dynamics (non-stationary environments), which limits cooperation (Zhang, Yang, and Zha 2020). This can be done by joint state and action spaces, which, however, require supportive mechanisms to cope with the exponentially growing joint spaces (Choudhury et al. 2021). Hence, action and state spaces are often disjoint and agents are trained by a global reward function to encourage cooperation (Wei et al. 2019a; Chen et al. 2020; van der Pol et al. 2022). In the following, we propose our state encoding to cope with these challenges."}, {"title": "4 Graph-Structured State Encoding", "content": "Following Devailly, Larocque, and Charlin (2022, 2024), we utilize a graph neural network on a heterogeneous graph to encode both static and dynamic state characteristics of individual intersections. This allows us to encode any intersection geometry regardless of the length of lanes and the number of lanes, approaches, movements and phases. By sharing the parameters across all intersections in the network,"}, {"title": "which in turn", "content": "3.  Therefore, E lifts the degeneracies of E using the symmetry-breaking log-distance formulation. We define\nTransition Prior Modelling only the dynamics within the boundaries of the intersection, would result in reactivity rather than proactivity, especially when l is small. To fix this, we interpret the road-network as a coordination graph, which allows us to induce additional context to each agent v \u2208 V. We define the connectivity of the coordination graph by movements,\nMe := {(i, o') | i' = l; (i', o') \u2208 M},\\qquad\nMe := {(i', l) | l = o'; (i', o') \u2208 M}.\\qquad\\qquad(6)\nThis gives a single-hop receptive field for every v \u2208 V. This locally interdependent structure (Yi et al. 2024) can be interpreted as modelling communication channels between v and its adjacent neighbour intersections. We use this to define a state transition prior\nPe = \u2211(i,o) \u2208 M\u2212e \u03c1(io)\u2211(i,o) \u2208 M+e \u03c1(oo),\\qquad\\qquad\\qquad(7)\nwhere 10 and 00 are the closest segments to the intersection's centre. If pe < 0, more vehicles are going to leave l."}, {"title": "where \u03b1s", "content": "Lane Coordinate Frames To break the permutation in-variance of the segment set, we define the centre of the in-tersection as a reference point and induce a positional en-coding on the segments relative to that point. We use a one-dimensional sinusoidal positional encoding (Vaswani et al. 2017) along segments on each lane and over lanes. Instead of additive fusion (Vaswani et al. 2017), we concate-nate the positional information with the density of the seg-ment. This preserves both identities, which improves expressiveness without the need of separate processing (Yu et al. 2023). Thus, we can define the segment feature vector by hs = [p(s)||pe(s)||pe] \u2208 Rd be the feature vector of a segment s \u2208 l.\nSegment-to-Movement Encoding We apply a graph at-tention network (Veli\u010dkovi\u0107 et al. 2017) to learn the mapping R\u00d7d \u2192 Rd'. To improve expressiveness, we use dynamic scoring (Brody, Alon, and Yahav 2022) to compute attention weights\n\u03b1s = exp u(hs)\u2211s'\u2208Ne exp u(hs\u2032)with u(hs) = \u03b1\u03c3 (Wahs),\\qquad\\qquad\\qquad(8)\n\u2208 Rd\u2032 and Wa \u2208 Rd\u00d7d\u2032 are learnable weights. Ne defines the segment set for lane l and o is a monotonic non-linearity, like Leaky-ReLU. We then compute a representation for each movement hm \u2208 Rd\u2032 by a weighted average of its segments, such that\nhm = \u03c3(bs + Wshres + \u2211s\u2208Ni \u03b1sWshs + \u2211s\u2208No \u03b1sWshs),\\qquad\\qquad\\qquad(9)\nwhere Ws \u2208 Rd\u00d7d\u2032 enables learnable residual connections and bs \u2208 Rd\u2032 being the bias term. Movement nodes do not"}, {"title": "5 Domain-Randomised Training", "content": "hold information initially, hence the update is independent of the original target node features hm.\nMovement-to-Phase Encoding The obtained movement node features {hm | m\u2208 M} form another heterogeneous directed acyclic sub-graph with the phase nodes. Instead of a sparsified graph, we use a fully-connected bipartite structure with additional edge features. Each connection between a movement m \u2208 M and a phase \u03c6 \u2208 \u03a6\u03c5 holds a scalar \u03b3m\u03c6 \u2208 {\u22121,0,1} as an edge feature indicating whether a movement is prohibited, protected or permitted during a phase. In literature, often only permitted, or pro-tected movements are considered (Zheng et al. 2019; Zang et al. 2020). We argue, that also the information about prohibited movements are essential to determine the energy of a phase. Furthermore, phase nodes are initialised by a binary flag h\u03c6 \u2208 {0,1} indicating whether the phase is currently active or not. This changes Eq. (8) and Eq. (9) to\nu(hm\u03c6) = \u03b1om\u03c3(Wmhm + Wmh\u03c6 + Wm\u03b3\u03b3m\u03c6)\\qquad\\qquad\\qquad(10)\nh\u03c6 = \u03c3(bm + Wmhres + \u2211m\u2208MAm\u03c6Wmhm),\nand\\qquad\\qquad\\qquadwhere \u03b1m\u03c6, bm \u2208 Rd\u2032 and Wm, Wm \u2208 Rd\u2032\u00d7d\u2032 are learn-able weights. Attention weights are computed as in Eq. (8) but normalised over M instead. Contrary to Veli\u010dkovi\u0107 et al. (2017), we embed node and edge features separately, which reduces the model complexity while still preserving expressiveness. Furthermore, we use the edge features and the initial phase flag only to compute the attention scores. Hence, the model can use \u03b3 to weight movement features during aggregation, but they do not infer otherwise with the movement information. As we use a directed acyclic graph, we do not face the identity issue discussed in general edge-based graph attention (Wang, Chen, and Chen 2021). This form of aggregation also preserves permutation invariance. In contrast to the level before, this is an important property for the encoding of phases, as they should be orientation independent (Zheng et al. 2019). The obtained phase node representations are further leveraged in an intra-level propagation phase, as discussed next."}, {"title": "which results in", "content": "Intra-Level Phase Propagation We model the connec-tion between phases as a fully-connected homogeneous graph with Jaccard coefficients J\u03c6\u03c6\u2032 \u2208 R+ between each phase pair \u03c6, \u03c6\u2032 \u2208 \u03a6\u03c5. The Jaccard coefficient encodes the intersection over the union of the green signals between the two phases. This structures the phase space by quantifying\nu(h\u03c6\u03c6\u2032) = \u03b1\u2032\u03c3(W\u03c6h\u03c6 + W\u03c6h\u03c6\u2032 + WJ\u03c6\u03c6\u2032) (11)\nh\u03c6 \u2190 \u03c3(b\u03c6 + Wh\u03c6 + \u2211\u03c6\u2032\u2208\u03a6\u03c5A\u03c6\u03c6\u2032Wh\u03c6\u2032)\nand\\qquad\\qquad\\qquadwhere a, b \u2208 Rd\u2032 and W, W\u03c6 \u2208 Rd\u2032\u00d7d\u2032 are learnable weights. Again, attention weights are computed as in Eq. (8) but normalised over \u03a6\u03c5 instead. After propagation, each node holds weighted information about all other phases, which renders a single layer sufficient.\nWeight-Sharing Our universal state encoding function al-lows using the model for each intersection. In this way, our model can be applied to any road-network size. Moreover, by sharing parameters among agents, the algorithms are essentially encouraged to converge to a region in parameter space that works well for arbitrary intersections and traffic conditions, thereby promoting generalization."}, {"title": "5 Domain-Randomised Training", "content": "Domain Randomization (DR) is a powerful technique for bridging the sim-to-real gap (Tobin et al. 2017). By introducing sufficient variability in the simulated source domain during training, DR enables the agent to generalize its policy to the target domain, treating it as another variant within its learned distribution. The core principle of DR involves configuring the environment based on a randomly sampled configuration \u03be \u223c \u039e, where \u039e represents the space of possible domain parameters. \u039e contains all traffic-networks under some degree of freedom, as well as different forms of traffic dynamics. The agent's objective is to find an optimal policy \u03c0* that maximizes the expected return across all possible environmental configurations, i.e., extending Eq. (1)\n\u03c0\u2217 = argmax\u03c0 E\u03be ECt E\u03c0 [\u2211\u221ek=0 \u03b3kR(t+1 +k) | Ct, \u03be] (12)\nwhere Ct \u223c \u03bc(Ct | \u03c0;\u03be) denotes the stationary distribution of the Markov chain under configuration \u03be and policy \u03c0. We sample the static environmental characteristics (like the number of intersections and lane lengths) from a uniform distribution a priori. For the dynamics, we use traffic flow modelling to define each flow f \u2208 F by its route, vehicle"}, {"title": "6 Experiments", "content": "ity patterns.\nand vehicle departure times.\n11 In literature, a Poisson process with a constant rate of \u03bbf ve-hicles per second with t \u2208 [0, tmax] is often used instead. However, the constant departure rates are often not realistic in practice (e.g. during rush hours). This approach allows for diverse departure patterns, including peaks and fluctuations, while still encompassing the possibility of constant departure rates.\n6 Experiments\nThe primary objective of our experiments is to show the abil-ity of TransferLight to transfer its control policy to novel scenarios without requiring any kind of re-training or fine-tuning. In all experiments, TransferLight is trained on ran-domly generated road-networks with random traffic dynam-ics and tested on a yet unseen benchmark. This allows us to quantify the generalisability of our method explicitly. In Section 6.1, we analysed various performance measures on multiple benchmarks (test scenarios) with several well-known baselines. As arterial scenarios are of specific inter-est for the community (Wei et al. 2019a), we conduced a de-tailed investigation of our model's generalisability on such scenario types (see Section 6.2). The software specifications of our implementations can be found in our open-sourced code.\nExchangeable Policy Heads The learnable hierarchical state encoding Section 4 maps states to action (phase) energies. The policy control function maps from this action energy space to action probabilities. This results in maximum flexibility when it comes to the policy function. In this work, we chose a Double DQN (Hasselt, Guez, and Silver 2016) and a A2C (Peng et al. 2018) as policy heads, but any other can be used instead."}, {"title": "6.1 Generalising different Scales", "content": "A general traffic signal control policy should be able to gen-eralise from single intersections to more complex road net-works. We demonstrate this ability by conducting experiments on either end. Table 1 compares our models to dif-ferent baselines on two single-intersection benchmarks. We analysed the number of vehicles, as for a single intersection this measure seems the most reasonable. We found that both TransferLight variants outperform all baselines on Colognel and perform quasi on par with MaxPressure, causing the least congestion.\nTo analyse how are policy scales to more complex road networks, we conducted an experiment on Cologne8 comprising 8 signalised intersections. We measured multiple popular traffic performance indicators during testing. We found that both TransferLight variants outperformed all heuristic and trained baselines. Note that, CoLight (Wei et al. 2019b) and SOTL (Reztsov 2014) are explicitly trained on Cologne8, whereas TransferLight generalises from random road-networks. Both trained baselines failed to control a subset of intersections, leading to early congestions and hence the worse performance. The results in Fig. 4 undermine the ability of TransferLight to generalise also to more complex scenarios. In the appendix, we rise the problem complexity even more to identify TransferLight's generalisation limits."}, {"title": "6.2 Arterial Signal Progression", "content": "A special type of coordination is signal progression, which attempts to coordinate the onset of green times of successive intersections along an arterial street in order to move road users through the major roadway as efficiently as possible (Wang, Abdulhai, and Sanner 2023). Intuitively, the hope here is to create a green wave in which green times are cascaded so that a large group of vehicles (also called a platoon) can pass through the arterial street without stopping. PressLight (Wei et al. 2019a) and MaxPressure were shown to maximise throughput and minimise travel time in"}, {"title": "7 Conclusion", "content": "We presented a novel framework designed for robust generalization across road-networks, diverse traffic conditions and intersection geometries. Our method can scale to any road-network through a decentralized multi-agent approach with global rewards and state transition priors to ensure proactive decisions. We used a hierarchical, heterogeneous, and directed graph neural network to encode any intersection geometry, which we train using a novel log-distance reward function. Generalization is further fostered by domain randomization during training. Through domain randomization during training, we additionally enhance generalization capabilities. This is particularly valuable for real-world appli-cations where traffic conditions can vary significantly due to events, road closures, or long-term changes in urban mobil-"}, {"title": "Limitations and Future Work", "content": "Our method shows already striking generalisation capabilities, which, however, need further improvement to cope with even larger road net-works. In future work, we aim to extend the concept of sym-metry breaking to the intersection's geometries. Mapping intersections to canonical forms, as in Jiang et al. (2024), collapses the state space to an exponentially smaller subspace. These canonical forms can be obtained from equivariant encodings (van der Pol et al. 2022) using canonical-isation priors (Kaba et al. 2023; Mondal et al. 2023) or by search (Schmidt and Stober 2024). This will drastically improve the sample efficiency of our model and render domain randomisation useless."}, {"title": "A Supplementary Material", "content": "The finite horizon of 0 > t > T. We also include a convergence illustration in Fig. 8. We found that both model versions converge within 3000 steps (as the performance stays within reasonable error-bounds constant afterwards). We skipped the first 100 steps to let the traffic spawn in the simulation and develop a natural flow.\nBaseline Details All heuristics (incl. Random, FixedTime, and MaxPressure) are custom implementations. All trainable baselines and related performance results are obtained using LibSignal (Mei et al. 2023). Nonetheless, we used the same routines to compute the high-level performance indicators presented in our performance plots.\nA.2 Further Experiments\nReward Comparison Fig. 9 compares the performance gains through our symmetry-breaking log-distance reward. We found that our log-distance reward improves all three target performance indicators over the simulated test span. These empirical results underpin our theoretical claims in Section 3.\nLimits of Generalisability The ability to generalise is of course facing limits at some range of problem complexity. We performed an additional experiment on ingolstadt21 comprising 21 intersections in a narrow urban environment. Figure 10 compares our method to various baselines under different performance measures on this benchmark scenario. After around 1200 time steps, TransferLight with either head starts diverging into a suboptimal sequence of phases. On the long run, this leads to congestions, which in turn lead to performance decreases among all measures. We dedicate our future work to prevent such situations to occur (under reasonable traffic demands)."}]}