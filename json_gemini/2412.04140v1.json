{"title": "UNDERSTANDING MEMORIZATION IN GENERATIVE MODELS VIA SHARPNESS IN PROBABILITY LANDSCAPES", "authors": ["Dongjae Jeon", "Dueun Kim", "Albert No"], "abstract": "In this paper, we introduce a geometric framework to analyze memorization in diffusion models using the eigenvalues of the Hessian of the log probability density. We propose that memorization arises from isolated points in the learned probability distribution, characterized by sharpness in the probability landscape, as indicated by large negative eigenvalues of the Hessian. Through experiments on various datasets, we demonstrate that these eigenvalues effectively detect and quantify memorization. Our approach provides a clear understanding of memorization in diffusion models and lays the groundwork for developing strategies to ensure secure and reliable generative models.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in generative models have revolutionized data generation tasks across diverse domains, including image synthesis [Rombach et al., 2022], natural language processing [Achiam et al., 2023, Touvron et al., 2023], video generating [Ho et al., 2022, Brooks et al., 2022], and molecular design [Alakhdar et al., 2024]. Among these, diffusion models have emerged as a particularly powerful framework, capable of producing high-quality and diverse outputs. Diffusion models [Ho et al., 2020, Song et al., 2021a] work by learning to reverse a predefined noise-adding diffusion process. Their ability to approximate complex data distributions through iterative denoising has achieved state-of-the-art results in applications ranging from image generation [Song et al., 2021b, Saharia et al., 2022] to text-to-image tasks [Rombach et al., 2022, Podell et al., 2024].\nDespite their successes, diffusion models face challenges such as overfitting and memorization [Carlini et al., 2023, Somepalli et al., 2023b, Webster, 2023], where models replicate training data rather than generalizing to new inputs. Memorization is especially problematic in scenarios involving sensitive data, as it risks privacy violations [Orrick, 2023, Joseph Saveri, 2023] and undermines model robustness. Addressing memorization is therefore critical for deploying diffusion models in trustworthy and secure settings.\nThere has been substantial prior work aimed at understanding memorization in diffusion models, which has contributed to a broader understanding of this phenomenon in generative models [Carlini et al., 2023, Somepalli et al., 2023a]. Existing approaches include analyzing probability manifolds using Local Intrinsic Dimensionality (LID) [Ross et al., 2024, Kamkari et al., 2024], leveraging spectral properties to characterize memorization [Ventura et al., 2024, Stanczuk et al., 2024], and employing score-based techniques to measure discrepancies between conditional and marginal scores [Wen et al., 2024]. Additionally, recent studies have explored cross-attention mechanisms to detect and analyze memorization [Ren et al., 2024, Chen et al., 2024].\nIn this work, we introduce a theoretical framework to characterize memorization in diffusion models through the lens of geometric analysis. We propose that memorization arises from isolated points in the learned probability distribution, which manifest as regions of sharpness in the probability landscape. This sharpness can be effectively identified by analyzing the eigenvalues of the Hessian of the log probability. Our approach provides a principled framework rooted in the geometry of the learned probability distribution, offering both interpretability and a solid theoretical foundation.\nBy analyzing the Hessian of the log probability and its eigenvalues, which correspond to the Jacobian of the score function, we develop a diagnostic tool to identify memorization. Our framework reveals that large negative eigenvalues of the Hessian serve as a reliable indicator of memorization, signaling the presence of isolated points in the learned distribution. Moreover, by inspecting the number of positive eigenvalues, we quantify the degree of memorization, enabling distinctions between categories such as template verbatim and matching verbatim memorization. This perspective also sheds light on how memorization reduces the effective degrees of freedom in the model's learned distribution, ultimately constraining its ability to generalize."}, {"title": "2 Related works", "content": "Understanding and Explaining Memorization Several studies [Somepalli et al., 2023b, Carlini et al., 2023, Wen et al., 2024] have investigated the memorization behavior of diffusion models (DMs), focusing primarily on factors such as prompts [Somepalli et al., 2023a], data duplication [Carlini et al., 2023, Somepalli et al., 2023b], and the size or complexity of the training data [Somepalli et al., 2023b]. Some works analyze this phenomenon from a geometric perspective, grounded in the widely accepted manifold learning conjecture that high-dimensional data resides on a relatively low-dimensional latent manifold [Fefferman et al., 2016, Pope et al., 2021], with exact memorization corresponding to a zero-dimensional manifold [Ross et al., 2024, Ventura et al., 2024, Pidstrigach, 2022]. This perspective has motivated several approaches for estimating the Local Intrinsic Dimensionality (LID) of individual data points [Stanczuk et al., 2024, Kamkari et al., 2024, Horvat and Pfister, 2024, Wenliang and Moran, 2023, Tempczyk et al., 2022], which are then employed to explain memorization phenomena [Ross et al., 2024, Ventura et al., 2024].\nAlthough our work aligns with some conclusions from previous studies, it differs in important ways. Unlike Yoon et al. [2023], Gu et al. [2023], which define memorization globally at the model level, our approach adopts a geometric perspective focused on local data points, aligning more closely with Ross et al. [2024], Bhattacharjee et al. [2023], Ventura et al. [2024]. However, in contrast to Ross et al. [2024], Bhattacharjee et al. [2023], which rely on ground truth density to define memorization, or Ventura et al. [2024], which examines the interplay between memorization and generalization, we make no assumptions about data densities and do not explicitly address generalization. Instead, we define memorization solely in terms of the local \u201csharpness\u201d of the learned density.\nAdditionally, when explaining LID, our method avoids the limitations of existing LID estimators. For instance, FLIPD [Kamkari et al., 2024] is sensitive to network architecture and unstable near t\u2192 0, where learned scores often diverge, while the Normal Bundle (NB) estimator [Stanczuk et al., 2024] incurs substantial computational costs in high-dimensional settings. By directly analyzing the Jacobian of the learned scores (equivalent to the Hessian of the log density), our approach eliminates parameter dependencies and avoids additional network training, consistently delivering accurate LID estimates across timesteps. Furthermore, it scales effectively to high-dimensional scenarios, such as those in Stable Diffusion, by leveraging Arnoldi iteration and torch.autograd.jvp for computational efficiency.\nWhile our work focuses exclusively on understanding the memorization phenomenon in diffusion models (DMs), it is inherently connected to the broader topic of generalization. Several studies have explored how DMs generalize, offering insights that complement our findings on memorization. Kadkhodaie et al. [2024] highlight that DMs possess an inductive bias that aids generalization, while Yi et al. [2023] attribute this ability to an optimization bias. Furthermore, Yoon et al. [2023] demonstrate a trade-off between memorization and generalization, showing that DMs tend to generalize more effectively when their capacity for memorization is constrained. These studies underscore"}, {"title": "3 Preliminaries", "content": "Score-based Diffusion Models. The foundation of diffusion models (DMs), widely used for image generation, lies in score-based models [Song et al., 2021c]. These models corrupt the given training data which follows $p_0(x_0)$ by adding random noise and then learn the score function, the gradient of the log density, $\\nabla_{x_t} \\log p_t(x_t)$. Subsequently, a reverse process is applied to generate new samples, starting from arbitrary noise, which follow the same distribution as the original data.\nWe define forward process and corresponding reverse process as\n$dx_t = f(x_t,t)dt + g(t)dw_t$\nand\n$dx_t = (f(x,t) \u2013 g^2(t)\\nabla_{x_t} \\log p_t(x_t)) dt + g(t)dw_t,$\nwhere $w_t$ and $\\bar{w_t}$ are forward and reverse-time standard Brownian motion.\nSince we cannot access to score function, we parametrize it with $s_\\theta(x, t)$ through neural network. Optimizing the score matching loss\n$L_{SM}(\\theta) = \\int_0^T \\mathbb{E}_{x(t)}[\\Vert s_\\theta(x_t,t) - \\nabla_{x_t} \\log p_t(x_t)\\Vert^2] dt$\nis intractable as we do not have actual score function, so we use alternative(equivalent) formula. Let us discretize the forward process as $x_t | x_0 \\sim \\mathcal{N}(a_t x_0, \\sigma_t^2 I)$ for $a_t \\in \\mathbb{R}, \\sigma_t \\in \\mathbb{R}_{++}$. For a fixed time point $t$, we can define\n$\\theta^* = \\underset{\\theta}{\\arg \\min} \\mathbb{E}_{x_0, \\epsilon} [\\frac{1}{\\sigma_t} \\Vert s_\\theta(x_t, t) + \\epsilon \\Vert^2], \\quad x_t = a_t x_0 + \\sigma_t \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I),$\nwhich is called denoising score matching(DSM). Practically, we can use Monte Carlo method to approximate integral over $t$.\nSharpness and Hessian. For a given function $f$ at a point $x$, the Hessian $\\nabla^2 f(x)$ encapsulates the second-order derivatives, providing a representation of the local curvature of $f$ around $x$. The eigenvectors of the Hessian define the principal axes of this curvature, which are often orthogonal, while the corresponding eigenvalues quantitatively describe the curvature along these directions. Positive eigenvalues indicate directions of local convexity, negative eigenvalues denote directions of local concavity, and zero eigenvalues correspond to flat regions. The magnitude of an eigenvalue reflects the steepness of the curvature, with larger absolute values indicating more pronounced variations. Thereby, by examining the eigenvalues of the Hessian at a specific point, we gain insights into the structure and geometry of the landscape at that point. In this paper, we primarily focus on the Hessian of the log density, which can also be interpreted as the Jacobian of the score function. Throughout the paper, we denote the Hessian of the log density as $H(x_t, t) := \\nabla^2_{x_t} \\log p_t (x_t)$ for the unconditional case and $H(x_t, t, c) := \\nabla^2_{x_t} \\log p_t (x_t | c)$ for the conditional case."}, {"title": "4 Understanding Memorization in Geometric Perspective", "content": "4.1 Sharpness in Probability Landscape and Memorization\nMemorization in diffusion models refers to the phenomenon where the learned data distribution becomes excessively concentrated at isolated points, typically corresponding to the training data. This occurs when the model overfits, effectively collapsing the learned probability distribution into individual samples. Biroli et al. [2024] describe this phenomenon in terms of phase transitions in the sampling path, which lead to such localized distributions.\nIn the sampling process, the score function guides the steepest ascent of the log-likelihood in the data space, analogous to gradient ascent [Das, 2024]. In deterministic scenarios, the sampling process converges to local modes of the target distribution. However, the addition of noise during the sampling process allows the model to explore broader regions of the data space. When memorization occurs, this broader exploration is compromised, and the learned density exhibits sharp, localized peaks centered on training samples.\nWe hypothesize that memorization can be characterized by the sharpness\nof the probability landscape, where sharpness corresponds to steep gra-\ndients and localized modes in the learned distribution. These regions of\nsharpness are indicative of isolated points in the learned density, which\ncan be effectively identified and quantified through the eigenvalues of the\nHessian of the log probability. Large negative eigenvalues reflect sharp\npeaks, providing a geometric signal of memorization.\nWhile memorization may arise from various factors, our focus is not on\nanalyzing these causes but rather on detecting and understanding the phe-\nnomenon itself. By concentrating on the sharpness of the learned density,\nour framework provides a consistent and practical approach to identifying\nmemorization in diffusion models.\nLastly, we note that this analysis applies to both conditional and non-\nconditional probability landscapes, depending on the context of the diffu-\nsion model's task or application.\n4.2 Detecting Memorization via Hessian Analysis\nWe propose that the eigenvalues of the Hessian are a key feature for under-\nstanding and detecting memorization in diffusion models. Specifically, we\nintroduce a novel metric: counting the strictly positive eigenvalues of the\nHessian to capture the degree of memorization. This metric effectively mea-\nsures the local intrinsic dimension of the probability manifold Ross et al.\n[2024], offering a geometric perspective on how memorization manifests\nin the learned distribution.\nWhen memorization occurs, the learned density of the model exhibits sharp,\nlocalized peaks around specific samples. This sharpness can be effectively\nidentified using the Hessian, which represents the second derivative of the\nlog probability density (or equivalently, the first derivative of the score\nfunction). Intuitively, if most eigenvalues of the Hessian are significantly\nnegative, it indicates that the sample resides at an isolated peak-an unmistakable geometric signal of memorization.\nConversely, a higher count of positive eigenvalues suggests that the sample lies on a smoother, higher-dimensional\nsurface of the learned distribution, allowing for variation in multiple directions, which is characteristic of generalized\nsamples.\nOne significant advantage of analyzing the Hessian is its ability to detect memorization at specific data points at any\ntimestep, even at the earliest stages of the sampling process. Our experiments reveal that Hessian eigenvalues differ\nnotably from the very beginning between cases where memorization occurs and those where it does not. Furthermore,\nmemorization is not limited to the exact reproduction of training data (Matching Verbatim, MV) but also includes\nreplication of broader features, such as backgrounds or artistic styles present in the training data (Template Verbatim,\nTV) [Webster, 2023]. The eigenvalues of the Hessian vary with the degree of memorization, effectively capturing\nchanges in the local intrinsic dimension of the probability manifold in these cases.\nThese findings strongly support our hypothesis that the Hessian eigenvalues serve as a robust geometric indicator of\nmemorization, providing both theoretical insights and practical tools for detecting and quantifying memorization in\ndiffusion models."}, {"title": "5 Experiments", "content": "In this section, we investigate deeper into the memorization phenomenon observed in diffusion models trained on various scale of datasets, utilizing our geometric framework for analysis. Specifically, we examine the Hessian eigenvalues (i.e., the Jacobian of the learned score) across the sampling path of the generated samples.\n5.1 Memorization in a simple Gaussian\nAs an illustrative example, we first analyze the memorization phenomenon in a two-dimensional setting. The true density is defined as a simple Gaussian, with an additional single point located far from the Gaussian distribution. Specifically, we sample 3,000 points from the Gaussian density and duplicate the distant point 100 times. This setup is\n(a) Learned scores and 3,100 training data colored in (b) Eigenvalues for each timestep in memorized and non-\nblue. memorized samples.\nFigure 2: Toy example of a 2D Gaussian mixture with one mode containing 3,000 dispersed points and another mode\nwith 100 highly concentrated points.\ndesigned to induce strong memorization at the duplicated point while maintaining a well-generalized region within\nthe Gaussian distribution, where no memorization is expected. By constructing this contrast, we aim to compare the\ncurvature of sampling path between the memorized and generalized regions.\nAfter training a diffusion model, we generate 5,000 samples. As expected, no signs of memorization appear for samples\non the Gaussian surface, while strong memorization is evident at the duplicated data point. Furthermore, as shown\nin Figure 2a, the learned scores around the duplicated point exhibit greater magnitudes, strongly pulling the sample\npaths toward the memorized point.\nTo further analyze, we select one sample each from the memorized and non-memorized regions. We compute the\nHessian, $H(x_t, t)$, for both samples across all 1,000 timesteps, as illustrated in Figure 2b. As expected, near timestep 0,\nthe eigenvalues of $H(x_t, t)$ for the memorized sample drop to negative values, indicating a sharp curvature. In contrast,\nthe eigenvalues for the non-memorized sample has eigenvalues both near zero, indicating locally smoother region.\n5.2 Memorization in MNIST\nWe further explore memorization in the MNIST dataset, which re-\nsides in a 784-dimensional space. To directly examine the geometric\nbehavior of memorized and non-memorized samples, we deliberately\nassign specific classes to induce memorization, thereby constructing\ndistinct conditional densities.\nSpecifically, we designate digits 3 and 9 as the non-memorized and\nmemorized classes, respectively. For digit 3, we randomly select\n3,000 unique images to represent the non-memorized class. Con-\nversely, for digit 9, we select a single image and duplicate it 30\ntimes to induce strong memorization within the model, similar to the\ntwo-dimensional setup. Using this modified dataset of 3,030 sam-\nples, we train a DDPM with a second-order loss to ensure accurate\ncomputation of the Hessian.\nNon memorized\nFigure 3: Generated samples of memorized\n(digit 9), and non-memorized (digit 3) classes.\nAs depicted in Figure 3, the model generates a wide variety of shapes for digit 3, indicating its ability to generalize\neffectively. In contrast, for digit 9, the model exclusively reproduces the single duplicated image, confirming that it has\nfully memorized this specific instance.\nFor analysis, we generate 10,000 samples for digit 3 using a DDIM sampler with 1,000 steps and compare each\ngenerated sample against the entire training set using calibrated $l_2$ distance [Carlini et al., 2023]. From this comparison,\nwe identify 2,000 samples that are clearly distinguishable from the training set and can reliably be considered free from\nmemorization. For digit 9, we directly generate 2,000 samples, as the model consistently reproduces the memorized"}, {"title": "6 Conclusion and Future works", "content": "Our study provides empirical evidence that the phenomenon of memorization in diffusion models can be effectively understood through a simple geometric framework: the sharpness of the density curvature. By analyzing Hessian eigenvalues, we systematically investigate and explain the differences in density curvature between memorized and non-memorized samples, ranging from toy datasets to large-scale models like Stable Diffusion.\nFor future work, we aim to conduct a more rigorous analysis of how conditioning via embeddings like CLIP en- coder [Radford et al., 2021] alters the distribution, offering deeper insights into the behavior of memorized data in large generative models. Additionally, developing effective mitigation strategies based on our findings represents a promising research direction. Given that our method can detect memorization at an early stage, it could be utilized to design proactive approaches for mitigating memorization. Furthermore, extending our analysis to one-step models, such as rectified flow [Liu et al., 2023], could pave the way for novel designs of sampling ODE trajectories."}, {"title": "B Arnoldi Iteration", "content": "Algorithm 1 Arnoldi Iteration using Jacobian-Vector Products\nRequire: Starting vector $b \\in \\mathbb{R}^d$, number of iterations $m \\leq d$, function jvp_func(v) that computes $Av$, threshold $\\epsilon$\nEnsure: Orthonormal basis $Q_m = [q_1, ..., q_m]$, upper Hessenberg matrix $H_m \\in \\mathbb{R}^{m\\times m}$\n1: Initialize $Q \\in \\mathbb{R}^{d\\times (m+1)}$, $h \\in \\mathbb{R}^{(m+1)\\times m}$\n2: Normalize the starting vector: $q_1 = \\frac{b}{||b||_2}$\n3: for $k= 1$ to $m$ do\n4:  Compute $v$ = jvp_func($q_k$)\n5:  for $j = 1$ to $k$ do\n6:   Compute $h_{j,k} = q_j^Tv$\n7:   Update $v = v - h_{j,k}q_j$\n8:  end for\n9:  Compute $h_{k+1,k} = ||v||_2$\n10:  if $h_{k+1,k} > \\epsilon$ then\n11:   Normalize $q_{k+1} = \\frac{v}{h_{k+1,k}}$\n12:  else\n13:   Terminate iteration\n14:  end if\n15: end for\n16: Adjust $H_m$ by removing the last row of h\n17: return $Q_m = [q_1, ..., q_m]$, $H_m = [h_{i,j}]_{i=1,...,m; j=1,...,m}$"}]}