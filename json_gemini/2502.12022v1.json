{"title": "Teaching LLMs According to Their Aptitude: Adaptive Reasoning for Mathematical Problem Solving", "authors": ["Xin Xu", "Yan Xu", "Tianhao Chen", "Yuchen Yan", "Chengwu Liu", "Zaoyu Chen", "Yufei Wang", "Yichun Yin", "Yasheng Wang", "Lifeng Shang", "Qun Liu"], "abstract": "Existing approaches to mathematical reasoning with large language models (LLMs) rely on Chain-of-Thought (CoT) for generalizability or Tool-Integrated Reasoning (TIR) for precise computation. While efforts have been made to combine these methods, they primarily rely on post-selection or predefined strategies, leaving an open question: whether LLMs can autonomously adapt their reasoning strategies based on their inherent capabilities. In this work, we propose TATA (Teaching LLMS According to Their Aptitude), an adaptive framework that enables LLMs to personalize their reasoning strategy spontaneously, aligning it with their intrinsic aptitude. TATA incorporates base-LLM-aware data selection during supervised fine-tuning (SFT) to tailor training data to the model's unique abilities. This approach equips LLMs to autonomously determine and apply the appropriate reasoning strategy at test time. We evaluate TATA through extensive experiments on six mathematical reasoning benchmarks, using both general-purpose and math-specialized LLMs. Empirical results demonstrate that TATA effectively combines the complementary strengths of CoT and TIR, achieving superior or comparable performance with improved inference efficiency compared to TIR alone. Further analysis underscores the critical role of aptitude-aware data selection in enabling LLMs to make effective and adaptive reasoning decisions and align reasoning strategies with model capabilities.", "sections": [{"title": "1 Introduction", "content": "Mathematical reasoning is a cornerstone of human intelligence, and enabling machines to solve mathematical problems is crucial for many applications (Ahn et al., 2024; He et al., 2024). Recent years have seen rapid progress in leveraging LLMs for mathematical reasoning, with advancements in con-"}, {"title": "2 Background", "content": "2.1 Rejection Fine-Tuning\nRejection fine-tuning (RFT) is a widely-adopted approach to enhance math reasoning abilities by augmenting the original training set using rejection sampling (Yuan et al., 2023). Suppose that the original training set $D_{orig} = \\{(x_i, y_i)\\}_{i=1}^{N}$ consists of N pairs of data points $(x_i, y_i)$. For each query $x_i$, M responses are generated by a teacher model (e.g., GPT-4): $\\{x_i, y_i^j\\}_{j=1}^{M}$. If $y^j \\neq y_i$, then the response $y^j$ is discarded, leading to the augmented training set $D_{aug} = \\{(x_i, y_i^j)\\}_{i=1,j=1}^{N, M_i}$, where $M_i \\leq M$ is the number of correct responses for query $x_i$. More details are given in Appendix A.1.\n2.2 TIR Inference Pipeline\nTool-Integrated Reasoning (TIR) combines natural language reasoning with Python code execution in an interleaved manner. When a Python code block is encountered, it is executed using a Python interpreter, and the resulting out-put, along with the previous context, is fed back into the LLM to facilitate further reasoning (see Algorithm 1). Solving math problems with TIR often requires multiple iterations of these interac-tions, which typically results in higher computa-tional costs compared to CoT. However, TIR offers"}, {"title": "2.3 Implicit Instruction Tuning", "content": "In-Context Learning (ICL) can be viewed as im-plicit instruction tuning (IIT), i.e., \u201cfine-tune\" the demonstration implicitly. Let $X_{ins}, X_{test} \\in \\mathbb{R}^{d_{in}}$ be the few-shot demonstration inputs and the test input, respectively. Suppose $W_K, W_V, W_Q \\in \\mathbb{R}^{d_{out} \\times d_{in}}$ are projection matri-ces to compute the attention queries, keys, and values. The self-attention is formulated as follows:\n$\\frac{W_V [X_{ins} || X_{test}] \\text{Softmax}(\\frac{W_K[X_{ins} || X_{test}] Q}{\\sqrt{d_{in}}})}{\\approx [W_VX_{test}(W_KX_{test})^T + W_VX_{ins}(W_KX_{ins})^T] Q}$"}, {"title": "3 The TATA Framework", "content": "3.1 Problem Setting\nMany advanced capabilities can be distilled through SFT. Building on this, we aim to teach LLMs to spontaneously se-lect between CoT and TIR using SFT by carefully selecting reasoning patterns tailored to different training queries. In this section, we formally formu-late our research question as an SFT data selection problem.\nData Structure Suppose we have a candidate dataset $D = \\{(x_i, y_i^j, z_i^j)\\}_{i=1,j=1}^{N, M_i}$ consisting of triplets in the form $(x_i, y_i^j, z_i^j)$ for the i-th training example, where $1 \\leq j \\leq M_i$. Here, $x_i$ represents the i-th training problem, while $y_i^j$ and $z_i^j$ denote the j-th CoT solution and TIR solution to this prob-lem, respectively. Notably, the TIR solution $z_i^j$ is adapted from $y_i^j$, meaning both solutions follow the same steps to solve the mathematical problem $x_i$, but differ in their reasoning formats: $y_i^j$ relies exclusively on natural language reasoning, whereas $z_i^j$ incorporates Python code blocks to perform cal-culations for certain reasoning steps.\nObjective Our objective is to construct an SFT dataset from the candidate dataset $D = \\{(x_i, y_i^j, z_i^j)\\}_{i=1,j=1}^{N, M_i}$ by incorporating suitable rea-soning patterns for different training queries. Specifically, for each problem $x_i$ in $D = \\{(x_i, y_i^j, z_i^j)\\}_{i=1,j=1}^{N, M_i}$, we need to decide whether to include its CoT solutions or TIR solutions in the SFT dataset. Formally, this involves determining whether $\\{(x_i, y_i^j)\\}_{j=1}^{M_i} \\subseteq D_{SFT}$ or $\\{(x_i, z_i^j)\\}_{j=1}^{M_i} \\subseteq D_{SFT}$. For example, CoT-only SFT constructs the dataset such that $\\{(x_i, y_i^j)\\}_{j=1}^{M_i} \\subseteq D_{SFT}, \\forall i$. In contrast, TIR-only SFT selects $\\{(x_i, z_i^j)\\}_{j=1}^{M_i} \\subseteq D_{SFT}, \\forall i$. Our work aims to go beyond static selection approaches by dynamically tailoring the reasoning paradigm to suit the spe-cific requirements of each training query, while also accounting for the base LLM's aptitude."}, {"title": "3.2 \u03a4\u0391\u03a4\u0391 Overview", "content": "Motivation Following the success of distillation through SFT, we aim to enable LLMs to adaptively select appropriate reasoning strategies based on their own aptitude, guided by the control of SFT data selection (see Section 3.1). Intuitively, if an LLM demonstrates improved performance on certain queries when trained with CoT solutions instead of TIR solutions, it suggests its inclination toward CoT reasoning in those cases. This prefer-ence can be extrapolated to new cases, where the model is expected to favor CoT for similar prob-lems during testing. The same principle applies to TIR-based reasoning. Inspired by IIT theory (see Section 2.3), LLMs can be indirectly \u201cfine-tuned\" with CoT or TIR examples through one-shot learn-ing, thereby replacing the need for actual SFT.\nOverview As depicted in Figure 2, our proposed framework, TATA, comprises four main steps: data construction, anchor construction, contribu-tion quantification, and data selection. In the data construction stage, we adapt an original training set, $D_{orig}$, containing CoT solutions, to form the candidate set $D = \\{(x_i, y_i^j, z_i^j)\\}_{i=1,j=1}^{N, M_i}$. This candidate set includes triplets of queries, a CoT solu-tion, and corresponding TIR solution. Next, dur-ing the anchor construction stage, a representative"}, {"title": "3.3 \u03a4\u0391\u03a4\u0391 Details", "content": "Data Construction We start with an origi-nal math training set (e.g., MATH) training set), denoted as $D_{orig} = \\{(x_i, Y_i)\\}_{i=1}^{N}$, which consists of N training exam-ples, where the i-th problem is represented as $x_i$ with its corresponding golden answer $y_i$. To fur-ther enhance the training set, we apply RFT (see Section 2.1), resulting in an augmented dataset, $D_{aug} = \\{(x_i, y_i^j)\\}_{i=1,j=1}^{N, M_i}$, where $y_i^j$ denotes the j-th augmented CoT solution for the i-th training problem $x_i$. Next, we convert each CoT solution $y_i^j$ into the TIR format $z_i^j$ by prompting a strong LLM (e.g., GPT-40). During this process, the origi-nal logic in $y_i^j$ is preserved, while Python blocks are introduced to handle necessary computations. This transformation produces a candidate dataset $D = \\{(x_i, y_i^j, z_i^j)\\}_{i=1,j=1}^{N, M_i}$, which is required for our problem setting (see Section 3.1).\nAnchor Construction To evaluate the impact of specific CoT or TIR solutions on the performance of LLMs, we construct an anchor set, denoted by $D_{anchor} = \\{(q_i, a_i)\\}_{i=1}^{A}$, where A is the size of the anchor set, $q_i, a_i$ is the i-th question and cor-responding ground-truth answer in $D_{anchor}$. We expect $D_{anchor}$ to be diverse, ensuring that accuracy on this set fairly reflects the LLMs' overall perfor-mance. To achieve this, we first encode all queries from $D_{orig}$ into vector representations using an em-bedding model (e.g., text-embedding-ada-002) and then cluster them into A distinct groups. The center of each cluster is selected to $D_{anchor}$. This approach takes the semantic diversity of questions into account, making $D_{anchor}$ a reliable indicator of LLMs' performance.\nContribution Quantization To quantify the contribution of CoT and TIR for each triplet $(x_k, y_k^j, z_k^j)$ in D to the LLMs' math reasoning abil-ities, we implicitly \"fine-tune\" the LLMs using CoT and TIR formats separately through one-shot learn-ing (see Section 2.3). For the k-th query $x_k$ and its"}, {"title": "4 Experimental Results", "content": "4.1 Experimental Setup\nTATA Implementation We select the training sets from GSM8K and Math as $D_{orig}$. For $D_{aug}$, we use the DART-Math-Hard dataset. We employ GPT-40 to rewrite CoT solutions into TIR format using carefully curated prompts and filter out triplets with anomalous TIR responses (e.g., those that lack a definitive conclusion re-garding the final answer). For embedding, we use text-embedding-ada-002 to encode all queries"}, {"title": "4.2 Main Results", "content": "Results presented in Table 1 demonstrate the effectiveness of our proposed TATA framework, which combines the strengths of CoT and TIR methods for mathematical reasoning tasks. Across various base models, model sizes, and benchmarks, TATA consistently achieves competitive or superior per-formance compared to standalone CoT and TIR approaches, highlighting its ability to leverage the complementary advantages of both methods.\nIn-Domain Performance For in-domain tasks (GSM8K and MATH), TATA achieves the highest average accuracy (ID AVG) in most cases, out-performing both CoT and TIR individually. This suggests that the integration of CoT's step-by-step reasoning with TIR's tool-assisted computations en-hances the model's ability to solve problems within its training domain. Notably, for larger models such as Qwen2.5-7B and Qwen2.5Math-7B, TATA"}, {"title": "Inference efficiency", "content": "The results in Table 2 demonstrate that our TATA not only improves ac-curacy but also enhances inference efficiency com-pared to standalone CoT and TIR methods. More detailed results are presented in Table 10. Across all model sizes, TATA achieves higher accuracy while maintaining lower token usage and fewer code executions than TIR, and it significantly re-duces computational overhead compared to TIR without sacrificing the benefits of tool integration. For instance, with Qwen2.5-7B, TATA achieves a 2.3% accuracy improvement over CoT while us-ing 9.1 fewer tokens per generation and only 1.4 code executions, compared to TIR's 2.63 code ex-ecutions. This balance between accuracy and ef-ficiency highlights TATA's ability to streamline reasoning processes, making it a computationally effective solution for mathematical reasoning tasks."}, {"title": "4.3 Ablation", "content": ""}, {"title": "5 Analysis and Discussion", "content": "5.1 Analysis of CoT scores and TIR scores\nTo further investigate how different LLMs exhibit varying reasoning patterns, we analyze the distri-bution of $S_{CoT}^k$ and $S_{TIR}^k$. As illustrated in Figure 3 (see also Appendix C.3), different base LLMs dis-play distinct distributions of $(S_{CoT}^k - S_{TIR}^k)$, indi-cating varying inclinations towards CoT and TIR reasoning for queries in the candidate set $D^* = \\{(x_i, y_i^j, z_i^j)\\}_{i=1}^N$. Interestingly, even base LLMs from the same family can demonstrate different ten-dencies towards CoT and TIR (e.g., Qwen2.5-0.5B vs. Qwen2.5-7B). Notably, Qwen2.5-7B exhibits"}, {"title": "5.2 Transferability of Data Selection between Different LLMs", "content": "To evaluate whether data selected by one LLM can benefit another LLM, we conducted additional experiments using Qwen2.5-0.5B to assess this type of transferability. Specifically, we fine-tuned Qwen2.5-0.5B on data selected by Qwen2.5-7B and LLaMA-3-8B, with the results in Table 5. As expected, compared to fine-tuning Qwen2.5-0.5B on its own selected data, fine-tuning on data se-lected by another LLM leads to a decline in TATA performance. This finding suggests that our TATA approach is base model-aware, emphasizing the principle of \"teaching LLMs according to their ap-titude.\" Interestingly, using data selected by LLMs within the same family (e.g., Qwen2.5-7B) yields more consistent performance compared to data se-lected by LLMs from a different family (LLaMA-3-8B). Complete results are in Appendix C.4."}, {"title": "5.3 Exploring Reinforcement Learning", "content": "Recent advancements in reinforcement learning (RL) have demonstrated promising results in enhancing long CoT reasoning. To explore the role of RL in the spontaneous selection between CoT and TIR, we employ Direct Preference Optimization (DPO) to LLMs fine-tuned with our TATA framework by constructing preference pairs based on the CoT and TIR scores of queries in the new candidate set $D^* = \\{(x_i, y_i^j, z_i^j)\\}_{i=1}^N$. Detailed experimental setup and methodologies are provided in Appendix C.5. As shown in Table 6, DPO achieves results comparable to those of TATA. The complete results are provided in Table C.5. This suggests that the original data has already been effectively learned by the base LLM during the SFT stage, and applying additional DPO on the same dataset yields minor improvement. This ob-servation aligns with LIMO , which argue that the capabilities of pretrained LLMs are"}, {"title": "6 Related Work", "content": "Math Reasoning with CoT and TIR CoT and TIR are two widely recognized approaches for rea-soning with LLMs. CoT offers interpretability and generalizability, while TIR can provide precise cal-culation results. Previous work on mathematical SFT has primarily focused on either CoT or TIR , with a few efforts to integrate both . For instance, MAmmoTH mainly adopts TIR but switches to CoT when code exe-cution fails due to errors or timeouts. However, it relies on separate prompts and manual inference controls to switch between them. Recent work has explored automatic selection between CoT and TIR , such as using an auxiliary LLM to determine CoT/TIR. However, these meth-ods rely on external planners to select CoT/TIR, not by LLMs themselves. In contrast, our work seeks to enable LLMs to spontaneously select the appropriate reasoning strategy without relying on external planners or manual interventions.\nData Selection Data selection plays a crucial role in training LLMs. Various"}, {"title": "7 Conclusion", "content": "We propose TATA, a novel and effective frame-work for mathematical reasoning with LLMs that enables models to dynamically align their reason-ing strategies, CoT or TIR, with their intrinsic strengths. By incorporating base-LLM-aware data selection during SFT, TATA tailors reasoning strate-gies to each model, empowering them to select an appropriate paradigm for during inference au-tonomously. Extensive experiments demonstrate that TATA achieves superior or comparable per-formance across both in-domain and OOD bench-"}, {"title": "Limitation", "content": "This study primarily focuses on the domain of mathematical reasoning. Extending the concept of adaptive tool use to more generalized reasoning scenarios represents a promising avenue for future research. The proposed approach concentrates on instance-level spontaneous selection between CoT and TIR. Investigating a more fine-grained, step-level selection strategy could be an interesting di-rection for future work. Our method mainly relies on the SFT stage, with the training data sourced from the GSM8K and MATH datasets. Further re-search incorporating reinforcement learning (e.g., or leveraging a more di-verse set of training data (e.g., could be interesting direc-tions to explore."}, {"title": "A Preliminaries", "content": "A.1 Rejection Fine-Tuning\nFor training LLMs, the original training datasets are often insufficient. To mitigate this issue, many studies adopt Rejection Fine-Tuning (RFT) to augment the original dataset, thereby increasing the training data size and improving model perfor-mance. RFT is a fine-tuning approach that uses synthesized data generated via rejection sampling.\nSuppose the original training set is $D_{orig} = \\{(x_i, y_i)\\}_{i=1}^N$, consisting of N data pairs $(x_i, y_i)$. The rejection sampling process works as follows: for each query $x_i$, a teacher model (e.g., GPT-4) generates M responses, resulting in $\\{x_i, y_i^j\\}_{j=1}^M$ where M is a predefined number (e.g., M = 10 in ). This yields N. M response ex-amples in total. A filtering process is then applied: if a response $y^j \\neq y_i$, it is discarded. T he result is the augmented training set $D_{aug} = \\{(x_i, y_i^j)\\}_{i=1,j=1}^{N, M_i}$, where $M_i < M$ represents the number of correct responses for query $x_i$. Notably, $M_i$ is often larger for simpler queries $x_i$, as these are more likely to produce correct responses.\nRFT is widely employed for improving mathe-matical reasoning in LLMs. Typically, the queries remain unchanged or are altered in a controlled way . This is be-cause the filtering stage of the rejection sampling process relies on the availability of ground-truth outputs.\nA.2 TIR Inference Pipeline\nTool-Integrated Reasoning (TIR) addresses mathe-matical problems by intertwining natural language reasoning with the execution of Python code. The process is initiated with gernerating a natural lan-guage reasoning step, denoted as $r_1$. When it is more advantageous to utilize programmatic tools, such as complex calculations, a Python code block, $a_1$, is created as guided by $r_1$. This code block is then run, and its result, $o_1$, is fed back into the model for further generation. This cycle is repeated until the maximal number of code blocks is reached or until the model concludes its answer within \"\\boxed{}.\". The entire reasoning path un-folds as $T = r_1a_1o_1... r_{n-1}a_{n-1}o_{n-1}r_n$, where $r_i$ is the i-th natural language reasoning step, $a_i$ de-notes the corresponding Python code block, and $o_i$ represents the output from executing the code. The complete inference workflow is detailed in Algorithm 1 (from Gou et al. (2023)). From Algorithm 1, TIR usually requires multiple generations based on previous reasoning paths and outputs returned by Python interpreter, which is more computation-ally expensive than CoT. However, TIR can provide more precise calculation results than CoT.\nA.3 Implicit Instruction Tuning\nIn-Context Learning (ICL) can be interpreted as a form of implicit instruction tuning, where the model is effectively \"fine-tuned\" using the given demonstrations in an implicit manner. Let $X_{ins}, X_{test} \\in \\mathbb{R}^{d_{in}}$ represent the few-shot demonstration inputs and the test input, re-spectively. We define the attention query vector as $Q = W_QX_{test}$, while the attention key and value vectors are given by $K = W_K[X_{ins}||X_{test}]$ and $V = W_V [X_{ins} || X_{test}]$, where || denotes concatena-tion. The projection matrices $W_K, W_V, W_Q \\in \\mathbb{R}^{d_{out} \\times d_{in}}$ are used to compute the attention queries, keys, and values. The self-attention mechanism for a single attention head in any given layer is formulated as follows:\n$Attention (K, V, Q) = \\frac{W_V [X_{ins} || X_{test}] \\text{Softmax}(\\frac{W_K[X_{ins} || X_{test}] Q}{\\sqrt{d_{in}}})}{}$"}]}