{"title": "Merging Context Clustering with Visual State Space Models for Medical Image Segmentation", "authors": ["Yun Zhu", "Dong Zhang", "Yi Lin", "Yifei Feng", "Jinhui Tang"], "abstract": "Medical image segmentation demands the aggregation of global and local feature representations, posing a challenge for current methodologies in handling both long-range and short-range feature interactions. Recently, vision mamba (ViM) models have emerged as promising solutions for addressing model complexities by excelling in long-range feature iterations with linear complexity. However, existing ViM approaches overlook the importance of preserving short-range local dependencies by directly flattening spatial tokens and are constrained by fixed scanning patterns that limit the capture of dynamic spatial context information. To address these challenges, we introduce a simple yet effective method named context clustering VIM (CCVIM), which incorporates a context clustering module within the existing ViM models to segment image tokens into distinct windows for adaptable local clustering. Our method effectively combines long-range and short-range feature interactions, thereby enhancing spatial contextual representations for medical image segmentation tasks. Extensive experimental evaluations on diverse public datasets, i.e., Kumar, CPM17, ISIC17, ISIC18, and Synapse demonstrate the superior performance of our method compared to current state-of-the-art methods. Our code can be found at https://github.com/zymissy/CCV\u0130M.", "sections": [{"title": "I. INTRODUCTION", "content": "MEDICAL image segmentation (MedISeg) plays a crucial role in the communities of scientific research and medical care, aiming at delineating the region of interest and recognizing pixel-level and fine-grained lesion objects [1]\u2013[4], which is critical for anatomy research, disease diagnosis, and treatment planning [5], [6]. Deep learning advancements have sped up the automation of MedISeg, resulting in improved efficiency, accuracy, and reliability when compared to manual segmentation technologies [7], [8]. In the past years, accurate MedISeg methods have been successfully used in daily clinical applications, e.g., nuclei segmentation in microscopy images [9], [10], skin lesion segmentation in dermoscopy images [11], and multi-organ segmentation in CT images [12], [13].\nVarious types of feature interactions in deep learning methods have a significant impact on the performance and capabilities of MedISeg models [14]\u2013[16]. Convolutional neural networks (CNNs) [17]\u2013[19] conceptualize an image as a grid of pixels and compute element-wise multiplication between kernel weights and pixel values in a local field. While CNNs are effective at capturing local features, their ability to model long-range feature interactions is limited, which affects their performance.\nTo address this limitation, Vision Transformer (ViTs) models have been introduced [20]\u2013[22]. ViTs treat an image as a sequence of patches, similar to tokens in natural language processing (NLP), allowing each token to attend to every token in the image and enabling ViTs to model long-range feature interactions. Due to the long-range feature interactions of ViT models, they can capture global contexts and demonstrate superior performance in MedISeg tasks [20], [23], [24]. However, the quadratic complexity of ViTs' long-range feature interactions in relation to the number of tokens has posed challenges in applying downstream applications.\nTo address these challenges, an efficient visual state space model called VMamba is introduced with linear complexity [25], [26]. VMamba draws inspiration from the Mamba, originally designed for NLP tasks [27]. The Mamba model processes input data causally, leveraging its causal properties to perform effectively and efficiently in language modeling tasks. However, the non-causal nature of images poses challenges for the Mamba model in handling image data using its causal processing approach. In contrast, VMamba addresses this issue by patching and flattening an image, then scanning the flattened image to integrate information from all locations in various directions, creating a global receptive field without increasing linear computational complexity. This innovative approach has led to the development of several Mamba-based models in the MedISeg domain [28], [29]. Despite the benefits of global scanning methods, they may overlook local features and spatial context information. LocalMamba introduces a local scanning method that confines the scan within a local window, enabling the capture of local dependencies while maintaining a global perspective [26]. However, existing fixed scanning approaches may struggle to adaptively capture spatial relationships.\nTo address the above problems, we introduce a simple yet effective Context Clustering Vision Mamba (CCViM) model, which is a U-shaped architecture designed for MedISeg that effectively captures both global and local feature interactions. our CCViM can address the limitations of fixed scanning strategies by proposing the context clustering selective state space model (CCS6) layer, which combines the cross-scan layer with our context clustering (CC) layer to capture local spatial visual contexts dynamically. Compared to convolution scanning images in a grid-like manner or attention exploring mutual relationships within a sequence, our CC layer views the image as a set of data points and organizes these points into different clusters, which can dynamically capture local feature interactions. Comprehensive experiments are conducted on nuclei segmentation [31], [32], skin lesion segmentation [33], [34], and multi-organ segmentation [35] tasks to demonstrate the superior performance effectively and efficiently of our CCViM in MedISeg.\nThe main contributions of this paper are as follows: (1) Introduction of a novel Mamba-based model for MedISeg, CCViM, which is effective and efficient. (2) Proposal of the CCS6 layer, which combines global scanning directions with the CC layer, enhancing the model's feature representation capability. (3) Proposal of the CC layer, which can dynamically capture spatial contextual information, improving model performance compared to fixed scanning strategies. (4) Comprehensive experiments conducted on five public MedISeg datasets show that CCViM outperforms existing models, demonstrating superior performance."}, {"title": "II. RELATED WORK", "content": "MedISeg is crucial for physicians in diagnosing specific diseases, as it delineates the regions of interest with precision [1]\u2013[3]. Automatic MedISeg has been extensively researched to address the limitations of manual segmentation, which is often time-consuming, labor-intensive, and subjective [5], [6], [36]. Consequently, deep learning methods such as convolutional neural networks (CNNs) [17]\u2013[19] and vision transformers (ViTs) [20], [24], [30], [37] have been widely applied in MedISeg tasks [38]. For instance, UNet [39] is favored for its simple architecture and robust scalability, which derives many U-shaped models for MedISeg tasks [36]. UNet++ [40] proposes nested encoder-decoder sub-networks with skip connections for MedISeg. TransUnet [20] proposes the first Transformer-based model for MedISeg tasks. Swin-Unet [24], a pure Transformer-based U-shaped model, adopts a hierarchical Swin Transformer [41] block with shifted windows as the encoder and performs effectively in MedISeg. However, CNNs' performance is limited with the local respective field, and Transformer-based models require quadratic complexity.\nTo overcome these challenges, Mamba [27] has been proposed to solve the computational efficiency problem and modeling long-range dependencies. In this paper, our research is also based on Mamba. Our contribution is to propose a more efficient local interaction mechanism.\nFeature interaction matters, and it greatly affects MedISeg's performance. CNNs have dominated the field of computer vision in recent years, benefiting from some key inductive biases e.g., locality and translation equivalence [14], [42]. The CNN's locality helps identify distinct and stable points in images, e.g., corners, edges, and textures, which are significant for detecting small and irregularly shaped target features in MedISeg [43], [44]. However, the local feature interaction cannot accurately address the complex and interconnected anatomical structures in MedISeg, which also requires long-range feature interactions to capture the global context and spatial relationships within the medical image [45]. In recent years, ViTs have emerged as effective long-range feature interaction methods in MedISeg tasks, e.g., Swin-Unet [24], UTNet [46], and Missformer [47]. Since ViTs abandon the local bias from CNNs, some researchers combine convolution and attention to acquire both local and global features, e.g., TransUnet [20] and Conformer [22]. Although the above methods inherit the advantages from short-range and long-range features and achieve better performance, the insights and knowledge are still restricted to CNNs and ViTs [48]. Besides, CNNs or ViTs are limited in modeling long-range dependencies or computational efficiency. Therefore, except for Mamba [27], we also attempt to employ a new paradigm for visual representation by using a clustering [48]. A clustering algorithm views an image as a set of points, allowing it to capture local topology information adaptively without introducing significant computational complexity."}, {"title": "C. Vision Mamba (ViM)", "content": "Mamba [27], originally designed for the NLP tasks, proposes a novel selective mechanism that focuses on relevant information or filters out irrelevant information. Mamba [27] eliminates the limitations in CNNs and Transformers, and achieves both effectiveness and efficiency. However, due to the intrinsic non-causal characteristic of images, there is a significant obstacle to Mamba's comprehension of images. Therefore, a series of Mamba-based models have been proposed to address this issue. For instance, the representative ViM [49] flattens spatial data into 1D tokens and scans these tokens in two different directions to address the non-casual problem in the vision domain. However, flattening spatial data into 1D tokens disrupts the natural 2D dependencies.\nTo address this problem, VMamba [25] introduces a cross-scan module to bridge the gap between 1D array scanning and 2D dependencies, effectively resolving the non-causal problem in visual images without compromising the receptive field. Besides, influenced by the success of VMamba [25], VM-UNet [28] introduces a visual Mamba UNet for MedISeg. However, these Mamba-based methods overlook the local features, which are both important in the vision domain. Therefore, LocalMamba [26] introduces a local scan strategy to capture both global and local information. However, such fixed scanning approaches are limited in their ability to dynamically capture spatial relationships. To address this challenge, we propose the CCS6 module, which integrates scanning modules with our CC to adaptively extract both global and local features while capturing spatial context information."}, {"title": "III. OUR METHOD", "content": "Mamba [27] is a causal visual state space model, which is both effective and efficient, identifying that a key weakness of the structured state space model (SSM) is its inability to perform content-based reasoning. To overcome this, Mamba introduces a selective state-space model (S6). The recurrent formula of SSM can be defined as follows:\n$h'(t) = Ah(t) + Bx(t),$\n$y(t) = Ch(t),$\nwhere $h(t) \u2208 R^N$ is the intermediate latent state to map $x(t) \u2208 R$ to $y(t) \u2208 R$. $A \u2208 R^{N\u00d7N}$ and $B\u2208 R^{N\u00d7N}$ are discretized by $A = exp(\u2206A)$ and $B = (\u2206A)^{-1}(exp(\u2206\u0391) \u2013 \u0395)\u00b7 \u0394\u0392$. A is the timescale parameter for discretizing the parameters A and B. Through discretization, continuous-time system SSM can be integrated into deep-learning models. $C \u2208 R^{N\u00d71}$ is the parameter matrix. E is the identify matrix. On the basis of SSM, S6 lets the SSM parameters be functions of the input, allowing the model to selectively focus on or filter out information. Making the parameters A, B, C depend on the input x, and the formulas are defined as follows:\n$S_B(x) = Linear_N(x),$\n$S_C(x) = Linear_N(x),$\n$s_\u25b3(x) = softplus(Parameter+Broadcast_D (Linear_1(x))),$\nwhere $Linear_N$, $Linear_1$ are fully connected (FC) layers to project the embedding dimension of x to N and 1. $Broadcast_D$ matches the dimension of the output of x to the Parameter. softplus is an activation function. Integrating S6, the Mamba not only achieves linear computation, but also performs excellent in language modeling tasks [27], [50]\u2013[52]. However, due to the inherent non-causal characteristics of images, it's difficult for Mamba-based models [27] to handle image data well. Fortunately, VMamba [25] proposes a cross-scan module to scan the patched and flattened image in four directions, which can address the non-causality of the image without compromising the field of reception and increasing computational complexity."}, {"title": "B. Overall Architecture", "content": "The overview of CCViM is illustrated in Fig. 2 (a), which is composed of a patch embedding layer, an encoder, a decoder, a final projection layer, and skip connections. CCViM is not a symmetrical structure like UNet [39], but adopts an asymmetric design. First, input the image $I \u2208 R^{3\u00d7W\u00d7H}$ into the patch embedding layer to divide I into non-overlapping patches of size 4 \u00d7 4, getting a feature map with dimensions W of $C\u00d7\\frac{W}{4}\u00d7\\frac{H}{4}\u00d7C$, where C is the number of feature channels. Then, input the feature map into the encoder network. Each stage in the encoder network is composed of two CCV\u0130M blocks and one patch merging layer, while the last stage only has two CCViM blocks without patch merging layer after them. The patch merging layer is for reducing the height and width of the feature maps and increasing the number of channels, and the channels of feature maps in the four stages are [C, 2C, 4C, 8C]. Thirdly, the feature maps are translated into the decoder, which also has four stages. Each stage of decoder is composed of one patch expanding layer and two CCViM blocks, while the first stage only has two CCViM blocks without patch expanding layer before them. The patch expanding layer is for increasing the height and width of the feature maps and reducing the number of channels, and the channels of feature maps in the four stages are [8C, 4C, 2C, C]. Furthermore, we simply employ the addition skip connections to capture low-level and high-level features. Besides, we adopt the most foundational Cross-Entropy and Dice loss for our MedISeg tasks. The CCViM block is the core component of CCViM. The input is first translated into the normalization layer and then split into two branches. In the first branch, the input passes through a linear layer followed by an activation layer. In the second branch, the input passes through a linear layer, depth-wise convolution, and an activation layer and then translates into the core module of the CCViM block: context clustering selective state space model (CCS6) layer. After normalizing the output of CCS6 layer, multiply it with the output from the first branch to merge the out of the two pathways. Finally, a linear layer is used to project the merged features onto the dimensions of the initial input features to establish residual connections."}, {"title": "C. Context Clustering Selective State Space Model", "content": "Context clustering selective state space model (CCS6) layer is the core component of the CCViM block, which adopts a selective state space model (S6) as its footstone. S6 processes the input data causally, resulting in only capturing vital information within the scanned part of the data, which is difficult for processing non-causal images. Based on which, numerous researches have also proposed various scan strategies to solve this problem well [25], [26], [28], [49]. However, these global scanning methods overlook the local features and the spatial context information in medical images. Furthermore, all these fixed scanning approaches cannot effectively capture spatial relationships adaptively. To overcome these challenges, we propose a CCS6 layer to extract both global and local features while capturing spatial context information in a learnable way. We adopt VMamba's [25] cross-scan module, which proposes a selective scan mechanism across four different directions. we patch and flatten the input image, and scan the flattened image in both horizontal and vertical directions to capture the global information. At the same time, our CC layer performs learnable local context clustering in local windows. Additionally, the CC layer employs two different numbers of clustering centers-4 and 25-to capture varying structural information. Consequently, there are two distinct CC layers and four different scanning directions available for selection. To avoid introducing too much computational complexity, in each CCS6 layer, we select four from the six choices. The detailed configuration of these choices will be introduced in section IV-B."}, {"title": "D. Context Clustering", "content": "Instead of using convolution or attention to extract information, we use the context cluster (CC) algorithm [48] for local extraction and spatial context information. In our CC layer, we view the image as a set of data points and group all points into clusters. Instead of clustering data points over the entire image, which will bring a significant computational cost. We split the image into distinct windows and limit the clustering operation to a local region. In each cluster, we aggregate the points into a center adaptively. after the patch embedding layer, the image $I \u2208 R^{3\u00d7W\u00d7H}$ is transformed into patched feature maps $F \u2208 R^{d\u00d7w\u00d7h}$. We convert the patched feature maps into a set of data points $P \u2208 R^{d\u00d7n}$, where $n = w \u00d7 h$ represents the total number of data points, d is the point feature dimension. These points are unordered and disorganized. We then group the data points into several clusters based on their similarities, ensuring that each point is assigned to only one cluster.\nFor CC layer, We first project P to $P_s \u2208 R^{n\u00d7d'}$ to compute the similarity, where d' is the new point feature dimension. We evenly propose t centers in each local window and compute the center feature by averaging its k nearest points. Then, we calculate the pair-wise cosine similarity between the resulting t center points and $P_s$ to get the similarity matrix $S\u2208 R^{t\u00d7n}$. Based on the similarity, we allocate each point to the most similar center to get t clusters. Furthermore, each cluster may exhibit a varying number of data points. In exceptional cases, some clusters may contain no data points, indicating that they are redundant. During the clustering, all data points in one cluster are dynamically aggregated into the center point based on the similarities. In a cluster, there is a small set of m data points, represented by $P_m \u2208 R^{m\u00d7d'}$. Calculating the similarity between the m data points $P_m \u2208 R^{m\u00d7d'}$ and the center, which is represented by $s\u2208 R^m$ and is the subset of similarity matrix S. We map the m data points $P_m \u2208 R^{m\u00d7d'}$ to a value space to get $P'_m \u2208 R^{m\u00d7d'}$. We also generate a value center $v_c$ of the $P'_m$ in the value space, which is like the clustering center proposal. Then the aggregated feature $g\u2208 R^{d'}$ is formulated as follow:\n$g=\\frac{1}{T}(v_c + \u03a3^m_{i=1}\u03c3 (\u03b1s_i + \u03b2) * P_i)$\ns.t., $T = 1 + \u03a3^m_{i=1}\u03c3 (\u03b1s_i + \u03b2),$\nwhere a and \u1e9e are learnable scalars used to scale and shift the similarity, and is a sigmoid function that re-scales the similarity to the range (0, 1). $p_i$ denotes i-th point in $P_m$. For numerical stability and to emphasize locality, we incorporate the value center $v_c$ in Eq. (3). To control the scale, normalizing the aggregated feature by a factor of T. Subsequently, adaptively dispatching the aggregated feature g to each data point in a cluster based on the similarity. This approach facilitates the communication of the points in the cluster, enabling them to share features in the cluster. For each data point $p_i$ of $P'_m$ in the cluster, updating it by:\n$p'_i = p_i + FC(\u03c3(\u03b1s_i + \u03b2) * g),$\nwhere s denotes similarity, using the same procedures as above. We apply a fully connected (FC) layer to project the feature dimension from the value space dimension d' to the original dimension d."}, {"title": "E. Post Processing", "content": "In the nuclei segmentation task (i.e., Kumar [31] and CPM17 [32]), we employ the post-processing method watershed algorithm to distinguish between individual nuclei instances. Following research [9], we create horizontal and vertical distance maps by calculating the distances from nuclear pixels to their centers of mass in both the vertical and horizontal directions. Within our model we predict the vertical distance maps $P_v$ and horizontal distance maps $P_h$. Additionally, we apply the Sobel operator to these distance maps to obtain the horizontal and vertical gradient maps. We then select the maximum value between these gradient maps: $M = max(Sobel(P_v), Sobel(P_h))$. This process aids in edge detection by calculating the gradient magnitude, thereby emphasizing areas of significant intensity change. Next, we calculate the markers, M, by applying a threshold function to the probability map P and gradient map $M_s$, where the markers are defined as $M = \u03b4(\u03c4(P,r) \u2013 \u03c4(M_s, k))$. Here, \u03c4(P,r) is a threshold function, with r being the threshold value. If the value exceeds r, it is set to 1; otherwise, it is set to 0. 8 is used to set negative values to 0. We then obtain the energy landscape $E = (1 \u2013 \u03c4(M_s,k)) \u00d7 \u03c4(P, h)$. Finally, M serves as the marker during marker-controlled watershed to determine how to split (P, h), guided by the energy landscape E."}, {"title": "F. Loss Function", "content": "For a fair comparison with other state-of-the-art methods, we employ the most fundamental loss functions in medical image segmentation: Cross-Entropy and Dice loss [9], [28], [53]. Cross-entropy ensures pixel-level classification accuracy, while Dice loss addresses the common issue of class imbalance by optimizing the overlap between the predicted and true segmentation [54]. As shown in Eq. (5) and Eq. (6), we combine Cross-Entropy and Dice loss to balance precise pixel-wise classification with overall segmentation performance.\n$L = L_{Ce} + L_{Dice}$\n$L_{ce} = -\u03a3^N_{i=1}\u03a3^C_{c=1} y_{i,c}log(\\hat{y}_{i,c})$\n$L_{Dice} = 1 - \\frac{2\u03a3^N_{i=1} y_i\\hat{y_i}+1}{\u03a3^N_{i=1} y_i + \u03a3^N_{i=1} \\hat{y}_i+1}$\nwhere, N denotes the total number of samples, and C represents the total number of categories. $y_{i,c}$ is an indicator of ground truth, equals 1 if sample i belongs to category c, and 0 if it does not. $\u0177_{i,c}$ is the probability that the model predicts sample i as belonging to category c. $y_i$ and $\u0177_i$ represent the ground truth and prediction, respectively."}, {"title": "IV. EXPERIMENTS", "content": "Datasets. We evaluate CCViM on five MedISeg datasets, which contain nuclei segmentation datasets (i.e., Computational Precision Medicine (CPM17) [32] and Kumar [31]), skin lesion segmentation datasets (i.e., ISIC17 [33] and ISIC18 [34]), and Synapse multi-organ segmentation dataset (i.e., Synapse [35]). These datasets are detailed as follows:\n\u2022 Kumar: The size of images is 1000 \u00d7 1000 pixels at 40\u00d7 magnification. The total number of nuclei is 21623. Following [31], we split the dataset into two different sub-datasets: (i) Kumar-Train, a training set with 16 images, and (ii) Kumar-Test, a test set with 14 images. Following previous research [9], we crop each training image into 540 \u00d7 540 with an overlap of 164 pixels, then resize into 250\u00d7250. Data augmentation, including flip and rotation, is applied to all methods. In the inference, the images are cropped into 250 \u00d7 250 with an overlap of 164 pixels.\n\u2022 CPM17: The size of given images is 500 \u00d7 500 to 600 \u00d7 600 pixels at 40\u00d7 or 20\u00d7 magnification. The total number of nuclei is 7570. Following the challenge [32], we split 32 images in the training dataset and 32 images in the test dataset. The data processing also follows the previous research [9].\n\u2022 ISIC17 & ISIC18: The International Skin Imaging Collaboration 2017 and 2018 challenge datasets (ISIC17 [33]\nand ISIC18 [34]) contain 2,150 and 2,694 images with segmentation masks, respectively. Following previous research [55], we allocate a 7:3 ratio for training and test sets, resize the images to 256 \u00d7 256, and apply data augmentation like flip and rotation.\n\u2022 Synapse: Synapse multi-organ segmentation dataset [35] comprises 30 abdominal CT cases with 3779 axial abdominal clinical CT images, including 8 types of abdominal organs (aorta, gallbladder, left kidney, right kidney, liver, pancreas, spleen, stomach). Following previous research [20], [24], we allocate 18 cases for training and 12 cases for testing. We resize the images to 224 \u00d7 224 and apply augmentation including flip and rotation.\nEvaluation metrics. (1) In nuclei segmentation [31], [32], we employ the ensemble dice (DICE), aggregated Jaccard index (AJI), panoptic quality (PQ), detection quality (DQ), and segmentation quality (SQ) as the main evaluation metrics. PQ is composed of DQ and SQ, offering precise quantification and interpretability for evaluating the performance of nuclei segmentation. (2) In skin lesion segmentation, to compare our CCViM with previous methods on ISIC17 [33] and ISIC18 [34] datasets, we employ mean intersection over union (mIoU), dice similarity coefffcient (DSC), accuracy (Acc), sensitivity (Sen), and specificity (Spe) as the main evaluation metrics. (3) In Synapse [35] multi-organ segmentation [35], we employ dice similarity coefffcient (DSC) and the 95% Hausdorff distance (HD95) as the main evaluation metrics."}, {"title": "B. Configuration of Scan Directions and Local Clusters", "content": "In this paper, we do not apply the redundancy 22 scan strategies [56] in each CCS6 layer, and there are only one, two, or three different scanning directions in each CCS6 layer. To extract local features and capture the spatial context information adaptively, we integrate our CC layer into each CCS6 layer. As shown in Fig. 3, each CCS6 layer contains only four modules, which include one, two, or three different scanning directions and one or two different CC layers. This design avoids adding additional computational overhead compared to previous models. There are a total of four different scanning directions to choose from, including horizontal, horizontal flipped, vertical, and vertical flipped. Given the different sizes and number of image targets, we adopt two different CC layers. One CC layer method has 4 cluster centers in a local region, and the other one has 25 cluster centers in a local region. In nuclei segmentation tasks [31], [32], there are many nuclei in one local region. In skin image segmentation tasks [33], [34], the target is large and may need lower point centers in one local region. We adopt the configuration provided by LocalMamba [26], with our CC lyaer replacing the local scan. Compared with the fixed local scan strategies in LocalMamba [26], our CC layer can dynamically cluster local features and capture spatial information adaptively."}, {"title": "C. Implementation Details", "content": "In our all experiments, we set the batch size to 32 and employ AdamW [57] optimizer with an initial learning rate of 1e-3. CosineAnnealingLR [58] is employed as the scheduler. We set the training epochs to 300. All experiments initialize the models with ImageNet [59] pretrained weights. All experiments are conducted on the PyTorch deep learning platform with a single NVIDIA GeForce RTX 3090 GPU."}, {"title": "D. Comparisons with State-of-the-art Methods", "content": "Results on nuclei segmentation. We compare our CCViM with CNN-based, Transformer-based, and Mamba-based models. Table I shows the results of different models on Kumar [31] and CPM17 [32] datasets. In Table I, the TransUnet [20] and Swin U-Net [24] have superior results than U-Net [39], especially on CPM17 [32] datasets. Compared to the methods above, VM-UNet [28] demonstrates enhanced performance, particularly on the PQ and Dice metrics. This demonstrates the effectiveness of the Mamba-based model. On the Kumar dataset [31], VM-UNet [28] performs worse than Hover-Net [9] due to the small and dense nature of the nuclei on this dataset, which requires local feature interactions to capture the subtle details of these objects. In contrast, our CCViM outperforms Hover-Net [9], improving the PQ, Dice, AJI, DQ, and SQ by 0.61%, 1.16%, 1.76%, 0.88% and -0.08%, respectively. This demonstrates that our CCViM is superior at capturing local features. On the CPM17 dataset, compared to VM-UNet [28], our CCViM has improved the PQ, Dice, AJI, DQ, and SQ by 0.48%, 0.44%, 0.82%, 0.82% and -0.24%, respectively. Overall, our CCViM has the best performance in the PQ metric, indicating its ability to achieve more precise separation of individual nuclei. As shown in Fig. 4, our CCViM demonstrates superior performance on both the Kumar [31] and CPM17 [32] datasets by accurately segmenting small and overlapping nuclei and delineating edges precisely. In contrast, other methods tend to merge distinct nuclei into a single entity or over-segment them. These results underscore the exceptional capability of our CC layer in local feature extraction, effectively capturing subtle differences and boundary details between nuclei, thereby significantly enhancing segmentation accuracy.\nResults on skin image segmentation. We compare our CCViM with several state-of-the-art models on the ISIC17 [33] and ISIC18 [34] datasets, Table II shows the main results. Compared with CNN-based (i.e., UNet [39] and UT-NetV2 [62]) and Transformer-based (i.e., TransFuse [63] and MALUNet [55]) methods, the Mamba-based models (i.e., VM-UNet [28], HC-Mamba [29]) have the superior performance, which demonstrates the effectiveness of Mamba-based models in MedISeg. From Table II, on the ISIC17 dataset, we can observe that our CCViM has improved the mIoU and DSC by 1.17% and 0.71% compared with VM-UNet [28]. On the ISIC18 dataset, our CCViM has improved the mIoU and DSC by 0.57% and 0.35% compared with VM-UNet [28]. These superior results demonstrate that our CCViM effectively addresses the limitations of Mamba-based models, particularly in preserving local features and spatial context. Fig. 5 shows the visualizations on the ISIC17 dataset, we can observe that our CCViM exhibits accurate and sharp contours, effectively capturing object boundaries. Fig. 6 shows the visualization on the ISIC18 [34] dataset, where our CCViM outputs highlight a closer alignment between the red and green contours, indicating that CCViM is more effective in accurately delineating skin lesions compared to VM-UNet [28]. The VM-UNet [28] often produces over-segmented or merged regions. While VM-UNet [28] performs well in some cases, CCViM demonstrates greater robustness, particularly when handling small lesions and irregular boundaries, offering a closer adherence to the ground truth. These visualizations further demonstrate that the local feature interactions in our CCViM can capture the subtle details of edge contours and irregularly shaped target features.\nResults on synapse multi-organ segmentation. We also compare our CCViM with state-of-the-art models on the Synapse dataset, where similar observations and conclusions can be observed. In Table III, Mamba-based models have superior performance compared with CNN-based and Transformer-based models. Compared with VM-UNet [28], our model has improved the DSC by 1.57% and has reduced the HD95 by 1.38%. Our model gets state-of-the-art results in segmenting 8 types of abdominal organs (aorta, gallbladder, left kidney, right kidney, liver, pancreas, spleen, stomach). Fig. 7 shows the visualizations, compared to VM-UNet [28], we can observe that our CCViM not only segments various organs more accurately but also delineates edges with greater precision. The superior performance further demonstrates the effectiveness of both global and local feature interactions in our CCViM."}, {"title": "E. Ablation Analysis", "content": "Superiority of the context clustering (CC) layer. To assess the effectiveness of our CC layer, we conduct the ablation experiments on the recent state-of-the-art models LocalVIM [26", "26": "using Kumar and ISIC17 datasets. The local scanning strategies in LocalVIM [26"}, {"26": "can effectively capture local dependencies while maintaining a global perspective, however, the scanning strategies rely on fixed propagation trajectories, which cannot adaptively capture local features and ignore the spatial context information. Our CC layer can cluster the local features and capture spatial context information in an adaptive way. Therefore, we exchange the local scanning strategies in LocalVIM [26"}, {"26": "with our CC layer. On Kumar, our CC layer has improved the PQ, Dice, AJI, DQ, and SQ metrics by 1.57%, 0.15%, 1.68%, 1.75% and 0.39% respectively, compared to LocalVIM [26"}, {"26": "."}, {"26": "."}, {"26": "."}]}