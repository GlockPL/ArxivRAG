{"title": "GET-UP: GEomeTric-aware Depth Estimation with Radar Points UPsampling", "authors": ["Huawei Sun", "Zixu Wang", "Hao Feng", "Julius Ott", "Lorenzo Servadei", "Robert Wille"], "abstract": "Depth estimation plays a pivotal role in autonomous driving, facilitating a comprehensive understanding of the vehicle's 3D surroundings. Radar, with its robustness to adverse weather conditions and capability to measure distances, has drawn significant interest for radar-camera depth estimation. However, existing algorithms process the inherently noisy and sparse radar data by projecting 3D points onto the image plane for pixel-level feature extraction, overlooking the valuable geometric information contained within the radar point cloud. To address this gap, we propose GET-UP, leveraging attention-enhanced Graph Neural Networks (GNN) to exchange and aggregate both 2D and 3D information from radar data. This approach effectively enriches the feature representation by incorporating spatial relationships compared to traditional methods that rely only on 2D feature extraction. Furthermore, we incorporate a point cloud upsampling task to densify the radar point cloud, rectify point positions, and derive additional 3D features under the guidance of lidar data. Finally, we fuse radar and camera features during the decoding phase for depth estimation. We benchmark our proposed GET-UP on the nuScenes dataset, achieving state-of-the-art performance with a 15.3% and 14.7% improvement in MAE and RMSE over the previously best-performing model.", "sections": [{"title": "1. Introduction", "content": "Understanding the 3D environment surrounding the ego vehicle is essential in the autonomous driving field, requiring the estimation of dense depth maps for 3D scene reconstruction. While learning-based monocular depth estimation methods [1, 6, 9, 12, 19, 22, 32] have outperformed traditional monocular-based approaches [35, 36, 46] in accuracy, they are still constrained by the lack of robust geometric constraints. To address this limitation, methods [7, 10, 15, 28, 30, 34, 42, 50] leveraging both depth sensors (i.e. LiDAR) and RGB images to first project LiDAR points onto the image plane, resulting in a sparse depth map. However, these methods require additional tasks such as surface normal estimation for improved feature learning [34].\nAlthough LiDAR provides detailed information about 3D scenes, it is prohibitively expensive and sensitive to weather conditions. In contrast, radar offers robust performance in all weather conditions and is more cost-effective than LiDAR. However, the absence of height information in radar data and noisy characteristics lead to significant errors when projecting radar points onto the image plane. To illustrate how this discrepancy complicates depth estimation, we analyzed the absolute depth differences between each radar point and its nearest corresponding LiDAR point on the 2D image plane across the dataset. As shown in Figure 2, the depth values associated with radar points frequently deviate significantly from those of LiDAR, which serves as the ground truth. This discrepancy highlights why LiDAR-camera depth completion algorithms, which typically propagate depth information from LiDAR points to surrounding pixels, are ill-suited for radar-camera setups, indicating that radar-specific algorithms need to be developed.\nStudies like [3, 16] directly project radar points onto the image plane, resulting in sparse and ambiguous radar projection maps. Others extend the height of each radar point [24, 29, 39, 41] or adopt two-stage processes producing semi-dense radar depth maps [26, 37, 40] to mitigate this issue. However, these methods often distort 3D geometric details, thereby limiting feature extraction in 2D space and introducing further noises into the radar data by directly altering the radar input.\nTo address these challenges, we propose GET-UP, a novel radar-camera depth estimation framework that utilizes radar input across two domains. Firstly, the 3D radar points are projected onto the image plane and densified by"}, {"title": "2. Related Work", "content": "Geometry-aware Depth Completion. Initial studies in LiDAR-camera depth completion [7, 15, 28, 30, 42, 50] predominantly perform depth completion within the 2D image plane by projecting sparse LiDAR points onto it, which fall short of capturing the underlying 3D geometric information. Instead, the following studies also extract features from the 3D perspective. Xiong et al. [49] employ a GNN by treating each image pixel as a graph node and establishing connections based on the k-Nearest-Neighbor (kNN) principle in 3D space. Further advancements include graph propagation techniques as seen in [53], enhancing multi-modal feature integration. Moreover, Point-Fusion [14], FuseNet [5], and [52] extract 3D features from 3D LiDAR points and consolidate 2D and 3D features.\nNevertheless, these approaches are constrained by relying on a predefined number of LiDAR points as input, which cannot handle the various number of radar points.\nRadar-Camera Depth Estimation. Radar point clouds are significantly sparser and noisier than LiDAR, presenting a challenge for generating dense depth maps from images and radar data. Lin et al. [23] directly project radar points onto the image plane, yielding highly-sparse and ambiguous radar maps. To mitigate the sparsity issue, [20, 24] extend radar points vertically, creating denser radar projection maps. Differing from direct radar-to-image projection, [26, 37, 40] propose two-stage architectures that explore one-to-many mapping from radar data to image pixels in the first stage, producing denser intermediate radar data for subsequent depth prediction.\nHowever, the densify processes in the existing studies introduce further noises into the radar data since they directly modify the radar input. Furthermore, the projection process overlooks the 3D geometric information of radar data.\nPoint Cloud Upsampling. Point cloud upsampling is a typical task for point cloud densification, which is designed to transform sparse and noisy point clouds into denser and cleaner counterparts [8, 21, 27, 33, 51]. This procedure typically begins with extracting point features, followed by point expansion and coordinate reconstruction, a methodology initially introduced by PU-Net [51]. Subsequently, PU-GAN [21] innovated by incorporating an adversarial network to optimize point distribution. Further, GNNs are utilized in PU-GCN [33] for both feature extraction and expansion phases to improve point cloud quality. Du et al. [8] advances by introducing a cascaded refinement network that employs a residual learning approach for incremental improvements. Nevertheless, point cloud upsampling task has not been used by existing radar-camera depth estimation methods to mitigate the radar point sparsity problem.\nIn this study, diverging from conventional radar-camera depth estimation techniques, which distort 3D geometric clues, we advance feature extraction by considering both 2D radar projection maps and 3D radar points. To address the challenges of sparsity and ambiguity inherent in radar point clouds, we are the first to introduce a point cloud up-sampling module into the depth estimation task. This module, distinctively utilizing existing LiDAR points as ground truth, aims to both densify the radar data and enhance the precision of radar point positioning."}, {"title": "3. Approach", "content": "This section introduces the innovations of our work. In Sec. 3.1, we explain the model architecture. Subsequently, our proposed radar feature extraction module, including five submodules, is described in Sec. 3.2. Finally, we present our decoder of the depth estimation task in Sec. 3.3."}, {"title": "3.1. Model Architecture", "content": "As visualized in Figure 3, our model processes two key inputs: an image $X_{RGB} \\in \\mathbb{R}^{H \\times W \\times 3}$ and a radar point cloud $r \\in \\mathbb{R}^{N \\times C_r}$, where N denotes the number of radar points in the current frame and $C_r$ is the number of features carried by the radar. Radar projection map $X_{Radar} \\in \\mathbb{R}^{H \\times W \\times C_R}$ is generated by projecting these N points onto the image plane, carrying specific attributes such as depth, velocities, and Radar Cross Section (RCS). Additionally, the point cloud r encompasses 3D positional data.\nThe RGB image $X_{RGB}$ is processed through a ResNet-34 encoder [13], yielding multi-scale features ${F_{img}}_i^{i=1}{5}$. Concurrently, $X_{Radar}$ and $r$ are processed by a dedicated radar feature extraction module, designed to extract coherent radar features by aggregating 2D and 3D information with five submodules. Notably, it includes a point cloud upsampling submodule aimed at leveraging precise LiDAR point positions to adjust and densify radar point representations. This submodule efficiently extracts features reflective of LiDAR positions, which are then used to enhance the radar-derived features. The output of this radar feature extraction process is a set of refined radar features ${F_{2d}}_i^{i=0}{5}$, including a generated feature $F_{2d}^0$ with the same size as the input image, and an upsampled 3D point cloud $R_{up}$. Sec. 3.2 provides a more detailed explanation of this process.\nThese processed image and radar features are subsequently fused through a gated fusion mechanism [37] and then fed into a depth estimation model [19]. This final step produces a comprehensive dense depth map $\\hat{D} \\in \\mathbb{R}^{H \\times W}$. More details are available in Sec. 3.3."}, {"title": "3.2. Radar Feature Extraction Module", "content": "The radar feature extraction module comprises five distinct submodules with $X_{Radar}$ and $r$ as inputs. Initially, the 2D feature extraction submodule processes $X_{Radar}$ to generate multi-scale feature maps. Subsequently, these maps, jointly with $r$, are input into the 3D feature extraction submodule, to distill 3D geometry-aware features. Afterwards, the 2D-3D feature aggregation submodule processed the 2D and 3D features, yielding enhanced and more reliable 3D feature representations. The obtained 3D features, integrated with $r$, undergo further enhancement in the point cloud upsampling submodule to increase data density and precision. Lastly, a feature refinement submodule is em-"}, {"title": "3.2.1 2D feature extraction submodule.", "content": "We introduce ASCB with sparse convolution layers [43] to address the challenge of highly sparse radar projections. This component adaptively adjusts the convolution kernel size based on the depth information of radar points. Initially, $X_{Radar}$ undergoes processing by the ASCB, followed by a ResNet-18 backbone to further refine the ASCB output, yielding five feature sets ${F_{2d}^a}_i^{i=1}{5}$ across different scales i, with each set $F_{2d}^a \\in \\mathbb{R}^{H \\times W \\times C_i}$. Suppose the radar point cloud comprising N points, where $r_{2d} = \\{(x_{2d}^j, y_{2d}^j)\\}_{j=1}^{N}$ represent $j^{th}$ point's projected pixel coordinate. Both these coordinates and the aforementioned multi-scale features are inputs to the point feature selection block.\nAdaptive sparse convolution block. The Sparse Convolutional Network [43] utilizes an observation mask in each sparse convolutional layer to filter out \"unobserved\" pixels from the input during the convolution. However, objects that are farther away from the ego-vehicle appear smaller on the image plane. This leads to a challenge that it may upsample the projected points into a wrong scale, since all points are treated equally by a single mask.\nTherefore, we propose the ASCB, which employs three binary observation masks to categorize radar detections by distance. Then, different convolution kernel sizes are selected for each group to enable precise feature propagation across different area sizes. Following a general statistical analysis of the projection size of common objects within our dataset, we select three distance range groups: [0, 40), [40, 70), and [70, +$\\infty$) meters. Within each range, a list of sparse convolutional layers with stride 1 is stacked to encode the input radar map according to the respective radar observation mask. We finally select the list of symmetric kernel sizes with [11, 7, 7, 5, 5, 3], [11, 7, 5, 5, 3, 3], and [11, 7, 5, 3] for the aforementioned three distance groups. The outputs of these three ranges are element-wise summed to generate the final output of this block. The detailed experiments are introduced in the supplementary material.\nPoint feature selection. After obtaining the multi-scale feature maps, we select the 2D features of each point at different scales. Thus, this block involves scaling the pixel coordinates according to the feature map's scale factor. At scale i, point features are extracted at the coordinates $(\\lfloor \\frac{x_{2d}}{2^i}\\rfloor, \\lfloor \\frac{y_{2d}}{2^i}\\rfloor )$, yielding a set of five point feature vectors ${f_{2d}^i}_{i=1}{5}$, with $f_{2d}^i \\in \\mathbb{R}^{N \\times C_i}$."}, {"title": "3.2.2 3D feature extraction submodule.", "content": "As illustrated in Figure 4, our graph model effectively incorporates five EdgeConv blocks [47], where the graph is constructed dynamically based on the KNN at each layer. This model aims to extract 3D point features from the input $r = \\{(x_{3d}^j, y_{3d}^j, z_{3d}^j, v_{x}^j, v_{y}^j, rcs_{i})\\}_{j=1}^{N}$, where the $j^{th}$ point is located at $(x_{3d}^j, y_{3d}^j, z_{3d}^j)$ in 3D space. The output of the first EdgeConv block $f_{3d}^a \\in \\mathbb{R}^{N \\times C_1}$, alongside $f_{2d}^a$, are input into a cross-attention block [45], with $f_{3d}^a$ acting as the query and $f_{2d}^a$ generating the keys and values. We also add skip connection after the cross-attention to mitigate the potential gradient vanishing issue, resulting in the refined feature $f_{3d}' \\in \\mathbb{R}^{N \\times C}$:\n$f_{3d}' = Attention(f_{3d}^aW_{3d}^q, f_{2d}^aW_{3d}^k, f_{2d}^aW_{3d}^v) + f_{3d}^a $ (1)\nSubsequently, $f_{3d}'$ progresses to the next EdgeConv block. We repeat the EdgeCov block and cross-attention five times, yielding five intermediate features ${f_{3d}^i}_{i=1}^{5}$, and they are further concatenated along the channel dimension processed by following MLP layers to generate the final feature output $f_{3d} \\in \\mathbb{R}^{N \\times C}$."}, {"title": "3.2.3 2D-3D feature aggregation submodule.", "content": "With $f_{3d}$ from the 3D feature extraction module and a set of 2D point features ${f_{2d}^i}_{i=1}^{5}$ at various scales i from the 2D feature extraction module, the cross-attention operation uses $f_{3d}$ as the query and $f_{2d}^i$ as both key and value, yielding five aggregated features ${f_{agg}^i}_{i=1}^{5}$, each matching the dimensions of $f_{2d}^i$:\n$f_{agg}^i = Attention(f_{3d}W_{agg}^q, f_{2d}^iW_{agg}^k, f_{2d}^iW_{agg}^v)$. (2)\nFurthermore, to derive a comprehensive global aggregated feature that spans all scales, the 2D features are concatenated across the channel dimension, and further apply cross-attention between this concatenated 2D feature and $f_{3d}$, resulting in a global aggregated feature $f_{agg} \\in \\mathbb{R}^{N \\times C}$:\n$\\bigoplus_{i=1}^{5} f_{2d}^i = f_{2d}^1 \\otimes f_{2d}^2 \\otimes f_{2d}^3 \\otimes f_{2d}^4 \\otimes f_{2d}^5 $\n$f_{agg} = Attention(f_{3d}W_{agg}^q, \\bigoplus_{i=1}^{5} f_{2d}^i W_{agg}^k, \\bigoplus_{i=1}^{5} f_{2d}^i W_{agg}^v)$, (3)\nwhere $\\otimes$ signifies the concatenation of features along the channel dimension."}, {"title": "3.2.4 Point cloud upsampling submodule.", "content": "As a key module of GET-UP, we enhance radar data quality by leveraging the precision of LiDAR data, aiming to accurately identify and rectify the positioning of radar points. We start by detailing the approach for generating ground truth data, followed by the upsampling model architecture.\nGround truth generation. Given the significant sparsity difference between LiDAR and radar point clouds-with radar detections being up to 1000$\\times$ sparser per frame [2]-it is impractical to use the entire LiDAR dataset as ground truth for upsampling. Instead, a subset of $N_L$ LiDAR points is selected for ground truth supervision. A naive approach is randomly sampling these $N_L$ points from the LiDAR point cloud. However, this method fails to account for the spatial relevance of LiDAR points to actual radar detections. To address this, we refine our selection process by first calculating the Chamfer distance [11] between radar and LiDAR points, then prioritize the N\u2081 LiDAR points with the smallest distances, effectively choosing those closest to the radar points as the upsampling ground truth $R_{gt}$.\nUpsampling model architecture. Similar to existing methodologies [8, 33], our approach focuses on learning the offsets of target points rather than directly predicting their 3D positions. Nevertheless, unlike the typical point cloud upsampling task, we encounter the challenge that the quantity of radar points varies from frame to frame, resulting in an unpredictable upsampling ratio across different frames. To solve this problem, our upsampling model, visualized in Figure 5, contains three components: a reshape block, $n_u$ upsample units with the upsampling rate $\\tau$, and a coordinate reconstruction block.\nInitially, radar points $r_{3d} = \\{(x_{3d}^i, y_{3d}^i, z_{3d}^i)\\}_{i=1}^{N}$, along with their global features $f_{agg}$, are processed by the reshape block, employing bilinear interpolation to scale the data to a predetermined number of points $n = N_L$. This generates a modified set of radar points $r_{3d}' = \\{(x_{3d}^i, y_{3d}^i, z_{3d}^i)\\}_{i=1}^{N_L}$ and their associated features $f_{agg}' \\in \\mathbb{R}^{n \\times C}$.\nInspired by [8], the process of our designed upsample unit involves duplicating the features $\\tau$ times and concurrently processing them through a transposed convolutional layer to derive new point features. These resultant features are then concatenated along the channel axis and refined through two MLP layers. Simultaneously, the input points are replicated by a factor of $\\tau$.\nUpon completing $n_u$ upsample units, we derive the upsampled feature vectors $F_{up} = \\{f_{up}^j\\}_{j=1}^{\\tau N}$ and the duplicated points $R_{up} = \\{(x_{up}^i, y_{up}^i, z_{up}^i)\\}_{j=1}^{\\tau N}$. The coordinate reconstruction block, utilizing $F_{up}$ as input, computes per-point offsets $\\Delta r$ through two MLPs. These offsets are subsequently added to the duplicated points, resulting in the final upsampled 3D point cloud $R_{up}$.\nThis module returns two outputs, the upsampled point cloud $R_{up}$ and the upsampled features $F_{up}$."}, {"title": "3.2.5 Feature refinement submodule.", "content": "This module augments the 2D features from two perspectives: utilizing aggregated features and incorporating up-sampled features. Firstly, we enrich the 2D feature maps ${F_{2d}^i}_{i=1}^{5}$ by integrating the aggregated points features ${f_{agg}^i}_{i=1}^{5}$ at each respective scale i. More precisely, for the $j^{th}$ point at the $i^{th}$ scale, its feature is added back to the projected pixel coordinates $(\\lfloor \\frac{x_{2d}}{2^i}\\rfloor, \\lfloor \\frac{y_{2d}}{2^i}\\rfloor)$ on $F_{2d}^i$. In parallel, the global aggregated feature $f_{agg}$ and the upsampled feature $F_{up}$ are projected onto the original, unscaled image plane served as a global feature map $F_{2d}^G$. Specifically, for the $j^{th}$ upsampled point $R_{up} = (x_{up}, y_{up}, z_{up})$, it is mapped to 2D coordinates using the camera's intrinsic and its associated upsampled feature is stored into $F_{2d}^G$. Importantly, any upsampled points in $R_{up}$ that fall outside the original image plane after projection is discarded.\nFinally, we concatenate $F_{2d}^G$ with the refined ${F_{2d}^i}_{i=1}^{5}$, yielding six comprehensive radar feature maps, which are passed to the decoder for the depth estimation task. Additionally, the upsampled 3D points $R_{up}$ are output to facilitate loss calculation for this specific branch."}, {"title": "3.3. Decoder", "content": "Our depth estimation framework is built upon the BTS model [19], leveraging the local planar guidance concept to enhance the upsampling process and extract more meaningful features. In this process, we incorporate both image features ${F_{img}^i}_{i=1}^{5}$ and radar features ${F_{2d}^i}_{i=0}^{5}$ as input"}, {"title": "3.4. Loss Functions", "content": "Our model employs two loss functions to facilitate depth estimation and point cloud upsampling tasks. To guide the depth estimation task, we follow methodologies from [37], accumulating LiDAR point clouds from neighboring frames to construct an accumulated depth map. Subsequently, we apply the scaffolding technique [48] to generate a dense depth map $D_s$. As demonstrated in [20], supervised by single scan depth $D_s$, improves the depth prediction accuracy. Thus, we utilize $D_s$ and $\\hat{D}$ to supervise our depth estimation task as follows:\n$L_{Depth} = \\frac{1}{N}\\sum_{x \\in \\Omega} |D_s(x) - \\hat{D}(x)| + \\frac{\\alpha}{N} \\sum_{x \\in \\Omega} |D(x) - \\hat{D}(x)|, $ (4)\nwhich only calculated within the sets of pixels where $D_s$ or $D$ are valid.\nThe Chamfer distance loss [11] is utilized to reduce the discrepancy between the upsampled point cloud $R_{up}$ and the ground truth $R_{gt}$:\n$L_{Up} = \\frac{1}{|R_{up}|} \\sum_{p \\in R_{up}} min_{q \\in R_{gt}}||p - q||_2 + \\frac{1}{|R_{gt}|} \\sum_{q \\in R_{gt}} min_{p \\in R_{up}}||p - q||_2$. (5)\nHere, $p$ represents a 3D point in $R_{up}$, and $q$ denotes a 3D point in $R_{gt}$. The term $|| \\cdot ||_2$ signifies the squared Euclidean distance.\nThe final loss function is a weighted sum of the individual losses: $L = L_{Depth} + \\alpha L_{Up}$, where $\\alpha$ is a weighting factor to balance the importance of the two tasks."}, {"title": "4. Experiments", "content": "This section first introduces the dataset and the implementation details. Then, we describe the evaluation metrics and compare our GET-UP with the existing methodologies in the quantitative and qualitative aspects. Finally, we conduct ablation studies to further underscore our proposed methods' effectiveness."}, {"title": "4.1. Dataset and Implementation Details", "content": "We utilize the nuScenes dataset [2], a comprehensive multi-sensor dataset dedicated to autonomous driving research, for the training and evaluation of our model. The"}, {"title": "4.2. Quantitative Results", "content": "In this study, we benchmark our GET-UP model against both image-based methods [1, 19, 32, 38] and existing radar-camera depth estimation approaches [20, 24-26, 37, 40], using the standard evaluation metrics, detailed in the supplementary material. The models are assessed on the official nuScenes test set across three evaluation ranges: up to 50 meters, 70 meters, and 80 meters, with detailed results presented in Table 1.\nOur GET-UP model demonstrates superior performance over image-only methods across all metrics. Specifically, it enhances the BTS baseline [19] by 33.8% in MAE and 23.3% in RMSE, highlighting the substantial benefits of incorporating radar data into image-based methods. Furthermore, we explored LiDAR-camera depth completion techniques [28, 44] using radar data to generate sparse depth maps. These attempts yielded unsatisfactory outcomes due to the even sparser and more ambiguous nature of radar-generated depth maps. Techniques such as those in [5, 14, 52] prove inappropriate for radar-camera depth estimation tasks because they rely on a predetermined number of LiDAR points as inputs, which is incapable of handling various number of radar points.\nFollowing the methodology of [37], our approach utilizes a single radar scan, contrasting with methods like [24-26] that employ multiple radar scans to enhance point cloud density. Remarkably, our method delivers superior results with fewer radar points compared to these approaches. Remarkably, GET-UP outperforms [20] by 18.6%, 18.3%, and 15.3% in MAE and 19.9%, 15.0%, and 14.7% in RMSE at the 50, 70, and 80-meter evaluation distances, respectively, demonstrating its efficacy in leveraging limited radar data for accurate depth estimation."}, {"title": "4.3. Qualitative Results", "content": "Fig. 7 showcases a comparative analysis of our GET-UP method against the baseline [19] and RadarNet [37]. Overall, our proposed GET-UP predicts depth maps with clearer object boundaries compared to RadarNet and the baseline, both at long and short ranges. For example, in the first row, GET-UP effectively distinguishes between the sky and the upper boundary of the track at a far distance. In the second and third rows, our method demonstrates greater robustness by accurately predicting the shapes of various objects."}, {"title": "4.4. Ablation Study", "content": "To further ascertain the efficiency of our GET-UP, we conduct a series of ablation studies to verify the effectiveness of each component. First, we analyze the impact of the ASCB. Secondly, the efficacy of the 3D feature extraction submodule is evaluated. At last, we quantify the reliability of the point cloud upsampling submodule."}, {"title": "4.4.1 Adaptive sparse convolution block.", "content": "Initially, we conducted experiments without utilizing any sparse convolution refinement, which directly extracts 2D features from radar projections. Subsequently, we compared our proposed ASCB against the conventional sparse convolution block [43], which employs a single mask alongside a sequence of kernel sizes set at [11, 7, 5, 3, 3]. Compared to the conventional sparse convolution block, our ASCB improves the RMSE with 7.7%."}, {"title": "4.4.2 Radar 3D feature extraction submodule.", "content": "In this section, we first conduct an experiment without 3D feature extraction. Thus, $X_{Radar}$ is processed solely through the ASCB and then by the ResNet encoder. The result underscores the effectiveness of incorporating geometry information and the critical role of the 2D-3D feature aggregation and refinement process in enhancing model performance. Then, we benchmark our attention-enhanced DGCNN against established models such as GCN [18], GCN2 [4], and the original DGCNN [47] architectures. Furthermore, we explore the optimal number of nearest neighbors (k) for each radar point to determine the most effective value, with the comparative results presented in Table 3. The findings clearly demonstrate the superior performance of our attention-enhanced model. Notably, a larger k value degrades performance since it tends to only capture global features due to the sparse nature of radar points. This leads to errors during feature extraction, underscoring the importance of carefully selecting k to balance detail capture and noise minimization. Our proposed 3D feature extraction module improves the MAE by 6.5% compared to the solely 2D feature extraction architecture."}, {"title": "4.4.3 Point cloud upsampling submodule.", "content": "To demonstrate the effectiveness of this module, we initially perform an experiment excluding the upsampling task. Subsequently, we conduct further experiments to identify the optimal number of upsampling units $n_u$ and the number of upsampled points $N_L$. All other components of the model remain unchanged during these evaluations. The results demonstrate that incorporating point cloud upsampling as an auxiliary task substantially enhances depth estimation accuracy. Moreover, given that the average count of radar points per frame is approximately 60, selecting an appropriate value for $N_L$ is crucial to ensure the efficacy of the upsampling process."}, {"title": "5. Conclusion", "content": "In this paper, we propose GET-UP, a geometry-aware algorithm designed to tackle the significant challenges in radar-camera depth estimation due to the inherent ambiguity and sparsity of radar data. Our approach integrates both 2D and 3D representations of radar data, utilizing an attention-enhanced DGCNN model for the extraction of 3D features without compromising 2D spatial context. To address the issue of radar data sparsity, we implement two strategies: the ASCB, which densifies radar data on the 2D plane to facilitate the extraction of 2D features and a point cloud upsampling task that enhances radar point density from a 3D perspective. GET-UP sets a new benchmark on the nuScenes dataset, improving 15.3% in MAE and 14.7% in RMSE over the previously best-performing model. Looking ahead, exploring diverse upsampling algorithms on radar point clouds and refining the integration of 2D and 3D radar features present valuable directions for further research."}, {"title": "6. Acknowledgement", "content": "Research leading to these results has received funding from the EU ECSEL Joint Undertaking under grant agreement n\u00b0 101007326 (project AI4CSM) and from the partner national funding authorities the German Ministry of Education and Research (BMBF)."}]}