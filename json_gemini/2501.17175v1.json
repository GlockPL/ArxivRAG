{"title": "Document-Level Sentiment Analysis of Urdu Text Using Deep Learning Techniques", "authors": ["A Irum", "M Ali Tahir", "S Latif"], "abstract": "Document level Urdu Sentiment Analysis (SA) is a challenging Natural Language Processing (NLP) task as it deals with large documents in a resource-poor language. In large documents, there are ample amounts of words that exhibit different viewpoints. Deep learning (DL) models comprise of complex neural network architectures that have the ability to learn diverse features of the data to classify various sentiments. Besides audio, image and video classification; DL algorithms are now extensively used in text-based classification problems. To explore the powerful DL techniques for Urdu SA, we have applied five different DL architectures namely, Bidirectional Long Short Term Memory (BiLSTM), Convolutional Neural Network (CNN), Convolutional Neural Network with Bidirectional Long Short Term Memory (CNN-BiLSTM), Bidirectional Encoder Representation from Transformer (BERT). In this paper, we have proposed a DL hybrid model that integrates BiLSTM with Single Layer Multi Filter Convolutional Neural Network (BiLSTM-SLMFCNN). The proposed and baseline techniques are applied on Urdu Customer Support data set and IMDB Urdu movie review data set by using pretrained Urdu word embeddings that are suitable for (SA) at the document level. Results of these techniques are evaluated and our proposed model outperforms all other DL techniques for Urdu SA. BILSTM-SLMFCNN outperformed the baseline DL models and achieved 83%, 79%, 83% and 94% accuracy on small, medium and large sized IMDB Urdu movie review data set and Urdu Customer Support data set respectively.", "sections": [{"title": "1. Introduction", "content": "With the tremendous increase in the availability of textual data in recent years, wide interest of the research community has been seen in different new and relatively more demanding areas in the field of Natural Language Processing (NLP). One such active research area of NLP is Sentiment Analysis (SA). SA tends to analyze a wide range of opinions regarding various subjects that are expressed by humans on different platforms. The objective of SA is to identify the sentiment and assign it a label in accordance with the particular subject Liu (2012). SA has gained massive attention recently as sentiment is considered the key component in maintaining the reputation of organizations associated directly with public.\nEnglish language has gained much attention for global-level research since the birth of NLP. In the last two decades, wide-scale research regarding each aspect of NLP has mostly been conducted in English language as it is computationally inexpensive to conduct experimentation in a resource-rich language. As compared to resource-rich languages, morphologically rich yet resource-scarce languages like Urdu, Arabic, Turkish, Hindi and Persian, etc. have been least utilized by researchers for NLP tasks as it is hard to cater word-level complex morphological structure of such languages Abdul-Mageed and Korayem (2010). The unique characters, complex morphology, and scarcity of linguistic resources of Urdu language have played a vital role in limited research regarding Urdu SA Daud et al. (2017).\nMost of the traditional work done in Urdu SA domain is based on Machine Learning (ML) models. ML models do not achieve the best results on large data sets and tend to learn only directed text features. The significant challenge regarding ML models is the selection of efficient features from the high-dimensional feature space [4]. Among the numerous feature selection methods available, there is no single method that works well with all types of ML classifiers Zia et al. (2015)."}, {"title": "1.1. URDU AND ITS ATTRIBUTES:", "content": "Urdu is the national language of Pakistan and it is world-wide notable language with 300 million speakers all across the globe Riaz (2012). It is 21st most spoken language in the world. It has a character set of 38 characters shown in Figure 1. Along with the complex morphological structure, it has the following distinctive features that make it a challenging language for computational tasks Syed et al. (2011b):\n\u2022 It is a mixture of different languages and has a lot of loan words from languages like Turkish, Arabic, Persian, Sanskrit, and even English. It has significant influence from other languages.\n\u2022 It has morphological variations, i.e., many words have a common root word.\n\u2022 Urdu has Nastalique writing style as shown in Figure 1. Nastalique writing style is complex in its nature."}, {"title": "1.2. CATEGORIZATION OF SENTIMENT ANALYSIS LEVELS:", "content": "SA is categorized into three contrasting levels, namely word-level, sentence-level and document-level Rhanoui et al. (2019). At word-level, the polarity of word is determined i.e. whether the word is positive, neutral or negative. At sentence-level SA, the polarity of whole sentence is calculated in order to examine the sentiment score of sentence. A sentence is a sequence of words that define the opinion of a person on a particular subject. It is usually done for data extracted from social networking websites. Furthermore, at document level, SA aims to classify the polarity of complete document by taking into consideration an increase of words and noise features that tend to distort the learning process resulting in difficulty in sentiment prediction of the document."}, {"title": "1.3. CHALLENGES REGARDING DOCUMENT-LEVEL URDU SENTIMENT ANALYSIS:", "content": "Besides the complex morphological structure and unique features of Urdu language, one of the key challenges met by researchers in applying Urdu SA is the availability of an annotated Urdu corpus. Scarcity of large, free of cost and publicly available standard corpus makes it difficult for researchers to expand the scope of their research in the field of Urdu SA. This obstacle has created hindrance in the application of text classification of Urdu text, particularly at document-level. Moreover, DL approaches are still unexplored in the domain of document-level Urdu SA. To fill these gaps, the main contributions of this study are:\n\u2022 A hybrid model BiLSTM-SLMFCNN is developed for document-level Urdu SA.\n\u2022 The performance of proposed and baseline DL models is analyzed on variable sized data sets.\n\u2022 The impact of data size on the performance of proposed and baseline DL models is evaluated.\n\u2022 Comparative analysis regarding to the performance of BiLSTM-SLMFCNN is done with well-known DL classifiers."}, {"title": "2. LITERATURE REVIEW", "content": "Classification of Urdu text by making use of lexical resources, feature selection methodologies and ML classifiers has been in progress since the last decade Ahmed et al. (2016). As different languages have contrasting dialect, morphological and lexical structure, the methods adopted for SA of resource-rich languages cannot be utilized for Urdu language Syed et al. (2010). Almost a decade ago, researchers started to focus on exploitation of the concept of Urdu SA. From Bag-of-Words (BoW) model to supervised ML algorithms, Urdu SA has evolved much over the years. However, there is still a research gap when it comes to Urdu SA by applying mainstream DL methodologies.\nKhattak et al. (2021) conducted an elaborated survey on Urdu SA. According to their research, Urdu has faced lack of interest from researchers' side because of issues such as segmentation, morphological dissimilarities, lexical inconsistencies and scarcity of acknowledged resources. Urdu SA has been burdened with a lot of shortcomings, for instance, lack of a gold-standard corpus, deficiency of sentiment lexicons, ineffective handling of negations and modifiers, complications in management of domain-specific words, slang and emoticons recognition etc."}, {"title": "3. DEEP LEARNING MODELS-A BACKGROUND", "content": "DL is the advanced area of ML wherein Artificial Neural Networks (ANN), which are inspired from human neural system, are used to learn features of huge observational data in order to predict about unseen test data. Deep learning models have been extensively deployed by researchers for SA and Opinion Extraction on resource-rich languages. The results of these models have strengthened the trust of researchers on Artificial Intelligence (AI). In this section, DL models RNN, CNN and BERT are explained in detail."}, {"title": "3.1. RECURRENT NEURAL NETWORKS:", "content": "Recurrent Neural Networks (RNN) is a sequence learning model in which inputs are interconnected. It represents the generalized form of feed forward NN in which nodes between model's hidden layers are connected and sequence features are learnt dynamically. RNN learns context of the sentence while training. Semantic information between words can be transferred by RNN but it is not capable of capturing long distance semantic link between different words. During the process of training the model, the gradient gradually gets decreased until it vanishes completely. Consequently, length of the sequential data becomes limited."}, {"title": "3.1.1. LONG SHORT-TERM MEMORY", "content": "In order to cater the issue of RNN, Schmidhuber and Hochreiter proposed Long Short-Term Memory (LSTM) model that tends to learn long-term dependencies between different words by making use of three gates namely input gate, forget gate and output gate Hochreiter (1997). The input gate accepts or blocks the sequential input, the forget gate enables or disables a neuron based on the weights being learned by the model. The output gate determines output value of the LSTM's unis."}, {"title": "3.1.2. BIDIRECTIONAL LSTM:", "content": "The traditional RNN model and LSTM can only propagate information in forward direction. This tendency allows these models to depend upon information processed before certain time. To cater this problem, Bidirectional LSTM is used Schuster and Paliwal (1997). It has been proven very helpful in situations where context of input is required. It tends to process data in two directions i.e. forward to backward and backward to forward, as it uses two hidden layers. By making use of two directions of time, input data from both past and future of the current time period is utilized to predict better results."}, {"title": "3.2. CONVOLUTIONAL NEURAL NETWORK:", "content": "The core concept of Convolutional Neural Networks (CNN) is local features extraction, weight distribution and down sampling. It captures local correlation between neighboring words by focusing on local connectivity patterns amongst adjacent layer neutrons. It can easily learn text features effectively from massive amount of text using 1-dimensional or 2-dimensional (word order) structure using convolutional layers. The convolution layer makes use of convolving filters of variable sizes in order to extract high-level textual features. Then, maxpooling layer is added to extract global features. Both convolutional and max pooling layers make it possible for the model to learn to figure out local indicators while staying indifferent to their position in sentence/document Rakhlin (2016). One major challenge regarding CNN is to figure out suitable number of filters and filter size. Large-sized filters effects the training process while fewer filter results in incorrect results."}, {"title": "3.3. BIDIRECTIONAL ENCODER REPRESENTATION FROM TRANSFORMERS (BERT):", "content": "Bidirectional Encoder Representation from Transformers (BERT) is a deep bidirectional model for general purpose language understanding that learns information from both left to right and right to left directions. It has largely been utilized in the domain of sentiment analysis Gao et al. (2019). BERT uses transformer model that applies attention mechanism and encoder-decoder model on NLP tasks. The attention procedure helps the model to concentrate on related parts of given input sequence Vaswani (2017). Both encoder and decoder separately have some underlying understanding of language and because of this understanding; transformer architecture can be used to build systems that perfectly understand language."}, {"title": "4. PROPOSED MODEL:", "content": "The proposed model combines two widely used NN models namely, BiLSTM and CNN. Figure 4 illustrates the proposed BiLSTM-SLMFCNN model. This hybrid model is combined in order to test the adaptation of BiLSTM with CNN, as both of them are renowned for their use in sentiment analysis. BiLSTM and CNN have diverse purposes in SA and they are considered to be the mainstream models for classifications tasks. BiLSTM classifies the emotions in text by using the semantics of textual sequence and it maintains the chronological order amid words in the document, hence it is capable to ignore unwanted words by using its delete gate. CNN extracts the latent semantic evidence of text by convolving the embedding text. It can extract as numerous textual features from the text as possible. Based on these facts, our research combines the above mentioned two models and proposes a hybrid DL model based on BiLSTM and CNN.\nAs compared to the traditional BiLSTM-CNN model, the BiLSTM-SLMFCNN model focuses on the association between textual features. In this research, we aim to find out that by combining the textual features extracted by BiLSTM and CNN algorithms, we can efficiently solve the issues related to Urdu sentiment classification.\nThe purpose behind combining these two neural network models is to generate a hybrid model that takes benefit of the strengths of both CNN and BiLSTM, so that it filters the information using BiLSTM, and uses them as CNN input. Thus, we present a model BiLSTM-SLMFCNN that meets this goal.\nA single input channel is used with predefined Urdu word embedding such that the word embedding vectors are used as BiLSTM input. Then, three filters of sizes 3, 4 and 5 are applied for 100 times each to obtain multiple feature maps"}, {"title": "5. EXPERIMENTAL STUDY:", "content": "The data set used in this research has been provided by a Pakistani Vehicle Tracking Company (VTC). The data set is related to company's customer support center calls where customers talk to company's customer service representative for their queries and complaints. This speech data is anonymized and transcribed into textual data. It is an annotated, small data set with 405 calls categorized as satisfied and 100 calls categorized as unsatisfied. Besides VTC's data set,"}, {"title": "5.2. Preprocessing:", "content": "Before using the text for any classification task, it is preprocessed so that clean, normalized and structured data is used for getting accurate results. Preprocessing helps to maintain data in a form that it has no redundancy and noise. The process is widely adapted by researchers to get cleaned data for better interpretation of applied model. In the implementation of our proposed model, we have applied text preprocessing to all the data sets for noise removal. First of all, tokenization is done in which each document's word is considered as a separate token. Then Urdu stop words are removed in order to make the data less prone to redundancy. Then, further preprocessing is done by removing any English alphabets, alphanumeric characters, URL's, whitespaces and punctuation marks. The preprocessed data is then fed into neural network so that model's performance can be evaluated. Common preprocessing steps are shown"}, {"title": "5.3. Experimental Setup:", "content": "After the process of data acquisition and transcription, data preprocessing is done. Urdu Preprocessing is itself a tedious task but it has been made easy by Urduhack\u00b2 library. For parameters setting, we have implemented grid search technique and on basis of careful implementation, we have applied 3-fold cross validation approach to find out optimized parameters for our proposed model and other baseline models namely BiLSTM, CNN, CNN-BiLSTM"}, {"title": "5.3.1. Word Embedding layer:", "content": "The pre-processed data set gives a meaningful and unique sequence of words and every word is assigned a unique ID. Word embedding layer assigns random weights to words and it eventually learns the embedding to embed all the words in the training data set. This layer is mostly used to learn word embeddings that can be saved for further use in any other model. In this study, we have used pre-trained Urdu word embeddings Haider (2018) trained on skipgram model in the embedding layer."}, {"title": "5.3.2. BiLSTM Layer:", "content": "BiLSTM layer takes the input of word vectors from embedding layer. It tends to keep the sequential order between the text data. It precisely detects the links between previous inputs and outputs."}, {"title": "5.3.3. Convolutional Layer:", "content": "A convolutional layer works as a feature-extracting module that aims to explore the combination between different sentences of the document by using multiple filters of size t. In this layer, multisize filters act as n-gram detectors where each filter looks for the particular class of n-grams and then assigns high scores to them. Those detected n-grams which have highest score then pass the max pooling stage. This layer uses multi filters of sizes 3, 4 and 5 which are applied 100 times each. After application of each filter, max pooling operation is implemented to reduce and update the data size. The outcome of all max pooling layers are then concatenated to serve as input to Dense layer."}, {"title": "5.4. Methodology:", "content": "In the proposed model BiLSTM-SLMFCNN, we have used three filter sizes of (3, 4 and 5) along with 100 feature maps. The number of CNN filters and the filter size affects the network performance and depends on the complexity of the problem in hand and the dataset. A large filter size usually slows down the training process while a small filter size decreases the performance.\nThe proposed model comprises of two sets of vectors, one is an input layer of the network and second one is the word embedding layer. The main notion is to capture the hidden semantic information in every word of each document. For this purpose, we have used pretrained neural word embedding proposed by the name of Urdu Word Embeddings Using these embeddings, each word is encoded as a three hundred dimensional vector in the feature space. The next layer is the BiLSTM layer which focuses on the important words in each document and with the help of delete gate; it does not focus on unnecessary words. 150 hidden units are implemented in Bidirectional layer. The output of the BiLSTM layer is then fed as input into the CNN layer. CNN is best known to extract features from the text. A single CNN conv2d layer with three multi size filters i.e. 3, 4 and 5 each of filter size 100 is implemented. Different sized multi filters are used to capture several feature maps of every filter size.\nMaxpooling is then applied on each feature map. After this process, the pooled features are concatenated to create a feature vector for dense layer. As a matter of fact, small data sets are more likely to face over fitting problem. Therefore, to overcome the issue of over fitting, L2 regularization is applied. Regularization is intended to manage a complex neural network that avoids overfitting as it impacts the performance of various deep learning models. We have used dropout and L2 regularization. It penalizes large weights so that neural network can be optimized.\nOptimization is used for training deep learning models for updating the parameters of neural network across multiple iterations. It helps to improve the performance of model. The opted parameters are Dropout rate, Batch Size, Activation Function and Learning Rate. With the best gained parameters, training process is completed. Details of optimized parameters are shown in Table 3. The training process is done with 32-batch size and 0.5 dropout rate. Softmax is selected as an activation function in the parameter setting.\nAll the experimentation is conducted in Google Colab's GPU environment. Through Colab, Google provides 13 GB RAM and 108 GB disk space. For assessing the performance of our proposed model effectively, we focus on identifying the best performing parameters of the model and then make a comparison with the BiLSTM, CNN, CNN-BiLSTM and BERT as baseline deep learning models on two data sets."}, {"title": "5.5. Evaluation Metrics:", "content": "Various evaluation metrics are widely used by researchers to evaluate the quality of classification model. In this paper, commonly used performance measures i.e. F1 Score, Accuracy and Area Under Curve(AUC) have been used."}, {"title": "6. RESULTS AND DISCUSSION:", "content": "During this study, the proposed model and other baseline models were applied on two data sets. In this section, the results of these techniques are discussed in detail.\nThe proposed model has outperformed other baseline models for all data sets. The VTC data set is small and the core purpose to conduct this research is to check the performance of deep learning models on small Urdu data set. For Urdu sentiment analysis, only machine learning side has been explored till now. As per our knowledge, this is first ever kind of research in Urdu sentiment analysis domain which focuses on the Urdu sentiment classification at document level by applying frequently used deep learning architectures. Results are carefully compared and discussed on the basis of following key factors:\n\u2022 Dataset size\n\u2022 Level of imbalance\n\u2022 Datasets Comparison\n\u2022 Models Comparison"}, {"title": "6.2. Level of Imbalance:", "content": "Our proposed model BILSTM-SLMFCNN gives highest F1 score and accuracy when applied on VTC and variable-sized IMDB data sets. BiLSTM-SLMFCNN has gained F1 score, accuracy and AUC value of 93.7, and 94% and 85.5 respectively when applied on VTC. Although VTC is a slightly imbalanced dataset but our proposed model has the ability to effectively handle imbalance dataset of varying imbalance level and different sizes. In the case of the imbalanced datasets, accuracy is not considered as a good performance measure Chen et al. (2018). For BILSTM-SLMFCNN, the difference between F1 score and accuracy is less for all datasets as compared to other models. On the basis of comparative analysis, BILSTM-SLMFCNN works well on imbalanced dataset as well."}, {"title": "6.3. Datasets Comparison:", "content": "The reason that our proposed model works best on VTC data set as compared to IMDB data sets is that VTC data set is very specific to vehicle tracking related user queries while IMDB data set relates to automatically translated movie reviews and it may not have been translated up to desired precision level. As VTC data set has been carefully transcribed and cross-checked, hence it gives better results when different deep learning architectures are applied on it."}, {"title": "6.4. Models Comparison:", "content": "Other than BiLSTM-SLMFCNN, B\u0130LSTM, CNN CNN-BiLSTM and BERT have also shown good results on both data sets. The ability of CNN to extract text features with help of convolutional layers makes it a powerful deep learning model. Furthermore, BERT also tend to show good performance on small and medium sized data sets as it is a deep bidirectional transformer model that uses Masked Language Model (MLM) for masking words and Next Sentence Prediction (NSP) to learn bidirectional context within a sentence and across different sentences in an efficient manner. However, it has not shown promising results on IMSB Large Urdu data set.\nTo conclude, the proposed model BiLSTM-SLMFCNN has an ultimate advantage over other deep learning models that the technique used in it is relatively straightforward and automatic in the sense that feature engineering is not involved in the whole process. Hence, it saves implementation cost, time and tends to improve the performance of BILSTM-SLMFCNN model. The conducted research plays the role of benchmark study in Document level Urdu sentiment analysis using deep learning methods along with the contribution of a proposed hybrid technique.\nThe proposed model BiLSTM-SLMFCNN has an ultimate advantage over other deep learning models that the technique used in it is relatively straightforward and automatic in the sense that feature engineering is not involved in the whole process. Hence, it saves implementation cost, time and tends to improve the performance of BiLSTM-SLMFCNN model. We have used average max-pooling instead of simple maxpooling layer which gave better results in terms of accuracy and F1 score from baseline approaches. Our proposed model used less number of parameters which consumed less memory and is efficient in terms of convolution layers."}, {"title": "7. CONCLUSION AND FUTURE WORK:", "content": "In this research, we have made an attempt to gain the attention of research community towards Urdu, a resource-poor language, by applying our proposed neural network based deep leaning model named BiLSTM-SLMFCNN on two Urdu data sets. The conducted research plays the role of benchmark study in document level Urdu SA using DL methods along with the contribution of a proposed hybrid technique."}, {"title": "CRediT authorship contribution statement", "content": "A Irum: Methodology, Data curation, Writing - Original draft preparation, Visualization. M Ali Tahir: Formal analysis, Supervision, Writing \u2013 review & editing, Project administration, Validation. S Latif: Review & editing, Validation."}, {"title": "", "content": "F1 - Score = 2 * Precision * Recall/Precision + Recall  (1)\nAccuracy =TP+TN/TP+TN+FP+ FN (2)\nSensitivity=TP/TP+FN (3)\nSpecificity=FP/TN+FP (4)"}]}