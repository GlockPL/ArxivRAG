{"title": "PLD+: Accelerating LLM inference by leveraging Language Model Artifacts", "authors": ["Shwetha Somasundaram", "Anirudh Phukan", "Apoorv Saxena"], "abstract": "To reduce the latency associated with autoregressive LLM inference, speculative decoding has emerged as a novel decoding paradigm, where future tokens are drafted and verified in parallel. However, the practical deployment of speculative decoding is hindered by its requirements for additional computational resources and fine-tuning, which limits its out-of-the-box usability. To address these challenges, we present PLD+, a suite of novel algorithms developed to accelerate the inference process of LLMs, particularly for input-guided tasks. These tasks, which include code editing, text editing, summarization, etc., often feature outputs with substantial overlap with their inputs\u2014an attribute PLD+ is designed to exploit. PLD+ also leverages the artifacts (attention and hidden states) generated during inference to accelerate inference speed. We test our approach on five input-guided tasks and through extensive experiments we find that PLD+ outperforms all tuning-free approaches. In the greedy setting, it even outperforms the state-of-the-art tuning-dependent approach EAGLE on four of the tasks. (by a margin of upto 2.31 in terms of avg. speedup). Our approach is tuning free, does not require any additional compute and can easily be used for accelerating inference of any LLM.", "sections": [{"title": "Introduction", "content": "Large language models have emerged as the foundational building blocks for a wide array of user-facing applications, enabling unprecedented capabilities in natural language processing and generation (Liu et al., 2023). However, the autoregressive decoding approach employed by these large language models introduces significant inference latency, a challenge that becomes more severe as the model size and generation length increase (Xia et al., 2024). This latency can pose a barrier to the integration of these models into interactive applications, underscoring the importance of developing efficient decoding strategies to address this fundamental limitation.\nOne strategy that has been proposed to mitigate the inference latency challenge faced by large language models is the Speculative Decoding paradigm, which operates based on the Draft and Verify principle to accelerate the inference process (Stern et al., 2018; Leviathan et al., 2023). The two key steps in this paradigm are (a) efficient generation of multiple future tokens in the drafting step and (b) parallel verification of the drafted tokens using the target Language Model to ensure quality and alignment.\nClassic drafting strategies usually either employ a smaller independent model to efficiently draft tokens or leverage the target LLM itself, utilizing techniques such as incorporating additional FFN heads (Stern et al., 2018; Cai et al., 2024) or layer skipping (Zhang et al., 2023b). However, these methods often require extensive tuning, which needs to be performed for every new model, and can be time and resource-intensive.\nIn contrast, the PLD/LLMA approach gets rid of the need for any additional draft model by simply selecting text spans from the input as drafts, thus being extremely simple and demonstrating speedup on \"input-guided tasks\" (Saxena, 2023; Yang et al., 2023a). Input-guided tasks are those with context-rich prompts, where the output is directly informed by or closely aligned with the input information, such as summarization, retrieval-augmented generation, and code/text editing. The simplicity and effective nature of PLD has resulted in its integration with the transformers library, underlining the importance of a plug-and-play method that achieves speedup on \"input-guided tasks\".\nIn this paper, we build upon PLD and propose Prompt Lookup Decoding+ (PLD+), which leverages the information present in the model artifacts (attentions and hidden states generated during inference) to generate better drafts. In essence, we"}, {"title": "Related Work", "content": "Speculative Decoding, introduced by Stern et al. (2018), represents a novel paradigm aimed at enhancing the efficiency of LLM inference. Instead of generating tokens sequentially, Speculative Decoding drafts multiple future tokens efficiently and then verifies them in parallel.\nDrafting strategies are distinctly categorized into Independent and Self-Drafting methodologies. Independent Drafting leverages external models for the drafting process. This method often necessitates special training or fine-tuning to align with the target LLM's output characteristics. Examples include the use of smaller models within the same model family to draft tokens for their larger counterparts (Chen et al., 2023; Leviathan et al., 2023), capitalizing on inherent architectural and training similarities to boost alignment and reduce additional training requirements.\nPLD/LLMA (Saxena, 2023; Yang et al., 2023b) is a simple yet highly effective independent drafting strategy for speeding up LLM inference in \"input-guided tasks\". It leverages string matching to generate candidates, capitalizing on the n-gram overlap between input and output. By replacing external"}, {"title": "Background: Autoregressive decoding and Speculative Decoding", "content": "Autoregressive Decoding: Given an input sequence $x = x_1, x_2, ... x_t$, an autoregressive language model $M_q$ generates the next token $x_{t+1} = M_q(x|x_1, x_2, ..., x_t)$, where $x_{t+1} \\sim q_{t+1}$, $q_{t+1}$ is the conditional probability calculated by the model $M_q$ over its vocabulary V. Based on the sampling scheme, the token $x_{t+1}$ is sampled from the probability distribution. The causal dependency of each decoding step on the results of previous decoding steps makes autoregressive decoding slow and inefficient due to its inability to fully utilize the parallelization capability of GPUs.\nSpeculative Decoding: Speculative Decoding (Stern et al., 2018; Leviathan et al., 2023), can be expressed as a Draft and Verify paradigm (Xia et al., 2024). For a given decoding step t, a drafting algorithm speculates K draft tokens $x_1, ..., x_k$ efficiently, which are then verified by the language model $M_q$. In the standard speculative decoding"}, {"title": "Our approach: PLD+", "content": "In the following sections, we explain the drafting \u00a74.2 and verification \u00a74.4 algorithms of our approach. Figure 1 provides an overview of the generation process using PLD+ for inference.\nFormally, given an input sequence $x = x_1, x_2, ... x_t$ passed to a language model $M_q$ for decoding step t, our approach predicts and verifies draft tokens $x_1, ..., x_k$ leveraging model attentions A or model hidden states H computed during the generation process. The value K denotes the number of draft tokens that are predicted. The hidden states H is a vector with dimensions (L, |x<t|, d) and the model attention states A is a vector with dimensions (L, G, |x<t|, |x<t|), where, L is the number of layers in the model, G is the number of attention heads per layer, |x<t| is the sequence length of the input tokens before decoding step t, d is the embedding size."}, {"title": "Drafting Algorithm", "content": "Our goal is to select an optimal token span from x such that we exploit the overlap between the input sequence and the generation sequence.\nTo achieve this, we first identify a set of positions P, where the last generated token $x_t$ occurs in the input token sequence x.\n$P = {j | x_j = x_t, j < t}$                                                            (1)\nTo maximize inference speedup, we need to rank these occurrences intelligently. We hypothesize that the artifacts computed during the generation process captures contextual information which"}, {"title": "Ranking occurrences using model attentions", "content": "The model attentions A computed by the model across different layers l and heads g is available for the sequence x<t. The most straightforward method of ranking occurrences is to aggregate all attention maps across l and g for token $X_{t-1}$ using max or sum operation and choose the position $j^*$ which has the highest value. Recent work in mechanistic interpretability (Olsson et al., 2022; Bansal et al., 2023) indicate that there exists induction heads which drive the in context learning ability of models. Induction heads are defined as attention heads that engage in two specific behaviors: prefix matching, which involves locating a previous instance of the current token within the context, and copying, which entails duplicating the sequence that follows the identified token. The behavior of induction heads can be highly useful for accelerating inference for input-guided tasks.\nIdentification of relevant attention heads: We identified heads that can be relevant by first generating the outputs ot \u2208 O and attentions A for a set of prompts. For each generated token ot present in the input x, we find the set of positions P, from where the token could have been \"copied\", i.e, positions where the generated token is present in the input. We then choose the position r* which has the maximum overlapping suffix with the generated output. We then iterate over all of the attention heads L \u00d7 G and keep track of the attention heads where the token position $X_{p*}$ has the maximum attention. We repeat this process for every prompt and for a given model Mq we identify relevant attention heads, $G^*$ from layers $L^*$.\nAfter identifying the relevant attention heads, we aggregate the attention scores from the heads in $G^*$ across layers $L^*$ using the max operation, and then we select the position $j^*$ that has the highest value.\n$j^* = argmax_{j \\in P} (max_{(l,g) \\in (L^*, G^*)} A^{l,g}_{t-1,j})$                       (2)"}, {"title": "Ranking occurrences using hidden states", "content": "The hidden states H computed by the model across different layers l is available for the sequence x<t. The hidden states corresponding to the last generated token xt is not available. Therefore, for each position $j \\in P$, we compute the cosine similarity between $H_{t-1}^{(1)}, H_{j-1}^{(1)}$ and select the occurrence $j^*$ which results in the highest similarity value.\n$j^* = argmax_{j \\in P} cos\\_sim(H_{t-1}^{(1)}, H_{j-1}^{(1)})$        (3)"}, {"title": "Draft Prediction", "content": "We predict $x_{j^*+i}, i = 1, ..., K$ as the future tokens, where K denotes the number of predicted draft tokens. The number of draft tokens K, the layer l and the head g are the hyper parameters.\ndraft tokens $x_i = x_{j^*+i}, i = 1, ..., K$"}, {"title": "Verification Algorithm", "content": "The goal of the verification phase is to ensure that the tokens generated by PLD+ are the same as those generated by standard autoregressive decoding. To achieve this, we first pass the input sequence x along with the draft tokens $x_i$ to obtain conditional probabilities for future positions (t + i, i = 1, . . . K) using Mq. Using these probabilities, we sample new tokens at the future positions. We verify if the sampled tokens match with the draft tokens at each position. After the first mismatch we discard the subsequent draft tokens."}, {"title": "Application Scenarios", "content": "Our motivation is to accelerate generation in tasks where the generation outputs have significant overlaps with the input context. As enumerated by Yang et al. (2023a): Multi-turn conversation, Retrieval Augmented Generation and Cache assisted Generation naturally fall under the input-guided tasks paradigm. We conduct our experiments on a broader range of tasks: code editing, text editing (short), text editing (long), multiturn conversation, and summarization."}, {"title": "Experimental Setup", "content": "For the tasks mentioned in Section 4.5, we sample data from the following datasets.\nCode Editing: We leverage CodeEditor- Bench_Plus, 1 one of the two datasets introduced by (Guo et al., 2024) to test the performance of LLMs in code editing tasks: debugging, translating, polishing and requirement switching. We randomly select 20 instances for each task, resulting in a total of 80 samples.\nText Editing (short): We leverage the text editing benchmark XATU 2, introduced by (Zhang et al., 2023a) to test the capabilities of LLMS for fine-grained instruction-based text editing. The benchmark accounts for tasks of various difficulties such as text simplification, grammar error correction, style transfer and information update. We randomly select 30 samples for each of the nine datasets resulting in a total of 270 samples.\nText Editing (long): We leverage the ArgRewrite V.2 corpus\u00b3 (Kashefi et al., 2022) which"}, {"title": "Baselines", "content": "We compare our method with seven speculative decoding approaches leveraging the Spec-Bench benchmark (Xia et al., 2024). In particular we compare our approach with Medusa (Cai et al., 2024), EAGLE (Li et al., 2024), Hydra (Ankner et al., 2024) (finetuning dependent approaches), and SpS (Chen et al., 2023), Lookahead (Fu et al., 2024), LLM-A/PLD (Yang et al., 2023a; Saxena, 2023), REST (He et al., 2023) ( tuning-free approaches)."}, {"title": "Metrics", "content": "Our evaluation centers on three key metrics: average throughput (tokens/sec), speedup (speculative vs. standard decoding), and average acceptance length (tokens accepted per generation)."}, {"title": "Implementation Details", "content": "We conduct our main experiments using the Vicuna-1.3 model series leveraging the code base of the Spec-Bench benchmark (Xia et al., 2024). For SpS, we used Vicuna-68m-v1.3 as the draft language model. We follow the default parameters for all of our speculative decoding baselines."}, {"title": "Hyperparameter Tuning", "content": "We used the summarization task as our validation set for hyperparameter tuning with Vicuna-7b-1.3. As explained in Section 4.2, PLD+ has two key hyperparameters: the number of draft tokens (K) and the layer (l) if model hidden states are used to accelerate inference. We first determined the optimal value for l by fixing K=10, then, for the selected layer, we tuned K. Using the average acceptance length metric, the best values were K=70 and l=9.\nWhen using model attentions for accelerating inference, PLD+ has two hyperparameters: the number of draft tokens (K) and the attention heads (g). As described in Section 4.2.1, we identified 379 induction heads (37% of the total heads in the model) and ranked them based on prefix matching and copying frequency. We found that using the top 50 heads gave the best average acceptance length. After fixing g as the top-50 heads, we tuned K and found the optimal value to be K=70."}, {"title": "Results and Analysis", "content": "Tables 1 (T=0, greedy decoding) and 2 (T=1, sampling) compare PLD+ with baselines across five input-guided tasks. We denote our approach as PLD+ (a) when using model attentions and PLD+ (h) when using hidden states. In the greedy decoding scenario, PLD+ either exceeds or matches performance of tuning-free baselines in all five tasks. For code editing, short text editing, and long text editing tasks, PLD+ even surpasses the best-performing fine-tuned approach, EAGLE (Li et al., 2024), by margins ranging from 0.24\u00d7 to 2.85\u00d7 depending on model size. These results support our hypothesis that leveraging overlap between input and generation accelerates inference.\nIn the sampling scenario, speedups for all methods decrease compared to greedy decoding due to the randomness introduced by sampling. Despite this, PLD+ still outperforms all tuning-free baselines except on the Multi-turn Conversation task, where it ranks second, trailing REST (He et al., 2023) by a maximum of 0.18. We manually reviewed the dataset and found that certain categories (roleplay, STEM, humanities) in the multi-"}, {"title": "Plug and play nature of PLD+", "content": "To demonstrate the versatility of PLD+, we conducted experiments on the summarization task using newer models like Mistral-7B-Instruct, Llama-2-7B-Chat, and Llama-2-13B-Chat with the greedy decoding strategy. Unlike many speculative decoding methods that require additional compute/-tuning and don't support all LLMs out of the box, PLD+ works seamlessly across models. We compare against baselines that support these models, and as shown in Table 3, PLD+ consistently out-"}, {"title": "Ablation study", "content": "In Section 4.2.1, we detailed how we identify relevant attention heads. Using Vicuna-7b-1.3 for the summarization task and average acceptance length"}, {"title": "How to choose attention heads for ranking occurrences?", "content": ""}, {"title": "Effect of number of draft tokens, K on performance of PLD+ and PLD", "content": "In this section we analyse the impact of K, the number of draft tokens on the performance of PLD and PLD+. We vary K from 10 to 100 and run experiments on the summarization task and use the average acceptance length metric to evaluate the performance. Figure 4 depicts the performance of PLD and PLD+ across different K values. From this figure, we can clearly see that PLD+ benefits from larger K values, and it would be more suitable for tasks where large spans of the output overlap with the input (Code Editing, Text Editing). We hypothesize that using model artifacts (hidden states / attentions) to select the draft tokens helps us find longer matching spans compared to simple n-gram matching."}, {"title": "Effect of layer l, on performance of PLD+(h)", "content": "In this section we analyse the impact of the l from which the hidden states are obtained for PLD+. For the Vicuna-1.3 family of models, we fix K=10 and compute the performance for every l on the summarization task and use the average acceptance length metric to evaluate the performance. Figure"}, {"title": "Conclusion", "content": "In this work, we propose Prompt Lookup Decoding+ (PLD+), a plug-and-play speculative decoding approach that can be easily integrated into the generation process of any language model without additional training. Instead of using a small language model as our drafter, we intelligently choose spans from the input as draft tokens. At each decoding step, we find candidate draft spans and choose the best span leveraging the model artifacts computed during generation. Through extensive experiments we found that PLD+ performs on par or even outperforms fine-tuning dependent speculative decoding approaches for the ubiquitous setting of input guided tasks."}, {"title": "Limitations and Future Work", "content": "Though PLD+ is a plug-and-play tuning-free decoding technique, it is important to note that the speedup is directly influenced by the nature of the task. PLD+ is best suited for the tasks where there is a significant overlap between the input and the model generations.\nAn important feature of PLD+ is the ability to point to the location in the context from which copying occurs. The task of identifying which parts of the context resulted in the generation of a token is known as input attribution and is of particular interest to the community (Li et al., 2023).\nIn the Appendix D, we provide preliminary results on the ability of our method to perform attribution. Our approach to attribution has a similar flavor to the approach employed by Phukan et al. (2024), who successfully use LLM hidden states to perform attribution of verbatim copied spans. We intend to perform a detailed analysis of attribution quality in the future.\nWe want to highlight that our method focuses on speeding up inference and guarantees the same generation as standard autoregressive decoding. Our method does not introduce any additional risks than those associated with LLMs such as bias, malicious use, etc."}, {"title": "Additional ablations", "content": ""}, {"title": "How many top-k induction heads should we use for PLD+ (a)?", "content": "Table 6 summarizes the effect of the number of induction heads on the performance of PLD+(a). We vary the number of top-k induction heads and perform experiments on the summarization dataset using Vicuna-7b-v1.3 as the LLM. We can see that we get the best performance when the number of induction heads is 50."}, {"title": "Does using averaged hidden state representations improve performance of PLD+ (h)?", "content": "In equation 3, averaged hidden state representations $\\bar{H}_{t}^{(1)}$ can be used to find the best occurrence. Let m denote the prefix length used when computing the averaged representation, $\\bar{H}_{t}^{(1)} = \\frac{\\Sigma_{i=1}^m H_{t}^{(1)}}{m}$. We conduct experiments on the summarization task using Vicuna-7B-v1.3 by varying the prefix length m. Table 8 indicates that using averaged hidden state representations does not improve performance and that as m increases performance decreases. In the generation setup, the hidden state of $x_{t+1}$ is contextualized by $x_t$ by design and $H_{t+1}^{(1)}$ would already be similar to $H_{t}^{(1)}$. We hypothesize that averaging the hidden states, $\\bar{H}_{t}^{(1)}$ might lead to a less discriminative representation."}, {"title": "Does thresholding improve performance of PLD+ (h)?", "content": "In equation 3, we can additionally impose a threshold, where we consider an occurence j only if the cosine similarity $cos(H_{t-1}^{(1)}, H_{j-1}^{(1)})$ is greater than the threshold \u03b8. Setting the optimal \u03b8 value can help in reducing the number of occurences that need to be ranked in equation 3. We vary the value of \u03b8 from -1 to 1 and perform experiments on the"}, {"title": "Dataset details", "content": "In this section, we add additional details about the datasets used for each of the input-guided tasks. In Table 9, we report the average number of words in the input to the LLM for each task. We offer a representative sample prompt for each category in the following subsections to illustrate the tasks."}]}