{"title": "On the Dimension of Pullback Attractors in Recurrent Neural Networks", "authors": ["Muhammed Fadera"], "abstract": "Recurrent Neural Networks (RNNs) are high-dimensional state space models capable of learning\nfunctions on sequence data. Recently, it has been conjectured that reservoir computers, a particular class\nof RNNs, trained on observations of a dynamical systems can be interpreted as embeddings. This result\nhas been established for the case of linear reservoir systems. In this work, we use a nonautonomous\ndynamical systems approach to establish an upper bound for the fractal dimension of the subset of\nreservoir state space approximated during training and prediction phase. We prove that when the input\nsequences comes from an Nin-dimensional invertible dynamical system, the fractal dimension of this set\nis bounded above by Nin. The result obtained here are useful in dimensionality reduction of computation\nin RNNs as well as estimating fractal dimensions of dynamical systems from limited observations of their\ntime series. It is also a step towards understanding embedding properties of reservoir computers.", "sections": [{"title": "1 Introduction", "content": "Reservoir computing (RC) framework has offered a computationally efficient way to train recurrent neural\nnetworks by randomly generating the internal weights and keeping them fixed while focusing training only\non the output layer [18, 25]. This allows RC systems to avoid the famous vanishing and exploding gradient\nproblem [26], and are one of the main contenders for biologically plausible temporal learning [8, 9]. Further-\nmore, large class of RC systems have been shown to be universal approximators of fading memory mappings\non left infinite input sequences [14, 11]. Consequently, even with the restriction on training, RC systems\nachieve state-of-the-art performance on a number of benchmark including as time-series classification [16],\nsystems identification [19], adaptive filtering [34], attractor reconstruction [24] etc.\nBeyond the theory of universal approximations, it is unclear how RC systems are able to achieve such\nperformance on various benchmark tasks. In past few years, there has been research to understand how RC\nsystems process inputs and make their decisions. In the case of sequence-to-sequence classifications, it has\nbeen shown for example in [31, 3] that reservoir computers trained with output feedback take advantage of\nstable fixed points and their relative locations to solve such tasks. As well as in the continuous output case,\nerrors in RC systems trained on such tasks have been identified with the emergence of spurious/untrained\nattractors [10, 3]. Transitions to such attractors may cause errors over short time steps in sequence-to-\nsequence classification problems and over longer time steps in continuous output problems [10].\nIn [15], the authors conjectured that reservoir computers trained on generic continuous observations of a\nsmooth invertible dynamical system on an m-dimensional compact manifold embeds the dynamics of the\ndynamical system (not the observations) it is state space. This has been settled in the case of linear reser-\nvoirs [13]. If this conjecture holds in the general case, it will have significant impact on how we understand\ncomputation in RC systems. First, the embedding of the dynamics on the manifold on to reservoir space"}, {"title": "2 Background", "content": "Suppose Nin \u2208 N and U is compact and the closure of an open subset of RNin where RNin is equipped with\nthe standard Euclidean metric\n$d(u, v) = \\sqrt{\\sum_{i=1}^{N_{in}}(u_i - v_i)^2}, \\quad u, v \\in \\mathbb{R}^{N_{in}}.$\nWe will take V to be the interior int U of U i.e the largest open set contained in U. We consider the Cartesian\nproduct U = U\u00b2 consisting of all bi-infinite sequences of Nin-dimensional vectors u = {u[n]}=-\u221e where\nthe terms u[n] \u2208 U is the nth of u for each n \u2208 Z. U is considered as topological space equipped with the\nproduct topology induced by the metric\n$D(u, v) = \\sup_{n \\in \\mathbb{Z}}{2^{-|n|}d(u[n], v[n])}.$\nRoughly speaking two input sequences are close in this topology if they are close at only finite number of\nterms and maybe arbitrarily far away from each other at other terms. By Tychonoff's theorem [21, Theorem\n37.3], U is compact in this topology. Another topology on U is the uniform topology induced by the metric\n$D_{unif}(u, v) = \\sup_{n \\in \\mathbb{Z}}{d(u[n], v[n])}.$\nUnless otherwise stated, we will consider the product topology on U.\nWe denote the set UZ- consisting of all left-infinite input sequences taking values in U as U\u00af and the\nset UZ+ of all right infinite input sequences taking values in U as U+. We denote C(U, X) as the space of\ncontinuous functions from U (endowed with the product topology) to X. Equipped with the uniform metric\n$\\rho(H, F) = \\sup_{u \\in U}{d(H(u), F(u))}, \\quad H, F \\in C(U, X),$\nC(U, X) is a complete metric space [21, Theorem 43.6].\nGiven an initial condition x[0] \u2208 RN with N \u2208 N, the dynamics of the internal states of a RNN with\nNr hidden units driven by an input sequence u \u2208 U+ is given by\n$x[n + 1] = g(Wx[n] + W_{in}u[n + 1] + W_{fb}y[n])$\n$y[n + 1] = h(W_{out}x[n+1]), \\quad n \\in \\mathbb{N}_0$\nwhere W\u2208 RN\u00d7Nr is called the recurrent matrix, Win \u2208 RN,\u00d7 Nin is the input-to-network coupling, Wfb \u2208\nIRN,\u00d7 Nout feeds back the output into the network and Wout \u2208 RN, \u00d7 Nout is the readout weights. The functions\nh and gare activation functions applied component-wise. The activation functions g is usually taken to be\nthe hyperbolic tangent tanh and h the identity. In the case that h is the identity, the dynamics reduces to\n$x[n+1] = g(Wx[n] + W_{in}u[n+1])$\n$y[n+1] = W_{out}x[n+1]$\nwhere W = W + WfbWout is the effective recurrent matrix. RNNs can also be adapted for multiple time-\nscales [33] using the Leaky integrator RNNs with dynamics\n$x[n + 1] = (1 - a)x[n] + ag(Wx[n] + W_{in}u[n + 1] + W_{fb}y[n]$\nat leak rate a [20]. In all three cases, the current state x[n+1] of an RNN depends on the previous\nstate x[n] and the current input u[n+1]. This observation will be important when we consider RNNs as\nnonautonomous dynamical systems."}, {"title": "2.1 RNNs as nonautonomous dynamical system", "content": "The input-driven dynamics of RNNs is best understood in the lens of nonautonomous dynamical systems\n[22]. There are two main formulations for dealing with nonautonomous dynamical systems: process formu-\nlation and the skew product flow formulation or the cocycle formulation\u00b9. The process formulation is useful\nwhen dealing with the properties of a single input sequence while the skew product flow formulation allows\nmore general treatment of all input sequence taking values in a compact set. Following [4], we will use the\nskew product flow formulation in this paper.\nWe define the canonical projection\n$\\pi_n: (\\mathbb{R}^{N_{in}})^{\\mathbb{Z}} \\rightarrow \\mathbb{R}^{N_{in}},$\n$\\pi_n (u) = u[n].$\nLet\n$\\theta: U \\rightarrow U$\n$\\lbrace u[n] \\rbrace_{n=-\\infty}^{\\infty} \\mapsto \\lbrace u[n+1] \\rbrace_{n=-\\infty}^{\\infty}$\nbe the left shift operator. The left shift operator is continuous (with respect to the product topology) since\neach of its projections \u03c0\u03b7(\u03b8(u)) = un+1 is continuous as a function from U to U. Furthermore\n$D(\\theta(u), \\theta(v)) = \\sup_{n \\in \\mathbb{Z}}{2^{-n}d(\\pi_n(\\theta(u)), \\pi_n(\\theta(v)))}$\n$= \\sup_{n \\in \\mathbb{Z}}{2^{-n}d(u[n + 1], v[n + 1])}$\n$= 2\\sup_{n \\in \\mathbb{Z}}{2^{-(n+1)}d(u[n + 1], v[n + 1])}$\n$= 2D(u, v).$\nIt is also invertible with inverse 0-1 as the right shift operator satisfying \u03c0\u03b7(0\u22121(u)) = u[n \u2212 1] which is also\ncontinuous. It can easily be checked that 000-1 is the identity on U and that\n$D(\\theta^{-1}(u), \\theta^{-1}(v)) = \\frac{1}{2}D(u, v).$\nThus (0,U) is a discrete time dynamical system with 0\u00ba (u) = u and\n$\\theta^{m+n}(u) = \\theta^m \\circ \\theta^n(u) \\quad \\text{for all } m, n \\in \\mathbb{Z} \\text{ and for all } u \\in U.$\nDefinition 2.1 (Skew product flow, [22]). Suppose X \u2286 RNr is a compact and the closure of an open subset\nof RNr. Let g: U \u00d7 X \u2192 RNr be a continuous map. A cocycle mapping \u03a6 : No \u00d7 U \u00d7 RN \u2192 RNr is defined\nas\n$\\Phi(0, u, x) := x \\quad \\forall u \\in U \\text{ and } \\forall x \\in \\mathbb{R}^{N_r},$\n$\\Phi(n, u, x) := g(\\pi_0(\\theta^n(u)), \\Phi(n - 1, u, x)) = g(u[n], \\Phi(n - 1, u, x)) \\quad \\forall n \\in \\mathbb{N}.$\nThe pair (\u03b8, \u03a6) is called a skew product flow with base space U and state space X.\nRemark 1. The assumption of compactness on the state space X is a natural one at least when dealing with\nrecurrent neural networks with squashing activation functions i.e their image is contained in a compact set.\nEven in the case of leaky-integrator RNNs, it can be shown that their is compact invariant set that eventually\ncontains the entire forward dynamics [4, Proposition B.1].\nThe map is called a cocycle because it satisfies the cocycle property. The proof of this is relatively\nstraightforward and is given in the appendix for completeness.\nLemma 2.1. The map satisfies the cocycle property: For any u and xo \u2208 X,\n$\\Phi(n + m, u, x_0) = \\Phi(n, \\theta^m(u), \\Phi(m, u, x_0)) \\quad \\text{for all } m, n \\in \\mathbb{N}_0.$\n2.1.1 Attractors, Entire Solutions and the Echo State Property\nAs explained in [22, Chapter 3], the behaviour of nonautonomous dynamical systems is best understood\nusing an indexed family A = {Au}u\u2208u of subsets of X. For each u \u2208 U, the set Au is called the u-fiber of A\nand the set A is called a nonautonomous set. A is said to be non-empty (resp. compact) if each of the fibers\nare non-empty (resp. compact). Given a non-empty and compact nonautonomous set A = {Au}{u\u2208u}, we\nsay that A is I-positively invariant if for any u EU\n$\\Phi(n, u, A_u) \\subset A_{\\theta^n(u)} \\quad \\text{for all } n \\in \\mathbb{N}, u \\in U.$"}, {"title": "3 Theoretical Results", "content": "In this section, we will show that under certain conditions, the fibers of the pullback attractor can be obtained\nas the image of a continuous map H from U to RNr. This result has appeared for example in [15, 12] as\nthe echo state mapping theorem in the context of manifolds. It is known that this map is also continuously\ndifferentiable if one considers the base space as a manifold and if the dynamics on this manifold is also\ncontinuously differentiable. However, we would not seek such regularity on the dynamics of input sequences.\nIn section 3.2, this map will be useful in showing that the box-counting dimension of the subset of RNr that\ncontains the pullback attractor associated with input sequences u contained in V\u2286 U is bounded above by\nthe box-counting dimension of V in the product topology, and the box-counting dimension of H(U) is always\nNr."}, {"title": "3.1 Pullback Attractor as the image of a continuous map", "content": "Let C(U, X) be the space of continuous functions from U to X endowed with the uniform metric (2). Let\nHo \u2208 C(U, X) and define\n$H_n(u) = \\Phi(1, u, H_{n-1}(\\theta^{-1}(u))) \\quad \\text{for } n \\in \\mathbb{N}$\nThe sequence of maps {H0, H1, . . ., Hn } is called the generalised echo state family of length n, following the\nnotation of [15]. The goal of this section is to generalise the result of [15] that if g : U \u00d7 X \u2192 X is uniformly\ncontracting in the state variable i.e there exist \u03bc < 1 such that for all x, z \u2208 X and for all u \u2208 U,\n$||g(u,x) - g(u, z) || \\leq \\mu||x - z||,$"}, {"title": "3.2 Fractal Dimension of The Pullback Attractor", "content": "In this section, we will provide a bound for the dimension of the image of H under certain assumptions on\nthe skew product flow (\u03b8, \u03a6). We will also consider the set\n$H_V = \\bigcup_{u \\in V}{H(u)}$\nfor any subset VC U. For ease of notation, we will denote Hu as H.\nThere are various definition of dimension that generalises the notion of dimension on the Euclidean space to\nmetric and topological spaces ([28] provides an excellent introduction to various definition of dimension and\ntheir applications in dynamical systems). Key aspects of these generalisation includes\n1. the dimension of a singleton is zero,\n2. the dimension of an open subset of an n-dimensional Euclidean space is n.\nWe will use the upper box-counting dimension or the upper fractal dimension as it provide an upper bound\nfor the other notions of dimension which are use to established bounds on the embedding property of sets.\nLet (Z, d) be a compact metric space. For any \u025b > 0, let N(Z, \u025b) be the minimum number of balls of radius\n\u025b needed to cover Z with centers in Z. Note that N(Z, \u025b) is finite for each \u025b > 0 since Z is compact. The\nupper fractal dimension is defined as\n$dim_B Z = \\limsup_{\\epsilon \\rightarrow 0} \\frac{\\log_2{N(Z, \\epsilon)}}{-\\log_2{\\epsilon}}$\nFor nonautonomous dynamical systems, the theory of dimension attempts to show that, under certain con-\nditions, the attractor of a nonautonomous dynamical system is finite dimensional [5, 27, 28, 2]. Dynamics"}, {"title": "4 Conclusions", "content": "In this paper, we provide two important results for the box-counting dimension of the pullback attractor of\nreservoir systems. First, using a result of Vishik and Chepyzhov [5], we demonstrate that the box-counting\ndimension of the pullback attractor of a driven reservoir system is bounded above by the box-counting di-\nmension of the space of input sequences in the product topology. A key ingredient to extending this result\nto a general embedding theorem for RC systems is to understand the properties of space of trajectories of\ninvertible dynamical systems in the product topology. As the skew product flow is dynamical system of the\nproduct space U \u00d7 X, a version of Takens' delay embedding theorem [27] applies to their compact invariant\nsets with finite box-counting dimension. In particular, if one can establish an embedding between a dynam-\nical system and its space of trajectories in the product topology, one can lift the question of whether or the\nnot the generalised synchronisation is an embedding into the corresponding question for the skew product\nflow. This approach, if it works, will avoid the intricacies of working for example on a compact manifold.\nIn the infinite dimensional case, we show that the pullback attractor always contains an open set and"}]}