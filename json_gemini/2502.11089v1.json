{"title": "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention", "authors": ["Jingyang Yuan", "Huazuo Gao", "Damai Dai", "Junyu Luo", "Liang Zhao", "Zhengyan Zhang", "Zhenda Xie", "Y. X. Wei", "Lean Wang", "Zhiping Xiao", "Yuqing Wang", "Chong Ruan", "Ming Zhang", "Wenfeng Liang", "Wangding Zeng"], "abstract": "Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.", "sections": [{"title": "1. Introduction", "content": "The research community increasingly recognizes long-context modeling as a crucial capability for next-generation large language models, driven by diverse real-world applications ranging from in-depth reasoning (DeepSeek-AI, 2025; Zelikman et al., 2022), repository-level code generation (Zhang et al., 2023a; Zhang et al.) and multi-turn autonomous agent systems (Park et al., 2023). Recent breakthroughs, including OpenAI's o-series models, DeepSeek-R1 (DeepSeek-AI, 2025), and Gemini 1.5 Pro (Google et al., 2024), enabling models to process entire codebases, lengthy documents, maintain coherent multi-turn conversations over thousands of tokens, and perform complex reasoning across long-range dependencies. However, the high complexity (Zaheer et al., 2020) of vanilla Attention (Vaswani et al., 2017) mechanisms emerges as a critical latency bottleneck as sequence length increases. Theoretical estimates indicate that attention"}, {"title": "2. Rethinking Sparse Attention Methods", "content": "Modern sparse attention methods have made significant strides in reducing the theoretical computational complexity of transformer models. However, most approaches predominantly apply sparsity during inference while retaining a pretrained Full Attention backbone, potentially introducing architectural bias that limits their ability to fully exploit sparse attention's advantages. Before introducing our native sparse architecture, we systematically analyze these limitations through two critical lenses."}, {"title": "2.1. The Illusion of Efficient Inference", "content": "Despite achieving sparsity in attention computation, many methods fail to achieve corresponding reductions in inference latency, primarily due to two challenges:\nPhase-Restricted Sparsity. Methods such as H2O (Zhang et al., 2023b) apply sparsity"}, {"title": "2.2. The Myth of Trainable Sparsity", "content": "Our pursuit of native trainable sparse attention is motivated by two key insights from analyzing inference-only approaches: (1) Performance Degradation: Applying sparsity post-hoc forces models to deviate from their pretrained optimization trajectory. As demonstrated by Chen et al. (2024), top 20% attention can only cover 70% of the total attention scores, rendering structures like retrieval heads in pretrained models vulnerable to pruning during inference. (2) Training Efficiency Demands: Efficient handling of long-sequence training is crucial for modern LLM development. This includes both pretraining on longer documents to enhance model capacity, and subsequent adaptation phases such as long-context fine-tuning and reinforcement learning. However, existing sparse attention methods primarily target inference, leaving the computational challenges in training largely unaddressed. This limitation hinders the development of more capable long-context models through efficient training. Additionally, efforts to adapt existing sparse attention for training also expose challenges:\nNon-Trainable Components. Discrete operations in methods like ClusterKV (Liu et al., 2024) (includes k-means clustering) and MagicPIG (Chen et al., 2024) (includes SimHash-based selecting) create discontinuities in the computational graph. These non-trainable components prevent gradient flow through the token selection process, limiting the model's ability to learn optimal sparse patterns.\nInefficient Back-propagation. Some theoretically trainable sparse attention methods suffer from practical training inefficiencies. Token-granular selection strategy used in approaches"}, {"title": "2.3. Native Sparsity as an Imperative", "content": "These limitations in inference efficiency and training viability motivate our fundamental redesign of sparse attention mechanisms. We propose NSA, a natively sparse attention framework that addresses both computational efficiency and training requirements. In the following sections, we detail the algorithmic design and operator implementation of NSA."}, {"title": "3. Methodology", "content": "Our technical approach spans algorithm design and kernel optimization. In the following subsections, we first introduce the background of our methodology. Then we present the overall framework of NSA, followed by its key algorithmic components. Finally, we detail our hardware-optimized kernel design that maximizes practical efficiency."}, {"title": "3.1. Background", "content": "Attention Mechanism is widely used in language modeling where each query token $q_t$ computes relevance scores against all preceding keys $k_{:t}$ to generate a weighted sum of values $v_{:t}$. Formally, for an input sequence of length $t$, the attention operation is defined as:\n$o_t = \\text{Attn} (q_t, k_{:t}, v_{:t})$\nwhere Attn denotes the attention function:\n$\\text{Attn} (q_t, k_{:t}, v_{:t}) = \\sum_{i=1}^t a_{t,i} v_i, \\quad a_{t,i} = \\frac{e^{\\frac{q_t k_i}{\\sqrt{d_k}}}}{\\sum_{i'=1}^t e^{\\frac{q_t k_{i'}}{\\sqrt{d_k}}}} \\qquad (2)$\nHere, $a_{t,i}$ represents the attention weight between $q_t$ and $k_i$, and $d_k$ is the feature dimension of keys. As sequence length increases, attention computation becomes increasingly dominant in the overall computational cost, presenting significant challenges for long-context processing.\nArithmetic Intensity is the ratio of compute operations to memory accesses. It intrinsically shapes algorithm optimization on hardware. Each GPU has a critical arithmetic intensity determined by its peak compute capability and memory bandwidth, calculated as the ratio of these two hardware limits. For computation tasks, arithmetic intensity above this critical threshold becomes compute-bound (limited by GPU FLOPS), while below it becomes memory-bound (limited by memory bandwidth).\nSpecifically for causal self-attention mechanism, during training and prefilling phases, batched matrix multiplications and attention computations exhibit high arithmetic intensity, making these stages compute-bound on modern accelerators. In contrast, auto-regressive decoding becomes memory-bandwidth constrained because it generates one token per forward pass while requiring loading the entire key-value cache, resulting in low arithmetic intensity. This leads to different optimization goals \u2014 reducing computation cost during training and prefilling, while reducing memory access during decoding."}, {"title": "3.2. Overall Framework", "content": "To leverage the potential of attention with natural sparse pattern, we propose replacing the original key-value pairs $k_{:t}, v_{:t}$ in Equation (1) with a more compact and information-dense set of representation key-value pairs $\\tilde{K}_t, \\tilde{V}_t$ given each query $q_t$. Specifically, we formally define the optimized attention output as follows:\n$\\tilde{K}_t = f_k(q_t, k_{:t}, v_{:t}), \\quad \\tilde{V}_t = f_v (q_t, k_{:t}, v_{:t})$\n$o = \\text{Attn} (q_t, \\tilde{K}_t, \\tilde{V}_t)$\nwhere $\\tilde{K}_t, \\tilde{V}_t$ are dynamically constructed based on the current query $q_t$ and the contextual memory $k_{:t}, v_{:t}$. We can design various mapping strategies to get different categories of $\\tilde{K}, \\tilde{V}$, and combine them as follows:\n$o = \\sum_{c \\in C} g_c \\cdot \\text{Attn} (q_t, \\tilde{K}_c, \\tilde{V}_c)$.\nAs illustrated in Figure 2, NSA have three mapping strategies $C = \\{cmp, slc, win\\}$, representing compression, selection, and sliding window for keys and values. $g_c \\in [0,1]$ is the gate score for corresponding strategy c, derived from input features via an MLP and sigmoid activation. Let $N_t$ denote the total number of remapped keys/values:\n$N_t = \\sum_{c \\in C} \\text{size}[\\tilde{K}_c]$.\nWe maintain a high sparsity ratio by ensuring$N_t \\ll t$."}, {"title": "3.3. Algorithm Design", "content": "In this subsection, we introduce the design of our remapping strategies $f_k$ and $f_v$: token compression, token selection, and sliding window."}, {"title": "3.3.1. Token Compression", "content": "By aggregating sequential blocks of keys or values into block-level representations, we obtain compressed keys and values that capture the information of the entire block. Formally, the compressed key representation is defined as:\n$\\tilde{K}^{cmp} = f_{cmp} (k_{:t}) = \\{(K_{id+1:id+l}) | 1 \\leq i \\leq \\lfloor \\frac{t}{d} \\rfloor \\}$\nwhere $l$ is the block length, $d$ is the sliding stride between adjacent blocks, and $p$ is a learnable MLP with intra-block position encoding to map keys in a block to a single compressed key. $\\tilde{K}^{cmp} \\in \\mathbb{R}^{d_k \\times \\lfloor \\frac{t}{d} \\rfloor}$ is tensor composed by compresion keys. Usually, we adopt $d < l$ to mitigate information fragmentation. An analogous formulation holds for the compressed value representation $\\tilde{V}^{cmp}$. Compressed representations capture coarser-grained higher-level semantic information and reduce computational burden of attention."}, {"title": "3.3.2. Token Selection", "content": "Using only compressed keys, values might lose important fine-grained information, motivating us to selectively preserve individual keys, values. Below we describe our efficient token selection mechanism that identifies and preserves the most relevant tokens with low computational overhead.\nBlockwise Selection. Our selection strategy processes key and value sequences in spacial continuous blocks, motivated by two key factors: hardware efficiency considerations and inherent distribution patterns of attention scores. Blockwise selection is crucial to achieve efficient computation on modern GPUs. That is because modern GPU architectures exhibit significantly higher throughput for continuous block accesses compared to random index-based reads. Also, blockwise computation enables optimal utilization of Tensor Cores. This architectural characteristic has established blockwise memory access and computation as a fundamental principle in high-performance attention implementations, as exemplified by FlashAttention's block-based design. Blockwise selection follows the inherent distribution patterns of attention scores. Prior works (Jiang et al., 2024) have shown that attention scores often exhibit spatial continuity, suggesting that neighboring keys tend to share similar importance levels. Our visualization in Section 6.2 also shows this spatial continuous pattern.\nTo implement blockwise selection, we first divide key, value sequences into selection blocks. To identify the most important blocks for attention computation, we need to assign importance scores to each block. Below we present our method for computing these block-level importance scores.\nImportance Score Computation. Computing block importance scores could introduce significant overhead. Fortunately, the attention computation of compression tokens produces intermediate attention scores that we can leverage to induce selection block importance scores, formulated as:\n$p_t^{cmp} = \\text{Softmax} (q_t K^{cmp}), \\qquad (8)$\nwhere $p_t^{cmp} \\in \\mathbb{R}^{\\lfloor \\frac{t}{d} \\rfloor}$ is the attention scores between $q_t$ and compression keys $K^{cmp}$. Let $l'$ denote the selection block size. When compression blocks and selection blocks share the same blocking scheme, i.e., $l' = l = d$, we can directly obtain the selection block importance scores $p_t^{slc}$ by $p_t^{slc} = p_t^{cmp}$ straightforwardly. For cases where the blocking schemes differ, we derive the importance scores for selection blocks according to their spatial relationship. Given $d | l$ and $d | l'$, we have:\n$p_t^{slc} [j] = \\sum_{m=0}^{\\frac{l'}{d}-1} \\sum_{n=0}^{d-1} p_t^{cmp} [j \\cdot d + m + n] \\qquad (9)$\nwhere [.] denotes the indexing operator for accessing vector element. For models employing GQA or MQA where key-value caches are shared across query heads, consistent block selection across these heads has to be ensured to minimize KV cache loading during decoding. The shared importance scores across heads in a group are formally defined as:\n$P_t^{slc'} = \\sum_{h=1}^{H} P_t^{slc, (h)} \\qquad (10)$\nwhere (h) in the superscript denotes the head index, and H is the number of query heads in each group. This aggregation ensures consistent block selection across heads within the same group."}, {"title": "3.3.3. Sliding Window", "content": "In attention mechanisms, local patterns typically adapt faster and can dominate the learning process, potentially preventing the model from effectively learning from compression and selection tokens. To address this issue, we introduce a dedicated sliding window branch that explicitly handles local context, allowing other branches (compression and selection) to focus on learning their respective features without being shortcutted by local patterns. Specifically, we maintain recent tokens $\\tilde{K}^{win} = k_{t-w:t}, \\tilde{V}^{win} = v_{t-w:t}$ in a window w, and isolate attention computations of different information sources (compression tokens, and selected tokens, sliding window) into separate branches. These branch outputs are then aggregated through a learned gating mechanism. To further prevent shortcut learning across attention branches with marginal computational overhead, we provide independent keys and values for three branches. This architectural design enables stable learning by preventing gradient interference between local and long-range pattern recognition, while introducing minimal overhead.\nAfter obtaining all three categories of keys and values $(\\tilde{K}^{cmp}, \\tilde{V}^{cmp}; \\tilde{K}^{slc}, \\tilde{V}^{slc}; \\text{ and } \\tilde{K}^{win}, \\tilde{V}^{win})$, we compute the final attention output following Equation (5). Together with the compression, selection, and sliding window mechanisms described above, this forms the complete algorithmic framework of NSA."}, {"title": "3.4. Kernel Design", "content": "To achieve FlashAttention-level speedup during the training and prefilling, we implement hardware-aligned sparse attention kernels upon Triton. Given MHA is memory-intensive and inefficient for decoding, we focus on architectures with shared KV caches like GQA and MQA following the current state-of-the-art LLMs. While compression and sliding window attention computations are readily compatible with existing FlashAttention-2 kernels, we introduce the specialized kernel design for sparse selection attention. If we were to follow FlashAttention's strategy of loading temporally continuous query blocks into SRAM, it would result in inefficient memory access since queries within a block may require disjoint KV blocks. To address this, our key optimization lies in a different query grouping strategy: for each position on the query sequence, we load all query heads within a GQA group (they share the same sparse KV blocks) into SRAM. Figure 3 illustrates our forward pass implementation. The proposed kernel architecture is characterized by the following key features:\n1. Group-Centric Data Loading. For each inner loop, load all heads' queries $Q \\in \\mathbb{R}^{[h,d_k]}$ in the group at position t and their shared sparse key/value block indices $I_t$."}, {"title": "5. Efficiency Analysis", "content": "We evaluate the computational efficiency of NSA against Full Attention on an 8-GPU A100 system. In efficiency analysis, we also configure the model with GQA group g = 4, heads per group h = 16, query/key dimension $d_q = 192$, and value dimension $d_v = 128$. Following the same settings in Section 4, we set NSA compression block size l = 32, sliding stride d = 16, selected block size l' = 64, selected block count n = 16, and sliding window size w = 512."}, {"title": "5.1. Training Speed", "content": "We compare the Triton-based implementations of our NSA attention and Full Attention with Triton-based FlashAttention-2 to ensure fair speed comparison across the same backend. As shown in Figure 6, our NSA achieves progressively greater speedups as context length increases, up to 9.0\u00d7 forward and 6.0\u00d7 backward speedup at 64k context-length. Notably, the speed advantage becomes more pronounced with longer sequences. This speedup stems from our"}, {"title": "5.2. Decoding Speed", "content": "The decoding speed of Attention is primarily determined by the memory access bottleneck, which is closely tied to the amount of KV cache loading. In each decoding step, Our NSA just needs to load at most $\\lceil \\frac{t}{s} \\rceil$ compression tokens, nl' selected tokens, and w neighbor tokens, where s is the cached sequence length. As shown in Table 4, our method exhibits a significant reduction in latency as the decoding length increases, achieving up to 11.6\u00d7 speedup at 64k context-length. This advantage in memory access efficiency also amplifies with longer sequences."}, {"title": "6. Discussion", "content": "In this section, we reflect on the development process of NSA and discuss key insights gained from our exploration of different sparse attention strategies. While our approach demonstrates promising results, understanding the challenges encountered with alternative strategies and analyzing attention patterns provides valuable context for future research directions. We first examine challenges with alternative token selection strategies that motivated our design choices, followed by visualizations that offer insights into attention distribution patterns."}, {"title": "6.1. Challenges with Alternative Token Selection Strategies", "content": "Before designing NSA, we explored adapting existing sparse attention methods to the training stage. However, these attempts encountered various challenges, prompting us to design a different sparse attention architecture:\nKey-Clustering Based Strategies. We examined clustering-based strategies like ClusterKV (Liu et al., 2024). These methods store Keys and Values from the same cluster in contiguous memory regions. While theoretically feasible for training and inference, they face three significant challenges: (1) Non-trivial computational overhead introduced by dynamic clustering mechanisms; (2) Operator optimization difficulties exacerbated by inter-cluster imbalances, especially in Mixture-of-Experts (MoE) systems, where skewed Expert Parallelism (EP) group execution times lead to persistent load imbalances; (3) Implementation constraints arising from the need for mandatory periodic reclustering and chunk-sequential training protocols. These combined factors create substantial bottlenecks, significantly limiting their effectiveness for real-world deployment."}, {"title": "7. Related Works", "content": "We review existing approaches that improve the efficiency of attention computation through sparse attention. These methods can be broadly categorized into three groups based on their core strategies: (1) fixed sparse pattern, (2) dynamic token pruning, and (3) query-aware selection. We introduce several representative works from each category."}, {"title": "7.1. Fixed Sparse Pattern", "content": "Sliding Window is a commonly used approach that allows the query to compute attention only within a fixed window. StreamingLLM (Xiao et al., 2023) addresses the challenges of processing long text streams by maintaining two critical portions of the context: an attention sink (early tokens) and a local context window. While these approaches effectively reduce memory and computation costs, their rigid pattern of ignoring contexts limits their performance on tasks requiring full context understanding."}, {"title": "7.2. Dynamic Token Pruning", "content": "H2O (Zhang et al., 2023b) implements an adaptive approach to reduce KV-cache memory usage during decoding. This method dynamically evicts tokens deemed less important for future predictions based on their recent utility according to attention score. SnapKV (Li et al., 2024) also introduces a token pruning strategy that reduces the KV cache by selectively retaining only the most crucial features, enabling efficient memory usage. SnapKV identifies important features through attention weight analysis and voting during prefilling, then updates KV cache by combining selected compressed features with recent context to maintain prompt consistency."}, {"title": "7.3. Query-Aware Selection", "content": "Quest (Tang et al., 2024) employs a blockwise selection strategy where each chunk's importance is estimated by product between query and coordinate-wise min-max of the key chunks. The results scores help to select top-n important key-value chunks for attention. InfLLM (Xiao et al., 2024) combines fixed patterns with retrieval by maintaining attention sinks, local context, and retrievable chunks. This method selects representative keys from each chunk to estimate chunk importance. HashAttention (Desai et al., 2024) formulates pivotal token identification as a recommendation problem by mapping queries and keys to Hamming space using learned functions. ClusterKV (Liu et al., 2024) achieves sparsity by firstly clustering keys and then selecting the most relevant clusters for attention computation based on query-cluster similarity."}, {"title": "8. Conclusion", "content": "We present NSA, a hardware-aligned sparse attention architecture for efficient long-context modeling. By integrating hierarchical token compression with blockwise token selection within a trainable architecture, our architecture achieves accelerated training and inference while maintaining Full Attention performance. NSA advances the state-of-the-art by demonstrating general benchmark performance matches full-attention baselines, exceeding modeling capability in long-context evaluations, and enhanced reasoning ability, all accompanied by measurable reductions in computational latency and achieving significant speedup."}]}