{"title": "Enhancing Lie Detection Accuracy: A Comparative Study of Classic ML, CNN, and GCN Models using Audio-Visual Features", "authors": ["Abdelrahman Abdelwahab", "Advait Bharathulwar", "Akshaj Vishnubhatla", "Arnav Kommaraju", "Ayaan Vaswani", "Bohan Yu"], "abstract": "Abstract-Inaccuracies in polygraph tests often lead to wrong-ful convictions, false information, and bias, all of which have significant consequences for both legal and political systems. Recently, analyzing facial micro-expressions has emerged as a method for detecting deception; however, current models have not reached high accuracy and generalizability. The purpose of this study is to aid in remedying these problems. The unique multimodal transformer architecture used in this study improves upon previous approaches by using auditory inputs, visual facial micro-expressions, and manually transcribed gesture annotations, moving closer to a reliable non-invasive lie detection model. Visual and auditory features were extracted using the Vision Transformer and OpenSmile models respectively, which were then concatenated with the transcriptions of participants' micro-expressions and gestures. Various models were trained for the classification of lies and truths using these processed and con-catenated features. The CNN Conv1D multimodal model achieved an average accuracy of 95.4%. However, further research is still required to create higher-quality datasets and even more generalized models for more diverse applications.\nIndex Terms-multimodal, polygraph, GCN, facial micro-expressions, multimodal model, deception detection, conv1d, CNN.", "sections": [{"title": "I. INTRODUCTION", "content": "Lie detection is a recurring focus of research and technolog-ical innovation in law enforcement and criminal justice. Ac-cording to a survey conducted by the University of Wisconsin-La Crosse, about 75% of survey respondents reported telling zero to two lies per day; lying comprised 7% of the total communication, with 79% of the lies being told face-to-face and 21% being mediated [3].\nCurrent technologies, such as polygraphs, have focused on biological responses such as blood pressure to detect lies. However, these methods are unpredictable and flawed easily. Recently, research has begun to focus on various indicators of deception, including facial micro-expressions and audio cues [4]. Facial micro-expressions (ME) are intentional or involuntary localized and momentary movements of the face, usually lasting less than 500 ms [2].\nDespite advances in lie detection techniques, traditional methods remain intrusive, subjective, and often inaccurate. Detecting deception through ME and speech analysis presents a significant challenge owing to the subtle and brief nature of these cues. This study aimed to address these limitations by developing a non-intrusive, objective, and highly accurate method for detecting deception using both ME and audio signals. Accurate lie detection is crucial in various fields, including security, legal systems, and psychological evaluation. The primary objective of this study was to establish an AI model that can differentiate between truth and deception with high accuracy by analyzing audio, visual cues in videos, and extracted gestures. Audio dialogue, visuals, and gestures help distinguish between deception and truthfulness, making them important features to consider [23]. Therefore, the Real-life Deception Detection Dataset from the University of Michigan was used, which includes 121 videos of deception and truthfulness and a CSV file for gestures. Visuals and audio were extracted from the videos, and OpenSMILE and Vision Transformer (ViT) were used to extract features from the audio and video, respectively. Classical machine learning models, such as Random Forest Classifiers and Logistic Regression can serve as accurate baseline references for binary classification tasks, such as truth and lie. Yet to build off of that, by leveraging advanced neural network models, such as Conv1D, Graph Convolutional Networks (GCN), and Convolutional Neural Network Long Short-Term Memory (CNN LSTM), the accuracy can be increased.\nThis study addresses the following research questions: How effective is the proposed AI model for detecting lies compared with traditional methods and some recent AI models? Which features carry the highest weights in prediction? Deception detection technology has the potential to revolutionize various fields. Law enforcement can improve interrogation outcomes and border security by identifying deceptive behaviors. In"}, {"title": "II. LITERATURE REVIEW", "content": "A. Prior solutions\nThe paper, titled Facial Micro-Expression Recognition Based on Deep Local-Holistic Networks, introduces a Deep Local-Holistic Network (DLHN) for micro-expression recog-nition, comprising two sub-networks: the Hierarchical Con-volutional Recurrent Neural Network (HCRNN) and the Ro-bust Principal Component Analysis Recurrent Neural Network (RPRNN). The HCRNN captures local spatiotemporal features using CNNs and BRNNs, whereas the RPRNN extracts global sparse features using RPCA and BLSTM networks. The DLHN was evaluated on four combined datasets (CASME I, CASME II, CAS(ME)2, and SAMM) and achieved an accuracy of 60.31%, outperforming several state-of-the-art methods [1].\nIn a study by Feng (2021), titled DeepLie: Detect Lies with Facial Expression (Computer Vision), the author developed a deep learning-based approach to lie detection using facial micro-expressions in video streams. This method employs a Siamese network architecture with triplet loss to effectively distinguish between truthful and deceptive expressions. The key components include the use of CNNs for feature ex-traction and a GRU-based RNN for sequence learning. The model achieved an 81.82% accuracy on the validation dataset. However, the study highlighted limitations due to the small size of the dataset, which may hinder the generalizability of the model. The authors suggest that future work should focus on incorporating multi-modal data (e.g., audio and text) and expanding the dataset to include more diverse scenarios [18].\nThe Hybrid Machine Learning Model for Lie Detection research dataset included thermal images. This study em-ployed a hybrid machine-learning approach that combines the strengths of CNNs and SVMs. CNNs were used to automatically extract relevant features from the input data automatically. The extracted features were then fed into an SVM for classification. The hybrid model demonstrated an accuracy of 58%. However, the complexity of the hybrid model, which combines CNNs and SVMs, can lead to higher computational costs and increased difficulty in scaling the approach to larger datasets [24].\nAudio-Visual Deception Detection Using the DOLOS Dataset study used a DOLOS dataset that combined syn-chronized audio and video signals. The study employed Ima-geNet pre-trained ViT as the backbone network for the visual modality and tokenized face images with a 2D-CNN module, resulting in a feature with dimensions of 64 \u00d7 256. For audio modality, the study adopted a pre-trained W2V2 model. The raw audio was tokenized using the 1D-CNN module, resulting in a feature size of 64 \u00d7 512 for each audio sample. The plug-in audio visual fusion model and multi-task learning achieved an accuracy of 66.84% [19].\nNumerous studies have been conducted using the Real-Life Trial Dataset. Camara et al (2024) summarized many studies conducted using this dataset. Ding et al. (2019) used a CNN model. ResNet served as the backbone of the facial expression and computed the temporal feature maps. This was used to achieve accuracy in conjunction with a Generative Adversarial Network (GAN). This model was reported to achieve a 97% accuracy, which is the highest among current deep learning models [21]. Among the non-deep learning models, the use"}, {"title": "B. Models Overview", "content": "1) Logistic Regression: Two classical machine learning models were considered for this study: Logistic Regression and Random Forest Classifier. Logistic regression is a binary classification technique based on a sigmoid function. This function is used to weigh features in a way that returns a value from zero to one [32]. The logistic regression function can be defined using equation [33]\n$P(y = 1 | X) = \\sigma(z) = \\frac{1}{1+e^{-z}}$\nWhere:\n\u2022 P(X) is the probability that the outcome y is 1 given the input X.\n\u2022 z is the linear combination of input features and their corresponding weights, defined as:\nz = w^X + b = W1x1 + W2x2 + \u00b7\u00b7\u00b7 + Wnxn + b\nwhere:\nX = [x1, x2,...,xn] are input features.\nw = [W1,W2,...,Wn] are the weights (parameters) associated with each feature.\nb is the bias (intercept term).\n\u2022 $\\sigma(z)$ is the sigmoid function, which squashes the output into the range (0, 1):\n$\\sigma(2) = \\frac{1}{1+ e^{-z}}$\nThis function creates an S-shaped curve that determines the binary classification. This model was tested in this study because of its application in sentiment analysis, and it has high success in facial expressions recognition, as shown by Goyani et al [32]. This may mean that the model will be useful in detecting facial micro-expressions, which were previously mentioned as being vital for detecting deception.\n2) Random Forest Classifier: The second model is the Random Forest Classifier (RF). In brief, Random Forest is an ensemble model that chain multiple decision trees during training to merge results, improve accuracy and reducing overfitting [30]. The decision trees were generated by using a bagging algorithm (voting majority). Many decision trees that can enter RF predict an output by forming a \"forest\"\nof classifiers that vote for the classification of an input. The RF classifier was considered for this study particularly for the applications of decision trees in sentiment analysis, specifically for speech emotion recognition [32]. Considering that speech plays a considerable part in lie detection, the RF classifier has potential to increase deception detection accuracy.\n3) Graph convolutional Network (GCN): A graph consists of nodes (vertices) and edges (connections between nodes). In a GCN, each node represents an entity, and the edges represent the relationships between these entities. The primary goal of GCNs is to learn node embeddings, which are vector representations of nodes that capture the graph's structural and feature information.\n4) CNN conv1d:\n\"Generally, 1D-CNNs are designed to handle one-dimensional data, such as time-series data, se-quences (e.g., text), or any data where the primary structure is along a single axis. The kernel (or filter) in a 1D-CNN moves in one dimension. If the data are represented as vectors [x1,x2,...,xn], the kernel slides over this vector to detect the patterns within the sequence. The shape of the kernel is a 1D array with dimension (k,), where k is the size of the kernel.\"\n- A. O. Ige and M. Sibiya, \"State-of-the-art in 1D Convolutional Neural Networks: A Survey,\" [35]\nIn a 1D Convolutional Neural Network, the kernel moves along a single axis of the input vector, effectively processing the data. The receptive field of a 1D-CNN kernel involves a contiguous segment of 1-D input. As the kernel slides across the input, it aggregates the information from k consecutive elements. For a given input sequence x and kernel w, the convolution operation in a 1D-CNN layer can be expressed as [35]:\n$(x * w)(t) = \\sum_{i=0}^{k-1} x(t + i) \u00b7 w(i)$\nwhere:\n\u2022 x is the 1-d input,\n\u2022 w is the kernel (or filter),\n\u2022 (x*w)(t) denotes the convolution of x and w at position t,\n\u2022 k is the size of the kernel,\n\u2022 x(t + i) is the element of the input sequence at position t+i,\n\u2022 w(i) is the element of the kernel at position i.\nForward propagation in 1D-CNN involves passing the input through one or more convolutional layers, pooling layers, and fully connected layers, such that feature map $Z_c$ is given as [35]:\n$Z_c = f_c(X * W_c + b_c)$\nwhere X is the input, $W_c$ denotes the filter weights, $b_c$ is the bias term, and $f_c$ is the activation function of the con-volution, and $X * W_c$ is the convolutional operation between filter weights and bias terms. The spatial dimension of $Z_c$ is reduced by aggregating information from nearby values through pooling, which is given as [35]:\n$A_p = P(Z_c)$\nwhere P denotes the pooling operation. Subsequently, a fully connected layer $Z_f$ combines the features learned from the convolutional and pooling operations, and the final activation function Y is used to obtain the output of the network. In ad-dition, backward propagation in 1D-CNN involves computing the gradients of the loss function with respect to the network's parameters, which are used to update the weights and biases. The backward propagation in the fully connected layer is as follows [35]:\n$\\frac{\\partial L}{\\partial y_f} = \\frac{\\partial L}{\\partial Z_f} \u00b7 f'_f(Z_F)$ (1)\nHere, L represents the loss function, and $f'_f$ denotes the activation function in the fully connected layer. The gradient\nof the loss with respect to the fully connected weights, $\\frac{\\partial L}{\\partial W_f}$, is given by [35]:\n$\\frac{\\partial L}{\\partial W_f} = \\frac{1}{m} \\frac{\\partial L}{\\partial Z_f} A_p^T$ (2)\nSimilarly, the gradient of the loss with respect to the fully connected biases, $\\frac{\\partial L}{\\partial b_f}$, is calculated as:\n$\\frac{\\partial L}{\\partial b_f} = \\frac{1}{m} \\sum(\\frac{\\partial L}{\\partial Z_f})$ (3)\nBackpropagation through the pooling layer is performed as outlined in Equation (3) (which is not shown in the image). For the convolutional layer, the backpropagation is given by [35]:\n$\\frac{\\partial L}{\\partial Z_c} = \\frac{\\partial L}{\\partial A_p}\u00b7 P'(Z_c)$ (4)\nThe gradient of the loss with respect to the convolutional weights, $\\frac{\\partial L}{\\partial W_c}$, is expressed as:\n$\\frac{\\partial L}{\\partial W_c} = \\frac{1}{m} \\frac{\\partial L}{\\partial Z_c} * X$ (5)\nFinally, the gradient of the loss with respect to the convo-lutional biases, $\\frac{\\partial L}{\\partial b_c}$, is computed as:\n$\\frac{\\partial L}{\\partial b_c} = \\frac{1}{m} \\sum(\\frac{\\partial L}{\\partial Z_c})$ (6)\nIn these equations, m denotes the batch size, and P'(Zc) represents the gradient of the pooling operation. The terms $\\frac{\\partial L}{\\partial W_c}$ and $\\frac{\\partial L}{\\partial b_c}$ correspond to the gradients of the loss with respect to the convolutional weights and biases, respectively. These gradients are then used to update the convolutional weights $W_c$ and biases $b_c$ using gradient descent [35]."}, {"title": "III. METHODS", "content": "A. Dataset Collection\nThe experiment was conducted under the following guiding question: Can a multimodal model of facial microexpressions and speech be used to classify human deception accurately? To successfully research a multimodal model that encompasses both a visual and speech encoder, a dataset containing both video and audio must be found.\nThe Multimodal Real Life Trial dataset was used in our experiments, which includes videos and handwritten elements, which are ideal for in-depth analysis. Each clip was labeled as deceptive or truthful and had visibility of the face of the speaker as well as the statements spoken by them during the duration of the clip. The dataset was composed of 121 testimonies, both truthful and deceptive, which were also manually transcribed and annotated with facial reactions.\nThe videos in this dataset had an average length of 28.0 seconds, with the deceptive videos averaging 27.7 seconds and the truthful videos averaging 28.3 seconds. The 56 distinct speakers in these clips were made up of 21 female and 35\nmale speakers, each between the ages of 16 to 60 [25]. This dataset was found on the University of Michigan's Deception Detection and Misinformation datasets list.\nB. Data Preprocessing (OpenSmile, ViT, manual annotations)\nIn this case, using audio, video, and textual analysis to predict the result can provide us a more accurate picture than rudimentary polygraphs and more freedom to innovate over related works and previous model pipelines. Primarily, to be able to fully grasp the importance of audio and video from the datasets, visual and auditory patterns can be extracted using two models: Vision Transformer (ViT) and OpenSmile.\nViT is an image recognition encoder. This was used to extract features from the visual data. ViT split the individual image frames of the video at a sampling rate of 50 Hz into different patches. These patches were linearly embedded with both patch and position embeddings. ViT's benefits of computational efficiency and accuracy come to light when iterating over large datasets, and thus, we use a pretrained ViT to fit this task so that it can outperform models such as ResNet and other CNNs as mentioned in [26]. The vectors from this linear projection were interpolated to match the dimensions of the audio and text features, which were then saved for concatenation [26].\nFurthermore, to satisfy the multimodal label, OpenSmile is our chosen processor, working in tandem with models such as a CNN and the simpler models mentioned below. OpenSmile, or open-source speech and music interpretation using large-space extraction, was first developed at the Technical Univer-sity of Munich to create SEMAINE, a fully socially conscious software [27]. OpenSmile's main function for that software\nis for audio and emotional analysis and feature extraction, which in this study's use case works well for identifying abnormalities in tone, hesitation, and emotional nuance be-tween inputs of lie and truth [28]. Via the clips provided in the dataset, the \"ffmpeg\" package is used to successfully extract audio from the videos in a \u201c.wav\" format at a sampling rate of 50 Hz, which will aid in concatenation with other features. In the context of this research, OpenSmile takes in wave-form input audio files and analyses features such as pitch, loudness energy, and mel-frequency cepstral coefficients (MFCC). Using the aforementioned features, aspects such as tone, rhythm, and timbre are saved in vectors to help classify our data into buckets of truths and lies [28].\nFinally, the features saved in the vector format from OpenS-mile, ViT, and handwritten annotations can be combined using simple concatenation. These were then converted into a tensor format for experimentation with various models.\nC. Dataset Curation and Analysis\nThe dataset was further curated to meet the project re-quirements. Because there was an imbalance in the number of truthful and deceptive videos, one of the deceptive videos was dropped from the dataset at random to create an even 1/2 split of 60 truthful to 60 deceptive videos. The dataset was then processed by splitting the audio files from the trial videos using FFmpeg, followed by OpenSmile to extract features from the audio. ViT was used to extract individual frames and extract features from videos.\nThe names of the different extracted feature files were placed in different split files for training, testing, and val-idation. The training set contained 70% of the dataset, the validation set contained 10% of the dataset, and the test set contained 20% of the dataset. These file names were used to identify the extracted feature files that were pulled during training, validating, and testing.\nFor CSV, the data were analyzed using pandas and seaborn. A heatmap was created to determine features that did not have a strong correlation with the target. In addition, KDE using seaborn was performed while making hue = 'class' to observe the data distribution . According to the analysis, any column that correlated less than 0.05 and the distribution of the classes (deception and truthful (0,1)) was approximately the same, was dropped. Furthermore, based on the characteristics of the dataset, pre-processing output tensors, and the number of samples, more simplistic"}, {"title": "D. Models", "content": "1) Convolutional Neural Networks: First, a Convolutional Neural Long Short-Term Memory Network (CNN LSTM), was used in our experiments. CNN LSTM models focus on spatial relations in images, describing videos, actions with text, and classification [29]. In this case, the CNN LSTM takes concatenated data of audio, visual, and annotated features and then classifies the data as truth or lie.\nThe second iteration of CNNs is a basic custom Convolu-tional Neural Network. Using a large variety of layers within a 1-dimensional CNN framework, the model matches well with the results of the data preprocessing. Furthermore, to regulate and standardize the data to avoid overfitting due to lack of samples, a dropout function and Max Pooling function, are used when needed in the layers. Further, for the classification aspect of the complex CNN, a classical sigmoidal function is used from the neural network-python module because sigmoid is good in binary classifi-cation; similarly, the loss function is Binary Cross Entropy. Equation 8 shows the sigmoid function, and Equation 9 shows the binary cross entropy loss function.\n$F_{max}(x) = max \\{x\\}=0$ (7)\n$Sigmoid(x) = \\sigma(x) = \\frac{1}{1+ exp(-x)}$ (8)\n$BCE = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i log(p_i) + (1 - y_i) log(1 \u2013 p_i)]$ (9)\n2) Classical Machine Learning Models:"}, {"title": "V. DISCUSSION", "content": "A. Interpretation of the collected results\nThe following are the commonly used evaluation metrics in classification reports: Precision is the ratio of correctly predicted positive observations to the total number of predicted positives. The metric helps determine the true pinpointed accuracy of positive results as well as gauging false positives. Second, recall is the ratio of correctly predicted positive observations to all observations in an actual class. In the context of the task, it answers the question: \"Of all the instances that were lies, how many were correctly predicted as a lie?\" Finally, the F1-score is the harmonic mean of Precision and Recall. It provides a single metric that balances both concerns, especially when there is an imbalanced dataset in which one class is more prevalent.\nThe results of the convld model are presented in Tables III and IV. This demonstrate the classification report and the accuracy's mean and standard deviation, and the illustrated performance in each trial in Fig. 9, where each trial has five folds, ensuring the model's reliability and accuracy. For example, the classification report of convld showed no bias in the model, with a standard deviation of 0.04, which is relatively low, with a mean of 95.4%, which is a respected accuracy in the context of lie detection using AI. In contrast, more typical models such as Logistic Regression or Random Forest Classifiers have lower average precision. The F-1 scores of both were close to equal, with the lie class at 83% and the truth class at 80%. Furthermore, the spectral GCN model was not proven to be successful, as seen by the difference in precision, one class reached 100% whereas the other was significantly lower, hinting at an extreme bias towards a lie or a truth. However, when we tried CNN convld on the training set without manual annotation (only audio and visuals were used), average accuracy of the model was still 95+%.\nB. The proposed model vs. previous studies\nshows a comparison of our model with those of previous studies. These figures help to answer the first research question of this paper: \"How effective is the proposed AI model in detecting lies compared to traditional methods and some recent AI models?\" For example, although the study [20] used a fusion of two different AI models, CNN and support vector machine, increasing the complexity and the need for high computational resources, our solution's best model used only conv1d and achieved higher accuracy. Its simplicity, along with its higher accuracy proves it to be a more effective model overall, especially in real-world situations.\nC. Interpretation of the proposed model\nExplainable AI (XAI) is used to interpret the model. For example, to explain the predictions of a machine learning model, SHAP (SHapley Additive exPlanations), a popular method for interpreting complex models, was used. It was used to generate a summary plot of the SHAP values, which visually represented the impact of each feature on the model's predictions. The plot helps to understand which features are the most influential and how they contribute to the model's output.\nThe Local Interpretable Model-agnostic Explanations (LIME) framework, an XAI technique, explains the predictions of a machine learning model. LIME provides insight into why a model makes a specific prediction by approximating the model locally with an interpretable model. It provides a local interpretation of a specific prediction using a neural network model. By focusing on one instance and showing how different features contribute to the prediction, LIME helps to make complex models more understandable, especially in a classification context. We used two samples, one for deception and one for truthful .\nBy analyzing feature 949 had the highest positive contribution (1.57) towards the \"Negative\" (truthful) class, indicating that when this feature is at a higher value, the model is more confident that the instance is not associated with lying. On the other hand, features 692 and 6827 had significant neg-ative contributions (-0.74 and -0.60, respectively), suggesting that when these features have lower values, the model is more\nlikely to classify the instance as \"Negative\" (truthful). The model is highly confident in predicting the \"Negative\" class (with a probability of 1.0), which is visualized by the zero probability for the \"Positive\" class. This suggests that a clear decision is made by the model based on a combination of feature values.\nFrom From , 3672, 2656, and 4835 features had values that strongly contributed to the \"Positive\" prediction, indi-cating that they were key indicators of deception according to the model. Features 1866 and 5882 contributed towards a \"Negative\" prediction (indicating truth) but were outweighed by the features supporting a \"Positive\" prediction."}, {"title": "VI. CONCLUSION", "content": "The results of this study highlight the critical role of multi-modal feature extraction techniques in advancing lie detection technologies. By leveraging audio features via OpenSmile, visual data through a Vision Transformer, and transcriptions of gestures and micro-expressions, the CNN Conv1D model achieved a high accuracy of 95.4%, surpassing many state-of-the-art approaches. Even without manual transcriptions, the model performed admirably at 95%, demonstrating the robust-ness of the architecture, particularly with a limited dataset. Both audio and visual features were vital to the performance of CNN Conv1D, and its success. Furthermore, the paper addressed the recommendation, which focuses on incorporat-ing multi-modal data, mentioned by a study's [24] author. Despite these promising results, the limitations of this study, particularly the small and homogeneous dataset-underscore the necessity for further research. It is crucial to expand dataset diversity to include participants from various demographic groups and scenarios. Explainable AI (XAI) revealed that audio and visual features were the most significant contributors\nto the model's decisions, indicating the importance of focusing on these modalities in future studies. However, integrating additional modalities such as thermal imaging, heart rate mon-itoring, and moisture tracking could further enhance model performance, especially in complex real-world applications. Addressing these challenges will not only improve model generalizability but also help mitigate ethical concerns related to biases in facial micro-expression recognition across dif-ferent racial and gender groups. Future work should explore ablation studies and alternative architectures to deepen our understanding of how multimodal learning can be optimized. By continuing to build on this research, we move closer to creating an accurate, reliable, and ethical alternative to tradi-tional polygraph tests, with potential applications in criminal justice and law enforcement."}]}