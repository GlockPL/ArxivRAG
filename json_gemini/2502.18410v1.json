{"title": "TSKANMixer: Kolmogorov\u2013Arnold Networks with MLP-Mixer Model for Time Series Forecasting", "authors": ["Young-Chae Hong", "Bei Xiao", "Yangho Chen"], "abstract": "Time series forecasting has long been a focus of research across diverse fields, including economics, energy, healthcare, and traffic management. Recent works have introduced innovative architectures for time series models, such as the Time-Series Mixer (TSMixer), which leverages multi-layer perceptrons (MLPs) to enhance prediction accuracy by effectively capturing both spatial and temporal dependencies within the data. In this paper, we investigate the capabilities of the Kolmogorov-Arnold Networks (KANs) for time-series forecasting by modifying TSMixer with a KAN layer (TSKANMixer). Experimental results demonstrate that TSKANMixer tends to improve prediction accuracy over the original TSMixer across multiple datasets, ranking among the top-performing models compared to other time series approaches. Our results show that the KANs are promising alternatives to improve the performance of time series forecasting by replacing or extending traditional MLPs.", "sections": [{"title": "Introduction", "content": "Time-series analysis is essential across a wide range of domains, including retail (B\u00f6se et al. 2017), finance (Taylor 2008), economics (Granger and Newbold 2014), transportation (Chen et al. 2001; Yin et al. 2021), energy (Mart\u00edn et al. 2010; Qian et al. 2019; Heidrich et al. 2020), healthcare (Bui et al. 2018; Kaushik et al. 2020), and climate (Wu et al. 2023), where understanding and forecasting temporal patterns is crucial for decision-making and planning. In recent years, various deep learning (DL)-based forecasting models, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), multi-layer perceptrons (MLPs), and Transformers, have been extensively studied to capture the complexity in real-world time-series datasets that are often multivariate with complex, non-linear dependencies among them (Wang et al. 2024b; Liu and Wang 2024).\nHowever, contrary to the common intuition that DL-based models should be more effective than univariate models, it is shown that Transformer-based models can indeed be significantly worse than simple univariate temporal linear models on many commonly used forecasting benchmarks since they suffer from overfitting (Nie et al. 2022; Zeng et al. 2023).\nInstead, recent work has demonstrated that simple univariate linear models can outperform such deep learning models on several commonly used academic benchmarks. Recently, Chen et al. (Chen et al. 2023), inspired by the well-known MLP Mixer architecture in computer vision (Tolstikhin et al. 2021), proposed a fully MLP-based architecture for time series forecasting, Time-Series Mixer (TSMixer), by alternatively stacking multiple MLPs to capture temporal information in the time-domain and cross-variate information in the feature-domain. The authors showed that state-of-the-art performance can be achieved without necessarily relying on Transformers by demonstrating TSMixer's superior performance on benchmarks like the M5 dataset.\nOn the other hand, more recently, Kolmogorov-Arnold Networks (KANs) (Liu et al. 2024) was proposed as a promising alternative to MLPs. Unlike traditional MLPs that have fixed activation functions on nodes, KANs utilize learnable activation functions on edges and perform instead a simple summation on nodes. The authors introduce KANS as a powerful new neural network architecture that can improve performance and interpretability compared to MLPs. This obviously opens opportunities for further improving deep learning models which rely heavily on MLPs (Liu et al. 2024).\nRecent research has explored the application of KANs for time-series. Xu et al. (Xu, Chen, and Wang 2024) investigated the use of KANs for time series forecasting and demonstrated that two KAN models significantly outperformed traditional forecasting methods. Similarly, Vaca-Rubio et al. (Vaca-Rubio et al. 2024) showed that KANs outperformed conventional Multi-Layer Perceptrons (MLPs) in a real-world satellite traffic forecasting task, providing more accurate results with considerably fewer learnable parameters. Finally, Genet et al. (Genet and Inzirillo 2024) proposed the adaptation of KANs to temporal sequences by combining recurrent neural networks (RNNs) and KANs. These researches confirm that the idea developed in the original KAN paper works well on real-world use cases and is highly relevant for time series analysis. In this paper, inspired by the KANs, we propose a new neural network architecture, TSKANMixer, by investigating the application of KANs to TSMixer for time series forecasting.\nThis paper is structured as follows. Section 2 presents the related work, providing fundamental background on KANS"}, {"title": "Related Work", "content": "Time-Series Mixer (TSMixer)\nTSMixer is an MLP-based architecture for time series forecasting (Chen et al. 2023), which analyzes the performance of linear models for time series forecasting rather than RNNS or Transformer-based frameworks and demonstrates its competitive performance on several time series forecasting benchmarks. TSMixer consists of multiple MLP layers across time and feature dimensions (i.e., time-mixing and feature-mixing MLP block) to capture time-domain temporal patterns and feature-domain cross-variate information alternatively with residual connections and batch norm. The residual designs ensure that TSMixer retains the capacity of temporal linear models. In contrast to recent Transformer-based models, the architecture of TSMixer is relatively simple to implement. Despite its simplicity, it demonstrates that TSMixer remains competitive with state-of-the-art models at representative benchmarks (Chen et al. 2023).\nKolmogorov-Arnold Network (KAN)\nAs MLPs are based on the universal approximation theorem (Cybenko 1989), which states that neural networks with a single hidden layer can approximate any continuous function with finite support, KANs rely on the Kolmogorov-Arnold representation theorem (Arnold 2009a,b). The theorem states that any multivariate continuous function $f(x)$ on a bounded domain, where $x = (x_1,...,x_n)$, can be written as a finite composition of continuous functions of a single variable and the binary operation of addition. Formally, a multivariate continuous function $f(x) : [0,1]^n \\rightarrow R$ can be represented by the finite composition of univariate functions (Liu et al. 2024):\n$f(x) = f(x_1,...,x_n) = \\sum_{j=1}^{2n+1} \\Phi_j( \\sum_{i=1}^{n} \\phi_{j,i}(x_i))$     (1)\nwhere an outer function is $\\Phi_j$: $R \\rightarrow R$ and an inner function is $\\phi_{j,i}$: $[0, 1] \\rightarrow R$.\nAs a MLP consists of layers where each layer performs a linear transformation followed by a non-linear activation function, a KAN layer can be defined as a matrix $\\Phi$ of univariate functions:\n$\\Phi(x) = {\\phi_{j,i}}, i = {1, ..., N_{in}}, j = {1, ..., N_{out}}$ (2)\nwhere the univariate functions $\\phi_{j,i}$ have trainable parameters and $n_{in}$ is the number of inputs and $n_{out}$ is the number of outputs.\nGenerally, KANs can be expressed by a composition of multiple KAN layers, $y = KAN(x) = (\\Phi_L \\circ \\dots \\circ \\Phi_1)(x)$ where $L$ is the number of layers. Then, the equation 1 for the Kolmogorov-Arnold representation theorem can be represented by a two-depth KAN layer of shape [n, 2n + 1,1], consisting of an inner layer with $n_{in} = n$ and $n_{out} = 2n+1$, and an outer layer with $n_{in} = 2n+1$ and $n_{out} = 1$ (Liu et al. 2024).\nWhile MLPs employ fixed activation functions on nodes, KANs employ learnable activation functions on edges (Liu et al. 2024). Specifically, KANs learn activation patterns dynamically by replacing traditional linear weights on MLPs with univariate functions parameterized as splines, where a spline is defined by the order k (the degree of the polynomial functions used to interpolate the curve between control points), and the number of intervals G (the number of segments between adjacent control points). During spline interpolation, the control points separated by G intervals are connected to form a smooth curve (Vaca-Rubio et al. 2024). Through learnable activation functions, KANs improve accuracy and interpretability while maintaining comparable or superior performance with more compact architectures across various tasks.\nVaca-Rubio et al. (Vaca-Rubio et al. 2024) demonstrate that KANs consistently outperform MLPs with lower error metrics while achieving better results with reduced computational resources in time series forecasting. However, due to their intrinsic architecture, KANs have (k + G) times more learnable parameters compared to MLPs (Yu, Yu, and Wang 2024). To enhance computational efficiency, several regularization techniques have proven effective in optimizing KAN training (Cheon 2024). Specifically, the incorporation of dropout, weight decay, and batch normalization not only accelerates convergence but also significantly improves the model's generalization capabilities. Additionally, Bayesian optimization can be leveraged to reduce the parameter search space for more efficient training (Snoek, Larochelle, and Adams 2012)."}, {"title": "TSKANMixer Architecture", "content": "In this paper, we explore and evaluate the application of a KAN layer to the MLP-based TSMixer architecture. We introduce three architectures of TSKANMixer as illustrated in Figure 2. The proposed models apply the KAN framework to learn complex, non-linear relationships in temporal data. The first proposed architecture, presented in Figure 2a, uses KAN for temporal projection on the time domain as an alternative to a fully-connected layer in TSMixer (Chen et al. 2023). It maps the time series from the input length L to the forecast horizon H by learning the complex relationships between past inputs and future predictions. The second proposed architecture, presented in Figure 2b, extends TSMixer by adding a new KAN-based time mixing layer between mixer layers and temporal projection to intensify the capability to uncover the temporal patterns in time series. All architectures use a two-depth KAN layer."}, {"title": "Experimental Results", "content": "In this section, we evaluate the forecasting performance of the two proposed TSKANMixer architectures, presented in Figures 2a and 2b. We evaluate the performance of our proposed TSKANMixer on commonly used benchmark datasets of multivariate time series that have no missing values and equal lengths across all series: Electricity Transformer Temperature (ETT) long-term forecasting dataset, introduced by Zhou et al. (Zhou et al. 2021), NN5 forecasting competition dataset (Taieb et al. 2012), Computational Intelligence in Forecasting (CIF) 2016 forecasting competition dataset (\u0160t\u011bpni\u010dka and Burda 2017), FRED-MD dataset (McCracken and Ng 2016), Exchange dataset(Lai et al. 2018) and Hospital dataset (Hyndman et al. 2008).\nDatasets The Electricity Transformer Temperature is a crucial indicator in the electric power long-term deployment. This dataset consists of two years of data from two separate counties in China. Each dataset includes the target variable \"oil temperature\" (OT) and six power load features (Zhou et al. 2021). We use publicly available data that have been pre-processed by Wu et al. (Wu et al. 2021). The NN5 dataset contains 111 time series of daily cash withdrawals from Automated Teller Machines (ATM) in the UK (Godahewa et al. 2021). The Computational Intelligence in Forecasting (CIF) 2016 contains 72 monthly time series. Out of these series, 24 series originate from the banking sector, and the remaining 48 series are artificially generated (Godahewa et al. 2021). In this paper, we use only the 48 series of equal length. The Hospital dataset collects 767 monthly time series showing patient counts related to medical products from January 2000 to December 2006 (Godahewa et al. 2021). The Exchange dataset is the collection of the daily exchange rates of eight foreign countries, including Australia, Britain, Canada, Switzerland, China, Japan, New Zealand and Singapore, ranging from 1990 to 2016 (Lai et al. 2018). The FRED-MD dataset contains 107 monthly time series showing a set of macro-economic indicators from the Federal Reserve Bank (McCracken and Ng 2016). Each dataset is standardized to achieve zero-mean normalization to ensure a fair comparison with TSMixer (Chen et al. 2023). We split the data to ensure that the test set's size closely matches the prediction length, maximizing the amount of data available for training. The statistics of the benchmark datasets and data splits are presented in Table 1.\nExperimental Setup We focus on evaluating the impact of the KAN layer on TSMixer by comparing it to the original architecture. Thus, we follow the experimental settings in the TSMixer research (Chen et al. 2023) for ETT datasets about data split and hyperparameters. We set the input length $L = 512$ as suggested in Chen et al. (Chen et al. 2023) and evaluate the results for a forecast horizon of $H = 96$. For TSKANMixer's hyperparameters on ETT, we employ a shallower architecture with fewer mixer blocks and a larger batch size compared to TSMixer. Specifically, while TSMixer uses 4 or 6 mixer blocks, TSKANMixer employs only 2 blocks. Similarly, the batch size differs significantly: 32 for TSMixer and 320 for TSKANMixer. To utilize PyKAN (Liu et al. 2024), which is implemented in PyTorch, we converted TSMixer's TensorFlow code to PyTorch to implement TSKANMixer. We verified the code conversion by comparing the results with those reported in the original TSMixer paper (Chen et al. 2023) using the ETT dataset, as shown in Table 4 in the Appendix.\nIn addition, we extensively perform experiments on various publicly available datasets that were not included in the original TSMixer paper (Chen et al. 2023). We conduct a grid search for TSMixer on the hyperparameter spaces: batch size = {8, 16, 32}, mixer blocks = {2,4,6}, dropout = {0.3, 0.5, 0.7, 0.9}, feature hidden size = {8, 16, 32, 64}, and learning rate = {0.0001, 0.001}. The models are trained for 1000 epochs with proper early stopping. We select the best configuration of TSMixer for the results shown in Table 2. For TSKANMixer's hyperparameters, we conducted manual exploration with limited parameter combinations, as an exhaustive grid search was computationally prohibitive due to the larger parameter space introduced by KAN parameters (e.g., B-spline grids, order of B-spline, and KAN hidden size). Training is also limited to 200 epochs with strict early stopping for the extended datasets. Further details on hyperparameters are summarized in Table 5 in the Appendix.\nAs benchmark comparisons, we select various state-of-the-art time series models including MLP-based Series-core Fused Time Series (SOFTS) (Han et al. 2024), MLP-based TimeMixer (Wang et al. 2024a), GNN-based Spectral Temporal Graph Neural Network (StemGNN) (Cao et al. 2020), Transformer-based Informer (Zhou et al. 2021), and Simple MLP for multivariate forecasting. All of these models use the same prediction length (H) and input length (L) for each dataset as we do for TSMixer (Chen et al. 2023). We calculate mean squared error (MSE) and mean absolute error (MAE) as the evaluation metrics. We minimize the mean square error (MSE) or the mean absolute error (MAE) as a loss function and evaluate it over a forecast horizon. All models were trained and tested on an ml.g4dn.xlarge GPU instance, powered by a single NVIDIA T4 GPU with 16GB memory.\nExperiments We evaluate two versions of TSKANMixer proposed in Figure 2 on popular multivariate forecasting benchmark datasets, comparing them against TSMixer and other state-of-the-art time series models. Table 2 summarizes the comprehensive comparison of 8 time series forecasting models across 10 datasets using MSE and MAE metrics. The top three results for each dataset are highlighted in bold, with the best performance underlined.\nOverall, the evaluation results in Table 2 show that no time series forecasting model dominantly outperforms others across all datasets. Among benchmark models, TSMixer and SOFTS demonstrate relatively better performance than other models, followed closely by Informer, while TimeMixer shows moderate performance. MLP and StemGNN exhibit lower accuracy. Notably, StemGNN encounters an out-of-memory issue on the ETT dataset. As a result, StemGNN's performance on the ETT dataset is not reported in Table 2.\nThe performance improvements of TSKANMixer models compared to TSMixer are indicated by percentage changes ($\\Delta$%) under TSKANMixer in Table 2. For instance, on the ETTh2 dataset, TSKANMixer (v02) shows a substantial -18.97% improvement in MSE and -9.41% in MAE over the TSMixer. The performance improvements from TSMixer show that the predictions obtained by one of TSKANMixer are better than the baseline TSMixer by Chen et al. (Chen et al. 2023) in MSE or MAE across eight datasets, except for CIF 2016 and FRED-MD. In particular, TSKANMixer demonstrates the best or second-best performance on ETTh1, ETTh2, ETTm1, ETTm2, NN5 daily, NN5 weekly, Hospital, and FRED-MD. Both versions of TSKANMixer achieved a top-three ranking 7 times each out of 10 datasets. The result implies that the KAN layer improves prediction performance over the original TSMixer architecture. As an exception, in the CFI 2016 case, all models show poor performance on multivariate predictions, showing significantly high MSE on the normalized dataset. Only StemGNN shows the best performance, and it is the only dataset where StemGNN ranks in the top three. This could imply that the dataset has different time series characteristics that are not captured by current variants of TSKANMixer and TSMixer.\nOn the other hand, TSKANMixer exhibits significantly slower training times compared to TSMixer due to the incorporation of the KAN layer. According to PyKAN (Liu et al. 2024), the primary bottleneck of KAN is its slow training process, as KANs introduce additional complexity and computations. The study reports that KANs are typically 10 times slower than MLPs, given the same number of parameters. The training limitation constrains the testing of TSKANMixer on larger datasets and hinders extensive hyperparameter tuning in this paper. In addition, TSKANMixer sometimes requires more epochs to complete training than TSMixer on ETT datasets. As a result, there is a case that TSKANMixer's training time is approximately up to 50 times slower than that of the original TSMixer as shown in Table 3.\nTo illustrate the slow training process, we visualize the training and validation losses over the training epochs for TSMixer and TSKANMixer, as shown in Figure 3. On the ETT datasets, TSMixer starts with a relatively low initial loss value compared to TSKANMixer. It reaches the best epoch at an earlier stage (e.g., less than 50 epochs) and starts overfitting afterwards. This is shown by the increasing validation loss and the divergence between its training loss and validation loss as the number of epochs increases in Figure 3a. On the other hand, TSKANMixer shows a poor initial loss value, but it steadily decreases the validation loss as the number of epochs increases without overfitting quickly (e.g., the best epoch happens after 50 epochs), as shown in Figure 3b. TSKANMixer effectively captures the underlying generalized patterns present in the data, rather than falling into local optima."}, {"title": "Conclusion and Future Work", "content": "In this paper, we explored the application of KAN to the TSMixer model for time series forecasting and introduced two variants of TSKANMixer. We demonstrate that the TSKANMixer models generally improve prediction performance over the original TSMixer models. This improvement is achieved by either replacing the fully-connected layer with KAN in temporal projection or by adding a timemixing layer with KAN. However, we also note that the KAN layer slows down the training process. This work highlights the promising application of KANs in time series analysis. We hope these results provide insights for future research on KAN for time-series forecasting models to improve the capability to capture complex patterns in time series data.\nFuture work could explore improving the training time for a generalized architecture having wider and deeper KANS beyond the current two-layer model. Additionally, developing a more efficient KAN implementation would facilitate comprehensive hyperparameter tuning on TSKANMixer, potentially unlocking the full potential of KAN-based models. Finally, further exploiting the interpretability and robustness of KAN-based models through symbolic regression could open opportunities to develop more effective and efficient time series models."}]}