{"title": "Do Large Language Models Know How Much They Know?", "authors": ["Gabriele Prato", "Jerry Huang", "Prasannna Parthasarathi", "Shagun Sodhani", "Sarath Chandar"], "abstract": "Large Language Models (LLMs) have emerged as highly capable systems and are increasingly being integrated into various uses. However, the rapid pace of their deployment has outpaced a comprehensive understanding of their internal mechanisms and a delineation of their capabilities and limitations. A desired attribute of an intelligent system is its ability to recognize the scope of its own knowledge. To investigate whether LLMs embody this characteristic, we develop a benchmark designed to challenge these models to enumerate all information they possess on specific topics. This benchmark evaluates whether the models recall excessive, insufficient, or the precise amount of information, thereby indicating their awareness of their own knowledge. Our findings reveal that all tested LLMs, given sufficient scale, demonstrate an understanding of how much they know about specific topics. While different architectures exhibit varying rates of this capability's emergence, the results suggest that awareness of knowledge may be a generalizable attribute of LLMs. Further research is needed to confirm this potential and fully elucidate the underlying mechanisms.", "sections": [{"title": "1 Introduction", "content": "Large Language Models are renowned for their ability to memorize vast amounts of information encountered during training (OpenAI, 2023; Touvron et al., 2023; Gemini Team, 2023). This information, stored in their parameters, can be recalled during inference, serving both for information retrieval and problem-solving (Vinyals and Le, 2015; Radford et al., 2019; Chung et al., 2022a; Geva et al., 2023). While it is well-established that LLMs can act as knowledge bases (Petroni et al., 2019; Heinzerling and Inui, 2021; AlKhamissi et al., 2022), the extent to which they understand their own knowledge is less clear (Liang et al., 2024). For instance, do these models know if or when they know the answer to a question (Kadavath et al., 2022; Yin et al., 2023)? Can they quantify their own expertise on a topic? Are they aware if some of their knowledge contradicts other information they possess? Can they differentiate between explicitly learned information and implicit knowledge?\nThese questions are crucial, as awareness of one's own knowledge and limitations is a vital aspect of any intelligent system. Without it, an AI could be prone to hallucinate (Ye et al., 2023; Xu et al., 2024), lie about its expertise (Azaria and Mitchell, 2023; Pacchiardi et al., 2023), overestimate its responses (Desai and Durrett, 2020; OpenAI, 2023), or contradict itself (Chen et al., 2023), all of which are undesirable traits for AI systems intended to be useful.\nThis study focuses on understanding whether LLMs know the extent of their knowledge on specific topics, such as individuals, locations, events or concepts. To explore this, we task LLMs with enumerating everything they know about a given topic-no more, no less. Should a model consistently recall just the right amount of information, it suggests an understanding of the scope of its own knowledge on that topic. Conversely, if a model does not know how much it knows, it may recall too little or hallucinate additional information.\nOur approach involves fine-tuning LLMs on the diary entries of various fictitious individuals. Each entry is treated as an individual document in our fine-tuning dataset, with each diarist authoring a random number of entries. During inference, we ask the models to recall all diary entries of a specified individual in chronological order. We then evaluate whether the recalled entries match the original entries both in terms of content and quantity. Figure 1 provides an illustrative example.\nWe benchmark the performance of the OPT (Zhang et al., 2022), Pythia (Biderman et al., 2023), and Flan-T5 (Chung et al., 2022b) suites of models. Our key findings are as follows:\n\u2022 All tested LLMs, if scaled sufficiently, demonstrate an understanding of how much they know. This capability appears to emerge at different rates depending on the architecture. For example, an OPT model of a particular size can perform this task effectively if the fine-tuning dataset is sufficiently large, whereas Pythia and Flan-T5 models of the same size require further scaling.\n\u2022 When these conditions are not met (i.e., insufficient scaling), models often recall a random number of diary entries, either recalling too few or hallucinating additional ones.\n\u2022 Interestingly, the number and length of documents do not impact model performance, demonstrating that models are equally effective at memorizing short and long documents, as well as recalling topics associated with a single document or multiple documents.\nFinally, we discuss potential factors responsible for the observed differences in the emergence of this capability. Overall, our work contributes to a deeper understanding of the inner workings of LLMs, shedding light on a not-so-well-understood aspect of these models.\nThe insights gained from this research advance our understanding of LLMs, shedding light on their operational capabilities and contributing to the ongoing exploration of their intricate dynamics."}, {"title": "2 Related Work", "content": "2.1 Knowledge Awareness\nLarge language models are widely recognized for memorizing a substantial amount of information during their training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020; Carlini et al., 2022; AlKhamissi et al., 2022; He et al., 2024). However, it remains unclear to what extent these models understand their own knowledge. Research to date has shown that LLMs can assess, with some degree of accuracy, whether they know the answer to a given question (Kadavath et al., 2022; Zhao et al., 2023; Amayuelas et al., 2023; Yin et al., 2023; Liang et al., 2024).\nWhile these studies primarily evaluate the model's ability to determine if it possesses the knowledge necessary to answer a question, they do not consider the quantity and source of this knowledge. For instance, the question \u201cIs Jupiter a planet?\u201d requires knowledge of a single fact, whereas \"Do you know all papers related to topic X?\u201d necessitates understanding multiple pieces of information, derived from various training samples.\nIn essence, locating a specific piece of information within a model's parameter space is different from retrieving multiple pieces of information and recognizing when the search is complete. Our research focuses on this latter aspect, seeking to determine whether LLMs comprehend the extent of their knowledge on specific topics.\n2.2 Implicit Knowledge Retrieval\nAt the heart of our methodology lies implicit knowledge retrieval. This involves prompting a model with a question, enabling it to retrieve knowledge stored within its parameters, and subsequently generating an answer based on the retrieved information (Vinyals and Le, 2015; Chung et al., 2022a; Geva et al., 2023). Considering the black box nature of deep neural networks (Alain and Bengio, 2016; Sudjianto et al., 2020; Buhrmester et al., 2021; Liang et al., 2021), this setup is frequently employed to deduce the inner workings and capabilities of such models (Porada et al., 2021; Berglund et al., 2023b; Allen-Zhu and Li, 2023a,b; Berglund et al., 2023a; Madsen et al., 2024), offering valuable insights into the knowledge and skills the model has acquired (Hendrycks et al., 2020; Chen et al., 2021; Cobbe et al., 2021; Hendrycks et al., 2021; Rein et al., 2023). Hence we consider it to be a fitting analytical approach for our investigation."}, {"title": "3 Methodology", "content": "The foundation of our analysis hinges on the ability of models to memorize and recall information. To avoid the influence of existing data, which might be part of the pre-training corpus of the language models we are benchmarking, we generate our own. This ensures that the models have never encountered the data during pre-training, thereby preventing any contamination of our results.\nIn essence, our approach involves: (i) generating the training documents, (ii) fine-tuning a language model using its pre-training objective to memorize these documents, and (iii) testing the language model's ability to recall all related documents. We delineate each stage of our framework in the following sections.\n3.1 Data Generation\nGiven N diarists, where N is a hyperparameter, we generate a random number of diary entries for each diarist, following the template:\n{name}'s Diary Entry {i}\n{attribute}:\n{attribute}\nwhere {name} is the diarist (e.g., \u201cTom\u201d) and {i} is the entry number (e.g., \"1\"). The document contains a random number of {attribute}, each selected randomly without replacement from a set (Table 3), along with a randomly chosen value (e.g., \"Time: Morning\"). Additionally, for each individual, we have one question following the format:\nRecall all of {name}'s diary\nentries, in order.\nThe answer to the question is the concatenation of the individual's diary entries in ascending order by entry numbers. Figure 1 illustrates examples of both generated documents and questions.\nTo effectively train the model, we incorporate 90% of the question-answer (Q/A) pairs, along with all diary entries, into the training set. The remaining 10% of the questions are evenly divided into a validation set and a test set. By adding Q/A examples to the training set, the model can learn the evaluation task, similar to the process of instruction-tuning.\nInitially, we trained the model first on the documents and then on the evaluation task. However, this approach led to catastrophic forgetting of the documents and overfitting on the Q/A examples. Therefore, we decided to fine-tune the model on both simultaneously to prevent these issues.\n3.2 Fine-Tuning & Evaluation\nTo benchmark an LLM, we begin by fine-tuning it using its pre-training objective, such as causal language modeling, on our training set. This fine-tuning process mirrors the standard training of an LLM on a text corpus. Depending on the architecture of the LLM, we format the input as follows:\n\u2022 Decoder-Only Models (e.g., OPT): For both diary entries and Q/A pairs, the training objective is causal language modeling. In the case of Q/A pairs, we concatenate the question with the answer into a single text sequence, separated by an end-of-line token ('\\n').\n\u2022 Encoder-Decoder Models (e.g., Flan-T5): When processing a diary entry, the first line (e.g., \"Tom's Diary Entry 1\u201d) is input to the encoder, and the decoder generates the entire document. For Q/A pairs, the question is fed to the encoder, and the decoder predicts the answer.\nThroughout the fine-tuning process, we periodically evaluate the model on the validation set. For decoder-only architectures, the model is prompted with a question with the goal of generating the corresponding answer. For encoder-decoder architectures, the question is given to the encoder and the decoder must produce the answer.\nWe fine-tune up until the validation performance plateaus. We then select the best checkpoint based on peak validation performance, and evaluate the model on our test set using the same procedure as with the validation set. Performance is measured in terms of accuracy, defined as the number of correctly answered questions. An answer is deemed correct if it matches the ground truth exactly, with no errors in the number of documents recalled and the content of each recalled document.\n3.3 Design Motivation\nRequiring the model to consolidate information from multiple training documents allows us to assess whether it understands the extent of its knowledge related to the individual in question. Specifically, during training, the model memorizes the diary entries. Then, in the evaluation phase, it needs to know how many documents to recall, meaning the model must know how many diary entries it knows about the individual. If a model consistently recalls the exact number of documents, it demonstrates an understanding of the scope of its knowledge regarding that individual. Conversely, a model which does not know how many documents it knows, would recall a random number.\nAs for our choice of using synthetic data, it allows us to precisely control its distribution and properties. This extends to the length and content of the documents, as well as the number of diary entries authored by an individual. By using attributes as the body of the documents, we can manage the entropy, ensuring that each sentence contains a fixed amount of information. Consequently, adding an additional sentence consistently increases the document's information by that fixed amount.\nThis approach enables us to examine how document length affects the model in a more controlled manner compared to using real data. While we have arbitrarily chosen individuals as the topic linking multiple documents, this could have been any other concept. We believe this choice does not impact the observed trends in the results.\nOverall, our benchmark is designed to facilitate the study of this problem and its key variables in a controlled environment, emulating the challenge faced by language models of memorizing information during training and understanding the extent of their knowledge concerning specific topics."}, {"title": "4 Experiments", "content": "4.1 Setup\nDataset. To evaluate the impact of the number of training examples on the model performance, we generate six datasets containing 1K to 64K diarists, with each successive dataset doubling in size compared to its predecessor. By incrementally enlarging the dataset size as described, models see a broader array of examples from which they can learn to derive their generative capabilities, while simultaneously being challenged to memorize a larger volume of documents.\nFor each individual, we generate 1 to 8 diary entries, with each entry consisting of 1 to 8 attributes. The training, validation and test sets each contain an equal distribution of individuals who have written one, two, three, etc. diary entries. Similarly, we maintain a uniform distribution for document lengths. Dataset details, such as the number of authors, diary entries, and Q/A pairs, are provided in Appendix A.\nModels. We benchmark the following suit of publicly available models: decoder-only OPT (7M to 2.7B) (Zhang et al., 2022) and Pythia (70M to 2.8B) (Biderman et al., 2023), and encoder-decoder Flan-T5 (80M to 3B) (Chung et al., 2022b). A comparison of these architectures is provided in Appendix B. Training hyper-parameters are provided in Appendix C. Unless specified otherwise, reported metrics are based on the test set.\n4.2 Results\nEffect of Architecture & Scale. We first evaluate the impact of architecture, model size, and dataset size on performance. We fine-tune each model on our datasets and report their performance as solid lines, labeled as 'standard setup' in Figure 2. The horizontal axis represents model size, the vertical axis indicates the percentage of correctly answered questions, and the line color signifies the dataset size. Each line on the plot corresponds to a specific architecture (e.g., OPT), ranging from the smallest to the largest model, trained on a particular dataset size. Due to the significant computational cost associated with these experiments, we do not exhaustively explore all combinations of dataset and model size. Instead, we focus on a representative subset of combinations, sufficient to analyze and discern the key trends effectively.\nFor the OPT suite, we observe a general trend where performance improves as both model size and dataset size increase. Beginning with the smallest variant, which consists of 7M parameters, performance initially improves as the dataset expands, peaking at 4K diarists. However, beyond this threshold, further scaling of the dataset leads to a decline in performance. This pattern suggests that while larger datasets enhance generalization, there comes a point where the model's capacity becomes saturated, causing diminishing returns or even a drop in effectiveness.\nIn contrast, the 125M-parameter OPT model demonstrates a markedly different behavior. This model is sufficiently large that increasing the dataset size up to the maximum tested\u201464K diarists\u2014results in consistent performance improvements. The difference in performance between this model trained on the smallest versus the largest dataset is particularly striking, highlighting the significant impact of dataset scaling when paired with a model of sufficient capacity.\nFurthermore, increasing the model size while keeping the dataset size constant generally leads to performance gains.\nThe Pythia models exhibit a similar trend to the OPT suite, where performance improves as both model size and dataset size increase. However, an interesting distinction emerges when comparing the two architectures: performance gains appear sooner in OPT models than in Pythia. Specifically, the 125M-parameter OPT model significantly outperforms the 160M-parameter Pythia model when trained on our largest datasets. This discrepancy underscores differences in how quickly the studied capability emerges depending on the underlying model architecture.\nFinally, the performance of Flan-T5 models exhibits a distinct pattern compared to the other architectures. On the smallest datasets, increasing model size alone does not lead to any noticeable improvements. Performance gains only begin to emerge at 783M parameters, and even then, only when trained on the two largest datasets.\nDue to computational limitations, we were unable to test the largest Flan-T5 model, with 2.8B parameters, on our largest datasets. However, the overall results suggest that this capability does indeed emerge with sufficient scale-though the rate at which it develops varies depending on the model architecture.\nEffect of Distributed Information. We compare the model performance against a second set of models trained in a simpler setup. Particularly, this second group of models is trained on identical datasets, but with all diary entries authored by the same individual merged into a single training document rather than each entry being its own document. This approach is equivalent to training the models on the answers directly, requiring them to simply memorize and recall single documents. The performance gap between these two setups highlights the added difficulty of dealing with information spread across multiple training documents. This distribution could affect how information is stored in the model's parameters, potentially making it harder for the model to consolidate it when it is dispersed.\nIn Figure 2, the results of training within this more straightforward setup are shown as dashed lines, labeled 'simplified setup'. In all cases, these models exhibit significantly improved performance compared to the same base model trained within the distributed setup. Interestingly, all Flan-T5 models achieve near-perfect accuracy in this simplified setup whereas OPT and Pythia suites do not, despite performing well and improving with scale.\nTo better illustrate the performance gap between both setups, we provide a clear visualization in Figure 3. The vertical axis shows the accuracy gap between the 'simplified' and 'standard' setup, for models of the same size, trained on datasets containing the same number of individuals. Results are grouped by model size, with colors denoting dataset size.\nFor the OPT models, the gap narrows as the dataset size increases, with the exception of the smallest model. In the case of Pythia, the gap only seems to narrow for larger models trained on sufficiently large datasets. Lastly, for Flan-T5, the performance gap barely shrinks as both dataset and model size scale, with the exception of the 780M parameter model trained on the largest datasets.\nIt remains unclear why Flan-T5 models perform so well in the simpler setup but so poorly in the standard setup. Given that the model has near perfect accuracy in the prior, its poor performance in the latter cannot be attributed to an issue in the methodology, as the process is the same in both cases. The only difference is that, in the latter case, the model must recall information from multiple documents rather than a single one. Therefore, the model specifically has an issue with this aspect.\nFor all models, it is uncertain whether their performance in both setups will continue to improve with scale and if the gap will eventually disappear.\nEffect of Number of Documents. Next, we explore how the number of documents to be consolidated and recalled impacts model performance. In Figure 4, we report accuracy grouped by the number of documents in the target answer (horizontal axis). Line color indicates model size. To maintain clarity, we display only the performance of models trained on the 8K diarist dataset, as the observed trends are consistent across other datasets. Notably, there are no results on the simpler setup in this and further analyses.\nSurprisingly, models do not demonstrate a decline in performance when more diary entries need to be recalled. Given the increased content to be generated, one might expect a higher propensity for errors in the model answers. However, this observation could be attributed to the model's capacity being sufficient, and performance deterioration might only appear when recalling a much greater number of documents.\nTo gain deeper insights into model behavior, we analyze the number of documents recalled by the models in comparison with the target number of documents (Figure 5 & 6).\nFor both OPT and Pythia models trained on a dataset of 8K diarists, smaller models appear to recall a random number of documents. However, as model size increases, the ability to accurately determine the appropriate number of documents to recall emerges.\nIn contrast, Flan-T5 models trained on the same 8K-diarist dataset consistently retrieve a seemingly random number of documents, regardless of model scale. Interestingly, when scaling up to a dataset of 32K diarists, Flan-T5 exhibits a pattern similar to that of OPT and Pythia\u2014where the capability to recognize how many documents should be recalled emerges as model size increases.\nEffect of Document Length. Previously, our method for measuring accuracy involved counting the number of model answers that matched the target answer exactly. We now shift our focus to evaluating the accuracy of individual documents within a model's answer, which we refer to as document accuracy.\nIn this analysis, we only consider the documents recalled by the model that are also present in the target answer, regardless of whether these documents are correct. Our objective is to examine how the length of the target documents influences the model's ability to recall them accurately. Hence, we restrict our analysis to this specific subset of documents, as we need a target for their length.\nFor these selected documents, we count those that are free of errors and represent this rate on the vertical axis of Figure 7. The performance is categorized by the target length on the vertical axis, and the line color indicates the size of the model. Once again, to maintain clarity, we display only the performance of models trained on the 8K diarist dataset.\nAcross all models, performance appears unaffected by document length, despite the expectation that longer documents might introduce more errors. One possibility is that the models are sufficiently large to handle even the longest tested documents without performance degradation. However, we hypothesize that much longer documents may eventually impair performance, which would require further testing.\nTo further understand model behavior, we analyzed the number of recalled sentences, in comparison with the target document length. The histograms in Figure 8 illustrate these distributions for each model, with color indicating the model size. Similar to our analysis of the number of recalled documents, we observe that model performance improves with scale: smaller models appear to recall a random number of sentences, whereas larger models consistently recall the exact number. Interestingly, however, the smallest Flan-T5 models recall the correct number of sentences, suggesting that they are better at memorizing documents than OPT and Pythia models of the same size.\nInvestigating Performance Discrepancies. Our results indicate that the ability to consolidate and accurately recall the correct number of documents varies depending on the model suite, but the underlying reasons for this discrepancy remain unclear. At a high level, these differences in performance could be due to several factors: architectural variations, the effectiveness of pre-trained weights for fine-tuning on this task, the fine-tuning hyperparameters, or a combination of these elements.\nTo investigate this further, we fine-tuned an OPT-125M, a Pythia-70M, and a Flan-T5 Small model, all with randomly initialized weights, using our dataset with 32K diarists. We then compare their performance against the pre-trained models that were fine-tuned on the dataset of the same size. Our findings reveal that the Pythia model initialized with random weights significantly outperforms the pre-trained weights (Table 1). This suggests that architectural differences are not responsible for the poor performance of this model. Instead, the issue lies in the capability of the pre-trained weights to be effectively fine-tuned for this specific task.\nRegarding Flan-T5, fine-tuning with randomly initialized weights does not appear to enhance performance when compared to fine-tuning the pre-trained model. This observation suggests that the model's architecture is responsible for the observed differences in performance.\nAlthough fine-tuning hyperparameters could also be a factor, we conducted a thorough search. Additionally, models in the simpler setup performed well and were trained with identical hyperparameters. Conversely, in the standard setup, while models were able to memorize the training samples and Q/A examples, the solutions learned by the pre-trained Pythia-70M and Flan-T5 Small does not generalize well to the validation and test Q/A, unlike the OPT model.\n4.3 Behavior Analysis\nOur analyses thus far have focused on prompting the model with a question and allowing it to generate an answer. Now, we examine how the models respond when prompted with a question followed by part of the answer. We experiment with the following combinations:\nA. The second document alone, skipping the first.\nB. The first document followed by the third, intentionally omitting the second.\nC. The first half of the first document only.\nD. The first half of the first document followed by the second document.\nE. The last document followed by the first.\nThese scenarios were tested using OPT and Pythia models trained on our largest dataset. We find that in all cases except the last, the models continued the answer seamlessly. Specifically:\nA. They follow the second document with the third, then the fourth, etc.\nB. They follow the third document with the fourth, then the fifth, and so on.\nC. They follow the first half of the first document with the second half, then proceed to the second document, the third, and so forth.\nD. They follow the second document with the third, then the fourth, and so on.\nE. They follow the first document with the second, third, etc., but eventually skip some documents, including the last one.\nThese results indicate that the position of tokens in the sequence is not a significant factor. Instead, the models demonstrate a robust ability to continue the sequence as long as the tokens are in a logical order.\n4.4 Comprehensive Analysis\nReflecting on our experimental observations, we can gain insights into the capabilities and failures of these models. We've observed that, given sufficient scale, the documents recalled by the models are typically of the correct length (Figure 8) and error-free (Figure 7). Additionally, models trained under the simplified setup successfully recall information from a single training document (Figure 2). Therefore, the issue appears not to lie in the content of the recalled documents but rather in the quantity of documents being recalled. Indeed, with improper scale, models seem incapable of recalling the correct number of documents, instead recalling a random number of documents (Figure 5).\nInterestingly, the smallest Pythia model performs better if fine-tuned starting from random weights rather than the pre-trained weights (Table 1), suggesting that the poor performance of the pre-trained weights cannot be completely attributed to an architectural reason. Instead, the issue partly appears to be with the pre-training weights failing to learn a solution that generalizes to the problem of recalling the correct number of documents, rather than merely memorizing the training samples. Why this discrepancy occurs, particularly in contrast to the larger pre-trained Pythia models remains unclear and warrants further research. Different hyperparameters could potentially enable the smaller models to generalize well to our problem, but it is uncertain if this can be achieved without severely degrading the language modeling capabilities of the pre-trained model.\nRegarding Flan-T5, given that the smallest model fine-tuned from scratch performs as poorly as the one fine-tuned from pre-trained weights, the root cause of the poor performance could be either architectural or due to improper hyperparameters. Additionally, the size of the model appears to influence its performance. Since Flan-T5 follows an encoder-decoder architecture, unlike the decoder-only structures of models such as OPT and Pythia, its parameters are divided roughly equally between the encoder and decoder. Consequently, the second largest Flan-T5 model's decoder is comparable in size to that of the third smallest Pythia model, which coincides with the point where performance begins to improve for Pythia (as seen in Figure 2). Models within the Pythia suite smaller than this threshold do not show significant performance gains. However, the smallest Pythia model, when trained from scratch, outperforms Flan-T5 under similar conditions. This highlights that architectural factors can hinder the emergence of capabilities for same sized models. As for scale, our hypothesis is that the smaller models lack the capacity to develop the necessary circuitry to perform this task, but further research will be necessary to pinpoint the exact cause and clarify the challenges faced by these smaller models."}, {"title": "5 Discussion", "content": "In addition to the questions raised in the previous section, the following additional questions should be considered.\n5.1 Language Modeling\nOne aspect not addressed in this study is whether models can perform the given task while retaining their language modeling capabilities. Due to the size of the models examined, repetitive fine-tuning on the training documents is necessary for them to memorize the data, which leads to overfitting on the task. Ideally, experiments would need to be conducted on much larger models, incorporating the training documents into the pre-training corpus, followed by standard instruction tuning. One of the tasks in this tuning would involve recalling all documents related to a given topic. This approach would help determine if a model can accomplish this in a manner that is useful for solving problems. Unfortunately, we currently lack the computational resources to conduct such experiments, and hence we leave this for future work.\n5.2 Information Distribution\nWe observed a notable performance gap between the standard and simplified setups, supporting the findings by Prato et al. (2023). Their research indicates that LLMs more easily recall multiple pieces of information when this information is contained in a single training sample rather than dispersed across multiple samples. This raises questions about how the distribution of topic-related information across multiple training documents affects an LLM's ability to gauge its knowledge. Particularly, the impact of this distribution on the internal mechanisms of the LLM is not well understood.\nNumerous studies have shown that language models can memorize entire passages and documents within their weights, enabling them to recall this information during inference (Carlini et al., 2020, 2022; Tirumala et al., 2022; Biderman et al., 2023; de Wynter et al., 2023; Chen et al., 2024). Consequently, the strong performance of models in the simpler setup, where they only need to recall information from a single document per topic, is not surprising.\nHowever, it remains unclear why recalling information from multiple documents presents a greater challenge. Specifically, how is this information encoded within the model parameters (Wallat et al., 2020; Dai et al., 2022; Meng et al., 2022) and how does dispersed information affect the recalling process? Understanding these mechanisms is crucial for improving the performance of language models, as many real-world problems necessitate recalling information from multiple training documents.\n5.3 Knowledge Awareness & Understanding\nWhile we have demonstrated that some LLMs possess an awareness of the extent of their knowledge concerning the topics in our benchmark, this does not necessarily mean that these models can gauge their knowledge across any topic.\nDetermining whether LLMs can accurately assess the scope of their understanding of topics from their pre-training corpus requires further investigation. Topics in practice could cover a wide array of subjects, including individuals, locations, events, and concepts. However, we believe that the specific type of topic is likely not an influential factor.\nThe critical element, in our view, is the breadth of these topics, which relates to the amount of information relevant for each. Our findings did not show a decline in model performance when recalling up to eight documents. Nevertheless, this observation might change if the number of documents were significantly increased. Further research is necessary to explore the limits and capabilities of models in handling broader topics.\nA more profound question is the extent to which LLMs understand the scope of their entire knowledge base, or at least subsets of it. Given the vast amount of information LLMs learn during training, comprehending the scope of this knowledge or its subsets seems incredibly challenging. Yet, it would be beneficial for a model to understand the extent of its own expertise.\nFinally, it is important to note that understanding the scope of one's knowledge concerning a topic does not imply an understanding of that topic itself. Whether LLMs truly comprehend the knowledge they have memorized is a different research question from ours and is an active area of investigation (Bender et al., 2021; Li et al., 2022; Gurnee and Tegmark, 2023)."}, {"title": "6 Conclusion", "content": "This study focused on determining whether LLMs possess an understanding of the span of their own knowledge on specific topics. Notably, we observed that all models, if scaled sufficiently, know how many documents are authored by the same person. Consequently, these LLMs know how much they know about these individuals; otherwise, they would sporadically recall too few or too many documents.\nMore specifically, we find that this capability emerges based on the model's architecture, its size, the dataset's size used for training, and the effectiveness of the pre-trained weights in learning a solution that generalizes, rather than simply memorizing the training samples.\nTo the best of our knowledge, this is the first paper to explore this capability in LLMs, demonstrating that certain models can assess the extent of their knowledge on specific topics. Further research is required to determine whether this phenomenon is common across LLMs.\nOverall, our research contributes to a deeper understanding of the capabilities and inner workings of these models. Grasping how aware LLMs are of their own knowledge and identifying any limitations in this regard is crucial, as this feature enhances the usefulness and trustworthiness of intelligent systems. Additional research is necessary to continue exploring this aspect."}, {"title": "7 Limitations", "content": "The potential insights from testing larger open-source models could be valuable to the community. However, computational limitations prevent us from conducting these analyses. We hope to undertake such experiments in the future."}, {"title": "8 Ethical Considerations", "content": "This research utilizes large language models trained on extensive textual datasets. While such models have demonstrated exceptional ability in generation, it is critical to highlight the ethical considerations that the data used for training these models inherently contains human biases. These, in turn, can manifest in the models' outputs. As such, it is essential when deploying such models, to critically evaluate their outputs, keeping in mind the likelihood of underlying bias."}]}