{"title": "Towards Kinetic Manipulation of the Latent Space", "authors": ["Diego Porres"], "abstract": "The latent space of many generative models are rich in unexplored valleys and\nmountains. The majority of tools used for exploring them are so far limited to\nGraphical User Interfaces (GUIs). While specialized hardware can be used for\nthis task, we show that a simple feature extraction of pre-trained Convolutional\nNeural Networks (CNNs) from a live RGB camera feed does a very good job at\nmanipulating the latent space with simple changes in the scene, with vast room for\nimprovement. We name this new paradigm Visual-reactive Interpolation, and the\nfull code can be found at https://github.com/PDillis/stylegan3-fun.", "sections": [{"title": "Introduction", "content": "Interaction with generative models such as Generative Adversarial Networks (GANs) [GPAM+14]\nand diffusion-based text-to-image models (T2I) [SWMG15] has been relinquished to the land of\nsoftware, specifically Graphical User Interfaces (GUIs). The main advantage of using these is the\nspeed at which they can be modified and shared between users, in addition to simplifying interaction\nwith the model, including it's high-dimensional latent space. However, we believe this also introduces\na rift between the generative models and their users, if not also with their creators, as we forego the\nmost basic interface: the human body.\nIndeed, [Fre08] notes, among other things: firstly, human beings possess a 'centric' quality (they\ninteract with their environment from the vantage point of a centre); secondly, we shouldn't separate\nhuman bodies from the minds, but as acting mind-body unities; and thirdly, the body is instrumental\nin communication both within the self as with the environment (be it motor or hormonal activity).\nOur goal is to create a system by which we can have a live performer be the one controlling the latent\nspace of a generative model with their body or facial movements and scenery changes via moving\nobjects in the scene.\nWe argue then, that there are two centers on an image, whether real or synthesized: the subject or\nfocus of the image, and the camera (real or virtual). As such, we should then aim at letting the camera\ncontroller to be another actor in the image synthesizing pipeline by changing the focused area of the\nperformer, lightning, or even camera lenses. For our purposes, we wish the two actors to interact\nthrough a manipulation of the latent space, made visible via the synthesized images that a pre-trained\nGenerator of a GAN will provide. This will allow the two to forge a common story. This work aims\nto document the creation of this tool."}, {"title": "Latent Space Interaction", "content": "Since our aim is to control the image synthesis generation during a live performance show, we\nwill quickly explore other already existing alternatives. Concretely, we will focus our attention to\nmanipulate the latent space of StyleGAN1/2/2-ADA [KLA18, KLA+19, KAH+20], some of the\nmost popular and widely trained networks, but we will look at inspiration for what has been done\nin others as well. Our focus is on GANs as as their sampling cost is low compared to other models,\nwhich translates to a smoother live performance, but hopefully this work (or an iteration of it) can be\nalso applied to other generative model such as T2I latent diffusion models [RBL+22].\nThere have been many recent efforts towards enhancing the interaction with generative models, both\noffline as well as during live shows. Most notably, [Bro20] uses and exploits the architecture of\nStyleGAN1/2 (noise injection, network bending [BLG20], style mixing, among others), as well as\nthe features of music itself for creating enthralling audio-reactive latent interpolations. This allows\nfor expressive coverage of the latent space and sets the stage for further expanding the our pipeline's\ncapacities.\n[Smi20] questions the interactive capabilities of a mouse and keyboard when trying to explore\nour interactions with trained GANs. A such, they program a MIDI controller to interact with\nStyleGAN1/2/2-ADA, selecting different controllers to change local or global variables, such as the\ntruncation factor & or for quick sampling of the latent space. While this requires adding hardware\nand its respective programming, it highlights the general need for better tools and methods of the to\nexplore the latent space of generative models.\nOn the other hand, [Bur21] seeks to project a live video feed into the disentangled latent space W of\nStyleGAN. A camera feed is iteratively projected to W, showcasing the image synthesis capabilities\nof the network. However, we must caution that this process has a model selection bias, as FFHQ's\nl atent space is perhaps the only model where we can effectively project any image [AQW19].\nIndependently of live shows, tools such as GANSpace [HHLP20] allow for automatically creating\ninterpretable controls in the latent space of StyleGAN or BigGAN [BDS18] by identifying important\nl atent directions using PCA. This method is limited to static sliders that are not always the directions\nthe user wishes to move towards, but we note that these could be mapped to different actions in the\nscene.\nWhile previous work such as Xoromancy [Cra19] has explored hand-based latent space manipulation,\nit relies on specialized hardware (Leap Motion) and fixed camera placement, limiting its accessi-\nbility and flexibility. Our approach significantly advances this concept by eliminating the need for\nspecialized equipment and broadening the scope of interaction.\nBy leveraging widely available computer vision models and affordable cameras, we eliminate the\nneed for specialized hardware, democratizing access to latent space manipulation. This allows users\nto implement this pipeline with their own trained models using readily available equipment. Our\nsystem expands beyond hand-based interaction, enabling users to craft images using their entire\nbodies, faces, or even objects in the scene."}, {"title": "Visual-reactive Interpolation", "content": "Due to the fact that StyleGAN lacks an encoder back into its latent space Z, unlike other architectures\nsuch as BIGAN [DKD16] or BigBiGAN [DS19], we must look at alternatives. Although using the\nl ast Fully Connected layer of the Discriminator has been proposed for this purpose as it has the\ndesired dimensionality [Por21], our tests have shown that the resulting encoding fails to cover the\nmajority of the latent space, resulting in nigh-static images.\nTraining one for each available pre-trained GAN is out of the question, so we opt to use pre-trained\nfeature extractors F. As such, we will use the intermediate representations of these, effectively acting\nas encoders into the latent space Z. We discard using SOTA models such as CLIP [RKH+21], as\nwe wish to minimize the footprint of this part of the model as much as possible. Likewise, Vision\nTransformers (ViT) [DBK+20] have shown great promise in image classification tasks, but our\nexperiments show that almost no benefit is seen when used in our pipeline. For now, we gear towards\nVGG16 [SZ15], as its intermediate representations have some desired characteristics, but note that\nfuture work can make use of more recent architectures that are lighter to run in the edge.\nWe start our testing by exploiting these feature extractors as well as the style-mixing characteristic of\nStyleGAN. Based on user feedback, we then move to more fine-grained control of specific parts of\nthe Generator. Lastly, we list some possible venues to take, but note that the possibilities are endless,\nespecially if we include multimodal inputs to our setting."}, {"title": "Test 1: Visual Encoding and Style Mixing", "content": "We will follow the notation of [GEB15] for the internal representations. A scene will be captured\nwith a camera C producing a frame x, which is then fed to a frozen pre-trained model F. When\nwe pass each frame through our network F, it will be encoded at each layer l as $F^l$, which has $N^l$\nfeature maps of size $M^l$. The size refers to the product of the height $h^l$ and width $w^l$ of the feature\nm aps, so $M^l = h^l \\cdot w^l$, with $h^l, w^l \\in Z^+$.\nThe dimensions of x must also be carefully selected, as this will also affect the real-time inference\nthat can be achieved by the available compute. Note that the camera C is not limited to being an\nRGB camera. Indeed, this technique works with any other type of sensor, so long as a pre-trained\nnetwork F is available. For more details, see Appendix A\nThe full pipeline for this process is shown in Figure 2 and a demo is shown in Figure 1. We then can\nuse a simple camera, position it in any position we want, and manipulate the scene it observes to\ngenerate different images.\nOur tests have shown that a monocular camera is enough to effectively manipulate the latent space of\na trained StyleGAN2. While we could make use of more specialized hardware, our aim is still to\nmake this pipeline as accessible as possible, with the only limitation being the available compute for\nreal-time rendering the images. The following steps can be better appreciated in Figure 2.\nInitial user testing at the ExperimentAI 2023 and the Festa de la Ci\u00e8ncia 2023 revealed that while\nusers were intrigued by this new interaction method, they quickly became disengaged due to the\nl ack of fine-grained control. Users found that changing objects in the scene sometimes resulted in\ncamera autofocusing, reverting the generated image to its original state. This feedback prompted us\nto explore more precise manipulation techniques in our subsequent tests.\nA live demo can be viewed in Figure 1. The input image has been encoded to follow a strict 4: 3\naspect ratio, but this can be modified."}, {"title": "Test 2: Manipulation of Learned Constants", "content": "Based on user feedback from Test 1 (Section 3.1) indicating a need for more fine-grained manipulation,\nwe turned our attention to manipulating specific parts of the Generator, particularly some of the\nl earned constants. Throughout training a StyleGAN2 Generator, it learns a constant parameter\n(G.synthesis.b4.const) that will aid it in positioning certain aspects of the generated image\n(eyes, nose, ears for faces, for example). If we corrupt this, then the network will lose track of where\nto place the eyes, for example, in turn generating multiple pairs of them. This is essentially what\nFlesh Digressions does, which we adapt to our setting.\nA simple way to corrupt this constant is by extracting keypoints from one or more human bodies\nusing MediaPipe or MMpose. Then we calculate the distance to the center of the image, this distance\nsetting the amount of corruption we introduce to the constant. We show a demo in Figure 3.\nOn the other hand, StyleGAN3 [KAL+21] learns a fixed affine transform to apply to the latent space\nin order to correctly translate and rotate the generated objects (depending on the model). We can then\ncalculate the angle between the vertical axis and the middle finger to know how much to rotate the\nimage and how far from the center to scale the image. We showcase a demo in Figure 4."}, {"title": "Future Work", "content": "While the code is readily available at https://github.com/PDillis/stylegan3-fun, we note\nthat it will be changing constantly as we introduce more options to the user, as well as perform more\ntests with it. The main bulk of work will be dedicated to port all of the previous tests to the GUI\nprovided in the StyleGAN3 repository.\nBeyond run-time optimization, the following ideas are planned to be explored to some extent in the\ncoming months, or at least variations of them. We note, however, that the proposed parameters to\nmanipulate are interchangeable, and even more can be manipulated from each model:\n\u2022 Use different feature representations for different parts of the image (e.g., use Equation 2),\nor have each feature control a different part of the disentangled latent vector w.\n\u2022 Use smaller footprint networks such as MobileNetV3 [HSC+19] or EfficientNet-B0 [TL19].\n\u2022 Conversely, use self-supervised visual features such as DINOv2 [ODM+23].\n\u2022 Use monocular depth estimation or optical flow estimation for manipulating the truncation\ntrick parameter & via e.g. the normalized average depth of the scene using pre-trained\nmodels such as Depth-Anything-V2-Small [YKH+24].\n\u2022 Use semantic segmentation or classification models for exploiting class-conditional Style-\nGAN models, whether for manipulating the latent vectors or the class vectors themselves.\n\u2022 Using PCA to extract notable directions in the latent space, as done in GANSpace [HHLP20],\nand move towards/away from them using specific facial gestures.\n\u2022 Add audiorreactive capabilities such as [Bro20], effectively generating audiovisual-reactive\ninterpolations, allowing for a live band to join the performance.\n\u2022 Add network bending [BLG20] capabilities to the extracted features, or for specific regions\nin the image.\n\u2022 Extend this work to T2I Diffusion Models [SWMG15, RBL+22], with a focus on Consis-\ntency Models [SDCS23, LTH+23] for real-time manipulation."}, {"title": "Feature extraction", "content": "After capturing the internal representations, we wish to convert these into\nuseful objects that the Generator may then use: latent vectors. We are then interested in a family of\nfunctions G that will transform our intermediate representation $F^l$ into a vector $w_{fake}$, i.e., turn the\nfeatures into fake latent vectors to feed our Generator G. We carefully select this family of functions\nsuch that,\n$G\\ni : F^l -> Z ~ N(0,I)$\nAfterwards, the mapping function f of StyleGAN (G.mapping) will then be in charge of mapping\nthese fake latents into the disentangled latent space W. This doesn't mean that we must obtain\nimmediately the fake latent from the output of the function g. We could make a composition of this\nencoding using different parts of an image, or even using different layers for different parts of the\ninput image x.\nFor example, at its most basic, g could simply be the channel-wise average of the feature representation\n$F^l$ of VGG16. More generally, we can also do a weighted average of different layers l (as is done in\nthe content loss in [GEB15]), e.g.:\n$g(F^l) = \\alpha_l \\frac{1}{\\sqrt{M^l}} \\sum_{i,j} F^l_{i,j}$\nwhere $F^l$ denotes the i-th filter of layer l at position j, and $\\alpha_l$ is the weight given to the representation\nat layer 1. Our experiments so far have shown that using a single layer is sufficient, but leave the\npossibility to use more to each individual case.\nWe note this could limit the expressiveness of our generator G, so we could still add the condition\nin Equation 1, that is, to normalize the output of g. However, we have found that Equation 2 is\nsufficient, mostly because of the selection of VGG16 and its separation of features in the last layers is\nchannel-wise, which matches the dimensionality of the default latent space Z of StyleGAN.\nThe dimensionality of StyleGAN's latent space has been $|Z| = 512$, which we\nwill use as default in this work. Then, for simplicity, we are interested in the layers that have\n$N^l = |Z| = 512$. Table 1 shows the convolutional layers in VGG16 that share this property (and we\nrecommend using), but note that our code is general enough that any layer (convolutional or not) in\nVGG16 can be used to obtain $w_{fake}$. The user should experiment and select the best option for their\nsetup.\nUsing a single $w_{fake}$ might suffice in some experiments to get the desired outcome,\nhowever this is not always a guarantee. To make better use of the expressivity of the trained\nStyleGAN, we can partition $F^l$ into different sections before passing it into g, each producing a\ndifferent disentangled latent vector w (e.g., $w_{coarse}$, $w_{middle}$, and $w_{fine}$). Setting a static latent vector\n$w_{static}$ with a random seed, then the style-mixing properties of StyleGAN will allow us to control\ndifferent aspects of the synthesized image. For example, one region of the input image will control\nthe fine details (colors) of the synthesized image, while in others control larger structures. A demo of\nperforming sytle mixing with the coarse and middle noise scales is shown in Figure 1."}]}