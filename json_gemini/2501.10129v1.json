{"title": "Spatio-temporal Graph Learning on Adaptive Mined Key Frames for High-performance Multi-Object Tracking", "authors": ["Futian Wang", "Fengxiang Liu", "Xiao Wang"], "abstract": "In the realm of multi-object tracking, the challenge of accurately capturing the spatial and temporal relationships between objects in video sequences remains a significant hurdle. This is further complicated by frequent occurrences of mutual occlusions among objects, which can lead to tracking errors and reduced performance in existing methods. Motivated by these challenges, we propose a novel adaptive key frame mining strategy that addresses the limitations of current tracking approaches. Specifically, we introduce a Key Frame Extraction (KFE) module that leverages reinforcement learning to adaptively segment videos, thereby guiding the tracker to exploit the intrinsic logic of the video content. This approach allows us to capture structured spatial relationships between different objects as well as the temporal relationships of objects across frames. To tackle the issue of object occlusions, we have developed an Intra-Frame Feature Fusion (IFF) module. Unlike traditional graph-based methods that primarily focus on inter-frame feature fusion, our IFF module uses a Graph Convolutional Network (GCN) to facilitate information exchange between the target and surrounding objects within a frame. This innovation significantly enhances target distinguishability and mitigates tracking loss and appearance similarity due to occlusions. By combining the strengths of both long and short trajectories and considering the spatial relationships between objects, our proposed tracker achieves impressive results on the MOT17 dataset, i.e., 68.6 HOTA, 81.0 IDF1, 66.6 AssA, and 893 IDS, proving its effectiveness and accuracy.", "sections": [{"title": "1 Introduction", "content": "Multi-Object Tracking (MOT) aims at tracking multiple objects simultaneously in a video, maintaining the identity of the objects, and generating their motion trajectories. It has wide applications in different fields such as video surveillance, autonomous driving, and video analytics. Despite many approaches proposed for multi-object tracking, fragmented trajectories or ID Switching (IDS) problems caused by frequent occlusions in crowded scenes and similar appearances are still significant challenges.\nTo address the ID Switching (IDS) issue, most state-of-the-art trackers use a combination of short and long trajectories for tracking [6] [2]. Due to the variability of scenarios, different specific schemes are required, for example, motion-based local trackers and appearance-based trackers [10], [11], [14], [15]. When there is heavy occlusion and uniformity of clothing, these methods often become highly specific to particular scenarios, making them not easily scalable to broader applications. Gao et al. [6] propose a hierarchical approach to processing video, with lower levels focusing on short-term associations and higher levels focusing on increasingly long-term scenes. Cetintas et al. [2] uses the same learnable model for all time scales, thus improving scalability. Many state-of-the-art MOT trackers use graph neural networks [7] to handle similar appearance problems [3] [12]. However, there are still some issues that exist in current works, e.g., many of these methods are highly specific. Cetintas et al. [2] use the same learnable model for all time scales, obviously, the scalability and flexibility can be improved. In addition, we are also inspired by the scenarios as shown in Fig. 2. More in detail, scenario 1 demonstrates that the time interval between the appearance of a tagged object and its occlusion to reappearance is not consistent. There is a tendency in scenario 2 to follow the wrong phenomenon due to similar clothing and similar location. However, few existing MOT systems take into account the spatial relationships between different objects within a single frame.\nTo address the issues mentioned above, in this paper, we propose a novel multi-object tracking by spatio-temporal graph learning on adaptive mined key frames. As shown in Fig. 3, given the input video frames, we first adopt a KFE (Key Frame Extraction) module to divide them into frame segments adaptively. The key frame extraction is conceptualized as a decision-making process through the application of the Q-learning algorithm, which capitalizes on the high performance capabilities of short trajectories. Furthermore, we harness spatio-temporal graph learning in conjunction with the Intra-frame Feature Fusion (IFF) module"}, {"title": "2 Methodology", "content": "2.1\nReinforcement Learning based Key Frame Mining\nThe KFE module is based on Q-learning, and we need to design the corresponding action selection, reward mechanism, active exploration strength, and table update strategy. The optimal segmentation strategy FSbest and the corresponding optimal reward score Kbest are iterated during the learning process. The action selection is divided into the first frame of the video segment \u0393a, and the last frame action selection \u0413\u044c, both of which have the same selection strategy, in order to avoid redundancy, we only illustrate the action selection strategy for the first frame. Fi+1 is the action selection used to record the choice that will be made next, QT is the Q-value used to record the total return expected to be obtained by taking a specific action in a certain state, F denotes the state, QT[F, \u0393] represents the expected return of the next choice Pi+1 in state F\u00e5, and Frange represents the ranges of values of Ia. we choose the action with the highest reported value in the Q-value as the next action, in addition to randomly choosing the next action with a certain exploration rate \u0454.\n\n\u0413\u0456+1 = \\begin{cases}\nmax(QT[F, \u0393]), \u03b7 < \u03b5 \\\\\nrandom(Frange), otherwise\n\\end{cases}\n\nwhere \u03b7 \u2208 (0,1), j \u2208 (0, N) and e = 0.1. We need to develop a reasonable reward mechanism to determine the direction of model optimization, we use the feature difference between the first and last frames between the video segments as the reward value, Ki+1 is the reward for the i+1th segmentation result of the current round.\n\nKi+1 = ((1 \u2013 \u03b1(fFi, fFi+1) + (1 \u2212 \u03b2(fFi, \u00a3Fi+1)) \u00d7 \u03b4 + \u00a7\n\nwhere fri represents the feature of the first frame of the ith video segment, fri represents the feature of the last frame of the ith segment, fp\u00e5 represents the feature of the first frame of the jth video segment, and fri represents the feature of the last frame of the jth segment. Additionally, \u03b4, \u00a7 are constants, and denotes the cosine similarity. Regarding the update of the Q-value, we follow the approach of classical algorithms, where the Q-value is dependent on Fi,\n\nFi+1, the learning rate q = 0.1, the reward I, and the discount factor a = 0.99.\n\n+1best = max(QT(Fi+1, \u0393\u0456)\n\nQT[F\u00b2, \u0393] = \u03bb \u00d7 q+ QT[F\u00b2, \u0393]\n\nwhere +1 represents the optimal action selection for the next step, \u03bb = Ki + \u03b1 \u00d7 QT[Fi+1, \u0393+1] \u2013 QT[F\u00b2, \u0393].\n\nWhen the video is fully segmented after the mth iteration, the final reward score for this round Ksum is denoted as\n\nKsum = \\frac{\u03a3i=1 \u043a\u0430}{len(FSm)}\n\nwhere is the reward for the ith segmentation result of the first frame of the current round and FSm is the segmentation strategy in the mth iteration. We evaluate whether the current segmentation result is attributed to the advanced optimal reward Kbest. If it is superior, we proceed to update the parameters FSbest and Kbest accordingly\n\n\\begin{cases}\nKbest = max(Kbest, Ksum)\\\\\nFSbest = max(FSbest, FSm)\n\\end{cases}\n\nThrough the aforementioned steps, we are able to achieve efficient segmentation results that fully exploit the inherent logic of the video, effectively combining the advantages of both short and long trajectories. Compared to SUSHI, our approach achieves up to +2.5 AssA and up to +1.7 IDF1 improvements, with a decrease in IDS. This indicates that the KFE module can effectively mitigate the issue of identity loss when objects reappear after occlusion. Thereby validating the effectiveness of our method. The complete inference procedure is outlined in Algorithm 1, where xa represents the state selection of the first frame in the jth video, x\u1ec9 represents the state selection of the last frame in the jth video.\n2.2 Spatial-temporal Relation Mining\nWhen occlusion of an object occurs, we can combine the surrounding objects to perform object recognition where the objects are too similar to each other and their positions are close. Inspired by the above ideas, we add IFF to solve the problem of occlusion and similar appearance. In performing the fusion of different levels, we first construct a graph G = (V, E) for the level, where the object is represented by node vi \u2208 V and the edge ei represent hypotheses for judging the relationship between nodes. We perform in-frame feature fusion before hierarchy merging. The initial scheme is to complement the features of the object with a certain percentage after averaging the surrounding objects.\n\nfv\u2081 = \u03b1 \u00d7 fvz + \u03b2 \u00d7 avg(fvm)"}, {"title": "2.3 Loss Function", "content": "Our proposed KFE method stands independent of the SUSHI Block and IFF frameworks. To determine the number of learning epochs, denoted as M = (LN-u) \u00d7 (LN-n) \u00d7 100, where u represents the shortest length of the video segment, n represents the maximum length of the video segment, and LN represents the total length of the video. We adopt the unfreezing strategy from the SUSHI Block, where subsequent levels are unfrozen after 500 iterations. We apply the focal loss on the generated edge classification scores and sum these losses across all levels to obtain the final loss [8]."}, {"title": "3 Experiments", "content": "3.1\nDatasets and Evaluation Metrics\nWe conducted our experiments on the public dataset MOT17. We used MOT17-Private for experimental effect validation. We follow the HOTA protocol [9] for quantitative evaluation, where HOTA focuses on overall tracking quality, AssA is used to measure association accuracy, and IDS focuses on measuring the number of identity switches during tracking. In addition, we also adopt the metrics MOTA and IDF1, which reflect the overall performance of detection and tracking as well as association accuracy, respectively, providing us with a multidimensional evaluation reference.\n3.2 Ablation Study\nTo demonstrate the effectiveness of the proposed modules, in Table 1 we conducted experiments on MOT17, where we describe the added features as: KFE (Key Frame Extraction), and IFF (Intra-frame Feature Fusion). We found that KFE and IFE improved the performance on the MOT17 set, where the IFF module improved by 1.1 HOTA and KFN improved by 1.2 HOTA. Adding both modules together improves 1.6 HOTA, 2.3 IDF1, and 3.3 AssA.\nIn Tab. 1 we started to use dynamic programming (DP) for segmentation, which has poor learnability and limited scalability, so we used the reinforcement learning Q-learning method, which has good applicability and scalability, and can be obtained from the experiment, Q-learning is more effective and improves the performance of the algorithm by 1.2 HOTA, 1.7 IDF1, and IDS decreased, so we finally chose the Q-learning algorithm. In the IFF module, there is a fusion between the Intra-frame features, we used a fixed ratio, and after the experiment in Tab. 2, we finally set a=0.4.\n3.3 Comparison on Public Benchmarks\nWe conducted experiments on the MOT17 [4] public dataset. As shown in Tab. 3 our tracker achieves 68.6 HOTA, and our method outperforms all methods under HOTA ordering in the MOT- challenge. On the dataset, using the same detection, our method outperforms existing comparisons including SUSHI [2], CoNo-Link [6], ByteTrack [13], OC-SORT [1] and StrongSORT++ [5], where StrongSORT++ is an offline version of StrongSORT that enhances trajectories by post-processing tracking offline. The benchmark results strongly demonstrate the advanced performance of our tracker."}, {"title": "4 Conclusion", "content": "In this study, we start with the offline multi-object tracking algorithm SUSHI and propose a new approach of adaptive video segmentation combined with Intra-frame surrounding object feature complementation. The proposed KFE takes full advantage of short trajectories and solves the IDS problem to some extent, and the feature complementation of the surrounding environment within the frame of the IFF module makes the object more informative, thus solving the problem of following the wrong object due to occlusion or similarity in appearance. Both modules improve the performance of multi-object tracking in terms of correlation in a good way."}]}