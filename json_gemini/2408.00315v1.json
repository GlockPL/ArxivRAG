{"title": "ADBM: Adversarial Diffusion Bridge Model for Reliable Adversarial Purification", "authors": ["Xiao Li", "Wenxuan Sun", "Huanran Chen", "Qiongxiu Li", "Yining Liu", "Yingzhe He", "Jie Shi", "Xiaolin Hu"], "abstract": "Recently Diffusion-based Purification (DiffPure) has been recognized as an effective defense method against adversarial examples. However, we find DiffPure which directly employs the original pre-trained diffusion models for adversarial purification, to be suboptimal. This is due to an inherent trade-off between noise purification performance and data recovery quality. Additionally, the reliability of existing evaluations for DiffPure is questionable, as they rely on weak adaptive attacks. In this work, we propose a novel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs a reverse bridge from the diffused adversarial data back to its original clean examples, enhancing the purification capabilities of the original diffusion models. Through theoretical analysis and experimental validation across various scenarios, ADBM has proven to be a superior and robust defense mechanism, offering significant promise for practical applications. Code will be made public soon.", "sections": [{"title": "Introduction", "content": "An intriguing problem in machine learning models, particularly Deep Neural Networks (DNNs), is the existence of adversarial examples [1, 2]. These examples introduce imperceptible adversarial perturbations leading to significant errors, which has posed severe threats to practical applications [3, 4]. Numerous methods have been proposed to defend against adversarial examples. But attackers can still evade most early methods by employing adaptive attacks [5, 6]. Adversarial Training (AT) methods [7-10] are recognized as effective defense methods against adaptive attacks. However, AT typically involves re-training the entire DNNs using adversarial examples, which is impractical for real-world applications. Moreover, the effectiveness of AT is often limited to the specific attacks it has been trained against, making it brittle against unseen threats [11, 12].\nRecently, Adversarial Purification (AP) methods [13, 14] have gained increasing attention as they offer a potential solution to defend against unseen threats in a plug-and-play manner without retraining the classifiers. These methods utilize the so-called purification module, which exploits techniques such as generative models, as a pre-processing step to restore clean examples from adversarial examples, as illustrated in Figure 1(a). Recently, diffusion models [15], one type of generative model renowned for their efficacy, have emerged as potential AP solutions [16]."}, {"title": "Preliminary and Related Work", "content": "Diffusion Models. Given a data distribution $q(x_0)$, DDPM [15] constructs a discrete-time Markov chain ${x_0,..., x_T}$ as the forward process for $x_0 \\sim q(x_0)$. Gaussian noise is gradually added to $x_0$ during the forward process following a scaling schedule ${\\beta_0, \\beta_1,\\cdots, \\beta_T}$, where $\\beta_0$ = 0 and $\\beta_T \\rightarrow 1$, such that $x_T$ is near an isotropic Gaussian distribution:\n$q(x_t|x_{t-1}) := \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_t I)$.\nDenote $\\alpha_t := 1 - \\beta_t$ and $\\bar{\\alpha}_t := \\prod_{i=1}^{t} \\alpha_i$, then\n$q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1 - \\bar{\\alpha}_t)I)$, i.e., $x_t(x_0, \\epsilon) = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon, \\epsilon \\sim \\mathcal{N}(0, 1)$.\nTo generate examples, the reverse distribution $q(x_{t-1}|x_t)$ should be learned by a model. But it is hard to achieve it directly. In practice, DDPM considers the conditional reverse distribution $q(x_{t-1}|x_t, x_0)$ and uses $x_\\theta(x_t, t)$ as an estimate of $x_0$, where\n$x_0(x_t, t) := (x_t - \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_\\theta(x_t,t))/\\sqrt{\\bar{\\alpha}_t}$.\nFor a given $x_0$, the training loss $L_a$ of diffusion models is thus defined as\n$L_a = E_{\\epsilon,t} [||\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon, t)||^2]$.\nTo accelerate the reverse process of DDPM, which typically involves hundreds of steps, Song et al. [22] propose a DDIM sampler based on the intuition that the multiple reverse steps can be performed at a single step via a non-Markov process. Song et al. [23] generalize the discrete-time diffusion model to continuous-time from the Stochastic Differential Equation (SDE) perspective.\nDiffusion Models for Adversarial Robustness. Recent studies have demonstrated the efficacy of diffusion models [15] in enhancing adversarial robustness in several ways. Some researches leverage much data generated by diffusion models to improve the AT performance [24, 25], but these AT-based methods do not generalize well under unseen threat models. Chen et al. [20] show that a single diffusion model can be transformed into an adversarially robust classifier using Bayes' rule, but at thousands of times the inference cost. DiffPure [16] employs a diffusion model as a plug-and-play pre-processing module to purify adversarial noise. Wang et al. [26] improved DiffPure by employing inputs to guide the reverse process of the diffusion model to ensure the purified examples are close to input examples. Zhang et al. [27] improved DiffPure by incorporating the reverse SDE with multiple Langevin dynamic runs. Zhang et al. [28] maximized the evidence lower bound of the likelihood estimated by diffusion models to increase the likelihood of corrupted images. DiffPure has also shown potential in improving certified robustness within the framework of randomized smoothing [29, 30]. Nevertheless, the practicality of randomized smoothing is greatly hindered by the time-consuming Monte Carlo sampling [31]. Different from these works, we present an diffusion-based purification method for empirical robustness in practical scenarios."}, {"title": "Reliable Evaluation for DiffPure", "content": "Before delving into the details of ADBM, it is important to discuss the white-box adaptive attack for diffusion-based purification first. As the original implementation of DiffPure needs dozens of prediction steps in the reverse process, it is challenging to compute the full gradient of the whole purification process due to memory constraints. To evaluate the robustness of diffusion-based purification, several techniques have been proposed. But we found that the adaptive evaluations for DiffPure remained insufficient, as detailed in Appendix A.1. We build on these previous insights [16, 20, 19] to develop a straightforward yet effective adaptive attack method against diffusion-based purification. We employ the gradient-based PGD attack [7], utilizing the full gradient calculation via gradient-checkpointing and incorporating a substantial number of EOT [5] and iteration steps."}, {"title": "Adversarial Diffusion Bridge Model", "content": "ADBM aims to construct a reverse bridge directly from the diffused adversarial data distribution to the clean data distribution. We derive the training objective for ADBM in Sec. 4.1, and explain how to obtain the adversarial noise for training ADBM in Sec. 4.2. The AP inference process using ADBM is described in Sec. 4.3. We finally show that ADBM has good theoretical guarantees for AP."}, {"title": "Training Objective", "content": "ADBM is a diffusion model specifically designed for purifying adversarial noise. It adopts a forward process similar to DDPM, with the difference that ADBM assumes the existence of adversarial noise $\\epsilon_a$ at the starting point of the forward process during training. This means that the starting point of the forward process is $x_\\epsilon = x_0 + \\epsilon_a$ for each $x_0$. Thus, according to Eq. (2), the forward process can be represented as $x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon$, $\\epsilon \\sim \\mathcal{N}(0,1)$, $0 \\leq t \\leq T$, where $T$ denotes the actual forward timestep when performing AP. $T$ is typically set to a lower value for AP, e.g., 100 in DiffPure, than that used in generative tasks, e.g., 1,000, to avoid completely corrupting $x_0$. We discuss how to obtain $\\epsilon_a$ in Sec. 4.2."}, {"title": "Reliable Evaluation for DiffPure", "content": "Before delving into the details of ADBM, it is important to discuss the white-box adaptive attack for diffusion-based purification first. As the original implementation of DiffPure needs dozens of prediction steps in the reverse process, it is challenging to compute the full gradient of the whole purification process due to memory constraints. To evaluate the robustness of diffusion-based purification, several techniques have been proposed. But we found that the adaptive evaluations for DiffPure remained insufficient, as detailed in Appendix A.1. We build on these previous insights [16, 20, 19] to develop a straightforward yet effective adaptive attack method against diffusion-based purification. We employ the gradient-based PGD attack [7], utilizing the full gradient calculation via gradient-checkpointing and incorporating a substantial number of EOT [5] and iteration steps."}, {"title": "In the reverse process of ADBM, the objective is to learn a Markov chain", "content": "In the reverse process of ADBM, the objective is to learn a Markov chain ${x_t}_{t:T\\rightarrow 0}$ that can directly transform from the diffused adversarial data distribution (i.e., $x_T$) to the clean data distribution (i.e., $x_0$), as shown in Figure 1(c). To achieve this, the starting point and the end point of $x_t$ should be defined as $x_T := x_T + \\epsilon_a$ and $x_0 := x_0$, respectively. Notably, $x_T$ contains adversarial noise, while $x_0$ does not. To explicitly align the trajectory of $x_t$ with the starting and ending points, we introduce a coefficient $k_t$, such that $x_t := x_t - k_t \\epsilon_a$ for $0 \\leq t \\leq T$, with $k_0 = 1$ and $k_T = 0$. With Bayes' rule and the property of Gaussian distribution, we have\n$q(x_{t-1}|x_t,x_0) = \\frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)} \\propto exp {-\\frac{1}{2}((x_t - \\sqrt{\\alpha_t}x_{t-1})^2 + (A(x_{t-1})^2 + Bx_{t-1}+C))};$,\nwhere $A$ is a constant independent of $\\epsilon_a$, $B$ is the coefficient of $x_{t-1}$ dependent on $\\epsilon_a$, and $C$ is a term without $x_{t-1}$. In inference, we expect that $\\epsilon_a$ in Eq. (5) can be eliminated since only $x_t$ is given by attackers and $\\epsilon_a$ cannot be decoupled from $x_t$ directly. Based on the property of Gaussian distribution, eliminating all terms related to $\\epsilon_a$ in $B$ and $C$ in Eq. (5) can be achieved by eliminating all terms related to $\\epsilon_a$ in $B$. This yields:\n$\\frac{\\sqrt{\\alpha_t}(\\sqrt{\\alpha_t}k_{t-1} - k_t)\\epsilon_a}{A_t} + \\frac{(\\sqrt{\\alpha_{t-1}} - k_{t-1})\\epsilon_a}{1-\\alpha_{t-1}} = 0$.\nTo satisfy $k_0 = 1$, $k_T = 0$, we can derive $k_t$ in Eq. (6) as:\n$k_t = \\sqrt{\\bar{\\alpha}_t}\\frac{\\sqrt{\\frac{\\bar{\\alpha}_T}{1 - \\bar{\\alpha}_T}}}{\\sqrt{\\frac{\\bar{\\alpha}_1}{1 - \\bar{\\alpha}_1}}}, 0 \\leq t \\leq T$.\nFor detailed derivations, please refer to Appendix B.1.\nFollowing Eq. (3), ADBM uses $x_0(x_t,t):= (x_t - \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_\\theta(x_t, t))/\\sqrt{\\bar{\\alpha}_t}$ to approximate the clean example $x_0$. Thus the loss of ADBM can be computed by\n$\\mathcal{L} = E_{\\epsilon,t}[||x_0 - x_0(x_t, t)||^2] = E_{\\epsilon,t}[||x_0 - (\\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_\\theta(x_t, t)}{\\sqrt{\\bar{\\alpha}_t}})||^2] = E_{\\epsilon,t}[|\\frac{k_t - \\sqrt{\\bar{\\alpha}_t}}{\\sqrt{\\bar{\\alpha}_t}}\\epsilon_a - \\frac{\\sqrt{1 - \\bar{\\alpha}_t}}{\\sqrt{\\bar{\\alpha}_t}}(\\epsilon - \\epsilon_\\theta(x_t, t))||^2]$\\n$\\ = E_{\\epsilon,t} [|\\frac{1-\\bar{\\alpha}_t}{\\bar{\\alpha}_t}\\frac{\\bar{\\alpha}_T}{\\sqrt{1-\\bar{\\alpha}_T}}\\epsilon_a - \\frac{\\sqrt{1 - \\bar{\\alpha}_t}}{\\sqrt{\\bar{\\alpha}_t}}(\\epsilon - \\epsilon_\\theta(x_t, t))||^2]$"}, {"title": "the final loss L\u266d of ADBM is given by", "content": "Omitting $\\frac{1 - \\alpha_t}{\\bar{\\alpha}_t}$ as in Ho et al. [15], for a given $x_0$, the final loss $L_b$ of ADBM is given by:\n$L_b =E_{\\epsilon,t}[|| \\frac{\\bar{\\alpha}_T\\sqrt{1 - \\bar{\\alpha}_t}}{(1 - \\bar{\\alpha}_T)\\sqrt{\\bar{\\alpha}_t}}\\epsilon_a + \\epsilon - \\epsilon_\\theta (x_t, t)||^2]$\n$x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon + (\\frac{\\bar{\\alpha}_T}{1-\\bar{\\alpha}_T})\\epsilon_a$. The training process of ADBM is detailed in the blue block of Figure 2.\nComparing Eq. (4) and Eq. (9), $L_a$ and $L_b$ are quite similar except that $L_b$ has two additional scaled $\\epsilon_a$ in both the input and prediction objective of $\\epsilon_\\theta$. Thus in practice, the training of ADBM can fine-tune the pre-trained diffusion checkpoint with $L_b$, avoiding training from scratch. As $t$ decreases to 0, the coefficient of $\\epsilon_a$ diminishes. This complies with the intuition that $\\epsilon_a$ is gradually eliminated."}, {"title": "Adversarial Noise Generation", "content": "We now proceed to the generation of adversarial noise required for ADBM training. A straightforward method for generating adversarial noise can be maximizing the loss $L_b$ of ADBM. However, the idea of deriving adversarial noise directly from the diffusion model itself might not align well with the objectives of the purification task. In this context, the primary goal is to ensure the classifier's accuracy on images after they have been purified.\nTo better align with this goal, we propose to generate adversarial noise with the help of the classifier. During ADBM training, we input $x_0$ into the classifier, where $x_0 = (\\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_\\theta(x_t,t)}{\\sqrt{\\bar{\\alpha}_t}})$.\nThe classification loss $L_c$ is then given by $L_c(f_\\theta(x_0), y)$, where $y$ denotes the category label of $x_0$ and $f_\\theta$ denotes the classifier. Finally, we computed $ \\nabla_{\\epsilon_a} L_c$ to obtain $\\epsilon_a$ that maximizes $L_c$. Figure 2"}, {"title": "AP Inference of ADBM", "content": "When using ADBM for AP, both the forward and reverse processes remain unchanged from the original pipeline of DiffPure, as shown in Figure 1(a). Any reverse samplers developed for diffusion models can be directly applied to the AP inference of ADBM without any modification, as ADBM only initiates the reverse process from a different starting point compared to traditional diffusion models. Therefore, to improve the practicality of ADBM, we can leverage fast sampling methods such as DDIM to accelerate the reverse process. As demonstrated in Sec. 3, the DDIM sampler efficiently conducts AP, even with a single reverse step."}, {"title": "Theoretical Analysis", "content": "We provide two theorems to show the superiority of ADBM for adversarial purification.\nTheorem 1. Given an adversarial example $x_t$ and assuming the training loss $L_b \\leq \\delta$, the distance between the purified example of ADBM and the clean example $x_0$, denoted as $||\\hat{x_0} - x_0||$, is bounded by $\\delta$ (constant omitted) in expectation when using a one-step DDIM sampler. Specifically, we have $E_{\\epsilon}[||\\hat{x_0} - x_0||^2] < \\frac{(1 - \\bar{\\alpha}_T)T}{\\bar{\\alpha}_T} \\delta$, where $\\frac{(1 - \\bar{\\alpha}_T)T}{\\bar{\\alpha}_T}$ is the constant.\nProof. Please see the full proof in Appendix B.2.\nTheorem 1 implies that if the training loss of ADBM converges to zero, it can perfectly remove adversarial noises by employing a one-step DDIM sampler. While for DiffPure, we cannot derive such strong theoretical guarantee (The bound provided in Theorem 3.2 of Nie et al. [16] is larger than $||\\epsilon_a||$ and thus cannot be zero). Moreover, the subsequent theorem demonstrates the superiority of ADBM over DiffPure."}, {"title": "Denote the probability of reversing the adversarial example to the clean example", "content": "Theorem 2. Denote the probability of reversing the adversarial example to the clean example using ADBM and DiffPure as $P(B)$ and $P(D)$, respectively. Then $P(\\cdot)$ can be computed as $P(\\cdot) = \\int 1_{x_0 \\notin D_a}P(x_0|x_t)dx_0$, where $D_a$ denotes the set of adversarial examples. If the timestep is infinite, the following inequality holds:\n$P(B) > P(D)$,\nwherein\nfor $P(B)$: $p(x_0|x_t) \\propto exp(-\\frac{||x_t - \\sqrt{\\alpha_t}x_0||^2}{2(1 - \\bar{\\alpha}_t)})$,\nfor $P(D)$: $p(x_0|x_t) \\propto exp(-\\frac{||x_t - \\sqrt{\\alpha_t}x_0||^2}{2(1 - \\bar{\\alpha}_t)})$.\nProof. The concept of infinite timestep can be viewed as dividing a finite length of time into infinitesimal intervals, which corresponds to the situation of the following SDEs proposed by Song et al. [23]. Denoting $w$ as the standard Wiener process, $\\bar{w}$ as the reverse-time standard Wiener process, and $p_t(x)$ the probability density of $x_t$, the forward process can be described by\n$dx = f(x, t)dt + g(t)dw$,\nwhere $f(x, t)$ and $g(t)$ denote the drift and diffusion coefficients, respectively. Under mild assumptions, the reverse process can be derived from:\n$d\\bar{x} = [f(\\bar{x}, t)dt - g(t)^2\\nabla_x \\log p_t(\\bar{x})]dt + g(t)d\\bar{w}$.\nIn this context, the reverse of a diffusion process is also a diffusion process, running backwards in time and given by the reverse-time SDE (Eq. (33)). Therefore, if timestep is infinite, ${x_t}_{t:0\\rightarrow T}$ and ${\\bar{x}_t}_{t:T\\rightarrow 0}$, as the solutions of Eq. (32) and Eq. (33) respectively, follow the same distribution. And due to the Bayes' rule,\n$p(x_0|x_t) = \\frac{p(x_t|x_0)p(x_0)}{p(x_t)}$,\n$= p(x_0)\\frac{p(x_t|x_0)}{p(x_t)} \\propto p(x_0)p(x_t|x_0)$.\nSince $x_t := x_t$ in ADBM and $\\hat{x_t} := x_t$ in DiffPure, then for all the examples,\n$P(B) = \\int 1_{x_0 \\notin D_a}p(x_0|x_t)dx_0 \\propto \\int 1_{x_0 \\notin D_a} p(x_0)p(x_t|x_0) dx_0 \\propto \\int 1_{x_0 \\notin D_a} exp(-\\frac{||x_t - \\sqrt{\\alpha_t}x_0 - (\\sqrt{\\alpha_t} - k_t)\\epsilon_a||^2}{2(1 - \\bar{\\alpha}_t)})p(x_0)dx_0$"}, {"title": "then we have", "content": "E [exp ( V = R = 3.  \nEa\n( 1 - \u1fb6\u03c4 )\u03b4.\nB.3 The Proof of Theorem 2"}]}