{"title": "ADBM: Adversarial Diffusion Bridge Model for Reliable Adversarial Purification", "authors": ["Xiao Li", "Wenxuan Sun", "Huanran Chen", "Qiongxiu Li", "Yining Liu", "Yingzhe He", "Jie Shi", "Xiaolin Hu"], "abstract": "Recently Diffusion-based Purification (DiffPure) has been recognized as an ef-\nfective defense method against adversarial examples. However, we find DiffPure\nwhich directly employs the original pre-trained diffusion models for adversarial\npurification, to be suboptimal. This is due to an inherent trade-off between noise\npurification performance and data recovery quality. Additionally, the reliability of\nexisting evaluations for DiffPure is questionable, as they rely on weak adaptive\nattacks. In this work, we propose a novel Adversarial Diffusion Bridge Model,\ntermed ADBM. ADBM directly constructs a reverse bridge from the diffused\nadversarial data back to its original clean examples, enhancing the purification\ncapabilities of the original diffusion models. Through theoretical analysis and\nexperimental validation across various scenarios, ADBM has proven to be a su-\nperior and robust defense mechanism, offering significant promise for practical\napplications. Code will be made public soon.", "sections": [{"title": "1 Introduction", "content": "An intriguing problem in machine learning models, particularly Deep Neural Networks (DNNs), is\nthe existence of adversarial examples [1, 2]. These examples introduce imperceptible adversarial\nperturbations leading to significant errors, which has posed severe threats to practical applications\n[3, 4]. Numerous methods have been proposed to defend against adversarial examples. But attackers\ncan still evade most early methods by employing adaptive attacks [5, 6]. Adversarial Training (AT)\nmethods [7-10] are recognized as effective defense methods against adaptive attacks. However, AT\ntypically involves re-training the entire DNNs using adversarial examples, which is impractical for\nreal-world applications. Moreover, the effectiveness of AT is often limited to the specific attacks it\nhas been trained against, making it brittle against unseen threats [11, 12].\nRecently, Adversarial Purification (AP) methods [13, 14] have gained increasing attention as they\noffer a potential solution to defend against unseen threats in a plug-and-play manner without retraining\nthe classifiers. These methods utilize the so-called purification module, which exploits techniques\nsuch as generative models, as a pre-processing step to restore clean examples from adversarial\nexamples, as illustrated in Figure 1(a). Recently, diffusion models [15], one type of generative\nmodel renowned for their efficacy, have emerged as potential AP solutions [16]. Diffusion models"}, {"title": "2 Preliminary and Related Work", "content": "Diffusion Models. Given a data distribution $q(x_0)$, DDPM [15] constructs a discrete-time Markov\nchain ${x_0,..., x_T}$ as the forward process for $x_0 \\sim q(x_0)$. Gaussian noise is gradually added to\n$x_0$ during the forward process following a scaling schedule ${\\beta_0, \\beta_1,\\cdots, \\beta_T}$, where $\\beta_0 = 0$ and\n$\\beta_T \\rightarrow 1$, such that $x_T$ is near an isotropic Gaussian distribution:\n$q(x_t|x_{t-1}) := \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI)$.\nDenote $\\alpha_t := 1 - \\beta_t$ and $\\bar{\\alpha}_t := \\Pi_{i=1}^{t}\\alpha_i$, then\n$q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1 - \\bar{\\alpha}_t)I)$, i.e., $x_t(x_0, \\epsilon) = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon, \\epsilon \\sim \\mathcal{N}(0, 1)$.\nTo generate examples, the reverse distribution $q(x_{t-1}|x_t)$ should be learned by a model. But it is hard\nto achieve it directly. In practice, DDPM considers the conditional reverse distribution $q(x_{t-1}|x_t, x_0)$\nand uses $x_\\theta(x_t, t)$ as an estimate of $x_0$, where\n$x_0(x_t, t) := (x_t - \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_\\theta(x_t, t))/\\sqrt{\\bar{\\alpha}_t}$.\nFor a given $x_0$, the training loss $\\mathcal{L}_d$ of diffusion models is thus defined as\n$\\mathcal{L}_d = \\mathbb{E}_{\\epsilon, t} [||\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon, t)||^2]$.\nTo accelerate the reverse process of DDPM, which typically involves hundreds of steps, Song et al.\n[22] propose a DDIM sampler based on the intuition that the multiple reverse steps can be performed\nat a single step via a non-Markov process. Song et al. [23] generalize the discrete-time diffusion\nmodel to continuous-time from the Stochastic Differential Equation (SDE) perspective.\nDiffusion Models for Adversarial Robustness. Recent studies have demonstrated the efficacy of\ndiffusion models [15] in enhancing adversarial robustness in several ways. Some researches leverage\nmuch data generated by diffusion models to improve the AT performance [24, 25], but these AT-based\nmethods do not generalize well under unseen threat models. Chen et al. [20] show that a single\ndiffusion model can be transformed into an adversarially robust classifier using Bayes' rule, but at\nthousands of times the inference cost. DiffPure [16] employs a diffusion model as a plug-and-play\npre-processing module to purify adversarial noise. Wang et al. [26] improved DiffPure by employing\ninputs to guide the reverse process of the diffusion model to ensure the purified examples are close to\ninput examples. Zhang et al. [27] improved DiffPure by incorporating the reverse SDE with multiple\nLangevin dynamic runs. Zhang et al. [28] maximized the evidence lower bound of the likelihood\nestimated by diffusion models to increase the likelihood of corrupted images. DiffPure has also shown\npotential in improving certified robustness within the framework of randomized smoothing [29, 30].\nNevertheless, the practicality of randomized smoothing is greatly hindered by the time-consuming\nMonte Carlo sampling [31]. Different from these works, we present an diffusion-based purification\nmethod for empirical robustness in practical scenarios."}, {"title": "3 Reliable Evaluation for DiffPure", "content": "Before delving into the details of ADBM, it is important to discuss the white-box adaptive attack\nfor diffusion-based purification first. As the original implementation of DiffPure needs dozens of\nprediction steps in the reverse process, it is challenging to compute the full gradient of the whole\npurification process due to memory constraints. To evaluate the robustness of diffusion-based\npurification, several techniques have been proposed. But we found that the adaptive evaluations for\nDiffPure remained insufficient, as detailed in Appendix A.1. We build on these previous insights\n[16, 20, 19] to develop a straightforward yet effective adaptive attack method against diffusion-based\npurification. We employ the gradient-based PGD attack [7], utilizing the full gradient calculation\nvia gradient-checkpointing and incorporating a substantial number of EOT [5] and iteration steps."}, {"title": "4 Adversarial Diffusion Bridge Model", "content": "ADBM aims to construct a reverse bridge directly from the diffused adversarial data distribution\nto the clean data distribution. We derive the training objective for ADBM in Sec. 4.1, and explain\nhow to obtain the adversarial noise for training ADBM in Sec. 4.2. The AP inference process using\nADBM is described in Sec. 4.3. We finally show that ADBM has good theoretical guarantees for AP."}, {"title": "4.1 Training Objective", "content": "ADBM is a diffusion model specifically designed for purifying adversarial noise. It adopts a forward\nprocess similar to DDPM, with the difference that ADBM assumes the existence of adversarial noise\n$\\epsilon_a$ at the starting point of the forward process during training. This means that the starting point of\nthe forward process is $x_\\tau = x_0 + \\epsilon_a$ for each $x_0$. Thus, according to Eq. (2), the forward process\ncan be represented as $x_t = \\sqrt{\\bar{\\alpha}_t}x_\\tau + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon, \\epsilon \\sim \\mathcal{N}(0,1), 0 \\leq t \\leq T$, where $T$ denotes the\nactual forward timestep when performing AP. $T$ is typically set to a lower value for AP, e.g., 100\nin DiffPure, than that used in generative tasks, e.g., 1,000, to avoid completely corrupting $x_0$. We\ndiscuss how to obtain $\\epsilon_a$ in Sec. 4.2."}, {"title": "4.2 Adversarial Noise Generation", "content": "We now proceed to the generation of adversarial noise required for ADBM training. A straightforward\nmethod for generating adversarial noise can be maximizing the loss $\\mathcal{L}_b$ of ADBM. However, the\nidea of deriving adversarial noise directly from the diffusion model itself might not align well with\nthe objectives of the purification task. In this context, the primary goal is to ensure the classifier's\naccuracy on images after they have been purified.\nTo better align with this goal, we propose to generate adversarial noise with the help of the classifier.\nDuring ADBM training, we input $x_0$ into the classifier, where $\\hat{x}_0 = \\sqrt{\\bar{\\alpha}_t}(x_t - \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_\\theta(x_t,t))$.\nThe classification loss $\\mathcal{L}_c$ is then given by $\\mathcal{L}_c(f_\\theta(\\hat{x}_0), y)$, where $y$ denotes the category label of $x_0$\nand $f_\\theta$ denotes the classifier. Finally, we compute $\\frac{\\partial L_c}{\\partial \\epsilon_a}$ to obtain $\\epsilon_a$ that maximizes $\\mathcal{L}_c$. Figure 2"}, {"title": "4.3 AP Inference of ADBM", "content": "When using ADBM for AP, both the forward and reverse processes remain unchanged from the\noriginal pipeline of DiffPure, as shown in Figure 1(a). Any reverse samplers developed for diffusion\nmodels can be directly applied to the AP inference of ADBM without any modification, as ADBM\nonly initiates the reverse process from a different starting point compared to traditional diffusion\nmodels. Therefore, to improve the practicality of ADBM, we can leverage fast sampling methods\nsuch as DDIM to accelerate the reverse process. As demonstrated in Sec. 3, the DDIM sampler\nefficiently conducts AP, even with a single reverse step."}, {"title": "4.4 Theoretical Analysis", "content": "We provide two theorems to show the superiority of ADBM for adversarial purification.\nTheorem 1. Given an adversarial example $x^\\tau$ and assuming the training loss $\\mathcal{L}_b \\leq \\delta$, the distance\nbetween the purified example of ADBM and the clean example $x_0$, denoted as $||\\hat{x}_0 - x_0||$, is bounded\nby $\\delta$ (constant omitted) in expectation when using a one-step DDIM sampler. Specifically, we have\n$\\mathbb{E}_\\epsilon [||\\hat{x}_0 - x_0||^2] < \\frac{(1-\\bar{\\alpha}_T)T}{\\bar{\\alpha}_T} \\delta$, where $\\frac{(1-\\bar{\\alpha}_T)T}{\\bar{\\alpha}_T}$ is the constant.\nTheorem 1 implies that if the training loss of ADBM converges to zero, it can perfectly remove\nadversarial noises by employing a one-step DDIM sampler. While for DiffPure, we cannot derive\nsuch strong theoretical guarantee (The bound provided in Theorem 3.2 of Nie et al. [16] is larger than\n$||\\epsilon_a||$ and thus cannot be zero). Moreover, the subsequent theorem demonstrates the superiority of\nADBM over DiffPure."}, {"title": "5 Experiments", "content": "Datasets and network architectures. We conducted comprehensive experiments on popular datasets,\nincluding SVHN [35], CIFAR-10 [21], and Tiny-ImageNet [36], together with a large-scale dataset\nImageNet-100 [37]. All these datasets consist of RGB images, whose resolution is 32 \u00d7 32 for\nSVHN and CIFAR-10, 64 \u00d7 64 for Tiny-ImageNet, and 224 \u00d7 224 for ImageNet-100. We adopted\nthe widely used WideResNet-28-10 (WRN-28-10), WRN-70-16, WRN-28-10, and ResNet-50 [39]\narchitectures as classifiers on SVHN, CIFAR-10, Tiny-ImageNet, and ImageNet-100, respectively.\nAs for the diffusion models, we employed the UNet architecture [40] improved by Song et al. [23],\nspecifically, the DDPM++ continuous variant. Pre-trained diffusion checkpoints are required for\nDiffPure. We directly used the checkpoint provided by Song et al. [23] for CIFAR-10 and we used\ntheir code to train the checkpoints for other datasets. These trained checkpoints were used in DiffPure\nand served as baselines for ADBM.\nFine-tuning settings of ADBM. The adversarial noise was computed in the popular norm-ball setting\n$||\\epsilon_a||_\\infty \\leq 8/255$. When computing $\\epsilon_a$, we used PGD with three iteration steps and a step size of\n8/255. Other settings followed the standard configuration used in Song et al. [23]. The fine-tuning\nsteps were set to 30K, which is about 1/10 the training steps of the original diffusion models. In each\nfine-tuning step, the value of $T$ in Eq. (9) was uniformly sampled from 100 to 200. Note that when\nfine-tuning the diffusion models, the parameters of the classifier were kept frozen. Additional settings\nare provided in Appendix C.1.\nDefense configurations of ADBM. Unless otherwise specified, the forward diffusion steps were set\nto be 100 for SVHN and CIFAR-10 and 150 for Tiny-ImageNet and ImageNet-100, respectively. The\nreverse steps were set to be five. The reverse process used a DDIM sampler. These configurations\nwere also applied to DiffPure for a fair comparison."}, {"title": "5.2 Robustness against White-Box Adaptive Attacks", "content": "We first evaluate ADBM against the reliable while-box adaptive attacks to show the worst-case\nadversarial robustness where the attacker has complete knowledge. Note that we expect a defense\nmethod to be robust not only on seen threats but also on unseen attack threats. Thus, unless otherwise\nspecified, we evaluated the models on three attack threats: $l_\\infty$, $l_1$, and $l_2$, with the bounds $\\epsilon_\\infty = 8/255$,\n$\\epsilon_1 = 12$, $\\epsilon_2 = 1$, respectively. Here $l_\\infty$ attack is considered the seen threat as ADBM was trained\nwith $l_\\infty$ adversarial noise, while $l_1$ and $l_2$ attacks can be regarded as unseen threats."}, {"title": "5.3 Robustness against Black-Box Attacks", "content": "We then considered the more realistic black-box attacks, where the attacker has no knowledge about\nthe defense mechanism, i.e., the purification model, and cannot access the gradients of models.\nInstead, the attacker can only query the model's output with query-based attacks or use substitute"}, {"title": "5.4 Ablation Study", "content": "The ablation studies were performed on CIFAR-10. The evaluation followed the setting in Sec. 5.2.\nReverse steps. We first investigated the influence of reverse steps on the adversarial robustness of\nADBM. The number of reverse steps is proportional to the inference cost. We used five reverse steps\nin the main experiments. Here we evaluated the robustness of ADBM with fewer steps to investigate\nwhether the number of steps can be further reduced. The results in Tab. 5 show ADBM was more\nrobust than DiffPure regardless of reverse steps. Notably, even with just one reverse step, ADBM\nmaintained its good robustness. We discuss the detailed inference cost related to this further in Sec. 6.\nAdversarial noise generation modes. We then analyzed the impact of different adversarial noise\ngeneration modes for ADBM. As discussed in Sec. 4.2, adversarial noise used for training ADBM\ncan be generated by various modes. We analyzed the contributions of our three key design choices in\nthe generation modes: using the classifier, fixing $t$, and fixing $x$. The results shown in Tab. 6 clearly\ndemonstrate that all of these designs are essential for the success of ADBM. Removing any of these\ndesigns hampers the proper computation of adversarial noise for ADBM training.\nEffectiveness on new classifiers. To assess the effectiveness of ADBM on new classifiers, we\nconducted a study to investigate its transferability. Specifically, we utilized the fine-tuned ADBM\ncheckpoint, trained with adversarial noise from a WRN-70-16 classifier, as the pre-processor for\na WRN-28-10 model and a vision transformer model [48] directly, denoted as ADBM(Transfer).\nThe results shown in Appendix D.4 demonstrate that ADBM(Transfer) achieved robust accuracies\ncomparable to ADBM directly trained with corresponding classifiers. This finding highlights the\npracticality of ADBM, as the fine-tuned ADBM model on a specific classifier can potentially be\ndirectly applied to a new classifier without requiring retraining. We guess this may be attributed to\nthe transferability of adversarial noise [49]."}, {"title": "6 Conclusion and Discussion", "content": "In this work, we introduce ADBM, a cutting-edge method for diffusion-based adversarial purification.\nTheoretical analysis supports the superiority of ADBM in enhancing adversarial robustness. With\nextensive experiments, we demonstrated the effectiveness of ADBM across various scenarios using"}, {"title": "A Evaluations on DiffPure", "content": "As the original implementation of DiffPure needs dozens of prediction steps in the reverse process,\nit is challenging to compute the full gradient of the whole purification process due to memory\nconstraints. To circumvent computing the full gradient, Nie et al. [16] originally employed the adjoint\nmethod (along with Expectation over Transformation (EOT) [5]) to compute an approximate gradient.\nBut recent works have identified it to be a flawed adaptive attack [19, 20]. As improved adaptive\nmechanisms, Lee and Kim [19] proposed a surrogate attack for DiffPure (to approximate the gradient\nof the iterative reverse procedure better) and Chen et al. [20] utilized the gradient-checkpointing\ntechnique [51] to trade time for memory space and compute the full gradient directly. Despite\nprevious efforts, we find that the adaptive evaluations for DiffPure remained insufficient. Specifically,\nthe surrogate attack failed to compute the full gradient, and the gradient-checkpointing attack in\nChen et al. [20] employed insufficient iteration steps or EOT samples, an issue underscored by Gao\net al. [52] which highlights the importance of adequate steps and samples for evaluating stochastic\npre-processing defenses like DiffPure.\nIn our evaluation, we used the full gradient of the whole reverse process and set the PGD iteration\nsteps to 200, with 20 EOT samples for each iteration. The cost of the evaluation is quite high,\nespecially in the context of the high reverse steps of the original DiffPure. To give a concrete example,\nfor a single input image, the noise prediction model (i.e., $\\epsilon_\\theta$) in the original DiffPure implementation\nwith 100 reverse steps needs to be queried a total of 400, 000 times.\nHowever, we think that such high efforts are worthwhile as an unreliable evaluation could create a false\nsense of security on defenses. Historical evidence has shown that many defenses initially considered\nrobust, were subsequently breached by more sophisticated and dedicated attacks [5, 6]. Our aim is\nto prevent a similar outcome for diffusion-based purification, advocating for the employment of a\nmeticulous and reliable attack evaluation methodology, regardless of the expense involved.\nOn the other hand, with our reliable evaluation, we have investigated the influence of forward\nsteps and reverse steps on the robustness of diffusion-based purification. Our following results in\nTab. 2 indicate that reverse steps will not significantly influence the robustness of DiffPure under\nthe reliable evaluation. This suggests that diffusion-based purification techniques might not benefit\nfrom increasing the number of reverse steps to complicate the attack process, as such strategies could\nfinally be neutralized by high-cost attacks similar to the one we have implemented. And when the\nreverse steps are reduced (e.g., five steps in our main experiments), the attack cost of our evaluation\nmethod is comparable to the most widely used AutoAttack benchmark [32]."}, {"title": "A.3 Additional Investigation on DiffPure with Our Evaluation", "content": "As shown in Tab. 2, we find that the reverse step does not significantly affect the robustness when\nusing the DDIM sampler. Considering the computational cost of attacks, we fixed the reverse step to\n5 with the DDIM sampler and focused on investigating the influence of forward steps on robustness.\nThe results are shown in Tab. A1. We observed a continuous decrease in clean accuracy and a\ncontinuous increase in robustness as the number of forward steps increased. This can be attributed to\nthe introduction of more noise during the forward process."}, {"title": "B Derivation of Equations and Proofs of Theorems", "content": "Following the Bayes' rule, we have\n$q(x_{t-1}|x_t,x_0) = \\frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)}$.\nSince $x_t^\\tau = x^\\tau - k_t\\epsilon_a$ and $x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon$, there is\n$x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon + (\\sqrt{\\bar{\\alpha}_t} - k_t)\\epsilon_a$.\nBased on the property of Gaussian distribution, $p(x_{t-1}|x_t, x_0)$ also must be Gaussian distribution,\nthus,\n$q(x_{t-1}|x_t,x_0) = \\frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)} \\propto exp(-\\frac{\\frac{(x_t - \\sqrt{\\alpha_t}x_{t-1}- (\\sqrt{\\alpha_t}k_{t-1} - k_t)\\epsilon_a)^2}{1 - \\alpha_t} + \\frac{(x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}}x_0 - (\\sqrt{\\bar{\\alpha}_{t-1}} - k_{t-1})\\epsilon_a)^2}{1 - \\bar{\\alpha}_{t-1}} }{2}) = exp(-\\frac{(A(x_{t-1})^2 + Bx_{t-1} + C(x_0, \\epsilon_a, x_t))^2}{2}),$\nwhere\n$A = \\frac{\\alpha_t}{1 - \\alpha_t} + \\frac{\\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_{t-1}},$\n$B = -2(\\sqrt{\\alpha_t}\\frac{x_t - (\\sqrt{\\alpha_t}k_{t-1} - k_t)\\epsilon_a}{1 - \\alpha_t} + \\sqrt{\\bar{\\alpha}_{t-1}}\\frac{\\sqrt{\\bar{\\alpha}_{t-1}}x_0+ (\\sqrt{\\bar{\\alpha}_{t-1}} - k_{t-1})\\epsilon_a}{1 - \\bar{\\alpha}_{t-1}})$.\nIn inference, we expect that $\\epsilon_a$ in Eq. (14) can be eliminated since only $x_t$ is given by attackers and\n$\\epsilon_a$ cannot be decoupled from $x_t$ directly. Based on the property of Gaussian distribution, eliminating\nall terms related to $\\epsilon_a$ in $B$ and $C$ in Eq. (5) can be achieved by eliminating all terms related to $\\epsilon_a$ in\n$B$. This yields:\n$\\frac{\\sqrt{\\alpha_t}(\\sqrt{\\alpha_t}k_{t-1} - k_t)\\epsilon_a}{1 - \\alpha_t} - \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} - k_{t-1}}{1 - \\bar{\\alpha}_{t-1}} = 0$.\nAlthough Eq. (16) cannot be directly solved, we can derive its recurrent form. Let us set $k_t = \\sqrt{\\bar{\\alpha}_t}\\gamma_t$,\nwhere $0 \\leq t \\leq T$ and $\\gamma_0 = 1, \\gamma_T = 0$. The Eq. (16) can deduce to\n$\\frac{\\sqrt{\\alpha_t}}{1 - \\alpha_t}(\\sqrt{\\bar{\\alpha}_t}\\gamma_{t-1} - \\sqrt{\\bar{\\alpha}_t}\\gamma_t) = \\frac{\\sqrt{\\alpha_t}(1 - \\sqrt{\\alpha_t})\\gamma_{t-1}}{1 - \\bar{\\alpha}_{t-1}}$.\nThen group the items according to timestep $(t$ and $t - 1)$:\n$(\\frac{\\alpha_t}{1 - \\alpha_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}})\\gamma_{t-1} = \\frac{\\alpha_t}{1 - \\alpha_t}\\gamma_t$.\nNow we have a recurrent equation about $\\gamma_t$. The equivalence holds for all $0 < t < T$. Therefore, by\nelucidating the relationship between $\\gamma_t$ and the initial value $\\gamma_1$, we can deduce an expression for each\n$\\gamma_t$. If we reorganize items in Eq. (18), it yields a recurrent equation:\n$\\gamma_{t-1} = \\frac{\\alpha_t}{1 - \\bar{\\alpha}_t}(\\frac{\\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_{t-1}} - 1) = \\Pi_{i=2}^t \\frac{\\alpha_i (1 - \\bar{\\alpha}_1)}{\\bar{\\alpha}_i (1 - \\bar{\\alpha}_1)} = \\frac{\\alpha_1 (1 - \\bar{\\alpha}_t)}{\\bar{\\alpha}_t (1 - \\bar{\\alpha}_1)}.$"}, {"title": "B.2 The Proof of Theorem 1", "content": "Theorem 1. Given an adversarial example $x^\\tau$ and assuming the training loss $\\mathcal{L}_b \\leq \\delta$, the distance\nbetween the purified example of ADBM and the clean example $x_0$, denoted as $||\\hat{x}_0 - x_0||$, is bounded\nby $\\delta$ in expectation (constant omitted) when using a one-step DDIM sampler. Specifically, we have\n$\\mathbb{E}_\\epsilon [||\\hat{x}_0 - x_0||^2] < \\frac{(1-\\bar{\\alpha}_T)T}{\\bar{\\alpha}_T} \\delta$, where $\\frac{(1-\\bar{\\alpha}_T)T}{\\bar{\\alpha}_T}$ is the constant.\nThis inequality holds when we use the DDIM reverse sampler and set the reverse step $s = 1$.\nAccording to Song et al. [22], the reverse process of DDIM is\n$x_{\\tau_{i-1}} = \\sqrt{\\bar{\\alpha}_{t_{i-1}}}(\\frac{x_{\\tau_{i}} - \\sqrt{1 - \\bar{\\alpha}_{t}}\\epsilon_{\\theta}(x_{\\tau_i}, T_i)}{\\sqrt{\\bar{\\alpha}_{\\tau_i}}}) + \\sqrt{1 - \\bar{\\alpha}_{t_{i-1}}}\\epsilon_\\theta(x_{\\tau_i}, T_i)$.\nwhere ${\\tau_0,..., \\tau_s }$ is a linearly increasing sub-sequence of $[0, . . ., t]$, $\\tau_0 = 1, \\tau_s = t$. Setting $s = 1$,\nthen it yields:\n$\\hat{x}_0 = \\sqrt{\\bar{\\alpha}_0}(\\frac{x^\\tau - \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_{\\theta}(x^\\tau, t)}{\\sqrt{\\bar{\\alpha}_t}}) + \\sqrt{1 - \\bar{\\alpha}_0}\\epsilon_{\\theta}(x^\\tau, t)$.\nSince $\\sqrt{\\alpha_0} = 1$, $x^\\tau = \\sqrt{\\bar{\\alpha}_t}x + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon$, $x = x_0 + \\epsilon_a$, and $t = T$, Eq. (24) can be written as\n$\\hat{x}_0 = x_0 + \\frac{\\sqrt{1 - \\bar{\\alpha}_T}}{\\sqrt{\\bar{\\alpha}_T}}(-\\epsilon_a + \\epsilon - \\epsilon_{\\theta}(x^\\tau, T)).$\nTherefore, the distance between $\\hat{x}_0$ and $x_0$ is\n$||\\hat{x}_0 - x_0|| = ||\\frac{\\sqrt{1 - \\bar{\\alpha}_T}}{\\sqrt{\\bar{\\alpha}_T}}(-\\epsilon_a + \\epsilon - \\epsilon_{\\theta}(x^\\tau, T))||,$\nwhere the second equivalence holds due to $t = T$ and $x_t^\\tau = x^\\tau - k_t\\epsilon_a = x^\\tau$. Considering that\n$\\mathcal{L}_b = \\mathbb{E}_{\\epsilon.t}[\n\\frac{1}{T}\\Sigma_{t=0}^{T}(-\\frac{\\sqrt{1 - \\bar{\\alpha}_t}}{\\sqrt{\\bar{\\alpha}_t}}\\frac{\\sqrt{\\alpha_1}}{\\sqrt{1 - \\bar{\\alpha}_1}}\\epsilon_a + \\epsilon - \\epsilon_{\\theta}(x^\\tau, t)\n)^2] < \\delta$,"}, {"title": "B.3 The Proof of Theorem 2", "content": "Denote the probability of reversing the adversarial example to the clean example\nusing ADBM and DiffPure as P(B) and P(D)", "holds": "nP(B) > P(D)", "P(B)": "p(x_0|x_t) \\propto exp(-$$ \n$$for P(D): p(x_0|\\hat{x"}, "t) \\propto exp(-$$ ).\nProof. The concept of infinite timestep can be viewed as dividing a finite length of time into\ninfinitesimal intervals, which corresponds to the situation of the following SDEs proposed by Song\net al. [23"], "from": "ndx = [f(x", "t": "T$$ \\rightarrow $$0, as the solutions of Eq. (32) and Eq. (33) respectively, follow the same distribution. And\ndue to the Bayes\u2019 rule,\n$$p(x_0|x_t) = p(x_0) $$ = $$ p(x_t|x_0) $$ =\n$$p(x_0) $$"}