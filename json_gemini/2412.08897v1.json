{"title": "NEURAL INTERACTIVE PROOFS", "authors": ["Lewis Hammond", "Sam Adam-Day"], "abstract": "We consider the problem of how a trusted, but computationally bounded agent (a\n'verifier') can learn to interact with one or more powerful but untrusted agents\n('provers') in order to solve a given task. More specifically, we study the case in\nwhich agents are represented using neural networks and refer to solutions of this\nproblem as neural interactive proofs. First we introduce a unifying framework\nbased on prover-verifier games (Anil et al., 2021), which generalises previously\nproposed interaction protocols. We then describe several new protocols for gener-\nating neural interactive proofs, and provide a theoretical comparison of both new\nand existing approaches. Finally, we support this theory with experiments in two\ndomains: a toy graph isomorphism problem that illustrates the key ideas, and a\ncode validation task using large language models. In so doing, we aim to create\na foundation for future work on neural interactive proofs and their application in\nbuilding safer AI systems.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent years have witnessed the proliferation of large machine learning (ML) systems (Villalobos\net al., 2022), useful for solving an increasingly wide range of tasks. Often, however, it can be\ndifficult to trust the output of these systems, raising concerns about their safety and limiting their\napplicability in high-stakes situations (Amodei et al., 2016; Bengio et al., 2023; Hendrycks et al.,\n2023). At the same time, traditional approaches in verification do not scale to today's most powerful\nsystems (Seshia et al., 2022). There is thus a pressing need to identify new angles via which to gain\nsuch assurances.\nIn response to this need, we take inspira-\ntion from interactive proofs (IPs) (Gold-\nwasser et al., 1985), one of the most\nimportant developments in computational\ncomplexity theory and cryptography. In\nan IP, a computationally bounded but\ntrustworthy verifier interacts with a more\npowerful but untrustworthy prover in or-\nder to solve a given problem.Under reasonable assumptions, it can be\nshown that such interactions can allow\nthe verifier to solve many more kinds of\nproblem than it could alone, all while lim-\niting the chance of being misled by the\nprover.\nIn this work, we investigate neural interactive proofs, in which the prover and verifier are represented\nby neural networks. Several different instances of this general idea have been proposed in recent\nyears (Irving et al., 2018; Anil et al., 2021; W\u00e4ldchen et al., 2022), though there is still a gap\nbetween the theory inspiring these methods and their practical implementation. Our work aims to\nhelp bridge this gap."}, {"title": "1.1 CONTRIBUTIONS", "content": "We seek to provide a comprehensive treatment of neural interactive proofs spanning both theory and\npractice. In particular, we provide the following contributions:\n(i) a unifying game-theoretic framework that generalises existing neural IP protocols;\n(ii) several new neural IP protocols, including those that allow for zero-knowledge proofs;\n(iii) a theoretical and empirical comparison of both new and existing protocols; and\n(iv) an extensible codebase for testing different protocols in different domains.\nIn so doing, we hope to create a foundation for future work on neural interactive proofs and their\napplication in building safer ML systems."}, {"title": "1.2 EXAMPLE", "content": "In order to aid the reader's intuition for the concepts we introduce, we provide an example scenario\nto which all neural IP protocols can be applied, and refer back to this throughout the paper.\nExample 1. Consider a domain of challenging programming tasks, each of which is defined by some\ncombination of natural language specification and implicit world knowledge. There is a powerful\nAl system (a \"prover\u201d) capable, in theory, of solving every task, but who does so in an inscrutable\nway that is difficult to trust. We would like to use a weaker but trusted AI system (a \"verifier\")\nwhich is incapable of solving the task by itself \u2013 to check that any code produced by the prover really\ndoes satisfy the corresponding specification. A pictorial representation is given in Figure 1.\nFormally, this example is a decision problem where the input is a specification and a piece of code.\nIn a neural IP, the agents learn to interact to decide whether the code satisfies the specification."}, {"title": "1.3 RELATED WORK", "content": "The most closely related work to ours was \u2013 until recently \u2013 that of Anil et al. (2021), who introduce\nprover-verifier games played between neural networks, which we generalise and build on. While\nan important first step, this work is limited by the formal strength of the proof systems that result\nfrom their specific protocol (as we show), and by its application only to small models and problem\ninstances. Similar to prover-verifier games are the works of Irving et al. (2018) and W\u00e4ldchen et al.\n(2022), whose proof systems make use of two provers in competition with one another and are\nstronger from a theoretical perspective, but are again only applied to very simple problems.\nMore recently, three papers (concurrent with our own and with each other) have sought to overcome\nsome of the practical limitations of these earlier works by evaluating protocols using LM agents.\nKenton et al. (2024) moves beyond earlier efforts (Michael et al., 2023; Khan et al., 2024) by con-\nsidering several tasks aside from question answering, and also computational (instead of merely\ninformational) asymmetries between the provers and verifiers. They find that multi-prover 'debate'\nprotocols outperform single-prover 'consultancy' protocols but that there is a relatively limited ben-\nefit to debate compared to the verifier baseline performance. The authors hypothesise that one reason\nfor this is that they do not train their models using the protocol (which is the focus of our work).\nKirchner et al. (2024) do train their agents to play prover-verifier games using multiple rounds of ex-\npert iteration Anthony et al. (2017), but only on the protocol introduced by Anil et al. (2021), which\nwe show has important theoretical limitations. They find that the helpful prover's accuracy and the\nverifier's robustness to adversarial attacks increase over the course of training, though their primary\nfocus is on the legibility of solutions to humans. Finally, and most recently, Arnesen et al. (2024)\ncombine several of the strengths of these two investigations by comparing multiple protocols and\ntraining the provers using a novel variant of Direct Preference Optimisation (Rafailov et al., 2023),\nthough they restrict their attention to question-answering. Mirroring Kenton et al. (2024), they find\nthat optimising the provers leads to higher verifier accuracy in debate but not consultancy, and that\ndebate training introduces stronger argumentation (as measured by the use of quotations)."}, {"title": "2 PRELIMINARIES", "content": "This section provides a brief technical background on games and interactive proofs, which are the\ntwo main building blocks of neural interactive proofs. In general, we index agents using superscripts\nand time (or other variables) using subscripts. Vectors \u00e6 are written in bold, and elements of sets $x \\in X$\nare written as lowercase and uppercase letters, respectively. $\u2206(X)$ denotes the set of distributions\nover $X$ and $1_s : X \u2192 {0,1}$ represents the indicator function for $S \u2286 X$, i.e. $1_s(x) = 1$ if and\nonly if $x \u2208 S$. Given a vector \u00e6, we write $x_{i:j}$ for $(x_i, ..., x_j)$ where $i < j$."}, {"title": "2.1 PROOF SYSTEMS", "content": "Interactive proofs are standardly defined with respect to a decision problem $(X, S)$, where $X$ is the\nset of problem instances and $S \u2282 X$ is the set of 'positive' instances. In Example 1, $X$ is the set of\nall specification-code pairs produced by the prover, and $S$ is the set of pairs where the code satisfies\nthe specification. The prover and verifier exchange messages from their message spaces $M_P$ and\n$M_u$ respectively. In our example, these could be the space of all text strings under a certain length.\nDefinition 1 (Goldwasser et al., 1985; Goldreich, 2001). An interactive proof system $(p,v)$ for\n$S \u2286 X$ comprises a prover $p$ and verifier $v$ which, given an input $x \u2208 X$, interact to (stochastically)\ngenerate a sequence of messages $m_{1:T}$ (a 'proof'). The (finite) sequence length $T$ is determined by\n$v$, whose eventual output is given by $m_y \u2208 {1,0}$, corresponding to 'accept' and 'reject', respec-\ntively. We denote this (stochastic) proof $m_{1:T}$ produced by $(p, v)$ on input $x$ as $(p,v)(x)$. We say\nthat $(p, v)$ is $(\u03b5_c, \u03b5_s)$-valid (or simply \u2018valid\u2019) for $\u03b5_c + \u03b5_s < 1$ if it satisfies:\n\u2022 Completeness: If $x \u2208 S$, then $P[(p, v)(x) = 1] \u2265 1 \u2212 \u03b5_c$;\n\u2022 Soundness: If $x \\notin S$, then $P[(p', v)(x) = 0] \u2265 1 - \u03b5_s$ for any prover $p'$.\nThe classes of decision problem $(X, S)$ for which there exists a valid interactive proof system de-\npend on the sets of provers $P$ and verifiers $V$ under consideration. For example, in the the original\nformulation due to Goldwasser et al. (1985), the prover is unbounded and the verifier is a probabilis-\ntic polynomial time Turing machine, which gives rise to the class IP (equal to PSPACE) (Shamir,\n1992).\nDefinition 2 (Goldwasser et al., 1985; Goldreich, 2001). We say that $(p, v)$ is $(\u03b5_k$-statistically) zero-\nknowledge if for every verifier $v'$ there is some verifier $z$ such that $max_x |E_m [P((p, v')(x) = \n m)]- P (z(x) = m)]| < \u03b5_k$. We call z a simulator.\nWhile validity can be viewed as a property of the verifier, being zero-knowledge can be viewed as\na property of the prover. Intuitively, $(p, v)$ is zero-knowledge if the verifier learns only whether\n$x \u2208 S$ and nothing else, i.e. $v'$ does not gain any additional power through their interaction with $p$\n(represented by the fact that $z \u2208 V$)."}, {"title": "2.2 GAMES", "content": "In this work, we study n-player games $G = (\u039d,\u03a3, L)$ where $N = {1,...,n}$ are the agents,\n$\u03a3 := \\times_{i \\in N} \u03a3^i$ is a product strategy space and $L$ contains loss functions $L^i: \u03a3 \u2192 R$ for $i \u2208\nN$. Each agent $i$ selects a strategy $\u03c3_i \u2208 \u03a3_i$ in an attempt to minimise their loss $L^i(\u03c3)$. More\nspecifically, we focus our attention on what we term 'messaging games', which centre around rounds\nof communication between the different agents via multiple channels. In Example 1, for instance,\nthe verifier might cross-reference portions of the code or the prover's answers by sending them to a\nsecond, independent prover via a separate channel.\nDefinition 3. In a messaging game $G = (\u039d,\u03a3, L; M, C, \u00b5)$, play proceeds by agents sending\nmessages $m_i \u2208 M_i$ via a number of channels $C \u2286 2^N$ according to a mechanism $\u00b5 : C \u00d7 N \u2192\n\u2206(2^N)$, which determines the set of agents $N' \u2286 N$ who can sent a message in channel $c \u2208 C$ at\ntime $t \u2208 N$. When $\u00b5(c,t)$ is deterministic we write $\u00b5(c,t) = N'$. Agents can only observe messages\nin channels they belong to, denoted $C(i) := {c \u2208 C : i \u2208 c}$, and cannot identify the sender of any\nmessage beyond the channel's other members. When $i \u2208 N' \\cap \u00b5(t, c)$, agent $i$ sends a message\n$m_{c,t}^i ~ \u03c3_i (M^i | (m_{c',1:t\u22121}^{c'})_{c'\u2208C(i)})$ based on their previously observed messages across $C(i)$.\nWhenever \u00d8 ~ $\u00b5(c, t)$, a random message $m_{c,t}^i ~ \u03c1 (M^i | (m_{c,1:t\u22121}^{c})_{c\u2208c})$ is sent. Finally, play\nterminates whenever a decision $m_{c\u2217} \u2208 M^\u2020 \u2286 M$ is sent in a special channel $c^\u2020 \u2208 C$. We drop $M$,\n$C$, and $\u00b5$ from the notation for $G$ when unambiguous or unimportant.\nWe use $G(\u03c3^i)$ to denote the $(n \u2212 1)$-player game induced when agent $i$ plays strategy $\u03c3^i$ in $G$, but\nwhere the remaining $n \u2212 1$ agents have not yet chosen their strategies. In practice, we assume that\neach agent's strategy space $\u03a3^i$ is defined by some finite number of parameters $\u0398^i$, and will often\nrefer to $\u03b8^i \u2208 \u0398^i$ instead of $\u03c3^i$. Within these games, we make use of two standard equilibrium\nconcepts, which can be defined both locally and globally.\nDefinition 4. A local Nash equilibrium (LNE) on $\u0398 \u2286 \u0472$ is a strategy profile $\u03b8^* \u2208 \u00d4$ such that:\n$\u03b8^* \u2208 argmin_{\u03b8^i \u2208\u0398^i} L^i (\u03b8^{-i}, \u03b8^i)$,\nfor all $i \u2208 [n]$. A local Stackelberg equilibrium led by player $i$ (LSE_i) on $\u2609 \u2286 \u0472$ is a strategy\nprofile $\u03b8^* \u2208 \u00d4$ such that:\n$\u03b8^* \u2208 argmin_{\u03b8^i \u2208\u0398^i} max_{\u03b8 \u2208 LNE(G(\u03b8^i))} L^i(\u03b8^{-i}, \u03b8^i)$.\nIf $\u2295 = 0$ then $0^\u2217$ is a (global) Nash/Stackelberg equilibrium (NE/SE). We denote the local and\nglobal NEs/i-led SEs of $G$ by LNE(G)/LSE_i(G) and NE(G)/SE_i(G), respectively. We consider\napproximate versions of these concepts, where the argmin for each agent $i$ has some tolerance\n$e^i \u2208 R_{\u22650}$. Given $e = (e^1, . . ., e^n)$, we denote the approximate equilibria as e-NE and e-SE."}, {"title": "3 PROVER-VERIFIER GAMES", "content": "Prover-verifier games (PVGs) were introduced by Anil et al. (2021) as a game-theoretic framework\nto incentivise learning agents to solve decision problems in a verifiable manner. Concretely, we\nconsider probabilistic decision problems $(X, S, P)$ where $P$ is a distribution over $X$. In Example 1,\nfor instance, there might be many kinds of programming task and solutions, jointly distributed ac-\ncording to $P$, with the set $S$ then representing the specification-code pairs. Upon receiving an input\n$x ~ P$, a verifier interacts with one or more provers according to a high-level protocol determined\nby the structure of the PVG to see if they can generate a 'proof' that $x \u2208 S$. The agents in the game\nreceive losses as a function of their strategies for interacting.\nIn the remainder of the section, we make the above setting and earlier assumptions more formal by\nintroducing a generalisation of PVGs based on the messaging games defined earlier. This generalised\ndefinition is sufficiently broad so as to capture several other protocols (Irving et al., 2018; W\u00e4ldchen\net al., 2022), as well as the new protocols that we introduce in this paper. A summary of the different\nprotocols is shown in Table 1."}, {"title": "4 NEURAL INTERACTIVE PROOFS", "content": "Unlike in the case of standard interactive proofs, we assume that the verifier has some access\nto a dataset of labelled examples and aims to learn a strategy for interacting with the prover\nthat generalises to any input. More formally, we assume that the verifier has access to a dataset\n$D = {(x_j, y_j)}_{1 \u2264 j \u2264 m}$ with each $x_j ~iid P$ and $y_j = 1_s(x_j)$. Letting $l(\u03c3, x) := E_{\u03c3} [m_T != y]$, the\nverifier's aim is to minimise $l$ across inputs $x \u2208 X$.\nIn this work, we consider two ways of operationalising this aim: the expected loss (or risk), and the\nworst-case loss. We denote the empirical risk with respect to $D$ as $L_{ER} (\u03c3) = \u2211_{(x,y)\u2208D} l(\u03c3, x)$,\nand the empirical worst-case loss by $L_{WC}(\u03c3) = max_{(x,y)\u2208D} l(\u03c3, x)$. Minimisers of these losses are\ndenoted $\u03c3^{ER}$ and $\u03c3^{WC}$ respectively. We drop $D$ from our notation when considering the full space\ninputs $X$, with expectations taken with respect to $P$ in the ER case.\nWhile the expected loss is by far the most common objective in contemporary ML, from the per-\nspective of safety-critical systems, we would ideally like to minimise the worst-case loss. In other\nwords, we wish to find some $\u03c3$ such that $L_{WC}(\u03c3) \u2212 L_{WC}(\u03c3^{WC})$ is small. Moreover, this is despite\nthe fact that $min_{N^o} L^i (\u03c3^{N^o} , \u03c3^{\u2212N^o})$ is large, as per Definition 5.\nConcretely, optimising $L_{ER}(\u03c3)$ in the code validation example aims to produce a verifier which on\nunseen specification-code pairs gives a decision that has a high probability of being correct, even if\nthe prover is able to fool it with incorrect code. Conversely, optimising $L_{WC}(\u03c3)$ aims to produce\na verifier that has low probability of being fooled on any code. Taking inspiration from the adp\nprotocol (formally defined in Appendix A), let us consider a PVG with, $n_p = n_v = 1$, $C = {c\u2020}$,\nand $\u00b5(t, c) = {t mod 2}$, and loss functions given by:\n$L^P(\u03c3) = L_{WC} (\u03c3 | y = 1) \u2212 L_{WC}(\u03c3 | y = 0)$,\n$L^v (\u03c3) = L_{WC} (\u03c3 | y = 1) + L_{WC}(\u03c3 | y = 0)$.\nWe refer to this protocol as nip, and it is straightforward to show that this corresponds closely to\nthe notion of an interactive proof.\nTheorem 1. Let (X, S, P) be a probabilistic decision problem that has a valid proof system and\n$G$ a nip game. Then $\u03c3$ is a valid IP system if and only if it is an approximate verifier-leading\nStackelberg equilibrium of G.\nWhile this proposition essentially reduces the problem of finding a valid proof system to the task\nof finding an equilibrium in a given game, this task is far from straightforward. In particular, there\nare two key difficulties. Firstly, there is the challenge of learning to minimise the worst-case (as\nopposed to the expected) loss. Secondly, there is the challenge of finding a Stackelberg equilibrium."}, {"title": "4.1 WORST-CASE Loss", "content": "The simplest approach to minimising the worst-case loss using finitely many data $D$ generated from\n$P$ is to ignore the worst-case performance and simply return some $\u03c3^{BR}_{ER}$. The question then becomes:\nwhen is minimising the empirical risk with respect to $D$ sufficient for minimising the worst-case\nrisk with respect to $X$? The following result shows that we can break this down into two properties\n(defined formally in Appendix B.2): (a) the empirical worst-case loss being similar to the actual\nworst-case loss; and (b) for a given $D$, the empirical worst-case loss of $\u03c3^{ER}$ being similar to that\nof $\u03c3^{WC}$. These conditions do not always hold, but can do when the decision problem is sufficiently\n'regular'.\nProposition 2. If $\u03a3$ has the worst-case uniform convergence property (a) and the worst-case ro-\nbustness property (b) then there is some $f^{WCUC} : (0,1)^2 \u2192 N$ such that for every $\u03b5, \u03b4 \u2208 (0,1)$, if\n$|D| \u2265 m^{WC}(\u03b5, d)$ then $L_{WC}(\u03c3^{ER}) \u2212 L_{WC}(\u03c3^{WC}) \u2264 \u03b5$ with probability $1 \u2212 \u0431$.\nAlternatively, we can introduce an adversary, $a$, whose strategy space is $S \u00d7 X \\backslash S$ and whose loss\nfunction is $L_a (\u03c3, (s,x)) = \u2212l(\u03c3,s) \u2212 l(0,x)$. We then replace the terms $L_{WC} (\u03c3 | y = i)$ in\nthe original loss functions for the prover and verifier with $l(\u03c3, s) \u2212 l(\u03c3, x)$ and $l(\u03c3, s) + l(\u03c3,x)$\nrespectively. The verifier-leading Stackelberg equilibria of the original nip game are then identical\nto the verifier-prover-leading Stackelberg equilibria in this new three-player game, denoted $G^a$.\nUnlike the classical learning-theoretic approach above in which we assumed we were given a fixed\ndataset $D$ of $(x, y)$ pairs, we are here assuming access to an adversary capable of outputting any\n$x \u2208 X$. This stronger assumption may not always hold, but when it does, learning can be more\nefficient (Goldman & Kearns, 1995).\nProposition 3. Let (X, S, P) be a probabilistic decision problem and $G$ a nip game. Then $(\u03c3^P, \u03c3^\u03c5)$\nis an approximate verifier-leading SE (e-SE_v) of G if and only if there is some $\u03c3^a$ such that\n$(\u03c3^P, \u03c3^\u03c5, \u03c3^a)$ is an approximate verifier-prover SE (e-SE_{v,p}) of $G^a$ (the adversarial version of G)."}, {"title": "4.2 SOLVING STACKELBERG GAMES", "content": "Computing Stackelberg equilibria can be naturally modelled as a bi-level optimisation problem. A\nstandard solution to such problems using gradient-based methods is to employ a timescale separation\n(Borkar, 2008). In particular, we take the sequential nature of the problem setting into account by\nexplicitly modelling the dependence of $O_p$ on $\u03b8_\u03c5$ and updating $O_p$ more quickly as part of an 'inner\nloop'. Fiez et al. (2020) show that if $\u03c3^o = o(a)$ then with high probability the following dynamics\nwill converge locally to the neighbourhood of a LSE:\n$\u03b8_{p,t+1}^\u03c5 = \u03b8_{p,t}^\u03c5 - \u03b1_p (t) \\cdot \u2207_p L^P$,\n$\u03b8_{\u03c5,t+1} = \u03b8_\u03c5 - \u03b1^o (t) \\cdot \u2207_\u03c5 L^v - [\u2207_{p} C^p (\u2207_{p} C^p )]^{-1}\u2207_{p} L^p$,\nwhere we drop the dependence on $\u03b8$ from our notation and write $\u2207_v$ and $\u2207_p$ for $Ver_L$ and $Ver_p$, re-\nspectively. These updates require computing an inverse Hessian vector product, which is intractable\nwhen $O_p$ is large. Replacing the term $[\u2207_{p} C^p (\u2207_{p} C^p )]^{-1}$ with $\u03b1_p (t + 1)$ leads to the LOLA (Learning with\nOpponent Learning Awareness) update (Foerster et al., 2018), which aims to actively influence the\nfuture policy updates of its opponents. While LOLA may fail to converge, interpolating between the\nLOLA update and LookAhead (Zhang & Lesser, 2010) leads to local convergence to stable fixed\npoints in differentiable games under self-play (Letcher et al., 2019)."}, {"title": "5 EXTENSIONS", "content": "Finally, we generalise the nip protocol along two natural dimensions in order to strengthen the\nproperties of the resulting proof systems."}, {"title": "5.1 MULTIPLE PROVERS", "content": "Multi-prover interactive proofs (MIPs) are a natural generalisation of classical IPs (Ben-Or et al.,\n1988), whose additional power results from the fact that while the two provers may correlate their\nstrategies, they are prevented from communicating with one another during their interactions with\nthe verifier (Babai et al., 1991). This allows the verifier to 'cross-examine' the provers.\nWe define the mnip protocol identically to the nip protocol, but now with two provers, $p_1$ and\n$p_2$, each of which has the same loss. Valid MIP systems are defined as in Definition 1, with the\nsoundness condition altered such that $v$ must be robust to any choice of $p_1, p_2$. Using a similar proof\nto that of Theorem 1, it can be shown that the equilibria of the mnip PVG correspond to valid MIP\nsystems. The only subtlety is that due to the provers' ability to coordinate on a joint strategy and\nshared random signal beforehand, we must consider correlated equilibria (defined in Appendix B.1).\nTheorem 2. Let (X, S, P) be a probabilistic decision problem that has a valid proof system and\n$G$ a mnip game. Then $\u03c3$ is a valid MIP system if and only if it is an approximate verifier-leading\ncorrelated Stackelberg equilibrium of G."}, {"title": "5.2 ZERO KNOWLEDGE", "content": "We now consider whether the prover can prevent the verifier from learning any more than to solve\nthe task at hand. We formalise this question as whether the resulting proof system is zero-knowledge\n(ZK). Far from a theoretical curiosity, this may have important practical implications, such as for\nthe prevention of model cloning/stealing via black-box access (Orekondy et al., 2019).\nRecall from Definition 2 that (p, v) is ZK if for any v' and any x such that y = 1, there is an-\nother weak agent $z \u2208 V$ capable of generating approximately the same distribution over message\nsequences as (v', p)(x), without interacting with p. While it is possible to introduce a precise ana-\nlogue of this condition, we wish to investigate whether the verifier can gain knowledge regardless of\nthe value of y.\nWe therefore provide a slightly stronger condition that requires statistical closeness between\n(v',p)(x, y) and z(x,y) for any x. In other words, short of the answer y, there is nothing else\nthat v' can learn by interacting with p. We can implement this protocol using a PVG by introducing"}, {"title": "6 EXPERIMENTS", "content": "Having characterised the theoretical power of various PVG protocols, it remains to be seen whether\nthey are sufficient for solving real-world problems empirically. Indeed, until very recently (Kenton\net al., 2024; Arnesen et al., 2024; Khan et al., 2024), no previous study has attempted to rigorously\ncompare different protocols, and despite this recent progress, it remains unclear what their relative\nstrengths and weaknesses are in practice. To provide further clarity, we conduct experiments in two\ndomains: a toy graph isomorphism problem that illustrates the key ideas, and a code validation task\nusing large language models. Further details about the setup of our experiments can be found in\nAppendix C, and we release a comprehensive codebase that supports the easy specification of new\nprotocols and their evaluation in new domains. Throughout this section, error bars represent one\nstandard deviation, taken across 10 seeds."}, {"title": "6.1 GRAPH ISOMORPHISM", "content": "As a warm-up exercise, we consider the problem of testing for graph isomorphism using a syn-\nthetic dataset of 10,000 graphs (each with approximately ten vertices) generated via the Erd\u0151s-R\u00e9nyi\nmodel. The verifiers and provers are implemented using a graph neural network (GNN) and trans-\nformer body, with standard feedforward heads used for sending messages and estimating the value\nof each state. Agents are trained using independent PPO (Schulman et al., 2017; Schroeder de\nWitt et al., 2020) and evaluated on a held-out test set. This simple setting has the benefit that by\nvarying the number of GNN layers we can precisely calibrate the capability of the agents to dis-\ntinguish between isomorphic and non-isomorphic graphs. By controlling the number of rounds of\nthe Weisfeiler-Leman test (Weisfeiler & Leman, 1968) needed to distinguish non-isomorphic graph\npairs in the dataset, we ensure a theoretical limit on the accuracy attainable by the verifier alone."}, {"title": "6.2 CODE VALIDATION", "content": "Our second experiment involves a much more complex problem: checking that a given Python\nprogram satisfies a natural language specification. In particular, we make use of the Automated\nProgramming Progress Standard (APPS) dataset (Hendrycks et al., 2021), which consists of Python\ncoding challenges with ground-truth solutions written by humans, as well as test cases for checking\nthe solutions. We create a new dataset by drawing two solutions for each problem, one of which\nwe insert a subtle bug into using GPT-40-mini (OpenAI, 2024), checking that the result is still valid\nPython code but that it fails on at least one of the test cases. An example is given in Appendix C.2.1,\nand the median (Levenshtein) edit distance when inserting a bug is seven. We use GPT-4o as the\nprovers and GPT-40-mini as the verifiers, fine-tuning both using eight rounds of expert iteration\n(Anthony et al., 2017). In each iteration, we fine-tune the models from scratch using all previous\nrollouts, following Kirchner et al. (2024). The models are then evaluated on a held-out test set."}, {"title": "7 DISCUSSION", "content": "Motivated by the problem of developing scalable methods for gaining assurances about the trust-\nworthiness of increasingly large models, we sought to provide a comprehensive treatment of neural\ninteractive proofs spanning both theory and practice. Such assurances will become increasingly im-\nportant as ML systems are used to complete tasks where it is inefficient or impossible for humans\nto check for correct behaviour a problem known as scalable oversight (Amodei et al., 2016; Leike\net al., 2018; Christiano et al., 2018). Our results contribute to growing body of work which ten-\ntatively suggests that such techniques may be increasingly viable, and moreover may be aided by\ncontinuing advances in AI capabilities Khan et al. (2024); Arnesen et al. (2024).\nOur present work has a number of limitations, however. First, the algorithms we use for training in\nour experiments do not make use of some of the more advanced methods described in Section 4.1\nand Section 4.2 (for the graph isomorphism task), or RL-based learning (for the code-validation\ntask), which would bring our empirical results closer to their theoretical underpinnings. Second, we\nonly evaluate the protocols on two domains which, while providing a suitable testbed for some of\nthe primary questions we ask in this paper, are far from representative of the increasingly wide range\nof tasks that contemporary AI systems can be used to solve. Third, we do not evaluate all variations\nof all protocols, such as debate with simultaneous vs. sequential messaging or \"open protocols in\nwhich the provers choose what outcome to argue for in training\u201d (Kenton et al., 2024).\nAside from addressing the limitations described above, the game-theoretic framework and codebase\nwe have introduced in this paper support the future development and evaluation of new protocols,\nwhich may provide better theoretical or empirical performance than the protocols we discuss here.\nAnother important avenue for further work is in closing the gap between theory and practice by\ndeveloping learning-theoretic results (as opposed to complexity-theoretic results based on abstract\nmodels of computation such as Turing machines) about the extent to which the computational abili-\nties of learning agents and the amount of data available to them affects the ability for weaker agents\nto verify stronger agents. We hope that with such advances, it will eventually be possible to generate\nmore rigorous arguments for the safety of models even more advanced than today's state of the art.\""}, {"title": "A ADDITIONAL PROTOCOLS", "content": "While a comprehensive comparison is beyond the scope of the present work", "S_P": "X \u2192 M^P$ \u2013 and $\u03a3^\u03c5$\ncontains the convex combinations of functions $\u03b4^\u03c5 : X \u00d7 M^P \u2192 Y$. The specification $\u00b5$ is such that"}]}