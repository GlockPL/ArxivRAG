{"title": "Deep Learning and Natural Language Processing in the Field of Construction.", "authors": ["R\u00e9my Kessler", "Nicolas B\u00e9chet"], "abstract": "This article presents a complete process to extract hypernym relationships in the field of construction using two main steps: terminology extraction and detection of hypernyms from these terms. We first describe the corpus analysis method to extract terminology from a collection of technical specifications in the field of construction. Using statistics and word n-grams analysis, we extract the domain's terminology and then perform pruning steps with linguistic patterns and internet queries to improve the quality of the final terminology. Second, we present a machine-learning approach based on various words embedding models and combinations to deal with the detection of hypernyms from the extracted terminology. Extracted terminology is evaluated using a manual evaluation carried out by 6 experts in the domain, and the hypernym identification method is evaluated with different datasets. The global approach provides relevant and promising results.", "sections": [{"title": "1 Introduction", "content": "The current era is increasingly influenced by the prominence of smart data and mobile applications. The work presented in this paper has been carried out in one industrial project (VOCAGEN) aiming at automating the production of structured data from human-machine dialogues. Specifically, the targeted application drives dialogues with people working in a construction area for populating a database reporting key data extracted from those dialogues. This application requires complex processing for both transcribing speeches and driving dialogues. The first process ensures good speech recognition in noisy environments. The second processing step is necessary to ensure the database contains accurate"}, {"title": "2 Industrial context", "content": "Figure 1 presents the context of this work in VOCAGEN project. Our industrial partner, Script & Go, developed an application for construction management dedicated to touch devices and wishes to set up an oral dialogue module to facilitate the construction sites data collect. The second industrial partner, Tykomz, develops a vocal recognition suite based on the Sphynx 4 toolkit (Meignier and Merlin, 2010, 10-19). This toolkit includes agglomerative hierarchical clustering methods using well-known measures such as BIC and CLR and provides elementary tools, such as segment and cluster generators, decoders and model"}, {"title": "3 Related works", "content": "The goal of ontology learning (OL) is to build knowledge models from text. OL uses NLP knowledge extraction tools to extract terminology and links between them (relationships). It is concerned with discovering knowledge from various data sources and representing them in an ontological structure. Thus ontology learning comprises a set of techniques to extract the core components of the ontology, i.e. concepts, taxonomic and ad hoc relations, and general axioms. For more details, (Asim et al., 2018, 1-24) provide an overview of existing techniques to accomplish the various subtasks of ontology learning."}, {"title": "3.1 Taxonomy Extraction", "content": "Our interest in this part of the work was focused on extracting taxonomic relations. A taxonomic relation occurs between two concepts where a concept is a superordinate of another concept. For example, the concept \"Fish\" is a superordinate of the concept \u201cShark\" (is-a(Shark, Fish)). We base our approach on the fact that the hypernyms (i.e. relations between terms a term may comprise several words in a text representing the fact that a term can be used instead of another term in a sentence, conveying more general or more specific meaning) suggest taxonomic relationships. The reference in the field of rule-based systems was developed by (Cunningham, 2002, 223-254). General Architecture for Text Engineering (GATE) is a Java collection of tools initially developed at the University of Sheffield in 1995."}, {"title": "3.2 Detection of hypernyms", "content": "Ontology learning refers to the automatic or semi-automatic building of ontology. It is concerned with discovering knowledge from various data sources and representing them in an ontological structure. Thus ontology learning comprises a set of techniques to extract the core components of one ontology i.e. concepts, taxonomic and ad-hoc relations, and general axioms. For more details, (Asim et al., 2018, 1-24) provide an overview of existing techniques to accomplish the various subtasks of ontology learning.\nThe main approaches found in the literature for extracting hypernyms are pattern-based and distributional, being the latter further divided into unsupervised and supervised. Pattern-based approaches are heuristic methods that predict hypernym relations between pairs of terms if those terms are related by one pattern matching with a given sentence. These patterns are either defined manually or extracted automatically. The earliest and most popular handcrafted patterns are introduced by Hearst (Hearst, 1992, 1-7), thus known as Hearst's patterns. Pattern approaches can be used for detecting hyperonyms in a text: this is not the case in the work presented in this paper because we extract hypernyms by using a terminology previously extracted and validated.\nDistributional-based approaches are based on the distributional hypothesis, suggesting that words sharing the same linguistic context tend to have a similar meaning (Harris, 1968, 1-6). Earlier works to predict hypernymy are unsupervised, usually based on symmetric measures such as cosine similarity (Salton and McGill, 1986, 1-15) and Lin similarity (Lin, 1998, 296-304). Various evolutions of earlier works have been proposed such as (Weeds et al., 2004, 1015-1021), (Shwartz et al., 2016, 425-435), (Roller et al., 2014, 1025-1036), enabling to take into account the inclusion of the context of each term in a pair.\n(Lenci and Benotto, 2012, 75-79) propose to take into account both the inclusion and the non-inclusion of the context of each term in a pair. Unsupervised methods are simple to implement and apply and there is no need for training data: however, they show low performance and are heavily domain-dependent.\nSupervised learning approaches rely on a training dataset to train a model. The model is then used to predict hypernym relations between terms in a pair. Most of these approaches are based on words embedding like Word2vec (Mikolov et al., 2013, 3111-3119) or Glove (Pennington et al., 2014, 2249-2259). (Levy et al., 2015, 970-976) present a study of supervised approaches on standard corpora in the field and compare state-of-the-art results on labeled datasets. The various supervised methods differ in the way they represent each candidate pair of words (x, y): In (Weeds et al., 2014, 2249-2259) and (Roller et al., 2014, 1025-1036), they use the difference between the embedding vector of terms y and the embedding vector of terms x (y-x) as a feature vector to train an SVM classifier. They conclude asymmetric representation performs better and the difference representation yields the best result. (Luu et al., 2016, 403-413) proposed an approach to encode hypernym properties by learning terms embedding that not only indicate the information of the hyponym and the hypernyms but also the contexts between them. Consequently, they define triples of hypernym, hy-"}, {"title": "5 Detection of Hypernym Relationships", "content": "This section focuses on relation extraction from the terminology previously obtained. Two methodologies are studied, the first use classical words embedding vectors as features, and the second one relies on end-to-end approaches. To validate the methodology, two other datasets are experimented with."}, {"title": "5.1 Methodology", "content": "Overview Figure 7 presents the architecture of the Words embedding-based module. In the first step, we use a training dataset (1) composed of pairs of terms that are known to be in a hypernym relationship (or hyponym relationship depending on the reading order). For instance, the pair \"kitchen, room\" represents a hyponym relationship because a kitchen is a room. We combine this training dataset with a words embedding model (\u2461) providing a vector for each term of each pair in the training dataset. A vector algebraic operation (3) is then applied between each word of the pair to create a hypernym relationship learning model (4). Such kind of model is fit using classical machine learning algorithms like Random Forests or Multilayer Perceptron. Finally, the obtained model is used to detect hypernym relationships in the extracted terminology.\nEmbedding models component Words embedding is a representation technique to represent any word in the low-dimensional space: words having similar representations are likely to be semantically similar. Each word found in an input dataset is projected to a vector model to obtain a semantic representation of the whole input dataset. We experimented with three words embedding models as follows. The first model is Word2vec (Mikolov et al., 2013, 3111-3119), an unsupervised and predictive neural words embedding technique to learn the word representation in low-dimensional space. We specifically use the skip-gram model, in which a word is used to predict the context using a neural network close to an autoencoder. Second is Glove, for Global vector for Word Representation (Pennington et al., 2014, 1532-1543). A co-occurrence word matrix is created from a text dataset for the training and is reduced in low-dimensional space which explains the variance of high-dimensional data and provides a word vector for each word. The last model is fastText (Bojanowski et al., 2017, 135-146), which is close to Word2vec. Unlike Word2vec which considers each word as a single unit and ignores the morphological structure of the word, fastText overcomes this limitation by considering each word as an n-gram of characters.\nNote that we do not mention in our experimental section the results obtained with Word2vec because they were systematically lower than those obtained with fastText.\nVector compositions and machine learning algorithms We tested four compositions for representing (x, y) as a feature vector: concat (x + y) Baroni and Lenci (2010), diff (y \u2212 x) Weeds et al. (2014), sum b + va and product Ub * va. We evaluated the performance of the following classification algorithms: SVM (Support-Vector Machine), RF (Random Forests), and MLP (Perceptron Multilayer), and two fusions of vote-based algorithms: the fusion so-called Hard (the majority prevails) and the so-called Soft (we sum the probabilities of pre-"}, {"title": "End-to-end based module", "content": "Overall Unlike the module presented previously based on classic words embedding, an end-to-end approach can be defined generically as a system that processes the entirety of a task in a manner unified, without manual or decoupled intermediate steps. In effect, the system takes raw data as input and produces the desired result, without requiring human interventions or third-party systems with the possible exception of minimal pre/post-processing. Usually, we use a pre-trained model to implement an end-to-end approach.\nFor our task, such an approach amounts to using two words as input to the model (the hyponym and the hypernym) and as the expected output the class label (i.e. in relation of hyperonymy or not). Thus, the model can be fine-tuned with the training dataset. We do not need to use precomputed embedding like Word2vec because the model will learn contextualized embedding weights during the training process.\nUsed algorithms We evaluated the performance of three algorithms which can be used in an end-to-end process. A mask-based algorithm RoBERTa (and this French version CamemBERT), and two causal models (which are also Large Language Models) Llama-2 and Claire-Mistral.\nROBERTa (Robustly Optimized BERT Pretraining Approach) is a pre-trained Transformers-type language model, developed by Facebook in 2019. It is an optimized and improved version of the BERT model, with pre-training on many more data.\nLLaMA-2 (Longform Language Model with Attention) is a Large Transformers-type Language Model, recently developed by Facebook in 2023. We use here the 7B model.\nCLAIRE/Mistral is a Large Language Model developed by French researchers in 2023 adapted from Mistral. Built on the Transformers architecture, it was pre-trained specifically on diarized French conversations.\nExperimental protocol and results are presented in the next section."}, {"title": "5.2 Experiments and results", "content": "Datasets We rely on three datasets of semantic relations, which are all used in various state-of-the-art approaches, for hypernym's evaluation. The first dataset is BLESS (Baroni et al., 2012, 23-32), which contains 1,337 hyponym-hypernym pairs. It is designed to evaluate distributional semantic models. It contains 200 distinct concepts. Concepts are named with single-word nouns in the singular form. Each concept has a set of related words. A concept and related word are labeled by one of the following five relations: co-hyponym, hypernym, meronym, attribute, and events. For instance, the dataset contains \"cat-hyper-animal\" where"}, {"title": "Deep L. and NLP in the Field of Construction.", "content": "More generally, a small portion of the relationships contained in the dataset are not IS-A type relationships, but rather Part-of type relationships or others, for example \"Europe\" and \"Eurasia\" or \"moldy\" and \"mushroom\". A cleaning step is underway to remove these relationships. Most classification errors stem from complex technical terms or terms that have different meanings in the context of construction. For example, \"poutre longrine\" translates to \"longrine beam\", a specific type of beam used in construction. Similarly, \u201ctuyau dauphin\" is a French technical term for a type of flexible hose used for suction and discharge of water, although it literally translates to \"dolphin hose\". The last category of errors involves brand mentions within relationship terms. For example, \"Bic pen\" or \"Merlin axe\", here Merlin is a brand of an axe. In our context, the model needs to understand sentences like \"put away the Merlins\" and identify that the user is referring to axes."}, {"title": "6 Conclusion and future work", "content": "The paper reports our experiments and results for building a precise and large terminology for the construction domain. Collecting terminology is indeed the first step towards a complete knowledge model containing both concepts and relationships. During our work we were faced to several problems: finding resources and selecting them for building an appropriate corpus, thinking and developing pre-processing for cleaning those resources, experimenting distinct measures for n-grams and selecting the most appropriate, improving results by adding linguistic patterns and Internet queries. The current results are quite promising according to the evaluation of the extracted terminology carried out by 6 experts in the field. However, as manual evaluation of the produced terminology proved to be time-consuming and laborious, we developed in a second time a model to automatically or semi-automatically system to validate the terminology using our second partner's knowledge model. Our goal was to propose a module for the detection of hypernyms performing and different combinations were tested for different corpora/languages. We explore two approaches for representing text data, the first uses classical words embedding vectors as features, and the second one relies on end-to-end approaches. Models fine-tuned with CamemBERT achieved a very promising F-score of 91.6%. While construction terminology may appear specialized and technical, deep learning approach with LLM and BERT model trained on a massive dataset seems to be sufficient for this task. From our French hypernymy relation model trained on VOCAGEN dataset and validated on the two other datasets, the next step will be to organize the extracted terminology to build a significantly richer taxonomy. This work validates the possibility of changing domains (a key objective for one of the partners) and maintaining good performance. We propose first improving results by merging the outputs of all algorithms, assuming they make errors on different data. A more in-depth analysis of the errors is necessary to validate this hypothesis. Another interesting perspective for this work would be to explore using the"}, {"title": "20 R\u00e9my Kessler et al.", "content": "knowledge model from our partner as input to enhance the model's prediction capabilities."}]}