{"title": "TrICy: Trigger-guided Data-to-text Generation with Intent aware Attention-Copy", "authors": ["Vibhav Agarwal", "Sourav Ghosh", "Harichandana BSS", "Himanshu Arora", "Barath Raj Kandur Raja"], "abstract": "Data-to-text (D2T) generation is a crucial task in many natural language understanding (NLU) applications and forms the foundation of task-oriented dialog systems. In the context of conversational AI solutions that can work directly with local data on the user's device, architectures utilizing large pre-trained language models (PLMs) are impractical for on-device deployment due to a high memory footprint. To this end, we propose TrICy, a novel lightweight framework for an enhanced D2T task that generates text sequences based on the intent in context and may further be guided by user-provided triggers. We leverage an attention-copy mechanism to predict out-of-vocabulary (OOV) words accurately. Performance analyses on E2E NLG dataset [1] (BLEU: 66.43%, ROUGE-L: 70.14%), WebNLG dataset [2] (BLEU: Seen 64.08%, Unseen 52.35%), and our Custom dataset related to text messaging applications, showcase our architecture's effectiveness. Moreover, we show that by leveraging an optional trigger input, data-to-text generation quality increases significantly and achieves the new SOTA score of 69.29% BLEU for E2E NLG. Furthermore, our analyses show that TrICy achieves at least 24% and 3% improvement in BLEU and METEOR respectively over LLMs like GPT-3, ChatGPT, and Llama 2. We also demonstrate that in some scenarios, performance improvement due to triggers is observed even when they are absent in training.", "sections": [{"title": "I. INTRODUCTION", "content": "Applications of Natural Language Generation (NLG) [3]\u2013[6] include tasks like text summarization, report generation, dialog response generation, etc. Data-to-text (D2T) in NLG deals with capturing a useful subset of an instance of data in a generated syntactically correct text sequence. For example, given an input data unit { name: \u201cJohn\u201d, phone:\n\u201c9876543210\u201d, email: \u201cjohn@example.com\", address:\n\u201c404, Cyber Street\" }, the aim may be to generate a text description like \"You can connect with John at 9876543210\nor john@example.com\".\nLarge Language Models (LLMs) like GPT-4 have recently\ntaken the NLP community by storm [7], [8]. A prominent\ntask solved by these is presenting of factual data in the form\nof (a) natural language responses, or (b) structured markup\nlike tables [9]. However, LLMs are unsuitable for on-device\nintegration due to large size (large number of parameters) and\nhigh latency. Moreover, a server-client deployment introduces\nthe risk of exposing end user's private data.\nContributions from recent NLP research have given an\nimpetus to D2T generation task. Gardent et al. [2] proposed\nWebNLG task to generate text descriptions of the given input\ntriplets describing facts. Around the same time, Novikova et al.\n[1] proposed E2E task to form restaurant reviews according\nto the given restaurant meaning representations (MRs).\nNoticeably, many applications of D2T, like knowledge-based\ndialog systems, struggle with Out-of-Vocabulary (OOV)\nwords that are frequent in input entities like Personally\nIdentifiable Information (PII) or dates. Thus, these typical\nD2T generation tasks require fetching the relevant entities\nfrom input and adding them into the output sequence, referred\nto as \"copying mechanism\" [10].\nHowever, while the key challenge in D2T generation\nremains the surface realization of content, an important\nshortcoming of existing proposals arises from the fact that\nmost approaches, ranging from simple template-based ones"}, {"title": "II. RELATED WORK", "content": "The concept of generating semantically coherent natural\nlanguage descriptions from non-linguistic data has been\nexplored since NLG became of interest among researchers\nworldwide. Owing to its versatile applicability, data-to-text\ngeneration has been researched and deployed in various\ndomains including statistical data summarization [11], health\ncare [12], stock market [13] and many more [14]\u2013[16]. The\nearly stages of research in this field involve simple rule-based\napproaches [12], [17], [18]. Although these are efficient for\nsimple tasks, they fail to work in complex scenarios [19]. A\nmajor drawback is that they require linguistic expert skills to\nformulate rules to achieve better performances.\na) Sequence-to-Sequence paradigm: The Seq2Seq\nparadigm is essentially based on the encoder-decoder frame-\nwork. The emergence of Seq2Seq networks and methods led\nto a significant change in the quality of data-to-text generation\nand overcame shortcomings found in rule-based systems.\nMei et al. [14] tackle the shortcomings of template-based\napproaches by introducing an LSTM-based encoder-aligner-\ndecoder architecture, outperforming previous results by 58.8%\nin the BLEU score on the Weather-Gov dataset [20]. Attention\nmechanisms [21] were introduced in the following years\ngaining immense popularity in neural network-based research.\nSeq2Seq models have also demonstrated their effectiveness\non response generation task. To improve user experience with\nchatbots where typical response generation may lead to trivial\nconversations, topic-aware sequence-to-sequence architecture\nwas introduced by Xing et al. [22]. More informative and\ninteresting responses are generated using a GRU based\nSeq2Seq architecture incorporating joint attention. Dziri et al.\n[23] introduced a hierarchical joint attention neural model that\nconsiders both context and topical attention to produce diverse\nand contextual responses compared to the previous baselines.\nAlthough neural networks achieve remarkable results in this\nfield of NLG, a significant setback in these approaches is the\npresence of many \"Unknown\" tokens in the final generated\noutputs, mainly due to the presence of Personally Identifiable\nInformation (PII), which may be required to be replicated in\nthe outputs as well."}, {"title": "III. MODEL DESCRIPTION", "content": "We formulate our data-to-text generation task as:\n$T: (K,I,D_e) \\rightarrow S_{g,c}$ (1)\nwhere in each input sample, $K$ is a set of words,\n${K_1, K_2,\\dots, K_{|x|}}$ , one of which can be an optional trig-\nger input, $I$ stands for intent class set, ${I_1, I_2,\\cdots, I_{|z|}}$,\n$D_e$ represents the set of \u201ccontext dictionary\u201d vectors,\n$(D_1, D_2,..., D_{|F|})$ , where each element $D_i$ denotes the input\ntoken sequence for field $f_i$ in an ordered field set $F$, and $S_{g,c}$\nis the set of all text sequences, comprising of descriptions that\nexpresses relations in $D$. Here, the subscripts $g$ and $c$ indicate\nthe association of $S_{ge}$ and $D_e$ with \"generate\u201d and \u201ccopy\u201d that\nare discussed in Section III-C.\nThe TrICy architecture consists of two encoders and\none decoder. Encoder $E_I$ incorporates the intent information\nalong with an optional trigger input, while encoder $E_D$\nextracts the representation of input data record, namely fields,\n$f$, and their corresponding values, $v$. An example of $f$ and\n$v$ in a meeting schedule data sample may include [ Name,\nTime_start, Time_end, Location ] and [ \u201cJohn\u201d,\n\u201c11 am\u201d, \u201c12 pm\u201d, \u201c104 board room\" ] respectively. The\ndecoder conditions on the concatenated representation using\ndistributed attention and predicts the target sequence using\ncopy mechanism.\nEach encoder reads different input data vectors to produce\nsequences of hidden states, i.e. hidden states from encoder $E_I$,\n$h_{EI}$, and hidden states from encoder $E_D$, $H_{ED}$.\nThe embedding layers in $E_D$ embed sequences of $N$\nfield tokens, $X_f = (X_{f_1}, X_{f_2},\\cdots, X_{f_N})$, and value tokens,\n$X_v = (X_{v_1}, X_{v_2},\\cdots, X_{v_N})$, by using contextual word\nembeddings. A bidirectional LSTM further encodes the input\nembeddings to learn input data representation. These hidden"}, {"title": "B. Bahdanau Attentive Read", "content": "The attention mechanism was first introduced in Seq2Seq\nto utilize the most relevant parts of the input sequence [21],\n[41]. Based on our ablation study (Section V-B), we utilize\nBahdanau et al. [21] in our final architecture, where we\ncompute the context vector as a weighted sum of the encoder\nhidden states. Thus, we use a context vector, $c_t$, to encode the\nentire source words that the system attends to for generating\nthe next word as defined below:\n$c_t = \\sum_{s=1}^{T} a_{ts}h_s$ (4)\n$a_{ts}=\\frac{exp (V' tanh (W_1s_{t-1} + W_2h_s))}{\\Sigma_{s'} exp (V' tanh (W_1s_{t-1} + W_2h_{s'}))}$\nwhere, $h_s$ represents hidden states of the encoder, $s_{t-1}$ is\ndecoder's previous time step state, and $W_1$, $W_2$ are weight\nmatrices learnt during training via backpropagation."}, {"title": "C. Decoding with Generate and Copy", "content": "As illustrated in Fig. 4, TrICy is based on encoder-decoder\nframework and uses a copy mechanism [10]. The inputs are\ntransformed by Encoders (Section III-A) into representation\n$H$ that is then read by the decoder to generate the output se-\nquence. TrICy predicts output words based on the probability\ndistribution of the generate mode and the copy mode, where\nthe latter copies tokens from input sources (fields and values).", "A. Dataset": "The predicted word at time $t - 1$ is\nused to update the state at $t$. Calculating the decoder hidden\nstate at each time step includes two attention mechanisms\nover the encoded source: attentive read and selective read. The\nattentive read is the Bahdanau et al. [21] attention mechanism\n(as discussed in Section III-B).\nIn selective read, we use weighted sum over the encoded\nsource states as well as the location-specific hidden state, $H$,\nof the predicted word in previous time step. Thus, we use\nthe copy probabilities assigned to each source token from the\nprevious decoder step, normalize them to sum to one, and\nthen use those to calculate a different weighted sum of the\nencoded states. Hence, the decoded state, $s_t$, is a function\nof the attentive read, selective read, and the predicted output\ntoken from the previous time step.", "a) Decoder state:": null}, {"title": "IV. EXPERIMENTAL SETUP", "content": "We use two public datasets and one Custom dataset for\nevaluation, as summarized in Table I:\na) E2E NLG [1]: The dataset contains pairs of meaning\nrepresentations (MR) and utterances in the restaurant domain,\nwhere an MR can include a subset of 8 fields.", "a) Prediction:": "The decoder utilizes to predict\nthe output sequence. We consider a vocabulary $V =\n{x_1, x_2,...,x_{|v|}}$ . Furthermore, we maintain unique words in\nthe input source, $X$, which may contain words, not in $V$. Then,\nthe extended vocabulary for source ($X_f \\cup X_v$) is ($V \\cup X$). In\nparticular, the probability of a token $y_t$ from the extended\nvocab at the $t^{th}$ decoding step is given as:\n$P(y_t) = P_{generate} (y_t |\\bigodot) + P_{copy} (y_t |\\bigodot)$ (5)\nwhere, $\\bigodot$ represents state of the decoder, which includes $s_{t-1}$,\n$y_{t-1}$, $c_t$, and $H$. Let $V_{generate}$, $V_{copy}$ be score functions: $V_{generate}$\nbeing the same as in the generic Bahdanau et al. [21] RNN\ndecoder-encoder, and copy using a tanh transformation, as in\nGu et al. [10]. Then, the final probability is:\n$P(y_t | \\bigodot) =\\begin{cases}\\frac{e^{generate} (y_t)}{\\sum_{v \\in V} e^{generate} (v)} & \\text{if } y_t \\in V\\\\\\frac{\\sum_{j:x_j=y_t} e^{copy} (x_j)}{\\sum_{x \\in X - V} e^{copy} (x)} & \\text{if } y_t \\in X - V\\end{cases}$ (6)\nwhere $Z$ is a normalizing factor, as given by:\n$Z = \\sum_{v \\in V} e^{generate} (v) + \\sum_{x \\in X} e^{copy} (x)$ (7)\nThe two decoding modes compete through a softmax $(\\sigma)$\nfunction due to the normalization term. Further, we add the\nprobabilities of all $x_j$ equal to $y_t$ in Equation (6) considering\nthat there may be multiple input words for decoding $y_t$. It\nmay be noted that $P_{copy} (y_t | \\bigodot) = 0$ if $y_t$ is not present in\nthe input words, whereas $P_{generate} (y_t | \\bigodot) = 0$ when $y_t$ only\nappears in the input."}, {"title": "D. Trigger input, K", "content": "To further control D2T generation, we introduce an optional\ninput, namely, trigger. Trigger is intended to be an input\nfeature during training, and hence, comprises of vocabulary\nwords. Moreover, it leads the output sequence as in Fig. 3. This\nis in contrast to prompting that typically alters the behavior\nof PLMs [40], seldom plays a role in their training [42], and\nneeds to be engineered either manually or automatically [39].\nBy adding a text token as an additional input for a portion of\nthe training set, we train the model to get directed predictions.\nWe define the ratio of training samples that are augmented with\ntrigger tokens as $t_{rx}$, and that of test or evaluation samples\nas $e_{rk}$. In our implementation, we use the first word of $S_{g,c}$\nas the trigger in the training set. For the samples that are left\nunaugmented, we use a special token, \u201c<SOS>\u201d as a start-of-\nsentence placeholder. We present an algorithm to compute the\noptimal setting for the usage of triggers ($t_r$) in Section V-C."}, {"title": "V. EVALUATION RESULTS", "content": "At the outset, we present a comparison of TrICy models\nwith SOTA models in section V-A. Then, we present an\nalysis of our architecture components through an ablation\nstudy in section V-B. We showcase how the optimal variant of\nTrICy, $M$, is arrived at in section V-C. Thereafter, we present\nfurther analysis and insights in the subsequent sections.\n1) On E2E NLG dataset: Table III presents a sample of\ntext sequences generated by TrICy and related work [43]\u2013[46]\nwhen trained on E2E dataset. Among LLMs, we select Llama\n2 as it the weights are open-source. Instead of exploring a\nvariety of advanced prompt engineering methods, which have\nbeen shown to be less effective with structured data [50],\nwe select a simplistic prompt capturing the values in $D_e$,\nas the effectiveness of this approach for D2T tasks has been\nestablished by Ferreira et al. [33]. We fine-tune the Llama 2\n7B model on the train split [1], on an NVIDIA RTX A6000\nGPU with 48 GB RAM, with the configuration detailed in\nAppendix A.\nFrom the aggregated BLEU scores in Table III, we observe\nthat when trigger is absent in test samples (i.e. class $C_1$),\nour $M_7$ model (introduced in Table V) performs better\nthan the former two approaches and comparable to the\npreviously known SOTA4 for this dataset [46]. We show\nthat, as expected, training TrICy with triggers (using leading\nwords from gold reference) supplied for all training samples\n($M$) leads to a sharp drop in $C_1$ as the model heavily relies\non the presence of trigger in input which is absent in test\nset. However, our best TrICy model, $M$, trained with the", "A. Comparison with SOTA": null}, {"title": "C. K triggers and the search for an optimum ratio, $t_r$", "content": "We conduct performance analysis on models trained with\nmultiple configurations based on $t_r$ introduced in Section\nIII-D. The BLEU and ROUGE-L values observed during\nthe evaluation of 5 notable model configurations, covering\n6 of the intent classes in the Custom dataset, are presented\nin Fig. 6. Here, in each configuration, the $t_{rk}$ is set to a\nconstant value, and the mean $(\\mu)$ BLEU and ROUGE-L F1\nscores are evaluated for two extreme scenarios of test set\n(i) where trigger word is absent in all evaluation samples, i.e.,\n$e_{TK} = 0.0$, denoted by 0K, and (ii) where trigger word is\npresent in all evaluation samples, i.e., $e_{rx} = 1.0$, denoted by\n+K. We may observe that for all values of $t_{rk}$, providing a\ntrigger input dramatically improves the BLEU and ROUGE-L\nscores. Furthermore, the improvement due to the introduction\nof trigger increases as $t_{rx}$ increases. The improvement with\nthe +K evaluation set is expected as intuitively increasing the\npresence of trigger input in training data trains the model to\npredict a directed text sequence that can be more aligned with"}, {"title": "D. Discussion and Applications", "content": "1) Controlled investigation into the effect of K, I, $D_e$:\nTable VI illustrates sample outputs from TrICy when trained\non the Custom dataset. We see that by simply varying input\nfor I (like CONTACT::SHARE \u2192 CONTACT::ACT), while\nretaining the same $D_e$ and K, output $S_{ge}$ is either a human-\nreadable text sequence or a markup sequence that can enable\nrendering of intelligent action prompts (like \"call a person\",\n\"edit a scheduled meeting\", etc.) on edge devices. Moreover,\nthe presence of trigger K leads to variations in output gener-\nations, as evident from the tabulated examples.\n2) Insights from User Trial: To evaluate the end-user per-\nspective of TrICy-generated output sequences, we conduct a\nclosed user trial using a custom Android app with $N_U (= 48)$\nusers. After going through a list of illustrated instructions\nand consenting to participation, each user is presented with\n$N_S(= 100)$ test samples. In each of these samples, the\nintent (like CONTACT::ACT or CALENDAR::SHARE) is pre-\nselected at random by the app, and a relevant information\nsample (like details of a contact or calendar event) is extracted\nfrom the existing user data. These are then passed to the on-\ndevice TrICy engine to generate a response sequence. The\nuser may accept the default suggestion (event = $A_d$) or add a\ntrigger text to request an alternative sequence. Thereafter, the\nuser can either accept the alternative sequence (event = $A_a$)\nor reject it (event = $A_r=A_d \\gt A_a=A_d \\lor A_a$). We then\naggregate these results across various intents and users and\ncompute the metrics, $P(A_a)$ and $P (A_a | A_d)$:\n$P(A_d) = \\frac{\\text{User accepts default suggestion}}{N_U x N_S}$ (11)\n$P (A_a | A_d) = \\frac{\\text{User accepts alternative suggestion}}{\\text{User accepts default suggestion}}$\nas the heuristic weight in Equation (10) for averaging the two\ncurves, and as depicted in Fig. 7, we compute $t_{rx}$ to be 0.33\nfor Custom dataset and 0.65 for E2E dataset. Similarly, we\ncompute $t_{rx}$ to be 0.24 for WebNLG dataset."}, {"title": "VI. CONCLUSION", "content": "We propose a novel D2T generation framework, TrICy,\nwhich utilizes the intent information and optional triggers for\ngenerating diverse and contextual outputs. We evaluate the\neffectiveness of attention+copy and intent through standard\nobjective metrics against two public and one Custom datasets.\nFurthermore, we explore the effect of proposed architecture\ncomponents through an ablation study in Section V-B. Our\nextensive experiments on trigger-guided training in Section\nV-C present a method to compute the optimal trigger ratio\nfor any training set. Leveraging message intent and triggers,\nour best TrICy model achieves the new SOTA performance\nwith lowest memory footprint."}, {"title": "LIMITATIONS", "content": "Our detailed experiments focus on English datasets and\ndo not evaluate other languages. Hallucination is a crucial\nproblem in natural language generation where the generated\ncontent is unfaithful to the provided source content. Data-to-\ntext generation is particularly prone to this [60]. Quantifying\nthe effect of such issues concerning triggers is not discussed\nin this paper, and we intend to explore this in future work."}, {"title": "ETHICAL CONSIDERATIONS", "content": "In this paper, we propose to generate syntactically correct\nsequences based on these inputs: data record, intent, and\noptional trigger. The flexibility offered by trigger input\nmay lead to increased vulnerability to adversarial attacks.\nHowever, we expect this to be minimizable by limiting the\nset of acceptable triggers. For all contributors taking part in\ndata curation and user trial, the due consent process has been\nundertaken for the purpose of this work."}]}