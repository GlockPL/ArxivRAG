{"title": "DEUCE: Dual-diversity Enhancement and Uncertainty-awareness\nfor Cold-start Active Learning", "authors": ["Jiaxin Guo", "C. L. Philip Chen", "Shuzhen Li", "Tong Zhang"], "abstract": "Cold-start active learning (CSAL) selects valuable instances from an unlabeled dataset for manual annotation. It provides high-quality data at a low annotation cost for label-scarce text classification. However, existing CSAL methods overlook weak classes and hard representative examples, resulting in biased learning. To address these issues, this paper proposes a novel dual-diversity enhancing and uncertainty-aware (DEUCE) framework for CSAL. Specifically, DEUCE leverages a pre-trained language model (PLM) to efficiently extract textual representations, class predictions, and predictive uncertainty. Then, it constructs a Dual-Neighbor Graph (DNG) to combine information on both textual diversity and class diversity, ensuring a balanced data distribution. It further propagates uncertainty information via density-based clustering to select hard representative instances. DEUCE performs well in selecting class-balanced and hard representative data by dual-diversity and informativeness. Experiments on six NLP datasets demonstrate the superiority and efficiency of DEUCE.", "sections": [{"title": "Introduction", "content": "Cold-start active learning (CSAL; Yuan et al., 2020a; Zhang et al., 2022b) has gained much attention for efficiently labeling large corpora from zero. Given an unlabeled corpus (i.e., the \u201ccold-start\" stage), it aims to acquire a small subset (seed set) for annotation. Such absence of labels can happen due to data privacy concerns (Holzinger,\n2016; Li et al., 2023), limited domain experts\u00b9 (Wu et al., 2022), labeling difficulty (Herde et al., 2021), quick expiration of labels (Yuan et al., 2020b; Zhang et al., 2021), etc. In real-world tasks with specialized domains (e.g., medical report classification with rare diseases; De Angeli et al., 2021), the complete absence of labels and lack of a posteriori knowledge pose challenges to CSAL.\nWhile active learning (AL) has been studied for a wide range of NLP tasks (Zhang et al., 2022b), the cold-start problem has been hardly addressed. At the cold-start stage, the model is untrained and no labeled data are available for validation. Traditional CSAL applies random sampling (Ash et al., 2020; Margatina et al., 2021), diversity sampling (Yu et al., 2019; Chang et al., 2021), or uncertainty sampling (Schr\u00f6der et al., 2022). However, random sampling suffers from high variance (Rudolph et al., 2023); diversity sampling is prone to easy examples and vector space noise (Eklund and Forsman, 2022); uncertainty sampling is prone to redundant examples, outliers, and unreliable metrics (W\u00f3jcik et al., 2022). Moreover, existing methods ignore class diversity, where the sampling bias often results in class imbalance (Krishnan et al., 2021). At worst, the missed cluster effect (Sch\u00fctze et al., 2006; Yu et al., 2019) can happen, i.e., clusters of weak classes are neglected. Tomanek et al. (2009) showed that an unrepresentative seed set gives rise to this effect. Learning is misguided, if started unfavorably.\nThe key challenge for CSAL lies in how to acquire a diverse and informative seed set. As a general heuristic (Dasgupta, 2011), a proper seed set\nshould strike a balance between exploring the input space for instance regions (e.g., diversity sampling) and exploiting the version space for decision boundaries (e.g., uncertainty sampling). Such hybrid CSAL strategies have been proposed based on combinations of neighbor-awareness (Hacohen et al., 2022; Su et al., 2023; Yu et al., 2023), clustering (Yuan et al., 2020a; Agarwal et al., 2021; M\u00fcller et al., 2022; Brangbour et al., 2022; Shnarch et al., 2022; Yu et al., 2023), and uncertainty estimation (Dligach and Palmer, 2011; Yuan et al., 2020a; M\u00fcller et al., 2022; Yu et al., 2023). However, existing methods fail to explore the label space to enhance class diversity and mitigate imbalance. Moreover, most methods perform diversity sampling followed by uncertainty sampling, treating both aspects in isolation.\nTo address these challenges, this paper presents DEUCE, a dual-diversity enhancing and uncertainty-aware framework for CSAL. It adopts a graph-based hybrid strategy to enhance diversity and informativeness. Different from previous works, DEUCE not only emphasizes the diversity in textual contents (textual diversity), but also diversity in class predictions (class diversity). This is termed dual-diversity in this paper. To achieve this in the cold-start stage, it exploits the rich representational and predictive capabilities of PLMs. For informativeness, the predictive uncertainty is estimated from a one-vs-all (OVA) perspective. This helps mining informative \u201chard examples\" for learning. Then, DEUCE further employs manifold learning techniques (McInnes et al., 2020) to derive dual-diversity information. This results in the novel construction of a Dual-Neighbor Graph (DNG). Finally, DEUCE performs density-based uncertainty propagation and Farthest Point Sampling (FPS) on the DNG. While propagation prioritizes representatively uncertain (RU) instances, FPS enhances the dual-diversity. Overall, DEUCE ensures a more diverse and informative acquisition.\nThe merits of DEUCE are attributed to the following contributions:\n\u2022 The dual-diversity enhancing and uncertainty aware (DEUCE) framework adopts a novel hybrid acquisition strategy. It effectively selects class-balanced and hard representative instances, achieving a good balance between exploration and exploitation in CSAL.\n\u2022 This paper proposes a graph-based dual-"}, {"title": "Related Work", "content": "2.1 Cold-start Active Learning (CSAL)\nAccording to the taxonomy of Zhang et al. (2022b), CSAL research for NLP can be categorized as informativeness-based, representativeness-based, and hybrid. As most methods are hybrid, the techniques and challenges for informativeness or representativeness are elucidated below.\n2.1.1 Informativeness\nUncertainty. The main metric for informativeness in CSAL is uncertainty, as it is more tractable in cold-start stages than others (e.g., gradients). High predictive uncertainty indicates difficulty for the model, thus valuable for annotation. Most existing methods use language models (LMs) for estimation. Common estimators include entropy (Zhu et al., 2008; Yu et al., 2023), LM probability (Dligach and Palmer, 2011), LM loss (Yuan et al., 2020a), and probability margin (M\u00fcller et al., 2022). However, several challenges exist in uncertainty estimation: (a) Often, a closed-world assumption is imposed. In other words, predictions are normalized such that they sum to 1. This hinders the expression of uncertainty, as it forces mapping to one of the known classes, ignoring options such as \u201cnone of the above\" (Padhy et al., 2020). (b) PLMs suffer from overconfidence (Park and Caragea, 2022; Wang, 2024). This requires calibration for more robust uncertainty estimation (Yu et al., 2023). (c) Task information is hardly considered. As a result, the uncertainty will not be related to the downstream task (output uncertainty), but rather its intrinsic perplexity (input uncertainty) (Jiang et al., 2021). PATRON (Yu et al., 2023) uses task-related prompts to tackle this issue.\n2.1.2 Representativeness\nDensity. To avoid outliers, density-based CSAL methods prefer \"typical\u201d instances. The method of Zhu et al. (2008) and TypiClust (Hacohen et al.,"}, {"title": "Missed Cluster Effect", "content": "The missed cluster effect (Sch\u00fctze et al., 2006; Tomanek et al., 2009) is an extreme case of class imbalance. It refers to when an AL strategy neglects certain classes (or clusters within classes). Sch\u00fctze et al. (2006) first recognized the missed cluster effect in the context of text classification. They suggested more use of domain knowledge. Knowledge extraction from PLMs is in harmony with this suggestion. Dligach and Palmer (2011) proposed an uncertainty-based approach to avoid the missed cluster effect in word sense disambiguation (WSD). However, it is based on task-agnostic LM probability. Marcheggiani and Arti\u00e8res (2014) showed that labeling relevant in-"}, {"title": "Methodology", "content": "In this section, the methodology of the proposed DEUCE is introduced. Section 3.1 first defines CSAL and declares the notations for the rest of this paper. The framework of DEUCE is then elaborated in Section 3.2."}, {"title": "Problem Formulation", "content": "This paper considers CSAL in a pool-based manner. Learning is initiated with a set of N unlabeled documents, $X := {x_i}_{i=1}^{N}$. A C-way text classification task is defined by a set of classes $Y := {y_j}_{j=1}^{C}$ taking values in a domain Y.\nGiven a labeling budget b < N, a CSAL strategy acquires a subset $X_s \\subset X$ with a fixed size $|X_s| = b$, such that the labeled subset $X_s$ boosts most performance when used as a training seed set. The performance is evaluated by fine-tuning a PLM $M_{\\theta}$ with $X_s$, and testing for its accuracy."}, {"title": "The DEUCE Framework", "content": "The proposed DEUCE framework is illustrated in Figure 1. Overall, the components of DEUCE serve the same goal\u2014to produce a seed set with high dual-diversity and informativeness.\n3.2.1 Embedding Module\nIn CSAL, data selection starts with only an unlabeled corpus. DEUCE leverages PLM embeddings, which guide the selection process towards more diverse and informative samples.\nSpecifically, the embedding module implements a prompt-based, verbalizer-free approach (Jiang et al., 2022). This requires only a single inference pass per document.\nTextual and predictive embedding. In a masked PLM, the bidirectional semantics can be condensed into a [MASK] token. In light of this, DEUCE extends Jiang et al. (2022)'s template\nwith double [MASK] tokens:\n$T_x := $\\newline This sentence: \"[X]\" means [MASK].\\newline Its [DOMAIN] is [MASK]. , \nwhere [DOMAIN] is the target domain Y, such as \"sentiment\". The hidden representations of [MASK] tokens are extracted as the textual $z_{x_i}$ and predictive embeddings $z_{\\hat{y}x_i}$. They capture the intrinsic and task-related semantics.\nHowever, raw embeddings suffer from template bias and length bias (Miao et al., 2023). DEUCE further applies template denoising (Jiang et al., 2022) to obtain the denoised embeddings $\\tilde{z}$.\nClass embedding. Predictions need to be paired with the known classes. Class embeddings $\\tilde{z}_{y_j}$ are generated from a prompt template $T_y$, similar to $T_x$:\n$T_y :=$ This [DOMAIN]: \u201c[Y]\u201d means [MASK]. , \nwhere [Y] is the placeholder for a class $y_j$.\n3.2.2 Prediction Module\nThis module aims to produce uncertainty-aware labels. With class information, DEUCE gains prior"}, {"title": "Dual-Neighbor Graph (DNG) Module", "content": "Graphs serve as a powerful tool for data selection by explicitly modeling data interrelationship. This enables the propagation of valuable information (e.g., uncertainty) and the selection of more diverse samples. To integrate textual and class diversity, DEUCE leverages manifold learning techniques (McInnes et al., 2020) on k-Nearest-Neighbor (kNN) graphs of both spaces.2\nkNN graph. The use of kNN arises from the neighborhood perspective of diversity. DEUCE aims to avoid selecting neighboring instances. In a kNN graph, an instance $x_i$ is connected with its k nearest neighbors ${x_{ij}}$ under some distance function $\u2206(\u00b7,\u00b7)$. Formally, the two metric spaces of kNN are defined as follows.\n\u2022 The textual space (X, $\u2206_z$) is defined by textual embeddings under co\u015fine distance, $\u2206_z(x_i, x_j) = arccos \\frac{\\tilde{z}_{x_i}\\tilde{z}_{x_j}}{\\|\\tilde{z}_{x_i}\\|\\|\\tilde{z}_{x_j}\\|}$.\n\u2022 The label space (X, $\u2206_{\\hat{y}}$) is defined by label vectors under l\u2081 distance, $\u2206_{\\hat{y}}(x_i, x_j) = \\|\\|\\hat{y}_i - \\hat{y}_j\\|\\|_1$.\nThe kNN graph from each space is denoted by $G_{\\tilde{z}}$ and $G_{\\hat{y}}$, respectively.\nGraph normalization. To unify textual and class diversity, DEUCE merges the two kNN graphs into one for graph-based sampling. However, across two distinct spaces, it is necessary to first normalize the distances (McInnes et al., 2020).\nTo ease notation, this part omits the subscript as $G \u2208 {G_{\\tilde{z}},G_{\\hat{y}}}$. For each $x_i$, DEUCE finds a normalization factor $\u03c4_i > 0$ that satisfies the equation\n$\\sum_{j=1}^{k} exp(\\frac{\u2206(x_i, x_{ij})}{\u03c4_i}) = log_2k$,\nwhere $p_i$ denotes $x_i$'s distance to its nearest neighbor. The weights $w$ of the normalized (directed) kNN graph $\\tilde{G}$, denoted by $\\tilde{G}$, is defined by\n$\\tilde{w}(i,(i,x_{ij}) = exp(\\frac{\u2206(x_i, x_{ij}) - p_i}{\u03c4_i})$.\nAfter normalization, the original kNN weights $w \u2208 [0, \u221e)$ are transformed to $\u1ff6 \u2208 (0,1]$.\nSymmetrization. To identify representative instances, DEUCE performs graph clustering. This requires symmetric kNN graphs.\nLet W denote the sparse weight matrix of $\\tilde{G}$. Since weights $\u1ff6 \u2208 [0,1]$, they can be interpreted as fuzzy memberships of neighborhood. Hence, symmetrizing W is equivalent to finding the fuzzy union (Dubois and Prade, 1982) of the neighbors W and reverse neighbors $W^T$:\n$W_{sym} = W \\sqcup W^T = W + W^T \u2013 W \\odot W^T$,\nwhere $\\odot$ is the Hadamard product. $W_{sym}$ defines the weights of the symmetric kNN graph $G_{sym}$. Its edges are denoted by $E_{sym}$.\nMerging. It is now appropriate to merge the two kNN graphs. This unifies textual and class diversity in one graph.\nAs merged, the Dual-Neighbor Graph (DNG) is an undirected graph $G_{dual} = (V, E, W_{dual})$. The edges $E$ are the union of edges in $G_{\\tilde{z},sym}$ and $G_{\\hat{y},sym}$. Moreover, E is divided into two types:\n\u2022 $E_1$ represents edges which only appear in either kNN graph, called single-neighbor edges;\n\u2022 $E_2$ represents edges which appear in both kNN graphs, called dual-neighbor edges. They connect neighboring documents which are similar in both textual semantics and class predictions."}, {"title": "Acquisition Module", "content": "DEUCE adopts a hybrid acquisition strategy. Overall, the goal is to produce a diverse and informative seed set. To achieve this, the acquisition module performs graph clustering, propagation, and traversal on DNG.\nHDBSCAN*. A group of similar documents with high predictive uncertainty indicates an area where the model's knowledge is lacking. By labeling one of the documents, the model predictions can be improved for similar ones in the area. Therefore, it is valuable to identify and prioritize such representatively uncertain (RU) groups for CSAL.\nClustering has been a common technique to group similar instances (\u00a72.1.2). However, traditional clustering methods (e.g., k-MEANS) are ill-suited, as the number of RU groups is unknown. Moreover, they force every instance into a cluster, while some instances may not belong to any RU group. Instead, DEUCE adopts density-based clustering, which identifies RU groups with a sufficient density (\u2265 $k_r$ similar documents).\nSpecifically, DEUCE applies HDBSCAN* (Campello et al., 2013, 2015) on the DNG, with minimum cluster size $k_r$. A document $x_i$ is either (a) clustered in an RU group $c_q$ with membership $p_i$, or (b) excluded as a non-RU outlier.\nUncertainty propagation. To prioritize RU documents, uncertainty information (\u00a73.2.2) is propagated and aggregated in RU groups. This is formulated as a single step of message propaga-tion:\n$u_i = u_i + \\sum_{x_j \\in c_q\\{x_i\\}} W_{dual}({x_i, x_j}) p_j u_j$.\nFPS. The final acquisition adopts a combination of diversity sampling and uncertainty sampling. First, DEUCE runs Farthest Point Sampling (FPS; Eldar et al., 1994) on the DNG. As the result only depends on the initial point, FPS is started from documents $x_i$ with top-k degrees. Each produces a candidate seed set $X^{(i)}$, which contains b dually diverse samples. Finally, DEUCE chooses the candidate with the highest propagated uncertainty:\n$X^* = arg max\\sum_{X^{(i)}} \\sum_{x_j \\in X^{(i)}} \\tilde{u}_j$.\nThe whole process is described in Algorithm 1."}, {"title": "Experiments and Results", "content": "4.1 Experimental Setup\nDatasets. DEUCE is evaluated on six text classification datasets: IMDb (Maas et al., 2011), Yelpfull (Meng et al., 2019), AG's News (Zhang et al., 2015), Yahoo! Answers (Zhang et al., 2015), DBpedia (Lehmann et al., 2015), and TREC (Li and Roth, 2002). Dataset statistics are shown in Table 1. All the datasets used in the experiments are publicly accessible. The original labels are removed to create a cold-start scenario.\nEvaluation metric. To evaluate the performance of the acquired seed set $X_s$, it is labeled and used for fine-tuning the PLM. The original labels of the seed set are revealed. The accuracy of the fine-tuned PLM on the test set is then reported. To be consistent with previous methods (Yu et al., 2023), the experiments adopt RoBERTa-base (Liu et al., 2019) as the backbone PLM.\nAnalysis metrics. To analyze the effect of dual-diversity enhancement, the class imbalance (IMB) and textual-diversity value of seed sets are reported. Both metrics are computed under budget b = 128. IMB (Yu et al., 2023) is defined as:\n$IMB = \\frac{max_{j=1}^{C}n_j}{min_{j=1}^{C}n_j}$,\nwhere $n_j$ is the number of instances from class $y_j$. Textual-diversity value (Ein-Dor et al., 2020; Yu et al., 2023) is defined as:\n$D = \\frac{1}{\\|X\\setminus X_s\\|} \\sum_{x_i\\in X\\setminus X_s} min_{x_j \\in X_s} \u2206(x_i,x_j)$,\nwhere $\u2206(x_i, x_j)$ is the Euclidean distance of SimCSE embeddings (Gao et al., 2021) of $x_i$ and $x_j$."}, {"title": "Accuracy Improvement", "content": "The main quantitative results of PLM fine-tuning performance with DEUCE and baseline CSAL methods are shown in Table 3. Results for baselines other than VOTE-k are from Yu et al. (2023). To report the standard deviation, each setup is repeated with 10 different random seeds. Figure 2 demonstrates a qualitative visualization of the b = 128 seed set from IMDb dataset, acquired by the latest baseline method VOTE-k and the proposed\nDEUCE. The t-SNE (van der Maaten and Hinton, 2008) method is used for visualization.\nFrom results in Table 3, it can be seen that DEUCE consistently outperforms other baselines, achieving up to a 2.5% gain on balanced datasets and up to 6.2% on the imbalanced dataset, TREC. DEUCE mainly benefits from that it enhances the class diversity as well as textual diversity. This can be concluded from the larger improvements on TREC. In over half of the setups, DEUCE also achieves the lowest standard deviation. In addition, DEUCE improves most when b is small. This"}, {"title": "Enhancement of Class Diversity", "content": "To verify the enhancement of class diversity, the class imbalance value (IMB; Yu et al., 2023) under b = 128 is reported in Table 4.\nFrom Table 4, it can be seen that DEUCE achieves the lowest average IMB value. This indicates that DEUCE enhances class diversity properly. In contrast, an IMB of \u221e emerges in the pure uncertainty-based (Entropy) and textual-diversity-based (Coreset) method. This indicates the missed cluster effect happens in their acquisition."}, {"title": "Enhancement of Textual Diversity", "content": "To measure the textual diversity of seed sets, the textual-diversity value (Ein-Dor et al., 2020; Yu et al., 2023) under b = 128 is reported in Table 5. Table 5 shows that DEUCE also achieves the highest average textual-diversity value. This indicates that DEUCE also enhances textual diversity properly. The improvement of textual-diversity value is not significant, compared to IMB value's (Table 4). This signals that DEUCE enhances more of class diversity than textual diversity, compared to other baselines. Such difference can be explained by the highest-uncertainty-candidate strategy, which acquires more information from the label space."}, {"title": "Quality of Textual Embedding", "content": "To analyze the quality of DEUCE's prompt-based, unsupervised text embeddings $z_{x_i}$ (\u00a73.2.1), they are compared with the supervised Sentence Transformer embeddings (Sentence Transformers, 2024) used in VOTE-k (Su et al., 2023). The correlations are computed across all the possible $\\binom{N}{2}$ pairs of their cosine similarity. Results on three datasets are reported in Table 6.\nFrom Table 6, a weak positive correlation is observed. Moreover, template denoising produces better embeddings, as it removes the biases from raw embeddings. Overall, the quality of textual embeddings is acceptable and adequate for cold-start acquisition."}, {"title": "Quality of Class Prediction", "content": "To analyze the quality of embedding-based class prediction $\\hat{y}$ (\u00a73.2.2), they are compared with gold labels. As uncertainty indicates unstable predictions, labels are arranged from the most confident (lowest $u_i$) to the least. Results are demonstrated in Figure 3."}, {"title": "Discussion", "content": "5.1 Comparison with LLM-based Methods\nThe landscape of NLP is rapidly evolving with generative large language models (LLMs). This section evaluates two potential LLM-based alternatives to DEUCE: serialization for acquisition and zero-shot Chain-of-Thought prompting. The following experiments are conducted with LLAMA 2 7B (Touvron et al., 2023).\n5.1.1 Serialization for Acquisition\nInspired by the work of Hegselmann et al. (2023), class and uncertainty information can be serialized into natural language for LLM-based acquisition. The process is designed to involve three passes. In the first pass, each unlabeled text is formalized as a multiple-choice problem for LLM. The prompt template T\u2081 is used to collect class and uncertainty information:\n$T_1:=$ This sentence: \u201c[X]\u201d What is its [DOMAIN]?\nAnswer Choices: (A) [CLASS A] (B) ...\nAnswer: (\nIn the second pass, LLM decides on whether each text should be selected. Predictive uncertainty is estimated by the entropy of first-pass predictions, bounded by log C. The extended template T2 is used to combine multiple information:\n$T_2 :=$ This sentence: \u201c[X]\u201d What is its [DOMAIN]?\nAnswer Choices: (A) [CLASS A] (B) ...\nAnswer: ([ANSWER]) [CLASS]\nUncertainty: [UNCERTAINTY %]\nIs it valuable for annotation? Yes or no?\nAnswer:\nIn the third pass, texts with top-b probabilities of T2 answered \"yes\" are selected as the seed set. LLM is then fine-tuned with the seed set under T\u2081. Finally, T\u2081 is applied on the fine-tuned LLM to report the test set accuracy.\nDue to resource constraints, LoRA (Hu et al., 2022) is used for fine-tuning, with r = a = 64. Results are reported in Table 7. Despite utilizing a mid-sized PLM, DEUCE outperforms serialization with LLM in most datasets. The decision process of LLM is also black-box. In contrast, DEUCE adopts graphs to explicitly capture the interplay of information, offering better interpretability.\n5.1.2 Zero-shot Chain-of-Thought\nZero-shot Chain-of-Thought (CoT) prompting (Kojima et al., 2022) with LLMs has emerged as a promising method in cold-start scenarios. This paper tests zero-shot CoT without and with explicit choices in prompts. The temperature of generation is set to 0, and a maximum of 256 tokens are generated. Results are shown in Table 8. From the results, fine-tuning PLM with DEUCE still outperforms 0-shot LLM predictions. In class-imbalanced and difficult datasets, performance gaps are greater. Lemon-picking shows"}, {"title": "Effect of Labeling Noise", "content": "Real-world annotations often involve noise. Northcutt et al. (2021) estimated an average of 2.6% labeling errors across 3 commonly-used NLP datasets. To evaluate DEUCE under labeling noise, experiments with artificial errors are conducted. As the gold labels may already contain around 3% errors, 7% of seed labels are randomly replaced by wrong labels. The final sets are expected to exhibit an error level of 4\u201310%. Results are reported in Table 10.\nFrom the results, a decrease in accuracy and an increase in standard deviation occur as expected. However, DEUCE still outperforms 0-shot CoT (Table 8) in nearly all setups, despite the added noise. This shows the robustness of DEUCE for fine-tuning to labeling noise."}, {"title": "Effect of Class Prediction Failure", "content": "For real-world cold-start tasks, the knowledge about classes might not be well exploited by the PLM. In the worst case, the PLM can fail to generate meaningful class predictions. To simulate this scenario, ablation experiments with random class predictions are conducted. In this setup, the predictive embeddings $z_{\\hat{y}x_i}$ are replaced with random vectors. This ablates class predictions. Results are reported in Table 11.\nAs class and uncertainty information are disarranged, DEUCE degenerates to single textual diversity and performance degradation occurs as expected. Nonetheless, DEUCE still outperforms Coreset selection (Sener and Savarese, 2018), a CSAL baseline which also purely utilizes textual diversity. This demonstrates DEUCE's effective-"}, {"title": "Performance of Few-shot Math Reasoning", "content": "DEUCE has the potential to generalize on other NLP tasks. To demonstrate this, DEUCE is tested on GSM8K (Cobbe et al., 2021), a dataset of math word problems. However, directly adapting RoBERTa to solving math problems is difficult due to its masked modeling nature. Instead, DEUCE is applied with RoBERTa to produce a seed set. Then, the seeds are taken as examples for few-shot Chain-of-Thought prompting (Wei et al., 2022) with LLAMA 2 7B. From the results,"}, {"title": "Conclusion", "content": "This paper presents DEUCE, a dual-diversity enhancing and uncertainty-aware CSAL framework via a prompt-based and graph-based approach. Different from previous works, it emphasizes dual-diversity (i.e., textual diversity and class diversity) to ensure a balanced acquisition. This is achieved by the novel construction of Dual-Neighbor Graph (DNG) and Farthest Point Sampling (FPS). DNG leverages the kNN graph structure of textual space and label space from a PLM. In addition, DEUCE prioritizes hard representative examples, so as to ensure an informative acquisition. This leverages density-based clustering and uncertainty propagation on the DNG. Experiments show the effectiveness of DEUCE's dual-diversity enhancement and uncertainty-aware mechanism. It offers an efficient solution for low-resource data acquisition. Overall, DEUCE's hybrid strategy strikes an important balance between exploration and exploitation in CSAL."}, {"title": "Limitations", "content": "Backbone LM. DEUCE leverages a discriminative PLM. However, state-of-the-art PLMs are primarily generative. Generative embedding models (e.g., Jiang et al., 2023) or adaptations (Yang et al., 2019; Gong et al., 2019; Zhang et al., 2022a) can be investigated and combined with DEUCE. For such approaches, their quality and efficiency should be carefully minded.\nExternal knowledge. In DEUCE, the only source of external knowledge is the language model. Incorporation of more domain knowledge, if possible, can improve the performance in the cold-start stage. As DEUCE adopts a prompt-based and graph-based acquisition, prompt engineering and knowledge graphs (Pan et al., 2024) can be investigated."}]}