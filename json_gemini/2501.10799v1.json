{"title": "Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback", "authors": ["Yen-Ting Lin", "Di Jin", "Tengyu Xu", "Tianhao Wu", "Sainbayar Sukhbaatar", "Chen Zhu", "Yun He", "Yun-Nung Chen", "Jason Weston", "Yuandong Tian", "Arash Rahnama", "Sinong Wang", "Hao Ma", "Han Fang"], "abstract": "Large language models (LLMs) have recently demonstrated remarkable success in mathematical reasoning. Despite progress in methods like chain-of-thought prompting and self-consistency sampling, these advances often focus on final correctness without ensuring that the underlying reasoning process is coherent and reliable. This paper introduces STEP-KTO, a training framework that combines process-level and outcome-level binary feedback to guide LLMs toward more trustworthy reasoning trajectories. By providing binary evaluations for both the intermediate reasoning steps and the final answer, STEP-KTO encourages the model to adhere to logical progressions rather than relying on superficial shortcuts. Our experiments on challenging mathematical benchmarks show that STEP-KTO significantly improves both final answer accuracy and the quality of intermediate reasoning steps. For example, on the MATH-500 dataset, STEP-KTO achieves a notable improvement in Pass@1 accuracy over strong baselines. These results highlight the promise of integrating stepwise process feedback into LLM training, paving the way toward more interpretable and dependable reasoning capabilities.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have recently shown remarkable capabilities in reasoning-intensive tasks such as coding (Chen et al., 2021; Li et al., 2022; Rozi\u00e8re et al., 2023) and solving complex mathematical problems (Shao et al., 2024; Azerbayev et al., 2024). Prompting strategies like chain-of-thought prompting (Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022; Adolphs et al., 2022) and self-consistency sampling (Wang et al., 2023) enhance these models' final-answer accuracy by encouraging them to articulate intermediate reasoning steps. However, a significant issue remains: even when these methods boost final-answer correctness, the internal reasoning steps are often unreliable or logically inconsistent (Uesato et al., 2022; Lightman et al., 2024).\nThis discrepancy between correct final answers and flawed intermediate reasoning limits our ability to trust LLMs in scenarios where transparency and correctness of each reasoning stage are crucial (Lanham et al., 2023). For example, in mathematical problem-solving, a model might produce the right answer for the wrong reasons (Lyu et al., 2023; Zheng et al., 2024), confounding our understanding of its true capabilities (Turpin et al., 2023). To address this, researchers are increasingly emphasizing the importance of guiding models to produce not just correct final answers, but also verifiable and faithful step-by-step solution paths (Uesato et al., 2022; Shao et al., 2024; Setlur et al., 2024).\nPrior work in finetuning has largely focused on outcome-level correctness, using outcome reward models to improve the probability of final-answer accuracy (Cobbe et al., 2021; Hosseini et al., 2024; Zhang et al., 2024). While effective, such an approach does not ensure that the intermediate reasoning steps are valid. Conversely, while process-level supervision through process reward models (PRMs) (Lightman et al., 2024; Wang et al., 2024; Luo et al., 2024) can guide models to follow correct reasoning trajectories, prior work has mainly used PRMs as a ranking method rather than a way to provide stepwise feedback. As a result, relying solely on process-level supervision may lead models to prioritize step-by-step correctness without guaranteeing a correct final outcome."}, {"title": "2 Methodology", "content": null}, {"title": "2.1 Problem Setup and Notation", "content": "We adopt the notation and setup similar to Setlur et al. (2024). Let $\\mathcal{D} = \\{(x_i, y_x)\\}$; be a dataset of math prob-lems, where each problem $x \\in \\mathcal{X}$ has an associated ground-truth solution sequence $y_x = (s_1, s_2, ..., s_{|y_x|}) \\in \\mathcal{Y}$.\nA policy model $\\pi_{\\theta}$, parameterized by $\\theta$, generates a response sequence $y = (s_1, s_2, ..., s_{|y|})$ autoregressively given the problem $x$, where each step $s_h$ is a reasoning step separated by a special token (e.g., \"## Step\").\nThe correctness of the final answer $y$ can be automatically determined by a rule-based correctness function $\\text{Regex}(y, y^*_x) \\in \\{0,1\\}$, which compares the model's final derived answer to the ground-truth final answer (Hendrycks et al., 2021). The model's final answer is explicitly denoted using a special format in the final step $s_{|y|}$, such as \\texttt{boxed}\\{$\\cdot$\\}, allowing the correctness function to easily extract and verify it. Our primary objective is to improve the expected correctness of the final answer:\n$$\\mathbb{E}_{x \\in \\mathcal{D}, y \\sim \\pi_{\\theta}(\\cdot | x)}[\\text{Regex}(y, y^*_x)].$$\nEnsuring a correct final answer does not guarantee logically sound intermediate reasoning. To address this, we incorporate a stepwise binary correctness signal $\\text{Prm}(x, y, s_h) \\in \\{0,1\\}$ for each reasoning step $s_h$. Unlike the final-answer correctness $\\text{Regex}$, this signal directly measures whether each intermediate step is locally valid and aligns with proper problem-solving principles, without strictly mirroring the reference solution steps. We obtain these stepwise correctness evaluations by prompting an LLM (Llama-3.1-70B-Instruct) as our process reward model (PRM), following the structured template in Appendix C.\nIn summary, we have two levels of binary signals:\n*   **Outcome feedback**: $\\text{Regex}(y, y^*_x) \\in \\{0,1\\}$ indicates if the final answers derived from $y$ is correct.\n*   **Stepwise feedback**: $\\text{Prm}(x, y, s_h) \\in \\{0,1\\}$ indicates if the intermediate reasoning step $s_h$ is correct.\nOur goal is to integrate both of these signals into the training objective of $\\pi_{\\theta}$. By doing so, we guide the model to produce not only correct final answers but also to maintain correctness, coherence, and reliability throughout its reasoning trajectory. This integrated approach will be formalized through the STEP-KTO framework."}, {"title": "2.2 KTO Background", "content": "KTO (Ethayarajh et al., 2024) aims to align a policy $\\pi_{\\theta}$ with binary feedback using a Kahneman-Tversky-inspired value function (Tversky and Kahneman, 2016). Rather than maximizing the log-likelihood of preferred outputs or directly using reinforcement learning, KTO defines a logistic value function that is risk-averse for gains and risk-seeking for losses.\nThe original KTO loss focuses on the final-answer level. Let:\n$$r_\\theta(x, y) = \\log \\frac{\\pi_{\\theta}(y | x)}{\\pi_{\\text{ref}}(y | x)},$$\n$$z_0 = \\text{KL}(\\pi_\\theta(y' | x) || \\pi_{\\text{ref}}(y' | x)),$$\n$$v(x, y) = \\begin{cases}\n\\lambda_D \\sigma(\\beta (r_\\theta(x,y) - z_0)) & \\text{if } \\text{Regex}(y, y^*_x) = 1, \\\\\n\\lambda_U \\sigma(\\beta (z_0 - r_\\theta(x,y))) & \\text{if } \\text{Regex}(y, y^*_x) = 0.\n\\end{cases}$$\nHere, $\\pi_{\\text{ref}}$ is a reference policy (typically the initial model checkpoint) that provides a baseline for comparison, $\\sigma$ is the logistic function, $\\beta > 0$ controls risk aversion, and $\\lambda_D, \\lambda_U$ are weighting coefficients. The $z_0$ term, where $y'$ denotes an arbitrary output sequence, serves as a reference point to ensure balanced optimization. The KTO loss at the outcome level is:\n$$L_{\\text{KTO}}(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x,y \\sim \\mathcal{D}}[\\lambda - v(x, y)],$$"}, {"title": "2.3 Step-KTO", "content": "While KTO ensures correctness of final answers, many reasoning tasks require validity at each intermediate step. We extend KTO by incorporating stepwise binary feedback $\\text{Prm}(x, y, s_h)$ to assess the quality of each reasoning step. We begin by defining an implied reward at the step level:\n$$r_\\theta(x, s_h) = \\log \\frac{\\pi_\\theta(s_h | x, s_{<h})}{\\pi_{\\text{ref}}(s_h | x, s_{<h})}.$$\nThis quantity can be viewed as the incremental advantage of producing step $s_h$ under $\\pi_\\theta$ compared to $\\pi_{\\text{ref}}$. It captures how much more (or less) reward is implied by choosing $s_h$ over the reference model's baseline likelihood, conditioned on the same context $(x, s_{<h})$. Next, we introduce a stepwise KL baseline:\n$$z_{\\theta}^{(step)} = \\text{KL}(\\pi_\\theta(s_h | x, s'_{<h}) || \\pi_{\\text{ref}}(s_h | x, s'_{<h})).\n$$\nAnalogous to $z_0$ at the outcome level, $z_{\\theta}^{(step)}$ serves as a local reference point. It prevents the model from gaining reward merely by diverging from the reference and ensures that improvements are grounded in genuine reasoning quality. Given the binary stepwise feedback $\\text{Prm}(x, y, s_h)$, we define a value function that parallels the outcome-level case. If a step $s_h$ is deemed stepwise-desirable, the model should increase its implied reward $r_\\theta(x,s_h)$ relative to $z_{\\theta}^{(step)}$ (Huang and Chen, 2024). Conversely, if $s_h$ is stepwise-undesirable, the model is encouraged to lower that implied reward. Formally:\n$$v^{(step)}(x, s_h) = \\begin{cases}\n\\lambda_D^{(step)} \\sigma(\\beta_{step} (r_\\theta(x, s_h) \u2013 z_{\\theta}^{(step)})) & \\text{if } \\text{Prm}(x, y, s_h) = 1, \\\\\n\\lambda_U^{(step)} \\sigma(\\beta_{step}(z_{\\theta}^{(step)} - r_\\theta(x,s_h))) & \\text{if } \\text{Prm}(x, y, s_h) = 0.\n\\end{cases}$$\nHere, $\\lambda_D^{(step)}, \\lambda_U^{(step)}$ and $\\beta_{step}$ mirror their outcome-level counterparts, controlling the strength of the reward or penalty at the granularity of individual steps. By leveraging these signals, the stepwise value function $v^{(step)}$ directs the model's distribution toward steps deemed correct and coherent, and away from those that are not. With these definitions, the stepwise loss is:\n$$L_{\\text{step}}(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x,y,s_h \\sim \\mathcal{D}^{(step)}}[\\lambda^{(step)} - v^{(step)}(x, s_h)].$$\nwhere $\\lambda^{(step)} = \\lambda_D^{(step)}$ if $\\text{Prm}(x, y, s_h) = 1$ and $\\lambda^{(step)} = \\lambda_U^{(step)}$ if $\\text{Prm}(x, y, s_h) = 0$.\nCombining the stepwise objective with the outcome-level KTO loss (Eq. 1) yields the final STEP-KTO objective:\n$$L_{\\text{STEP-KTO}}(\\pi_\\theta, \\pi_{\\text{ref}}) = L_{\\text{KTO}}(\\pi_\\theta, \\pi_{\\text{ref}}) + L_{\\text{step}}(\\pi_\\theta, \\pi_{\\text{ref}}).$$\nThis composite loss encourages the model to produce not only correct final answers but also to refine each intermediate step. By jointly optimizing outcome-level and stepwise-level feedback, STEP-KTO ensures that the model's entire reasoning trajectory\u2014from the earliest steps to the final solution\u2014is both correct and coherent."}, {"title": "2.4 Iterative Training", "content": "We train our models using an iterative procedure inspired by previous alignment methods that refine a model's parameters over multiple rounds (Zelikman et al., 2022; Yuan et al., 2024; Pang et al., 2024; Prasad et al., 2024). For Llama-3.3-70B-Instruct, we use it directly as our seed model $M_0$. For Llama-3.1 models, we first perform supervised finetuning on the training data before using them as $M_0$. Starting from $M_0$, we refine it iteratively to obtain $M_1, M_2,..., M_T$ using the following procedure:\n1.  **Candidate Generation**: For each problem $x \\in \\mathcal{D}$, we sample 8 candidate solutions $y^k \\sim \\pi_{M_t}(\\cdot | x)$ using temperature T = 0.7 and nucleus sampling with $p = 0.95$ (Holtzman et al., 2020). This stochastic decoding strategy encourages diverse candidate solutions, aiding both positive and negative sample selection.\n2.  **Outcome Assessment**: We evaluate each candidate $y^k$ against the ground-truth solution $y^*_x$ using the outcome correctness function $\\text{Regex}(y^k, y^*_x)$. If no sampled solutions are correct, we include the ground-truth solution $y^*_x$ as a positive sample, as suggested by Pang et al. (2024). If all sampled solutions are correct, we discard this problem in the current iteration to prioritize learning from problems where the model can still improve."}, {"title": "3 Experiments", "content": null}, {"title": "3.1 Task and Datasets", "content": "We evaluate our approach on established math reasoning benchmarks derived from high school competition-level exams. These tasks test the model's ability to solve challenging mathematical problems spanning various domains and difficulty levels. All problems require the model to produce a final answer, which is often a number, a simplified expression (e.g., $\\frac{1}{2}$ or $1 \\pm \\sqrt{19}$), or a short textual response (e.g., \u201ceast\u201d).\n*   **MATH-500**: A curated subset of 500 problems drawn from the MATH dataset (Hendrycks et al., 2021), selected as in Lightman et al. (2024). These problems cover seven subjects: Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus, ensuring a broad evaluation of mathematical reasoning skills.\n*   **AMC23**: A test set of 40 problems from the American Mathematics Competitions (AMC 12, 2023)2. These problems are known for their subtlety and problem-solving depth, providing a stringent test of reasoning ability and accuracy.\n*   **AIME24**: A test set of 30 problems from the American Invitational Mathematics Examination (AIME, 2024)3. Each problem typically requires multiple steps of intricate reasoning, posing a higher-level challenge that further differentiates models based on their capacity to follow extended solution paths\nTo evaluate the correctness of the model's outputs, we follow standard practices in mathematical LLM evaluation (Hendrycks et al., 2021). First, we parse the model's generated solution using regular expressions to extract the final answer. Then, we employ SYMPY\u2074 to check for mathematical equivalence between the generated answer and the ground-truth solution. This approach ensures a fair comparison that accounts for minor stylistic or representational differences in the final answer.\nWe report results using two standard metrics:\n*   **Pass@1**: The ratio that a single greedy completion $y$ from $\\pi_\\theta$ is correct.\n*   **Maj@8**: The accuracy obtained by generating 8 candidate solutions $y^k \\sim \\pi_{\\theta}(\\cdot | x)$ at temperature T = 0.7 (Ackley et al., 1985; Ficler and Goldberg, 2017) and selecting the majority answer, as introduced"}, {"title": "3.2 Baseline Methods", "content": "We evaluate our proposed STEP-KTO against several strong baseline approaches for mathematical reasoning. All methods are trained using offline iterative optimization, with online preference learning left as future work:\n*   **RFT (Rejection Finetuning)**: Following Yuan et al. (2023), this method performs supervised finetuning on the filtered dataset $\\{(x, y) \\in \\mathcal{D}_M^+ | c_{out} = 1\\}$, retaining only solutions with correct final answers. Unlike STEP-KTO, RFT does not incorporate any explicit preference or reward signals, instead directly mimicking the ground-truth solutions.\n*   **IRPO (Iterative Reasoning Preference Optimization)**: Extending the ideas of DPO (Rafailov et al., 2023), IRPO applies iterative training with pairwise preferences at the outcome level, enhanced by an additional NLL loss term to stabilize training (Pang et al., 2024). IRPO does not utilize stepwise feedback, focusing solely on outcome correctness for model refinement.\n*   **KTO (Kahneman-Tversky Optimization)**: KTO (Ethayarajh et al., 2024) applies a HALO-based loss derived from the Kahneman-Tversky value function (see \u00a72.2), instilling human-like risk aversion and asymmetric weighting of gains and losses. Like the other outcome-level methods, it does not incorporate stepwise correctness signals.\n*   **SimPO and IPO**: SimPO (Meng et al., 2024) and IPO (Azar et al., 2024) are both variants of DPO that optimize from pairwise preferences at the outcome level only. Unlike DPO, which relies on the Bradley-Terry model (Bradley and Terry, 1952) and can overemphasize deterministic preferences, SimPO and IPO apply simpler transformations to directly utilize preference probabilities. They primarily target safer, more stable optimization rather than introducing explicit mechanisms to enhance the reasoning performance.\n*   **Step-DPO**: A variant of DPO that optimizes stepwise preferences rather than outcome-level preferences (Lai et al., 2024). By identifying and correcting specific erroneous reasoning steps, Step-DPO aims to provide more granular supervision for long-chain reasoning tasks. However, it requires additional data processing to construct stepwise preference pairs and relies on rejection sampling to filter out incorrect intermediate steps."}, {"title": "3.3 Implementation Details", "content": "We use AdamW ($\\beta_1 = 0.9, \\beta_2 = 0.95$, weight decay = 0.1) with a linear warmup for the first 100 steps and a cosine decay schedule that reduces the learning rate to 0.1$\\times$ its initial value. The starting learning rate is $1.0 \\times 10^{-6}$, and we apply global norm gradient clipping of 1.0. The effective global batch size is set to approximately one million tokens, and we train for about 2000 steps, periodically evaluating our models during training on the hold-out test set from MATH (Hendrycks et al., 2021)6 to select the best checkpoint for each method. For IRPO, we use an NLL weight of 0.2. We set $\\beta = 0.1$ for all methods. All training jobs are run on 64 H100 GPUs."}, {"title": "3.4 Main Results", "content": "Table 1 presents our main results, comparing STEP-KTO with various baseline methods and commercial systems across the MATH-500, AMC23, and AIME24 benchmarks. We report both Pass@1 and Maj@8 accuracy, as described in \u00a73. Overall, STEP-KTO consistently outperforms the baselines that rely solely on outcome-level correctness, such as KTO, IRPO, SimPO, and IPO, as well as simpler methods like RFT.\nFor instance, on MATH-500 with the 8B Llama-3.1-Instruct model, STEP-KTO achieves a Pass@1 of 63.2%, improving from the baseline KTO model's 60.6% and substantially surpassing IRPO and RFT. On AMC23, STEP-KTO attains a Pass@1 of 47.5%, outperforming baselines by a notable margin. On AIME24, where problems require especially intricate multi-step reasoning, STEP-KTO sustains its advantage, demonstrating that the stepwise supervision is particularly valuable for more challenging tasks. Scaling to the 70B further improves results. Llama-3.1-70B-Instruct with STEP-KTO reaches a Pass@1 of 76.2% on MATH-500 and continues to excel on AMC23 (60.0%) and AIME24 (16.7%). Llama-3.3-70B-Instruct with STEP-KTO model pushes performance higher still, with STEP-KTO achieving 79.6% on MATH-500, 70.5% on AMC23, and 29.6% on AIME24. Although larger models also benefit from outcome-only alignment techniques, STEP-KTO still delivers consistent gains, indicating that even powerful models trained on extensive data can be further improved by targeting intermediate reasoning quality. Compared to strong proprietary models, STEP-KTO-enhanced Llama models remain competitive and close the performance gap. For example, while"}, {"title": "3.5 Iterative Training", "content": "Table 2 illustrates how model performance evolves over multiple iterative training rounds ($M_1, M_2, M_3$) when starting from the same seed model $M_0$ (Llama-3.1-8B-Instruct). We compare STEP-KTO against other iterative methods such as IRPO, KTO, and Rejection Finetuning.\nOverall, STEP-KTO not only achieves higher final performance but also improves more consistently across iterations. For instance, on MATH-500, STEP-KTO progresses from 59.4% Pass@1 at $M_1$ to 63.2% at $M_3$, surpassing the gains observed by IRPO and KTO at the same checkpoints. Similarly, on AMC23 and AIME24, STEP-KTO shows steady iterative improvements, reflecting the cumulative value of integrating both process- and outcome-level feedback. In contrast, Rejection Finetuning (RFT) and IRPO exhibit less stable gains across iterations, with performance sometimes plateauing or even regressing at later rounds. KTO does improve over iterations, but not as robustly as STEP-KTO, highlighting that stepwise feedback adds tangible benefits beyond what outcome-level optimization alone can achieve.\nThese results underscore the importance of iterative refinement. While simply applying preference-based or rejection-based finetuning may yield some initial improvements, STEP-KTO's combined stepwise and outcome-level guidance drives steady, sustained enhancements in mathematical reasoning quality, iteration after iteration."}, {"title": "3.6 Comparison with Step-DPO", "content": "While Step-DPO (Lai et al., 2024) also aims to improve reasoning by focusing on intermediate steps, our method STEP-KTO differs significantly in its approach. Step-DPO identifies erroneous steps and uses rejection sampling to generate correct continuations, requiring substantial computational resources. In contrast, STEP-KTO combines stepwise and outcome-level signals to ensure global solution coherence while remaining computationally efficient. Empirically, Step-DPO shows limited gains after the first iteration (M1), achieving 56.8% Pass@1 on MATH-500, while STEP-KTO reaches 59.4%. For implementation, we follow"}, {"title": "3.7 Preference Optimization Variants", "content": "Table 2 compares STEP-KTO against several baselines after iterative training starting from the 8B seed model $M_0$. Focusing on MATH-500 at M1, STEP-KTO achieves 59.4% Pass@1\u2500outperforming IPO (52.6%), SimPO (55.8%), and even stronger baselines like IRPO (58.2%) and KTO (56.2%). On AMC23 and AIME24 at $M_1$, while STEP-KTO's initial improvements are more modest than IRPO, it remains competitive with other variants and demonstrates stronger subsequent gains. For instance, by M3, STEP-KTO reaches 47.5% Pass@1 on AMC23, surpassing all baseline methods, and also ties for the highest Pass@1 (16.7%) on AIME24. Collectively, these results underscore the importance of integrating stepwise correctness signals with outcome-level preferences."}, {"title": "3.8 Evaluating Reasoning Quality", "content": "To assess the internal consistency of solutions with correct final answers, we evaluate the proportion of solutions that, despite having correct final answer $\\text{Regex}(y, y^*_x) = 1$, contain at least one erroneous intermediate step. We use the ProcessBench (Zheng et al., 2024) as our evaluation framework, which is prompted to identify the earliest error in the generated solution $y$, as detailed in its benchmark construction. Additionally, we utilize the critique capabilities of the QwQ-32B-Preview model (Qwen, 2024) to identify the first error in the reasoning. We prompt QwQ using the prompt detailed in Appendix C. We then measure the percentage of correctly answered problems where QwQ identifies at least one erroneous intermediate step.\nTable 3 shows the percentage of correctly answered solutions containing errors in reasoning steps, starting from the initial 8B seed model $M_0$, which produces reasoning steps containing errors in 27.3% of its correctly answered solutions on the MATH-500 test set. Both STEP-KTO and KTO reduce the prevalence of such errors across iterations, with STEP-KTO showing a greater and more consistent reduction from 27.3% at $M_0$ to 19.9% at $M_3$, compared to KTO's more modest improvement to 21.1%."}, {"title": "4 Related Work", "content": "Outcome-Oriented Methods A significant body of work aims to refine LLMs purely based on their final outputs. Large-scale instruction tuning has shown that aligning models with human values or preferences enhances instruction-following capabilities (Ouyang et al., 2022; Touvron et al., 2023). Outcome-level feedback is often implemented through Reinforcement Learning from Human Feedback (RLHF), as demonstrated by InstructGPT (Ouyang et al., 2022), or through direct preference optimization techniques such as DPO (Rafailov et al., 2023), KTO (Ethayarajh et al., 2024), SimPO (Meng et al., 2024) and IPO (Azar et al., 2024). These methods optimize the probability of correct or preferred final answers by comparing model-generated candidates against human or synthetic labels. Approaches like RLAIF (Lee et al., 2023) and Constitutional AI (Bai et al., 2022b) go further by introducing AI-generated feedback or predefined ethical rules to reduce reliance on human annotations. More recent refinements, such as CGPO (Xu et al., 2024), attempt to improve\nProcess-Level Feedback and Verification Process Reward Models (PRMs) (Lightman et al., 2024; Uesato et al., 2022; Xiong et al., 2024; Luo et al., 2024) focus on stepwise correctness signals. They assign local binary labels or values to each reasoning step, thereby guiding the model to follow logically consistent and provably correct solution trajectories. This paradigm aligns closely with efforts in math reasoning tasks, where datasets like PRM800K (Lightman et al., 2024), CriticBench (Lin et al., 2024), and ProcessBench (Zheng et al., 2024) include detailed reasoning chains and facilitate step-level evaluations. Techniques leveraging PRMs have been integrated into decoding strategies (Li et al., 2023; Chuang et al., 2024; Wang et al., 2024), re-ranking (Cobbe et al., 2021), filtering (Dubey et al., 2024; Shao et al., 2024), or iterative improvement loops such as STaR (Zelikman et al., 2022) and ReST (G\u00fcl\u00e7ehre et al., 2023; Singh et al., 2024). More recent work used synthetic feedback or automatic checks to scale up these stepwise annotations (Wang et al., 2024; Lightman et al., 2024; Chiang and Lee, 2024; Huang and Chen, 2024), showing modest but consistent gains. While process-level guidance can refine stepwise correctness, it does not guarantee full alignment with ground-truth solutions. Models may still produce incorrect final outcomes if the reasoning chain fails to converge or if the reward signal is exploited by artificially repeating trivial steps (Gao et al., 2024).\nIntegrating Outcome- and Process-Level Signals Recognizing the limitations of relying on only outcome-level or only process-level supervision, recent studies propose combining both signals to align the entire reasoning trajectory with correct and faithful solutions. For example, FactTune (Tian et al., 2024), and FactAlgin (Huang and Chen, 2024) incorporated factuality evaluators and PRMs to produce preference pairs for alignment, demonstrating that mixing final-answer correctness with stepwise verification yields better factual performance. Similarly, Uesato et al. (2022) and Shao et al. (2024) leveraged feedback for both steps and final outputs to improve math reasoning. Although these methods often target general instruction following or long-form factual generation, their principle using multiple granularities of supervision-holds equally for complex domains like math reasoning. Still, many of these approaches face challenges in scaling to very difficult problem sets, balancing the complexities of outcome-level correctness with the subtleties of stepwise coherence, and ensuring that iterative improvements do not plateau prematurely (Bai et al., 2022a; Xu et al., 2023; Singh et al., 2024)."}, {"title": "5 Conclusion", "content": "This work proposes STEP-KTO, a training framework that leverages both outcome-level and process-level binary feedback to guide large language models toward more coherent, interpretable, and dependable reasoning. By integrating stepwise correctness signals into the alignment process, STEP-KTO improves the quality of intermediate reasoning steps while maintaining or even enhancing final answer accuracy. Our experiments on challenging mathematical reasoning benchmarks demonstrate consistent gains in performance, particularly under iterative training and for complex reasoning tasks. These findings underscore the value of aligning not only final outcomes but also the entire reasoning trajectory. We envision STEP-KTO as a stepping stone toward more reliable reasoning in LLMs."}, {"title": "Limitations", "content": "While our STEP-KTO framework shows promise in improving both outcome-level correctness and the internal coherence of reasoning steps, several limitations remain.\nFirst, outcome-level feedback signals can be noisy or imperfect. In mathematical reasoning, even automatically verified final answers may occasionally fail to capture all nuances of correctness. For instance, subtle formatting differences or unconventional but valid representations might lead to false negatives. This noise can limit the precision of the training signal, potentially hindering further improvements.\nSecond, our approach currently relies on access to ground-truth solutions for both final answers and (implicitly)"}]}