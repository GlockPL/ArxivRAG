{"title": "Understanding is Compression", "authors": ["Ziguang Li", "Chao Huang", "Xuliang Wang", "Haibo Hu", "Cole Wyeth", "Dongbo Bu", "Quan Yu", "Wen Gao", "Xingwu Liu", "Ming Li"], "abstract": "We have previously shown all understanding or learning are compression, under reasonable assumptions. In principle, better understanding of data should improve data compression. Traditional compression methodologies focus on encoding frequencies or some other computable properties of data. Large language models approximate the uncomputable Solomonoff distribution, opening up a whole new avenue to justify our theory.\nUnder the new uncomputable paradigm, we present LMCompress based on the understanding of data using large models. LMCompress has significantly better lossless compression ratios than all other lossless data compression methods, doubling the compression ratios of JPEG-XL for images, FLAC for audios and H264 for videos, and tripling or quadrupling the compression ratio of bz2 for texts. The better a large model understands the data, the better LMCompress compresses.", "sections": [{"title": "1. Introduction", "content": "Before the reader starts to read this article, we invite you to reminisce about how you have compressed data: When you see a tiger, didn't you store it as a \"large cat\"? When you see 3.141592 ... didn't you just note it down as a \\(\\pi\\)? When you see a bird, didn't you only focus on its unique features like size and color?\nYes, you have learned, understood, and compressed. In Jiang et al. (2023), we have mathematically proved that all human or animal learning or understanding are compression, under reasonable assumptions. However, we did not give a method of how to effectively find a way to compress the data using the fact that we thought we \"understood\" the data.\nTraditional compression methods depend on various fre-\nquency concerns, or Shannon entropy, or other computable prop-erties. While being computable, such methods have reached their limits after many years of research. Our new theory depends on Kolmogorov complexity, information distance Bennett et al. (1998), and Solomonoff's universal distribution Li and Vitanyi (2019). Such metrics are not computable. GPT may be seen as approximating the uncomputable Solomonoff distribution with a lot of data. In this paper, we advocate a new paradigm of compression using such approximations by large models, hence compression by understanding the data. See Fig. 1.\nA precursor of our work has been independently published by us Huang et al. (2023) and by a DeepMind team Del\u00e9tang et al. (2023), with preceding work in Bellard (2021) with similar ideas. These works demonstrated that arithmetic coding with a generative large model can improve the best traditional text compressors such as gzip a few folds. Del\u00e9tang et al. (2023) also achieved the state of the art of classical image compression, and moderately improved classical audio compression.\nThis paper intends to comprehensively justify the idea of bet-ter understanding implies better compression. We will focus on lossless compression for clean comparisons. In order to demon-strate that better understanding implies better compression, we use image GPT rather than a plain large language model (LLM) to compress images and videos, retrain an LLM with a small amount of audio data to compress audios, and employ domain-specific finetuned LLMs to compress domain texts. Lossless compression experiments show that we significantly improve compression ratios on all types of data: texts, images, videos, and audios. Our method is several folds better than traditional algorithms, and a large margin better than the plain LLMs other-wise."}, {"title": "2. Background", "content": ""}, {"title": "2.1. Solomonoff Prior", "content": "In 1960's, Solomonoff (1964) proposed a theory of prediction. Consider an infinite sequence w of events over a finite alphabet \u03a3. Given an increasingly longer initial segment \\(w_{1:n} \\in \\Sigma^n\\) of"}, {"title": "2.2. Multimedia Compression", "content": "Multimedia compression refers to the process of reducing the size of digital media data, such as text, images, audio, and video, without compromising the essential information they contain. This is achieved by identifying and eliminating redundancies within the data, resulting in a more compact representation that requires less storage space or bandwidth for transmission.\nThe three key techniques widely used in multimedia compres-sion are prediction, transformation, and entropy coding.\nPrediction-based compression leverages the inherent patterns and correlations within the data to predict future or missing elements. In audio compression, future audio frames can be predicted based on previous frames, as seen in techniques like linear predictive coding (LPC) (Auristin and Mali, 2016). Video compression often employs prediction, both within frames (intra-frame prediction) and between frames (inter-frame prediction), as demonstrated in the H.264 video codec (Wiegand et al., 2003).\nTransformation-based compression aims to convert the data representation from a higher-dimensional space to a lower-dimensional space, reducing the overall data size. In audio, image, and video compression, techniques like the Discrete Co-sine Transform (DCT)(Ahmed et al., 1974) are widely used to transform the digital media into the frequency domain, where high-frequency components can be selectively discarded with minimal impact on visual quality. The transformed data can then be quantized and encoded using entropy coding methods.\nEntropy coding is a lossless compression technique that elim-inates statistical redundancies in the data by assigning shorter"}, {"title": "2.3. Arithmetic Coding", "content": "Our compression will be based on arithmetic coding, so we introduce it in more detail to make the paper self-contained.\nArithmetic coding maps any sequence of symbols into a num-ber in the interval [0, 1).\nLet \\(\\Sigma\\) be a finite alphabet with total order \u2264. For any integer n \u2265 0, there is a probability mass function \\(p_n\\) on \\(\\Sigma^n\\), which satisfies that \\(p_n(\\omega) = \\Sigma_{x\\in \\Sigma} p_{n+1}(\\omega x)\\) for any sequence \\(\\omega \\in \\Sigma^n\\).\nArbitrarily fix a sequence \\(\\omega = \\omega_1\\omega_2\u00b7\u00b7\u00b7\\omega_n \\in \\Sigma^n\\) for some n > 0. The arithmetic encoder compresses the sequence symbol by symbol while iteratively narrowing down the interval \\(I_0 = [0, 1)\\). Specifically, for any \\(1 \\leq k \\leq n\\), let \\(I_{k-1} = [a_{k-1}, b_{k-1})\\) be the interval obtained after encoding \\(\\omega_{k-1}\\). To encode \\(\\omega_k\\), we define the interval \\(I_k = [a_k, b_k)\\) as follows:\n\\[a_k = a_{k-1} + (b_{k-1} - a_{k-1}) \\frac{\\sum_{x \\in \\Sigma, x \\leq \\omega_k} p_k(\\omega_{1:(k-1)}x)}{p_{k-1}(\\omega_{1:(k-1)})}\\] (1)\n\\[b_k = a_{k-1} + (b_{k-1} - a_{k-1}) \\frac{\\sum_{x \\in \\Sigma, x < \\omega_k} p_k(\\omega_{1:(k-1)}x)}{p_{k-1}(\\omega_{1:(k-1)})}\\] (2)\nwhere\n\\[p(\\omega_k | \\omega_{1:(k-1)}) = \\frac{p_k(\\omega_{1:(k-1)}\\omega_k)}{\\sum_{x \\in \\Sigma, x \\leq \\omega_k} p_k(\\omega_{1:(k-1)}x)}\\]\nFinally, we select a value \\(\\lambda \\in I_n\\) that has the shortest binary representation. It serves as the arithmetic code for \\(\\omega\\). Note that the length \\(l(\\lambda)\\) of the binary representation of \\(\\lambda\\) is\n\\[l(\\lambda) \\leq [-\\log_2(b_n - a_n)] + 1\\]\n\\[= [-\\log_2 \\prod_{i=1}^n p(\\omega_i | \\omega_{1:(k-1)})] + 1\\]\n\\[= [-\\log_2 p_n(\\omega)] + 1.\\]\nOn the other hand, given \\(\\lambda\\), we can reconstruct the sequence \\(\\omega\\) in a similar manner. Starting with the interval \\(I_0 = [0, 1)\\), assume that we have recovered \\(\\omega_{1:(k-1)}\\) and obtained the interval \\(I_{k-1} = [a_{k-1}, b_{k-1})\\) that contains \\(\\lambda\\). The kth element of \\(\\omega\\) is the unique symbol \\(\\omega_k \\in \\Sigma\\) for which \\(I_k = [a_k, b_k)\\) contains \\(\\lambda\\). Here, \\(a_k\\) and \\(b_k\\) are defined as shown in Formulas (1) and (2).\nHence, arithmetic coding is a lossless compression method."}, {"title": "3. Methods", "content": "Traditional compression methods, whether lossy or lossless, depend on a computable function. We propose LMCompress, a new Kolmogorov paradigm of compression depending on the uncomputable Solomonoff distribution. The Solonomoff distri-bution is approximated by large models with never ending input data. The compression ratio should go up with better approxi-mation of Solomonoff distribution and better understanding of data.\nIt turns out that we have already long passed the critical point. We demonstrate that the data is already sufficient to improve lossless compression of texts, images, videos and audios by several folds, unthinkable for traditional methods, far passing the Shannon entropy bound.\nThe basic process of LMCompress is as follows. First, we decompose the original data into a sequence of tokens. Then, we feed this token sequence into a large generative model, which outputs the predictive distribution for each token. Finally, we use arithmetic coding to losslessly compress the original data based on these predictive distributions. As shown below, the tokenization method and the large generative model may vary according to the type of the original data."}, {"title": "3.1. Image Compression", "content": "We will use the image-GPT model (iGPT, Chen et al. (2020)) as the generative large model. Our choice of iGPT is driven by two key factors.\nFirstly, iGPT is a large-scale vision model that has been trained on a vast corpus of images, equipping it with a robust understanding of visual data. This makes iGPT well-suited for analyzing and processing images.\nSecondly, iGPT is an autoregressive large vision model, which means that when presented with a sequence of pixels, it can generate predictive probability for each pixel in the sequence. This capability is a prerequisite for employing arithmetic coding.\nTo compress an image using iGPT, we first concatenate the image's rows from top to bottom, transforming the two-dimensional visual data into a one-dimensional sequence of pixels. This pixel sequence is then fed into iGPT for processing.\nHowever, due to the limited context window of iGPT, the entire pixel sequence cannot be input to the model all at once. Instead, we divide the sequence into non-overlapping segments, each of which can fit within iGPT's context window. These individual segments are then fed into iGPT sequentially and processed in a piece-by-piece manner."}, {"title": "3.2. Video Compression", "content": ""}, {"title": "3.2.1. Lossless Video Compression", "content": "To the best of our knowledge, all existing open-source large video models do not naturally output probabilities. As a result, we have opted to circumvent this limitation by leveraging the image-based generative model iGPT instead. Since a video is fundamentally a sequence of frames, we propose to compress each individual frame using the iGPT model.\nAt this stage, we have chosen not to exploit the inter-frame information for compression. There are two reasons for this:"}, {"title": "3.2.2. Lossy Video Compression", "content": "In addition to lossless compression, the more common appli-cation of video compression is lossy compression. Therefore, we have also conducted research on the improvement of LLM in video lossy compression. Existing works such as DCVC series (DCVC Li et al. (2021); DCVC-HEM Li et al. (2022); DCVC-FM Li et al. (2024)) typically draw on the residual coding-based framework. However, in recent years, Artificial Intelligence Generated Content (AIGC) has developed rapidly with the ac-cumulation of generative large model. The concept \"generative compression\" mentioned in Santurkar et al. (2017) is getting a lot of attention in image compression area (Yang and Mandt (2024); Relic et al. (2024)), and these methods perform promis-ing results.\nEssentially, due to learned data distribution from training dataset, the generative large models are able to learn more com-pact feature representations, flexible motion estimation mech-anisms and superior signal reconstruction capabilities, which bring a lot of help for compression performance.\nTo compare DCVC series with methods using generative model, we set that the bit rate of both to be the same. Inspirited by Xu et al. (2024) which give a paradiam of transform-coding based method linked with generative large model, we try to use DCVC results as posterior and sample from diffusion model with the manifold constrained gradient without a strict measure-ment consistency projection step as Chung et al. (2022) does. Because gradient of DCVC is necessary, we use a proxy loss function, replacing the quantization step with additive uniform noise [Ball\u00e9 et al. (2017)]."}, {"title": "3.3. Audio Compression", "content": "Audios, as a type of sequential data, should undoubtedly exploit the sequential modeling ability of auto-regressive models. To capture long term patterns, state-of-the-art large audio models tend to discretize an audio into tokens, which is inevitably a lossy transformation. To achieve lossless compression, we establish a model that handles audio at the level of signal, rather than tokens.\nBasically, the audio consists of a sequence of frames, each of which can be represented by a constant number of bytes."}, {"title": "3.4. Text Compression", "content": "Large language models have demonstrated impressive capabil-ity in compressing general texts. Intriguingly, they have potential to achieve even better compression ratios, provided that the texts to be compressed are restricted to specific domains.\nThe key lies in adapting the LLM to better understand the target domain. This is implemented by incorporating an adap-tation layer and fine-tuning the LLM via doamin-specific texts, tailoring the model to the characteristics of the domain.\nThen, to compress a text in the target domain, we feed it into the fine-tuned LLM. The LLM will estimate the next-token probabilities for the text, which can be leveraged by arithmetic coding to perform domain-aware compression."}, {"title": "4. Results", "content": "The metric of compression performance on a dataset, the com-pression ratio, is the ratio of the size of the original data to that of the compressed data. In general, the bigger the compression ratio, the better the compression performance."}, {"title": "4.1. Image Compression", "content": "Dataset We validate the compression ratios of images on two benchmark datasets, ISLVRC2017 (Russakovsky et al. (2015)) and CLIC2019 professional. The ILSVRC2017 dataset is a large-scale dataset containing millions of labeled images across thousands of categories derived from ImageNet. The CLIC2019 dataset is designed for evaluating and benchmark-ing image compression algorithms. It contains high-quality images with various characteristics such as natural scenes, tex-tures, patterns, and structures. These images are representative of real-world scenarios encountered in photography, multimedia, and visual content. Since the ISLVRC2017 dataset is extremely large, we extract 10000 windows of each dataset to perform the arithmetic coding with large vision language model. When we test the impact of window size on compression performance, the number of windows will change to keep the total raw data size consistent.\nIn our experiment, raw data length is always 10 Megabytes since we just extract ten thousand windows of each dataset.\nIt can be seen that LMCompress significantly outper-forms traditional compression algorithms, achieving double the compression ratio compared to methods such as JPEG2000 Skodras et al. (2001)."}, {"title": "4.2. Video Compression", "content": ""}, {"title": "4.2.1. Lossless Video Compression", "content": "Datasets We use the video data from Xiph.org. The Xiph has over 1000 videos. All video sequences are in the uncompressed YUV4MPEG format. Since the LLM is slow to autoregress at the pixel level, we selected two typical videos to test the effectiveness of LMCompress to test the compression ratios. One of the videos(bowing.y4m) is a static scene while the other video(bus.y4m) is a dynamic scene. The bus.y4m video contains 150 frames, while the bowing.y4m video contains 300 frames.In the static scene, the background is stationary and most of the pixel changes occur in the foreground. In the dynamic scene, most of the video content changes as the target moves, causing the majority of the background pixels to change as well."}, {"title": "4.2.2. Lossy Video Compression", "content": "Datasets The experimentation and analysis of the meth-ods are performed using videos from CIPR SIF Sequences at Xiph.org. Due to the resolution limitations of the diffusion model, we scale the video size to 256x256.\nMetric We use Peak-Signal-Noise-Ratio (PSNR) to mea-sure distortion, bpp(bits per pixel) to measure bitrate and Fr\u00e9chet Inception Distance (FID) to measure perceptual quality.\nImplementation details We follow the diffusion training setting and hyperparameters in Chung et al. (2022), which uses stochastic gradient descent to optimize the intermediate samples. To achieve this in the presence of quantization, we use a proxy loss function based on a continuous relaxation of the probability model, replacing the quantization step with additive uniform noise (Ball\u00e9 et al. (2017)). The forward measurement operator is specified as in DCVC (Li et al. (2021))."}, {"title": "4.3. Audio Compression", "content": "Dataset We use LibriSpeech ASR corpus (Panayotov et al. (2015)) and LJSpeech (Ito and Johnson (2017)) as datasets to test audio compression. Both datasets were collected from the LibriVox project which covers nearly 1000 hours of 16kHz En-glish speech in audiobooks. Since the datasets are too large, we extract the first Gigabyte from the train-clean-100 split of the LibriSpeech corpus and the first 256 Megabytes from LJSpeech. The audio streams are transformed into strings of ASCII char-acters before being compressed. The strings are further divided into non-overlapping segments of 2048 bytes, so that every seg-ment does not exceed the context window size of the Llama models. The segments are compressed independently.\nAudio-understanding We build our model for audio com-pression by conducting supervised LORA (Hu et al. (2022)) fine-tuning on the Llama3-8B model. Note that Llama3-8B was pretrained on normal texts. To tailor it for audio compression, we use the first 64 Megabytes in the dev-clean split of the Lib-riSpeech corpus as the training data for fine-tuning, with rank 8 and alpha 32. Note that no data from the LJSpeech dataset is involved in the fine-tuning process."}, {"title": "4.4. Text Compression", "content": "Dataset Our benchmarks for domain-aware text compres-sion are the MeDAL (Wen et al. (2020)) dataset and the Pile of Law (Henderson et al. (2022)) dataset. The domains of the datasets are medicine and law, respectively. Specifically, MeDAL is created from PubMed abstracts which are released in the 2019 annual baseline and primarily serves as a corpus for medical abbreviation understanding. Pile of Law is a dataset of legal and administrative texts compiled from 35 sources. In the experiments, we extract the first 1104 Megabytes from MeDAL and the eurlex split from the Pile of Law corpus. Again, we di-vide the texts into segments of 2048 bytes so that every segment fits the context window of the Llama models. The segments are compressed independently.\nDomain-aware compression To help the LLM understand-ing specific domains, supervised LoRA fine-tuning is applied. For each domain dataset, we use the first 64 Megabytes for training, the next 16 Megabytes for validation, and all of the remaining for testing. The results are illustrated in Table 5.\nWe observe that LMCompress outperforms all the baselines. Its compression ratio on either dataset almost triples those of the best traditional methods. Compared to raw Llama3-8B, LM-Compress improves the compression ratio by 8.5% on MeDAL and nearly by 38.4% on Pile of Law. Again, we get evidence that better understanding leads to better compression.\nIt is worth noting that the \"arithmetic coding + LLM\" paradigm has a universal upper bound on compression ratios, which is determined by the the inherent probability distribution of the texts. Let us focus on \\(X^n\\) with probability distribution \\(p_n\\). Suppose that \\(p_n\\) is not known and we only have an approx-imate probability distribution \\(q_n\\). This is the case for LLMS, where the next-token probabilities are estimated. Then we apply arithmetic coding with \\(q_n\\), which in the worst case needs \\([-log_2 p_n(w)]\\) bits to represent a sequence \\(w \\in X^n\\). Therefore, the expected number of bits needed to compress an length-n se-quence is \\(E_{p_n} [[-log_2 q_n(w)]] \\approx E_{p_1} [-log_2 q_n(w)] = H(p_n, q_n)\\). Here, \\(H(p_n, q_n)\\) is the cross-entropy between the true distribu-tion and the estimated distribution. According to the theory of cross-entropy, if and only if \\(q_n = p_n\\), we get the expected short-est arithmetic coding which is the entropy of \\(X^n\\) with probability distribution \\(p_n\\).\nThe 6G communication, especially when the bandwidth is limited from the satellites, will be significantly benefited by understanding the data, with large models at both ends of com-munication to encode and decode. As the large models are specialized as agents, assisted with RAG, AI will understand our data to be transmitted much better. Conceivably, the research presented here can be extended to the domain of lossy compres-sion. When the data need to be encrypted, our compression needs to be done before encryption. One can even imagine that the sides with better models broadcast open compressed mes-sages allowing only those with equal models to decipher as a first level of encryption, at no extra cost.\nOf course, exploring techniques to effectively incorporate inter-frame information remains an important area for future research. But for now, our pragmatic decision to compress videos by individually encoding their constituent frames appears to be a viable and effective strategy."}, {"title": "5. Conclusion", "content": "Communication in the past was generally governed by the Shannon paradigm, with compression ratio upper bounded by Shannon entropy. While exploring other computable features can further improve compression, large models may be seen to approximate the uncomputable Solomonoff distribution hence opening a new Kolmogorov paradigm of compression. As we have shown, this new way of lossless compression has achieved several folds of improvements on various kinds of data. This new Kolmogorov paradigm allows us to systematically understand the data we transmit, shattering the Shannon entropy upper bound in a great scale."}]}