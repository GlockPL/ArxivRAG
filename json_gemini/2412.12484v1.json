{"title": "Evolutionary Optimization for Designing Variational Quantum Circuits with High Model Capacity", "authors": ["Samuel Yen-Chi Chen"], "abstract": "Recent advancements in quantum computing (QC) and machine learning (ML) have garnered significant attention, leading to substantial efforts toward the development of quantum machine learning (QML) algorithms to address a variety of complex challenges. The design of high-performance QML models, however, requires expert-level knowledge, posing a significant barrier to the widespread adoption of QML. Key challenges include the design of data encoding mechanisms and parameterized quantum circuits, both of which critically impact the generalization capabilities of QML models. We propose a novel method that encodes quantum circuit architecture information to enable the evolution of quantum circuit designs. In this approach, the fitness function is based on the effective dimension, allowing for the optimization of quantum circuits towards higher model capacity. Through numerical simulations, we demonstrate that the proposed method is capable of discovering variational quantum circuit architectures that offer improved learning capabilities, thereby enhancing the overall performance of QML models for complex tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Quantum computing (QC) holds the potential to achieve substantial speed-ups over classical computing in solving various computationally hard problems [1]. At the same time, advancements in artificial intelligence and machine learning (AI/ML) technologies are enabling computational agents to achieve human-level or even superhuman performance across a range of tasks. A natural question arises: how can these two technologies be integrated to harness quantum advantages? Despite the limitations of current quantum devices, such as imperfections and a constrained number of qubits, a hybrid computing framework that combines both quantum and classical paradigms has been proposed. Variational Quantum Algorithms (VQAs) [2] represent a class of hybrid algorithms where quantum parameters are optimized using classical techniques, such as gradient-based optimizers or gradient-free meta-heuristics. VQAs enable the development of architectures like Quantum Neural Networks (QNNs) to tackle a range of AI/ML tasks, including classification [3]\u2013[6], time-series prediction [7], natural language processing [8]-[11], and reinforcement learning [12]-[21]. The design of high-performance Quantum Neural Network (QNN) architectures requires expertise in quantum information science to develop models that can efficiently encode input signals into quantum states and employ sequences of quantum gates that facilitate an effective learning process. In this paper, we address this challenge using Evolutionary Quantum Architecture Search (EvoQAS) to discover circuits with a high effective dimension. Specifically, we propose a quantum circuit architecture encoding method to represent QNN models, enabling a mutation mechanism to modify this representation. After several generations of evolution, the resulting circuit representation is used to generate QNN models with a high effective dimension, which corresponds to increased model capacity."}, {"title": "II. RELATED WORK", "content": "Quantum Architecture Search (QAS) seeks to discover efficient and high-performance quantum circuits for tasks such as generating desired quantum states [22]\u2013[28], identifying optimal circuits for solving chemical ground states [28], [29], addressing optimization problems [28], [30], or conducting machine learning tasks [31]-[38]. Techniques such as reinforcement learning (RL) [22], [23], [31] and evolutionary algorithms [24], [32]-[34] have been applied to perform QAS. This paper differs from previous work in that we are not focused on finding a quantum circuit architecture for a specific ML task or for determining a chemical ground state. Instead, our search aims to identify a model that satisfies a specific metric of model complexity, which implies potential performance in areas such as model capacity and generalization capability."}, {"title": "III. QUANTUM NEURAL NETWORKS", "content": "The fundamental building block of a Quantum Neural Network (QNN) is the Variational Quantum Circuit (VQC), also referred to as Parameterized Quantum Circuit (PQC). The learnable parameters in VQCs are optimized using classical computing resources, as illustrated in Figure 1. A VQC typically consists of three primary components: the encoding circuit, the variational circuit, and the final quantum measurement. The purpose of encoding circuit $U(x)$ is to transform the input vector $x$ into a quantum state $U(x)|0\\rangle^{\\otimes n}$, where $|0\\rangle^{\\otimes n}$ is the ground state of the quantum system and $n$ represents the number of the qubit. The encoded state then go through the variational circuit and becomes $W(\\Theta)U(x)|0\\rangle^{\\otimes n}$. Consider the scenario in which the variational (parameterized or learnable) circuit $W(\\Theta)$ is constructed by multiple layers of trainable circuit layer $V_i(\\theta_j)$ (depicted in Figure2), denoted as $W(\\Theta) = \\prod_{i=1}^M V_i(\\theta_j)$, where $\\Theta$ represents the collection of all trainable parameters ${\\theta_1,\\cdots,\\theta_n}$. Then the quantum state vector generated by the encoding circuit and variational circuit can be shown as,\n$|\\Psi\\rangle = W(\\Theta)U(x) |0\\rangle^{\\otimes n} = \\left( \\prod_{i=1}^{M} V_{i}(\\theta_{j}) \\right) U(x) |0\\rangle^{\\otimes n}$"}, {"title": "IV. EVOLUTIONARY QUANTUM ARCHITECTURE SEARCH", "content": "Consider the task of constructing a quantum circuit C, which is composed of several sub-components $S_1, S_2, . . . , S_n$. Each sub-component $S_i$ is associated with a set of available circuit options $B_i$, where $|B_i|$ represents the number of feasible choices for that particular sub-component. Therefore, the total number of possible configurations for the circuit C can be expressed as $N = |B_1|\\times |B_2|\\times \\cdots \\times |B_n|$. Here, our goal is to construct a quantum circuit in which the encoding sub-circuit $S_1$ and variational sub-circuit $S_2$ are to be found by certain evolutionary search algorithms. For a concrete example, we consider the case that the encoding sub-circuit can be constructed by Hadamard gates H and single-qubit rotations (Rx, Ry and Rz). The total number of possible encoding sub-circuit $B_1$ is therefore $|B_1| = 2 \\times 3 = 6$. For the variational sub-circuit, although there can be multiple ways to entangle the qubits, let's assume that we have two possible ways to perform the entanglements and again three single-qubit rotations (Rx, Ry and Rz) as in the encoding sub-circuit. Then, the total number of possible variational sub-circuit $B_2$ is therefore $|B_2| = 2 \\times 3 = 6$. The total number of realization for this circuit is therefore $N = B_1 \\times B_2 = 36$. The assumed circuit components in this work is shown in Figure 3. We can further consider the circuit with multiple layers of variational sub-circuit $S_2, S_3, \\cdots, S_n$. Circuits of this type are frequently employed when a larger number of trainable parameters is necessary to achieve the desired performance. When this is the case, the search space for the circuit grows to be $N = |B_1|\\times |B_2| \\times \\cdots \\times |B_n| = |B_1|\\times |B_2|^{n-1}$, with the assumption that there is a fixed number of allowed combinations for variational sub-circuit. To enable algorithmic search for quantum circuit combinations, we design the circuit representation as a dictionary data structure R.\n$R = \\left\\{\\begin{array}{ll} \\text{\"encoding\\_layer\"} & \\rightarrow (\\{x_1, x_2\\},), \\\\ \\text{\"variational\\_layer\"} & \\rightarrow [(\\mathbb{y}^{(1)}_1, y^{(1)}_2),\\cdots,(\\mathbb{y}^{(N)}_1, y^{(N)}_2)] \\end{array}\\right\\}$ (2)\nwhere $x_1 \\in \\mathbb{R}^{\\text{NUM\\_H\\_LAYERS}}, x_2 \\in \\mathbb{R}^{\\text{NUM\\_ROTATIONS}}$, $y^{(i)}_1 \\in \\mathbb{R}^{\\text{NUM\\_ENTANGLING}}, y^{(i)}_2 \\in \\mathbb{R}^{\\text{NUM\\_ROTATIONS}}$, $N =$ num_of_var_layers. For example, in the \"encoding_layer\", the vector $x_1$ controls whether or not Hadamard gates H are used to initialized the quantum state. Hence, the vector $x_1$ is in $R^2$. With the same logic, the vector $x_2$ controls which kind of rotations (e.g. Rx, Ry and Rz) is used for encoding values into quantum states, therefore $x_2$ is in $R^3$. The representation of the \u201cvariational_layer\u201d is consistent, where $y^{(i)}_1$ governs the entangling component and $y^{(i)}_2$ governs the rotation component of the i-th variational sub-circuit, respectively. Given a circuit representation R, sampling is required to get the actual circuit for performance evaluation. For example, the vector $x_1 \\in R^2$ will be converted into probabilities by softmax function. The actual circuit component will be sampled from this distribution, and a corresponding one-hot representation will be generated. Once this representation is created, the circuit can be constructed based on the predefined rules, such as those outlined in Table I. The representation R allows for easy mutation, as the encoding consists of floating-point numbers. By introducing small perturbations, we can subtly alter the quantum circuit properties without causing significant changes."}, {"title": "V. METHODS", "content": "In this paper, we present EvoQAS-ED, a QAS method designed to discover QNN architectures that meet a specified model complexity metric. The metric we use to assess QNN models is the effective dimension, which provides an indication of the model's capacity [39]. Our approach adheres to the definition of effective dimension as outlined in [39].\nDefinition 1. The effective dimension of a statistical model $M_{\\Theta} := {p(\\cdot, \\cdot; \\theta) : \\theta \\in \\Theta}$ with respect to $\\gamma\\in (0,1]$, a $d$-dimensional parameter space $\\Theta \\subset R^d$ and $n \\in N, n > 1$ data samples is defined as\n$\\text{log}\\gamma d_{y,n}(M_{\\Theta}) := 2 \\frac{\\text{log}\\left( \\frac{1}{V_{\\Theta}} \\int_{\\Theta} \\frac{\\text{det} (I_d + \\frac{\\gamma n}{2 \\text{log} n}F(\\theta)) }{ \\text{log} \\left( 1+ \\frac{\\gamma n}{2 \\text{log} n} \\right)^{d}} d\\theta \\right) } {\\frac{\\gamma n}{2 \\text{log} n}}$ (3)\nwhere $V_{\\Theta} := \\int_{\\Theta} d\\theta \\in R^+$ is the volume of the parameter space. $F(\\theta) \\in R^{d\\times d}$ is the normalised Fisher information matrix defined as $F_{ij}(\\theta) := \\frac{E_{(x,y) \\sim p} [ \\partial_{\\theta_i} \\text{log} p(x,y; \\theta) \\partial_{\\theta_j} \\text{log} p(x,y; \\theta) ]}{ \\text{Tr}(F(\\theta)) } d\\theta$, where the normalisation ensures that $\\int_{\\Theta} \\frac{\\partial_{\\theta_i} \\text{Tr}(F(\\theta))}{\\partial_{\\theta_i}} d\\theta = d$.\nThe Fisher information matrix is defined as $F(\\theta) = E_{(x,y) \\sim p} [\\text{log} p(x,y; \\theta) \\text{log} p(x,y; \\theta)^T] \\in R^{d\\times d}$ and can be approximated by the empirical Fisher information matrix $F_k(\\theta) = \\sum_{j=1}^k \\text{log} p(x_j,y_j; \\theta) \\text{log} p(x_j,y_j; \\theta)^T$. Specifically, we employ the effective dimension as the fitness function in our evolutionary search method. The primary constraint is that the QNN model must be differentiable."}, {"title": "VI. EXPERIMENTS", "content": "In EvoQAS-ED, we run simulations with a population size of $P = 50$. In each generation, we select the top 10 agents with the highest effective dimension (ED) scores to serve as parents. These parents are then mutated to create new circuit representations R for the next generation. The mutation process consists of adding Gaussian noise to the original circuit representation, $R \\leftarrow R + \\sigma \\epsilon$, where $\\sigma$ (set to 0.02) represents the mutation power, and $\\epsilon$ is Gaussian noise sampled from the normal distribution $N(0, I)$. In Figure4 and Figure 5, we present the results of the evolutionary optimization over 1000 generations. We conducted two experiments with slightly different configurations: the dataset size n used to evaluate the effective dimension $d_{\\gamma,n}$. As shown in both Figure4 and Figure5, there is no significant difference between the two settings. Additionally, we observe that the rewards for the top agents converge after only a few generations. These top agents are saved for further analysis. We selected dataset sizes of n = 1000 and n = 2000 to estimate the effective dimension because current QML training often relies on smaller datasets. This preference for smaller datasets is likely due to limitations in existing quantum simulations or hardware, as well as the desire to investigate the behavior of QML models in the small data regime. After 1000 generations of training, we sampled from the evolved quantum circuit representations with high effective dimension. A selection of circuits generated through this process is displayed in Figure 6. We analyze the behavior of these QNN architectures using different dataset sizes n, as described in Equation 3. In most cases/models, the effective dimension $d_{\\gamma,n}(M_{\\Theta})$ increases with n and eventually saturates. However, it is not guaranteed that $d_{\\gamma,n}(M_{\\Theta})$ will grow for all types of models, as noted in [39]. In Figure 7, we analyze the trend of effective dimension with respect to dataset size n. The three QNN architectures generated by the proposed evolutionary search method (as shown in Figure 6) exhibit the expected behavior: the effective dimension increases and then saturates. In contrast, the two baseline classical neural networks (NNs) show different behaviors. These classical NNs are designed to have a comparable number of trainable parameters to the QNNs. The key distinction is that one of the classical NNs lacks a non-linear activation function, such as ReLU. We observe that all QNNs identified by our method achieve a significantly higher effective dimension than the classical NNs with similar model sizes. Furthermore, even without non-linear activation functions, the QNNs display an effective dimension growth trend similar to that of classical NNs equipped with non-linear activations like ReLU. Next, we analyze the eigenvalue spectrum of the Fisher information matrix for the discovered QNNs and their corresponding classical NNs. Prior research [40] has shown that the Fisher information matrix of non-linear classical NNs tends to be highly degenerate, with a few large eigenvalues. This leads to a flat parameter space in most directions, as indicated by near-zero eigenvalues, while certain directions are highly distorted due to a few significantly large eigenvalues. Under certain conditions, the Hessian matrix coincides with the Fisher information matrix and exhibits a similar behavior [41]\u2013[43]. Such spectra have been reported to impede training and may result in suboptimal outcomes [44]. Similar effects have also been observed in quantum model training [39]: quantum models affected by the barren plateau phenomenon (vanishing gradient problem) exhibit Fisher information spectra with an increasing concentration of eigenvalues near zero as the number of qubits grows. On the other hand, models with Fisher information spectra not concentrated around zero are less likely to suffer from barren plateaus [39]. In this study, we numerically simulate one of our discovered QNN architectures with varying numbers of qubits to analyze their Fisher information spectra. We also compare these results to a classical NN counterpart with a comparable number of inputs (and number of parameters). In Figure 8, we observe that the QNN exhibits a more evenly distributed eigenvalue spectrum of the Fisher information matrix. This distribution remains consistent as the number of qubits increases from 4 to 7. In contrast, the eigenvalues in classical NN models, with varying input sizes (and thus different model parameters), are mostly concentrated around zero. This concentration is particularly evident in the subplots of each histogram, which depict the eigenvalue distribution over smaller ranges. Additionally, unlike in the classical NN, there are no outlying eigenvalues in the QNN."}, {"title": "VII. CONCLUSION", "content": "In this paper, we introduce EvoQAS-ED, which incorporates a quantum circuit representation method to efficiently discover QNN architectures with high effective dimension, indicating high model capacity. The proposed framework is adaptable and can be extended to various QNN metrics. Additionally, it holds the potential to integrate with existing QAS methods to search for high-performance QNN architectures tailored to a wide range of QML challenges."}]}