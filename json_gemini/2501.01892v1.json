{"title": "QuArch: A Question-Answering Dataset for AI Agents in Computer Architecture", "authors": ["Shvetank Prakash", "Andrew Cheng", "Jason Yik", "Arya Tschand", "Radhika Ghosal", "Ikechukwu Uchendu", "Jessica Quaye", "Jeffrey Ma", "Shreyas Grampurohit", "Sofia Giannuzzi", "Arnav Balyan Fin Amin", "Aadya Pipersenias", "Yash Choudhary", "Ankita Nayak", "Amir Yazdanbakhsh", "Vijay Janapa Reddi"], "abstract": "We introduce QuArch, a dataset of 1500 human- validated question-answer pairs designed to evaluate and enhance language models' understanding of computer architecture. The dataset covers areas including processor design, memory systems, and performance optimization. Our analysis highlights a significant performance gap: the best closed-source model achieves 84% accuracy, while the top small open-source model reaches 72%. We observe notable struggles in memory systems, interconnection networks, and benchmarking. Fine-tuning with QuArch improves small model accuracy by up to 8%, establishing a foundation for advancing AI-driven computer architecture research. The dataset and leaderboard are at https://harvard-edge.github.io/QuArch/.", "sections": [{"title": "I. INTRODUCTION", "content": "Generative Artificial Intelligence (GenAI) has transformed domain-specific tools across diverse fields such as medicine, mathematics, law, finance, and software engineering [1]. In contrast, hardware engineering has lagged significantly in adopting AI-driven solutions. This gap is evident in both the limitations of current language models (LMs) and the scarcity of specialized datasets tailored for hardware. For instance, engineering tasks often perform poorly on general benchmarks like MMLU-Pro [2], highlighting the inadequacy of existing models in understanding domain-specific intricacies. While electronic design automation (EDA) has seen recent progress with datasets for tasks such as register-transfer level (RTL) generation [3], [4], security analysis [5], and verification [6], computer architecture remains underrepresented. Without resources to benchmark and advance AI models, the field is limited in its ability to improve AI-driven solutions.\nDatasets play a key role in enabling AI agents. While general-purpose datasets provide broad knowledge, domain- specific datasets are indispensable for developing expertise in areas like computer architecture. These targeted datasets enable AI models to not only demonstrate foundational under- standing but also tackle advanced problem-solving tasks within specific domains [7]. Mastery of domain knowledge is a pre- requisite for sophisticated reasoning [8], [9]. In architecture, such proficiency is essential for developing practical AI-driven tools and agents [10]. Without a deep understanding of core concepts such as processor execution, memory hierarchy, and parallelism-it becomes impossible to conduct analyses of the complex trade-offs inherent in system design."}, {"title": "II. RELATED WORK", "content": "Recent efforts have explored the use of GenAI in hardware design. For example, NVIDIA's ChipNeMo [11] introduced foundation models tailored for chip design tasks, while other studies have focused on developing LM-based tools for RTL generation [4] and hardware verification [6]. Additionally, evaluation datasets in this domain have been created for specific implementation tasks, such as VerilogEval [3] for RTL generation, and general-purpose benchmarks like MMLU [12], which assess engineering knowledge broadly across disci- plines. However, none of these prior works specifically eval- uate LM understanding of computer architecture concepts. This critical gap limits the ability to assess and advance LM capabilities for architectural challenges. QuArch addresses this unmet need by introducing a focused question-answering dataset specifically designed to evaluate architectural knowl- edge (Figure 1). The dataset combines synthetic data gen- eration [13] with rigorous expert validation, ensuring both comprehensive coverage and high-quality questions."}, {"title": "III. QUARCH", "content": "In this section, we discuss the construction and characteris- tics of the first version of the dataset: QuArch v0.1."}, {"title": "A. Dataset Curation: The Archipedia Corpus", "content": "The construction of QuArch follows a systematic process, as depicted in Figure 2. We first curated \u201cArchipedia,\"\u00b9 a term we use to describe a comprehensive compilation of computer architecture knowledge that was assembled for this work. Archipedia synthesizes five decades of information, draw- ing from academic literature, educational materials, technical documentation, and industry sources across the computing landscape. This extensive corpus captures the evolution of the field over the past 50 years, incorporating contributions from leading institutions, researchers, and organizations globally. Currently, the corpus exceeds 1 billion tokens in size.\nArchipedia covers the full spectrum of computing systems, from foundational topics in computer architecture to cutting- edge technologies. It includes domains such as VLSI design\""}, {"title": "B. Dataset Generation: QA Creation", "content": "The knowledge curation phase (Steps 1 and 2 in Figure 2) established a foundation of well-accepted architectural concepts and principles by leveraging diverse sources. This effort utilized the Archipedia corpus, which provided a com- prehensive resource for generating questions for QuArch.\nIn the QA generation phase (Step 3), commercial LMs were used to synthesize questions grounded in the academic content of Archipedia to ensure technical rigor. The LMs were tasked with creating cloze-style multiple-choice QAs [7] to balance educational value with practical assessment.\nThe validation phase (Step 4) involved a multi-tiered review process that combined human expertise with LM as- sistance. Questions derived from undergraduate-level sources were reviewed by an expert with graduate-level architectural expertise, supplemented by LM validation using the \u201cLLM-as- a-judge\" technique [14]. Advanced topics were evaluated by a pool of eight experts, and QAs were independently validated by three reviewers who reached consensus to ensure accuracy.\nTo further enhance the validation process, human experts and LM reviewers received contextual fragments of the source text, transforming the task into a focused reading compre- hension exercise. This approach enabled the identification and removal of questions lacking definitive answers or those too narrowly scoped for meaningful assessment. The validation process, facilitated through the Label Studio platform [15], ensured the resulting dataset effectively tests both foundational principles and complex system trade-offs. The final dataset (Step 5) supplemented these expert-validated questions with additional QAs freely available from an online education platform, enriching the dataset's depth and coverage."}, {"title": "C. Dataset Coverage: Architecture Topics", "content": "QuArch v0.1 contains 1,547 question-answer pairs. It cap- tures the breadth of architecture in 13 core areas derived from key themes of the past decade (Figure 3). Processor"}, {"title": "IV. RESULTS", "content": "To evaluate the state of computer architecture knowledge embedded in LMs, we assess knowledge retrieval capabilities of SOTA models and explore opportunities to improve them."}, {"title": "A. Experimental Setup", "content": "We evaluated both open-source and closed-source language models on QuArch. The evaluation included large-scale mod- els such as GPT-40, Claude-3.5 Sonnet, and Gemini-1.5 Pro, as well as open-source models with parameter counts ranging from 1B to 70B from Google, Meta, and Mistral AI. Each model was presented with questions in a multiple-choice format, requiring the selection of the correct answer from four options. Models were prompted to \"act as computer architec- ture experts\" and were evaluated in a zero-shot setting, with no additional context provided beyond the questions them- selves. This setup was designed to test the models' baseline"}, {"title": "B. Understanding of Architecture Concepts", "content": "Figure 4 presents the baseline performance of LMs on QuArch. The top-performing model achieves 84% accuracy, reflecting a relatively strong but incomplete understanding of architecture concepts. In particular, a substantial knowledge gap exists: the best-performing small open-source LM (<10B parameters) underperforms by 12% on the same questions. The observed performance ceiling of 84% suggests that current LMs still have significant room for improvement in under- standing the fundamentals of computer architecture concepts. These findings have important implications for the develop- ment of agentic tools for hardware design [10]. While current models exhibit a reasonable grasp of basic architectural con- cepts, they may require supplementary support or verification mechanisms when addressing complex system-level decisions."}, {"title": "C. Analysis by Architecture Topics", "content": "Figure 5 presents a heatmap illustrating LM performance across various architecture topics. Each cell contains the raw accuracy values for a specific topic and corresponding model. To account for substantial differences in raw accuracy due to varying model capacities, the heatmap employs color gradients to represent performance relative to each model's overall accuracy. Dark green denotes the strongest performance for a given model, while dark red highlights the weakest, offering a clearer perspective on relative strengths and weaknesses.\nThe analysis reveals distinct patterns in how LMs com- prehend different architecture topics. Models exhibit their strongest performance in topics such as EDA concepts, IP design and manufacturing, parallel processing architecture"}, {"title": "D. QuArch as an Architecture Benchmark", "content": "To validate the effectiveness of QuArch as a benchmark, we evaluate its ability to distinguish between the capabilities of different LMs and compare it to established QA benchmarks. An effective benchmark should strike a balance-it should neither be trivial to solve nor overly challenging so that it's unattainable. QuArch satisfies this criterion, as even the top- performing models, such as GPT-40 and Claude 3.5-Sonnet, achieve accuracies of only 83\u201384%. This highlights substantial room for improvement in architectural understanding.\nThis performance ceiling is consistent with what is ob- served on other well-established QA benchmarks. As shown in Table I, the same models achieve comparable performance (88-89% [18], [19]) on general knowledge benchmarks like MMLU [12], which include engineering-related QAs. This suggests that QuArch's difficulty aligns well with other tech- nical assessments. In contrast, GPQA [20], one of the most challenging benchmarks available, achieves lower accuracies (54-59%) due to its hand-crafted questions that require ad- vanced QA skills beyond knowledge retrieval. QuArch's posi- tioning between MMLU and GPQA demonstrates its value as a meaningful and balanced measure of model capabilities. Fur- thermore, the room for improvement, particularly in advanced architecture topics, highlights QuArch's potential to track progress in LMs' understanding of computer architecture. However, further expansion and refinement of the dataset will be necessary to fully realize its benchmarking potential."}, {"title": "E. QuArch as an Architecture Training Dataset", "content": "ML datasets serve dual purposes: benchmarking and train- ing. In this section, we investigate whether QuArch can enhance the domain-specific knowledge of LMs through fine- tuning. To this end, we fine-tuned instruction-tuned variants of small open-source LMs using an 80-20 train-test split. To ensure robustness in the training evaluation, we employed repeated random train-test splitting with five different seeds.\nTable II reports mean test set accuracy improvements after fine-tuning on QuArch across different train-test splits. The results indicate significant performance gains, even with the relatively small size of the dataset. On average, instruction- tuned variants of Gemma-2-2B and Llama-3.2-3B demon- strated improvements ranging from 5.4% to 8.3%. These sub- stantial gains underscore the potential of QuArch to enhance LMs' understanding of computer architecture. Moreover, the results highlight the importance of developing larger, diverse datasets to further advance AI-based solutions in this domain."}, {"title": "V. CONCLUSION", "content": "QuArch is the first question-answering dataset for com- puter architecture, providing a means to evaluate domain knowledge. Through QuArch, we uncover both the strengths and limitations of SOTA LMS to reveal substantial room for improvement in this domain. QuArch benchmarks knowledge retrieval an essential foundation for advancing the integration of AI in computer architecture-but future datasets must build on QuArch to evaluate more complex capabilities, including advanced reasoning, system-level planning, and architectural design. Realizing these goals will require large-scale collabo- ration between academia and industry, ensuring AI tools for architecture evolve to meet the field's growing demands."}]}