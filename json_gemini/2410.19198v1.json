{"title": "MAP: MULTI-HUMAN-VALUE ALIGNMENT PALETTE", "authors": ["Xinran Wang", "Qi Le", "Ammar Ahmed", "Enmao Diao", "Yi Zhou", "Nathalie Baracaldo", "Jie Ding", "Ali Anwar"], "abstract": "Ensuring that generative AI systems align with human values is essential but chal-lenging, especially when considering multiple human values and their potentialtrade-offs. Since human values can be personalized and dynamically change overtime, the desirable levels of value alignment vary across different ethnic groups,industry sectors, and user cohorts. Within existing frameworks, it is hard to definehuman values and align AI systems accordingly across different directions simul-taneously, such as harmlessness, helpfulness, and positiveness. To address this,we develop a novel, first-principle approach called Multi-Human-Value Align-ment Palette (MAP), which navigates the alignment across multiple human valuesin a structured and reliable way. MAP formulates the alignment problem as an op-timization task with user-defined constraints, which define human value targets. Itcan be efficiently solved via a primal-dual approach, which determines whethera user-defined alignment target is achievable and how to achieve it. We conducta detailed theoretical analysis of MAP by quantifying the trade-offs between val-ues, the sensitivity to constraints, the fundamental connection between multi-valuealignment and sequential alignment, and proving that linear weighted rewards aresufficient for multi-value alignment. Extensive experiments demonstrate MAP'sability to align multiple values in a principled manner while delivering strong em-pirical performance across various tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in artificial intelligence (AI) have highlighted the critical need for aligningAl systems with human values, a concept known as human value alignment (Griffith et al., 2013;Arumugam et al., 2019; Gabriel, 2020). The alignment can serve the purpose of generating outcomesthat are better suited for human ethics (Griffith et al., 2013), personalized needs (Kirk et al., 2024),or reduced harmful content (Bai et al., 2022). This alignment has traditionally been pursued byadjusting AI behavior to adhere to specific attributes via preference datasets or reward functions.This process involves finetuning the original model according to the optimization problem:\n\n$\\max E_{x \\sim D, y \\sim p(\\cdot|x)} \\{ R(x, y) - \\beta D_{KL} (p(\\cdot | x) || p_0(\\cdot | x)) \\}.$ \n\nHere, P denotes the class of all distributions, $p_0$ is the distribution that represents the generativemodel to align, p is the distribution that represents the aligned model, R is a reward function thatquantifies the preference level of any given pair of prompt x and generation y, $D_{KL}$ measures theKL-divergence, and $\\beta > 0$ is a regularization hyperparameter. This formulation has deep conceptualroots in the Bayesian decision theoretic framework (Bissiri et al., 2016). Specifically, if we considerx as observed data and y as a parameter $\\theta$, the problem (1) can be expressed as $E_{\\theta \\sim p(\\theta)} \\{log p(x |$"}, {"title": "Challenges.", "content": "Despite significant progress in the domain, aligning AI models to multiple valuessimultaneously presents several unresolved challenges. First of all, as demonstrated in Figure 1,aligning with one value, such as helpfulness, harmlessness, or humor, could inadvertently diminishanother. These observations lead us to pose a critical question: How can we quantify and achievethe concurrent enhancement of multiple human values without compromise?\n\nMoreover, in the RLHF approach as described by the problem (1), it is unclear how to specify thehyperparameter \u03b2 and reward function R so that the aligned model p improves upon, or at least notworse than, $p_0$ in all human values in one shot (without trial-and-error). Even if we can try all possi-ble combinations, there is no theoretical justification that linearly combining individual reward func-tions is sufficient to obtain the Pareto Frontier. The DPO method, while simplifying the alignmentprocess through a direct empirical risk optimization, still does not address the issue of integratingmultiple data sources, which have their underpinning still at the problem (1). Recent studies havedemonstrated the sensitivity of results to different weights used in these aggregations (Bai et al.,2022). To highlight this point, in Figure 2, we visualize the range of possible \u03bb and of desirable\u03bb (which actually admits Pareto improvement on all the values). Figure 2(b) shows how addingadditional value-to-align narrows the range of desirable A compared with Figure 2(a."}, {"title": "Contributions.", "content": "We introduce the Multi-Human-Value Alignment Palette (MAP), a principled ap-proach designed to rigorously aligning multi-dimensional values with a provable guarantee. Similarto an artist's color palette, MAP enables the intricate blending of multiple human values to \u201cpaint\u201dthe Al's behavior with a broad spectrum of preference shades. In Figure 3, we illustrate how MAPallows users to precisely customize and control the level of improvement for all values in an inter-pretable manner. The proposed MAP introduces several technical innovations and contributions:\n\u2022 Formulation. We propose a novel problem formulation that allows one to align multiple humanvalues using user-defined constraints, which we term \"value palettes.\" Each palette acts as a con-straint that represents a preferred level of alignment, allowing us to \u201cMAP\u201d from any targeted valuelevels specified by the user to a particular reward function for (1). This precise one-to-one mappingensures exact adjustments to model behavior.\n\u2022 Theory. We provide theoretical analysis within the MAP framework quantifies the representationof the solution, its sensitivity to changes in the value palette, and its feasible operational range.This leads to a deeper understanding of the inherent trade-offs among various values. Furthermore,we investigate the range of realizable value levels and demonstrate that a linear combination ofindividual reward functions, is sufficient to reach the Pareto Frontier. We also establish a crucial"}, {"title": "2 MAP: MULTI-HUMAN-VALUE ALIGNMENT PALETTE", "content": null}, {"title": "2.1 PROBLEM FORMULATION", "content": "The formulation in (1) can be seen as maximizing the expected reward while imposing a regulariza-tion to minimize unnecessary deviations from the original model. This insight leads us to define avalue alignment through a statistical functional constraint:\n\n$E_{x \\sim D, y \\sim p(\\cdot|x)}r(x, y) \\geq c$,\n\nwhich is interpreted as the expected rewards, or realized levels, under a value preference must be atleast c. Likewise, to align m \u2265 1 value preferences, we introduce the following MAP problem:\n\n$\\min_{p \\in P} E_{x \\sim D, y \\sim p(\\cdot|x)} D_{KL} (p(\\cdot | x) || p_0(\\cdot | x)) \\text{ s.t. } E_{x \\sim D, y \\sim p(\\cdot|x)}r_i(x, y) \\geq c_i, \\forall i = 1,...,m$.\n\nWe denote $c = [c_1,..., c_m]^T$ as the value palette. With a solution p, we refer to $E_{x \\sim D, y \\sim p(\\cdot|x)} (r(x,y) = [r_1(x, y),..., r_m(x, y)]^T)$ as the realized value levels. We write u > v ifthe two vectors are of the same size and $u_i > v_i$ for each entry i.\nTheorem 1 (Representation of MAP solution). The solution to the MAP problem (3) is\n\n$p_\\lambda(y | x) = \\frac{1}{Z(x, \\lambda)} p_0(y|x)\\text{exp}\\lambda^T r(x,y)$,\n\nwhere $\\lambda^T r(x,y) = \\sum_{i=1}^m \\lambda_i r_i(x, y)$, for some $\\lambda \\geq 0$. Moreover, assuming that r(x, y) is nottrivially a constant on the support set of x, y, the above A is the unique solution in the problem:\n\n$\\max_{\\lambda>0} g(\\lambda) - log Z(\\lambda) + \\lambda^T c$,"}, {"title": "2.2 REALIZABLE VALUE LEVELS OF THE MAP PROBLEM AND PARETO FRONTIER", "content": "We first show that the MAP problem can be written as the original alignment problem (1) with aparticular reward function that is simply a linear combination of individual rewards."}, {"title": "2.3 COMPUTATIONAL SOLUTION TO MAP THROUGH A PRIMAL-DUAL APPROACH", "content": "In this section, we introduced a practical framework to solve MAP problem defined in (3), illustratedin Figure 5 and Algorithm 1.\n\nOnce a user defines a value palette according to Remark 2), in Step 2, we derive \u03bb from c as perTheorem 1. We then approximate Problem (5) as follows:\n\n$\\max_{\\lambda \\geq 0} g(\\lambda) = \\frac{1}{n} \\sum_{j=1}^n log e^{\\lambda^T r(x_j,y_j)} + \\lambda^T c,$\n\nwhere the dataset $\\{(x_j, y_j)_{j=1}^n\\}$ serves as a finite-sample approximation of the distribution $p_0(x, y)$with $y_1,..., y_n$, generated conditional on respective prompts $x_1, ..., x_n$. It can be verified that boththe original problem (5) and its approximation (9) are concave, allowing for the use of gradient"}, {"title": "2.4 SIMULTANEOUS VERSUS SEQUENTIAL OR GREEDY ALIGNMENT", "content": "To align a model to a specified value palette c, a natural baseline method is sequentially aligningindividual values. That is, we sequentially update the model to align each value to an entry in cuntil all values have been addressed. This section elaborates on this method and compares it withthe MAP procedure.\n\nSequential alignment algorithm. Suppose we have aligned with one value palette $c_{(e-1)}$ and thenupdate with $c_{(e)}$, l = 1, 2, . . . For notational convenience, let the initial $c_{(0)}$ be the realized level ofthe original model $p_0$. At the end of the l-th alignment, the aligned distribution can be expressed as$p_{(e)} (y | x) \\propto p_0(y | x) exp(\\lambda^T r(x, y))$ for some vector $\\lambda_{(l)}$.\n\nGiven the value palettes $\\{c_{(e)}\\}_{l=1,2,...,}$ we recursively obtain $\\lambda_{(e)}$ from $\\lambda_{({-1})}$. Like the problem(5), the alignment objective at the beginning of the l-th alignment is:\n\n$\\max_{\\lambda>0} g_{(e)} (\\lambda) = -log E_{x \\sim D, y \\sim P_{(e-1)}(x)}e^{\\lambda^T r(x,y)} + \\lambda^T c_{(l)}$.\n\nLike the problem (9), we can numerically solve (11) by addressing the problem:\n\n$\\max_{\\lambda>0} g_{(e)} (\\lambda) - log \\Big( \\text{Softmax}(\\lambda^T_{(l-1)} r_{1:n}) e^{\\lambda^T_e r_e} \\Big) + \\lambda^T c_{(l)}$.\n\nConnection between Sequential and Simultaneous Alignment. The problem (11) establishes aconnection between the (l \u2013 1)-th and l-th alignment. Recall that m denotes the number of values orthe dimension of c. In a canonical setting where one value is aligned at a time, define $\\{c_{(e)}\\}_{l=1,2,...}$by: $c_{(e)} = [-\\infty, ..., c_\\ell, -\\infty, ...]$, for l < m, where the l-th element is the same as that of c,and the others are trivially negative infinity. For l > m, we can re-align the first value, and so on.Namely, we replace the above $c_e$ in $c_{(l)}$ with $c_{\\ell mod m}$. The following result shows this sequential,iterative alignment process converges to the joint alignment using MAP.\n\n$\\Delta$Theorem 5. Let $p_{(0)} \\Delta p_0, p_{(1)}, p_{(2)}$, ... be the sequence of distributions obtained by sequentiallyaligning the original model according to the single-value MAP objective:\n\n$\\min_{p \\in P} E_{x \\sim D, y \\sim p(\\cdot|x)} D_{KL} (p(\\cdot | x) || P_{(l-1)}(\\cdot | x)) \\text{ s.t. } E_{x \\sim D, y \\sim p(\\cdot|x)}r_{\\ell'}(x, y) \\geq c_{\\ell'},$\n\nwhere l' = (l mod m). Assuming r(x, y) is not trivially a constant on the support set of (x, y), thissequence weakly converges to $p_\\lambda(c)$, which is the solution to the MAP problem in (4)."}, {"title": "3 EXPERIMENTAL STUDY", "content": "We generate prompts from two data sources: Anthropic harmless data (Anthropic, 2024), whichincludes human requests delineated between the tags \"Human:\" and \"Assistant:\", and IMDBdata (IMDB, 2024) from which we retain movie reviews exceeding 30 characters in length. Forbackbone models, we employ OPT-1.3B (Zhang et al., 2022) and Llama2-7B-chat (Touvron et al.,2023), which have demonstrated robust language modeling capabilities in previous assessments. Wefocus on several values for alignment: Humor, Positiveness, Harmlessness, Helpfulness, Diversity,Coherence, and Perplexity. The Humor reward is assessed using the logits from a humor detec-tion model (Dhiab, 2024), while Positiveness uses a DistilBERT model trained on IMDB sentimentanalysis (Lvwerra, 2024). Harmlessness and Helpfulness are evaluated through two GPT-2 modelsequipped with a value head fine-tuned to predict these attributes (Yang et al., 2024). Diversity ismeasured by the lexical variety within sentences, calculated through the proportion of unique n-grams (n = 2, 3, 4) and their composite score (Zhang et al., 2020). Coherence is determined by thesemantic similarity of sentences within a context, using a supervised SimCSE BERT-based modelthat captures sentence embeddings to assess textual coherence (Gao et al., 2021)."}, {"title": "3.2 EFFECTIVENESS OF MAP FOR SIMULTANEOUS MULTI-VALUE ALIGNMENT", "content": "For the conversational task, we use Anthropic data as input prompts x to align the OPT-1.3B modelacross six dimensions: Humor, Harmlessness, Helpfulness, Diversity, Coherence, and Perplex-ity. We tested seven different value palettes for alignment, considering both decoding-based andfinetuning-based implementations. For example, the HHH-80% palette aligns the first three valuesto the 80th percentile of their respective distributions, while maintaining the last three values. Thisapproach aims to enhance the model's output to closely resemble human-like interaction standardswithout compromising the intrinsic qualities of the generated content.\n\nAdditional experiments, including an ablation study with the Llama2-7B-chat model, are detailedin Appendix 3.3. Notably, the HHH-80% palette was determined to be feasible by Algorithm 1Step 2 and its results are therefore included. This indicates that the Llama2-7B-chat model, whichhas a larger complexity than OPT-1.3B, allows for more extensive multi-value alignment. We alsoexplored a sentiment-controlled open generation task using a random trunk of IMDB data as inputprompts to align the OPT-1.3B model."}, {"title": "3.3 ABLATION STUDY WITH LARGER MODEL", "content": "Adopting the same experimental framework as described in Subsection 3.2, we substituted the OPT-1.3B model with the Llama2-7B-chat model. Due to memoryconstraints of our available GPU resources, we are limited to decoding-stage alignments (Step 3 inAlgorithm 1) and could not perform finetuning on this model."}, {"title": "3.4 SIMULTANEOUS VERSUS SEQUENTIAL ALIGNMENT", "content": "To corroborate Theorem 5, we conducted experiments comparing the MAP with a sequential align-ment strategy, under the same experimental settings as described in Section 3.2. Specifically, wealign the OPT-1.3B model for a conversational task using Anthropic data. We implement sequentialalignment through one and five rounds. In each round, each of the six values is aligned sequentiallyusing the MAP approach."}, {"title": "3.5 COMPARISON WITH OTHER BASELINE APPROACHES", "content": "To demonstrate the effectiveness of MAP in reliably aligning multiple values, we conduct an exper-iment showing that MAP can identify desirable outcomes with randomly sampled A. According toTheorem 4, we sample \u03bb randomly from the range (co, B) and retain the feasible ones according toMAP's feasibility check. From these, we select A vectors with a bounded $l_1$-norm less than 6. Wecompare this with a standard MORL approach where \u03bb is randomly generated from su, where s isuniformly sampled from (0,6) and u is uniformly sampled from the probability simplex. For bothapproaches, we implement two alignment strategies:"}, {"title": "4 CONCLUSION", "content": "The proposed MAP offers a structured approach to aligning multiple human values, enabling preciseadjustments to meet diverse user preferences. Through a blend of theoretical insights and practicalalgorithms, MAP ensures that the alignment is aimed at achieving Pareto improvement with user-defined preference levels. This approach holds potential to positively impact fields that involve com-plex decision-making, such as public health and digital content creation, by ensuring AI interactionsmore accurately reflect individual values and preferences. Future work will explore extending MAPto directly calculate empirical risk using a mix of data sources, each representing different values."}, {"title": "A ADDITIONAL EXPERIMENTAL RESULTS", "content": null}, {"title": "A.1 HYPERPARAMETERS IN DATA GENERATION AND MODEL TRAINING", "content": "Our experiments were conducted using a single Nvidia A100 GPU. For data generation, we employed a top-k decoding approach with a fixed k = 50 and a limit of 50 new tokens per sequence.\n\nIn terms of model fine-tuning, we utilized the TRL package (von Werra et al., 2020) for DPO andPPO training. Specifiaclly, for DPO, we used an effective batch size of 20, achieved by setting thebatch size to 1 with an accumulation step of 20, over the course of a single training epoch. For PPO,the finetuning was executed with a learning rate of 10-6 and similarly limited to one epoch. Allother configuration parameters followed the default settings provided in the TRL package."}, {"title": "A.2 SAMPLE GENERATIONS", "content": "With the same experimental setting as in Section 3.2, Table 5 provides a snapshot of generated responses. It showcases how different value palettes of the OPT-1.3B models lead to varied generationoutcomes."}, {"title": "B ADDITIONAL DISCUSSIONS", "content": null}, {"title": "B.1 MORE ON RELATED WORK", "content": "The language model alignment methods can be broadly categorized into two approaches: training-based methods, which involve finetuning models during the training phase, and decoding-basedmethods, which rely on guided decoding during the inference phase.\n\nWithin the training-based approaches, two prominent techniques are reinforcement learning fromhuman feedback (RLHF) (Griffith et al., 2013; Arumugam et al., 2019; Bai et al., 2022) and directpreference optimization (DPO) (Rafailov et al., 2023).\n\nRLHF is a multi-step process to align large language models with human values and preferences.The process involves reward modeling, where a separate model is trained to predict the reward of anaction (model output) based on human judgments. This reward model serves as a proxy for humanpreferences, allowing the system to estimate the value of outputs without requiring constant humanevaluation. The training dataset typically consists of triplets (x, Y\u0131, Yw), where x is the prompt, andy\u0131 (lose) and yw (win) are two model-generated responses. Human experts rate these responsesfrom a particular perspective, such as quality, relevance, or appropriateness, with y\u03b9 < Yw. Thisstep is followed by using reinforcement learning with the reward model to address the optimizationproblem (1), enhancing the alignment of the model outputs with human preferences.\n\nDPO is recent approach to human value alignment that optimizes models based on explicit prefer-ences from pairwise comparisons or rankings. Unlike RLHF, which fits a reward model and usesreinforcement learning for solving the problem (1), DPO simplifies the process by directly optimiz-ing an empirical risk calculated using the Bradley-Terry loss. The reward score in Bradley-Terryloss is defined as: $r(x,y) \\triangleq \\beta \\log \\frac{P_\\omega(y|x)}{P_0(y|x)}$. This leads to the DPO objective:\n\n$L_{DPO}(P_\\omega;P_0) = -E_{(x,y_w,y_l) \\sim D} log \\sigma \\Bigg( \\beta \\Big( log \\frac{P_\\omega(y_w | x)}{P_0(y_w | x)} -log \\frac{P_\\omega(y_l|x)}{P_0(y_l|x)} \\Big) \\Bigg) $\n\nHere, $p_w$ represents the generative model (parameterized by w) being aligned, and $p_0$ is the originalmodel. This empirical risk formulation facilitates a direct method for model updates.\n\nWhile DPO simplifies the modeling process, it typically relies on preference datasets focused on asingle value, which complicates the integration of multiple preference aspects. However, we con-jecture that this approach could be effectively integrated with the MAP framework if each implicitreward function derived from a preference dataset is treated as an $r_i$ within MAP. Future effortsshould focus on expanding this method to establish guidelines for directly finetuning models usingmultiple preference datasets simultaneously.\n\nAnother direction of study focuses on decoding-stage alignment. Recent work by Khanov et al.(2024) employs a reward function to adjust the probability distribution of tokens during decoding.It can be regarded as an approximation to the sampling from the aligned model $p(y | x) \\propto p_0(y |x)\\cdot e^{r(x,y)}$. In a different approach, Ji et al. (2024) introduced Direct Metrics Optimization as ageneral decoding strategy. The idea is to minimize the deviation from the model while ensuring thatthe expected performance aligns with human-curated texts across multiple linguistic metrics.\n\nThe challenges inherent in aligning multiple human values naturally lead to the application of multi-objective optimization (MOO). MOO involves the simultaneous minimization of multiple, often"}]}