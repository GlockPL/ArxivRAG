{"title": "ADVWAVE: STEALTHY ADVERSARIAL JAILBREAK ATTACK AGAINST LARGE AUDIO-LANGUAGE MODELS", "authors": ["Mintong Kang", "Chejian Xu", "Bo Li"], "abstract": "Recent advancements in large audio-language models (LALMs) have enabled speech-based user interactions, significantly enhancing user experience and accelerating the deployment of LALMs in real-world applications. However, ensuring the safety of LALMs is crucial to prevent risky outputs that may raise societal concerns or violate Al regulations. Despite the importance of this issue, research on jailbreaking LALMs remains limited due to their recent emergence and the additional technical challenges they present compared to attacks on DNN-based audio models. Specifically, the audio encoders in LALMs, which involve discretization operations, often lead to gradient shattering, hindering the effectiveness of attacks relying on gradient-based optimizations. The behavioral variability of LALMs further complicates the identification of effective (adversarial) optimization targets. Moreover, enforcing stealthiness constraints on adversarial audio waveforms introduces a reduced, non-convex feasible solution space, further intensifying the challenges of the optimization process. To overcome these challenges, we develop AdvWave, the first jailbreak framework against LALMs. We propose a dual-phase optimization method that addresses gradient shattering, enabling effective end-to-end gradient-based optimization. Additionally, we develop an adaptive adversarial target search algorithm that dynamically adjusts the adversarial optimization target based on the response patterns of LALMs for specific queries. To ensure that adversarial audio remains perceptually natural to human listeners, we design a classifier-guided optimization approach that generates adversarial noise resembling common urban sounds. Furthermore, we employ an iterative adversarial audio refinement technique to achieve near-perfect jailbreak success rates on black-box LALMs, requiring fewer than 30 queries per instance. Extensive evaluations on multiple advanced LALMs demonstrate that AdvWave outperforms baseline methods, achieving a 40% higher average jailbreak attack success rate. Both audio stealthiness metrics and human evaluations confirm that adversarial audio generated by AdvWave is indistinguishable from natural sounds. We believe AdvWave will inspire future research aiming to enhance the safety alignment of LALMs, supporting their responsible deployment in real-world scenarios.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have recently been employed in various applications, such as chatbots (Zheng et al., 2024b; Chiang et al., 2024), virtual agents (Deng et al., 2024; Zheng et al., 2024a), and code assistants (Roziere et al., 2023; Liu et al., 2024). Building on LLMs, large audio-language models (LALMs) (Deshmukh et al., 2023; Nachmani et al., 2023; Wang et al., 2023; Ghosh et al., 2024; SpeechTeam, 2024; Gong et al., 2023; Tang et al., 2023; Wu et al., 2023; Zhang et al., 2023; Chu et al., 2023; Fang et al., 2024; Xie & Wu, 2024) incorporate additional audio encoders and decoders, along with fine-tuning, to extend their capabilities to audio modalities, which facilitates more seamless speech-based interactions and expands their applicability in real-world scenarios. Ensuring that LALMs are properly aligned with safety standards is crucial to prevent them from generating harmful responses that violate industry policies or government regulations, even in the face of adversarial jailbreak attempts (Wei et al., 2024; Carlini et al., 2024).\nDespite the significance of the issue, there has been limited research on jailbreak attacks against LALMs due to their recent emergence and the unique technical challenges they pose compared to deep neural network (DNN)-based attacks (Alzantot et al., 2018; Cisse et al., 2017; Iter et al., 2017; Yuan et al., 2018). Unlike end-to-end differentiable DNN pipelines, LALM audio encoders involve discretization operations that often lead to gradient shattering, making vanilla gradient-based optimization attacks less effective. Additionally, since LALMs are trained for general-purpose tasks, their behavioral variability makes it more difficult to identify effective adversarial optimization targets compared to DNN-based audio attacks. The requirement to enforce stealthiness constraints on adversarial audio further reduces the feasible solution space, introducing additional complexity to the challenging optimization process.\nTo address these technical challenges, we introduce AdvWave, the first approach for jailbreak attacks against LALMs. To overcome the issue of gradient shattering, we propose a dual-phase optimization framework, where we first optimize a discrete latent representation and then optimize the input audio waveform using a retention loss relative to the optimal latent. To tackle the difficulty in adversarial target selection caused by the behavioral variability of LALMs, we propose an adaptive adversarial target search method. This method transforms malicious audio queries into benign ones by detoxifying objectives, collecting LALM responses, extracting feasible response patterns, and then aligning these patterns with the malicious query to form the final adversarial target. To address the additional challenge of stealthiness in the jailbreak audio waveform, we design a sound classifier-guided optimization technique that generates adversarial noise resembling common urban sounds, such as car horns, dog barks, or air conditioner noises. The AdvWave framework successfully optimizes both effective and stealthy jailbreak audio waveforms to elicit harmful responses from LALMs, paving the way for future research aimed at strengthening the safety alignment of LALMs.\nFurthermore, we introduce an iterative adversarial audio refinement method to jailbreak black-box LALMs, where gradient-based optimization is not viable. Specifically, we craft prompts that guide LLMs in refining adversarial prompts through a series of empirical strategies, including role-playing scenarios, persuasive language, and prefix constraints, based on feedback from a judge model. These refined prompts are then converted into adversarial audio using OpenAI's TTS API. Our approach demonstrates both effectiveness and efficiency, successfully jailbreaking the SOTA black-box LALM GPT-4O-S2S API in just 30 queries, with near-perfect success.\nWe empirically evaluate AdvWave on three SOTA LALMs with general-purpose capabilities: SpeechGPT (Zhang et al., 2023), Qwen2-Audio (Chu et al., 2023), and Llama-Omni (Fang et al., 2024). Since there are no existing jailbreak attacks specifically targeting LALMs, we adapt SOTA text-based jailbreak attacks\u2014GCG (Zou et al., 2023), BEAST (Sadasivan et al., 2024), and AutoDAN (Liu et al., 2023)\u2014to the LALMs' corresponding LLM backbones, converting them into audio using OpenAI's TTS APIs. Through extensive evaluations and ablation studies, we find that: (1) AdvWave consistently achieves significantly higher attack success rates on white-box LALMs compared to strong baselines, while maintaining high stealthiness; (2) AdvWave attains near-perfect attack success rates against the black-box GPT-4O-S2S API, outperforming other baselines by a substantial margin; (3) the adaptive target search method in AdvWave improves attack success rates across various LALMs; (4) the sound classifier guidance effectively enhances the stealthiness of jailbreak audio without compromising attack success rates, even when applied to different types of environmental noise; and (5) GPT-4O-S2S exhibits varying levels of vulnerability across different safety categories, yet AdvWave successfully jailbreaks it across all categories."}, {"title": "2 RELATED WORK", "content": "Large audio-language models (LALMs) have recently extended the impressive capabilities of large language models (LLMs) to audio modalities, enhancing user interactions and facilitating their deployment in real-world applications. LALMs are typically built upon an LLM backbone, with an additional encoder to map input audio waveforms into the text representation space, and a decoder to map them back as output. One line of research (Deshmukh et al., 2023; Nachmani et al., 2023; Wang et al., 2023; Ghosh et al., 2024; SpeechTeam, 2024; Gong et al., 2023; Tang et al., 2023; Wu et al., 2023) focuses on LALMs tailored for specific audio-related tasks such as audio translation, speech recognition, scenario reasoning, and sound classification. In contrast, another line of LALMS (Zhang et al., 2023; Chu et al., 2023; Fang et al., 2024; Xie & Wu, 2024) develops a more general-purpose framework capable of handling a variety of downstream tasks through appropriate audio prompts. Despite their general capabilities, concerns about the potential misuse of LALMs, which could violate industry policies or government regulations, have arisen. However, given the recent emergence of LALMs and the technical challenges they introduce for optimization-based attacks, there have been few works into uncovering their vulnerabilities under jailbreak scenarios. In this paper, we propose the first jailbreak attack framework targeting advanced general-purposed LALMs and demonstrate a remarkably high success rate, underscoring the urgent need for improved safety alignment in these models before widespread deployment.\nJailbreak attacks on LLMs aim to elicit unsafe responses by modifying harmful input queries. Among these, white-box jailbreak attacks have access to model weights and demonstrate state-of-the-art adaptive attack performance. GCG (Zou et al., 2023) optimizes adversarial suffixes using token gradients without readability constraints. BEAST (Sadasivan et al., 2024) employs a beam search strategy to generate jailbreak suffixes with both adversarial targets and fluency constraints. AutoDAN (Liu et al., 2023) uses genetic algorithms to optimize a pool of highly readable seed prompts, minimizing cross-entropy with the confirmation response. COLD-Attack (Guo et al., 2024b) adapts energy-based constrained decoding with Langevin dynamics to generate adversarial yet fluent jailbreaks, while Catastrophic Jailbreak (Huang et al., 2024) manipulates variations in decoding methods to disrupt model alignment. In black-box jailbreaks, the adversarial prompt is optimized using feedback from the model. Techniques like GPTFuzzer (Yu et al., 2023), PAIR (Chao et al., 2023), and TAP (Mehrotra et al., 2023) leverage LLMs to propose and refine jailbreak prompts based on feedback on their effectiveness. Prompt intervention methods (Zeng et al., 2024; Wei et al., 2024) use empirical feedback to design jailbreaks with persuasive tones or virtual contexts. However, due to the significant architectural differences and training paradigms between LLMs and LALMs, these jailbreak methods, designed for text-based attacks, are ineffective when applied to LALMs. Issues such as gradient shattering, behavioral variability, and the added complexity of stealthiness in audio modality attacks limit their success. To address this gap, we introduce AdvWave, the first effective jailbreak method for audio modalities in LALMs."}, {"title": "3 ADVWAVE: STEALTHY ADVERSARIAL JAILBREAK AGAINST WHITE-BOX LALMS", "content": "Recent studies (Borsos et al., 2023; Chu et al., 2023; Fang et al., 2024; Gong et al., 2023; Wu et al., 2023; Xie & Wu, 2024) have introduced advanced large audio-language models (LALMs) that enhance human interaction through speech. These models are built upon large language models (LLMs) as their foundation, incorporating an audio encoder to project audio inputs into text representation space and a decoder to map the text space back into audio space. The encoder, decoder, and LLM backbone are jointly fine-tuned on audio-text data and undergo additional instruction fine-tuning to better align with human preferences and safety standards.\nThreat model of jailbreak against LALMs. We initiate jailbreak attacks on LALMs to elicit unsafe responses by submitting adversarial audio waveforms. Formally, we define the target LALM as $M: A \\times T\\rightarrow R$, where M maps the input from the joint audio space A and text space T to the response space R, which can output audio, text, or a combination of both, depending on the model's architecture. Let $a \\in A$ represent the original malicious audio input (e.g., an audio query such as \"Instruct on how to use a firearm to commit a crime\"), and $t \\in T$ represent the text instruction template (e.g., \"Answer the question in the audio\"). The target response $r \\in R$ (e.g., \"Here are the step-by-step instructions: ...\") is the harmful output that the attack aims to trigger. The objective of the jailbreak attack is to modify the audio waveform to $a_{adv}$ in a stealthy way and in order to provoke the target response r, formulated as the following optimization problem:\n$\\min_{a_{adv} \\in A} L_{adv} (M(a_{adv}, t), r), s.t. S(a, a_{adv}) \\ge s$  (1)\nwhere $L_{adv} (\\cdot, \\cdot)$ represents the adversarial loss function that measures the misalignment between the model response $M(a_{adv}, t)$ and the target response r, while $S(\\cdot, \\cdot): A \\times A \\rightarrow R$ is a function that quantifies the stealthiness of the adversarial audio $a_{adv}$ relative to the original audio a. A higher score indicates greater stealthiness, and $s \\in R$ is the constraint ensuring the adversarial audio remains sufficiently stealthy.\nTechnical challenges of LALMs jailbreak. Solving the jailbreak optimization problem in Equation (1) presents several technical challenges: (1) the audio encoder in LALMs contains non-differentiable discretization operators, leading to the gradient shattering problem, which obstructs direct gradient-based optimization; (2) LALMs exhibit high variability in response patterns, complicating the selection of effective target response for efficient optimization; and (3) enforcing the stealthiness constraint to jailbreak audio further reduces the feasible solution space, introducing additional complexity to the challenging optimization process. To address these challenges, we propose a dual-phase optimization paradigm to overcome the gradient shattering issue in the audio encoder in Section 3.2. We develop an adaptive target search algorithm to enhance optimization effectiveness aginst the behaviour variability of LALMs in Section 3.3. We also tailor the stealthiness constraint for the audio domain and introduce classifier-guided optimization to enforce this constraint into the objective function in Section 3.4. We provide the overview of AdvWave in Figure 1."}, {"title": "3.2 DUAL-PHASE OPTIMIZATION TO OVERCOME GRADIENT SHATTERING", "content": "Gradient shattering problem. A key challenge in solving the optimization problem in Equation (1) is the infeasibility of gradient-based optimization due to gradient shattering, caused by non-differentiable operators. In LALMs like SpeechGPT (Zhang et al., 2023), audio waveforms are first mapped to an intermediate feature space, where audio frames are tokenized by assigning them to the nearest cluster center, computed using K-Means clustering during training. This tokenization aligns audio tokens with the text token vocabulary, facilitating subsequent inference on the audio-language backbone. However, the tokenization process introduces nondifferentiability, disrupting gradient backpropagation towards the input waveform during attack, thus making vanilla gradient-based optimization infeasible.\nFormally, let $x \\in R^d$ represent the intermediate feature (generated by audio encoder) with dimensionality d, and let $c_i \\in R^d$ ($i \\in \\{1, ..., K\\}$) be the cluster centers derived from K-Means clustering during the training phase of LALMs. The audio token ID for the frame with feature x is determined via nearest cluster search: $I(x) = arg\\, min_{i \\in \\{1,...,K\\}} |x - c_i|_2$. After tokenization, the resulting audio token IDs are concatenated with text token IDs for further inference. During the tokenization process in the intermediate space after audio encoder mapping, the arg min operation introduces nondifferentiability, inducing gradient shattering issue.\nDual-phase optimization to overcome gradient shattering. To address this issue, we introduce a dual-phase optimization process that enables optimization over the input waveform space. (1) In Phase I, we optimize the audio token vector using the adversarial objective $L_{adv}$. (2) In Phase II, we optimize the audio waveform $a_{adv}$ using a retention loss $L_{retent}$ to enforce retention regarding the optimum token vector optimized in Phase I.\nFormally, the LALM mapping $M(\\cdot, \\cdot)$ can be decomposed into three components: the audio encoder, the tokenization module, and the audio-language backbone module, denoted as $M = M_{encoder} \\circ M_{tokenize} \\circ M_{ALM}$. The audio encoder $M_{encoder}: A \\times T \\rightarrow R^{L_A \\times d} \\times R^{L_T \\times d}$ maps the input audio waveform and text instruction template into audio features and text features with maximal lengths of audio frames $L_A$ and maximal lengths of text tokens $L_T$ (with dimensionality d). The tokenization module $M_{tokenize}: R^{L_A \\times d} \\times [R^{L_T \\times d} \\rightarrow \\{1, ..., K\\}^{L_A} \\times \\{K + 1,..., N\\}^{L_T}$ converts the features into token IDs via nearest-neighbor search on pre-trained cluster centers in the feature space. This means that $\\{1,..., K\\}$ represent audio token IDs, while $\\{K + 1, ..., N\\}$ represent text token IDs. Also, let $I_A \\in \\{1, ..., K\\}^{L_A}$ represent the audio token vector and $I_T \\in \\{K + 1, ...,N\\}^{L_T}$ represent the text tokens after the tokenization module $M_{tokenize}$. The audio-language backbone module $M_{ALM}: \\{1, ..., K\\}^{L_A} \\times \\{K + 1,..., N\\}^{L_T} \\rightarrow R$ maps the discrete audio and text token vectors into the response space. Note that we assume that the text token vector $I_T$ is fixed and non-optimizable since it does not depend on the input audio waveform (i.e., the decision variable of the jailbreak optimization).\nSince the tokenized vector $I_A$ shatters the gradients, we directly view it as the decision variable in Phase I optimization:\n$I_A = arg\\, min_{I_A \\in \\{1,...,K\\}^{L_A}} L_{adv} (M_{ALM}(I_A, I_T), r)$ (2)\nwhere $I_A^*$ represents the optimized adversarial audio token vector that minimizes the adversarial loss $L_{adv}$, thereby triggering the target response r.\nThen, the next question becomes: how to optimize the input audio waveform $a_{adv}$ to enforce that the audio token vector matches the optimum $I_A^*$ during Phase I optimization. To achieve that, we define a retention loss $L_{retent}: R^{L_A \\times d} \\times \\{1, ..., K\\}^{L_A} \\rightarrow R$, which takes the intermediate feature and target audio vector as input and output the alignment score. In other words, the retention loss $L_{retent}$ enforces that the audio token vector matches the optimum adversarial ones from Phase I optimization. We apply triplet loss to implement the retention loss:\n$L_{retent}(x, I) = \\sum_{j \\in \\{1,..., L_A\\}} max \\left( |x_j - c_{I_j}|_2 - \\max_{i \\in \\{1,...,K\\}\\{I_j\\}} |x_j - c_i|_2 + \\alpha, 0 \\right)$ (3)\nwhere $\\alpha$ is a slack hyperparameter that defines the margin for the optimization. The retention loss enforces that for each audio frame (indexed by j), the encoded feature $x_j$ should be close to the cluster center of target token ID $c_{I_j}$ and away from others. We also implement simple mean-square loss, but we find that the triplet loss facilitates the optimization much better.\nFinally, Phase II optimization can be formulated as:\n$a_{adv}^* = arg\\, min_{a_{adv} \\in A} L_{retent} (M_{encoder}(a_{adv}, t), I_A^*)$ (4)\nwhere $a_{adv}^*$ is the optimized adversarial audio waveform achieving minimal retention loss $L_{retent}$ between the mapped features by the audio encoder module $M_{encoder}(a_{adv}, t)$ and the target audio token vector $I_A^*$, which is optimized to achieve optimal adversarial loss during Phase I."}, {"title": "3.3 ADAPTIVE ADVERSARIAL TARGET SEARCH TO ENHANCE OPTIMIZATION EFFICIENCY", "content": "With the dual-phase optimization framework described in Equations (2) and (4), we address the gradient shattering problem in LALMs and initiate the optimization process outlined in Equation (1). However, we observe that the optimization often fails to converge to the desired loss level due to the inappropriate selection of the target response r. This issue is particularly pronounced because of the high behavior variability in LALMs. When the target response r deviates significantly from the typical response patterns of the audio model, the effectiveness of the optimization diminishes. This behavior variability occurs at both the model and query levels. At the model level, different LALMs exhibit distinct response tendencies. For example, SpeechGPT (Zhang et al., 2023) often repeats the transcription of the audio query to aid in understanding before answering, whereas Qwen2-Audio (Chu et al., 2023) tends to provide answers directly. At the query level, the format of malicious user queries (e.g., asking for a tutorial/script/email) leads to varied response patterns.\nAdaptive adversarial optimization target search. Due to the behavior variability of LALMs, selecting a single optimization target for all queries across different models is challenging. To address this, we propose dynamically searching for a suitable optimization target for each query on a specific model. Since LALMs typically reject harmful queries, the core idea is to convert harmful audio queries into benign counterparts through objective detoxification, then analyze the LALM's response patterns, and finally fit these patterns back to the malicious query as the final optimization target. The concrete steps are as follows: (1) we prompt the GPT-40 model to paraphrase harmful queries into benign ones (e.g., converting \"how to make a bomb\" to \"how to make a cake\") using the prompt detailed in Appendix A.1; (2) we convert these modified, safe text queries into audio using OpenAI's TTS APIs; (3) we collect the LALM responses to these safe audio queries; and (4) we prompt the GPT-40 model to extract the feasible response patterns of LALMs, based on both the benign modified queries and the original harmful query, following the detailed prompts in Appendix A.2. We directly validate the effectiveness of the adaptive target search method in Section 5.4 and provide examples of searched targets in Appendix A.4."}, {"title": "3.4 STEALTHINESS CONTROL WITH CLASSIFIER-GUIDED OPTIMIZATION", "content": "Adversarial audio stealthiness. In the image domain, adversarial stealthiness is often achieved by imposing $l_p$-norm perturbation constraints to limit the strength of perturbations (Madry, 2017) or by aligning with common corruption patterns for semantic stealthiness (Eykholt et al., 2018). In the text domain, stealthiness is maintained by either restricting the length of adversarial tokens (Zou et al., 2023) or by limiting perplexity increases to ensure semantic coherence (Guo et al., 2024a). However, in the audio domain, simple perturbation constraints may not guarantee stealthiness. Even small perturbations can cause significant changes in syllables, leading to noticeable semantic alterations (Qin et al., 2019). To address this, we constrain the adversarial jailbreak audio, by appending an audio suffix $a_{suf}$, consisting of brief environmental noises to the original waveform, a. This ensures that the original syllables remain unaltered, and the adversarial audio blends in as background noise, preserving semantic stealthiness. Drawing from the categorization of environmental sounds in (Salamon & Bello, 2017), we incorporate subtle urban noises, such as car horns, dog barks, and air conditioner hums, as adversarial suffixes. To evaluate the stealthiness of the adversarial audio, we use both human judgments and waveform stealthiness metrics to determine whether the audio resembles unintended noise or deliberate perturbation. Further details are provided in Section 5.1.\nClassifier-guided stealthiness optimization. To explicitly enforce the semantic stealthiness of adversarial audio during optimization, we introduce a stealthiness penalty term into the objective function, relaxing the otherwise intractable constraint. Inspired by classifier guidance in diffusion models for improved alignment with text conditions (Dhariwal & Nichol, 2021), we implement a classifier-guided approach to direct adversarial noise to resemble specific environmental sounds. We achieve this by incorporating an environmental noise classifier, leveraging an existing LALM, and applying a cross-entropy loss between the model's prediction and a predefined target noise label $q \\in Q$ (e.g., car horn). This steers the optimized audio toward mimicking that type of environmental noise. We refer to this classifier-guided cross-entropy loss for stealthiness control as $L_{stealth}: A \\times Q \\rightarrow R$. The optimization problem from Equation (1), with stealthiness constraints relaxed into a penalty term, can now be formulated as:\n$\\min_{a_{adv} \\in A} L_{adv} (M(a_{adv}, t), r) + \\lambda L_{stealth} (a_{adv}, q_{target})$  (5)\nwhere $q_{target}$ represents the target sound label and $\\lambda \\in R$ is a scalar controlling the trade-off between adversarial optimization and stealthiness optimization."}, {"title": "3.5 ADVWAVE FRAMEWORK AGINST WHITE-BOX LALMS", "content": "Finally, we summarize the end-to-end jailbreak framework, AdvWave, which integrates the dual-phase optimization from Section 3.2, adaptive target search from Section 3.3, and stealthiness control from Section 3.4.\nGiven a harmful audio query $a \\in A$ and a target LALM $M(\\cdot, \\cdot) \\in M$ from the model family set M, we first apply the adaptive target search method, denoted as $F_{ATS}: A \\times M \\rightarrow R$, to generate the adaptive adversarial target $r_{ATS} = F_{ATS}(a, M)$. Next, we perform Phase I optimization, optimizing the audio tokens to minimize the adversarial loss with respect to the target $r_{ATS}$ as follows:\n$I_A^* = arg\\, min_{I_A \\in \\{1,...,K\\}^{L_A}} L_{adv} (M_{ALM}(I_A, I_T), r_{ATS})$ (6)\nIn Phase II optimization, we optimize the input audio waveform to enforce retention to the optimum of Phase I optimization in the intermediate audio token space while incorporating stealthiness control, formulated as:\n$a_{adv}^* = arg\\, min_{a_{adv} \\in A} L_{retent} (M_{encoder}(a_{adv}, t), I_A^*) + \\lambda L_{stealth} (a_{adv}, q_{target})$ (7)\nwhere $a_{adv}^*$ is the optimized audio waveform that ensures alignment between the encoded audio tokens and the adversarial tokens $I_A^*$ via the retention loss $L_{retent}$. The complete pipeline of AdvWave is presented in Figure 1."}, {"title": "4 ADVWAVE: EFFICIENT ADVERSARIAL JAILBREAK AGAINST BLACK-BOX LALMS", "content": "In Section 3, we introduce the AdvWave framework for jailbreaking white-box LALMs by employing dual-phase optimization to address gradient shattering, adaptive adversarial target search to improve optimization efficiency, and classifier-guided optimization is used to enforce stealthiness. In this section, we also present an effective method for jailbreaking black-box LALMs, where gradient-based optimization is impractical.\nSimilar to the white-box jailbreak scenario, we define the target LALM as $M: A \\times T\\leftrightarrow R$, where M maps inputs from the joint audio space A and text space T to the response space R, which can produce audio, text, or a combination, depending on the model's architecture. Let $J: R \\times (T \\times A) \\rightarrow [0, 1]$ be a judge model that scores the LALM's response, where a higher score indicates a more effective jailbreak response for a given audio-text input pair. The jailbreak process can then be formulated as the following optimization problem:\n$\\max_{a_{adv} \\in A} J (M(a_{adv}), a, t)$  (8)\nTo address the optimization problem in a gradient-free manner in the black-box jailbreak setting, we employ a refinement model $\\pi$ that iteratively updates the adversarial jailbreak audio $a_{adv}$, guided by the reward provided by the judge model:\n$a_{adv} \\leftarrow \\pi(a_{adv}, J(M(a_{adv}), a, t))$  (9)\nFor the text modality in black-box jailbreaks, the refinement model can be instantiated using systematic graph-based prompt refinements (Mehrotra et al., 2023; Chao et al., 2023), reinforcement learning-based policy models (Chen et al., 2024), or genetic algorithms for evolving prompts (Liu et al., 2023). In the audio modality black-box jailbreaks, we find that simple LLM-based adversarial prompt refinements are sufficient to generate effective jailbreak prompts. Specifically, we craft prompts that ask LLMs to refine the adversarial prompts using several empirical techniques, such as role-playing scenarios, persuasive tones, and prefix enforcements, based on feedback from the judge model. These refined prompts are then converted into adversarial audio $a_{adv}$ using OpenAI's TTS API. The complete prompt we use is provided in Appendix A.5. Our method proves highly effective and efficient, successfully jailbreaking the state-of-the-art black-box LALM GPT-4O-S2S API within just 30 queries, achieving near-perfect success.\nAdvWave framework on ALMs with different architectures. Some ALMs such as (Tang et al., 2023) bypass the audio tokenization process by directly concatenating audio clip features with input"}, {"title": "5 EVALUATION RESULTS", "content": "We evaluate three open-source Large audio-language models (LALMs) with general capacities: SpeechGPT (Zhang et al., 2023), Qwen2-Audio (Chu et al., 2023), and Llama-Omni (Fang et al., 2024), and one close-source LALM GPT-4O-S2S API. All these models are built upon LLMs as the core with additional audio encoders and decoders for adaptation to audio modalities. Each model has undergone instruction tuning to align with human prompts, enabling them to handle general-purpose user interactions. For these reasons, we selected these three advanced LALMs as our target models.\nBaselines. We consider two types of baselines: (1) unmodified audio queries from AdvBench-Audio for vanilla generation (Vanilla), and (2) transfer attacks from text-domain jailbreaks on AdvBench, where jailbreak prompts optimized for text are transferred to audio using OpenAI's TTS APIs. As discussed in Section 3.1, there is currently no adaptive jailbreak method for LALMs due to the challenge of gradient shattering. Therefore, we transfer state-of-the-art (SOTA) jailbreaks from the text domain to the audio domain as strong baselines. Specifically, we use three SOTA jailbreaks: GCG (Zou et al., 2023), BEAST (Sadasivan et al., 2024), and AutoDAN (Liu et al., 2023). GCG optimizes adversarial suffixes using token gradients without readability constraints. BEAST employs a beam search strategy to generate jailbreak suffixes with adversarial targets and fluency constraints. Auto-DAN uses genetic algorithms to optimize a pool of highly readable seed prompts, which achieves high fluency but involves significant augmentation of the original prompt. These three jailbreaks are selected based on their advanced effectiveness and varying degrees of readability, which may influence their jailbreak effectiveness in the audio domain. We denote the transfer of these attacks to audio modalities as GCG-Trans, BEAST-Trans, and AutoDAN-Trans, respectively. We select the surrogate text models for these text jailbreaks based on the backbone LLMs of each audio-language model: Llama2 for SpeechGPT, Qwen2 for Qwen2-Audio, and Llama2 for Llama-Omni.\nEvaluation metrics. We assess the effectiveness of jailbreak attacks using two key metrics: the attack success rate (ASR) and the stealthiness score (Sstealth) of the adversarial audio queries. For the attack success rate, we evaluate both word-level detection (ASR-W) as in (Zou et al., 2023), and semantics-level judgment using an LLM-based model (ASR-L) as in (Xie et al., 2024). Specifically, for ASR-W, a jailbreak is considered successful if none of the rejection phrases from the list used in (Zou et al., 2023) (e.g., \"I'm sorry,\" \"I cannot help you\") appear in the LALM responses. For ASR-L, we use a fine-tuned LLM judge model from (Xie et al., 2024) to determine if the LLM's response is harmful and aligned with the user's query. It is important to note that harmfulness detection is performed on the text output of the LALMs, as we found that using audio models for direct judgment lacks precision. This highlights the need for future work on fine-tuning audio models to evaluate harmfulness directly in the audio modality. However, since we observe that the audio and text outputs from LALMs are generally well-aligned, using an LLM judge for text evaluation remains a reasonable approach.\nWe also assess the stealthiness of the adversarial audio waveform using the stealthiness score Sstealth (where higher values indicate greater stealthiness), defined as:\n$S_{stealth} = (S_{NSR} + S_{Mel-Sim} + S_{Human})/3.0$  (10)\nHere, $S_{NSR}$ represents the noise-signal ratio (NSR) stealthiness, scaled by 1.0 \u2013 NSR/20.0 (where 20.0 is an empirically determined NSR upper bound), ensuring the value fits within the range [0, 1]. $S_{Mel-Sim}$captures the cosine similarity (COS) between the Mel-spectrograms of the original and adversarial audio waveforms, scaled by (COS +1.0)/2.0 to fit within [0, 1]. $S_{Human}$ is based on human"}, {"title": "5.2 ADVWAVE ACHIEVES SOTA ATTACK SUCCESS RATES ON DIVERSE LALMS WHILE MAINTAINING IMPRESSIVE STEALTHINESS SCORES", "content": "We evaluate the word-level attack success rate (ASR-W), semantics-level attack success rate (ASR-L) using an LLM-based judge, and the stealthiness score (Sstealth) as defined in Equation (10), on SpeechGPT, Qwen2-Audio, and Llama-Omni using the AdvBench-Audio dataset. The results in Table 1 highlight the superior effectiveness of AdvWave across both attack success rate and stealthiness metrics compared to baseline methods. Specifically, for all three models, SpeechGPT, Qwen2-Audio, and Llama-Omni, AdvWave consistently achieves the highest values for both ASR-W and ASR-L. On average, AdvWave achieves an ASR-W of 0.838 and an ASR-L of 0.746, representing an improvement of over 50% compared to the closest baseline, AutoDAN-Trans. When comparing ASR performance across different LALMs, we observe that SpeechGPT poses the greatest challenge"}]}