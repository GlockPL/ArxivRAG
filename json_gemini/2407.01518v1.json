{"title": "Towards Multimodal Open-Set Domain Generalization and Adaptation through Self-supervision", "authors": ["Hao Dong", "Eleni Chatzi", "Olga Fink"], "abstract": "The task of open-set domain generalization (OSDG) involves recognizing novel classes within unseen domains, which becomes more challenging with multiple modalities as input. Existing works have only addressed unimodal OSDG within the meta-learning framework, without considering multimodal scenarios. In this work, we introduce a novel approach to address Multimodal Open-Set Domain Generalization (MM-OSDG) for the first time, utilizing self-supervision. To this end, we introduce two innovative multimodal self-supervised pretext tasks: Masked Cross-modal Translation and Multimodal Jigsaw Puzzles. These tasks facilitate the learning of multimodal representative features, thereby enhancing generalization and open-class detection capabilities. Additionally, we propose a novel entropy weighting mechanism to balance the loss across different modalities. Furthermore, we extend our approach to tackle also the Multimodal Open-Set Domain Adaptation (MM-OSDA) problem, especially in scenarios where unlabeled data from the target domain is available. Extensive experiments conducted under MM-OSDG, MM-OSDA, and Multimodal Closed-Set DG settings on the EPIC-Kitchens and HAC datasets demonstrate the efficacy and versatility of the proposed approach.", "sections": [{"title": "1 Introduction", "content": "Domain generalization (DG) and adaptation (DA) significantly improve the ro- bustness and adaptability of machine learning models, enabling them to per- form effectively across diverse and previously unseen environments [61]. This en- hancement ensures that the models are more readily transferable to real-world applications, such as autonomous driving [13, 15] and action recognition [10]. To address the challenges posed by distribution shifts, a wide range of DG and"}, {"title": "2 Related Work", "content": "Self-supervised Learning generates supervisory signals based on the input data, alleviating the need for expensive manual labeling and improving perfor- mance across almost all types of downstream tasks, including domain generaliza- tion [6] and anomaly detection [21]. The majority of self-supervised tasks have so far focused on a single modality. Examples of such self-supervised tasks in- clude predicting the relative position of image patches [12], colorizing grayscale"}, {"title": "3 Methodology", "content": "In this section, we first introduce the formulation of the Multimodal Open-Set Domain Generalization problem. Subsequently, we present our novel framework designed to address this challenge. This framework incorporates the use of self- supervised pretext tasks, specifically the proposed Masked Cross-modal Trans- lation and Multimodal Jigsaw Puzzles, complemented by an entropy weighting mechanism. Finally, we extend our framework to tackle Multimodal Open-Set Domain Adaptation. A visual representation of our approach is depicted in Fig. 1."}, {"title": "3.1 Multimodal Open-Set Domain Generalization", "content": "In the context of MM-OSDG, we work with several source domains, denoted as $S = \\{D_1, D_2, ..., D_s\\}$, each sharing a common label space $C$. We also consider an unseen target domain $T = \\{D_T\\}$ that introduces an extended label space $C\\cup U$, with the stipulation that $C$ and $U$ do not overlap $C\\cap U = \\emptyset$. Each source domain, indexed by $s$ and comprising $N_s$ samples, is represented as $D_s = \\{(x_i^s, y_i^s)\\}_{i=1}^{N_s}$ following the distribution $P_{xy}^s$. Here, $x_i^s$ denotes the $i$-th sample from the sample space $X$ and $y_i^s$ is the corresponding label, which belongs to"}, {"title": "3.2 Motivation: Self-supervised Pretext Tasks", "content": "Self-supervised pretext tasks are commonly classified into generative and con- trastive methods [43]. Contrastive approaches, such as context spatial rela- tions [20, 45] and instance discrimination [8, 24], focus on encouraging class-invariance between different instances [43], thereby contributing to generaliza- tion. However, recent findings [55] indicate that contrastive methods may tend to learn specific patterns of categories, which are beneficial for classification, but less conducive to understanding intrinsic data distributions of in-distribution sam- ples. In contrast, generative methods [11, 23] force the network to learn the real data distribution of in-distribution samples during training, enhancing the diver- gence between in-distribution and out-of-distribution samples [39], making them particularly valuable for open-class detection. Motivated by these observations, we propose using both generative and contrastive tasks to achieve robust gen- eralization and reliable open-class detection. Accordingly, we propose two novel multimodal self-supervised pretext tasks aligned with these two objectives. Mul- timodal self-supervised tasks are challenging, requiring the joint consideration of all modality properties to ensure meaningful outcomes."}, {"title": "3.3 Generative Task: Masked Cross-modal Translation", "content": "The cross-modal translation module introduced in SimMMDG [14] can be re- garded as an auto-encoding generative approach. It aims to ensure the mean-"}, {"title": "3.4 Contrastive Task: Multimodal Jigsaw Puzzles", "content": "Context-based spatial relation prediction [12, 20] falls within the category of contrastive objectives [43]. Previous studies [6, 48] have explored learning visual representations through solving Jigsaw puzzles, a task that involves recovering an original image from its shuffled parts. In this work, we extend the Jigsaw puzzle-solving task to multimodal scenarios. Given the heterogeneity of different modalities, shuffling them in the input space and then randomly combining them poses a challenge. Therefore, our approach involves shuffling in the embedding space for all modalities. Given the embedding $E_i$ of the $i$-th modality, we split it into $P$ parts as $E_i = [E_{i,1}, E_{i,2}, ..., E_{i,P}]$ with equal length. Similarly, for the"}, {"title": "3.5 Entropy Weighting and Minimization", "content": "Distinct modalities may contribute differently to the final predictions under var- ious circumstances. For instance, in recognizing the \"running\" action within a noisy gym, video data may be more reliable than audio data for the final predic- tion. Conversely, in scenarios characterized by low environmental light or partial shading of the camera, audio input may prove more dependable. To address the need for balancing the contributions of different modalities during train- ing, we propose employing the prediction entropy of each modality to weigh their respective losses. The entropy $H(\\hat{y})$ of model predictions $\\hat{y}$ is calculated as $H(\\hat{y}) = -\\sum_c p(\\hat{y}_c) \\log p(\\hat{y}_c)$, where $\\hat{y}_c$ represents the probability of class $c$. En- tropy serves as a metric for prediction confidence, with higher confidence yielding lower entropy values. For each prediction $\\hat{y}^k$ from the $k$-th modality, we calculate an entropy $H(\\hat{y}^k)$ and a cross-entropy loss $L_{cls}^k = CE(\\hat{y}^k, y)$. The objective is to assign less weight to the modality with higher entropy in contributing to the final prediction loss. Given $M + 1$ entropy values $H(\\hat{y}^o), H(\\hat{y}^1), ..., H(\\hat{y}^M)$, the weight of the $k$-th prediction is calculated by:\n$w_k = \\frac{e^{-H(\\hat{y}^k)/T}}{\\sum_i e^{-H(\\hat{y}^i)/T}}$\nwhere $T$ is the temperature. Then, the final prediction loss is defined as:\n$L_{cls} = \\sum_k w_kL_{cls}^k$\\nSimilar to previous work [60], we also add an entropy minimization loss as:\n$L_{EntMin} = \\sum_k H(\\hat{y}^k)$"}, {"title": "3.6 Final Loss and Inference", "content": "The final loss is obtained as the weighted sum of the previously defined losses:\n$L = L_{cls} + \\alpha_1L_{MaskedTrans} + \\alpha_2L_{MulJig} + \\alpha_3L_{EntMin}$,\nwhere $L_{cls}$ is the cross-entropy loss for classification. The hyperparameters $\\alpha_1$, $\\alpha_2$, and $\\alpha_3$ control the relative importance of the Masked Cross-modal Transla- tion, Multimodal Jigsaw Puzzles, and Entropy Minimization, respectively.\nDuring inference, we use $\\hat{y}$ as the final prediction. Following the approach outlined in previous research [63], we employ a threshold on the prediction con- fidence. Samples with a confidence level lower than the specified threshold are subsequently labeled as belonging to an open class."}, {"title": "3.7 Extension to Multimodal Open-Set Domain Adaptation", "content": "Different from MM-OSDG, in MM-OSDA, we have access to sample instances $D_T = \\{(x_i^T, y_i^T)\\}_{i=1}^{N_T}$ from the target domain during training. However, the labels $y_i^T$ for each $x_i^T$ are not available, where $y_i^T \\in C\\cup U$, encompassing both known and unknown classes. The first step is to distinguish the known samples ($y_i^T \\in C$) and unknown samples ($y_i^T \\in U$) from $D_T$ to mitigate the risk of negative transfer. Given a sample $x^t$ from the target domain, we calculate a prediction $\\hat{y}^o = \\delta(f(x))$ and use $\\max(\\hat{y}^o)$ as the prediction confidence to identify unknown classes. The assumption is that the model exhibits less confidence in predicting unknown samples compared to known ones. A target sample is classified as unknown if $\\max(\\hat{y}^o)$ falls below a threshold $\\tau$, where we typically set $\\tau = 0.5$.\nAfter filtering out all unknown samples from the target domain, we apply our self-supervised pretext tasks to the remaining known samples $D_T$ to align the source and target domains. For a given target sample from $D_T$, we first calculate the embedding for each modality as $[E_1, ..., \\bar{E}_k, ..., \\bar{E}_M]$ and then use $L_{MaskedTrans}$ and $L_{MulJig}$ to calculate the losses associated with different self-supervised pretext tasks. Additionally, we incorporate $L_{EntMin}$ into the predic- tion of target samples. Finally, we can redefine our loss under the MM-OSDA setting as:\n$L = L_{cls} + \\alpha_1(L_{MaskedTrans} + L'_{MaskedTrans}) + \\alpha_2(L_{MulJig} + L'_{MulJig})$\n$+ \\alpha_3(L_{EntMin} + L'_{EntMin})$,\nwhere $L$ and $L'$ correspond to different losses applied to source and target domain data respectively."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setting", "content": "Dataset. We use the EPIC-Kitchens [10] and HAC [14] datasets for experiments and adjust these to the open-set setups. We follow the experimental protocol"}, {"title": "4.2 Results", "content": "Multimodal Open-set DG. Tab. 2 and Tab. 3 present the results under the multimodal open-set DG setting, where the model is trained on several source domains with multiple modalities and tested on a single target domain with open classes. To verify the adaptability of our framework to various modali- ties, we conduct experiments by combining any two modalities, as well as all three modalities. The results on the EPIC-Kitchens dataset demonstrate that MOOSA consistently outperforms all baselines by a substantial margin across all cases, yielding average improvements in $HOS$ of up to 5.63%. Notably, our performance further improves when combining all three modalities, surpassing the results achieved with any two modalities. Conversely, some baseline methods"}, {"title": "4.3 Ablation Studies and Analysis", "content": "Disparate label sets across domains. In previous MM-OSDG experiments, we assumed that the label sets of all source domains were identical, and the target domain included all label sets from source domains along with additional unknown classes. However, in practical scenarios, different source domains may comprise distinct label sets, while the target domain might not include all label sets present in the source domains. Tab. 7 illustrates the results under this more challenging setup, where our method consistently demonstrates improvements in comparison to other baselines. The details of the label split under this setup are provided in Tab. 15 and Fig. 4."}, {"title": "5 Conclusion", "content": "In this paper, we introduce the Multimodal Open-Set Domain Generalization problem and propose a novel solution MOOSA employing self-supervised pre- text tasks. Our approach involves the development of two complementary multi- modal self-supervised tasks, namely Masked Cross-modal Translation and Mul- timodal Jigsaw Puzzles, aimed at learning multimodal representative features for robust generalization and effective open-class detection. To maintain a bal- ance in the loss across different modalities, we introduce an Entropy Weighting mechanism during training. Furthermore, we extend our framework to address Multimodal Open-Set Domain Adaptation. Through extensive evaluations on the EPIC-Kitchens and HAC datasets, our method demonstrates superior per- formance compared to competing baselines. Notably, our approach exhibits ro- bustness across variations in label sets, openness levels, and parameter choices. In future work, methods proposed in Multimodal OOD Detection [16] could be good alternatives for open-class detection."}, {"title": "A Details on Dataset Splits", "content": "For each dataset, we show the exact class splits for each domain.\nEPIC-Kitchens dataset [47] consists of three domains, denoted as D1, D2, and D3, corresponding to actions recorded in three different kitchen styles. This dataset comprises eight actions, namely, 'put', 'take', 'open', 'close', 'wash', 'cut', 'mix', and 'pour'. All three domains share a common label set of eight classes. We organize the action names in alphabetic order and assign an index to each category, 0-close, 1-cut, 2-mix, 3-open, 4-pour, 5-put, 6-take, 7-wash. Each do- main is employed as the target domain, while the other two domains function as source domains, forming three distinct cross-domain tasks (D2, D3 \u2192 D1, and D1, D3 \u2192 D2, and D1, D2 \u2192 D3). To construct the open-set situations, we designate the first class as unknown and the remaining classes as known. The specific categories contained in each domain are shown in Tab. 11.\nHAC dataset [14] comprises seven actions (\u2018sleeping', 'watching tv', 'eating', 'drinking', 'swimming', 'running', and 'opening door') executed by humans, ani- mals, and cartoon figures, forming three distinct domains H, A, and C. The three domains have the same label set of 7 classes. We organize the action names in alphabetic order and assign an index to each category, 0-drinking, 1-eating, 2- opening door, 3-running, 4-sleeping, 5-swimming, 6-watching tv. We use each domain as the target domain and the other two domains as source domains to form three cross-domain tasks (A, C \u2192 H, and H, C \u2192 A, and H, A \u2192 C). In the creation of open-set scenarios, the first class is designated as unknown, whereas the remaining classes are considered known. The specific categories contained in each domain are shown in Tab. 12."}, {"title": "B Implementation Details", "content": "Our experiments encompass three modalities: video, audio, and optical flow. We adopt SimMMDG [14] as the base framework by splitting the features within each modality into modality-specific and modality-shared components due to its demonstrated efficacy in multimodal DG tasks. In line with [14], we employ"}, {"title": "C More Experimental Results", "content": "Multimodal Open-set DG. Tab. 13 presents the results under the Multi- modal Open-set DG setting on HAC dataset [14] with different combinations of any two modalities, as well as all three modalities. Our method consistently outperforms all baselines by a significant margin in most cases, yielding average improvements in HOS of up to 3.37%.\nMultimodal Closed-set DG. Detailed results under the Multimodal Closed- set DG setting with different combinations of any two modalities, as well as all three modalities, are presented in Tab. 14. In comparison with the current SOTA multimodal DG baseline SimMMDG [14], our method attains further improve- ments up to 1.30%. This observation suggests that the proposed multimodal self-supervised pretext tasks and entropy weighting mechanism contribute to the model's ability to learn more discriminative features for enhanced general- ization."}, {"title": "D Ablation Study", "content": "Class Splits for Different Label Sets across Domains. In this setup, we assume different source domains have disparate label sets and the target domain"}, {"title": "E Motivation behind MOOSA for Open-set DG Setting", "content": "For Open-set DG setting, we need to address both domain shift and category shift. Self-supervised tasks (i.e., solving Jigsaw puzzles [6], predicting image rotations [4]) have shown their efficacy in mitigating domain shift by cap- turing invariance to bridge domain gaps. Inspired by solving unimodal Jigsaw puzzles on images, we propose the novel Multimodal Jigsaw Puzzles (MulJig) in multimodal scenarios to learn domain-invariant features and address the high"}]}