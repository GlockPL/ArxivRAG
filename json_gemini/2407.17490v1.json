{"title": "AMEX: Android Multi-annotation Expo Dataset for Mobile GUI Agents", "authors": ["Yuxiang Chai", "Siyuan Huang", "Yazhe Niu", "Han Xiao", "Liang Liu", "Dingyu Zhang", "Peng Gao", "Shuai Ren", "Hongsheng Li"], "abstract": "AI agents have drawn increasing attention mostly on their ability to perceive environments, understand tasks, and autonomously achieve goals. To advance research on AI agents in mobile scenarios, we introduce the Android Multi-annotation EXpo (AMEX), a comprehensive, large-scale dataset designed for generalist mobile GUI-control agents. Their capabilities of completing complex tasks by directly interacting with the graphical user interface (GUI) on mobile devices are trained and evaluated with the proposed dataset. AMEX comprises over 104K high-resolution screenshots from 110 popular mobile applications, which are annotated at multiple levels. Unlike existing mobile device-control datasets, e.g., MoTIF [2], AITW [19], etc., AMEX includes three levels of annotations: GUI interactive element grounding, GUI screen and element functionality descriptions, and complex natural language instructions, each averaging 13 steps with stepwise GUI-action chains. We develop this dataset from a more instructive and detailed perspective, complementing the general settings of existing datasets. Additionally, we develop a baseline model SPHINX Agent and compare its performance across state-of-the-art agents trained on other datasets. To facilitate further research, we open-source our dataset, models, and relevant evaluation tools. The project is available at https://yuxiangchai.github.io/AMEX/.", "sections": [{"title": "1 Introduction", "content": "AI assistants on mobile devices, such as Siri on iPhones, Bixby on Samsung devices, and Xiao AI on Xiaomi smartphones, have become increasingly prevalent in recent years. While these assistants are adept at managing various routine tasks like setting alarms, conducting web searches, and reporting weather conditions, their capabilities are predominantly confined to interacting with system-built applications. Furthermore, although many can interface with third-party apps via APIs, the lack of universal API support across different mobile operating systems often hampers their performance.\nIn contrast, human users can complete tasks on mobile devices purely based on visual information from the screen. Inspired by this, researchers are exploring alternative approaches [3, 10, 30] that process vision and natural language inputs, specifically screenshots in mobile environments. We refer to these as Mobile GUI-Control Agents, or GUI Agents for short. These agents are designed to manipulate the user interface elements on the screen directly, with the abilities to interpret natural language commands and analyze screenshot layouts and element functionalities, which theoretically enable agents to execute any task on any app."}, {"title": "2 Related Work", "content": "GUI-Control Datasets. Table 1 compares several popular GUI-control datasets on Android. While some works [5, 15, 20] focus on the web platform, on Android OS, many works [19, 29, 33] have focused on identifying various types of GUI elements, often assigning numerous classes to different elements. Other studies [2, 14, 23] primarily emphasize action-observation pairs during instructional operations, but their annotations are limited and often require supplemental View Hierarchy (VH) data for each screenshot. Furthermore, these instructions are typically too simplistic for real-world tasks. AITW [19] provides both screen GUI element annotations and instructions, but it includes only a small portion of instructions for third-party apps, with most operations conducted on Chrome and other system-built apps. Additionally, each instruction is repeated multiple times, resulting in significant data redundancy. It also relies on the pre-trained IconNet for automatic annotations, which unfortunately leads to numerous mis-annotated types and misaligned bounding boxes. In response, AITZ [32] filters AITW thoroughly, selecting 2.5K unique instructions and episodes, and introduces the Chain-of-Action-Thought framework to annotate action results and page descriptions better. Despite this refinement, AITZ contains only 18K screen-action pairs.\nGUI-Control Agents. Recent advancements have leveraged the extensive world knowledge and robust embodied capabilities of Large Language Models (LLMs) [1, 8, 9, 11, 12, 22] for complex task planning and reasoning within GUIs. A notable approach involves employing business-level generalist models like GPT-4v directly as GUI-control agents. Works [28, 34] have utilized these models, employing extensive prompt engineering to guide the LLM in executing complex tasks. However, the effectiveness of these methods is inherently limited by the capabilities of the available generalist MLLMs [4, 13, 16, 24] and requires meticulous prompt design to achieve optimal results. Alternatively, another research line focuses on fine-tuning smaller LLMs on GUI-specific datasets to imbue them with domain-specific knowledge, thereby enhancing their operational efficiency. For example, CogAgent [10] enhances performance in GUI-related tasks by integrating a high-resolution cross-module that fuses image features from various levels. Similarly, MobileAgent [6]"}, {"title": "3 Android Multi-annotation EXpo (AMEX)", "content": "When receiving an instruction, a human user first analyzes the overall screen layout to form a basic understanding of the current Android environment. The user then identifies interactive elements and areas, and assesses the functionalities of those elements. Finally, the user breaks down the instruction into simple, step-by-step actions on each screen. Based on this human cognitive process, we design three levels of annotations in our proposed AMEX training set: (i) GUI interactive element grounding"}, {"title": "3.1 Data Collection Pipeline", "content": "An overview of the data collection pipeline is illustrated in Figure 3. The raw data is collected through two methods: human instruction-following GUI manipulations and autonomous GUI controls. Human GUI manipulations involve recording stepwise operations for each instruction, and simultaneously storing screenshots and each screen's XML data before each stepwise operation. In parallel, an autonomous script controls emulators to collect additional screenshots and their XMLs. These two subsets comprise the entire raw dataset. Then for each screenshot, initial bounding boxes of interactive elements and their in-app descriptions (if available) are parsed from the corresponding XML. Human annotators then review each screenshot to filter out all the misaligned boxes, which serve as the interactive element grounding annotations. With the in-app descriptions, GPT generates the functionalities of the selected elements and provides descriptions for the whole screenshot. Human annotators then further check the descriptions of functionalities. More detailed methods are discussed in the following sections and Appendix A.1.3."}, {"title": "3.2 Level I: GUI Interactive Element Grounding", "content": "Existing datasets [19, 32] typically classify elements on the screen, such as icons, texts, and images, based on their types. Instead of adhering to the traditional classification paradigm, we define interactive elements more broadly as any elements that users can interact with, regardless of their specific types (see Figure 2a). Specifically, interactive elements in our dataset are only categorized into two subsets: (i) clickable elements and (ii) scrollable elements.\nClickable Elements are the most common components in a screen. They typically include clickable icons, images, texts, and compounds that combine several categories. Figure 2b illustrates various clickable elements in red boxes. We also include certain \"typeable\u201d elements, such as search bars, because most typeable elements require a prior click action to enable typing.\nScrollable Elements typically occupy larger areas on the screen. In most cases, a scrollable element area supports a pair of actions, such as \u201cscroll down\" and \"scroll up\u201d or \u201cscroll left\u201d and \u201cscroll right.\" Generally, the vertical scrollable area forms the main frame in the middle of the screen, while the horizontal scrollable area serves as a \"carousel\" for listing different categories or activities. Figure 2b illustrates these two types of scrollable elements.\nAs mentioned in Section 3.1, the collection of screenshots and their XML data utilizes two methods. Here, we provide a more detailed pipeline for executing an autonomous script to perform operations on an Android emulator. The script is designed to traverse an app in an unconstrained manner and collect data. It performs actions (see Appendix A.1.1) at regular time intervals to allow each page to fully load. After waiting, the script captures the current screenshot along with the corresponding XML file. To ensure efficiency and prevent redundancy, we limit the maximum number of operation steps per script execution. Additionally, a break mechanism is implemented to exit potential dead loops, ensuring the script progresses effectively. Upon the raw data (yellow part in Figure 3), we extract bounding boxes for interactive elements from the XML associated with each screenshot. While scrollable elements are typically parsed with high quality, clickable elements are more problematic. This issue is from the fact that app developers often design elements with overlapping or layered coverage, leading to the generation of bounding boxes even for elements that are visually hidden from the user's view. To address this, annotators manually inspect each screenshot, identifying and selecting only the interactive elements that are actually visible within the interface (see Appendix A.1.2)."}, {"title": "3.3 Level II: GUI Screen and Element Functionality Descriptions", "content": "Previous works on GUI elements often rely on predefined class names to convey the underlying meaning of each element. However, this classification-based method has significant limitations."}, {"title": "3.4 Level III: Instructions with GUI-Action Chains", "content": "Our focus for GUI agents is predominantly on third-party applications rather than system-built apps. We select an average of three apps per category from the Google Play Store. Due to the significant increase in app functionalities over the past years, we estimate that a sample size of approximately 50 apps can adequately represent most common use scenarios.\nThe instruction generation process comprises three phases. Initially, human annotators create 5-10 complex instructions for each target app, considering its specific purposes and capabilities. These initial instructions, combined with relevant app metadata collected online, serve as input for ChatGPT. It then generates a larger set of 80-100 instructions that exhibit similar structure and intent to the human-provided examples. For example, if a human annotator provides the instruction \u201cOpen Google Maps. Find the shortest route from the current location to Empire State Building by car drive,\u201d ChatGPT might produce \u201cOpen Google Maps. Find the fastest route from the current location to Rockefeller Center by public transit.\u201d However, due to ChatGPT's limitations on understanding real-world constraints, human filtering is adopted to remove any unreasonable or impractical instructions. Notably, instructions within AMEX are more complex than those in the AITW dataset, with an average step count of 12.8, nearly double that of AITW.\nWe define our action space for stepwise GUI operations similarly to AITW: {TAP, SCROLL, TYPE, PRESS_BACK, PRESS_HOME, PRESS_ENTER, TASK_COMPLETE, TASK_IMPOSSIBLE}.TAP actions are characterized by identical touch and lift coordinates, while SCROLL actions involve distinct touch and lift coordinates. TYPE actions are annotated with a type_text attribute specifying the input text. The three PRESS actions correspond to system-level button presses (back, home, enter). TASK_COMPLETE and TASK_IMPOSSIBLE serve as terminal flags for instructions. Additionally, for instructions that involve information query (e.g., \"What is the lowest price of the men's belt?\"), we associate the TASK_COMPLETE action with a region of interest, which is defined as the bounding box of the area on the current screenshot, where the answer is expected to appear (see examples in Appendix A.3). This comprehensive action space allows us to fully simulate a wide range of typical use cases.\nThe collection tool is designed to simultaneously record the screen and action pair together with the XML file. The details of GUI-action chain collection can be found in Appendix A.1.4."}, {"title": "3.5 AMEX Statistics and Test Set", "content": "Data statistics. Table 2 lists the statistics of the AMEX dataset. AMEX has approximately 104K screenshots, 1.6M GUI interactive elements for grounding, 712K GUI element functionality descriptions, and 3K instructions. Figure 4 illustrates the distribution of GUI interactive elements per screen and GUI operation steps along with the word cloud of the instructions.\nTest set. Instructions and their annotated screen-action chains from 9 out of 55 apps are cho- sen as the test set, including those on Booking, Gmail, NBC News (News), SHEIN, Citymapper (CM), Microsoft To Do (ToDo), Signal, Yelp and YouTube Music (Music), covering different scenarios. The number of test instructions is 362, around 10% of the total instructions.\nThe evaluation of AMEX test set is similar to the existing dataset AITW [19]. We perform the action matching to get the partial action match score. The partial action matching score is the number of correct actions divided by the episode length [14]. To ensure the correctness of the test set, we carefully review the trajectory annotations in the unseen test set, where we drop the former step of PRESS_BACK, which indicates the wrong operation of the annotator. However, we keep the PRESS_BACK step to evaluate the agent's self-correction ability."}, {"title": "4 SPHINX-GUI Agent", "content": "Integrating data from AMEX with the existing AITW dataset, we train the SPHINX-GUI Agent tailored for GUI-related tasks as the baseline for AMEX dataset. This Agent is initialized from SPHINX [7] and is specifically trained to generate actions to manipulate the GUI effectively, which are different from the more flexible output formats typical of standard MLLM tasks such as QA. To incorporate GUI-specific intelligence into SPHINX, we have implemented several key modifications:\nInstruction Dataset Construction. We transform the AMEX data and the publicly available AITW dataset into a VQA format conducive to instruction understanding, thereby adapting the MLLM as a GUI agent. To embed GUI-specific knowledge within the MLLM, we developed four distinct VQA tasks, detailed in the instruction templates provided in Table 3.\nStructured Representation. To effectively guide the agent in task completion, we incorporate the goal into the input prompt. To prevent redundant actions, we also include a history of actions formatted as Xhistory = [at\u2212k,..., at], where each a denotes an action type, accompanied by a touch point for actions such as SCROLL or TAP. The parameter k denotes the length of the history. Action trajectories from both AITW and AMEX datasets are converted into this uniform representation.\nGiven the extensive nature of GUI screenshots and the small relative size of some target elements within the screen, we adopt the \u201cany resolution\" approach from SPHINX [7]. This method initially partitions the input image into sub-images, then encodes separately. The LLM processes all the"}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Evaluation Setup and Compared LVLMS", "content": "We evaluate different GUI agents including SPHINX Agent developed in Section 4 on AITW and our AMEX dataset respectively. For AITW, we simultaneously train SPHINX Agent on all the subsets and then assess all test sets. We downsample GoogleApps subset to 10% to avoid data imbalance. The training and test split setting follows the one in [19]. For AMEX, we evaluated our SPHINX agents on the test set as described in Section 3.5. Our models are benchmarked against several agents including Auto-UI [30], SeeClick [3], CogAgent [10] and CoCo-Agent [17]."}, {"title": "5.2 Experiment Results", "content": "The experiment outcomes for AITW and AMEX are presented in Tables 4 and 5 respectively. As indicated in Table 4, SPHINX Agent (SphAgent) trained on AITW exhibit competitive performance on the AITW dataset relative to the baselines. Notably, the introduction of the AMEX dataset enhanced the overall performance by approximately 2.5%, with significant gains observed in the \u201cGeneral\u201d (5%), \"Single\" (5%) and \u201cWebShopping\u201d (2%) tasks, which indicates the strong complement from AMEX dataset. The GUI element functionality descriptions are served as the \u201cSingle\u201d step instructions which strongly promote the performance in the \u201cSingle\u201d tasks, while 3K instructions from general third-party apps also boost the performance in the \u201cGeneral\u201d tasks. \u201cWebShopping\u201d is the category with the most complicated tasks in AITW, which also benefits from the complex instructions provided by AMEX."}, {"title": "5.3 Cross-Domain Experiments", "content": "We also examine the generalization capabilities of our agent across different domains. Specifically, the agent trained exclusively on the AITW dataset is tested on the AMEX dataset. The results, as detailed in the first row of Table 5, reveal significant findings. Agent trained only on AITW, shows satisfactory performance on the AITW dataset itself. However, when evaluated on the AMEX dataset, there is a notable decrease in performance. This performance degradation can likely be attributed to several factors: 1) Higher complexity in AMEX. The AMEX dataset generally involves longer operational sequences compared to those in AITW, presenting a higher complexity level that the agent might not have been exposed to during training. 2) Domain gap. There are substantial differences in the visual textures and language instructions between the two datasets. These variations could hinder the agent's ability to effectively transfer its learned knowledge to a new, unfamiliar domain. Additionally, we observed a significant performance degradation for the SeeClick agent, originally trained on AITW. Beyond the previously mentioned reasons, this decline may also stem from the fact that the original SeeClick model was designed to process only 224 \u00d7 224 image inputs. In contrast, the AMEX dataset comprises high-resolution images and requires interactions with tiny icons. The high-definition images in AMEX introduce new challenges for GUI agents."}, {"title": "5.4 AITW Human Evaluation Results", "content": "As specified in AITZ [32], several types of error cases are identified in the AITW test set (see Figure5). To assess the unreliability of the original AITW test set and its evaluation methods, we propose AITW-HE (Human Evaluation), a refined subset of the AITW test set. Human annotators evaluate two subsets derived from the original test set. One subset is randomly chosen from episodes where SphAgent receives low scores (Mis-Match Random), i.e., about 66 score compared to the overall 78.72 shown in Table 4, indicating a high mismatch between inference results and AITW annotations. The other subset is randomly selected from the remaining episodes (Random). Annotators first filter out repeated and redundant screenshots (see Figure 5a). For each remaining screenshot, they then record whether the SphAgent-inferred action and the original AITW annotation are correct. Figure 6 illustrates cases where annotators mark both the inferred action and the original annotation as correct, even though they interact with different elements.\nTable 6 presents the accuracy scores from human evaluation. The table indicates that the low SphAgent scores in the MMR subset are primarily due to unsatisfactory annotations (AITW Anno), which are used as ground truth during the evaluation. Both SphAgent and AITW Anno in the \u201cMMR\u201d subset have lower scores than those in the \u201cRandom\u201d subset, highlighting that the tasks in the MMR subset are relatively more challenging. Furthermore, SphAgent achieves better human evaluation results than the original annotations, demonstrating its effectiveness and close alignment with human judgment. Comparing the results from Table 4 and Table 6, the notable differences in overall scores (i.e., 78.72%, 86.43%, and 93.95%) underscore the unreliability and misleading nature of the AITW test set and its evaluation methods."}, {"title": "5.5 Ablation Study on Multi-level Annotations", "content": "To validate the effectiveness of the multi-level annotations proposed in the AMEX dataset, we conducted an ablation study. As shown in Table 7, the agent achieved a performance gain of approximately 1.0 when aided by L1 annotations. When trained with L1 and L3 annotations, the agent exhibited a performance gain of about 1.6 compared to the baseline. The gain from L2 annotations is approximately 2.3. Furthermore, training the agent with the complete set of annotations resulted in a performance gain of approximately 3.5. The ablation study results demonstrate that each level of annotations in AMEX enhances the final agent's performance."}, {"title": "6 Discussions", "content": null}, {"title": "6.1 Limitations and Future Work", "content": "Multi-lingual Most existing datasets are limited to English, with UGIF [23] being a notable exception, as it includes instructions and screenshots in eight languages. The AMEX dataset contains a small number of screenshots in Chinese and Spanish, primarily due to strict registration and login requirements for Chinese apps and a lack of expertise in other languages. Future work should incorporate multi-lingual screenshots, functionalities, and instructions to create a more robust and comprehensive multi-lingual environment for GUI agents."}, {"title": "6.2 Ethical Considerations", "content": "\u2022 The accounts registered and logged in are all for testing purposes, not including any personal information. The dataset doesn't contain any private or personal information.\n\u2022 The dataset, if misused, could be exploited for undesirable purposes, such as anti-fraud mecha- nisms and anti-script verification codes (see Appendix A.4), potentially leading to harm.\n\u2022 Annotators received remuneration in line with local wage standards for their annotation works."}, {"title": "7 Conclusion", "content": "As AI agents become more prevalent, mobile GUI agents are emerging as a research hotspot. To address the lack of fundamental understanding of GUI elements in existing datasets, we present the Android Multi-annotation EXpo (AMEX) dataset, which includes three levels of annotation to"}, {"title": "A Appendix / supplemental material", "content": null}, {"title": "A.1 Pipeline Details", "content": null}, {"title": "A.1.1 Autonomous script details", "content": "The autonomous script controls the emulator using three actions: TAP, SCROLL, TYPE.\n\u2022 For the TAP action, we employ two algorithms. The first randomly selects a clickable element on the current screen, while the second computes the index of a clickable element using a formula to ensure the elements chosen are likely unique. We apply one of these algorithms randomly for different executions.\n\u2022 For the SCROLL action, we classify whether an area is vertically or horizontally scrollable by setting a width-height ratio threshold, R. If the element's ratio exceeds R, it is considered horizontally scrollable; otherwise, it is vertically scrollable. We then randomly select a scrollable element on the current screen and perform a scroll action based on its type.\n\u2022 For the TYPE action, we pre-define a list of phrases relevant to the category of apps being tested. For example, [\"Women's dress\u201d, \u201cNike sneakers\", ...] for clothing shopping apps. These phrases are primarily used in search scenarios."}, {"title": "A.1.2 GUI clickable element filtering", "content": "The raw XML information sometimes contains the elements that are covered by other elements or layers. Figure 7 illustrates the same image before and after filtering. Blue boxes in Figure 7a are those elements under the current active layer and they still show up from XML parsing. Thus we need human annotators to filter out these blocked elements to get Figure 7b."}, {"title": "A.1.3 GUI screen and element functionality description collection details", "content": "We apply the Set-of-Mark (SoM) [27] technique when using GPT-40 as the description generator. The SoM technique is a visual prompting method designed to enhance the visual grounding capabil- ities of large multimodal models (LMMs), such as GPT-4V, by overlaying visual marks on image regions. This involves partitioning an image into semantically meaningful regions and adding distinct marks (e.g., alphanumeric characters, masks, or boxes) to these regions. It demonstrates significant improvements in precision and accuracy over traditional prompting methods and other state-of-the-art models. Figure 8 shows the screenshot with SoM technique."}, {"title": "A.1.4 GUI-Action chain collection details", "content": "Human annotators are each assigned a random selection of apps and their associated instructions, which they are asked to complete in a natural manner. In contrast to the AITW dataset, our collection methodology allows annotators to make errors and take incorrect steps, leading to a greater preva- lence of PRESS_BACK actions. This is motivated by our observation that agents trained on existing datasets exhibit difficulty navigating back to previous pages due to insufficient experience with the PRESS_BACK action. Additionally, after completing an information query task, annotators are asked to manually mark the region of interest on the screenshot using our annotation tool."}, {"title": "A.1.5 Collection resources details", "content": "\u2022 GUI interactive element grounding takes approximately 3000 human-hour to filter bounding boxes described in Appendix A.1.2.\n\u2022 GUI screen and element descriptions use GPT-40 API, which consumes about 600 dollars.\n\u2022 Instructions with GUI-action chains take approximately 200 human-hour."}, {"title": "A.2 Experiment details", "content": "In our implementation, we utilize the internlm-7b variant of the SPHINX-X model, as detailed in [7]. The pre-trained checkpoint for this model was sourced from the official repository mentioned in [21]. For image processing, the input images, each sized 1024 \u00d7 1024, are segmented into sub-images. Visual features from these sub-images are extracted using two distinct visual encoders: DINOv2 [18] and ConvNext [25]. To ensure compatibility in feature dimensions across different modules, linear projection layers are employed to align the channel dimensions. Regarding the model's parameter settings, as outlined in Section 4, we configure the history window size to four. Additionally, we introduce a special token, <ICON>, specifically designed to identify interactive elements within the interface, strengthening the model's interpretability and responsiveness to user interactions. The agent is trained on a cluster with 3 nodes, each with eight NVIDIA A100 (80GB) GPUs. The fine-tuning was completed in four epochs."}, {"title": "A.3 More AMEX examples", "content": "See Figure 9 for more examples of GUI interactive elements grounding and description.\nSee Figure 10 and Figure 11 for more examples of instruction with GUI-action chains."}, {"title": "A.4 Examples of Ethical Problems", "content": "Figure 12 shows examples of anti-script mechanism where the agent can correctly enter the verification codes."}]}