{"title": "AMEX: Android Multi-annotation Expo Dataset for Mobile GUI Agents", "authors": ["Yuxiang Chai", "Siyuan Huang", "Yazhe Niu", "Han Xiao", "Liang Liu", "Dingyu Zhang", "Peng Gao", "Shuai Ren", "Hongsheng Li"], "abstract": "AI agents have drawn increasing attention mostly on their ability to perceive envi-\nronments, understand tasks, and autonomously achieve goals. To advance research\non AI agents in mobile scenarios, we introduce the Android Multi-annotation\nEXpo (AMEX), a comprehensive, large-scale dataset designed for generalist mo-\nbile GUI-control agents. Their capabilities of completing complex tasks by directly\ninteracting with the graphical user interface (GUI) on mobile devices are trained\nand evaluated with the proposed dataset. AMEX comprises over 104K high-\nresolution screenshots from 110 popular mobile applications, which are annotated\nat multiple levels. Unlike existing mobile device-control datasets, e.g., MoTIF [2],\nAITW [19], etc., AMEX includes three levels of annotations: GUI interactive ele-\nment grounding, GUI screen and element functionality descriptions, and complex\nnatural language instructions, each averaging 13 steps with stepwise GUI-action\nchains. We develop this dataset from a more instructive and detailed perspective,\ncomplementing the general settings of existing datasets. Additionally, we develop a\nbaseline model SPHINX Agent and compare its performance across state-of-the-art\nagents trained on other datasets. To facilitate further research, we open-source\nour dataset, models, and relevant evaluation tools. The project is available at\nhttps://yuxiangchai.github.io/AMEX/.", "sections": [{"title": "1 Introduction", "content": "AI assistants on mobile devices, such as Siri on iPhones, Bixby on Samsung devices, and Xiao AI on\nXiaomi smartphones, have become increasingly prevalent in recent years. While these assistants are\nadept at managing various routine tasks like setting alarms, conducting web searches, and reporting\nweather conditions, their capabilities are predominantly confined to interacting with system-built\napplications. Furthermore, although many can interface with third-party apps via APIs, the lack of\nuniversal API support across different mobile operating systems often hampers their performance.\nIn contrast, human users can complete tasks on mobile devices purely based on visual information\nfrom the screen. Inspired by this, researchers are exploring alternative approaches [3, 10, 30] that\nprocess vision and natural language inputs, specifically screenshots in mobile environments. We\nrefer to these as Mobile GUI-Control Agents, or GUI Agents for short. These agents are designed to\nmanipulate the user interface elements on the screen directly, with the abilities to interpret natural\nlanguage commands and analyze screenshot layouts and element functionalities, which theoretically\nenable agents to execute any task on any app."}, {"title": "2 Related Work", "content": "GUI-Control Datasets. Table 1 compares several popular GUI-control datasets on Android. While\nsome works [5, 15, 20] focus on the web platform, on Android OS, many works [19, 29, 33] have\nfocused on identifying various types of GUI elements, often assigning numerous classes to different\nelements. Other studies [2, 14, 23] primarily emphasize action-observation pairs during instructional\noperations, but their annotations are limited and often require supplemental View Hierarchy (VH)\ndata for each screenshot. Furthermore, these instructions are typically too simplistic for real-world\ntasks. AITW [19] provides both screen GUI element annotations and instructions, but it includes\nonly a small portion of instructions for third-party apps, with most operations conducted on Chrome\nand other system-built apps. Additionally, each instruction is repeated multiple times, resulting\nin significant data redundancy. It also relies on the pre-trained IconNet for automatic annotations,\nwhich unfortunately leads to numerous mis-annotated types and misaligned bounding boxes. In\nresponse, AITZ [32] filters AITW thoroughly, selecting 2.5K unique instructions and episodes, and\nintroduces the Chain-of-Action-Thought framework to annotate action results and page descriptions\nbetter. Despite this refinement, AITZ contains only 18K screen-action pairs.\nGUI-Control Agents. Recent advancements have leveraged the extensive world knowledge and\nrobust embodied capabilities of Large Language Models (LLMs) [1, 8, 9, 11, 12, 22] for complex\ntask planning and reasoning within GUIs. A notable approach involves employing business-level\ngeneralist models like GPT-4v directly as GUI-control agents. Works [28, 34] have utilized these\nmodels, employing extensive prompt engineering to guide the LLM in executing complex tasks.\nHowever, the effectiveness of these methods is inherently limited by the capabilities of the available\ngeneralist MLLMs [4, 13, 16, 24] and requires meticulous prompt design to achieve optimal results.\nAlternatively, another research line focuses on fine-tuning smaller LLMs on GUI-specific datasets\nto imbue them with domain-specific knowledge, thereby enhancing their operational efficiency.\nFor example, CogAgent [10] enhances performance in GUI-related tasks by integrating a high-\nresolution cross-module that fuses image features from various levels. Similarly, MobileAgent [6]"}, {"title": "3 Android Multi-annotation EXpo (AMEX)", "content": "When receiving an instruction, a human user first analyzes the overall screen layout to form a basic\nunderstanding of the current Android environment. The user then identifies interactive elements and\nareas, and assesses the functionalities of those elements. Finally, the user breaks down the instruction\ninto simple, step-by-step actions on each screen. Based on this human cognitive process, we design\nthree levels of annotations in our proposed AMEX training set: (i) GUI interactive element grounding"}, {"title": "3.1 Data Collection Pipeline", "content": "An overview of the data collection pipeline is illustrated in Figure 3. The raw data is collected through\ntwo methods: human instruction-following GUI manipulations and autonomous GUI controls. Human\nGUI manipulations involve recording stepwise operations for each instruction, and simultaneously\nstoring screenshots and each screen's XML data before each stepwise operation. In parallel, an\nautonomous script controls emulators to collect additional screenshots and their XMLs. These\ntwo subsets comprise the entire raw dataset. Then for each screenshot, initial bounding boxes of\ninteractive elements and their in-app descriptions (if available) are parsed from the corresponding\nXML. Human annotators then review each screenshot to filter out all the misaligned boxes, which\nserve as the interactive element grounding annotations. With the in-app descriptions, GPT generates\nthe functionalities of the selected elements and provides descriptions for the whole screenshot. Human\nannotators then further check the descriptions of functionalities. More detailed methods are discussed\nin the following sections and Appendix A.1.3."}, {"title": "3.2 Level I: GUI Interactive Element Grounding", "content": "Existing datasets [19, 32] typically classify elements on the screen, such as icons, texts, and images,\nbased on their types. Instead of adhering to the traditional classification paradigm, we define\ninteractive elements more broadly as any elements that users can interact with, regardless of their\nspecific types (see Figure 2a). Specifically, interactive elements in our dataset are only categorized\ninto two subsets: (i) clickable elements and (ii) scrollable elements.\nClickable Elements are the most common components in a screen. They typically include clickable\nicons, images, texts, and compounds that combine several categories. Figure 2b illustrates various\nclickable elements in red boxes. We also include certain \"typeable\u201d elements, such as search bars,\nbecause most typeable elements require a prior click action to enable typing.\nScrollable Elements typically occupy larger areas on the screen. In most cases, a scrollable element\narea supports a pair of actions, such as \u201cscroll down\" and ", "scroll left": "nd \u201cscroll right.\"\nGenerally, the vertical scrollable area forms the main frame in the middle of the screen, while the\nhorizontal scrollable area serves as a \"carousel\" for listing different categories or activities. Figure 2b\nillustrates these two types of scrollable elements.\nAs mentioned in Section 3.1, the collection of screenshots and their XML data utilizes two methods.\nHere, we provide a more detailed pipeline for executing an autonomous script to perform operations\non an Android emulator. The script is designed to traverse an app in an unconstrained manner and\ncollect data. It performs actions (see Appendix A.1.1) at regular time intervals to allow each page to\nfully load. After waiting, the script captures the current screenshot along with the corresponding XML\nfile. To ensure efficiency and prevent redundancy, we limit the maximum number of operation steps\nper script execution. Additionally, a break mechanism is implemented to exit potential dead loops,\nensuring the script progresses effectively. Upon the raw data (yellow part in Figure 3), we extract\nbounding boxes for interactive elements from the XML associated with each screenshot. While\nscrollable elements are typically parsed with high quality, clickable elements are more problematic.\nThis issue is from the fact that app developers often design elements with overlapping or layered\ncoverage, leading to the generation of bounding boxes even for elements that are visually hidden from\nthe user's view. To address this, annotators manually inspect each screenshot, identifying and selecting\nonly the interactive elements that are actually visible within the interface (see Appendix A.1.2)."}, {"title": "3.3 Level II: GUI Screen and Element Functionality Descriptions", "content": "Previous works on GUI elements often rely on predefined class names to convey the underlying\nmeaning of each element. However, this classification-based method has significant limitations."}, {"title": "3.4 Level III: Instructions with GUI-Action Chains", "content": "Our focus for GUI agents is predominantly on third-party applications rather than system-built apps.\nWe select an average of three apps per category from the Google Play Store. Due to the significant\nincrease in app functionalities over the past years, we estimate that a sample size of approximately 50\napps can adequately represent most common use scenarios.\nThe instruction generation process comprises three phases. Initially, human annotators create 5-10\ncomplex instructions for each target app, considering its specific purposes and capabilities. These\ninitial instructions, combined with relevant app metadata collected online, serve as input for ChatGPT.\nIt then generates a larger set of 80-100 instructions that exhibit similar structure and intent to the\nhuman-provided examples. For example, if a human annotator provides the instruction \u201cOpen Google\nMaps. Find the shortest route from the current location to Empire State Building by car drive,\"\nChatGPT might produce \"Open Google Maps. Find the fastest route from the current location to\nRockefeller Center by public transit.\u201d However, due to ChatGPT's limitations on understanding real-\nworld constraints, human filtering is adopted to remove any unreasonable or impractical instructions.\nNotably, instructions within AMEX are more complex than those in the AITW dataset, with an\naverage step count of 12.8, nearly double that of AITW.\nWe define our action space for stepwise GUI operations similarly to AITW: {TAP, SCROLL, TYPE,\nPRESS_BACK, PRESS_HOME, PRESS_ENTER, TASK_COMPLETE, TASK_IMPOSSIBLE}. TAP actions are\ncharacterized by identical touch and lift coordinates, while SCROLL actions involve distinct touch and\nlift coordinates. TYPE actions are annotated with a type_text attribute specifying the input text. The\nthree PRESS actions correspond to system-level button presses (back, home, enter). TASK_COMPLETE\nand TASK_IMPOSSIBLE serve as terminal flags for instructions. Additionally, for instructions that\ninvolve information query (e.g., \"What is the lowest price of the men's belt?\"), we associate the\nTASK_COMPLETE action with a region of interest, which is defined as the bounding box of the area on\nthe current screenshot, where the answer is expected to appear (see examples in Appendix A.3). This\ncomprehensive action space allows us to fully simulate a wide range of typical use cases.\nThe collection tool is designed to simultaneously record the screen and action pair together with the\nXML file. The details of GUI-action chain collection can be found in Appendix A.1.4."}, {"title": "3.5 AMEX Statistics and Test Set", "content": "Data statistics. Table 2 lists the statistics of the AMEX dataset. AMEX has approximately 104K\nscreenshots, 1.6M GUI interactive elements for grounding, 712K GUI element functionality descrip-\ntions, and 3K instructions. Figure 4 illustrates the distribution of GUI interactive elements per screen\nand GUI operation steps along with the word cloud of the instructions.\nTest set. Instructions and their annotated screen-action chains from 9 out of 55 apps are cho-\nsen as the test set, including those on Booking, Gmail, NBC News (News), SHEIN, Citymapper\n(CM), Microsoft To Do (ToDo), Signal, Yelp and YouTube Music (Music), covering different\nscenarios. The number of test instructions is 362, around 10% of the total instructions.\nThe evaluation of AMEX test set is similar to the existing dataset AITW [19]. We perform the\naction matching to get the partial action match score. The partial action matching score is the\nnumber of correct actions divided by the episode length [14]. To ensure the correctness of the test\nset, we carefully review the trajectory annotations in the unseen test set, where we drop the former\nstep of PRESS_BACK, which indicates the wrong operation of the annotator. However, we keep the\nPRESS_BACK step to evaluate the agent's self-correction ability."}, {"title": "4 SPHINX-GUI Agent", "content": "Integrating data from AMEX with the existing AITW dataset, we train the SPHINX-GUI Agent\ntailored for GUI-related tasks as the baseline for AMEX dataset. This Agent is initialized from\nSPHINX [7] and is specifically trained to generate actions to manipulate the GUI effectively, which\nare different from the more flexible output formats typical of standard MLLM tasks such as QA. To\nincorporate GUI-specific intelligence into SPHINX, we have implemented several key modifications:\nInstruction Dataset Construction. We transform the AMEX data and the publicly available AITW\ndataset into a VQA format conducive to instruction understanding, thereby adapting the MLLM as a\nGUI agent. To embed GUI-specific knowledge within the MLLM, we developed four distinct VQA\ntasks, detailed in the instruction templates provided in Table 3.\nStructured Representation. To effectively guide the agent in task completion, we incorporate\nthe goal into the input prompt. To prevent redundant actions, we also include a history of actions\nformatted as \\(X_{\\text{history}} = [a_{t-k},..., a_t]\\), where each \\(a\\) denotes an action type, accompanied by a touch\npoint for actions such as SCROLL or TAP. The parameter k denotes the length of the history. Action\ntrajectories from both AITW and AMEX datasets are converted into this uniform representation.\nGiven the extensive nature of GUI screenshots and the small relative size of some target elements\nwithin the screen, we adopt the \u201cany resolution"}, {"title": "5 Experiments", "content": "5.1 Evaluation Setup and Compared LVLMS\nWe evaluate different GUI agents including SPHINX Agent developed in Section 4 on AITW and our\nAMEX dataset respectively. For AITW, we simultaneously train SPHINX Agent on all the subsets\nand then assess all test sets. We downsample GoogleApps subset to 10% to avoid data imbalance.\nThe training and test split setting follows the one in [19]. For AMEX, we evaluated our SPHINX\nagents on the test set as described in Section 3.5. Our models are benchmarked against several agents\nincluding Auto-UI [30], SeeClick [3], CogAgent [10] and CoCo-Agent [17].\n5.2 Experiment Results\nThe experiment outcomes for AITW and AMEX are presented in Tables 4 and 5 respectively. As\nindicated in Table 4, SPHINX Agent (SphAgent) trained on AITW exhibit competitive performance on\nthe AITW dataset relative to the baselines. Notably, the introduction of the AMEX dataset enhanced\nthe overall performance by approximately 2.5%, with significant gains observed in the \u201cGeneral\u201d (5%),\n\"Single\" (5%) and \u201cWebShopping\u201d (2%) tasks, which indicates the strong complement from AMEX\ndataset. The GUI element functionality descriptions are served as the \u201cSingle\u201d step instructions\nwhich strongly promote the performance in the \u201cSingle\u201d tasks, while 3K instructions from general\nthird-party apps also boost the performance in the \u201cGeneral\u201d tasks. \u201cWebShopping\" is the category\nwith the most complicated tasks in AITW, which also benefits from the complex instructions provided\nby AMEX.\n5.3 Cross-Domain Experiments\nWe also examine the generalization capabilities of our agent across different domains. Specifically,\nthe agent trained exclusively on the AITW dataset is tested on the AMEX dataset. The results, as\ndetailed in the first row of Table 5, reveal significant findings. Agent trained only on AITW, shows\nsatisfactory performance on the AITW dataset itself. However, when evaluated on the AMEX dataset,\nthere is a notable decrease in performance. This performance degradation can likely be attributed\nto several factors: 1) Higher complexity in AMEX. The AMEX dataset generally involves longer\noperational sequences compared to those in AITW, presenting a higher complexity level that the agent\nmight not have been exposed to during training. 2) Domain gap. There are substantial differences\nin the visual textures and language instructions between the two datasets. These variations could\nhinder the agent's ability to effectively transfer its learned knowledge to a new, unfamiliar domain.\nAdditionally, we observed a significant performance degradation for the SeeClick agent, originally\ntrained on AITW. Beyond the previously mentioned reasons, this decline may also stem from the fact\nthat the original SeeClick model was designed to process only 224 \u00d7 224 image inputs. In contrast,\nthe AMEX dataset comprises high-resolution images and requires interactions with tiny icons. The\nhigh-definition images in AMEX introduce new challenges for GUI agents.\""}, {"title": "5.4 AITW Human Evaluation Results", "content": "As specified in AITZ [32], several types of error cases are identified in the AITW test set (see Figure5).\nTo assess the unreliability of the original AITW test set and its evaluation methods, we propose\nAITW-HE (Human Evaluation), a refined subset of the AITW test set. Human annotators evaluate\ntwo subsets derived from the original test set. One subset is randomly chosen from episodes where\nSphAgent receives low scores (Mis-Match Random), i.e., about 66 score compared to the overall\n78.72 shown in Table 4, indicating a high mismatch between inference results and AITW annotations.\nThe other subset is randomly selected from the remaining episodes (Random). Annotators first filter\nout repeated and redundant screenshots (see Figure 5a). For each remaining screenshot, they then\nrecord whether the SphAgent-inferred action and the original AITW annotation are correct. Figure 6\nillustrates cases where annotators mark both the inferred action and the original annotation as correct,\neven though they interact with different elements.\nTable 6 presents the accuracy scores from human evaluation. The table indicates that the low\nSphAgent scores in the MMR subset are primarily due to unsatisfactory annotations (AITW Anno),\nwhich are used as ground truth during the evaluation. Both SphAgent and AITW Anno in the \u201cMMR\u201d\nsubset have lower scores than those in the \u201cRandom\u201d subset, highlighting that the tasks in the MMR\nsubset are relatively more challenging. Furthermore, SphAgent achieves better human evaluation\nresults than the original annotations, demonstrating its effectiveness and close alignment with human\njudgment. Comparing the results from Table 4 and Table 6, the notable differences in overall scores\n(i.e., 78.72%, 86.43%, and 93.95%) underscore the unreliability and misleading nature of the AITW\ntest set and its evaluation methods."}, {"title": "5.5 Ablation Study on Multi-level Annotations", "content": "To validate the effectiveness of the multi-level annotations proposed in the AMEX dataset, we\nconducted an ablation study. As shown in Table 7, the agent achieved a performance gain of\napproximately 1.0 when aided by L1 annotations. When trained with L1 and L3 annotations, the\nagent exhibited a performance gain of about 1.6 compared to the baseline. The gain from L2\nannotations is approximately 2.3. Furthermore, training the agent with the complete set of annotations\nresulted in a performance gain of approximately 3.5. The ablation study results demonstrate that each\nlevel of annotations in AMEX enhances the final agent's performance."}, {"title": "6 Discussions", "content": "6.1 Limitations and Future Work\nMulti-lingual Most existing datasets are limited to English, with UGIF [23] being a notable\nexception, as it includes instructions and screenshots in eight languages. The AMEX dataset contains\na small number of screenshots in Chinese and Spanish, primarily due to strict registration and login\nrequirements for Chinese apps and a lack of expertise in other languages. Future work should\nincorporate multi-lingual screenshots, functionalities, and instructions to create a more robust and\ncomprehensive multi-lingual environment for GUI agents."}, {"title": "7 Conclusion", "content": "As AI agents become more prevalent, mobile GUI agents are emerging as a research hotspot. To\naddress the lack of fundamental understanding of GUI elements in existing datasets, we present\nthe Android Multi-annotation EXpo (AMEX) dataset, which includes three levels of annotation to"}, {"title": "A Appendix / supplemental material", "content": "A.1 Pipeline Details\nA.1.1 Autonomous script details\nThe autonomous script controls the emulator using three actions: TAP, SCROLL, TYPE.\n\u2022 For the TAP action, we employ two algorithms. The first randomly selects a clickable element on\nthe current screen, while the second computes the index of a clickable element using a formula to\nensure the elements chosen are likely unique. We apply one of these algorithms randomly for\ndifferent executions.\n\u2022 For the SCROLL action, we classify whether an area is vertically or horizontally scrollable by\nsetting a width-height ratio threshold, \\(R\\). If the element's ratio exceeds \\(R\\), it is considered\nhorizontally scrollable; otherwise, it is vertically scrollable. We then randomly select a scrollable\nelement on the current screen and perform a scroll action based on its type.\n\u2022 For the TYPE action, we pre-define a list of phrases relevant to the category of apps being tested.\nFor example, [\"Women's dress\u201d, \u201cNike sneakers\u201d, ...] for clothing shopping apps. These phrases\nare primarily used in search scenarios.\nA.1.2 GUI clickable element filtering"}, {"title": "A.1.3 GUI screen and element functionality description collection details", "content": "We apply the Set-of-Mark (SoM) [27] technique when using GPT-40 as the description generator.\nThe SoM technique is a visual prompting method designed to enhance the visual grounding capabil-\nities of large multimodal models (LMMs), such as GPT-4V, by overlaying visual marks on image\nregions. This involves partitioning an image into semantically meaningful regions and adding distinct\nmarks (e.g., alphanumeric characters, masks, or boxes) to these regions. It demonstrates significant\nimprovements in precision and accuracy over traditional prompting methods and other state-of-the-art\nmodels."}, {"title": "A.1.4 GUI-Action chain collection details", "content": "Human annotators are each assigned a random selection of apps and their associated instructions,\nwhich they are asked to complete in a natural manner. In contrast to the AITW dataset, our collection\nmethodology allows annotators to make errors and take incorrect steps, leading to a greater preva-\nlence of PRESS_BACK actions. This is motivated by our observation that agents trained on existing\ndatasets exhibit difficulty navigating back to previous pages due to insufficient experience with the\nPRESS_BACK action. Additionally, after completing an information query task, annotators are asked\nto manually mark the region of interest on the screenshot using our annotation tool."}, {"title": "A.1.5 Collection resources details", "content": "\u2022 GUI interactive element grounding takes approximately 3000 human-hour to filter bounding\nboxes described in Appendix A.1.2.\n\u2022 GUI screen and element descriptions use GPT-4o API, which consumes about 600 dollars.\n\u2022 Instructions with GUI-action chains take approximately 200 human-hour."}, {"title": "A.2 Experiment details", "content": "In our implementation, we utilize the internlm-7b variant of the SPHINX-X model, as detailed in [7].\nThe pre-trained checkpoint for this model was sourced from the official repository mentioned in [21].\nFor image processing, the input images, each sized 1024 \u00d7 1024, are segmented into sub-images.\nVisual features from these sub-images are extracted using two distinct visual encoders: DINOv2 [18]\nand ConvNext [25]. To ensure compatibility in feature dimensions across different modules, linear\nprojection layers are employed to align the channel dimensions. Regarding the model's parameter\nsettings, as outlined in Section 4, we configure the history window size to four. Additionally, we\nintroduce a special token,  , specifically designed to identify interactive elements within the\ninterface, strengthening the model's interpretability and responsiveness to user interactions. The agent\nis trained on a cluster with 3 nodes, each with eight NVIDIA A100 (80GB) GPUs. The fine-tuning\nwas completed in four epochs."}, {"title": "A.3 More AMEX examples", "content": "See Figure 9 for more examples of GUI interactive elements grounding and description.\nSee Figure 10 and Figure 11 for more examples of instruction with GUI-action chains."}, {"title": "A.4 Examples of Ethical Problems", "content": "Figure 12 shows examples of anti-script mechanism where the agent can correctly enter the verification\ncodes."}]}