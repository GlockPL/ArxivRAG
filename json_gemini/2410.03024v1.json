{"title": "FLOW MATCHING WITH GAUSSIAN PROCESS PRIORS\nFOR PROBABILISTIC TIME SERIES FORECASTING", "authors": ["Marcel Kollovieh", "Marten Lienen", "David L\u00fcdke", "Leo Schwinn", "Stephan G\u00fcnnemann"], "abstract": "Recent advancements in generative modeling, particularly diffusion models, have\nopened new directions for time series modeling, achieving state-of-the-art per-\nformance in forecasting and synthesis. However, the reliance of diffusion-based\nmodels on a simple, fixed prior complicates the generative process since the data\nand prior distributions differ significantly. We introduce TSFlow, a conditional\nflow matching (CFM) model for time series that simplifies the generative problem\nby combining Gaussian processes, optimal transport paths, and data-dependent\nprior distributions. By incorporating (conditional) Gaussian processes, TSFlow\naligns the prior distribution more closely with the temporal structure of the data,\nenhancing both unconditional and conditional generation. Furthermore, we propose\nconditional prior sampling to enable probabilistic forecasting with an uncondition-\nally trained model. In our experimental evaluation on eight real-world datasets,\nwe demonstrate the generative capabilities of TSFlow, producing high-quality un-\nconditional samples. Finally, we show that both conditionally and unconditionally\ntrained models achieve competitive results in forecasting benchmarks, surpassing\nother methods on 6 out of 8 datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "Diffusion (Sohl-Dickstein et al., 2015; Ho et al., 2020) and score-based generative models (Song\net al., 2020) have demonstrated strong performance in time series analysis, effectively capturing the\ndistribution of time series data in both conditional (Rasul et al., 2021; Tashiro et al., 2021; Bilo\u0161 et al.,\n2023; Alcaraz & Strodthoff, 2022) and unconditional (Kollovieh et al., 2023) settings. However,\nthese models typically transform non-i.i.d. distributions of time series data into a simple isotropic\nGaussian prior by iteratively adding noise leading to long and complex paths. This can hinder the\ngenerative process and potentially limit the models' performance.\nConditional Flow Matching (CFM) (Lipman et al., 2022; Tong et al., 2023) provides an efficient\nalternative to diffusion models and simplifies trajectories by constructing probability paths based on\nconditional optimal transport. The model is trained by regressing the flow fields of these paths and\ncan accommodate arbitrary prior distributions. Despite its advantages, the application of CFM to\ntime series forecasting remains unexplored.\nIn this work, we simplify the trajectories of generative time series modeling through informed\npriors, specifically using Gaussian Processes (GPs) as prior distributions within the CFM framework.\nBy aligning the prior distribution more closely with the underlying temporal dynamics of real-\nworld data, we simplify and shorten the probability paths. This approach not only reduces the\ncomplexity of the learned transformations but also enhances the model's performance in conditional\nand unconditional generation tasks. Furthermore, we demonstrate how both conditionally trained and\nunconditionally trained models can be leveraged for probabilistic forecasting. We propose conditional\nprior sampling and demonstrate how to use guidance (Dhariwal & Nichol, 2021) to bridge the gap\nbetween conditional and unconditional generation, allowing for flexible application of unconditional\nmodels without the need for conditional training procedures.\nOur empirical evaluations show that using conditional GPs as priors improves generative modeling\nand forecasting performance, surpassing various baselines from different frameworks on multiple"}, {"title": "2 BACKGROUND: CONDITIONAL FLOW MATCHING", "content": "Conditional Flow Matching (CFM) was recently introduced as a framework for generative modeling\nby learning a flow field to transform one distribution into another (Lipman et al., 2022). The learned\nflow field then yields a transformation $1 that maps a sample x0 ~ 90 from a prior distribution qo,\ne.g., an isotropic Gaussian, to a transformed sample 1(x0) that follows the data distribution q\u2081, i.e.,\n$1(x0) ~ q1. The flow field \u00d8t is parametrized through a vector field ue, which is learned in a simple\nregression task."}, {"title": "2.1 PROBABILITY FLOWS", "content": "The flow ot(x) describes the path of a sample x following the time-dependent vector field ut(x) :\n[0, 1] \u00d7 Rd \u2192 Rd and is itself the solution to the ordinary differential equation\n$\ndot_t(x) = u_t((x)) dt, _0(x) = x_0$.\n(1)\nFor a given density po, Qt induces a time-dependent density called a probability path via the push\nforward operator pt(x) := ([t]*Po)(x) = po(\u03c6\u03c4\u00b9(x)) det[d\u00b9/dx(x)]. Our goal is to find a simple,\ni.e., short and straight, probability path pt whose boundary probabilities align with our prior and data\ndistribution, i.e., po \u2248 qo and P1 \u2248 91.\nIf we now consider two flows $t and 1, their induced probability paths pt and p\u2081 will be equal if their\nflow fields ut and u are equal. Consequently, we can train a generative model for a data distribution\nq1 by matching a model ue to a flow field ut corresponding to a probability path between a prior\ndistribution qo and 91.\nLipman et al. (2022) propose to model the marginal probability paths as a mixture of conditional\npaths, i.e.,\n$P_t(x) = \\int p_t(x | z)q(z)dz$.\n(2)\nMore specifically, they choose low-variance Gaussian paths centered on each data point x\u2081, i.e.,\npt(x | x1) = N(tx1, (1 \u2013 (1 \u2013 5min)t)I), with z = {x1} and q = q1, which result in an isotropic\nprior distribution, i.e., qo = N(0, I). These conditional probability paths have a closed form for their\nconditional flow field ut (x | x1). Furthermore, they show that the marginal flow field arising from\nthese conditional flow fields generates the approximate data distribution q\u2081 and derive the Conditional\nFlow Matching objective:\n$L_{CFM}(\\theta) = E_{t\\sim U[0,1],x_1\\sim q_1,x\\sim p_t(x|x_1)}||u_\\theta(t, x) - u_t (x | x_1)||^2$.\n(3)"}, {"title": "2.2 COUPLINGS", "content": "Tong et al. (2023) generalize Conditional Flow Matching to encapsulate arbitrary source distributions.\nThis is achieved by including not only samples from the target distribution but also from the source"}, {"title": "2.3 MINI-BATCH OPTIMAL TRANSPORT", "content": "A natural choice for joint distributions is the optimal transport coupling between source and target\ndistributions. In practice, however, finding the optimal transport map \u03c0is only feasible for small\ndatasets. Instead, Tong et al. propose a mini-batch variant of the algorithm. For each batch of\ndata {x}1 seen during training, they sample B points xo ~ qo and then compute \u3160 between\nthese batches. This makes finding \u3160 computationally feasible while retaining the benefits of the\noptimal transport coupling empirically (Tong et al., 2023). Choosing q as the optimal transport map \u03c0\nbetween the prior and data distribution makes both training and sampling more efficient by lowering\nthe variance of the objective and straightening the probability paths."}, {"title": "3 TSFLOW", "content": "In this section, we present our main contributions: (1) TSFlow, a novel flow matching model\ntailored for probabilistic time series forecasting (see Fig. 1). TSFlow supports both unconditional\nand conditional generation, i.e., generation without or with partially observed time series. (2) We\nexplore non-i.i.d. prior distributions within the CFM framework and demonstrate that incorporating\na Gaussian process prior improves unconditional generation by simplifying the optimal transport\nproblem. Further, (3) we show how an unconditional model can be employed for forecasting by\nconditioning it during inference via Langevin dynamics and guidance techniques. Finally, (4) we\ndescribe how to use data-dependent prior distributions in the form of Gaussian process regression to\ntrain TSFlow for conditional forecasting directly.\nProblem Statement. Consider a univariate time series y \u2208 R\u00b9 of length L from the data distribu-\ntion q1 (y). We will denote with (yp yf) = y the split of the time series into an observed past yp and\nan unknown future yf we aim to predict.\nOur objective is to capture the conditional distribution q1 (y | yp) using a generative model pe (y | yp).\nBy marginalizing over the noisy time series x0, we decompose the conditional model into a conditional"}, {"title": "3.1 UNCONDITIONAL MODELING", "content": "We begin by modeling the data distribution q1 (y) using an unconditional CFM model pe (y), which\nis optimized using mini-batch optimal transport couplings (see Sec. 2.3 and Alg. 1) and does not\nincorporate any conditioning information during training.\nTo better capture the temporal correlations in the time series y, we explore Gaussian process priors\ninstead of the default isotropic Gaussian distribution. By aligning the prior distribution qo more\nclosely with the data distribution q1, we simplify the optimal transport problem and enhance the\nmodel's ability to learn temporal patterns (Sec. 3.1.1). Then, we describe how to condition the\nunconditional model on observed data yp during inference by incorporating past observations into the\ngeneration process via Langevin dynamics and guidance, effectively transforming it into a conditional\none (Sec. 3.1.2 and 3.1.3)."}, {"title": "3.1.1 INFORMED PRIOR DISTRIBUTIONS", "content": "Previous work by Tong et al. (2023) have shown that pairing prior and data samples based on the\noptimal transport map accelerates training, reduces the number of Neural Function Evaluations (NFEs)\nrequired during inference, and enhances the model's performance by shortening the conditional paths\nand straightening the learned velocity field.\nWe can further enhance this effect by choosing a domain-specific prior distribution qo closer to the\ndata distribution q\u2081 than the standard isotropic Gaussian, yet similarly easy to specify and sample\nfrom. In the context of time series, we propose to employ Gaussian process (GP) priors GP(0, K)\nwith a kernel function K(t, t') (Rasmussen & Williams, 2005). Gaussian processes are well-suited\nfor modeling time series data because they naturally capture temporal correlations. By choosing K,\nwe can incorporate dataset-specific structures and temporal patterns into the prior without requiring\nan extensive training process.\nWe explore three kernel functions that reflect different types of data characteristics: squared exponen-\ntial (SE), Ornstein-Uhlenbeck (OU), and periodic (PE) kernels, defined respectively as:\n$K_{SE}(t, t') = exp(-\\frac{d^2}{l^2})$, $K_{OU}(t, t') = exp(-\\frac{d}{l})$ and $K_{PE}(t, t') = exp(-\\frac{sin^2(d)}{l^2})$,\nwhere d = t - t' and l is a non-negative parameter that adjusts the length scale of the kernel.\nThe squared exponential kernel KSE produces infinitely smooth samples, making it suitable for\nmodeling time series with a high degree of smoothness. The Ornstein-Uhlenbeck kernel Kou is\nclosely related to Brownian motion and ideal for modeling data with a rougher structure. The periodic\nkernel KPE captures repeating patterns in the data by modeling the covariance between points as a\nfunction of their periodic differences, making it suitable for time series with periodic behavior. We\nprovide samples drawn from these priors in App. A.4.\nEffect on the Optimal Transport Problem. To understand the effect of the prior distribution, we\nmeasure the Wasserstein distance between batches {x\u00b2) ~ qo} drawn from the prior and batches"}, {"title": "3.1.2 CONDITIONAL PRIOR SAMPLING", "content": "To adapt the unconditional model for conditional generation after training, we propose conditional\nprior sampling, which conditions the prior distribution as qo (xo | yp). It is important to note that\nthis only conditions the prior distribution, not the generation process itself, which we discuss in\nSec. 3.1.3.\nGiven an observation yp, we aim to find a corresponding sample x0 from the prior distribution that\naligns with it. Formally, this entails sampling from the distribution qo (xo | yp). If we have access to\nits score function \u2207xo log qo (xo | yp), we can sample from this distribution via Langevin dynamics:\n$x_0^{(i+1)} = x_0^{(i)} - \\tau \\nabla_{x_0} log \\ q_0 (x_0^{(i)} | y^p) + \\sqrt{2\\tau} \\xi_i; \\quad with \\xi_i \\sim N(0, I)$\n(7)\nwhere 7 is a fixed step size. With sufficient iterations, Eq. (7) converges to the conditional distribution\n(see Durmus & Moulines (2017) for a convergence analysis).\nBy applying Bayes' rule, we express the conditional score function as:\n$\\nabla_{x_0} log \\ q_0 (x_0 | y^p) = \\nabla_{x_0} log \\ q_1 (y^p | x_0) + \\nabla_{x_0} log \\ q_0 (x_0)$.\n(8)\nHere, the term \u2207xo log q1 (yp | x0) guides xo towards evolving into a sample (after solving the\nODE) aligning with the observation yp, while the term \u2207xo log qo (x0) ensures adherence to the prior\ndistribution's manifold.\nAs qo (xo) is a GP, we can compute its likelihood in closed form. However, the term q1 (yp | X0)\nis unknown and requires more attention. We follow Kollovieh et al. (2023) and model it as an\nasymmetric Laplace distribution centered on the output of the flow $0,1 learned by our model:\n$q_1 (y^p | x_0) = ALD(y^p | \\phi_{\\theta,1} (x_0), \\kappa)$,\n(9)\nleading to the quantile loss after applying the logarithm with quantile \u043a. This aligns better with\nprobabilistic forecasting, which is evaluated using distribution-based metrics. To accelerate the\nevaluation of the score function, we approximate the integration of the flow field in $0,1 in Eq. (9)\nusing a small number of Euler steps.\nBy dynamically choosing the number of iterations in the Langevin dynamics, we can control how\nstrongly the sample should resemble the observation. We provide a pseudocode in Alg. 3."}, {"title": "3.1.3 GUIDED GENERATION", "content": "To perform conditional generation more effectively, we can employ guidance techniques (Dhariwal\n& Nichol, 2021) to modify the score function of the model, directly conditioning the generation\nprocess on the observed data yP. Unlike the previous method, where we only conditioned the prior\ndistribution, we now adjust the dynamics of the generation process itself.\nWe start by modeling the conditional score function using Bayes' rule, similar to Eq. (8):\n$\\nabla_{x_t} log \\ P_t (x_t | y^p) = \\nabla_{x_t} log \\ P_t (y^p | x_t) + \\nabla_{x_t} log \\ P_t (x_t)$.\n(10)\nHere, the term log pt (yp | xt) acts as a guidance term that steers the generation process toward\nproducing samples consistent with the observed past yp.\nTo incorporate this guidance into the generation process, we adjust the vector field ue by subtracting\nthe guidance score, scaled by a factor s:\n$\\tilde{u}_\\theta(t, x_t) = u_\\theta(t, x_t) - s \\cdot \\nabla_{x_t} log \\ p_t (y^p | x_t)$.\n(11)\nWe provide a detailed derivation of uo(xt, t) in App. C. By integrating this modified vector field\n\u0169 over time, we generate samples that are conditioned on the observed data yp. The parameter s\nallows us to control the influence of the conditioning, with larger values of s resulting in samples that\nmore closely resemble the observed past. To model the guidance score, we again use an asymmetric\nLaplace distribution centered around the output of the flow as before:\n$P_t(y^p | x_t) = ALD(y^p | \\phi_{\\theta,1}(x_t), \\kappa)$,\n(12)\nwhere \u043a \u0456\u0455 a parameter controlling the asymmetry of the distribution.\nBy incorporating the guidance term into the vector field and appropriately modeling the guidance\nscore, we effectively condition the generation process to produce forecasts consistent with the\nobserved past, enhancing the model's ability to generate accurate and coherent time series predictions."}, {"title": "3.2 CONDITIONAL MODELING", "content": "TSFlow imposes additional structure on the prior distribution qo in the form of a Gaussian process,\na non-parametric time series model. We build upon this and propose an alternative training and\ninference approach for conditional generation, i.e., forecasting, with TSFlow. Instead of only\nconditioning the sampling process on an observed sample yp, we additionally condition the prior\ndistribution on the observed past data by approximating 90(xo | yp) with qo (X0 | yp).\nWhile Tong et al. (2023) couple x and y via the optimal transport map \u03c0 during training (Sec. 2.3),\nwe exploit the GP prior and define the joint distribution as q(x0,y) = q1 (y) qo (xo | yp). We\ncondition the model using equation Eq. (6) during inference. Consequently, our choice of joint\ndistribution q(x0, y) effectively trains the model with a conditional prior distribution. This aligns\nthe training process with how the model is employed during inference and improves forecasting\nperformance (see Sec. 4.2).\nGaussian Process Regression. Since qo (x0) is a GP, we can compute qo (xo | yp) analytically. We\nbegin by factorizing the conditional prior as:\n$q_0 (x_0 | y^p) = q_0 (x_f | y^p) \\ q_0(x_p | y^p)$,\nwhich follows by assuming independence of xp and xf given yp. We choose qo(x | yp) = N(x |\nyp, I) to retain the information from the observation. For the unobserved future part, we employ\nGPR to model the conditional distribution (Rasmussen & Williams, 2005). Specifically,\n$q_0(x_f | y^p) = N(x_f | \\mu_{f\\mid p}, \\Sigma_{f\\mid p})$,\n(13)\nwith\n$\\mu_{f\\mid p} = \\Sigma_{fp}\\Sigma_{pp}^{-1}y_p \\quad and \\quad \\Sigma_{f\\mid p} = \\Sigma_{ff} - \\Sigma_{fp}\\Sigma_{pp}^{-1}\\Sigma_{pf}$,\n(14)\nwhere \u03a3ff = K(f, f), \u03a3fp = K(f,p), \u03a3pf, Epp = \u039a(p,p) correspond to covariance matrices\ncomputed using the kernel function K."}, {"title": "4 EXPERIMENTS", "content": "In this section, we present our empirical results and compare TSFlow against various baselines using\nreal-world datasets. Our primary objectives are threefold: First, to determine whether the generative\ncapabilities are competitive to other generative frameworks. Second, to investigate the impact of\ndifferent prior distributions on the results. Third, to evaluate how TSFlow compares to other models\nin probabilistic forecasting. Additionally, we aim to explore how conditional prior sampling of the\nunconditional version of TSFlow fares against its conditionally trained counterpart.\nDatasets. We conduct experiments on eight univariate time series datasets from various domains\nand with different frequencies from GluonTS (Alexandrov et al., 2020). Specifically, we use the\ndatasets Electricity (Dheeru & Taniskidou, 2017), Exchange (Lai et al., 2018), KDDCup (Godahewa\net al., 2021), M4-Hourly (Makridakis et al., 2020), Solar (Lai et al., 2018), Traffic (Dheeru &\nTaniskidou, 2017), UberTLC-Hourly (FiveThirtyEight, 2016), and Wikipedia (Gasthaus et al., 2019).\nWe provide further details about the datasets in App. A.1.\nBaselines. To benchmark the unconditional performance of TSFlow, our experiments include\nTimeVAE (Desai et al., 2021) and TSDiff (Kollovieh et al., 2023), representing different types of\ngenerative models. To assess the forecasting performance of TSFlow, i.e., conditional generation, we\ncompare against various established time series forecasting methods. This includes traditional statisti-\ncal methods such as Seasonal Naive (SN), AutoARIMA, and AutoETS (Hyndman et al., 2008). In the\ndomain of deep learning, we evaluate TSFlow against established models including DLinear (Zeng\net al., 2023), DeepAR (Salinas et al., 2020), TFT (Temporal Fusion Transformers) (Lim et al., 2021),\nWaveNet (Oord et al., 2016), and PatchTST (Nie et al., 2022). Finally, we extend our comparison to\ninclude the four diffusion-based approaches CSDI (Tashiro et al., 2021), TSDiff (Kollovieh et al.,\n2023), SSSD (Alcaraz & Strodthoff, 2022), and (Bilo\u0161 et al., 2023) which represent generative models\nin probabilistic time series forecasting. We provide more information in App. \u0410.3.\nEvaluation Metrics. To assess the generative capabilities of TSFlow in an unconditional setting,\nwe calculate the 2-Wasserstein distance between the synthetic and real samples. Specifically, we\ngenerate 10,000 samples using our defined context length for this evaluation. Additionally, to compare\nthe quality of the synthetic samples in a downstream task, we employ the Linear Predictive Score\n(LPS) (Kollovieh et al., 2023), which is defined as the (real) test CRPS of a linear regression model\ntrained on synthetic samples.\nTo evaluate the probabilistic forecasts, we use the Continuous Ranked Probability Score (CRPS)\n(Gneiting & Raftery, 2007), defined as\n$CRPS(F^{-1},y) = \\int \\Delta_{\\kappa}(q,y) d\\kappa = \\int 2\\Lambda_{\\kappa}(F^{-1}(\\kappa), y) d\\kappa$,\nwhere A\u3047(q, y) = (k \u2212 1 {y<q})(y \u2212 q) represents the pinball loss at a specific quantile level \u043a and F\nis the cumulative distribution function of the forecast. CRPS is a proper scoring function that reaches\nits minimum when the forecast distribution F coincides with the target value y. As computing the\nintegral is generally not feasible, we follow previous works (Rasul et al., 2021; Kollovieh et al.,\n2023; Rasul et al., 2020) and approximate the CRPS using nine uniformly distributed quantile levels\n{0.1, 0.2,..., 0.9}. The randomized methods approximate F using 100 samples."}, {"title": "5 RELATED WORK", "content": "Diffusion Models and Conditional Flow Matching. Diffusion models have been applied to\nseveral fields and achieved state-of-the-art performance (Ho et al., 2020; Hoogeboom et al., 2022;\nLienen et al., 2024). Various works have demonstrated the effectiveness of unconditional models in\nconditional tasks through diffusion guidance (Epstein et al., 2023; Dhariwal & Nichol, 2021; Bansal\net al., 2023; Nichol et al., 2021; Avrahami et al., 2022), solving inverse problems (Kawar et al.,\n2022), or iterative resampling (Lugmayr et al., 2022). Conditional Flow Matching (Lipman et al.,\n2022) is a recent approach proposed as an alternative to and generalization of diffusion models. This\nframework has been extended to incorporate couplings between data and prior samples, and trained\nto solve the dynamic optimal transport problem (Tong et al., 2023), and has been further adapted to\ngeneral geometries (Chen & Lipman, 2023). Albergo et al. (2023) demonstrated the applicability\nof data-dependent couplings for stochastic interpolants to improve in-painting and super-resolution\nimage generation. Lastly, recent work shows the potential for conditional generation by solving an\noptimization problem and differentiating through the flow (Ben-Hamu et al., 2024).\nGenerative Models for Time Series. Various generative models have been adapted to time series\nmodeling, including Generative Adversarial Networks (GANs) (Yoon et al., 2019), normalizing\nflows (Rasul et al., 2020; Alaa et al., 2020), and Variational Autoencoders (VAEs) (Desai et al.,\n2021). Recent works have successfully applied diffusion models to time series. The first approach,\nTimeGrad (Rasul et al., 2021), applies a diffusion model on top of an LSTM to perform autoregressive\nmultivariate time series forecasting, which was later extended by Bilo\u0161 et al. (2023) to settings with\ncontinuous time and non-isotropic noise distributions. Note that unlike TSFlow, Bilo\u0161 et al. (2023)\nneither use optimal transport paths nor couplings. CSDI (Tashiro et al., 2021) and SSSD (Alcaraz &\nStrodthoff, 2022) perform probabilistic forecasting and imputation via conditional diffusion models.\nWhile CSDI uses transformer layers for their backbone, SSSD makes use of S4 layers (Gu et al.,\n2021) to model temporal dynamics. Lastly, TimeDiff (Shen & Kwok, 2023) explores conditioning\nmechanisms, and TSDiff (Kollovieh et al., 2023) introduced an unconditional diffusion model that\nallows conditioning during inference time through diffusion guidance."}, {"title": "6 CONCLUSION", "content": "In this work, we introduced TSFlow, a novel conditional flow matching model for probabilistic time\nseries forecasting. TSFlow leverages flexible, data-dependent prior distributions and optimal transport\npaths to enhance unconditional and conditional generative capabilities. Our experiments on eight\nreal-world datasets demonstrated that TSFlow consistently achieves state-of-the-art-performance.\nWe found that non-isotropic Gaussian process priors, particularly the periodic kernel, often led\nto better performance than isotropic priors, even with fewer neural function evaluations (NFEs).\nThe conditional version of TSFlow with Gaussian process regression priors showed improvements\nover diffusion-based approaches, further emphasizing the effectiveness of our proposed methods.\nAdditionally, we demonstrated that the unconditional model can be effectively used in a conditional\nsetting via Langevin dynamics, offering additional flexibility in various forecasting scenarios."}, {"title": "Limitations and Future Work", "content": "While TSFlow has demonstrated remarkable performance across\nmultiple benchmarks and reduced the computational costs compared to its diffusion-based prede-\ncessors, it has been tested only on univariate time series. Future work could extend this approach\nto multivariate time series by leveraging multivariate Gaussian processes that capture dependencies\nacross dimensions. Additionally, exploring source distributions with intractable likelihoods, such as\nneural forecasting methods, might further improve the performance."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "To ensure the reproducibility of our results, we provide a detailed description of our experimental\nsetup, including the benchmark datasets and evaluation metrics, in Sec. 4. Additionally, we outline\nthe hyperparameters used in App. A.2 and include pseudocode for our methods in App. A.8."}]}