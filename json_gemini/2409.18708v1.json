{"title": "Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with ASCII Art to Mask Profanity", "authors": ["Sergey Berezin", "Reza Farahbakhsh", "Noel Crespi"], "abstract": "We introduce a novel family of adversarial attacks that exploit the inability of language models to interpret ASCII art. To evaluate these attacks, we propose the ToxASCII benchmark and develop two custom ASCII art fonts: one leveraging special tokens and another using text-filled letter shapes. Our attacks achieve a perfect 1.0 Attack Success Rate across ten models, including OpenAI's ol-preview and LLaMA 3.1.\nWarning: this paper contains examples of toxic language used for research purposes.\nWith this research, we aim to drive the development of more resilient toxicity detection systems and enhance the robustness of language models overall.", "sections": [{"title": "Introduction", "content": "In nature, birds on islands without predators often lose their ability to fly, as the lack of continuous competition removes the evolutionary pressure to maintain this ability. Similarly, in the field of natural language processing, the effectiveness of toxicity detection systems can stagnate without the constant challenge of new adversarial attacks. To address this, we introduce a new attack strategy that highlights vulnerabilities in current toxicity detection models.\nOur research shows that ASCII art fonts - visual representations of text created using letters, numbers, and symbols arranged in specific patterns, can be used to bypass modern toxicity detectors. By leveraging the fact that text can be interpreted not only semantically but also spatially (i.e., not just by reading the characters but by recognizing the shapes they form), we demonstrate that it is possible to reliably bypass detection mechanisms, including those based on large language models (LLMs).\nAdditionally, we show that techniques like embedding special tokens (e.g., <EOS>) can render toxic text virtually invisible to these models. This exposes a significant weakness in the robustness of contemporary toxicity detection systems, emphasizing the need for further advancements in the field."}, {"title": "Related Work", "content": "ASCII art has evolved as a form of visual communication since the late 19th century (The Brooklyn Daily Eagle, 1875), with notable contributions like Flora F.F. Stacey's typewriter art (Stacey, 1898) and Harmon and Knowlton's \"Studies in Perception I\" (Harmon and Knowlton, 1967). With the rise of the internet, ASCII art naturally transitioned into online communication (Danet, 2001)."}, {"title": "Adversarial Attacks on Toxicity Detection", "content": "The most recent review by Villate-Castillo et al. (2024) categorizes five main types of adversarial attacks on toxicity detection systems:\nVisual attacks: These attacks use similar-looking characters to alter the input (Wang et al., 2023c). Some variants employ invisible Unicode characters to perturb the model's detection (vis, 2022).\nPhonetic attacks: This type involves altering words or inserting phonetically similar variants, which bypass the toxicity filters while maintaining similar meanings (Wang et al., 2023a).\nNegation attacks: These attacks introduce negations in the text to reverse the meaning or perturb the model's toxicity score. (Alexiou and Mertoguno, 2023).\nTrigger word-based attacks: These attacks involve inserting specific words or characters designed to disrupt the model's outputs. (Zhang et al., 2021; Berezin et al., 2023).\nMisspelling-based attacks: These attacks perturb sentences and words by introducing typographical noise, such as intentional misspellings, making it harder for models to detect toxicity (Rodriguez and Rojas-Galeano, 2018)."}, {"title": "ASCII-art in adversarial attacks", "content": "Regarding application of ASCII art in adversarial attacks, Recent research has exposed significant vulnerabilities in large language models (LLMs). Jiang et al. (2024) introduced ArtPrompt, an jail-break attack using ASCII art to bypass LLM alignment mechanisms. Zhou et al. (2024) extended this by using Virtual Context, where special tokens enhance the success of jailbreak attacks, further revealing weaknesses in LLM security protocols.\nOn the side of attacking toxicity detection and content moderation models, Wang et al. (2023b) proposed Metamorphic Testing for Textual Content Moderation, which systematically tests moderation systems by generating varied toxic content through metamorphic transformations as so character substitution and others."}, {"title": "Our Contribution", "content": "We introduce ASCII art as a novel direction for detection evasion adversarial attacks. For this, we create the ToxASCII benchmark and develop two custom ASCII art fonts: one leveraging special tokens and another filled with text.\nWe systematically test developed attacks on ten modern LLM architectures, alongside a character substitution baseline attack.\nOur evaluation focuses not only on the models' ability to interpret ASCII art but also on their capability to detect it.\nFurthermore, we presented the defence mechanisms against aforementioned attacks."}, {"title": "Experiments", "content": "We developed a benchmark called ToxASCII using 269 manually selected, easily human-readable fonts from the Art library version 6.2 (Haghighi, 2024). Each font was used to write 26 toxic phrases, each representing a different letter of the English alphabet, to test the models' recognition capabilities for each character. This setup simulates an adversarial attack scenario in which an adversary attempts to pass an offensive message using ASCII art.\nTo maintain the integrity of the test and avoid data leakage, we specifically chose fonts that do not involve using the same letter to spell itself (e.g., avoiding cases where the letter \"S\" is constructed from smaller \"s\" letters).\nMore information about the benchmark can be found in Appendix B."}, {"title": "Experimental Setup", "content": "We conducted all experiments using an Nvidia H100 GPU, totaling 455 GPU hours. For inference and training, we utilized the August 2024 release of the Unsloth library (unslothai, 2024). Metrics were obtained using scikit-learn version 1.5.1 (Pedregosa et al., 2011). All metrics are obtained as the average of 4 runs. For more details on the models' parameters, refer to Appendix C."}, {"title": "Results", "content": "We tested various large language models and toxicity detection models using our ToxASCII benchmark and compared results with reproduced homoglyphs attack. Across all settings, we achieved a perfect Attack Success Rate of 1.0 (Table 1).\nAll tested models frequently guessed that the content represented benign phrases like \"hello\" or \"hello world\", suggesting their exposure to ASCII art font during training. They were able to to recognize the form of the ASCII art text, but were unable to decode it properly.\nWe also evaluated the performance of specialized toxicity detection models. OpenAI's Moderation API (2024), Detoxify (2020), and the Google Perspective API (2024a) all failed to detect any toxic content in the ASCII representations."}, {"title": "Special Tokens Fonts", "content": "While being unable to interpret ASCII art, LLMs have shown the ability to detect its presence in the text (see Table 2). This capability suggests a potential approach for content moderation by simply filtering out all messages containing ASCII art, classifying them as potentially toxic.\nTo address this limitation, we identified and exploited a vulnerability in the tokenization process of large language models by constructing ASCII art fonts entirely from special tokens, such as <|SEP|> and <lendoftext|>, selecting these tokens according to the attacked model's tokenizer. Example of such font in shown on Figure 1.\nThese special tokens are typically used within language models to indicate parts of the input or to signal the end of a sequence. Being unexpected by the model, carefully positioned special tokens can alter model's performance. Example of such a disruption can be found in Appendix A.\nOur findings showed that any ASCII art structure composed of special tokens was way harder to detect for LLMs. The models also consistently failed to perform basic tasks on such inputs, such as counting the characters."}, {"title": "Text-Filled Fonts", "content": "Another variant of our attack involves creating ASCII fonts that use normal, coherent text to fill the shapes of larger letters. The objective of this approach is to design text that is readable to humans when interpreted as individual large letters, while the model reads and processes only the filler text within each letter shape. An example of this technique is illustrated in Figure 2.\nTo evaluate the effectiveness of this method, we constructed a text-filled font and tested it across multiple models. We inputted these fonts as plain text, and, for multimodal models, as text files and screenshot images. Across all scenarios, models only processed the filler text, ignoring larger letter structure. This discrepancy allowed us to pass messages uninterpreted, bypassing the recognition capabilities of the models (Table 1). Moreover, this technique reduced the models' ability to detect ASCII art, as shown in Table 2."}, {"title": "Defence", "content": "To counteract the vulnerabilities identified in our study, we implemented several defense strategies to enhance the robustness of language models against ASCII art-based adversarial attacks."}, {"title": "Adversarial Training", "content": "We experimented with adversarial training on LLaMA 3.1. In our initial approach, we created a dataset of five palindromic phrases, split into individual words and phrases, each written in various ASCII art fonts and paired with their corresponding natural language labels. By exposing the models to these adversarial examples during training, we aimed to enhance their ability to recognize and correctly interpret ASCII art representations of any text. However, this approach achieved limited success with LLaMA-3.1, as the model struggled to generalize beyond the specific phrases used in training.\nTo address this, we restructured the dataset by varying the fonts while keeping the phrases constant. This new experimental setup resulted in significant improvements in interpretation, indicating that the model could generalize more effectively across different fonts but struggled to generalize across different phrases."}, {"title": "Model Performance", "content": "LLAMA-3.1 8B: The attack success rate was reduced from 1.0 to 0.35.\nLLaMA-3.1 70B: A more substantial improvement, with the attack success rate dropping from 1.0 to 0.18.\nBoth models were trained for 20 epochs. More detains about adversarial training can be found in Appendix C.\nIn addition, we evaluated the LLaMA-3.1 70B model, trained on regular ASCII art fonts, against our custom-crafted fonts:\nSpecial Token-Based Attacks: The success rate decreased slightly from 1.0 to 0.96.\nText-Filled Fonts: Similarly, the success rate dropped from 1.0 to 0.96."}, {"title": "Defense Against Special Tokens and Text-Filled Fonts", "content": "For attacks involving special tokens we recommend parsing strategies that split these special tokens during preprocessing. This approach transforms the problem into a regular ASCII text interpretation task, enabling the model to recognize and appropriately handle sequences containing special tokens.\nTo address attacks using text-filled fonts, we used Optical Character Recognition (OCR) techniques to extract and analyze the text. Applying image downscaling or selected convolution kernels over image can render the filler text unreadable, helping OCR to focus on only the big letters.\nIn our experiments with OCR tools like Tesseract (Google, 2024b) and EasyOCR (JaidedAI, 2024), achieving consistent results across different fonts and phrases was challenging. While there is no universal solution, correct predictions can still be made on a case-by-case basis. This highlights the need for fine-tuning when dealing with text-filled fonts, underscoring the broader challenge of using OCR for ASCII-style art.\nThis strategy could effectively neutralize attempts to hide harmful content within larger ASCII characters. However, care must be taken to ensure that both the larger, visible letters and the filler text are properly analyzed. Without this the system faces a risk of inverse attacks, missing toxic filler text inside a neutral ASCII art."}, {"title": "Conclusion", "content": "In this paper, we introduced a novel class of adversarial attacks that exploit the limitations of language models in interpreting ASCII art. Through the development of the ToxASCII benchmark and two custom ASCII art fonts-leveraging special tokens and text-filled letter structures-we demonstrated how such attacks can successfully bypass content moderation systems. Our experiments across ten large language models, resulted in a perfect 1.0 Attack Success Rate, highlighting significant vulnerabilities in current model architectures.\nThese findings underscore the need for more robust defense mechanisms in toxicity detection systems. We presented defence strategies against the attack, improving models resilience, especially when generalizing across fonts. However, models still struggled to generalize across distinct phrases and more complex adversarial examples. Future work should focus on developing more advanced techniques to detect and mitigate such attacks, as well as improving models' ability to interpret and process spatial and visual representations of text."}, {"title": "Limitations", "content": "Despite the promising results achieved in our study, several limitations warrant consideration. First, our attacks were tested exclusively on a set of specific large language models (LLMs) and toxicity detection systems. While we achieved a 1.0 Attack Success Rate across all models tested, the generalizability of our findings to other models or future versions remains uncertain. The rapid pace of advancements in LLMs and their underlying architectures could impact the effectiveness of our attack strategies over time.\nSecond, our custom ASCII art fonts and the ToxASCII benchmark, though effective in evading detection in this context, may not capture all possible variations of adversarial ASCII art that could be developed. Representing different languages, using different arrangements of characters, more complex shapes, or other types of non-linguistic representations could potentially evade detection in ways not covered by our benchmark. This underscores the potential for more complex attack designs that extend beyond the scope of the ASCII art techniques presented here.\nAnother limitation lies in the defense mechanisms we proposed. While adversarial training demonstrated improvements in detecting ASCII art, the models struggled to generalize beyond the specific phrases and fonts used during training. This suggests that the current defenses may not be robust enough to handle unforeseen variations in ASCII art, particularly in real-world scenarios where attacks can be highly diverse. Furthermore, our special-token-based defenses and OCR approaches were effective in specific cases but did not consistently perform across all fonts and configurations. For example, attacks using text-filled fonts with benign content require further refinement in OCR techniques to fully neutralize them.\nFinally, our experiments focused on text-based and multimodal models with no analysis of how audio, video, or image-based toxicity detection systems might respond to similar attacks.\nIn summary, while our work presents a significant step forward in identifying vulnerabilities in LLMs and toxicity detection systems, more research is needed to assess the generalizability, scalability, and long-term effectiveness of both the attacks and defenses outlined in this paper."}, {"title": "Ethical Considerations", "content": "The development and testing of adversarial attacks on toxicity detection systems, as explored in this research, raise important ethical considerations. While our work aims to improve the robustness and security of AI models, we must acknowledge the potential misuse of these techniques. Adversarial methods that exploit vulnerabilities in content moderation systems could be maliciously used to bypass filters, spread harmful content, or evade detection of illegal activities. Therefore, maintaining ethical responsibility in the dissemination and application of these findings is crucial.\nThe societal impact of ineffective toxicity detection, especially on marginalized communities - is a key concern, emphasizing the need for secure and inclusive digital spaces.\nWe explicitly state that the toxic language examples used in this research are solely for scientific purposes. Our intent is to identify vulnerabilities and strengthen detection systems. We encourage responsible disclosure and collaboration within the research community to develop more robust defenses."}, {"title": "Mitigations", "content": "To address the risks posed by these adversarial attacks, we employed adversarial training, which demonstrated promise in reducing attack success rates. However, these defenses are not comprehensive, particularly against more complex attacks like those using special tokens. Future research should focus on improving the generalization of defenses and ensuring that models evolve alongside emerging adversarial techniques.\nWe advocate for continuous updates to defense strategies and combining automated detection with human oversight to maintain robust content moderation systems."}, {"title": "Responsible Use", "content": "We strongly encourage the responsible disclosure of vulnerabilities to model developers and active collaboration within the research community to ensure that these techniques are used to enhance security, rather than to exploit it."}, {"title": "Example of How Special Tokens Disrupt LLM Interpretation of ASCII Art", "content": "To illustrate how special tokens like <|end|> interfere with the spatial structure of ASCII art and compromise the performance of language models in detecting the content, we conducted the following experiment using the microsoft/Phi-3.5-mini-instruct model's tokenizer:\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n# Regular ASCII art"}]}