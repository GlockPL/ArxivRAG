{"title": "REAL: Response Embedding-based Alignment for LLMs", "authors": ["Honggen Zhang", "Igor Molybog", "June Zhang", "Xufeng Zhao"], "abstract": "Aligning large language models (LLMs) to human preferences is a crucial step in building helpful and safe AI tools, which usually involve training on supervised datasets. Popular algorithms such as Direct Preference Optimization rely on pairs of AI-generated responses ranked according to human feedback. The labeling process is the most labor-intensive and costly part of the alignment pipeline, and improving its efficiency would have a meaningful impact on AI development. We propose a strategy for sampling a high-quality training dataset that focuses on acquiring the most informative response pairs for labeling out of a set of AI-generated responses. Experimental results on synthetic HH-RLHF benchmarks indicate that choosing dissimilar response pairs enhances the direct alignment of LLMs while reducing inherited labeling errors. We also applied our method to the real-world dataset SHP2, selecting optimal pairs from multiple responses. The model aligned on dissimilar response pairs obtained the best win rate on the dialogue task. Our findings suggest that focusing on less similar pairs can improve the efficiency of LLM alignment, saving up to 65% of annotators' work.", "sections": [{"title": "1 Introduction", "content": "Large Language models (LLMs), empowered by the enormous pre-trained dataset from the Internet, show the power to generate the answers to various questions and solutions to challenging tasks. However, they might generate undesirable content that is useless or even harmful to humans (Wang et al., 2024). Additional training steps are required to optimize LLMs and, thus, align their responses with human preferences. For that purpose, Christiano et al. (2017) proposed Reinforcement learning from human feedback (RLHF). It consists of estimating the human preference reward model (RM) from response preference data (Ouyang et al., 2022) and steering LLM parameters using a popular reinforcement learning algorithm of proximal policy optimization (PPO). RLHF requires extensive computational resources and is prone to training instabilities.\nRecently, direct alignment from preference (DAP) approach, which does not explicitly learn the reward model, has emerged as an alternative to RLHF (Zhao et al., 2022; Song et al., 2023; Zhao et al., 2023; Xu et al., 2023; Rafailov et al., 2024). Direct Preference Optimization (DPO) (Rafailov et al., 2024; Azar et al., 2023) is a milestone DAP method. It formulates the problem of learning human preferences through finetuning LLM with implicit reward model using a training set D = {xi, y_{i}^{+}, y_{i}^{-}}1N, where xi is the ith prompt, y_{i}^{+}, y_{i}^{-} are the corresponding preferred and not-preferred responses.\nDPO requires the explicit preference signal from an annotator within the dataset. Some DPO variations such as Contrastive Post-training (Xu et al., 2023), RPO(Song et al., 2023) were proposed to augment D using AI but might generate low-quality pairs. The labeling rough estimate is $0.1 - $1 per prompt; with 100,000-1,000,000 prompts, it would cost approximately $100,000 to augment the dataset. Some other DPO variation methods(Guo et al., 2024; Yu et al., 2024) actively choose better samples using additional annotators at the cost of increased computations.\nIn this paper, we propose a novel method for enhancing DPO learning with efficient data selection (see Fig. 1). We should only train DPO on the most informative subset of samples in D. Inspired by works in contrastive learning(Chen et al., 2020), we connect the usefulness of response pair (y_{i}^{+}, y_{j}^{+}) to the cosine similarity between their representations in the embedding space. Sampling similar pairs"}, {"title": "2 Related work", "content": "Direct Alignment of Language Models: Despite RLHF's effectiveness in aligning language models (LMs) with human values, its complexity and resource demands have spurred the exploration of alternatives. Sequence Likelihood Calibration (SLiC)(Zhao et al., 2022) is a DAP method to directly encourage the LLMs to output the positive response and penalize the negative response.\nChain of Hindsight (CoH) (Liu et al., 2023) is equivalent to learning a conditional policy. DPO (Rafailov et al., 2024) directly optimizes LMs using a preference-based loss function to enhance training stability in comparison to traditional RLHF. DPO with Dynamic \u03b2 (Wu et al., 2024) introduced a framework that dynamically calibrates \u03b2 at the batch level, informed by the underlying preference data. Existing work(Azar et al., 2023) identified that DPO were susceptible to overfitting and introduced Identity Preference Optimization (IPO) as a solution to this issue. The generative diversity of LLM deteriorated and the KL divergence grew faster for less preferred responses compared with preferred responses, and they proposed token-level DPO (TDPO)(Zeng et al., 2024) to enhance the regulation of KL divergence.\nData Quality in Direct Alignment Due to the large data needed for the Direct Alignment, PRO(Song et al., 2023) proposed preference ranking with listwise preference datasets, which could directly implement alignment in the fine-tuning process. However, fine-tuning was constrained by the limitations of available data and the imperfections inherent in human-generated data. Contrastive Post-training (Xu et al., 2023) tries to build more datasets using other LLMs to advance the training process of DPO without considering the mistakes. Recently, similar to active learning to select samples based on current models (Settles, 2009), (Guo et al., 2024; Morimura et al., 2024) use the AI as an annotator to monitor the quantity of data pairs for each training step but it will be expensive. (Yu et al., 2024) use LLMs to design a refinement function, which estimates the quality of positive and negative responses. LESS (Xia et al., 2024) is an optimizer-aware and practically efficient algorithm to estimate data influences on gradient Similarity Search for instruction data selection. However, this data selection is online so needs more computation."}, {"title": "3 Background", "content": "LLM alignment refers to the process of training a language model to assign a higher probability to a response y with a higher human preference reward Rxy. An estimation r of the reward is used in practice. It is imperative to ensure that for a given prompt x the estimated reward r(x, y) is close to the true reward Rxy for each response. The problem of learning the best estimator for the reward"}, {"title": "3.1 LLM Alignment to Human Feedback", "content": "The human feedback rarely comes in the form of the true reward samples (y, Rxy). Ranking or pairwise comparison of responses is more common. There are two sample responses (y+, y\u2212) that correspond to a single prompt x in the pairwise comparison case. Human subjects provide a preference to label them as Rxy+ > Rxy\u2212x where y+ and y\u2212 are the preferred and non-preferred responses, respectively. This method of labeling does not explicitly provide the true reward signal. However, alignment can still be performed by applying the Reinforcement Learning from Human Feedback (RLHF) algorithm using binary human preference data. The Bradley-Terry(Bradley and Terry, 1952) model defines the log-odds\nlog p*/(1-p*) = r(x,y^+) \u2013 r(x,y^-)  (2)\nWhere p is the preference probability. Modeling the margins of the response pair can also be viewed in the perspective of estimating the true reward in Eq. 1. Assume the ground truth reward for r(x,y+) and r(x, y\u2212) is 1 and 0 respectively. The difference between the estimated reward and the truth is Eq(y+) [r(x, y+) \u2013 1] + Eq(y\u2212) [r(x,y+) \u2013 0] = E[r(x, y+) \u2013 r(x, y\u2212) \u2013 1]."}, {"title": "3.2 Direct Language Model Alignment", "content": "RLHF is expensive, and we cannot guarantee that the reward model will be optimal. Recently, Direct alignment from preferences methods have emerged to replace the RLHF when aligning the LLMs. Direct Preference Optimization(DPO)(Rafailov et al., 2024) optimizes the policy \u03c0 directly as an alternative to reward model r. Given the static dataset of D = {xi, y_{i}^{+}, y_{i}^{-}}1N sampled from the human preference distribution p, the objective becomes:\nLDPO = Ex,y+,y- [logo (Blog \u03c0\u03b8(y+|x)/\u03c0ref(y+|x)/\u03c0\u03b8(y\u2212|x)/\u03c0ref(y\u2212|x))] (5)"}, {"title": "4 Method", "content": "We proposed identifying and selectively labeling high-quality pairs to reduce the error in the label while improving performance. The loss LDPO (Eq.5) is estimated based on the empirical dataset D. This thus raises the question of how to build the empirical dataset D to the better approximate expectation of Ex,y+,y- in terms of the samples of prompt x and response pairs (y+,y\u2212). However, the DPO assumes that we have access to the preferences through a dataset D = {xi, Y., Yi}1N, where N is the data size. One can show that as the size of the dataset D grows (Azar et al., 2023), becomes a more and a more accurate estimate of the true reward Rxy. Without the large of N, how to sample a good quality of D matters for the learning of the implicit reward."}, {"title": "4.1 Consistent Responses Embedding Over Training", "content": "In the processing of learning the LLM using DPO, our goal is to increase the difference between the conditional probability \u03c0\u03b8(y+|x)/\u03c0ref(y+|x) and \u03c0\u03b8(y\u2212|x)/\u03c0ref(y\u2212|x). The parameterized conditional probability \u03c0\u03b8(y|x) is changing over the training. However, in the fine-tuning stage, the independent probability \u03c0\u03b8(y) might change a little since the model has been pre-trained on the large dataset already.\n\u03c0\u03b8(y|x) \u221d \u03c0\u03b8(x|y)\u03c0\u03bf(y) (6)\nIn our assumption, independent probability almost consists over the fine-tuning process i.e. \u03c0\u03b81 (y) \u2248 \u03c0\u03b8t+1(y). In other words, the subset parameters \u03c6 of \u03b8 which decide the independent probability of \u03c0\u03c6(y) are not changed.\nUnlike the encoder model, such as BERT, we define the embedding of response y from the hidden state of LLM. It is always extracted from the last hidden layer of LLMs. For a response y we use the activation matrix Y to denote the matrix of activations of the last hidden layer of LLM. This matrix has its first dimension equal to the length of the sequence y and the second dimension equal to the model dimension. Note that the invariant output of activation (softmax) induces that the row is unchanged or uniformly translated. See proof in the Appendix B. Applying the last linear layer g(\u00b7) of the LLM to the rows of this matrix would result in logits of the distribution over the vocabulary for every position in the sequence. The embedding of the response pair is defined as the average of the activation matrix Y over the first (sequence length) dimension. The dimension of the embedding is equal to the dimension of the model, and its j \u2013 th component can be calculated as\nyi = 1/S \u03a3i=1SYij (7)\nwhere S is tokens of y.\nTo generate the embeddings, we use the base model that is to be aligned. While it might be beneficial to extract the features on the fly from the model as it is being aligned, static embedding strikes a good balance between relevance and computational efficiency since the embedding have to be computed once and in bulk. We conducted experiments to measure the correlation between the embedding extracted from the base and the aligned models, finding that they ranged between 0.7 and 0.85, demonstrating a great deal of similarity. Study of the feasibility of using the embedding formed by the base model throughout the entire alignment process is further studied in Section B.1.\nBased on the consistent embedding space, we would extract the represent pair (yi, yj) to label. The represent pair (yi, yj) will have a better quality and produce less error in the label. Due to the vocabulary size N is always extremely large, we instead use the input of Y\u2032 = g\u22121(Y) as the substitution of the Y."}, {"title": "4.2 Pair Selection", "content": "Selecting a high-quality dataset is necessary to save more labeling effort and training resources. In our hypothesis, selecting the different responses will affect the learning result (learning human preference)(Dubey et al., 2024). SLiC(Zhao et al., 2023) calculates the sequence score based on the target response y*. Due to the target preference y* has already been selected, it is hard to make the selected yi surpass the quality of y*. In our dataset construction, our response pair will not depend on the prompt x, but also the y*. Hence we leverage the property of invariant sentence embedding space without using target response y*. See the diagram Fig. 1 of our selection.\nIn representation learning, the embedding similarity is commonly used to calculate the similarity of images(Chen et al., 2020), sentences(Mikolov, 2013; Zhang et al., 2024). The inner product of yTi yj is common to use to measure the similarity. The normalized one yTi yj/(||Yi|| ||Yj||) is referring the cosine similarity cos(Yi, Yj).\nFor the large of unlabeled response pairs {Yl, Ym}, we try to extract the valuable pairs for labeling based on the embedding similarity. By ranking the training response pairs, the LLMs will obtain the human preference for such responses. The similar responses are hard to rank and thus put the model on a tricky problem but might obtain labeling error. In contrast, the dissimilar responses might learn an easier problem with less error in the label.\nWe build the random sub-set Drandom as the baseline first by extracting random pairs form {Y1, Y2,\u00b7, Yk}.\nWe also define similar pairs and dis-similar pairs as the hard sub-set and easy sub-set, respectively.\nDhard = {Yi, yj|cos(Yi, yj) \u2264 cos(ys, yt)\u2200s, t} (8)\nDeasy = {Yi, yj|cos(Yi, yj) \u2265 cos(ys, yt)\u2200s, t} (9)\nAfter obtaining the response pairs: random, hard, and easy, we just need to label the sub-set to obtain the preference pairs."}, {"title": "5 Experiments", "content": "For the experiments, we used two types of datasets in terms of the number of responses. The Anthropic HH-RLHF helpfulness base and harmlessness base datasets (Bai et al., 2022) represent the first type. It consists of 43k and 44k prompts and corresponding response pairs, generated primarily by proprietary language models trained by Anthropic and not released to the public. The second type is the SHP2 (Ethayarajh et al., 2022) dataset, which consists of 4.8M collective human preferences over multiple responses from Reddit or StackExchange posts. We only use Reddit with questions and multiple responses. The score of each response is calculated from the number of positive and negative votes. Appendix A describe more data details."}, {"title": "5.2 Experiment on Anthropic HH-RLHF", "content": "The Anthropic HH-RLHF datasets consist of one response pair for each prompt. For each prompt x from a dataset, we calculate the similarity of the corresponding response pair (y+, y\u2212) using the embeddings extracted from the base model \u03c0ref to be aligned. By ranking the data tuples (x, y+, y\u2212) based on similarity values, we categorize the 50% of the tuples with the highest similarity scores as the \"hard\" dataset and the bottom 50% as the \"easy\" dataset. We randomly select 50% of the tuples from the entire Anthropic HH-RLHF as the \"random\" baseline dataset. The \"random\" dataset represents the data distribution that would be the default for a state-of-the-art alignment system. The detail of the experiment setting can be found in Appendix C."}, {"title": "5.2.1 Results:Easy is better for alignment", "content": "We supervised fine-tuned (SFT) the base model using the chosen response from the training split of the entire dataset to get the Tref. After that, we sample \"hard\" and \"easy\" and \"random\" sub-sets from Anthropic-HH, Then, we use the \"hard,\" \"easy,\" and \"random\" sub-sets to align the SFT model Tref using the DPO procedure.\nWe evaluate the checkpoints on the test dataset Dtest over the alignment process. Following (Yu et al., 2024; Xia et al., 2024), we employ loss and margins as the target metrics and use the implicit reward score extracted from the model to calculate them. Given a prompt x and the corresponding chosen and rejected responses y+ and y\u2212, the margins are defined as\nMargins = 1/|Dtest| \u03a3x,y+,y- r(x,y+) - r(x,y-)  (10)\nand the loss is defined as\nLoss = 1/|Dtest| \u03a3x,y+,y- logo(\u03b2(r(x,y+) \u2013 r(x,y-))) (11)\nwhere the implicit reward r(x,y) is calculated from the reference and aligned models probability outputs as r(x,y) = Blog \u03c0\u03b8(yx)/\u03c0ref(yx). We use both margins and loss to analyze the DPO results with a higher degree of confidence. Generally, large margins do not necessarily induce small losses and vice versa. We depict the dynamics of the loss on the"}, {"title": "5.2.2 Discussion", "content": "We further interpret the superiority of the \"easy\" sub-set for alignment purposes.\nErrors in the labeling for different sub-set: The reason for the superior quality of the \"easy\" alignment sub-set might be the decreased frequency of erroneous and biased data points within the training dataset. In the dataset, we assume that the chosen response y+ will be more preferred by the human than the rejected response y\u2212 so y+ > y\u2212. However, we might made a mistake: indeed the y\u2212 > y+. In contrastive learning, such data points would be called \"false negatives\"(Zhang et al., 2024). We randomly selected 200 response pairs from \"easy\" and \"hard\" sub-sets sorted using the embeddings from all three base models under consideration: Phi1.5, Pythia-2.8B and LLaMa2-7B. We formed a baseline by randomly choosing 200 pairs from the Helpfulness and Harmlessness datasets. The GPT4 was used to re-label the pairs into chosen and rejected responses. In Figure 3, we plot the proportion of response pairs that were assigned the same labels as the ones recorded in the original dataset. The \"hard\" subset, which consists of more similar response pairs, evidently also contains more response pairs where the ranking is unclear, resulting in more incorrect pairs than the \"random\" and \"easy\" subsets. Similarly, the \"easy\" subset contains more confidently ranked pairs and fewer erroneous response labels, which results in a higher dataset quality. It is consistent with the research that filters similar samples such as Llama3 (Dubey et al., 2024).\nEasy bring lower reward: We also compare the training process of different sub-sets, as shown in Fig.4. From Figure 4a and Figure 4b, the Easy will converge to a lower reward than the Hard one so it is more sensitive to the margins r(x,y+) \u2013 r(x,y\u2212) change. Slight changes in the reward for the \"easy\" will bring more margins than \"hard\" and \"random\". The \"hard\" has the larger reward on both chosen and rejected so will induce a smaller increase on the margins. Therefore, the Easy obtains better margins in the training dataset, as shown in Fig. 4c.\nAnother explanation for the observed results might be that the prompts corresponding to the Easy response pairs have benign statistical qualities as, by construction of the subsets, the prompts are not shared between Easy and Hard subsets. To rule out this possibility, we conduct the experiments on another dataset, SHP2, where a single prompt corresponds to multiple responses."}, {"title": "5.3 Experiment on SHP2", "content": "In this section, we consider a scenario in which all prompts will be shared for all sub-sets. We used SHP2 Reddit folder, where each prompt is a Reddit post with a question/instruction and the responses are the top-level comments for that post. In this setting, we need to select the responses for each prompt. For k responses {Y1, Y2,\uff65\uff65\uff65, Yk}, we select the one pair from it. Based on the selection strategy from Section 4.2, we have the \"hard\", \"easy\", and \"random\" subsets.\nIn addition, we consider a scenario of balancing the similarities and errors in the label, to selecting the centers of clusters in the embedding space. We have the \"centroid\" sub-set Dcentroids = {x, y1, ym}, where\nyi = argmini\u2208C1 ||yi - u1||2\nym = argminj\u2208C2 ||yj - u2||2,\nwhere u\u2081 and u2 are the two clustering centers C1 and C2 in the embedding space. The centroid pairs are the nearest points to the centers. We compute the clustering center based on the K-means, i.e,\n{u1, u2} = argminu1,u2 \u03a3i=1 \u03a3y\u2208Ci ||y - uz||2 (12)"}, {"title": "5.3.1 Results", "content": "The experiments on SHP2 were conducted using the Phi1.5 model. Similarly to the experiments on Anthropic-HH-RLHF, we first fine-tune the base model on the training split of the SHP2 dataset. Due to the large dataset, we randomly select 1/3 data (with different random seeds) from the whole training data to do the SFT. Using the SFT model as the reference, we conduct the DPO alignment with four selected subsets of response pairs.\nTo evaluate the aligned models, we randomly select 100 prompts from the SHP2 test split and generate the responses using five versions of the Phi1.5 checkpoints: SFT, \"random\", \"hard\", \"easy\", and \"centroid\". We use AlpacaEval-GPT4 to judge the relative alignment of the test responses to human preferences. Specifically, we directly compare the responses of such checkpoints to the responses of the SFT reference model to calculate the win rate depicted in Fig. 5b. As shown in Fig. 5a, the \"hard\" has smaller margins and larger losses compared to others. We add the \"centroid\" method to compare with the Easy. Both Easy and Centroid checkpoints show good results, with \"centroid\" being slightly ahead in terms of loss and margin. By mixing the \"easy\" and the \"random\", \"centroid\" obtained a better result on both metrics than Easy. AlpacaEval-GPT4 are used to calculate the win rate with the reference model. The results are presented in Fig. 5b. \"centroid\" has the best Win and Easy has the best lose rate. This experiment provides evidence that using centroid and \"easy\" data to train the model by DPO will lead to a more safety and helpful generated model."}, {"title": "6 Conclusion", "content": "This paper proposes a high-quality data selection method for DPO alignment, showing that an effective selection strategy improves LLM alignment efficiency. By considering the embedding space of response, we found that the dissimilar pair of responses will better align LLMs with human preferences. The similar pair might have more chance to obtain the error in the label from the data itself while it will obtain a larger gradient. Our method will benefit the data selection to avoid labeling larger human preference pairs. Choosing the data from the offline model embedding will also save much computation compared to the active data selections."}, {"title": "A Appendix A", "content": "In this section, we list the dataset of HH-RLHF and SHP we used for training and testing.\nHH Training dataset build step:\n1.  Regardless of the prompt, we calculate the embedding similarity for all response pairs given the base model (Phi1.5, Pythia 2.8B LLaMa2-7B). Note that the embeddings are obtained by the average of tokens.\n2.  We rank the response pairs based on the similarity on descent. Selecting the first 50% response pairs as the Easy dataset and the rest of 50% response pairs as the Easy dataset.\n3.  We also random select the 50% data as the Random dataset\nThus, we will obtain three sub-dataset. The Random and Easy, Hard will have 50% overlapped.\nFor the SHP2, we use the Reddit folder dataset. It is the response from the people's answer to the question from Reddit. Each response has a score which is evaluated by the positive votes and negative votes. We exclude the response pairs length over 512 and the ratio of chosen score and rejected score less than 1.5. After that, we obtained 35.6k response pairs. Each prompt has three (most of) or four responses. Training dataset build step:\n1.  For each prompt, we calculate the embedding similarity of the combination of four or three responses. For example, the three responses will have C3 = 3 response pairs.\n2.  We rank the 3 response pairs based on the similarity on descent. Selecting the largest similar value of response pairs as the Hard one and the smallest similar value of response pairs as the Easy one.\n3.  We cluster the 3 response based to two groups based on embedding. We select the most nearest centroid response as the center of the group. The centers will regard as the the Centroid pairs.\n4.  We also random select two responses from the 3 responses to build the Random dataset"}, {"title": "B Appendix B", "content": "The invariant logit (output of softmax) induces that the row is unchanged or uniformly translated. Proof:\nGiven zt as the current vector for the input of softmax, dt is the vector of arbitrary real numbers (not constant), representing the differences applied to each component of zt. Thus, we have\nzt+1 = zt + dt (13)\nThe softmax function for a component i of zt is:\nsoftmax(z) = exp(z)/(\u03a3j exp(z)) (14)\nFor the Zt+1:\nsoftmax(z+1) = exp(z+d)/(\u03a3j exp(z+d)) (15)\nThe presence of di in the exponent alters the relative scaling of each component. The softmax function effectively amplifies or diminishes the differences between the components of zt in a non-linear fashion due to the exponential operation. When dt is not uniform, the exponential increase or decrease applied to each zi is different, affecting the proportion of each exp(zi + di) in the sum \u03a3j exp(zi + dj), hence altering the softmax output.\nThe only case where dt does not change the softmax output is when all di all equal, essentially making dt a uniform vector, i.e. dt = c1. Thus, we have the\nsoftmax(z+1) = exp(z+c)/(\u03a3j exp(z+c)) = softmax(z) (16)"}, {"title": "B.1 Evolution of embeddings throughout alignment", "content": "The definition (7) of the embedding of a response is model-dependent. Thus, it is natural to assume that the embedding of a response obtained using different checkpoints of the model being aligned will vary. In this Section, we describe the experiment aimed at verifying that the responses' embedding does not change substantially throughout the fine-tuning process. Thus, neglecting the distribution shift during the training process is acceptable, and exploiting the base model for the response selection process (the process of identifying easy and hard response pairs) is justified.\nIn this section, we trained the complete dataset and saved 6 checkpoints for every 1,000 steps. Fig. 6a shows the changes of the difference (y\u00b3 (t + 1) \u2013 y(t))2 over time. The upper part Y is obtained with the sentence (x, y) and the lower part Y is obtained with only the response y. We can see the change is extremely small for the independent Y. We also plot the similarity s(y+,y\u2212) of 20 pairs over time, as shown in Fig. 6b. The similarity distribution is consistent over time."}, {"title": "C Appendix C", "content": "Experments Setting: We use three different-sized base LLMs (Phi1.5-1.3B, Pythia 2.8B, and LLama2 7B) as the backbone in our experiments. Using the dataset of the preferred prompt-response pairs (x,y+), we run the supervised finetuning (SFT) procedure with each base model to obtain their SFT versions. The supervised fine-tuning does not require a pairwise comparison of the responses and can be performed on prompt-response pairs outside the alignment dataset. However, our experiments use the preferred response pairs from the dataset under consideration (Anthropic HH-RLHF or SHP2). After the SFT stage, we run separate DPO procedures on each of the selected sub-sets (easy, hard, ect.) with each of the three SFT LLMs. This procedure is illustrated in Algorithm 1. All experiments are run on 4 A100 GPUs. We set the batch size as 64, 32, and 16 for Phi1.5-1.3b, Pythia 2.8, and LLama 7b, respectively. Algorithm C shows the pseudocode of alignment using subdata."}, {"title": "D Appendix D", "content": "System: You are a highly efficient assistant, who evaluates and selects the best large language model (LLMs) based on the accuracy and harmless of their responses to a given instruction. This process will be used to create a leaderboard reflecting the most accurate and human-preferred answers.\nUser: I'll provide you with prompts given to these models and their corresponding outputs. Your task is to assess these responses, and select the model that produces the best output from a human perspective."}]}