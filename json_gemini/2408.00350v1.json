{"title": "A Simple Background Augmentation Method for Object Detection with Diffusion Model", "authors": ["Yuhang Li", "Xin Dong", "Chen Chen", "Weiming Zhuang", "Lingjuan Lyu"], "abstract": "In computer vision, it is well-known that a lack of data diversity will impair model performance. In this study, we address the challenges of enhancing the dataset diversity problem in order to benefit various downstream tasks such as object detection and instance segmentation. We propose a simple yet effective data augmentation approach by leveraging advancements in generative models, specifically text-to-image synthesis technologies like Stable Diffusion. Our method focuses on generating variations of labeled real images, utilizing generative object and background augmentation via inpainting to augment existing training data without the need for additional annotations. We find that background augmentation, in particular, significantly improves the models' robustness and generalization capabilities. We also investigate how to adjust the prompt and mask to ensure the generated content comply with the existing annotations. The efficacy of our augmentation techniques is validated through comprehensive evaluations of the COCO dataset and several other key object detection benchmarks, demonstrating notable enhancements in model performance across diverse scenarios. This approach offers a promising solution to the challenges of dataset enhancement, contributing to the development of more accurate and robust computer vision models.", "sections": [{"title": "1 Introduction", "content": "The quest for robust and accurate object detection [13,14,18,41] and instance segmentation [16] models is a cornerstone for contemporary computer vision research. These models underpin a variety of applications, from autonomous vehicles [21] to medical image analysis [31,60], necessitating high levels of precision and reliability. Central to the development of these models is the availability of large, diverse, and accurately annotated datasets. However, creating such datasets is fraught with challenges, including the high costs of data collection and annotation, privacy concerns, and the potential for dataset bias. For"}, {"title": "2 Related Work", "content": "This section provides literature reviews of object detection, data augmentation, and text-to-image generation for visual tasks."}, {"title": "Object Detection", "content": "Object detection aims to simultaneously predict the category and corresponding bounding box for the objects in the images. Generally, object detectors [18,41,42] are trained on a substantial amount of training data with bounding box annotations and can only recognize a predetermined set of categories present in the training data, e.g. MS COCO dataset [30]. Both backbone architecture and the detection neck/head framework are important for object detection performance. For backbone, Vision Transformer [10,17,25,32] has emerged as an effective backbone alternative compared to convolutional neural networks [19]. These transformer-based models leverage self-attention mechanisms to capture global dependencies within the image. Moreover, the development of efficient and scalable detectors, such as EfficientDet [50], further underscores the importance of optimizing both accuracy and computational efficiency."}, {"title": "Data Augmentation", "content": "Data augmentation [26, 49] plays an indispensable role in the deep learning model, as it forces the model to learn invariant features, and thus helps generalization. The data augmentation is applied in many areas of vision tasks including object recognition [7,8,27], video perception [62], and semantic segmentation [45]. Apart from learning invariant features, data augmentation also has other specific applications in deep learning. For example, adversarial training [15,52] leverages data augmentation to create adversarial samples and thereby improves the adversarial robustness of the model. In terms of data augmentation in object detection, Zoph et.al [64] propose reinforcement learning to search for complex augmentation policies including color transformations, geometric transformations, and bounding box transformations. CutMix [61] proposes a spatial mixing strategy for augmenting the detection results. RandAugment [8] uses a reduced search space for searching both classification and detection augmentation policies."}, {"title": "Text-to-Image Data for Visual Tasks", "content": "Traditional synthetic data are acquired through the renderings from the graphics engines [11,38,43]. However, this type of synthesis cannot guarantee the quality and diversity of the generated data, resulting in a large gap with real-world data. Recent success in generative models has made synthesizing photo-realistic and high-fidelity images possible, which could be used for training the neural networks for image recognition due to their unlimited generation. For example, early works explored Generative Adversarial Networks (GAN) [6] for image recognition tasks. [2] uses a class-conditional GAN to train the classifier head and [63] uses StyleGAN to generate the labels for object segmentation. [23] adopts the GAN as a generator to synthesize multiple views to conduct contrastive learning. Until recently, the text-to-images models [9, 36, 44, 46] have been leveraged to synthesize high-quality data for neural network training due to their effectiveness and efficiency. [20] adopts GLIDE [37] to synthesize images for classifier tuning on the CLIP model [39]. However, simply tuning the classifier may not fully explore the potential of synthetic data. StableRep [51] proposes to use Stable Diffusion for generating pre-trained datasets for contrastive learning and leverages the synthetic data from different random seeds as the positive pairs. [1] explores synthesizing data under ImageNet label space and produces improved performance. Fill-up [48] balances the long-tail"}, {"title": "3 Generative Background Augmentation", "content": "In this section, we introduce our generative background augmentation method using the text-to-image diffusion model to increase the diversity of the original dataset. We first discuss the principle of the Inpainting technique and then adapt this method to our background augmentation framework."}, {"title": "3.1 Generative Augmentation with Inpainting", "content": "Our primary data synthesis model is Stable Diffusion [44]. Formally, the Stable Diffusion generates images from a random Gaussian noise z ~ N(0, 1) by applying the denoising model \\(\\epsilon(\\cdot)\\) repeatedly. We denote the text language prompt embedding as s. In this work, we apply the widely-used classifier-free guidance [22] which linearly combines the conditional estimate and the unconditional estimate of noise, given by\n\n\\(\\epsilon(s, z_t) = w\\epsilon(s, z_t) + (1 - w)\\epsilon(z_t),\\)\n\nwhere w is the guidance scale and t is the step of the denoising process. After multiple denoising steps, Stable Diffusion employs a decoder D to decode the latent to the image.\n\nGiven an input image x, the inpainting task is to re-generate selective areas in the image indicated by a mask variable m while keeping other pixels unchanged. Since Stable Diffusion conducts the generation process in the latent space, we consider inpainting in the latent space as well. First, the encoder projects input image to latent space \\(x = E(x)\\). Similarly, the mask variable is also projected to the latent space by resizing it \\(m = \\text{resize}(m)\\) to have the same size of x. Now, the inpainting denoising step can be formulated as\n\n\\(\\hat{\\epsilon}(s, x_t) = m \\epsilon(s, x_t) + (1 \u2013 m) (x_t - x_{t-1}).\\)\n\nHere, \\(\\odot\\) denotes the element-wise multiplication operation. The inpainting process described above modifies only the masked region, denoted as m, within the original image's latent space. Owing to the localized nature of Stable Diffusion's decoding mechanism, this method can also provide a reasonable assurance that solely the region specified by m will be regenerated within the image space."}, {"title": "3.2 Object Augmentation or Background Augmentation?", "content": "Data augmentation serves the role of increasing data amount and diversity to optimize for generalization performance. Traditionally, data augmentation in object detection is conducted through image transformations. For example, Zoph"}, {"title": "3.3 Utility-Aware Background Augmentation", "content": "In this section, we concentrate on background augmentation techniques with high training utility. An optimal background augmentation approach must avoid introducing extraneous objects, as these can confound the training process due to the lack of corresponding annotations. Additionally, it is crucial for a background augmentation method to preserve the integrity of existing objects, thereby maintaining a high level of object-mask alignment. We have investigated several design strategies to fulfill these requirements."}, {"title": "Prompt Selection of Background Augmentation", "content": "A critical choice for data generation is the text prompts for guiding the inpainting process. For object augmentation, one can safely adopt the class label as the text prompt while for background augmentation a text description for the whole background is needed. Some datasets like MS COCO provide image caption annotations for each image, which could be a potential text prompt for background augmentation. However, applying an image caption as a text prompt directly brings two disadvantages in practice. First, the image caption usually contains object descriptions as well. Using this caption will generate additional objects in the background. Second, not all detection and segmentation datasets contain image captions. It would require other captioning models to generate the new caption for the image,"}, {"title": "Background Mask Erosion", "content": "During the process of background augmentation, we occasionally observed that objects would inadvertently extend into the background region. This phenomenon is illustrated in Fig. 2(b), where the giraffe appears larger than its corresponding mask, thereby disrupting the alignment with its initial annotation. To address this issue, we introduce the concept of background mask erosion. This technique employs a minimum filter to the background mask, specifically, \\(m_t = \\text{MinimumFilter}(m_b, k)\\). We empirically determined that a kernel size k of 7 is effective in preventing object extension. Fig. 3(a) demonstrates the process of eroding the background mask with the minimum filter, and Fig. 3(b) shows a much better result after adopting our background mask erosion."}, {"title": "Adaptive Augmentation Freedom", "content": "Considering the variation in background region sizes across different images, there is an increased likelihood of errors during background augmentation such as the introduction of undesired objects or the unwarranted extension of objects into the background areas for images with larger backgrounds, potentially leading to the generation of harmful synthetic data. In the Stable Diffusion inpainting process, the number of diffusion steps can be adjusted to calibrate the extent of modifications applied to the original"}, {"title": "3.4 Training with Background Augmented Data", "content": "Unlike traditional data augmentation which modifies the image tensor on the fly [8], our method generates the additional images and expands the dataset size. Hence, our data augmentation can be combined with conventional data"}, {"title": "4 Experimental Evaluation", "content": "In this section, we investigate the impact of the augmented data by using our simple background augmentation framework. We first visualize several examples of data augmentation and then report object detection/instance segmentation performance on MS COCO [30] and PASCAL VOC [12] datasets. We also conduct several ablation studies and investigations into different data regimes."}, {"title": "4.1 Visualization", "content": "We demonstrate a variety of synthesis results (as well as the original image and the background mask image) using our method on the MS COCO dataset [30], as shown in Fig. 5. Our approach can modify the background of training data to various extents. In certain instances, the new background remains within a similar scene as the original (e.g., images at the second row and first and second columns), yet exhibits different styles and content. Conversely, some augmented backgrounds may represent entirely new scenes. For example, in the fourth row and first column, the original background of a stop sign is transformed into a purple sky, while maintaining the original stop sign objects unchanged. In the fifth/sixth row second column, our method converts a background into a photo hanging on the wall.\n\nThese background augmentations enhance the diversity of the training dataset without introducing additional annotation burdens or compromising the quality of existing annotations."}, {"title": "4.2 Object Detection Evaluation", "content": "We start with the evaluation of our method on the MS COCO dataset. We train a Faster-RCNN [42] with FPN [28], using the ResNet-50 [19] backbone. We also provide results of other datasets and model architectures in Sec. 4.6.\n\nImplementation Details. Specifically, we sampled 1%, 5%, 10%, and 25% of the original training dataset. By default, background augmentation was applied to the selected training images, generating one augmented copy per image. Both the original and the augmented images were utilized during training. For the noise sampling schedule, we employed Stable Diffusion v2-1 [44] and DPMSolver [34, 35]. We resized the image resolution to 512 \u00d7 512 for the inpainting process and subsequently restored the images to their original resolution post-inpainting."}, {"title": "Comparison to Baselines", "content": "We compare our method against a baseline Faster-RCNN model that does not utilize generative background augmentation. Both the baseline model and our approach incorporate conventional data augmenta-"}, {"title": "4.3 Comparisons on Transformer Architecture", "content": "We also evaluate our background augmentation on Transformer-based architecture. Specifically, we use Swin Transformer small [32] as backbone, Mask-RCNN [3,18] as detection framework to train on the full MS COCO dataset. The training hyper-parameters are kept the same as the original configurations [32], i.e. AdamW [33] optimizer (initial learning rate of 0.0001, weight decay of 0.05, and batch size of 16), and 3x schedule (36 epochs with the learning rate decayed"}, {"title": "4.4 Ablation Studies", "content": "In this section, we conduct ablation studies that verify the design choices of our background augmentation pipeline. In order to do this, we generate different sets of COCO training data and combine them with original data to train a ResNet-50 RetinaNet [29]. We use a similar training schedule and hyper-parameters with previous experiments. In particular, we compare several cases: (1) using object augmentation or background augmentation, (2) using background augmentation but with or without our proposed techniques. Specifically, for prompt input we either use the COCO caption as prompt or our proposed simple prompt method, and for background mask erosion as well as the adaptive diffusion steps, we choose to either apply them or not."}, {"title": "4.5 Augmenting More Data", "content": "In previous experiments, we only generate 1x of COCO data. This is still far from the ability of text-to-image models as it can generate a different image by randomly sampling a different z. In this section, we explore the limits of our background augmentation. Specifically, we experiment with two cases. In the first case, we repeatedly generate \u03b1 times data for each image and study the effect of increasing \u03b1 (called uniform sampling). In the second case, we increase the ratio of augmenting small objects in the training data since detecting small objects often poses more challenges than detecting large objects [24]. Therefore, we increase the proportion of small object detection based on the background area ratio, which is called non-uniform sampling. In total, we also generate \u03b1x copy of COCO data but with more augmented data for small objects in this case.\n\nWe evaluate these two scaling cases in the 10% COCO data regime. The \u03b1 is set to 1, 2, 5, and 10 (Note that \u03b1 = 0 indicates no background augmentation applied). We use ResNet-50-based Faster RCNN [42] to evaluate different data regimes. The training hyper-parameters are kept the same with Sec. 4.2. We report four different metrics: mAP, mAPsmall, mAPmedium, and mAPbig, as shown in Fig. 6.\n\nFrom the figure, we find that uniformly increasing the number of augmented data does not consistently increase the mAP performance. Observing the results from different sizes of the objects, we find that the major cause is the small object performance, which is decreasing when \u03b1 \u2265 2. Instead, non-uniform sampling prioritizes the small objects in augmentation and ensures more diverse images are created in small object detection, leading to better-performed detectors."}, {"title": "4.6 Evaluation on PASCAL VOC", "content": "To evaluate the effectiveness of our proposed background augmentation on an entirely different dataset and another detection algorithm, we train a Faster R-CNN [42] model and a RetinaNet model [29] with a ResNet-50 backbone on PASCAL VOC dataset [12]. We combine the training sets of PASCAL VOC 2007 and PASCAL VOC 2012, and test our model on the PASCAL VOC 2007 test set (4952 images). Our evaluation metric is the bounding box mean average precision (mAPbox). The training hyper-parameters are kept the same with previous experiments and the backbone model is initialized from the ImageNet pre-trained checkpoint [19]. We also include the performance of RandAugment in the PASCAL VOC evaluation.\n\nThe results are summarized in Tab. 4, where we can find our method leads to a significant increase in terms of box m\u0410\u0420."}, {"title": "5 Conclusion", "content": "In this work, we propose a simple yet effective background augmentation framework for object detection and instance segmentation. We directly utilize the existing mask annotation to perform Inpainting to create new content in the training data. Through a set of rigorous explorations, we confirm that background augmentation, instead of object augmentation, can achieve higher potential. We also propose several techniques to ensure the background contains no additional object so that the generated data can be directly used for augmentation. Results show that our method can be effectively used to improve the detection performance. Our method has certain limitations: for images containing extremely large objects, background augmentation can hardly alter the content (as shown in Fig. 6 the improvement of mAPLarge is relatively small). In this case, our method could be combined with an effective object augmentation (that may require finetuning of the text-to-image model) to enrich the diversity of this type of data."}]}