{"title": "A Simple Background Augmentation Method for Object Detection with Diffusion Model", "authors": ["Yuhang Li", "Xin Dong", "Chen Chen", "Weiming Zhuang", "Lingjuan Lyu"], "abstract": "In computer vision, it is well-known that a lack of data diversity will impair model performance. In this study, we address the challenges of enhancing the dataset diversity problem in order to benefit various downstream tasks such as object detection and instance segmentation. We propose a simple yet effective data augmentation approach by leveraging advancements in generative models, specifically text-to-image synthesis technologies like Stable Diffusion. Our method focuses on generating variations of labeled real images, utilizing generative object and background augmentation via inpainting to augment existing training data without the need for additional annotations. We find that background augmentation, in particular, significantly improves the models' robustness and generalization capabilities. We also investigate how to adjust the prompt and mask to ensure the generated content comply with the existing annotations. The efficacy of our augmentation techniques is validated through comprehensive evaluations of the COCO dataset and several other key object detection benchmarks, demonstrating notable enhancements in model performance across diverse scenarios. This approach offers a promising solution to the challenges of dataset enhancement, contributing to the development of more accurate and robust computer vision models.", "sections": [{"title": "1 Introduction", "content": "The quest for robust and accurate object detection [13,14,18,41] and instance segmentation [16] models is a cornerstone for contemporary computer vision research. These models underpin a variety of applications, from autonomous vehicles [21] to medical image analysis [31,60], necessitating high levels of precision and reliability. Central to the development of these models is the availability of large, diverse, and accurately annotated datasets. However, creating such datasets is fraught with challenges, including the high costs of data collection and annotation, privacy concerns, and the potential for dataset bias. For"}, {"title": "2 Related Work", "content": "This section provides literature reviews of object detection, data augmentation, and text-to-image generation for visual tasks."}, {"title": "3 Generative Background Augmentation", "content": "In this section, we introduce our generative background augmentation method using the text-to-image diffusion model to increase the diversity of the original dataset. We first discuss the principle of the Inpainting technique and then adapt this method to our background augmentation framework."}, {"title": "3.1 Generative Augmentation with Inpainting", "content": "Our primary data synthesis model is Stable Diffusion [44]. Formally, the Stable Diffusion generates images from a random Gaussian noise z ~ N(0, 1) by applying the denoising model \u2208(\u00b7) repeatedly. We denote the text language prompt embedding as s. In this work, we apply the widely-used classifier-free guidance [22] which linearly combines the conditional estimate and the unconditional estimate of noise, given by\n$$e(s, z_t) = w e(s, z_t) + (1 \u2212 w)e(z_t),$$\nwhere w is the guidance scale and t is the step of the denoising process. After multiple denoising steps, Stable Diffusion employs a decoder D to decode the latent to the image.\nGiven an input image x, the inpainting task is to re-generate selective areas in the image indicated by a mask variable m while keeping other pixels unchanged. Since Stable Diffusion conducts the generation process in the latent space, we consider inpainting in the latent space as well. First, the encoder projects input image to latent space x = E(x). Similarly, the mask variable is also projected to the latent space by resizing it m = resize(m) to have the same size of x. Now, the inpainting denoising step can be formulated as\n$$\\hat{e}(s, x_t) = m \\odot e(s, x_t) + (1 \u2013 m) \\odot (x_t - x_{t\u22121}).$$\nHere, $\\odot$ denotes the element-wise multiplication operation. The inpainting process described above modifies only the masked region, denoted as m, within the original image's latent space. Owing to the localized nature of Stable Diffusion's decoding mechanism, this method can also provide a reasonable assurance that solely the region specified by m will be regenerated within the image space."}, {"title": "3.2 Object Augmentation or Background Augmentation?", "content": "Data augmentation serves the role of increasing data amount and diversity to optimize for generalization performance. Traditionally, data augmentation in object detection is conducted through image transformations. For example, Zoph"}, {"title": "3.3 Utility-Aware Background Augmentation", "content": "In this section, we concentrate on background augmentation techniques with high training utility. An optimal background augmentation approach must avoid introducing extraneous objects, as these can confound the training process due to the lack of corresponding annotations. Additionally, it is crucial for a background augmentation method to preserve the integrity of existing objects, thereby maintaining a high level of object-mask alignment. We have investigated several design strategies to fulfill these requirements.\nPrompt Selection of Background Augmentation. A critical choice for data generation is the text prompts for guiding the inpainting process. For object augmentation, one can safely adopt the class label as the text prompt while for background augmentation a text description for the whole background is needed. Some datasets like MS COCO provide image caption annotations for each image, which could be a potential text prompt for background augmentation. However, applying an image caption as a text prompt directly brings two disadvantages in practice. First, the image caption usually contains object descriptions as well. Using this caption will generate additional objects in the background. Second, not all detection and segmentation datasets contain image captions. It would require other captioning models to generate the new caption for the image,"}, {"title": "3.4 Training with Background Augmented Data", "content": "Unlike traditional data augmentation which modifies the image tensor on the fly [8], our method generates the additional images and expands the dataset size. Hence, our data augmentation can be combined with conventional data"}, {"title": "4 Experimental Evaluation", "content": "In this section, we investigate the impact of the augmented data by using our simple background augmentation framework. We first visualize several examples of data augmentation and then report object detection/instance segmentation performance on MS COCO [30] and PASCAL VOC [12] datasets. We also conduct several ablation studies and investigations into different data regimes."}, {"title": "4.1 Visualization", "content": "We demonstrate a variety of synthesis results (as well as the original image and the background mask image) using our method on the MS COCO dataset [30], as shown in Fig. 5. Our approach can modify the background of training data to various extents. In certain instances, the new background remains within a similar scene as the original (e.g., images at the second row and first and second columns), yet exhibits different styles and content. Conversely, some augmented backgrounds may represent entirely new scenes. For example, in the fourth row and first column, the original background of a stop sign is transformed into a purple sky, while maintaining the original stop sign objects unchanged. In the fifth/sixth row second column, our method converts a background into a photo hanging on the wall.\nThese background augmentations enhance the diversity of the training dataset without introducing additional annotation burdens or compromising the quality of existing annotations."}, {"title": "4.2 Object Detection Evaluation", "content": "We start with the evaluation of our method on the MS COCO dataset. We train a Faster-RCNN [42] with FPN [28], using the ResNet-50 [19] backbone. We also provide results of other datasets and model architectures in Sec. 4.6.\nImplementation Details. Specifically, we sampled 1%, 5%, 10%, and 25% of the original training dataset. By default, background augmentation was applied to the selected training images, generating one augmented copy per image. Both the original and the augmented images were utilized during training. For the noise sampling schedule, we employed Stable Diffusion v2-1 [44] and DPMSolver [34, 35]. We resized the image resolution to 512 \u00d7 512 for the inpainting process and subsequently restored the images to their original resolution post-inpainting."}, {"title": "Comparison to Baselines", "content": "We compare our method against a baseline Faster-RCNN model that does not utilize generative background augmentation. Both the baseline model and our approach incorporate conventional data augmenta-"}, {"title": "4.3 Comparisons on Transformer Architecture", "content": "We also evaluate our background augmentation on Transformer-based architecture. Specifically, we use Swin Transformer small [32] as backbone, Mask-RCNN [3,18] as detection framework to train on the full MS COCO dataset. The training hyper-parameters are kept the same as the original configurations [32], i.e. AdamW [33] optimizer (initial learning rate of 0.0001, weight decay of 0.05, and batch size of 16), and 3x schedule (36 epochs with the learning rate decayed"}, {"title": "4.4 Ablation Studies", "content": "In this section, we conduct ablation studies that verify the design choices of our background augmentation pipeline. In order to do this, we generate different sets of COCO training data and combine them with original data to train a ResNet-50 RetinaNet [29]. We use a similar training schedule and hyper-parameters with previous experiments. In particular, we compare several cases: (1) using object augmentation or background augmentation, (2) using background augmentation but with or without our proposed techniques. Specifically, for prompt input we either use the COCO caption as prompt or our proposed simple prompt method, and for background mask erosion as well as the adaptive diffusion steps, we choose to either apply them or not."}, {"title": "4.5 Augmenting More Data", "content": "In previous experiments, we only generate 1x of COCO data. This is still far from the ability of text-to-image models as it can generate a different image by randomly sampling a different z. In this section, we explore the limits of our background augmentation. Specifically, we experiment with two cases. In the first case, we repeatedly generate a times data for each image and study the effect of increasing a (called uniform sampling). In the second case, we increase the ratio of augmenting small objects in the training data since detecting small objects often poses more challenges than detecting large objects [24]. Therefore, we increase the proportion of small object detection based on the background area ratio, which is called non-uniform sampling. In total, we also generate ax copy of COCO data but with more augmented data for small objects in this case.\nWe evaluate these two scaling cases in the 10% COCO data regime. The a is set to 1, 2, 5, and 10 (Note that a = 0 indicates no background augmentation applied). We use ResNet-50-based Faster RCNN [42] to evaluate different data regimes. The training hyper-parameters are kept the same with Sec. 4.2. We re-port four different metrics: mAP, mAPsmall, mAPmedium, and mAPbig, as shown in Fig. 6.\nFrom the figure, we find that uniformly increasing the number of augmented data does not consistently increase the mAP performance. Observing the results from different sizes of the objects, we find that the major cause is the small object performance, which is decreasing when a \u2265 2. Instead, non-uniform sampling prioritizes the small objects in augmentation and ensures more diverse images are created in small object detection, leading to better-performed detectors."}, {"title": "4.6 Evaluation on PASCAL VOC", "content": "To evaluate the effectiveness of our proposed background augmentation on an entirely different dataset and another detection algorithm, we train a Faster R-CNN [42] model and a RetinaNet model [29] with a ResNet-50 backbone on PASCAL VOC dataset [12]. We combine the training sets of PASCAL VOC 2007 and PASCAL VOC 2012, and test our model on the PASCAL VOC 2007 test set (4952 images). Our evaluation metric is the bounding box mean average precision (mAPbox). The training hyper-parameters are kept the same with previous experiments and the backbone model is initialized from the ImageNet pre-trained checkpoint [19]. We also include the performance of RandAugment in the PASCAL VOC evaluation.\nThe results are summarized in Tab. 4, where we can find our method leads to a significant increase in terms of box m\u0410\u0420."}, {"title": "5 Conclusion", "content": "In this work, we propose a simple yet effective background augmentation framework for object detection and instance segmentation. We directly utilize the existing mask annotation to perform Inpainting to create new content in the training data. Through a set of rigorous explorations, we confirm that background augmentation, instead of object augmentation, can achieve higher potential. We also propose several techniques to ensure the background contains no additional object so that the generated data can be directly used for augmentation. Results show that our method can be effectively used to improve the detection performance. Our method has certain limitations: for images containing extremely large objects, background augmentation can hardly alter the content (as shown in Fig. 6 the improvement of mAPLarge is relatively small). In this case, our method could be combined with an effective object augmentation (that may require finetuning of the text-to-image model) to enrich the diversity of this type of data."}]}