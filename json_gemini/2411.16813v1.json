{"title": "Fine-Tuning LLMs with Noisy Data for Political Argument Generation", "authors": ["Svetlana Churina", "Kokil Jaidka"], "abstract": "The incivility in social media discourse complicates the deployment of automated text generation models for politically sensitive content. Fine-tuning and prompting strategies are critical, but underexplored, solutions to mitigate toxicity in such contexts. This study investigates the fine-tuning and prompting effects on GPT-3.5 Turbo using subsets of the CLAPTON dataset of political discussion posts, comprising Twitter and Reddit data labeled for their justification, reciprocity and incivility. Fine-tuned models on Reddit data scored highest on discussion quality, while combined noisy data led to persistent toxicity. Prompting strategies reduced specific toxic traits, such as personal attacks, but had limited broader impact. The findings emphasize that high-quality data and well-crafted prompts are essential to reduce incivility and improve rhetorical quality in automated political discourse generation.", "sections": [{"title": "Introduction", "content": "Research in controlled text generation has progressed substantially, focusing on generating text that aligns with specific content requirements while managing stylistic and tonal dimensions effectively. Techniques such as fine-tuning with specialized datasets and leveraging diverse large language models (LLMs) have been employed to improve the relevance and appropriateness of generated text. However, significant challenges persist, particularly in applying these advancements to politically sensitive contexts on social media platforms.\nSocial media platforms, as critical arenas for political discourse, bring unique challenges, especially regarding incivility in automated text. In this domain, two critical decision points emerge: the role of fine-tuning settings and the impact of prompting strategies. First, there is the problem of dataset quality. Our hypothesis is that fine-tuning on noisy platform data amplifies uncivil traits in model outputs. The problem can be generalized in terms of one of cross-domain adaptation for the generalization of a next-word-prediction model trained on a particular type of dataset and then prompted to work for prompts based on a different dataset or contextual setting. While there has recently been a spurt in studies that explore LLM fine-tuning for cross-domain text classification (Nasir, Sharma, and Jaidka 2023), their findings were inconclusive about whether fine-tuning on these subsets matters in comparison to the massive datasets that LLMs are already trained on. Furthermore, it is not known whether any benefits of fine-tuning would extend to the generative context.\nThe second problem that arises is that the different in outputs is often cued to the variance in the prompting strategies. Here, we expect that instruction variance significantly influences model behavior, yet it is not clear whether, given their ability to steer model output, they would only dampen the effects of fine-tuning on noisy data, or render it to be completely ineffective. To address these intertwined challenges, our research focuses on two objectives:\n\u2022 To explore the outcome of approaches for controlled text generation that are fine-tuned on uncivil and noisy data.\n\u2022 To investigate prompting methods that enable models to generate text that maintains content relevance and tonal accuracy while eschewing undesirable traits.\nOur study aims to mitigate the risks associated with deploying language models in politically sensitive environments and to enhance the overall efficacy of controlled text generation on social media platforms."}, {"title": "Background", "content": "Previous studies examining online political discourse are rooted in Habermas' concept of the deliberative public sphere (Habermas 1984; Gastil 2008). These studies focus on the essential criteria for public deliberation, emphasizing the use of language that fosters consensus, articulates arguments supported by evidence, encourages responses, and demonstrates empathy towards others (Stromer-Galley 2007; Steenbergen et al. 2003; Esteve Del Valle, Sijtsma, and Stegeman 2018; Friess and Eilders 2015). The stylistic definitions and measurements in this study are informed by the CLAPTON corpus (Jaidka 2022), an annotated Corpus (in English) for the Linguistic Analysis of Political Talk"}, {"title": "Data", "content": "The CLAPTON dataset (Jaidka 2022) was used to create inputs, fine-tune models, and analyze the effects of training data on output quality. Four subsets of the dataset were created to evaluate how varying the data source and removing incivility affected the fine-tuned models' performance:\n\u2022 CLAPTON (T+R): This subset includes data from both Twitter and Reddit, with a total of 25,527 data points.\n\u2022 CLAPTON (R): This subset contains only Reddit data, consisting of 8,682 data points.\n\u2022 CLAPTON (T+R, no incivility): This subset includes combined Twitter and Reddit data but excludes any instances of incivility, resulting in 23,970 data points.\n\u2022 CLAPTON (T): This subset consists solely of Twitter data, with a total of 16,845 data points.\nThe datasets were split into training and validation sets, where the training sets were used to fine-tune GPT3.5-turbo models on inputs with style labels. On the other hand, held out validation set were first pre-processed using KeyBERT to constitute weighted keyphrases and only then used as model inputs for the different model variants. The purpose of pre-processing the inputs was to ensure that the model did not infer style or tone directives through the input text."}, {"title": "Method", "content": "To examine the effect of training data on fine-tuned models, we compared the outputs generated from models fine-tuned on different subsets of the CLAPTON dataset, aiming to identify patterns linked to incivility. To examine the dampening effect of prompting strategies, we modified the prompts such as appending direct instructions to reduce incivility and evaluated their effects across zero-shot, few-shot, and the fine-tuned models obtained from the first step."}, {"title": "Results", "content": "Using the Perspective API, we scored the generated arguments across various dimensions of quality, including Respect, Compassion, Curiosity, Affinity, and Toxicity.\nThe overall toxicity metric, which reflects undesirable traits in the generated outputs, highlights notable differences between subsets. We zoom into these differences in Figures 2 (a) and (b), where we see a spike in the toxicity of fine-tuned models from Twitter and Reddit both across all subcategories of toxicity reported by the Perspective API. The only notable exceptions are reductions in per-sonal attacks targeting authors and commentors, suggesting a slight improvement in this specific dimension. However, the broader trend indicates that fine-tuning on these datasets amplifies other forms of toxicity, emphasizing the challenges of mitigating undesirable traits in domain-specific fine-tuning processes.\nConsidering the quality dimensions, Table 2 provides a detailed comparison of the quality metrics across different models and prompting strategies. Among the GPT-3.5 turbo outputs, the few-shot on the CLAPTON (R) subset achieved the highest scores for Respect (0.582), Compassion (0.606), Curiosity (0.698), and Affinity (0.694), reflecting its ability to produce more nuanced and considerate outputs. Conversely, the CLAPTON (T+R) subset across zero-shot, few-shot, and fine-tuning showed consistently lower scores for these metrics, with Respect capped at 0.218 and Compassion at 0.292. However, despite its lower scores in these categories, CLAPTON (T+R) achieved the lowest Toxicity score (0.176), while the highest Toxicity score was observed in the CLAPTON (T) subset (0.194).Notably, a comparison between the zero-shot results of CLAPTON (R) and CLAP-\u03a4\u039f\u039d (T) reveals that the Twitter subset displays significantly higher Toxicity scores, aligning with the general perception of more toxic content on that platform.\nPrompting GPT-3.5 specifically to reduce incivility, effectively implementing a tone directive, resulted in noticeable improvements in Respect, Compassion, and Affinity scores, while the Toxicity score remained unchanged at 0.187. For example, Respect improved from 0.218 to 0.482, Compassion from 0.292 to 0.438, and Affinity from 0.298 to 0.545. These improvements highlight the potential of tone directives to guide models toward generating more respectful and empathetic outputs, even when starting from datasets with lower baseline scores."}, {"title": "Rhetorical insights", "content": "For outputs from the Twitter-based dataset, we further probed their rhetorical complexity, and observed that they lag far behind human-authored arguments in terms of integrating argumentative elements, such as alignment, disagreement, social expectations, and external claims, to emphasize their points.\nWe examined the generated outputs for the presence of phrases related to rhetorical moves for argument alignment, authority, and persuasion. The Alignment and Authority arguments remain the most argumentatively robust, with a wider variety of alignment and authority moves across categories compared to the generated outputs.\nCertain rhetorical moves from the AAWD corpus, such as 'credentials' and 'experiential claims,' either appear minimally or not at all in GPT-3.5 turbo-generated arguments. This suggests domain-specific limitations when comparing auto-generated content to the diversity and depth of human-written arguments. Table 3 underscores that human-written"}, {"title": "Discussion and Conclusion", "content": "The study confirms that fine-tuning on domain-specific subsets, such as Reddit-exclusive data, enhances the civility and rhetorical quality of outputs. This aligns with prior work suggesting that targeted training data from similar platforms is crucial for effective generalization in tasks like hate speech detection (Swamy, Jamatia, and Gamba\u00a8ck 2019; Fortuna, Soler Company, and Wanner 2021). However, our findings extend this by emphasizing that the distribution of labels, such as incivility indicators, plays a pivotal role in fine-tuning outcomes.\nWhile fine-tuning strategies proved effective in reducing specific toxic traits, such as personal attacks, their broader impact on overall toxicity and rhetorical depth was limited. On the other hand, while prompts can steer model outputs and mitigate certain undesirable behaviors, their influence cannot fully compensate for noisy training data. These findings suggest that prompting works best as a complementary strategy, reinforcing the effects of fine-tuning rather than replacing it. Furthermore, the rhetorical analysis revealed that even fine-tuned models struggle to match the richness of human-written arguments, particularly in alignment and authority moves. While fine-tuning improved certain metrics, the inability to replicate human-like rhetorical diversity suggests limitations in current LLM architectures. This finding"}, {"title": "Limitations", "content": "While this study provides valuable insights into the performance of GPT-3.5 for argument generation and toxicity mitigation, certain limitations should be noted. First, although GPT-40 and other advanced models may yield better results, we deliberately chose GPT-3.5 due to its balance of performance and computational efficiency. Its smaller size and lower resource requirements make it more practical for applications in cost-constrained environments and accessible for reproducibility by other researchers.\nAdditionally, our focus on GPT-3.5 aligns with the goal of benchmarking a widely adopted model as a baseline for understanding the nuances of argument quality and toxicity reduction. Exploring newer or domain-specific models, while potentially fruitful, was outside the scope of this work and is identified as an avenue for future research.\nFinally, while we only evaluated GPT-3.5 against specific datasets and metrics, broader comparisons across diverse models or datasets could provide further generalizability. Future work could address these aspects by including GPT-40 or more advanced models, specialized fine-tuned models, or alternative architectures."}, {"title": "GPT-3.5 turbo finetuning settings", "content": "The prompt used for finetuning GPT-3.5 turbo models to generate text with reciprocity and justification styles was:\n{'role': 'system', 'content': \"You are\nan assistant capable of transforming\ntext into a 'justification\u2019\nor 'reciprocity' variant.\"}\n{'role': 'user', 'content': {\n'style': <reciprocity/justification/none>,\n<input text follows here>\n}}"}]}