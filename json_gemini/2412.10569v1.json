{"title": "Learning to Merge Tokens via Decoupled Embedding for Efficient Vision Transformers", "authors": ["Dong Hoon Lee", "Seunghoon Hong"], "abstract": "Recent token reduction methods for Vision Transformers (ViTs) incorporate token merging, which measures the similarities between token embeddings and combines the most similar pairs. However, their merging policies are directly dependent on intermediate features in ViTs, which prevents exploiting features tailored for merging and requires end-to-end training to improve token merging. This paper proposes Decoupled Token Embedding for Merging (DTEM) that enhances token merging through a decoupled embedding learned via a continuously relaxed token merging process. Our method introduces a lightweight embedding module decoupled from the ViT forward pass to extract dedicated features for token merging, addressing the restriction from using intermediate features. The continuously relaxed token merging, applied during training, enables us to learn the decoupled embeddings in a differentiable manner. Thanks to the decoupled structure, our method can be seamlessly integrated into existing ViT backbones and trained either modularly by learning only the decoupled embeddings or end-to-end by fine-tuning. We demonstrate the applicability of DTEM on various tasks, including classification, captioning, and segmentation, with consistent improvement in token merging. Especially in the ImageNet-1k classification, DTEM achieves a 37.2% reduction in FLOPs while maintaining a top-1 accuracy of 79.85% with DeiT-small. Code is available at https://github.com/movinghoon/dtem.", "sections": [{"title": "1 Introduction", "content": "Transformers [30] have become the dominant and most popular architecture in machine learning, excelling in various modalities and tasks. In computer vision, Vision Transformers (ViTs) [9, 29] have achieved state-of-the-art performance, outperforming conventional backbones in tasks such as image classification [29], object detection [4], and segmentation [15], as well as multimodal applications such as image captioning [32] and visual question answering [17]. A key factor in the success of ViTs is their ability to capture long-range dependencies between patches or tokens, regardless of their spatial positions, using self-attention. However, due to self-attention, ViTs have high computational and memory costs that increase quadratically with the number of tokens. Consequently, there has been significant interest in developing methods to improve the computational efficiency of ViTs.\nIn this pursuit, token reduction [25, 23, 39, 22] aims to progressively reduce the number of tokens in Transformer layers, often adhering to predefined reduction rates. Early approaches [25, 23, 39] propose to prune unimportant tokens based on their contribution to the task, as measured by scoring functions. Yet, simply pruning tokens leads to information loss, often resulting in significant performance degradation in high reduction rates. Alternatively, approaches based on token merging [21, 26, 40, 2, 20, 33, 12] aim to combine redundant tokens instead of removing them. Such redundancy is measured by the similarity between the tokens based on intermediate features in ViT, such as token- or key-embeddings. Token merging has several advantages over pruning; it can"}, {"title": "2 Background", "content": "Given a Transformer that takes N input tokens $X \\in \\mathbb{R}^{N\\times d}$, the objective of token merging is to gradually merge r tokens at each Transformer block, reducing the number of tokens to $X \\in \\mathbb{R}^{(N-r)\\times d}$. Here, the r denotes the reduction rate. To this end, prior works conduct the merging in two steps, grouping and merging, which are expressed as:\n$E = Group(S)$,\n(1)\n$X = Merge(X, E)$,\n(2)\nwhere $S \\in \\mathbb{R}^{N\\times N}$ denotes the similarity matrix of tokens, e.g., $S_{ij} = cos(x_j,x_j)$. Given the similarity S, the grouping operator (Eq. 1) identifies pairs of tokens to merge and represents them in the reachability matrix $E \\in \\{0,1\\}^{N\\times N}$ with $(N - r)$ connected components, where $e_{ij} = 1$ indicates that the ith and jth tokens belong to the same component and will be merged. The merging operator (Eq. 2) then combines all connected tokens in E by pooling.\nThe performance of the above framework highly depends on the choice of the grouping operator, as it dictates the merging policy (i.e., which tokens to merge), and computing the reachability matrix can be costly. Early works employ clustering algorithms [21, 40], but they tend to be slow due to the iterative procedure and often suffer from performance drops due to the dramatic distribution shift from X to X caused by aggressive clustering.\nRecently, ToMe [2] introduced Bipartite Soft Matching (BSM) as an efficient grouping operator of Eq. 1. To parallelize the computation, BSM divides the input tokens into two disjoint sets A and B, and constructs a bipartite graph. Then for each node i \u2208 A, it chooses an edge with highest similarity $\\arg \\max_{j \\in \\{S\\}\\_{ij}}$, and choose the r most similar edges afterwards to obtain the sparse adjacency matrix $E' \\in \\{0,1\\}^{\\|A\\|\\times\\|B\\|}$ where $\\sum_{ij} l'{\\scriptsize ij} = r$. The merging is performed by combining the connected tokens in E', where the connected components can be easily found since each token in A has at most one edge. ToMe [2] also proposes tracking the size of the combined tokens and accounts for it in self-attention. Specifically, given a vector $m \\in \\mathbb{R}^{N-r}$ representing the size of combined tokens, the proportional attention is used in the QKV self-attention layers by:\n$A = \\sigma(\\frac{QK^T}{\\sqrt{d}} + log\\,m)$,\n(3)\nwhere o denotes the softmax function.\nLimitations Although the success of merging depends mostly on grouping operation (Eq. 1), the grouping depends entirely on the similarity of the intermediate ViT feature (X or K) in the prior works. This is mainly because the grouping comprises discrete operators, such as clustering and matching, that prevent the gradient flow through grouping (Eq. 1). Thus, the only viable option to improve the merging is by updating the intermediate feature X by back-propagating through the merging operator (Eq. 2). However, it leads to extensive end-to-end training of the entire network, preventing off-the-shelf usage and resulting in suboptimal performance due to the conflict between the token feature required for optimal merging and task performance.\nWe provide more discussion on related work in the supplementary materials (A.3)."}, {"title": "3 Method", "content": "Our objective is to improve token merging by learning the decoupled embedding specifically tailored for merging. To this end, we base our method on the standard token merging framework introduced in the previous section (Eqs. 1, 2). Instead of directly leveraging the ViT features X for grouping, we propose to learn additional per-token embedding modules $Z = f(X; \\phi)$, which are decoupled from the forward pass of the ViT and used only to compute the similarity S in Eq. 1 by $s_{ij} = COS(Z_i, Z_j)$ (Sec. 3.1). Since the grouping operator is entirely dependent on similarity, we can directly modulate the grouping (or merging) policy by learning Z. Furthermore, since the embedding is decoupled from the ViT forward pass, enhancements in merging can be achieved modularly without altering the ViT parameters but only learning the embedding Z.\nTo enable our model to learn such embeddings through merging, we propose a continuous relaxation of the grouping and merging operators in Eq. 1 and Eq. 2, respectively. Specifically, our relaxed"}, {"title": "3.1 Decoupled Embedding Module", "content": "We first describe the choice of the embedding module decoupled from the forward pass of ViTs. To facilitate token merging at each Transformer block, we introduce per-token projection layers into each block $l \\in \\{1, ..., L\\}$:\nDecoupled embedding $Z : z_l = f(x_l; \\phi_l)$,\n(4)\nToken similarity $S : S_{ij} = cos(z_i, z_j)$,\n(5)\nwhere $X \\in \\mathbb{R}^{N\\times d}$ is the input to the self-attention and $Z \\in \\mathbb{R}^{N\\times d'}$ denotes the output decoupled embedding with $d' \\ll d$. The output embeddings will be used solely to shape the merging policy (i.e., deciding which tokens to merge) in the grouping operator based on the similarity S.\nMinimizing additional run-time and computational overheads is essential to the embedding module design. In our approach, we employ a token merging between the self-attention and feed-forward layer following [18, 2]. It allows parallelizing the computation of the attention and decoupled embedding, avoiding the potential overhead that comes from serialization. Moreover, we discover that even a shallow module, consisting solely of an affine transformation, can achieve improvement with minimal computational expense (Sec. 4.4). This further minimizes the number of additional parameters to less than 1% and enables the training of the module with a small amount of data."}, {"title": "3.2 Soft Grouping", "content": "Given the similarity matrix S obtained from the decoupled token embeddings Z, soft grouping aims to approximate the grouping operation through a continuous relaxation that enables differentiation. However, building a general continuous grouping operator of Eq. 1 is challenging since the output reachability matrix is inherently discrete.\nInstead, we employ BSM [2] as our target grouping operator, which offers the benefit of bypassing the reachability matrix and allows for merging to be defined directly on the adjacency matrix $E' \\in \\{0,1\\}^{\\|A\\|\\times\\|B\\|}$. To be a valid approximation of the grouping performed by BSM, the soft grouping operator should produce a continuous adjacency matrix $\\tilde{E} \\in [0, 1]^{\\|A\\|\\times\\|B\\|}$ that satisfies two key conditions. Firstly, it should simulate r distinct edges with high values, thereby implementing the valid merging policy, i.e., combining the r most similar token pairs. Secondly, each node in A should be associated with at most one edge (i.e., $\\sum_{j \\in w}\\tilde{e}{\\scriptsize ij} \\leq 1$) to simplify the identification of connected components in E, thus avoiding the complexity of computing a reachability matrix.\nTo achieve this, we propose a soft grouping that revises the differentiable top-k operator from [36]. Starting with $S^1 = S$, we repeat the subsequent steps for each t = 1, 2, ..., r:\n$A^t = \\sigma(S^t / \\tau)$,\n(6)\n$S{\\scriptsize ij}^{\\scriptsize t+1} = s{\\scriptsize ij}^t + log(1 - \\sum{\\scriptsize j \\in B} a{\\scriptsize ij}^t)$,\n(7)\nwhere o denotes the global softmax function and $\\tau > 0$ represents a temperature scale that regulates the relaxation. In each step t, this process computes the $A^t \\in [0, 1]^{\\|A\\|\\times\\|B\\|}$ with $\\sum{\\scriptsize i \\in A, j \\in B} a{\\scriptsize ij}^t = 1$, representing the soft-argmax. Subsequently, the similarity $S^t$ is updated to suppress the entire outbounding edges from the softly selected nodes in A by Eq. 6. Afterward, the soft adjacency matrix $\\tilde{E}$ is defined as follows:\n$\\tilde{e}{\\scriptsize ij} = \\frac{a{\\scriptsize ij}^{\\ast}}{\\max(1, sg(\\sum a{\\scriptsize ij}^{\\ast}))}$, where $A^{\\ast} = \\sum{\\scriptsize t=1}^r A{\\scriptsize t}$,\n(8)\nwith $\\sum{\\scriptsize i}j \\tilde{e}{\\scriptsize ij} \\leq r$, representing the total number of selected edges. The clipping function, composed of a max operator (max) and stop-gradient (sg), is introduced to ensure that the resulting $\\tilde{E}$ is a valid continuous adjacency matrix."}, {"title": "3.3 Soft Merging", "content": "While the soft grouping and the resulting soft adjacency matrix effectively approximate the grouping process, it is crucial to design the merging operator to incorporate such soft decisions. In our approach, for the given soft adjacency matrix where $\\tilde{e}{\\scriptsize ij}$ corresponds to tokens i \u2208 A and j\u2208 B, our soft merging is designed such that the ith token merges into the jth token in proportion to the value of $\\tilde{e}{\\scriptsize ij}$.\nOur soft merging operator applies the asynchronous updates on tokens in two sets, A and B. For each token j\u2208 B, the operator update their feature $x_j$ and the effective size $m_j$ by aggregating tokens in A based on the soft adjacency matrix E from Section 3.2 by:\n$\\tilde{x}{\\scriptsize j} = \\frac{m{\\scriptsize j}x{\\scriptsize j} + \\sum{\\scriptsize i \\in A} \\tilde{e}{\\scriptsize ij}m{\\scriptsize i}x{\\scriptsize i}}{m{\\scriptsize j} + \\sum{\\scriptsize i \\in A} \\tilde{e}{\\scriptsize ij}m{\\scriptsize i}}, \\ \\ \\ \\ m{\\scriptsize j} = m{\\scriptsize j} + \\sum{\\scriptsize i \\in A} \\tilde{e}{\\scriptsize ij}m{\\scriptsize i}$.\n(9)\nOne the other hand, for each token i \u2208 A, the operator update only its effective size $m_i$ while maintaining the feature:\n$m{\\scriptsize i} = m{\\scriptsize i} (1 - \\sum{\\scriptsize j \\in B}\\tilde{e}{\\scriptsize ij})$.\n(10)\nNote that with the binary adjacency matrix E', the effective size of the tokens in A reduces to zero by Eq. 10 if they have outbounding edges. Such tokens will be excluded from the subsequent merging process by Eq. 9. This process is simulated continuously during training with our soft adjacency matrix (i.e., each token will be continuously absorbed into others), while it is used to actually reduce the tokens at inference using a discretized adjacency matrix.\nInterestingly, we observe that the decoupled embedding, trained with soft merging at a high reduction rate r, generalizes well to lower rates r' < r. This is presumably because the decoupled embedding is learned to sort r most similar token pairs by the relaxed top-k operator (Eqs. 6, 7), thereby including the sorting for smaller reduction rates r' < r."}, {"title": "3.4 Training and Inference", "content": "Training Thanks to decoupled embedding modules, training can be conducted in two distinct ways: modular and end-to-end training. In modular training, we train only the embedding modules while keeping the ViT parameters frozen. This allows our method to fully leverage off-the-shelf models while effectively adapting only the merging policy to each task. In end-to-end training, we jointly train all ViT parameters along with our embedding modules. Since our continuous merging operators do not reduce the number of tokens during training, we alternate updates between the embedding layers and ViT parameters to save computation. Specifically, when updating the ViT parameters, we fix the embedding layers and use the discretized grouping and merging operators, which allows token reduction in the ViT forward pass, greatly enhancing the efficiency. Conversely, when updating the embedding modules, we apply the soft grouping and merging operators while fixing the ViT parameters. We alternate this procedure with much more frequency on ViT updates, since the embedding layers have considerably smaller parameters (\u22481%) and hence quickly converge. For both modular and end-to-end training, we simply train our method to minimize the task loss.\nInference For inference, we discretize the continuous operators in the grouping and merging processes, and perform the hard token merging utilizing the learned decoupled embeddings. As explained in Sec. 3.2 and Sec. 3.3, our soft grouping and merging modules are asymptotically equivalent to BSM of ToMe. Consequently, we employ BSM to speed up the inference."}, {"title": "4 Experiments", "content": "We apply our method, DTEM, for token merging in image classification, captioning, and segmentation. In Sec. 4.1, we first evaluate our method in the ImageNet-1k [8] classification with two setups:"}, {"title": "5 Conclusion", "content": "We propose Decoupled Token Embedding for Merging (DTEM) that improves token merging via decoupled token embedding derived directly from the token merging process. Our method introduces the decoupled embedding, learned through our continuously relaxed token merging, to exploit dedicated features for token merging. The decoupled embedding enhances token merging by resolving the dependency of token merging on intermediate features and enables modular training, effectively utilizing the frozen pre-trained models. We experiment with DTEM on classification, captioning, and segmentation using various pre-trained ViT models. The experimental results demonstrate that our method consistently improves token merging, highlighting the importance of features tailored specifically for token merging."}]}