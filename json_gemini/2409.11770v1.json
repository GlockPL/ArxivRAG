{"title": "Knowledge Adaptation Network for Few-Shot Class-Incremental Learning", "authors": ["Ye Wang", "Yaxiong Wang", "Guoshuai Zhao", "Xueming Qian"], "abstract": "Few-shot class-incremental learning (FSCIL) aims to incrementally recognize new classes using a few samples while maintaining the performance on previously learned classes. One of the effective methods to solve this challenge is to construct prototypical evolution classifiers. Despite the advancement achieved by most existing methods, the classifier weights are simply initialized using mean features. Because representations for new classes are weak and biased, we argue such a strategy is suboptimal. In this paper, we tackle this issue from two aspects. Firstly, thanks to the development of foundation models, we employ a foundation model, the CLIP, as the network pedestal to provide a general representation for each class. Secondly, to generate a more reliable and comprehensive instance representation, we propose a Knowledge Adapter (KA) module that summarizes the data-specific knowledge from training data and fuses it into the general representation. Additionally, to tune the knowledge learned from the base classes to the upcoming classes, we propose a mechanism of Incremental Pseudo Episode Learning (IPEL) by simulating the actual FSCIL. Taken together, our proposed method, dubbed as Knowledge Adaptation Network (KANet), achieves competitive performance on a wide range of datasets, including CIFAR100, CUB200, and ImageNet-R.", "sections": [{"title": "1. Introduction", "content": "Human beings are naturally endowed with the ability to continuously learn new knowledge without forgetting old knowledge. However, it's a nontrivial task for most existing deep models. To empower deep models with incremental learning ability, many researchers engage in the research known as Class-Incremental Learning (CIL) and propose many elegant and effective methods. As one of the critical components for achieving such success, sufficient training samples are accessible for most existing CIL methods to learn new classes. In some scenarios, e.g., foreign object recognition on railway tracks, the number of available training data is limited, making these methods often suffer from the overfitting problem. Regarding this, the task of few-shot class-incremental learning (FSCIL) is designed to incrementally learn new classes using a few samples while not forgetting previously learned classes.\nDue to the practical and challenging nature of FSCIL, the research interest of many scholars is ignited for this task. The mainstream FSCIL works solve this task by model decoupling strategy, which means the encoder is frozen in incremental learning sessions and only the classifier weights or features are updated with various modules. Despite advancements achieved by these methods, most are built on the model trained from scratch. Under the context of FSCIL, a fatal shortcoming of such a learning framework is that new data representations are essentially weak, resulting in weak performance in incremental sessions as evidenced in these methods. For this issue, the remarkable success in representation achieved by CLIP leads us to believe that CLIP is a promising FSCIL solver. Nevertheless, due to the intrinsic challenges of FSCIL and lack of task-specific knowledge, adapting the CLIP to this task is nontrivial. Concretely, to adapt the CLIP to FSCIL well, we must address a major challenge, i.e. how to efficiently tune the CLIP to match the task-specific context of FSCIL using limited samples.\nTo tackle this challenge, we propose a Knowledge Adapter (KA) module (see Section 4.1). Concretely, the KA learns the task-specific knowledge of FSCIL with adaptation parameters and utilizes a query-based knowledge fusion (QKF) mechanism to integrate the task-specific knowledge into the CLIP using adaptation parameters. More precisely, our proposed method defines the knowledge as refining the representation to better fit incremental sessions. However, the remaining problem is how to refine the representation. In fact, when applying general-purpose LLMs to specific scenarios, such as the medical knowledge Q&A (Figure 1(a)), the lack of expertise knowledge often leads to suboptimal answers. To address this issue, one commonly used approach is the Retrieval Augmented Gen-"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Few-shot learning", "content": "Few-shot learning (FSL) defines a task that aims to achieve fast adaptation to new classes using limited samples. To address this issue, recent works can be roughly divided into three groups, the metric-based, optimization-based, and hallucination-based methods. The metric-based methods focus on modeling discriminative relations between classifier weights and test features. For example, MAI et al. propose a method named Attentive Matching Network (AMN) that utilizes feature-level attention to learn discriminative inter-class relations. Unlike previous FSL methods, the optimization-based methods focus on learning a satisfactory initialization for new class learning. For example, the classical optimization-based method MAML designs a two-steps optimization strategy which utilizes support samples in the inner loop and query samples in the outer loop to optimize the model. In such a way, MAML can help the model learn to use a few support samples to achieve fast adaptation for new classes in the inference stage. As the most intuitive and straightforward solution, the hallucination-based methods generates fake samples or features to provide more training samples for new class learning. Despite differences existed in different methods, the common learning paradigm in FSL is to organize the training data in the form of meta task which is similar to the setting of inference task. As evidenced in existing FSL methods, such a learning paradigm can improve the model's generalization ability effectively. In this paper, our proposed Incremental Pseudo Episode Learning (IPEL) is motivated by this."}, {"title": "2.2. Class-incremental learning", "content": "Class-incremental learning (CIL) aims to achieve incremental learning of new classes while retaining previously learned knowledge. The main challenge in CIL is the notorious catastrophic forgetting problem. To address this issue, recent works can be divided into four groups, the rehearsal-based, regularization-based, isolation-based, and prompt-based methods. The regularization-based methods utilizes the knowledge distillation techniques to prevent the learned model from being over-optimized by new data. For example, PODNet proposes to distill features of each layer to prevent the catastrophic forgetting problem. Despite the effectiveness of regularization-based methods in maintaining the old knowledge, the model's plasticity is also constrained. Unlike these methods, the rehearsal-based methods store and replay old exemplars to make the model retrospect old knowledge when learning new classes. The isolation-based methods split the model into two parts, where one part is frozen to keep old knowledge and another part is trained to learn new knowledge. With the development of foundation models, the prompt-based methods adopt the foundation model as network pedestal to provide a satisfactory initialization for new class learning and utilizes prompt to learn new knowledge. The effectiveness of most CIL methods on new classes relies on tremendous data, but our proposed method only needs a few samples."}, {"title": "2.3. Few-shot class-incremental learning", "content": "FSCIL inherits the characteristics of CIL and FSL. As a research hotspot, Cheraghian propose to distill the semantic information to mitigate the notorious catastrophic forgetting problem in FSCIL. Kang et al. propose a method dubbed SoftNet that splits the main model into two sub-networks based on their importance to old classes and only train the less important sub-network to learn new classes. Zhanget al. propose a method that decouples the feature learning and classifier learning, where an adaption module is further devised to update the classifier based on the context between classifier weights. Similarly, Zhuet al. propose a method that utilizes the relation between old classifier weights and new classifier weights to update the global classifier. Wang et al. argue that only updating classifier weights is insufficient and propose a method that updates both classifier weights and corresponding features. Zhang et al. devise a meta-learning scheme that mimics the multi-step incremental learning setting to construct pseudo incremental tasks and demonstrates such a scheme helps improve the model's plasticity. Hersche et al. design an algorithm that stores old features and replays them to finetune a projection layer to output representative features. Most existing methods are built on the model trained from scratch. However, the efficacy of popular CLIP in solving FSCIL is rarely explored. In this paper, we focus on adapting the CLIP to FSCIL."}, {"title": "2.4. Retrieval Augmented Generation", "content": "In recent years, Large Language Models (LLMs) have shown stunning general content generation capabilities and have been applied to various downstream scenarios, such as chatbots. However, due to the lack of specialized domain knowledge, using naive LLMs can not effectively solve the relevant issues that exist in downstream tasks. To address this issue, various Retrieval Augmented Generation (RAG) methods are proposed to activate the power of Large Language Models (LLMs)  to solve specialized tasks well. The core idea of RAG is to retrieve necessary knowledge from a specialized knowledge repository to enhance targets. For example, REALM and REPLUG set the retriever to be trainable to effectively retrieve relevant knowledge. Asai et al. argue that knowledge retrieved by previous RAG methods may not be helpful for response generation and propose a method that utilizes reflection tokens to filter irrelevant knowledge. The designation of our Knowledge Adapter (KA) is inspired by these works."}, {"title": "3. Preliminaries", "content": "Formally, let {$D^0,C^0$}\u2192{$D^1,C^1$}\u2192...\u2192{$D^i,C^i$}(i >\n1) denote the data stream, where $D^i$ and $C^i$ denote the image data and label space of the i-th session. Each $D^i$ consists of a training set $D^i_{train}$ and a test set $D^i_{test}$, where $D^i_{train}$ and $D^i_{test}$ satisfy $D^i_{train}$ \u2229 $D^i_{test}$ = \u2205. In different sessions, $C^i$ \u2229 $C^j$ = \u2205(i \u2260 j). When the learning stage comes to the i-th session, only $D^i_{train}$ is available, but we need to evaluate our model using $D^0_{test}$ \u222a...\u222a $D^i_{test}$ (i > 0). The specialty lies in FSCIL is that $D^0_{train}$ provides sufficient training samples, while $D^i_{train}(j > 0)$ only includes a few training samples. For example, each class contained in $D^0_{train}$ of the benchmark dataset CIFAR100 has 500 training sample, while that of $D^i_{train}(j > 0)$ has only 5 training samples. The imbalanced data distribution and the scarcity of training samples make the FSCIL challenging.\nWhile being amazed by the excellent transfer ability of the CLIP, more and more researchers are devoting themselves to exploring the adaptation of the CLIP to downstream tasks, such as class-incremental learning , few-shot learning. However, though the CLIP can provide satisfactory initial representation for new classes which is extremely important in FSCIL, we surprisingly find such explorations for FSCIL are rare. In this paper, we fill the gap."}, {"title": "4. Methodology", "content": "To solve the challenging FSCIL, our proposed method employs the image branch\u00b9 of CLIP as the network pedestal and fuses the task-specific knowledge into the CLIP to make the CLIP output more fitting features. Figure 2 depicts the proposed framework, we deploy a Knowledge Adapter (KA, Section 4.1) module to learn the task-specific knowledge, where the KA consists of a knowledge vector library and a query-based knowledge fusion mechanism. In KA, the knowledge vector library summarizes the data-specific knowledge from available data. Next, the query-based knowledge fusion integrates the resulting knowledge into the middle feature to achieve the refinement of the encoder's output. Furthermore, since the number of training samples is limited and the old data is not available in incremental sessions, the KA's parameters are optimized using the Incremental Pseudo Episode Learning (IPEL, Section 4.2) to foster the transfer of learned knowledge to the incremental sessions. During the incremental stage, the following steps are taken: (1) summarize knowledge from the training data into the knowledge vector library, (2) use the query-based"}, {"title": "4.1. Knowledge Adapter", "content": "Let $f = f_e \u2295 f_m \u2295 f_p \u2295 \u00a2$ denote the image branch of CLIP, where $f_e, f_m,$ and $f_p$ respectively represent early-stage, middle-stage, and post-stage encoding layers 2, \u2295 indicates the classifier parameterized by $\u03b8_c$.\nKnowledge vector library is used to summarize the data-specific knowledge from the training data, which is a preliminary step for the following query-based knowledge fusion. Concretely, we first input a training image $x \u2208 R^{C\u00d7H\u00d7W}$ to the $f_e$ and get the embedding feature $x_e = f_e(x) \u2208 R^{(L+1)\u00d7D}$ constituted by a series of tokens with the length L + 1, where the first token $x \u2208 R^{1\u00d7D}$ is often called the [class] token, the rests are called the [patch] tokens. Then, because the [class] token captures the instance's global information, a straightforward"}, {"title": "4.2. Incremental Pseudo Episode learning", "content": "Unlike typical class incremental learning giving ample data in incremental stages, FSCIL struggles to support adjustments to previously learned knowledge with scarce samples after running into the incremental stage. To surmount this challenge, incremental pseudo episode learning (IPEL) simulates incremental learning using the base session's data, aiming to smooth the acquired knowledge for the incremental stage and prime the actual incremental learning. In IPEL, there are three main steps: random episode task construction, pseudo adaptation learning, and pseudo balance learning.\nRandom episode task construction builds a series of pseudo learning tasks using the data in the base session for knowledge transfer. Similar to the actual incremental learning, pseudo tasks consist of new classes' data, including a support set and query set, as well as the test set of old classes. These data components are constructed as follows: (i) Pseudo new session $D_{pn}$. Formally, we sample two disjoint datasets from $D_{train}^0$ to construct the support set $S'$ and query set Q with the well-known N-way-K-shot setting, which means N classes are randomly sampled from $C^0$ as the pseudo new classes and K samples for each class are randomly selected from $D_{train}^0$ to construct the support set S. In IPEL, S serves as the pseudo training set while Q serves as the pseudo test set. (ii) Pseudo test set of old classes $D_{test}^p$. To balance plasticity and stability, we consider the remaining classes in $C^0$ as the pseudo old classes. Then, among pseudo old classes, we randomly select several samples from $D_{train}^0$ as the test set $D_{test}^p$ of pseudo old classes. (iii) Pseudo old knowledge $M^{p0}$. Given the known pseudo old classes, we select their corresponding knowledge from M to construct $M^{p0}$.\nOverall, for each episode, we use the combination {$D_{pn}$, $D_{test}^p$, $M^{p0}$} to form a pseudo learning task.\nPseudo adaptation learning acts on the constructed pseudo tasks to propagate and smooth the historical knowledge to match the context of few-shot class-incremental learning, enabling the subsequent few-shot incremental tasks to be warmed up and executed with greater ease. Concretely, we first summarise the pseudo new knowledge $M_{pn}$ from S with the step described in Section 4.1. Then, we input the S and Q to the encoder and utilize the query-based knowledge fusion to get the refined features $f(x^s, M_{pn})$ and $f(x^q, M_{pn})$ of S and Q, where $x^s \u2208 S$ and $x^q \u2208 Q$. Next, we compute the prototypes of pseudo new classes by averaging $f(x, M_{pn})$ that belong to the same class and use the computed prototypes to initialize pseudo new classifier weights $\u03b8_{pn} \u2208 R^{N\u00d7D}$. After that, we employ $\u03b8_{pn}$ to make predictions for $f(x^q, M_{pn})$ as follows:\n$P_q = softmax(\u03b1 < f(x^q, M_{pn}), \u03b8_{pn} >),$"}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Datasets", "content": "CIFAR100. The CIFAR100 dataset consists of 100 classes, where each dataset consists of 500 training samples and 100 test samples. We follow to split into 1 base session and 8 incremental sessions, where the base session includes 60 classes and each incremental session has 5 classes. Further, except for the base session, we only sample 5 training samples for each class in incremental sessions.\nCaltech-UCSD Birds-200-2011. The CUB200 dataset consists of 11,788 images from 200 classes. We follow to construct 11 learning sessions, i.e. 1 base session and 10 incremental sessions, where the base session includes 100 classes and each incremental session consists of 10 classes. Similarly, except for the base session, each class in incremental sessions only be provided with 5 training samples.\nImageNet-R. Previous FSCIL methods often adopt miniImageNet, the subset of ImageNet , as the third benchmark dataset. However, this dataset has a large overlap with the large-scale data used in the CLIP. To make a convincing evaluation, we follow the practice in class-incremental learning and adopt the ImageNet-R as the"}, {"title": "5.2. Implementation Details", "content": "Architecture and Training. We adopt PyTorch to implement our proposed method and the image branch of CLIP-ViT-B/32 as our backbone. Our knowledge adapter is implemented using a Transformer block and plugged into the 10th encoder layer, where the knowledge stored in the knowledge vector library is summarized from the 4th encoder layer. We set the maximum epoch to 50 and adopt Adam as the optimizer, where the learning rate starts from 0.03 and decays with cosine annealing. Following , we resize all images to 224\u00d7224. Random resized crop, random horizontal flip, and color jitter are employed to preprocess the data. The scaling factor \u03b1, the $\u03bb_{adapt}$ and $\u03bb_{balance}$ are set to 16.0, 1.5, and 2.0, respectively. At the incremental pseudo episode learning stage, we randomly sample 200 pseudo incremental learning tasks for each epoch, where each pseudo incremental learning task is constructed with the 20-way-10-shot sampling setting, which means we randomly sample 20 classes from the label space of base session as pseudo new classes and 10 images for each sampled pseudo new class to construct the support set S. As for the query set Q, 15 images for each pseudo new class are randomly sampled. To construct the pseudo test set of old classes $D^p_{test}$, we directly sample 128 images from the training data of pseudo old classes for computation efficiency. Though we mainly focus on how to adapt the CLIP to the context of FSCIL, we also adapt our proposed method to ResNet-18 for a fair comparison, where we use the data-specific knowledge provided by CLIP and plug the knowledge adapter in the last layer. More concretely, the data-specific knowledge is summarized from the 4th encoder layer of the CLIP, then a projection layer with a hidden dim of 2048 to map the feature dimension (768) of CLIP to that (512) of ResNet-18.\nEvaluation Metrics. We use the average accuracy $Avg.= 1/n+1 \u2211^n_{i=0} Ai$ to measure overall performance across sessions, and the performance drop rate $PD= A_0 \u2212 A_n$ to quantify forgetting as , where $A_0$ and $A_n$ refers to the accuracy of the first session and the last session."}, {"title": "5.3. Quantitative Comparisons", "content": "To validate the effectiveness of our proposed method, we conduct comprehensive comparisons with previous methods, including, previous class-incremental learning methods (iCaRL , EEIL , and NCM ) and few-shot class-incremental learning (FSCIL) methods (TOPIC , SPPR , F2M , CEC , CLOM , C-FSCIL , MetaFSCIL , FACT , ALICE ,"}, {"title": "5.5.5 Performance under different backbones", "content": "To investigate the effectiveness of our proposed method under different backbones, we conduct several comparative experiments using various backbones. The results are shown in Table 7, where the baseline refers to directly using the pretrained model to perform FSCIL. The significant improvements across different backbones demonstrate that our proposed method is capable of effectively adapting the ViT-based pretrained model to FSCIL."}, {"title": "5.6. More validations for our proposed method", "content": "To further explore the effectiveness of our method, we construct more few-shot class-incremental learning datasets and evaluate our model, including miniImageNet which is the subset of ImageNet , the fine-grained datasets Oxford Flower102 , cars196 , and food101 . We use the setting shown in Table 8 to split each dataset and report the experimental results in Figure 6. We can see that despite varying degrees of data leakage across datasets, our proposed method consistently outperforms CLIP."}, {"title": "6. Conclusion", "content": "In this paper, we explore the adaptation of CLIP and its efficacy in few-shot class-incremental learning. Regarding the two challenges that exist in adapting the CLIP to match the context of FSCIL, we propose the knowledge"}]}