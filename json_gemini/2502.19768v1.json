{"title": "Obtaining Example-Based Explanations from Deep Neural Networks", "authors": ["Genghua Dong", "Henrik Bostr\u00f6m", "Michalis Vazirgiannis", "Roman Bresson"], "abstract": "Most techniques for explainable machine learning focus on feature attribution, i.e., values are assigned to the features such that their sum equals the prediction. Example attribution is another form of explanation that assigns weights to the training examples, such that their scalar product with the labels equals the prediction. The latter may provide valuable complementary information to feature attribution, in particular in cases where the features are not easily interpretable. Current example-based explanation techniques have targeted a few model types only, such as k-nearest neighbors and random forests. In this work, a technique for obtaining example-based explanations from deep neural networks (EBE-DNN) is proposed. The basic idea is to use the deep neural network to obtain an embedding, which is employed by a k-nearest neighbor classifier to form a prediction; the example attribution can hence straightforwardly be derived from the latter. Results from an empirical investigation show that EBE-DNN can provide highly concentrated example attributions, i.e., the predictions can be explained with few training examples, without reducing accuracy compared to the original deep neural network. Another important finding from the empirical investigation is that the choice of layer to use for the embeddings may have a large impact on the resulting accuracy.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks (DNN) play an indispensable role in modern society, including in areas such as autonomous driving, AI-assisted medical diagnosis, and financial fraud prevention. However, due to their 'black box' nature, there has been growing concern about the inner workings of these networks [4]. Explainable artificial intelligence (XAI) aims to produce models for which the predictions"}, {"title": "2 EBE-DNN", "content": "The proposed approach, EBE-DNN, which is outlined in algorithm 1, operates by leveraging embeddings extracted from a specific layer of a deep neural network (DNN) to generate example attributions and predict the label of a test example. Given a set of training examples X with their corresponding labels y, and a deep neural network D, embeddings are generated with respect to a specified layer l, upper bounded by DL (the number of layers in D), resulting in a transformed set of examples X'. The transformation step aims to convert the high-dimensional input examples to a more useful feature space, reflecting both low-level and high-level properties. Similarly, the test example Xn+1 is converted into its embedding Xn+1 using the same transformation function. The k nearest neighbors of the test instance in the transformed space are retrieved, using some distance metric (\u03b4). From these examples, which in the algorithm are denoted as X'(1), ..., X'(k), where each (i) corresponds to the index of the example with rank i, the resulting example attribution for the test instance are obtained, i.e., N = (X(1), Y(1)), ..., (X(k), Y(k)). Finally, the predicted label for the test instance is given by the mode of the labels from the retrieved examples. The algorithm implicitly assumes uniform weighting of the k returned examples, i.e., each weight is 1/k, but alternative weighting schemes are possible."}, {"title": "3 Empirical Investigation", "content": "In this section, we investigate the example attributions generated by EBE-DNN and evaluate its predictive performance across three widely studied image classification tasks. Our primary objective is to demonstrate that EBE-DNN effectively provides example attributions for test images without compromising the predictive accuracy of the employed DNN."}, {"title": "3.1 Experimental Setup", "content": "We evaluate EBE-DNN using three widely adopted datasets: MNIST [9], Fashion-MNIST [19], and CIFAR-10 [8]. MNIST is a dataset of grayscale 28 \u00d7 28 images of handwritten digits (0-9), while Fashion-MNIST consists of grayscale 28 \u00d7 28 images of clothing items, such as T-shirts, pants, and shoes. CIFAR-10 is a more diverse dataset containing grayscale 36 \u00d7 36 images of 10 classes, including animals (e.g., cats, dogs, horses) and vehicles (e.g., cars, trucks, airplanes). MNIST and Fashion-MNIST each have 60 000 training images and 10 000 test images, while CIFAR-10 contains 50 000 training images and 10 000 test images.\nWe select ResNet18 [5] as the backbone model for EBE-DNN due to its proven effectiveness in image processing tasks. ResNet18 consists of 4 blocks and 20 convolutional layers, which are sequentially numbered from shallow to deep layers for reference. Among these, layers 8, 13, and 18 perform downsampling, enabling the network to reduce spatial dimensions. After training ResNet18 on each dataset using PyTorch [13] with a batch size of 128, a learning rate of 0.001, and 60 epochs, the model weights are frozen to ensure consistency during embedding extraction.\nTo generate example attributions, embeddings are obtained from each convolutional layer of ResNet18 for both training and test images. These embeddings, representing the semantic features learned by ResNet18, differ across layers due to the hierarchical nature of feature abstraction in networks. To investigate the impact of the choice of layer, we apply EBE-DNN to each layer, i.e., l\u2208 {1,...,20}. The function KNN in the algorithm is using the Scikit-learn implementation [14], with cosine similarity as the distance metric. This metric was chosen as it is particularly well-suited for high-dimensional spaces and is robust to magnitude scaling introduced by operations like convolution, pooling, and batch normalization.\nFor illustration, we first present some example attributions provided by EBE-DNN. To investigate whether the example attribution comes at a cost in terms of predictive performance (accuracy), EBE-DNN is compared to the underlying DNN (ResNet18)."}, {"title": "3.2 Illustration of Example Attributions", "content": "In this section, we present example attributions generated by EBE-DNN for two CIFAR-10 test images, a cat and an automobile, demonstrating how the method identifies training examples that contribute to the model's predictions, here using k = 10 neighbors and embeddings from layer 14 of ResNet18. In addition, we explore how example attributions vary across different layers for the automobile test image, showing how the supporting training examples may change with the choice of convolutional layer.\nExample Attribution for a Cat Image In Fig. 2a, the example attribution for a test image of a cat is shown, with all the training examples that contribute to a prediction being ranked. By inspecting the retrieved examples, some key"}, {"title": "Example Attribution Across Layers", "content": "In Fig. 3, example attributions for the above automobile test image are presented, using embeddings from different convolutional layers of ResNet18.\nIn the top three layers, the retrieved images primarily reflect low-level properties, such as color and texture. For instance, many of the retrieved examples share the blue-and-white color scheme of the test image, which corresponds to the car's blue body and white background. Additionally, texture patterns, such as the smooth surfaces of the car body, are captured prominently. However, the retrieved images exhibit minimal structural consistency, as the embeddings at these layers focus on localized patterns rather than overall object structures. Notably, higher-ranked images (e.g., rank 1 and 2) generally show better alignment with the test image's color distribution and texture patterns compared to lower-ranked images, which display greater variability in these properties.\nAs the embeddings progress through middle layers, such as layers 7, 9, and 14, the retrieved examples begin to incorporate more high-level and discriminative properties. Properties like the car hood's shape, the placement of headlights, and the presence of visible tires become more prominent. These layers achieve a balance by preserving low-level details, such as color and texture, while simultaneously capturing structural features critical for classification. The rank order in this stage reflects the relevance of structural consistency: higher-ranked images exhibit more similar frontal shapes and component placements to the test image, while lower-ranked images still align structurally but may vary slightly in less significant features, such as the angle of the car hood or the positioning of tires.\nIn the deeper layers, such as layers 18, 19, and 20, the example attributions generalize further, capturing more abstract and category-level properties. While higher-ranked images maintain a closer similarity to the test image in terms of critical features like wheels and overall vehicle structure, lower-ranked images deviate more, often representing generalized characteristics of the automobile class. For instance, some lower-ranked images, such as layer 19 rank 8 and layer 20 rank 6, appear visually dissimilar to the test image, yet they retain essential discriminative properties, such as the presence of wheels and a land vehicle's structural form. This suggests that deeper layers prioritize semantic alignment over instance-specific details.\nThrough these observations of example attributions generated by EBE-DNN, we arrive at a conclusion consistent with [20]: deeper layers seem to generally produce more discriminative features. This affinity demonstrates that the EBE-DNN method through the example attributions offer some insights into ResNet18's classification mechanism and that it provides a transparent view of the decision-making process."}, {"title": "3.3 Predictive Performance", "content": "In this section, we compare the predictive performance of EBE-DNN to the original DNN, and investigate in particular the effect of the layer to use for the embeddings and the number of examples used in the explanations."}, {"title": "4 Concluding Remarks", "content": "A novel approach to obtaining example attributions from deep neural networks, called EBE-DNN, has been introduced, which combines KNN with the feature extraction capabilities of DNN to generate example attributions that are both interpretable and aligned with model predictions. By associating test examples with the training data that influence their predictions, EBE-DNN offers a transparent and interpretable framework for emulating the predictions of deep learning models, while maintaining competitive predictive performance.\nOur investigation has highlighted the progression of properties captured across ResNet18 layers. Early layers focus on low-level properties like color and texture, which provide surface-level similarities but lack structural information. Middle layers extract distinctive structural features critical for classification, achieving a balance between preserving low-level details and capturing high-level abstractions. Deeper layers generalize to category-level properties, enabling the model to recognize diverse instances within a class but often at the expense of fine-grained details. These insights are consistent with prior work indicating that higher layers produce more discriminative features, demonstrating the interpretability and effectiveness of EBE-DNN in forming example attributions.\nIn terms of predictive performance, EBE-DNN achieves accuracy comparable to or exceeding ResNet18 across datasets, with middle layers delivering the best results. By leveraging embeddings that capture structural features critical for classification, EBE-DNN balances interpretability and predictive performance. The method performs consistently across different settings of k (number of examples to include in the attributions), with k = 10 providing a reasonable trade-off between capturing sufficient training examples for accurate predictions and maintaining interpretability by avoiding excessively large sets of retrieved examples.\nWhile effective, EBE-DNN presents potential opportunities for further improvement. One area for future work is addressing the dependence on the choice of convolutional layers, which currently acts as a hyperparameter requiring tuning. Developing automated or adaptive strategies for selecting the most effective layers for attributions and predictions could enhance usability and efficiency. Additionally, while KNN provides interpretable example attributions, it can become computationally expensive for high-dimensional embeddings or large-scale datasets. Future research could explore scalable implementations or alternative similarity-based approaches to reduce computational cost. Other directions concern investigating alternative distance metrics and forming predictions by weighted voting, e.g., using the distances. Another direction for future research is to complement the example attributions with feature attributions, to more clearly determine the factors influencing the model's predictions. Another direction is a more formal identification and measurement of the concepts used by the model at each layer to make two pictures closer or farther in the latent space. Finally, further exploration is needed to determine whether the EBE-DNN framework is equally effective in other domains, such as text or time-series data, potentially broadening its applicability across machine learning tasks."}]}