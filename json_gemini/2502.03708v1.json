{"title": "Aggregate and conquer: detecting and steering LLM concepts by combining nonlinear predictors over multiple layers", "authors": ["Daniel Beaglehole", "Adityanarayanan Radhakrishnan", "Enric Boix-Adser\u00e0", "Mikhail Belkin"], "abstract": "A trained Large Language Model (LLM) contains much of human knowledge. Yet, it is difficult\nto gauge the extent or accuracy of that knowledge, as LLMs do not always \"know what they know\"\nand may even be actively misleading. In this work, we give a general method for detecting semantic\nconcepts in the internal activations of LLMs. Furthermore, we show that our methodology can be eas-\nily adapted to steer LLMs toward desirable outputs. Our innovations are the following: (1) we use a\nnonlinear feature learning method to identify important linear directions for predicting concepts from\neach layer; (2) we aggregate features across layers to build powerful concept detectors and steering\nmechanisms. We showcase the power of our approach by attaining state-of-the-art results for detecting\nhallucinations, harmfulness, toxicity, and untruthful content on seven benchmarks. We highlight the\ngenerality of our approach by steering LLMs towards new concepts that, to the best of our knowledge,\nhave not been previously considered in the literature, including: semantic disambiguation, human lan-\nguages, programming languages, hallucinated responses, science subjects, poetic/Shakespearean English,\nand even multiple concepts simultaneously. Moreover, our method can steer concepts with numerical\nattributes such as product reviews. We provide our code (including a simple API for our methods) at\nhttps://github.com/dmbeaglehole/neural_controllers.git.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) show expert level performance on a broad range of scientific, mathematical,\nlanguage, and factual assessment tasks [2, 25]. Yet LLMs know more than they express in their initial\nresponses. Consider the example from Carlini [10], where the authors asked GPT-3.5 for the number of staff\nat the British Broadcasting Company (BBC). When asked directly, the model claimed that it did not have\nthe exact number. However, as the authors showed, this information along with other specific details could\nbe elicited indirectly. Furthermore, methods such as Chain-of-Thought prompting [38] can elicit correct\noutputs from models despite initially incorrect responses, by having the model review and correct its own\noutputs. Uncovering LLM knowledge and mapping it to human interpretable concepts is a fundamental\nopen problem.\nA prominent approach to this challenge has focused on unsupervised methods, such as sparse autoen-\ncoders, to extract interpretable patterns, or features, from internal activations in LLMs [7, 12, 21, 40]. This\nwork has led to the discovery of a number of features corresponding to concepts and entities (a notable\nexample is the Golden Gate Bridge [7]). Moreover, by adjusting the magnitude of internal activations cor-\nresponding to these features, it was possible to steer the LLM to focus on one of these identified concepts.\nFor instance, researchers at Anthropic demonstrated that modifying activations related to the Golden Gate"}, {"title": "2 Detection and steering by aggregating nonlinear predictors", "content": "Below, we describe our framework for detecting concepts and steering LLMs by aggregating multiple concept\nvectors from the layer-wise activations. Our approach combines two key technical innovations: (1) detecting\nconcepts by aggregating layer-wise concept predictors, and (2) learning concept vectors through nonlin-\near feature-learning models (e.g., Recursive Feature Machines (RFMs) [28]). We begin by mathematically\ndescribing the general framework for how we detect concepts from LLM activations.\nIn this work, we consider standard decoder-only transformers, such as Llama and Gemma series models\n[8, 33, 34], which are trained to predict the next token auto-regressively. Inference with these models occurs\nas follows. A prompt or a context window is first converted into a sequence of T input tokens that are\none-hot encoded, i.e. each token is mapped to a vector in $\\mathbb{R}^V$, where V is the vocabulary size. The input\nis formed by stacking these tokens into a single matrix $X \\in \\mathbb{R}^{T\\times V}$. For any input X, LLMs implement\nnext-token prediction functions $f : \\mathbb{R}^{T\\times V} \\rightarrow \\mathbb{R}^V$ of the form:\n$A_1(X) = E(X)$, (Embedding layer)\n$A_\\ell(X) = B_\\ell(A_{\\ell-1}(X))$ for $\\ell \\in \\{2, ..., L\\}$, (Activations of the $\\ell$-th block)\n$f(X) = R(A_L(X))$, (Final output)\nwhere $E : \\mathbb{R}^{T\\times V} \\rightarrow \\mathbb{R}^{T\\times k}$ is called an embedding layer, each of the $A_\\ell(X) \\in \\mathbb{R}^{T\\times k}$ is a matrix of activations,\neach of the functions $B_\\ell : \\mathbb{R}^{T\\times k} \\rightarrow \\mathbb{R}^{T\\times k}$ is a block, and $R : \\mathbb{R}^{T\\times k} \\rightarrow \\mathbb{R}^V$ is a readout layer. The final output\ntoken is obtained by sampling the from the output of the readout layer treated as a probability distribution.\nIn transformer-based LLMs, blocks typically consist of self-attention layers followed by fully-connected layers,\neach with skip connections and a normalization step [8], however our approach relies only on the outputs of\nthe blocks. Given this setup, we describe how we detect concepts and steer LLMs below."}, {"title": "Detection", "content": "To train predictors to detect a specified concept, we first curate a training dataset of prompt-\nlabel pairs $\\{X^{(i)}, y^{(i)}\\}_{i=1}^n$ where $X^{(i)} \\in \\mathbb{R}^{T\\times d}$ and $y^{(i)} \\in \\{0,1\\}$, with 1 indicating that the prompt belongs to\nthe concept and 0 indicating that it does not.\\u00b9 See Appendix A for an example of our approach to generating\nlabeled prompts. We next train layer-wise predictors, $\\{f_\\ell\\}_{\\ell=1}^L$, to predict the concept labels from activations\non these prompts. In particular, letting $a \\in \\mathbb{R}^k$ denote the Tth row of $A_\\ell(X^{(i)})$, we train $f_\\ell : \\mathbb{R}^k \\rightarrow \\mathbb{R}$\nto map $a_\\ell^{(i)}$ to $y^{(i)}$. From each trained predictor, we next extract a set of $m \\le k$ features $\\{c_{\\ell,j}\\}_{j=1}^m$ with\n$\\|c_{\\ell,j}\\|_2 = 1$, which we refer to as concept vectors (when $m = 1$, we indicate the concept vector as $c_\\ell$). For\neach layer, we then project the activations $a_\\ell^{(i)}$ onto each of the $c_{\\ell,j}$ to produce a vector $b_{\\ell}^{(i)} \\in \\mathbb{R}^m$. We then\naggregate these projected activations across layers by concatenating these L projected vectors into a vector\n$b^{(i)} = [b_1^{(i)}, ..., b_L^{(i)}] \\in \\mathbb{R}^{Lm}$. To detect concepts, we finally train a predictor $f : \\mathbb{R}^{Lm} \\rightarrow \\mathbb{R}$ that maps $b^{(i)}$ to\nlabel $y^{(i)}$.\nWe are now faced with a choice of predictor and feature extraction method. In addition to standard\nlinear or logistic regression, we consider Recursive Feature Machines (RFMs) [28] used with kernel machines.\nRFMs are an algorithm for enabling feature learning in any nonlinear predictive model such as a kernel\nmachine (See Appendix B for a review of kernel machines and RFM). Namely, after training a predictor\n$f : \\mathbb{R}^k \\rightarrow \\mathbb{R}$, an RFM computes the Average Gradient Outer Product (AGOP) of the trained predictor to\nextract features in the form of a $k\\times k$ feature matrix, M. We select our m concept vectors by computing\nthe top m eigenvectors of M.\nBelow, we outline the specific form of RFM that we use in this work. Let $X = [x^{(1)},...,x^{(n)}]^T \\in \\mathbb{R}^{n\\times d}$\nand $y = [y^{(1)}, ...,y^{(n)}]^T \\in \\mathbb{R}^n$ denote training data. For any $x, z \\in \\mathbb{R}^d$, the (Mahalanobis) Laplace kernel\nwith bandwidth parameter $L$ and symmetric, positive-semi-definite matrix M takes the form\n$K_M(x,z) = \\exp\\left(-\\frac{1}{L} (x - z)^T M (x - z)\\right)$.\nAbusing notation, we write $K_M(A, B) \\in \\mathbb{R}^{n\\times m}$ for matrices $A \\in \\mathbb{R}^{n\\times d}, B \\in \\mathbb{R}^{m\\times d}$ to indicate the matrix of\npairwise evaluations between all rows of A and B. Letting $M_0 = I$, RFM iterates the following two steps for\nTiterations:\nStep 1: $f_t(z) = K_{M_t}(z, X) a_t$ where $a_t = [K_{M_t} (X, X)]^{-1}y \\in \\mathbb{R}^n$, (Kernel Ridge Regression)\nStep 2: $M_{t+1} = \\frac{1}{n} \\sum_{i=1}^n \\nabla f_t(x^{(i)}) \\nabla f_t(x^{(i)})^T \\in \\mathbb{R}^{d\\times d}$. (AGOP matrix)\nNote the gradient in the AGOP computation is of the predictor with respect to its inputs (and not, e.g., its\nparameters). Hence, the AGOP matrix extracts the directions in input space that most influence predictions\nat iteration t, as changes in the inputs in these relevant directions will have greater influence on the classifier's\nprediction. In our case, the predictor is usually a classifier for a particular concept, hence the top eigenvectors\nof the AGOP are orthogonal directions that are relevant to this concept.\nRFM is useful for detecting concepts that are nonlinear functions of activations and may depend on a\nsubspace in activation space, not just a single direction, as the AGOP matrix provides multiple eigenvectors.\nThis is in contrast to typical approaches for concept extraction, which rely on the use of linear methods such as\nlinear / logistic regression, Principal Component Analysis (PCA), or Difference-in-means (See Appendix B.3\nfor an overview of these methods)."}, {"title": "Steering", "content": "Given a set of concept vectors $\\{c_\\ell\\}_{\\ell=1}^L$ (1 concept vector per layer), we steer LLMs toward a\nconcept by replacing activations $A_\\ell(X)$ during the forward pass with $\\tilde{A}_\\ell(X)$ defined as follows. Letting $A_{\\ell,i}$\ndenote the ith row of $A_\\ell$,\n$\\tilde{A}_{\\ell,i}(X) := A_{\\ell,i}(X) + \\epsilon c_\\ell$ for $\\ell \\in [L], i \\in [T]$,\nwhere $\\epsilon \\in \\mathbb{R}$ is a constant parameter termed the control coefficient. Values of $\\epsilon$ vary per concept and\nper model. To steer for multiple concepts simultaneously, we simply let $c_\\ell$ be a linear combination of the"}, {"title": "3 Detection results", "content": "We show that combining (1) aggregation of layer-wise predictors and (2) RFM probes at each layer and (3)\nusing RFM as an aggregation method gives state-of-the-art results for detecting concepts from LLM activa-\ntions. We outline results below and note that additional experimental details are provided in Appendix D."}, {"title": "Benchmark datasets", "content": "To evaluate concept predictors, we consider seven commonly-used benchmarks in\nwhich the goal is to evaluate LLM prompts and responses for hallucinations, toxicity, harmful content, and\ntruthfulness. To detect hallucinations, we use HaluEval (HE) [17], FAVA [23], and HaluEval-Wild (HE-Wild)\n[41]. To detect harmful content and prompts, we use AgentHarm [1] and ToxicChat [20]. For truthfulness,\nwe use the TruthGen benchmark [14]. Labels for concepts in each benchmark were generated according\nto the following procedures. The TruthGen, ToxicChat, HaluEval, and AgentHarm datasets contain labels\nfor the prompts that we use directly. HE-Wild contains text queries a user might make to an LLM that\nare likely to induce hallucinated replies. For this benchmark, we perform multiclass classification to detect\nwhich of six categories each query belongs to. LAT [42] does not apply to multiclass classification and\nLLM-Check [30] detects hallucinations directly, not queries that may induce hallucinations. Hence, we mark\nthese methods with 'NA' for this task. For FAVA, we follow the binarization used by Sriramanan et al.\n[30] on this dataset, detecting simply the presence of at least one hallucination in the text. For HE, we\nconsider an additional detection task in which we transfer the directions learned for the QA task to the\ngeneral hallucination detection (denoted HE(Gen.) in Figure 1)."}, {"title": "Evaluation metrics", "content": "On these benchmarks, we compare various predictors based on their ability to detect\nwhether or not a given prompt contains the concept of interest. We evaluate the performance of detectors\nbased on their test accuracy as well as their test F1 score, a measure that is used in cases where the data\nis imbalanced across classes (see Appendix B.4). For benchmarks without a specific train/validation/testing\nsplit of the data, we randomly sampled multiple splits and reported the average F1 scores and (standard)\naccuracies of the detector fit to the given train/validation set on the test set on instruction-tuned Llama-"}]}