{"title": "TASK-UNAWARE LIFELONG ROBOT LEARNING WITH RETRIEVAL-BASED WEIGHTED LOCAL ADAPTATION", "authors": ["Pengzhi Yang", "Xinyu Wang", "Ruipeng Zhang", "Cong Wang", "Frans Oliehoek", "Jens Kober"], "abstract": "Real-world environments require robots to continuously acquire new skills while retaining previously learned abilities, all without the need for clearly defined task boundaries. Storing all past data to prevent forgetting is impractical due to storage and privacy concerns. To address this, we propose a method that efficiently re-stores a robot's proficiency in previously learned tasks over its lifespan. Using an Episodic Memory (EM), our approach enables experience replay during training and retrieval during testing for local fine-tuning, allowing rapid adaptation to pre-viously encountered problems without explicit task identifiers. Additionally, we introduce a selective weighting mechanism that emphasizes the most challeng-ing segments of retrieved demonstrations, focusing local adaptation where it is most needed. This framework offers a scalable solution for lifelong learning in dynamic, task-unaware environments, combining retrieval-based adaptation with selective weighting to enhance robot performance in open-ended scenarios.", "sections": [{"title": "INTRODUCTION", "content": "Lifelong learning seeks to endow neural networks with the ability to continually acquire new skills while retaining previously learned knowledge. This balance between stability and plasticity is cru-cial as models face sequences of tasks over time. While significant progress has been made in apply-ing lifelong learning to domains such as computer vision (Huang et al., 2024; Du et al., 2024) and natural language processing (Shi et al., 2024; Razdaibiedina et al., 2023), the challenges are more pronounced in robotics. Robots are expected to adaptively learn and solve unseen tasks throughout their operational lifespan (Thrun & Mitchell, 1995). Their interactions with dynamic environments introduce complexities absent in static data domains; a single misstep in task execution can result in complete failure. Moreover, robotics is constrained by limited data availability due to the expense and complexity of real-world interactions (Zhu et al., 2022; Du et al., 2023). These factors not only intensify the difficulty of continual learning in robotics but also demand more robust lifelong learning capabilities.\nExisting methods for lifelong robot learning typically require robots to learn a sequence of tasks, each distinguished by domain, scenario, scene, or task goals (Liu et al., 2024; Yang et al., 2022; Wan et al., 2024; Parakh et al., 2024). In those settings, robots often depend on specific task identi-fications with clear boundaries usually provided as task IDs or explicit descriptions to specify the task they are working on (Liu et al., 2023). However, in dynamic real-world settings, it is im-practical to predefine tasks or assign specific IDs, as robots are likely to encounter a vast array of unpredictable situations, with tasks that may be subdivided into smaller components of varying granularity. Therefore, approaches that rely on specific task identifications with clear boundaries are unrealistic and unscalable (Koh et al., 2021).\nTo address these challenges, we propose a novel task-unaware lifelong robot learning framework with visuomotor policies, utilizing vision perceptions as well as diverse paraphrased language de-scriptions. This framework enables robots to continually learn and adapt without explicit task IDs. We employ our method in manipulation scenarios based on the LIBERO benchmark (Liu et al., 2024). Our approach leverages pre-trained models to generate consistent embeddings across differ-ent tasks and training phases, thereby mitigating the embedding drift that often occurs in sequential learning scenarios (Liu et al., 2023; Kawaharazuka et al., 2024). We adopt Experience Replay (ER) baseline (de Masson D'Autume et al., 2019) to rehearse samples from previous tasks, helping to maintain learned skills and reduce forgetting.\nDespite these measures, some degree of forgetting remains inevitable due to the multitasking nature of lifelong learning and the robot's limited access to previous demonstrations. Drawing inspiration from human learning processes where individuals revisit tasks they once knew but have forgotten details we introduce an efficient local adaptation mechanism. Humans often perform quick re-views using limited resources and try to retrieve memory to rebuild their knowledge, allowing them to efficiently regain proficiency without relearning all aspects of the task (Sara, 2000). Similarly, our mechanism enables the robot to locally adapt to previously encountered problems rapidly and regain skills through fast fine-tuning, using the same episodic memory employed for experience replay during training.\nGiven the indistinct task boundaries, we leverage retrieval-based mechanisms (Du et al., 2023; van Dijk et al., 2024; de Masson D'Autume et al., 2019) to retrieve data most similar to the current task based on vision and language input similarities. To adapt the model effectively - especially focusing on the most challenging phases where the robot's performance deviates we first perform a few episodes of rollouts to obtain \"feedback\" on the model's performance before local adaptation: these rollouts are then used for automatic selective weighting by comparing them with the retrieved demonstrations without human intervention (Spencer et al., 2022; Mandlekar et al., 2020). The weighted samples facilitate the local adaptation phase, thereby improving performance.\nIn summary, the key contributions of our solution are:\n\u2022 Retrieval-Based Adaptation for Blurred Task Boundaries: During testing, relevant past demonstrations are retrieved from episodic memory to adapt the neural network locally, enabling the robot to quickly regain proficiency on previously encountered tasks without relying on explicit task boundaries.\n\u2022 Selective Weighting Mechanism: A weighting mechanism emphasizes the most challeng-ing segments of the retrieved demonstrations, optimizing real-time adaptation.\n\u2022 Paradigm for Memory-Based Lifelong Robot Learning: We demonstrate that our ap-proach can be applied to different memory-based robotic lifelong learning algorithms dur-ing test time, serving as a paradigm for skill restoration.\nThis framework allows robots to continually learn and adapt in dynamic environments without re-quiring predefined task identifiers or boundaries, making it highly practical and scalable for real-world applications. By combining retrieval-based adaptation with selective weighting, our method offers a robust solution to the challenges of lifelong robot learning in open-ended settings."}, {"title": "RELATED WORK", "content": "A key challenge in lifelong robot learning is catastrophic forgetting, where learning new tasks ad-versely affects performance on previously learned tasks (Parisi et al., 2019). Traditional lifelong learning methods often rely on explicit task identifiers or clear task boundaries to structure the learning process (Wan et al., 2024; Xie & Finn, 2022), such as Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017) and PackNet (Mallya & Lazebnik, 2018). In real-world robotic appli-cations, robots operate in dynamic environments where tasks are not clearly segmented, making explicit task identifiers impractical (Kim et al., 2024).\nRecent efforts in lifelong reinforcement learning (Xie & Finn, 2022) and areas like task and mo-tion planning (Mendez-Mendez et al., 2023), online object model reconstruction (Lu et al., 2022), interactive instruction-following agents (Kim et al., 2024), multi-task learning (Wang et al., 2022), interactive imitation learning (Spahn et al., 2024), and SLAM (Yin et al., 2023; Gao et al., 2022; V\u00f6disch et al., 2022) show progress. Robot manipulation skills also evolve through interactions, aiding adaption when task executions fail (Parakh et al., 2024). Memory-based algorithms and se-lective weighting (Sun et al., 2022; Koh et al., 2021; Shim et al., 2021; Aljundi et al., 2019) enhance learning by prioritizing informative samples. Replay buffer methods (He et al., 2020; Mai et al., 2021; Caccia et al., 2021) have demonstrated success. However, there is still a lack of progress in settings where the model is unaware of task boundaries during both training and inference (Lee et al., 2020; Chen et al., 2020; Ardywibowo et al., 2022).\nA benchmark for lifelong robot learning, particularly focusing on manipulation tasks, has been in-troduced in LIBERO (Liu et al., 2024). Methods such as TAIL (Liu et al., 2023) rely on specific task identifiers, which can be limiting in dynamic environments, while Lotus (Wan et al., 2024) involves a pretraining phase to establish an initial skill set, providing a foundation for further continuous learning."}, {"title": "TASK-UNAWARE CONTINUAL LEARNING", "content": "Despite the success of continual learning with clearly labeled task sequences, there still remains a gap in progress within settings where the model is unaware of task boundaries both in training and inference, an online situation more reflective of real-world scenarios. Many attempts (Lee et al., 2020; Chen et al., 2020; Ardywibowo et al., 2022) focus on learning specialized parameters using expanding network structures. Memory-based algorithms remain effective by prioritizing informa-tive samples (Sun et al., 2022), removing less important training samples (Koh et al., 2021), improv-ing decision boundaries (Shim et al., 2021), and increasing gradient diversity (Aljundi et al., 2019). Methods aiming to exploit replay buffer (He et al., 2020; Mai et al., 2021; Caccia et al., 2021) have also demonstrated notable success."}, {"title": "INFORMATION RETRIEVAL FOR ROBOTICS", "content": "Information retrieval techniques have been used to optimize robotic behaviors by retrieving relevant actions from memory in novel tasks (Du et al., 2023). For example, path following based on image retrieval improves visual navigation (van Dijk et al., 2024), and incremental learning helps humanoid robots adapt to new environments by recalling past behaviors (B\u00e4rmann et al., 2023). Retrieval has also enabled skill transfer from videos (Papagiannis et al., 2024) and affordance transfer for zero-shot manipulation (Kuang et al., 2024), allowing robots to manipulate objects without prior training."}, {"title": "PRELIMINARY", "content": "Our robot utilizes a visuomotor policy learned through behavior cloning to execute manipulation tasks by mapping sensory inputs and task descriptions to motor actions. In a task-unaware lifelong learning setting, we adopt a continual learning framework where task boundaries are blurred by employing multiple paraphrased descriptions to define task goals, rather than relying on explicit task identifiers. This approach enhances the policy's ability to generalize across varied instructions and tasks.\nThe policy is trained by minimizing the discrepancy between the predicted actions and the expert actions derived from demonstrations. Specifically, we optimize the following loss function across a sequence of tasks {Tk} with corresponding demonstrations Dk = {\u03c41,..., \u03c4lTk}. Notably, Dk is not fully accessible for k < K due to the use of experience replay data from Episodic Memory M:\n$\\theta^{*} = \\arg \\min_{\\theta} \\sum_{k=1}^{K} \\frac{1}{l_k} \\sum_{\\{o_{\\leq t}, a_t\\} \\sim D_k, g \\sim G_k} L(\\pi_{\\theta} (o_{\\leq t}, g), a_t)$"}, {"title": "RETRIEVAL-BASED WEIGHTED LOCAL ADAPTATION FOR LIFELONG ROBOT LEARNING", "content": "In this section, we outline our proposed method depicted in Figure 1. To effectively interact with complex physical environments, the network integrates multiple input modalities, including visual inputs from workspace and wrist cameras, proprioceptive inputs of joint and gripper states, and paraphrased task descriptions.\nInstead of training all modules jointly in an end-to-end manner, we employ pretrained visual and lan-guage encoders that leverage prior semantic knowledge. Pretrained encoders enhance performance on downstream manipulation tasks (Liu et al., 2023) and are well-suited to differentiate between var-ious scenarios and tasks without relying on explicit task identifiers or clear task boundaries. Their consistent representations when new tasks continue to come is essential for managing multitask problems and retrieving relevant data to support our proposed local adaptation during test time.\nWhen learning new tasks, the robot preserves previously acquired skills by replaying prior manipu-lation demonstrations stored in an episodic memory M, which contains a small subset of previous task demonstrations (Chaudhry et al., 2019). Trained with the combined data from the latest sce-narios and episodic memory M, the model can acquire new skills while mitigating catastrophic forgetting of old tasks, thereby maintaining a balance between stability and plasticity (Wang et al., 2024). Figure 2 illustrates the network architecture, and implementation details are provided in Section A.2."}, {"title": "DATA RETRIEVAL", "content": "During deployment, we first retrieve the most relevant demonstrations from episodic memory M based on similarity to the current scenario. Due to the blurred task boundaries, some tasks share similar visual observations but differ in their task objectives, while others have similar goals but involve different backgrounds, objects, etc. To account for these variations, we compare both visual inputs from the workspace camera (Du et al., 2023) and task descriptions (de Masson D'Autume et al., 2019) using L2 distances of their embeddings. The retrieval process follows a simple rule:\nDR = \u03b1\u03c5 \u00b7 D\u03c5 + \u03b1\u03b9 \u00b7 Dl,\nwhere DR is the weighted retrieval distance, D\u03c5 represents the distance between the embeddings of the scene observation from the workspace camera, and Dl depicts the distance between the task description embeddings. The parameters \u03b1\u03c5 and \u03b1\u03b9 control the relative importance of visual and language-based distances. Based on the distances DR, the most relevant demonstrations can be retrieved from M."}, {"title": "WEIGHTED LOCAL ADAPTATION", "content": "To make the best use of the limited data, we enhance their utility by assigning weights to critical or vulnerable segments in each retrieved demonstration. Specifically, before testing, the robot performs several rollouts on the encountered task using the existing model trained during the lifelong learning phase. This procedure allows us to evaluate the model's performance and identify any forgetting effects, akin to a preliminary quiz before the final exam (as illustrated in step 2, the reviewing phase in Figure 1).\nWhen failed trajectories are identified, we compare each image in the retrieved demonstrations against all images from the failed trajectories using the L2 distances of their embeddings. This comparison yields a distance vector for each demonstration, where each value represents the min-imal distance between a demonstration frame and all images from the failed rollouts. This metric determines whether a particular frame has occurred during the rollout. Through this process, we identify the Separation Segment frames in the demonstrations where the behavior deviates from what was executed during the failed rollouts (see Figure 3). Since these Separation Segments high-light behaviors that should have occurred but did not, we consider them vulnerable segments that contribute to the failure. We assign higher weights to these frames which will scale the losses during local adaptation. Detailed heuristics and implementation specifics are provided in Appendix A.4."}, {"title": "ADAPTATION WITH FAST FINETUNING", "content": "Finally, we fine-tune the network's parameters to better adapt to the current task using the retrieved demonstrations, focusing more on the difficult steps identified through selective weighting. Notably, the episodic memory M contains the same data used during training for experience replay and during deployment for local adaptation. No additional demonstrations are available to the robot at test time. Despite this limited data, our experiments demonstrate that the model can effectively recover learned skills and improve its performance across various tasks. Overall, the proposed weighted adaptation is formalized as follows:\n$\\theta^{*} = \\arg \\min_{\\theta} \\sum_{n=1}^{\\tilde{N}} \\sum_{t=1}^{\\tilde{l_n}} w_{t,n} L(\\pi_{\\theta} (o_{\\leq t,n}, g_n), a_{t,n})$"}, {"title": "EXPERIMENTS", "content": "We conduct a comprehensive set of experiments to evaluate the effectiveness of our proposed retrieval-based weighted local adaptation method for lifelong robot learning. Specifically, our ex-periments aim to address the following key questions:\n1. Effect of Blurry Task Boundaries: How do blurry task boundaries influence the model's performance and data retrieval during testing?\n2. Advantages of Retrieval-Based Adaptation: Does retrieval-based weighted local adap-tation enhance the robot's performance across diverse tasks?\n3. Impact of Selective Weighting: Is selective weighting based on rollout errors effective in improving task performance?\n4. Generalizability: Can our method be applied to different memory-based lifelong robot learning approaches, serving as a paradigm that enhances the performance during test time by restoring previous knowledge and skills?\n5. Robustness: Due to blurry task boundaries and retrieval imprecision, the retrieved demon-strations may not necessarily belong to the same task. How resilient is our method to inaccuracies in memory retrieval?"}, {"title": "EXPERIMENTAL SETUP", "content": "We evaluate our proposed methods using LIBERO (Liu et al., 2024): libero_spatial, libero_object, libero_goal, and libero_different_scenes. These environments feature a variety of objects and layouts. The first three benchmarks all include 10 distinct tasks, each with up to 50 demonstrations collected in simulation with different initial states of objects and the robot. Specifically, libero_different-scenes is created from LIBERO's provided LIBERO_90, which encompasses 20 tasks from distinct scenes.\nFor each task, we paraphrased the assigned single goal description into diverse descriptions to ob-scure task boundaries. These enriched descriptions were generated by rephrasing the original task descriptions from the benchmark using a large language model provided by Phi-3-mini-4k-instruct Model (mini-4k instruct, 2024), ensuring consistent meanings while varying phraseology and syn-tax. Please see Section A.3 for more details."}, {"title": "BASELINES", "content": "We evaluate our proposed method against the following baseline approaches:\n1. Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017): A regularization-based approach that constrains updates to the network's parameters to prevent catastrophic for-getting of previously learned tasks.\n2. Experience Replay (ER) (Chaudhry et al., 2019): A core component of our training setup, ER utilizes stored episodic memory to replay past demonstrations, helping the model main-tain previously acquired skills and mitigate forgetting. As a baseline, we evaluate the stan-dalone performance of ER without additional retrieval-based adaptation techniques.\n3. Average Gradient Episodic Memory (AGEM) (Hu et al., 2020): Employs a memory buffer to constrain gradients during the training of new tasks, ensuring that updates do not interfere with performance on earlier tasks.\n4. AGEM with Weighted Local Adaptation (AGEM-WLA): An extension of AGEM that incorporates weighted local adaptation during the testing phase, enhancing the model's ability to adapt to specific tasks based on retrieved demonstrations. This allows us to assess the generalizability of our proposed method as a paradigm framework on other memory-based lifelong learning approaches.\n5. PackNet (Mallya & Lazebnik, 2018): An architecture-based lifelong learning algorithm that iteratively prunes the network after training each task, preserving essential nodes while removing less critical connections to accommodate subsequent tasks. However, its pruning and post-training phases rely heavily on clearly defined task boundaries, making PackNet a reference baseline when task boundaries are well-defined."}, {"title": "\u039cETRICS", "content": "Our primary focus is on the success rate of task execution, as it is a crucial metric for manipulation tasks in interactive robotics. Consequently, we adopt the Average Success Rate (ASR) as our primary evaluation metric to address the challenge of catastrophic forgetting within the lifelong learning framework, evaluating success rates on three random seeds across all diverse tasks within the same benchmark."}, {"title": "MODEL, TRAINING, AND ADAPTATION", "content": "As illustrated in Figure. 2, our model utilizes pretrained encoders for visual and language inputs: R3M (Nair et al., 2022) for visual encoding, Sentence Similarity model (SS Model) (SentenceSimi-larity, 2024) for language embeddings, and a trainable MLP-based network to encode proprioceptive inputs. Embeddings from ten consecutive time steps are processed through a transformer-based tem-poral encoder, with the resulting output passed to a GMM-based policy head for action sampling. Specifically, R3M, a ResNet-based model trained on egocentric videos using contrastive learning, captures temporal dynamics and semantic features from scenes, while Sentence Similarity Model"}, {"title": "RESULTS", "content": "To address Question 2, we compared our proposed method, Weighted Local Adaptation (ER-WLA), with all baseline approaches. As shown in Table 1, ER-WLA consistently outperforms baselines of EWC, AGEM, ER, and AGEM-WLA, which do not rely on clear task boundaries. By incorporating local adaptation during test time our method mirrors how humans review and reinforce knowledge when it is partially forgotten the continually learning robot could also regain its proficiency on previous tasks.\nIn contrast, PackNet serves as a reference method, as it requires well-defined task bound-aries. However, as the number of tasks increases, the network's trainable capacity under Pack-Net diminishes, leaving less flexibility for future tasks. This limitation becomes evident in the libero_different_scenes benchmark, which includes 20 tasks. PackNet's success rate drops significantly for later tasks, resulting in poor overall performance and highlighting its constraints on plasticity compared with our proposed ER-WLA approach.\nAdditionally, when we applied WLA to the AGEM baseline (resulting in AGEM-WLA), it also im-proved its performance, demonstrating the effectiveness of our method as a paradigm for memory-based lifelong robot learning methods. These findings also support our conclusions regarding Ques-tion 4."}, {"title": "ABLATION STUDIES", "content": "We performed two ablation studies to validate the effectiveness of our implementation choices and address Questions 1, 3, and 5.\nSelective Weighting. In the first ablation, we evaluated the impact of selective weighting on libero_spatial, libero_object, and libero_goal benchmarks to demonstrate its im-portance for effective local adaptation. We compared two variants of our method: 1) ER-ULA,"}, {"title": "CONCLUSION AND DISCUSSION", "content": "In this paper, we introduced a novel task-unaware lifelong robot learning framework that combines retrieval-based local adaptation with selective weighting during test time. Our approach enables robots to continuously learn and adapt in dynamic environments without explicit task identifiers or predefined boundaries. Leveraging an episodic memory M, our method retrieves relevant past demonstrations based on visual and language similarities, allowing the robot to fine-tune its policy locally. The selective weighting mechanism enhances adaptation by prioritizing the most challeng-ing segments of the retrieved demonstrations. Notably, our framework is not only robust, but is"}, {"title": "APPENDIX", "content": null}, {"title": "NOTATIONS", "content": null}, {"title": "IMPLEMENTATION AND TRAINING DETAILS", "content": null}, {"title": "NETWORK ARCHITECTURE AND MODULARITIES", "content": null}, {"title": "TRAINING HYPERPARAMETERS", "content": "Table 6 provides a summary of the essential hyperparameters used during training and local adap-tation. The model was trained on a combination of A40, A100, and L40S GPUs, while we also leveraged multi-GPU configurations to accelerate the training process. For each task, demonstration data was initially collected and provided by LIBERO benchmark. However, due to version discrep-ancies that introduced visual and physical variations in the simulation, we reran the demonstrations with the latest version to obtain updated observations. It is important to note that occasional roll-out failures occurred because different versions of RoboMimic Simulation (Mandlekar et al., 2021) utilize varying versions of the MuJoCo Engine (Todorov et al., 2012).\nTask performance was evaluated every 10 epochs using 20 parallel processes to maximize efficiency. The best-performing model from these evaluations was retained for subsequent tasks. After training on each task, we reassessed the model's performance across all previously encountered tasks."}, {"title": "BASELINE DETAILS", "content": "We follow the implementation of all baselines and hyperparameters for individual algorithms from (Liu et al., 2024), maintaining the same backbone model and episodic memory structure as in our approach. During the training phase, we also apply the same learning hyperparameters outlined in Table 6."}, {"title": "DETAILS ABOUT TASK-UNAWARE SETTING", "content": "In this paper, we blur task boundaries by using multiple paraphrased descriptions that define the task goals. The following section elaborate more details about our dataset and process of task description paraphrase."}, {"title": "DATASETS STRUCTURE", "content": "Our dataset inherent the dataset from LIBERO (Liu et al., 2024), maintaining all the attributes and data. Additionally, we add demo description to each demonstration to achieve task unawareness and augmented description to augment language description during training (See Figure 5). Unlike the dataset from LIBERO, which groups demonstrations together under one specific task, our dataset wrap all demonstrations with random order to eliminate the task boundary."}, {"title": "DESCRIPTION PARAPHRASE", "content": "We leverage the Phi-3-mini-4k-instruct model (mini-4k instruct, 2024) to paraphrase the task de-scription. The process and prompts that we use are illustrated in Figure 6."}, {"title": "DETAILS ABOUT SELECTIVE WEIGHTING", "content": "In this section, we introduce our Selective Weighting mechanism in detail."}, {"title": "DETAILED HEURISTICS AND IMPLEMENTATIONS", "content": "To assign weights to retrieved demonstrations, we analyze the distance between demonstration and failed rollout trajectories. Typically, the comparison distance increases as the failed rollout diverges from the demonstration.\nDue to the multi-modal nature of robotic actions and visual observation noise, raw distance compar-isons can be erratic. To address this, we smooth the distance curves using a moving window. Despite smoothing, the trend may remain jittery, making it challenging to pinpoint a single separation point where performance deviates. Instead, we identify a range of frames representing the Separation Segment where the distances worsen, indicating vulnerable steps in a manipulation task.\nWe apply two thresholds to detect the segment. Specifically, we locate frames where the distance falls between and of the maximum observed distance. We focus on the last occurrence within this range to account for possible initial divergent paths that later converge. Once identified, we extend the separation segment by 15 frames before and after to mitigate noise effects.\nFor each frame within the separation segment, we add a weight of 0.3 to the initially uniform weight vector. This process is repeated for up to five failed rollouts per retrieved demonstration. After processing all demonstrations, we clip the weights to a maximum of 2 and normalize the weight vector to maintain a consistent loss function scale, ensuring stable gradient updates.\nDuring adaptation, the resulting weights ($w_{t,n}$) are integrated into the loss function as described in equation 3. This approach enhances the influence of critical samples while reducing the impact of less relevant ones, thereby improving the model's learning efficiency."}, {"title": "DETAILED ABLATION STUDIES ON SELECTIVE WEIGHTING", "content": "The average success rate per benchmark is illustrated in Table 2. The detailed results on each task are shown in Table 7, Table 8, and Table 9."}, {"title": "DETAILED TESTING RESULTS", "content": "We selected 20 typical scenarios among libero_90. The list of those scenarios can be found in Table 10. Additionally, the testing results of our method and baselines including ER-WLA, ER, Packnet, are listed in Table 11"}]}