{"title": "ShiftAddAug: Augment Multiplication-Free Tiny Neural Network with Hybrid Computation", "authors": ["Yipin Guo", "Zihao Li", "Yilin Lang", "Qinyuan Ren"], "abstract": "Operators devoid of multiplication, such as Shift and Add, have gained prominence for their compatibility with hardware. However, neural networks (NNs) employing these operators typically exhibit lower accuracy compared to conventional NNs with identical structures. ShiftAddAug uses costly multiplication to augment efficient but less powerful multiplication-free operators, improving performance without any inference overhead. It puts a ShiftAdd tiny NN into a large multiplicative model and encourages it to be trained as a sub-model to obtain additional supervision. In order to solve the weight discrepancy problem between hybrid operators, a new weight sharing method is proposed. Additionally, a novel two stage neural architecture search is used to obtain better augmentation effects for smaller but stronger multiplication-free tiny neural networks. The superiority of ShiftAddAug is validated through experiments in image classification and semantic segmentation, consistently delivering noteworthy enhancements. Remarkably, it secures up to a 4.95% increase in accuracy on the CIFAR100 compared to its directly trained counterparts, even surpassing the performance of multiplicative NNs.", "sections": [{"title": "1. Introduction", "content": "The application of deep neural networks (DNNs) on resource-constrained platforms is still limited due to their huge energy requirements and computational costs. To obtain a small model deployed on edge devices, the commonly used techniques are pruning[15, 28], quantization[14, 38], and knowledge distillation[16]. However, the NNs designed by the above works are all based on multiplication. The common hardware design practice in digital signal processing tells that multiplication can be replaced by bit-wise shifts and additions[13, 42] to achieve faster speed and lower energy consumption. Introducing this idea into NNs design, DeepShift[11] and AdderNet[5] proposed ShiftConv operator and AddConv operator respectively.\nThis paper takes one step further along the direction of multiplication-free neural networks, proposing a method to augment tiny multiplication-free NNs by hybrid computation, which significantly improves accuracy without any inference overhead. Considering that multiplication-free operators cannot restore all the information from the original operator, tiny NNs employing ShiftAdd calculations exhibit pronounced under-fitting. Drawing inspiration from NetAug[4], ShiftAddAug chooses to build a larger hybrid computing NN for training and sets the multiplication-free part as the target model used in inference and deployment. The stronger multiplicative part as augmentation is expected to push the target multiplication-free model to a better condition.\nIn augmented training, the hybrid operators share weights, but because different operators have varying weight distributions, effective weights for multiplication may not suit shift or add operations. This led us to develop a strategy for heterogeneous weight sharing in augmentation.\nFurthermore, since NetAug limits augmentation to width, ShiftAddAug aims to extend this by exploring depth and operator change levels. Thus, we adopt a two-step neural architecture search strategy to find highly efficient, multiplication-free tiny neural networks.\nShiftAddAug is evaluated on MCU-level tiny models. Compared to the multiplicative NNs, directly trained multiplication-free NNs can obtain considerable speed improvement (2.94\u00d7 to 3.09\u00d7) and energy saving (\u219367.75%~69.09%) at the cost of reduced accuracy. ShiftAddAug consistently enhances accuracy(\u21911.08%~4.95%) while maintaining hardware efficiency. Our contributions can be summarized as follows:\n\u2022 For the multiplication-free tiny neural network, we propose hybrid computing augmentation that leverages multiplicative operators to enhance the target multiplication-free network. While maintaining the same model structure, yields a more expressive and ultra-efficient network.\n\u2022 A new weight sharing strategy is introduced for hybrid computing augmentation, which solves the weight discrepancy in heterogeneous (e.g., Gaussian vs. Laplacian)"}, {"title": "3. ShiftAddAug", "content": "Shift. The calculation for the shift operator parallels that of standard linear or convolution operators using weight W, except that W is rounded to the nearest power of 2. Bit-shift and bit-reversal techniques are employed to achieve calculation results equivalent to those obtained through traditional methods as Equ. 1. Inputs are quantized before computation and dequantized upon obtaining the output.\n$\n\\begin{aligned}\n&S = \\operatorname{sign}(W) \\\\\n&\\left\\{\n\\begin{array}{l}\nY = X W=X\\left(S 2^{P}\\right), \\text { train. } \\\\\nY=\\sum_{i, j} \\sum_{k} \\pm\\left(X_{i, k}<< P_{k, j}\\right), \\text { eval. }\n\\end{array}\n\\right.\n\\end{aligned}\n$\nAdd. Add operator replaces multiplication with subtractions and l\u2081 distance since subtractions can be easily reduced to additions by using complement code.\n$Y_{m, n, t}=\\sum_{i=0}^{d} \\sum_{j=0}^{d} \\sum_{k=0}^{C_{i n}}\\left|x_{m+i, n+j, k}-F_{i, j, k, t}\\right|$\nNetAug. Network Augmentation encourages the target tiny multiplicative NNs to work as a sub-model of a large model expanded in width. The target tiny NN and the augmented large model are jointly trained. The training loss and parameter updates are as follows:\n$L_{\\text {aug }}=\\mathcal{L}\\left(W_{t}\\right)+\\alpha \\mathcal{L}\\left(W_{a}\\right), W_{t} \\in W_{a}$\n$W_{n+1}=W_{t}-\\eta\\left(\\frac{\\partial L}{\\partial W_{t}}+\\alpha \\frac{\\partial L}{\\partial W_{a}}\\right)$\nwhere L is the loss function, Wt is the weight of the target tiny NN, Wa is the weight of the augmented NN, and Wt is a subset of Wa."}, {"title": "3.2. Hybrid Computing Augment", "content": "ShiftAddAug goes a step further based on NetAug[4], using strong operators to augment weak operators.\nTaking an n-channel DepthWise Conv as an example, NetAug expands it by a factor of a, resulting in a convolution weight for an channels. During calculation, only the first n channels are used for the target model, while the augmented model employs all an channels. After trained as Equ. 3, the important weights within an channels are reordered into the first n, and only these n channels are exported for deployment.\nAs shown in Fig. 1, ShiftAddAug uses [0,n) channels (target part) and [n, an) channels (augmented part) for different calculation methods. The target part will use multiplication-free convolution (MFConv, ShiftConv[11] or AddConv[5] can be chosen) while multiplicative convolution (MConv, i.e. original Conv) is used as the augmented part.\nBecause the channels of Conv are widened, the input of each convolution is also widened and can be conceptually split into the target part Xt and the augmented part Xa, so does the output Yt, Ya. In ShiftAddAug, Xt and Yt mainly carry information of MFConv, while XA and YA are obtained by original Conv.\nHere three types of operators commonly used to build tiny NNs are discussed: Convolution (Conv), DepthWise Convolution (DWConv), and Fully Connected (FC) layer. The hybrid computing augmentation for DWConv is the most intuitive: split the input into Xt and Xa, then use MFConv and MConv to calculate respectively and connect the obtained Yt and Ya in the channel dimension. For Conv, We use all input X to get Ya through MConv. But to get Yt, we still need to split the input and calculate it separately, and finally add the results. Since the FC layer is only used as the classification head, its output does not require augmentation. We divide the input and use Linear and ShiftLinear to calculate respectively, and add the results. If bias is used, it will be preferentially bounded to multiplication-free operators.\n$\n\\begin{aligned}\n&\\text { DW Conv: }\\left\\{\n\\begin{array}{l}\nY_{t}=M F C o n v\\left(X_{t}\\right) \\\\\nY_{a}=M C o n v\\left(X_{a}\\right) \\\\\nY=\\operatorname{cat}\\left(Y_{t}, Y_{a}\\right)\n\\end{array}\n\\right. \\\\\n&\\text { Conv: }\\left\\{\n\\begin{array}{l}\nY_{t}=M F C o n v\\left(X_{t}\\right)+M C o n v\\left(X_{a}\\right) \\\\\nY_{a}=M C o n v(X) \\\\\nY=\\operatorname{cat}\\left(Y_{t}, Y_{a}\\right)\n\\end{array}\\right. \\\\\n&\\text { FC: }\\left\\{\n\\begin{array}{l}\nY_{t}=\\text { ShiftLinear}\\left(X_{t}\\right) \\\\\nY_{a}=\\text { Linear }\\left(X_{a}\\right) \\\\\nY=Y_{t}+Y_{a}\n\\end{array}\\right.\n\\end{aligned}\n$"}, {"title": "3.3. Heterogeneous Weight Sharing", "content": "Dilemma. The important weights will be reordered to the target part at each end of the training epoch (l\u2081 norm for importance). This is a process of weight sharing and is the key to effective augmentation.\nHowever, the weight distribution of the multiplication-free operator is inconsistent with the original Conv. It causes the weight discrepancy, i.e. good weights in original Conv may not be good in MFConv. As shown in Fig. 2, the weight in the original Conv conforms to Gaussian distribution, while ShiftConv has spikes at some special values. The weight in AddConv conforms to the Laplace distribution. The weight in ShiftConv is the one of the original Conv plus a Laplace distribution with a small variance.\nShiftAddNas[44] adds a penalty term to the loss function, guides the weight to conform to the same distribution. It affects the network to achieve its maximum performance. The Transformation Kernel they proposed also doesn't work on our approach since the loss diverges as Tab. 8. We argue that their approach makes training unstable. This dilemma motivated us to propose our heterogeneous weight sharing strategy.\nSolution: heterogeneous weight sharing. To solve the dilemma above, we propose a new heterogeneous weight sharing strategy for the shift and add operators. This method is based on original Conv and remap parameters to weights of different distribution through a mapping function R(\u00b7). In this way, all weights in memory will be shared under the Gaussian distribution, but will be remapped to an appropriate state for calculation.\nWhen mapping the Gaussian distribution to the Laplace distribution, we hope that the cumulative probability of the original value and mapping result is the same. Firstly, calculate the cumulative probability of the original weight in Gaussian. Then put the result in the percent point function of Laplacian. The workflow is shown in Fig. 3. The mean and standard deviation of the Gaussian can be calculated through the weights, but for the Laplace, these two values need to be determined through prior knowledge.\n$\n\\begin{aligned}\n&W_{l}=R\\left(W_{g}\\right)=r\\left(F C\\left(W_{g}\\right)\\right) \\\\\n&r(\\cdot)=\\operatorname{ppf}_{L}\\left(\\operatorname{cpf}_{G}(\\cdot)\\right) \\\\\n&c p f_{G}(x)=\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\int_{-\\infty}^{X} e^{\\frac{-\\left((x-\\mu)^{2}\\right)}{2 \\sigma^{2}}} d x \\\\\n&p p f_{L}(x)=\\mu-b * \\operatorname{sign}\\left(x-\\frac{1}{2}\\right) * \\ln \\left(1-2\\left|x-\\frac{1}{2}\\right|\\right)\n\\end{aligned}\n$\nWhere Wg is the weight in original Conv that conforms to the Gaussian distribution, and W\u2081 is the weight obtained by mapping that conforms to the Laplace distribution. FC is a fully connected layer, which is previously trained and frozen in augmented training. We need this because the weights don't fit the distribution perfectly. $c p f_{G}(\\cdot)$ is the cumulative probability function of Gaussian, $p p f_{L}(\\cdot)$ is the percentage point function of Laplace."}, {"title": "3.4. Neural Architecture Search", "content": "To obtain SOTA multiplication-free model at the tiny model size, a two-stage NAS is proposed.\nBased on the idea of augmentation, ShiftAddAug starts from a multiplicative SuperNet and cuts a deep SubNet from it as depth-augmented NN. Then select some layers on the SubNet to form the tiny TargetNet for final use. The TargetNet should meet the pre-set hardware limitation. This setup allows the TargetNet to be a part of the SubNet, facilitating joint training with weight sharing as Equ. 3. Layers within the SubNet that are not selected for the TargetNet serve as a form of depth augmentation. Moreover, the layers used for deep augmentation are initially selected but gradually phased out from the target network in training progresses.\nA new block mutation training is also proposed, which tends to gradually transform multiplication operators into multiplication-free states during training to make the training process more stable. The training process starts with all multiplications, and the layers of the target network become multiplication-free one by one from shallow to deep. At the end of training, a completely multiplication-free TargetNet can be obtained.\nWhile ShiftAddNas[44] directly uses hybrid computing to train their SuperNet and directly cut SubNets that meet the hardware requirements, we start from the multiplicative SuperNet and split the search process into two steps. The middle step is used for augmented training, which is the unique part of ShiftAddAug.\nCombining the Width Augmentation and Expand Augmentation we used in section 3.2, we construct our search space for the augmentation part according to Tab. 2.\nWe follow tinyNAS[22] to build SuperNet and cut SubNet. Then use evolutionary search to search for subsequent steps."}, {"title": "4. Experiments", "content": "Datasets. We conduct experiments on several image classification datasets, including CIFAR10[19], CIFAR100[19], ImageNet-1K[10], Food101[1], Flowers102 [29], Cars[18], Pets [31]. We also evaluated our method on VOC[12] and OpenEDS[30] for segmentation task."}, {"title": "4.2. ShiftAddAug vs. Baseline", "content": "We validate our method on MobileNetV2[33], MobileNetV3[17], MCUNet[22], ProxylessNAS[2], MobileNetV2-Tiny[22]. ShiftAddAug provides consistent accuracy improvements (average \u21912.82%) for ShiftConv augmentation over the multiplicative baselines. For AddConv augmentation, it improves the accuracy compared with direct training (average \u21911.59%). The resulting model will be faster (3.0\u00d7 for Shift) and more energy-efficient (68.58% for Shift and $52.02% for Add) due to the use of hardware-friendly operators. As shown in Tab. 3, these multiplication-free operators usually hurt the performance of the network. Changing all operators to Shift will cause 10.82% accuracy drop on average compared to the multiplication baseline. But after using our method, the accuracy increased by \u21913.63% on average under the same energy cost.\nIn addition, our method achieves higher results than multiplicative NetAug on some models (MobileNetV3:\u21911.17%, MCUNet:\u21911.44%, ProxylessNAS:\u21911.54%). This means that our method enables the multiplication-free operator to be stronger than those of the original operator.\nTo verify the generality of our method, we also conduct experiments on more datasets. As shown in Tab. 4, our method can achieve \u21910.89% to 14.28% accuracy improvements on different datasets. Hybrid computing augmentation works better on smaller models and datasets with less classification. On Flower102, MobileNetV2-w0.35 has \u21913.83% accuracy improvements with our method, while MCUNet has only \u21911.47%. This shows that a smaller model capacity can achieve a better effect on this dataset. The larger the model, the smaller the gain brought by augmentation. The same phenomenon also occurs in CIFAR10. For bigger datasets such as ImageNet, even if it is augmented, the capac-"}, {"title": "4.3. ShiftAddAug vs. SOTA Mult.-Free Models", "content": "We compare ShiftAddAug over SOTA multiplication-free models, which are designed manually for tiny computing devices, on CIFAR-10/100 to evaluate its effectiveness. As shown in Fig. 5, the model structures we use are smaller and have better energy performance. With ShiftAddAug, the accuracy still exceeds existing work. For DeepShift and AdderNet, our method boosts \u21910.67% and \u21911.95% accuracy on CIFAR100 with $84.17% and 91.7% energy saving. Compared with the SOTA shift quantization method APOT[21], we achieve an improved accuracy of \u21913.8%. With the same accuracy on CIFAR10, our model saves \u219384.2% of the energy compared with Deepshift, and $56.45% of the energy compared with AdderNet."}, {"title": "4.4. ShiftAddAug on Segmentation Task", "content": "To further verify the effectiveness of our method, we conduct experiments on the semantic segmentation task. We choose VOC[12] for general performance and OpenEDS[30] for specific tasks on MCU level devices. As shown in Tab. 5, ShiftAddAug has greater improvement on OpenEDS, which illustrates the application potential of our method.\nFrom the results shown in Fig.6, we can see that the model trained with augmentation will have fewer abnormal segmentation areas."}, {"title": "4.5. ShiftAddAug on Neural Architecture Search", "content": "Based on hybrid computing augmentation, we introduce neural architecture search into our method to get stronger tiny neural networks. We conduct our experiments on Cifar-10/100 and compare them with the results of ShiftAddNAS[44] which is better than multiplication-based FBNet[41]. As shown in Tab. 6, the multiplication-free model we obtained achieved higher accuracy (\u21913.61%) than ShiftAddNas. For hybrid-computed models, we have to use a smaller input resolution (96 instead of 160) and larger models. While the input resolution of ShiftAddNas is 32, this would give us 9\u00d7 the number of calculations if we have the same model architecture. Even so, we can still save 37.1% of calculations on average with similar accuracy."}, {"title": "4.6. Ablation Study", "content": "Hybrid Computing Augment. In order to prove that hybrid computing works better, we add an experiment using only multiplication-free operators for augmentation. We exert experiments based on NetAug, and replace all the original operators with Shift operators. The difference from our method is that the Shift operator is also used in the augmentation part, while our method uses multiplication in it. As shown in Tab. 7, it yields an average accuracy improvement of \u21911.02%.\nThen without using the heterogeneous weight sharing (HWS) method, just augmenting tiny NNs with the multiplicative operator will cause \u21930.9% accuracy drop on average due to the weight tearing problem. However, the situation changed after we applied for HWS. Compared with using the shift operator for augmentation, the accuracy increased by \u21912.2%.\nHeterogeneous Weight Sharing. Since the help of HWS on training results has been discussed above, here we visualize the weight distributions of Conv layers in tiny NNs under three scenarios, (a) w/o weight sharing; (b) heterogeneous weight sharing; (c) weight after remapped, as shown in Fig. 7. We consistently observe that the three operators exhibit different weight distributions without weight sharing. With our HWS, the saved weights are based on the original Conv and conform to Gaussian distribution. After remapping, the weights can show different distribution states in Shift/Add Conv calculations.\nDifferent methods for solving weight tearing are also compared. As shown in Tab. 8, both the ShiftAddNas method and direct linear remapping will cause loss to diverge. The additional penalty has a larger impact on tiny NNs, causing accuracy to decrease.\nAdditionally, Tab. 9 shows why weight sharing is important, which also explains why we have to face the problem of heterogeneous weights. Tab. 10 tells that HWS is not a parameterized trick directly improving accuracy, but just a compensation in augmentation.\nNeural Architecture Search. Our neural architecture search approach is dependent on our proposed hybrid computing augmentation. And it can help the multiplication-free operator to be as strong as the original operator. Block augmentation and block mutation help us further improve the performance of multiplication-free tiny NNs. As shown in Tab. 11, under similar energy consumption and latency, block augmentation improves accuracy by \u21910.5%, and block mutation improves by \u21910.79%. Combining all methods, the accuracy of the target model is increased by \u21911.22%."}, {"title": "4.7. Limitation", "content": "Since we use additional multiplication structures for augmentation, training consumes more resources. This is consistent with NetAug.\nIn terms of memory usage, which depends on the size of the augmented NN, ShiftAddAug usually doubles or triples it compared with direct training. This makes ShiftAddAug not suitable for on-device training."}, {"title": "5. Conclusion", "content": "In this paper, ShiftAddAug is proposed for training multiplication-free tiny neural networks, which can improve accuracy without expanding the model size. It's achieved by putting the target multiplication-free tiny NN into a larger multiplicative NN to get auxiliary supervision. To relocate important weights into the target model, a novel heterogeneous weight sharing strategy is used to approach the weight discrepancy caused by inconsistent weight distribution. Based on the work above, a two stage neural architecture search is utilized to design more powerful models. Extensive experiments on image classification and semantic segmentation task consistently demonstrate the effectiveness of ShiftAddAug."}]}