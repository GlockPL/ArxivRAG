{"title": "LARGE LANGUAGE MODEL CRITICS FOR EXECUTION-FREE EVALUATION OF CODE CHANGES", "authors": ["Aashish Yadavally", "Hoan Nguyen", "Laurent Callot", "Gauthier Guinet"], "abstract": "Large language models (LLMs) offer a promising way forward for automating\nsoftware engineering tasks, such as bug fixes, feature additions, etc., via multi-\nstep LLM-based agentic workflows. However, existing metrics for evaluating such\nworkflows, mainly build status and occasionally log analysis, are too sparse and\nlimited in providing the information needed to assess the quality of changes made.\nIn this work, we designed LLM-based critics to derive well-structured and rigor-\nous intermediate/step-level, execution-free evaluation proxies for repo-level code\nchanges. Importantly, we assume access to the gold test patch for the problem (i.e.,\nreference-aware) to assess both semantics and executability of generated patches.\nWith the gold test patch as a reference, we predict executability of all editing lo-\ncations with an F1 score of 91.6%, aggregating which, we can predict the build\nstatus in 84.8% of the instances in SWE-bench. In particular, such an execution-\nfocused LLM critic outperforms other reference-free and reference-aware LLM\ncritics by 38.9% to 72.5%. Moreover, we demonstrate the usefulness of such a\nreference-aware framework in comparing patches generated by different agentic\nworkflows. Finally, we open-source the library developed for this project, which\nallows further usage for either other agentic workflows or other benchmarks. The\nsource code is available at https://github.com/amazon-science/code-agent-eval", "sections": [{"title": "1 INTRODUCTION", "content": "Source code within repositories is typically complex and inter-dependent. As a result, code changes\n(referred to as edits) are often spread across multiple functions, classes, and/or files. This makes\nday-to-day software engineering (SE) tasks \u2013 such as fixing bugs or adding new features onerous\nfor developers. The inter-dependence can also introduce different forms of syntactic, semantic, or\nlogical errors, which may impact multiple locations. Thus, developers often find themselves in\niterative editing cycles, where they must repeatedly build, identify and fix failures.\nRecent improvements in large language models (LLMs) (OpenAI, 2023; Anthropic, 2024; Touvron\net al., 2023) has prompted the use of LLM agents \u2013 systems capable of interacting with its envi-\nronment to make rational decisions \u2013 for automating these complex SE tasks through multi-step\nprocesses (Zhang et al., 2024; Wang et al., 2024). We refer to such an orchestration of autonomous\nor semi-autonomous agents as agentic workflows. Despite their promise, evaluating the effectiveness\nand reliability of these workflows poses significant challenges.\nExisting metrics for evaluating agentic workflows primarily include build success status and, in some\ncases, log analyses. However, these are limited. Firstly, retrieving such metrics requires setting up\na test environment and running the test suite, which is either impossible whenever considering a\ngeneric range of code repositories in industrial applications or both laborious and time consuming.\nSecondly, they are too sparse and provide a narrow view of the overall performance, which is not\nsufficient to assess the quality of the changes made. For example, build status does not provide in-\nsights into functional correctness or performance under various conditions. Thirdly, analyzing logs\ncan be cumbersome and may not provide actionable insights without significant manual interpreta-\ntion."}, {"title": "2 AUTOMATIC EVALUATION USING LARGE LANGUAGE MODELS", "content": ""}, {"title": "2.1 A PRIMER ON USING LLMS FOR CODE EVALUATION", "content": "In the domain of source code, recent work has shown a promise in the use of LLMs for the automated\nevaluation of code generation tasks, demonstrating their potential to assess code quality and func-\ntional correctness without actual execution (Zhou et al., 2023; Zhuo, 2024; Dong et al., 2023). In this\ncontext, two primary classes of such metrics are widely used: reference-free and reference-aware.\nThe former leverage LLM's understanding of source code and their alignment with the given intent\n(i.e., natural language description used for code generation) to assess code quality and functional\ncorrectness (Zhou et al., 2023; Zhuo, 2024). Conversely, reference-aware metrics compare the gen-\nerated code with a ground-truth reference implementation, evaluating how closely the former aligns\nwith the latter, thus providing a direct measure for functional correctness. These are effective for the\nassessment of generated code. However, a task (e.g., GitHub issue) can be resolved in multiple ways\nby making non-unique code edits at different locations in the repository. Therefore, such automated\napproaches are not directly extensible for the qualitative assessment of agentic workflow-generated\npatches."}, {"title": "2.1.1 PROBLEM FORMULATION", "content": "Let's assume a set of coding tasks $T\u2208T$, typically represented by a goal specified in natural\nlanguage and a code repository to modify. These tasks can generally represent new feature design,\ncode migration, bug fixing, unit test generation ... In all generality, we model candidate solutions\nto the task T as code patches $P = {P1, P2, ..., PN }$, representing the before/after difference. Note\nthat the candidate patches $pi$ can originate from multiple sources, including humans, LLM/agent\nworkflows, or even by artificial perturbations of correct solutions.\nThe goal of our work is to design an evaluation score $S : p \u2192 {0,1}$ or $S : p\u2192 R$ that assesses the\nquality of candidate patches $pi$ with respect to the task T. Importantly, we assume the access to a\nground-truth patch $p\u2217$ that successfully resolves the task T. We allow our score S to depend on that\nground-truth patch $p\u2217$. In that regard, our problem can be seen as supervised or reference-aware. We\nrelax this assumption in Section 4.3.\nIn order to measure the quality of our evaluation score with regards to the two goals above, we\nconsider the following metrics:\nSuccess Discrimination Granted a set of successful and failed patches, we consider in sections\n4.2 and 4.4 classification models for binarized evaluation score S to map the score to a task success\ncriteria such as build status. From this criteria, we assess classical accuracy metrics such as as\nprecision, recall and F1.\nProgress Monotonicity By using labeled patches whose advancement status is known (e.g.,\npatches generated during agent trajectory) or by artificially perturbing existing patches, we generate\nset of tuples such as p\u2081 <= p2, with respect to a given notion of order and assess that S(p1) \u2264 S(p2).\nSeveral such proxy notions of order, i.e. task advancement, can be designed, either in reference-free\nor reference-aware mode. For instance, percentage of passing tests, number of line of code changed,\nnumber of files modified, correctness of the changes, edit distance between the reference and the\ncandidate.... We show the limitations of such methods with respect to the initial goal yet illustrate\nhow our method positively correlates with these metrics."}, {"title": "2.2 LLM CRITICS FOR EXECUTION-FREE PATCH EVALUATION", "content": "Code modifications from an agentic workflow might resolve certain issues while potentially failing\nto address others, or even introduce new ones. To assess the correctness and effectiveness of such\ngenerated patches, our framework employs test-centric LLM critics. These utilize the unseen tests\nextracted from gold test patch as a reference (i.e., reference-aware) and individually predict whether\neach of the tests pass or not. For this purpose, we consider two variants of the generated patch:\n1. Context Enhancement: By default, a patch shows only 3 lines of context around each\nhunk, which may not be sufficient for accurately predicting test outcomes due to limited\nunderstanding of input propagation. To address this, we expand the context to include\nadditional lines that span the entire functions or methods containing the code changes.\nSuch context enhancement provides a more reliable test-centric evaluation of patches.\n2. Source Code: Next, we extract the functions or methods containing the code changes after\napplying the patch. We refer to these as post-commit functions. The rationale, in this regard,\nis that the new unseen tests are likely evaluating the post-commit functions.\nSuch a design represents a micro-evaluation of patches, as it assesses the generated patches in the\ncontext of the unseen tests, individually. The predictions for each test reveals how the changes affect\na particular aspect of the code. This is particularly useful to track progress in potential failures with-\nout actual execution, while establishing a comparative framework for different agentic workflows.\nTo determine the overall build status after applying an agentic workflow-generated patch, we aggre-\ngate the individual test oracle predictions. We determine a build success if our LLM critic predicts\nall of the new unseen tests to pass. In contrast, if even one of the tests is predicted to fail, we deter-\nmine a build failure. Note that more ensemble strategies can be explored for this purpose, which is\nbeyond the scope of this work. In summary, our test-centric framework enables an execution-free,\nmacro-evaluation of whether the generated patch successfully addresses all intended functionalities\nby aggregating the micro-evaluations based on new unseen tests."}, {"title": "2.2.1 UNCERTAINTY QUANTIFICATION WITH BLACK-BOX CONFIDENCE MEASURES", "content": "As described in Section 2.2, we first formulate our micro-evaluation of patches using LLM critics\nas a binary classification task. To calibrate these predictions, we assume that all parameters dur-\ning inference are unknown. We elicit confidence estimates by prompting the LLM to express its\nconfidence in unseen test pass/fail prediction as a value between 0 and 100. When combined with\nChain-of-Thought (COT) prompting (Wei et al., 2022), such verbalized confidence measures have\nbeen shown to be useful for improving the reliability of the LLM's predictions (Xiong et al., 2024)."}, {"title": "3 EXPERIMENT SETUP", "content": ""}, {"title": "3.1 DATASET", "content": "SWE-bench (Jimenez et al., 2024) is a benchmark comprising real-world software engineering tasks,\nwith each instance containing pairs of GitHub issues and corresponding pull requests. While the\nformer describes the desired changes to the codebase, the latter includes the actual code changes\nmade by human developers in resolving the issue and the test cases to validate the changes. Here,\nthe goal of the agentic workflow is to interact with the unfixed repository snapshot, and attempt to\nfix the issue. The generated patch is tested against the new unseen tests as well as existing tests\nimpacted. In this paper, we consider a canonical subset of SWE-bench (dubbed SWE-bench-Lite)\ncontaining 300 instances collected from 11 popular Python projects, where the gold change patch\ncontains at most 3 edits in a single file."}, {"title": "3.2 MODELS AND AGENTIC PATCHES", "content": "We conduct our main experiments using claude-3-opus as the LLM critic. As patches, we utilize\nthe generated patches from factory-code-droid (Droid, 2024), sweagent-gpt4 (Jimenez\net al., 2024), Gru (Gru.ai, 2024) and codestory-aide-mixed (Aide.dev, 2024). These are\nselected to be representative agentic trajectories over the benchmark, as further discussed in Section"}, {"title": "4 EVALUATING THE EVALUATORS", "content": ""}, {"title": "4.1 LLM CRITICS FOR MICRO-EVALUATION OF PATCHES", "content": "As described in Section 3.1, a task instance contains gold tests that an agentic patch is expected to\npass. To enable a micro-evaluation of patches (i.e., in predicting test oracles), we leverage isolated,\ntest-aware LLM critics that are given access to each of the unseen tests independently as a reference.\nIn this experiment, we evaluate our test-centric framework against a random baseline. Except when\nusing a less effective agentic workflow (e.g. RAG + ChatGPT-3.5 on the SWE-bench leaderboard),\nwhere a higher number of tests fail, other agents have significantly more passing ones (ratio of\npassing to failing tests for factory-code-droid is 85:15). This indicates errors are typically\nconcentrated in fewer editing locations within a multi-hunk patch. We account for such a class\nimbalance by weighing the probabilities for our baseline proportional to the class frequencies.\nIn Table 1, we report the performance of our isolated, test-aware LLM critics using both patch\nvariants for test oracle prediction. We can see that the LLM critic with context-enhanced patches\nperforms the best, outperforming other baselines by 7.4% to 12.7%. Interestingly, the imbalance in\npassing and failing tests yields a strong random baseline, which records better performance than even\nthe LLM critic with source code (i.e., post-commit functions). However, as we show in Section 4.2,\nthis is not useful in predicting build outcomes. Upon further analysis, we observed that the LLM\ncritic utilizing source code instead of context-enhanced patches helps identify failing tests better. In\ncontrast, the latter helps identify around 25% more passing tests. This may be because, in this case,\nthe LLM understands the purpose of the test in the context of the changes better."}, {"title": "Confidence Scores v/s Test Complexity.", "content": "Here, we explore the verbalized confidence scores from\nthe isolated, test-aware LLM critic, and its relationship with test complexity. In Figure 3 (left), we\nsee a clear pattern: the LLM critic tends to make more correct predictions with a high confidence\nthan incorrect ones, thus suggesting that the LLM's verbalized confidence scores are a reliable indi-\ncator of its predictions. Using test lengths as a proxy to complexity, in Figure 3 (right), we can see\nthat this is particularly true for tests with lower complexity. Based on these analyses, we chose to\nautomatically assume failures when the LLM critic assesses a patch with low confidence (\u2264 65%)\nfor tests with high complexity (test length > 50). We observed that such thresholding helps improve\nthe specificity (i.e. true negative rate) by 24.1% (8.3% \u2192 10.3%)."}, {"title": "4.2 LLM CRITICS FOR MACRO-EVALUATION OF PATCHES", "content": "To enable the macro-evaluation of patches (i.e., in predicting build outcomes), in our test-centric\nframework, we next aggregate the predictions from the isolated, test-aware LLM critics (as in Sec-\ntion 4.1). If even one test among all unseen tests is expected to fail, we predict a build failure.\nOtherwise, we predict that the patch would successfully pass the build."}, {"title": "4.2.1 BASELINES", "content": "We select multiple baselines to evaluate our test-centric framework in build status prediction:\n(a) Random: First, to establish a reference for more sophisticated LLM critics, we aggregated\nthe predictions from the random baseline in Section 4.1, with the assumption that even a\nsingle failing test prediction results in a build failure. Such an approach helps us measure\nthe importance of aggregating reasoning-based test oracle predictions from LLM critics as\nopposed to random ones, in determining build outcomes.\nEvery task instance in SWE-Bench comes with a gold patch, containing: the actual code changes\nhuman developers made to resolve the issue, and the test cases that validate the changes. In our test-\ncentric framework, we consider only the test cases as the reference. Here, aiming to evaluate whether\nthe generated patch and the gold change patch are equivalent, we use the latter as the reference.\n(b) Edit Distance: In this baseline, we leverage a pre-trained code language model, Code-\nBERT (Feng et al., 2020), to retrieve the embeddings for both the generated patch and the\nreference. Next, we compute the cosine similarity between the two to quantify the similarities\nbetween their semantic contents. To determine the build outcome, we perform a grid search on\nthe validation set to identify an optimal threshold, which we then apply to all task instances.\n(c) Change-Aware: Here, we design LLM critics probing for patch equivalence. Given the can-\ndidate and gold change patch pairs, these determine whether they would result in the same\nfunctional outcome. To this end, we assume two patch variants: one, default patches with 3\nlines of context around each hunk; two, \u00b1 function-level patches (as described in Section 2.2).\nNext, to assess the importance of aggregating micro-evaluations, we compared with:\n(d) Holistic, Test-Aware: In this baseline, we design LLM critics to predict the collective outcome\nof all new unseen tests. A positive prediction indicates that all tests pass, i.e., build success.\nConversely, a negative prediction indicates that at least one of the tests fails, signifying a build\nfailure. To this end, the holistic, test-aware LLM critics take the generated patch and all corre-\nsponding reference tests as inputs. Here, we consider both patch variants (as in Section 2.2)."}, {"title": "4.2.2 EXPERIMENTAL RESULTS", "content": "Table 2 shows the performance comparison for build status prediction. We can see that aggregating\nthe test oracle predictions from isolated, test-aware LLM critics yields the best performance, pre-\ndicting the build outcomes with an F1-score of 82.1%. Notably, we observe an improvement over\nall baselines by 38.9% to 159%, and over other LLM critics by 38.9% to 68.2%.\nWe can see that aggregating the random baseline in Section 4.1 performs poorly. This asserts the\ncomplexity of the task, and explains the need for aggregating reasoning-based test oracle predic-\ntors. Furthermore, we improve upon the edit distance-based approach by 72.1%. Interestingly, we\nobserved that this baseline did not capture even a single build failure. This could be due to Code-"}, {"title": "4.3 REFERENCE HELPS, BUT IS NOT ALWAYS AVAILABLE!", "content": "In our test-centric framework, we use the new unseen tests as the reference. However, in real-world\nscenarios, this assumption might not hold. In this experiment, aiming to assess the importance of\nsuch a reference, we compare against two reference-free approaches.\nAll SWE-bench dataset instances contain: (a) Problem Statement, which represents a natural lan-\nguage description of desired changes to the codebase, and (b) Hints, which represent natural lan-\nguage suggestions on how to solve the problem. These are often used by agentic workflows in gen-\nerating candidate patches. Here, we design baselines that use the Problem Statement, and Problem\nStatement + Hints, to determine if the generated candidate patch helps solve the task description.\nIn Table 3, we report the results for reference-free baselines. We can see that our test-centric frame-\nwork outperforms both baselines by 72.5% and 65.5%, respectively. Note that hints usually contain\nlow-level details, such as pseudo-code suggestions to the original human task worker. While this\nmight be useful to generate candidate patches with agents, these are not particularly useful to evalu-\nate the generated patches. This is also reflected in the comparison of both baselines, with negligible\nimprovements upon including the hints to the LLM critic. Moreover, we can see that enhancing the\npatches with additional context does not help either, showing inconsistent trends."}, {"title": "4.4 COMPARING AGENTIC PATCHES WITH TEST-AWARE LLM CRITICS", "content": "In this section, we compare four agentic workflows from the SWE-bench-Lite leaderboard,\ncodestory-aide-mixed, Gru, factory-code-droid, and swe-agent+gpt4. Each of these\nsuccessfully resolve 43%, 36%, 31%, and 18% of all task instances, respectively. In Figure 4\n(left), we plot the LLM-predicted test pass rates for these instances, computed from test ora-\ncies predicted by our test-aware LLM critics for all three agentic patches. The results show that\npatches from swe-agent+gpt 4, which are of lower quality and fail more tests, correspond to lower\ntest pass rates, particularly evident in the range of 0.0 - 0.6. On the other hand, patches from\ncodestory-aide-mixed, which are of relatively higher quality, achieve higher test pass rates,\nwith greater counts between 0.9 and 1. These trends are as expected, underscoring the reliability of\nusing predicted test pass rates as a proxy for evaluating progress across different workflows.\nOne of the goal is to use for a given instance labeled patches p1,...,pn whose advancement status\nis known: for instance, test pass rate provides an order over the patches such that p1 \u2264 ... \u2264 pn.\nHere, we aim to assess how our evaluation score S correlates with this order.\nAccordingly, we ranked the agentic workflows in decreasing order of LLM-predicted test pass rates\nfor all corresponding candidate patches. We then compared these rankings with the true rank-orders\nof the workflows and computed the Spearman rank-order correlation (p) between them, to quantify\nthe degree of agreement between the predicted and true rank orders. In Figure 4 (right), we show the\ndistribution of these correlation coefficients for all task instances. As a baseline, we also computed\nthe edit distance between the candidate and true patches and created a rank order based on cosine"}, {"title": "4.5 HARNESSING DIFFERENT LLMS IN TEST-AWARE EVALUATION", "content": "In Table 5, we compare the performance of our test-aware LLM critics in both micro and macro-\nevaluation settings for three LLMs from Anthropic (Anthropic, 2024): claude-3-sonnet,\nclaude-3-opus, claude-3-5-sonnet We can see that both claude-3-opus and\nclaude-3-5-sonnet achieve comparable performance in predicting test oracles, which is 6.8%\nbetter than when using claude-3-sonnet. By aggregating these micro-assessments, with\nclaude-3-opus, our test-aware LLM critic predict build outcomes the best, with an F1-score of\n81.8%. These findings underscore the LLM-agnostic nature of our test-aware LLM critics."}, {"title": "5 RELATED WORK", "content": "Large Language Models for Code. Recent software engineering (SE) research has focused on the\nuse of machine learning-based approaches for SE tasks including program synthesis (Li et al., 2022;\nNijkamp et al., 2023), vulnerability detection (Fu & Tantithamthavorn, 2022), automated program\nrepair (Li et al., 2020; Ahmed & Devanbu, 2023), test generation (Sch\u00e4fer et al., 2023), etc. These\nhave traditionally been limited to code snippets (often at the method-level) extracted from software\nrepositories. With the advances in large language models (LLMs), there has been a shift in focus\ntowards extending these to the repository-level, for SE tasks like code (Bi et al., 2024; Deng et al.,\n2024; Pan et al., 2024) and patch (i.e., code change) generation (Zhang et al., 2024; Bairi et al.,\n2024). In this work, we design test-aware LLM critics to evaluate code changes."}, {"title": "Benchmarks for Repository-Level Coding Tasks.", "content": "Building on method-level (Chen et al., 2021;\nAustin et al., 2021) and class-level (Du et al., 2023) code generation benchmarks, there has been a\nrise in repository-level code generation benchmarks in academia. CrossCodeEval (Ding et al., 2023),\nCoderEval (Yu et al., 2024), RepoBench (Liu et al., 2024) support multilingual code generation tasks\nutilizing cross-file context extracted from real-world open-source repositories. Extending beyond\ncode completion, SWE-bench (Jimenez et al., 2024) introduces a broader set of challenges involving\npatch generation, grounded in real-world software engineering tasks like bug fixing, feature addition\nor enhancement, etc. However, SWE-bench is limited to Python task instances. As a first step toward\nmultilingual support, SWE-bench-java (Zan et al., 2024) was developed to extend this framework\nto Java. We evaluate our LLM critics on the Python task instances in SWE-bench. However, these\ncan be easily extended to new programming languages and repositories, providing a foundation for\nassessing the code execution-specific understanding of LLMs across other languages."}, {"title": "Automated Evaluation of Large Language Models in Coding Tasks.", "content": "Traditionally, the generated code can be evaluated statically, in terms of software quality (e.g. read-\nability, complexity (Oman & Hagemeister, 1992)). In particular, code changes in a repository can be\nassessed via program differencing (Apiwattanapong et al., 2004) and change impact analysis (Ren\net al., 2005) \u2013 useful to determine the effect of a change on the rest of the repository. However, none\nof these approaches account for the correctness of the generated code or patches.\nBy matching against a reference solution, semantic-based metrics such as BLEU (Papineni et al.,\n2002), ROUGE (Lin, 2004), and CodeBLEU (Ren et al., 2020) or neural based metrics such as\nCodeBERT (Feng et al., 2020) help establish match-based evaluation proxies. However, these are\nlimited to source code and do not capture program semantics well nor correlate efficiently with\nhuman judgment (Eghbali & Pradel, 2004; Tran et al., 2019). Recent work has proposed the use of\nLLMs for such an evaluation, thus helping establish execution-free evaluation proxies which probe\nfor correctness. These include both reference-free (Zhou et al., 2023; Zhuo, 2024) and reference-\naware approaches (i.e., those utilizing human developer-written code or tests) (Dong et al., 2023).\nHowever, these are not extensible for the evaluation of patches and still exhibit limited efficiency.\nIn this work, we design both reference-free and test-reference-aware evaluation proxies (the latter\nsignificantly outperforming the former)."}, {"title": "6 CONCLUSION", "content": "In conclusion, we designed LLM-based critics to derive execution-free evaluation proxies for repo-\nlevel code changes. With the gold test patch as a reference, we predict executability of all editing\nlocations with an accuracy of 91.5%, aggregating which, we can predict the build status in 82.1%\nof the instances in SWE-bench. In particular, such an execution-focused LLM critic outperforms\nother reference-free and reference-aware LLM critics by 38.9% to 72.5%. Most notably, we observe\nthat aggregating fine-grained assessments of candidate patches leads to better performance than\nevaluating them as a whole and that using tests as a reference proves more effective than code\nchanges, more so because these enable fine-grained assessments.\nNatural extensions of our work include investigating benchmarks from other programming lan-\nguages, directly incorporating this execution score in agentic framework by relaxing the reference-\naware character of our evaluation, and utilizing the LLM input to proactively design better tests."}]}