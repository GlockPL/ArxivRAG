{"title": "FANFOLD: Graph Normalizing Flows-driven Asymmetric Network for Unsupervised Graph-Level Anomaly Detection", "authors": ["Rui Cao", "Shijie Xue", "Jindong Li", "Qi Wang", "Yi Chang"], "abstract": "Unsupervised graph-level anomaly detection (UGAD) has attracted increasing inter- est due to its widespread application. In recent studies, knowledge distillation-based methods have been widely used in unsupervised anomaly detection to improve model efficiency and generalization. However, the inherent symmetry between the source (teacher) and target (student) networks typically results in consistent outputs across both architectures, making it difficult to distinguish abnormal graphs from normal graphs. Also, existing methods mainly rely on graph features to distinguish anomalies, which may be unstable with complex and diverse data and fail to capture the essence that differentiates normal graphs from abnormal ones. In this work, we propose a Graph Normalizing Flows-driven Asymmetric Network FOr Unsupervised Graph-Level Anomaly Detection (FANFOLD in short). We introduce normalizing flows to unsupervised graph-level anomaly detection due to their successful application and superior quality in learning the underlying distribu- tion of samples. Specifically, we adopt the knowledge distillation technique and apply normalizing flows on the source network, achieving the asymmetric network. In the training stage, FANFOLD transforms the original distribution of normal graphs to a standard normal distribution. During inference, FANFOLD computes the anomaly score using the source-target loss to discriminate between normal and anomalous graphs. We conduct extensive experiments on 15 datasets of different fields with 9 baseline methods to validate the superiority of FANFOLD.", "sections": [{"title": "1 Introduction", "content": "Graph-level anomaly detection aims to identify anomalous patterns or outliers within the overall structure or behavior of graphs [1, 2, 3], which plays a crucial role in detecting potential threats across various domains such as social networks, biological systems, and transportation networks [4]. Unsupervised graph-level anomaly detection utilizes algorithms to automatically identify irregularities within graphs, providing a scalable and adaptable solution that detects new anomalies without requiring labeled data. Recently, knowledge distillation (KD)-based approaches have been widely adopted in unsupervised anomaly detection tasks due to their unique ability to enhance model efficiency and generalization capabilities [5, 6, 7, 8, 9]. The rationale behind this approach lies in the fact that the target network, having been exclusively trained on normal data, can only effectively replicate the source network's outputs on such data. Subsequently, during the testing phase, the disparity between the outputs of the target network and source network serves as an indicator of anomalies. Despite several attempts that have been made to apply knowledge distillation for unsupervised graph-level anomaly detection [10, 11, 12, 13], there still exist certain issues.\nFirstly, existing knowledge distillation based graph-level anomaly detection models mainly adopt symmetric networks, indicating that the target network (student) and source network (teacher) have similar or even identical architectures, e.g., existing works mainly adopt two identical GNN encoders for the source network and target network in their framework [2, 14, 15]. Nevertheless, in graph-level anomaly detection tasks, the challenge arises from the fact that traditional knowledge distillation approaches expect larger discrepancies between the outputs of two networks for detecting potential anomalies. However, as shown in Figure 1 (a), the inherent symmetry between the source (teacher) network and target (student) network typically yields consistent outputs across both architectures, thereby making it difficult to distinguish abnormal graphs from normal graphs.\nSecondly, traditional knowledge distillation based anomaly detection methods mainly involve a simple architecture, e.g., an encoder, with the main goal of improving the encoder's feature embedding ability. However, these methods may demonstrate instability when processing complex and diverse graph data. In fact, these obtained embeddings can be distinguished from the perspective of their distribution in the latent space [2, 10, 12], which can reduce the perturbation caused by specific features on detection. More importantly, adopting a distribution-centric approach enables a more accurate capture of the intrinsic characteristics of normal graphs (e.g., as shown in Figure 1 (b)), which in turn augments the sensitivity and accuracy of anomaly detection algorithms. Therefore, there is an urgent need for a method that can consider anomaly detection from the perspective of sample embedding distribution and distribution density.\nTo tackle the aforementioned challenges, we propose a novel graph normalizing flows-driven asym- metric network for unsupervised graph-level anomaly detection, named FANFOLD. Specifically, based on the widely acknowledged fact that anomalies tend to occur in low-density areas of a dis- tribution [16], we introduce normalizing flows to map the empirical distribution of typical graph structures onto a Gaussian standard distribution within the source network. This mapping is designed to consider both the distribution of the sample embeddings and their density, thereby establishing the desired asymmetry within the network architecture. In the training stage, we first train an encoder via a reconstruction task on the source network, then we further train the normalizing flows based on the pre-trained encoder to achieve transformation from the original distribution of normal graphs to a standard normal distribution. In the inference stage, we utilize the difference in output between the source network and the target network to discriminate normal graphs and anomalous graphs. Our contributions are summarized as follows:\n\u2022 We propose a novel graph normalizing flows-driven asymmetric network for unsupervised graph-level anomaly detection (FANFOLD in sort). To our best knowledge, we are the first to apply the graph normalizing flows and asymmetric network to unsupervised graph-level anomaly detection tasks, integrating their advantages and enabling them to work together efficiently."}, {"title": "2 Related Work", "content": "Graph-level anomaly detection aims to identify abnormal graphs among normal ones. These anoma- lous graphs often represent a minority but contain crucial patterns [1]. In recent years, there has been a surge of noteworthy scholarly endeavors. OCGIN [17] is the first representative model that integrates one-class classification with the graph isomorphism network (GIN) [18]. This integration significantly enhances the capability and accuracy of graph-level anomaly detection by combining the strengths of both techniques. GLocalKD [2] implements joint random distillation to detect both local and global graph anomalies. This is achieved by training one graph neural network to predict the output of another graph neural network, which has its weights fixed at random initialization. GOOD-D [10] presents a novel perturbation-free graph data augmentation that avoids introducing perturbations. It employs hierarchical contrastive learning to enhance the detection of anomalous graphs by identifying semantic inconsistencies at multiple levels, thus improving the overall accuracy and robustness of anomaly detection in graph data. SIGNET [3] presents a multi-view subgraph information bottleneck framework. It derives anomaly scores and delivers explanations at the subgraph level."}, {"title": "2.2 Normalizing Flows", "content": "Normalizing flows are a family of generative models that create tractable distributions, allowing for efficient and exact sampling and density evaluation. A normalizing flow is a transformation of a com- plex distribution into a more simple probability distribution (e.g., a standard normal) by a sequence of mappings that are usually invertible and differentiable [19]. RealNVP [20] expands the scope of these models by incorporating real-valued non-volume preserving (real NVP) transformations. These transformations are powerful, stably invertible, and learnable, leading to an unsupervised learning algorithm that offers exact log-likelihood computation, precise sampling, efficient inference of latent variables, and an interpretable latent space. DifferNet [21] utilizes a multi-scale feature extractor that allows the normalizing flow to assign meaningful likelihoods to the images. As normalizing flows are well-suited to handle low-dimensional data distributions, it leverages the descriptive features extracted by convolutional neural networks to estimate their density via normalizing flows. AST [22] focuses on industrial defect detection tasks (which refer to RGB and 3D data), it analyses the advantage of utilizing normalizing flows and enhances student-teacher networks by integrating a bijective normalizing flow as the teacher. GNF [23] introduces a reversible GNN model for graph generation and prediction tasks. In the supervised scenario, it passes messages using significantly less memory, enabling it to scale to larger graphs. In the unsupervised scenario, it integrates graph normalizing flows with an innovative graph auto-encoder to develop a generative model of graph structures."}, {"title": "2.3 Knowledge Distillation", "content": "Knowledge distillation involves transferring knowledge from a large and complex teacher model to a structurally simpler and smaller student model effectively [24, 25]. T2-GNN [26] focuses on the challenge that when both features and structure are incomplete, the disparity between them caused by missing randomness intensifies their mutual interference, potentially leading to incorrect completions that adversely impact node representation. It independently designs feature-level and structure-level teacher models to offer specific guidance for the student model. MuGSI [27] proposes a GNN-to-MLP distillation framework for graph classification task, and it enables efficient structural knowledge distillation at various granular levels (i.e., graph-level, subgraph-level, and node-level distillation). GLocalKD [2] applies knowledge distillation into graph-level anomaly detection, it"}, {"title": "3 Method", "content": "In this section, we provide the details of FANFOLD. The framework is shown in Figure 2, which consists of four key stages: Data Pre-Processing, Source network encoder Pre-Training, Normalizing Flows and Target network."}, {"title": "3.1 Preliminaries", "content": "In this work, an undirected attributed graph is defined as $G = (V, E, X)$, where $V$ is the set of nodes and $E$ represents the set of edges. The topology information of $G$ is represented by an adjacent matrix $A \\in R^{n \\times n}$, with $n$ being the number of nodes. $A_{i,j} = 1$ when an edge exists between node $v_i$ and node $v_j$, otherwise $A_{i,j} = 0$. $X \\in R^{n \\times d_{attr}}$ denotes the feature matrix of nodes. Each row in $X$ corresponds to the feature vector of a node, with $d_{attr}$ dimensions. The set of graphs is represented as $G = {G_1, G_2, ..., G_m}$, where $m$ indicates the number of graphs in the set $G$. During the training phase, the model is exclusively trained on normal graphs. In the inference phase, FANFOLD aims to identify and distinguish the anomalous graphs from the normal ones given a graph set $G$."}, {"title": "3.2 Data Processing", "content": "For a graph, both node attribute and graph topology information are important for UAGD tasks. Thus, we first employ perturbation-free graph augmentation strategy [10, 28] to get structure encoding $X_{struc}$. Then, we concatenate structure encoding and attribute feature to form the initial feature representation by $X_{init} = [X || X_{struc}]$, where $||$ indicates concatenation operation. Based on this processing, we can make use of both the node attribute and topology information of graphs."}, {"title": "3.3 Source Network Encoder Pre-Training", "content": "In this section, we first design an encoder utilizing the Graph Neural Network (GNN) to map graphs into a structured continuous space. The encoder takes the${A}$ and ${X_{init}}$ as input, and output nodes embeddings $H_G = {h_i}_{i=1}^n \\in R^{n \\times d}$. To preserve the complex structure characteristics, we use the binary cross entropy loss function as Eq. 1 to approximate the adjacent matrix, which can also make connected nodes closer in embedding space."}, {"title": "3.4 Normalizing Flows", "content": "The normalizing flows acts as the bijector between feature space and normalization latent space, which means the normalizing flows transform the feature of the input graphs from the original distribution to the normal distribution (i.e., the standard normalization distribution). We model $Z \\thicksim \\prod_{i=1}^N \\mathcal{N}(z_i | 0, I)$ in our normalization latent space. Next, we will focus on the implementation process. To achieve reversibility, the node embedding $\\hat{H}_G$ from the GNN is split into two components along the embedding dimension: $\\hat{H}_t^{(0)}$ and $\\hat{H}_t^{(1)}$. The process of normalizing flows can be described in detail as follows:\n$\\begin{aligned}\nH_{t+1}^{(0)} &= H_t^{(0)} \\odot e(\\mathcal{F}_1(H_t^{(1)})) + \\mathcal{F}_2(H_t^{(1)})\\\\\n\\hat{H}_{t+1}^{(1)} &= H_t^{(1)} \\odot e(\\mathcal{G}_1(H_{t+1}^{(0)})) + \\mathcal{G}_2(H_{t+1}^{(0)})\\\\\n\\hat{H}^{(i)} &= H^{(i)}\n\\end{aligned}$\nwhere $\\odot$ represents the element-wise product, t is the step of message passing. $\\mathcal{F}$ and $\\mathcal{G}$ are sub-networks that can be considered as Message Passing Neural Networks (MPNNs). Finally, $\\hat{H}^{(0)} = concat(\\hat{H}_T^{(0)}, \\hat{H}_T^{(1)})$ is used as the output of the source network. $concat(\\cdot)$ is the concatenation of $\\hat{H}^{(0)}$ and $\\hat{H}^{(1)}$. Inspired by the work of Liu et al.[29], we use a change of variables to provide the rule for exact density transformation. We assume $H_t \\thicksim P(H_t)$, then the density in terms of $P(H_{t-1})$ is given by:\n$P(H_{t-1}) = |det \\frac{\\partial H_t}{\\partial H_{t-1}}| P(H_t)$.\nThus, we have the following equation:\n$P(\\mathcal{Z}) = P(\\hat{H}_T) \\prod_{t=1}^T |det \\frac{\\partial H_t}{\\partial H_{t-1}}|$.\nwhere $H_0$ is the input node embedding. The Jacobian matrices are lower triangular, hence making density computations tractable, $det| \\cdot |$ is Jacobian determinant. We minimize the negative log likelihood with $P(\\hat{H}_T)$ as the normal distribution by the following equation:\n$L_{nf} = \\frac{|| \\mathcal{Z} ||_2^2}{2} - \\sum_{t=1}^T log |det \\frac{\\partial H_t}{\\partial H_{t-1}}|$."}, {"title": "3.5 Target Network", "content": "In contrast to the source network, the target network employs a GIN architecture. Although they differ in structure, they maintain consistency in input and output. The target network similarly computes node embeddings $\\hat{H}_G$ based on the input $G = (A, X)$. To facilitate the subsequent computation of distances between the outputs of the two networks, it is essential to ensure that the dimensions of $\\hat{H}_G$ in the target network match those of the source network. Following the common practice in the field, we aim for consistency in both node-level and graph-level representations. We obtain the graph representation through the READOUT function, which can involve operations such as averaging, maximum pooling, or minimum pooling. In our work, we opt for maximum pooling to derive the graph representation. This process can be described as follows:\n$\\hat{h}_g = [max \\hat{h}_{i,1}, max \\hat{h}_{i,2},..., max \\hat{h}_{i,d}]$.\nThe target network is trained by minimizing the distance between its outputs and those of the source:\n$\\mathcal{L}_{graph} = \\frac{1}{m} \\sum_{G \\in \\mathbb{G}} f_a(\\hat{h}_G, z_G), \\quad \\mathcal{L}_{node} = \\frac{1}{m} \\sum_{G \\in \\mathbb{G}} \\frac{1}{n} (\\sum_{v_i \\in V_G} f_a(\\hat{h}_i, z_i))$\nwhere $m$ is the size of graph set and $n$ is the size of graph. $f_a$ represents an arbitrary vector distance calculation method. We jointly minimize the $\\mathcal{L}_{graph}$ and $\\mathcal{L}_{node}$ to learn normal graph pattern and feature distribution:\n$\\mathcal{L}_{target} = (1 - \\beta) \\mathcal{L}_{graph} + \\beta \\mathcal{L}_{node}$,\nwhere $\\beta$ is a tunable balancing factor to weigh the impact of $\\mathcal{L}_{graph}$ and $\\mathcal{L}_{node}$. In the inference stage, we employ the same strategy to estimate the anomaly level of each graph. The score for each graph is determined by:\n$Score = f_a(\\hat{h}_G, z_G) + \\frac{1}{n} (\\sum_{v_i \\in V_G} f_a(\\hat{h}_i, z_i))$\nThe underlying idea behind this computation method is that, after sufficient training, the source network captures the characteristic and distribution patterns of normal graphs, and this information is also acquired by the target network. Consequently, when the input $G$ is normal, the output of the target network closely aligns with that of the source network, leading to a score approaching 0. However, when the input $G$ is abnormal, the target network has not encountered such cases during training, resulting in increased disparity between the outputs of the two networks and hence a target score close to 1."}, {"title": "4 Experiments", "content": "We compare the proposed model with nine representative baselines. Under the non-end- to-end method, we focus on two categories: (i) kernel + detector. The Weisfeiler-Lehman kernel (WL) [30] and propagation kernel (PK) [31] are used to obtain representations. Then, we apply one-class SVM (OCSVM) [32] and isolation forest (iF) [33] to detect anomalies. This combination yields four baselines: PK-OCSVM, PK-iF, WL-OCSVM and WL-iF; (ii) GCL model + detector. We select two classic graph-level contrastive learning models, i.e., InfoGraph [34] and GraphCL [35], to obtain representations first. We then use iF as the detector to identify anomalies, resulting in InfoGraph-iF, GraphCL-iF. For the end-to-end method, we select three representative models: OCGIN [17], GLocalKD [2] and GOOD-D [10]."}, {"title": "4.2 Overall Performance", "content": "The AUC results of FANFOLD and nine baselines w.r.t AUC and Avg.Rank on 15 datasets are reported in Table 2. As shown in Table 2, our proposed FANFOLD achieves first place on 9 datasets, secured second place on 2 datasets, and demonstrates competitive performance on the remaining datasets. In addition, FANFOLD attains the highest average rank among all comparative methods across 15 datasets. Based on our observations, graph kernel-based methods show the weakest performance among tested baselines due to their limited capacity to detect regular patterns and crucial graph information, which hinders their effectiveness with complex datasets and leads to subpar results. GCL-based methods exhibit a moderate level of performance, indicating the competitive potential of graph contrastive learning for UGAD tasks. To conclude, the competitive performance of our proposed model demonstrates the necessity and superiority of implementing asymmetric network with normalizing flows for graph-level anomaly detection. Such results also prove that FANFOLD is intrinsically capable of capturing the core characteristics of normal graphs, thereby achieving superior anomaly detection performance."}, {"title": "4.3 Ablation Study", "content": "In this study, we conduct a comprehensive ablation study to explore the importance of the target network and source network structure in FANFOLD. The results are shown in Figure 3. To this end, we design the following variant models for comparative analysis: (1) Non-ST: it only retains the source network and performs anomaly detection through reconstruction loss. Non-ST performs relatively poor, with its average AUC results dropping by approximately 15% compared to other variants. This indicates that a single network has limitations in capturing complex features of the data and handling various data types. (2) Asy-ST: it employs a symmetric source-target network. This variant shows significant performance improvement, indicating that constructing a symmetric ST network enables the model to capture the diversity and complexity of the data more effectively. (3) Non-NF: it adopts an asymmetric ST network structure without introducing normalizing flows. Non-NF achieves about a 2% performance improvement on multiple datasets, demonstrating that an asymmetric structure can enhance the differences between the outputs of the source network and the target network, thereby improving model performance. To conclude, FANFOLD introduces normalizing flows into the source network, which results in about 10% performance improvement on the BZR and HSE datasets, which proves that normalizing flows allows the model to capture the essential characteristics of normal graphs."}, {"title": "4.4 Hyper-Parameter Analysis", "content": "We explore the impact of the balance factors a and \u1e9e as defined in Equations 2 and 9. a and \u1e9e are tuned in the range of 0 ~ 1 respectively, and the results are illustrated in Figure 5. As shown in Figure 5, different datasets have varying sensitivity to a and B. For the AIDS and COX2 datasets, different balance factors have a minimal impact on the AUC value, showing relatively stable performance. In contrast, for the HSE, IMDB-BINARY, and PPAR-gamma datasets, parameter changes have a significant impact on AUC values, resulting in substantial fluctuations. For these datasets, adjusting parameters a and \u1e9e can optimize the AUC performance, where the highest AUC values occur when a is between 0.6 to 0.8 and 8 is between 0.4 to 0.8."}, {"title": "4.5 Visualization", "content": "We utilize t-SNE for visualizing the graph embeddings learned by FANFOLD at different stages. The result is illustrated in Figure 6. After initial processing, the source network's embeddings show some separation between normal and anomalous graphs with noticeable overlap and broad distribution, indicating a partial but not substantial distinction between the sample types. Then, with normalizing flows, the separation between normal and anomalous samples becomes clearer and their distribution more concentrated. Also, it can be noticed that the distribution of anomalous samples becomes more scattered overall in target network embedding. This leads to a clearer distinction between the embedding of anomalous samples in the target network and the source network, ultimately achieving the goal of anomaly detection. In the anomalous evaluation stage, normal samples have lower anomaly scores, primarily concentrated around 0.5, while anomalous samples have higher anomaly scores, approaching 1. The experimental results demonstrate that the step-by-step processing through Source Network Embedding, Normalizing Flows, and Target Network Embedding progressively enhances the separation between samples, validating the effectiveness of the model in detecting anomalies."}, {"title": "5 Conclusions and Limitations", "content": "In this work, we propose a novel graph normalizing flows-driven asymmetric network for unsupervised graph-level anomaly detection, named FANFOLD. To the best of our knowledge, this is the first application of normalizing flows and asymmetric network in unsupervised graph-level anomaly detection. Specifically, we employ knowledge distillation techniques and apply normalizing flows to the source network. Such a method considers anomaly detection from the perspective of sample distribution and distribution density, capturing the essence of normal and anomalous graphs. In addition, the design of asymmetric network driven by normalizing flows helps to further distinguish the anomalous graphs from the normal ones. Extensive experiments on 15 datasets validate the superiority of FANFOLD. Considering limitations, we exercise strict control during the training phase to ensure that the training data does not contain any anomalous graphs. While it aligns with existing work, it diverges from real-world scenarios and incurs extra costs."}, {"title": "A Pseudo Code", "content": "Algorithm 1: FANFOLD\nInput : Graph set: $\\mathbb{G} = {G_1, G_2, ..., G_m}$;\nOutput: The anomaly scores for each graph Scoreg;\nInitialize : The trainable parameters $\\Theta$ and $\\theta$ for the Source Network, and $\\mathbb{\\check{O}}$ for the Target Network;\nTraining Phase\nfor i = 1 to s_epochs do\n| Compute node embeddings $\\hat{H}_G$ through GNN;\n| Perform a gradient descent step on Eq. 2 w.r.t. $\\Theta$;\nend\nfor i = 1 to n_epochs do\n| with fixed $\\Theta$:\n| Compute node embeddings $\\hat{H}_G$;\n| Transform the node embeddings $\\hat{H}_G$ to a latent vector $\\mathcal{Z}_G$ through Eq. 3;\n| Minimizing the loss Eq. 6;\nend\nfor i = 1 to t_epochs do\n| Compute node embeddings $\\hat{H}_G$;\n| with fixed $\\Theta$, $\\theta$:\n| Source-Network calculates $\\mathcal{Z}_G$;\n| Compute graph representations $h_G$ and $\\hat{h}_G$ through Eq. 7;\n| Perform a gradient descent step on Eq. 9 w.r.t. $\\mathbb{\\check{O}}$;\nend\nInference Phase\nfor $G_i$ in Graph set $\\mathbb{G}$ do\n| Caculate anomaly scores via Eq.10;\nend"}, {"title": "B Supplement of Experiments", "content": "Number of GCN Layers. We analyze the impact of GCN layers k in the source network, where k \u2208 1, 2, 3, 4, 5. The results are shown in Figure 7. Overall, most datasets achieve optimal results when the number of layers k is 2. Beyond this, model performance either plateaus or gradually declines. Deeper GCNs do not improve performance but are more computationally costly.\nDimensionality Size. To analyze the impact of dimensionality size of the source network and target network on model performance, we alter the hidden dimension from 2 to 512. The AUC result on six dataset is shown in Figure 8. For a portion of the dataset, such as AIDS and DD, FANFOLD's AUC rapidly increases with the source network to highest value at hidden dimension 4 ~ 32, then remains stable. At the same time, for these datasets, the model is relatively insensitive to changes in the dimensions of the target network, achieving optimal results around 8, 32, and 128. In the datasets COX2, IMDB-BINARY, and DHFR, the model is more sensitive to the hidden layer dimensions of the source network and Tthe target network. The overall trend shows that as the dimensions increase, the AUC fluctuates and rises until it reaches a peak, after which performance begins to decline. In most datasets, the source and target networks exhibit the highest AUC at specific dimensions, although the optimal dimensions vary. Additionally, different datasets have varying sensitivities to hidden dimensions, and selecting the appropriate hidden dimensions can significantly enhance the model's AUC performance."}]}