{"title": "Hybrid Heterogeneous Clusters Can Lower the Energy Consumption of LLM Inference Workloads", "authors": ["Grant Wilkins", "Srinivasan Keshav", "Richard Mortier"], "abstract": "Both the training and use of Large Language Models (LLMs) require large amounts of energy. Their increasing popularity, therefore, raises critical concerns regarding the energy efficiency and sustainability of data centers that host them. This paper addresses the challenge of reducing energy consumption in data centers running LLMs. We propose a hybrid data center model that uses a cost-based scheduling framework to dynamically allocate LLM tasks across hardware accelerators that differ in their energy efficiencies and computational capabilities. Specifically, our workload-aware strategy determines whether tasks are processed on energy-efficient processors or high-performance GPUs based on the number of input and output tokens in a query. Our analysis of a representative LLM dataset, finds that this hybrid strategy can reduce CPU+GPU energy consumption by 7.5% compared to a workload-unaware baseline.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) such as OpenAI's GPT-4 [24] and Google's PaLM [4] have become emblematic of the AI revolution, driving significant advancements not only in natural language un-derstanding, generation, and translation but also in summarizing and contextualizing large volumes of textual data. Characterized by their extensive scale and depth, their deployment demands substan-tial computational resources and hence poses significant challenges in terms of energy consumption and operational efficiency [38]. The increasing application of LLMs across diverse sectors further compounds these challenges, because datacenters, which are re-sponsible for a considerable portion of global electricity consump-tion, must balance performance targets for LLM tasks running on heterogeneous hardware with the need for energy efficiency [7, 21]. Reducing the energy efficiency of LLMs thus emerges as both a technical challenge and an environmental imperative [22].\nTraditional data center designs often struggle to best exploit the capabilities of heterogeneous hardware-based LLMs, particularly when trying to minimize energy consumption without sacrific-ing output quality and latency [6]. However, this challenge also presents an opportunity to innovate in datacenter architecture and management. We show that by rethinking how GPU resources are allocated and managed, there is potential to significantly reduce the energy footprint of LLM deployments while maintaining or even enhancing computational performance.\nWe find that a dynamic task-scheduling model that assigns LLM tasks to GPUs based on the resulting energy efficiency can reduce overall energy. Moreover, implementing a workload-aware system for input and output token processing can further reduce energy usage. Thus, a hybrid datacenter task allocation model, which al-locates different tasks to different hardware accelerators based on their system demands, can reduce the overall energy consumption of LLM inference compared to a workload-unaware baseline.\nOur contributions are as follows:\n(1) We analyze the energy consumption and runtime of several 7B-parameter LLMs' across various hardware configurations.\n(2) We propose and evaluate a workload-aware scheduler for LLMs that optimizes energy efficiency based on the size of input and output token loads, demonstrating a 7.5% decrease in energy consumption over non-workload-aware baselines.\n(3) We release a comprehensive dataset and benchmark suite for evaluating the energy efficiency of LLM inference, enabling researchers and practitioners to assess the impact of their design choices.\nThrough these contributions, we hope to support more sustain-able and cost-effective AI inference deployments.\nThe remainder of this paper is as follows: Section 2 provides background information on LLM inference and energy consump-tion in Al systems. Section 3 formulates the problem and introduces our cost function. Section 4 details the methods used for bench-marking LLM inference on diverse systems. Section 5 presents the performance results of LLM inference across multiple hardware con-figurations. Section 6 proposes and evaluates our energy-optimal hybrid data center design. Finally, Section 7 discusses related works, and Section 8 summarizes the conclusions of the paper."}, {"title": "2 BACKGROUND", "content": "Transformer-based neural network architectures have led to im-pressive gains in the performance of LLMs for language under-standing and generation [5]. LLMs such as OpenAI's GPT-4 [24] and Google's Gemini [32] have demonstrated human-level profi-ciency on many language benchmarks while requiring billions of parameters and massive datasets for training. The inference phase of LLMs involves utilizing a trained model to make predictions based on new, unseen data. Unlike the training phase, which is typically a one-time, compute-intensive process that occurs offline, inference is an ongoing, real-time process that directly impacts end-user experiences [7]. This phase is critical as it represents the point at which AI capabilities become accessible to users.\nInference in LLMs can be computationally expensive due to sev-eral factors: (1) Model Size: The sheer size of these models, often billions of parameters, necessitates significant computational power to process each query [38]. (2) Latency Expectations: Many appli-cations based on LLMs, such as digital assistants, automated writing aids, and real-time translators, require low-latency responses [35]. (3) Scalability: The ability to scale inference operations to accom-modate varying user demands without degradation in response times is crucial."}, {"title": "2.2 Energy Consumption in AI Systems", "content": "Recent reports have found that the computational requirements for state-of-the-art Al entail massive energy consumption and carbon emissions [7, 21, 26, 29, 38]. The energy intensity of AI systems can be broadly divided into the energy required for training versus inference after models are deployed [13]. Training complex models on massive datasets is an energy-intensive process, with estimates finding that training GPT-3 required 1,287 megawatt-hours of en-ergy [26]. LLMs can also have huge emissions depending on deploy-ment scale and hardware efficiency [29]. For example, over a year of use, inference by LLMs on cloud infrastructure can consume over 25\u00d7 more energy than training a model [7]. Optimizing software and hardware specifically for AI workloads is thus essential [3]."}, {"title": "2.3 Heterogeneous Systems for Efficient Computing", "content": "Modern systems demonstrate a complex interplay between scale, architecture, workload behavior and efficiency objectives. The ar-chitecture of compute nodes can significantly impact the energy efficiency and processing capabilities of large-scale computing sys-tems [18]. Conventional server architectures based on multicore CPUs face energy proportionality and scalability limitations for modern data-intensive workloads [20]. Several researchers have explored heterogeneous server configurations to improve energy ef-ficiency [12, 15, 16, 19]. Distributed solutions can translate to lower energy efficiency, as communication overheads dominate [9]. Still, specialized clusters like NVIDIA's DGX show 4x better performance per watt over conventional servers [30]."}, {"title": "3 PROBLEM FORMULATION", "content": "To model the operational demands of a hybrid, heterogeneous data-center hosting LLMs, we define a cost function to reflect the work-load distribution across different systems. We define a cost function \\(U(m, n, s)\\) that accounts for both energy consumption and runtime:\n\\[U(m, n, s) = \\lambda E(m, n, s) + (1 \u2013 \\lambda)R(m, n, s),\\]\nwhere \\(m\\) and \\(n\\) denote the number of input and output tokens, respectively. \\(\\lambda \\in [0, 1]\\) is a tunable parameter that balances the weight of energy efficiency versus speed. \\(E(m, n, s)\\) is the energy consumed by system \\(s\\) to process \\(m\\) input tokens and generate \\(n\\) output tokens, measured in joules. \\(R(m, n, s)\\) is the time required to process these tokens on system \\(s\\), measured in seconds.\nOur objective is to minimize the total cost across all tasks and systems:\n\\[\\min_{{Q_s}_{s\\in S}} \\sum_{s \\in S} \\sum_{(m, n) \\in Q_s} U(m, n, s)\\]\ns.t. \\(\\cup_{s\\in S} Q_s = Q\\)\n\\( \\forall s: Q_s \\cap Q_{s'} = \\emptyset \\forall s \\ne s'\\)\nwhere \\(S\\) is the set of all systems, \\(Q\\) is the total set of queries, \\(Q_s\\) is the subset of queries assigned to system \\(s\\).\nThis model ensures that each query is processed exactly once, optimizing for energy efficiency or quick response times, depending on the operational needs, as parameterized by \\(\\lambda\\). We note, however, that certain systems may be better suited to specific tasks, based on the workload characteristics, such as the need for rapid response times. Adjustments in \\(\\lambda\\) allow the datacenter to shift its focus be-tween minimizing energy consumption and reducing runtime as operational priorities change."}, {"title": "4 METHODS", "content": "Here, we describe the methods and tools we use to benchmark LLM inference. In all cases, we use Huggingface's Accelerate [11] to stan-dardize hardware optimization for inference across all platforms. T his library takes advantage of the available accelerator resources and shards models accordingly to minimize intermediate commu-nication and maximize the distributed capabilities for computation across the devices."}, {"title": "4.1 Model Selection", "content": "Our study employs three open-source LLMs for their capabilities and ability to run on diverse hardware efficiently: (1) Falcon [2], (2) Llama-2 [33], and (3) Mistral (7B parameters) [17]. These models were selected to represent a spectrum of architectures and training corpora. We subject each model to a series of standardized NLP tasks to evaluate their energy consumption during inference."}, {"title": "4.1.1 Falcon", "content": "The Falcon (7B) [2] model utilizes multi-query atten-tion, significantly reducing memory requirements and increasing processing speed. The model's training on the bilingual RefinedWeb dataset enhances its applicability across diverse linguistic contexts."}, {"title": "4.1.2 Llama-2", "content": "We select Llama-2 (7B) for its optimization in di-alogue tasks and its improvements in safety and helpfulness. The"}, {"title": "4.2 Energy Profiling of Diverse Systems", "content": "Depending on the platform, we profile each system's energy con-sumption during inference using customized setups that capture runtime and energy or power metrics. Here, we describe how we monitor the energy usage of NVIDIA GPUs, Apple Silicon CPU/GPU, Intel CPUs, and AMD CPUs."}, {"title": "4.2.1 NVIDIA GPUs", "content": "We use PyJoules [27], a Python-based en-ergy measurement library, to quantify the energy consumption associated with inference on NVIDIA GPUs. PyJoules provides an interface to NVML [23], providing a software-defined energy usage assessment for targeted NVIDIA devices. This tool offers real-time energy consumption of GPUs for a given tracked process, which is a critical component of our analysis given the GPU-heavy compu-tation involved in LLM inference."}, {"title": "4.2.2 Apple Silicon CPU/GPU", "content": "No standard energy measurement tools are available for profiling energy and power usage for Ap-ple Silicon through an API like PyJoules or RAPL. Therefore, we employ a daemon-based approach to poll macOS' powermetrics utility, providing a detailed view of the energy usage during model inference. To capture the energy consumption of the M1 GPU, we execute the powermetrics command through a Python subprocess. This command returns the percentage of the CPU power each CPU top process uses and the total CPU and GPU power consumption in 200ms intervals. This interval was chosen after testing to find the finest granularity measurement without incurring a significant CPU overhead for the I/O of buffering the large powermetrics output into memory.\nThe energy monitoring is conducted concurrently with the LLM inference. A separate thread is dedicated to running the powermetrics command, ensuring real-time data collection. Post-inference, the collected data is processed to extract the recorded power data and then find the energy consumption through integration over the runtime. The GPU energy consumption, \\(E_{Total,GPU}\\), is straightfor-ward to calculate for each recorded power value, \\(P_{GPU,i}\\), at each timestep \\(\\Delta t_i\\).\n\\[E_{Total,GPU} = \\sum_i P_{GPU,i} \\Delta t_i.\\]\nThe CPU power draw data is less clear, as many processes run on the CPU. However, an \"energy impact factor\" through powermetrics allows us to infer how much power our Python inference process uses. Therefore, we calculate the CPU energy, \\(E_{Total,CPU}\\), by mul-tiplying \\(P_{CPU,i}\\) by the \"energy impact factor,\" which we denote as \\(\\alpha_i\\), at each timestep:\n\\[E_{Total,CPU} = \\sum_i (\\alpha_i P_{CPU,i}) \\Delta t_i.\\]"}, {"title": "4.2.3 Intel CPUs", "content": "For Intel CPUs, we leverage PyJoules, a Python-based energy measurement library similar to our approach for NVIDIA GPUs. This tool supports RAPL (Running Average Power Limit) interfaces, enabling us to obtain fine-grained energy con-sumption data [36]. We focus on two primary RAPL domains: Pack-age 0 and Package 1, which correspond to the entire CPU package's energy consumption, including all cores in the package.\nPyJoules allows us to capture the energy usage of these domains in real time, enabling us to profile the energy consumption specif-ically during model inference tasks. To account for base energy consumption unrelated to our inference process, we conduct a pre-analysis phase to measure the CPU's average idle power draw. This idle measurement is then subtracted from the total energy con-sumption during inference to accurately determine the net energy expenditure attributable to the inference process.\nWe instrument our code to query the RAPL readings at the start and end of the inference task, calculating the energy consumption as follows:\n\\[\\begin{aligned}E_{Total,CPU} &= \\sum_i \\left((P_{Package-0,i} - P_{Package-0,Idle}) \\right. \\\\\n&+ \\left.(P_{Package-1,i} - P_{Package-1,Idle} ) \\right) \\Delta t_i,\\end{aligned}\\]\nwhere \\(P_{Package-0,i}\\) and \\(P_{Package-1,i}\\) represent the power draw from Package 0 and Package 1, respectively, and \\(P_{Package-0,Idle}\\) and \\(P_{Package-1,Idle}\\) represent the average idle power draw of the CPU packages, respectively."}, {"title": "4.2.4 AMD CPUs", "content": "We adopt a different strategy for AMD CPUs due to the absence of a Python API. Instead, we utilize AMD\u00b5Prof's timechart feature, which provides detailed power draw metrics for every core on the chip at fine-grained intervals. By polling AMD\u00b5Prof at 100ms intervals, we can capture the power draw of each physical core throughout the model inference process.\nTo ensure we accurately attribute the energy consumption to our inference task, we monitor the CPU core residency through psutil. This information allows us to identify and record the specific cores actively engaged in the inference process at each time step. The total energy consumption for the inference task is then calculated by summing the power usage across all active cores and summing over the product of the power usage and time of inference, as follows:\n\\[E_{Total, CPU} = \\sum_{core} \\sum_i P_{core,i} \\Delta t_i\\]\nwhere \\(P_{core,i}\\) represents the power draw of an individual core at each time step, \\(i\\)."}, {"title": "5 LLM INFERENCE PERFORMANCE ON DIVERSE CLUSTERS", "content": ""}, {"title": "5.1 Hardware and Software Versions", "content": "The systems we profile are shown in Table 1. We consider these sys-tems as they demonstrate three prominent CPU manufactures and different generations of GPUs. We utilize PyTorch v2.0.1, Torchvi-sion v0.15.2, Numpy v1.26.0, Huggingface v0.20.2, and Accelerate v0.26.1."}, {"title": "5.2 Experimental Strategy", "content": "To comprehensively evaluate the performance of different system configurations across various models, we conducted a series of controlled experiments. We systematically varied the number of input and output tokens to measure their effects on runtime and energy consumption under two main experimental conditions. In each experiment we do not allow for key-value caches to be re-used to ensure our testing environment is standardized."}, {"title": "5.2.1 Vary Input Tokens", "content": "For the first experimental condition, we executed inference requests with increasing input token sizes, rang-ing from 8 to 2048 tokens, while maintaining a fixed output token size of 32. This setup allowed us to isolate the impact of input size on the system's performance and energy efficiency."}, {"title": "5.2.2 Vary Output Tokens", "content": "In the second set of experiments, we varied the output token limit from 8 to 4096 tokens, keeping the input token size constant at 32. This approach helped us understand how increasing output demands affect the runtime and energy consumption of the systems tested."}, {"title": "5.2.3 Randomization and Stopping Criteria", "content": "Each experiment was conducted in a randomized order to mitigate any potential bias introduced by the sequence of tests. To ensure the reliability of our results, we adhered to strict criteria for statistical confidence. Each configuration was tested repeatedly until either of two conditions was met: (1) The measured runtime had to be within 0.5 seconds of the actual mean runtime with 95% confidence. (2) A maximum of 25 trials were conducted for each setting if the first condition could not be met."}, {"title": "5.3 Input Token Analysis", "content": "Here, we present the impacts on runtime, energy consumption per token, and throughput for LLMs across different hardware config-urations while varying the number of input tokens. We perform these experiments using the suite of systems outlined in Table 1 with the models outlined in Section 4.1. In our experiments on the Palmetto Intel+V100 system, the V100 GPU had an out-of-memory error beyond 1024 output tokens for Falcon (7B).\nOur runtime measurements show a significant increase as in-put tokens grow. As depicted in Figure 1(a), all systems exhibit a nonlinear escalation in runtime with increasing token counts, with the M1-Pro system showing the most significant magnitude. This trend highlights the computational burden imposed by larger input sizes, particularly on smaller systems that are not as well designed to handle extensive workloads.\nFor all systems, we notice that throughput follows a \"roofline model\" with increasing input tokens [37]. Figure 1(b) illustrates these dynamics, indicating an increase in throughput for all systems until a certain point where inference becomes bound by compute and not by the overhead of the software, as described by roofline performance models [37].\nEnergy efficiency varies markedly across different systems. The M1-Pro demonstrates consistently low energy consumption per to-ken, particularly for smaller input sizes, as shown in Figure 1(c). This efficiency reflects the M1-Pro's design optimization for low-power operations. In contrast, the Swing AMD+A100, while capable of handling more significant token inputs more efficiently, consumed more energy per token for small workloads yet became more en-ergy efficient at larger input token sizes, underscoring a trade-off between workload size and energy efficiency."}, {"title": "5.4 Output Token Analysis", "content": "Here we examine the performance trends associated with increasing the number of output tokens for our LLMs and systems of interest, specifically focusing on runtime, energy consumption per token, and throughput. In our experiments, the M1-Pro also could not generate more than 512 output tokens without significant runtime penalties. For the Palmetto Intel+V100 system, the V100 GPU had an OOM error beyond 1024 output tokens for Falcon (7B) and for all models beyond 2048 tokens.\nRuntime significantly increases with the number of output to-kens across all systems. As illustrated in Figure 2(a), the escala-tion in runtime is pronounced, particularly as the output token count reaches higher magnitudes. This increase is indicative of the substantial computational effort required by LLMs to generate successive tokens.\nIn Figure 2(b), we observe a decrease in throughput across all systems as the number of output tokens increases. This trend high-lights the inherent computational complexity involved in generat-ing larger sequences of tokens in LLM tasks. As the output token count grows, the system must process each additional token, re-calculating the context and updating internal model states [34]. This not only increases the total computation per query but also leads to a greater accumulation of processing time per token, which consequently lowers the overall throughput.\nEnergy consumption per token also shows an increasing trend as the number of output tokens grows. Displayed in Figure 2(c), this trend underscores the energy-intensive nature of producing larger outputs. Systems such as the M1-Pro, while generally more energy-efficient, begin to consume more energy per token as output"}, {"title": "5.5 Comparing the Input and Output Analyses", "content": "When comparing Figure 1(a) and Figure 2(a), we observe that in-creases in the number of output tokens result in a more considerable increase in runtime than increases in input tokens. The computa-tional complexity of processing input tokens primarily involves encoding the input context, which occurs once per input sequence and follows a more linear computational trajectory. In contrast, generating output tokens is inherently more complex and iterative. Each new output token requires the model to run through all its layers to predict the next token based on an ever-expanding context, which includes both the initial input and all previously generated tokens [34]. This ongoing computation involves recalculating atten-tion across an increasing number of tokens, updating hidden states, and generating a probability distribution over the vocabulary for each new token. Consequently, as the number of output tokens grows, the computational load increases significantly, leading to more significant runtime increases than processing input tokens.\nThe impacts on runtime also translate to the throughput, de-picted in Figure 1(b) and Figure 2(b). There is a noticeable decline in throughput as output tokens increase, more so than input to-kens. The decrease in throughput for output tokens is primarily due to the heightened computational requirements for generating subsequent tokens, where each token's generation slows down as the sequence lengthens. Furthermore, the energy per token also increases as output tokens grow, as shown in our analysis. The energy required to generate each output token becomes significant due to longer passes through the transformer network. We contrast this with the energy consumption when processing input tokens, which, despite increasing, does so at a less steep rate."}, {"title": "6 ENERGY-OPTIMAL HYBRID DATACENTER FOR LLM INFERENCE", "content": "Considering the performance results we collect from LLM inference across multiple systems, we notice that there is an energy-optimal way to construct a hybrid datacenter with a combination of M1 Pro's and A100s. The intuition behind this is that the energy expended per token for the M1 Pro is lower than that of the A100 up to a certain point in the number of input and output tokens as seen in Figures 1(c) and 2(c). However, the energy efficiency characteristics are different when varying the number of input and output tokens, and therefore, we will proceed with separate analyses."}, {"title": "6.1 Number of Input Tokens Analysis", "content": "Suppose we have a hybrid data center with M1-Pros and A100s. Then, we have some workload for an LLM, a set of queries with some outputs. In such a configuration, we implement a scheduling heuristic based on a cutoff threshold, \\(T_{in}\\), for input token length. This heuristic dictates that queries with \\(n \\le T_{in}\\) tokens are pro-cessed on M1 Pro systems, which we have shown have good energy efficiency with handling smaller computational loads. Conversely, queries with \\(n > T_{in}\\) tokens leverage the greater computational abil-ity of A100 GPUs, which offer greater energy-per-token advantages for larger tasks despite their higher power usage. We point out that this is the same method mentioned in the problem formulation in Eqn. 2, where our queries \\(Q\\) are partitioned into \\(Q_{M1}\\) and \\(Q_{A100}\\) strictly on input and output size.\nTo find an optimal threshold \\(T_{in}\\) empirically, we analyze the to-ken distribution in prompts from the Alpaca [31] dataset, a bench-mark dataset frequently used in model fine-tuning. This dataset comprises 52K prompts, offering a diverse range of lengths akin to a typical workload in systems like GPT-4 [24]. The distribution of input tokens, visualized in our analysis (see Fig. 3(a)), serves as a proxy for understanding the variegated nature of LLM workloads.\nThe energy component of our cost function, split over the token threshold, is as follows:\n\\[\\begin{aligned}E_{Total,in} = &\\sum_{m=1}^{T_{in}} m f_{in}(m) E_{M1,in} (m) + \\\\\n&\\sum_{m=T_{in}+1}^{M} m f_{in}(m) E_{A100,in} (m),\n\\end{aligned}\\]\nwhere \\(E_{Total,in}\\) represents the total energy consumption for a given dataset of input lengths \\(m\\) with corresponding frequencies \\(f_{in}(m)\\), and \\(E_{M1,in} (m)\\) and \\(E_{A100,in} (m)\\) denote the mean energy per token for varying the input token size for the M1-Pro and A100 systems, respectively. Utilizing this model with our dataset enables the ap-proximation of total energy consumption for various threshold settings, offering insights into the energy dynamics of hybrid dat-acenter operation. In Figure 4, we show the energy and runtime"}, {"title": "6.2 Number of Output Tokens Analysis", "content": "We want to use the same scheduling heuristic and performance model to determine a threshold \\(T_{out}\\) for the number of output tokens. Except this time, we have different frequencies \\(f_{out} (n)\\) for the \\(n\\) output tokens and different mean energy per token for varying the output token size, \\(E_{M1,out} (n)\\) and \\(E_{A100,out} (n)\\). We also utilize the distribution of the number of output tokens in the Alpaca dataset (see Fig. 3(b)). We revise our performance model as follows:\n\\[\\begin{aligned}E_{Total,out} = &\\sum_{n=1}^{T_{out}} n f_{out} (n) E_{M1,out} (n) \\\\\n&+ \\sum_{n=T_{out}+1}^{N} n f_{out} (n) E_{A100, out} (n).\n\\end{aligned}\\]"}, {"title": "6.3 Balancing Energy Efficiency and Runtime Performance", "content": "Our analysis of both input and output token processing within a hybrid, heterogeneous datacenter framework has led to the identifi-cation that with certain thresholds at \\(T_{input} = 32\\) and \\(T_{output} = 32\\), we can strategically allocate tasks to M1 Pro systems or A100 GPUs based on token count, optimizing for energy efficiency.\nShifting the token distribution leverages the M1 Pro's superior energy efficiency for input and output tasks up to the threshold, beyond which we utilize the A100's computational power. This policy saves energy as smaller-token tasks are handled by the more efficient M1 Pro for outputs up to the threshold. However, this energy optimization comes at the expense of increased runtime, which is particularly noticeable in output token generation where the M1 Pro, despite its efficiency, does not match the A100's speed.\nThe energy-runtime trade-off presents a favorable scenario for applications that have low runtime sensitivity. For instance, batch processing of LLM tasks, such as overnight data analyses or non-time-critical computations, can benefit significantly from this energy-efficient configuration. Similarly, free or not directly monetized services, where the cost of computation impacts operational sus-tainability, stand to gain from minimizing energy expenditures even at the cost of longer processing times.\nThis approach also opens discussions on Quality of Service (QoS) for LLMs, an area that still needs to be explored [1, 35]. Traditional QoS metrics often prioritize speed and reliability, but energy effi-ciency may also become a critical QoS dimension for LLM applica-tions, particularly in energy-constrained or cost-sensitive scenarios."}, {"title": "7 RELATED WORK", "content": ""}, {"title": "7.1 Hybrid and Energy Efficient Heterogeneous Data Centers", "content": "Recent studies in optimizing data center architectures for deep learn-ing have highlighted the necessity of energy-efficient scheduling and task allocation across diverse hardware. Gu et al. [10] explore GPU clusters' energy-efficient scheduling, revealing substantial im-provements in power utilization without considering diverse GPU types for different task requirements. This work highlights a gap in understanding how various GPU configurations could enhance energy efficiency further. Similarly, Patel et al. [25] demonstrate the benefits of hybrid computing environments, emphasizing FPGA over GPU diversity. This focus leaves room to explore the specific impacts of different GPU classes in such settings.\nIn the realm of LLMs, Zhao et al. [39] introduce strategies like phase-aware partitioning and adaptive quantization in heteroge-neous clusters but do not integrate energy considerations into their analysis, which is crucial for understanding the real-world appli-cability of these models in power-sensitive environments. On the other hand, Radovanovi\u0107 et al. [28] and Chien et al. [7] discuss broader aspects of carbon-aware computing and reducing the car-bon impact of Al inference, respectively. These works emphasize the importance of node/device-level energy metrics, often over-looked in typical LLM deployment strategies, thus underscoring the need for detailed energy consumption profiling across different models and hardware types."}, {"title": "7.2 LLM Inference as a Service", "content": "Further focusing on energy consumption, Hu et al. [14] analyze deep learning workloads in GPU datacenters, offering insights into energy conservation strategies through workload scheduling. This research aligns with our objectives by confirming the critical role of scheduling in reducing energy footprints. Anderson et al. [3] propose carbon-aware datacenter software that could complement physical hardware adjustments by making energy and carbon met-rics visible to application developers, encouraging more energy-efficient coding practices.\nAddressing service quality, Wang et al. [35] study the efficiency and reliability of LLM serving, highlighting the challenges of main-taining high-quality service while managing computational loads effectively. This perspective is pertinent as it underscores the trade-off between performance and energy efficiency, which is central to our study. Lastly, Desislavov et al. [8] provide a timely examination of trends in Al inference energy consumption, arguing that while performance has increased dramatically, energy consumption has not escalated at the same pace, thanks to hardware optimizations and algorithmic innovations. This outlook is necessary as it sug-gests the potential for further optimizations in LLM inference tasks, which are typically energy-intensive."}, {"title": "8 CONCLUSIONS", "content": "By carefully analyzing the energy and runtime of heterogeneous compute hardware to host LLMs, we show that a hybrid, hetero-geneous datacenter and a cost-based scheduling framework can allocate LLM tasks to accelerators that are best suited to run them,"}]}