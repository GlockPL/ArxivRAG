{"title": "Integrating Evidence into the Design of XAI and AI-based Decision Support Systems: A Means-End Framework for End-users in Construction", "authors": ["Peter E.D. Love", "Jane Matthews", "Weili Fang", "Hadi Mahamivanan"], "abstract": "Explainable artificial intelligence (XAI) aims to develop instruments to understand the 'black box' workings of AI models. While XAI is attracting interest in the construction and engineering management literature, limited attention has been placed on incorporating 'evidence' to support its instruments and AI-based decision support systems (DSSs). Without integrating evidence into the design of XAI and DSSs, the reliability of outputs generated is open to questioning. In recognition of this problem, this paper uses a narrative review to develop a theoretical evidence-based means-end framework to build an epistemic foundation to uphold XAI instruments so that the reliability of outcomes generated from DSSs can be assured and better explained to end-users. The implications of adopting an evidence-based approach to designing DSSs in construction are discussed with emphasis placed on evaluating the strength, value, and utility of evidence needed to develop meaningful human explanations (MHE) for end-users. While the developed means-end framework is focused on end-users, stakeholders can also utilize it to create MHE, though they will vary due to their different epistemic goals. Including evidence in the design and development of XAI and DSSs will improve decision-making effectiveness, enabling end-users' epistemic goals to be achieved. As the proposed means-end framework is developed from a broad spectrum of literature, it is suggested that it can be used not only in construction but also in other engineering domains where there is a need to integrate evidence into the design of XAI and DSSs.", "sections": [{"title": "I INTRODUCTION", "content": "Artificial intelligence (AI) is having a profound influence on all aspects of work, and with advances being made at an accelerated rate, a myriad of opportunities (e.g., task automation, predictive analysis, and real-time decision-making) and challenges (e.g., job displacement, data bias and algorithmic fairness and data privacy and security) manifest [1]. Indeed, AI is a transformative technology that is reshaping the workplaces of many industrial sectors, such as finance, healthcare, manufacturing, and transport [1-3]. Still, within construction, the rate of AI adoption has been cautious despite its potential benefits and widespread uptake in other sectors [1], [2]. While it is also acknowledged that AI can improve decision-making by quickly dealing with and processing large amounts of information, some construction organization managers are mindful of the risk of automation bias [2], [3], [4]. In this instance, when making judgments and decisions, people adopt a 'satisficing' behavior' tending to over-rely on automated systems and technologies when making decisions, often leading to errors or suboptimal outcomes [3],[5]. This bias occurs when people place excessive trust in automated tools, such as Al algorithms and decision support systems (DSS), to the extent that they may ignore or undervalue their judgment, even when the automated system might not be entirely reliable or appropriate [3]. The hesitancy in construction organizations' adoption of AI is not due to their reluctance; instead, they need an understanding and knowledge about what, why, how, and when benefits can be realized without compromising their operations [2]. Adding to the mix, many AI-based DSS and business (decision) intelligence applications (collectively referred to as DSS\u00b9) that"}, {"title": "", "content": "1 In this paper a DSS is a computer-based system leverages AI technologies to assist decision-makers in analyzing data, evaluating scenarios, and making informed decisions. A DSS that incorporates Al results in automating tasks, learning from data, and providing predictive and"}, {"title": "", "content": "have been developed-bespoke or off-the-shelf-in construction are unable to explain their autonomous decisions and actions to human users as they are reliant on \u2018black box' data-driven AI algorithms that produce useful information without revealing any information about their internal workings [6], [7]. As a result, eXplainable AI (XAI) has become essential for enabling human users to understand and trust the outputs that DSSs provide, especially those enabled by generative AI models (e.g., Generative Adversarial Networks, Variational Auto-encoders, and Reinforcement Learning and Transformer-based models) where data-driven content and scenarios provide solutions for a wide variety of tasks [8-13]. In short, an XAI instrument aims to make its behavior more intelligible to humans by providing explanations so that end-users can comprehend and trust the outputs generated by AI algorithms [6]. An in-depth review of XAI in construction specifically examining its precepts (i.e., explainability and interpretability), how it relates to different types of Al models (e.g., transparent and opaque), and levels of post-hoc explainability (e.g., model-agnostic and model-specific) can be found in Love et al. [6]. However, the review of Love et al. [6] is technologically driven, paying limited attention to user-centric design practices based on 'evidence' and evaluation techniques of XAI instruments, which have been identified as critical for designing effective DSSs [2], [11-13]. Likewise, this has been the case in several reviews of XAI conducted in other fields [14-22]. As XAI evolves, it has become increasingly acknowledged that the design of instruments is often based on poor quality evidence (e.g., to support their intuitive decision-making), the presence of confounding variables, or bias training and test data (e.g., overfitting), which can"}, {"title": "", "content": "prescriptive insights. Business intelligence is a subset of DSS. Here decision intelligence seeks aims to help automate decision-making using AI. The goal of decision intelligence is to design, model, align, execute, monitor, and tune decision models and processes. Off-the-shelf examples of DSSs available in construction are Autodesk's Construction IQ, Kwant.ai and Trackunit IrisX to name a few."}, {"title": "", "content": "impact the interpretation of DSSs [9], [10], [23], [24-26]. A case in point is reported by DeGrave et al. [27], who demonstrated that deep learning models applied to detect COVID-19 from chest radiographs relied on \u201cconfounding factors rather than medical pathology, creating an alarming situation in which the systems [models] appear accurate, but fail when tested in new hospitals\" [p.610]. This finding reinforces the risk of relying solely on evidence from association studies (i.e., statistical associations), where confounder factors are present, to make causal claims [28]. Association studies alone are insufficient to provide reliable explanations of outcomes generated from Al models [28]. Dur\u00e1n [26] cogently points out that no single scientific explanation can be used to characterize and provide an understanding of the empirical world, suggesting that explanatory pluralism is required to inform decision-making. As XAI gains traction in construction, it can be observed that the strength of evidence is generally weak (e.g., relying upon expert beliefs and case study demonstrations), rendering the proposed explanations obtained from an Al model questionable [29-47]. With construction organizations at the cusp of the AI adoption technology cycle and XAI beginning to receive attention, albeit at a slow pace compared to other fields (e.g., law and medicine), an opportunity exists to take heed of contemporary thinking and learn from their insights and experiences [26-28], [48]. Thus, if construction organizations are to take advantage of the benefits of AI, DSSs must be interpretable and transparent ab initio in their design so that trustworthy and ethical solutions can be generated [2], [6], [49]. Against this backdrop, this paper aims to address the following research question: How can evidence effectively inform the design and development of XAI and DSS systems to ensure their"}, {"title": "", "content": "generated outcomes are explainable and that the epistemic needs of an end-user [construction organization] are met? A narrative review is used to tackle this question and develop a robust theoretical framework to ensure end-users are provided meaningful human explanations (MHE) for the outputs produced from a DSS. An MHE aims to provide a clear and succinct description of the processes used to explain 'why' and \u2018how' an output is derived. The paper begins by explaining and justifying the research approach adopted (Section II). The concept of XAI is briefly examined to establish the paper's setting. However, a detailed synthesis within the context of construction can be found in Love et al. [2] [6] and Naser [38] (Section III). A means-end framework is then introduced based on emergent developments of XAI from an extensive range of literature (e.g., computer science, philosophy, and medicine) to provide a structure for effectively utilizing evidence to design DSSs. Hence, its outputs are explainable, and end-user decision-making is enhanced (Section IV). The introduction of the means-end approach provides a segue to examine the nature of evidence required to support the design and development of XAI and DSSs (Section V). Then, the implications of a means- end approach to support the design of DSSs in construction are discussed (Section VI) before presenting the paper's conclusions and limitations and identifying avenues for future research (Section VII)."}, {"title": "II RESEARCH APPROACH", "content": "A literature review aims to provide a comprehensive, critical, and cohesive understanding of existing research on a topic; it is a valuable approach to knowledge production and keeping abreast of developments within a field [50]. Hence, the review presented in this paper aims to create a firm foundation for advancing knowledge and facilitating the development of evidence-based XAI theory and the design of DSSs in construction [51]. While the computer"}, {"title": "", "content": "science and information systems literature, for example, are embracing the need for evidence to be incorporated into the design of more effective DSSs and provide understandable explanations of Al-generated outputs, the construction and engineering management (CEM) literature remains silent on the matter [9], [10]. Reviews can take various forms, such as bibliographic, critical, integrative, scoping, and state- of-the-art [50]. The type selected will vary depending on the research context, objectives, and the specific aims of a study. Systematic reviews and meta-analyses, for example, are commonly used to examine AI-based applications and technology that have received widespread attention in the CEM literature [52-56]. Structured and rigorous reviews can be performed using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology-guidelines designed to help researchers improve transparency, quality, and reproducibility [57]. Indeed, systematic reviews are valuable tools for synthesizing research evidence but have limitations. They are time-consuming to perform, overly reliant on the quality of included studies, prone to publication bias, and unable to address multifaceted issues simultaneously, resulting in oversimplified conclusions necessitating cautious interpretation [58]. In sum, systematic reviews require a vast body of work on a specific topic to derive meaningful conclusions."}, {"title": "A. Narrative Review", "content": "With a limited body of work on XAI in the CEM literature and evidence not yet being used to support explanations and the design of DSS, a systematic review is inappropriate to address this paper's research question. However, a narrative review is better suited for under- researched areas and where new insights or ways of thinking are required [59]. Narrative"}, {"title": "", "content": "reviews are suitable for synthesizing a broad body of research for a complex issue where a detailed, nuanced description and interpretation are needed to advance new ideas, as will be presented in this paper [60]. While there are no consensus reporting guidelines for conducting narrative reviews, as there are with systematic reviews (e.g., PRIMSA), the research process adopted is outlined in Figure 1 [58-60]. The initial search focused on examining the content of XAI reviews and applications to garner an understanding of developments and research gaps. The research question provided the guardrail for developing the search strategy using Scopus\u2122, Google Scholar, Web of Science\u2122, the Cornell University open access repository of preprints and postprints (i.e., arXiv), and the table of content searches in journals. The search process was iterative, with additional keywords added as part of a trial-and-error approach to selecting appropriate articles (including those from mainstream computer science conferences) for inclusion in the review. Numerous books from established publishers are also utilized to reinforce and provide supplementary evidence to support our line of inquiry, which covers a range of fields and journals (Figure 1):\n\u2022 Construction engineering and management (e.g., Advances in Engineering Software, Automation in Construction, AIC, ASCE Journal of Management in Engineering, ASCE Journal of Computing in Civil Engineering, ASCE CIE, Reliability and Engineering Safety, RESS, and IEEE Transactions on Engineering Management, IEEE TEM);\n\u2022 Computer science (e.g., Artificial Intelligence, AI, Advanced Engineering Informatics, AEI, Artificial Intelligence Review, Information Fusion, Nature Machine Intelligence,"}, {"title": "", "content": "SIAM Journal on Computing, International Journal of Human-Computer Studies and the Proceedings of the ACM on Human-Computer Interaction)\n\u2022 Philosophy (e.g., International Studies in the Philosophy of Science, Phil. of Science, Studies in History and Philosophy of Science, Synthese, and the Stanford Encyclopedia of Philosophy) and\n\u2022 Medicine (e.g., BMC Public Health, Computers in Biology and Medicine, European Journal of Clinical Investigation, Journal of Clinical Epidemiology, and PLoS Medicine). The authors of this paper comprise acknowledged experts from a variety of fields, civil engineering, computer science, digital engineering, information systems, and management, which was subsequently drawn upon to determine if an article was to be included in the review based on the research question identified in Figure 1. The authors were reflexive during the selection of articles, striving to maintain a neutral standing when deciding their inclusion in the review and the development of the propagated means-end framework. Furthermore, including multiple authors with diverse backgrounds helps bring balance to synthesizing the literature."}, {"title": "III EXPLAINABLE ARTIFICIAL INTELLIGENCE", "content": "Broadly, Al models can be classified as being [6]: (1) transparent (e.g., linear regression, logistic regression, decision trees, and K-Nearest Neighbours, K-NN), which are easy to understand and interpret without requiring additional tools and techniques. The structure and operations of models are relatively straightforward to understand, and predictions made by a model can be traced to input features, enabling them to be understood; and (2) opaque (e.g., Support Vector Machine, SVM, Random Forest, RF, Recurrent Neural Networks, RNN and Convolutional Neural Networks, CNN), which typically have complex structures that render them difficult to understand and thus determine how decisions are derived. While opaque models are more accurate and perform better on complex tasks than transparent AI models, their internal workings are not readily interpretable as they are a 'black box'. The concept of XAI is concerned with developing techniques to understand the \u2018black box' workings of AI models [6], [8], [10], [11-23]. Thus, XAI aims to transform AI models (including its subset of machine learning) from untrustworthy \u2018black boxes' into transparent, understandable, and trustworthy tools. An array of XAI instruments have been developed to (partly) eliminate the perceived opacity of Al models [15]. As opaque Al models lack inherent interpretability, post-hoc methods generate explanations (e.g., feature important scores, saliency maps, and SHapley Additive exPlanations, SHAP). Two of the most well-known and widely used XAI instruments in construction are [6], [14], [15], [61], [62-65]:\n1.\nLocal interpretable model-agnostic explanations (LIME): For example, Leuthe et al. [32] leverage XAI to inform end-users (i.e., property owners) how AI models predict building energy consumption using LIME to explain how multiple features impact its performance. In a similar vein, Thisovithan et al. [46] utilize machine learning models to"}, {"title": "", "content": "predict the fundamental period of masonry infill in reinforced concrete (RC) framed structures using both LIME and SHAP to visualize and provide an understanding and explanation of why an RF was the \u2018best fit' model; and\n2.\nCounterfactual explanations (CFE): For example, Zhan et al. [40] applied an XAI approach using counterfactual explanations to help understand decision-making surrounding risks when considering control ground settlement during tunnel construction. Taking a different approach, Naser and \u00c7ift\u00e7iogu [64] examine the influence of causal discovery and inference to evaluate the fire resistance of RC columns. They demonstrate that including algorithmic causal discovery in machine learning models improves the interpretability of results. In the case of LIME, explanations are local, explaining predictions for only a specific part of the data rather than the global behavior of a model. For example, a model might be approximated locally by a match with an intuitive linear function (e.g., regression algorithms such as Lasso and Ridge), rendering it interpretable as its behavior can be understood [24], [66]. As LIME is model agnostic, it can provide the outputs for several AI methods [67]. However, Garreau and von Luxburg [68] point out that data perturbations and poor choice in model parameters can result in LIME's missing important features, resulting in generated explanations being subject to variability. Akin to LIME, CFEs \u2013 an explanation providing a link between what could have happened had an input to a model been changed in a particular way are afforded after a final machine learning model is produced [40], [64]. A CFE describes an outcome by considering alternative scenarios or events that did not happen but could have happened. Within the context of XAI, they focus on a model's input data, evaluating those changes where an input"}, {"title": "", "content": "feature would lead to a change in a predicted output. The design of CFEs is a challenge requiring an understanding of an Al model's decision boundaries and changing the input variables sufficiently to affect the decision [69]. However, the CFEs generated should be based on real-world evidence and be actionable and diverse to reveal trade-offs and causal implications for various scenarios [69]. As CFE focuses on examining a model's input-output relation, they can intuitively explain this association without opening the 'black box', therefore omitting the need for its mathematical facets, resulting in the outcome generated to be unearthed [24]. Overall, XAI aims to design and build DSSs capable of performing complex inference by generating natural language explanations [70]; in doing so, it provides instruments that produce explanations of AI methods based on evidence. Nevertheless, as mentioned above, such evidence has been missing from XAI approaches used to augment the design and development of DSSs in construction. Access to and using evidence to support XAI is critical to developing transparent DSSs that produce understandable, reliable, trustworthy outputs, enabling stakeholders (e.g., developers, regulators, business managers, and end-users) to make informed decisions [2]. While it is essential to consider stakeholders when creating XAI and DSSs, this paper's primary focus is on satisfying end-users' epistemic goals of construction organizations. The issues raised in this paper will also be relevant to stakeholders, even though the evidence and explanations they require may differ."}, {"title": "IV MEANS-END EPISTEMOLOGY", "content": "Explanations, in the context of AI, have traditionally been interpreted in two ways [71-78]: (1) as part of the reasoning process itself; and (2) its usage and functional aspects, aiming to ensure the reasoning process and output are understandable to the user. In construction, prevailing AI"}, {"title": "", "content": "research tends to deliver high-end classifications in the form of labeling, clustering, and pattern recognition to enable the interpretation of a DSS (model) and its outputs [78-88]. For instance, Zhong et al. [83] use deep learning algorithms (i.e., bi-directional long, short-term memory, and conditional random field) to automatically extract and classify procedures for compliance checking from quality documents. Likewise, Wang et al. [85] classify the text of defects contained in reports using an RF-Synthetic Minority Over-Sampling Technique (RF-SMOTE) and SHAP to enable engineers to understand the logic behind the model's generated outputs. Even though AI techniques are used to identify features, and a model produces significant weights to evaluate cases, the relationships between them and the output classification can be indirect and tenuous [89]. A small permutation in an unrelated aspect of the data can result in significant features being weighted differently. Thus, varying initial settings, even slightly, can result in different models being constructed [89]. Classifications are not explanations as they do not provide the same insights and knowledge into the reasoning behind how data is categorized into predefined classes or labels [26]. A cursory examination of AI research in the CEM literature reveals that explanations based on bona fide scientific evidence are not relied upon to account for outputs generated [24], [29-47], [78]. A case in point can be found in the work of Mostifi et al. [90], who use an ensemble 'voting' classifier, a machine learning technique, to classify and predict rework costs (i.e., correction in construction from 2527 non-conformance reports. However, prevailing evidence indicates that machine learning techniques cannot predict rework costs. Rework is not a risk but an uncertainty, which is probabilistically unmeasurable [91]. Non-conformances requiring rework typically arise due to human errors, which cannot be predicted as project environments, prone to volatility, uncertainty, complexity, and ambiguity (VUCA), juxtaposed with"}, {"title": "", "content": "workplace conditions, influence their occurrence [92], [93]. Moreover, non-conformances are often scant in detail and incomplete, with causes being shoe-horned into artificially constructed categories that do not reflect the context and actualities of an event [79]. Regardless of how an explanation is interpreted, it needs to be recognized that an explanation itself is a malleable concept [78]. However, it is outside the scope of this paper to enter a discussion about the intricacies and nuances of an explanation, which are examined in Salmon [72], Woodward and Ross [94], and S\u00f8rmo et al. [95]. In line with Buchholz [24], Salmon's [72] epistemic conception of explanation is adopted in this paper, which describes an explanation as fundamentally tied to people's understanding and knowledge. An explanation is successful under this conception if it enhances people's understanding of the phenomenon. It emphasizes the cognitive and informational aspects of explanation, focusing on how well it improves people's epistemic state. Taking this view of an explanation enables people to understand 'why' or 'how' something occurs within a given context, helping them organize and integrate new information with existing knowledge. A robust body of evidence is needed to develop an explanation; the means (i.e., methods and processes) by which it is derived also requires consideration, as it can vary due to the different epistemic ends (i.e., goals and outcomes related to knowledge and understanding) that need to be met. Hence, to effectively utilize evidence, a means-end epistemology, which focuses on understanding and evaluating the means used to achieve specific epistemic ends, is required to generate explanations based on accumulated evidence [24], [72]. Here, epistemic normativity addresses questions about what should be believed, how beliefs should be formed, and what constitutes sound reasoning or justification. To this end, epistemic normativity is concerned"}, {"title": "", "content": "with the standards that guide peoples' cognitive endeavors, ensuring that their beliefs, reasoning processes, and methods of inquiry align with principles that promote knowledge, truth, and rationality."}, {"title": "A. A Means-end Framework", "content": "A means-end epistemology's central normative criterion the standard or principle that determines the epistemic strengths and shortcomings of a belief, reason, or method of inquiry is explained by the principle of instrumental rationality [24], [96]. In this instance, individuals aim to select the most effective means to achieve their goals. Thus, to design and develop DSSs that are explainable and interpretable for end-users in construction, a means-end framework in Figure 2 is proposed. This proposed framework is intended to ensure that the norms and standards for good belief- forming practices (epistemic normativity) align with rational selection and use compelling evidence and methods (instrumental rationality) to achieve an organization's epistemic goals. This integration ensures that epistemic practices are normatively justified and practically effective; thus, end-users can have confidence in the outputs and explanations provided. The aim of XAI is to provide the instruments (means) to produce understandable explanations of AI methods to end-users (epistemic ends) [24]. For example, in the case of LIME, the epistemic end explains an individual prediction in a specific region of data aiming to imbue trust in this extrapolation, with the means being the local approximations drawn from the AI model [24]. Likewise, in the case of CFE, the epistemic end is to explain the input-output relation of a model, with the means being the counterfactual statements [24]."}, {"title": "V EVIDENCE-BASED \u03a7\u0391\u0399 AND DSS DESIGN", "content": "Finding and using evidence to support XAI and the design and development of a DSS is necessary to provide trustworthy explanations about Al model outputs. The means-end framework in Figure 2 provides a pathway for end-users to realize their desired epistemic goals. End-users need to feel confident that their decisions are based on explanations derived from"}, {"title": "", "content": "2 Algorithmic recourse refers to the ability to interpret algorithmic decisions enabling decision-makers to change outcomes within the context of CFE, for example."}, {"title": "", "content": "the best available evidence to inform and help them understand why a given output occurs. Likewise, this would also be the case for all other stakeholders. At this juncture, the shortcomings of some prevailing XAI post-hoc explainability instruments need to be pointed out, as they can only provide end-users with partial explanations and do not provide answers to why questions [6], [26]. Post-hoc explainability (i.e., model-agonistic) instruments, such as LIME and SHAP, aim to provide an explanation of a \u2018black-box' algorithm using an interpretable predictor to make visible its internal workings by tracing its path-dependency (i.e., a given function(s) is related to an output). In doing so, an explanation of how and not why an output is generated [26]. This situation arises as post-hoc explainability is transparency-conditional, with explanations and predictions bound to a mediating predictor rather than the inner workings of the \u2018black box' [26]. As a result, to obtain an explanation of how a formal representation between a \u2018black box' model and the interpretable predictor must exist. In this instance, a formal representation to provide structured ways to understand and quantify the relationship between complex and simplified models is required and created using concepts such as [102-105]:\n\u2022 Isomorphism between a \u2018black box' model f and an interpretable predictor g implies a one-to-one correspondence where the structure and behavior of f and g are identical. For every input x1, the output of f matches the output of g exactly, and their internal decision- making processes are structurally equivalent. So, let X be the input space, and Y be the output space. The models f: X \u2192 Y and g: X \u2192 Y are isomorphic if there exists a bijective function h: X \u2192 X such that:\n\u2200x \u2208 X, f(x) = g(h(x)) and g(x) = f(h-1(x)) [Eq.1]"}, {"title": "", "content": "\u2022\nIf f is a neural network and g is a rule-based model that exactly replicates the decision boundaries and outputs of f for all inputs, they are isomorphic.\n\u2022\nPartial isomorphism between a 'black box' model f and an interpretable predictor g indicates that the models are isomorphic only within a subset S \u2286 X of the input space; that is, g accurately mimics f within S. Thus, a formal representation is expressed as:\n\u2200x \u2208 S, f (x) = g(h(x)) and g(x) = f(h\u00af\u00b9(x)) [Eq.2]\n\u2022\nIf f is a complex model used for a wide range of inputs but g is a simpler model that accurately mimics f for a specific type of input (e.g., a specific range or category), they exhibit partial isomorphism over that subset.\n\u2022\nSimilarity between a 'black box' model f and an interpretable predictor g implies that g approximates f such that their outputs are close in a predefined sense (e.g., distance metric or statistical measure). This output does not require a one-to-one correspondence but rather a high correlation or similar statistical properties. So, let X be the input space, and Y be the out space. The models f: X \u2192 Y and g: X \u2192 Y are similar if there exists are similarity measure d: Y x Y \u2192 R such that for some small \u0454 \u2265 0:\nEx~x[d(f(x), g(x))] \u2264 \u0454 [Eq.3]\nWhere d could be any appropriate distance measure (e.g., Euclidean distance [107] and Kullback-Leibler divergence [108]). Suppose f is a deep neural network and g is a linear regression model trained to approximate f outputs. Their predictions are similar if the average difference between their outputs is within an acceptable range e."}, {"title": "", "content": "Putting aside the brief description above showing how an explanation is generated, Hassija et al. [103] discuss the advantages and disadvantages of post-hoc XAI instruments, thus providing readers with further light on these issues. To answer a why question requires moving beyond the navigation of an algorithmic path to explain the causal-mechanical or inferential dependence between X and Y and the corresponding effect [26], [105]. Accordingly, an established explanation needs to be supplemented with objective relations of dependence, such as a causal linkage between X and Y [26]. Therefore, multiple and complementary sources of evidence to demonstrate causal linkages are essential in this instance."}, {"title": "A. Hierarchy of Evidence", "content": "The quality of evidence influences the reliability and validity of a generated explanation. Evidence hierarchies, such as that presented in Table I, rank different research or evaluation study designs based on the rigor of their methods. As Table I shows, the greater the number of high-quality studies included in the analysis and the more rigorous the research, the higher the evidence rating. Research with the strongest indication of effectiveness, such as systematic reviews, meta-analyses, and randomized controlled trials (RCTs), is usually considered at the top of an evidence hierarchy in fields such as medicine. An example of evidence that may be used to inform a CFE within a means-end framework is presented in Figure 3."}, {"title": "B. Evidential Pluralism", "content": "With multiple sources of evidence needed to establish knowledge claims and explain outputs, the theory of Evidential Pluralism\u2014an account of the epistemology of causation that maintains that to establish a causal claim, there needs to be an existence of a correlation and mechanism (Figure 4) is utilized within the means-end framework to provide XAI instruments and the DSS with the means to ensure it is grounded on explainable causal discovery [28], [128], [129]. While Evidential Pluralism aligns with mixed methods, it is fundamentally different as it provides a normative philosophical theory about what needs to be established to provide a presence of causation contributing to the propagation of MHE for DSS outputs [128]. Simply put, mixed methods are a methodological orientation employing variations in qualitative and quantitative data, methods, and designs [77]."}, {"title": "", "content": "Figure 4 presents the crux of Evidential Pluralism, indicating that to ascertain a causal claim, both a correlation and the existence of mechanisms need to be established. A correlation and linking mechanism underpin the claim that A is a cause of B [128], [130]. Notably, association studies identify and analyze statistical associations between variables, helping to understand whether and how variables are related. They cannot prove causation; they only focus on correlation (i.e., strength and direction of a relationship between variables) and tests of significance (i.e., determine whether a relationship in data is statistically significant)."}, {"title": "Association Studies", "content": "Association studies can test a hypothesis (C1) that a putative cause and effect is correlated with potential confounders (Figure 4). Similarly, they can be used to indirectly verify if a mechanism accounts for a correlation (C2). Controlling for multicollinearity (e.g., removing highly correlated predictors or applying Principal Component Analysis to reduce dimensionality) and"}, {"title": "", "content": "confounders (e.g., adding covariates to a regression model to adjust for their effects) is critical for ensuring the validity of a study, as it provides the ability to infer that there is a mechanism of action giving rise to a correlation. With association studies, there is a need to be mindful and deal with confounding variables; caution also needs to be taken with \u2018tests of significance' (i.e., hypothesis testing) as they can influence the reliability of evidence when inferring a causal claim as: (1) p-values being misinterpreted: (2) thresholds of p <0.05 (or greater), for example, for statistical significance resulting in binary thinking (significant versus non-significant) rather than a nuanced understanding of the data and the actualities of practice; and (3) a lack of reproducibility of results due p-values being sensitive to varying conditions. In addition to these limitations, which can impact the reliability of evidence used to design and develop XAI and DSSs, there is also a need to consider how well outputs correlate with expected outcomes or ground truths when association studies are relied upon in areas such as: [6], [129]:\n\u2022 Performance evaluation: Precision, recall, and accuracy metrics\u2014essentially quantifying the association between the predictions of an Al model and ground truth labels have been the mainstay in construction for justifying a model's performance [131], [132]. However, there are limitations with these performance metrics, which can impact their ability to be interpreted correctly. For example, precision and accuracy metrics can be misinterpreted by imbalanced datasets and are insensitive to false negatives. Likewise, a trade-off between recall and precision often prevails. Hence, maximizing recall can result in a decrease in precision and vice versa.\n\u2022 Bias detection: Examining whether a model's decisions are correlated with sensitive attributes such as demographics (e.g., race, gender, and groups) and individual"}, {"title": "", "content": "characteristics (e.g., age and physical qualities) [133]. If an Al model's decisions disproportionately (dis)favor a particular individual or group, this can indicate a biased association that needs to be addressed. Several methods can be used to identify and address bias, depending on the context of construction (e.g., detecting unsafe behavior [133], posture analysis [134], and workforce activity [135]), ranging from statistical and predictive parity checks to more complex intersectional fairness metrics. Incompleteness of data, incorrectly if groups have similar statistical outcomes, intersectional complexity, an inability to cater to varying contexts in which a model operates (i.e., generating false positives or negatives), and changes in data are just a few of the limitations applied to overcome the detection of bias in DSSs; and\n\u2022 Causal inference: Emphasis is based on determining a cause-and-effect relationship between variables and establishing a better understanding of the directional influence between variables than correlation. Causal inference models commonly used in XAI studies in construction are Directed Acyclic Graphs (DAGs) and variants thereof [39], [64], CFEs [40], and Granger Causality [136]. However, models of this type are subjected to several drawbacks as confounders can go unmeasured. Multiple explanations can also fit observed data, introducing a degree of ambiguity and an inability to generalize causal relations, particularly in high-dimensional datasets. Aside from these disadvantages, causal inference models cannot demonstrate causality in an absolute sense, reinforcing the need to identify specific linking mechanisms to verify a claim. While association studies take a coarse approach to establishing a link between cause and effect, mechanistic studies can complement them by uncovering how the instances of A are responsible for those of B."}, {"title": "Mechanism Studies"}]}