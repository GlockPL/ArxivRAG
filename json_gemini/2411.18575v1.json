{"title": "Functional relevance based on\nthe continuous Shapley value", "authors": ["Pedro Delicado", "Cristian Pach\u00f3n-Garc\u00eda"], "abstract": "The presence of Artificial Intelligence (AI) in our society is increas-\ning, which brings with it the need to understand the behaviour of AI\nmechanisms, including machine learning predictive algorithms fed with\ntabular data, text, or images, among other types of data. This work\nfocuses on interpretability of predictive models based on functional\ndata. Designing interpretability methods for functional data models\nimplies working with a set of features whose size is infinite. In the\ncontext of scalar on function regression, we propose an interpretability\nmethod based on the Shapley value for continuous games, a mathemat-\nical formulation that allows to fairly distribute a global payoff among a\ncontinuous set players. The method is illustrated through a set of ex-\nperiments with simulated and real data sets. The open source Python\npackage ShapleyFDA is also presented.\nKeywords: Interpretability, explainability, functional data analysis,\nShapley value, continuous game theory, machine learning, artificial in-\ntelligence.", "sections": [{"title": "Introduction", "content": "Technological advances in recent years have affected data analysis signifi-\ncantly. For instance, modern smart watches are able to monitor health in-\ndicators sampling data several times per minute (Masoumian Hosseini et al.\n2023). This way of obtaining data leads to treat them as if they were func-\ntions depending on a continuous argument (time, wavelength, etc.). Thus,\nin turns, allows us to use tools from Functional Analysis to explore that\ndata.\nFunctional Data Analysis (Ramsay and Silverman 2005), or FDA for\nshort, is a branch of Statistics whose theoretical foundations come from\nFunctional Analysis (Horv\u00e1th and Kokoszka 2012; Kokoszka and Reimherr\n2017). As a brief summary, FDA considers an observation to be a function"}, {"title": "Background", "content": "This section introduces and contextualises those fields that facilitate the de-\nvelopment of this proposal. On the one hand, in Section 2.1, we provide\nan overview to the main aspects related to the Functional Data Analy-\nsis concerning to the regression problem. On the other hand, Section 2.2\nsummarises the topic of Interpretable Machine Learning in the context of\nmultiple regression problem.\nIt is noteworthy that an examination of the intersection between Func-"}, {"title": "Functional Data Analysis", "content": "This section is intended to provide a summary of FDA as well as to in-\ntroduce the notation used throughout this work. Ferraty and Vieu (2006)\ndefine a functional random variable as a random variable X taking values\nin an infinite dimensional space (or functional space). An observation (or\nrealisation) X of X is called a functional data.\nA functional data set X1,...,Xm is the observation/realisation of m\nfunctional random variables X1,..., Xm identically distributed as X. Let\nI = [a, b] \u2286 R, usually we work with data that are elements of\n$L^2(I) = \\{g: I \\rightarrow R, s.t. \\int g(t)^2dt < \\infty \\}$.\nTherefore, we refer to X or X(t) interchangeably (and the same applies to\nX1,..., Xm).\nIn practise, the functional data X\u00a1(t) are not available for all t in the\ninterval [a, b], but only for a finite grid {t1, ...,tr} with ta < td+1. One way\nof representing the observed functional data is by means of a matrix X of\nsize m \u00d7 T, where xjt = Xj(t), j \u2208 {1, ..., m} and t \u2208 T = {t1, ...,tr}.\nMost of statistical techniques of multivariate analysis have been adapted\nto FDA. Therefore, we can find the mean function \u00b5(t) = E[X(t)], the\ncovariance function \u03c3(t, u) = E[x(t)X(u)]\u2212\u03bc(t)\u03bc(u) or even more advanced\nelements such as dimensionality reduction (Functional Principal Component\nAnalysis or Functional Muldimensional Scaling) among others (Hall, Poskitt,\nand Presnell 2001; Hall, M\u00fcller, and Wang 2006).\nRegarding regression problems, we differentiate three ways to approach\nthis issue, which depend on the type of data: (1) scalar response on func-\ntional regressor, (2) functional response on scalar regressor and (3) func-\ntional response on functional regressor. This work focuses on modelling as\na scalar response on a functional predictor.\nSpecifically, let (X,Y) be a pair of random variables, where Y \u2208 R\nand X \u2208 L\u00b2(I), the goal is to obtain f : L2(I) \u2192 R such that minimises\nE[(f(X) \u2013 Y)\u00b2]. Usually a training data set {(Xj, yj)}=1 is provided with\nthe goal of looking for f. Recall that X; is called input variable/feature, yj\noutput variable/target and f machine learning model or prediction model.\nThe linear model is probably the simplest approach to regression. We say\nthat (X, Y) follows a functional linear regression model if\nY = a + \\int X(t) \\beta(t)dt\n+ \u20ac,"}, {"title": "Interpretable machine learning", "content": "Breiman (2001) identifies two distinct cultures within the field of data anal-\nysis: the modeling culture, based on statistical inference, and the predic-\ntion culture, focused on machine learning techniques. In addition, Breiman\n(2001) points out a potential trade-off between predictive ability and in-\nterpretability of models. The predictive accuracy of machine learning al-\ngorithms, such as neural networks or random forests, often exceeds that of\nstatistical models, such as linear or logistic regression. However, statistical\nmodels provide a simpler understanding of the relationship between the re-\nsponse variable and the input variables. In fact, machine learning algorithms\nare often referred to as \"black boxes\" because of their inability to provide\nunderstandable explanations of the reasons behind their predictions. Never-\ntheless, Breiman (2001) calls for the development of procedures that would\nallow for better interpretation of algorithmic models without compromising\ntheir predictive ability.\nIn the last two decades, a powerful research line (known as Interpretable\nMachine Learning, IML, or eXplainable Artificial Intelligence, XAI) has been\ndeveloped to provide interpretability tools to algorithmic models. This vast\nliterature has given rise to a considerable number of review papers (see for\ninstance, Barredo Arrieta et al. 2020) and three monographs on IML/XAI:\nMolnar (2022), Biecek and Burzykowski (2021) and Mas\u00eds (2021), as well as\nmany functions and packages in R and Python.\nIn IML/XAI a distinction is done between global and local interpretabil-\nity. On the one hand, global interpretability tools measure the importance\nor relevance of each explanatory variable in the prediction process over their\nwhole support. On the other hand, local interpretability tools provide mean-\ningful explanations of why the prediction model returns a certain estimated\nresponse for a given individual, identified with a particular combination of\nthe predicting variables values.\nIt is also relevant to classify IML/XAI methods as model-specific or\nmodel-agnostic. The first category includes interpretability methods that\nare designed to interpret a particular prediction algorithm (such as random\nforests or neural networks), exploiting its internal structure for interpreta-\ntion. In contrast, model-agnostic interpretability methods can be applied to\nany prediction model. They only need to evaluate the prediction model on\ndata from the training or test sets, or perturbations of either, and do not\nhave access to the internal structure of the prediction model.\nIn this paper, we are interested in global model-agnostic interpretability\nmethods. In the usual framework of a prediction problem with p explanatory\nvariables and one response to which a prediction model (from Statistics or"}, {"title": "Interpretability based on game theory", "content": "This section is devoted to connecting the Shapley value for finite games,\na concept from Game Theory, and interpretability for multivariate data.\nWe also introduce how the Shapley value is considered when the game is\ncontinuous. This serves as a theoretical framework to develop our proposal,\nwhich is explained in Section 4."}, {"title": "Finite games", "content": "In this Section we follow Winter (2002) to present the Shapley value. The\noriginal work can be found in Shapley (1953). Given a set of players N =\n{1,...,n}, a game is a function \u03bd : 2N \u2192 R+, where 2N is the set of all\nsubsets of N and v(0) = 0. The mapping v is known as the payoff function\nand v(S) is interpreted as the payoff the coalition S receives for having\nplayed that game.\nLet QN be the space of all possible games with players in N. A value is\nan operator : Q\u2116 \u2192 (R+)", "R+)": "where the i-th component of the vector, Oui (or\ni whenever the game can be omitted), represents the value of the player i\nwhen playing game v. Shapley defines a set of axioms and proves that there\nis a unique value satisfying the axioms, being the following ones:\n\u2022 Efficiency: the first axiom states that the sum of all the values must\nbe equal to the total payoff:\n$\\sum_{i=1}^{n} \\phi_i = v(N)$.\n\u2022 Symmetry: first, we need to define the symmetric property. Players\ni,i' \u2208 N are said to be symmetric with respect to the game v if for\neach S CN such that i,i' \u2209 S, v(S\u222a {i}) = v(S\u222a {i'}). If i and i'\nare symmetric, then pi = i'.\n\u2022 Dummy: player i is a dummy player (with respect to the game v) if\nfor every S \u2208 N, v(S\u222a{i}) \u2013 \u03bd(S) = 0. If player i is a dummy player,\nthen i = 0."}, {"title": "Interpretability based on finite games", "content": "Consider a multiple linear regression model with response y and p explana-\ntory variables (or, equivalently, features) x1,...,xp, where y, x\u00bf \u2208 Rm. Let\ny = 1/m \\sum_{j=1}^{m} Yj and let \u0177j be the j-th fitted value by ordinary least squares\n(OLS). The coefficient of determination is defined as\nR\u00b2 = 1-$\\frac{\\sum_{j=1}^{m} (4j \u2013 \\hat{y}_j)^2}{\\sum_{j=1}^{m}(4j \u2013 \\bar{y})^2}$,\nand it is a commonly used measure of the overall quality of the estimated\nmodel. It is also well known that R\u00b2 is equal to the squared sampling corre-\nlation coefficient between the observed responses yj and the fitted values \u0177j.\nWhen the p explanatory variables are uncorrelated, it can be proved that\np\nR\u00b2 = \\sum_{i=1}^{p} R_i^2,\nwhere R is the coefficient of determination in the simple linear regression\nof y against the i-th explanatory variable xi fitted by OLS. Therefore, R\nis the contribution of xi to the global quality measure R2, and it is a good\nmeasure of the relevance of x in the model.\nThe preceding decomposition of R\u00b2 is not applicable in the presence of\ncorrelated explanatory variables. Lipovetsky and Conklin (2001) propose\nan alternative decomposition based on the Shapley value. Specifically, the\nauthors propose to consider the p explanatory variables as the set of players\nand the game v as the coefficient of determination R2. Consequently, v(S)\nis the coefficient of determination R\u00b2(S) in the regression of y against the\nvariables belonging to S fitted by OLS."}, {"title": "Continuous games", "content": "The primary objective of this work is to incorporate interpretability within\nthe context of FDA. Therefore, it is considered to extend the previous rele-\nvance measure based on the Shapley value to prediction models with scalar\nresponse and a functional regressor. In this context, the set of players is\ninfinite, or, equivalently, the game becomes continuous. Consequently, the\nShapley value must be defined for continuous games, also known as games\nwith a continuum of players (Shapley 1961; Aumann 1964). There exists\na collection of publications devoted to this topic (Kannai 1966; Aumann\nand Shapley 1974; Hart and Neyman 1988; Neyman 1994, among others).\nObtaining the Shapley value for continuous games is not as direct as in the\nfinite case, since a slightly more extensive theory is needed.\nLet I = [0,1] and B = B(I) be the associated Borel \u03c3-algebra. A (con-\ntinuous) game is a function v : B \u2192 R+ such that v(0) = 0. I is called\nthe set of players, B the set of coalitions, (I,B) the space of players and\nv the payoff function. Although we work with I = [0, 1], the forthcoming\ndevelopment is valid for any interval [a, b] C R.\nA game is said monotonic if T C S implies v(T) \u2264 \u03bd(S). Let Q be a\ncollection of monotonic games in (I, B). Q is a linear space over (I,B) if\nV1 + V2 \u2208 Q for any v\u2081 and 12, both in Q. An automorphism in (I, B) is a\none-to-one measurable function @ from I onto I with 0-1 measurable. Let G\nbe the group of automorphisms of (I, B). Given a game v, an automorphism\n\u03b8\u2208 G defines a new game from v: (0\u2217v)(S) = v(0(S)) for all S \u2208 B. Q is\nsymmetric if \u03b8\u03bd\u2208 Q for all v\u2208Q and all 0 \u2208 G.\nA measure \u03bc on (I,B) is a monotonic game with the property of \u03c3-\nadditivity: for all countable collections {Ej}j\u22651 of pairwise disjoint sets in\nB, \u03bc (Uj\u22651Ej) = \\sum_{j\u22651}\u03bc(Ej). Let M be the set of measures defined on\n(I,B). Note that Mitself is a symmetric and linear subset of monotonic\ngames.\nA value on a linear symmetric space of monotonic games Q is a mapping\n\u03c8 : Q \u2192 M satisfying the following properties (Aumann and Shapley 1974,"}, {"title": "Relevance based on the continuous Shapley value", "content": "In this section, we present a framework for defining a relevance function for\nmachine learning models with functional predictors. The goal is to rank\nthe points t in the interval I = [0,1] according to their importance when\npredicting the response variable. We make use of the asymptotic approach\ndescribed in Section 3.3. In addition, we adapt our proposal to perform\nfeature selection."}, {"title": "Feature importance", "content": "We start by assuming that a test data set is available: (Xj, yj), j \u2208 {1, ..., m},\nwhere X; \u2208 L\u00b2(I) and yj \u2208 R. Additionally, it can be assumed that one has\naccess to an already trained prediction model, f : L\u00b2(I) \u2192 R, and that the\ndata used to train such a model are independent from the test data. This\nwork focuses on the regression context, but it can be easily adapted to the\nclassification setup.\nOne of the key elements is how to define the game v : B \u2192 R+ which\nwill allow us to find the relevance function. Let S\u2208 B, roughly speaking\n\u03bd(S) should be the proportion of variability of y = (y1,..., Yym) in the test\nsample explained by the prediction model f when only the information of S\nis considered.\nA first attempt could be to retrain the model only considering the points\nt\u2208 S, but this strategy would eventually be computationally very expensive\nas it would need to be performed for each S\u2208 B. This strategy has certain\nsimilarities with the LOCO approach to relevance features in prediction\nmodels with a finite number of predictors, described in Section 2.2. There,\nwe have reported that Delicado and Pe\u00f1a (2023) overcome the LOCO issues\nusing ghost variables: the conditional expectation of one feature, given the\nrest of them. An analogous approach is employed in this work.\nConcretely, we propose the creation of a new data set in which the data\nfrom t\u2208 S are retained while the remaining data are inferred from S, the\nnew data set is created using the conditional expectation, that is,\nx(t) = x(t) 1s(t) + &;(t) 1,sc(t),\nwith\nX;(t) = E(x(t) | {X(u) = X;(u) : u \u2208 S}),"}, {"title": "Feature selection based on infinite games", "content": "Our framework can be adapted to perform variable selection based on Shap-\nley value, which requires the use of a continuous game. We propose to use\ntwo distinct games: the first one is inspired by the minimum Redundancy"}, {"title": "ShapleyFDA package", "content": "An open source Python package, ShapleyFDA, has been released to PyPI to\ncompute the Shapley value function according to the methodology explained\nin this work. It contains functionalities for feature importance (Section 4.1)\nas well as for feature selection (Section 4.2). Given that the calculations are\ninherently matrix-intensive, the fundamental component of ShapleyFDA is\nbased on the NumPy package (Harris et al. 2020).\nAs a large volume of computations must be considered, an efficient im-\nplementation has been conducted, whereby intermediate results are stored\nin memory for future reference. For instance, given permutations \u03c0\u03b9\n= (13, 14, 11, 12) and \u03c02 = (13, 14, 12, 11) the set of players preceding player 1\nin \u03c0\u2081 is the same as the set of players preceding 2 in \u03c02. Upon the initial\ncomputation of the value for p\u2081 = {13, 14}, it is stored. This approach\nguarantees that the same value will not be calculated twice, but rather used\nas many times as is necessary.\nShapleyFDA assumes that the machine learning model has already been\ntrained. Furthermore, the packageis designed so that it can handle multiple\nprediction models simultaneously to obtain the Shapley value function for\neach of them, as well as for feature selection methods described in Section\n4.2. Consequently, the same set of random permutations is used when ob-\ntaining multiple Shapley value functions. Moreover, the input test data set\nmust be provided in the matrixwise formulation, as detailed in Section 2.1.\nFinally, the package incorporates functionalities to display both figures,\nthe histogram-type function and the polygonal-type function (Section 4.1).\nApart from the released version in PyPI, it also has a development version,\nwhich is available at GitHub."}, {"title": "Experiments", "content": "Two types of experiments are conducted with the objective of showing the\nperformance of our proposed methodology. In the first type, described in\nSection 5.1, we simulate data and then use three different machine learning\nmodels to predict the response variable. Since we control the data generation\nprocess, we know in advance which points are important. The second type,\nwhich is explained in Section 5.2, consists of exploring the Tecator data\nset (Borggaard and Thodberg 1992). The code utilised in the experimental\nconfiguration is accessible via GitHub, github.com/pachoning/shapley_\nfda_experiments. Simulated data are also available. Since their size is\nhigh and GitHub is not meant to store large volume of data, they are placed\nseparately from the code at https://bit.ly/4crYVt1."}, {"title": "Simulated data", "content": "We begin by explaining how the pair of random variables (X,Y) are ob-\ntained. Recall that our work just considers the scalar response on functional\nregressor (Section 2.1). In addition, the interval considered is the unitary\none, i.e., I = [0,1]. Regarding X we consider 3 different ways of genera-\ntion, namely Fourier expansion, symmetric Fourier expansion and Brownian\ntrend (short name for Brownian motion with a trend). The first one consists\nof linear combinations of the Fourier basis,\nXF(t) = \\sum_{s=0}^{r} Z_s \\phi_s(t),\nwhere Zs are independent draws of a standard normal distribution, {\u00a3s(t)}s\u22650\nis the Fourier basis on I, and r is an even number.\nThe goal of the second one is to obtain symmetric trajectories with\nrespect to the middle point of I. This is achieved by building the functions\nas\nXsF(t) = XF(t) + XF(1 \u2013 t).\nWhen simulating data for the previous two cases, XF(t) and XsF(t), the\nvalue of r is fixed to 30. The third functional random variable taken into\naccount is a Brownian motion with a trend. Let B(t) be a Brownian motion\nfor t \u2208 I. Then, the functional random variable is\nXB(t) = B(t) + t.\nEvery functional data set is stored in a matrix X of size m \u00d7 T, where m\ntakes 2 different values, 200 and 500, and T is equal to 101, being t\u2081 = 0 and\ntk = tk-1 +0.01, k = 2, . . ., 101. Regarding the partition I = {I1, ..., In},\nwe consider n = 20 and all the intervals with the same length.\nIn order to build the target variable Y, a transformation is applied to\nthe functional random variable, Y : L\u00b2(I) \u2192 R, leading to Y = Y (X) + \u20ac,"}, {"title": "Real data", "content": "Tecator data set has been widely used within the FDA literature (see, for\ninstance, Ferraty and Vieu 2006). This data set consists of 215 spectrometric\ncurves of meat samples and it measures the near infrared (NIR) absorbance\nA(w) as a function of wavelength w (in nanometres). In addition, each curve\nis associated with three distinct quantities: the percentage of fat, water, and\nprotein in the meat sample. The percentage of fat is used here as the target\nvariable.\nFollowing Ferraty and Vieu (2006, Section 7.2.1), we divide the data set\ninto learning sample (curves 1 to 160; 74.4%) and testing sample (curves\n161 to 215; 55 curves, 25.6%). Given that we differentiate between training\nand validation sets, we divide the learning sample randomly into training\nset (128 curves; 59.6%) and validation set (32 curves; 14.8%). The training\nset is employed for fitting the same machine learning algorithms used in\nSection 4.1: FLM, FKNN and FNN. The validation set is used to perform\nhyperparamenter tuning, and the test data set serves to compute the distinct\nShapley value functions.\nThe prediction algorithms are used to model the target variable as a"}, {"title": "Conclusions", "content": "In light of the considerable extent of the presence of Artificial Intelligence in\nour society, it is of particular importance to gain insight into the mechanisms\nthrough which it learns. This work represents a novel approach to addressing\nthe global interpretability of prediction models within the field of Functional\nData Analysis.\nOur framework is based on Game Theory for games with a continuum of\nplayers, extending the Shapley value to the case of a set of infinite regressors.\nThe central piece of this work is the Shapley value function, that measures\nthe relevance of all points of the interval where the functional data are\nobserved. In addition, we enrich this methodology with tools from feature\nselection. Alongside this manuscript, we present an open source Python\npackage that implements this framework.\nWe have illustrated the performance of our work by means of a set of\nexperiments. On the one hand, we use simulated data where the relevant\npoints were known beforehand. The results have shown that these points\nhave been successfully captured by the explainability method. On the other\nhand, we have explored the Tecator data set, where the second derivative\nhas been used to model the data. According to our proposal, the most"}, {"title": "Minimum Redundancy Maximum Relevance", "content": "Ding and Peng (2005) present minimum Redundancy Maximum Relevance\n(mRMR for short), a framework to perform variable selection in prediction\nproblems with a finite number of predictors, where they look for a reasonable\ntrade-off between relevance and redundancy.\nLet (W, Y), W \u2208 RP, Y \u2208 R a pair of random variables. First, an\nassociation metric J for pairs of variables is defined. The original work\nuses the mutual information metric, but any other can be used: Pearson\ncorrelation, Spearman rank correlation, etc. Afterwards, they propose to\nmeasure both relevance and redundancy respectively as:\nRel(F) = \\frac{1}{card(F)} \\sum_{t \\in F} J(W_t, Y),\nRed(F) = \\frac{1}{card^2(F)} \\sum_{s,t \\in F} J(Wt, Ws),\nwhere F C {1,...,p} and card(F) means the cardinal of F. The mRMR\nalgorithm aims at maximising the relevance while maintaining a minimal\nlevel of redundancy. To obtain an appropriate set of relevant features, the\nalgorithm employs the following strategy:\n1. Select the variable that maximises Rel(F) among all sets of the type\nF\u2081 = {i}, i \u2208 {1,...,p}.\n2. Then, the variables should be added to the set Fin a sequential man-\nner, with the goal of maximising Rel(F) \u2013 Red(F) or Rel(F)/Red(F).\n3. Finally, stop when a certain criteria is reached.\nAn exhaustive comparative study based on mRMR variable selection\nfor functional data is done in Berrendero, Cuevas, and Torrecilla (2016).\nThe authors consider a functional explanatory variable X(t) and its discrete\nrepresentation, a vector of length T, to carry out such a study. Taking into\naccount the previous notation for mRMR, p = T, W\u2081 = X(t1),...,W\u2081 =\nX(tr) and FC {t1,..., tr}. Let us remark that in this manuscript we only\nuse Equations (4) and (5), without going through steps 1, 2 and 3."}, {"title": "Distance correlation", "content": "The distance correlation (Sz\u00e9kely, Rizzo, and Bakirov 2007) is a measure\nof dependence between two random vectors S1, S2, where S1 \u2208 RP and\nS2 \u2208 Rq. Let $, $1 and 2 be the characteristic function of (S1, S2), S1\nand S2 respectively. The distance covariance is defined as the positive square\nroot of\nV2 (S1, S2) = \\int_{\\mathbb{R}^{p+q}} |\\varphi(u, v) \u2013 \\varphi_1(u)\\varphi_2(v)|^2w(u,v)dudv,\nwith w(u, v) = (CpCq||u||p+P||v||1+q)-1, where cr =$\\frac{\\pi^{(1+r)/2}}{\\Gamma((1+r)/2)}$ is half the sur-\nface area of the unit sphere in R\u2033+1 and || \u00b7 ||r is the Euclidean norm in\nR\". The distance correlation is the standardised version of the distance\ncovariance: it is defined as the positive square root of\nD2(S1, S2) = $\\frac{V^2 (S_1, S_2)}{\\sqrt{V^2(S_1, S_1)V^2 (S_2, S_2)}}$,\nIt is of particular importance to emphasise that distance correlation char-\nacterises independence: S\u2081 and S2 are independent if and only if D\u00b2 (S1, S2) =\n0. It is also noteworthy that the dimensions of the spaces in which both ran-\ndom vectors are situated are not required to be equal.\nIn the original paper, the authors proposed an estimator based on dis-\ntances. Let (S11, S21) . . ., (S1m, S2m) be a sample of (S1, S2). The double\ncentring matrix of distances is computed as\nAhe = ahe \u2013 \u0101h. \u2013 \u0101. + \u0101..,\nwith elements ane = ||S1h \u2013 S1e||p, \u0101h. = 1/m \\sum_{l=1}^{m} ane, \u0101.e = 1/m \\sum_{h=1}^{m} anl\nand \u0101.. = 1/m\u00b2 \\sum_{h,l=1}^{m} ane where h,l \u2208 {1,...,m}. Similarly is defined\nBhe using the sample of S2. The empirical (square) distance covariance is\ndefined as\nV2(S1, S2) =$\\frac{1}{m^2}$ \\sum_{h,l=1}^{m} Ane Bhe\nThe empirical (square) distance correlation is computed by means of stan-\ndardising the empirical distance covariance:\nD2 (S1, S2) =$\\frac{V^2(S_1, S_2)}{\\sqrt{V^2(S_1, S_1)V^2(S_2, S_2)}}$\""}, {"title": "Estimating conditional expectations in a Gaus- sian process", "content": "In this section we describe how &;(t) = E(x(t) | {X(u) = X;(u) : u \u2208 S})\nis estimated, 1 \u2264 j \u2264 m. We maintain the same notation as in the main"}, {"title": "Results of simulation study", "content": "In this section we present the results of the remaining scenarios, which are:\n\u2022 m\n200 and \u03b7 = 0.25. See Figure 11 and Table 2.\n\u2022 m\n500 and \u03b7 = 0.05. See Figure 12 and Table 3.\n\u2022 m\n500 and \u03b7 = 0.25. See Figure 13 and Table 4."}]}