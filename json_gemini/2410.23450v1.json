{"title": "Return Augmented Decision Transformer for Off-Dynamics Reinforcement Learning", "authors": ["Ruhan Wang", "Yu Yang", "Zhishuai Liu", "Dongruo Zhou", "Pan Xu"], "abstract": "We study offline off-dynamics reinforcement learning (RL) to utilize data from an easily\naccessible source domain to enhance policy learning in a target domain with limited data. Our\napproach centers on return-conditioned supervised learning (RCSL), particularly focusing on the\ndecision transformer (DT), which can predict actions conditioned on desired return guidance and\ncomplete trajectory history. Previous works tackle the dynamics shift problem by augmenting\nthe reward in the trajectory from the source domain to match the optimal trajectory in the\ntarget domain. However, this strategy can not be directly applicable in RCSL owing to (1)\nthe unique form of the RCSL policy class, which explicitly depends on the return, and (2) the\nabsence of a straightforward representation of the optimal trajectory distribution. We propose\nthe Return Augmented Decision Transformer (RADT) method, where we augment the return in\nthe source domain by aligning its distribution with that in the target domain. We provide the\ntheoretical analysis demonstrating that the RCSL policy learned from RADT achieves the same\nlevel of suboptimality as would be obtained without a dynamics shift. We introduce two practical\nimplementations RADT-DARA and RADT-MV respectively. Extensive experiments conducted\non D4RL datasets reveal that our methods generally outperform dynamic programming based\nmethods in off-dynamics RL scenarios.", "sections": [{"title": "1 Introduction", "content": "Off-dynamics reinforcement learning (Eysenbach et al., 2020; Jiang et al., 2021; Liu et al., 2022;\nLiu and Xu, 2024) is commonly employed in decision-making scenarios such as autonomous driving\n(Pan et al., 2017) and medical treatment (Laber et al., 2018; Liu et al., 2023), where training the\npolicy through extensive trial-and-error in the real target environment is often costly, unethical, or\ninfeasible. A practical approach involves training the RL policy in source environments with similar\ndynamics that are easier to simulate or access. However, the dynamic shift between the source and\ntarget environments results in a challenging sim-to-real gap, which can lead to catastrophic failures\nwhen deploying the policy learned from the source environment to the target environment."}, {"title": "2 Related Work", "content": "Off-dynamics reinforcement learning (RL). It is a type of domain adaptation problem in RL,\ndrawing on concepts from transfer learning (Pan and Yang, 2009). The DARC algorithm (Eysenbach\net al., 2020) addresses this domain adaptation challenge in the online setting by proposing a reward\naugmentation method that matches the optimal trajectory distribution between the source and target\ndomains. Building on this, DARA (Liu et al., 2022) utilizes reward augmentation to supplement a\nlimited target dataset with a larger source dataset. Unlike DARC and DARA, which are based on\ndynamic programming, our work adopts the adaptation setting of DARA and introduces a novel\naugmentation method tailored for RCSL, specifically focusing on the Decision Transformer.\nReturn Conditioned Supervised Learning (RCSL). It is a general framework for powerful\nsupervised methods in offline RL (Brandfonbrener et al., 2022). Notable works such as RvS (Emmons\net al., 2021) and Decision Transformer (DT) (Chen et al., 2021) have shown competitive performance\ncompared to traditional RL methods. The core idea of RCSL is to condition policies on a desired\nreturn. In this paper, we primarily focus on DT, which is a specific instance of RCSL and conducts\noffline RL through sequence generation. The generalization potential of DT has inspired researchers\nto explore its use in various settings. For example, Zheng et al. (2022); Xu et al. (2022) leverage\nthe DT in the offline-to-online RL and meta RL respectively. However, no prior work has explicitly\nexplored the adaptation capabilities of DT in the off-dynamics RL setting."}, {"title": "3 Preliminary", "content": "Sequential Decision-Making. We consider a general sequential decision-making problem. At each\nstep t, the agent receives an observation $o_t$ from the environment. Based on the history up to step t,\nthe agent makes action $a_t$ and receives the reward $r_t$ from the environment. The agent interacts\nwith the environment in episodes with a length H. We use $\\tau = (o_1, a_1, r_1,\\dots,o_H,a_H,r_H)$ to denote\na whole trajectory, and we use $g(t) = \\sum_{t=1}^H r_t$ to denote the cumulative return of the trajectory. We\nmodel the environment as a Markov Decision Process (MDP) M, which consists of (S, A, p, r, H).\nHere S is the state space, each state s represents the possible history up to some time step t, i.e.,\n$s = (o_1,a_1, r_1,\\dots, o_t)$. A is the action space, $p(s'|s, a)$ is the transition dynamic that determines\nthe transition probability for the agent to visit state s' from current state s with the action a. r(s,a)\ndenotes the reward function. We re-define a trajectory as $\\tau = (s_1, a_1, r_1,\\dots,s_H,a_H,r_H)$. We assume\nthat each s corresponds to one single time step t = t(s), and we denote $g_\\tau(s) = E_{\\tau\\sim\\pi}[g(T)|s_1 = s]$.\nThen the goal of the agent is to learn a policy $\\pi : S \\rightarrow A$ that maximizes the expected accumulated\nreward $J(\\pi) := E_{\\tau\\sim\\pi}[g(\\tau)]$. We denote the optimal policy as $\\pi^*$.\nOffline RL and Decision Transformer. We consider the offline reinforcement learning setting.\nGiven a dataset D, the goal of the agent is to learn $\\pi^*$ from D. We assume that the trajectories in"}, {"title": "4 Return Augmentation for Goal Conditioned Supervised Learning", "content": "4.1 Return-Augmented Framework\nDT has the potential to address offline off-dynamics reinforcement learning challenges, as shown in\nTable 2. However, it still has certain limitations. To overcome these, we propose a general framework\nthat efficiently learns the optimal policy for the target environment using the combined dataset\n$D^S \\cup D^T$. Leveraging the return-conditioning nature of DT, we introduce a return augmentation\ntechnique that modifies returns in the offline source dataset through a transformation function. This\napproach allows the policy derived from the augmented source dataset to effectively approximate\nthe optimal policy of the target environment, as illustrated in the following equation, where $\\pi^\\zeta$\nrepresents a strong candidate for approximating the optimal policy of the target environment and $\\psi$\nis the carefully chosen transformation function.\n$\\pi^\\zeta = \\mathop{\\mathrm{argmin}}_{\\pi} \\hat{L}(\\pi)$\n$:= - \\sum_{\\tau \\in D^S} \\sum_{1 < t \\le H} \\log \\pi(a_t|s_t, \\psi(g(\\tau))).$\nWe call our method Return Augmented DT (RADT). Next we introduce two methods to construct\n$\\psi$, based on the dynamics-aware reward augmentation (DARA) technique (Eysenbach et al., 2020;\nLiu et al., 2022), and a direct return distribution matching method."}, {"title": "4.2 Dynamics-Aware Reward Augmentation", "content": "We first summarize the idea of dynamics-aware reward augmentation (DARA) (Eysenbach et al.,\n2020; Liu et al., 2022). Let $p^T(s'|s, a)$ denote the transition dynamic of the target environment,\nand $p^S(s'|s, a)$ denote the source environment. According to the connection of RL and probabilistic"}, {"title": "4.3 Direct Matching of Return Distributions", "content": "The reward augmentation strategy in RADT-DARA stems from the probabilistic inference view of\nRL which matches the distribution of the learning trajectory in the source domain with that of the\noptimal trajectory in the target domain (Eysenbach et al., 2020). However, it does not fully capture\nthe power of DT, which is able to induce a family of policies that are conditioned on the return-to-go\nf. By varying f, DT enables the generation of a diverse range of policies, including the optimal one.\nIn contrast, RADT-DARA assumes a single, fixed target policy, and thus its augmentation strategy\ncannot generalize across multiple policies induced by varying f in DT. As a result, it cannot find\nthe desired return conditioned policy when evaluated with a different f in the target domain. This\nmotivates us to find a return transformation method $\\psi$ to guarantee that $\\pi_\\xi(a|s) \\approx \\pi(a|s)$ for all f.\nWe consider a simplified case where both $D^S$ and $D^T$ are generated by following the same\nbehavior policy $\\beta(a|s)$. We use $d_S(A)$ and $d_T(A)$ to denote the probability for event A to happen\nunder the source and target environments following $\\beta$. With a slight abuse of notation, we use $g_S$ and\n$g_T$ to denote the return following the behavior policy. Then we characterize the learned policies by"}, {"title": "4.4 Sample Complexity of Off-Dynamics RCSL", "content": "In this section, we provide an overview of the sample complexity for off-dynamics RCSL. Let $N^S$\nrepresent the number of trajectories in the source dataset $D^S$ and $N^T$ the number of trajectories\nin the target dataset $D^T$. We define $J_T(\\pi)$ as the expected cumulative reward under any policy $\\pi$\nwithin the target environment. Our theorem is established based on the following assumption.\nAssumption 4.2 (Theorem 1, Brandfonbrener et al. 2022). (1) (Return coverage) $P(g =\nf(s_1)|s_1) \\ge \\alpha_f$ for all initial states $s_1$. (2) (Near determinism) $P(r \\ne r(s, a) \\text{ or } s' \\ne T(s,a)|s, a) \\le \\epsilon$\nat all s, a for some functions T and r. (3) (Consistency of f) $f(s) = f(s') + r$ for all s.\nAssumption 4.3 (Theorem 2, Brandfonbrener et al. 2022). (1) (Bounded occupancy mismatch)\n$\\frac{d_{\\pi^{RCSL}(s)}}{P_\\beta(s)} \\le C_f$ for all s. (2) (Return coverage) $P_f (g = f(s)|s) \\ge \\alpha_f$ for all s. (3) (Domain\noccupancy overlap) $\\frac{d_f(s)}{\\delta_f(s)} \\le \\gamma_f$ for all s."}, {"title": "5 Experiments", "content": "In this section, we first outline the fundamental setup of the experiment. We then describe\nexperiments designed to address specific questions, with each question and its corresponding answer\ndetailed in a separate subsection."}, {"title": "5.1 Basic Experiment Setting", "content": "Tasks and Environments. We study established D4RL tasks in the Gym-MuJoCo environ-\nment (Fu et al., 2020), a suite built atop the MuJoCo physics simulator, featuring tasks such as\nlocomotion and manipulation. Particularly, we focused on three environments: Walker2D, Hopper,\nand HalfCheetah.\nIn tackling the off-dynamics reinforcement learning problem, we differentiate between the Source\nand the Target environments. The Target environment is based on the original Gym-MuJoCo\nframework, whereas the Source environment is constructed using the following methods. For further\ndetails on the source environment, please refer to the Appendix C.\nBodyMass Shift: Change the mass of the body in the source environment.\nJointNoise Shift: Add a noise (randomly sampling in [-0.05, +0.05]) to the actions.\nKinematic Shift:\nWalker2D Environment: The rotation range of the joint on the right leg's foot is modified\nfrom [-45, 45] to [-0.45, 0.45].\nHopper Environment:\n* The rotation range of the joint on the head is adjusted from $[-150,0]$ to [-0.15, 0].\n* The rotation range of the joint on the foot is changed from [-45, 45] to [-18, 18].\nHalfCheetah Environment: The rotation range of the joint on the back leg's thigh is modified\nfrom [-0.52, 1.05] to [-0.0052, 0.0105].\nMorphology Shift: In the Walker2D environment, we adjust the size of the right leg's thigh. In\nthe Hopper environment, we modify the size of the head. In HalfCheetah, we alter the size of\nboth thighs.\nDataset. For the Target Dataset corresponding to the Target Environment, we leverage the\nofficial D4RL data to construct two datasets: 10T and 1T. The 10T dataset comprises ten times"}, {"title": "5.2 Decision Transformer in Off-dynamics Reinforcement Learning", "content": "To assess the effectiveness of DT in off-dynamics reinforcement learning scenarios, we first train the\nDT model on a source dataset and subsequently evaluate its performance in a target environment."}, {"title": "5.3 RADT for Off-Dynamics RL", "content": "Here we discuss how to implement RADT-DARA and RADT-MV in practice. We implement\nRADT-DARA based on the dynamic-aware reward augmentation method proposed in Liu et al.\n(2022). For RADT-MV, it involves training the CQL model across both the Target and Source\nEnvironments to derive the respective value functions, denoted as $Q_T$ and $Q_S$. The derived value\nfunctions are then used to relabel the returns of trajectories in the original dataset. More specifically,\nthe relabeled return $\\hat{g}^S$ is calculated as defined in (4.4). Within this framework, we use $\\mu^S(s, a)$ to\ndenote $Q_S(s, a)$, and $Q_T(s, a)$ corresponds to $\\mu^T (s, a)$. For the computation of $\\sigma_S(s, a)$ and $\\sigma_T(s, a)$,\nwe employ the following methodology: For a given state s, we use the policy of CQL on the source\ndataset to obtain n available actions ${a_1^S, a_2^S,..., a_n^S}$ given the state s, with the corresponding Q\nvalues ${Q_S(s, a_1^S), Q_S(s, a_2^S),..., Q_S(s, a_n^S)}$, and n available actions ${a_1^T, a_2^T,..., a_n^T}$ in the target\nenvironment obtained from the CQL policy trained over the target dataset, with the corresponding\nQ values ${Q_T(s, a_1^T), Q_T(s, a_2^T), ..., Q_T(s,a_n^T)}$. The standard deviations $\\sigma_S(s, a)$ and $\\sigma_T(s, a)$ are\nthen calculated as specified as follows.\n$\\sigma_S(s,a) = std(Q_S(s, a_1^S), Q_S(s, a_2^S),...,Q_S(s, a_n^S)),$\n$\\sigma_T(s, a) = std(Q_T(s, a_1^T), Q_T(s, a_2^T), ..., Q_T(s,a_n^T)).$\nFor the detailed discussion of this method, please see Section 4. Recall that in (4.4), we need to\ncalculate the ratio $\\frac{\\sigma_T(s,a)}{\\sigma_S(s,a)}$. We introduce a clipping technique to avoid the instability issue caused by\nextremely large or small $\\frac{\\sigma_T(s,a)}{\\sigma_S(s,a)}$. In detail, we clip $\\frac{\\sigma_T(s,a)}{\\sigma_S(s,a)}$ with an upper bound $\\theta_1$ and lower bound\n$\\theta_2$. We later show that such a trick helps to improve the performance of RADT-MV. Ultimately,"}, {"title": "5.4 Ablation Study", "content": "In this section, we present an ablation study to identify the key factors affecting RADT algorithm\nperformance. Most results are provided for the Walker2D and Hopper environments, with additional\nablation studies in the HalfCheetah environment available in Appendix D. This study aims to address\nthe following research questions.\nHow do RADT methods perform under more significantly shifted source environments?"}, {"title": "6 Conclusion and Future Work", "content": "We introduced the Return Augmented Decision Transformer (RADT) method for off-dynamics\nreinforcement learning which augments the return function of trajectories from the source environment\nto better align with the target environment. We presented two practical implementations of\nour method: RADT-DARA, derived from existing dynamic programming reward augmentation\ntechniques, and RADT-MV, which matches the mean and variance of the return function under\na Gaussian approximation of the return distribution. Through rigorous theoretical analysis, we\nshowed that RADT when trained only on the source domain's dataset can achieve the same level\nof suboptimality as policies learned directly in the target domain. Our extensive experiments in\nMuJoco environments demonstrate that RADT outperforms baseline algorithms in off-dynamics\nRL. This work establishes RADT as a promising method for leveraging source domain data to\nimprove policy learning in target domains with limited data, thus addressing key challenges in offline,\noff-policy, and off-dynamics RL. Future work could explore extensions and applications of RADT in\nmore diverse RL environments and further refine the augmentation techniques to enhance efficiency\nand effectiveness."}, {"title": "A Sample Complexity of Off-Dynamics RCSL", "content": "In this section, we provide the rigorous analysis of the sample complexity of the off-dynamics RCSL.\nTo this end, we first define some useful notations. We assume there are $N^S$ trajectories in the source\ndataset $D^S$, and $N^T$ trajectories in the target dataset $D^T$. Denote $P_f^S$ as the joint distribution of\nstate, action, reward and return-to-go induced by the behavior policy $\\beta$ in the source environment,\nand $P_f^T$ in the target environment. Denote $d^\\pi$ as the marginal distribution of state s induced by any\npolicy $\\pi$ in the source environment, and $d^\\pi_T$ in the target environment.\nDenote $J^T(\\pi)$ as the expected cumulative reward under any policy $\\pi$ and the target environment.\nFor any return-to-go g in the source dataset $D^S$, we transform g by an oracle defined in (4.3) with\nothers remain the same, then we get a modified dataset $\\tilde{D}^S$. We denote the mixed dataset as\n$\\tilde{D} = D^T \\cup \\tilde{D}^S$.\nWe first show the sample complexity of DT with only the samples from the target dataset $D^T$.\nIf we only use the offline dataset $D^T$ collect from the target environment, i.e., at training time we\nminimizes the empirical negative log-likelihood loss:\n$\\hat{L}^T(\\pi) = - \\sum_{\\tau \\in D^T} \\sum_{1 < t \\le H} \\log \\pi(a_t | s_t, g(s_t)).$\nThen we get the following sample complexity guarantee based on the result in Brandfonbrener\net al. (2022).\nCorollary A.1. There exists a conditioning function f : S $\\rightarrow$ R such that assumptions (1)-(3)\nin Assumption 4.2, (1) and (2) in Assumption 4.3 hold. Further assume assumptions (1)-(3) in\nAssumption 4.4 hold. Then for some $\\delta \\in (0,1)$, with probability at least 1 - $\\delta$, we have\n$J^T(\\pi^*) - J^T(\\hat{\\pi}_f) \\le O( \\frac{H^2}{\\alpha_f}(\\sqrt{C_f}(\\frac{\\log |\\Pi|/\\delta}{N^T})^{1/4} + \\sqrt{\\epsilon_{approx}}) + \\frac{1}{\\alpha_f}H^2)$.\nNow we consider the case of mixed dataset, where we train our policy on both the target dataset\nand the source dataset using the proposed returned conditioned decision transformer methods.\nNote that the size of the target environment dataset is usually small, while the size of the source\nenvironment dataset is much larger, that is, $N^T \\ll N^S$. If we incorporate the modified source dataset\ninto the supervised learning, that is, we minimize the following empirical negative log-likelihood loss:\n$\\hat{L}^{mix}(\\pi) = - \\sum_{\\tau \\in \\tilde{D}} \\sum_{1 < t \\le H} \\log \\pi(a_t | s_t, g(s_t)).$  (A.1)\nAn observation is that, with the modified source dataset, the regret $J^T(\\pi^*) - J^T(\\hat{\\pi}_f)$ can be\nsignificantly reduced. We state this observation in the following theorem, which is the formal version\nof Theorem 4.5.\nTheorem A.2. There exists a conditioning function f such that Assumptions 4.2 and 4.3 hold.\nFurther assume Assumption 4.4 holds. Then for some $\\delta \\in (0,1)$, with probability at least 1 - $\\delta$, we\nhave\n$J^T (\\pi*) - J^T(\\hat{\\pi}_f) \\le O\\left(\\frac{C_f \\frac{N^S}{N^S+\\gamma_f N^T}}{\\alpha_f \\frac{N^S}{N^S+\\gamma_f N^T}} H^2\\left(\\sqrt{C} \\left(\\frac{\\log |\\Pi|/\\delta}{N^T + N^S}\\right)^{1/4} +\\sqrt{\\epsilon_{approx}}\\right) +\\frac{1}{\\alpha_f}H^2\\right)$. (A.2)"}, {"title": "B Proof of Theorem A.2", "content": "Lemma B.1 (Corollary 1 of Brandfonbrener et al. (2022)). Under the assumptions in Assumption 4.2,\nthere exists a conditioning function such that\n$J^T (\\pi^*) - J^T (\\pi_f^{RCSL}) \\le \\epsilon (\\frac{1}{\\alpha_f} - 3) H^2$.\nLemma B.2 (Lemma 1 of Brandfonbrener et al. (2022)). For any two policies $\\pi$, $\\pi'$, we have\n$||d_\\pi - d_{\\pi'}||_1 \\le 2H \\cdot \\mathbb{E}_{s \\sim d_{\\pi^\\ddagger}}[TV(\\pi(\\cdot|s)||\\pi'(\\cdot|s))].$\nWe define $d^{mix} := \\frac{N^T}{N^T+N^S}d^{\\ddagger}_T + \\frac{N^S}{N^T+N^S}d^{\\ddagger}_S$. Define\n$\\mathcal{L}(\\hat{\\pi}) =  \\mathbb{E}_{s \\sim d^{mix},g \\sim \\mathcal{P}_{T}(\\cdot |s)}[D_{KL} (\\mathcal{P}(\\cdot|s, g)|| \\hat{\\pi}(\\cdot|s, g))].$\nTheorem B.3. Consider any function f: S  $\\rightarrow$  R such that the assumptions in Assumption 4.3 hold.\nThen for any estimated RCSL policy that conditions on f at test time (denoted by $\\hat{\\pi}_f$), we have\n$J^T (\\pi_f^{RCSL}) - J^T (\\hat{\\pi}_f) < \\frac{C_f}{\\alpha_f} H^2 \\sqrt{2\\mathcal{L}(\\hat{\\pi})}.$\nProof. By definition and Lemma B.2, we have\n$J^T(\\pi_f) - J^T(\\hat{\\pi}_f) = H \\left( \\mathbb{E}_{P_{\\pi_f}^T}[r(s,a)] - \\mathbb{E}_{P_{\\hat{\\pi}_f}^T}[r(s,a)] \\right)\n<H \\cdot ||d_{\\pi_f} - d_{\\hat{\\pi}_f} ||_1\n<2 \\cdot \\mathbb{E}_{s \\sim d_{\\pi_f}^\\ddagger} [TV(\\pi_f(\\cdot|s)||\\hat{\\pi}_f(\\cdot|s))]H^2$.\nNext, we have\n$2 \\cdot \\mathbb{E}_{s \\sim d_{\\pi_f}^\\ddagger} [TV(\\pi_f(\\cdot|s)||\\hat{\\pi}_f(\\cdot|s))]\n$= \\mathbb{E}_{s \\sim d_{\\pi_f}^\\ddagger} \\left[\\int P_f^T(a|s,f(s)) - \\hat{\\pi}_f(a|s, f(s))| \\right]$\n$= \\mathbb{E}_{s \\sim d_{\\pi_f}^\\ddagger} \\left[\\int \\frac{P(f(s)|s)}{P(f(s)|s)} \\left|P_f^T(a|s,f(s)) - \\hat{\\pi}_f(a|s, f(s))| \\right]$\n$\\le 2 \\frac{C_f}{\\alpha_f} \\mathbb{E}_{s \\sim d_{\\pi_f}^\\ddagger, g \\sim \\mathcal{P}_{T}(\\cdot |s)} [TV(P(a|s, f(s))||\\hat{\\pi}(a|s, f(s)))]$"}, {"title": "C Detailed Experiment Setting", "content": "C.1 Environment and Dataset\nIn this section, we provide details of the environments and datasets used in our experiments.\nWe evaluate our approaches in the Hopper, Walker2D, and HalfCheetah environments, using the\ncorresponding environments from Gym as our target environments. For the target datasets, we\ncreate two distinct datasets: one with a small amount of data (1T) and another with a large amount\nof data (10T). The 10T dataset contains ten times the number of trajectories as the 1T dataset.\nWe employ BodyMass shift, JointNoise shift, Kinematic shift, and Morphology shift to construct\nthe source environments. The following descriptions provide detailed insights into the process of\ncreating these source environments.\nBodyMass Shift: Adjust the body mass of the agents. We change the mass of the body in\nthe Gym environment. For detailed body mass settings, refer to Table 4\nJointNoise Shift: Introduce noise to the agents' joints. We add the noise to the actions when\ncollecting the source data. For detailed information on the joint noise settings, refer to Table 4.\nKinematic shift:\nWalker2D - Broken Right Foot: The rotation range of the joint on the right leg's\nfoot is modified from [-45, 45] to [-0.45, 0.45]. The detailed modifications to the XML\nfile are as follows:\n<joint axis=\"0 -1 0\" name=\"foot_joint\" pos=\"0 0 0.1\" range\n=\"-0.45 0.45\" type=\"hinge\"/>"}]}