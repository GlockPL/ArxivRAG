{"title": "MusicFlow: Cascaded Flow Matching for Text Guided Music Generation", "authors": ["K R Prajwal", "Bowen Shi", "Matthew Lee", "Apoorv Vyas", "Andros Tjandra", "Mahi Luthra", "Baishan Guo", "Huiyu Wang", "Triantafyllos Afouras", "David Kant", "Wei-Ning Hsu"], "abstract": "We introduce MusicFlow, a cascaded text-to-music generation model based on flow matching. Based on self-supervised representations to bridge between text descriptions and music audios, we construct two flow matching networks to model the conditional distribution of semantic and acoustic features. Additionally, we leverage masked prediction as the training objective, enabling the model to generalize to other tasks such as music infilling and continuation in a zero-shot manner. Experiments on MusicCaps reveal that the music generated by MusicFlow exhibits superior quality and text coherence despite being over 2~5 times smaller and requiring 5 times fewer iterative steps. Simultaneously, the model can perform other music generation tasks and achieves competitive performance in music infilling and continuation. Our code and model will be publicly available.", "sections": [{"title": "1. Introduction", "content": "Audio generation has recently received a lot of attention from the research community as well as the general public. Making sound automatically has a lot of practical applications, including voice acting, podcast making, creating foley sound effects (Luo et al., 2023), making background music for movies (Liu et al., 2023c), and can greatly reduce the barrier for audio content creation. In terms of research, audio generation poses a few challenges due to its long-term structure and complex interaction between channels (e.g., multiple events may appear at the same time), thus being a suitable testbed for generative models.\nModeling approaches for audio generation has rapidly progressed over the past few years due to the development of sophisticated generative methods such as autoregressive language models (Kreuk et al., 2022; Wang et al., 2023) and non-autoregressive approaches (Le et al., 2023; Vyas et al., 2023; Liu et al., 2023a). A significant portion of the generative models are focused on speech and general sound, where state-of-the-art (SOTA) models (Vyas et al., 2023) are able to generate speech in diverse styles or general sound events in highly realistic manner. Compared to these two common modalities, music generation is a particularly challenging problem as it requires modeling long-term temporal structures (Agostinelli et al., 2023) and full frequency spectrum (M\u00fcller, 2015). Compared to typical sound events (e.g., dog barking), it contains harmonies and melodies from different instruments. Music pieces often consist of multiple tracks, which can be intricately woven together and may involve significant interference.\nWith the improvement of audio tokenizers (Zeghidour et al., 2021; D\u00e9fossez et al., 2022) and generative models, the quality of generated music has been greatly improved in recent works (Agostinelli et al., 2023; Copet et al., 2023). However, many prior works are built upon language models (Agostinelli et al., 2023; Copet et al., 2023; Yang et al., 2023), which requires a computationally expensive autoregressive inference procedure with number of forward passes proportional to the sequence length. This is worsened because many such models are based on a hierarchical set of units (e.g., Encodec tokens (Copet et al., 2023)), which brings another factor up to the computation. Despite the usage of non-autoregressive models such as diffusion models (Liu et al., 2023b; Huang et al., 2023; Forsgren & Martiros, 2022; Schneider et al., 2023), these approaches require hundreds of denoising steps during inference to achieve high performance. On the other hand, most of the existing models perform generation in a single stage, which models the audio waveform (Huang et al., 2023) or its low-level representation such as VAE features (Liu et al., 2023b) conditioned on text description directly. As music audios contains rich structural information and its text description can be very detailed (e.g., This is a live recording of a keyboardist playing a twelve bar blues progression on an electric keyboard. The player adds embellishments between chord changes and the piece sounds groovy, bluesy and soulful.), such approaches commonly fail to capture the"}, {"title": "2. Related Work", "content": "Early works on music generation are mostly on constrained scenarios, such as generating audios for a specific style (e.g., Jazz (Hung et al., 2019)) or a specific instrument (e.g., piano (Hawthorne et al., 2018)). More recent works shift the focus to generating music from free-form natural language descriptions. Typically, the language description is encoded by a pre-trained text encoder, which is then used for conditioning the model. One big class of the generation backbone falls into the category of language models (Agostinelli et al., 2023; Copet et al., 2023). In this type of model, an audio is quantized into discrete units through an auto-encoder (e.g., SoundStorm (Zeghidour et al., 2021), Encodec (D\u00e9fossez et al., 2022)). The language model is built to model the distribution of these units. During inference, the units sampled from the language model is decoded back into raw waveforms with the decoder directly without an explicit vocoder. The units are sampled either autoregressively (Copet et al., 2023; Agostinelli et al., 2023; Yang et al., 2023) or in conjunction with non-autoregressive unit decoding (Ziv et al., 2024). Diffusion-based music generation is typically built on top of the audio spectrogram. AudioLDM2 (Liu et al., 2023b) employs a variational auto-encoder to compress the spectrogram, where a DDIM (Song et al., 2020) model is trained with the compressed features. During inference, the generation is first decoded with the VAE decoder and transformed to waveform with a vocoder. Similar approaches include Riffusion (Forsgren & Martiros, 2022), which directly fine-tunes a stable diffusion model with spectrograms; MeLoDy (Lam et al., 2024) which proposes a LM-guided Diffusion with a focus on fast sampling speed; and Noise2Music (Huang et al., 2023), which also builds a diffusion-based vocoder; and StableAudio (Evans et al., 2024) which takes a latent diffusion approach, again with a focus on fast inference."}, {"title": "3. Method", "content": "Most of the existing methods directly learns the music distribution conditioned on text, which models the low-level audio features directly. In this work, our cascaded model is bridged by semantic features, which are learned separately with a self-supervised model. A similar approach to ours is MusicLM (Agostinelli et al., 2023), which learns two language models generating semantic and acoustic units respectively. However, our model relies on flow matching, which offers improved efficiency. Its non-autoregressive nature also enables the model to better leverage context and generalize to other tasks."}, {"title": "3.1. Background: Flow matching", "content": "Introduced in (Lipman et al., 2023), flow matching is a method addressing continuous transformation of probability densities. Specifically, it studies flow, a time-dependent diffeomorphic mapping $t : [0, 1] \u00d7 R^d \u2192 R^d$, defined via the ordinary differential equation (ODE):\n$\\frac{d}{dt}\\Phi_t(x) = \\nu_t(\\Phi_t(x))$\n$\\nu_t : [0,1] \u00d7 R^d \u2192 R^d$, namely a vector field, is parameterized by a neural network $\u03b8$ and learned by minimizing the flow matching objective: $L_{FM} = E_{t,p_t(x)}||V_t(x;\u03b8) \u2014 u_t(x)||^2$, where $p_t(x)$ is a probability density path and $u_t(x)$ is the corresponding vector field. As both $p_t(x)$ and $u_t(x)$ are generally unknown, Lipman et al. (2023) proposes minimizing the following conditional flow matching objective, which is equivalent to minimizing $L_{FM}$:\n$L_{CFM} = E_{t,p(x|x_1),q(x_1)}||V_t(x; \u03b8) \u2013 u_t(x|x_1)||^2$\nConsidering Gaussian distributions for $p_t(x|x_1) = N(x|\u03bc_t(x_1), \u03c3_t(x_1)^2I)$, the target vector field for Equation 2 can be solved in closed form: $u_t(x|x_1) = \\frac{\u03bc_t(x_1) - \u03c3_t(x_1)}{\u03c3_t(x_1)^2}(x - \u03bc_t(x_1)) + \u03bc'_t(x_1)$. Several diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021) can be described under the same framework with specific conditional probability paths of $\u03c3_t(x_1)$ and $\u03bc_t(x_1)$. Specifically, Lipman et al. (2023) considers a conditional probability path with Gaussian mean and standard deviation changing linearly in time with $\u03bc_t(x) = tx$ and $\u03c3_t(x) = 1\u2212(1\u2212\u03c3_{min})t$, which produces an optimal transport displacement mapping between conditional distributions. Due to its efficiency in both training and inference (Lipman et al., 2023; Le et al., 2023), we always stick to this conditional probability path as the default setting throughout the paper."}, {"title": "3.2. Problem Formulation", "content": "We now describe the music generation task and the general methodology based on flow matching that we employ. Given a dataset consisting of audio-text pairs (x, w),"}, {"title": "3.3. A Cascaded Flow-matching Approach", "content": "where $x \u2208 R^{T\u00d7C}$ (T: number of timesteps, C: number of channels) is the music audio and $w = {w_1, w_2, .., w_n}$ (wi: words) is the corresponding textual description represented as a sequence of words, the goal is to build a text-conditioned music generation model p(x|w). In addition to generating music from scratch, we further consider two practical tasks: music continuation $p(x_{t_1:T}|x_{1:t_1}, w)$ and music infilling $p(x_{t_1:t_2}|x_{1:t_1}, w, x_{t_2:T})$, with $t_1, t_2 \u2208 [0,T]$. In order to allow the model to perform all the text-guided music generation, we formulate our approach as an in-context learning task following (Le et al., 2023). Specifically, given a binary temporal mask m for a music track x, we train a conditional flow matching model predicting the vector field in the masked regions of the music track $x_m = x \u2299 m$ while conditioning on the unmasked regions of the music track $x_{ctx} = x \u2299 (1 \u2013 m)$ and the text caption w about the music piece. Formally, we train with the following flow matching loss: $L_{CFM} = E_{t,m,p(x|x_1),q(x_1,w)}||m \u2299 (v_t(x, x_{ctx}, w; \u03b8) \u2013 u_t(x|x_{ctx}, w))||^2$.\nIn addition to increasing the model capacity, such masked prediction objective also benefits generative modeling in general, as is shown in (Li et al., 2023a; Le et al., 2023). Within this framework, the three tasks of TTM, music continuation and music infilling can be conceptualized as setting specific mask values for $p(x_m|(1 \u2013 m) \u2299 x, w)$, where m is set to be $1_{t_1:T}, [0_{1:t_1}, 1_{t_1:T}]$ and $[0_{1:t_1}, 1_{t_1:t_2}, 0_{t_2:T}]$ respectively.\nTraining flow matching to directly generate music conditioned on text captions is difficult (Liu et al., 2023b) given the vast number of potential music tracks corresponding to a single text caption. As the text caption lacks the fine-grained information to adequately describe a music track, we propose to condition on a latent music representation that describes the music at the frame level.\nMusicFlow is thus divided into two stages: semantic modeling and acoustic modeling. The first stage outputs latent representations $h = (h_1, h_2, ....h_M) \u2208 R^{M\u00d7D}$ conditioned on a text caption w. In the second stage, we condition on the latent representations from the first-stage model, and a text caption w to output low-level acoustic features of N frames, $x = (x_1, x_2, ..., x_N) \u2208 R^{N\u00d7C}$. Note h and x are monotonically aligned. Both stages are inherently stochastic, meaning there are multiple potential (h, x) pairs for a given caption w. Therefore, we model these two stages separately with flow matching. In both stages, we predict masked vector fields as discussed before and provide detailed descriptions on the two stages below."}, {"title": "3.4. Stage 1: Music Semantic Flow Matching from Text", "content": "Our first-stage model consists of generating the semantics of the music piece conditioned on the text description. Here the semantics refer to high-level musical information instead of fine-grained details such as the general audio quality, which are inferred from the text description. For music, the semantics can refer to the melody and rhythm or harmony in a piano piece, analogous to the linguistic content in speech.\nSemantic latent representation One natural way of representing music is through music transcription. Transcripts in music typically refer to some notation system (e.g., music scores) that indicates the pitches, rhythms, or chords of a musical piece. A notable advantage of music transcript is its interpretability as it is human-readable and thus poses easy alignment with humans. However, for large-scale audio datasets, the associated music transcripts are usually not readily available, while manual annotation involves a non-trivial amount of labeling efforts. Automatic music transcription is a challenging task (Benetos et al., 2019) and the existing approaches (Bittner et al., 2022; Hawthorne et al., 2021; Hsu & Su, 2021; Su et al., 2019; Hawthorne et al., 2018) are heavily restricted to a single-instrument setting (e.g. piano, solo vocals, etc.).\nTo address the challenge of acquiring music transcriptions, we adopt HuBERT (Hsu et al., 2021), a popular self-supervised speech representation learning framework, to obtain frame-level semantic features, which can be regarded as a form of pseudo transcription. In essence, a HuBERT model consists of masked prediction of hidden units from the raw audio, which are inferred initially from MFCC and iteratively refined with layerwise features. For speech, HuBERT units have shown to correlate well with phonemes (Hsu et al., 2021) and its intermediate features entails rich semantic information (Pasad et al., 2023). In music understanding tasks, HuBERT has been successfully applied in source separation (Pasini et al., 2023), shedding light on its potential for capturing musical characteristics. As the original HuBERT model is pre-trained with speech only, we re-train HuBERT using music data following the original recipe. Training details are given in Section 4.\nSemantic flow matching Given a HuBERT model H, one can extract the semantic features from its lth layer l: $h = H(x) \u2208 R^{M\u00d7C_h}$, where $C_h$ is the HuBERT feature dimension. The layer index l is tuned in practice. A text-conditioned semantic flow-matching model p(h|w) can be trained given text-feature pairs (h, w). As described in Section 3.2, we adopt the masked prediction objective by conditioning on the context $h_{ctx} = m \u2299 h$, where m is a span mask of length M. More formally, we adopt the following training objective for the semantic modeling stage: $L_{H-CFM} = E_{t,m,p(h|h_1),q(h_1,w)}||M \u2299 (v_t(h, h_{ctx}, w; \u03b8) \u2013 u_t(h|h_{ctx}, w))||^2$. Cross-attention layers are integrated into"}, {"title": "3.5. Stage 2: Music Acoustic Flow Matching from text and semantics", "content": "the backbone model, enabling it to attend to the text description w, akin to (Rombach et al., 2021).\nAs an alternative to modeling the distribution of dense features, one can quantize the layerwise features h into units u and model the unit distribution p(uw) instead. For this case, a straightforward method is to build an autoregressive language model, which factorizes p(uw) =\n$\\prod_{n=1}^{M} P(u_n| u_{1:n-1}, w)$. Using a semantic LM has been explored in (Agostinelli et al., 2023) in hierarchical LMs for music generation. We also noticed its effectiveness when combined with flow matching, as will be shown in Section 4. However, this hybrid model is unsuitable for music infilling task due to its left-to-right nature.\nThe second-stage model aims to infer the low-level acoustic information (e.g., volume, recording quality) implied by the semantic tokens. Directly predicting raw waveforms ensures the completeness of information while imposes the challenge of modeling the long sequences. To balance between quality and sequence length, we use Encodec (D\u00e9fossez et al., 2022) to map raw"}, {"title": "4. Experiments", "content": "waveforms into dense feature sequences. In a nutshell, Encodec (D\u00e9fossez et al., 2022), an auto-encoder based on residual vector quantization, comprises of an encoder E and decoder D. During training, we map raw waveforms into acoustic features with the encoder E: $e = E(x) \u2208 R^{N\u00d7C_e}$, where $C_e$ is the feature dimension of encodec.\nAcoustic flow matching The second-stage flow matching aims to model the following conditional distribution: p(eh, w). Similar to semantic flow matching, we apply masked prediction and the corresponding training objective is formulated as: $L_{E-CFM} = E_{t,m,p(e|h_1,e_1),q(e_1,h_1,w)}||m\u2299 (v_t(e, e_{ctx}, h_1, w; \u03b8) \u2013 u_t(e|e_1, h_1, e_{ctx}, w))||^2$. As the semantic and acoustic features are aligned (N/M \u2248 sre/sru, sr: sample rate), we simply linearly interpolate the HuBERT feature sequence h to length N before feeding it into encoding.\nNote though Encodec includes multiple different codebooks to quantize the latent features, we directly model the dense feature sequence from the encoder E without any quantization. This avoids the length increase brought by using multiple codebooks, where the total number of discrete tokens is K-1 times more than the dense feature length. Thus, it eliminates the necessity of carefully designing interleaving pattern of discrete tokens to account for dependencies"}, {"title": "3.6. Classifier-free guidance", "content": "During inference, we sequentially sample the HuBERT features h and encodec features \u00ea using the estimated vector field $v_t(h, h_{ctx}, w; \u03b8_h)$ and $v_t(e, e_{ctx}, h, w; \u03b8_e)$ following the ODE equation 1. The acoustic features are decoded into waveforms via the decoder D of the Encodec.\nAs is common in diffusion models, classifier-free guidance is a widely used technique to balance sample diversity and text coherence. Thus we also adopt it in our cascaded generation framework. For flow matching, using classifier-free guidance (Zheng et al., 2023) consists of computing a linear combination between conditional and unconditional vector field: $\\tilde{v}_t(h, w, h_{ctx}; \u03b8_h) = (1 + a_h)v_t(h, w, h_{ctx} ; \u03b8_h) \u2013 a_h v_t^{uncond}(h; \u03b8_h)$ and $\\tilde{v}_t(e, e_{ctx}, h, w; \u03b8_e) = (1 + a_e)v_t(e, e_{ctx}, h, w; \u03b8_e) \u2013 a_e v_t^{uncond}(e; \u03b8_e)$.\nIn order to model the unconditional vector field $v_t^{H,uncond}$ and $v_t^{E,uncond}$ with $\u03b8^h$ and $\u03b8^e$, we randomly drop the conditions (e.g., text, the contextual features) in both flow models with probability $p^H$ and $p^E$ in training, whose values are also tuned."}, {"title": "4.1. Experimental Setup", "content": "Data We use 20K hours of proprietary music data (~400K tracks) to train our model. We follow the original recipe in (Hsu et al., 2021) to train the music HuBERT model with music data. For data preprocessing, we filter out all the vocal tracks and resample all the data to 32kHz and perform channel-wise averaging to downmix all multi-channel music into mono. Only text descriptions are retained for training, while the other metadata such as genre, BPM and music tags are discarded. We evaluate our model on MusicCaps (Agostinelli et al., 2023), which incorporates 5.5K 10s-long audio samples annotated by expert musicians in total. For subjective evaluation, we use the 1K genre-balanced subset following (Agostinelli et al., 2023).\nImplementation details We follow (Le et al., 2023) for backbone architectures in both stages, which are Transformers (Vaswani et al., 2017) with convolutional position embeddings (Baevski et al., 2020), symmetric bi-directional ALiBi self-attention bias (Press et al., 2021) and UNet-style skip connections. Specifically, the transformers of the first and second stage include 8 and 24 layers of 12 attention heads with 768/3072 embedding/feed-forward network (FFN) dimension, leading to 84M and 246M parameters (see Section 4.4.2 for ablation on model size). The models are trained with an effective batch size of 480K frames, for"}, {"title": "4.4. Ablation Study", "content": "300K/600K updates in two stages respectively. For efficiency, audios are randomly chunked to 10s during training. For masking, we adopt the span masking strategy and the masking ratio is randomly chosen between 70 \u2013100%. Condition dropping probabilities (i.e., $p^H$ and $p^E$) are 0.3 for both stages. We use the Adam optimizer (Kingma & Ba, 2014) with learning rate 2e-4, linearly warmed up for 4k steps and decayed over the rest of training.\nObjective evaluation We evaluate the model using the standard Frechet Audio Distance (FAD) (Kilgour et al., 2019), Frechet Distance (FD) and KL divergence (KLD) based on the pre-trained audio event tagger PANN (Kong et al., 2019), and Inception score (ISC) (Salimans et al., 2016), which are adapted from sound generation and has been widely used in prior works for text-to-music generation (Agostinelli et al., 2023; Huang et al., 2023; Copet et al., 2023; Li et al., 2023a). Specifically, FAD and FD measure distribution-level similarity between reference samples and generated samples. KLD is an instance level metric computing the divergence of the acoustic event posterior between the reference and the generated sample for a given description. The metrics are calculated using the audioldm_eval toolkit. To measure how well the generated music matches the text description, we use CLAP2 similarity, defined as the cosine similarity between audio and text embeddings.\nSubjective evaluation In addition to the objective metrics mentioned above, we further conduct subjective evaluation with human annotators. The study consists of multiple pairwise studies following the evaluation protocol of Agostinelli et al. (2023). Specifically, each human annotator is presented with pairs of audio clips generated by two different systems and is required to give their preference based on how well the generated music captures the elements in the text description."}, {"title": "4.2. Main Results", "content": "Below we analyze the impact of different design choices in MusicFlow, particularly focusing on the necessity of a two-stage cascade and how model scales differently in each stage."}, {"title": "4.3. Infilling and Continuation", "content": "Table 1 compares our model to prior works in text-to-music generation on MusicCaps in terms of objective metrics. Given the variation in models used for evaluation in prior works, we primarily rely on FAD, which is computed using the vggish feature (Kilgour et al., 2019) and serves as a unified benchmark across different studies. Specifically, when evaluating MusicGen, we opt for its medium version due to its overall superior performance compared to other variants (Copet et al., 2023). For MusicGen and AudioLDM2, we use the public model checkpoints in order to get FD, ISc and CLAP similarity since these metrics were not reported in the paper.\nOne advantage of MusicFlow is its ability to handle multiple audio-conditioned generative tasks, such as infilling and continuation, with a single model. These tasks have also been explored in (Li et al., 2023a), albeit without reported quantitative metrics. Due to lack of baselines, we compare the model performance to the our own text-to-music model, as detailed in Table 1. For the infilling task, we infill the middle 70% of the audio segment. For the continuation task, given the beginning 30% of the audio clip, the model generates the remaining 70%.\nIn MusicFlow, we additionally present the results of a model with the first stage functioning as a language model, predicting HuBERT units as detailed in Section 3. The language model includes 24 transformer layers, 16 attention heads, and a hidden dimension of 1024, leading to ~ 300M parameters in total.\nIn comparison to all prior works, our model exhibits a significant reduction in size, with parameter reduction ranging from 50% to 80%, while remaining competitive in terms of generation quality. Compared with a standard diffusion model - AudioLDM-2, MusicFlow achieves a 10% lower FAD (3.13 \u2192 2.82) with approximately 50% fewer parameters. Similarly, compared to the language-model-based MusicGen, our approach shows a 20% improvement in FAD (3.40 \u2192 2.82) while using only 20% of the parameters. These results highlight the efficiency of our approach.\nIt's noteworthy that MusicLM (Agostinelli et al., 2023) shares similarities with ours, incorporating semantic and acoustic modeling stages based on language models. However, we surpass this approach by roughly 30% in FAD with less than 65% of the parameters. Additionally, in contrast to the current state-of-the-art model on MusicCaps, Jen-1 (Li et al., 2023a), our results shows a mixture of results. While falling behind in FAD, we outperform it in KL divergence with only half of the parameters.\nLM vs. FM for first stage In addition to our main approach, we investigate the integration of a language model for first-stage modeling. Both approaches share the second-stage model. According to the last two rows of table 1, using a first-stage LM yields marginally superior results compared to using a flow matching model. This implies that semantic features in music audios possess discrete structures, which can be well captured by an auto-regressive language model. Nonetheless, for the sake of model efficiency and task generalization, we adhere to using the flow matching cascade moving forward.\nSubjective evaluation Figure 4.2 shows the pairwise comparison between our model and prior works. In particu-"}, {"title": "4.4.1. SINGLE-STAGE VS. TWO-STAGE MODEL", "content": "lar, we compare MusicFlow to AudioLDM2 and MusicGen, which are the only two publicly available models in Table 1. For our model, we use the bidirectional FM+FM configuration in Table 1. Our model surpasses both AudioLDM2 and MusicGen. This observation aligns with the objective metrics presented in Table 1. However, it's worth noting that there is still a gap between our model and the ground-truth.\nInference Efficiency In Table 1, we only lists the model size, which is one aspect of model efficiency. In Figure 3, we plot how FAD changes when we vary the number of function evaluations (NFE) during inference. For flow matching and AudioLDM2, this is achieved by adjusting the number of iterative steps in the ODE solver and DDIM steps, respectively. Since MusicFlow involves two flow matching models, we simply aggregate the NFE of the two modules as the final NFE we plot. For comparison, we further show the MusicGen, which runs a fixed number of auto-regressive steps. As shown in Figure 3, MusicFlow outperforms MusicGen (FAD: 3.13 vs. 3.40) by using 20% of inference steps. Running with longer steps further improves the performance. The final model takes only 50% the network forward passes of MusicGen. AudioLDM2 exhibits a similar trend to ours, although its generation quality consistently lags behind with the same number of inference steps.\nTable 2. Performance of MusicFlow on various music generation tasks on MusicCaps dataset. We compare with AudioLDM-2 (Liu et al., 2023b) for text-to-music and AudioLDM for music infilling and continuation.\nWe compare MusicFlow to a simple flow matching baseline of directly generating music based on text descriptions with-"}, {"title": "4.4.2. EFFECT OF MODEL SIZE", "content": "As is shown in Table 4.2, our model effectively uses the context to enhance audio generation. In both settings, using a 3s audio context enables a nearly 50% reduction in FAD. The text-to-audio similarity is slightly increased in infilling (0.44 \u2192 0.45). We hypothesize this may be because the CLAP model struggles to discern fine-grained details in the text description. Hence, we conduct a subjective study to measure text faithfulness. The MOS scores of text-to-music, conditnuation and infilling are respectively 3.34 \u00b1 0.18, 3.47 \u00b1 0.18, 3.42 \u00b1 0.19 with 95% confidence interval. This confirms an improvement in text faithfulness through context utilization.\nAdditionally, among other metrics, we compute the CLAP-Audio score, defined as the cosine similarity between the embeddings of the generated and ground-truth audios. Compared to text-only generation, the generated audio achieved higher scores, suggesting better acoustic matching through context conditioning. Finally, we measure the CLAP similarity between the generated segment and the original context (CLAP-SIM). Both settings achieve scores close to 1, implying coherence between the generation and context.\nout the intermediate HuBERT features in Table 3. Including a stage of HuBERT prediction consistently improves the performance across various metrics regardless of model size. HuBERT-based flow matching brings a ~ 30% relative improvement in terms of FAD. Note while we increased the size of the single-stage model to 431M, it did not yield additional gains, despite having more parameters."}, {"title": "4.4.3. NUMBER OF TRAINING ITERATIONS", "content": "Empirically, we observed the performance of our models is heavily influenced by the model size. In this analysis, we delve into the impact of model size in each stage.\nSecond-stage: Text + HuBERT to Music. We first examine how the size of the second-stage model, specifically the Text + HuBERT features \u2192 music, affects the overall performance. We keep the best first-stage model and scale the second-stage model by altering the number of transformer layers and the hidden dimension of each layer (see Table 4). The performance improves as we increase the number of layers until reaching 24 layers. Beyond this point, increasing the number of layers or feature dimensions results in degradation, suggesting a potential overfitting issue of the model.\nWe notice the performance of both stages in MusicFlow is sensitive to the number of training iterations. Generally, longer training boosts performance, as can be seen from Table 6. While varying the number of training iterations, we maintains the sizes of best models from Table 5 and 4. Comparing the two stages, longer training consistently enhances performance in the second stage, while there is a degradation in performance with further increases in training iterations in the first stage. This aligns with our observations in model scaling, which highlight the different tendencies of model overfitting in both stages.\nFirst-stage: Text to HuBERT. We fix the configuration for our second-stage model based on the above findings and vary only the first-stage configuration (see Table 5). Unlike the second-stage, where the best model is with 24 transformer layers, our best first-stage model for Text \u2192 HuBERT feature prediction is notably smaller with an optimal configuration of only 8 layers. According to Table 5, smaller models typically perform equally well or even better than their larger counterparts in the first-stage model. We hypothesize that predicting HuBERT features is simpler than predicting the low-level Encodec features, particularly for shorter music pieces with standard music structures, as the former consists of learning only the coarse-grained semantics. Consequently, a larger variant is more susceptible to overfitting compared to the second-stage scenario."}, {"title": "4.4.4. CHOICE OF SEMANTIC LATENT REPRESENTATION", "content": "4.4.4. CHOICE OF SEMANTIC LATENT REPRESENTATION\nThe first stage model predicts semantic latent representations conditioned on text tokens. The choice of the semantic latents has an impact on the final performance. In addition to HuBERT units, we also experiment with MERT units (Li et al., 2023b) using the officially released pre-trained music model. In Table 7, we can see that it is clearly worse compared to using HuBERT units."}, {"title": "4.4.5. CHOICE OF ACOUSTIC LATENT REPRESENTATION", "content": "Table 7. Impact of using a different semantic latent representation instead of HuBERT. We compare with MERT (Li et al., 2023b) units below.\nThe"}]}