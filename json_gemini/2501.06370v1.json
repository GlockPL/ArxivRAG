{"title": "Towards a Probabilistic Framework for Analyzing and Improving LLM-Enabled Software", "authors": ["Juan Manuel Baldonado", "Flavia Bonomo-Braberman", "V\u00edctor A. Braberman"], "abstract": "Ensuring the reliability and verifiability of large language model (LLM)-enabled systems remains a significant challenge in software engineering. We propose a probabilistic framework for systematically analyzing and improving these systems by modeling and refining distributions over clusters of semantically equivalent outputs. This framework facilitates the evaluation and iterative improvement of Transference Mod-els-key software components that utilize LLMs to transform inputs into outputs for downstream tasks. To illustrate its utility, we apply the framework to the autoformalization problem, where natural language documentation is transformed into formal program specifications. Our case illustrates how probabilistic analysis enables the identification of weaknesses and guides focused alignment improvements, resulting in more reliable and interpretable outputs. This principled approach offers a foundation for addressing critical challenges in the development of robust LLM-enabled systems.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, Large Language Models (LLMs), such as GPT-4 [12] and Gemini [16], have demonstrated remarkable capabilities across diverse applications. These successes are largely attributed to their instruction-following abilities and in-context learning. By conditioning on task-specific instructions (zero-shot) or a small set of examples (few-shot), LLMs have been shown to perform a wide array of tasks effectively [3]. This adaptability has led to a proliferation of applications leveraging LLMs to elicit various downstream tasks via prompting. One area where LLMs have seen significant application is software engineering, including testing and verification [2]. The integration of LLMs into software systems is a rapidly growing trend, but it presents numer-ous challenges [5]. Among these challenges is the lack of guarantees regarding task performance, including phenomena such as hallucination [7]. Moreover, selecting answers based solely on the highest-probability outputs can be sometimes misleading due to surface form competition [6]. Another critical limitation is the absence of a reliable density estimation function, which impedes entropy-based analyses that could estimate the certainty LLMs assign to their answers [4]."}, {"title": "II. PRELIMINARIES", "content": ""}, {"title": "A. Large Language Models", "content": "These predictive models are parameterized functions, whose parameter sets are often initially found by optimizing an ob-jective or loss function for next-token prediction with respect to a training corpus. Thus, at its core, an LLM yields a probability distribution over the set of tokens or vocabulary (i.e., $P(t_k|x)$, the probability of the k-th token being $t_k$ given the context $x$). LLMs are typically used in its generative role by performing some particular decoding approach us-ing the next-token predictive model [11]. In general, LLM-enabled software leverages them by injecting prompts that condition continuations/responses (i.e., LLM predicts next-tokens conditioned by the prompt), thus hopefully eliciting the desired downstream tasks. Task-agnostic prompting strategies are frequently used (e.g. [10], [17])."}, {"title": "B. Autoformalization", "content": "As indicated, our illustration is based on the problem of autoformalization in software verification. This is the trans-formation from natural language descriptions (docstrings, typ-ically available in development processes) into some formally correct and automatically verifiable format [15] which enables automated reasoning on program's correctness. LLMs have shown great promise in helping to automate and bridge this gap between informal descriptions and formal specifications (e.g. [14]). In our illustration, we choose Dafny [9] as the target formal language and particularly the pre and post condition sections."}, {"title": "III. TRANSFERENCE MODELS", "content": "Firstly, we argue that in most LLM-enabled applications, it is possible to understand their LLMs interactions as part of the implementation of one or more \u201cTransference Models\u201d (TMs). Those components use (typically, prompt-modulated) LLMs to elicit downstream tasks as the key (but not unique) means for transforming some input data into some output data. That transformation has potentially stochastic nature given the underlying predictive model and (sampling-based) decoding strategy of the LLMs. Thus, following the formalization of Sun et al. [14], TMs behavior can be formulated as a function of input-output pairs into reals, that is, $T : I \u00d7 O \u2192 R$, where $T(i, o)$ denotes the probability that $i \u2208 I$ is transferred to $o \u2208 O$, and for $i \u2208 I$, $T(i, \u00b7)$ is a probability distribution over O. As an example, in autoformalization proposals, typically, the TM is a stochastic process that links natural language with annotations as the expected type of input-output pairs (e.g., [2], [14]).\nTMs might be implemented straightforwardly by zero-shot instruction prompting that includes some representation of the input. However, TMs could also be much more sophisticated. For instance, they could include statically or dynamically orchestrated external tools (e.g., some feedback signal for corrective tasks), general reactive LLM-triggered external computation [18], the chaining of a series of lower-level transference models [8], etc. In fact, TMs are the core of promptware and the component under analysis in our approach thus we assume input/output transformation performed can be validated or verified by developers/testers (or LLMs, in the future)."}, {"title": "IV. DISTRIBUTIONS OVER SEMANTIC DOMAINS", "content": "Given that Language Models are density estimation func-tions that assign probability to every possible string, our aim is to gain insight on the potential behavior of an LLM-based solution (e.g., when testing it, when evaluating it, when validating, when engineering it, when over-sighting it, etc.) it is key to embrace the stochastic nature of generative AI and approximate and characterize the yielded underlying probability distribution of transference models. Moreover, we concur with others (e.g. [4], [6]) that, at least for analysis time, probability should be deemed assigned to concepts, not strings: there are often many (or even infinite) strings that represents a given idea equally well (e.g., in the autoformalization problem, logically equivalent annotations) and share the same meaning when embedded into the abstract domain 01.\nSecondly, we also hypothesize that, when the meaning classes that get the most of the probability mass are mis-aligned with expected transference model result, the nature for misalignment can be, in general, stated in terms of mistakes humans have already studied and categorized in the corre-sponding problem domain. For instance, for the autoformal-ization problem studied, \u201ctoo weak/strong pre/post conditions\" are well-known concepts of formalization mistakes ontology. As we will show later, they are also typically the perfect fit for characterizing situations when the most frequent meaning class is not aligned with the formalization task goal."}, {"title": "A. Computing Distributions of Transference Models", "content": "Given the potential richness of TMs, decoding strategies plus the fact next-token probabilities are not always accessible, for a given input, we approximate the TM probability distri-bution by the empirical categorical distribution on meaning-classes that are identified and built on-the-fly by clustering generated outputs (e.g., by checking annotations to be SMT-equivalent). More concretely, re-execution of code of the trans-ference model with the same input triggers the (many times stochastic) decoding adapters [11] of the involved LLMs and thus the entire TM behaves as a black box stochastic process (whose behavior depends on hyperparameters and settings of those adapters like temperature, etc.). Yielded outputs are then clustered according to the equivalence relation and empirical distribution on classes is computed."}, {"title": "V. PROBABILITY DISTRIBUTIONS AND IMPROVEMENT", "content": "We say that, for a given TM input, (empirical) distribution over meaning classes is aligned when the class with the largest assigned probability is a correct one. When this is not the case, we indicate two scenarios: either the distribution is misaligned: there is a class that is aligned although not the winning"}, {"title": "VI. ILLUSTRATIVE RESULTS ON AUTOFORMALIZATION", "content": "Next we show an anecdotal illustration in line with stated hypothesis (no claim of generalization). We use an existing dataset of Dafny programs; the CloverBench dataset consists of 62 small hand-written example programs similar to those found in standard computer science textbooks [14]. Each program in the dataset consists of a single method and includes a Dafny implementation, annotations specifying pre-conditions and post-conditions, and a docstring that describes program's intention. We extract from those programs docstrings and method signatures and we use pre and post conditions as the ground-truth to assess alignment of the most probable meaning class. We get the empirical approximation by re-execution of the transference model (30 times in this case) and clustering with the assistance of a SMT solver. For the sake of simplicity, we treat each syntactically-invalid result as its own class, rather than assigning probability mass to the edit-distance close valid class. Compared to collapsing all invalid results into a single class, it may lead to less concentration in verdicts, especially when the model assigns significant probability to diverse invalid outputs. We opted for the Gemini 1.5 Flash model with a context of 1000 tokens. Hyperparameters were the default ones for that LLM: topk with k = 40, topp with p = 0.95 and temperature 0.7. This means we work with a generative-process distribution that is more skewed (and tail-truncated) than the underlying prompt-modulated next-token prediction model distribution. Yet, this illustrates a plausible generative-process distribution of the LLM running at the core of the TM which stochastic behavior one wants to approximate under such decoding settings. More details and data can be found in [1]."}, {"title": "VII. CONCLUSIONS AND FUTURE WORK", "content": "We illustrate some principled engineering insights gained when probability distributions over meaning-classes are re-covered from the stochastic behavior of transference models. This paper illustrates concentration and misalignment as the \"debugging\" guidance but likely other focus might be rele-vant as characterizing inputs that lead to non concentrated distributions. Several RQs and experiments are necessary to claim generalizability and impact of the approach (e.g., when does improvement on a test set translate into generalized improvement?, which is the impact of decoding strategy in the analysis?, etc.). If that were the case, several conceptual and automation paths are possible including the definition of a richer language to predicate on clustered distributions and notions of compositionality of TMs. Last but not least, it is foreseeable the help of AI-based solutions to charac-terize misalignments and troublesome/adversarial inputs and even the assistance in decomposition, prompt-engineering and hyperparameter tuning."}]}