{"title": "Neural MP: A Generalist Neural Motion Planner", "authors": ["Murtaza Dalal", "Jiahui Yang", "Russell Mendonca", "Youssef Khaky", "Ruslan Salakhutdinov", "Deepak Pathak"], "abstract": "The current paradigm for motion planning generates solutions from scratch for every new problem, which consumes significant amounts of time and computational resources. For complex, cluttered scenes, motion planning approaches can often take minutes to produce a solution, while humans are able to accurately and safely reach any goal in seconds by leveraging their prior experience. We seek to do the same by applying data-driven learning at scale to the problem of motion planning. Our approach builds a large number of complex scenes in simulation, collects expert data from a motion planner, then distills it into a reactive generalist policy. We then combine this with lightweight optimization to obtain a safe path for real world deployment. We perform a thorough evaluation of our method on 64 motion planning tasks across four diverse environments with randomized poses, scenes and obstacles, in the real world, demonstrating an improvement of 23%, 17% and 79% motion planning success rate over state of the art sampling, optimization and learning based planning methods. Video results available at mihdalal.github.io/neuralmotionplanner.", "sections": [{"title": "I. INTRODUCTION", "content": "Motion planning is a longstanding problem of interest in robotics, with previous approaches ranging from potential fields [1]\u2013[3], sampling (RRTs and Roadmaps) [4]\u2013[10], search (A*) [11]\u2013[13] and trajectory optimization [14]\u2013[17]. Despite being ubiquitous, these methods are often slow at producing solutions since they largely plan from scratch at test time, re-using little to no information outside of the current problem and what is engineered by a human designer. Since motion-planning is a core component of the robotics stack for manipulation, its speed, capability and ease of use form a core bottleneck to developing efficient and reliable manipulation systems.\nOn the other hand, humans can generate motions in a closed loop manner, move quickly, react to various dynamic obstacles, and generalize across a wide distribution of problem instances. Rather than planning open loop from scratch, people draw on their vast amounts of experience moving and"}, {"title": "II. RELATED WORK", "content": "Approaches for Training General-Purpose Robot Policies\nPrior work on large scale imitation learning using expert demonstrations [18], [19], [26]\u2013[29] has shown that large models trained on large datasets can demonstrate strong performance on challenging tasks and some varying levels of generalization. On the other hand, sim2real transfer of RL policies trained with procedural scene generation has demonstrated strong capabilities for producing generalist robot policies in the locomotion regime [21]\u2013[23], [25]. In this work, we combine the strengths of these two approaches to produce powerful neural motion planning policies. We propose a method for procedural scene generation in simulation and combine it with large scale imitation learning to produce strong priors which we transfer directly to over 64 motion planning problems in the real world.\nProcedural Scene Generation for robotics Automatic scene generation and synthesis has been explored in vision and graphics [30]\u2013[33] while more recent work has focused on embodied AI and robotics settings [28], [34]\u2013[36]. In particular, methods such as Robogen [35] and Gen2sim [36] use LLMs to propose tasks and build scenes using existing 3D model datasets [37] or text-to-3D [38], [39] and then decompose the tasks into components for RL, motion-planning and trajectory optimization to solve in simulation. Our method is instead rule-based rather than LLM-based, is designed specifically for generating data to train neural motion planners (see Sec. III-A), and demonstrates that policies trained on its data can indeed be transferred to the real world. MotionBenchmaker [40], on the other hand, is similar to our data generation method in that it autonomously generates scenes using programmatic assets. However, the datasets generated by MotionBenchmaker are not realistic: floating robots, a single major obstacle per scene and primitive objects that are spaced far apart. By comparison, the scenes and data generated by our work (Fig. 2) are considerably more diverse, containing additional programmatic assets that incorporate articulations (microwave, dishwasher), multiple large obstacles per scene (up to 5), complex meshes sampled from Objaverse [37], and tightly packed obstacles.\nNeural Motion Planning Finally, there is a large body of recent work [41]\u2013[47] focused on imitating motion planners in order to accelerate planning. MPNet [41], [43], [48] trains a network to imitate motion planners, then integrates this prior into a search procedure at test time. Our method leverages large scale scene generation and sequence modeling, enabling it to use a faster optimization process at test time while obtaining strong results across a diverse set of tasks. \u039c\u03c0Nets [42] trains the SOTA neural motion planning policy using procedural scene generation and demonstrates transfer to the real world. Our approach is similar, albeit with 1) much more diverse scenes via programmatic asset generation and complex real-world meshes, 2) a more powerful learning architecture and multi-modal output distributions and 3) test-time optimization to improve performance at deployment, enabling significantly improved performance over \u039c\u03c0Nets."}, {"title": "III. NEURAL MOTION PLANNING", "content": "Our approach enables generalist neural motion planners, by leveraging large amounts of training data generated in simulation via expert planners. The policies can generalize to out-of-distribution settings by using powerful deep learning architectures along with diverse, large-scale training data. To further improve the performance of these policies at deployment, we leverage test time optimization to select the best path out of a number of options. We now describe each of these pieces in more detail."}, {"title": "A. Large-scale Data Generation", "content": "One of the core lessons of the deep learning era is that the quality and quantity of data is crucial to train broadly capable models. We leverage simulation to generate vast datasets for training robot policies. Our approach generates assets using programmatic generation of primitives and by sampling from diverse meshes of common objects. These assets are combined to create complex scenes resembling real world scenarios (Fig. 2), as described in Alg. 1.\nProcedural Generation From Primitives How do we generate a large enough number of diverse environments to train a generalist policy? Hand designing each environment is tedious, requiring significant human effort per scene, which doesn't scale well. Instead, we take the approach of procedural scene generation, using a set of six parametrically variable categories - shelves, cubbies, microwaves, dishwashers, open boxes, and cabinets. These categories are representative of a large set of objects in everyday scenarios that robots encounter and have to avoid colliding with. Each category instance is constructed using a combination of primitive cuboid objects and is parameterized by category specific parameters which define the asset. Specifically a category instance g is comprised of N cuboids g = {X0..Xi...XN}, which satisfy the category level constraint given by C(g). For controlled variation within each category, we make use of parametric category specific generation functions X (p) = {Xo..xi.XN}, s.t. C(X(p)), where p specifies the size and scale of each of the cuboids, their relative positions, and specific axes of articulation. The constraint C(.) relates to the relative positions, scales and orientations of the different cuboids, e.g for the microwave category the constraint ensures each of the walls are of the same height, and that the microwave has a hinge door."}, {"title": "B. Generalist Neural Policies", "content": "We would like to obtain agents that can use diverse sets of experiences to plan efficiently in new settings. In order to build generalist neural motion planning policies, we need an observation space amenable to sim2real transfer, and utilize an architecture capable of absorbing vast amounts of data.\nObservations: We begin by addressing the sim2real transfer problem, which requires considering the observation and action spaces of the trained policy. With regards to observation, point-clouds are a natural representation of the scene for transfer [42], [49]\u2013[52], as they are 3D points grounded in the base frame of the robot and therefore view agnostic, and largely consistent between sim and real. We include proprioceptive and goal information in the observations, consisting of the current joint angles qt, the target joint angles g, in addition to the point-cloud PCD.\nNetwork Architecture: We require an architecture capable of scaling with data while performing well on multi-modal sequential control problems, e.g. motion planning. To that end, we design our policy \\( \\pi \\) (visualized in Fig. 3) to be a sequence model to imitate the expert using a notion of history which is useful for fitting privileged experts using partially observed data [28]. In principle, any sequence modeling architecture could be used, but in this work, we opt for LSTMs for their fast inference time and comparable performance to Transformers on our datasets (see Appendix). We operate the LSTM policy over joint embeddings of PCDt, qt, and g with a history length of 2. We encode point-clouds using PointNet++ [53], while we use MLPs to encode qt and gt. We follow the design decisions from \u039c\u03c0Nets regarding point-cloud observations: we segment the robot point-cloud, obstacle point-cloud and the target robot point-cloud before passing it to PointNet++. For each time-step, we concatenate the embeddings of each of the observations together into"}, {"title": "C. Deploying Neural Motion Planners", "content": "Test time Optimization While our base neural policy is capable of solving a wide array of challenging motion planning problems, we would still like to ensure that these motions are safe to be deployed in real environments. We enable this property by combining our learned policy with a simple light-weight optimization procedure at inference time. This relies on a simple model that assumes the obstacles do not move and the controller can accurately reach the target way-points. Given world state s = [q, e] (e is the environment state), the predicted world state is s' = [q+\\(\\hat{q}\\), e] where \\( \\hat{q} \\) is the policy prediction. With this forward model, we can sample N trajectories from the policy using the initial scene point-cloud to provide the obstacle representation and estimate the number of scene points that intersect the robot using the linear forward model. We then optimize for the path with the least robot-scene intersection in the environment, using the robot Signed Distance Function (SDF). Specifically, we optimize the following objective at test time:\n\\[\n\\min_{ \\tau \\sim p_{\\pi_{\\theta}}} \\sum_{t=1}^{t=T} \\sum_{k=1}^{k=K}  1\\{SDF_{q_t} (PCD_k) < \\epsilon\\}\n\\]\nin which \\( p_{\\pi_{\\theta}} \\) is the distribution of trajectories under policy \\( \\pi_{\\theta} \\) with a linear model as described above, PCD is the kth point of the obstacle point-cloud (with max K = 4096 points) and \\( SDF_{q_t} \\) is the SDF of the robot at the current joint angles. In practice, we optimize this objective with finite samples in a single step, computing the with minimal objective value by selecting the path with minimal objective value across 100 trajectories. We include a detailed analysis of the properties of our proposed test-time optimization approach in the Appendix.\nSim2real and Deployment For executing our method on a real robot, we predict delta joint way-points which we then linearly interpolate and execute using a joint space controller. Our setup includes four extrinsically calibrated Intel RealSense cameras (two 435 and two 435i) positioned at the table's corners. To produce the segmented point cloud for input to the robot, we compute a point-cloud of the scene using the 4 cameras, segment out the partial robot cloud using a mesh-based representation of the robot to"}, {"title": "IV. EXPERIMENTAL SETUP", "content": "In our experiments, we consider motion planning in four different real world environments containing obstacles (see Appendix). Importantly, these are not included as part of the training set, and thus the policy needs to generalize to perform well on these settings. We begin by describing our environment design, then each of the environments, and finally our evaluation protocol and comparisons.\nEnvironment Design We evaluate our motion planner on tabletop motion planning tasks which we subdivide into environments, scenes, and configurations. We evaluate on four different environments, with each environment containing 1-2 large receptacles that function as the primary obstacles. For each environment, we have four different scenes which involve significant pose variation (over the entire tabletop) of the primary obstacles, table height randomization, as well as randomized selection, pose and orientation of objects contained within the receptacles. For each environment, we have two scenes with obstacles and two without obstacles. For each scene, we evaluate on four different types of start (\\(q_0\\)) and goal (g) angle pairs: 1) free space to free space, 2)"}, {"title": "V. EXPERIMENTAL RESULTS", "content": "To guide our evaluation, we pose a set of experimental questions. 1) Can a single policy trained in simulation learn to solve complex motion planning tasks in the real world? 2) How does Neural MP compare to SOTA neural planning, sampling-based and trajectory optimization planning approaches? 3) How well does Neural MP extend to motion planning tasks with objects in-hand? 4) Can Neural MP perform dynamic obstacle avoidance? 5) What are the impacts key ingredients of Neural MP have on its performance?\nFree Hand Motion Planning In this set of experiments, we evaluate motion planning the robot's hand is empty (Table I)."}, {"title": "VI. DISCUSSION AND LIMITATIONS", "content": "In this work, we present Neural MP, a method that builds a data-driven policy for motion planning by scaling procedural scene generation, distilling sampling-based motion planning and improving at test-time via refinement. Our model demonstrably improves over the sampling-based planning in the real world, operating 2.5x-20x faster than AIT* while improving by over 20% in terms of motion planning success rate. Notably, our model generalizes to a wide distribution of task instances and demonstrates favorable scaling properties. At the same time, there is significant room for future work to improve upon, our model 1) is susceptible to point-cloud quality, which may require improving 3D representations via implicit models such as NeRFs [55], 2) does not still handle tight spaces well, a capability which could be potentially acquired via RL fine-tuning of the base policy and 3) is slower than simply running the policy directly due to test-time optimization, which can be addressed by leveraging learned collision checking [56], [57]."}, {"title": "VIII. ADDITIONAL REAL WORLD RESULTS AND ANALYSIS", "content": "A. Detailed Free Hand Motion Planning Results\nIn this section we perform additional analysis of the free hand motion planning results from the main paper. We include a more detailed version of the main result table (Tab. III). In this table, we additionally include the average (open loop) planning time per method and the average rate of safety violations. Safety violations are defined to occur where there are collisions, the robot hits its joint limits or there are torque limit errors. The open loop planning time for neural methods such as ours or \u039c\u03c0Nets involves simply measuring the total time taken for rolling out the policy and test time optimization (TTO). We find that sampling-based planners in general never collide when executed. If they produce a safety violation, it is only because they find a trajectory that is infeasible for the robot to execute on the hardware, due to joint or torque limit errors. Neural motion planning methods have much higher collision rates, though Neural MP has a significantly lower collision rate than \u039c\u03c0Nets, which we attribute to test-time optimization pruning out bad trajectories. We also note that not all collisions are created equal: some are slight, lightly grazing the environment objects while still achieving the goal, while others can be catastrophic, colliding heavily into the environment. In general, we found that our method tends to produce trajectories that may have slight collisions, though most of these are pruned out by TTO. With regards to planning time, \u039c\u03c0Nets is the fastest method, as our method expends additional compute rolling out 100x more trajectories and then selecting the best one using SDF-based collision checking.\nB. Detailed In-hand Motion Planning Results\nIn this section, we extend the in-hand results shown in the main paper with additional baselines (AIT*-80s, AIT*-10s and \u039c\u03c0Nets). For this evaluation (see Tab. IV, we consider two of the four in-hand motion planning objects, namely joystick and book. We find sampling-based methods are able to perform in-hand motion planning quite well, matching the performance of our base policy as well as our method without Objaverse data. We also see that \u039c\u03c0Nets is unable to perform in-hand motion planning on any of the evaluated tasks. This is likely because that network was not trained on data with objects in-hand, demonstrating the importance of including in-hand data when training neural motion planners. Finally, there is a significant gap in performance between our method with and without test-time optimization; pruning out\ncolliding trajectories at test time is crucial for achieving high success rates on motion planning tasks.\nC. Test-time Optimization Analysis\nTo analyze what the test-time optimization procedure is doing, we first note that the base policy can sometimes produce slight collisions with the environment due to the imprecision of regression. As a result, when sampling from the policy, it is often likely that the policy will lightly graze objects which will count as failures when motion planning. We visualize a set of trajectories sampled from the policy here on our website for the real-world bins task. Observe that for some of the trajectories, the policy slightly intersects with the bin which would cause it to fail when executing in the real world, while for others it simply passes over the bin completely without colliding. We estimate the robot-scene intersection of all of these trajectories by comparing the robot SDF to the scene point-cloud and plot the range of values in Fig. 5. We observe that 25% of trajectories do not collide with the environment, and we select for those. In principle, one could further optimize by selecting the trajectory that is furthest from the scene (using the SDF). In practice, we did not find this necessary and that selecting the first trajectory among those with the fewest expected collisions performed quite well in our experiments."}, {"title": "IX. ABLATIONS", "content": "We run additional ablations analyzing components of our method in simulation using a subset of our dataset (100K trajectories) and include additional details for experiments discussed in the main paper.\nLoss Types For training objective, we evaluate 4 different options: GMM log likelihood (ours), MSE loss, L1 loss, and PointMatch loss (\u039c\u03c0Nets). PointMatch loss involves computing the 12 distance between the goal and the pre-dicted end-effector pose using 1024 key-points. We plot the results on held out scenes in Fig. 6. We find that GMM (ours) outperforms L2 loss, L1 loss, and PointMatch Loss (\u039c\u03c0Nets) by (7%, 12%, and 24%) respectively. One reason this may be the case is that sampling-based motion planners produce highly multi-modal trajectories: they can output entirely different trajectories for the same start and goal pair when sampled multiple times. Since Gaussian Mixture Models are generally more capable of capturing multi-modal distributions, they can hence fit our dataset well. At the same time, the PointMatch [42] loss struggles significantly on our data: it cannot distinguish between 0 and 180 degree flipped end-effector orientations, resulting in many failures due to incorrect end-effector orientations.\nObservation Components We evaluate whether our choice of observation components impacts the Neural MP's performance. In theory, the network should be able to learn as well from the point-cloud alone as when the proprioception is included, as the point-cloud contains a densely sampled point-cloud of the current and goal robot configurations. However, in practice, we find that this is not the case. Instead, removing either q or g or both severely harms performance as seen in Fig. 6. We hypothesize that including the proprioception provides a richer signal for the correct delta action to take.\nRNN History Length In our experiments, we chose a history length of 2 for the RNN, after sweeping over values of 2, 4, 8, 16 based on performance. From Fig. 6 we see history length 2 achieves the best performance at 94%, while using lengths 4, 8 and 16 achieve progressively decreasing success rates (92.67, 68, 14.67). One possible reason for this is that since point-clouds are already very dense representations that cover the scene quite well, the partial observability during training time is fairly low. A shorter history length also leads to faster training, due to smaller batches and fewer RNN unrolling steps.\nEncoder Size Finally, we briefly evaluate whether en-coder size is important when training large-scale neural motion planners. We train 3 different size models: small (4M params), medium (8M params) and large (16M params). From the results in Fig. 6, we find that the encoder size does not affect performance by a significant margin (94%, 93%, 92%) respectively and that the smallest model in fact performs best. Based on these results, we opt to use the small, 4M param model in our experiments.\nArchitecture Ablation In this experiment, we evaluate how different sequence modelling methods (Transformers and ACT [54]) and simpler action decoders such as MLPS compare against our design choice of using an LSTM. All methods are trained with the same dataset (of 1M trajecto-ries), with the same encoder and GMM output distribution (with the exception of ACT which uses an L1 loss as per the ACT paper). We then evaluate them on held out motion planning tasks (Fig. V which are replicas of our real-world tasks (Bins and Shelf). We note several findings: 1) ACT performs poorly, largely due to its design choice of using an L1 loss which prevents it from handling planner multi-modality effectively, 2) Neural MP with an MLP action decoder also performs significantly worse than LSTMs and Transformers, as it is unable to use history information effectively to reason about the next action 3) Transformers and LSTMs perform similarly, with the Transformer variant performing marginally better, but with significantly slower inference time (2x). Hence we opt to use LSTM policies for our experimental evaluation, but certainly our method is amenable to any choice of sequence modeling architecture that performs well and has fast inference.\nDataset Ablation Finally, we evaluate the quality of different dataset generation approaches for producing gen-eralist neural motion planners. We do so by training policies on three different datasets (Neural MP, \u039c\u03c0Nets [42], and MotionBenchMaker [40]) and evaluated on held out motion planning tasks in simulation. We train each model to conver-gence for 10K epochs and then execute trajectories on two held out tasks that mirror our real world tasks: RealBins and RealShelf. For fairness, we do not include any Objaverse meshes in these tasks, since MPiNets and MotionBench-Maker only have primitive objects. Still, we find that our dataset performs best by a wide margin (Tab. VI). In general,"}, {"title": "X. PROCEDURAL SCENE GENERATION DETAILS", "content": "In this section we provide additional details regarding the data generation methods we develop for training large scale neural motion planners.\nA. Procedural Scene Generation\nWe formalize our procedural scene generation as a compo-sition of randomly generated parameteric assets and sampled Objaverse meshes in Alg. 1\nObjaverse sampling details The Objaverse are sampled in the task-relevant sampling location of the programmatic asset(s) in the scene, such as between shelf rungs, inside cubbies or within cabinets. Similar to the programmatic assets, these Objaverse assets are also sampled from a category generator Xobj(p). Here the parameter p specifies the size, position, orientation of the object as well as task-relevant sampling location of the object in the scene, such as between shelf rungs, inside cubbies or within cabinets. As discussed in the main paper, we propose an approach that iteratively adds assets to a scene by adjusting their position using the effective collision normal vector, computed from the existing assets in the scene. We detail the steps for doing this in Alg. 1.\nB. Motion Planner Experts\nWe use three techniques to improve the data generation throughput when imitating motion planners at scale.\nHindsight Relabeling Tight-space to tight-space problems are the most challenging, particularly for sampling-based planners, often requiring significant planning time (up to 120 seconds) for the planner to find a solution. For some problems, the expert planner is unable to find an exact solution and instead produces approximate solutions. Instead of discarding these, note that we use a goal-conditioned imitation learning framework, where we can simply execute the trajectories in simulation and relabel the observed final state as the new goal.\nReversibility We further improve our data generation throughput by observing that since motion planners inher-ently produce collision-free paths, the process is reversible, at least in simulation. This allows us to double our data throughput by reversing expert trajectories and re-calculating delta actions accordingly. Additionally, for a neural motion planner to be useful for practical manipulation tasks, it must be able to generate collision free plans for the robot even when it is holding objects. To enable such functionality, we augment our data generation process with trajectories where objects are spawned between the grippers of the robot end effector. There are transformed along with the end-effector during planning in simulation. We consider the object as part of the robot for collision checking and for the sake of our visual observations. In order to handle diverse objects that the robot might have to move with at inference time, we perform significant randomization of the in-hand object that we spawn in simulation. Specifically, we sample this object from the primitive categories of boxes, cylinders or spheres, or even from Objaverse meshes of everyday articles. We randomize the scale of the object between 3 and 30 cm along the longest dimension, and sample random starting locations within a 5cm cube around the end-effector mid-point between grippers.\nSmoothing Importantly, we found that naively imitating the output of the planner performs poorly in practice as the planner output is not well suited for learning. Specifically, plans produced by AIT* often result in way-points that are far apart, creating large action jumps and sparse data coverage, making it difficult to for networks to fit the data. To address this issue, we perform smoothing using cubic spline interpolation while enforcing velocity and acceleration limits. The implementation from \u039c\u03c0Nets performs well in practice, smoothing to a fixed 50 timesteps with a max spacing of 0.1 radians. In general, we found that smoothing is crucial for learning performance as it ensures the maximum action size is small and thus easier for the network to fit to.\nC. Data Pipeline Parameters and Compute\nIn Table VII, we provide a detailed list of all the param-eters used in generating the data to train our model.\nCompute In order to collect a vast data of motion planning trajectories, we parallelize data collection across a cluster of 2K CPUs. It takes approximately 3.5 days to collect 1M trajectories."}, {"title": "XI. NETWORK TRAINING DETAILS", "content": "We first describe additional details regarding our neural policy, and then discuss how it is trained. Following the design decisions of MnNets [42], we construct a segmented point-cloud for the robot, consisting of the robot point-cloud, the target goal robot point-cloud and the obstacle point-cloud. Here we note two key differences from \u039c\u03c0Nets: 1) our network conditioned on the target joint angles, while \u039c\u03c0Nets only does so through the segmented point-cloud, 2) we condition on the target joint angles, not end-effector pose, decisions that we found improved adherence to the overall target configuration. For in-hand motion planning, we extend this representation by considering the object in-hand as part of the robot for the purpose of segmentation.\nWe include a hyper-parameter list for our neural motion planner in Table VIII. We train a 20M parameter neural network across our dataset of 1M trajectories. The Point-Net++ encoder is 4M parameters and outputs an embedding of dimension 1024. We concatenate this embedding with the encoded qt and g vectors and pass this into the 16M parameter LSTM decoder. The decoder outputs weights, means, and standard deviations of the 5 GMM modes. We then train the model with negative log likelihood loss for 4.5M gradient steps, which takes 2 days on a 4090 GPU with batch size of 16."}, {"title": "XII. REAL WORLD SETUP DETAILS", "content": "In this section, we describe our real world robot setup and tasks in detail and perform analysis on the perception used for operating our policies.\nA. Real Robot Setup\nHardware For all of our experiments, we use a Franka Emika Panda Robot, which is a 7 degree of freedom manip-ulator arm. We control the robot using the manimo library (https://github.com/AGI-Labs/manimo) and perform all of experiments using their joint position controller with the default PD gains. The robot is mounted to a fixed base pedestal behind a desk of size .762m by 1.22m with variable height. For sensing, we use four extrinsically calibrated depth cameras, Intel Realsense 435 / 435i, placed around the scene in order to accurately capture the environment. We project the depth maps from each camera into 3D and combine the individual point-clouds into a single scene representation. We then post-process the point-cloud by cropping it to the workspace, filtering outliers and denoising, and sub-sampling a set of 4096 points. This processed point-cloud is then used as input to the policy.\nRepresentation Collision Checking and Segmentation\nIn order to perform real world collision checking and robot point-cloud segmentation, we require a representation of the robot to check intersections with the scene (collision checking) and to filter out robot points from the scene point-cloud (segmentation). While the robot mesh is the ideal candidate for these operations, it is far too slow to run in real time. Instead, we approximate the robot mesh as spheres (visualized in Fig. 7) as we found this performs well in practice while operating an order of magnitude faster. We use 56 spheres in total to approximate the links of the robot as well as the end-effector and gripper. These have radii ranging from 2cm to 10cm and are defined relative to the center of mass of the link. This representation is a conservative one: it encapsulates the robot mesh, which is desirable for segmentation as this helps account for sensing errors which would place robot points outside of the robot mesh.\nRobot Segmentation In order to perform robot segmen-tation in the real world, we use the spherical representation to filter out robot points in the scene, so only the obstacle point-cloud remains. Doing so requires computing the Signed Distance Function (SDF) of the robot representation and then checking the scene point-cloud against it, removing points from the point-cloud in which SDF value is less than threshold 8. For the spherical representation, the SDF computation is efficient: for a sphere with center C and radius r, the SDF of point x is simply \\( ||x - C||^2 - r \\). In our experiments, we use a threshold & of 1cm. We then replace the removed points with points sampled from the robot mesh of the robot. This is done by pre-sampling a robot point-cloud from the robot mesh at the default configuration, then performing forward kinematics using the current joint angles qt and transforming the robot point-cloud accordingly. Replacing the real robot point-cloud with this sampled point-cloud ensures that the only difference between sim and real is the obstacle point-cloud.\nReal-world Collision Checking Given the SDF, collision checking is also straightforward, we denote the robot in collision if any point in the scene point-cloud (this is after robot segmentation) has SDF value less than 1cm. Note this means that first state is by definition collision free. Also, this technique will not hold if performing closed loop planning, in that case this method would always denote the state as collision free as the points with SDF value less than 1cm would be segmented out for each intermediate point-cloud.\nOpen Loop Deployment For open-loop execution of neural motion planners, we execute the following steps: 1) generate the segmented point-cloud at the first frame, 2) predict the next trajectory way-point by computing a forward pass through the network and sampling an action, 3) update the current robot point-cloud with mesh-sampled point-cloud at the predicted way-point, and 4) repeat until goal reaching success or maximum rollout length is reached. The entire trajectory is then executed on the robot after the rollout. Please see Alg. 2 for a more detailed description of our open-loop deployment method.\nB. Tasks\nBins This task requires the neural planner to perform collision avoidance when moving in-between, around and inside two different industrial bins pictured in the first row of"}]}