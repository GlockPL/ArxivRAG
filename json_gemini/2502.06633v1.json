{"title": "Combining Large Language Models with Static Analyzers for Code Review Generation", "authors": ["Imen Jaoua", "Oussama Ben Sghaier", "Houari Sahraoui"], "abstract": "Code review is a crucial but often complex, subjective, and time-consuming activity in software development. Over the past decades, significant efforts have been made to automate this process. Early approaches focused on knowledge-based systems (KBS) that apply rule-based mechanisms to detect code issues, providing precise feedback but struggling with complex, context-dependent cases. More recent work has shifted toward fine-tuning pre-trained language models for code review, enabling broader issue coverage but often at the expense of precision. In this paper, we propose a hybrid approach that combines the strengths of KBS and learning-based systems (LBS) to generate high-quality, comprehensive code reviews. Our method integrates knowledge at three distinct stages of the language model pipeline: during data preparation (Data-Augmented Training, DAT), at inference (Retrieval-Augmented Generation, RAG), and after inference (Naive Concatenation of Outputs, NCO). We empirically evaluate our combination strategies against standalone KBS and LBS fine-tuned on a real-world dataset. Our results show that these hybrid strategies enhance the relevance, completeness, and overall quality of review comments, effectively bridging the gap between rule-based tools and deep learning models.", "sections": [{"title": "I. INTRODUCTION", "content": "Code review is a common software engineering practice that plays a crucial role in maintaining code quality, identifying bugs, and fostering a culture of continuous improvement within development teams [1], [2]. By rigorously inspecting code changes, reviewers provide valuable feedback that can improve software both before and after it is integrated into the system. Modern code review goes beyond defect detection; it aims to enhance the overall quality of software changes by ensuring maintainability, reliability, and adherence to best practices [3], [4].\nHowever, the code review process is often perceived as complex, time-consuming, and subject to various biases, especially in large-scale projects. Factors such as varying levels of developer expertise, interpersonal dynamics, and the lack of standardized guidelines can introduce inconsistencies, ultimately impacting the efficiency and robustness of the codebase [5]. Furthermore, the evolving nature of coding standards and best practices requires constant adaptation, adding further complexity to the process.\nTo address these challenges, there has been growing interest in automating the code review process [6]. Initial efforts primarily involved the deployment of knowledge-based systems (KBS), particularly static analysis tools like PMD, FindBugs, and SonarQube. These tools use predefined rule sets to identify common coding issues, systematically scanning the codebase for violations and improving the early stages of development by flagging rule violations in the code. Although effective at detecting surface-level issues, static analyzers are limited by their dependence on manually defined rules, which require frequent updates to remain relevant. Their rigidity makes it difficult to handle complex, context-dependent issues that require a deeper understanding of code intent [7]. Consequently, these tools often fall short in adapting to the dynamic and context-sensitive nature of software projects, where factors like architecture, team culture, and specific project requirements demand more flexible and intelligent solutions.\nRecent advances in large language models and natural language processing have sparked significant interest in using pre-trained language models to automate code review workflows. These learning-based systems (LBS) aim to overcome the limitations of knowledge-based detection by providing a more nuanced understanding of code, enabling them to identify deeper faults, recommend improvements, and even anticipate the potential impact of code changes on the overall system [8]. Although these models capture a broader range of issue patterns than static analysis alone, their precision still remains below acceptable levels [9], [10].\nAs is common in the automation of software development tasks, some solutions achieve high precision but with limited coverage, while others offer broader coverage at the expense of precision. The most promising approaches, therefore, are those that balance these trade-offs by combining solutions to optimize both precision and coverage. We hypothesize that integrating static analysis with learning-based systems can enhance the effectiveness of automated code review generation tools. In this paper, we explore three combination strategies aimed at integrating the structured knowledge from static analysis into the pipeline for building and operating a large language model (LLM) for code review generation. Specifically, our approach integrates knowledge at three distinct"}, {"title": "II. BACKGROUND", "content": "A. Code Review Automation\nCode review is a widely adopted practice among software developers where a reviewer examines changes submitted in a pull request [3], [5], [11]. If the pull request is not approved, the reviewer must describe the issues or improvements required, providing constructive feedback and identifying potential issues. This step involves review commment generation, which play a key role in the review process by generating review comments for a given code difference. These comments can be descriptive, offering detailed explanations of the issues, or actionable, suggesting specific solutions to address the problems identified [5].\nVarious approaches have been explored to automate the code review comments process [9], [12], [13]. Early efforts centered on knowledge-based systems, which are designed to detect common issues in code. Although these traditional tools provide some support to programmers, they often fall short in addressing complex scenarios encountered during code reviews [14]. More recently, with advancements in deep learning, researchers have shifted their focus toward using large-language models to enhance the effectiveness of code issue detection and code review comment generation.\nB. Knowledge-based Code Review Comments Automation\nKnowledge-based systems (KBS) are software applications designed to emulate human expertise in specific domains by using a collection of rules, logic, and expert knowledge. KBS often consist of facts, rules, an explanation facility, and knowledge acquisition. In the context of software development, these systems are used to analyze the source code, identifying issues such as coding standard violations, bugs, and inefficiencies [15]\u2013[18]. By applying a vast set of predefined rules and best practices, they provide automated feedback and recommendations to developers. Tools such as FindBugs [19], PMD [20], Checkstyle [21], and SonarQube [22] are prominent examples of knowledge-based systems in code analysis,\noften referred to as static analyzers. These tools have been utilized since the early 1960s, initially to optimize compiler operations, and have since expanded to include debugging tools and software development frameworks [23], [24].\nC. LLMs-based Code Review Comments Automation\nAs the field of machine learning in software engineering evolves, researchers are increasingly leveraging machine learning (ML) and deep learning (DL) techniques to automate the generation of review comments [3], [11], [25]\u2013[28]. Large language models (LLMs) are large-scale Transformer models, which are distinguished by their large number of parameters and extensive pre-training on diverse datasets. Recently, LLMs have made substantial progress and have been applied across a broad spectrum of domains. Within the software engineering field, LLMs can be categorized into two main types: unified language models and code-specific models, each serving distinct purposes [29].\nCode-specific LLMs, such as CodeGen [30], StarCoder [31] and CodeLlama [32] are optimized to excel in code comprehension, code generation, and other programming-related tasks. These specialized models are increasingly utilized in code review activities to detect potential issues, suggest improvements, and automate review comments [13], [29].\nD. Retrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) is a general paradigm that enhances LLMs outputs by including relevant information retrieved from external databases into the input prompt [33]. Traditional LLMs generate responses based solely on the extensive data used in pre-training, which can result in limitations, especially when it comes to domain-specific, time-sensitive, or highly specialized information. RAG addresses these limitations by dynamically retrieving pertinent external knowledge, expanding the model's informational scope and allowing it to generate responses that are more accurate, up-to-date, and contextually relevant [34].\nTo build an effective end-to-end RAG pipeline, the system must first establish a comprehensive knowledge base. It requires a retrieval model that captures the semantic meaning of presented data, ensuring relevant information is retrieved. Finally, a capable LLM integrates this retrieved knowledge to generate accurate and coherent results [10].\nE. LLM as a Judge Mechanism\nLLM evaluators, often referred to as LLM-as-a-Judge, have gained significant attention due to their ability to align closely with human evaluators' judgments [35], [36]. Their adaptability and scalability make them highly suitable for handling an increasing volume of evaluative tasks.\nRecent studies have shown that certain LLMs, such as Llama-3 70B and GPT-4 Turbo, exhibit strong alignment with human evaluators, making them promising candidates for automated judgment tasks [37]\nTo enable such evaluations, a proper benchmarking system should be set up with specific components: prompt design,"}, {"title": "III. PROPOSED APPROACH", "content": "For the task of review comment generation, Knowledge-Based Systems (KBS) draw on codified rules and expert knowledge to deliver feedback that is consistent with established coding standards and best practices. Static analyzers, a prominent example of KBS, systematically follow predefined guidelines to detect code issues, offering reliable and structured feedback. While KBS achieve high precision, they are limited in scope, covering only a subset of possible issues encountered during code changes. In contrast, Learning-Based Systems (LBS) harness the adaptive potential of language models, which, by training on historical data, can recognize intricate patterns and generate contextually relevant review comments. This adaptability allows LBS to cover a broader range of issues present in the dataset, though often at the expense of precision. In this work, we conjecture that by combining these two strategies, it is possible to achieve the best of both approaches, namely, broader issue coverage coupled with improved precision.\nA. Overview\nFigure 1 illustrates our approach, outlining three strategies to combine Knowledge-Based Systems (KBS) and Learning-Based Systems (LBS) to enhance code review automation.\nThese strategies leverage KBS insights at different stages of the LBS pipeline, specifically during data preparation, inference, and final output. The three strategies are as follows:\n\u2022 Data-Augmented Training (DAT): In this strategy, we enhance the training dataset by augmenting a real-world dataset with synthetic data generated from both KBS and LBS. This enriched dataset is then used to fine-tune a language model, enabling the LBS to incorporate both data-driven patterns and rule-based knowledge. This combination helps the model gain a more comprehensive understanding of code review patterns, improving robustness in varied review scenarios.\n\u2022 Retrieval-Augmented Generation (RAG): Here, KBS insights are integrated directly into the LBS inference process. Through RAG, relevant information is dynamically retrieved from KBS (i.e., static analysis results) and injected into the prompts during generation. By incorporating the results of the KBS into the instruction, the LBS aligns its responses with established coding standards and practices, providing feedback grounded in structured, rule-based knowledge.\n\u2022 Naive Concatenation of Outputs (NCO): This strategy merges the feedback generated by KBS and LBS after inference, combining their outputs to produce a unified code review. By consolidating KBS's rule-based precision with LBS's contextual depth, NCO offers a comprehensive review that covers a broader range of potential issues.\nThese strategies allow the LBS to benefit from the structured, rule-based insights of KBS, enhancing its ability to generate accurate, contextually appropriate, and standards-compliant code review comments.\nB. Baseline Model Preparation and Static Analyzers Selection\nWhile our approach is applicable to a wide range of LLMs and static analysis tools, we propose a specific configuration to illustrate the three strategies and establish the baselines for validating our conjecture. To set up the baseline systems, we first defined the LBS. We fine-tuned a large language model on an extensive code review dataset [25], referred to as DM, which pairs code changes with detailed reviews.\nThe selected model for fine-tuning is CodeLlama-7b, trained for comment generation (i.e., generating review comments from code changes) with the following hyperparameter settings. The training was conducted on four NVIDIA RTX 3090 GPUs, using a batch size of 4 per device. To boost efficiency, we applied gradient accumulation with a step size of 4, updating the optimizer only after multiple batches. We used 4-bit quantization to improve memory and computational efficiency. Additionally, we employed Quantized Low-Rank Adaptation (QLORA) [39], a Parameter-Efficient Fine-Tuning (PEFT) technique, with $r = 16$, $a = 32$, and $dropout = 0.05$. This method decomposes weight updates into low-rank matrices, reducing the parameters needed for fine-tuning and optimizing training efficiency [39].\nThis resulted in a model, denoted as $M_i$, capable of generating detailed human-like code reviews. Since $M_i$ represents the LBS component and was trained using the data from DM, we used the test set to generate reviews by both static analyzers and the fine-tuned model $M_i$.\nC. Data Augmented Training\nAs shown in Figure 2a, this strategy involves retraining the LBS using an augmented dataset $D_a$, which includes review comments generated by both, static analyzers and the fine-tuned model $M_i$. Through this retraining process, the LBS learns from both data sources, producing a more refined model referred to as $M_{FT}$.\nA simple approach to augmenting the dataset would have been to apply static analysis to the code in DM, and add or concatenate the generated comments with the existing ones. However, this method does not guarantee data quality within the augmented dataset and fails to account for the insights inferred by the LBS $M_i$. Therefore, we employ an ensemble learning approach where the two distinct sources\u2014the LBS and the KBS\u2014serve as experts to generate data for fine-tuning a model. The underlying rationale is that both KBS and LBS reviews are inherently synthetic. By combining their outputs, we achieve a more balanced and consistent fine-tuning process.\nTo produce the augmented dataset $D_a$, we designed a two-step process (i.e., data generation and data filtering), as depicted in Figure 3.\nIn the Data Generation phase, we used the original dataset $D_o$ as input. For each code change c, we employed our fine-tuned model $M_i$ to generate four context-aware, human-like reviews. Simultaneously, static analyzer rules were applied to each source code f to produce structured and precise feedback. Each static analyzer generated a report containing several reviews, including the start and end sections of code"}, {"title": "IV. EVALUATION", "content": "In this section, we first define the research questions that guide our study, followed by a detailed description of the experimental setup. We then present and interpret the results.\nThe goal of our evaluation is to determine whether integrating knowledge into the LBS enhances the accuracy of generated comments while maintaining adequate coverage compared to using only the LBS or KBS. To structure our evaluation, we define the following research questions:\nRQ1: Manual evaluation of accuracy: How accurate are the reviews generated by our hybrid approaches compared to those produced by the baseline?\nTo address this question, we perform a manual assessment involving two reviewers on a sample of 10% of the test set, comparing the accuracy of comments generated by our hybrid approaches against those produced by a fine-tuned language model and static analyzers.\nRQ2: Alignment of LLM judgments with human assessments (sanity check): To what extent do LLM judgments align with human assessments?\nGiven the size of the test set and the five different comment generation options to evaluate, human assessment is impractical for the entire dataset. As an alternative, we employ an LLM to perform assessments across the whole test set. To evaluate the LLM's ability to mimic human judgment, we replicate the manual assessment procedure used for RQ1, substituting the human reviewers with the LLM and measuring the level of agreement between LLM and human evaluations.\nRQ3: LLM evaluation of generated reviews: How does the accuracy of the reviews generated by our hybrid approaches compare to those produced by the baseline when evaluated by an LLM?\nWe use the LLM-as-a-judge approach to assess the accuracy of the generated reviews, comparing results from our hybrid approaches to those from the baseline, specifically the learning-based models and static analyzers.\nRQ4: Ranking of reviews based on coverage: How do our hybrid approaches compare to the baseline in terms of coverage?\nFor this question, we use an LLM-as-a-judge to conduct comparative evaluations, ranking reviews generated by our approaches and the baseline according to coverage criteria.\nB. Experimental Setup\nTo address our research questions, we conducted a series of experiments. To enable comparison across the different hybrid approaches, we used the test set from our augmented dataset, $D_a$, ensuring that all entries were unseen by the fine-tuned model $M_{FT}$. Since the retrieval-augmented generation and naive concatenation of outputs approaches are applicable only to code changes with both KBS and LBS reviews, we filtered the test set to include only samples containing reviews from both sources. As illustrated in Figure 7, we examined the code changes in the selected test set, creating a unified code difference for analysis whenever overlaps were found. For example, if the test set contains a code difference $diff_1$ with an LBS review comment $r_1$ and a code difference $diff_2$ having a KBS review comment $r_2$, we merged both code differences into a unified difference, $diff_u$, as shown in Figure 7, forming a new triplet $(diff_u, r_1, r_2)$ in our test set. Out of the 7,884 total samples, we identified 1,245 common code differences that included both KBS and LBS reviews.\nFor each of these selected code changes, we generated five types of reviews: the static analyzer review, the review generated by $M_i$, the review generated by $M_{FT}$, the retrieval-augmented generation review and the naive concatenated review.\nFor RQ1, we randomly selected 10% of the 1,245 test set examples for a manual evaluation focused on accuracy. All reviews were anonymized and presented to the evaluators in a randomized order. This approach minimizes confirmation bias, preventing evaluators from unconsciously favoring certain methods by being unaware of the source of each review."}, {"title": "V. RELATED WORK", "content": "A. Code review automation\nSeveral approaches were proposed to assist code review. Gupta et al. [51] introduced an LSTM-based model trained on positive and negative (code, review) pairs, selecting candidate reviews based on code similarity and predicting relevance scores. Siow et al. [11] enhanced this with multi-level embeddings, leveraging word-level and character-level representations to better capture the semantics of code and reviews.\nWith the advent of large language models, the focus has shifted toward generative models to fully automate code review tasks. Tufano et al. [52] developed a transformer-based model to suggest code edits before and after code review, later enhancing it by pre-training T5 on Java and technical English [26]. Li et al. [25] pre-trained CodeT5 on a multilingual dataset and fine-tuned it for code review tasks like quality estimation, review generation, and code refinement. Sghaier et al. [5] further advanced this area by applying cross-task knowledge distillation to address successive code review tasks jointly, enhancing performance and promoting tasks' interdependence.\nCurrent research efforts have significantly advanced the automation of code review, introducing different and innovative approaches that enhance code review tasks. However, the performance of these automated approaches remains limited in terms of correctness, as indicated by low BLEU scores, suggesting that further refinement is needed to achieve higher accuracy and reliability as expected in practical software development contexts.\nB. LLMs and static analysis combination\nRecent research has increasingly focused on enhancing LLM-based solutions for software engineering using several techniques. One is by integrating them with static analysis tools, addressing the challenge of reducing inaccurate or incomplete results.\nIn automated program repair, RepairAgent [53] employs static analysis to gather contextual data that guides LLM-driven code correction, while PyTy [54] relies on type-checking mechanisms to validate the accuracy of LLM-generated candidates in resolving static type inconsistencies.\nFor software testing, approaches like TECO [55] apply static analysis to derive semantic features for training transformers in test completion, while ChatTester [56] and TestPilot [57] utilize similar techniques to prepare contextual information that supports iterative LLM-based test code repair processes. For bug detection, LLMs were combined with static analysis to reduce false positives. SkipAnalyzer [58] and GPTScan [59] use static analysis to validate LLM predictions, while D2A [60] and ReposVul [61] refine bug labeling and re-rank predictions. For code completion, STALL+ [62] integrates static analyzers with LLMs through a multi-phase approach involving prompting, decoding, and post-processing.\nThese studies illustrate the effectiveness of combining LLMs with static analysis across tasks like program repair, bug detection, testing, and code completion. However, this integration has yet to be explored for code review."}, {"title": "VI. CONCLUSION", "content": "In this paper, we explored hybrid approaches that combine knowledge-based systems (KBS) with large language models (LLMs) to enhance the automation of code review. By integrating KBS-derived insights at three stages\u2014data preparation (Data-Augmented Training, DAT), inference (Retrieval-Augmented Generation, RAG), and post-inference (Naive Concatenation of Outputs, NCO)\u2014we aimed to leverage the strengths of both KBS and learning-based systems (LBS) to generate more accurate and comprehensive code review comments. Our empirical evaluation showed that combination approaches offer distinct advantages. RAG emerged as the most effective, improving both accuracy and coverage of review comments by enriching the generation process with structured, contextually relevant knowledge from static analysis tools. DAT achieved broad coverage by exposing the LLM to diverse issue types in training, sometimes at the expense of precision. NCO, while straightforward, achieved moderate improvements in coverage.\nThese findings underscore the potential of combining static analysis tools with LLMs to address the limitations of individual approaches in automated code review. Future work will involve exploring more sophisticated integration of knowledge into open-weight LLMs. We also plan to expand this methodology to additional programming languages and exploring bytecode-level analysis for greater depth. Furthermore, the integration of more advanced static analyzers and dynamic analysis tools could further enhance coverage and precision, ultimately contributing to a more robust and versatile code review automation pipeline."}]}