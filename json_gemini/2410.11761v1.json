{"title": "SLIDECHAT: A LARGE VISION-LANGUAGE ASSISTANT FOR WHOLE-SLIDE PATHOLOGY IMAGE UNDERSTANDING", "authors": ["Ying Chen", "Guoan Wang", "Yuanfeng Ji", "Yanjun Li", "Jin Ye", "Tianbin Li", "Bin Zhang", "Nana Pei", "Rongshan Yu", "Yu Qiao", "Junjun He"], "abstract": "Despite the progress made by multimodal large language models (MLLMs) in computational pathology, they remain limited by a predominant focus on patch-level analysis, missing essential contextual information at the whole-slide level. The lack of large-scale instruction datasets and the gigapixel scale of whole slide images (WSIs) pose significant developmental challenges. In this paper, we present SlideChat, the first vision-language assistant capable of understanding gigapixel whole-slide images, exhibiting excellent multimodal conversational capability and response complex instruction across diverse pathology scenarios. To support its development, we created SlideInstruction, the largest instruction-following dataset for WSIs consisting of 4.2K WSI captions and 176K VQA pairs with multiple categories. Furthermore, we propose SlideBench, a multimodal benchmark that incorporates captioning and VQA tasks to assess SlideChat's capabilities in varied clinical settings such as microscopy, diagnosis. Compared to both general and specialized MLLMs, SlideChat exhibits exceptional capabilities, achieving state-of-the-art performance on 18 of 22 tasks. For example, it achieved an overall accuracy of 81.17% on SlideBench-VQA (TCGA), and 54.15% on SlideBench-VQA (BCNB). We will fully release SlideChat, SlideInstruction and SlideBench as open-source resources to facilitate research and development in computational pathology.", "sections": [{"title": "1 INTRODUCTION", "content": "Computational pathology aims to improve the analysis of digitized tissue samples, such as whole slide images (WSIs), by applying artificial intelligence to aid in the diagnosis, identification, and understanding of disease (Song et al., 2023). Recently, the development of this field has gained rapid momentum, mainly driven by breakthroughs in the visual foundation model (Chen et al., 2024b; Xu et al., 2024a; Vorontsov et al., 2024). These models learn generalized representations by pre-training on large-scale data and perform well in various downstream tasks, including rare cancer detection and biomarker prediction. Building on this base, integration with the powerful Large Language Models (LLMs) further advances the development of the Multimodal Large Language Model (MLLMs) (Lu et al., 2024b), which has made great strides in responding to more complex, open-ended visual queries, enabling it to serve as a versatile assistant at various stages of medical care, including clinical decision-making, education, and research (see Figure 1).\nNevertheless, there are three major challenges that hinder the development and use of pathology MLLMs for real-world clinical applications. First, it is challenging to develop a MLLMs architecture that can effectively capable of gigapixel whole slides (e.g., 100,000 \u00d7 100,000 pixels). Existing models (Lu et al., 2024b; Sun et al., 2024; Seyfioglu et al., 2024) often process whole slides by"}, {"title": "2 RELATED WORKS", "content": "Whole Slide Image Analysis Whole slide images are pivotal in modern pathology, enabling comprehensive analysis of tissue samples for tasks such as predicting patient prognosis, classifying cancer subtypes, and identifying biomarkers (Song et al., 2023; Shao et al., 2023; Li et al., 2024b; Spronck et al., 2023). Recent studies have leveraged pathology foundational models (Wang et al., 2024; Ahmed et al., 2024; Xu et al., 2024b) to enhance WSIs analysis, either through fine-tuning for specific downstream tasks or by employing zero-shot prediction approaches in CLIP (Radford et al., 2021) style. Although these models are effective in task-specific applications, their reliance on fine-tuning or limited zero-shot capabilities restricts their generalizability across diverse and complex user instructions.\nMLLMs in Computational Pathology The paradigm of MLLMs enables to effectively respond to more complex, open-ended visual queries while processing pathology image, thus providing significant value across various medical stages. PathChat (Lu et al., 2024b) is a vision-language assistant designed for pathology, developed with 450K private instruction pairs to handle both visual and natural language queries. QuiltInstruct (Seyfioglu et al., 2024) is a large-scale dataset comprising 107K question-answer pairs. Building on QuiltInstruct, Quilt-LLAVA (Seyfioglu et al., 2024) is a model designed for diagnostic reasoning across multiple image patches, leveraging its extensive question-answer pairs to accurately interpret complex H&E data. PathAsst (Sun et al., 2024) combines a pathology-specific CLIP model with Vicuna-13b (Chiang et al., 2023) to create a multimodal generative foundational model tailored for pathology. However, current MLLMs primarily focus on patch or region-of-interest (ROI) data, limiting their utility for slide-level clinical applications where broader contextual understanding is crucial."}, {"title": "3 SLIDECHAT", "content": "To achieve the goal of analyzing gigapixel whole-slide images in a multimodal setting, as shown in Figure 2, SlideChat consists of four key designs: the patch-level encoder, the slide-level encoder, the multimodal projector module, and the large language model. Our method starts by partitioning the WSI into smaller 224 \u00d7 224 pixel patches, making it computationally feasible to process such large images. These patches are then passed through a well-trained, frozen patch-level encoder (Lu et al., 2024a), which extracts localized features from each individual patch, capturing fine-grained details such as cellular structures. Building on this, we employ LongNet (Ding et al., 2023; Xu et al., 2024a) as slide-level encoder to enhance the patch-level embeddings and capture global patterns across the entire slide. This encoder uses sparse attention mechanisms to aggregate both local and global contextual information, enabling the model to perceive intricate local features while capturing the broader context, which is critical for comprehensive pathological assessments. Following the slide-level encoding, SlideChat incorporates a multimodal projection layer that maps these aggregated visual features into a unified space aligned with the LLM. This ensures that the visual features extracted from the WSIs are effectively transformed into representations compatible with the language model, facilitating seamless integration and interaction between visual and textual data. Concurrently, the model accepts natural language instructions from users, such as \"What is the type of tumor in the image?\". These textual queries are processed by the LLM, which comprehends the textual input and integrates it with the visual features extracted from the WSIs, enabling accurate and contextually relevant diagnostic responses. This multimodal reasoning capability allows SlideChat to provide accurate and contextually relevant answers to complex pathology-related questions, thereby supporting clinical decision-making, education, and research across various medical stages."}, {"title": "3.1 ARCHITECTURE", "content": "To achieve the goal of analyzing gigapixel whole-slide images in a multimodal setting, as shown in Figure 2, SlideChat consists of four key designs: the patch-level encoder, the slide-level encoder, the multimodal projector module, and the large language model. Our method starts by partitioning the WSI into smaller 224 \u00d7 224 pixel patches, making it computationally feasible to process such large images. These patches are then passed through a well-trained, frozen patch-level encoder (Lu et al., 2024a), which extracts localized features from each individual patch, capturing fine-grained details such as cellular structures. Building on this, we employ LongNet (Ding et al., 2023; Xu et al., 2024a) as slide-level encoder to enhance the patch-level embeddings and capture global patterns across the entire slide. This encoder uses sparse attention mechanisms to aggregate both local and global contextual information, enabling the model to perceive intricate local features while capturing the broader context, which is critical for comprehensive pathological assessments. Following the slide-level encoding, SlideChat incorporates a multimodal projection layer that maps these aggregated visual features into a unified space aligned with the LLM. This ensures that the visual features extracted from the WSIs are effectively transformed into representations compatible with the language model, facilitating seamless integration and interaction between visual and textual data. Concurrently, the model accepts natural language instructions from users, such as \"What is the type of tumor in the image?\". These textual queries are processed by the LLM, which comprehends the textual input and integrates it with the visual features extracted from the WSIs, enabling accurate and contextually relevant diagnostic responses. This multimodal reasoning capability allows SlideChat to provide accurate and contextually relevant answers to complex pathology-related questions, thereby supporting clinical decision-making, education, and research across various medical stages."}, {"title": "3.2 DATA", "content": "SlideInstruction There is a notable lack of large-scale multimodal pathology datasets supporting the training of vision-language assistants for whole-slide image understanding. To support the training of SlideChat, we develop SlideInstruction, a comprehensive instruction dataset, sourced from the TCGA database, comprising 4,915 whole slide image (WSI)-report pairs from 4,028 patients. Figure 3 illustrates our entire data curation pipeine. We initially prompt GPT-4 to refine the pathology reports, clean up the noise in the report including unrelated symbols, technical details of pathology department procedures, specimen handling and processing information, redundant administrative or legal statements, and some repeated information. For the refined pathology reports, we further employ GPT-4 to generate high-quality multimodal data, comprising two main components: (1) WSI-Caption Data: We craft concise, clinically relevant summaries for each whole slide image by prompting the language model to extract key pathological findings. These summaries were structured into coherent paragraphs that highlighted crucial clinical details such as diagnostic results, tumor characteristics, margin status, and lymph node involvement, ensuring the caption dataset is both focused and informative. (2) WSI Instruction-Following Data: To enhance the model's ability to follow instructions and improve its comprehension of pathology images, we leveraged GPT-4 to generate tailored question-and-answer pairs for each WSI report. Drawing inspiration by PathChat (Lu et al., 2024b), we structure these questions into three \"broad\u201d categories-microscopy, diagnosis, and clinical considerations\u2014which represent key stages in the pathology workflow, and thirteen \"narrow\" categories focusing on specific aspects within each stage (Figure 1 B). Our carefully crafted prompts are detailed in Appendix A.2.2. To create a comprehensive instructional dataset, we generated two open-ended and two closed-ended QA pairs within each narrow category for every WSI report. Regarding the train/test split, it is worth noting that the WSI-report datasets from TCGA includes two types: (a) one report linked to multiple WSIs, and (b) one report linked to a single WSI. For type (a), where specific diagnostic details may not align perfectly with each WSI, we include all WSIs in the training set to introduce some \"noisy data\", which can enhance model robustness. For type (b), 80% of WSIs are allocated to the training set and 20% to the test set. Finally, there are 4,181 WSIs for training and 734 WSIs for testing. Consequently, we construct a large-scale training set named SlideInstruction, comprising 4,181 WSI captions and 175,753 instruction-following VQA pairs across various broad and narrow categories.\nSlideBench To systematically evaluate the performance of SlideChat, We incorporate the remaining 734 WSI captions along with a substantial number of closed-set VQA pairs to establish evaluation benchmark. First, we construct a test set named SlideBench-Caption based on the WSI-Caption data to evaluate the model's ability to generate accurate and coherent descriptions of whole slide images. Secondly, we construct SlideBench-VQA (TCGA) based on closed-set visual question-answering (VQA) pairs along with test WSIs, aiming to evaluate various aspects of model performance. As shown in Figure 3 (B), to improve the quality of the testing benchmarks, we employ four advanced large language models, including GPT-4 (Achiam et al., 2023), InternLM2-Chat-7B (Cai et al., 2024), Qwen-7B-Chat (Bai et al., 2023), and DeepSeek-7B-Chat, to filter closed-set VQAs by predicting answers based solely on the question text. Any questions for which at least three of these models provided correct answers are subsequently excluded. Following this automated filtering, five expert pathologists are invited to review and amend the remaining questions. The review process are guided by the following criteria: (1) Whether the correct answer necessitates image interpretation; (2) Whether the question and its corresponding answer are logically and coherently structured; and (3) Whether the question aligns appropriately with the designated broad and narrow categories. QA pairs failing to meet these criteria are excluded by the pathologists. Consequently, the SlideBench-VQA (TCGA) comprises 7,827 VQAs across 13 categories, with some examples illustrated in Figure 3 C. Additionally, we incorporate the in-the-wild Early Breast Cancer Core-Needle Biopsy (BCNB) WSI dataset (Xu et al., 2021), which encompasses a diverse patient"}, {"title": "3.3 Two-STAGE TRAINING", "content": "Stage 1: Cross-Domain Alignment. SlideChat adopts a two-stage training approach (see Figure 2 B). In the first stage, the primary objective is to align the large language model's (LLM) word embeddings with the visual features extracted from whole slide images. This alignment enables the LLM to interpret visual representations from the slide-level encoder, facilitating the effective utilization of the intricate features within the slides. During this stage, SlideChat is trained to generate descriptive captions using 4.2K WSI-caption pairs from SlideInstruction. Specifically, only the slide-level encoder and projection matrix are updated, while the patch-level encoder and LLM weights remain fixed.\nStage 2: Visual Instruction Learning. In the second stage, we focus on visual question-answering tasks to train the model to accurately respond to domain-specific questions concerning whole slide images. During this phase, the model develops the ability to handle a broad range of multimodal instructions, enabling it to generate answers by effectively integrating both visual and textual information. For example, the model must perform various pathology tasks, such as describing the extent of tumor invasion or assessing the degree of cellular differentiation. To accomplish this, we utilize 176K WSI VQAs from SlideInstruction in the second training stage, allowing the slide encoder, projection layer, and large language model components to be fully trainable to ensure comprehensive adaptability. This training approach significantly enhances the model's capability to handle diverse pathology-related tasks, thereby increasing its effectiveness in real-world clinical and research settings."}, {"title": "4 EXPERIMENT", "content": "We conducted following experiments to evaluate three key aspects of SlideChat: (1) its whole slide image captioning capability, which assesses proficiency in generating descriptive captions that accurately summarize the critical pathological features of a WSI; (2) its visual question-answering (VQA) ability across various complex pathological scenarios and its generalizability in zero-shot settings; and (3) SlideChat's ability to process gigapixel WSIs, capturing both essential global context and intricate details, thereby enhancing its performance compared to patch-level multimodal large language models. For WSI captioning baselines, we benchmark against MI-Gen (Chen et al., 2023), a state-of-the-art method specifically designed for this task. Given that existing MLLMs cannot handle the gigapixel scale of whole slide images, we establish baseline comparisons using two approaches: (1) randomly selecting 30 patches from each WSI and inputting them into MLLMs (e.g., GPT-4 (Achiam et al., 2023), LLaVA-Med (Li et al., 2024a), MedDr (Li et al., 2024a)), followed by a majority voting scheme to generate slide-level predictions; and (2) directly inputting a WSI thumbnail, resized to 1024\u00d71024 pixels, into the MLLMs. For VQA tasks, we further evaluate performance by comparing against random prediction baselines and text-only models, thereby assessing"}, {"title": "5 CONCLUSION", "content": "In this work, we present SlideChat, the first vision-language assistant capable of understanding gigapixel whole-slide images. Furthermore, we creat SlideInstruction, a largest comprehensive WSI instruction-following dataset to develop SlideChat, as well as SlideBench, a multi-modal benchmark designed to evaluate SlideChat across diverse scenarios. SlideChat demonstrates excellent chat abilities and achieves state-of-the-art performance on 18 tasks.\nWe bridge the gap between MLLMs and pathology images at the whole-slide level with SlideChat, and believe that it represents a significant advancement towards general pathology and general medical artificial intelligence (GMAI)."}, {"title": "A SLIDEINSTRUCTION AND SLIDEBENCH", "content": "In this section, we present the sources of the constructed SlideInstruction and SlideBench, which are derived from ten TCGA datasets as well as the BCNB challenge dataset. The Table 5 provides a detailed overview of the specific number of WSIs."}, {"title": "A.1 DATA SOURCE", "content": "In this section, we present the sources of the constructed SlideInstruction and SlideBench, which are derived from ten TCGA datasets as well as the BCNB challenge dataset. The Table 5 provides a detailed overview of the specific number of WSIs."}, {"title": "A.2 CURATION SCOPE AND PROMPT", "content": "In this section, we illustrate the various dimensions of VQAs in SlideInstruction and SlideBench, ensuring comprehensive coverage of diverse pathological scenarios. This includes 3 broad categories and 13 narrow categories. Below are the contents for each category, which help to delineate their scope and meaning, thereby enabling GPT to extract high-quality question-answer pairs more effectively."}, {"title": "A.2.1 ScOPE", "content": "Microscopy This category involves assessing the ability to generate microscopy descriptions of pathology images, focusing on clinically relevant features:\n\u2022 Tissue Architecture and Arrangement: Questions in this category should evaluate the understanding of overall tissue structure and spatial organization within a histological section.\n\u2022 Cytomorphological Characteristics: These questions should focus on the detailed description of individual cell morphology, including nuclear and cytoplasmic features.\n\u2022 Tumor Characteristics: Questions under this category should assess the ability to identify and describe features specific to tumors, such as tumor differentiation, invasion, and specific patterns associated with different types of tumors.\n\u2022 Histopathological Changes: This category should include questions that evaluate the recognition and description of pathological changes in tissue, such as necrosis, inflammation, fibrosis, and other alterations that indicate disease processes.\nDiagnosis This category tests the ability of models to suggest a reasonable diagnosis based on histological images and relevant clinical context:\n\u2022 Disease Detection: Questions in this category should evaluate the model's ability to identify the presence or absence of a disease based on histological features and clinical information.\n\u2022 Disease Classification: These questions should focus on distinguishing between different types or subtypes of diseases, assessing the model's capability to classify conditions accurately based on morphological and histopathological criteria."}, {"title": "Clinical", "content": "This category tests the ability of models to retrieve and apply clinically relevant background knowledge about diseases:\n\u2022 Treatment Guidance: Questions in this category should assess the model's ability to recommend appropriate treatment options based on the disease in question, considering factors such as disease stage, patient demographics, and any specific clinical guidelines.\n\u2022 Prognostic Assessment: These questions should focus on evaluating the model's ability to predict the likely course and outcome of a disease, including survival rates, potential complications, and long-term outcomes based on clinical and pathological data.\n\u2022 Risk Factors: Questions under this category should test the model's knowledge of risk factors associated with specific diseases, including genetic, environmental, and lifestyle factors that may influence disease development or progression.\n\u2022 Biomarker Analysis: This category should include questions that evaluate the ability to identify and interpret biomarkers relevant to the diagnosis, prognosis, or treatment of diseases, emphasizing their role in personalized medicine and targeted therapy."}, {"title": "A.2.2 DESIGNED PROMPTS", "content": "Report Cleaning Prompt. The prompt used to clean up the report from the original TCGA report is represented in Table 6. This process effectively eliminates extraneous noise from the report, thereby establishing a more solid foundation for caption and QA pairs generation."}, {"title": "B EXPERIMENT", "content": "B.1 IMPLEMENTATION DETAILS\nWe preprocessed each WSI by segmenting it into 224 \u00d7 224 nonoverlapping patches at a 20x magnification level, excluding background regions. We implemented our model using the Xtuner (Contributors, 2023) toolkit and trained it across two stages on 8 \u00d7 NVIDIA A100 GPUs. The training"}, {"title": "B.2 ABILITY SHOWCASE", "content": "B.2.1 CAPTIONING ABILITY\nThe examples shown in Figure 7 illustrate the capability of our model, SlideChat, to effectively perform whole-slide image captioning tasks. SlideChat demonstrates its proficiency in generating detailed and contextually accurate summaries for complex pathological whole-slide images, accurately capturing key clinical findings and pathological features. Whether summarizing broad findings, explaining pivotal details, or highlighting core results, SlideChat showcases an advanced understanding of whole-slide images, providing concise yet informative reports that align with clinical terminology and expectations.\nB.2.2 VQA ABILITY\nFigure 8 showcases the conversational examples of SlideChat, demonstrating its ability to accurately answer a range of questions based on WSIs, covering diverse aspects such as histological classifications, tumor grading, lymph node involvement, and treatment decisions. SlideChat effectively interprets complex pathological data, engages in nuanced question-and-answer exchanges, and delivers clinically relevant responses. This reflects its potential as an intelligent assistant capable of supporting pathologists in diagnostic decision-making by providing insightful, context-aware dialogue grounded in visual pathology data.\n\u0412.2.3 \u0421\u043eMPARING MODEL OUTPUTS\nFigure 9 presents a comparative analysis of the outputs from SlideChat and other models within SlideBench. The examples illustrate SlideChat's remarkable capacity to precisely classify tumors, identify distinct histological features, and describe the structural organization of tumor cells from WSIs. SlideChat demonstrates a unique proficiency in capturing both local and global features-seamlessly integrating detailed microscopic characteristics with broader contextual understanding to deliver accurate and clinically meaningful interpretations. In contrast, existing models are limited to processing small pathology images, often yielding ambiguous or incorrect classifications. This underscores SlideChat's advanced capability in comprehending whole-slide images by incorporating both intricate details and a comprehensive visual perspective."}, {"title": "B.3 DETAILED TEST PERFORMANCE", "content": "B.3.1 PERFORMANCE ON SLIDEBENCH-VQA (TCGA)\nThe results presented in the tables demonstrate a comprehensive evaluation of SlideChat's performance on SlideBench-VQA (TCGA) in comparison to other existing models across microscopy, diagnosis, and clinical tasks. In microscopy, SlideChat significantly outperforms its counterparts, achieving a notable overall accuracy improvement of 14.34 points over the nearest model. This strong performance is consistent across sub-tasks, such as tissue architecture analysis, tumor characteristics identification, and cytomorphological assessment, showcasing SlideChat's advanced capability to analyze both detailed cellular structures and broader histopathological changes. In the diagnostic tasks, SlideChat also demonstrates superior accuracy, with an overall gain of 15.49 points, excelling in disease detection, classification, staging, grading, and differential diagnosis. The clinical analysis results further validate the model's strength, with SlideChat outperforming other methods by 10.01 points overall, particularly excelling in treatment guidance, biomarker analysis, and risk factor assessment. These results illustrate SlideChat's capability to seamlessly handle complex medical data and deliver reliable insights across multiple clinical and diagnostic domains, indicating its potential as a robust tool for comprehensive pathology analysis.\nB.3.2 PERFORMANCE ON SLIDEBENCH-VQA (BCNB)\nThe evaluation of SlideChat on SlideBench-VQA (BCNB), a real-world dataset designed for zero-shot testing, further underscores its ability to generalize effectively to unseen data. SlideChat demonstrates an overall accuracy improvement of 12.71 points compared to other models, showcasing its ability to generalize well across diverse and complex breast cancer-related tasks. SlideChat's performance is particularly strong in identifying tumor type, ER status, PR status, and HER2 status, demonstrating a nuanced understanding of critical histopathological features. Nevertheless, in the more complex tasks of HER2 Expression, Histological Grading, and Molecular Subtype classification, SlideChat still exhibits potential for improvement, highlighting specific areas that warrant further refinement to enhance its overall performance."}]}