{"title": "Interpretable deep learning illuminates multiple structures fluorescence imaging: a path toward trustworthy artificial intelligence in microscopy.", "authors": ["Mingyang Chen", "Luhong Jin", "Xuwei Xuan", "Defu Yang", "Yun Cheng", "Ju Zhang"], "abstract": "Live-cell imaging of multiple subcellular structures is essential for understanding subcellular dynamics. However, current techniques often require multiple rounds of staining, leading to photobleaching and reduced dye stability. Here, we present the Adaptive Explainable Multi-Structure Network (AEMS-Net), a deep-learning framework that enables simultaneous prediction of two subcellular labels from a single image acquisition. The model normalizes staining intensity and prioritizes critical image features by integrating attention mechanisms and brightness adaptation layers. Leveraging the Kolmogorov-Arnold representation theorem, our model decomposes learned features into interpretable univariate functions, enhancing the explainability of complex subcellular morphologies. We demonstrate that AEMS-Net tracks dynamic changes in mitochondrial morphology during cell migration, requiring only half the conventional staining procedures. Notably, this approach achieves over 30% improvement in imaging quality compared to traditional deep learning methods, establishing a new paradigm for long-term, interpretable live-cell imaging that advances the ability to explore subcellular dynamics.", "sections": [{"title": "Introduction", "content": "The task of multi-structural observation in live-cell microscopy imaging plays a critical role in subcellular biology. It enables researchers to investigate interactions between different types of intracellular structures, such as microtubules and mitochondria. This capability is essential for understanding disease mechanisms\u00b9, identifying potential therapeutic targets\u00b2, and drug screening\u00b3. Traditional methods enable the observation of interactions between multiple organelles in cells by combining fluorescent probes with distinct emission wavelengths and multi-color fluorescence microscopy.\nHowever, existing multichannel fluorescence microscopy has several significant limitations. These include spectral overlap4,5, which can cause signal crosstalk between channels, and photobleaching 6,7, which limits imaging duration. Additionally, phototoxicity 8,9 from long-term light exposure can damage live cells. Furthermore, the large volume of data generated requires complex computational analysis, and the high costs and technical demands of multichannel systems can be prohibitive. These challenges emphasize the need for continued imaging and data processing advancements. Deep learning has emerged as a promising research frontier in fluorescence microscopy imaging, offering novel approaches to overcome traditional imaging limitations. Despite its potential, the inherent black-box nature10 of these models introduces critical challenges regarding interpretability and reliability\u00b9\u00b9, which remain key concerns in biological research applications.\nPrevious research has focused on specific applications of deep learning in"}, {"title": "Results", "content": "Previous contents have examined the drawbacks of multiple staining rounds. Here, we present subcellular microscopy images comparing double-staining versus single-staining approaches (Fig. 1a). Our goal centered on reconstructing mitochondria and microtubules through a single staining procedure, which would not only streamline laboratory protocols but also preserve subcellular viability - establishing a more robust foundation for live-cell imaging. To achieve this, we integrated the Kolmogorov-Arnold representation theorem into U-Net architecture, resulting in an innovative deep learning framework: AEMS-Net (Fig. 1b). This neural network incorporates KAN convolution40, attention mechanisms, and adaptive brightness layer to reconstruct multi-subcellular structures. Within AEMS-Net, we initially overlay mitochondrial and microtubular structures from double-staining images while feeding the original image as paired input to the network (Fig. 1c). This approach ensures that AEMS-Net effectively captures both structural features. The network then processes these inputs through multiple KAN convolution layers (Fig. 1d, Supplementary Fig. S3) with downsampling operations (Supplementary Fig. S1), preserving multi-scale features that undergo enhancement through attention modules. During subsequent feature fusion, original features enter through residual connections\u2074\u00b9, maintaining stable gradient flow throughout model training. To address brightness variations arising from staining overlay, we introduced an adaptive brightness pooling layer (Fig. 1e) after the final up sampling operation, enabling AEMS-Net to minimize brightness-related interference in separation and reconstruction tasks."}, {"title": "Discussion", "content": "Multiple fluorescence staining introduces adverse effects for live cell imaging, including spectral crosstalk and phototoxicity. While deep learning approaches have alleviated these challenges, they remain fundamentally limited by poor interpretability. The advancement of subcellular imaging proceeds along two parallel trajectories: developing novel live-cell and subcellular fluorescence techniques, and enhancing established methodologies. Within this broader context, our research focuses on two challenges: circumventing the limitations inherent in multiple staining protocols while establishing transparent deep learning frameworks that foster confidence in AI-driven biomedical applications.\nIn this work, we introduce AEMS-Net for subcellular structure separation and reconstruction. Compared to the widely adopted U-Net architecture, AEMS-Net demonstrates three distinct advantages: First, it exhibits enhanced learning and representation capabilities, capturing subcellular structural features with higher reconstruction quality scores despite limited training data. Second, AEMS-Net leverages Layer-CAM to illuminate factors influencing separation and reconstruction, quantifying pixel-level significance through heatmap visualization an interpretability analysis applicable to any encoder-decoder network architecture. Third, AEMS-Net enables expandable training for diverse subcellular structures. For instance, the traditional analysis of five subcellular structures demands five separate fluorescence"}, {"title": "Data and Code Availability", "content": "All relevant data are included within the article and its Source Data. Due to size limitations, the training datasets can be obtained from the corresponding author upon request. The original data for Fig. 2, Table 1, 2, and Supplementary Table 3,4, are provided as a Source Data File.\nThe source code and pre-trained models used in this study are available at the following GitHub link:"}, {"title": "Methods", "content": "The COS-7 cells were cultured in high glucose Dulbecco's modified Eagle's medium (DMEM) (Gibco, #11965092), supplemented with 10% fetal bovine serum (Sigma-Aldrich, #F8313) and 1% penicillin\u2013streptomycin (Beyotime, #C0222) at 37\u00b0C in a humidified 5% CO2 incubator. Cells were planted into a 35-mm glass bottom dish (Cellvis, #D35-20-1-N) for fluorescence imaging experiments.\nFor the training, testing and validation steps, COS-7 cells were transfected with EMTB-3\u00d7eGFP and stained with MitoTracker Orange (Thermo Fisher, M7510) to label the microtubules (MTs) and mitochondria (Mito) respectively. To demonstrate the application potential, we co-transfected COS-7 cells with EMTB-3\u00d7eGFP and Tom20-mEmerald.\nThe images were acquired by Olympus FV3000 fluorescence microscope equipped with 488 nm and 561 nm laser lines. The objective lens is 100X (1.45 NA). The confocal imaging speed for each image is around 5s.\nThe experimental protocol involved the independent preparation and staining of mitochondrial and microtubule specimens for microscopic visualization. The corresponding subcellular architectures were meticulously integrated while maintaining rigorous segregation among training, validation, and testing cohorts. A systematic random-sampling approach was implemented to augment the dataset dimensionality and enhance the deep learning model efficacy. This methodology encompassed the extraction of 256\u00d7256-pixel regions from each subcellular ensemble, followed by comprehensive intra-group image superimposition analyses. The systematic approach generated an extensive image repository encompassing 2,142 training specimens, 117 validation specimens, and 720 test specimens (Supplementary Table 2). Although inherent variations in fluorescence intensity were observed between mitochondrial and microtubule channels, the deliberate omission of image preprocessing aimed to minimize operational complexity and resource requirements for the research community. Network performance optimization incorporated intensity normalization through maximum intensity scaling, mapping pixel values to the interval [0,1]. Subsequently, the images underwent transformation into 256\u00d7256\u00d7c PyTorch tensors, wherein c denotes the channel dimensionality, facilitating neural network training procedures."}, {"title": "Loss Function and Training Details", "content": "Segmentation of complex subcellular structures requires precise loss function design that captures the intricate spatial relationships between subcellular components. We developed a unified loss function that simultaneously addresses the segmentation of mitochondria and microtubules, integrating a joint loss approach with L1 regularization to enhance structural representation and generalization. The proposed loss function is defined as:\n$L_{total} = L_{mito} + L_{micro} + 2 \u00d7 L_{1reg}$ (1)\nWhere $L_{mitochondria}$ and $L_{microtubules}$ represent the respective structure-specific segmentation losses, and A \u00d7 $L_{1reg}$ denotes the L1 regularization term that promotes sparsity in the model parameters.\nSeveral equations jointly regulate the loss functions for mitochondria and microtubules, as represented by the combined loss function shown below:\n$L_{mito/micro} = W_{mse} \u00d7 L_{mse} + W_{grad} \u00d7 L_{grad} + W_c \u00d7 L_c + W_{focal} \u00d7 L_{focal}$ (2)\n$L_{mse} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$ (3)\n$L_{grad} = \\frac{1}{n} \\sum_{i=1}^{n} (\\nabla y_i - \\nabla \\hat{y}_i)$ (4)\n$L_{contrastive} = -log(\\frac{exp(sim(y_i, \\hat{y}_i)/T)}{\\sum_{j=1}^{n} exp(sim (y_i, \\hat{y}_j)/T)})$ (5)\n$L_{focal} = -a(1 \u2212 p_t)^\u03b3 \u00d7 log(p_t)$ (6)\nThe proposed loss function is a weighted combination that integrates four distinct sub-loss components: mean squared error loss ($L_{mse}$), gradient loss ($L_{grad}$), contrastive loss ($L_c$), and focal loss ($L_{focal}$). The weight coefficients for each component are denoted as, $W_{mse}, W_{grad}, W_c$, and $W_{focal}$ respectively.\nFor $L_{mse}$ (Mean Squared Error Loss), n denotes the number of samples, where $y_i$ represents the true value of the i-th sample and $\\hat{y}_i$ indicates the predicted value of the i-th sample.\nFor $L_{grad}$ (Gradient Loss), $\\nabla y_i$ denotes the gradient of the true values and $\\nabla \\hat{y}_i$ represents the gradient of the predicted values.\nFor $L_{contrastive}$ (Contrastive Loss), $sim(y_i, \\hat{y}_i)$ indicates the similarity measure between the true values and the predicted values. T is the temperature parameter used to adjust the smoothness of the distribution, and exp() denotes the exponential function.\nFor $L_{focal}$ (Focal Loss), \u03b1 is the adjustment parameter, pt represents the predicted probability value, and y is the focusing parameter used to adjust the weight of hard-to-classify samples.\nThe AEMS-Net processes input images with dimensions of 256\u00d7256 pixels. The initial preprocessing stage involves image transformation into PyTorch tensors, followed by convolution operations that expand the channel dimension to 64. Through sequential downsampling operations, the spatial dimensions undergo progressive"}, {"title": "Evaluation Metrics", "content": "We use PSNR, NRMSE, and SSIM as evaluation metrics for the test set, and each metric can be calculated using the following formulas:\n$PSNR = 20 \u00d7 log_{10}(\\frac{MAX_I}{\\sqrt{\\frac{1}{W\u00d7H} \\sum_{i=1}^{W} \\sum_{j=1}^{H}((U(i,j)-V(i,j))^2}})$ (7)\n$NRMSE = \\sqrt{\\frac{\\sum_{i=1}^{W} \\sum_{j=1}^{H}(U(i, j) \u2013 V(i,j))^2}{\\sum_{i=1}^{W} \\sum_{j=1}^{H}(U(i, j))^2}}$ (8)\n$SSIM(x, y) = \\frac{(2\u03bc_x\u03bc_y + C_1)(2\u03c3_{xy} + C_2)}{(\u03bc_x^2\u03bc_y^2 + C_1) (\u03c3_x^2\u03c3_y^2 + C_2)}$ (9)\nMAXI represents the maximum pixel value of the image. U(i,j) denotes the pixel value of the original image at the position (i, j), while V(i,j) indicates the pixel value of the predicted image at the same position. W and H refer to the width and height of the image, respectively, and i and j are the index values for the pixel coordinates. Furthermore, \u03bc\u03c7 and \u00b5y represent the mean values of images x and y, respectively, and ox and oy denote the standard deviations of images x and y. The term oxy signifies the local covariance between images x and y. C\u2081 and C2 are constant terms added to avoid division by zero. Lastly, x and y refer to the original image and the predicted image, respectively.\nWe used Python code to calculate these three metrics, utilizing the functions from the skimage.metrics library.(https://pypi.org/project/scikit-image)"}, {"title": "Generalization and Interpretability Studies", "content": "In the present investigation, we conducted a rigorous evaluation of two deep learning architectures, AEMS-Net and U-Net, utilizing the Application dataset, which encompasses a comprehensive collection of single-stained microscopic images depicting multiple subcellular structures (Supplementary Table 1). This dataset captures complex biological dynamics, including mitochondrial motility, fusion events, and fission processes along microtubule networks across temporal dimensions.\nThe trained models were challenged with unmodified Application dataset inputs to evaluate their generalization capabilities on novel, previously unexamined data. Our systematic analysis focused on specific temporal intervals, examining the comparative efficacy of AEMS-Net and U-Net in two critical aspects: the extraction of individual structural elements and the reconstruction of dynamic multi-structural interactions. The experimental results demonstrated that AEMS-Net exhibited superior performance across both evaluation metrics (Fig. 3, 4, Supplementary Fig.S3, S4).\nRecognizing the importance of model generalization, we employed the interpretability module of AEMS-Net to conduct a detailed analysis of the input images (Fig. 5, Supplementary Fig.S2). This process began with a forward pass to save the feature maps, followed by a backward pass to compute gradients, yielding both positive and negative activation values. The subsequent feature processing module allowed us to save the interpretable activation maps from all encoder and decoder layers (Supplementary Fig. S6-9).\nUpon analyzing the interpretable activation maps of AEMS-Net and U-Net (Supplementary Fig.S11), it became evident that our method aligns more closely with human intuitive reasoning. AEMS-Net effectively delineated the slender characteristics of microtubules and the spherical nature of mitochondria. At the same time, U-Net struggled to provide such clarity, with minimal distinction between the positive and negative activation maps. This comprehensive evaluation underscores the robustness and interpretability of our method in the context of subcellular imaging analysis."}]}