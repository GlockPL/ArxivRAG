{"title": "MUTAGREP: Execution-Free Repository-Grounded Plan Search for Code-Use", "authors": ["Zaid Khan", "Ali Farhadi", "Ranjay Krishna", "Luca Weihs", "Mohit Bansal", "Tanmay Gupta"], "abstract": "When a human requests an LLM to complete a coding task using functionality from a large code repository, how do we provide context from the repo to the LLM? One approach is to add the entire repo to the LLM's context window. However, most tasks involve only fraction of symbols from a repo, longer contexts are detrimental to the LLM's reasoning abilities (Kuratov et al., 2024), and context windows are not unlimited. Alternatively, we could emulate the human ability to navigate a large repo, pick out the right functionality, and form a plan to solve the task. We propose MUTAGREP (Mutation-guided Grounded Repository Plan Search), an approach to search for plans that decompose a user request into natural language steps grounded in the codebase. MUTAGREP performs neural tree search in plan space, exploring by mutating plans and using a symbol retriever for grounding. On the challenging LongCodeArena benchmark, our plans use less than 5% of the 128K context window for GPT-40 but rival the coding performance of GPT-40 with a context window filled with the repo. Plans produced by MUTAGREP allow Qwen 2.5 Coder 32B and 72B to match the performance of GPT-40 with full repo context and enable progress on the hardest LongCodeArena tasks. Project page: zaidkhan.me/MutaGReP", "sections": [{"title": "1. Introduction", "content": "Code generation systems powered by LLMs are routinely tasked with writing new code given an existing large codebase. One approach to conditioning an LLM's generation on a repository is to utilize its working memory by concatenating all files in the codebase into a massive prompt. This is an inefficient use of finite context, because many programming tasks require only a small fraction of all symbols (functions, classes, global variables etc) in the codebase. Recent in- vestigation also shows that leading LLMs are effective at utilizing less than 20% of their context lengths with a sharp decline in performance with increasing reasoning complexity (Kuratov et al., 2024). Can we do better?\nHuman programmers are able to understand complex code- bases with a much smaller biological working memory. They achieve this by decomposing the target task into smaller steps and then using code search tools to identify relevant symbols (functions, classes etc) to use in each step. This is often an iterative process where the programmer interacts with the codebase to develop a realizable plan based on the symbols available in the codebase. The realizable plan can then be implemented."}, {"title": "2. Related Work", "content": "Repository-grounded code generation. Existing work on repo-level code generation has explored two distinct directions. One line of work focuses on building software engineering agents that can edit real-world codebases to solve"}, {"title": "3. Method", "content": "Overview We formulate repository-grounded planning as a search problem over the space P of possible plans. Given the large space of possible plans and the semi-structured nature of plans, we employ a tree-based search algorithm"}, {"title": "3.1. Successor Function and Grounding", "content": "The successor function \\(s : P \\rightarrow P(P)\\) determines how we explore the space of possible plans. For a given plan \\(p = [x_1,..., x_n]\\) where each \\(x_i = (t_i, B_i)\\), the successor function must generate new plans while attempting to ensure each step remains grounded in symbols B available in the codebase."}, {"title": "3.1.1. SUCCESSOR FUNCTION VARIANTS", "content": "We consider two choices of the successor function:\nMonotonic. The monotonic successor function \\(s_m\\) preserves all steps in the parent plan, only mutating the plan by adding new steps. Formally, given a plan \\(p = [(t_1, B_1), ..., (t_n, B_n)]\\), \\(s_m(p)\\) generates plans of the form \\([(t_1, B_1), ..., (t_n, B_n), (t_{n+1}, B_{n+1})]\\) where \\(t_{n+1}\\) is a new intent and \\(B_{n+1} \\subseteq B\\) contains the symbols needed to implement it. This ensures the search progressively builds longer plans while maintaining previously discovered steps, evocative of monotonic relaxations in planning (Bonet & Geffner, 2001; McDermott, 1999; Hoffmann & Nebel, 2001).\nUnconstrained. An unconstrained successor function \\(s_u\\) may perform arbitrary modifications to any part of the plan. For a plan p, \\(s_u(p)\\) can generate plans with modified, deleted, or reordered steps, while maintaining the requirement that each step \\((t, B)\\) is grounded (\\(B\\subseteq B\\)). This allows the search to escape local optima by making dramatic changes to plans, similar to mutation operators in evolutionary search for planning (Justesen et al., 2018; Perez et al., 2013).\nBoth successor functions are implemented using an LLM with appropriate prompts (Appendix C). The number of successors or branching factor is a crucial hyper parameter that allows us to control the allocation of the test-time compute budget - a larger branching factor allows a greater exploration of the plan space P. Given a branching factor of f, we sample f times from the LLM (GPT-40) to generate f successors."}, {"title": "3.1.2. PLAN GROUNDING", "content": "To guide the successor function and aid node scoring (for ranking), we need to ground each step intent in symbols found in the codebase that might be used to implement each step. This is achieved through the grounding function \\(g: L \\rightarrow P(B)\\) which maps a natural language intent t to relevant symbols in the codebase \\(B \\subseteq B\\). This is challenging due to the semantic gap between high-level natural language intents and low-level code implementations (Liang et al., 2022). Rather than attempting direct intent-to-code matching, we bridge this gap through an intermediate repre- sentation."}, {"title": "3.2. Scoring Function and Exploration Strategies", "content": "The scoring function \\(h : P \\rightarrow R\\) plays two crucial roles in our approach. First, it enables informed search algorithms (e.g. best-first search) by guiding exploration toward promising regions of the plan space. Second, it allows selecting the most promising plans to pass to downstream code generation, even when using uninformed search strategies like depth-first search which simply rely on node expansion order via a stack data structure to determine the next plan to mutate.\nDesigning an effective ranking function is challenging because we need to impose an ordering over the plan space that correlates with two key properties: (1) the likelihood that a plan achieves the user's intent, and (2) the feasibility of implementing each step with the grounded symbols. Unlike traditional planning scenarios where the scoring function emerges naturally from the environment, we must construct a scoring function that can evaluate plans without executing them."}, {"title": "3.2.1. SCORING FUNCTION VARIANTS", "content": "Symbol Diversity Scorer implements a heuristic based on symbol coverage. For a plan \\(p = [(t_1, B_1), ..., (t_n, B_n)]\\):\n\\(h_{sym}(p) = |\\cup_{i=1}^n B_i|\n(1)\nThis rewards plans that incorporate a diverse set of symbols from the codebase, based on the intuition that effective plans likely require integrating multiple components. While"}, {"title": "Decomposed Likert Scorer", "content": "draws inspiration from recent work showing that decomposing evaluation into fine-grained criteria improves assessment reliability (Saad-Falcon et al., 2024). We construct a scoring function that evaluates both plan-level and step-level properties using a large language model as a judge. Given the user query, the plan, and symbol definitions, we ask an LLM to produce the following judgement scores one a 7-point Likert scale (Likert, 1932):\n\u2022 A plan-level accuracy score \\(l_p\\) assessing whether the plan solves the user request\n\u2022 Step-level feasibility scores \\(l_1,..., l_n\\) evaluating whether each step intent \\(t_i\\) is realizable with the grounded symbols \\(B_i\\)\nDuring informed search, we aggregate these scores into a single value to pick the next node for exploration:\n\\(h_{likert}(p) = \\frac{1}{2} (l_p + \\frac{1}{n} \\sum_{i=1}^n l_i)\\) (2)\nHowever, for final plan selection after search completes, we empirically found that a hierarchical sorting approach is more effective. Plans are first sorted by their plan-level score \\(l_p\\), with ties broken by the average step-level score \\(\\frac{1}{n}\\sum_{i=1}^n l_i\\). This two-level sorting ensures we prioritize plans that are likely to achieve the user's intent while using step-level feasibility as a secondary criterion.\nOracle Scorer. For some of our ablations, we use symbol recall (% of ground truth symbols in generated plans) to score plans. Since this requires reference code this is not a practical setting but allows us to study the effect of components like successor function and tree-search algorithms on plan search performance in a controlled setting."}, {"title": "3.2.2. EXPLORATION STRATEGIES", "content": "Our framework allows for plugging in any tree-search algorithm to guide the exploration of plan space P. We primarily use best-first search in our experiments to make use of the scoring function for informed exploration while using depth- first search as an uninformed search baseline. We do not use breadth-first search because it is particularly wasteful for monotonic successor function as it spends most of its search budget on early stage incomplete plans. We leave more complex algorithms like MCTS for future work."}, {"title": "4. Experiments", "content": "Benchmark. We evaluate our plans and code generated from our plans using the LongCodeArena (LCA) bench-"}, {"title": "4.1. System-Level Comparisons", "content": "First, we evaluate plan search as part of the end-to-end system that generates code given a user query. This compares the overlap scores for the code generated from our plans to alternative approaches. We use GPT-40 (128K context window) for both plan search and for generating code from plans. Specifically, we compare the following approaches:\n\u2022 Instruction Only: The model receives only the user query with no additional context from the codebase.\n\u2022 ReAct: In our plan search framework, a ReAct baseline is equivalent to setting branching factor to 1 with a monotonic successor function resulting in a linear chain instead of a tree of plans. The final plan is pro- vided as context for code generation.\n\u2022 Plan Search: Our approach using unconstrained successor function, informed best-first search (branching factor=3, budget=80), and the symbol diversity scorer. The resulting plan is provided as context for code gen-"}, {"title": "4.2. Successor Function Ablation", "content": "We evaluate how the choice of successor function impacts plan search performance. We compare two variants of the successor function: (i) monotonic, which can only add new steps and (ii) unconstrained, which can modify any part of the plan (see Section 3.1). To isolate the effect of the successor function, we fix other components of the system: informed search (best-first) with branching factor of 3, maximum tree depth of 20, and use an oracle ranker that scores plans based on their recall of ground truth symbols for the user query. We use GPT-40 to guide the plan search and vary the search budget (nodes expanded) from 20 to 160.\nFigure 5 shows plan's symbol recall (percentage of ground truth symbols found) as a function of search budget. First, note that for both successor functions, performance improves with increasing search budget. This re-affirms the role of scaling test-time compute to improving reasoning performance. Next, we see that that unconstrained mutation consistently outperforms monotonic mutation across all budgets, with the gap being particularly pronounced at lower budgets (+30% at budget=20). The unconstrained successor achieves its peak performance with fewer steps as compared to monotonic successor suggesting more efficient exploration of the plan space through non-monotonic changes. We also show the performance of ReAct (linear search with a monotonic successor as described in Section 4.1) for reference. Tree search significantly outperforms linear search, regardless of the choice of successor function."}, {"title": "4.3. Traversal Ablation", "content": "We now investigate how different search strategies and branching factors affect plan quality. We compare informed search (best-first) against uninformed search (depth-first) as well as linear search. For this experiment, we use the unconstrained successor function and budget of 160 with a maximum tree depth of 20. As in the previous experiment, we use an oracle ranker for informed search to establish an upper bound on achievable performance.\nAs shown in Figure 6, informed search significantly outperforms uninformed and linear search strategies with branching factor of 3 showing healthy gains over 2. This is because informed search makes a more efficient use of the compute budget by exploring more promising parts of the plan space P. Both tree-search strategies do much better than linear search which does not explore alternative solutions."}, {"title": "4.4. Scoring Function Ablation", "content": "Previous ablations used an oracle scorer to establish the potential of different search strategies. In practice we need"}, {"title": "4.5. Enhancing Other LLMs with Searched Plans", "content": "Next, we examine whether plans searched by our system can enhance the performance of other language models,"}, {"title": "4.6. Impact of Plans on Hard LongCodeArena Tasks", "content": "We analyze performance on the most challenging tasks in LongCodeArena (the 10% of tasks for which GPT-40 with the entire repository as context makes the least progress). Figure 8 compares providing the full repository as context (red) against using our tree-searched plans as context (blue)."}, {"title": "5. Conclusion", "content": "MUTAGREP automatically enriches user queries with repo- grounded plans found through execution-free tree search. Our system decomposes high-level requests into detailed plans where each step pairs natural language intent with relevant codebase symbols. Through experiments on Long- CodeArena, we demonstrated that our plans: (1) are ef- fective as context for code generation; (2) enable weaker models to match stronger models' performance; and (3) enable progress on challenging tasks where even frontier models with full repository context struggle. Our results show that grounded plan search is a promising direction for improving code-use while maintaining efficiency and interpretability."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "B. More Analysis of Hard Tasks on LongCodeArena", "content": "To better understand the robustness of our approach, we analyze worst-case and average-case performance on the most challenging tasks in LongCodeArena (bottom 10th percentile by full-repository performance). Figure 9 compares three approaches using GPT-40 as the code generator: providing the full repository as context (red), using ReACT-generated plans (orange), and using our tree-searched plans (blue). For each approach, we sample 5 solutions per task and show both the average performance (top of bars) and minimum performance (bottom of error bars) using the API overlap metric.\nOur approach consistently outperforms ReACT-style planning, showing better average performance on 11 out of 13 tasks. More importantly, the worst-case performance with our plans (indicated by the bottom of the blue bars) often exceeds the average performance of both baselines, suggesting that tree-searched plans lead to more reliable code generation. This is particularly evident in repositories like moviepy, where our approach's minimum performance (45.45%) far exceeds both the ReACT average (12.73%) and full repository average (0%).\nThese results demonstrate that systematic tree search produces more robust plans than either naive context inclusion or linear planning approaches, particularly on challenging tasks where standard approaches struggle to make progress."}, {"title": "C. Successor Function Prompts", "content": "Our successor functions rely on prompts to guide the LLM in mutating plans. Figure 10 shows the prompt template used for the monotonic successor function, which can only add new steps while preserving existing ones. Figure 11 shows the prompt for the unconstrained successor function, which can modify any part of the plan."}, {"title": "D. Scoring Function Prompts", "content": "The Likert scoring function uses the prompt shown in Figure 12 to evaluate plans. The prompt breaks down evaluation into two aspects: (1) whether the overall plan achieves the user's intent and (2) whether each step is feasible with its retrieved symbols. The scoring is done on a 7-point Likert scale."}, {"title": "E. Code Generation Prompts", "content": "The code generation prompts are designed to evaluate different approaches to providing repository context. Figure 13 shows how we present plans as structured context for code generation. Figure 14 demonstrates the full-repository baseline approach where the entire codebase is provided as context. Figure 15 shows the minimal instruction-only setting which provides no additional context beyond the user query."}]}