{"title": "Data Science Students Perspectives on Learning Analytics: An Application of Human-Led and LLM Content Analysis", "authors": ["Raghda Zahran", "Jianfei Xu", "Huizhi Liang", "Matthew Forshaw"], "abstract": "Domain: This article contributes to the field of learning analytics within higher education, focusing specifically on insights generated by postgraduate students specialising in data science. It aims to enrich the broader discourse on relevant learning analytics in education.\n\nObjective: This study is part of a series of initiatives at a UK university designed to cultivate a deep understanding of students' perspectives on analytics resonating with their unique learning needs. It explores the results of collaborative data processing undertaken by postgraduate students who examined an Open University Learning Analytics Dataset (OULAD), thus illuminating the connections between their experiences in data science and their comprehension of learning analytics.\n\nMethods: A qualitative approach was adopted, integrating a Retrieval-Augmented Generation (RAG) and a Large Language Model (LLM) technique with human-led content analysis to gather information about students' perspectives based on their submitted work (conceptual understanding, formulated questions, and interpretations) of learning analytics. The study involved 72 postgraduate students in 12 groups, all enrolled in a four-week group Innovation Project as part of their module in data science.\n\nFindings: The analysis of group work revealed diverse insights into essential learning analytics from the students' perspectives. All participating groups adopted a structured data science methodology, employing various techniques, including linear regression analysis, influence assessments, and predictive analytics. The questions formulated by the student groups were categorised into seven themes, reflecting their specific areas of interest. Although there is variation in the selected variables to examine and interpret the correlations, a consensus was found regarding the general results. Only a limited number of groups engaged in predictive analytics, including those that used multiple models and critically assessed their levels of accuracy.\n\nConclusion: A significant outcome of this study is that students specialising in data science exhibited a deeper understanding of learning analytics, effectively articulating their interests through inferences drawn from their data analyses. While a human-led content analysis enabled a general understanding of the students' perspectives, the use of LLM provided nuanced insights. Continued discussions and research focused on the scoping process and designing learning analytics and its influence on student experiences are crucial for developing a comprehensive understanding within educational contexts.", "sections": [{"title": "Introduction", "content": "Learning analytics in higher education is an evolving field focused on the measurement, collection, analysis, and reporting of data concerning university students and their contexts. A significant emphasis is placed on providing meaningful information for both students and staff, which addresses one of the primary challenges faced in higher education (EDUCAUSE, 2024). Furthermore, fostering a culture of data-driven decision-making is essential (Jisc & KPMG, 2024). This underscores that deriving actionable insights from this data, along with establishing robust governance structures, is a key priority for institutions seeking to enhance\n1"}, {"title": "Previous Evaluation of Students' Perspectives", "content": "The literature on learning analytics has undergone a significant transformation, increasingly prioritising student-centred approaches. This shift underscores the importance of designing software tools and processes that focus on the needs and experiences of users (Alfredo, Echeverria, Jin, Swiecki, et al., 2024; Topali et al., 2024; Buckingham Shum et al., 2024). Learning analytics has emerged as a promising framework to better support and understand students' learning processes (Schumacher & Ifenthaler, 2018)."}, {"title": "Human-Centred Design", "content": "A key aspect of this evolution is the recognition that effective learning analytics must employ interdisciplinary methodologies, incorporating insights from human-computer interaction (HCI), educational design, and software engineering (Martinez-Maldonado et al., 2016; Dollinger & Lodge, 2018; Revano & Garcia, 2021; Tsai & Martinez-Maldonado, 2022). Designing analytics with users, rather than for them, presents challenges related to the specialised skills that users may lack. Therefore, a collaborative approach is essential, ensuring that designers work closely with students and educators to develop tools that genuinely enhance learning experiences."}, {"title": "Frameworks and Models", "content": "Recent studies examining student perspectives have highlighted the significance of frameworks such as Learning Design Analytics Layers (AL4LD) (Hern\u00e1ndez-Leo, Martinez-Maldonado, Pardo, Mu\u00f1oz-Crist\u00f3bal, & Rodr\u00edguez-Triana, 2019) and the Learning Analytics Model (LAM) (De Freitas et al., 2015). These frameworks exemplify innovative approaches that inform learning design decisions, contributing to the emerging field of human-centred learning analytics (HCLA) that prioritises student needs.\nThe adoption of human-centred design principles (HCD) is crucial for engaging students effectively. However, implementing these principles presents challenges in creating actionable learning analytics solutions that balance stakeholder agency with the often technocratic tendencies of data science (Martinez-Maldonado, 2023). By prioritising student engagement, educators can develop more responsive analytics that enhance the learning experience."}, {"title": "Stakeholder Engagement", "content": "Engaging stakeholders, including students who may be generalists but adept in analytics, is vital (Martinez-Maldonado, 2023). Although their insights into learning processes are valuable, many studies indicate a lack of conceptual understanding of analytics among these individuals (Mcpherson et al., 2016; Gasevic, Tsai,"}, {"title": "Evidence-Based Practices", "content": "The application of evidence-based practices is increasingly relevant in educational contexts. Learning analytics holds the potential to generate actionable insights that inform teaching practices and enhance student success (Kuromiya, Majumdar, & Ogata, 2020). Recent systematic reviews highlight learning analytics' capacity to improve feedback mechanisms in higher education, bridging the gap between data collection and pedagogical application (Ifenthaler & Yau, 2020; Sahin & Ifenthaler, 2021; Banihashem, Noroozi, van Ginkel, Macfadyen, & Biemans, 2022)."}, {"title": "Challenges and Ethical Considerations", "content": "Despite the acknowledged potential of learning analytics to transform higher education, several challenges remain, including resource allocation, stakeholder participation, and ethical considerations (Tsai, Poquet, Ga\u0161evi\u0107, Dawson, & Pardo, 2019; West et al., 2020; Joseph-Richard & Uhomoibhi, 2021). Addressing these issues is critical for institutions seeking to fully realise the benefits of learning analytics, such as enhanced teaching and improved administrative efficiencies.\nAs the field evolves, the successful implementation of learning analytics increasingly relies on agile leadership capable of navigating environmental pressures and managing conflicts (Tsai et al., 2019). Furthermore, understanding students' perceptions of data privacy and the ethical implications of analytics practices is essential for fostering trust and ensuring respect for personal and professional boundaries (West et al., 2020). However, an overemphasis on data-driven processes often dominated by data scientists and technologists can obscure the nuanced social dimensions of educational contexts. A socio-technical approach is essential for developing and implementing effective learning analytics solutions, considering the interplay of human factors to achieve holistic educational outcomes."}, {"title": "The Role of Artificial Intelligence", "content": "Recent studies have begun to explore the integration of artificial intelligence (AI) within student-centred design frameworks, highlighting its potential to transform educational settings (Alfredo, Echeverria, Jin, Yan, et al., 2024; Topali et al., 2024). AI technologies, including machine learning and natural language processing, are increasingly recognised as tools for analysing student-generated content and enhancing learning experiences (Samadi et al., 2024).\nThe literature underscores the growing significance of AI in education, particularly in learning analytics, feedback systems, and engagement analysis. AI-driven tools have demonstrated the ability to improve collaborative learning assessments by providing real-time insights into group dynamics and individual contributions (Pradhan, Nimkar, & Jhajharia, 2024). Additionally, automating feedback processes through AI can yield timely and personalised responses, which are crucial for student engagement and motivation. Addressing algorithmic biases in AI systems is vital for promoting educational equity, ensuring that all students benefit from these technologies (Asatryan et al., 2024).\nThe integration of Large Language Models (LLMs) is also being explored for their capacity to personalise learning experiences and refine assessment methodologies. These models can adapt to individual learning styles and preferences, thus enhancing the overall educational experience (Rashid, Gehringer, & Khosravi,"}, {"title": "Limitations and Gaps in the Literature", "content": "Despite the advancements in learning analytics literature, significant limitations and gaps warrant further exploration. One major limitation is the technocentric design of many learning analytics tools, which often neglects the diverse needs and perspectives of students and educators. This oversight can hinder their effectiveness in real-world educational contexts. Furthermore, while many studies emphasise personalisation and real-time feedback, there is a lack of inclusive design that integrates these elements into a cohesive approach. Ethical concerns regarding data privacy, informed consent, and transparency of analytics practices are also inadequately addressed, raising questions about the trustworthiness of these systems.\nMoreover, the literature often focuses on descriptive studies rather than experimental or longitudinal research, limiting the understanding of the long-term impact of learning analytics on student outcomes. There is a pressing need for empirical investigations into the effectiveness of specific learning analytics interventions, particularly in diverse disciplinary contexts, to better inform the design and implementation of these systems. The gap in participation among stakeholders who are generalists or new to learning analytics highlights the importance of developing data and analytics-aware skills among students and staff, as their contributions are crucial for the successful implementation of learning analytics practices.\nTo address these gaps in engagement, this study focuses on integrating data science students through a participatory co-design project-based approach. By involving students with data science expertise and transferable skills, the study aims to foster deeper collaboration between students and educators. This approach enhances the educational experience and empowers students to actively contribute to the analytics process, ensuring that the developed tools are user-friendly and grounded in a solid understanding of both data and the learning environment. As institutions prepare to embrace this new paradigm, the involvement of students with dual competencies will be vital in designing effective, human-centred learning analytics solutions."}, {"title": "Method", "content": "This study employs a qualitative approach that integrates human-led content analysis with retrieval-augmented generation (RAG) and large language models (LLMs). Qualitative Content Analysis (QCA) is a method for drawing inferences from texts and other meaningful materials, allowing researchers to interpret their findings through a structured process (Bengtsson, 2016; Krippendorff, 2022; Marvasti, 2019; Silverman, 2024). To enrich our analysis further, we incorporate a quantitative perspective, enabling the systematic identification of emerging themes."}, {"title": "Study Design", "content": "This study utilises content analysis to examine the themes present in the objectives, interpretations, tools, and techniques found in student reports. We validate our findings through replicated and diverse analyses. The combination of retrieval-augmented generation (RAG) and large language models (LLMs) aims to enhance the precision and relevance of the generated content (Dai, Xiong, & Ku, 2023; Arslan, Ghanem, Munawar, & Cruz, 2024), facilitating a deeper understanding of the data itself. Ultimately, our goal is to provide nuanced insights into the students' learning experiences, illuminating the complexities of their educational journeys."}, {"title": "Findings Based on Human Analysis", "content": "In this study, we aimed to address gaps concerning student engagement in the design of learning analytics services. Recent studies characterised by human-centred learning analytics design have predominantly"}, {"title": "Student Conceptual Understanding", "content": "Our first research question was: How does students' work reflect their understanding and application of relevant learning analytics? By adopting a structured approach to data mining, the questions and conclusions formulated by the groups indicated a profound understanding of the aims of learning analytics and the data derivable from available datasets concerning student engagement and achievement. However, our analysis revealed that while most groups adhered to the Cross-Industry Standard Process for Data Mining (CRISP-DM) framework-which consists of the phases: business understanding, data understanding, data preparation, modelling, evaluation, and deployment\u2014their work lacked grounding in the existing learning analytics literature. This absence could have provided them with crucial direction to formulate their questions and interpretations, thereby enriching their understanding.\nImplementing the CRISP-DM framework, the groups' data preparation involved the utilisation of the dataset by selecting relevant data for analysis and formatting it appropriately. Key steps included:\n\u2022 Outlier Detection: Identifying and addressing outliers to ensure data integrity, such as excluding outliers like the course or module home page from calculations of engagement or extreme grades.\n\u2022 Filtering Confounding Variables: Removing variables that could distort the analysis.\n\u2022 Reducing Duplicates: Ensuring data uniqueness to enhance accuracy.\n\u2022 Addressing Missing Values: Employing methods such as imputation to fill gaps in the dataset.\nAdditionally, categorical variables were encoded using label encoding, and perfect collinearity was eliminated to refine the dataset further. Statistical calculations were performed on assessment scores and virtual learning environment (VLE) interactions, including:"}, {"title": "Student Collaborative Design and Development", "content": "This section explores the collaborative design and development processes of data science students in the context of learning analytics, guided by the research question: What specific questions and variables do data science students articulate in their learning analytics co-design and development process?\nThis inquiry consists of two primary strands: the students' co-design approach and the specific questions and variables they formulate, reflecting their interests and perspectives on useful analytics.\nThe innovative course design, alongside a group innovation project, encourages students to engage in co-designing and developing their analytical perspectives. While this structure is intended to promote a collective viewpoint, the reality of students working independently outside the classroom raises questions about the depth of their collaboration and individual contributions. Measures have been implemented to mitigate collaboration bias, including direct supervision by administrators and the requirement for individual reflective reports. However, examining the effectiveness of these measures fell outside the scope of this study.\nTo investigate the second strand, a thematic analysis was conducted on the questions formulated by the students. This analysis revealed that the insights generated by data science students effectively addressed pressing questions pertinent to their areas of interest. However, the questions tended to be limited in scope, lacking overarching targets and depth that could yield more insightful findings. Even though the groups examined the same dataset, their conclusions varied, exposing both similarities and contradictions in their analyses.\nThe inquiries posed by the students illustrate their interest in exploring multiple dimensions of performance, including assessments, engagement, and personal factors such as disabilities. This holistic approach allows for a comprehensive understanding of how various elements interact in the educational environment. The language used in formulating these questions reflects the students' data science training, with a strong emphasis on measurable outcomes that can be investigated through statistical methods. Their inquiries incorporate specific terminology and concepts, including relationships, impact, effect, and prediction, thereby establishing a more technical framework for analysing educational data, distinguishing this approach from more traditional studies.\nTo guide their research objectives, some groups formulated overarching questions, such as: \"Do students have a higher chance of success the more they interact with Newcastle University's educational technology?\"\nWhile most groups generated fragmented questions, the primary questions often led to several sub-questions aimed at achieving their research objectives. Some analyses prompted the formulation of additional questions, indicating that the process was exploratory and illuminated areas of uncertainty and complexity that merit ongoing investigation."}, {"title": "Student Interpretations and Insights", "content": "While similarities existed in the questions formulated by students, their interpretations diverged significantly. The reflections in their reports demonstrated a keen enthusiasm for exploring the data and articulating insights. Each group employed structured approaches to interpret the data and analytics, selecting variables and providing reasoned justifications for their choices. Overall, the analyses were specific to the dataset, resulting in varied interpretations among groups.\nDepending on the selected subset of variables and courses, students' interpretations reflected a diverse range of possibilities. For instance, some groups concluded that certain variables were correlated, while others contested these findings. Furthermore, some groups conducted additional analyses, discovering correlations within specific subsets of data that did not exist in others. This indicates that a more detailed examination could yield nuanced insights.\nMost groups inferred that the data alone cannot provide definitive correlations or causal relationships due to the complexities of students' contexts. They highlighted the existence of external variables not included in the dataset, which could confound their interpretations and enhance understanding. Most conclusions were contextual and speculative, indicating a level of uncertainty. Some groups displayed greater confidence in the relationships and influences between variables based on statistical significance than others.\nSeveral groups identified insights beyond the established objectives. For instance, one group discovered that longer courses were associated with higher dropout rates and that submission timing influenced final scores. This finding underscores the students' critical thinking abilities and the dynamic, contextual nature"}, {"title": "Insights from the QCALA Analysis", "content": "The QCALA uncovered diverse insights into students' perspectives on effective learning analytics. By employing a dual-channel analysis of human and model approaches, we identified key commonalities and unique insights, which are presented below:"}, {"title": "Common Findings", "content": "\u2022 Both human and model analysis approaches demonstrate that the groups focus on examining student behaviours and interactions within educational contexts. They leverage learning analytics to uncover patterns and trends.\n\u2022 There is an emphasis on exploring the relationship between engagement metrics and academic outcomes, aiming to identify key performance drivers and address challenges in connecting behaviours to measurable results.\n\u2022 Both approaches employed robust data analysis techniques such as Exploratory Data Analysis (EDA) within frameworks like CRISP-DM and utilized tools like Python to deliver actionable insights."}, {"title": "Insights from Human Analysis", "content": "\u2022 Human analysis reveals that the groups widely utilise the CRISP-DM framework but do not delve into its detailed implementation methods.\n\u2022 A positive correlation between engagement with Virtual Learning Environments (VLEs) and improved performance was highlighted, noting that increased clicks are linked to better academic outcomes.\n\u2022 Patterns such as the connection between early assignment submissions and higher scores were identified, though finer distinctions-like the impact of submission timing on different types of assessments were not explored."}, {"title": "Insights from RAG LLM Model Analysis", "content": "\u2022 The findings from model analysis build upon the results of human analysis by offering more detailed and nuanced insights.\n\u2022 Model analysis validates the application of the CRISP-DM framework, exploring its components in greater depth, including data understanding, preparation, modelling, evaluation, and deployment.\n\u2022 In-depth exploration of engagement metrics revealed specific correlations, such as the relationship between the number of clicks and outcomes like pass rates, distinctions, and withdrawals across various modules.\n\u2022 The timing of submissions was examined, showing that students who submit assessments within the first 250 days of a course generally achieve better results than those submitting later.\n\u2022 Additionally, the analysis indicates differences in performance between CMAs (Computer Marked Assignments), TMAs (Tutor Marked Assignments), and final exams, with TMAs often proving to be the most reliable predictor of success."}, {"title": "Conclusion of Insights", "content": "While the findings from human analysis provide a strong foundational understanding, they sometimes lack the detail present in model analysis. Conversely, model analysis offers richer context but may lack the succinctness of human insights. Together, these findings create a comprehensive view of student engagement and learning analytics, highlighting the importance of integrating both approaches for a more holistic understanding."}, {"title": "Limitations", "content": "This study is subject to several limitations that should be acknowledged:\n\u2022 Timescale of the Innovation Project: The research was conducted within the constrained timeframe of Semester 2 of the Academic Year 2022, which may have impacted the depth and breadth of the findings.\n\u2022 Semi-Controlled Environment: The study was conducted in a semi-controlled setting, characterised by specific time constraints, predetermined problem statements, and defined objectives, along with reliance on an open-source dataset and available resources. This context may limit the generalisability of the results.\n\u2022 Focus on Outcomes: The emphasis on measurable outcomes has led to a deliberate exclusion of contextual factors and social aspects that could provide a more nuanced understanding of the learning environment and its dynamics."}, {"title": "Conclusion", "content": "This study aims to illuminate the connections between students' practical experiences in data science and their conceptual understanding of learning analytics, ultimately contributing to the broader discourse on meaningful learning in education. The findings indicate that students with specialised skills are capable of providing informed insights into service design. By adopting a structured approach, critically selecting variables, and employing effective analysis and visualisation tools, students demonstrate their ability to generate valuable insights throughout the service design and development process. Furthermore, the groups' methods for formulating questions and deriving meaning from the data suggest that analytics involve an exploratory and inductive approach to understanding students' experiences.\nThe methods employed in this study included a combination of qualitative and quantitative analyses, utilising both human analysis and RAG LLM model analysis. The human analysis provided foundational insights through group discussions and data exploration, while the model analysis offered more nuanced, data-driven perspectives. This mixed-methods approach enabled a comprehensive understanding of student engagement and performance, integrating subjective interpretations with objective metrics. By leveraging these diverse methodologies, the study enriches the overall narrative of learning analytics.\nOverall, these results highlight the necessity for continuous dialogue around analytics to foster a shared understanding between students and university staff. Such conversations can explore tailored approaches that enhance the learning experience, ensuring that both students and educators benefit from a collaborative environment. By actively engaging in these discussions, institutions can better support student success and promote meaningful learning outcomes in the ever-evolving landscape of education."}]}