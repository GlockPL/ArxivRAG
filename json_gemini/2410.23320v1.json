{"title": "LINA-SPEECH: GATED LINEAR ATTENTION IS A FAST AND PARAMETER-EFFICIENT LEARNER FOR TEXT-TO-SPEECH SYNTHESIS", "authors": ["Th\u00e9odor Lemerle", "Nicolas Obin", "Axel Roebel", "Vaibhav Srivastav", "Harrison Vanderbyl"], "abstract": "Neural codec language models have achieved state-of-the-art performance in text-to-speech (TTS) synthesis, leveraging scalable architectures like autoregressive transformers and large-scale speech datasets. By framing voice cloning as a prompt continuation task, these models excel at cloning voices from short audio samples. However, this approach is limited in its ability to handle numerous or lengthy speech excerpts, since the concatenation of source and target speech must fall within the maximum context length which is determined during training.\nIn this work, we introduce LINA-SPEECH, a model that replaces traditional self-attention mechanisms with emerging recurrent architectures like Gated Linear Attention (GLA). Building on the success of initial-state tuning on RWKV, we extend this technique to voice cloning, enabling the use of multiple speech samples and full utilization of the context window in synthesis. This approach is fast, easy to deploy, and achieves performance comparable to fine-tuned baselines when the dataset size ranges from 3 to 15 minutes. Notably, LINA-SPEECH matches or outperforms state-of-the-art baseline models, including some with a parameter count up to four times higher or trained in an end-to-end style. We release\u00b9 our code and checkpoints. Audio samples are available at https://theodorblackbird.github.io/blog/demo_lina/.", "sections": [{"title": "1 INTRODUCTION", "content": "Scaling text-to-speech Betker (2023) (TTS) models and data has led to dramatic improvements with regard to quality, diversity, and cloning capabilities. Leveraging neural audio codec Zeghidour et al. (2021); D\u00e9fossez et al. (2023) and next-token prediction have shown state-of-the-art results in zero-shot voice cloning, extending in-context learning abilities observed primarily on natural language to the codec language. Under this setting, zero-shot voice cloning is formulated as a prompt continuation task and provides state-of-the-art results starting from 3s of prompt audio. In contrast with prior works, this approach put more pressure on the pre-training stage, where large-scale speech datasets are needed in order to get sufficient in-context learning abilities and less on domain knowledge. In this direction, the transformer has been the leading architecture for scalable auto-regressive speech models."}, {"title": "2 RELATED WORK", "content": "Large-Scale TTS State-of-the-art large-scale TTS models heavily rely on transformer architectures, both autoregressive (AR) Wang et al. (2023); Betker (2023); Lyth & King (2024) and non-autoregressive (NAR) Chang et al. (2022); Shen et al. (2024); Le et al. (2023). NAR transformers, particularly those based on diffusion or flow-matching, traditionally require either precomputed durations or an auxiliary generative model. While producing fine-grained duration annotations can be challenging for noisy, large-scale datasets, recent approaches have adopted coarser duration estimates, such as word- or sentence-level measurements Yang et al. (2024a). AR models, on the other hand, have shown strong performance when trained on in-the-wild data, eliminating the need for intermediate feature representations.\nAlthough NAR models often outperform AR models in terms of inference speed and robustness, they struggle with issues like over-smoothness Yang et al. (2024a); Ren et al. (2022), which leads to reduced diversity and less expressive prosody. Recent research seeks to blend NAR and AR techniques: Xin et al. (2024) introduces explicit duration modeling in an AR transformer to enhance robustness, while Yang et al. (2024b) explores AR generative models for prosody and duration modeling atop a NAR flow-matching acoustic model. While recent large-scale AR acoustic modeling relies heavily on neural audio codecs, AR and NAR methods Le et al. (2023); Betker (2023); Shen et al. (2024); Meng et al. (2024) have proven to be effective in both data-space (e.g., mel spectrogram) and latent-space modeling at scale.\nZero-shot TTS Zero-shot text-to-speech (TTS) refers to the task of synthesizing speech from unseen samples during inference. Traditional methods includes the use of a speaker encoder that generates embeddings for conditioning Wang et al. (2018). Multi-sample approaches such as Mega-TTS2 Jiang et al. (2024) have been introduced to bridge the gap with fine-tuning methods by better capturing prosodic elements that a single excerpt may not fully encapsulate. In contrast, large-scale TTS models leverage in-context learning capabilities, with techniques like prompt continuation Wang et al. (2023); Peng et al. (2024b) and infilling strategies Le et al. (2023) showing success using"}, {"title": "Parameter Efficient Fine-Tuning", "content": "Parameter Efficient Fine-Tuning (PEFT) Xu et al. (2023) focuses on identifying the optimal subset of parameters for fine-tuning a model, resulting in a compact variation of the original model. PEFT has gained popularity across large language models and other large-scale generative models, enabling adaptation on a single GPU in a fraction of the pretraining time. These methods include LoRA Hu et al. (2022); Dettmers et al. (2023) and variants. Alternative techniques involve tuning embeddings that are not parameters during training, such as prompt-tuning Lester et al. (2021); Liu et al. (2022). PEFT can outperform full fine-tuning in small dataset scenarios, where full fine-tuning may lead to catastrophic forgetting. Most of these techniques were initially applied to natural language processing or image generation. Qi et al. (2024) evaluates the use of LORA for domain adaptation on emotional TTS."}, {"title": "3 PRELIMINARIES", "content": "Given an input $X \\in \\mathbb{R}^{N \\times d}$ self-attention for auto-regressive modeling derives three linear projections: the query matrix $Q \\in \\mathbb{R}^{N \\times d_k}$, the key matrix $K\\in \\mathbb{R}^{N \\times d_k}$, the value matrix $V \\in \\mathbb{R}^{N \\times d_v}$, and a causal mask $M_{i,j} = 1_{i<j} M \\in \\mathbb{R}^{N \\times N}$. The parallel form of attention is defined as:\nAttention(Q, K, V) = Softmax$\\left(\\frac{QK^T}{\\sqrt{d}}M\\right)V$,\nwhere $\\odot$ denotes element-wise multiplication, and admits the sequential form,\nAttention(Q, K, V)_t =$\\frac{\\sum_{i=1}^t exp(q_t k_i) v_i}{\\sum_{i=1}^t exp(q_t k_i)}$,\nduring inference."}, {"title": "3.1 LINEAR ATTENTION", "content": "Katharopoulos et al. (2020) proposed to replace the softmax in self-attention with a general kernel function $k$ and its associated feature map $\\phi$. This approach known as linear attention can be expressed as:\nLinearAttention(Q, K, V)_t = $\\frac{\\sum_{i=1}^t \\phi(q_t) \\phi(k_i)^T V_i}{\\sum_{i=1}^t \\phi(q_t) \\phi(k_i)^T}$.\nDenoting,\n$S_t = \\sum_{i=1}^t \\phi(k_i) \\phi(k_i)^T V_i$, $Z_t = \\sum_{i=1}^t \\phi(k_i)^T$, $O_t = \\frac{\\phi(q_t) S_t}{\\phi(q_t) Z_t}$.\nThis can be expressed through the update rule:\n$S_t = S_{t-1} + \\phi(k_t) v_t$, $Z_t = Z_{t-1} + \\phi(k_t)^T$, $O_t = \\frac{\\phi(q_t) S_t}{\\phi(q_t) Z_t}$,\nwhich reveals that it essentially functions as a recurrent neural network with a matrix-valued state.\nThe choice of $\\phi$ being the linear kernel ($\\phi = Id$) has been a popular line of research Peng et al. (2024a); Yang et al. (2024c); Sun et al. (2023), furthermore it has been observed that in practice the normalization term can be omitted thus simplifying to:\n$S_t = S_{t-1} + k_t v_t$, $O_t = q_t S_t$,\nwhere $S_t$ acts as a constant size kv-cache in traditional self-attention transformer."}, {"title": "3.2 GATED LINEAR ATTENTION (GLA)", "content": "While linear attention provides a constant memory footprint and achieves linear time complexity during inference, its parallel form remains constrained by quadratic time complexity, and the recurrent form poses challenges for efficient training on modern hardware. Recent advances in linear-complexity language models\u2014such as RWKV-6 Peng et al. (2023; 2024a), GLA Yang et al. (2024c), and Mamba Gu & Dao (2024); Dao & Gu (2024) demonstrate that introducing data-dependent gating mechanism Sun et al. (2023); Peng et al. (2023) substantially closes the performance gap with self-attention transformers. Additionally, various techniques have been proposed to enhance hardware efficiency for linear-scaling language models, including the prefix-sum algorithm Gu & Dao (2024); Katsch (2023) and chunk-wise computation Yang et al. (2024c); Dao & Gu (2024); Sun et al. (2023).\nFor these reasons, Gated Linear Attention Yang et al. (2024c) (GLA) comes with a data-dependent structured gating mechanism, resulting in the following update rule:\n$S_t = G_t \\odot S_{t-1} + k_t v_t$, $O_t = q_t S_t,$\nwhere $G_t$ is a decay matrix that modulates the contribution of past states."}, {"title": "4 METHOD", "content": "LINA-SPEECH is an auto-regressive generative model $p_\\theta$ designed to approximate the distribution of neural audio codec token sequence, denoted as $c$, conditioned on text input $x$, with $\\theta$ representing the model parameters, that is:\n$P_\\theta(c|x) = \\prod_{t=1}^T P_\\theta(c_t|c_{<t}, x).$"}, {"title": "4.1 MODEL ARCHITECTURE AND INFERENCE", "content": "Model architecture The text encoder is a non-causal transformer encoder with self-attention as time-mixing operator and SwiGLU Shazeer (2020) as a feed-forward network. It employs ROPE positional encoding Su et al. (2024). The acoustic model includes both an audio encoder and a decoder, featuring a causal transformer architecture with GLA as a time-mixing operator, SwiGLU Shazeer (2020) as a feed-forward network, and no positional encoding.\nThe decoder takes input from the audio encoder and a cross-attention layer between the text and audio encoder outputs. To improve robustness, we used the position-aware cross-attention from Lemerle et al. (2024), and replaced sinusoidal positional encoding with convolutional positional encoding for enhanced training stability."}, {"title": "4.2 INITIAL-STATE TUNING", "content": "We have seen that Gated Linear Attention achieves linear complexity by replacing the expanding kv-cache of traditional transformers with a constant-sized memory represented by the matrix-valued state $S_t$ in Equation (6). During training all layer states are initialized to zeros, that is $S_0 = 0$.\nRecent work stemming from the RWKV community Peng et al. (2023; 2024a); Fish (2024) has demonstrated that this type of memory can be subject to PEFT for domain adaptation or instruction tuning of large language models. Since the state encodes past information without expanding on the time axis, it offers a compact alternative to prompt-tuning. To the best of our knowledge, neither initial-state tuning nor prompt-tuning has been applied to speech synthesis.\nWe found out that:\n\u2022 This approach is robust to the choice of hyper-parameters across datasets. In practice, we use the same learning rate X = 0.1 and two passes over the dataset with a batch size of 8 utterances for all examples and a maximum of 40 step.\n\u2022 When restricting to 3-15 minutes of speech, the state matrix can be parameterized as a rank-1 matrix (that is $S_0 = k_0 v_0$), reducing the parameters set to a pair of vectors per head and per layer, without significant performance degradation, (see Figure 2).\n\u2022 The tuning is fast, lasting less than 20 seconds on average on a RTX3080."}, {"title": "5 EXPERIMENTS", "content": "5.1 EXPERIMENTAL SETUP\nDatasets We trained LINA-SPEECH on a publicly available English subset of MLS2 Lacombe et al. (2024) which consists of 10k hours of librivox recordings. We do not use the provided transcription and rather use the Automatic Speech Recognition (ASR) model NeMo\u00b3. We also added both LibriTTS Zen et al. (2019) and its restored version LibriTTS-R Koizumi et al. (2023) with their normalized transcripts. We used WavTokenizer Ji et al. (2024)4 as a neural audio codec that encodes speech at a rate of 75 token/s, with a codebook size of 4096 Koizumi et al. (2023). For text input, we learn a byte-pair encoding tokenizer with a vocabulary size of 256 trained on the lower-cased transcripts from LibriTTS. For initial-state tuning, we used the Expresso dataset Nguyen et al. (2023) as an out-of-domain resource for adaptation. This dataset consists of studio-quality samples, each labeled with specific emotions or styles.\nModel Configuration We provide detailed model configuration and hyper-parameters in Appendix A.\nTraining and Inference The main model is trained for next-token prediction with cross-entropy loss for 500k steps with a batch size of approximately 100k tokens (\u2248 22min of speech). We use AdamW optimizer with a learning rate of 2 \u00d7 10-4 10-4, a cosine learning rate schedule with linear warmup for the first 1k steps, a weight decay of 0.1 and gradient clipping of 1.. We group samples of similar lengths within 10 buckets in order to avoid padding. The training takes about 4 days on two RTX4090. We rely on the official hardware efficient implementation of GLA Yang & Zhang (2024)."}, {"title": "5.2 OBJECTIVE METRICS", "content": "We measure word error rate (WER) and character error rate (CER) using the same ASR model from NeMo as for speech transcription. We also measured speaker similarity as the cosine similarity of WavLM Chen et al. (2022) embedding of target and synthesized speech using a pretrained checkpoint 5."}, {"title": "5.3 SUBJECTIVE METRICS", "content": "We conducted subjective experiment using Mean Opinion Score (MOS) to measure the naturalness and similarity to the target speaker via the platform Prolific. Details of the subjective experiment are described in Appendix B."}, {"title": "5.4 BASELINES", "content": "The baselines include:\n\u2022 The TTS enhanced version of VoiceCraft Peng et al. (2024b), a decoder-only transformer trained on GigaSpeech and librilight. They provide an EnCodec model specifically trained for speech.\n\u2022 StyleTTS2 Li et al. (2024) an end-to-end TTS model that leverages latent diffusion for style modeling.\n\u2022 A popular unofficial reproduction Plachtaa of VALLE-X Zhang et al. (2023) that leverages an official EnCodec model and VocosSiuzdak (2023).\n\u2022 Parler-TTS Lacombe et al. (2024), is a series of reproduction of Lyth & King (2024) that allows synthesis controlled by textual description of the voice. Interestingly, this reproduction differs from the original paper by separating text and audio sequence and employing cross-attention between the two modalities instead of self-attention on the concatenation of both, making the architecture closer to LINA-SPEECH. They leverage DAC Kumar et al. (2024) as audio codec. We use two different checkpoints for LibriTTS6 and Expresso 7 evaluations."}, {"title": "5.5 EXPERIMENTS", "content": "\u2022 Experiment #1: Zero-shot voice cloning We evaluate zero-shot voice cloning against the baselines with the exception of Parler-TTS on the clean test split of LibriTTS.\n\u2022 Experiment #2: Initial-state Tuning Since Parler-TTS voice adaptation is limited to a subset of speakers from the training set, we evaluate our model using one initial-state tuned per speaker. For LibriTTS it consists of a list of names that we give to Parler as a textual"}, {"title": "5.6 RESULTS AND DISCUSSION", "content": "5.6.1 EXPERIMENT #1: ZERO-SHOT VOICE CLONING\nTable 1 presents the results of the objective and subjective evaluation conducted on the task of zero-shot voice cloning."}, {"title": "5.6.2 EXPERIMENT #2: INITIAL-STATE TUNING", "content": "Table 2 presents the results of the objective and subjective evaluation specifically designed for the comparison of the initial-state tuning in LINA-SPEECH against Parler-TTS. The evaluation was conducted for both in and out of domain tasks: LibriTTS was used for the in-domain evaluation, and Expresso was used for the out-of-domain evaluation.\nObjective evaluation LINA-SPEECH presents systematically the best performance for all metrics as compared to Parler-TTS, both for in- and out- of domain datases excepted for SECS on Expresso. We observe lower WER and CER than the ground truth that we attribute to the synthetic transcripts for evaluation that do not reflect the training distribution.\nSubjective evaluation For the in-domain task, LINA-SPEECH presents the highest score on LibriTTS both in terms of naturalness and similarity. On Expresso LINA-SPEECH presents a comparable N-MOS to Parler. Secondly, LINA-SPEECH presents a significantly higher similarity to the speaker as compared to Parler-TTS. On Expresso we observed that Parler showed catastrophic failures where the conditioning is partially or totally ignored."}, {"title": "6 LIMITATIONS AND FUTURE WORK", "content": "Audio codec We observed that, while WavTokenizer Ji et al. (2024) does not generalize as effectively across different voices, languages, and recording conditions compared to EnCodec D\u00e9fossez et al. (2023) and DAC Kumar et al. (2024), its semantically richer latent space may offer significant improvements on the overall quality for generation. This could help explain why LINA-SPEECH performs better than end-to-end or larger models, especially on LibriTTS. Further experiments are needed to better understand the impact of neural audio codecs on speech modeling.\nInitial-state Tuning In this work, we focused on small-to-medium single-speaker domain adaptation (e.g., 3-15 minutes) and observed that using minimal datasets (e.g., 1\u201320 excerpts) could result in quality degradation compared to prompt continuation from a single excerpt, suggesting that such cases may need specialized treatment. Investigating the scalability of this method on larger datasets compared to fine-tuning is left for future work. We sometimes observe misreading at the onset of the synthesis, which can be absorbed by providing a short audio prompt.\nStreamable Although LINA-SPEECH demonstrates nearly linear time complexity within its context window, additional work is needed in order to make it seamlessly streamable. To achieve this, we plan to explore chunk-based text encoding and windowed cross-attention to enable fully linear, streamable synthesis."}, {"title": "7 CONCLUSION", "content": "In this paper, we introduced LINA-SPEECH, a parameter-efficient model for text-to-speech synthesis. Our architecture combines Gated Linear Attention with initial-state tuning, demonstrating capabilities that rival or surpass strong neural codec language model baselines. We evaluated LINA-SPEECH against other text-to-speech synthesis models through both objective and subjective metrics on in-domain and out-of-domain speech corpora. These experiments reveal that LINA-SPEECH is 1) an efficient zero-shot learner for voice cloning via prompt continuation, and 2) that initial-state tuning serves as an effective PEFT approach for conditioning on limited data. Our comparisons show that LINA-SPEECH is highly competitive against much larger models, even those specifically trained or fine-tuned on dedicated datasets."}, {"title": "A MODEL ARCHITECTURE", "content": "Lina-Speech is a 169M parameters encoder-decoder architecture with text-conditioning through cross-attention.\nText Encoder Our text encoder consists of a stack 6 non-causal transformer block with SwiGLU feed-forward network Shazeer (2020), base dimension 1024. We use dropout with rate 0.1 on the outputs of each block and RoPE positional encoding.\nAudio Encoder and Decoder It consists for each of a stack of 6 causal GLA transformer with base dimension 1024. The expansion factor of key projection is set to 0.5. We do not use dropout.\nCross-Attention We use PACA cross-attention Lemerle et al. (2024) with convolutional embedding. We do not use RoPE."}, {"title": "B SUBJECTIVE EVALUATION", "content": "Subjective evaluation was also conducted in order to compare the benchmark of algorithms by using human perception. In those evaluations, the human subjects were asked to rate the naturalness of speech samples (N-MOS) and their similarity to a reference speaker (S-MOS) using Mean Opinion Score (MOS) ITU-T P.800.2 (2013) as commonly used in the literature for evaluation text-to-speech synthesis systems. In the following, we provide all details of the experimental protocol following observations reported in Kirkland et al. (2023) and recommendations presented in Cheng-Han et al. (2023). This is achieved in order to improve the transparency and reproducibility of the proposed protocol.\n\u0392.1 METHODOLOGY\nEach listener was assigned a single experiment across the 3 we introduced. It consists of 12 evaluation and measured for a median time of experiment of 7 min.\nB.1.1 CREATION AND PRESENTATION OF THE SPEECH STIMULI\nLibriTTS clean - Zero shot We group pair of sentences from a same speaker in the test split. We first draw a prompt candidate randomly between 2 and 5 seconds, then we draw a sample so that the concatenation of both samples remains within 16s in order to account of the context window of the baselines, we discard speakers that do not contains at least 2 samples that satisfy this condition. We draw 40 pairs in this manner.\nInitial State Tuning In order to adapt our evaluation to Parler that does not provide test set, we synthesized random sentences with Llama3.1 8B with the following prompt.\n\" Please generate a diverse set of 100 random sentences designed for evaluating text-to-speech synthesis quality. Ensure that:\n\u2022 Variety in sentence length: Sentences should be between 12 and 20 words long.\n\u2022 Phonetic coverage: Include a wide range of phonemes, syllable structures, and sounds.\n\u2022 Incorporate both simple and complex sentence structures.\n\u2022 Diverse syntax and styles: Use varied syntactic forms, rhythms, and styles to capture different speech patterns and intonations.\n\u2022 Natural and conversational tone: The sentences should sound natural, like everyday speech, while still offering variability.\n\u2022 Descriptive and vivid: Include a mix of action-oriented, descriptive, and emotional content to test prosody and emotional intonation.\n\u2022 Non-repetitive: Ensure that all sentences are distinct from each other, with no repetition in structure or wording.\n\"\nThe sentences should be suitable for use in evaluating prosody, naturalness, and phonetic diversity in a text-to-speech system. In the Expresso dataset, we omitted all excerpts where transcripts contained non-verbal instruction such as \"laugh\", \"breathe\".\nB.1.2 INTERFACE FOR THE EXPERIMENT\nThe experiment was conducted online and implemented in Python. The experiment was preceded by general recommendation to the subject before starting the evaluation:\n\u2022 Please use headphones or earphones in a quiet environment.\n\u2022 Adjust the sound level so you can hear subtle sound differences.\nEach experimental run consisted in the evaluation by the subject of 15 speech samples. The order of presentation of the speech stimuli was randomised before each experiment in order to avoid any"}, {"title": "B.1.3 INSTRUCTIONS GIVEN TO THE SUBJECTS", "content": "In each experimental run, the subjects were asked to evaluate the naturalness of the speech sample and its similarity to a speech sample of a reference speaker. The speech sample of the reference speaker consists of a real speech sample of the reference speaker which was selected systematically different from the speech sample under evaluation. By doing so, we were intending to prevent the subject from confusing the general perception of a speaker identity with one single realisation on a particular utterance and thus trying to mitigate the linguistic biased in the perception of speaker similarity.\nThe following instructions were given to the subjects:\n\u2022 In this experiment, you have to judge a speech sample with respect to a reference speech sample.\n\u2022 The reference and the sample to judge does not pronounce the same utterance.\nFor each speech sample, please rate :\n\u2022 The speech NATURALNESS: to which extent you judge the speech sample as natural as real human speech?\n\u2022 The SIMILARITY to the reference speaker: to which extent the speech sample is judged close to the reference speaker?\nComplementary recommendations were provided either to precise the definition or the task to be achieved:\n1. The aspects of speech naturalness include: fluency and appropriateness of pronunciation and prosody, and diversity in the expression of styles and emotions.\n2. The samples may have different recording conditions or background noise. As the scope of this experiment is only focused on speech naturalness, please try to ignore them during your evaluation."}, {"title": "B.1.4 PSYCHOMETRIC MEASUREMENTS AND ASSESSMENT METHODOLOGY", "content": "The Mean Opinion Score (MOS) ITU-T P.800.2 (2013) was used to measure naturalness and similarity, using a Likert scale ranging from 1 to 5. For both criteria, we used the original scale, as suggested in Kirkland et al. (2023):\n1(Bad), 2(Poor), 3(Fair), 2(Good), 5(Excellent)\nFollowing the MUSHRA methodology ITU-R BS.1534-3 (2015), we additionally incorporated hidden references of real speech samples for each evaluation run."}, {"title": "B.2 SUBJECTS RECRUITMENT", "content": "B.2.1 RECRUITMENT PLATFORM\nProlific crowd-sourcing platform was used for the experiment: https://www.prolific.com/."}, {"title": "B.2.2 LANGUAGE BACKGROUND AND GEOGRAPHIC LOCATION OF THE EVALUATORS", "content": "For this evaluation, we used the following filters available in this plateform to recruit subjects:\n\u2022 Location: USA or UK\n\u2022 Language: First language and Primary language and Fluent languages = English\nThe combination of these filters ended up into 91k subjects potentially skilled for the evaluation."}, {"title": "B.2.3 SUBJECTS QUALIFICATION", "content": "We applied several filters to assess the qualification of the subjects, in order to reject those who do not fulfill the necessary conditions to be considered qualified for the evaluation.\nThe list of conditions is listed as follows:\n\u2022 non-native English speaker\n\u2022 rate below 3 any real speech sample on the N-MOS\n\u2022 time spent to complete the experiment is below 3m 30s\n\u2022 the mean MOS of the subject deviates from the overall mean of all subjects by more than two standard deviations, as proposed by Kim et al. (2024)\nThe subject was considered not qualified if at least one of the conditions was not fulfilled.\n165 subjects participated in the experiment. Applying these filters, the qualification rate of the subjects was about 76%. We rejected 39 subjects from a total of 165, so the total of qualified subjects was 126 whose ratings were further used for analysis."}, {"title": "B.2.4 PARLER-TTS PROMPTING", "content": "For synthesizing speech from Parler checkpoints, we use the following textual description as described in the official documentation for optimal sample quality:\nLibriTTS \"{speaker}'s voice, the recording is of very high quality, with the speaker's voice sounding clear and very close up.\"\nExpresso \"{speaker} speaks in a {style} tone with high quality audio.\""}]}