{"title": "A comprehensive GeoAI review: Progress, Challenges and Outlooks", "authors": ["Anasse Boutayeb", "Iyad Lahsen-cherif", "Ahmed El Khadimi"], "abstract": "In recent years, Geospatial Artificial Intelligence (GeoAI) has gained traction in the most relevant research works and industrial applications, while also becoming involved in various fields of use. This paper offers a comprehensive review of GeoAI as a synergistic concept applying Artificial Intelligence (AI) methods and models to geospatial data. A preliminary study is carried out, identifying the methodology of the work, the research motivations, the issues and the directions to be tracked, followed by exploring how GeoAI can be used in various interesting fields of application, such as precision agriculture, environmental monitoring, disaster management and urban planning. Next, a statistical and semantic analysis is carried out, followed by a clear and precise presentation of the challenges facing GeoAI. Then, a concrete exploration of the future prospects is provided, based on several informations gathered during the census. To sum up, this paper provides a complete overview of the correlation between AI and the geospatial domain, while mentioning the researches conducted in this context, and emphasizing the close relationship linking GeoAI with other advanced concepts such as geographic information systems (GIS) and large-scale geospatial data, known as big geodata. This will enable researchers and scientific community to assess the state of progress in this promising field, and will help other interested parties to gain a better understanding of the issues involved.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence (AI) has become one of the most influential technological advances in human history [1], involving radical _metamorphoses in numerous active sectors, such as healthcare, finance, commerce, transport, etc. AI relies on several algorithms and methods to emulate human cognition, enabling data analysis in any quantity or complexity, achieving advanced automation of the most complicated processes, and helping to make the most crucial decisions. It can also perform various tasks with unprecedented precision. For example, AI facilitates the exploitation of medical images while enabling the automatic and rapid detection of tumours; it also facilitates the analysis of the most complex genetic data [2]. On the other hand, AI algorithms optimise production and exploitation of renewable energies, predict consumer demand and the optimum location for a large surface area [3]. In addition, AI can make an effective contribution to improving teaching methods, by adapting the pace and content of courses to students' levels, while analysing consumer purchasing behaviour [4], improving sales techniques and boosting yields [5]. In short, AI is currently a promising vector for efficiency and development, guaranteeing exceptional performance.\nFurthermore, geospatial information has a particular impact on people's daily lives, making it easy to navigate in urban and rural areas, locating schools, parking lots, stores, hospitals and medical centers with great precision, and finding directions to these places very quickly. In addition, geospatial technologies are at the heart of cutting-edge fields such as agriculture [6, 7], environment [8, 9], defense [10], urban planning [11, 12, 13] and healthcare [14, 15], while offering a new dimension of analysis and reflection, by combining information from various acquisition systems, specifically satellites, drones, GPS receivers, etc. This enables a high-level community of researchers and industrialists to address today's global challenges in every field, to participate in the era of digital transformation, and to ensure sustainability and global development.\nMore specifically, AI is a key tool allowing the management and exploitation of geospatial data, paving the way for a new era of analysis and interpretation, particularly when it comes to massive and complex data, or when it comes to sophisticated tasks. As a result, AI can perform these tasks while guaranteeing high accuracy, including the processing of remote sensing images to produce land use maps [16], the segmentation of agricultural crops [17], the detection of cars on road networks [18], the analysis of changes in spatio-temporal time series [19], and the use of precipitation to make weather forecasts [20]. Additionally, powerful AI models are designed to exploit temperature and vegetation informations in order to quantify the degree of drought over a vast geographical area, massive trajectory data can also be exploited to model high-performance route calculation algorithms. AI can also be used to predict the weather of floods by analysing remote sensing images and rainfall level measurements [21]. It should be noted that this multitude of applications of artificial intelligence in relation to geospatiality has given rise to a new concept, namely GeoAI, which is carefully explored in this paper, indicating the issues involved, the level of progress achieved and futurs trends. The remainder of this paper is organized as follows: section 2 presents the issues at stake in the subject, the Research Questions (RQs) to be answered and the research directions that have followed, section 3 introduces the theme of artificial intelligence, covering the main concepts and models, as well as new emerging approaches, section 4 develops the research directions defined above, leaving the final section to provide exhaustive answers to the research questions posed, and to introduce the related challenges and the future prospects, presented as results of the exploration carried out."}, {"title": "2 Issues, key questions and research directions", "content": "2.1 History and definitions\nLooking at history of the synergy between AI and geospatial area, it is noted that this coupling began as early as the 80s and 90s, through the works of Smith [22], Estes et al. [23], Couclelis [24], Openshaw and Openshaw [25]. A major milestone in the history of GeoAI is the \"Association for Computing Machinery (ACM SIGSPATIAL conference\" in 2017, where a specific workshop is dedicated to the subject, defining GeoAI explicitly as the coupling of Artificial Intelligence techniques to geographic information or geospatial data [26, 27]. Moreover, several definitions of GeoAI have been provided. According to Gao [28], GeoAI can be regarded as:\na branch of AI whose objective is to implement intelligent computer programs aiming to imitate human perception, particularly with regard to spatial reasoning and geographical dynamics.\nLi and Hsu [29] also define GeoAI as :\na new field of interdisciplinary research that uses geospatial Big Data while leveraging and developing AI for location-based analysis.\nFurthermore, emphasizing the applicative aspect of the geospatial field, Kausica et al. [30] assert that :\nGeoAI combines AI algorithms with geospatial methods to extract the most significant information, with the purpose of performing sophisticated tasks.\n2.2 Related topics, problematic and census approach\nAs presented above, GeoAI highlights a fundamental aspect that will be exploited in this paper, namely the synergy between AI and its most advanced techniques, on the one hand, and geospatial data, on the other. At a deeper level, the importance of linking GeoAI to geographic information systems (GIS) should be noted, as this enables geospatial data to be catalogued for appropriate manipulation, analysis and presentation [31]. Figure 1 illustrates the contribution of GIS as an ideal collaborator with AI, due to its efficiency and standardization.\nAnother fundamental aspect to highlight is the use of big geodata, a concept referring to large-scale geospatial data, which emphasises the use of AI models to manage, process and use this type of data, taking into account its variety of sources, natures and scales [32].\nTo address the subject of GeoAI comprehensively, it is essential to emphasize the importance of citing the various applications of GeoAI, including urban planning, logistics and transportation, agriculture, water resource management, and many others. In fact, the importance of listing GeoAI research works by application domains allows to follow a semantically consistent approach, i.e. each sub-domain to be developed is completely distinct, in terms of research objects, from the others. Furthermore, and to situate the aim of this work in relation to previous works, a great deal of research is carried out on this subject, as shown in Table 1, while touching on different sub-branches of geospatial field, but without giving a detailed overview, nor focusing on a wide range of application areas.\nThe problem at hand, is to provide a comprehensive review of the GeoAI's state of the art in terms of application domains, differing from previous work both in terms of the completeness of the inventory and in terms of research objectives.\nTo address this issue, a scoping and design phase is carried out, which consists of choosing the working methodology and clearly identifying the preliminary issues. Then, a definition of research questions is fundamental to orientate the census to be carried out. Next, a determination of research axes is worked out to identify key elements of the concerned investigation, using an advanced keyword search on Scopus database [33] and Google Scholar engine [34], on the one hand, and refining the choice of fields of application to be discussed, on the other. As illustrated in Figure 2, the papers are selected on the basis of a set of criterias:\n* Innovative contribution: given that the final objective of this work is to explore the level of progress achieved.\n* Diversification of selection themes: the aim is to cover all areas in question without duplicating the subjects tackled.\n* Quality of papers: those mainly published by renowned publishers such as Elsevier [35], the Institute of Electrical and Electronics Engineers (IEEE) [36], Springer [37] and Multidisciplinary Digital Publishing Institute (MDPI) [38] are selected.\n2.3 Research questions and directions\nA crucial step in this research is to first formulate the research questions in order to clarify the objectives to be achieved and to guide the work to be carried out. Three questions are important to ask:\n* RQ1: How can GeoAI methods contribute to effective and accurate results in various fields of application?\n* RQ2: What are the challenges currently facing GeoAI?\n* RQ3: What are the future prospects for GeoAI? More specifically, what are the promising themes to be explored in this context?\nThroughout the rest of this paper, relevant research focusing on the application of creative AI methods to geospatial domain are identified. To this end, the following research directions are chosen for exploration, according to their growing importance and their presence in the most relevant research works:\n* Precision agriculture;\n* Urban planning, logistics and transportation;\n* Environmental management;\n* Water resources management and precipitation forecasting;\n* Natural disaster monitoring;\n* Healthcare."}, {"title": "3 Artificial Intelligence : definitions and models", "content": "According to Xu et al. [39] Artificial Intelligence (AI) corresponds to :\na discipline referring to the simulation of human intelligence by machines, with the aim of imitating human behavior in different situations.\nMoreover, Santosh et al. [40] define Machine Learning (ML) as :\na set of techniques for approximating a function that maps an input space to an output space, while extracting meaningful, non-redundant information from data samples.\nFurthermore, LeCun et al. [41] presents Deep Learning (DL) as :\na branch of Machine Learning using multi-layer architectures to learn multiple representations of data, it is noted that deep learning uses the back-propagation technique to adjust the internal parameters of the model in order to recalculate these representations at each epoch.\nIn this section, a comprehensive overview of all commonly used AI methods and models is given, along with the main concepts put forward in the context in question.\n3.1 Machine Learning (ML) models\nIn order to carry out the various tasks commonly performed by AI, such as regression where the values of scalars are predicted, classification where a label is assigned to a specific piece of data, or clustering where a partitioning of data is proposed, a multitude of machine learning models are presented in the literature. In this subsection, different models are explored depending on the nature of the data used. In fact, two types of learning are distinguished: supervised learning, when the training data are labeled, and unsupervised learning, in the opposite case [42].\n3.1.1 Supervised learning\nIn this context, the data used for training is labeled, i.e. each data record is associated with a specific value. This value is then employed during the learning phases as a reference value, used to represent the links between inputs and outputs [43]. In this subsection, the most notable regression and supervised classification models are explored.\nLinear Regression: is an approach for modelling the relationship between a scalar dependent variable and explanatory variables, the case of a single explanatory variable being called simple linear regression [44], the relative equation is as follows:\n$\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\epsilon$,\nwhere:\n* y is the variable to be predicted,\n* $\\hat{y}$ is the value of y predicted by the model,\n* $x_1, x_2... x_n$ are the independent variables used,\n* $\\beta_0, \\beta_1 ... \\beta_n$ are the coefficients of this regression,\n* $ \\epsilon$ is the error committed by the regression.\nThe aim of this model is to learn the regression coefficients $\\beta_0, \\beta_1,..., \\beta_n$ by minimizing the value $\\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2$, where m is the number of data points.\nLogistic Regression: proposed by Cox [63], logistic regression calculates the probability of a variable belonging to a given class by optimizing the involved coefficients. The associated probability function is a sigmoid function:\n$P(y = 1|x) = \\frac{1}{1+e^{-(\\beta_0+\\beta_1x_1+\\beta_2x_2+...+ \\beta_nx_n)}}$,\nwhere:\n* y is the binary variable to be predicted (class membership),\n* $X_1, X_2... X_n$ are the independent variables used,\n* $\\beta_0, \\beta_1 ... \\beta_n$ are the coefficients to be adjusted.\nDecision trees: is a model used for regression and classification tasks, the principle behind which is to divide the training data into sub-batches for each node of the tree, with a set of decision criteria defining the depth of the tree until the final predictions are reached [64].\nRandom Forest (RF): is a supervised model combining several decision trees to perform predictive tasks such as classification, regression, etc. Introduced by Breiman [65], the principle of Random Forest consists in training each tree from a batch of data by random selection, the final result is then obtained from an average of results from all the trees, Figure 4 presents the principle of this model.\nSupport Vector Machine (SVM): proposed by Cortes and Vapnik [67], it is a model used for classification. The mathematical principle is to search for the hyperplane (n-dimensional generalization of a plane) that best maximizes the distance between the hyperplane and the various data classes.\nFigure 5 illustrates SVM model for a binary classification problem, the optimal hyperplane is constructed to separate the positive classes from the negative ones as best as possible, while maximizing the margin between its classes, two supporting hyperplanes (dotted lines) define the positive and negative boundaries.\nFor the multi-class problem, several algorithms are used to transform it into several binary classification problems, such as One-vs-Rest and One-vs-One [68].\nK-Nearest Neighbors (KNN): is a model used for classification and regression, as shown in Figure 6, KNN consists in identifying, for a given measure, the K closest points in terms of distance [70]. The value assigned to the candidate point is determined by the most frequent label of the K points, in the case of classification, and by averaging the K neighboring values, in the case of regression.\nGradient boosting: is a supervised technique used for classification and regression, which involves combining the prediction results of several decision trees. It is noted that Gradient boosting refines the prediction results of subsequent trees on the basis of the previous ones.\nGradient boosting uses ensemble learning, the principle of which is to use several ML algorithms to obtain, via several voting mechanisms, results that are better than those obtained by each of the methods alone. Several methods are used in ensemble learning, such as Bagging [71], Boosting [72] and Stacking [73].\nMore specifically, Extreme Gradient Boosting (XGBoost) is an optimized implementation of the Gradient Boosting algorithm. Introduced by Chen and Guestrin [74], XGBoost stands out for its computational efficiency and resource management.\n3.1.2 Unsupervised models\nAs stated by Barlow and Horace [75], unsupervised learning algorithms allows to learn the characteristics and representations of data without having any information about data labels. This seems very practical, given that, in many cases, the data are not annotated. In the following, unsupervised ML models are listed.\nK-means is a clustering method grouping a set of data points into K groups. Each measurement is taken and assigned to the nearest group, by measuring the distance between the point in question and the centroid of each group, then, this centroid is recalculated taking into account the new point added. This operation is repeated until the shape of groups becomes immutable [76]. Mathematically, groups are formed by minimizing the sum of the squared distances between the measurements and the centroids of the groups, i.e:\n$\\sum_{i=1}^{K} \\sum_{x \\in S_i} ||x - \\mu_i||^2$,\nwhere:\n* x is the measurement in question,\n* $\\mu_i$ is the centroid of the cluster,\n* $||.||_2$ is the L2 norm.\nThe K value is a key parameter in K-means algorithm, as it refers to the number of clusters, and therefore the clustering structure implemented, thus, it directly affects the model performance in terms of prediction quality and convergence.\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) is a clustering model proposed by Ester et al. [77], enabling a set of data points to be grouped together on the basis of their density. The principle of this algorithm consists in using a central point while defining a neighborhood radius, and a minimum number of points to be grouped together in each cluster. The strength of DBSCAN lies in the fact that it enables measurement points of various shapes and sizes to be grouped together, and does not require the number of resulting groupings to be specified.\n3.2 Deep Learning models\nThese are model architectures mimicking the functioning of the human brain. Deep Learning (DL) uses Deep Neural Networks (DNN) as a basic approach, highlighting two key concepts: gradient descent to calculate the weights of each neuron, and back-propagation to calculate the gradients of the cost function by involving the propagation of error backwards [78]. In this section, the main Deep Learning models are listed.\n3.2.1 Multi Layer Perceptron (MLP)\nProposed by Rosenblatt [79], MLP represents the most elementary Deep Neural Network architecture. The first layer transmits input data representation to the hidden layers, where each neuron is connected to all neurons in the next layer. This transmission of information between layers is based on non-linear activation functions applied to each neuron, enabling the model to better understand relevant dependencies between data. Other probabilistic activation functions are applied to output layer in the case of classification, or a linear activation function in the case of regression, in order to achieve meaningful results [80].\nIn spite of its simplicity, MLP is used in particular for tasks such as games, text and voice recognition.\n3.2.2 Convolutional Neural Networks (CNN)\nConvolutional Neural Network is a specific Deep Neural Network (DNN). As shown in Figure 7, it is broken down into the following elements, convolutional layers for Feature Extraction (FE), pooling layers for dimension reduction and Fully Connected layers (FC) for result classification. It is noted that several researchers have introduced the concept of CNN, such as Zhang et al. [81] and LeCun et al. [82], but the first work to implement a base of CNN is Waibel et al, who in 1987 proposed a Time-Delay Neural Network (TDNN) [83], which can be considered as a one-dimensional convolutional neural network. CNN have demonstrated notable efficiency for several computer vision tasks, including classification, segmentation and object detection. In this context, a multitude of models have been developed, such as VGG [84], U-Net [85], ResNet [86] and YOLO [87].\n3.2.3 Recurrent Neural Networks (RNN)\nRNN is a class of deep neural networks designed specifically to process sequential data including text, voice, time series, etc. Introduced in numerous works such as those by Hopefield [88] and Elman [89], The special feature of the RNN is the recursiveness of its connections, enabling this type of network to remember previous entries."}, {"title": "3.2.4 Auto-Encoders (AE)", "content": "It is a particular neural network for unsupervised generative tasks. Presented by Hinton et al. [94], the principle is to pass the data representation into a latent space in order to reduce dimensuality, and then reconstruct the data by transposed convolution. As illustrated in Figure 8, the encoder compresses the data while extracting the essential features captured, whereas the decoder aims to reconstruct the input structure through these features.\nAuto-encoders are used for several tasks such as image compression [95], anomaly detection [96] and denoising [97]."}, {"title": "3.2.5 Generative Adversarial Networks (GAN)", "content": "As their name suggests, Generative Adversarial Networks are used to generate data that resembles real ones as closely as possible. Proposed by Goodfellow et al. [98], a GAN consists of two networks, a generator creating data from a latent space, often a Gaussian or uniform distribution [99], and a descriminator that distinguishes between \"false\" and \"real\" data, enabling the generator to improve accuracy of the data it produces. Figure 9 describes this process.\nSeveral applications use the basic GAN architecture, such as art [100], image enhancement [101, 102], Video synthesis [103] and image translation [104]."}, {"title": "3.3 Advanced notions of Artificial Intelligence", "content": "In addition to Artificial Intelligence architectures, it is worth mentioning a number of advanced concepts linked to this subject. These concepts refer to mainly \u201ctheoretical\u201d approaches, allowing traditional algorithms and methods to make unprecedented advances. Indeed, reinforcement, federated, ensemble, multimodal and transfer learning are presented, along with attention mechanisms."}, {"title": "3.3.1 Reinforcement learning", "content": "Reinforcement learning is an ML technique in which the agent improves iteratively its behavior on the basis of rewards or penalties considered as the results of its actions, representing the agent's interaction with a given environment, This environment, modeled by a set of states, stands for the external setting to which the agent tries to evolve its future state. To do this, the agent tries to improve its decision making during the learning process[105].\nNumerous variants of reinforcement learning are proposed, such as State-Action-Reward-State-Action (SARSA) [106], Q-Learning [107] and Proximal Policy optimization [108]."}, {"title": "3.3.2 Federated learning", "content": "Introduced by Mcmahan et al. [109], this innovative approach enables multiple entities (physical or logical), to contribute to the training of a model without exchanging directly users data. Each entity produces its own updated widgets based on this training, a central server combines then these updates to improve the model under consideration. Figure 10 summarizes this process in three steps.\nOne should note that federated learning architecture offers a multitude of remarkable attributes, including reduced communication costs, task scalability, security, confidentiality and data decentralization, but faces a multitude of problems such as data imbalance and system failures [110]."}, {"title": "3.3.3 Multimodal learning", "content": "According to Ngiam et al. [111], multimodal learning involves the linking of information from multiple sources, for example, several categories of data including visual data, sound and text can be used to perform a specific task, such as recognising the exact representations of words or analysing feelings [112].\nSeveral approaches fit into the multimodal learning, including the early fusion, where multimodality is used from the start of model training, by combining features extracted from different sources of trained data, and the late fusion, where each modality is trained separately, the results of each prediction are then weighted towards the end of the process [113], as well as the shared representation learning, where all the modalities are projected into a single latent space. The aim of the latter approach is to capture the relationships between these different modalities in order to align and exploit them optimally [114]."}, {"title": "3.3.4 Transfer learning", "content": "To overcome the problem of lack of data for a specific task, transfer learning exploits data relating to a source task or obtained in domains related to the main task [115, 116]. In practical terms, this involves adjusting the weights of an initial model, often trained on a large scale dataset, to a much smaller one.\nAccording to Weiss et al. [117], transfer learning improves learning while improving performance with respect to the configuration offered, by making better use of the training data, and by enormously reducing the cost of calculating weights. Despite this, transfer learning faces several challenges, in particular the negative transfer, referring to the case where the target domain is too different from the source one, as well as the non-availability of labelled data, whether for the source or target domain, a problem that is proving difficult to overcome."}, {"title": "3.3.5 Attention mechanisms", "content": "It is an advanced technique allowing a model to concentrate on the essential parts of data. The contribution of this concept is to be able to focus on relevant data by imitating the way humans pay attention to the interesting elements around them [118]. Figure 11 describes an example of attention mechanism with respect to the constituents of a sequence, for instance, in this schema, remotely sensed image patches. Vector embedding is performed before calculating attention scores for each patch, followed by normalization using a probabilistic function for gradient stabilization and scaling, then, weighted vector calculation and linear aggregation are implemented to combine the relevant extracted representations.\nMany creative models are based on these mechanisms, for example:\nTransformers: introduced by Vaswani et al. [119], transformers are architechtures used mainly in Natural Language Processing (NLP), but adapted to multiple applications such as computer vision, via Vision transformers (ViTs) [120].\nAs indicated in Figure 12, a transformer consists of an encoder processing the inputs and a decoder generating the output. It is noted that each of these two components is made up of a single layer of feed forward neural networks, as well as residual addition and normalization to preserve the information transmitted by the model, along with positional encoding to manage sequence order. Each encoding-decoding layer is reinforced by a self-attention mechanism in order to extract dependencies between sequence components, including image patches, tokens, etc. In the attention mechanism in question, three vectors are calculated to compute attention scores, the Query (Q) representing the searched element, the Key (K) reflecting the input element, used by the query, and the Value (V) vector referring to the actual value related to the processed component of sequence."}, {"title": "3.4 Evaluation metrics", "content": "A variety of metrics are employed to evaluate the accuracy of models, depending on the nature of concerned tasks. In this subsection, the metrics mentioned are presented in order to give the reader a clear and concise overview.\n3.4.1 Mean Absolute Error (MAE)\nThis is a simple metric for calculating the accuracy of a regression model, by measuring the magnitude of model's prediction errors [123]:\n$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y_i}|$,\nwhere:\n* n is the number of observations,\n* for an observation i:\n- $y_i$ and $\\hat{y_i}$ are respectively the actual and the predicted value.\n- $|.|$ is the absolute value.\n3.4.2 Mean Square Error (MSE)\nUsed also to measure the quality of a regression model [124], the formula involved is as follows:\n$MSE = \\frac{1}{n} \\sum_{i=1}^{n}(y_i-\\hat{y_i})^2$,\nRoot Mean Square Error (RMSE) is a metric estimating the magnitude of prediction errors for a given model [124]. The associated formula is:\n$RMSE = \\sqrt{MSE}$.\n3.4.3 Mean Average Precision Error (MAPE)\nMAPE is a metric used mainly in regression tasks in order to calculate the predictive accuracy of a given value [125]. This metric gives a normalized measure of prediction errors and is calculated as follows:\n$MAPE = \\frac{100}{n} \\sum_{i=1}^{n} |\\frac{y_i-\\hat{y_i}}{y_i}|$,\nComparing the metrics presented so far, it is clear that MAE offers a more direct measure by giving equal weight to all errors, that MSE is more sensitive to errors than MAE and MAPE, since it amplifies the errors measured. Additionally, the RMSE is easier to interpret than MSE, since it is expressed in the same unit of observations, while MAPE allows relative error to be measured in percentage terms, offering the possibility of evaluating the efficiency of models driven by data at different scales."}, {"title": "3.4.4 Determination coefficient R2", "content": "R2 is a statistical measure, introduced by Wright [126], to measure the degree to which a regression model truly describes the relationship between its variables, by calculating the proportion of variance of the dependent variable from independent variables in the involved model. Mathematically, this measure is calculated as follows:\n$R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$,\nwhere, for an observation i:\n* $y_i$ is the real value of the dependent variable,\n* $\\hat{y_i}$ is the value predicted by the model,\n* $\\bar{y}$ is the average of the real values over the n observations of the dependent variable.\n3.4.5 Confusion matrix\nThis is a visualization of the predictions of a classification model with respect to ground truth values. It has the following form:\nConfusion matrix gives a detailed overview of the predictions obtained, while clearly identifying the best predicted classes and the errors made [127].\n3.4.6 Precision\nPrecision is an indicator measuring the degree of identification of positive predictions for a classification model [128]. In the case of binary classification, it is obtained as follows:\n$Precision = \\frac{TP}{TP+FP}$,\nwhere TP represents the number of true positives, i.e. cases correctly identified as positive, and FP is the number of false positives, i.e. cases incorrectly identified as positive. In the case of multiple classes, precision is calculated by averaging the precision obtained for each class using arithmetic or weighted average.\n3.4.7 Recall\nRecall is another measure quantifying the level at which a classification model identifies positive classes from the set of positive instances in measurement sample [128], for binary classification:\n$Recall = \\frac{TP}{TP+FN}$,\nwhere FN is the number of false negatives, i.e. cases incorrectly identified as negative.\nLike precision, recall in generic cases is deduced by averaging the recalls of each concerned class.\n3.4.8 Accuracy\nIt is a performance metric, defined as the ratio between the correct predictions of a classification model and the total number of predictions, for the case of binary classification:\n$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$"}, {"title": "3.4.9 F1-score", "content": "This metric evaluates the ability of a classification model to effectively predict positive individuals, by making a compromise between precision and recall [128]. The combination of precision and recall is presented in the form of a harmonic mean:\n$F1-score = 2 \\times \\frac{Precision \\times Recall}{Recall + Precision}$,\nF1-score is a metric giving a balanced evaluation of model's performance, useful when there is an imbalance between positive and negative classes.\n3.4.10 Jaccard index\nJaccard's index, often known as Intersection over Union (IoU), is an evaluation metric used in segmentation and detection tasks. It relates to the ratio between the intersection and the union of predictions and the ground truth [129]:\n$IoU = \\frac{Area \\ of \\ Intersection}{Area \\ of \\ Union}$\n3.4.11 mean Average Precision (mAP)\nIt is an evaluation metric often used for detection tasks. It corresponds to the average value of mean accuracies of each class [130]:\n$mAP = \\frac{1}{N} \\sum_{i=1}^{N} AP_i$,\nFor a determined object class i, the average accuracy APi is obtained by plotting the model's Accuracy-Recall curve, it corresponds to the area under this curve.\nNote: It is very common to use the mAP50 metric, measuring the mAP value for an IoU threshold of 50%, since it offers a compromise between high object location accuracy and error tolerances of this location.\n3.4.12 EAO (Expected Average Overlap)\nIt is a metric assessing the performance of object tracking models on videos. It is calculated by averaging the IOUs obtained on the image sequence in question [131]:\n$EAO = \\frac{1}{N} \\sum_{i=1}^{N} IoU(i)$,\nwhere:\n* N is the number of image sequences making up the video,\n* IoU(i) is the IOU of the ith image."}, {"title": "3.5 Geospatial datasets", "content": "The development of accurate and reliable models depends essentially on the quality of training datasets. In addition", "132": ".", "133": "natural resources management [134", "135": ".", "available": "n* EOSDIS datasets : These are public datasets from twelve Distributed Active Archive Centers (DAACs)", "include": "n- Land Processes Distributed Active Archive Center (LPDAAC) [136", "137": "n- National Snow and Ice Data Center Distributed Active Archive Center (NSIDCDAAC) [138", "139": "n- Physical"}]}