{"title": "Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning", "authors": ["Haozhe Ma", "Zhengding Luo", "Thanh Vinh Vo", "Kuankuan Sima", "Tze-Yun Leong"], "abstract": "Reward shaping addresses the challenge of sparse rewards in reinforcement learning by constructing denser and more informative reward signals. To achieve self-adaptive and highly efficient reward shaping, we propose a novel method that incorporates success rates derived from historical experiences into shaped rewards. Our approach utilizes success rates sampled from Beta distributions, which dynamically evolve from uncertain to reliable values as more data is collected. Initially, the self-adaptive success rates exhibit more randomness to encourage exploration. Over time, they become more certain to enhance exploitation, thus achieving a better balance between exploration and exploitation. We employ Kernel Density Estimation (KDE) combined with Random Fourier Features (RFF) to derive the Beta distributions, resulting in a computationally efficient implementation in high-dimensional continuous state spaces. This method provides a non-parametric and learning-free approach. The proposed method is evaluated on a wide range of continuous control tasks with sparse and delayed rewards, demonstrating significant improvements in sample efficiency and convergence stability compared to several baselines.", "sections": [{"title": "1 Introduction", "content": "Environments with sparse and delayed rewards present notable challenges for reinforcement learning (RL), despite its significant progress in various fields. In such contexts, as the reward structure lacks immediate signals, agents receive feedback only after long horizons, making the ability to quickly and effectively discover beneficial samples crucial for successful learning [Ladosz et al., 2022]. To address this, a straightforward solution is to reconstruct the reward models by introducing auxiliary signals to assess the agent's behavior at each step, which has led to the popular technique of Reward Shaping (RS) [Sorg et al., 2010a,b, Gupta et al., 2022]. Inverse reinforcement learning (IRL), which extracts reward functions from human knowledge or expert demonstrations, represents an intuitive approach within this framework. However, this heavily relies on extensive human input and data, which can be difficult to obtain especially in complex environments. Alternatively, fully autonomous approaches have emerged as an attractive direction.\nAutomatically maintained shaped rewards can be generally categorized into two branches: intrinsic motivation based signals, which are task-agnostic, and hidden value based signals, which are commonly task-related. The former mainly introduces exploration bonuses to encourage agents to explore a wider"}, {"title": "2 Related Work", "content": "Reward shaping (RS) techniques can be broadly categorized based on the source of learning: either from human knowledge or agent's own experiences. Techniques drawing reward models from human knowledge rely on pre-existing expertise, where methods such as Inverse Reinforcement Learning (IRL) [Arora and Doshi, 2021, Ramachandran and Amir, 2007, Ziebart et al., 2008, Hadfield-Menell et al., 2016] and Inverse Optimal Control (IOC) [Schultheis et al., 2021] aim to derive reward or objective functions from expert demonstrations. Another branch within this approach, Imitation Learning (IL) [Ho and Ermon, 2016, Duan et al., 2017, Brown et al., 2020], attempts to clone human behaviors from offline data without explicitly modeling reward structures. Following this, transferring the extracted reward models"}, {"title": "3 Preliminaries", "content": "Reinforcement Learning (RL) aims to train an agent to interact with an environment, which is commonly modeled as a Markov Decision Process (MDP). An MDP represented as (S, A, T, R, \u03b3), involves four main components: S is the state space, A is the action space, \\(T : S \u00d7 A \u00d7 S \u2192 [0, 1]\\) is the probability of transitioning from one state to another given a specific action, and R: S\u2192 R is the reward model. The objective in RL is to learn a policy \u03c0(a|s) that maximizes the expected cumulative rewards \\(G = \u0395[\u03a3_{t=0}^{\u221e}\u03b3^t R(s_t)]\\), where \u03c0(a|s) indicates the likelihood of selecting action a in state s, and \u03b3 is the discount factor. Model-free RL methods include updating value functions, directly optimizing policies, or employing an actor-critic framework [Sutton and Barto, 2018].\nBeta Distribution is defined on the interval [0,1], making it ideal for modeling proportions or probabilities. It is parameterized by \u03b1 and \u03b2, which are interpreted as prior counts of successes and failures of a binary outcome. The probability density function of a Beta-distributed variable X is:\n\\(f(x; \u03b1, \u03b2) = \\frac{1}{B(\u03b1, \u03b2)} x^{\u03b1-1}(1-x)^{\u03b2-1}\\),\nwhere B(\u03b1, \u03b2) is a normalization factor. The key characteristic of Beta distribution is its adaptability: as more data accumulates, the values of \u03b1 and \u03b2 increase, the distribution's shape becomes narrower, thereby increasing confidence in the estimated probabilities. This feature is particularly beneficial in adaptive online learning, aligning closely with our objective of balancing exploration and exploitation.\nKernel Density Estimation (KDE) is a non-parametric approach to approximate the probability density function of a random variable from data samples. Given n data points \\(\\{x_i\\}^n_{i=1}\\), KDE smooths these points to approximate the density function as follows:\n\\(\\hat{d}(x) = \\frac{1}{nh} \\sum_{i=1}^{n} K(\\frac{x - x_i}{h})\\),\nwhere h is the bandwidth, K(.) is a kernel function such as Gaussian kernel, Laplacian kernel, or Cauchy kernel. KDE is particularly useful in scenarios where the actual distribution is not well-defined or complex, such as continuous state spaces in our RL environments."}, {"title": "4 Methodology", "content": "We propose a Self-Adaptive Success Rate based reward shaping mechanism (SASR) to accelerate RL algorithms in sparse-reward environments. Figure 1 illustrates the principles of the SASR mechanism: The diagram consists of two parts representing the early and late stages of learning. From the early stage to the late stage, the Beta distributions modeling the estimation of the true success rates evolve from being more stochastic to more deterministic as experiences accumulate. This progression closely aligns with the agent's exploration-exploitation balance. The process of Beta distributions' evolution and generation of shaped rewards from them are detailed in Section 4.1. Additionally, to achieve highly efficient computation, we leverage KDE and RFF to estimate success and failure counts, which are used to derive the corresponding Beta distributions, as detailed in Section 4.2. Lastly, Section 4.3 presents the integration of SASR into the RL agent and the overall algorithmic flow."}, {"title": "4.1 Self-Adaptive Success Rate Sampling", "content": "We consider the following augmented reward function for SASR:\n\\(R^{SASR}(s) = R^E(s) + \u03bbR^S(s)\\),\nwhere \\(R^E(s)\\) is the environmental reward, \\(R^S(s)\\) is the additional shaped reward, and \u03bb is a weight factor to control its relative scale. When \u03bb \u2192 0, we can recover the original reward model. We assign the shaped reward \\(R^S(s_i)\\) of a given state based on its success rate \u2013 defined as the ratio of the state's presence in successful trajectories to its total occurrences. This metric provides a meaningful and informative reward from a statistical perspective, as a higher success rate indicates a greater likelihood and contribution for the state to guide the agent towards successful task completion, thus deserving a higher shaped reward. Formally, the success rate based shaped reward \\(R^S(s_i)\\) is given by:\n\\(R^S(s_i) = f(\\frac{N_s(s_i)}{N_s(s_i) + N_F(s_i)})\\),\nwhere \\(N_s(s_i)\\) and \\(N_F(s_i)\\) denote the counts of appearances for state \\(s_i\\) in historical successful and failed trajectories respectively, and f(\u00b7) is a mapping function that adjusts the magnitude of rewards: f: [0,1]\u2192 [\\(R_{min}, R_{max}\\)].\nGiven the counts \\(N_s(s_i)\\) and \\(N_F(s_i)\\) derived from prior experiences, directly using a deterministic success rate \\(\\frac{N_s(s_i)}{N_s(s_i)+N_F(s_i)}\\) may lead to overconfidence in the estimation of the true value. To address this, we model the success rate estimation from a probabilistic perspective, inspired by the principles of Thompson sampling, an effective algorithm widely used to balance exploration and exploitation in multi-armed bandit problems [Thompson, 1933, Agrawal and Goyal, 2012]. We model the success rate of each state as an approximated variable in a Beta distribution, setting the shape parameters \u03b1 = \\(N_s(s_i)\\) + 1 and \u03b2 = \\(N_F(s_i)\\) + 1:\nr ~ Beta(r; \u03b1, \u03b2) = \\(\\frac{1}{B(N_s(s_i) + 1, N_F(s_i) + 1)}r^{N_s(s_i)}(1 \u2013 r)^{N_F(s_i)}\\),\nwhere the beta function B(\u00b7,\u00b7) is the normalization factor. By sampling from this distribution, we obtain a probabilistic estimate of the state's success rate. This sampled value, \\(r_s\\), is then processed through the mapping function f(\u00b7) to produce the shaped reward: \\(R^S(s_i) = f(r_s)\\).\nAs \\(N_s(s_i)\\) and \\(N_F(s_i)\\) progressively increase throughout the learning process, they influence the shape and sampling variability of the Beta distribution. Generating the shaped reward from these evolving Beta distributions offers several advantages:"}, {"title": "4.2 Highly Efficient Beta Distribution Derivation", "content": "In this section, we present the methods to obtain the success and failure counts, \\(N_s(s_i)\\) and \\(N_F(s_i)\\), which are required for deriving the Beta distributions. In tabular environments with low-dimensional, discrete, and finite state spaces, tracking these counts can be straightforwardly accomplished using two tables. However, most real-world environments involve high-dimensional, continuous, and infinite state spaces, making it impractical to maintain exact counts for every state.\nTo address this challenge, we use Kernel Density Estimation (KDE) to approximate the densities of successes and failures from accumulated experience. Specifically, we maintain two buffers, Ds and DF, to store the states in successful and failed trajectories, respectively. By treating these states as scattered data instances distributed across the state space, KDE estimates the density as:\n\\(d_x(s_i) = \\frac{1}{|D_x|} \\sum_{j=1}^{|D_x|} K(s_i - s_j), X \u2208 \\{S, F\\}\\),\nwhere K(\u00b7) is the kernel function, and |\\(D_x\\)| is the total number of states in the corresponding buffer. We select the Gaussian kernel in our implementation. The estimated density \\(d_x(s_i)\\) serves as a proxy for the likelihood of encountering state \\(s_i\\) under success or failure scenarios, providing a statistically sound basis for estimating \\(N_x(s_i)\\). By multiplying \\(d_x(s_i)\\) with the total number of observed states N, and representing states as k-dimensional vectors, the count \\(\\tilde{N}_x(s_i)\\) is estimated as:\n\\(\\tilde{N}_x(s_i)\\) = N \u00d7 \\(d_x(s_i)\\) = \\frac{N}{|D_x|} \\sum_{j=1}^{|D_x|} exp(-\\frac{||s_i - s_j||^2}{2\u03c3^2}), X \u2208 \\{S, F\\},\nwhere \u03c3 is a hyperparameter that controls the bandwidth of the Gaussian kernel.\nWe further integrate Random Fourier Features (RFF) [Rahimi and Recht, 2007] to reduce computational complexity, as calculating the Gaussian kernel can be expensive, especially in scenarios involving high-dimensional state spaces and large buffers. RFF approximates the kernel function of the original k-dimensional states through an inner product of M-dimensional randomized features:\n\\(K(s_i, s_j) \u2248 z(s_i)^Tz(s_j), z(s) = \\sqrt{\\frac{2}{M}} cos(W^Ts + b),\\)"}, {"title": "4.3 The SASR Mechanism for RL agents", "content": "Building upon the SASR reward, we employ the soft actor-critic (SAC) algorithm of Haarnoja et al. [2018a] as the foundation for our RL agent. Let \\(\u03c0_A^P\\) be parameterized policy network and \\(Q_\u03d5^P\\) be parameterized Q-network. We optimize the Q-function by minimizing the mean squared error (MSE) between the predicted Q-value and the temporal difference (TD) target:\n\\(L(\u03d5) = E_{(s_t, a_t) ~ D} [\\big(Q_\u03d5(s_t, a_t) \u2013 (r_t + \u03bbR^S(s_t) + \u03b3Q_{\u03d5'}(s_{t+1}, a_{t+1}))\\big)^2],\nwhere \\(Q_{\u03d5'}\\) is obtained from a secondary frozen target network to maintain a fixed objective [Mnih et al., 2015]. It is worth noting that the environmental reward \\(r_t\\) is retrieved from the replay buffer. Conversely, the shaped reward \\(R^S(s_t)\\) is computed in real-time using the most recently updated \\(N_s(s_t)\\) and \\(N_F(s_t)\\), thereby capturing the latest advancements in learning progress.\nWe optimize the policy network by maximizing the expected Q-value and the entropy of the policy \u0397(\\(\u03c0_A^P\\)(\u00b7|st)) [Haarnoja et al., 2018b]:\n\\(L(\u03b8) = E_{a_t ~ \u03c0_\u03b8(\u00b7|s_t)} [\u2013 Q_\u03d5(s_t, a_t) + log \u03c0_\u03b8(a_t|s_t)].\nThe outline of the SASR is summarized in Algorithm 1. We maintain two buffers to store successful and failed states and estimate the success and failure counts from them. Subsequently, shaped rewards"}, {"title": "5 Comparison and Discussion", "content": "Baselines. We compare SASR with eight baselines to benchmark its performance: (a) RL with an Assistant Reward Agent (ReLara) [Ma et al., 2024], (b) RL Optimizing Shaping Algorithm (ROSA) [Mguni et al., 2023], (c) Exploration-Guided RS (ExploRS) [Devidze et al., 2022], (d) Count-based static hashing exploration (#Explo) [Tang et al., 2017], (e) Random Network Distillation (RND) [Burda et al., 2018], (f) Soft Actor-Critic (SAC) [Haarnoja et al., 2018a], (g) Twin Delayed Deep Deterministic Policy Gradient (TD3) [Fujimoto et al., 2018], and (h) Proximal Policy Optimization (PPO) [Schulman et al., 2017]. Algorithms (a) to (e) are all reward shaping methods, involving either exploration bonuses or auxiliary agents to provide additional rewards to tackle the sparse-reward challenge, while algorithms (f) to (h) are advanced RL algorithms. For the implementation of these baselines, we employ either the CleanRL library [Huang et al., 2022] or codes provided by the authors.\nSASR, on the other hand, utilizes the success rate as auxiliary information to assess each state's contribution to task completion. This metric is consistent with the agent's primary objectives, and incorporating this meaningful signal into the temporal difference updating at each step significantly enhances learning efficiency. Without this integration, the agent would have to wait until the end of the trajectory to obtain informative signals. Furthermore, SASR regulates an efficient exploration strategy by injecting substantial random rewards during the early phases to optimize the agent in"}, {"title": "5.2 Effect of Self-Adaptive Success Rate Sampling", "content": "SASR introduces a novel self-adaptive mechanism that balances exploration and exploitation by maintaining the randomness of the shaped rewards. Figure 4 illustrates the generated rewards at five stages of learning in the AntStand task. Figure 4 (bottom) represents the learning curve, and Figure 4 (top) illustrates the distribution of the generated rewards over the \"height of the ant\" feature, one of the dimensions in the state vector.\nAs learning progresses, the evolving shaped rewards exhibit two crucial attributes: the values transition from random to meaningful, and the variance decreases from uncertain to deterministic. In the early stages of training, the shaped rewards involve significant randomness due to higher uncertainty. Although these random signals contain limited information, they prompt the agent to take tiny optimization steps in diverse directions, effectively shifting the anchors of the policies to various regions. This strategy broadens the actions generated by SAC's stochastic policy, encouraging the agent's exploration and increasing the diversity of collected samples. From another perspective, the perturbations on the rewards can be seen as a relaxation of the assumption that only novel states should be rewarded, instead, randomly reward any state, which also shows its empirical effectiveness. In the later stages, the rewards are observed to converge to closely match the height of the robot, a meaningful and intuition-aligned metric, which enhances the agent's exploitation of its policy. This self-adaptive mechanism"}, {"title": "5.3 Ablation Study", "content": "We conduct ablation studies to investigate five key components of SASR: (1) sampling from Beta distributions versus using the deterministic success rates; (2) the retention rate \u03d5, which controls the volume of historical information; (3) the bandwidth h of the kernel density estimation; (4) the number of dimensions M of RFF features; and (5) the weight factor \u03bb, which adjusts the scales of the shaped rewards. We select six representative tasks to report the experimental results, and the quantitative data is provided in Appendix C.\nSampling from Beta distributions. We examine a variant of SASR that omits sampling from the Beta distribution and instead directly uses the success rate \\(N_s(s_i)/(N_s(s_i) + N_F(s_i))\\). Comparative results are shown in Figure 5. In the early stages of learning, limited experience makes this success rate an inaccurate estimate of the true value. Using this overly confident fixed value can mislead the agent. Additionally, removing the process of sampling from the Beta distribution disrupts the agent's exploration encouraged by random rewards, leading to slower convergence. The results highlight the importance of sampling from the Beta distribution for effective learning.\nRetention rate \u03d5. The retention rate controls the counts' estimation, subsequently impacting the Beta distribution and sampled rewards. We assess various retention rates in Figure 6. A high retention rate, such as \u03d5 = 1, preserving all samples, results in a densely populated and redundant experience pool, causing the Beta distribution to be prematurely overconfident, which can degrade performance. Conversely, a lower retention rate \u03d5 = 0.01 requires more iterations to accumulate sufficient samples, thereby slowing the convergence to meaningful rewards and affecting the agent's convergence speed. The results suggest that carefully balancing the retention rate is crucial, and promising future work could involve adaptive adjustment of the retention rate during the learning process.\nBandwidth h of Gaussian kernel. We evaluate different bandwidths h of the Gaussian kernel for density estimation, as shown in Figure 7. Beyond fixed bandwidths, we also test a linearly decreasing bandwidth configuration (h : 0.5 \u2192 0.1), which represents progressively increasing confidence in KDE."}, {"title": "6 Conclusion and Discussion", "content": "We propose SASR in this paper, a self-adaptive reward shaping algorithm based on success rates, to address the sparse-reward challenge. SASR achieves an exploration and exploitation trade-off mechanism by generating shaped rewards from evolving Beta distributions. Experiments demonstrate that this adaptability significantly enhances the agent's convergence speed. Additionally, the implementation of KDE and RFF formulates a highly efficient, non-parametric, and learning-free approach to deriving Beta distributions. This mechanism also provides a sound alternative to traditional count-based RS strategies, adapting effectively to continuous environments. Our evaluations confirm the superior performance of SASR in terms of sample efficiency and learning stability, and its robustness across various continuous-control tasks.\nWhile SASR is designed for environments with sparse rewards, in settings where the environmental rewards are dense, the introduction and maintenance of additional shaped rewards could be unnecessary. On the other hand, SASR operates on the premise that sparse rewards indicate task completion to align the success rate notion. This correlation is generally straightforward, yet it may not intuitively apply in all contexts. Exploring how to extend SASR to more general sparse and even dense reward scenarios presents a promising direction for further research. Moreover, the derivation of Beta distributions heavily relies on the samples in the success and failure buffers. Currently, our method does not capture the relationships and varying importance of different states within the same trajectory, making the algorithm relatively sensitive to the retention rate, a crucial hyperparameter for achieving promising performance. Therefore, developing an adaptive retention rate or better mechanisms for maintaining the two buffers are crucial avenues for future enhancements."}, {"title": "A Derivation of Random Fourier Features", "content": "We incorporate Random Fourier Features (RFF) [Rahimi and Recht, 2007] to approximate the kernel functions in the KDE process for the SASR algorithm. Let the original state be k-dimensional, denoted as s \u2208 Rk, and the kernel function be k(si, sj). RFF approximates the kernel function by projecting the input k-dimensional space into a M-dimensional feature space using a mapping function z : Rk \u2192 RM. The RFF-based kernel function is then defined as follows:\nk(si, sj) \u2248 z(si)Tz(sj),\nWe provide the derivation of the RFF approximation in this section.\nFirst, we clarify that RFF primarily targets shift-invariant kernels, that satisfy k(si, sj) = k(si \u2013 sj). Common shift-invariant kernels include Gaussian kernels, Laplacian kernels, and Cauchy kernels. Given a shift-invariant kernel function k(\u0394), we perform the inverse Fourier transform:\nk(si, sj)\n=\\Rk eiwT(si-sj) dw\n= Ew [eiwT (8-8j)],\nwhere we can consider w ~ p(w) based on the Bochner's theorem, and p(w) is called the spectral distribution of kernel function. For the three types of shift-invariant kernels, the corresponding spectral distributions are listed in Table 2:\nNext, we perform the Euler's formula transformation, which retains only the cosine term since we are dealing with real-valued functions, the kernel function can be further derived as:\nk(Si, Sj) = Ew [eiwT (si-sj)]\n= Ew [cos(wT (si - Sj))]\n= Ew [cos(wT (si - Sj))] + Ew [Eb[cos(wT (si + sj) + 2b)]]\n= Ew [Eb[cos(WT (si \u2013 sj)) + cos(wT (si + sj) + 2b)]]\n= Ew [Eb[\u221a2 cos(wTsi + b) \u221a2cos(wTsj + b)]],\nwhere b ~ Uniform(0, 2\u03c0). Equation 17 holds since Eb~Uniform(0,2\u03c0) [cos(t + 2b)] = 0 for any t. Equation 19 is obtained from cos(A - B) + cos(A + B) = 2cos(A) cos(B), where A = wTsi + b, B = wTsj + b.\nWe define the mapping zw,b(s) = \u221a2cos(wTs + b), then the kernel function can be approximated by"}, {"title": "B Derivation of Computation Complexity", "content": "In this section, we derive the computational complexity to retrieve the success or failure counts Ns and NF for each iteration. Suppose the buffer size of Dx is D, the batch size of B is B, the corresponding counts are retrieved by calculating:\n\\(\\tilde{N}_x = N \u00d7 z(B)^T z(Dx),\\)\nwhere the mapping function is defined as:\nz(s) = \\sqrt{\\frac{2}{M}} cos(W^Ts+b), W\u2208RkxM, b\u2208RM.\nFor each state, the mapping function calculation involves:\n\u2022 Matrix multiplication WTs: kM.\n\u2022 Addition WTs + b: M.\n\u2022 Cosine calculation cos(WTs + b): M.\nTherefore, the computational complexity for calculating z(s) for one state is O(kM).\nFor each pair of states (si, sj), calculating the kernel involves M multiplications and M \u2013 1 additions, thus, the complexity is O(M).\nFor each iteration, we calculate the RFF mapping for all states in the buffer and the batch and then compute the kernel between them. The complexities involve three parts: RFF mapping for the buffer, RFF mapping for the batch and kernel calculation:\nO(DkM) + O(BkM) + O(MDB).\nSince the first two terms O(DkM) and O(BkM) are dominated by the last term O(MDB) when the buffer size and the batch size are large, the overall computational complexity to retrieve the corresponding counts can be approximated as O(MDB)."}, {"title": "C Supplementary Experimental Results for Ablation Study", "content": "We provide the detailed quantitative results of the ablation study in this section."}, {"title": "D Network Structures and Hyperparameters", "content": "D.1 Network Structures\nFigure 10 illustrates the structures of the policy network and Q-network employed in our experiments. The agent utilizes simple multilayer perceptron (MLP) models for these networks. Given the use of stochastic policies, the policy network features separate heads to generate the means and standard deviations of the inferred normal distributions, which are then used to sample actions accordingly.\nD.2 Hyperparameters\nWe have observed that SASR demonstrated high robustness and was not sensitive to hyperparameter choices. Table 8 shows the set of hyperparameters that we used in all of our experiments."}, {"title": "E Compute Resources", "content": "The experiments in this paper were conducted on a computing cluster, with the detailed hardware configurations listed in Table 9. The computing time for the SASR algorithm in each task (running 1,000,000 steps) was approximately 6 \u00b1 2 hours."}]}