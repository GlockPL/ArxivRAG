{"title": "Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning", "authors": ["Haozhe Ma", "Zhengding Luo", "Thanh Vinh Vo", "Kuankuan Sima", "Tze-Yun Leong"], "abstract": "Reward shaping addresses the challenge of sparse rewards in reinforcement learning by constructing denser and more informative reward signals. To achieve self-adaptive and highly efficient reward shaping, we propose a novel method that incorporates success rates derived from historical experi-ences into shaped rewards. Our approach utilizes success rates sampled from Beta distributions, which dynamically evolve from uncertain to reliable values as more data is collected. Initially, the self-adaptive success rates exhibit more randomness to encourage exploration. Over time, they become more certain to enhance exploitation, thus achieving a better balance between exploration and exploitation. We employ Kernel Density Estimation (KDE) combined with Random Fourier Features (RFF) to derive the Beta distributions, resulting in a computationally efficient implemen-tation in high-dimensional continuous state spaces. This method provides a non-parametric and learning-free approach. The proposed method is evaluated on a wide range of continuous control tasks with sparse and delayed rewards, demonstrating significant improvements in sample efficiency and convergence stability compared to several baselines.", "sections": [{"title": "Introduction", "content": "Environments with sparse and delayed rewards present notable challenges for reinforcement learning (RL), despite its significant progress in various fields. In such contexts, as the reward structure lacks immediate signals, agents receive feedback only after long horizons, making the ability to quickly and effectively discover beneficial samples crucial for successful learning [Ladosz et al., 2022]. To address this, a straightforward solution is to reconstruct the reward models by introducing auxiliary signals to assess the agent's behavior at each step, which has led to the popular technique of Reward Shaping (RS) [Sorg et al., 2010a,b, Gupta et al., 2022]. Inverse reinforcement learning (IRL), which extracts reward functions from human knowledge or expert demonstrations, represents an intuitive approach within this framework. However, this heavily relies on extensive human input and data, which can be difficult to obtain especially in complex environments. Alternatively, fully autonomous approaches have emerged as an attractive direction.\nAutomatically maintained shaped rewards can be generally categorized into two branches: intrinsic motivation based signals, which are task-agnostic, and hidden value based signals, which are commonly task-related. The former mainly introduces exploration bonuses to encourage agents to explore a wider"}, {"title": "Related Work", "content": "Reward shaping (RS) techniques can be broadly categorized based on the source of learning: either from human knowledge or agent's own experiences. Techniques drawing reward models from human knowledge rely on pre-existing expertise, where methods such as Inverse Reinforcement Learning (IRL) [Arora and Doshi, 2021, Ramachandran and Amir, 2007, Ziebart et al., 2008, Hadfield-Menell et al., 2016] and Inverse Optimal Control (IOC) [Schultheis et al., 2021] aim to derive reward or objective functions from expert demonstrations. Another branch within this approach, Imitation Learning (IL) [Ho and Ermon, 2016, Duan et al., 2017, Brown et al., 2020], attempts to clone human behaviors from offline data without explicitly modeling reward structures. Following this, transferring the extracted reward models"}, {"title": "Preliminaries", "content": "Reinforcement Learning (RL) aims to train an agent to interact with an environment, which is commonly modeled as a Markov Decision Process (MDP). An MDP represented as (S, A, T, R, \u03b3), involves four main components: S is the state space, A is the action space, T : S \u00d7 A \u00d7 S \u2192 [0, 1] is the probability of transitioning from one state to another given a specific action, and R: S\u2192 R is the reward model. The objective in RL is to learn a policy \u03c0(\u03b1|s) that maximizes the expected cumulative rewards G = \u0395[\u03a3t=oy*R(st)], where \u03c0(\u03b1|s) indicates the likelihood of selecting action a in state s, and y is the discount factor. Model-free RL methods include updating value functions, directly optimizing policies, or employing an actor-critic framework [Sutton and Barto, 2018].\nBeta Distribution is defined on the interval [0,1], making it ideal for modeling proportions or probabilities. It is parameterized by a and \u1e9e, which are interpreted as prior counts of successes and failures of a binary outcome. The probability density function of a Beta-distributed variable X is:\n\n$f(x; \\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)} x^{\\alpha-1}(1-x)^{\\beta-1},$ \n\nwhere B(\u03b1, \u03b2) is a normalization factor. The key characteristic of Beta distribution is its adaptability: as more data accumulates, the values of a and \u1e9e increase, the distribution's shape becomes narrower, thereby increasing confidence in the estimated probabilities. This feature is particularly beneficial in adaptive online learning, aligning closely with our objective of balancing exploration and exploitation.\nKernel Density Estimation (KDE) is a non-parametric approach to approximate the probability density function of a random variable from data samples. Given n data points {xi}i=1 , KDE smooths these points to approximate the density function as follows:\n\n$\\hat{d}(x) = \\frac{1}{nh} \\sum_{i=1}^{n} K(\\frac{x - x_i}{h}),$\n\nwhere h is the bandwidth, K(.) is a kernel function such as Gaussian kernel, Laplacian kernel, or Cauchy kernel. KDE is particularly useful in scenarios where the actual distribution is not well-defined or complex, such as continuous state spaces in our RL environments."}, {"title": "Methodology", "content": "We propose a Self-Adaptive Success Rate based reward shaping mechanism (SASR) to accelerate RL algorithms in sparse-reward environments. From the early stage to the late stage, the Beta distributions modeling the estimation of the true success rates evolve from being more stochastic to more deterministic as experiences accumulate. This progression closely aligns with the agent's exploration-exploitation balance. The process of Beta distributions' evolution and generation of shaped rewards from them are detailed in Section 4.1. Additionally, to achieve highly efficient computation, we leverage KDE and RFF to estimate success and failure counts, which are used to derive the corresponding Beta distributions, as detailed in Section 4.2. Lastly, Section 4.3 presents the integration of SASR into the RL agent and the overall algorithmic flow."}, {"title": "Self-Adaptive Success Rate Sampling", "content": "We consider the following augmented reward function for SASR:\n\n$R^{SASR}(s) = R^{E}(s) + \\lambda R^{S}(s),$\n\nwhere RE(s) is the environmental reward, RS(s) is the additional shaped reward, and A is a weight factor to control its relative scale. When \u5165 \u2192 0, we can recover the original reward model. We assign the shaped reward RS(si) of a given state based on its success rate \u2013 defined as the ratio of the state's presence in successful trajectories to its total occurrences. This metric provides a meaningful and informative reward from a statistical perspective, as a higher success rate indicates a greater likelihood and contribution for the state to guide the agent towards successful task completion, thus deserving a higher shaped reward. Formally, the success rate based shaped reward RS(si) is given by:\n\n$R^{S}(s_i) = f(\\frac{N_s(s_i)}{N_s(s_i)+N_F(s_i)}),$\n\nwhere Ns(si) and NF(si) denote the counts of appearances for state si in historical successful and failed trajectories respectively, and f(\u00b7) is a mapping function that adjusts the magnitude of rewards: f: [0,1]\u2192 [Rmin, Rmax].\nGiven the counts Ns(si) and NF(si) derived from prior experiences, directly using a deterministic success rate $\\frac{N_s(s_i)}{N_s(s_i)+N_F(s_i)}$ may lead to overconfidence in the estimation of the true value. To address this, we model the success rate estimation from a probabilistic perspective, inspired by the principles of Thompson sampling, an effective algorithm widely used to balance exploration and exploitation in multi-armed bandit problems [Thompson, 1933, Agrawal and Goyal, 2012]. We model the success rate of each state as an approximated variable in a Beta distribution, setting the shape parameters a = Ns(si) + 1 and \u1e9e = NF(si) + 1:\n\n$r~ Beta(r; \\alpha, \\beta) = \\frac{1}{B(N_s(s_i) + 1, N_F(s_i) + 1)} r^{N_s(s_i)} (1 - r)^{N_F(s_i)},$\n\nwhere the beta function B(\u00b7,\u00b7) is the normalization factor. By sampling from this distribution, we obtain a probabilistic estimate of the state's success rate. This sampled value, rs, is then processed through the mapping function f(\u00b7) to produce the shaped reward: RS(si) = f(rs).\nAs Ns(si) and NF(Si) progressively increase throughout the learning process, they influence the shape and sampling variability of the Beta distribution. Generating the shaped reward from these evolving Beta distributions offers several advantages:"}, {"title": "Highly Efficient Beta Distribution Derivation", "content": "In this section, we present the methods to obtain the success and failure counts, Ns(si) and NF(Si), which are required for deriving the Beta distributions. In tabular environments with low-dimensional, discrete, and finite state spaces, tracking these counts can be straightforwardly accomplished using two tables. However, most real-world environments involve high-dimensional, continuous, and infinite state spaces, making it impractical to maintain exact counts for every state.\nTo address this challenge, we use Kernel Density Estimation (KDE) to approximate the densities of successes and failures from accumulated experience. Specifically, we maintain two buffers, Ds and DF, to store the states in successful and failed trajectories, respectively. By treating these states as scattered data instances distributed across the state space, KDE estimates the density as:\n\n$d_x(s_i) = \\frac{1}{|D_x|} \\sum_{j=1}^{|D_x|} K(s_i - s_j), X \\in {S,F},$\n\nwhere K(\u00b7) is the kernel function, and |Dx| is the total number of states in the corresponding buffer. We select the Gaussian kernel in our implementation. The estimated density dx(si) serves as a proxy for the likelihood of encountering state si under success or failure scenarios, providing a statistically sound basis for estimating Nx(si). By multiplying dx(si) with the total number of observed states N, and representing states as k-dimensional vectors, the count \u00d1x(si) is estimated as:\n\n$\\tilde{N}_x(s_i) = N \\times d_x(s_i) = \\frac{N}{|D_x|} \\sum_{j=1}^{|D_x|} exp(-\\frac{||s_i - s_j||^2}{2 \\sigma^2}), X \\in {S, F},$\n\nwhere o is a hyperparameter that controls the bandwidth of the Gaussian kernel.\nWe further integrate Random Fourier Features (RFF) [Rahimi and Recht, 2007] to reduce computational complexity, as calculating the Gaussian kernel can be expensive, especially in scenarios involving high-dimensional state spaces and large buffers. RFF approximates the kernel function of the original k-dimensional states through an inner product of M-dimensional randomized features:\n\n$K(s_i, s_j) \\approx z(s_i)^Tz(s_j), z(s) = \\sqrt{\\frac{2}{M}} cos(W^T s + b),$"}, {"title": "The SASR Mechanism for RL agents", "content": "Building upon the SASR reward, we employ the soft actor-critic (SAC) algorithm of Haarnoja et al. [2018a] as the foundation for our RL agent. Let P be parameterized policy network and QAP be parameterized Q-network. We optimize the Q-function by minimizing the mean squared error (MSE) between the predicted Q-value and the temporal difference (TD) target:\n\n$L(\\phi) = (Q_{\\phi}(s_t, a_t) - (r + \\lambda R^{S} (s_t) + Q_{\\phi'}(s_{t+1}, a_{t+1})))^2,$\n\nwhere Q is obtained from a secondary frozen target network to maintain a fixed objective [Mnih et al., 2015]. It is worth noting that the environmental reward re is retrieved from the replay buffer. Conversely, the shaped reward RS(st) is computed in real-time using the most recently updated Ns(st) and NF(st), thereby capturing the latest advancements in learning progress.\nWe optimize the policy network by maximizing the expected Q-value and the entropy of the policy \u0397(\u03c0\u0391\u03a1(\u00b7|st)) [Haarnoja et al., 2018b]:\n\n$L(\\theta) = E_{a_t ~ \\pi_{\\theta}(\\cdot|s_t)} [ - Q_{\\phi}(s_t, a_t) + log \\pi_{\\theta}(a_t|s_t)].$\n\nThe outline of the SASR is summarized in Algorithm 1. We maintain two buffers to store successful and failed states and estimate the success and failure counts from them. Subsequently, shaped rewards"}, {"title": "Comparison and Discussion", "content": "Baselines. We compare SASR with eight baselines to benchmark its performance: (a) RL with an As-sistant Reward Agent (ReLara) [Ma et al., 2024], (b) RL Optimizing Shaping Algorithm (ROSA) [Mguni et al., 2023], (c) Exploration-Guided RS (ExploRS) [Devidze et al., 2022], (d) Count-based static hashing exploration (#Explo) [Tang et al., 2017], (e) Random Network Distillation (RND) [Burda et al., 2018], (f) Soft Actor-Critic (SAC) [Haarnoja et al., 2018a], (g) Twin Delayed Deep Deterministic Policy Gradient (TD3) [Fujimoto et al., 2018], and (h) Proximal Policy Optimization (PPO) [Schulman et al., 2017]. Algorithms (a) to (e) are all reward shaping methods, involving either exploration bonuses or auxiliary agents to provide additional rewards to tackle the sparse-reward challenge, while algorithms (f) to (h) are advanced RL algorithms. For the implementation of these baselines, we employ either the CleanRL library [Huang et al., 2022] or codes provided by the authors.\nOur findings indicate that SASR surpasses the baselines in terms of sample efficiency, learning stability, and convergence speed. The primary challenges in these environments are attributed to delayed and sparse rewards alongside long horizons, making exploration essential for timely obtaining successful trajectories. Although the exploration strategies of algorithms such as ExploRS, #Explo, and RND are designed to reward novel states, effectively expanding the early exploration space with the direct additional target, they continue focusing on discovering novel states, overlooking the implicit values of these states, which makes them fail to return to the final objectives.\nSASR, on the other hand, utilizes the success rate as auxiliary information to assess each state's contribution to task completion. This metric is consistent with the agent's primary objectives, and incorporating this meaningful signal into the temporal difference updating at each step significantly enhances learning efficiency. Without this integration, the agent would have to wait until the end of the trajectory to obtain informative signals. Furthermore, SASR regulates an efficient exploration strategy by injecting substantial random rewards during the early phases to optimize the agent in"}, {"title": "Effect of Self-Adaptive Success Rate Sampling", "content": "SASR introduces a novel self-adaptive mechanism that balances exploration and exploitation by maintaining the randomness of the shaped rewards. As learning progresses, the evolving shaped rewards exhibit two crucial attributes: the values transition from random to meaningful, and the variance decreases from uncertain to deterministic. In the early stages of training, the shaped rewards involve significant randomness due to higher uncertainty. Although these random signals contain limited information, they prompt the agent to take tiny optimization steps in diverse directions, effectively shifting the anchors of the policies to various regions. This strategy broadens the actions generated by SAC's stochastic policy, encouraging the agent's exploration and increasing the diversity of collected samples. From another perspective, the perturbations on the rewards can be seen as a relaxation of the assumption that only novel states should be rewarded, instead, randomly reward any state, which also shows its empirical effectiveness. In the later stages, the rewards are observed to converge to closely match the height of the robot, a meaningful and intuition-aligned metric, which enhances the agent's exploitation of its policy. This self-adaptive mechanism"}, {"title": "Ablation Study", "content": "We conduct ablation studies to investigate five key components of SASR: (1) sampling from Beta distributions versus using the deterministic success rates; (2) the retention rate 4, which controls the volume of historical information; (3) the bandwidth h of the kernel density estimation; (4) the number of dimensions M of RFF features; and (5) the weight factor A, which adjusts the scales of the shaped rewards.\nSampling from Beta distributions. We examine a variant of SASR that omits sampling from the Beta distribution and instead directly uses the success rate Ns(si)/(Ns(si) + NF(si)). In the early stages of learning, limited experience makes this success rate an inaccurate estimate of the true value. Using this overly confident fixed value can mislead the agent. Additionally, removing the process of sampling from the Beta distribution disrupts the agent's exploration encouraged by random rewards, leading to slower convergence. The results highlight the importance of sampling from the Beta distribution for effective learning.\nRetention rate 6. The retention rate controls the counts' estimation, subsequently impacting the Beta distribution and sampled rewards. We assess various retention rates. A high retention rate, such as $ = 1, preserving all samples, results in a densely populated and redundant experience pool, causing the Beta distribution to be prematurely overconfident, which can degrade performance. Conversely, a lower retention rate $ = 0.01 requires more iterations to accumulate sufficient samples, thereby slowing the convergence to meaningful rewards and affecting the agent's convergence speed. The results suggest that carefully balancing the retention rate is crucial, and promising future work could involve adaptive adjustment of the retention rate during the learning process.\nBandwidth h of Gaussian kernel. We evaluate different bandwidths h of the Gaussian kernel for density estimation,. Beyond fixed bandwidths, we also test a linearly decreasing bandwidth configuration (h : 0.5 \u2192 0.1), which represents progressively increasing confidence in KDE."}, {"title": "Network Structures and Hyperparameters", "content": "The agent utilizes simple multilayer perceptron (MLP) models for these networks. Given the use of stochastic policies, the policy network features separate heads to generate the means and standard deviations of the inferred normal distributions, which are then used to sample actions accordingly.\nWe have observed that SASR demonstrated high robustness and was not sensitive to hyperparameter choices."}, {"title": "Compute Resources", "content": "The experiments in this paper were conducted on a computing cluster. The computing time for the SASR algorithm in each task (running 1,000,000 steps) was approximately 6 \u00b1 2 hours."}, {"title": "Conclusion and Discussion", "content": "We propose SASR in this paper, a self-adaptive reward shaping algorithm based on success rates, to address the sparse-reward challenge. SASR achieves an exploration and exploitation trade-off mechanism by generating shaped rewards from evolving Beta distributions. Experiments demonstrate that this adaptability significantly enhances the agent's convergence speed. Additionally, the implementation of KDE and RFF formulates a highly efficient, non-parametric, and learning-free approach to deriving Beta distributions. This mechanism also provides a sound alternative to traditional count-based RS strategies, adapting effectively to continuous environments. Our evaluations confirm the superior performance of SASR in terms of sample efficiency and learning stability, and its robustness across various continuous-control tasks.\nWhile SASR is designed for environments with sparse rewards, in settings where the environmental rewards are dense, the introduction and maintenance of additional shaped rewards could be unnecessary. On the other hand, SASR operates on the premise that sparse rewards indicate task completion to align the success rate notion. This correlation is generally straightforward, yet it may not intuitively apply in all contexts. Exploring how to extend SASR to more general sparse and even dense reward scenarios presents a promising direction for further research. Moreover, the derivation of Beta distributions heavily relies on the samples in the success and failure buffers. Currently, our method does not capture the relationships and varying importance of different states within the same trajectory, making the algorithm relatively sensitive to the retention rate, a crucial hyperparameter for achieving promising performance. Therefore, developing an adaptive retention rate or better mechanisms for maintaining the two buffers are crucial avenues for future enhancements."}]}