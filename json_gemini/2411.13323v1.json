{"title": "Are Large Language Models Memorizing Bug Benchmarks?", "authors": ["Daniel Ramos", "Claudia Mamede", "Kush Jain", "Paulo Canelas", "Catarina Gamboa", "Claire Le Goues"], "abstract": "Large Language Models (LLMs) have become integral to various software engineering tasks, including code generation, bug detection, and repair. To evaluate model performance in these domains, numerous bug benchmarks containing real-world bugs from software projects have been developed. However, a growing concern within the software engineering community is that these benchmarks may not reliably reflect true LLM performance due to the risk of data leakage. Despite this concern, limited research has been conducted to quantify the impact of potential leakage.\nIn this paper, we systematically evaluate popular LLMs to assess their susceptibility to data leakage from widely used bug benchmarks. To identify potential leakage, we use multiple metrics, including a study of benchmark membership within commonly used training datasets, as well as analyses of negative log-likelihood and n-gram accuracy. Our findings show that certain models, in particular codegen-multi, exhibit significant evidence of memorization in widely used benchmarks like Defects4J, while newer models trained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage. These results highlight the need for careful benchmark selection and the adoption of robust metrics to adequately assess models capabilities.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) have become ubiquitous for various software engineering tasks. Assessing these models' abilities in context, beyond the basic evaluations typically performed upon release (e.g., on HumanEval [1]), benefits from realistic benchmarks that represent real-world software development tasks. Two significant such tasks are bug finding, through automated fault localization (FL) [2]; and bug fixing, through automated program repair [3] (APR). The Software Engineering community has released numerous bug benchmarks for evaluating success on these tasks, consisting of real bugs from open-source software projects. Notable such datasets include, for example, Defects4J [4] (Java) and BugsInPy [5] (Python); similarly, ML researchers recently introduced SWEBench [6]."}, {"title": "II. METHODOLOGY", "content": "Figure 2 overviews our methodology, which comprises three major components: (1) data collection (Section II-A, (2) model selection (Section II-B), and (3) evaluation (Section III)."}, {"title": "A. Data Collection & Filtering.", "content": "We select widely used bug benchmarks across common programming languages, gathering ground-truth files containing reference solutions for each bug fix. To provide a likely not-leaked dataset for comparison, we also curated a set of recent open-source repositories from GitHub.\nTo collect benchmarks of interest, we reviewed program-repair.org and selected highly starred benchmarks across three programming languages: BugsCpp [11], Defects4J [4], and BugsInPy [5]. We next included two recent datasets to serve as reference points: Gitbug-Java [12], which was recently published to address the leakage issue, and SWEBench-Lite [6], due to its rising popularity for code-related tasks [13], [14], [15]. We choose SWEBench-Lite instead of SWEBench due to computational constraints.\nWe collected the ground truth files for each patched bug. To reduce computational costs and prevent bias towards particular files, we removed files with more than 85% overlap to produce a unique sample set. Duplicate files appear when multiple bugs in a dataset occur in the same file (e.g., Defects4J in project Lang, Bugs 1 and 3 affect the same file). We kept the oldest file for consistency. We collected only fixed files since these correspond to the solutions a model may have memorized.\nWe also collected a new dataset of 3214 GitHub repositories written in Java and Python. We targeted repositories from 2024 to reduce the likelihood that current state-of-the-art LLMs have seen them. To narrow our selection to likely high-quality, indicative repositories, we applied a minimum threshold of 100 stars, a proxy for community interest and engagement. To ensure that we were mining novel datasets for LLMs, we applied the MinHash technique [16] with Locality Sensitive Hashing (LSH) [17] to filter repositories potentially overlapping with existing models' training data. Because of code duplication, some files in new 2024 repositories might be identical to those in older repositories. As a result, we collected all repositories created between 2022 and the end of 2023 with greater than 100 stars to exclude the repositories"}, {"title": "B. Model Selection.", "content": "We select a combination of models used for fault localization [25], program repair [26], and vulnerability detection [27].  These models are from families of well-known code-related models, including codegen-multi, the LLaMa 3.1 family, the Gemma 2 family, StarCoder, and Mistral. Note, we could not find detailed information about the training data for Mistral's models, apart from anecdotal evidence on public forums."}, {"title": "C. Leakage Detection", "content": "We follow strategies from prior work [9], [10] to evaluate models for potential data leakage. Membership operates at the repository level; Negative Log Likelihood and N-gram accuracy, at the file level (i.e., the compute model familiarity with a given file). In the bug datasets, these are the fixed (patched) files; in our novel dataset, they are randomly sampled files (Section II-A).\nMembership: If a repository is included in a widely used pretraining dataset, many models have probably seen that repository's code. We do not have direct access to, nor knowledge of, the training datasets for all evaluation models. However, we have partial information about the use of pre-training datasets, such as for open-source models, and closed-source models are likely to use them as well. We therefore assess membership via whether a benchmark's repositories are present in TheStack [28], a dataset of permissibly licensed source code in 358 programming languages intended for training and fine-tuning code-based models. Given its size and popularity, several models report having trained on it, such as StarCoder 2 [21]; other closed-source models are likely to also use it. We used the Am I in the Stack tool\u00b9 on each benchmark repository, across the several versions of TheStack.\nNegative Log Likelihood (NLL): NLL evaluates how closely an input sequence aligns with patterns the model has learned during training in terms of how \"natural\" the sequence appears to the model. If the model has seen a data point during training, we expect it to have a lower NLL on that point compared to unseen data. If the model has encountered a data point many times during training, NLL is expected to be particularly low (i.e., close to zero) compared to arbitrary code.\nTo compute NLL, we use the reference implementation publicly available on HuggingFace.\u00b2 Calculating the exact NLL for lengthy sequences is usually impractical because LLMs are trained on the limited context (moreover, we cannot fit an entire sequence in memory). Therefore, we split lengthy solution files into overlapping chunks, processed them individually, and combined them using a striding technique. We use strides of 512 tokens when a sequence does not fit into the model's context window.\nN-gram accuracy: N-gram accuracy measures the extent to which a model's output exactly matches a reference sequence at the level of n-grams (i.e., contiguous sequences of n tokens). High n-gram accuracy indicates that the model's output closely resembles the reference text, suggesting memorization. N-gram accuracy of 1.0 indicates the model can produce a sequence verbatim.\nWe follow prior work [9] and use 5-grams (5-grams strike a balance between compute efficiency and metric accuracy). Since most files cannot fit the context window, we use striding to cover the entire sequence. Following Xu et al. [9], we compute 5-grams from five uniformly distributed starting points per stride. For each starting point, we provide the model"}, {"title": "III. RESULTS", "content": "This section presents results assessing possible leakage of bug benchmarks in base models, using the metrics described in Section II-C: membership in TheStack (Section III-A), Negative Log Likelihood (Section III-B), and N-gram accuracy (Section III-C). We also perform a regression analysis of model characteristics, NLL, and N-gram accuracy (Section III-D) to better understand characteristics of models that influence data leakage. (Note that we subsequently discuss implications in Section IV, and limitations and threats to the validity of our experiments in Section V.)"}, {"title": "A. Membership", "content": "Table III shows benchmark membership in three versions of TheStack. The table excludes our new Java and Python data from 2024, as TheStack only includes data to 2023. Of all repositories, the new Gitbug-Java benchmark has the lowest membership. TheStack contains high proportions of Defects4J and SWEBench-Lite.\nWhile membership does not necessarily mean a model trained on TheStack has seen a specific fixed file (e.g., if a bug-fixing patch was applied after the dataset's cut-off date), the model may still be familiar with a project's source code. This familiarity could lead to higher-quality patches or results. This is not inherently problematic but is a critical factor to consider when assessing the model's potential for generalization."}, {"title": "B. Negative Log Likelihood", "content": "Figure 3 shows NLL values for families of open-source models, allowing us to examine trends in familiarity across benchmarks. Note that Negative Log Likelihood (NLL) depends on tokenization and architecture. This means we can only directly compare NLL values within model families.\nFigure 3 shows that Defects4J consistently has the lowest NLL across all models. This strongly suggests potential data leakage. This is particularly evident with codegen-multi, which has very low NLL (0.15) for Defects4J. This matches our observations in Figure 1 and suggests that codegen-multi has memorized the Defects4J solutions. We observe comparably low NLL (0.38) on the Gemma 2 27B model for Defects4J relative to other benchmarks and repositories.\nInterestingly, codegen-multi 6B exhibits low NLL (0.38) on SWEBench-Lite compared to other models, despite being the oldest model in our evaluation, trained on older data, and the fact that SWEBench-Lite was published recently. This is because the projects in the benchmark existed prior to benchmark publication, as we also saw in the membership analysis (Table III). Moreover, although SWEBench-Lite is a new benchmark, the bug fixes date as early as 2017.\nFor all other models, the NLL values are fairly consistent across non-Defects4J benchmarks. As expected, the new repositories we collected exhibit higher NLL compared to Defects4J, Bugs Cpp, BugsInPy, and SWEBench-Lite. Evaluation benchmarks are derived from prominent projects and may have been seen at a pretraining time multiple times, unlike our new repositories, which likely were not. Note, however, a potential confound, which is that our new repositories may be different in distribution compared to the models' training data, which may contribute to higher NLL."}, {"title": "C. 5-Gram Match", "content": "Figure 5 shows 5-gram match results, a complementary assessment of potential model memorization (full tables, including other models are in the appendix). Note that, due to differences in model vocabularies and tokenization, interpretation of 5-gram match (similar to NLL) can differ across models.\nDefects4J consistently exhibits the highest 5-gram match across all model families. Conversely, 5-gram accuracy on Gitbug-Java is relatively similar to that of new repositories for all models, which aligns with the expectation that most repositories in the Gitbug-Java benchmark were not included in pretraining data (as detailed in Table I). For example, both codegen-multi and Gemma 2 show significantly higher 5-gram match differences between Defects4J and new repositories (i.e., 34 percentage points and 14 percentage points, respectively). Moreover, codegen-multi achieves over 82% 5-gram match on Defects4J, strongly suggesting that it has likely memorized much of the benchmark's solutions.\nAs expected, new repositories generally exhibit lower 5-gram match across all models, with averages of 47% and 48% for Java and Python, respectively. These values happen likely due to the presence of common coding patterns [29]. Conversely, and in line with our NLL findings, codegen-multi shows a notably high 5-gram match on SWEBench-Lite, even though it is a recently published dataset, as it incorporates older data."}, {"title": "D. How Do Model Characteristics Influence Risk of Leakage?", "content": "To gain deeper insights, and using the data collected during the study, we estimate regression models for negative log-likelihood (NLL) and 5-gram accuracy. The regressions allow us to explore relationships and (potentially) causality that metrics alone cannot reveal. For example, while codegen-multi presents low NLL values and high 5-gram accuracy, it is unclear what drives this outcome. By employing regression analysis, we can identify factors such as training parameters and budget as significant influences on these metrics. Therefore, for NLL, we use a mixed-effects linear model, while for 5-gram accuracy (i.e., a probability), we use a mixed-effects logistic regression. As predictors, we include the model's number of parameters and number of tokens used during pretraining (i.e., training budget), which serves as a proxy for the unique token count. We acknowledge that reporting the exact number of unique tokens would be a more precise metric, but such data is often unavailable. We also account for potential variability caused by differences in the datasets by including dataset type as random effect."}, {"title": "IV. DISCUSSION", "content": "Our evaluation provides compelling evidence that data leakage is an especially significant issue for Defects4J (V1.5).This is evident from the lower NLL values, higher 5-gram accuracy. In particular, codegen-multi achieves 82% 5-gram accuracy on Defects4J, while both CodeLlama 7B and Gemma-2 27B attain 64%. Moreover, given that Defects4J is incorporated into the widely-used pretraining dataset (TheStack), with 80% membership, eliminating this leakage in future models is likely nearly impossible. We also observe similarly low NLL values in SWEBench-Lite for codegen-multi.\nNewer benchmarks BugsInPy and BugsCpp exhibit lower leakage risk. A smaller percentage of their repositories are indexed in the latest version of TheStack. While BugsInPy and BugsCpp exhibit slightly lower NLL compared to new repositories, their 5-gram accuracy and NLL values are not significantly different from newer, more-likely-unseen benchmarks like Gitbug-Java. This suggests that, for now, researchers can use these benchmarks with a relatively low risk of data leakage. However, as pretraining datasets continue to evolve, we recommend that researchers regularly assess TheStack membership, 5-gram match and NLL values of these benchmarks, especially compared to more recent data, to monitor and mitigate potential data contamination.\nWe also observe that newer models, e.g., the LLaMa 3.1 family, seem to have lower leakage (likely due to being trained on more data). Nonetheless, we still observed cases where LLaMa 3.1 can output solution files despite little context (e.g., Figure 6a).\nWe suggest researchers consider supplementing their evaluations with more recent benchmarks such as Gitbug-Java. Benchmarks like Gitbug-Java, which focus on recent bugs and patches, are less likely to have been included in pretraining datasets compared to established benchmarks. Leveraging these newer benchmarks can provide more reliable evaluations for assessing model's capabilities."}, {"title": "V. LIMITATIONS AND THREATS", "content": "Data Collection: Despite efforts to filter out older GitHub repositories when collecting new data, we anecdotally observed instances where files appeared to be adaptations of existing files (e.g., from 2018) Our filtering process may not have perfectly excluded legacy code. We moreover cannot guarantee that the new repositories are identically distributed compared to those in the benchmarks. To mitigate this issue, given that the repositories in the benchmarks tend to be highly recognizable projects, we applied a >100-star filter to argue that the selected projects are of comparable quality.\nTrain + Test Splits: For benchmarks like Defects4J (v1.5) and Bugs InPy, the patch files and buggy files are very similar (typically a patch involves only changing a small number of lines). LLMs may have only seen the train split of these benchmarks at pretraining time. This would result in high 5-gram match and low NLL on patch files, even if only buggy files were leaked. Nonetheless, we observed multiple cases where models output patched files verbatim. We mitigate this by looking at trends in NLL and 5-gram match rather than absolute numbers in our analysis.\nForgetting: The findings presented in this paper primarily address leakage in the context of base models. Empirical results show that models can \"forget\" portions of their pre-training data during fine-tuning [30]. That said, while full-scale model fine-tuning may reduce leakage risks, more recent fine-tuning strategies such as the addition of adapter layers often \"freeze\" pretrained weights. This practice can inadvertently increase the likelihood of data leakage. Furthermore, larger models exhibit a stronger tendency to memorize training data [31], suggesting that while fine-tuning may mitigate leakage for smaller models, it is more likely to exacerbate leakage concerns in larger models. We leave empirical assessment of these risks to future work."}, {"title": "VI. RELATED WORK", "content": "Large Language Models (LLM): LLMs have shown promise across a wide range of natural language [32], [33] and code generation tasks [34], [35], [36]. Current state of the art LLMs include open source models such as CodeGen, CodeLlama, LlaMa 3.1, Gemma and PhiCoder and closed-source models such as GPT-40, Claude 3.5, and Gemini 1.5. LLMs have shown state of the art performance in APR, suggesting patches to buggy code segments, detecting vulnerabilities and helping pinpoint buggy lines in a piece of code [25], [26], [37].\nDespite the widespread success of LLMs in APR tasks, there are still significant concerns regarding data leakage. This is prevalent for LLMs trained on large, publicly available code repositories, as these models might \"remember\" solutions from benchmark datasets, resulting in inflated performance.\nMeasuring Data Leakage: This task is still an open challenge. Three common ways to measure data leakage are perplexity or NLL, n-gram match, and model performance. Perplexity approaches [10], [9] quantify leakage by comparing loss across benchmarks. N-gram approaches [9], [38], prompt a model with context and measure the overlap of generated code with the ground truth. Finally, performance approaches [39], [40] measure the difference in performance between a benchmark and a transformed version of the benchmark. While we use techniques from the literature, we are the"}, {"title": "VII. CONCLUSION", "content": "In this paper, we measure data leakage risks across multiple widely used bug benchmarks and state-of-the-art open and closed-source models. Older benchmarks, especially Defects4J, exhibit higher memorization signals in models, particularly in smaller, older models with lower training budgets like codegen-multi 6B. Newer datasets such as Gitbug-Java, BugsInPy, BugsCpp show lower leakage risk, with similar NLL and 5-gram match to new 2024 repositories. We also find that newer models with higher training budgets display significantly lower risks of data leakage. Our findings suggest that researchers should consider both the models they use in evaluation and pair older benchmarks, such as Defects4J, with more recent benchmarks, such as Gitbug-Java, to minimize data contamination risk."}]}