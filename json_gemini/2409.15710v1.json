{"title": "Autotuning Bipedal Locomotion MPC with GRFM-Net for Efficient Sim-to-Real Transfer", "authors": ["Qianzhong Chen", "Junheng Li", "Sheng Cheng", "Naira Hovakimyan", "Quan Nguyen"], "abstract": "Bipedal locomotion control is essential for humanoid robots to navigate complex, human-centric environments. While optimization-based control designs are popular for integrating sophisticated models of humanoid robots, they often require labor-intensive manual tuning. In this work, we address the challenges of parameter selection in bipedal locomotion control using DiffTune, a model-based autotuning method that leverages differential programming for efficient parameter learning. A major difficulty lies in balancing model fidelity with differentiability. We address this difficulty using a low-fidelity model for differentiability, enhanced by a Ground Reaction Force-and-Moment Network (GRFM-Net) to capture discrepancies between MPC commands and actual control effects. We validate the parameters learned by DiffTune with GRFM-Net in hardware experiments, which demonstrates the parameters' optimality in a multi-objective setting compared with baseline parameters, reducing the total loss by up to 40.5% compared with the expert-tuned parameters. The results confirm the GRFM-Net's effectiveness in mitigating the sim-to-real gap, improving the transferability of simulation-learned parameters to real hardware.", "sections": [{"title": "I. INTRODUCTION", "content": "Bipedal locomotion control involves studying and developing algorithms that enable bipedal/humanoid robots to walk, balance, and navigate various human-centric terrains. This field is motivated by the desire to create robots that can operate and perform tasks that are challenging or dangerous for humans, and assist in areas such as industrial plants, rehabilitation, and disaster response missions.\nIn the past decade, there has been growing interest in studying bipedal locomotion, including footstep planning [1], [2], autonomous navigation [3], locomotion control [4]-[6], and safety guarantees [7], [8]. Among these studies, bipedal locomotion control remains the most fundamental topic of humanoid robotics research. Gradient-based optimizations have always been popular approaches for bipedal locomotion planning and control, including methods such as Whole-Body Control (WBC) [6], [9], Model Predictive Control (MPC) [10]\u2013[12], and direct Trajectory Optimization (TO) [13], [14]. As an early bipedal locomotion technique, the Zero-Moment-Point (ZMP) method combines with the Linear Inverted Pendulum Model (LIPM) for footstep planning with preview control [15]. Hybrid Zero Dynamics (HZD) later emerged as a favorable approach, integrating the robot's dynamics with impact modeling to achieve highly dynamic bipedal locomotion [16], [17]. More recently, force-based modeling has been employed in bipedal locomotion MPC and TO, utilizing models such as centroidal dynamics [4], [18] and single rigid-body dynamics [19]. These approaches enable the generation of compliant motions, enhancing the robot's ability to adapt to external forces and terrain perturbations. With advancements in efficient solvers and computational hardware, these optimization-based control designs have become increasingly sophisticated, featuring greater model complexity, multi-objective optimization, and higher-dimensional optimization spaces. Employing full-order dynamics modeling in finite-horizon optimization problems becomes more and more common [11], [20]. As a result, these complex controllers often require extensive, unintuitive, and even expert-level parameter tuning to achieve reasonable performance [21], which raises the barrier for newcomers and complicates the efficient development of new control algorithms.\nTo address the challenges in the parameter selection, previous works have explored Bayesian optimization (BO) to auto-tuning controller parameters used for bipedal locomotion and control [22]-[25]. Bayesian optimization is a model-free optimization algorithm and is suitable for finding global optima for black-box optimization functions. However, BO scales poorly with the dimensionality of the parameter space, where the search space grows exponentially. A recent work [26] addresses the scalability issue by applying domain knowledge for partitioning the parameter space and using alternating BO algorithms to iteratively update the parameters on the subspace. In contrast to the model-free BO approach for parameter learning, model-based autotuning methods [27]\u2013[31] offer more efficient parameter search. Using autodifferentiation, these methods can easily acquire the first-order information (compared with BO which only uses the zeroth-order information) that can efficiently guide the parameter update. Notably, the state-of-the-art model-based auto-tuning method, DiffTune [28], has achieved better tracking accuracy than BO in fewer trials in auto-tuning a nonlinear quadrotor control system.\nIn this work, we apply DiffTune to learn the optimal parameters of a linear locomotion MPC for bipedal walking with different trajectories and to demonstrate the feasibility of autotuning high-dimensional optimization problems for under-actuated legged locomotion. A dilemma in applying DiffTune (or other differentiable-programming-based learning methods) is the tradeoff between fidelity and differentiability of the dynamics model for legged robots. One option is the highly accurate full-order dynamics, which, though, is not easily differentiable due to its complexity, including discrete variables from ground contact models and the need for inverse kinematics solutions for swing leg control. Another option is the Single Rigid Body Model (SRBM), which offers differentiability but lacks fidelity due to simplifying assumptions. To tackle this dilemma, we take the SRBM for its differentiability and seek to improve its fidelity with a Ground Reaction Force-and-Moment Network (GRFM-Net). The GRFM-Net is designed to capture the discrepancies between the MPC command and the actual control effect in a data-driven manner, accounting for actuator dynamics, contact interaction, and leg transmission mechanics. It improves the simulation accuracy while maintaining the differentiability for parameter learning. We train the GRFM-Net with data collected from a high-fidelity (nondifferentiable) simulator due to the hardware and sensor limits in obtaining high-quality data. With the fidelity enhanced by GRFM-Net, DiffTune can find optimal parameters that improve the trajectory tracking accuracy when evaluated in hardware experiments. In contrast, DiffTune without GRFM-Net showed significantly worse performance, especially in more complex trajectories, highlighting the critical role of GRFM-Net in mitigating the sim-to-real gap and ensuring robust parameter learning across varying levels of task difficulty.\nOur contributions are summarized as follows: (i) We apply DiffTune to learn model-based locomotion MPC's parameters for bipedal walking, which leverages the first-order information to achieve more efficient parameter learning than commonly used Bayesian optimization that only uses zeroth-order information. (ii) We propose GRFM-Net, a data-driven method to enhance the fidelity of a differentiable low-fidelity simulator, which reduces the sim-to-real gap when deploying the simulation-learned parameters to a real physical system."}, {"title": "II. BACKGROUND", "content": "We first introduce the robot and the dynamics models used in this work. HECTOR V2 bipedal robot is a successor of the HECTOR humanoid introduced in [32]. HECTOR biped consists of 5-DoF legs with ankle pitch actuation, shown in Fig. 1. Standing at 70 cm and weighing 12 kg, the biped has maximum knee joint torque of 67 Nm.\nHECTOR's full-order dynamics equation of motion in generalized coordinates is described as follows:\n$$H(q)\\ddot{q} + C(q, \\dot{q}) = \\Gamma + J_i(q)^T\\lambda_i,$$  (1)\nwhere $H \\in \\mathbb{R}^{16\\times16}$ is the mass-inertia matrix and $C \\in \\mathbb{R}^{16}$ is the joint-space bias force term. The joint-space state $q \\in \\mathbb{R}^{16}$ includes float-base CoM position $p_c$, Euler angles $e$, and joint positions $q_j$; $\\Gamma = [0_6; \\tau_j]$ represents the actuation in the generalized coordinate; $\\lambda_i$ and $J_i$ represent the external force applied to the system and its corresponding Jacobian matrix; $i \\in \\{0, 1\\}$ indexes the left and right leg (foot).\nWhen using the full-order nonlinear dynamics models in finite-horizon optimal control (e.g., online MPC), one often faces computational challenges, which require linearization within solvers like Sequential Quadratic Programming and Differential Dynamics Programming. For real-time deployment, these methods also need specific tailoring [20], [33]. Thus, simplified dynamics still remain a practical choice for achieving reliable, high-frequency solutions in optimization-based control [21].\nOur prior work [19] proposed a force-and-moment-based single rigid-body dynamics model (SRBM) approach in bipedal locomotion MPC. The SRBM, illustrated in Figure 1, neglects the light-weight lower thigh and calf links and assumes the upper body and hips as a rigid body with a constant moment of inertia (MoI). The rigid body is actuated by world-frame ground reaction forces and moments (GRFM), denoted by $F_i$ and $M_i$, respectively. The equations of motion are described as\n$$m(\\ddot{p_c} + g) = \\sum_{i=0}^{1} F_i,$$ (2a)\n$$\\frac{d}{dt}(I\\omega) = \\sum_{i=0}^{1}\\{r_i \\times F_i + M_i\\},$$ (2b)\nwhere $m$ is the mass of the robot's rigid body; $g$ is the gravity vector; $I$ is the world-frame MoI; $\\omega$ is the CoM angular velocity vector; $r_i$ is the distance vector from CoM $p_c$ to the center of foot $i$."}, {"title": "B. SRBM-based Bipedal Locomotion MPC", "content": "Whenever a foot is in contact (i.e., stance foot), the SRBM-based MPC will solve for optimal GRFM for the corresponding foot to remain balanced. In the bipedal locomotion MPC, we linearize the SRBM and design a convex MPC formulation that can be solved through quadratic programming efficiently. We choose to include the robot CoM position $p_c$, linear velocity $\\dot{p_c}$, Euler angles $e$, angular velocity $\\omega$, and gravity vector $g$ as the optimization state variables $x = [e; p_c; \\omega; \\dot{p_c}; g]$ to linearize the SRBM dynamics in (2) and form the discrete-time state-space equation (4a) at time step $k$. The control input $u$ includes the 3-D GRFM of both feet and $u = [F_{0}; F_{1}; M_{0}; M_{1}]$. The finite-horizon optimization problem with $N$ steps can be written as\n$$\\min_{x, u} \\sum_{k=0}^{N-1} \\frac{1}{2} \\bigg[\\lVert x_k - x_k^{ref} \\rVert_Q^2 + \\lVert u_k \\rVert_R^2\\bigg] $$ (3)\nsubject to:\nDynamics:   $x_{k+1} = \\bar{A}_k x_k + \\bar{B}_k u_k$ (4a)\nFriction pyramid $\\mu F_{z,i,k} \\leq F_{x,i,k} \\leq \\mu F_{z,i,k}$  (4b)\n $\\mu F_{z,i,k} \\leq F_{y,i,k} \\leq \\mu F_{z,i,k}$\nForce limit:  $0 \\leq F_{z,i,k} \\leq F_{max}$ (4c)\nMoment X:  $M_{x,i,k} = 0$ (4d)\nLine foot:   $-l_h F_{z,i,k} \\leq M_{y,i,k} \\leq l_t F_{z,i,k}.$ (4e)\nThe objective of the problem is to drive the state $x$ close to the reference $x^{ref}$ and minimize the control input $u$. These objectives are weighted by diagonal matrices $Q \\in \\mathbb{R}^{15\\times15}$ and $R \\in \\mathbb{R}^{12\\times12}$; $\\mu$ is the ground friction coefficient; $F_{max}$ stands for the maximum vertical force a foot can exert on the ground. When the $i$-th leg is in the swing phase, $F_{max} = 0$. Constraints (4d-4e) follow contact wrench cone [34] and line-foot constraints [32], where $l_t$ and $l_h$ are robot toe and heel lengths, respectively.\nThe force-to-torque mapping of the stance leg $i$ is\n$$T_{stance} =  \\begin{bmatrix} F_i \\\\ M_i \\end{bmatrix} = \\begin{bmatrix} J_v^T \\\\ J_w^T \\end{bmatrix}$$ (5)\nwhere $J_v$ and $J_w$ are the velocity and angular velocity parts of the world-frame contact Jacobian matrices.\nThe swing foot is under an additional swing leg joint-space PD controller. The desired swing foot position $p_c^{des}$ is generated through heuristic policies [35]:\n$$p_c^{des} = p_c + \\frac{\\dot{p_c} \\Delta t}{2}$$ (6)\nwhere $\\Delta t$ is the swing duration.\nWe use an inverse kinematics solver to convert $p_c^{des}$ to joint-space commands for PD control law and torque command $\\tau_{swing}$. The resulting joint torque commands for stance leg $\\tau_{stance}$ and swing leg $\\tau_{swing}$ are then used to actuate the bipedal robot."}, {"title": "C. Controller auto-tuning and DiffTune", "content": "We briefly review controller auto-tuning and DiffTune for a general system in the sequel. Consider a discrete-time dynamical system\n$$x_{j+1} = f(x_j, u_j), x_0 \\text{ given,}$$ (7)\nwhere $x_j$ and $u_j$ are the state and control, respectively, with appropriate dimensions. The control is generated by a feedback controller that tracks a reference state $x^{ref}$ such that\n$$u_j = h(x_j, x^{ref}, \\theta),$$ (8)\nwhere $\\theta \\in \\Theta$ denotes the controller's parameters, and $\\Theta$ represents a feasible set of parameters that can be analytically or empirically determined for a system's stability.\nThe controller's auto-tuning (or parameter learning) task adjusts $\\theta$ to minimize an evaluation criterion, denoted by a loss function $L(\\cdot)$, which is a function of the desired states, actual states, and control actions over a time interval of length $T$. An illustrative example is the quadratic loss of the tracking error and control-effort penalty, where $L(x_{1:T}, x_T^{ref}, u_{0:T-1}; \\theta) = \\sum_{j=1}^{T} \\lVert x_j - x^{ref}_j \\rVert^2 + \\sum_{j=0}^{T-1} \\lambda \\lVert u_j \\rVert^2$ with $\\lambda > 0$ being the penalty coefficient. We will use the short-hand notation $L(\\theta)$ for conciseness in the rest of the paper.\nNote that we distinguish between the time indices used in auto-tuning ($j$ and $T$ for closed-loop systems) and those in MPC ($k$ and $N$ for open-loop MPC solution). For the closed-loop system, at time $j$, the instantaneous state $x_j$ is set to initialize $x_{k=0}$ in (4). After solving the MPC problem, the initial optimal control action $u_{k=0}$ is applied as $u_j$ for the closed-loop system. Additionally, the horizon $T$ of auto-tuning (for performance evaluation) is generally much longer than the MPC's planning horizon $N$.\nTo summarize, we formulate the controller auto-tuning as the following parameter optimization problem\n$$\\min_{\\theta \\in \\Theta} L(\\theta)$$\nsubject to $x_{j+1} = f(x_j, u_j), x_0 \\text{ given, }$   (P)\n$u_j = h(x_j, x_j^{ref}, \\theta), j \\in \\{0, 1, ..., T-1\\}.$ \nIf the loss function $L(\\cdot)$, dynamics (7), and controller (8) are all differentiable, then we are able to apply DiffTune to learn the parameters and minimize the loss. DiffTune uses a projected gradient descent algorithm [36] to update the parameters iteratively $\\theta \\leftarrow P_\\Theta(\\theta - \\beta \\nabla_\\theta L)$, where $\\beta$ is the learning rate and $P_\\Theta$ is the projection operator that projects the updated parameter value to the feasible set $\\Theta$. To obtain the gradient $\\nabla_\\theta L$, we start with the chain rule:\n$$\\nabla_\\theta L = \\sum_{j=1}^T \\frac{\\partial L}{\\partial x_j} \\frac{\\partial x_j}{\\partial \\theta} + \\sum_{j=0}^{T-1} \\frac{\\partial L}{\\partial u_j} \\frac{\\partial u_j}{\\partial \\theta}$$\nThe gradients $\\partial L/\\partial x_j$ and $\\partial L/\\partial u_j$ can be determined once $L$ is chosen, and we use sensitivity propagation to obtain the sensitivity states $\\partial x_j / \\partial \\theta$ and $\\partial u_j / \\partial \\theta$ by:\n$$\\frac{\\partial x_{j+1}}{\\partial \\theta} = \\nabla_{x_j} f \\frac{\\partial x_j}{\\partial \\theta} + \\nabla_{u_j} f \\frac{\\partial u_j}{\\partial \\theta},$$ (10a)\n$$\\frac{\\partial u_j}{\\partial \\theta} = \\nabla_{x_j} h \\frac{\\partial x_j}{\\partial \\theta} + \\nabla_{\\theta} h, $$ (10b)\nwith $\\partial x_0 / \\partial \\theta = 0$. The notations $\\nabla_{x_j} f, \\nabla_{u_j} f, \\nabla_{x_j} h$, and $\\nabla_{\\theta} h$ refer to the Jacobians $\\partial f/\\partial x, \\partial f/\\partial u, \\partial h/\\partial x$, and $\\partial h/\\partial \\theta$ is evaluated at the state $x_j$ and control $u_j$. Since the dynamics are known, one can access $\\partial f/\\partial x$ and $\\partial f/\\partial u$ using autodifferentiation tools like CasADi or PyTorch. The control relevant Jacobians $\\partial h/\\partial x$ and $\\partial h/\\partial \\theta$ are from differentiating the MPC control policy, which is detailed in [30]."}, {"title": "III. DIFFTUNE SETUP FOR LOCOMOTION MPC", "content": "In this section, we apply DiffTune to learn the optimal parameters of HECTOR's locomotion MPC controller under the scenarios of walking with different trajectories. We will introduce the choice of loss function and differentiable dynamics in the sequel."}, {"title": "A. Loss function design", "content": "We use the following loss function to guide the parameter optimization by DiffTune:\n$$L = \\alpha_1 L_{Eul} + \\alpha_2 L_{pos} + L_{ctrl},$$  (11)\nwhere $L_{Eul} = \\sum_{j=1}^{T} \\lVert e_j - e_j^{ref} \\rVert^2$ and $L_{pos} = \\sum_{j=1}^{T} \\lVert p_{c,j} - p_{c,j}^{ref} \\rVert^2$ accounts for the tracking error in the float-base attitude and position, respectively; $L_{ctrl} = \\sum_{j=1}^{T} \\lVert u_j - u_{j-1} \\rVert^2$ accounts for the smoothness of consecutive control actions. The weights $\\alpha_1$ and $\\alpha_2$ are hyperparameters to tradeoff between tracking performance and control smoothness (acquiring better tracking performance tends to require more violent control inputs, which can lead to serious jittering), as well as balancing the losses of different units and orders of magnitude."}, {"title": "B. Choice of dynamics for differentiable programming", "content": "A seemingly ideal choice of dynamics to be applied for parameter learning (i.e., $f(\\cdot)$ in problem (P)) via differentiable programming is the full-order dynamics (1), since it is a high-fidelity description of the robot's full-body motion with respect to the actuations. However, it is not a suitable choice with differentiable-programming-based learning for the following two reasons: (i) The full-order dynamics (1) models a floating-base rigid-body system with actuation purely in the joint space (i.e., joint torques). When modeling a legged robot, this choice of control interface additionally requires the ground contact model to simulate interactions (i.e., constraint forces) at contact points, which introduces discrete variables into the dynamics and hence violates the assumption of differentiable dynamics by differentiable programming (and DiffTune). (ii) The goal of this work is to learn the optimal control parameters for the MPC stance controller. If one applies the full-order dynamics (1), then the swing leg's motion and joint control are also needed for propagating such dynamics. The swing leg requires a separate inverse kinematics solution to map foot positions into joint space commands, where the solution is often obtained through iterative programming, whose differentiation requires unrolling [37]. Hence, this procedure complicates the computation. These limitations indicate that the full-order dynamics (1) is not a suitable choice for parameter learning for a stance controller with differentiable programming.\nFollowing the reasons listed above, the SRBM (2) is well-suited for differentiable learning: the dynamics itself is differentiable, and the control interface (using GRFM) matches the locomotion controller's output. However, since the SRBM itself is simplified (with underlying assumptions on the negligible weights of lower thigh and calf links and constant MoI of the upper body and hips), performing iterative parameter learning with this dynamical model in DiffTune will introduce distribution shifts [28], which reduces the applicability of the learned parameters as the number of iterations grows.\nTo tackle these issues, we introduce a Ground Reaction Force-and-moment Network (GRFM-Net) to enhance the fidelity of the bipedal SRBM in a data-driven manner, thus allowing for the augmented model to closely approximate the physical system. For the sake of generality, we use the following control-affine system to represent the commonly used dynamical model in simulation:\n$$x_{j+1} = f(x_j) + g(x_j)u_j, x_0 \\text{ given.}$$ (12)\nThe nominal dynamics (12) govern the major evolution of the system, where $u_j$ denotes the control action produced by a controller. However, the nominal model (12) cannot fully capture the evolution of the states on a physical robot due to factors that are hard to model or parameters that may be challenging to identify, which constitutes a major cause for the sim-to-real gap. Among them, one of the major sources of the sim-to-real gap is caused by the discrepancy between commanded control action $u$ (as the output of the controller) and control effect $\\bar{u}$ (as the actual force/moment/actuation that drives the system). Specifically, in the context of legged robots the control effect $\\bar{u}$ is altered from the commanded control action $u$ due to the physics of the low-level actuators and motors, as well as complex contact physics with the ground. We treat the control effect $\\bar{u}$ as a mapping $\\phi(\\cdot)$ of the control action and its history, i.e., $\\bar{u}_j = \\phi(u_{j-w:j})$, where $w$ is the length of historic control actions that influence the current control effect. This results in the augmented dynamics that describe the evolution of the actual system state $x$, i.e.,\n$$x_{j+1} = f(x_j) + g(x_j) \\phi(u_{j-w:j}), x_0 \\text{ given.}$$ (13)\nDue to the heavily non-linear and time-varying properties of the effects we mentioned above, modeling how the control command is turned into the real control effect is not an easy task. In terms of the HECTOR robot, when executing the MPC-generated GRFM (corresponding to $u$), due to the unavoidable mechanical transmission backlash and frictions, the actual GRFM exerted from the ground to the robot (corresponding to $\\bar{u}$) has noticeable variations compared to the MPC-commanded GRFM control action. Therefore, we propose to learn the mapping $\\phi$ in a data-driven approach, where a deep neural network (DNN) $\\Phi$ can approximate the mapping $\\phi$, and we name the network by GRFM-Net. More importantly, the DNN can be put back into the simulator (for differentiable learning), which will elevate the fidelity of the simulation by augmenting the actuation model. The augmentation yields a simulated system that behaves more accurately like the physical system given the same control commands (than the original simulated system). Moreover, existing key differentiable features of the nominal dynamics (12) can be preserved and leveraged for DiffTune. Our idea of GRFM-Net is from the actuator network (AN) [38]. However, unlike AN, which only considers a short chain of discrepancy between motor command (position and speed) and motor output (torque), our mapping from the MPC-produced GRFM to the actual GRFM contains a longer chain, including contact interaction, mechanical system behaviors, and the low-level actuators' own dynamics."}, {"title": "C. Data collection and training of the GRFM-Net", "content": "Due to the limited sensor precision and hardware design challenges within constrained spaces, most legged robots do not have accurate force sensors at the foot. To mitigate the issue in data collection, we built a MATLAB/Simulink-based high-fidelity simulator (HFS) equipped with accurate ground effect sensors at the ground contact points. In HFS, the bipedal robot dynamics model was built with the Simscape Multibody library and was represented as a multi-link rigid-body system with elastic ground contact. It is capable of simulating mechanical properties such as backlash, joint damping, and linkage friction to offer more realistic mechanical behavior. We collect the data pairs of control commands and control effects (both 12-dimensional vectors, including 3-dimensional forces and moments at both feet) from HFS under various reference trajectories, making the GRFM measurements a good representative of the real ground reaction (and the label of GRFM-Net training).\nWe separate force and moment data as they have different physical units and train two GRFM-Nets in the same training setting. Each GRFM-Net is a multi-layer perception with 18 inputs (history length $w = 3$), three hidden layers (each with 64 neurons), and six outputs. We use the softsign activation function [38] and conduct layer normalization. We use Adam optimizer and the learning rate is set to 0.001. To avoid overfitting, we use an L2 regularization with a weight of 0.001. The two GRFM-Nets reach the plateau after training 200 epochs, both in training and validation losses. The final MSE loss for force and moment GRFM-Nets are 12.78 and 0.55, respectively."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "We first examine the improvement in fidelity by the GRFM-Net. We test the straight-line walking in the HFS, recording force and moment of (i) MPC output, (ii) GRFM-Net output, and (iii) HFS sensor readings. It can be observed that the gap between sensor readings and GRFM-Net output is smaller than the gap between sensor readings and MPC output, indicating that GRFM-Net is effective in capturing the discrepancies between the control commands and control effects and hence improving simulation fidelity."}, {"title": "B. Hardware test results", "content": "We select three trajectories \u2013 straight line, C-shape curve, and S-shape curve with increasing levels of control challenge for testing the effectiveness of GRFM-Net-involved DiffTune. For each trajectory, we test HECTOR's walking performance under four sets of MPC parameters, including those found by DiffTune with GRFM-Net (named by \"the proposed\") and three baselines: (i) nominal untuned MPC parameters (for showing the tracking performance improvement by the proposed compared with initial parameters), (ii) expert-tuned MPC parameters (for comparing the proposed parameters with best manually tuned parameters), and (iii) parameters by DiffTune without GRFM-Net (as ablation to examine the role of GRFM-Net in addressing the sim-to-real gap). For each trajectory, we ran DiffTune with and without GRFM-Net for ten iterations, with parameters initiated from the nominal untuned MPC parameters. We used the following hyperparameters in all cases: the weights in the loss function are $\\alpha_1 = 1 \\times 10^5$ and $\\alpha_2 = 2 \\times 10^5$; the learning rates are $\\beta_Q = 0.05$ and $\\beta_R = 0.08$ for $Q$ and $R$ matrices in the MPC cost function. At test time, we deployed the four sets of MPC parameters on the HECTOR V2 bipedal robot to walk each trajectory and recorded the robot's position, pose, and control actions at 1000 Hz.\nThe three trajectories are defined as follows:\nTrajectory 1: Straight line\n$$\\dot{v}_x^{des} = 0.5 \\text{ m/s},  \\dot{\\omega}_z^{des} = 0.$$ (14)\nTrajectory 2: C-shape curve\n$$\\dot{v}_x^{des} = 0.25 \\text{ m/s},  \\dot{\\omega}_z^{des} = 0.25\\pi \\text{ rad/s}$$ (15)\nTrajectory 3: S-shape curve\n$$\\dot{v}_x^{des} = 0.5  \\text{ m/s}$$  \n$$\\dot{\\omega}_z^{des} = \\begin{cases}  0.5\\pi  \\text{ rad/s},  \\ 0 < t \\le 2  \\text{ s} \\\\ -0.5\\pi  \\text{ rad/s},  \\ 2 < t \\le 4 \\text{ s} \\end{cases}$$ (16)\nThe illustration of the three trajectories is shown in Fig. 3, as well as the plots for pose and position tracking by the different MPC parameters. The losses are shown in Table I.\nDiffTune with GRFM-Net found MPC parameters that reduce the weighted total loss $L$ by 68.7%, 29.7%, 41.9% compared with the nominal untuned parameters and 19.4%, 40.5%, 17.0% compared with the expert-tuned parameters, demonstrating the optimality of the learned parameters. In terms of position and attitude tracking, the parameters learned by DiffTune and GRFM-Net achieve better accuracy than the expert-tuned parameters, with a marginally higher control loss $L_{ctrl}$ (with increase no more than 9%) than the latter. Note that the parameters by DiffTune without GRFM-Net resulted in a large total loss, especially when running challenging trajectories (C- and S-shape curves). This result confirms that the sim-to-real gap can be a serious issue, leading to unsatisfactory performance on the physical system, whereas the GRFM-Net can successfully mitigate the gap by augmenting the simulator to closely approximate the physical system."}, {"title": "V. CONCLUSIONS", "content": "In this work, we applied DiffTune to the HECTOR robot to optimize locomotion MPC controller parameters for bipedal walking across multiple trajectories. We addressed the low-fidelity limitations of the differentiable SRBM by introducing the GRFM-Net, which enhances model fidelity by mapping MPC control actions to actual control effects. Trained using data from a high-fidelity simulator, the GRFM-Net's fidelity improvement was verified when integrated with the SRBM. Experiments with the HECTOR robot on various trajectories show that DiffTune, combined with GRFM-Net, improves tracking accuracy and maintains smooth control, reducing total loss by up to 40.5% compared to expert-tuned MPC parameters. These results highlight the critical role of GRFM-Net in closing the sim-to-real gap for differentiable-programming-based parameter learning, especially in challenging locomotion tasks. Furthermore, the proposed data collection and training framework is generalizable to other platforms, rendering more efficient learning while holding sufficient fidelity.\nOngoing work involves improving the robustness of Diff-Tuned parameters under disturbances during deployment. Our future work will focus on expanding DiffTune to enable online parameter tuning and applying it to full-order and contact dynamics for more generalized usage. Additionally, we look to develop a pipeline for auto-tuning MPC parameters across multiple trajectories to improve generalization."}]}