{"title": "Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic Evaluation Framework for LLMS", "authors": ["Wanying Wang", "Zeyu Ma", "Pengfei Liu", "Mingang Chen"], "abstract": "While various vertical domain large language models (LLMs) have been developed, the challenge of automatically evaluating their performance across different domains remains significant in addressing real-world user needs. Current benchmark-based evaluation methods exhibit rigid, purposeless interactions and rely on pre-collected static datasets that are costly to build, inflexible across domains, and misaligned with practical user needs. To address this, we revisit the evaluation components and introduce two definitions: Benchmark+, which extends traditional QA benchmarks into a more flexible \u201cstrategy-criterion\" format; and Assessment+, which enhances the interaction process for greater exploration and enables both quantitative metrics and qualitative insights that capture nuanced target LLM behaviors from richer multi-turn interactions. We propose an agent-based evaluation framework called TESTAGENT, which implements these two concepts through retrieval augmented generation and reinforcement learning. Experiments on tasks ranging from building vertical domain evaluation from scratch to activating existing benchmarks demonstrate the effectiveness of TESTAGENT across various scenarios. We believe this work offers an interesting perspective on automatic evaluation for LLMs.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) (Brown et al., 2020; Bubeck et al., 2023) have demonstrated remarkable capabilities across various tasks, reshaping numerous industries and giving rise to many vertical domain LLMs and corresponding applications (Bommasani et al., 2021; Huang et al., 2024). Nonetheless, due to their auto-regressive generation manner, LLMs exhibit high output variability. Effective evaluation can provide a better understanding of their capabilities and limitations.\nHowever, defining \"effective evaluation\" requires us to listen to the voices of those involved in the LLMs industry (See Appendix D for details). To meet the evaluation challenge and understand practical demands and bottlenecks, we conducted online surveys, on-site visits, and document reviews with 50 companies in the LLM industry (Peng et al., 2024; Guo et al., 2023)."}, {"title": "2 Methodology", "content": "In this section, we elaborate on our improvements and reflections on Benchmark and Assessment in Section 2.1 and Section 2.2, respectively. They are implemented through TESTAGENT. Additionally, methods for activating existing static benchmarks are discussed in Section 2.3."}, {"title": "2.1 Revisiting Benchmark", "content": "Commonly used benchmarks are typically composed of static \u201cquestion-answer\u201d pairs (Q, A), structured as follows: $B := \\{(Q^i, A^i)\\}_{i=1}^N$, where N represents the total number of questions in the benchmark and i denotes the i-th pair. A wide range of benchmarks have been developed to evaluate the performance of LLMs on general tasks. These benchmarks typically require the target LLM to generate short responses to predefined questions, such as multiple-choice answers or brief text, following a relatively fixed evaluation process. However, general benchmarks are insufficient for vertical domains, as they fail to address the specific needs of specialized areas. Furthermore, scenarios in these vertical domains cannot be effectively assessed through a limited number of fixed interactions. For example, in medical consultation scenarios, developers require LLMs to handle multiple rounds of interactions with the user while maintaining a stable consultation strategy for each condition.\nTo address these challenges, we expand the form of benchmarks to \u201cstrategy-criterion\" pairs, enhancing the benchmark's dynamism. Specifically, we define the benchmark as follows:\n$B^*(D;T; M; K) := \\{(S^i(\\cdot), C^i(\\cdot))\\}_{i=1}^N$,\nwhere\n$S^i = \\{S_j^i | j = 1,2,\\dots, n_i\\}$,\n$S_j^i = \\begin{cases}K(D;T) & \\text{if } j = 0\\\\ Pr(H_{j-1}; D;T) & \\text{if } j > 0\\end{cases}$,\n$C^i = K(D;T; S^i)$.\nHere, $S^i$ represents the strategy corresponding to the i-th element in $B^*$. The strategy $S^i$ is an abstraction of questions sequence, comprising $n_i$ specific question distributions $S_j^i$ that are generated based"}, {"title": "2.1.1 Topic-based Initial Question Generation", "content": "In different domain scenarios, there may be expectations for LLMs to handle specific topics, such as \"disease consultation\" and \"department introduction\" topics in the medical scenarios. These specific demands are crucial for generating test cases that align with user needs in practical applications. Consequently, the current research question can be summarized as follows:\nRQ1: What should we discuss?\nWe employ RAG technology to address RQ1. First, a domain-specific knowledge base D will be fed into the kernel model K for understanding, allowing for the extraction of key knowledge points to form an RAG system. Then, we use prompt engineering to frame the user's topics of interest T, generating questions $Q_0$ that are more aligned with user concerns. These questions not only include direct inquiries but also encompass contextual scenarios, similar to role-playing, making them closer to real downstream task scenarios. The detailed prompt can be found in the Appendix G. Note that $Q_0$ only determines the initial question in a"}, {"title": "2.1.2 Two-Stage Criteria Generation", "content": "We also need to establish evaluation criteria. In other words, we must address the following research questions:\nRQ2: How should the question be evaluated?\nWe construct evaluation criteria C from a general topic to a question-refined level. It first locates the category of the topic, then searches and extracts the relevant documents for the question, thus helping to identify user-specific goals when interacting with the model. The motivation behind this procedure is to enhance the controllability of criterion generation and ensure that the question-relevant criteria align with the patterns of specific topics.\nReasonable criteria should include two aspects: the first relates to content-specific criteria, denoted as G, while the second pertains to text-related criteria E, such as fluency and whether the tone is human-like. Therefore, the pattern of the criteria can be formalized as follows:\n$C(D;T; S_0) \\rightarrow \\{G, E\\}$."}, {"title": "Metric", "content": "Having the question and criteria, we can now evaluate the answer from the target model M. To achieve quantitative evaluation, we introduce an evaluation metric $R_C$, defined as\n$R_C(A) := K(C; A)\n=\\frac{1}{n}\\sum_{i=1}^n 1(G_i, A) + \\frac{1}{m} \\sum_{j=1}^m L(E_j, A)$,\nwhere n is the total number of key information in criteria, m is the full marks of the textual characteristics index, $G_i$ and $E_j$ denotes the i-th content-specific criteria and the j-th text-related criteria, separately. A represents the answer from the target model M. 1(x, y) is the indicator function, returning 1 if y includes x correctly and 0 otherwise, L(x, y) denotes the integer score(from 1 to 10), evaluating how well y satisfies x based on the relevant context.\nThe score for each element in {G, E} \u2208 C is provided by K. While this approach may slightly reduce reliability, it presents minimal difficulty for K, as each score targets a specific aspect. We conducted a meta-evaluation specifically targeting its consistency; see Appendix B for details.\nIn practice, questions will be generated dynamically following the strategy $S^i$, while the evaluation criteria remain consistent within an interaction sequence i."}, {"title": "2.2 Revisiting Assessment", "content": "General benchmarks assess target LLM's capabilities using specific metrics, such as accuracy in multiple-choice questions (MMLU (Hendrycks et al., 2020)), success rates in planning tasks (MINT (Wang et al., 2024b)), and general semantic aspects like coherence and relevance. Although these metrics are widely used, the target LLM M interacts with each test case only once or a few times, however, in real-world, users often often have multiple interactions with the LLM around a topic. Therefore, these metrics fail to capture the deeper performance of M. In this section, we aim to implement multi-turn dialogues in assessment with a stronger emphasis on exploration and obtaining rich dialogue logs. Based on these dialogues, more evaluation metrics that fully capture the dynamic information can be developed."}, {"title": "2.2.1 RL-based Strategy Generator", "content": "As mentioned above, we attempt to address the following question:"}, {"title": "RQ3: How can we make the testing process more exploratory?", "content": "Initial questions have been generated in Section 2.1.1. The next step is to generate follow-up questions. Recall the definition of interaction history in (1): $H_n = \\{(Q_k, A_k, K(C; A_k))\\}_{k=0}^n$, where each question $Q_{j+1}$ is sampled from the distribution $S_{j+1} = Pr(\\cdot|H_j; D;T), (j = 0, \\dots, n)$, based on its interaction history. This process can be formulated as a sequential decision-making problem and trained using reinforcement learning (RL) with a policy $\\pi_\\theta$, such that $Q_{j+1} := \\pi_\\theta(H_j; D;T)$.\nHowever, treating raw text questions $Q_{j+1}$ as actions would result in an excessively large action space. To facilitate the RL training, we attempt to generalize $Q_{j+1}$ through a finite set of actions $A = \\{a_1,a_2,...,a_m\\}$, and define the policy as $\\pi_\\rho : H \\rightarrow A$, where H is the set of all possible histories. As a result, the question is derived from the action space A, knowledge base space D and topic space T via the kernel model mapping k : $A\\times D\\times T \\rightarrow Q$, where Q is the space of possible queries. The overall function f is expressed as:\n$Q_{j+1} := f(H_j; D; T)\n= K(\\pi_\\theta(H_j); D; T)$,\nwhich indicates that $Q_{j+1}$ is generated by first selecting an action a through $\\pi_\\theta(H_j)$ and subsequently applying a to the kernel model K.\nIn practical scenarios, users may ask follow-up questions to target LLM M when additional information is needed. Conversely, if the M's responses are contradictory or incorrect, a natural behavior is to challenge M by asking why. To this end, we train an RL model to choose whether to ask follow-up questions or challenge the answers from the target model M. Now, we present the design of the state space, action space, and reward."}, {"title": "State", "content": "The state space is comprised of the following information:\n1. Question embedding. To better capture the question type and the difficulty, which can influence decision-making, the initial question is embedded into the state representation instead of using the raw text directly.\n2. Score. The score and its variation help assess the degree of knowledge acquisition. Therefore, we incorporate the current score, score variation, and the cosine similarity between consecutive responses into the state space."}, {"title": "Action", "content": "The action space A includes asking follow-up questions ($a_f$) and challenge further ($a_c$)."}, {"title": "Reward", "content": "The reward is defined as\n$r_t = |R_C(A_t) - R_C(A_{t-1})\n+ (1-\\text{cos}(A_t, A_{t-1}))|$\nwhere $A_t$ and $A_{t-1}$ are the summarized current and previous responses, facilitating the comparison of information change. Intuitively, a large reward is assigned when more information is obtained from the target model or when the model produces contradictions, while a smaller reward is given in other cases. The goal is to extract the maximum information embedded within the target model, enabling rapid exploration of the model's boundaries.\nWe use the Proximal Policy Optimization (PPO) (Schulman et al., 2017) as the RL algorithm. The trained policy provides guidance for multi-turn interactive evaluation, addressing RQ3. RL policy offers dynamic alternative evaluation in a realistic paradigm rather than the static question-answer format of common benchmarks, enabling a more comprehensive assessment of model performance."}, {"title": "2.2.2 Qualitative and Quantitative Evaluation", "content": "The dynamic interaction with the target LLM M can be implemented with the strategy generator. Compared to previous evaluation procedures, such dynamic exploration allows us to delve deeper into a specific topic, thus performing more comprehensive qualitative and quantitative evaluations. Now, we need to address the following research question:"}, {"title": "RQ4: What insights can we derive from these records?", "content": "For quantitative evaluations, the last-turn score and the score variance before and after the challenge question serve as a basis for assessing the professionalism and stability of the target LLM. To perform more in-depth qualitative assessments, we leverage the capabilities of the kernel model K to conduct a comprehensive evaluation of the target model, examining stability and professionalism based on historical dialogues. Thus, we address RQ4 by providing a thorough evaluation that spans both structured metrics and unstructured performance aspects."}, {"title": "2.3 Activating Existing Benchmarks", "content": "For vertical domain scenarios, our approach has automated the processes of constructing test datasets, interaction, and evaluation. However, we still need to address the final research question:"}, {"title": "RQ5:How can we introduce dynamism to the existing benchmarks?", "content": "Existing benchmarks usually employ predefined metrics such as Exact Match, F1-Score, and so on to assess model accuracy. However, the flexible and varied expressions of LLMs make it challenging for these metrics to assess the alignment between model-generated long texts and ground-truth answers. These metrics may fail to capture nuanced correspondences and can even result in misjudgments. More importantly, this is a static dataset, whereas, in real-world reading comprehension tasks, users often pose multiple questions and engage in follow-up inquiries. Additionally, the benchmark lacks the ability to challenge responses, making it difficult to evaluate the stability of the LLM's performance.\nWe achieve dynamic evaluation with minimal adjustments to our framework. For static questions benefiting from the existing question-answer pairs in the benchmark, there is no need to generate additional questions or evaluation criteria. We still adopt the interactive method mentioned above, but the reward is determined by comparing model-generated answers with ground-truth answers in the benchmark to assess the correctness of the model's responses.\nSpecifically, we use the classic reading comprehension benchmark, SQUAD (Rajpurkar et al., 2016), as an example to demonstrate our dynamic extension process. In SQUAD, given a context paragraph, multiple independent questions and corresponding answers are provided. We autonomously select whether to ask a follow-up question or challenge the given answer, thereby enabling the integration of existing questions and the dynamic extension of the benchmark."}, {"title": "3 Experiments", "content": "In this section, we conduct experiments aimed at thoroughly evaluating the TESTAGENT. We explore both automatic evaluation in vertical domains from scratch and the activation of existing benchmarks in general domains. First, we conducted a meta-evaluation of the key components of TESTAGENT to validate its effectiveness. Subsequently, we report the main evaluation results across diverse models comprising both open-source and closed-"}, {"title": "3.1 Experiments Setup", "content": "Models and Domains. We consider government and medical tasks in the vertical domain and reading comprehension tasks in the general domain. To perform evaluation, we adopt DeepSeek-V2.5 (DeepSeek-AI, 2024) as the kernel model. The target model we assess in the medical domain including ZhongJing (Yang et al., 2024), Bianque (Chen et al., 2023), DoctorGLM (Xiong et al., 2023), DeepSeek-V2.5 and GPT-40 (Achiam et al., 2023). For the government and the reading comprehension tasks, we evaluate GPT-40 and DeepSeek-V2.5. In the RL module, the policy and value network are both 3-layer MLPs with 256-128 hidden states, trained for 6000 steps.\nKnowledge Bases. In the medical domain, we collect common disease documents from OpenKG (Peng et al., 2021) and department introductions from CareGPT (Wang et al., 2023) to build the knowledge base. The disease consultation and de-"}, {"title": "3.2 Meta-Evaluation", "content": "We conducted meta-evaluations of both the benchmarks generated by TESTAGENT and the scoring of the kernel model. In various scenarios, we submitted a subset of interaction records to human experts(See Appendix D for details), who rated the performance across multiple aspects. Detailed information can be found in Appendix B."}, {"title": "3.3 Domain Evaluation", "content": "We report the main result of vertical and general domain evaluations from quantitative and qualitative perspectives in Table 1. The quantitative metrics encompass three key aspects: last-turn $R_c$ summation (Score), as well as the mean $\\Delta (\\mu)$ and"}, {"title": "3.3.1 Vertical Domain", "content": "For different vertical domains, benchmarks can be rapidly constructed and dynamically evaluated by building a domain-specific knowledge base through the TESTAGENT framework. We examine two representative cases: the highly specialized domain of government affairs and the comparatively less specialized domain of healthcare.\nMedical Domain As demonstrated in the first category of Table 1, GPT-40 achieves the highest Score (26.80) in the medical domain, indicating its superior domain expertise compared to other models. DeepSeek-V2.5 follows closely with a Score of 26.68, while BianQue and DoctorGLM underperform, with Scores of 14.09 and 16.58, respectively. Regarding professionalism, GPT-40 excels with an overall score of 7, reflecting relatively high factual accuracy, user need satisfaction, and clarity. However, BianQue and DoctorGLM struggle with professionalism and stability, indicating limitations in handling domain-specific queries and multi-turn interactions. Although ZhongJing, BianQue, and DoctorGLM are specialized medical models, their smaller model sizes and limited training data result in suboptimal performance in realistic, multi-turn medical dialogue scenarios.\nGovernment Domain. In the government domain, DeepSeek-V2.5 marginally outperforms GPT-40 (23.42 vs. 23.39). Its variance (0.0050) in score changes is slightly higher than GPT-40 (0.0037), suggesting that DeepSeek-V2.5 may provide more diverse answers within this domain. Their professionalism and stability perform similarly, with DeepSeek-V2.5 showing a slight edge in handling challenges, suggesting it is more robust in government-related scenarios. This result aligns with expectations, as DeepSeek-V2.5 is recognized for its superior performance in Chinese language tasks, as evidenced by its strong results in AlignBench (Liu et al., 2024), and the government"}, {"title": "3.3.2 General Domain", "content": "In the reading comprehension domain, GPT-40 attains the highest Score of 48.93 and demonstrates significantly strong professionalism and stability, achieving an overall score of 9 and 10, separately. Specifically, it excels in factual accuracy, clarity, and strong coherence across multiple turns and robustness in handling follow-up questions and challenges. The minimal fluctuations ($\\Delta (\\mu) = 0.0047, \\Delta (\\sigma^2) = 0.0006$) when facing challenges, along with high scores in the Chall. metric, further indicating its stable responses. DeepSeek-V2.5 also performs well, with Score of 48.70 and overall professional and stable scores of 9. Both models perform exceptionally in this reading comprehension task compared to their performance in vertical domains, highlighting the models' effectiveness in broader, non-domain-specific tasks."}, {"title": "3.4 Ablation Study", "content": "The ablation study on the RL module is provided in Appendix A. The result emphasizes the role of the RL module and state space design in improving model performance across multiple domains, reinforcing the importance of these components for multi-turn decision-making. Additionally, the analysis of traditional evaluation metrics in Appendix C reveals the limitations of static benchmarks, reinforcing the superiority of our more intelligent evaluation. Case studies in Appendix F exemplify real-world applications, illustrating the nuanced interactions and decision points that arise during conversations."}, {"title": "4 Conclusion", "content": "In this paper, we revisit and extend two key components of existing LLM evaluation, resulting in Benchmark+ and Assessment+. Building upon these, we introduced an agent-based evaluation framework called TESTAGENT, which implements these two concepts. It can automatically construct \"strategy-criterion\" pairs around domain knowledge, which form the extended benchmark, and autonomously perform exploratory interactions and assessment. Our experimental results demonstrate the framework's versatility and effectiveness. TESTAGENT streamlines benchmark generation and offers comprehensive LLM assessment across"}, {"title": "5 Limitaions", "content": "Concerning the scope of the domain, we currently handle text-based data for the benchmark construction, excluding the multi-modal data. Note that our proposed framework can still be applied when utilizing an off-the-shelf multi-modal large model as the kernel model, which sheds light on the more general multi-modal domain exploration.\nOur method, while insightful, operates under the hypothesis that LLMs can accurately evaluate the model performance. The scoring of each response depends on the kernel model, which may raise concerns about the reliability of the metrics. Our approach generates criteria using a RAG-based process before scoring, rather than relying solely on direct scoring, facilitating the scoring precision improvement of the kernel model. To further enhance the credibility of the metrics, we will release a hybrid evaluation strategy that combines rule-based and semi-rule-based evaluation methods.\nCurrently, our action space in RL is limited to two types of interaction actions, which may constrain the diversity of interactions. A promising direction is to explore more varied interaction types that simulate real-world scenarios. In future work, we will explore more interaction forms, and delve into a deeper and more comprehensive evaluation of model performance."}, {"title": "A Ablation Study", "content": "We investigate the impact of the trainable decision-making RL module and its components in TESTAGENT through an ablation study, with the results shown in Table 2. Specifically, we compare three variants of TESTAGENT: TESTAGENT (Skeleton), which refers to the model framework before RL training; TESTAGENT (noE), where the question embedding is removed from the state space; and TESTAGENT (Vanilla) is our framework without RL module, which decide the follow-up question or challenge merely with prompt.\nThe evaluation metrics focus on the average return of a conversation after RL training, the model challenge completion rate (Challenge Ratio), and the model follow-up question completion rate (Question Ratio). Specifically, the model is expected to challenge when the score change is small, indicating insufficient new information, and to ask follow-up questions when the score change is large, signifying relevant information has been obtained. The actual number of challenges (or follow-up questions) divided by the expected number yields the corresponding ratios."}, {"title": "B Meta-Evaluation", "content": "Meta-Evaluation on Benchmark+. We randomly sample 50 sets of conversations from each domain, and five human experts independently rate the generated questions and criteria across the following five dimensions on a scale from 1 to 5: Relevancy evaluates whether the generated questions are domain-appropriate and aligned with the specific topic under analysis. Scenarios evaluates how well the questions are grounded in realistic user contexts, reflecting practical, situation-based use cases. Coherence focuses on the thematic consistency across multiple turns in a dialogue. Completeness gauges whether the generated criteria cover the key aspects to answer the generated questions. Lastly, Logic measures the logical structure and reasoning within the generated criteria."}, {"title": "C Difference Assessment", "content": "We compute Pearson, Spearman, and Kendall-Tau correlation coefficients to gauge the difference between TESTAGENT's scores and conventional metrics ROUGE (Lin, 2004). As shown in Table 6, TESTAGENT exhibits a notable difference from the ROUGE metric, highlighting its effectiveness in evaluating dynamically generated, open-ended conversations. ROUGE metric often errs when measuring the similarity between long sentences and standard answers, While TESTAGENT demonstrates a robust capability in handling lengthy texts, accurately assigning scores to reflect correctness. This explains the observed differences in their correlation coefficients."}, {"title": "D Evaluation Subjects", "content": "Human. All of our experts are researchers with a minimum of a master's degree in fields related to artificial intelligence. They have reached a consensus on the intended use of the data. Each annotator is provided with the following instructions: \"You are presented with several conversations between a target model and an evaluation model. Please assign a score to the target model's response, ranging from 0 to 1, following the given criteria. Your score should be definitive and take into account both the factual accuracy and linguistic coherence of the response.\"\nIndustry. The LLM industry chain is divided into three layers: the infrastructure layer, the model and tools layer, and the application layer. Our survey covers 16 companies in the infrastructure layer, with leading enterprises including Shanghai Supercomputer Center, Tencent Yangtze River Delta Artificial Intelligence Advanced Computing Center, Alibaba Cloud East China Intelligent Computing Center, and Huawei Software Technologies Co., Ltd. The model and tools layer includes 20 companies, with key players such as Shanghai SenseTime Intelligent Technology Co., Ltd., Shanghai Artificial Intelligence Laboratory, Shanghai Xiyu Technology Co., Ltd., and Huawei Software Technologies Co., Ltd. The application layer consists of 34 companies, with leading enterprises including Transwarp Technology (Shanghai) Co., Ltd., iFLYTEK Co., Ltd., Shanghai Kuanyu Digital Technology Co., Ltd., Alibaba Network Technology Co., Ltd., and Silicon Intelligence Technology Co., Ltd."}, {"title": "E Related Works", "content": "General Evaluation. Various benchmarks have been proposed to evaluate the broad abilities of large language models (LLMs). For knowledge, MMLU (Hendrycks et al., 2020), C-Eval(Huang et al., 2023) and CMMLU (Li et al., 2023) are commonly used, while comprehension can be tested through TriviaQA (Joshi et al., 2017) and SQUAD (Rajpurkar et al., 2016). Mathematical reasoning abilities are assessed with GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), and coding skills are evaluated through the HumanEval (Chen et al., 2021) dataset, which focuses on code generation tasks. BIG-Bench (Srivastava et al., 2022) is a broad benchmark for general tasks, covering diverse topics, such as linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, etc. While the above benchmarks provide a comprehensive assessment of LLM performance in general domains, they are still confined to predefined domains. In evaluating vertical domain systems, it faces limitations including the high cost of dataset construction and criterion development. Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) has demonstrated a strong ability to provide up-to-date and helpful external knowledge, making it widely adopted across various tasks to support generative AI in producing high-quality outputs (Fan et al., 2024). In this paper, we leverage RAG to retrieve and utilize domain-specific knowledge bases, thereby enabling RAG to contribute to the automatic construction of"}, {"title": "F Case Study in Medical Domain", "content": "The following cases illustrate the evaluation process in detail, demonstrating that TESTAGENT can effectively generate benchmarks for vertical domains and perform automatic assessment that meets realistic needs."}, {"title": "Initial Questions for Disease Topic", "content": "1. What are the common symptoms of decompression sickness?\n2. What are the main clinical manifestations of SARS?\n3. What are the emergency treatment methods for allergic shock?\n4. What are the arterial blood gas analysis criteria for respiratory failure?\n5. What are the common symptoms of tuberculous empyema?\n6. How can the occurrence of septic shock be prevented?\n7. What are the main causes of sick building syndrome?\n8. How can the occurrence of respiratory foreign bodies in children be prevented?"}, {"title": "General Criterion for Disease Topic", "content": "1. Disease Name:\nClearly state the full name of the disease, including common aliases.\n2. Disease Description:\nProvide a concise and detailed definition or background information, covering causes, symptoms, and related important features."}, {"title": "3. Disease Classification", "content": "Mention the medical classification of the disease, such as internal medicine, respiratory medicine, pediatrics, etc."}, {"title": "4. Preventive Measures", "content": "List feasible and effective preventive methods."}, {"title": "5. Causes of Illness", "content": "Clarify the main pathogenic factors and related triggers."}, {"title": "6. Pathogenesis", "content": "Briefly explain the biological mechanism or pathological process of the disease."}, {"title": "7. Symptoms", "content": "List specific and common symptoms or signs."}, {"title": "8. Insurance Status", "content": "Indicate whether the disease is covered by health insurance."}, {"title": "9. Incidence Probability", "content": "Provide data on the incidence or probability of occurrence of the disease."}, {"title": "10. Susceptible Population", "content": "Identify high-risk groups or those prone to the disease."}, {"title": "11. Transmission Method", "content": "Explain the main transmission routes of the disease; if non-contagious, specify as such."}, {"title": "12. Comorbid Conditions", "content": "Mention other health problems or complications that may coexist with this disease."}, {"title": "13. Consultation Department", "content": "Recommend the medical department patients should visit."}, {"title": "14. Treatment Methods", "content": "Describe the main treatment approaches, including both pharmacological and non-pharmacological treatments."}, {"title": "15. Treatment Duration", "content": "Provide an approximate time range for treatment."}, {"title": "16. Cure Probability", "content": "Provide data on the cure rate for the disease."}, {"title": "17. Common Medications", "content": "List common medications used to treat this disease."}, {"title": "18. Treatment Costs", "content": "Provide an estimated cost range for treating this disease."}, {"title": "19. Diagnostic Tests", "content": "List possible examinations and tests needed for diagnosing this disease."}, {"title": "20. Recommended Diet", "content": "Provide a list of foods suggested for patients."}, {"title": "21. Prohibited Foods", "content": "Provide a list of foods patients should avoid."}, {"title": "22. Recommended Recipes", "content": "Provide one or two suitable recipes or food combinations for patients."}, {"title": "23. Recommended Medications", "content": "List specific brands or manufacturers of recommended medications."}, {"title": "24. Detailed Medication Information", "content": "Provide detailed information on some recommended medications, including drug names and manufacturers."}, {"title": "Refined Criterion for Question: \"What are the emergency treatment methods for allergic shock?\"", "content": null}, {"title": "1. Disease Name", "content": "Clearly state the full name of the disease: Anaphylactic Shock"}, {"title": "2. Disease Description", "content": "Provide a concise and detailed definition or background information: Anaphylactic shock is a systemic rapid allergic reaction primarily characterized by acute peripheral circulation insufficiency, triggered by specific allergens that are generally harmless to the human body in patients with an allergic constitution.\nCover causes, symptoms, and related important features: Causes include antibiotics, local anesthetics, vitamins, etc.; symptoms include skin itching, hives, difficulty breathing, chest tightness, coughing, abdominal pain, nausea, vomiting, dizziness, and pallor."}, {"title": "3. Disease Classification", "content": "Mention the medical classification of the disease: Medical Encyclopedia, Emergency Medicine"}, {"title": "4. Preventive Measures", "content": "List feasible and effective preventive methods: Consult detailed allergy history before medication, minimize unnecessary injections, prefer oral formulations, and observe allergic patients for 15\u201320 minutes after injections. Conduct skin prick tests to avoid using medications that may yield positive results."}, {"title": "5. Causes of Illness", "content": "Clarify the main pathogenic factors and related triggers: Endocrine substances, enzymes, pollen extracts, food, antisera, occupational exposure proteins, insect venoms, etc."}, {"title": "6. Pathogenesis", "content": "Briefly explain the biological mechanism or pathological process: External antigenic substances entering the body can stimulate the immune system to produce corresponding antibodies. The production of IgE varies significantly among individuals due to genetic differences. These specific IgE have a strong affinity for target cells in the skin, bronchi, blood vessel walls, etc. When the same antigen contacts a sensitized individual again, it can trigger a widespread Type I hypersensitivity reaction."}, {"title": "7. Symptoms", "content": "List specific and common symptoms or signs: Pale lips, fainting, altered consciousness, hives, consciousness disorders, palpitations, hypotension, etc."}, {"title": "8. Insurance Status", "content": "Indicate whether the disease is covered by health insurance: No"}, {"title": "9. Incidence Probability", "content": "Provide data on the incidence or probability of occurrence of the disease: 0.001%"}, {"title": "10. Susceptible Population", "content": "Identify high-risk groups or those prone to the disease: Individuals with an allergic constitution"}, {"title": "11. Transmission Method", "content": "Explain the main transmission routes of the disease; if non-contagious, specify as such: Non-contagious"}, {"title": "12. Comorbid Conditions"}]}