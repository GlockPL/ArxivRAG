{"title": "PIC2O-Sim: A Physics-Inspired Causality-Aware Dynamic Convolutional Neural Operator for Ultra-Fast Photonic Device FDTD Simulation", "authors": ["Pingchuan Ma", "Haoyu Yang", "Zhengqi Gao", "Duane S. Boning", "Jiaqi Gu"], "abstract": "Optical simulation plays an important role in photonic hardware design flow. The finite-difference time-domain (FDTD) method is widely adopted to solve time-domain Maxwell equations. However, FDTD is known for its prohibitive runtime cost as it iteratively solves Maxwell equations and takes minutes to hours to simulate a single device. Recently, AI has been applied to realize orders-of-magnitude speedup in partial differential equation (PDE) solving. However, AI-based FDTD solvers for photonic devices have not been clearly formulated. Directly applying off-the-shelf models to predict the optical field dynamics shows unsatisfying fidelity and efficiency since the model primitives are agnostic to the unique physical properties of Maxwell equations and lack algorithmic customization.\nIn this work, we thoroughly investigate the synergy between neural operator designs and the physical property of Maxwell equations and introduce a physics-inspired AI-based FDTD prediction framework PIC20-Sim. PIC20-Sim features a causality-aware dynamic convolutional neural operator as its backbone model that honors the space-time causality constraints via careful receptive field configuration and explicitly captures the permittivity-dependent light propagation behavior via an efficient dynamic convolution operator. Meanwhile, we explore the trade-offs among prediction scalability, fidelity, and efficiency via a multi-stage partitioned time-bundling technique in autoregressive prediction. Multiple key techniques have been introduced to mitigate iterative error accumulation while maintaining efficiency advantages during autoregressive field prediction. Extensive evaluations on three challenging photonic device simulation tasks have shown the superiority of our PIC20-Sim method, showing 51.2% lower roll-out prediction error, 23.5 times fewer parameters than state-of-the-art neural operators, providing 300-600\u00d7 higher simulation speed than an open-source FDTD numerical solver. Our code is open sourced at link.", "sections": [{"title": "1 Introduction", "content": "Photonics has shown great potential in high-performance, energy-efficient computing, communication, and sensing due to the fast propagation speed, high bandwidth, and high degree of freedom of photons. Finite-difference time-domain (FDTD) simulation is a widely adopted numerical method to simulate the spectral response of photonic structures. FDTD iteratively solves the time-dependent Maxwell partial differential equations (PDEs) in a discrete mesh to emulate electromagnetic (EM) wave propagation. However, FDTD simulation is considerably time-consuming as it updates the EM field"}, {"title": "2 Background: AI for PDE Solving", "content": "Recently, scientific machine learning algorithms have been widely explored to help solve fundamental PDE problems with orders-of-magnitude faster speed. PINNs and data-driven neural operators represent two branches of research where physics is either added as a hard constraint or ignored to remove domain knowledge requirements. In the field of AI for optics, physics-informed models, e.g., WaveTorch [8], directly embed the PDE updating rules in the recurrent neural network (RNN) cells to leverage the GPU-accelerated inference engine for faster iteration. With a small enough spatiotemporal resolution, these methods have a theoretical guarantee on the solved fields, while their speedup is rather limited due to a large number of iterations. Also, oversimplified equations in the RNN cells make it hard to match the golden results from commercial tools.\nPhysics-augmented models, e.g., MaxwellNet [12], WaveYNet [4], adopted a standard U-Net structure and incorporated Maxwell residual loss in the training objective to learn an optical field that honors physical constraints. Recently, SineNet [27] was proposed to mitigate the temporal misalignment caused by the skip connection between multi-scale features in U-Net by cascading multiple U-Net and hence reducing the temporal misalignment.\nHowever, the synergy between model architecture and the underlying physical constraints of Maxwell equations remains under-explored. SoTA neural operators might not suit the optical FDTD due to the unique properties of Maxwell equations. Besides, prior work often focuses on single-iteration prediction tasks without handling the error accumulation effects with autoregressive prediction."}, {"title": "3 Proposed PIC20-Sim Framework", "content": "First, we formulate the FDTD method for photonic device simulation. FDTD starts by injecting an eigenmode light source into the device and simulating the light propagation via sequential time-marching. To obtain the response at multiple wavelengths in one shot, The incident source typically has a Gaussian-shaped envelope centered at frequency $f_e$ with a frequency width of $f_w$, thus carrying a wide range of wavelengths for broadband simulation. FDTD method discretizes the time-domain"}, {"title": "3.1 Understanding the FDTD simulation for light propagation in photonic devices", "content": "Maxwell equation and iteratively update of the electric fields. For the detail of the electric field updating rule, please refer to Appendix A.1\nConsiderable computational complexity of FDTD. FDTD is time-consuming as its convergence depends on fine-grained space-time resolution ($\\Delta t$, $\\Delta x$, $\\Delta y$) to capture the light-speed signal propagation accurately, e.g., a typical timestep is $\\Delta t = 0.167 \\text{ fs}$, and a space resolution is around 1/15 of the wavelength, leading to high computational complexity of $O(N_xN_yT)$, where $N_x$, $N_y$ are solving domain dimension and $T$ is the simulation timespan. Usually, to improve convergence, electric and magnetic fields will be alternatively updated on an interleaved 2D Yee's grid, which further increases the computation cost by 4 times. Hence, a fast prediction method that can skip tens of thousands of FDTD time-marching steps and directly reconstruct the spatio-temporal field dynamics will significantly speed up time-domain photonic device simulation.\nCausality-constrained space-time locality. Near-future light field prediction tasks constrain wave propagation in a local spatial region due to limited light speed, as indicated by the light cone in Fig. 2. This indicates that any fields outside the light cone will be non-causal to the center field, which implies a confined theoretical spatial receptive field (RF) of any prediction model. A model with an overly small RF lacks the information to reconstruct the center field. Similarly, a model with an overly large RF or even global views beyond the light cone can potentially learn non-physical mapping as it mixes irrelevant, non-causal information. Besides spatial locality limited by light speed, the light field shows temporal locality. Based on Maxwell equations, the field distribution at timestep $t$ solely depends on the electromagnetic wave in the previous timestep $t - 1$, which indicates that it is theoretically unnecessary to capture a long context in the model design as preferred in other sequence/time-series modeling tasks. Therefore, it is important to design a model with a carefully selected space-time receptive field that honors causality.\nPermittivity-dependent light propagation. The aforementioned causality-constrained space-time locality indicates that it is suitable to employ a convolution-based neural operator with a carefully selected spatial receptive field to propagate the light waves from a causal neighboring region to the center location. However, static convolution operations may not be sufficient to model the light-matter interaction in the photonic device as the local wave propagation behavior at coordinate $[m, n]$ is a function of material permittivity $\\epsilon[m, n]$ at that specific location, shown in Fig. 1(c). Such a property implies the convolutional filter that emulates the wave propagation mechanism should contain dynamic values based on the local material permittivity, which inspires us to propose a dynamic permittivity encoding in the convolutional filters."}, {"title": "3.2 Proposed PIC20-Sim framework", "content": null}, {"title": "3.2.1 Framework overview", "content": "Figure 3 illustrates our autoregressive time-bundled PIC20-Sim framework. To potentially handle prediction over a long time horizon, PIC2O-Sim autoregressively pre-"}, {"title": "3.2.2 Resolution-preserved shift-invariant domain discretization", "content": "To deal with various sizes of the input device, previous work [5] adopted a scale-adaptive domain discretization that scales devices of all physical sizes to the same image size since Fourier neural operators formulate the function mapping in a fixed domain $\\Omega$. In contrast, our PIC20-Sim adopts preserves the resolution ($\\Delta x$, $\\Delta_y$) with padding if necessary to maintain shift-invariance. This design choice brings two major advantages: (1) Shift-invariant is more scalable and generalizable than resolution-invariant discretization in our problem. The prediction model is trained on ground-truth fields simulated with high enough resolutions to guarantee FDTD accuracy, i.e., a wavelength contains 15-20 pixels. Predicting fields at higher resolution does not pragmatically bring benefits. Furthermore, a clear issue of downsizing is its inability to handle unseen large devices, where a large downsampling factor will cause severe information loss. The shift-invariant property of convolution, instead, allows the model to predict light propagation in an arbitrarily large domain without downsampling-induced loss as long as the spatial resolution stays the same. (2) Training efficiency benefit. To avoid downsampling-induced information loss, the domain-adaptive method tends to scale all devices to a large image size, which causes high costs during training and inference. In contrast, PIC20-Sim scales all devices to the same pixel resolution, e.g., $\\Delta_x = \\Delta_y = 140nm$ and pads devices to the maximum image size only in this mini-batch for parallel batched processing as shown in Fig. 4. In this way, we can avoid information loss due to the downsampling of large devices and improve speeds on small devices."}, {"title": "3.2.3 Model input/output definition: permittivity, input fields, and sources", "content": "At one iteration, the model takes the permittivity $\\epsilon_r$ and $T_{in}$-frame input fields $E_{in} \\in \\mathbb{R}^{T_{in} \\times H \\times W}$ before the target prediction timestep, and $T$-frame light sources $J_{1:T}$ as the PDE variables. For 2D simulation, the line-shaped Gaussian eigenmode source $J_{1:T}$ is injected at the port center. The existence of a source in the system is the fundamental difference and also the challenging part compared to other source-free PDE systems. The right-hand side of the Maxwell equation is not zero but a time-varying function $J(t)$, such that each frame of field is potentially impacted by all previous injected light sources. We formulate the variable light source within the prediction time horizon as a $T$-frame video, where all fields are masked to zero except the line-shaped region of light source at the input port. In this way, the FDTD prediction task is translated to a masked video restoration task, given that previous frames and the video patches at the source location are unmasked hints."}, {"title": "3.3 Efficient physics-inspired dynamic convolutional neural operator architecture", "content": "Based on the space-time causality in Section 3.1, our PIC20-Sim model $\\Psi_{\\theta}$ is built with local-view convolutions to restrict the receptive field. We introduce the detailed architecture as follows."}, {"title": "Convolutional field encoder.", "content": "PIC20-Sim starts with a convolutional encoder to project the previous light fields and incident light sources to a $D$-dimensional latent space: $a(r) \\rightarrow \\omega(r), \\forall r \\in \\Omega$, where $a = {E_{in}; J_{1:T}} \\in \\mathbb{R}^{(T_{in}+T) \\times M \\times N}$ and $\\omega(r) \\in \\mathbb{R}^{D \\times M \\times N}$. The encoder has two blocks, each containing a point-wise convolution followed by a residual block of 3$\\times$3 depthwise convolution, layer normalization, and the GELU activation function."}, {"title": "Causality-constrained permittivity-aware convolutional backbone.", "content": "The backbone of PIC20-Sim consists of $L$-layer residual blocks, each including a depthwise convolution, dilated position-adaptive convolution (DPAConv), layer normalization, and GELU activation function. A dedicated convolutional device encoder shown in Fig. 3 takes the inverse of permittivity map $1/\\epsilon_r(r)$ as input and extracts a shared local geometry information $f(\\frac{1}{\\epsilon_r})$ for all $L$ DPAConv layers in the backbone.\nFigure 6 illustrates a $K \\times K$ DPAConv module with dynamic permittivity-adaptive kernels. Each DPAConv operation within a size-$K$ window $\\pi(\\mathbb{i})$ at pixel position $i$ is formulated as $z(i) = \\Sigma_{j \\in \\pi(\\mathbb{i})} (K(f_i, f_j) \\epsilon_r) W^T(j)) \\cdot v^l(j)$. Inspired by dynamic convolution (PAConv) [19], the convolutional filter weights applied to a sliding window on the feature map is the Hadamard product of a statically-learned convolutional filter $W \\in \\mathbb{R}^{D \\times D \\times K \\times K}$ and a dynamic permittivity-adaptive kernel $K(f_i, f_j)(\\epsilon_r) \\in \\mathbb{R}$, shown in Fig. 6. The dynamic kernel $K$ projects the permittivity features $f$ into high-dimensional space via a Gaussian kernel function $K(f_i, f_j)(\\epsilon_r) = exp(-\\frac{||f_i - f_j||^2}{\\sigma})$, which helps the model understand light-matter interaction and learn how light wave propagates dynamically through a path with heterogeneous material permittivities.\nThis dynamic convolution shows strong modeling capability to capture wave propagation principles. However, standard PAConv [19] causes a practical challenge with considerable memory and runtime costs, especially during training. As the prediction frames $T$ increase, the required receptive field and convolutional kernel size increase linearly. When a large-kernel convolution requires dynamic position-specific kernels, the memory cost is bottlenecked by the largest intermediate tensor $K \\omega \\in \\mathbb{R}^{D \\times H \\times W \\times K \\times K}$, e.g., when D = 96, H = W = 256, K = 21, a single tensor takes >10 GB. To reduce the computation and memory burden, we modify it as a dilated position-adaptive convolution with an additional depthwise convolution ahead of it to aggregate local features and avoid information loss from dilation. Kernel size $K$ is the key design parameter for DPAConv that determines the receptive fields of the model. As Figure 7 shows, insufficient receptive field degrades the performance intensively, we empirically suggest using a receptive field that is 30 pixels larger than the theoretical value. For detailed kernel size selection, please see Appendix A.3\nPrediction head. At the end of the model, we simply use two point-wise convolutions with layer normalization and GELU in between to project it back to the required prediction frames."}, {"title": "3.4 Autoregressive prediction with multi-stage partitioned time-bundling", "content": "Time-bundling significantly speeds up the prediction as it generates multiple frames at one shot and reduces iterations during autoregression [3]. However, we claim bundling too many timesteps is harmful to scalability and prediction fidelity. (1) Scalability: Given that the required RF linearly expands when predicting more output frames, it has quadratic parameter count, quadratic computation cost, and higher optimization difficulty with larger convolutional kernels. Moreover, predicting more frames requires more model capacity; the hidden dimension $D$ also needs to increase accordingly."}, {"title": "4 Result", "content": null}, {"title": "4.1 Experimental Setup", "content": "Benchmarks. We evaluate different methods on three representative and challenging photonic device types, including tunable multi-mode interference (MMI) with complicated interference patterns, micro-ring resonator (MRR) with sensitive coupling and resonance effects, and Metaline with highly discrete permittivity distributions and fine-grained structures. Those practical devices post significant challenges and haven't been evaluated in the literature. We use the open-source FDTD software package MEEP [15] to generate the simulation videos. All videos are resized to have a spatial resolution of ($\\Delta x = \\Delta y = 140nm$, $\\Delta_t = 1 \\text{ fs}$). Details are in the Appendix 5.\nTraining settings and evaluation metrics. Since all frames have the same importance in FDTD, we use averaged per-frame normalized L2-Norm as the training loss function and also evaluation metric, i.e., $N-L2Norm = \\frac{1}{T} \\Sigma_{t=1}^{T} \\frac{||\\Psi_{\\theta}(E_{in}, E_r, J_{1:T})[t] - E^*[t]||_2}{||E^*[t]||_2}$. We use frames per second (FPS) to evaluate the prediction speed. Detailed training settings can be found in Appendix A.4."}, {"title": "4.2 Main Result", "content": "We compare 7 models in Table 1, including (1) global-view Fourier-domain neural operators: FNO [11] and its factorized variant F-FNO [22], the SoTA optical FDFD NN surrogate NeurO-Light [5], a Koopman neural operator (KNO) that models the time marching in the linear Koopman space [23]; and local-view convolution-based neural operators: a 16-layer SimpleCNN with static 2D convolution, SineNet [27] with a cascaded multi-stage UNet structure for temporal modeling, and our proposed dynamic convolutional neural operator PIC20-Sim. Note that for a fair comparison, the full"}, {"title": "4.3 Ablation Study and Discussion", "content": "Input field frames $T_{in}$ and dilation rate $s$. As a key hyperparameter, we select 10 frames of input fields $E_{in}$, and we choose a dilation rate of 4 to balance fidelity, speed, and parameter efficiency. For details, please refer to Appendix A.3."}, {"title": "5 Conclusion and Limitation", "content": "In this work, we present a physics-inspired causality-aware AI-accelerated FDTD solving framework PIC20-Sim for ultra-fast photonic device simulation. We deeply analyze and organically incorporate physical constraints into our model primitive designs, honoring space-time causality and permittivity-dependent wave propagation principles. Cross-iteration error mitigation techniques have been proposed to compensate for the distribution shift issue during time-bundled autoregressive prediction with balanced scalability, long-term prediction fidelity, and efficiency. Compared to SoTA Fourier-based and convolutional neural operators on three challenging photonic device types, our PIC20-Sim outperforms them with 49.1% less prediction error and 23.5 times fewer parameters. 300-600 \u00d7 speedup has been demonstrated over open-source FDTD solvers on average. One potential limitation is that our framework still observes rapidly accumulated errors with large-iteration rollout even though we have various error suppression methods. As a future direction, error suppression during auto-regressive prediction or even a completely new formulation beyond auto-regression will be further investigated to support long timespan optical FDTD simulation."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Optical simulation detail", "content": "For TM polarized electric field, the FDTD updating rule is shown as follows:\n$H_\\text{z} ^{q+\\frac{1}{2}} (m, n + \\frac{1}{2}) = H_\\text{z} ^{q-\\frac{1}{2}} (m, n + \\frac{1}{2}) - \\frac{\\Delta t}{\\mu[m, n + 1]\\Delta} (E_\\text{x} ^{q}(m, n + 1) - E_\\text{x} ^{q}(m, n))$ $E_\\text{x} ^{q+1}(m, n) = E_\\text{x} ^{q}(m, n) + \\frac{\\Delta t}{\\epsilon[m, n] \\Delta}  [H_\\text{z} ^{q+\\frac{1}{2}} (m+\\frac{1}{2}, n) - H_\\text{z} ^{q+\\frac{1}{2}} (m-\\frac{1}{2}, n) \\frac{\\Delta t}{\\epsilon[m, n] \\Delta} (H_\\text{z} ^{q+\\frac{1}{2}} (m, n + \\frac{1}{2}) + H_\\text{z} ^{q+\\frac{1}{2}} (m, n - \\frac{1}{2})]$, where $m$, $n$, $q$ represent discrete counterpart of $x$, $y$, and $t$ in continuous domain. The $\\frac{1}{2}$ shown in the index refers to the points at the middle point of edges in Yee's grid."}, {"title": "A.2 Dataset Generation", "content": "For MMI, we randomly generate 20 devices to train and 5 devices to test, for MRR, due to it is longer than that of MMI and Metaline, we generate 6 devices to train and 5 to evaluate, for Metaline, 32 device to train and 8 device to test, based on the variable settings and distributions in Table 5. Each device has an individual simulation for each input port. MMIs sweep over 3 ports and generate a total of 75 simulation videos; MRRs only have 1 input port and generate a total of 11 simulation videos (much longer); Metalines sweep over 3 ports and generate a total of 120 simulation videos. The time interval between two frames is 1 fs, i.e., $\\Delta t = 1$ fs.\nHow to sample video patches as training/validation/test dataset. First, according to different device type, we select different numbers of devices for train and test as demonstrated above. During training, for each example, we randomly select one device and one port and slice a video segment. The video segment has a randomly sampled starting frame index of $i$. $E_{i;i+T_{in}}$ will be the input fields. When sampling the starting frame index of $i$, considering the imbalanced temporal distribution of the source, which means that the source only exist within the first 560 frames approximately, we attribute more probability to sample the starting frames before 560 frames. To be specific, for MMI and Metaline, starting frames that contains source have twice of the probability to be sampled and for MRR, since the video is much longer than that of MMI and Metaline, the starting frames have 6 times of probability to be sampled. Sources $J_{T_{in}:T_{in}+T}$ will be extracted from $E_{T_{in}:T_{in}+T}$ at the input port region. $E_{T_{in}:T_{in}+T}$ serves as the target fields. Examples across epochs are totally randomly sampled.\nFor validation and inference, we uniformly slice the videos with an offset of 16 frames, i.e., $i = 0, 16, 32,...$ We do not resample the starting frames with sources in validation and test. All video slices have bilinear interpolated to have the same spatial resolution ($\\Delta_x = \\Delta_y = 140nm$). Since our mini-batch size during training and inference is, no padding is added."}, {"title": "A.3 Hyper-parameter Selection", "content": "How to determine kernel size $K$. Kernel size $K$ is the key design parameter for DPAConv that determines the receptive fields of the model. To predict the fields after $T$ timesteps, aware of space-time causality, we estimate the furthest distance the wave can propagate in the medium as $R = \\Sigma_{i=0}^T c_0 \\Delta t$. Since most light fields are confined in the waveguide region, we use the relative permittivity of the waveguide $\\epsilon_w$ to calculate the theoretical receptive field $R \\approx T\\frac{c_0}{\\sqrt{\\epsilon_{wg}}} + 30$. Empirically, we recommend a 30 pixels larger receptive field than the theoretical value $R \\approx \\frac{Tc_0}{\\sqrt{\\epsilon_{wg}}} + 30$ to obtain the best fidelity as shown in Figure 11 in which we sweep the receptive field for different timesteps to predict. Then,"}, {"title": "A.4 Training/inference Settings", "content": "We adopt Adam optimizer with an initial learning rate of 2e-3, following a cosine learning rate decay schedule and a minimum learning rate of 1e-5 for all the baselines except for the SineNet for which we use the suggested initial learning rate 2e-4 by its author and ended at 1e-6 All models are trained and evaluated on two servers with 8 NVIDIA A6000 GPUs. The runtime for all neural network models is averaged across 5 runs per photonic device in the test dataset. The runtime for the CPU numerical FDTD solver MEEP is evaluated on a 64-core AMD EPYC 7763 64-Core Processor."}, {"title": "A.5 Model Architecture Details", "content": "We compare our method with SoTA Fourier-domain neural operators and CNN models. We set a full mode for Fourier-domain neural operators to enable them to learn local window operations. For CNN models, we maintain similar layers and the number of parameters for fair comparison. And for Fourier kernel integral operation models, we try to choose as many modes as possible to capture the local wave behavior.\nFNO [11]: We construct a 4-layer FNO with Fourier modes of (128, 128), hidden channel of 36. The total parameters are 340M for FNO and MRR. For Metaline, the data are padded to 168, so the full mode is (68, 68), with hidden channel of 36, the total parameters are 146M.\nF-FNO [22]: We construct a 12-layer F-FNO whose modes, for MMI and MRR that padded to 256 x 256 is (128, 129) and for metaline which is padded to 168 \u00d7 168, is (84, 85). The number of parameters is 4.5M and 3.3M, respectively.\nKNO [23]: We construct a 2-layer KNO whose modes, for MMI and MRR that padded to 256 \u00d7 256 is (128, 129) and for Metaline which is padded to 168 \u00d7 168, is (84, 85). The number of parameters is 171.8M and 74.6M, respectively.\nNeuroLight [5]: We construct a 6-layer NeurOLight whose modes, for MMI and MRR that padded to 256 \u00d7 256 is (128, 129) and for Metaline which is padded to 168 \u00d7 168, is (84, 85). The number of parameters is 2.2M and 1.6M, respectively.\nSimpleCNN: We construct a 16-layer SimpleCNN. The kernel size is set to be 15 for MMI, and Metaline has the same $\\epsilon_r$, and hence the same required receptive field, and the kernel is set to be 21 for MRR due to the high-speed light because of the relatively small $\\epsilon_r$. The number of channels is set to 32 so that the number of parameters is within a reasonable range, which is 3.8M for MMI and Metaline and 7.3M, respectively.\nSineNet [27]: We construct a SineNet with 8 waves for MMI and MRR whose was padded to 256 x 256, the number of downsampling and upsampling blocks in each wave is 4, and the initial hidden channel is 24 so that the number of parameters keeps reasonable. For Metaline, to cooperate with its size which is 168 \u00d7 168, we changed the number of downsampling and upsampling blocks to 3 in each block and to compensate, we set the initial channels to 42 and the number of parameters is 30M\nPIC20-Sim: For the device encoder, we use a single convolution layer by a depth-wise convolution layer followed by layer normalization in ConvNeXt[13] style and GELU, a skip connected is added connecting from input of the depth-wise convolution to the end of GELU. Then, the above structure is copied once and cascaded together to form our device encoder. The output channel for the four convolutional layers is 1 \u2192 72 \u2192 72 \u2192 48 \u2192 48. For MMI and Metaline, the kernel size is [3 3 5 5], and for MMR, the kernel size is [5 5 5 5]. The device encoder is padded with replicate mode. For the field encoder, we use almost the same configuration as the device encoder except for the number of channels; the channels are now becoming # of input fields + # of sources \u2192 72 \u2192 72 \u2192 72 \u2192 72. The kernel size becomes [1 3 1 3], and the fields are padded using zero padding. For the hidden state adaptor, we use a single point-wise convolution, with input channels of 72+72=144 and output channels of 72. For the backbone, we use 8 layers. For MMI and Metaline, each layer has a local aggregation depth-wise convolution layer whose channel number is 72 and kernel size equals 5, a DPAConv layer with kernel size 5 and dilation 4 to provide enough receptive field followed by layer normalization and GELU, all these modules are included within skip connection. For MRR, the basic structure remains the same except for the kernel size for DPAConv, which becomes 7 to provide a wider receptive field. For the decoder, we use two point-wise convolutional layers in which the first one lifts the D-dimension feature to a 512-dimension vector and the second one projects it back to"}, {"title": "A.6 Prediction Result Visualization", "content": "In this section, we select the best performed baselines and our PIC20-Sim to show their performance in 160 frames prediction on different devices. We sampled the 160-frames whole video every 40 frames and to make the error more obvious, the error is plotted with a smaller scale from -0.02 to +0.02\nFigure 14 shows the visualization of the 160 frames prediction of the selected baselines and PIC20-Sim on MMI. Since the relative simple structure, the error among all the baselines are small. However, there is still an obvious performance gap between PIC20-Sim and other well performed baselines.\nFigure 15 shows the visualization of the 160 frames prediction of the selected baselines and PIC20-Sim on MRR in which a pulse of light is propagating through the slim ring waveguide and coupled to the straight waveguide. More complicated device raises a more challenging task for these surrogates. FFNO suffers from the Fourier integral operation and shows huge error. SineNet, also, had a bad performance due to the irrelevant features. CNN based operators obtained better fidelity and the PIC20-Sim, due to its physics causal dynamic kernel, have a slightly better performance than SimpleCNN.\nFigure 13 shows the visualization of the 160 frames prediction of the selected baselines and PIC20-Sim on Metaline. The Metaline has the most complicated structure, which causes the larger performance gap between PIC20-Sim and SimpleCNN as expected"}]}