{"title": "IDENTIFYING AND ADDRESSING DELUSIONS FOR TARGET-DIRECTED DECISION MAKING", "authors": ["Mingde Zhao", "Tristan Sylvain", "Doina Precup", "Yoshua Bengio"], "abstract": "We are interested in target directed agents, which produce targets during\ndecision-time planning, to guide their behaviors and achieve better generaliza-\ntion during evaluation. Improper training of these agents can result in delusions:\nthe agent may come to hold false beliefs about the targets, which cannot be prop-\nerly rejected, leading to unwanted behaviors and damaging out-of-distribution\ngeneralization. We identify different types of delusions by using intuitive ex-\namples in carefully controlled environments, and investigate their causes. We\ndemonstrate how delusions can be addressed for agents trained by hindsight rela-\nbeling, a mainstream approach in for training target-directed RL agents. We val-\nidate empirically the effectiveness of the proposed solutions in correcting delu-\nsional behaviors and improving out-of-distribution generalization.", "sections": [{"title": "INTRODUCTION", "content": "One important use of Reinforcement Learning (RL) is to train agents with skills generalizable\nto target tasks for application, after learning from limited training tasks, where the evaluation\nand training task distributions may differ (Ada et al., 2024). Despite numerous efforts to bridge\nthe \u201cgeneralization gap\u201d over discrepancies between the two sets of tasks, generalization abilities\nof RL agents remain largely unsatisfactory. This means that learned skills become less useful to\ntasks in practice, greatly limiting the scope of applications and research interests. Recent works\nattributed this to existing agents' lack of reasoning abilities to face Out-Of-Distribution (OOD)\nchanges (Di Langosco et al., 2022), as most agents either operate solely based on intuition (includ-\ning model-free methods and offline model-based methods) (Kahneman, 2017). These suggestions\nled to the development of agents that reason adaptively in novel situations.\nThus, some decision-time planning agents are developed, which make use of their planning out-\ncomes, which we call targets, to adapt in OOD scenarios (Alver & Precup, 2022). Specifically, tar-\ngets are self-generated (sub-)goals produced during decision-time planning. The \u201ctarget-directed\u201d\nagents make use of generators to propose targets, as well as optional estimators that evaluate the\nfavorability of the targets, to assist solving the given tasks. These processes however provide no\ninherent assurance to the selected targets.\nIn psychiatry, delusions depict the inability to reject false beliefs (Kiran & Chaudhury, 2009), which\nin the RL context translates to false beliefs that are natural results of an agent's learning. Particu-\nlarly problematic for target-directed agents, when false beliefs cannot be resolved through training,\ndelusional behaviors may appear in the form of chasing nonexistent or unreachable targets, or those\nbeyond safety constraints (Bengio et al., 2024). Delusions limit the agents' situational understand-\ning, and lead to even worse generalization in the target tasks. Delusions stem from the incoordina-\ntion of the generator and the estimator, respectively: problematic targets should not be generated,\nand even if they were, they should not be favored(Corlett, 2019). While delusions have various\nsources, this paper focuses on the misevaluation by the estimator caused by improper training\nprocesses, a combination of inadequate update rules and inappropriate training data distributions.\nIn this paper, we demonstrate how to preemptively address failure modes induced by delusions\nin the target tasks by training the estimators properly. With clear diagnoses in the designed en-\nvironments, we identify types of delusions, and investigate more proper designs of the training"}, {"title": "PRELIMINARIES", "content": "RL & Problem Setting.: An RL agent takes actions to interact with an environment to gain cu-\nmulative reward. The interaction may be modeled as a Markov Decision Process (MDP) M =\n(S, A, P, R, d, \u03b3), where S and A are the set of states and actions, P : S \u00d7 A \u2192 Dist(S) is\nthe state transition function, R : S \u00d7 A \u00d7 S \u2192 R is the reward function, d : S \u2192 Dist(S) is\nthe initial state distribution, and \u03b3 \u2208 (0, 1] is the discount factor. An agent needs to learn a pol-\nicy \u03c0 : S \u2192 Dist(A) that maximizes the value, i.e. the expected discounted cumulative reward\n\u0395\u03c0,\u03a1\u0399\u03a3\u03a4R(St, At, St+1)|So ~ d], where T\u2081 denotes the time step when the episode termi-\nnates. Often, environments are partially observable, which means, instead of a state, the agent\nreceives an observation Xt+1, with which the agent needs to infer the state from the history.\nTarget-Directed RL offers a perspective to\nbetter identify delusions. It abstracts exist-\ning methods that belong to the intersection of\ndecision-time planning and generalized goal-\nconditioned RL (Nasiriany et al., 2019; Zadem\net al., 2024). The framework emphasizes an al-\ngorithmic design, where a generator proposes\ncandidate targets, and an (optional) estimator\nevaluates them and select one target for (op-\ntionally target-conditioned) policies to execute\n(Dayan & Hinton, 1992; Parr & Russell, 1997).\nThe two components correspond to the belief\nformation and belief evaluation systems, re-\nspectively, whose incoordination causes delu-\nsions in the human brain (Kiran & Chaudhury,\n2009), as shown in Fig. 1.\nHindsight Relabeling is a mainstay ap-\nproach to train conditioned policies, estimators, as well as temporally-extended generators. Train-\ning target-directed agents with only contemporary targets (the ones being followed) can lead to\npoor performance (Dai et al., 2021), especially for agents that propose their own targets, where\ncontemporary targets may be low-quality, hard to achieve, or lack in diversity (Moro et al., 2022;\nDavchev et al., 2021). Hindsight Experience Replay (HER) addresses this by creating a buffer\nof training data for the learner to sample from and filling it with relabeled transitions, which\nare augmented with alternative targets (Andrychowicz et al., 2017). HER augments a transition\n(xt, At, rt+1, Xt+1) with an additional observation x (or its some encoding), the relabeled target.\nEach (re)labeled transition constitutes a source-target pair, with one decision point for the current\nstep and another for the relabeled target. Relabeling strategies, which correspond to the sources\nof x, are critical for the performance of HER-trained agents (Shams & Fevens, 2022). Most pop-\nular choices are trajectory-level, meaning x comes from the same trajectory as xt and are thus\nconducted at the end of each episode. These include \"future\", where x = xt, with t' > t, and\n\"episode\", with 0 < t' < T\u2081.\nSSM In most works, failure modes of target-directed agents are often overlooked, possibly due to\nthe complexity of benchmark environments. To better identify the causes, and provide intuitive\nexamples, we craft a set of fully-observable environments based on the MiniGrid-BabyAI frame-\nwork (Chevalier-Boisvert et al., 2018b; Hui et al., 2020), named SwordShieldMonster (SSM\nfor short). In SSM, the agent moves one step at a time in four directions to navigate fields with\nepisode-terminating, randomly placed lava traps. The density of traps is controlled by a difficulty\nparameter d, which guarantees a viable path to success, i.e. acquiring a sparse terminal reward.\nAgents must collect a sword and a shield, both randomly placed, before reaching the \u201cmonster\u201d\nguarding the treasure. Reaching the monster prematurely ends the episode without reward. Visu-\nalizations of SSM are in Fig. 2, along with examples of delusional behaviors discussed later. The"}, {"title": "DELUSIONS IN TARGET-DIRECTED RL", "content": "Delusions can lead to agent chasing outcomes that damage the agents' generalization in the target\ntasks or those with catastrophic consequences. We would like to identify delusions, develop corre-\nsponding mitigating strategies, s.t. through learning on the training tasks, the agents can identify\nand avoid delusional behaviors in the target tasks. We look into the two high-level components of\ntarget-directed frameworks, i.e. the generator and the estimator, for the sources of delusions and\nthe possible causes."}, {"title": "GENERATOR DELUSIONS", "content": "Most learned generators are subjected to unwanted generalization, and hence will inevitably gen-\nerate problematic targets, a behavior sometimes referred to as hallucination (Jafferjee et al., 2020).\nWhile hallucination can be reduced, they are generally unavoidable. In nature, hallucinations are\nsymptoms of delusions, as the issue of unwanted generalization is unlikely to be resolved by the\nagent autonomously. While delusional behaviors are not all from problematic targets, generator\nhallucinations are a major source of risks related to delusions.\nGenerators are often implemented as neural networks and trained in various ways, where direct\ncauses of problematic target generation could be complicated and hard to enumerate. Hence, we\nfocus on identifying the types of problematic targets based on their characteristics."}, {"title": "TYPE G.1 - \u039d\u039f\u039dEXISTENT", "content": "The first type of problematic targets includes those that do not correspond to valid states in the\ntask MDP. They can be classified into: 1) Invalid: semantically invalid targets for the task, such\nas a target singleton of an imagined SSM observation without the agent position; 2) Impossible:\nsemantically valid but unreachable targets like a SSM target cell surrounded by lava traps, (such\nas in Fig. 2 a). G.1 targets only cause delusional behavior if the estimator produces matching Type\nE.1 delusions, to be discussed later. Imperfectly generated targets corresponding to real states may\nnot be problematic, as estimators may filter out imperfections, possibly using a state encoder."}, {"title": "TYPE G.2 - TEMPORARILY UNREACHABLE", "content": "The second type includes those that correspond to states in the task MDP, but (some) cannot be\nreached from the current state. Unlike G.1, these targets are only temporarily inappropriate. There"}, {"title": "ESTIMATOR DELUSIONS", "content": "An estimator evaluates the favorability of proposed targets, based on some relationship between\nthe current state and the candidates, which often involve assessing the implementability of tar-\ngets, represented as, e.g., distances, etc. Acting as a firewall, the estimator of a target-directed\nagent can assist the selection among the proposed targets, bearing the potential to filter out delu-\nsional behaviors. Agents without estimators, e.g. Hafner et al. (2022), must accept proposed targets\nunconditionally, and thus are at significant risk of delusions (chasing G.1 and G.2). Importantly, the\nincorporation of estimators merely delegates the responsibility of addressing delusional behaviors\nto the estimator.\nEstimators' delusional beliefs about targets can directly cause delusional behaviors. For an estima-\ntor to learn to reject delusions, we need 1) effective learning rules that can punish the favorability of\nproblematic targets, and 2) training data that can counteract delusions through the learning rules.\nA lack of either can result in blind spots of knowledge about targets, delusional or non-delusional."}, {"title": "TYPE E.0 - MISEVALUATING NON-DELUSIONAL TARGETS", "content": "The standalone type of delusions includes false estimations about non-delusional targets. E.0 delu-\nsions can lead to undesirable or delusional targets being favored. E.0(-induced) behaviors can hurt\nthe agents' generalization abilities."}, {"title": "TYPE E.1 - MISEVALUATING G.1 TARGETS", "content": "This type of delusion appears when an estimator mis-evaluates the favorability of a G.1 target. If\neffective learning rules are present, this can still be caused by the lack of training data sampling\nprocedure (e.g., existence of relabeled transitions with G.1 targets in HER). In Fig. 2 a), an example\nis visualized. Note that the resulting E.1 behaviors, i.e. those chasing selected G.1 targets, can be\npotentially catastrophic if the G.1 targets are beyond safety constraints."}, {"title": "TYPE E.2 - MISEVALUATING G.2 TARGETS", "content": "Similar to E.1, in Fig. 2 b), an example of delusional behavior is visualized, a result of a combination\nof G.2 (temporarily unreachable) and E.2. E.2 behaviors can hurt the agents' generalization abilities."}, {"title": "ADDRESSING DELUSIONS: STRATEGIES APPLIED TO HINDSIGHT RELABELING", "content": "Identifying delusions enables mitigation strategies. We propose general solutions for existing\ntarget-directed agents, coming from a spectrum of training procedures. Given the popularity of\nHER-based training and the generality behind hindsight relabeling, we provide examples of apply-\ning our strategies to address delusions in HER-based training, demonstrating empirical benefits.\nAssume we are dealing with a hypothetical target-directed agent with a dual-component architec-\nture as in Fig. 1, learning both components exclusively from hindsight-relabeled transitions. The\ngenerator proposes targets corresponding to potentially distant states, and the estimator learns\nthe favorability of targets (via an oracle or more complex methods) by punishing the unachieved\nor unsafe targets present in the training data. To address delusions, we combine update rules and\ntraining data distribution. By assuming this framework, we skip discussing update rules, as they\ndepend on the agent's design."}, {"title": "ASSISTIVE STRATEGIES FOR ADDRESSING DELUSIONS", "content": "We first introduce two strategies to address delusions, which are then materialized as two hindsight\nrelabeling strategies for HER. Later, we compare the ideas behind the new strategies with existing\nones, and study how the two groups can be combined to create mixtures for estimators and hybrids\nfor the generator-estimator duo."}, {"title": "\"GENERATE\": LET ESTIMATORS LEARN About CANDIDATES (TO BE GENERATED)", "content": "The first strategy, named \"generate\", is to let the estimator learn about targets that could be proposed\nat decision time, s.t. it could figure out preemptively that problematic targets are not favorable.\nIn Zhao et al. (2024), the authors identified a delusional behavior resulting from, E.1 delusions in the\nlanguage of this paper, and they proposed to train the estimator additionally with candidate targets\nproposed by the generator. With HER, we can transform this auxiliary loss into a Just-In-Time\n(JIT) HER strategy that, when a transition is sampled for training, relabels it with a conditionally\ngenerated target by the generator. \"generate\" is particularly effective for training estimators, as\nthe estimators will get exposed to all kinds of problematic targets (that the generator could offer\u00b9.\n\"generate\" requires the use of the generator, thus it incurs additional computational burden.\nAdapting \"generate\" beyond HER should be straightforward. Whenever a target is involved (during\nestimator learning), we can replace it with generated targets."}, {"title": "\"PERTASK\": LET ESTIMATORS LEARN ABOUT EXPERIENCED TARGETS", "content": "The second strategy, namely \u201cpertask\u201d, is to expose the estimator to targets that were experienced\nbefore, s.t. it could figure out that some targets are not favorable from the current state.\nWe materialize \"pertask\" into an assistive atomic relabeling strategy that relabels transitions with\nobservations from the same training task, sampled across the entire memory. \"pertask\" works on a\ntask level, creating more diverse source-target pairs than trajectory-level strategies like \"future\"\n& \"episode\", which bias the training data distribution toward shorter-distance source-target pairs.\nThis approach additionally brings exposure to the estimator to learn against E.2 delusions and to\nthe long-distance source-target pairs, thus mitigating E.0 caused by short trajectories.\nIt is worth noting that \"pertask\u201d also biases the training data distribution, making the agent spread\nout its efforts into learning the source-target pairs that are potentially far away from each other.\nDespite the increment to training data diversity, long-distance pairs are less likely to contribute to\nbetter decision-making compared to the shorter-distance in-episode ones offered by \u201cepisode\u201d, as\nshown later in experiments.\nAdapting \"pertask\" beyond HER requires algorithmic designs recording all past observations.\nBoth strategies help the estimator figure out the features shared by problematic targets, s.t. OOD\ndelusions can also be identified. Note that \"pertask\" cannot be used to address E.1 delusions. How-\never, \"generate\" can be used for E.2 if the generator generates the respective problematic targets."}, {"title": "MIXTURES", "content": "Creating a mixture of sources of training data increases the diversity of source-target combinations.\nFor HER specifically, each atomic strategy, enumerated in Tab. 1, exhibits a tradeoff in estimation\naccuracy among all sorts of source-target pairs, including short-distance and long-distance ones\ninvolving only valid targets, and those involving problematic targets.\nA mixture of more-than-one atomic strategies in certain proportions while relabeling (Nasiriany\net al., 2019; Yang et al., 2021a), can be used to achieve a tradeoff in HER-based training, s.t. the\nshortcomings of each atomic strategy are mitigated by the introduction of others."}, {"title": "HYBRID STRATEGIES - A 2-SLOTTED APPROACH", "content": "While an estimator needs the exposure to problematic targets to counteract delusions, a generator\nbenefits from learning to generate useful targets, and should avoid exposure to problematic targets.\nThus, generators and estimators often have conflicting needs during training.\nEach source of training data has its own biases. Atomic hindsight relabeling strategies in HER,\nas shown in Tab. 1, can simultaneously help one component while hindering another. Instead of\ntrying to tradeoff the needs of both components with a single source of training data, we propose a\nhybrid (2-slotted) approach, allowing the generator and estimator to receive data tailored to their\nneeds, through two independent relabeling processes. This combination of 2 slots and mixtures\ncan produce various hybrid strategies, as demonstrated in Sec. 6.4.\nSeparating training data for the generator and estimator based on their needs is straightforward\nin training procedures beyond HER."}, {"title": "RELATED WORKS", "content": "Target-Directed Agents. Dual-component frameworks are explicitly investigated with temporal\nabstraction for purposes such as path planning (Nasiriany et al., 2019), OOD generalization (Zhao\net al., 2024) and task decomposition (Nair & Finn, 2020; Zadem et al., 2024). Davchev et al. (2021)\nemploys the similar framework to assist exploration, without using HER for training.\nHindsight Relabeling (Andrychowicz et al., 2017) is crucial for enhancing sample efficiency in\ntarget-directed RL, as it enables learning from failed experiences (Dai et al., 2021). From the per-\nspective of source-target pair distribution, from which we similarly developed this paper, improve-\nments were proposed, including a divergence maximization approach (Zhang & Stadie, 2022), ad-\ndressing distributional change (Bai et al., 2023), and multistep relabeling strategies (Yang et al.,\n2021b), etc. Kuang et al. (2020) proposed to prioritize rarely-seen achieved transitions by utilizing\nthe density distribution. HER's success was centered on its performance gains via sample efficiency\n(in non-delusional cases), around which most follow-up works revolved as well.\nWhile, performance deficiencies of target-directed frameworks can be resulted from multiple fac-\ntors, which include delusions. Shams & Fevens (2022) studied the performance of atomic strategies\nfrom the view of sample efficiency, without looking into the failure modes. Deshpande et al. (2018)\ndetailed experimental techniques in sparse reward settings using \"future\". In (Yang et al., 2021a),\na mixture strategy similar to \"generate\" improved non-delusional estimations, though the impact\non delusions was not explored, possibly because of the lack of an appropriate RL environment.\nNasiriany et al. (2019) used a mixture of up to 3 atomic strategies for producing effective training\ndata in a single-task setting, while its contribution to addressing delusions was not investigated.\nThe performance of existing target-directed agents are often limited by their exclusive reliance on\n\"future\" or \"episode\u201d (He et al., 2020), whose delusions this paper intends to address.\nDelusions in RL. Delusions can stem from various other improper designs. For example, Lu\net al. (2018) identified delusions caused by the limitations of function approximators for greedy\npolicies in modelfree RL; Often accompanying hallucinations, delusions can arise in non-target-\ndirected frameworks as well. In an offline planning framework such as Dyna, state hallucinations\ncan irreversibly destabilize value estimations (Jafferjee et al., 2020). In Di Langosco et al. (2022);\nShah et al. (2022), the authors classified goal misgeneralization, a class of delusional behaviors with\nwhich an agent competently pursues an undesired target leading in novel test situations. Zhao et al.\n(2024) gave first examples of delusional behaviors caused by improper hindsight relabeling."}, {"title": "EXPERIMENTS", "content": "To investigate the effectiveness of our strategies against delusions and how they can help target-\ndirected agents achieve better generalization, we choose a combination of environments with\nclearly defined delusional cases, as well as methods whose estimators can be easily analyzed. This\nleads to our 4 sets of experiments, coming from a combination of 2 environments (one dominantly"}, {"title": "ENVIRONMENTS & SETTINGS", "content": "For environments, we favor tasks where the identified delusions are present and intuitive to inspect,\nfor which the introduced SSM offers us clear advantage. Introduction and the results on another\nenvironment are presented in the Appendix.\nFor each training seed run, we sample and preserve 50 training tasks from a distribution of tasks\nof size 12 \u00d7 12 and difficulty 8 = 0.4. All agents are trained for 1.5 \u00d7 106 interactions by randomly\nselecting one of the 50 tasks for each training episode. To speed up training, the initial state\ndistributions span all the non-terminal states in each training task. Note that the change of initial\nstates also increases risks of E.2, due to the presence of dense episode-terminating lava traps and\nrelatively short MELs (128 for SSM)."}, {"title": "EVALUATIVE CRITERIA", "content": "We employ the following criteria to investigate the changes in agent's estimations and behaviors:\nEstimation Error - E.0", "Non-Delusional": "At each evaluation point", "Frequencies": "We also monitor the frequency of a problematic target (G.1\nor G.2) being chosen by the agents", "Performance": "We analyze the changes in agents' OOD\ngeneralization performance, due to the strategies introduced to handle delusions. The evaluation\ntasks are sampled from a gradient of OOD difficulties - 0.25, 0.35, 0.45 and 0.55. For aggregated\nOOD performance, such as in Fig. 3 g), we sample 20 tasks from each of the 4 OOD difficulties,\nand combine the performance across all 80 episodes, which have a mean difficulty matching the\ntraining tasks. By comparing the resulted performances of atomic strategies with those of the\nnewly proposed hybrids, we can also have an intuitive grasp of the \"delusion gap\", which is the\namount of performance degradation caused by delusions. To maximize difficulty, the initial state is\nf"}]}