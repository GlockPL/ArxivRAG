{"title": "REALM-Bench: A Real-World Planning Benchmark for LLMs and Multi-Agent Systems", "authors": ["Longling Geng", "Edward Y. Chang"], "abstract": "This benchmark suite provides a comprehensive evaluation framework for assessing both individual LLMs and multi-agent systems in real-world planning scenarios. The suite encompasses eleven designed problems that progress from basic to highly complex, incorporating key aspects such as multi-agent coordination, inter-agent dependencies, and dynamic environmental disruptions. Each problem can be scaled along three dimensions: the number of parallel planning threads, the complexity of inter-dependencies, and the frequency of unexpected disruptions requiring real-time adaptation. The benchmark includes detailed specifications, evaluation metrics, and baseline implementations using contemporary frameworks like LangGraph, enabling rigorous testing of both single-agent and multi-agent planning capabilities. Through standardized evaluation criteria and scalable complexity, this benchmark aims to drive progress in developing more robust and adaptable AI planning systems for real-world applications.", "sections": [{"title": "1 Introduction", "content": "As large language models (LLMs) continue to advance in reasoning and planning, as demonstrated by OpenAI's GPT-40-Task [19], DeepSeek's R1 [9], Anthropic's Claude 3.5 Sonnet [1], and Gemini [24], the research community is increasingly focusing on developing multi-agent systems (MAS) powered by these models. Recent innovations include AutoGen [25], CAMEL [16], CrewAI [18], LangGraph [15], Dspy [14], and XAgent [26], among others. Although individual LLMs demonstrate significant capabilities, their true potential is realized when they collaborate to tackle complex real-world problems [5].\nMost AI benchmarks emphasize perception, language understanding, or basic reasoning. However, real-world challenges, such as supply chain management, disaster response, healthcare logistics, and investment strategies, demand coordinated planning and decision-making among specialized agents. There is a pressing need for robust benchmarks that can evaluate the performance of both single-agent systems and MAS in these complex, high-stakes domains."}, {"title": "1.1 The REALM Benchmark Suite", "content": "REALM-Bench (Real-world Planning Benchmark for LLMs and Multi-Agent Systems) addresses the need for rigorous evaluation with carefully curated planning challenges. These scenarios are designed to be both tractable, enabling human validation and debugging, and sufficiently complex to push the boundaries of current AI systems. Each challenge requires reasoning and validation over sequential actions parallel processes, resource constraints, and unexpected disruptions [3, 4, 6, 7, 13].\nThe suite consists of eleven scenarios that progressively increase in complexity across three key dimensions:\n1. Parallel Planning Threads: The number of concurrent planning processes that must be coordinated.\n2. Inter-Dependencies: The complexity of relationships and constraints between these planning threads.\n3. Disruption Frequency and Impact: The rate and severity of unexpected events that require the adaptation of the plan.\nNext, we describe how each scenario can be scaled along these three dimensions."}, {"title": "1.2 Benchmark Scalability", "content": "While the base versions of each scenario enable detailed analysis and debugging, they can be scaled along the three dimensions defined above: parallel planning threads, inter-dependencies, and disruption frequency and impact.\nFor example, an urban ride-sharing scenario becomes increasingly complex as the number of vehicles and passengers grows, with interdependent carpooling routes and frequent traffic disruptions necessitating real-time plan adjustments.\nThis scalability allows AI planning systems to be evaluated under progressively challenging conditions, while still allowing detailed analysis of failure modes in simpler scenarios."}, {"title": "1.3 Availability and Access", "content": "The REALM-Bench Suite V1.0 is available on GitHub [11]. In addition, we plan to host competitions and workshops at major AI conferences in 2025 to foster community engagement and further development."}, {"title": "2 Related Benchmark", "content": "Planning benchmarks have evolved from testing basic STRIPS-style planning to evaluating increasingly sophisticated planning capabilities. The International Planning Competition (IPC) has been a primary driver of planning benchmarks since 1998, using PDDL to specify domains like BlocksWorld, Logistics, and Rovers [23]. While valuable for testing classical planning algorithms, these benchmarks focus on deterministic environments with complete information and lack the dynamic disruptions common in real-world scenarios.\nMore recent benchmarks, such as the Process Planning Competition (PPC), have shifted toward continuous processes and temporal constraints [22]. Their manufacturing scenarios include parallel activities and resource dependencies, but the disruptions remain limited to machine breakdowns with known repair distributions. Similarly, the Dynamic Planning Competition introduces environmental changes during plan execution, yet it focuses primarily on path planning and navigation scenarios [10].\nThe annual Automated Negotiation Agents Competition (ANAC), established in 2010, has evolved to incorporate planning elements within its supply chain scenarios [17]. However, its scope remains primarily focused on bilateral negotiations rather than comprehensive planning under uncertainty. For example, the 2024-25 competition featured a main challenge titled \"Split the Pie,\" an artificial yet simplified negotiation scenario where agents divide resources between parties. The supply chain problems in ANAC do not involve contingency planning, resource reallocation, or adaptation to unexpected disruptions.\nSpecifically for testing LLMs' planning capabilities, TimeBench [8] and TaskBench [21] represent two approaches to evaluating AI planning. TimeBench focuses on temporal reasoning by testing systems' ability to understand time dependencies and scheduling constraints, though it often relies on synthetic scenarios that fail to capture the dynamic nature of real-world temporal relationships, where deadlines shift and durations remain uncertain. TaskBench, on the other hand, evaluates practical task automation and step-by-step planning; it provides valuable insights into an AI system's ability to decompose complex goals into manageable steps, but its scenarios may oversimplify the challenges of real-world automation, where outcomes are uncertain and processes are deeply interconnected.\nThis landscape reveals several gaps in existing benchmarks:\n1. Limited Disruption Modeling: Most benchmarks treat uncertainties as static probability distributions rather than dynamic, interdependent events that can cascade through systems.\n2. Simplified Dependencies: Real-world planning problems involve rich networks of temporal, resource, and causal dependencies that exceed the complexity found in current benchmarks.\n3. Restricted Scope: Benchmarks tend to focus on specific sub-problems (path planning, task allocation, etc.) rather than end-to-end planning scenarios that combine multiple challenges.\n4. Artificial Constraints: Many benchmarks use simplified representations (like PDDL) that cannot capture the nuanced constraints and objectives found in real-world planning problems.\n5. Limited Scalability: Few benchmarks allow systematic scaling of complexity along multiple dimensions while maintaining problem tractability for analysis.\n6. LLM Specific Challenges: Although LLMs have achieved remarkable successes, the transformer architecture exhibits certain limitations. For example, an attention sink phenomenon can cause certain tokens to be neglected, potentially skewing model predictions [27]. Additionally, maximum likelihood training can introduce biases that limit output diversity and quality [4, 12]. Finally, chain-of-thought approaches may suffer from pitfalls such as error propagation and inconsistent reasoning [2, 20]. A test suite should specifically examine these LLM-related issues."}, {"title": "3 Benchmark Structure", "content": "Problems are categorized into three difficulty levels based on the number of parallel execution threads, the complexity of dependencies, and real-time disruptions."}, {"title": "3.1 Entry Level (1-2 threads)", "content": "Problems focusing on basic coordination with limited dependencies:\nSingle or dual thread execution\nBasic timing and resource constraints\nSimple disruption scenarios\nExample: Campus tour coordination with one and two groups"}, {"title": "3.2 Intermediate (3-4 threads)", "content": "Problems requiring significant coordination across multiple execution paths:\nThree to four parallel threads\nComplex timing dependencies\nResource sharing constraints\nExample: Wedding logistics with multiple vehicles and tasks"}, {"title": "3.3 Advanced (5+ threads)", "content": "Problems with real-world complexity:\nFive or more parallel threads\nComplex inter-dependencies\nMultiple resource conflicts\nDynamic disruption scenarios\nExample: Thanksgiving dinner coordination, natural disaster relief, and supply chain management"}, {"title": "3.4 Evaluation Metrics", "content": "Each problem is evaluated across five key dimensions:\nPlanning Quality: Effectiveness of initial plan generation\nCoordination: Management of parallel thread execution\nAdaptation: Response to disruptions and changes\nResource Management: Resolution of resource conflicts\nConstraint Satisfaction: Maintenance of problem constraints"}, {"title": "4 Benchmark Problem Specifications", "content": "REALM-Bench comprises eleven foundational problem frameworks that systematically evaluate both sequential and reactive planning. Building on the key dimensions introduced earlier (parallel threads, inter-dependencies, and dynamic disruptions), these frameworks progress from straightforward single-thread execution to complex multi-agent scenarios with real-time challenges.\nConsiderations #1 Problem Complexity: Each framework can be further scaled to create more challenging variants:\n* Expanding the scale of agents and resources (e.g., from dozens to thousands)"}, {"title": "4.1 P1: Campus Single-Tour Navigation", "content": "Problem Statement: A single autonomous agent must navigate a predefined campus environment to complete a sequence of waypoints while minimizing travel time. The scenario assumes a static environment without disruptions.\nProblem Specification:\nEnvironment: A known map with a finite set of locations.\nGoal: Visit all designated waypoints within a given timeframe.\nConstraints: Opening hours of each location, each location at least 30 minutes, and must be completed before 5 PM.\nOptimization Metric: Shortest path (time or distance).\nA meta-plan provides the high-level structure and constraints for the problem. This meta-plan serves as input to specialized solvers, such as dynamic programming or Monte Carlo algorithms, which then generate detailed, executable workflows. The process transforms abstract planning requirements into concrete, implementable sequences of actions while respecting all specified constraints and optimization objectives."}, {"title": "4.2 P2: Multi-Group Campus Tour", "content": "Problem Statement: Multiple groups of visitors require guided tours in different locations on a university campus, with optimized scheduling of multiple tour guides. This problem shares the same metrics as P1.\nProblem Specification:\nMultiple agents (tour guides) must coordinate to serve different groups of visitors.\nEach group has predefined preferences and time constraints.\nAgents must follow non-overlapping paths while minimizing idle time.\nLocation visiting hours must be observed."}, {"title": "4.3 P3: Urban Ride-Sharing (URS)", "content": "Problem Statement: Optimize real-time ride assignments for multiple vehicles and passengers in an urban environment, balancing efficiency, fuel use, and service quality.\nProblem Specification:\nCity Map: A graph G = (V, E) where locations V and roads E have distances and travel times.\nRide Requests: A set of requests R, each defined by:\nPassenger ID, pickup/drop-off locations, time windows.\nVehicles: A set of available vehicles K, each with:\nLocation, battery/fuel level, passenger capacity and speed."}, {"title": "P4: Urban Ride Sharing (URS) with Disruptions", "content": "Problem Specification: Details are provided including two disruption scenarios: airport route traffic delay and local road closure. Other possible disruptions could involve cancelation of passengers or late arrivals."}, {"title": "P5: Wedding Reunion", "content": "Problem Specification: Table 5 presents a coordinated wedding event travel problem. Several friends arrive at different times and locations before a 3:00 PM wedding photo session. The challenge includes managing two vehicles for airport pick-ups (aimed at those who cannot drive or wish to cut costs) and completing critical errands, such as collecting the wedding gift and retrieving formal attire from the tailor. All activities must be scheduled to ensure that everyone arrives at the wedding venue before the photo time. This problem introduces more constraints than the URS problems in P3 and P4, and it also lays the groundwork for a more challenging disruption case discussed in P8."}, {"title": "P6: Thanksgiving Dinner", "content": "Consider a Thanksgiving dinner scenario in which a family of five must return home in a Boston suburb for dinner. The problem involves coordinating departure times, managing travel logistics (including possible traffic delays), and ensuring timely arrival. formalizes these challenges as a sequential planning problem.\nThis scenario also lays the groundwork for a more advanced disruption case, which has proven difficult for standalone LLMs, as discussed in P9.\nProblem Specification:\nSetup:\n* Mom (Sarah) hosts dinner at 6:00 PM in Boston.\n* Family arrivals:\n* Dad (James) from San Francisco, lands at 1:00 PM ET.\n* Sister (Emily) from Chicago lands at 2:30 PM.\n* Brother (Michael) driving from NY arrives at 3:00 PM.\n* Grandma, who is healthy, needs to pick up nearby.\n* Constraints:\n* James must rent a car post-landing.\n* Emily needs an airport pickup (no alternatives).\n* Turkey requires 4 hours to cook; someone must be home once it's in the oven for safety."}, {"title": "P7: Disaster Relief Logistics Problem", "content": "Problem Specification: summarizes the problem."}, {"title": "P8: Wedding Reunion with Disruptions", "content": "Problem Extension: This problem extends P5 with road closures and dynamic rerouting.\nThe disruption scenario becomes more challenging because, when a road closure is announced, the planner must know each vehicle's current location to determine whether it is affected. Since LLMs are inherently stateless, they cannot keep track of previous scheduling events and thus struggle to adapt the plan in real-time."}, {"title": "P9: Thanksgiving Dinner with Disruptions", "content": "This problem extends P6 by introducing flight delays. Specifically, when a flight from SFO to BOS is delayed by t hours, the new arrival time is confirmed at the originally expected arrival time minus the flight's scheduled duration. Although this early notice provides an opportunity to adjust travel and dinner plans, current LLM-based systems fail to leverage this information in a timely manner, only beginning to react at the original arrival time and missing the window for earlier intervention."}, {"title": "P10: Global Supply Chain", "content": "presents a comprehensive problem in data center GPU deployment that captures the complexity of large-scale infrastructure projects. The objective is to complete a 1 million GPU data center in 15 months while minimizing total costs. The problem encompasses procurement decisions between NVIDIA (15k/unit) and AMD (10k/unit) GPUs, where each vendor has different maintenance risks (20% vs 50% of unit price over one year) and quarterly shipment capacities.\nThe construction process is organized around 50,000 GPU clusters, which require coordinated deployment of power, cooling, and networking infrastructure. Each cluster demands significant resources: 150 MW of power capacity and 1 million gallons per day of cooling water. Infrastructure development follows strict dependencies: Power and cooling systems must be operational before networking installation can begin, and each cluster must complete testing before becoming operational."}, {"title": "P11: Stock Prediction/Forecasting", "content": "Consider a stock market prediction scenario where an automated system must forecast future stock prices while integrating multiple data streams and accounting for market dynamics. The problem involves processing real-time data, managing prediction updates, and responding to market events. formalizes this as a sequential planning problem, presenting a comprehensive framework for building an adaptive prediction system. The problem statement details the requirements for creating a robust prediction pipeline that can handle real-time market data, maintain high accuracy, and adapt to changing market conditions. A sample workflow demonstrating the system's architecture and data flow is provided in Table 9.\nProblem Specification:\nSetup:\nObjective: Predict a portfolio of stock prices for time t, enabling actionable insights for risk management, investment decisions, and automated trading.\nHistorical Data:\n* Training period: 2010-2020.\n* Validation period: 2021-2023.\n* Real-time data feed required during operation.\nData Sources: Yahoo Finance, SEC EDGAR, news feeds, macroeconomic indicators.\nRequired Features:\nTechnical Indicators: MA, MACD, RSI, Bollinger Bands.\nMarket Context: Sector indices, competitor stock performance, correlation metrics.\nExternal Events: Economic reports, policy changes, earnings announcements.\nSentiment Analysis: Real-time news, social media, financial disclosures.\nConstraints:\nProcessing latency: Under 5 minutes for real-time predictions.\nPrediction confidence interval: 95% confidence bounds required."}, {"title": "4.4 Execution Example: Urban Ride Sharing", "content": "For a benchmark suite, we encourage users to devise novel methodologies to solve the problems. However, for this illustrative example of Urban Ride Sharing (URS) (P3), we employ a customized specification language that extends PDDL (Planning Domain Definition Language) and workflow networks to support:\n* Dynamic constraints and real-time updates.\n* Integration with streaming data sources.\n* Explicit representation of uncertainty.\n* Temporal and spatial dependencies.\nThe solver, whether manual or automatic, approaches the planning problem in three steps:\n1. Convert the problem statement into a formal specification:\n* Key objectives and constraints.\n* Required resources and their capabilities.\n* Performance metrics and success criteria.\n* Temporal and spatial dependencies.\n2. Transform the specification into a workflow graph:\n* Nodes represent processing stages, decision points, or actions.\n* Edges capture dependencies, data flow, and execution sequence.\n* Agents are assigned to both nodes and edges.\n* Concrete parameters and thresholds are specified.\n3. Select and apply solving algorithms:\n* Test multiple solution approaches (e.g., dynamic programming, Monte Carlo).\n* Evaluate solutions against specified metrics.\n* Select and validate the best-performing solution.\n* Present results with performance analyses.\nWe walk through these three steps to solve the URS problem."}, {"title": "4.4.1 URS Problem Specification", "content": "Already presented in Table 3."}, {"title": "4.4.2 URS Workflow", "content": "Figure 2 shows the workflow representation of the URS problem."}, {"title": "4.4.3 URS Results", "content": "Figure 3 presents an optimal solution with a total travel distance of 87 km, outperforming both GPT-40-Task and DeepSeek R1 (Figure 4), which require 123 km. This represents a significant improvement of 41.37%."}, {"title": "5 Conclusion", "content": "REAL-Bench represents a significant step toward systematically evaluating AI systems' capabilities in real-world planning scenarios. By providing 11 carefully designed problems that progress in complexity, the benchmark enables researchers to:\nAssess planning capabilities in multiple dimensions of difficulty.\nTest system performance on real-world planning challenges.\nEvaluate handling of unexpected interruptions and adaptations.\nCompare different approaches using standardized metrics.\nThe benchmarks are designed to be both tractable for systematic evaluation and challenging for current systems. Each problem can be scaled along multiple dimensions, including the number of parallel threads, complexity of dependencies, and frequency of disruptions, allowing researchers to progressively stress-test their systems. Inclusion of validation metrics and baseline implementations facilitates meaningful comparisons between different approaches.\nLooking ahead, we envision this benchmark suite evolving with community contributions and feedback. Future extensions might include more complex scenarios, additional evaluation metrics, and expanded validation tools. For instance, in many workflows, transaction properties must be preserved:\nAtomicity: An operation either completes entirely or not at all, with no partial execution state (e.g., a ride-sharing trip must either complete fully or be canceled entirely).\nIdempotency: Multiple identical requests produce the same outcome as a single request, preventing duplicate actions (e.g., multiple identical order submissions should not result in multiple orders).\nMost importantly, by providing a common framework for evaluating planning capabilities of both individual LLMs and multi-agent systems, we hope to accelerate progress toward more robust and capable AI planning systems that can handle real-world complexity and uncertainty. The REALM benchmark suite, along with detailed documentation and baseline implementations, will be available as an open source resource after the peer review process."}, {"title": "Appendices", "content": "In these Appendices, we provide sample implementations of selected problems, illustrating that each problem specification is implementable and can produce feasible solutions. This serves as a verification of completeness for the problem definitions. We encourage readers to review the code and use it as a reference for designing improved solutions to these challenges.\nThe three problems selected are P3, P4, and P11, representing sequential planning, reactive planning, and the most complex planning scenario, respectively."}, {"title": "A P3 & P4: Urban Ride Sharing Sample Implementation", "content": "This appendix presents an implementation of P3 and P4: Urban Ride Sharing without and with interrupts, using LangGraph [15]."}, {"title": "A.1 Agentic Workflow Formulation", "content": "In this first stage, we define agents to manage the nodes of the workflow, including data collection, route planning, vehicle dispatch, traffic adjustment, monitoring and alert, and logging agents. At the end, we use the >> syntax to specify dependencies among agents. The transition from problem specifications to workflow formulation is handled automatically by MACI [6] in LangGraph."}, {"title": "A.2 P3: Execute Meta Plan without Disruption", "content": "Now that the meta-plan workflow has been constructed, the second step involves providing real data for workflow execution. The following code snippet illustrates how vehicle and passenger locations are specified, followed by the corresponding agent executions."}, {"title": "A.3 P4: Execute Meta Plan with Disruption", "content": "This section depicts the execution of the meta-plan (workflow) under traffic disruption. In this scenario, the system dynamically identifies traffic delays and integrates real-time updates into the planning process. Each agent plays a crucial role: the data collection agent continuously monitors traffic conditions; the route planning agent adjusts travel routes based on congestion data; and the dispatch agent ensures that vehicle assignments are optimized despite delays. The workflow demonstrates how coordinated agent interventions, including dynamic rerouting and schedule adjustments, effectively mitigate the impact of heavy traffic, maintaining timely transportation, and enhancing overall operational efficiency."}, {"title": "B P11: Stock Prediction Sample Implementation on LangGraph", "content": "This appendix presents an implementation of the P11 Stock Prediction problem using LangGraph [15]. The implementation demonstrates how AI-driven planning can be applied to financial forecasting by leveraging historical data, technical indicators, and real-time market updates."}, {"title": "B.1 Agentic Workflow Formulation", "content": "In this first stage, we define agents to manage the nodes of the workflow, including data collection, feature extraction, model training, integration, and alert. we use the >> syntax to specify dependencies among agents."}, {"title": "B.2 Workflow Execution With Historical Data for Prediction", "content": "Following the dependency graph presented above (also see Figure 1), the agents are executed in sequence: starting with data collection, then feature extraction, followed by model training, and finally prediction. For brevity, while data for five stocks is provided, we illustrate the execution flow using only APPL."}, {"title": "B.3 Output: Alert Generation", "content": "Based on the problem specification in Section 4.3, when the prediction confidence is \u2265 85%, a recommended trading action is sent to the subscriber as an alert.\nBased on the above prediction, on 1/3, 1/5, and 1/7 the prediction confidence exceeds the threshold and the stock's moving direction is up, down, and up, respectively. Therefore, the alerts issued before the opening of the market on those three days are: BUY, SELL, and STRONG BUY, respectively."}]}