{"title": "Efficient Training with Denoised Neural Weights", "authors": ["Yifan Gong", "Zheng Zhan", "Yanyu Li", "Yerlan Idelbayev", "Andrey Zharkov", "Kfir Aberman", "Sergey Tulyakov", "Yanzhi Wang", "Jian Ren"], "abstract": "Good weight initialization serves as an effective measure to reduce the training cost of a deep neural network (DNN) model. The choice of how to initialize parameters is challenging and may require manual tuning, which can be time-consuming and prone to human error. To overcome such limitations, this work takes a novel step towards building a weight generator to synthesize the neural weights for initialization. We use the image-to-image translation task with generative adversarial networks (GANs) as an example due to the ease of collecting model weights spanning a wide range. Specifically, we first collect a dataset with various image editing concepts and their corresponding trained weights, which are later used for the training of the weight generator. To address the different characteristics among layers and the substantial number of weights to be predicted, we divide the weights into equal-sized blocks and assign each block an index. Subsequently, a diffusion model is trained with such a dataset using both text conditions of the concept and the block indexes. By initializing the image translation model with the denoised weights predicted by our diffusion model, the training requires only 43.3 seconds. Compared to training from scratch (i.e., Pix2pix), we achieve a 15x training time acceleration for a new concept while obtaining even better image generation quality.", "sections": [{"title": "1 Introduction", "content": "Efficient training for deep neural networks (DNN) not only accelerates the model development process but also reduces the requirements for computational resources and costs. Many prior works have investigated efficient training strategies, such as sparse training [2, 9, 12, 21, 22, 35, 38, 40, 42] and low-bit training [34, 37, 41]. However, achieving efficient training is often hindered by challenges in initializing model weights effectively. While some efforts have been conducted in the domain of weight initialization [1, 6, 7, 10, 17, 43], determining the appropriate schemes to use across different tasks remains challenging. Tuning parameters for weight initialization can be time-consuming and prone to human error, leading to sub-optimal performance and increased training time."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Efficient Training", "content": "Efficient training of DNNs has been a central point in machine learning research, aiming to reduce computational costs and memory requirements during model training while maintaining model performance. Sparse training methods [2, 9, 12, 21, 22, 35, 38, 40, 42] explore faster DNN training by applying sparse masks to the model. Static sparse training [22, 35, 38] executes traditional training after first pruning the model with a fixed sparse mask, which typically results in lower accuracy and higher computation and memory consumption for the pruning stage. On the contrary, dynamic mask methods [2, 9, 12] start with a sparse model structure from an untrained dense model and then combine sparse topology exploration with the sparse model training, which adjusts the sparsity topology during training while maintaining a low memory footprint. Besides applying sparse masks on weights and gradients, some recent works [21, 40, 42] also investigate incorporating data efficiency with different data selection approaches for better training accelerations. Meanwhile, another direction of the research explores low-bit training of DNNs to pursue model training efficiency [34, 37, 41]. However, using lower precision typically leads to an accuracy drop. A good weight initialization is essential to stabilize training, enable a higher learning rate, accelerate convergence, and improve generalization. Existing works explore rescaling paradigms [1, 7, 17, 43] or leverage the relationship between layers [6, 10]. However, determining schemes to use is still a challenging task and prone to human errors for different tasks. Meanwhile, LoRA methods [8, 16] aim to exploit the inherent low-rank structure present in DNN weight matrices to reduce computational complexity and memory requirements for fine-tuning from a pre-trained model on a specific, smaller dataset to specialize its performance on a particular task or domain. By keeping the original model unchanged and adds small, changeable parts to each layer of the model, LoRA methods can significantly reduce the number of parameters and operations required for forward and backward passes, which serves as an effective complement in the efficient training direction."}, {"title": "2.2 HyperNetwork", "content": "HyperNetworks have emerged as a promising approach in the field of generative AI by generating model parameters. HyperDreamBooth [30] introduces a HyperNetwork capable of generating personalized weights from a single image. By leveraging these weights within the diffusion model, Hyper DreamBooth enables"}, {"title": "3 Motivations and Challenges", "content": "Effective weight initialization is crucial for stabilizing training, facilitating a faster learning rate, expediting convergence, and enhancing generalization ability. However, identifying good weight initializations across different tasks remains challenging. Inspired by recent advances in HyperNetwork, we hope to investigate whether we can build a weight generator to obtain good weight initialization, thus reducing training time and resource consumption. Unlike the popular image/video generation, little research effort has been paid to explore weight generation. Building such a weight generator is promising yet challenging. The first significant challenge comes from the different layer types within DNN architectures. The weights in each layer exhibit diverse sizes and shapes, necessitating a weight generation approach capable of accommodating this heterogeneity. Second, the weight generator must possess the capacity to generate a substantial number of parameters efficiently, ensuring comprehensive coverage across the network. Third, the inference of the weight generator should be fast and efficient to save time in obtaining the weights for a new task. Addressing these challenges holds promise for building better DNN training paradigms with higher efficiency and effectiveness of deep learning systems. Thus, in this work, we study the construction of the weight generator for better weight initializations. We aim to show the generation ability not only restricted to the weight initialization for a single model architecture on a certain dataset, such as ResNet-18 on CIFAR-10 as in [39], but across the models for different tasks. To achieve this, we take the generation of initialization weights for GANs for image-to-image translation tasks as an example to show our methods due to the ease of collecting diverse datasets for the GAN models. Our method is not restricted to the GAN architecture or the image-to-image translation task."}, {"title": "4 Method", "content": "Our objective is to train a weight generator to predict the weight initializations for different tasks. We take GANs for image-to-image translation tasks as an example to demonstrate the effectiveness of our method. When there is a new"}, {"title": "4.1 Dataset Collection", "content": "In order to effectively train a weight generator for generating weight initializations of GAN models across various concepts, we need to collect a large-scale ground-truth weight value dataset for different concepts. To obtain the ground-truth weight value dataset, a large-scale prompt dataset becomes crucial. By using the concepts/styles in the prompt dataset, we can achieve image collection with diffusion models to obtain a substantial collection of images representative of each target concept. The images for each concept/style are further leveraged to train the GANs for the obtaining of the ground-truth GAN weights.\nAs the foundation of data preparation for weight generator training, the prompt dataset should include diverse visual concepts/styles to enable the weight generator to learn comprehensive representations for initializing GANs tailored"}, {"title": "4.2 Data Format Design for Weight Generator", "content": "To train a weight generator capable of efficiently producing weight initializations for GAN models across diverse concepts, it is important to design the weight format for both training and inference. The objective is whenever a new concept is provided as the input to the weight generator, it can generate the weight initialization of all layers for the concept. Given there exist multiple different types of layers within the model such as fully connected (FC), convolutional (CONV), and batch normalization (BN) layers, and the varying sizes and dimensions across the layers, designing the appropriate data format becomes crucial and challenging. Furthermore, the scale of weights in a GAN model is typically on the scale of millions, posing more challenges to the data format design.\nA larger amount of weights to be predicted leads to more difficulties for the weight generator. To alleviate this, we apply Low-Rank Adaptation (LoRA) [16] to different layers to greatly reduce the number of weights to be predicted. For instance, for a CONV layer i with weights $w_i \\in \\mathbb{R}^{c \\times f \\times k_h \\times k_w}$, we apply two low-rank matrices with rank $r_i$, i.e. $W_A \\in \\mathbb{R}^{c \\times r_i \\times k_h \\times k_w}$ as LoRA down layer,"}, {"title": "4.3 Weight Generator Training", "content": "Using our dataset of weight values, we train a generative model that learns to provide the weight initializations for other concepts/styles. We model the weight initialization space of GANs through a diffusion process. The generator is a UNet weight information creator $\\hat{e}_{\\theta}$ parameterized by $\\theta$ for 1-dimensional vectors, which is demonstrated in Fig. 2. We diffuse the weight block $w_n$ from a real weight distribution $p(w_n)$ into a noisy version and train the denoising UNet to gradually reverse this process, generating weights from Gaussian noise. The training can be formulated as the following noise prediction problem:\n$\\min_{\\theta} E[||\\hat{e}_{\\theta}(w^t_n, t, n, \\tau(T)) - \\epsilon||^2],\\text{                  }(1)$\nwhere $t$ refers to the time step; $\\epsilon$ is the ground-truth noise; $w^t_n = a_tw_n + \\sigma_t \\epsilon$ is the noisy weight for block $n$; $a_t$ and $\\sigma_t$ are the strengths of signal and noise,"}, {"title": "4.4 Fast Fine-Tuning with Generated Weight Initializations", "content": "When a new concept/style $T$ arises, the weight initializations can be obtained by conducting inference for the trained weight generator $\\hat{e}_{\\theta}$ for each weight block $n$. To achieve fast acquisition of weight initializations, we employ a direct reconstruction method to avoid the iterative denoising process. More specifically, at the selected time stept that leans to the noise side, we forward the denoising diffusion model to predict the noise $\\hat{e}_{\\theta}(w^t_n, t, n, \\tau(T))$, and we conduct a direct recovery to obtain the real weight $w_n$:\n$\\hat{w}_n = \\frac{1}{a_t}(w^t_n - \\sigma_t\\hat{e}_{\\theta}(w^t_n, t, n, \\tau(T))).\\text{             }(3)$\nAfter conducting inference for all of the $N$ weight blocks, we can obtain the weight initialization $\\{w_n\\}_{n=1}^N$ for the concept/style $T$. To capture the details of the new concept/style better, a further fine-tuning process for the GAN weights is leveraged with the conditional GAN loss as follows\n$\\min_{W_{lora}} \\max_{W_d} \\mathbb{E}_{x, x_T, z, T} [||\\hat{x}_T - G(x, z, T; w_g, W_{lora})||_1] +\\newline \\qquad \\mathbb{E}_{x, x_T} [log D(x, x_T; w_d)] + \\mathbb{E}_{x, z, T} [log(1 - D(x, G(x, z, T; w_g); w_d))],\\text{            }(4)$\nwhere $\\hat{x}_T$ denotes images generated by the diffusion model conditioned on the concept T of the target style, G is the generator with original weights $w_g$, and"}, {"title": "5 Experiments", "content": "In this section, we provide the detailed experimental settings, results of our proposed method compared to baseline methods, and the ablation studies. More details as well as some ablation studies can be found in the Appendix."}, {"title": "5.1 Experiment Settings", "content": "Baselines. We compare our method with image-to-image translation methods like pix2pix [18] (image generator with 9 ResNet blocks), pix2pix-zero-distilled that distills Co-Mod-GAN [45] from pix2pix-zero [25], and efficient GAN training methods E2GAN [14].\nPrompt Dataset Preparation. We first use ChatGPT-3.5 [4] to collect prompts for the three categories: 1) art, 2) characteristic, and 3) facial modification concepts as discussed in Sec. 4.1. After filtering out repeated and unmeaning prompts, we get 226 art concepts, 441 character concepts, and 26 facial modification concepts. We reserve 20 art concepts, 20 character concepts, and 5 facial modification concepts for test use, never used during the weight generator training. By combining the concepts across different categories that are not reserved as test concepts and filtering, the prompt dataset is enriched with another 84477 concepts. We further augment the obtained concepts with Vicuna [5] for concepts with the same meaning but different expressions and filter meaningless ones, which leads to an additional 4126 augmented art concepts, 8070 augmented characteristic concepts, and 245 augmented facial modification concepts.\nPaired Image Preparation. After the prompt dataset is collected, we generate images for GAN training for each concept. We verify our method on 1,000 images from the FFHQ dataset [19] with image resolution as 256 \u00d7 256. The images in the target domain are generated with several different text-to-image diffusion models, including Stable Diffusion [29], Instruct-Pix2Pix [3], Null-Text"}, {"title": "5.2 Experimental Results", "content": "Qualitative Results. The synthesized images in the target domain obtained by our method and other methods are shown in Fig. 3. The original images are listed in the leftmost column, and the synthesized images for the target concept obtained by diffusion models, pix2pix, pix2pix-zero-distilled, E2GAN, and ours are shown from top to bottom. The tasks span a wide range, such as changing the age, artistic styles, and characteristic styles. According to the results, the models obtained by ours can modify the original images to the target concept domain by fast fine-tuning with the weight initializations from the weight generator. For instance, for the Jacob Lawrence paintings prompt on the FFHQ dataset, our model generates more meaningful images compared to all baseline methods. As for the albino person prompt, our method edits the image as desired while having fewer artifacts. We provide more qualitative results in the Appendix.\nQuantitative Results. We compare the quantitive results and training time consumption between our method and other baseline methods, and the results are provided in Tab. 1. Note that for each concept, pix2pix-zero-distilled and pix2pix are trained on the whole training dataset of 800 samples"}, {"title": "Ablation Studies", "content": "We conduct ablation studies on the block size and weight grouping rules. Due to the huge training cost of the weight generator on the entire training dataset, we conduct small-scale experiments for the ablation study. We overfit the weight generator solely on the weights corresponding to a particular concept of interest. The approach provides a precise assessment of how well a particular configuration captures a specific concept. Any discrepancies between the overfitted results and the ground-truth values can be attributed directly to the efficacy of the chosen configuration.\nFor the block size study, we investigate different block size settings including 128, 256, and 512, on two randomly selected concepts grey hair and Batik. The block size selection is based on the size of all layers in the GAN model. The results show that the block size selection has an impact on the weight generation performance. Setting a larger block size leads to a faster weight generation process. However, the FID performance is the best when the block size is set as 256, while the generation time is slightly slower than a block size of 512. The results indicate that the appropriate selection of the block size to divide grouped weights is important for achieving good performance of the weight generator.\nFor the weight grouping before weight division, we study 3 different rules including 1) group the LORA down layer, LoRA up layer, and the following BN layer if applicable for each layer i to one group, and append the reshaped 1-dimensional weight vectors one by one; 2) group the LoRA down layer, LoRA up layer, and the following BN layer if appli"}, {"title": "6 Conclusion", "content": "This paper studies to generate good weight initializations with a weight generator to reduce the training cost of a DNN. Leveraging the image-to-image translation task with GANs as a case study, we demonstrate the feasibility and effectiveness of our approach. Through the division of weights into equal-sized blocks and the incorporation of block indexes, we mitigate the complexity of varied layer characteristics and a large number of weights. By training a diffusion process with both textual concept conditions and block indexes, the weight generator produces weight initializations for new concepts/styles efficiently with a one-step direct recovery. We conduct extensive experiments on different concepts to demonstrate the effectiveness of our proposed framework. By leveraging the synthesized weight initializations, we can achieve better FID performance with much fewer training costs across various concepts/styles than baseline methods including conventional GAN training and efficient GAN training approaches.\nWe reduce the time consumption for obtaining the model for a new concept by 4.6\u00d7 while improving the FID performance by 3.93 than efficient GAN training baseline and reduce the total training time by 15x than training from scratch (i.e., Pix2pix [18]) with better FID performance."}, {"title": "7 Discussion of limitations", "content": "The development of a weight generator to synthesize improved weight initializations can increase the efficiency and efficacy of model training. To prepare the training data for the weight generator, we leverage diffusion models to edit real images, thereby obtaining edited images that encompass a wide range of concepts. This approach allows us to create paired data spanning various concepts/styles, which provide the foundation for training diverse GAN weights for different generation domains. However, the quality of the generated images plays a pivotal role in influencing the performance of the trained GAN model, consequently impacting the performance of the weight generator. While diffusion models offer a powerful tool for image editing, the quality and fidelity of the generated images may not always meet the desired standards. Furthermore, utilizing diffusion models for data collection remains expensive. Developing efficient techniques to rapidly construct well-paired and high-quality images from diffusion models would greatly enhance the training of the weight generator."}]}