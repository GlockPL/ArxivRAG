{"title": "Efficient Training with Denoised Neural Weights", "authors": ["Yifan Gong", "Zheng Zhan", "Yanyu Li", "Yerlan Idelbayev", "Andrey Zharkov", "Kfir Aberman", "Sergey Tulyakov", "Yanzhi Wang", "Jian Ren"], "abstract": "Good weight initialization serves as an effective measure to reduce the training cost of a deep neural network (DNN) model. The choice of how to initialize parameters is challenging and may require manual tuning, which can be time-consuming and prone to human error. To overcome such limitations, this work takes a novel step towards building a weight generator to synthesize the neural weights for initialization. We use the image-to-image translation task with generative adversarial networks (GANs) as an example due to the ease of collecting model weights spanning a wide range. Specifically, we first collect a dataset with various image editing concepts and their corresponding trained weights, which are later used for the training of the weight generator. To address the different characteristics among layers and the substantial number of weights to be predicted, we divide the weights into equal-sized blocks and assign each block an index. Subsequently, a diffusion model is trained with such a dataset using both text conditions of the concept and the block indexes. By initializing the image translation model with the denoised weights predicted by our diffusion model, the training requires only 43.3 seconds. Compared to training from scratch (i.e., Pix2pix), we achieve a 15x training time acceleration for a new concept while obtaining even better image generation quality.", "sections": [{"title": "1 Introduction", "content": "Efficient training for deep neural networks (DNN) not only accelerates the model development process but also reduces the requirements for computational resources and costs. Many prior works have investigated efficient training strategies, such as sparse training [2, 9, 12, 21, 22, 35, 38, 40, 42] and low-bit training [34, 37, 41]. However, achieving efficient training is often hindered by challenges in initializing model weights effectively. While some efforts have been conducted in the domain of weight initialization [1, 6, 7, 10, 17, 43], determining the appropriate schemes to use across different tasks remains challenging. Tuning parameters for weight initialization can be time-consuming and prone to human error, leading to sub-optimal performance and increased training time."}, {"title": "2 Related Work", "content": "Efficient training of DNNs has been a central point in machine learning research, aiming to reduce computational costs and memory requirements during model training while maintaining model performance. Sparse training methods [2, 9, 12, 21, 22, 35, 38, 40, 42] explore faster DNN training by applying sparse masks to the model. Static sparse training [22, 35, 38] executes traditional training after first pruning the model with a fixed sparse mask, which typically results in lower accuracy and higher computation and memory consumption for the pruning stage. On the contrary, dynamic mask methods [2, 9, 12] start with a sparse model structure from an untrained dense model and then combine sparse topology exploration with the sparse model training, which adjusts the sparsity topology during training while maintaining a low memory footprint. Besides applying sparse masks on weights and gradients, some recent works [21, 40, 42] also investigate incorporating data efficiency with different data selection approaches for better training accelerations. Meanwhile, another direction of the research explores low-bit training of DNNs to pursue model training efficiency [34, 37, 41]. However, using lower precision typically leads to an accuracy drop. A good weight initialization is essential to stabilize training, enable a higher learning rate, accelerate convergence, and improve generalization. Existing works explore rescaling paradigms [1, 7, 17, 43] or leverage the relationship between layers [6, 10]. However, determining schemes to use is still a challenging task and prone to human errors for different tasks. Meanwhile, LoRA methods [8, 16] aim to exploit the inherent low-rank structure present in DNN weight matrices to reduce computational complexity and memory requirements for fine-tuning from a pre-trained model on a specific, smaller dataset to specialize its performance on a particular task or domain. By keeping the original model unchanged and adds small, changeable parts to each layer of the model, LoRA methods can significantly reduce the number of parameters and operations required for forward and backward passes, which serves as an effective complement in the efficient training direction."}, {"title": "2.2 HyperNetwork", "content": "HyperNetworks have emerged as a promising approach in the field of generative AI by generating model parameters. HyperDreamBooth [30] introduces a HyperNetwork capable of generating personalized weights from a single image. By leveraging these weights within the diffusion model, Hyper DreamBooth enables the generation of personalized faces with high subject details in diverse styles and contexts followed by a fast fine-tuning process. HyperDiffusion [11] operates directly on MLP weight instead of directly applying generative modeling on implicit neural fields. It first collects a dataset of neural field multilayer perceptrons (MLPs) overfitting on 3D or 4D shapes. The dataset is then used to train the HyperNetwork, which is an unconditional generative model, to predict the MLP weights for 3D or 4D shape generation. Neural Network Diffusion [39] works on generating model weights for image classification tasks. However, the weight generation is based on first collecting multiple trained model weights for specific model architecture on the target dataset. Furthermore, it only works on generating the weights for two normalization layers."}, {"title": "3 Motivations and Challenges", "content": "Effective weight initialization is crucial for stabilizing training, facilitating a faster learning rate, expediting convergence, and enhancing generalization ability. However, identifying good weight initializations across different tasks remains challenging. Inspired by recent advances in HyperNetwork, we hope to investigate whether we can build a weight generator to obtain good weight initialization, thus reducing training time and resource consumption. Unlike the popular image/video generation, little research effort has been paid to explore weight generation. Building such a weight generator is promising yet challenging. The first significant challenge comes from the different layer types within DNN architectures. The weights in each layer exhibit diverse sizes and shapes, necessitating a weight generation approach capable of accommodating this heterogeneity. Second, the weight generator must possess the capacity to generate a substantial number of parameters efficiently, ensuring comprehensive coverage across the network. Third, the inference of the weight generator should be fast and efficient to save time in obtaining the weights for a new task. Addressing these challenges holds promise for building better DNN training paradigms with higher efficiency and effectiveness of deep learning systems. Thus, in this work, we study the construction of the weight generator for better weight initializations. We aim to show the generation ability not only restricted to the weight initialization for a single model architecture on a certain dataset, such as ResNet-18 on CIFAR-10 as in [39], but across the models for different tasks. To achieve this, we take the generation of initialization weights for GANs for image-to-image translation tasks as an example to show our methods due to the ease of collecting diverse datasets for the GAN models. Our method is not restricted to the GAN architecture or the image-to-image translation task."}, {"title": "4 Method", "content": "Our objective is to train a weight generator to predict the weight initializations for different tasks. We take GANs for image-to-image translation tasks as an example to demonstrate the effectiveness of our method. When there is a new concept/style, we can query the weight generator to provide the weight values for the initialization. The weight generator is modeled with a diffusion process, as illustrated in Fig. 1. Different from image diffusion models that reverse a clean image from a purse noise, our framework targets turning the noise into weight values used for the initialization. By plugging in the predicted weight values, a fast fine-tuning process is conducted to achieve the efficient training of the GAN models for the target style. The core of our framework is the design of the weight generator. To build this weight generator, we elaborate on how to create the training dataset for the weight generator in Sec. 4.1, the data format for the training and inference of the weight generator in Sec. 4.2, the architecture and training objective of the weight generator in Sec. 4.3, and the fine-tuning process after the weight prediction is Sec. 4.4."}, {"title": "4.1 Dataset Collection", "content": "In order to effectively train a weight generator for generating weight initializations of GAN models across various concepts, we need to collect a large-scale ground-truth weight value dataset for different concepts. To obtain the ground-truth weight value dataset, a large-scale prompt dataset becomes crucial. By using the concepts/styles in the prompt dataset, we can achieve image collection with diffusion models to obtain a substantial collection of images representative of each target concept. The images for each concept/style are further leveraged to train the GANs for the obtaining of the ground-truth GAN weights.\nAs the foundation of data preparation for weight generator training, the prompt dataset should include diverse visual concepts/styles to enable the weight generator to learn comprehensive representations for initializing GANs tailored to specific tasks. However, the process of collecting such a dataset poses great challenges. Ensuring diversity and representativeness across different concepts/styles demands considerable data. Moreover, the collected prompts are further used to generate images in the target concept/style with diffusion models.\nTo construct our prompt dataset for training a reliable weight generator for GAN weight initialization, we adopt a systematic approach that integrates both large language models (LLMs) for style generation and augmentation to ensure richness and diversity in conceptual representation. We begin by sketching out three broad categories: 1) art concepts, 2) characteristic concepts, and 3) facial modification concepts. Within each category, we leverage a large language model (ChatGPT-3.5 [4]) to ask it to generate a spectrum of textual descriptions encompassing various concepts. By filtering out redundant concepts/styles, we further conduct an augmentation method by querying a large language model (Vicuna [5]) to provide concepts/styles with similar meanings but different representations. To further enrich the prompt dataset, we permute and combine concepts/styles across different categories. Through the process, we are able to curate a large-scale prompt dataset that not only spans diverse conceptual domains but also captures intricate stylistic differences, providing the foundation for the training of the weight generator for better weight initialization.\nAfter the prompt dataset collection, we use the diffusion models to edit real images to obtain the edited images for each concept/style in the prompt dataset, forming pairs of data for GAN training. Here, we adopt a generator with a hybrid of ResNet blocks and transformer blocks as in paper [14] due to the effectiveness of the model and the hybrid architecture design to show the generation ability of our method on different types of layers. Following the GAN training process, we build a dataset of weights from GAN checkpoints for different concepts/styles. To further augment the weight value dataset, we save K checkpoints through the training process for each concept/style after the FID performance converges."}, {"title": "4.2 Data Format Design for Weight Generator", "content": "To train a weight generator capable of efficiently producing weight initializations for GAN models across diverse concepts, it is important to design the weight format for both training and inference. The objective is whenever a new concept is provided as the input to the weight generator, it can generate the weight initialization of all layers for the concept. Given there exist multiple different types of layers within the model such as fully connected (FC), convolutional (CONV), and batch normalization (BN) layers, and the varying sizes and dimensions across the layers, designing the appropriate data format becomes crucial and challenging. Furthermore, the scale of weights in a GAN model is typically on the scale of millions, posing more challenges to the data format design.\nA larger amount of weights to be predicted leads to more difficulties for the weight generator. To alleviate this, we apply Low-Rank Adaptation (LoRA) [16] to different layers to greatly reduce the number of weights to be predicted. For instance, for a CONV layer i with weights \\(w_i \\in \\mathbb{R}^{c \\times f \\times k_h \\times k_w}\\), we apply two low-rank matrices with rank \\(r_i\\), i.e. \\(W_A \\in \\mathbb{R}^{c \\times r_i \\times k_h \\times k_w}\\) as LoRA down layer, and \\(W_B \\in \\mathbb{R}^{r_i \\times f \\times 1 \\times 1}\\) as LoRA up layer, to approximate the weight change. By doing so, the total amount of weights to be predicted is reduced from 7.06M to 0.22M. We show that finetuning LoRA weights are sufficient to transfer the generative domain of the GAN model. Though greatly reducing the weight number, directly predicting all 0.22M weights simultaneously through inference of the weight generator once is still challenging. It requires a large weight generator with huge computation and memory burdens.\nTo tackle this, we partition the weights into groups to mitigate the computational complexity and enhance the feasibility of fitting the weight generator into memory during both training and inference. As different layers have different statistical characteristics, we group the LORA down and up layers for each layer i, with the associated BN layers if applicable, into one group. Still, each group has a different number of weights and shapes. Thus, we further flatten the weights into 1-dimensional vectors and divide the weights into N equal-sized blocks, each with b weights. Thus, the data format is denoted as < n, wn, T >, where n is the block index, \\(w_n \\in \\mathbb{R}^b\\) is the flattened 1-dimensional weight vector for the n-th weight block, and T denotes the text prompt of current concept/style. The advantages of using such a data format include: 1) works for different types and shapes of layers; 2) reduces the computation complexity and difficulty for prediction; and 3) makes the weight generator easier to fit into memory."}, {"title": "4.3 Weight Generator Training", "content": "Using our dataset of weight values, we train a generative model that learns to provide the weight initializations for other concepts/styles. We model the weight initialization space of GANs through a diffusion process. The generator is a UNet weight information creator \\(\\hat{c}_\\theta\\) parameterized by \\(\\theta\\) for 1-dimensional vectors, which is demonstrated in Fig. 2. We diffuse the weight block \\(w_n\\) from a real weight distribution \\(p(w_n)\\) into a noisy version and train the denoising UNet to gradually reverse this process, generating weights from Gaussian noise. The training can be formulated as the following noise prediction problem:\n\\[\\min_\\theta E \\[\\lVert \\epsilon - \\epsilon_\\theta(\\bar{w}\\_t, t, n, \\tau(T)) \\rVert^2\\],\n(1)\nwhere t refers to the time step; \\(\\epsilon\\) is the ground-truth noise; \\(\\bar{w}\\_t = \\alpha\\_t w\\_n + \\sigma\\_t \\epsilon\\) is the noisy weight for block n; \\(\\alpha\\_t\\) and \\(\\sigma\\_t\\) are the strengths of signal and noise, respectively, decided by a noise scheduler; \\(\\tau\\) is a frozen text encoder such as CLIP [27]. To incorporate the block index as a further conditioning mechanism in our weight generator, we adopt a strategy inspired by the sinusoidal positional encoding commonly used in sequence-to-sequence models [36]. We compute a sinusoidal block index encoding, which serves to provide the weight generator with information about the position of each weight block within all the model weights. Specifically, let N denote the total number of weight blocks and d denote the dimensionality of the encoding. The sinusoidal block index encoding SinEnc(n, d) for block index n is computed as follows:\n\\[SinEnc(n, 2i) = sin\\left(\\frac{n}{10000^{2i/d}}\\right), \\quad SinEnc(n, 2i + 1) = cos\\left(\\frac{n}{10000^{2i/d}}\\right),\n(2)\nwhere i ranges from 0 to \\(\\lfloor \\frac{d-1}{2} \\rfloor\\). The sinusoidal encoding is then fed into embedding layers to obtain the block index embedding emb_n. Finally, the block index embedding emb_n is combined with the time step embedding emb_t, represented as emb = emb_n + emb_t, to be leveraged in each residual block in the generator. Thus, the weight generator has access to the block index throughout the denoising process. From the results, we observe that the block index n can model the weights from different blocks effectively without the necessity to condition on prior predicted weights, while greatly reducing the computations."}, {"title": "4.4 Fast Fine-Tuning with Generated Weight Initializations", "content": "When a new concept/style T arises, the weight initializations can be obtained by conducting inference for the trained weight generator \\(\\hat{c}_\\theta\\) for each weight block n. To achieve fast acquisition of weight initializations, we employ a direct reconstruction method to avoid the iterative denoising process. More specifically, at the selected time step t that leans to the noise side, we forward the denoising diffusion model to predict the noise \\(\\epsilon\\_\\theta(\\bar{w}\\_t, t, n, \\tau(T))\\), and we conduct a direct recovery to obtain the real weight \\(w_n = \\bar{w}\\):\n\\[w\\_n = \\frac{1}{\\alpha\\_t} \\left(\\bar{w}\\_t - \\sigma\\_t \\epsilon\\_\\theta(\\bar{w}\\_t, t, n, \\tau(T))\\right).\n(3)\nAfter conducting inference for all of the N weight blocks, we can obtain the weight initialization \\(\\{w\\_n\\}\\_{n=1}^N\\) for the concept/style T. To capture the details of the new concept/style better, a further fine-tuning process for the GAN weights is leveraged with the conditional GAN loss as follows\n\\[\\min\\_{\\mathbf{W}\\_{lora}} \\max\\_{\\mathbf{W}\\_d} E\\_{\\mathbf{x}, \\tilde{\\mathbf{x}}, \\mathbf{z}, T} [\\lVert \\tilde{\\mathbf{x}}\\_T - G(\\mathbf{x}, \\mathbf{z}, T; \\mathbf{w}\\_g, \\mathbf{W}\\_{lora}) \\rVert\\_1] + E\\_{\\mathbf{x}, \\tilde{\\mathbf{x}}\\_T} [log D(\\mathbf{x}, \\tilde{\\mathbf{x}}\\_T; \\mathbf{w}\\_d)] + E\\_{\\mathbf{x}, \\mathbf{z}, T} [log(1 - D(\\mathbf{x}, G(\\mathbf{x}, \\mathbf{z}, T; \\mathbf{w}\\_g); \\mathbf{w}\\_d))],\n(4)\nwhere \\(\\tilde{x}\\_T\\) denotes images generated by the diffusion model conditioned on the concept T of the target style, G is the generator with original weights \\(w_g\\), and the LoRA weights \\(w\\_{lora}\\), D denotes the discriminator function parameterized by \\(w_d\\), respectively, z is a random noise introduced to increase the stochasticity of output, and \\(\\lambda\\) can be used to adjust the relative importance between two loss terms. During the fine-tuning process, the generator only optimizes the LORA weights \\(w\\_{lora}\\) which are initialized with the predictions \\(\\{w\\_n\\}\\_{n=1}^N\\). By initializing the GAN weights from predictions, we are able to use much fewer training epochs to reach the same or even better FID performance. Besides fine-tuning after prediction, we also consider incorporating the GAN training loss in Eq. (4) to the weight prediction loss in Eq. (1). However, through experiments, we find out that combining these two loss terms is not able to provide better performance, but leads to more computation costs for training the weight generator."}, {"title": "5 Experiments", "content": "In this section, we provide the detailed experimental settings, results of our proposed method compared to baseline methods, and the ablation studies. More details as well as some ablation studies can be found in the Appendix."}, {"title": "5.1 Experiment Settings", "content": "Baselines. We compare our method with image-to-image translation methods like pix2pix [18] (image generator with 9 ResNet blocks), pix2pix-zero-distilled that distills Co-Mod-GAN [45] from pix2pix-zero [25], and efficient GAN training methods E2GAN [14].\nPrompt Dataset Preparation. We first use ChatGPT-3.5 [4] to collect prompts for the three categories: 1) art, 2) characteristic, and 3) facial modification concepts as discussed in Sec. 4.1. After filtering out repeated and unmeaning prompts, we get 226 art concepts, 441 character concepts, and 26 facial modification concepts. We reserve 20 art concepts, 20 character concepts, and 5 facial modification concepts for test use, never used during the weight generator training. By combining the concepts across different categories that are not reserved as test concepts and filtering, the prompt dataset is enriched with another 84477 concepts. We further augment the obtained concepts with Vicuna [5] for concepts with the same meaning but different expressions and filter meaningless ones, which leads to an additional 4126 augmented art concepts, 8070 augmented characteristic concepts, and 245 augmented facial modification concepts.\nPaired Image Preparation. After the prompt dataset is collected, we generate images for GAN training for each concept. We verify our method on 1,000 images from the FFHQ dataset [19] with image resolution as 256 \u00d7 256. The images in the target domain are generated with several different text-to-image diffusion models, including Stable Diffusion [29], Instruct-Pix2Pix [3], Null-Text Inversion [23], ControlNet [44], and InstructDiffusion [13]. The generated images with the best perceptual quality among diffusion models are selected to form the real images into paired datasets. To perform training and evaluation of GAN models, we divide the image pairs from each target concept into training/validation/test subsets with the ratio as 80%/10%/10%.\nGround-truth GAN Weights Preparation. With the paired images, we collect the ground-truth GAN weights to train the weight generator. We apply LORA to each layer of the generator, leading to 0.2256M weights for each concept to learn. We follow the standard approach that alternatively updates the generator and discriminator [15]. The training is conducted from an initial learning rate of 2e-4 with mini-batch SGD using Adam solver [20]. The total training epochs are set to 100. The obtained weights are grouped following Sec. 4.2 and divided with a block size of 256, resulting in a total of 854 weight blocks. When dividing the weights into equal-sized blocks, zeros are padded when necessary.\nTraining Settings. The weight generator is trained with AdamW optimizer [20]. The initial learning rate is set as le-5, the weight decay is set as 0.01, and the training batch size is set as 512. The weight generator training is conducted with 4 or 8 nodes, each with 8 NVIDIA A100 GPUs with 40GB or 80GB memory. The block size for weight division is set as 256. For the following fine-tuning process, to show a fair comparison with the efficient GAN training approach E2GAN, we adopt the same cluster size as 400 for each concept. The initial learning rate is set as 0.0015 and the fine-tuning epochs are set as 20.\nEvaluation Metric. We compare the performance of our efficient weight generalization by comparing images generated by models obtained via our approach and baseline methods. The evaluation is achieved by calculating Clean FID proposed by [26] on the test sets of the paired images."}, {"title": "5.2 Experimental Results", "content": "Qualitative Results. The synthesized images in the target domain obtained by our method and other methods are shown in Fig. 3. The original images are listed in the leftmost column, and the synthesized images for the target concept obtained by diffusion models, pix2pix, pix2pix-zero-distilled, E2GAN, and ours are shown from top to bottom. The tasks span a wide range, such as changing the age, artistic styles, and characteristic styles. According to the results, the models obtained by ours can modify the original images to the target concept domain by fast fine-tuning with the weight initializations from the weight generator. For instance, for the Jacob Lawrence paintings prompt on the FFHQ dataset, our model generates more meaningful images compared to all baseline methods. As for the albino person prompt, our method edits the image as desired while having fewer artifacts. We provide more qualitative results in the Appendix.\nQuantitative Results. We compare the quantitive results and training time consumption between our method and other baseline methods, and the results are provided in Tab. 1. Note that for each concept, pix2pix-zero-distilled and pix2pix are trained on the whole training dataset of 800 samples"}, {"title": "Ablation Studies", "content": "We conduct ablation studies on the block size and weight grouping rules. Due to the huge training cost of the weight generator on the entire training dataset, we conduct small-scale experiments for the ablation study. We overfit the weight generator solely on the weights corresponding to a particular concept of interest. The approach provides a precise assessment of how well a particular configuration captures a specific concept. Any discrepancies between the overfitted results and the ground-truth values can be attributed directly to the efficacy of the chosen configuration.\nFor the block size study, we investigate different block size settings including 128, 256, and 512, on two randomly selected concepts grey hair and Batik. The block size selection is based on the size of all layers in the GAN model. The results show that the block size selection has an impact on the weight generation performance. Setting a larger block size leads to a faster weight generation process. However, the FID performance is the best when the block size is set as 256, while the generation time is slightly slower than a block size of 512. The results indicate that the appropriate selection of the block size to divide grouped weights is important for achieving good performance of the weight generator.\nFor the weight grouping before weight division, we study 3 different rules including 1) group the LORA down layer, LoRA up layer, and the following BN layer if applicable for each layer i to one group, and append the re-shaped 1-dimensional weight vectors one by one; 2) group the LoRA down layer, LoRA up layer, and the following BN layer if applicable for each layer i to one group, concatenate the weights through the channel dimension and reshape it to 1-dimensional vector; 3) view each layer as a single group and reshape to 1-dimensional vector. Besides the weight grouping rules, all the other settings are the same as the default setting for the main results. We show the comparisons of the 3 rules in Tab. 3. From the results, we can see that Rule 1) and Rule 2) both perform much better than Rule 3), which indicates the importance of grouping the layers belonging to the same layer i into one group. The rationale behind this might be related to the different statistics among different layers. Furthermore, Rule 1) and Rule 2) do not have obvious performance differences, which means after grouping weights for the same layer i together, it is not necessary to take different channels separately. Combining the ablation studies, we get the default setting used in the main results, which correspond to use rule 1) to group weights and set the block size as 256."}, {"title": "6 Conclusion", "content": "This paper studies to generate good weight initializations with a weight generator to reduce the training cost of a DNN. Leveraging the image-to-image translation task with GANs as a case study, we demonstrate the feasibility and effectiveness of our approach. Through the division of weights into equal-sized blocks and the incorporation of block indexes, we mitigate the complexity of varied layer characteristics and a large number of weights. By training a diffusion process with both textual concept conditions and block indexes, the weight generator produces weight initializations for new concepts/styles efficiently with a one-step direct recovery. We conduct extensive experiments on different concepts to demonstrate the effectiveness of our proposed framework. By leveraging the synthesized weight initializations, we can achieve better FID performance with much fewer training costs across various concepts/styles than baseline methods including conventional GAN training and efficient GAN training approaches. We reduce the time consumption for obtaining the model for a new concept by 4.6\u00d7 while improving the FID performance by 3.93 than efficient GAN training baseline and reduce the total training time by 15x than training from scratch (i.e., Pix2pix [18]) with better FID performance."}, {"title": "7 Discussion of limitations", "content": "The development of a weight generator to synthesize improved weight initializations can increase the efficiency and efficacy of model training. To prepare the training data for the weight generator, we leverage diffusion models to edit real images, thereby obtaining edited images that encompass a wide range of concepts. This approach allows us to create paired data spanning various concepts/styles, which provide the foundation for training diverse GAN weights for different generation domains. However, the quality of the generated images plays a pivotal role in influencing the performance of the trained GAN model, consequently impacting the performance of the weight generator. While diffusion models offer a powerful tool for image editing, the quality and fidelity of the generated images may not always meet the desired standards. Furthermore, utilizing diffusion models for data collection remains expensive. Developing efficient techniques to rapidly construct well-paired and high-quality images from diffusion models would greatly enhance the training of the weight generator."}]}