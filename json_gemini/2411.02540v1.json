{"title": "GraphXAIN: Narratives to Explain\nGraph Neural Networks", "authors": ["Mateusz Cedro", "David Martens"], "abstract": "Graph Neural Networks (GNNs) are a powerful tech-\nnique for machine learning on graph-structured data, yet\nthey pose interpretability challenges, especially for non-\nexpert users. Existing GNN explanation methods often\nyield technical outputs such as subgraphs and feature im-\nportance scores, which are not easily understood. Build-\ning on recent insights from social science and other Ex-\nplainable AI (XAI) methods, we propose GraphXAIN, a\nnatural language narrative that explains individual pre-\ndictions made by GNNs. We present a model-agnostic\nand explainer-agnostic XAI approach that complements\ngraph explainers by generating GraphXAINs, using Large\nLanguage Models (LLMs) and integrating graph data, in-\ndividual predictions from GNNs, explanatory subgraphs,\nand feature importances. We define X\u0391\u0399 Narratives and\n\u03a7\u0391\u0399 Descriptions, highlighting their distinctions and em-\nphasizing the importance of narrative principles in ef-\nfective explanations. By incorporating natural language\nnarratives, our approach supports graph practitioners\nand non-expert users, aligning with social science research\non explainability and enhancing user understanding and\ntrust in complex GNN models. We demonstrate GraphX-\nAIN's capabilities on a real-world graph dataset, illustrat-\ning how its generated narratives can aid understanding\ncompared to traditional graph explainer outputs or other\ndescriptive explanation methods.", "sections": [{"title": "1 Introduction", "content": "The exponential growth in the complexity of machine\nlearning models has led to architectures reaching bil-\nlions of parameters, resulting in significant improve-\nments in their performance. [3,8,19,29]. As these com-\nplex 'black-box' models, characterized by their high ac-\ncuracy yet lack of interpretability, continue to evolve,\nthe demand for transparency has intensified [26]. Ex-\nplainable Artificial Intelligence (XAI) has emerged to\naddress this challenge by enhancing the trustworthi-\nness and transparency of complex decision-making pro-\ncesses [12].\nGraph Neural Networks (GNNs) [30] have recently\ngained notable success and have become state-of-the-art\nsolutions for modelling relational data characterized by\nnodes connected via edges [15, 33, 35]. However, the\nneed for interpretability in GNNs remains [16, 18, 36]."}, {"title": "2 Related Work", "content": "Several approaches have been proposed to enhance the\nexplainability of machine learning models across vari-\nous modalities, including image data [1,11,29, 31, 32],\ntabular data [2,19,29], natural language [19,29,32], and\nunstructured data such as graphs [18,36]. One of the\nmost popular XAI methodologies are post-hoc expla-\nnations, which aim to interpret deep learning models\nafter the training stage [26]. These methods encompass\nfeature importance measures, visualisation techniques,\nand surrogate models. For instance, SHAP (SHap-\nley Additive explanations) [19] and LIME (Local In-\nterpretable Model-agnostic Explanations) [29] estimate\nthe contribution of each feature to a particular predic-\ntion, thereby shedding light on the model's decision-\nmaking process for specific instances.\nHowever, despite these advancements, challenges\npersist in ensuring that explanations are both method-\nologically accurate [7] and meaningful to end users\n[20, 27]. Explanations must bridge the gap between"}, {"title": "2.1 Explainability in Machine Learning", "content": "technical complexity and user comprehension, neces-\nsitating a careful balance between fidelity and inter-\npretability [4, 25]. The need for explainability meth-\nods that are understandable to non-technical users is\nprimarily crucial in sensitive domains such as health-\ncare, finance, and legal systems, where understanding\nthe deep learning model's decision-making process is\nessential for trust and transparency [26, 29]."}, {"title": "2.2 Graph Neural Networks Explain-\nability", "content": "GNNs are increasingly used for modelling relational\ndata in domains such as social networks, molecular\nstructures, and knowledge graphs [15, 30, 33]. Their\ncomplex architectures, however, pose challenges for un-\nderstanding and interpreting their predictions. The\nfirst method developed to address this issue is GNNEx-\nplainer [36], which provides instance-level explanations\nby identifying a compact subgraph and relevant node\nfeatures that are most influential for a specific predic-\ntion. GNNExplainer formulates the explanation task as\nan optimisation problem, maximising the mutual infor-\nmation between the explanatory subgraph with a subset\nof node features, and the input graph that is subject to\nexplain.\nAmong other graph explanation methods, Lucic et\nal. (2021) [18] introduced CF-GNNExplainer which al-\nters the GNNExplainer to answer 'what-if' questions\nusing the counterfactual explanation approach. Rather\nthan merely identifying influential features or sub-\ngraphs, CF-GNNExplainer searches for minimal per-\nturbations to the input graph that would alter the\nGNN model's prediction by edge deletion. This method\ndemonstrates how small changes in graph structure or\nnode features impact outcomes, enhancing understand-\ning of the model's decision-making process.\nAlthough the aforementioned state-of-the-art graph\nexplanation frameworks are methodologically sound,\nthey merely provide users with additional graphs and\nnode feature importance values that are not easily inter-\npretable by end-users, thereby limiting their practical\nutility and compelling users to construct the explana-\ntory narrative themselves.\nIncorporating natural language into GNN expla-\nnations could bridge the gap between technical out-\nputs and human understanding by translating complex\nmodel reasoning into accessible narratives, thereby en-\nhancing user comprehension and trust. However, pre-\nvious methods to generate them are not tailored to the\npopular GNNExplainer outputs and/or provide descrip-\ntions rather than narratives (see below for a discussion\nof the important differences). Giorgi et al. (2024) [6]\naddressed this issue by using LLMs to generate tex-\ntual explanations for counterfactual graphs. However,\ntheir explanations lack contextual information and do\nnot illustrate cause-and-effect relationships, resulting in\ndescriptive communication rather than narrative expla-\nnations [4] (see Appendix B.2 for examples).\nHe et al. (2024) [10] used LLMs to generate nat-"}, {"title": "3 Methods", "content": "Incorporating natural language into GNN expla-\nnations could bridge the gap between technical out-\nputs and human understanding by translating complex\nmodel reasoning into accessible narratives, thereby en-\nhancing user comprehension and trust. However, pre-\nvious methods to generate them are not tailored to the\npopular GNNExplainer outputs and/or provide descrip-\ntions rather than narratives (see below for a discussion\nof the important differences). Giorgi et al. (2024) [6]\naddressed this issue by using LLMs to generate tex-\ntual explanations for counterfactual graphs. However,\ntheir explanations lack contextual information and do\nnot illustrate cause-and-effect relationships, resulting in\ndescriptive communication rather than narrative expla-\nnations [4] (see Appendix B.2 for examples).\nHe et al. (2024) [10] used LLMs to generate nat-"}, {"title": "3.1 \u03a7\u0391\u0399 Narratives and Descriptions", "content": "Research in communication theory and psychology indi-\ncates that narrative-based explanations are more acces-\nsible and memorable than descriptive methods, mak-\ning them effective for conveying scientific evidence to\nnon-expert audiences [4]. Moreover, narratives are pro-\ncessed more rapidly by individuals without prior knowl-\nedge and are more engaging and persuasive, thereby en-\nhancing trust and understanding of AI models [4,20,25].\nFurther, as narrative communication relies on con-\ntextual cause-and-effect relationships, it is considerably\nmore challenging to fragment a narrative into smaller,\nmeaningful segments without either significantly alter-\ning the interpretation of these segments or disrupt-\ning the coherence of the original narrative [4]. Conse-\nquently, narratives are often perceived as storytelling,\ncharacterised by a coherent structure comprising an in-\ntroduction, main body, and conclusion. In contrast,\ndescriptive, context-free communication can be read-\nily fragmented into smaller units while still effectively\nconveying the necessary information [4]. The XAIsto-\nries [20] framework addresses aforementioned explana-\ntion limitations by enhancing the narrative communi-\ncation of SHAP and CF explanations, aligning with the\nresearch on human-AI interactions [4, 25].\nHaving identified the need to distinguish between\nnarrative and descriptive explanations in the context of\nXAI, we propose definitions for both terms, drawing on\nresearch from the social sciences [4,25].\nDefinition 1 (\u03a7\u0391\u0399 Narrative) A \u03a7\u0391\u0399 Narrative\nprovides a structured, story-like representation of a\nmodel's prediction. Narrative explanations illustrate\nthe relationships between key features in a context-\ndependent manner, providing a coherent and compre-\nhensive understanding of how the model arrives at spe-\ncific outcomes."}, {"title": "Definition 2 (\u03a7\u0391\u0399 Description)", "content": "A \u03a7\u0391\u0399 Descrip-\ntion provides a static presentation of key features or\nattributes relevant to a model's prediction, delivered in\na context-free and fact-based manner.\nAn example description would list the most impor-\ntant features and neighbouring nodes. This relates\nclosely to the data-to-text or graph-to-text [5,28].\nFigure 5 provides an XAI Description (XAID) example for\nthe XAI solution provided by GNNExplainer shown in\nFigure 1. Clearly, an XAID is less valuable than an\nXAIN, as descriptions are less accessible and memo-\nrable than narrative communication methods, making\nthem less effective for conveying scientific evidence to a\nbroader audience [4, 20]."}, {"title": "3.2 From GNN models to Natural Lan-\nguage Narratives", "content": "To overcome the issue of purely technical explana-\ntions of GNN models' predictions, we propose GraphX-\nAINs-natural language narratives for graphs. We pro-\npose the following definition of GraphXAIN, a \u03a7\u0391\u0399 Nar-\nratives for graphs:\nDefinition 3 (GraphXAIN) A GraphXAIN is a\nXAI Narrative tailored for graph-structured data.\nOur solution involves converting subgraph struc-\ntures and feature importance metrics derived from\ngraph explainers into GraphXAINS - coherent natural\nlanguage narratives. The workflow, presented in Fig-\nure 2, begins with the training of a GNN classification\nmodel on a relational graph dataset. Our approach is\nnot limited to pure graphs; data can include associ-\nated node and edge features. Further, using the graph\nexplainer method, we extract relevant subgraphs and\nfeature importances. These technical outputs are then\ntransformed and prompted into an LLM for further in-\nference. The LLM generates a natural language nar-\nrative that explains the prediction of the target node's\nlabel, highlighting the contributions of specific neigh-\nbouring nodes, corresponding edges, and node features\nin a context-dependent manner, resulting in a coherent\ncause-and-effect narrative presented to the end user.\nImportantly, our framework is agnostic to the graph\ndata type, graph model, and graph explainer, allow-\ning its application across various graph scenarios. By\ntranslating technical explanations into coherent narra-\ntives, we aim to make graph model explanations more\nintuitive and accessible for practitioners."}, {"title": "4 Experiments", "content": "We conducted our experiments on a real-world graph\nNBA dataset\u00b9, which includes players' performance\nstatistics from the NAB 2016-2017 season, alongside\nvarious personal attributes such as height, weight, age,"}, {"title": "4.1 Dataset", "content": "and nationality. The graph was constructed by link-\ning NBA players based on their relationships on the\nTwitter platform. After preprocessing, filtering out dis-\nconnected nodes, and ensuring the graph is undirected,\nwhere any connection on the Twitter platform results\nin a mutual edge, the final graph comprised of the total\nnumber of 400 nodes (players), 42 node features, and\n21,242 edges (Twitter connections).\nThe node classification task involves predicting\nwhether a player's salary exceeds the median salary.\nPrior to training, the data were randomly divided into\ntraining, validation, and test sets in a 60/20/20 split."}, {"title": "4.2 Graph Model", "content": "We trained a two-layer Graph Convolutional Network\n(GCN) [15] model with 16 hidden channels. For train-\ning we used the Binary Cross-Entropy loss function and\nthe AdamW [17] optimizer with a learning rate of 0.001\nand a weight decay of 5\u00d710-4. The training process\ncontinued through 1400 epochs, resulting in the GCN\nmodel achieving a 0.80 test AUC."}, {"title": "4.3 Graph Explainer", "content": "To obtain subgraph and feature importance explana-\ntions, we used GNNExplainer [36], the current state-\nof-the-art method for explaining GNN models. The\nGNNExplainer formulates the explanation task as an\noptimisation problem, aiming to maximise the mutual\ninformation between the explanatory subgraph and a\nsubset of node features, relative to the input graph un-\nder consideration. The training process for GNNEX-\nplainer was conducted for 200 epochs, adhering to the\ndefault settings recommended for this explainer by the\nauthors. Nevertheless, in our framework, any graph ex-\nplainer may be used as long as it provides an explana-\ntory subgraph."}, {"title": "4.4 Large Language\nPrompts", "content": "\u03a7\u0391\u0399 Narratives are derived from the GNNExplainer\noutput, with the use of LLM (GPT-40). Using a specif-\nically developed prompt, we guide the LLM to produce\ncoherent and relevant narratives that accurately reflect\nthe graph model's prediction. The language model is\nprovided with a prompt containing the target node's\nfeature values, along with feature importance and the\nsubgraph derived from GNNExplainer. Additionally,\nfeature values of the nodes within the subgraph and\nthe final GNN model prediction for the target node\nare included. In our visualisations, we use the seven\nmost important features and restrict the subgraph to\nseven nodes, aligning with Miller's (1956) [24] theory\non cognitive limits, which suggests that seven pieces of\ninformation represent an optimal amount for receiving,\nprocessing and recalling. The full prompt schema is\npresented in Appendix A."}, {"title": "5 Results", "content": "In the following, we provide examples demonstrating\nvarious automatically generated GraphXAINs to ex-\nplain the GNN model's final prediction of the target\nnode. It is important to emphasize that the results\npresented are not selectively chosen. From the graph\ndataset, five node indices were randomly selected. The\nGraphXAINs are subsequently generated for each of the\nselected nodes and are presented in Figure 3 and Fig-\nure 4. For the remaining GraphXAIN examples, see\nAppendix B.1."}, {"title": "6 Conclusion and Future Work", "content": "In this work, we have introduced GraphXAIN, a novel\nsolution that transforms explanatory subgraphs and\nfeature importance metrics into coherent \u03a7\u0391\u0399 Nar-\nrative explanations for GNN models. The proposed"}, {"title": "B.2 Natural Language Counterfactual Explanations for Graphs Using Large Lan-\nguage Models (Giorgi et al., 2024)", "content": "This section presents three examples of textual explanations proposed by Giorgi et al. (2024) [6] for counterfac-\ntual explanations of node classification using a GNN model trained on the Cora dataset. The Cora dataset is a\nstandard citation network dataset in which nodes represent research papers, and edges denote citation relation-\nships, with the task being to classify each paper into one of seven categories. Since the authors do not provide\nany output examples in their article, the examples presented below are taken from the article's official GitHub\nrepository. These examples were not selectively chosen but represent the first three explanations generated by\nthe best-performing model, as reported in the paper and available in the article's official GitHub repository2:\n\u2022 \"The target node (node 15) was originally classified as \u2018Probabilistic_Methods' in the factual graph. However,\nin the counterfactual graph, the classification changes to 'Reinforcement_Learning'. Despite the features of\nnode 15 remaining identical, the classification shifts due to changes in the classification of its neighboring\nnodes. Specifically, nodes 4 and 12, which were previously classified as\u2018Probabilistic_Methods', are now\nreclassified as 'Reinforcement_Learning'. This change in the neighborhood context strongly influences the\nclassification of node 15, leading to its reclassification as 'Reinforcement_Learning'. The only neighbor that\nremains consistent is node 19, but its influence is outweighed by the changes in nodes 4 and 12. Thus, the\nprimary factor driving the change in classification is the altered classification of neighboring nodes, rather\nthan any intrinsic change in the features of node 15 itself.\" (Line 4)\n\u2022 \"In the factual scenario, Node 3 is classified as 'Theory' and is connected to Nodes 0, 9, 10, and 13. In\nthe counterfactual scenario, Node 3 is reclassified as 'Case_Based' and is only connected to Node 10. The\nchange in classification is influenced by the loss of connections to Nodes 0, 9, and 13, and the retention of\na connection to Node 10, which is classified as \u2018Case_Based.\" (Line 8)\n\u2022 \"In the factual scenario, Node 0 is classified as 'Theory' and is connected to both Nodes 1 and 4. Node 1\nshares similar features with Node 0 and is also classified as 'Theory'. However, Node 4, which is classi-\nfied as 'Case-Based', introduces a conflicting influence. In the counterfactual scenario, the connection to\nNode 4 is removed, isolating Node 0 from the 'Case-Based' influence. As a result, Node 0 is reclassified as\n'Case_Based', reflecting the reduced influence of 'Theory' nodes and the absence of conflicting 'Case_Based'\nconnections. This counterfactual explanation highlights how the removal of a single connection can signifi-\ncantly alter the classification of a node, providing insight into the decision-making process of the graph-based\nmodel.\" (Line 12)\nAccording to Dahlstrom (2014) [4] and our proposed definitions, these examples constitute an XAI Description\nrather than an XAI Narrative."}, {"title": "B.3 Explaining Graph Neural Networks with Large Language Models: A Coun-\nterfactual Perspective for Molecular Property Prediction (He et al., 2024)", "content": "Presented below are three examples of textual explanations generated by the method proposed by He et al.\n(2024) [10] for counterfactual explanations in the context of molecular property prediction using a Graph Neural\nNetwork (GNN) model trained on a chemical molecule dataset. This dataset comprises molecular structures in\nwhich nodes represent atoms and edges signify chemical bonds, with the primary objective being the prediction\nof specific molecular properties. These three examples are the only instances provided by the authors in their\narticle [10]:\n\u2022 \"This molecule contains a cyclohexane ring, a dithiane ring, a ketone group, and a thiocarbonyl group, in\nwhich the ketone group may be the most influential for AIDS treatment.\" (Page 3, Figure 2)\n\u2022 \u201cThe molecule contains hydroxylamine, cyclohexane, sulfone, and thioether functional groups, in which\nhydroxylamine may be the most influential for AIDS treatment.\" (Page 4, Figure 4)\n\u2022 \"This molecule contains a cyclohexane ring, a dithiane ring, a ketone group, and a hydrazine group, in\nwhich the hydrazine group may be the most influential for AIDS treatment.\" (Page 6, Figure 5)\nAgain, the explanations presented by He et al. (2024) [10] are delivered in a context-free and fact-based\nmanner, without illustrating underlying relationships or cause-and-effect relationships, thereby categorising them\nas XAI Descriptions rather than XAI Narratives."}, {"title": "B.4 TAGExplainer: Narrating Graph Explanations for Text-Attributed Graph\nLearning Models (Pan et al., 2024)", "content": "Presented below are three instances of textual explanations proposed by Pan et al. (2024) [27] for Text-Attributed\nGraph (TAG) learning models using their TAGExplainer method. The authors applied TAGExplainer to the\nCora, DBLP, and Book-History datasets to generate natural language explanations that incorporate elements of\nnarrative communication. These three examples are the only instances provided by the authors in their article,\neach corresponding to the aforementioned datasets, respectively:\n\u2022 \"The classification of the ROOT node into the 'Reinforcement Learning' category can be explained as\nfollows:\n\u2022 ROOT mentions reinforcement learning, learning algorithms, and hierarchical control structures in-\ndicating that it focuses on reinforcement learning methodologies and their applications. Node-1\ndiscusses transfer learning in reinforcement learning, highlighting how learning agents can adapt to\nmultiple tasks.\n\u2022 Node-1.1 - Node-1.8 provide more context about various reinforcement learning architectures and ap-\nproaches, including modular Q-learning and hierarchical reinforcement learning.\n\u2022 Node-2 focuses on learning in continuous domains with delayed rewards, a pertinent aspect of rein-\nforcement learning, particularly in real-world applications.\n\u2022 Node-2.1 discusses generalization in reinforcement learning using neural networks, which is relevant\nbut less directly tied to the prediction.\n\u2022 Node-3: This node discusses the need for hierarchies within control systems and the improvements in\nnested Q-learning, which are highly relevant to the classification label.\nIn summary, the classification of the ROOT node into\u2018Reinforcement Learning' is well-supported by the\nstrong presence of key reinforcement learning terms within the node itself and its direct connections to Nodes\n1, 2 and 3, which also focus on reinforcement learning methodologies.\" (Page 10, Figure 4b)\n\"The classification of the ROOT node into the 'Cloud computing' category can be explained as follows:\nROOT: The presence of keywords such as 'user', 'interfaces', and 'service-oriented approach' indicates\nthat the paper is focused on topics central to cloud computing, particularly how users interact with cloud\nservices and the modular nature of these services.\nNode-1: While Node 1 discusses \u2018graphical user interfaces' and 'domain-specific languages', which are\nrelevant to cloud computing, it is slightly less directly related to the core concepts of cloud computing\ncompared to the ROOT node.\nNode-1.1: This node emphasizes 'user interface requirements' and 'requirements analysis', which are\ncrucial for developing effective cloud applications that meet user needs.\nNode-1.2: This node focuses on 'stereotypes' and 'domain-specific terms', highlighting the importance\nof understanding user interactions and the context in which cloud services are utilized.\nIn summary, the classification of the ROOT node into 'Cloud computing' is well-supported by the presence\nof key terms related to user interaction and service-oriented architectures. The direct connection to Node\n1, which discusses user interfaces, further reinforces this classification, while the additional insights from\nNodes 1.1 and 1.2 emphasize the importance of user-centric design in cloud computing applications.\" (Page\n16, Figure 6b)\n\"The classification of the ROOT node into the 'Europe' category can be explained as follows:\nROOT: The presence of keywords such as\u2018cambodia\u2019, \u2018year', and \u2018translation' indicates a context that\nmay involve historical or cultural discussions relevant to Europe, particularly in terms of colonial his-\ntory and cultural exchanges. The mention of 'english' and 'french' highlights the linguistic dimensions\nthat are significant in European contexts.\nNode-1: This node discusses Michael D. Coe, an anthropologist specializing in Southeast Asia and the\nKhmer civilization. While it provides historical context, the focus on Southeast Asia may dilute its\ndirect relevance to Europe. However, the terms \u2018civilizations' and 'ancient' could connect to European\nhistorical interests.\nNode-2: This node is more directly relevant as it discusses the destruction of Cambodia during the\nNixon-Kissinger era, a significant historical event that involved European powers' interests in Southeast\nAsia. The emphasis on \u2018destruction' and 'cambodia' alongside key historical figures suggests a critical\nperspective on the geopolitical dynamics involving European countries."}, {"title": "A Prompt Schema", "content": "Figure 6 presents the schema of the prompt used within the proposed framework.\n\"Your goal is to generate a textual explanation or narrative explaining why a graph explainer produced a certain target node's explanation subgraph and feature importance for a Graph\nNeural Network (GNN) model's prediction of a target node instance.\nTo achieve this, you will be provided with the following information:\n**Machine Learning Task Information**: Details about the machine learning task.\n**Dataset Information**: Information about the dataset used.\n**Target Node's Data**: Information about target node's features.\n**Target Node's Categorical Data**: Information about target node's categorical/binary features.\n**Target Node's Number of Edges and Labels Ratio**: Information on target's node edges.\n**Target Node's Feature Importance**: Node feature importance information.\n**Target's Subgraph Nodes' Feature Values and Percentiles**: Target'subgraph's node data.\n**Target's Subgraph Edge's Influence Importance**: Target's subgraphs node data on edge importance.\n**Target's Subgraph Nodes' Labels**: Target's subgraphs node data on labels.\n**Model's Prediction**: Target node index and predicted class.\n### Machine Learning Task Information:\n{placeholder_1}\n### Dataset Information:\n{placeholder_2}\n### Target Node's Data:\n{placeholder_3}\n### Target Node's Categorical Data:\n{placeholder_4}\n### Target Node's Number of Edges and Labels Ratio:\n{placeholder_5}\n### Target's Subgraph Nodes' Feature Values and Percentiles:\n{{placeholder_6}\n### Target's Subgraph Edge's Importance Weights:\n{placeholder_7}\n### Target's Subgraph Nodes' Labels:\n{placeholder_8}\n### Model's Prediction:\n{placeholder_9}\nGenerate a fluent and cohesive narrative that explains the prediction made by the model. In your answer, please follow these rules:\n**Format-related rules**:\n1) Start the explanation immediately.\n2) Limit the entire answer to {placeholder_10} sentences or fewer.\n3) Only mention the top {placeholder_11} most important features in the narrative.\n4) Do not use tables or lists, or simply rattle through the features and/or nodes one by one. The goal is to have a narrative/story.\n**Content related rules**.\n1) Be clear about what the model actually predicted for the target node with index {placeholder_12}.\n2) Discuss how the features and/or nodes contributed to final prediction. Make sure to clearly establish this the first time you refer to a feature or node.\n3) Discuss how the subgraph's edge importance contribute to final prediction. Make sure to clearly establish this the first time you refer to an edge connection.\n4) Consider the feature importance, feature values, averages, and percentiles when referencing their relative importance.\n5) Begin the discussion of features by presenting those with the highest feature importance values first. The reader should be able to tell what the order of importance of the features is\nbased on their feature importance value.\n6) Provide a suggestion or interpretation as to why a feature contributed in a certain direction. Try to introduce external knowledge that you might have.\n7) If there is no simple explanation for the effect of a feature, consider the context of other features and/or nodes in the interpretation.\n8) Do not use the feature importance numeric values in your answer.\n9) You can use the feature values themselves in the explanation, as long as they are not categorical variables.\n10) Do not refer to the average and/or percentile for every single feature; reserve it for features where it truly clarifies the explanation.\n11) When discussing a node's categorical data, make sure to indicate whether the presence (1) or absence (0) of a feature is contextually informative and/or significantly contributes to\nthe explanation. State that it is one of the posibble values among that category.\n12) When discussing the connections between the nodes, relate how the influence of a node's relationship might impact final prediction.\n13) When you refer to node and edges, keep in mind that the target node is a {placeholder_13} and edges are {placeholder_14} in this dataset.\n14) Tell a clear and engaging story, including details from both feature values and node connections, to make the explanation more relatable and interesting.\n15) Use clear and simple language that a general audience can understand, avoiding overly technical jargon or explaining any necessary technical terms in plain language.\""}, {"title": "B Examples of XAI Explanations in Natural Language for GNN\nModels", "content": "In this section, we present the remaining examples generated by our framework. Figure 7 and Figure 8 provide\nfurther insight into the interpretability of the GNN's model prediction by presenting coherence narratives, which\ndemonstrates the GraphXAIN's consistency across a range of randomly sampled nodes. Each example was\ngenerated following the same methodology, with nodes randomly sampled using the random.sample(range(0,\nlen(data.x)), 5) function and set_seed(42) to ensure reproducibility. The examples showcase the explanatory\ndepth of GraphXAINs in capturing the reasoning behind the GNN model's predictions.\nIn the truncated subgraph visualisations presented in both Figure 7 and Figure 8, GNNExplainer is instructed\nto reduce the subgraph to the seven most influential nodes according to edges' weight importance. As a result, this\nprocess occasionally results in disconnected nodes within the truncated subgraph, as GNNExplainer prioritises\nnode importance over connectivity in the simplified view. Consequently, these subgraph representations can\nappear more unintuitive or fragmented. A complementary natural language narrative is therefore argued to be\nessential to bridge these interpretative gaps, providing end users with a coherent understanding of the GNN\nmodel's final prediction, regardless of the connectivity of the subgraph.\nFor example, to obtain a connected subgraph for node 379, GNNExplainer requires at least 14 nodes in this\nparticular example. In this larger subgraph, previously disconnected nodes 7 and 263, become connected to the\ntarget node 379, revealing their substantial influence on the final prediction. However, as the subgraph expands, it\nbecomes increasingly complex and challenging to interpret intuitively. Thus, a complementary natural language\nnarrative is needed to provide a coherent understanding of the GNN model's prediction. Figure 9 presents\nGraphXAIN's output for a connected subgraph containing the 14 most influential nodes for the target node's\n379 prediction."}]}