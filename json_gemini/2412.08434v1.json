{"title": "Mitigating Out-of-Entity Errors in Named Entity Recognition: A Sentence-Level Strategy", "authors": ["Guochao Jiang", "Ziqin Luo", "Chengwei Hu", "Zepeng Ding", "Deqing Yang"], "abstract": "Many previous models of named entity recognition (NER) suffer from the problem of Out-of-Entity (OOE), i.e., the tokens in the entity mentions of the test samples have not appeared in the training samples, which hinders the achievement of satisfactory performance. To improve OOE-NER performance, in this paper, we propose a new framework, namely S+NER, which fully leverages sentence-level information. Our S+NER achieves better OOE-NER performance mainly due to the following two particular designs. 1) It first exploits the pre-trained language model's capability of understanding the target entity's sentence-level context with a template set. 2) Then, it refines the sentence-level representation based on the positive and negative templates, through a contrastive learning strategy and template pooling method, to obtain better NER results. Our extensive experiments on five benchmark datasets have demonstrated that, our S+NER outperforms some state-of-the-art OOE-NER models.", "sections": [{"title": "1 Introduction", "content": "Named entity recognition (NER) (Li et al., 2022) plays a vital role in many downstream tasks including knowledge graph construction (Xu et al., 2017), information retrieval (Banerjee et al., 2019), question answering (Aliod et al., 2006), etc. NER task aims to recognize the span(s) of entity mention(s) in one input sentence, and further type the entity(s) mentioned by the span(s). For example, given a sentence \"Aurora Couture aims to illuminate the fashion industry with its unique designs and commitment to ethical practices\", the model should output 'Aurora Couture' as the entity mentioned and type the entity as 'Brand'.\nIn recent years, many deep NER models (Ma and Hovy, 2016; Zhu et al., 2018; Wang et al., 2020, 2022; Zhu et al., 2023; Jiang et al., 2024b) have exhibited state-of-the-art (SOTA) performance. Nevertheless, these models' performance declines when many words (tokens) in the entity mentions have not appeared in the training set before, which is referred to as Out-of-Entity (OOE) problem (Mikheev et al., 1999). As shown in Figure 1, the F1 score of the representative models (SpanNER (Fu et al., 2021), MIENR (Wang et al., 2022) and DSPERT (Zhu et al., 2023)) drops apparently as the OOE rate increases. It is probably because the span representations of OOE words have not been fine-tuned during model training, while most of the previous models achieve NER mainly based on the span representations.\nThe previous NER models achieve the NER of OOE (OOE-NER) mainly through leveraging external knowledge, OOE word embedding, or contextualized embedding. The methods of external knowledge (Zhang and Yang, 2018; Li et al., 2018; Jiang et al., 2024a; Shi et al., 2024) enhance their recognition capabilities by incorporating external information. However, external knowledge especially domain-specific knowledge, may not be obtained conveniently and cheaply. The methods of OOE word embedding (Fukuda et al., 2020; Peng et al., 2019) strive to construct more robust embeddings for OOE words, but they only leverage the semantics of the OOE words themselves without considering other useful information such as sentence-level context. Comparatively, the methods of contextualized embedding (Hu et al., 2019) incorporate the contextual information of the mentions besides the OOE word embeddings to achieve NER, through learning the appropriate embeddings of the tokens near the OOE words. Nonetheless, these models have not fully exploited the sentence-level contextual information of OOE words which is indeed helpful to improve OOE-NER performance further.\nIntuitively, the semantic and syntactic information in the context of a new entity's mention can prompt us to recognize the entity's name and identify its type correctly. Recall the sentence illustrated in the first paragraph, when 'Aurora Couture' is replaced with another span, we still believe the new span is also the name of a brand or company. Another example is \u201cXXX is a wonderful city.\u201d, where the type of 'XXX' can be recognized as location easily through understanding the whole sentence. Inspired by it, we propose a new NER framework, namely S+NER, which fully leverages the mention's contextual information to enhance OOE-NER performance. Since the OOE words in the target mention have not appeared in the training samples resulting in unsatisfactory OOE embeddings, our S+NER additionally incorporates the representation of the whole sentence, which is generated by a BERT-based encoder (Devlin et al., 2019). Furthermore, S+NER refines the sentence representation by adopting a contrastive learning strategy. As we mentioned before, contextual information is highly correlated with the correct understanding of the entity and its type. As a result, an optimal context representation in terms of accurate NER should be close to the representation of the positive template involving the entity span and its correct type, and simultaneously far away from the negative templates involving the entity span and its incorrect types. That is the basic principle of our proposed contrastive learning strategy.\nOur main contributions in this paper are summarized as follows."}, {"title": "1. We propose a novel framework S+NER based an effective contrastive learning method that fully leverages the sentence-level information of the mention(s) to achieve improved OOE-NER performance.", "content": null}, {"title": "2. We utilize the large language model (LLM) to generate a template set for OOE-NER that is both high-quality and semantically rich. To enhance performance, we incorporate sentence-level information from various templates using a pooling method designed for multiple templates.", "content": null}, {"title": "3. We conduct extensive experiments on some benchmark datasets to justify S+NER's advantage on OOE-NER over some SOTA OOE-NER models.", "content": null}, {"title": "2 Related Work", "content": "The rest of our paper is organized as follows. We briefly introduce the research works related to our work in Section 2 and detail our proposed framework including its special designed components in Section 3. Then, we display our extensive experiment results and provide in-depth analysis in Section 4. At last, we conclude our work in Section 5."}, {"title": "2.1 NER Solutions for OOE Problem", "content": "By now, in the research field of NER, the OOE problem has been addressed through three primary categories of approaches as follows.\nThe first category incorporates external knowledge to tackle the OOE issue. Zhang and Yang (2018) exemplify this by constructing extensive entity dictionaries, thereby enhancing the model's look-up capability. However, this approach often compromises the model's generalization ability. Li et al. (2018) attempted to mitigate this by introducing part-of-speech tags as external knowledge. However, this method is contingent on the availability of such tags and a high-quality external knowledge base, which is often challenging to procure.\nThe second category focuses on enhancing OOE word embeddings. For instance, Bojanowski et al. (2017) utilized each word's character-level n-gram to represent the OOE word embedding, given the absence of OOE words at the character level. Pinter et al. (2017) employed character-level Recurrent Neural Networks (RNNs) to capture morphological features. Other methods involve using known words during the training process to match OOE words, subsequently replacing the OOE word embeddings with the embeddings of the matched known words. Peng et al. (2019) trained a student network to predict the closest word embedding for OOE words in the representation space. Fukuda et al. (2020) leveraged the known words similar in character surface form to the OOE words. However, these methods typically yield a static OOE embedding, neglecting the exploitation of contextual information.\nThe third category leverages contextual information to enhance the representation of OOE words. Hu et al. (2019) reformulated the OOE problem as a K-shot regression problem, predicting OOE embedding by aggregating K context features. Recently, researchers have utilized pre-trained language models (PLMs) (Peters et al., 2018; Devlin et al., 2019; Liu, 2019; He et al., 2021), given their demonstrated proficiency in embedding contextual words with contextual information. However, Yan et al. (2021) found that BERT does not always outperform BiLSTM-CRF in capturing contextual information. Jiang et al. (2024b) introduced ToNER to solve NER by using an entity type matching model to identify potential entity types in the sentence and leverage contextual information. In contrast to these methods, our proposed S+NER model leverages sentence-level contextual information, as opposed to word-level and character-level embeddings."}, {"title": "2.2 NER Solutions with Contrastive Learning", "content": "Contrastive Learning is effective in representation learning practice, which is commonly aimed at improving the alignment and uniformity of the representation space to optimize the learned representations (Wang and Isola, 2020). Therefore, contrastive learning is widely applied in NER to improve entity representations thus further improving model performance. ERICA (Qin et al., 2021) introduces an entity-level pre-training task called entity discrimination, which optimizes entity representation by contrasting the semantics of different types of entities. CONTaiNER (Das et al., 2022) introduces contrastive learning at the token-level to reduce the distance between token embeddings of the same entity type and increase that between different types, alleviating the overfitting problem during downstream task transfer. BINDER (Zhang et al., 2023) contrasts the entity type description with all the tokens in the input text from both the entity and token perspectives, to enhance nested and discontinuous entity extraction. Similar ideas are also found in TONER (Jiang et al., 2024b). CLLMFS (Zhang et al., 2024) incorporates contrastive loss into the instruction fine-tuning phase to enhance the LLM's understanding of entity mentions and entity types. In this work, we utilize contrastive learning to optimize the model's comprehension of sentence-level contextual information, thereby improving OOE-NER performance."}, {"title": "3 Methodology", "content": "In this section, we first introduce the span classification task for NER (in Section 3.1). Then, we introduce the model design in S+NER (in Section 3.2), which are used to obtain optimal context representation for enhanced OOE-NER."}, {"title": "3.1 Backbone Model", "content": "In recent years, the model architecture for NER with pre-trained language models (PLMs) has changed from the initial sequence labeling task (Chiu and Nichols, 2016; Akbik et al., 2018; Yan et al., 2019) to the span classification task (Xue et al., 2020; Yamada et al., 2020; Fu et al., 2021; Zhu et al., 2023). We choose span-based tasks for NER as the backbone of our S+NER for the following two reasons.\n1. Span-based model can extract the explicit information of entity mention spans, which can be easily leveraged by an advanced NER model (Zhu et al., 2023).\n2. Compared with other sequence labeling models of NER, Span-based model has demonstrated better generalization capability on OOE scenarios (Fu et al., 2021).\nThe classical span-based model for NER mainly consists of three parts: token representation layer, span representation layer, and span classification layer, which are introduced in turn as follows."}, {"title": "3.1.1 Token Representation Layer", "content": "Formally, let $X = \\{x_1,x_2,\\cdots, x_n\\}$ represent the input sentence sequence, $h_i$ denote the representation or hidden state of the i-th token $x_i$ respectively. In most cases, they are as follows:\n$h_1,h_2,\\ldots, h_n = Encoder(x_1,x_2,...,x_n),$(1)\nwhere Encoder(\u00b7) can be implemented with any network structure with context encoding function, e.g., LSTM (Hochreiter and Schmidhuber, 1997), Transformer (Vaswani et al., 2017) and so on. In most cases, $h_i \\in R^d, 1 \\leq i \\leq n$, which means that the dimension of token embedding vector is equal to the dimension of token representation vector for PLMs. d is the dimension of the PLM, e.g. d = 768 for BERT-base and d = 1024 for BERT-large."}, {"title": "3.1.2 Span Representation Layer", "content": "Denote $S = \\{s_1, s_2, ..., s_m\\}$ is the set of all potential spans in sequence X where $b_i$ and $e_i$ are the start position index and the end position index of span $s_i = (b_i, e_i), 1 \\leq i \\leq m$, respectively. For each span $s_i$, a label $y_i$ is assigned to indicate whether $s_i$ is the entity span of a certain type or non-entity span (denoted by O in most NER tasks). For example, given a sentence X = \u201cMilan is wonderful.\u201d, its potential span set is S = $\\{(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)\\}$, and the corresponding label set is {LOC, O, O, O, O, O} where LOC indicates the span \u2018Milan' is a location entity.\nFor the span $s_i = (b_i, e_i)$, its representation consists of two parts in the classical span-based NER model: boundary embedding and span length embedding.\n\u2022 Boundary Embedding: For the NER model based on span classification, the text dimension information of the span itself must be the key to downstream tasks. One method is to use the representation of each token in a span and concatenate them together as the information representation of this part. However, this method will produce representations of different dimensions for spans of different lengths, and for spans that are too long, the representation dimension will be too large. The classic method is to use the representation of two boundary tokens of span. Formally, denote $z^b_i$ as the boundary embedding. It is the concatenation of the start token's representation $h_{b_i}$ and the end token's representation $h_{e_i}$.\n$z^b_i = [h_{b_i}; h_{e_i}] \\in R^{2d}.$(2)\n\u2022 Span Length Embedding: If only the boundary embedding discussed above is used as a span representation for subsequent entity classification, this method will lose the explicit positional information, even if the Transformer architecture has its own positional encoding in most cases (Gehring et al., 2017; Su et al., 2024). At the same time, due to the use of boundary embedding, the length of the span itself cannot be used. Therefore, in general methods, an additional learnable span length embedding is added as part of the span representation. Formally, denote $z_i^{len} \\in R^{d'}$ as the span length embedding, which is initialized randomly and learned through model training.\nThen, denote $z_i$ as the span $s_i$'s representation, which is the concatenation of the boundary embedding and span length embedding:\n$z_i = [z^b_i; z_i^{len}] \\in R^{2d+d'}.$(3)"}, {"title": "3.1.3 Span Classification Layer", "content": "Upon acquiring the span representation $z_i$, it is fed into a fully connected neural network. The purpose of this operation is to classify the span in accordance with the entity type label that the model suggests for the span $s_i$. Formally, we denote the span classifier as a function F. This function F maps a span representation of dimension 2d + d' to a vector of dimension |Y|, where Y represents the entity type set. The resultant vector F(zi) embodies scores that correspond to each unique label for the span si. The model's predicted label $\\hat{y}$ for the span si is determined by identifying the label that corresponds to the highest score in the vector. For the overlapped spans, only the span with the largest score is reserved at last. Using this heuristic decoding method to achieve flat NER (Li et al., 2022) can avoid predicting overlapped spans."}, {"title": "3.1.4 Learning Objective", "content": "Suppose $y_i$ is the true label of $s_i$, the loss function with respect to a training sample (sentence) in span-based NER model is\n$L_1 = - \\sum_{s_i \\in S} CE(F(Z_i), y_i),$(4)\nwhere S is the sentence's span set from which the overlapped spans have been removed, and CE(\u00b7, \u00b7) is the cross entropy function for score vector F(zi) and corresponding true label $y_i$."}, {"title": "3.2 S+NER", "content": "Besides the span's token-level features, we further incorporate the sentence-level information of the span, i.e., the sentence representation, into our framework to achieve NER. Specifically, the sentence representation is further refined by our proposed contrastive learning strategy. In this subsection, we introduce the procedure of generating and refining the sentence representation in our S+NER, of which the overall architecture is shown in Figure 2."}, {"title": "3.2.1 Generating Original Context Representation", "content": "As we emphasized in Section 1, the contextual information of the mention (span) is significant for a NER model to recognize the entity. An intuitive way to incorporate contextual information is to directly leverage the sentence's representation into which the semantic and syntactic information of the sentence are encoded. Given the power of pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) on understanding sentences, we employ the BERT-based encoder to encode the input sentence into an embedding at first.\nFormally, given an input sentence sequence $X = \\{x_1,x_2,..., x_n\\}$, its original sentence representation is obtained by\nc = SentenceEmbedding(X) = f(X) \\in R^d,$(5)\nwhere SentenceEmbedding(\u00b7) and f(\u00b7) represents the sentence encoder's operations. In this paper, we use the average pooling of all token representations in a sentence to obtain c.\nThen, c can be used as an auxiliary feature to help the model recognize the target span si more accurately. Thus si's representation zi is expanded with three parts: boundary embedding $z^b_i$, span length embedding $z^{len}_i$ and sentence representation c, that is\n$z_i = [z^b_i; z^{len}_i; c] \\in R^{3d+d'} .$(6)\nAt last, we feed expanded zi into the label classifier which is the same as the span-based NER model's classification layer, to predict the label of Si."}, {"title": "3.2.2 Refining Context Representation with Templates", "content": "Specifically, we formally define the positive/negative template as follows, which is used in our contrastive learning as the positive/negative golden standard of the context representation. Take an empty template T = \u201c[SPAN] is a [TYPE] entity.\" as an example, where the position of [SPAN] will be filled with the span si, and the position of [TYPE] will be filled with si's correct or incorrect type (label). Thus, we get the si's positive template and negative template by filling T respectively with its positive and negative labels. To make the encoder understand the template more effectively, we further translate the label into a term of natural language before filling T, e.g., 'LOC' is translated into 'location'."}, {"title": "4 Experiment", "content": "In this section, we display and analyze the results of evaluating our S+NER's OOE-NER performance upon five benchmark OOE datasets, compared with the S+NER NER models. At the same time, we have also conducted ablation experiments on S+NER to verify that each component in the S+NER is reasonable and effective."}, {"title": "4.1 Datasets and Evaluation Metric", "content": "We conducted our experiments on five datasets including WNUT2017 (Derczynski et al., 2017), JNLPBA (Collier and Kim, 2004), TwitterNER (Zhang et al., 2018), CoNLL2003-Typos (Wang et al., 2021) and CoNLL2003-OOE (Wang et al., 2021).\nAs evaluating previous NER models (Fu et al., 2021; Wang et al., 2022), we report the entity-level micro F1 scores of all compared models in our experiments. Table 1 gives some basic information on the training set and the test set of these datasets, including the number of sentences, the number of entities, and the proportion of OOE entities that do not appear in the training set but appear in the test set."}, {"title": "4.2 Baselines and Implementation Details", "content": "We compared our framework with the baselines including BERT-Tagger (Devlin et al., 2019), BERT-CRF, SpanNER (Fu et al., 2021), DataAug (Dai and Adel, 2020), InferNER (Shahzad et al., 2021), COFEE (Fukuda et al., 2020), MINER (Wang et al., 2022) in the experiments.\nThe implementation details of our experiments are as follows. We choose the BERT-large (Devlin et al., 2019) as S+NER's span and sentence encoder in the following comparison experiments. The learning rate for the span classification layer is set to 5e-5, the learning rate for BERT is set to le-5, and the dropout rate for the span classification layer is set to 0.2. In order to make a trade-off between the effectiveness and performance of the model, for each input sentence exceeding 128 tokens, we only reserved its first 128 tokens. To limit the number of all extracted spans on the affordable level, we set the maximum length of a span as 4. We conducted our experiments on 1 NVIDIA GeForce RTX 4090 GPU. The checkpoint with the best performance on the validation set will be evaluated on the test set to report the final result. The F1 scores of all experimental results are obtained by averaging the results of five random experiments. Some important hyperparameter settings are listed in Table 3, most of which were decided based on our tuning studies."}, {"title": "4.3 Overall Performance Comparisons", "content": "Table 2 shows the NER performance of our S+NER and all baselines on the five datasets, where the best and second best score in each dataset are bolded and underlined, respectively. In addition, we directly report the scores of InferNER, CoFEE, and MAML provided in the paper of MINER since their source codes have not been published. Except for the JNLPBA dataset, our proposed S+NER outperforms all other baselines on the WNUT2017, TwitterNER, CONLL2003-Typos, and CoNLL2003-OOE datasets. Compared to the sub-optimal model, S+NER shows a noticeable performance improvement, especially on the WNUT2017 dataset with a 100% OOE ratio. This demonstrates that S+NER has better robustness in more severe OOE scenarios. Based on the average performance scores across the five datasets, our proposed S+NER achieves the best overall results in OOE-NER. SpanNER serves as the foundation of the span-based NER model, outperforming the two basic sequence tagging models, BERT-Tagger and BERT-CRF. DataAug enhances SpanNER with data augmentation, further improving its OOE-NER capabilities. MINER incorporates the information bottleneck theory into SpanNER to better handle OOE-NER, achieving overall sub-optimal results. By fully leveraging sentence-level information, S+NER surpasses all baselines in OOE-NER, highlighting the crucial role of context information in OOE-NER."}, {"title": "4.4 Ablation Study", "content": "To validate the efficacy of S+NER, we conducted an ablation study, the results of which are presented in Table 4. In this study, we compared the performance of three methods on the WNUT2017, JNLPBA, and TwitterNER datasets. The first method is the original SpanNER, the second is SpanNER enhanced with context representation, and the third is an extension of the second method, incorporating template pooling and contrastive learning to optimize context representation, which we refer to as S+NER. The ablation study conducted on these three datasets consistently demonstrates that both context representation and contrastive learning, with the templates for context representation, positively impact the performance of the model."}, {"title": "4.5 Effects of Different Templates", "content": "As previously discussed, S+NER utilizes a template-based contrastive learning and template pooling method. To evaluate the performance of single template on S+NER for OOE-NER, we designed different templates inspired by Cui et al. (2021) and obtained the results in WNUT2017 shown in Table 5. From the experimental results, it is evident that S+NER is sensitive to template selection, with different templates directly affecting the final OOE-NER performance. The template pooling method, based on multiple templates, concurrently achieved optimal results, which underscores the efficacy of template pooling."}, {"title": "4.6 Effects of Different Encoders", "content": "We have argued that the special designs in S+NER, i.e., incorporating the context representation and refining it by contrastive learning, are model-agnostic. They can be plugged into the models with a sentence encoder. To verify this characteristic, we compared the variants of SpanNER and S+NER with different pre-trained language models as the encoder, including BERT-large (Devlin et al., 2019), ROBERTa-large (Liu, 2019) and DeBERTa-large (He et al., 2021). The comparison results shown in Table 6 imply that different pre-trained language models have different capabilities of natural language understanding, resulting in different NER performances. In addition, no matter which pre-trained language model is adopted as the encoder, S+NER consistently outperforms SpanNER, justifying the effectiveness of our special designs."}, {"title": "4.7 Effects on Different OOE Rates", "content": "We focus on the OOE-NER task in this paper, so we further investigated the compared models' performance in the scenarios with different OOE rates. To this end, we re-partitioned the training and test set to achieve different OOE rates in the dataset, meanwhile keeping the sentence and entity number as close as possible to the original dataset.\nWe display the results of our S+NER on TwitterNER compared with SpanNER, DSpERT and the previous competitive model MINER in Figure 3, where their performance on six different OOE rates are displayed. It shows that, although all models' performance degrades as the OOE rate increases, S+NER outperforms the three baselines on all OOE rates consistently. This result highlights the benefits of our proposed S+NER in OOE-NER compared to previous models. As the OOE rate increases (>80%), all models exhibit a significant drop in performance, underscoring the severity of the OOE issue."}, {"title": "5 Conclusion", "content": "In this paper, we propose a novel framework S+NER to handle Out-of-Entity Named Entity Recognition, in which we have two major designs: incorporating the sentence representation of the input sequence and then refining it with a contrastive learning and template pooling strategy. These two designs can help our framework better understand the contextual information of the target entity, and thus alleviate the OOE problem for achieving better OOE-NER. Our extensive experiments demonstrate our S+NER's advantage over the SOTA OOE-NER models. In addition, we also conduct experiments to examine the impact of various templates, pre-trained language model encoders, and OOE rates on the performance of S+NER. Through case studies, we intuitively demonstrate that S+NER outperforms other models."}, {"title": "Limitations", "content": "As we analyzed in the experiment section, our proposed framework relies on the PLM's capability of understanding the span's context. For the data from a professional field or a special domain, a PLM may not well understand the contexts since it is generally pre-trained with the corpus of open domains. In such scenario, re-pre-training the PLM or fine-tuning with the data from the special domain is expected for achieving satisfactory NER performance."}]}