{"title": "Exploring Scaling Trends in LLM Robustness", "authors": ["Nikolaus Howe", "Micha\u0142 Zajac", "Ian McKenzie", "Oskar Hollinsworth", "Tom Tseng", "Pierre-Luc Bacon", "Adam Gleave"], "abstract": "Language model capabilities predictably improve from scaling a model's size and training data. Motivated by this, increasingly large language models have been trained, yielding an array of impressive capabilities. Yet these models are vulnerable to adversarial prompts, such as \u201cjailbreaks\" that hijack models to perform undesired behaviors, posing a significant risk of misuse. Prior work indicates that computer vision models become more robust with model and data scaling, raising the question: does language model robustness also improve with scale? We study this question empirically, finding that larger models respond substantially better to adversarial training, but there is little to no benefit from model scale in the absence of explicit defenses.", "sections": [{"title": "1. Introduction", "content": "Language models have demonstrated a range of impressive capabilities in tasks such as general reasoning (Hendrycks et al., 2021), graduate-level Q&A (Rein et al., 2023), and code generation (Chen et al., 2021). This growth in capabilities has fueled rapid deployment, with ChatGPT becoming one of the fastest-growing consumer applications in history (Hu, 2023). Moreover, language models are increasingly integrated into larger systems enabling them to take actions in the real world using external tools (OpenAI, 2023; Anthropic, 2024; Google, 2024) and pursue long-term open-ended goals (Richards, 2024; Kinniment et al., 2024).\nThe advent of language models enables many new tasks to be solved by Al but also introduces novel classes of security vulnerabilities. In particular, a wide variety of adversarial prompts can hijack models (Wei et al., 2023; Zou et al., 2023; Anil et al., 2024). This enables malicious users to bypass safety fine-tuning performed by the designer, unlocking harmful capabilities such as generating compelling misinformation (Spitale et al., 2023; Chen and Shu, 2024). Innocent users are also at risk from attackers using methods such as indirect prompt injections (Abdelnabi et al., 2023) to exploit LLM-driven applications without any awareness or participation by the user.\nA key question is whether future, more capable systems will naturally become more robust, or if this will instead require a dedicated safety effort. Although current attacks are concerning, the risks could grow still greater with future models capable of more dangerous actions, such as assisting with biological weapon development (Mouton et al., 2023), or with greater affordances to interact with the world (Sharkey et al., 2023), such as a virtual assistant for a CEO of a major company. Prior work has found that superhuman Go systems (Wang et al., 2023) are vulnerable to attack, demonstrating that impressive capabilities do not guarantee robustness. However, work has also found that scaling unlabeled pretraining data (Hendrycks et al., 2019; Carmon et al., 2022; Alayrac et al., 2019) and model size (Xie and Yuille, 2019; Huang et al., 2023) improves adversarial robustness in computer vision.\nTo answer this question, we conduct an empirical investigation into scaling trends for the adversarial robustness of language models. These trends enable us to forecast the robustness of future models, and give us insight into how the offense-defense balance might shift over time. For example, does the cost of conducting an attack against more capable models grow faster"}, {"title": "2. Related Work", "content": "Adversarial examples were first identified in image classifiers (Szegedy et al., 2014), but have since been found for systems performing image captioning (Xu et al., 2019; Zhang et al., 2020), speech recognition (Cisse et al., 2017; Alzantot et al., 2018; Sch\u00f6nherr et al., 2018), and reinforcement learning (Huang et al., 2017; Gleave et al., 2020; Ilahi et al., 2022). Moreover, a range of adversarial threat models (Gilmer et al., 2018) give rise to viable attacks.\nMost recently, many qualitatively different vulnerabilities have been found in language models, from human-understandable \u201cjailbreaks\u201d (Wei et al., 2023) to seemingly gibberish adversarial suffixes (Wallace et al., 2021; Zou et al., 2023). Simple methods such as perplexity filtering and paraphrasing defend against some of these attacks (Jain et al., 2023). However, these defenses can easily be bypassed by more sophisticated methods (Zhu et al., 2023). Adversarial training shows more promise as a defense (Ziegler et al., 2022), and is the focus of our analysis.\nThe determinants of adversarial robustness have been well-studied in computer vision. One line of scholarship proposes a fundamental tradeoff between robustness and accuracy (Tsipras et al., 2019): exploitable models are simply relying on non-robust features (Ilyas et al., 2019), which improve training performance but hurt robustness. Other work has emphasized what does improve robustness. Scaling unlabeled pretraining data (Hendrycks et al., 2019; Carmon et al., 2022; Alayrac et al., 2019), model depth (Xie and Yuille, 2019) and model width (Huang et al., 2023) improves adversarial robustness in computer vision. However, other work shows that computer vision adversarial robustness scales too slowly to be a full solution (Debenedetti et al., 2023; Bartoldson et al., 2024).\nLanguage model scaling laws (Hestness et al., 2017; Rosenfeld et al., 2019; Kaplan et al., 2020; Hoff-"}, {"title": "3. Experimental Methodology", "content": "We test models in the binary classification setting, as it is the simplest context in which to study LLM robustness. Crucially, binary classification allows us to measure robustness by the attack success rate, defined as the proportion of examples correctly classified by the model before attack that are incorrectly classified after attack. We adapt pretrained models for classification by replacing the unembedding layer with a randomly initialized classification head, and then fine-tune the models on each task.\nTasks We consider four tasks in our experiments, the latter two developed by us for this project:\n\u2022 Spam (Metsis et al., 2006): Given the subject and body of an email, is it spam or not?\n\u2022 IMDB (Maas et al., 2011): Given a movie review,"}, {"title": "4. Fine-tuning", "content": "Figure 1 shows the robustness of fine-tuned models against the GCG attack. The attack is generally less successful on larger models, but model size alone does not explain all the variance in attack success rate. We observe similarly large random variation in attack success across model sizes on other tasks and with other attacks; for more details, see Appendix D.2.\nAs described in Section 3, we use the Pythia models, which range from 7.6M to 11.6B parameters after replacing the unembedding matrix with a classification head. We fine-tune all models for a single epoch with default hyperparameters from HuggingFace Transformers (Wolf et al., 2019), except for the learning rate which we set to le-5. All models reach > 83% accuracy on all tasks, with larger models generally performing better (see Appendix D.1 for the final validation performance of all models on all tasks). We then evaluate the fine-tuned models against adversarial attacks on an unseen validation dataset.\nTo understand the source of the variability in model robustness shown by our experiments, we varied 1) the pretraining checkpoint, 3 and 2) the random seeds used to initialize the classification head before fine-tuning. Both factors led to significant variability in model robustness, with pretraining checkpoint contributing significantly more variability. The variability was comparable or greater than that of an order of magnitude of model scaling, indicating that out-of-the-box robustness on a given task is heavily influenced by the randomness of the pretraining procedure itself.\nThis initial result suggests that we cannot rely on scale alone to solve the problem of robustness. However, in practice, we would apply a defense to a model prior to deploying it in a security-critical setting. In the following section, we consider whether scale enables defenses to more effectively improve model robustness."}, {"title": "5. Adversarial training", "content": "In this section, we explore how model size impacts robustness when performing adversarial training. Figure 2 evaluates the robustness of Pythia models to the GCG attack when adversarially trained against the same attack. We see a much cleaner trend than in the fine-tuning only case: larger models gain robustness"}, {"title": "5.1. Robustness transfer", "content": "In practice, we often do not have the luxury of knowing the exact attack method an adversary may employ against our model. For practical deployments, we therefore need adversarial training on a handful of attacks to provide more general robustness against other unforeseen attacks. In this subsection, we study whether we observe this transfer in robustness between attacks\u2014and how model scale affects the transfer.\nFirst, we explore whether robustness from adversarial training transfers to a stronger attack from the same family. To do this, we adversarially train using the procedure described above with GCG for 10 iterations as our training attack. We then evaluate on GCG for 30 iterations, a stronger attack. Figure 3a shows that larger models are more robust to this in-distribution, stronger attack. Although the transfer is imperfect\u2014the models do, of course, lose against 30-iteration GCG more than against 10-iteration GCG-the performance is much better than the undefended (fine-tuned) models, which lose approximately 100% of the time.\nThis is a promising result. Yet, what happens if our models experience an attack that is not only stronger but also uses a different method than the one on which they were adversarially trained? We investigate this question by performing adversarial training against RandomToken and evaluating against the GCG attack. Figure 3b shows models adversarially trained on"}, {"title": "6. Conclusion", "content": "Our results demonstrate that larger Pythia models benefit more from adversarial training than smaller Pythia models across a variety of classification tasks. An important direction for future work is to validate this trend holds in a broader variety of settings. In particular, we plan to study generative tasks and how factors such as task complexity affect robustness. We also plan to investigate different model families, including larger models.\nA key application of scaling trends is to inform appropriate sizing of models to maximize robustness given a fixed defender compute budget. Although larger models are more sample efficient with a fixed number of adversarial training time steps, each adversarial training step is more computationally expensive than with smaller models. For example, Figure 2 shows that performing 8 adversarial training rounds on the 17.6M parameter model results in better robustness than performing 4 adversarial training rounds on the 44.7M parameter model, and a quick calculation shows that it is slightly less expensive to train (see Appendix E.3). However, using a smaller model is not always better, since there are diminishing returns to adversarial training, with larger models appearing to converge to be more robust.\nAlthough scale can improve robustness, our results make clear that it is far from the only determinant. For example, a small adversarially trained model is more robust than a large model fine-tuned only on clean data. We expect that achieving robust language models will require innovations in defense techniques as well as scaling model pretraining and defense training. Scaling trends both enable us to measure how far we are from achieving robustness by scale alone and enable us to identify defense techniques that can better leverage scale to produce more robust models."}, {"title": "A. Models", "content": "In this work, we use the Pythia suite (Biderman et al., 2023), a collection of 10 autoregressive language models\nof different sizes, all pretrained for one epoch on the Pile (Gao et al., 2020). Model checkpoints are provided\nevery thousand steps; for the experiments presented in this work, we always start from the final checkpoint (the\nmain revision on HuggingFace Hub) unless otherwise specified.\nWe reproduce the model sizes of the Pythia suite in Table 1. Note that the number of parameters differs\nfrom that given in the model name because we use the models for classification tasks, which replaces the\nunembedding layer with a (smaller) classification head."}, {"title": "B. Datasets", "content": "We consider four datasets in this paper. Two of them are pre-existing datasets that we use from HuggingFace\nHub: Spam (Metsis et al., 2006) and IMDB (Maas et al., 2011). Two are synthetic datasets that we generate\nourselves: PasswordMatch and WordLength. For representative datapoints of these datasets, see Table 3.\nSince the context window for the Pythia model family is 2048 tokens (Biderman et al., 2023), we must be\ncareful not to run models on datapoints that are longer than this threshold. For fine-tuning, presented in\nSection 4, we train on the entire dataset, filtering out the (very few) datapoints which exceed 2000 tokens. We\ncap at 2000 tokens instead of the 2048 token context length to leave room for adversarial attacks, special tokens,\nand any other additional tokens we might need. Table 2 shows the number of datapoints in each dataset, as\nwell as the number of datapoints that exceed 2000 tokens.\nFor the PasswordMatch task, we allow attacks to replace the 'user-provided' password, instead of treating\nthe prompt as immutable and appending new text only after it."}, {"title": "C. Adversarial Attacks", "content": "The primary attack we use is GCG from Zou et al. (2023). We use the simple, single-prompt version described\nin Algorithm 1 of Zou et al. (2023) with the modifiable subset I set to be the final N tokens of the prompt\n(except for PasswordMatch, where there is a final separator after the attack tokens; see Table 3). We use\na suffix of length N = 10, batch size B = 128, and k = 256 top substitutions for all experiments. We use\nT = 10 iterations for most experiments, using T = 30 to evaluate robustness transfer from adversarially training\non a weaker attack (T = 10).\nWe describe the baseline RandomToken algorithm in Algorithm 1. RandomToken is designed to be similar to\nGCG except that RandomToken does not use gradient-guided search. Instead, for each iteration we replace each\ntoken in the adversarial suffix with a new token chosen uniformly at random from the vocabulary of the model.\nWe then evaluate the new prompt to see if it has caused the model to give an incorrect answer and stop the\nattack if it has. If no iteration was successful, we return the adversarial suffix from the final iteration.\nTo make sure the baseline is a fair comparison, we constrain the attacks to use the same maximum number of\nforward passes. To do this, we compute the number of forward passes used by GCG as B \\times T = 1280 and thus\nperform up to 1280 iterations of RandomToken."}, {"title": "D. Fine-tuning", "content": "For each task, we fine-tune each model for a single epoch. The final validation accuracies are shown in Table 4."}, {"title": "E. Adversarial Training and Transfer", "content": "The overall adversarial training procedure is presented in Figure 12."}, {"title": "E.1. Adversarial Training", "content": "Below, we show plots of adversarial training using the GCG and RandomToken attacks across the four tasks. We\nuse three seeds per model, and present attack success rate after 10 and 30 rounds of adversarial training."}, {"title": "E.2. Transfer", "content": "As presented in Section 5.1, we also evaluate how models adversarially trained with one attack generalize to\ndefending against other attacks. We present two collections of plots: first, models trained on the 10-iteration\nGCG attack and evaluated with the 30-iteration GCG attack; second, models trained on the RandomToken attack\nand evaluated on the (10-iteration) GCG attack. In the first case, all model sizes are able to generalize to being\nsomewhat robust against the stronger attack, though larger models do so both faster and to a greater extent.\nBy contrast, in the second case, only the larger models are able to generalize within the 10 adversarial training\nrounds studied."}, {"title": "E.3. Complexity Calculation", "content": "In Section 6, we compare the relative complexity of adversarially training a larger model for fewer rounds or a\nsmaller model for more rounds. In this section, we provide a worked example. We use a batch size of 8 for\nboth the 17.6M and 44.7M models. We start with 2000 datapoints in the train dataset and add 200 datapoints\neach round. This means that after 4 rounds of training, each model will have seen \\sum_{i=1}^{4} (250 + i \\cdot 25) = 1250\nbatches, and after 8 rounds of training, \\sum_{i=1}^{8} (250 + i \\cdot 25) = 2900 batches. If we update model parameters\nonce per batch, this means that after 4 rounds, the 44.7M parameter model will have had 44.7M\\cdot 1250 = 55875M\ngradient updates, while after 8 rounds, the 17.6M parameter model will have had 17.6M \\cdot 2900 = 51040M\ngradient updates."}]}