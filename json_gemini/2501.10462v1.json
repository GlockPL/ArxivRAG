{"title": "BloomScene: Lightweight Structured 3D Gaussian Splatting for Crossmodal Scene Generation", "authors": ["Xiaolu Hou", "Mingcheng Li", "Dingkang Yang", "Jiawei Chen", "Ziyun Qian", "Xiao Zhao", "Yue Jiang", "Jinjie Wei", "Qingyao Xu", "Lihua Zhang"], "abstract": "With the widespread use of virtual reality applications, 3D scene generation has become a new challenging research frontier. 3D scenes have highly complex structures and need to ensure that the output is dense, coherent, and contains all necessary structures. Many current 3D scene generation methods rely on pre-trained text-to-image diffusion models and monocular depth estimators. However, the generated scenes occupy large amounts of storage space and often lack effective regularisation methods, leading to geometric distortions. To this end, we propose BloomScene, a lightweight structured 3D Gaussian splatting for crossmodal scene generation, which creates diverse and high-quality 3D scenes from text or image inputs. Specifically, a crossmodal progressive scene generation framework is proposed to generate coherent scenes utilizing incremental point cloud reconstruction and 3D Gaussian splatting. Additionally, we propose a hierarchical depth prior-based regularization mechanism that utilizes multi-level constraints on depth accuracy and smoothness to enhance the realism and continuity of the generated scenes. Ultimately, we propose a structured context-guided compression mechanism that exploits structured hash grids to model the context of unorganized anchor attributes, which significantly eliminates structural redundancy and reduces storage overhead. Comprehensive experiments across multiple scenes demonstrate the significant potential and advantages of our framework compared with several baselines.", "sections": [{"title": "Introduction", "content": "Currently, there is a growing demand for 3D content in virtual reality. However, creating 3D content is time-consuming and requires deep expertise, making 3D content generation a challenging frontier. In the 2D domain, sufficient annotated datasets have greatly contributed to the development of text-to-image generation models (Rombach et al. 2022), enabling users to generate images through natural language. However, the shortage of annotated 3D datasets limits the application of supervised learning in 3D content generation (Ouyang et al. 2023). To address this challenge, recent studies (Poole et al. 2022; Lin et al. 2023) extract 2D priors from diffusion models through a time-consuming distillation process to optimize the generation of 3D content. However, these methods (Wang et al. 2024) have limitations when extended to fine-grained scenes with outward-facing viewpoints. Therefore, several methods (H\u00f6llein et al. 2023; Ouyang et al. 2023; Hou et al. 2024) that combine pre-trained text-to-image generation models (Rombach et al. 2022) with monocular depth estimators (Bhat et al. 2023; Ranftl et al. 2020) are receiving increasing attention due to their advantages in complex 3D scene generation.\nSome methods (H\u00f6llein et al. 2023; Fridman et al. 2024) generate 3D indoor scenes represented by mesh using a progressive framework but are prone to distorted or over-smoothing regions when applied to outdoor scenes. With the wide application of NeRF (Mildenhall et al. 2021) in novel view synthesis tasks, Text2NeRF (Zhang et al. 2024) generates 3D scenes represented by NeRF with a progressive framework. Although this method can generate high-quality scenes, the generation time is still quite long. Recently, 3D Gaussian Splatting (3DGS) (Kerbl et al. 2023) has been widely used for high-quality scene generation due to its excellent generation quality and real-time rendering capabilities. Among them, LucidDreamer (Chung et al. 2023) and Text2Immersion (Ouyang et al. 2023) use a progressive generation framework that follows the optimization goals of 3DGS to achieve domain-free 3D scene generation. Although previous 3DGS-based approaches have made some progress in 3D scene generation, they still suffer from the following limitations: (i) Rely only on photometric loss in the scene optimization process, lack sufficient regularization techniques, and are prone to artifacts and ambiguities.(ii) 3DGS requires millions of 3D Gaussians to represent each scene, resulting in high memory requirements, increasing storage costs, and end-device burden.\nTo address the above problems, we propose Bloom-"}, {"title": "Preliminaries", "content": "Methodology\n3D Gaussian Splatting (3DGS) (Kerbl et al. 2023) introduces the 3D Gaussians as differential volumetric representations of radiance fields, allowing high-quality real-time novel view synthesis. A set of splats is initialized from the calibrated camera poses and the sparse point clouds produced through Structure-from-Motion (SfM) (Snavely, Seitz, and Szeliski 2006). Each Gaussian is represented by position \u00b5 and covariance matrix \u2211, denoted as $G(x) = e^{-(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}$. The covariance can be decomposed from a scaling matrix S and rotation matrix R, expressed as $\\Sigma = RSST RT$ with S. To render the color, 3DGS further optimizes opacity and Spherical Harmonic (SH) coefficients, following the point-based differential rendering by rasterizing anisotropic splats with a-blending, denoted as:\n$\\widehat{C} = \\sum_{i}^{N} c_{i} \\alpha_{i} \\prod_{j}^{i-1} (1 - \\alpha_{j}), D = \\sum_{i}^{N} d_{i} \\alpha_{i} \\prod_{j}^{i-1} (1 - \\alpha_{j}),$ (1)\nwhere $c_{i}$ and $\\alpha_{i}$ denote the color and opacity of the Gaussian, and $d_{i}$ is the z-axis of the points by projecting the center of 3D Gaussians \u00b5 to the camera coordinate."}, {"title": "Crossmodal Progressive Scene Generation", "content": "Previous methods (Wang et al. 2024) have made progress in the single-object generation, but it is difficult to ensure texture and structural coherence when generating complex scenes with outward-facing viewpoints. To realize cross-modal 3D scene generation, we propose a crossmodal Progressive Scene Generation (PSG) framework to incrementally generate lightweight and high-quality scenes with reference to previous work (Ouyang et al. 2023; Chung et al. 2023). The main workflow of the proposed PSG is shown in Figure 1, which consists of four main phases: point cloud construction, 3D Gaussians initialization, hierarchical Depth Prior-based Regularization (DPR) mechanism to optimize the quality of the 3DGS-generated scenes, and Structured Context-guided Compression (SCC) for reducing the storage overhead. All these phases constitute the PSG framework for realizing cross-modal, high-quality 3D scenes.\nPoint Cloud Construction. Given a text prompt y, our goal is to generate a 3D scene that matches y in a crossmodal manner. We use a text-conditioned image inpainting model $F_{inpaint}$ and a monocular depth estimator $F_{d}$ to progressively inpaint and update the scene. The pre-trained text-to-image diffusion model $F_{t2i}$ is used to generate the initial image $I_{0}$ from text prompt y. If the input is an image without a corresponding text description, the pre-trained image-to-text generation model $F_{i2t}$ is used to generate the corresponding text prompt y from $I_{0}$, constituting an image-text pair. $F_{d}$ is then used to obtain the depth map $D_{0}$ from $I_{0}$."}, {"title": "Hierarchical Depth Prior-based Regularization", "content": "3DGS represents the scene more realistically through numerous 3D Gaussians with geometric and appearance attributes. The scenes generated by 3DGS in the progressive scene generation framework tend to be ambiguous and artifactual since the scene contains millions of attributes of Gaussian distributions optimized only via gradient descent based on photometric loss. Previous work (Yuan et al. 2024; Li et al. 2024) utilizes score distillation to achieve 3D scenes with consistency, which improves the quality of novel view synthesis to some extent. Despite their progress, some limitations remain: (1) Lack of precise constraints on 3D cues and depth information in the optimization process. (2) Neglecting effective supervision of the visual and geometric smoothness of the scene. The above issues limit the realism and continuity of 3D scene generation. To this end, we propose a hierarchical Depth Prior-based Regularization (DPR) mechanism that implements multi-level regularization on the 3D Gaussians utilizing high-quality depth prior. Specifically, we implement joint constraints on the depth maps generated by 3DGS at the pixel level and distribution level by utilizing the Huber loss and Central Moment Discrepancy (CMD), respectively. Furthermore, the bilateral filter is leveraged to enhance the continuity of the depth information. In the following, the depth map $\\widehat{D}$ is obtained by the monocular depth estimator $F_{d}$. 3DGS estimates the z-depth map D of all pixel by the Equation (1).\nDepth Estimation Accuracy Constraints. We utilize a multi-scale constraint paradigm at the pixel level and distribution level to achieve accurate estimation of depth information. The depth of object edges is difficult to estimate and inaccurate in depth maps. The edges of objects tend to be regions with large image gradients. Thus, to apply more attention to the edges, we design a gradient-aware Huber-based depth loss for implementing pixel-level depth constraints and adaptive depth regularization, denoted as follows:\n$L_{DPR}^{cpixel} =\\begin{cases}  \\frac{g_{rgb}}{9 |\\Omega|} \\sum_{p} \\sum_{q \\in \\Omega} ||D - \\widehat{D}||_{1}, & \\text{if } ||D - \\widehat{D}||_{1} > \\delta\\\\  \\frac{g_{rgb}(D-\\widehat{D})^{2}}{2\\delta^{2}} , & \\text{otherwise}  \\end{cases}$ (7)\nwhere $g_{rgb} = exp(-\\nabla)$ and $\\nabla$ is the gradient of the current aligned RGB image, $\\delta = 0.2 max ||D - \\widehat{D}||_{1}$, and $|\\Omega|$ indicates the total number of pixels in D. Image edges with larger gradients are dynamically assigned smaller learning weights. Constraining two depth maps only at the pixel level ignores the discrepancy of their distributions. Therefore, we implement distribution-level alignment between depth maps based on CMD, which has been widely used in domain adaptation to estimate the discrepancy between two domains (Zellinger et al. 2019). CMD can utilize higher-order moments to effectively capture higher-order statistical information without kernel function dependence. For a random variable X, the k-th central moment is given by: $\\mu_{k} = E [(X - E[X])^{k}]$, where E[X] denotes the mean of X. For two distributions P and Q, the CMD computes the discrepancy by summing the differences of their corresponding central moments up to order K :\n$D_{CMD} (P,Q) = \\sum_{k=1}^{K}  \\frac{1}{k!}||\\mu_{k}^{P} - \\mu_{k}^{Q}||^{2}_{2}$.  (8)\nwhere $\\mu_{k}^{P}$ and $\\mu_{k}^{Q}$ are the k-th central moments of distributions P and Q, respectively, and $|| ||_{2}$ represents the Euclidean norm. The CMD-based depth loss is expressed as:\n$L_{DPR}^{CMD} = D_{CMD} (D, \\widehat{D}).$ (9)\nDepth Smoothness Constraints. To address the problem that object boundaries in 3DGS-rendered images often appear to have nonsmooth edges, we propose a depth loss based on the bilateral filter (Tomasi and Manduchi 1998). Bilateral filtering is a typical nonlinear filtering method that simultaneously considers both the space and value domain information, allowing the removal of depth noise while preserving the boundaries and details of the image. Given two pixels p and q in the depth map with coordinates (i, j) and (m, n) respectively. The spatial kernel and color kernel of bilateral filtering are denoted as:\n$L_{DPR}^{smooth} = \\frac{1}{|\\mathcal{N}(p)|} \\sum_{q \\in \\mathcal{N}(p)} G_{s} (p,q) \\cdot G_{c}(p,q) \\cdot (\\widehat{D}_{p} - \\widehat{D}_{q})^{2},$ (10)\nwhere $|\\mathcal{N}(p)|$ is the number of pixels in the neighborhood of pixel p, $\\widehat{D}_{p}$ is the depth value at pixel p, spatial kernel is denoted as $G_{s}(p, q) = exp(-\\frac{(i-m)^{2}+(j-n)^{2}}{2 \\sigma^{2}_{s}})$, and color kernel is denoted as $G_{c}(p, q) = exp(-\\frac{||D_{p} - D_{q}||^{2}}{2 \\sigma^{2}_{c}})$. Consequently, the loss of DRP is expressed as:\n$L_{DPR} = \\lambda_{1} L_{DPR}^{cpixel} + \\lambda_{2} L_{DPR}^{CMD} + \\lambda_{3} L_{DPR}^{smooth},$ (11)\nwhere $\\lambda_{1}$, $\\lambda_{2}$, and $\\lambda_{3}$ are set to 0.7, 0.1 and 1.0, respectively."}, {"title": "Structured Context-guided Compression", "content": "The microscopic 3D Gaussians with optimizable geometric and appearance attributes in 3DGS make it a powerful advantage for rendering a variety of scenes. Nevertheless, a complex and larger-scale scene often requires a prohibitively large number of 3D Gaussians for fine-grained representation, resulting in significant storage overhead. Furthermore, in real-world applications, low-cost and lightweight models are more conducive to deployment and rapid scene generation. Due to the unorganized and sparse properties of 3D Gaussians (Chen and Wang 2024), compressing 3D Gaussians is a challenging task. Mainstream 3DGS compression methods mostly focus only on the \"values\" (Fan et al. 2023; Navaneet et al. 2023), ignoring the structural correlation between their 3D Gaussians, resulting in a large amount of structural redundancy and inefficient compression. Scaffold-GS (Lu et al. 2024) introduces anchors to cluster nearby relevant 3D Gaussians and utilizes the anchors' properties to predict the 3D Gaussians' properties. Although Scaffold-GS exploits the spatial correlations among 3D Gaussians, the independence of anchors leads to a large number of sparse and disordered anchors that are difficult to compress. HAC (Chen et al. 2025) models the relationship among the anchors to some extent, but it insufficiently quantifies the anchors, leading to sub-optimal storage compression results. To take full advantage of the correlation between un-organized anchors, inspired by Scaffold-GS and HAC, we propose a Structured Context-guided Compression (SCC) mechanism that utilizes a structured hash feature mesh to model the context of the anchor attributes.\nDescription of anchors. In Scaffold-GS, each anchor is composed of a location $x_{a} \\in R^{3}$ and an anchor attribute $A = \\{f_{a} \\in R^{D_{a}}, l \\in R^{6}, o \\in R^{3K}\\}$, where each component represents anchor feature, scaling, and offsets, respectively. During the rendering phase, the anchor feature is fed into the MLPs to generate attributes for 3D Gaussians, whose locations are determined by adding $x_{a}$ and o, where I is utilized to regularize both locations and shapes of the Gaussians. The attributes inferred from the anchor attributes by neighboring 3D Gaussians should be similar. Thus, following the methodology of HAC, we utilize a structured hash grid to model the inherent spatial consistency of independent anchors. The core idea is to use the hash feature $f_{h}$, obtained by implementing trilinear interpolation in the hash grid, to model the context of anchor attributes. There is rich mutual information between anchor feature $f_{a}$ and hash feature $f_{h}$, thus maximizing the conditional probability of both can reduce the entropy of the feature and bit consumption (Chen et al. 2025).\nAnchor feature modeling. To facilitate entropy coding, the values of A must be quantized into a finite set. HAC utilizes uniform distribution-based noise and rounding operations to implement quantization during the training and testing phases, respectively, which are not sufficiently dynamic and smooth. Therefore, we propose a dynamic quantization strategy. Specifically, for the i-th anchor x, we denote fi as any of its $A_{i}$'s components: $f_{i} \\in \\{f_{i}, l_{i}, o_{i}\\} \\in R^{D}$, where D$\\in \\{D_{a}, 6, 3K\\}$ is its respective dimension. In the training phase, we construct a Gaussian noise to update the features, denoted as:\n$\\widehat{f_{i}} = f_{i} + N(0,\\omega_{i}^{2}),$ (12)\nwhere $\\omega_{i} = \\eta_{i} \\cdot Tanh(F_{q}(f_{h}))$ with $\\eta_{i} \\in \\{2.5e - 1, 2.5e - 4, 5e - 2\\}$, and $F_{q}$ is an MLP for generating factors to dynamically optimize quantization. In the inference phase, we utilize a semi-soft rounding operation to make the quantized results closer to the true values, but still retain some discretization, expressed as:\n$\\widehat{f_{i}} = \\frac{\\omega_{i}}{\\tau} + Tanh(\\frac{f_{i} -k\\omega_{i}}{\\tau}) \\cdot \\omega_{i}$ (13)\nwhere the smoothing hyperparameter $\\tau$ is 1. To measure and reduce the bit consumption of $\\widehat{f_{i}}$ during training, we need to estimate its probability in a microscopic manner. All three attributes of the anchors exhibit statistical tendencies of Gaussian distributions (Chen et al. 2025). Thus, based on the independence of the anchor attributes, we construct Gaussian distributions for all anchor attributes, with \u03bc and \u03c3 in the respective distributions estimated by an MLP $F_{g}$ from $f_{h}$. The probability of $\\widehat{f_{i}}$ is computed as:\n$p(\\widehat{f_{i}}) = \\int_{\\widehat{f_{i}}-\\omega}^{\\widehat{f_{i}}+\\omega}  \\Phi_{\\mu,\\sigma} (x) dx,$ (14)\nwhere \u03a6 represents the probability density function and $\\mu_{i},\\sigma_{i} = F_{g}(f_{h})$. Ultimately, we define the entropy loss as the sum of the bit consumption of all $f_{i}$:\n$L_{entropy} = \\beta \\sum_{f \\in \\{f_{a},l,o\\}} \\sum_{i=1}^{N} \\sum_{j=1}^{D_{i}} (-log_{2}p^{j} (\\widehat{f_{i}})),$ (15)\nwhere $\\beta = \\frac{1}{N(D_{a}+6+3K)}$, N is the number of anchors, $D_{a} = 50$ is the anchor feature dimension, K = 10 is the number of learnable offsets and $f_{i}^{j}$ means the j-th dimension value of $f_{i}$. Minimizing the entropy loss achieves a high probability estimation of p(fi) that guides the learning of the contextual model. The SCC loss is denoted as:\n$L_{SCC} = \\lambda_{4} L_{vol} + \\lambda_{5} L_{entropy},$ (16)\nwhere $\\lambda_{4}$ and $\\lambda_{5}$ are set to to 1e - 2 and 2e - 3, $L_{vol}$ is the regularization term defined in (Lu et al. 2024)."}, {"title": "Optimization Objectives", "content": "The final loss we use for optimization is defined as follows:\n$L = L_{RGB} + L_{DPR} + L_{SCC},$ (17)\nwhere $L_{RGB}$ is the original photometric loss proposed in (Kerbl et al. 2023)."}, {"title": "Experiments", "content": "To achieve a fair and comprehensive comparison, we select 9 text prompts describing the indoor, outdoor, and artistic style scenes: (1) A living room with a lit furnace, couch and cozy curtains, bright lamps that make the room look well-lit. (2) A cozy living room in Christmas. (3) A small cabin on top of a snowy mountain, Disney style. (4) A suburban street in North Carolina on a bright, sunny day. (5) Simple museum, pictures, paintings, artistic, best quality, dimly lit. (6) A children's room filled with toys and books. (7) A sunroom with floor-to-ceiling windows overlooking the garden, comfortable chairs, and a coffee table inside. (8) A sunny beach with fine sand and blue water, with a backdrop of blue sky and white clouds. (9) A winter snow scene with snow-covered trees and houses."}, {"title": "Implementation Details", "content": "To maximize the generalization ability of the proposed BloomScene, we use pre-trained models to build the entire architecture. Specifically, Stable Diffusion v1.5 (Rombach et al. 2022) is used to generate the initial image from the text prompt. If the input is an image without a corresponding text description, LLaVa (Contributors 2023) is used to generate the corresponding text prompt from the image, constituting an image-text pair. We use the Stable Diffusion v1.5 Inpainting model (Rombach et al. 2022) as the text-conditioned image inpainting model. We use ZoeDepth (Bhat et al. 2023) as the monocular depth estimator. To generate 3D scenes, we move the camera with a rotation of 0.63 radians. All experiments are done on a single NVIDIA A800 GPU. All experimental results are averaged over multiple experiments using five different random seeds."}, {"title": "Comparison with State-of-the-Art Methods", "content": "We compare the proposed BloomScene with five representative and reproducible methods, including progressive 3D scene generation methods: Text2Room (H\u00f6llein et al. 2023), Invisible-stitch (Engstler et al. 2024) and LucidDreamer (Chung et al. 2023), and perpetual view generation methods: SceneScape (Fridman et al. 2024) and WonderJourney (Yu et al. 2024). We use the open-source codebase of the above models and modify the inputs to start from the same initial images and text prompts."}, {"title": "Qualitative Results", "content": "We perform an intuitive qualitative analysis. We show the rendered RGB images of our method and baseline methods in the new viewpoints in Figure 2 and Figure 3. We have the following observations: (i) SceneScape, WonderJourney, and Invisible-stitch generate relatively complete scene content, but clear breaks and geometric distortions can be observed in boxed areas. (ii) Text2Room uses a polygonal mesh to represent the scene, but its mesh fusion threshold filtering scheme results in incomplete detection of stretched regions, leading to distorted and over-smoothing areas in the scene. (iii) LucidDreamer is currently the most visually effective progressive scene generation method but suffers from artifacts and geometric distortions in boxed areas. (iv) In contrast, our method preserves the necessary scene structures, significantly reduces artifacts and geometric distortions, and provides high-quality and realistic rendered results."}, {"title": "Ablation Studies", "content": "To verify the necessity of the different components, we perform comprehensive ablation experiments using the same set of text prompts. Figure 4 shows the rendered results and Table 2 shows the average quantitative results for multiple scenes. (i) Firstly, DPR is removed from BloomScene. The decreased performance and the worse depth rendered results indicate that effective supervision of depth information and smoothness during optimization is crucial in the realism and continuity of 3D scenes. (ii) Moreover, we replace SCC with the original 3DGS. The dramatic increase in scene storage overhead indicates that compression for complex and larger-scale scenes is very necessary. (iii) Eventually, we remove the loss terms from DPR. The degraded and worse performance in depth map smoothness and accuracy indicate that the various loss items of DPR are necessary."}, {"title": "Conclusion", "content": "In this paper, we propose BloomScene, a lightweight structured 3D Gaussian splatting for crossmodal scene generation. Specifically, a crossmodal progressive scene generation framework is proposed to incrementally generate coherent scenes. Furthermore, we propose a hierarchical depth prior-based regularization mechanism that utilizes multi-level constraints on depth accuracy and smoothness to enhance the realism and continuity of the generated scenes. Finally, we propose a structured context-guided compression mechanism that utilizes structured hash grids to model the context of unorganized anchor attributes, thus significantly reducing storage overhead. Comprehensive qualitative and quantitative experiments across multiple scenarios show that the proposed framework has significant advantages over several baselines. Our framework opens up more possibilities for future virtual reality applications."}]}