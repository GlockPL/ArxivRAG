{"title": "Model Counting in the Wild", "authors": ["Arijit Shaw", "Kuldeep S. Meel"], "abstract": "Model counting is a fundamental problem in automated reasoning with applications in probabilistic inference, network reliability, neural network verification, and more. Although model counting is computationally intractable from a theoretical perspective due to its #P-completeness, the past decade has seen significant progress in developing state-of-the-art model counters to address scalability challenges.\nIn this work, we conduct a rigorous assessment of the scalability of model counters in the wild. To this end, we surveyed 11 application domains and collected an aggregate of 2262 benchmarks from these domains. We then evaluated six state-of-the-art model counters on these instances to assess scalability and runtime performance.\nOur empirical evaluation demonstrates that the performance of model counters varies significantly across different application domains, underscoring the need for careful selection by the end user. Additionally, we investigated the behavior of different counters with respect to two parameters suggested by the model counting community, finding only a weak correlation. Our analysis highlights the challenges and opportunities for portfolio-based approaches in model counting.", "sections": [{"title": "Introduction", "content": "Given a Boolean formula F (often presented in conjunctive normal form), the problem of model counting is to compute the number of solutions of the formula. Model counting is a fundamental problem in computer science and has been studied by theoreticians and practitioners alike for the past four decades. From the perspective of theoreticians, model counting is a central problem in computational complexity: the seminal work of Valiant (1979) established that the problem of model counting is #P-complete, where #P is the class of counting problems whose decision versions lie in NP. Toda's celebrated result (Toda 1989) showed that a single call to a #P-oracle suffices to solve a problem in the entire polynomial hierarchy; formally, PH \u2286 P#P. On the other hand, from practitioners' perspectives, model counting emerges as a central problem in a wide variety of domains such as quantitative software verification (Teuber and Weigl 2021; Girol, Farinier, and Bardin 2021), probabilistic inference (Darwiche 2004), network reliability (Kabir and Meel 2023), cryptography (Beck, Zinkus, and Green 2020), synthesis (Golia, Roy, and Meel 2020), product lines (Sundermann et al. 2023; Kuiter et al. 2022), neural network verification (Baluta et al. 2019), and information flow (Bang et al. 2016). Consequently, despite theoretical hardness (in the worst case), there has been a demand for the development of algorithms and tools for model counting.\nThe earliest algorithmic approaches to model counting were pioneered in the early 2000s, combining advances in conflict-driven clause learning with knowledge compilation (Darwiche 2004; Sang et al. 2004). Subsequently, approaches in the early 2010s based on universal hashing and SAT solving were developed to obtain probably approximate counters (Gomes, Sabharwal, and Selman 2006; Chakraborty, Meel, and Vardi 2013). Since then, there has been a significant surge of interest in the development of model counters. This development has led to substantial improvements in the runtime performance of state-of-the-art counters (Thurley 2006; Lagniez and Marquis 2017; Sharma et al. 2019; Korhonen and J\u00e4rvisalo 2021; Lai, Meel, and Yap 2021), best evidenced by the launch of the model counting competition in 2020 (Fichte, Hecher, and Hamiti 2020).\nThe model counting competition also allowed for the standardization of input and output formats, thereby making it easy to use different counters uniformly. The yearly competition also provides a snapshot of the performance of different counters. Given the annual snapshot of performance, one might be tempted to rely on the model counting competition to provide guidance on what counter to use in practice: for example, pick the winner of the latest model counting competition. Such a strategy is generally expected to fare well but may not be optimal since the objective of competitions is often to focus on benchmarks that are difficult. While selection focused on difficult benchmarks brings forth the weaknesses of state-of-the-art techniques, it may not guide the behavior of counters in the real world.\nThe primary objective of our investigation is the study of the scalability of model counters in the wild. To this end, we focused on the six state-of-the-art model counters, which have consistently performed well in model counting competitions over the past three years and rely on differing underlying"}, {"title": "Notations and Preliminaries", "content": "Let X be the set of Boolean variables, and let F be a Boolean formula in Conjunctive Normal Form (CNF) defined over variables in X. An assignment \u03c3 : X \u2192 {0,1} is called a satisfying assignment or a solution if o makes F evaluate to True. Given a set of projection variables P \u2286 X, a projection of assignment o to the set P is the subset of assignments only to the variables of P.\nModel Counting. Let Sol(F) denote the number of so- lutions of a given formula F. The model counting prob- lem is determining |Sol(F)|. An exact model counter takes in formula F, and returns |Sol(F)|. An approxi- mate model counter takes in a formula F, tolerance pa- rameter \u025b, confidence parameter 8 and returns c such that Pr[ |Sol(F)|/(1+\u03b5) \u2264 c \u2264 (1 + \u03b5)|Sol(F)|] \u2265 1 \u2212 \u03b4.\nProjected Model Counting. Let Sol(F)\u2193s denote the set of projected assignments satisfying the given formula F and a projection set S. The problem of projected model counting is to compute Sol(F)\u2193s. An exact projected model counter takes in formula F, and returns |Sol(F)\u2193s|. An ap- proximate projected model counter takes in a formula F, projection set S, parameters \u025b, and \u03b4, and returns c such that Pr[ |Sol(F)\u2193s|/(1+\u03b5) \u2264 c \u2264 (1 + \u03b5)|Sol(F)\u2193s|] \u2265 1 \u2212 \u03b4.\nTo differentiate between model counting and projected model counting, we use the term non-projected model counting to refer to model counting without projection.\nIndependent Support. For a given assignment & over X and a subset of variables S \u2286 X, let \u03c3\u03b9\u03c2 represent the assignment of variables restricted to S. Given a Boolean formula F over the set of variables X and a projection set SCX, a subset of variables I such that I \u2286 S is called independent support (or simply support) of S if \u2200\u03c31, \u03c32 \u2208 Sol(F), \u03c31|I = \u03c32|I \u21d2 \u03c31|S = \u03c32|S. Several preprocessing techniques for model counting have been proposed, which compute a small independent support for the input formula and simplify the formula based on that support (Lagniez, Lonca, and Marquis 2016; Soos and Meel 2019).\nTreewidth. Treewidth is a measure of how tree-like a graph is. A tree decomposition of a graph G = (V, E) is a pair (T, {Bi}i\u2208I) where T is a tree with nodes indexed by I, and {Bi}i\u2208I are subsets of V (bags) such that:\n1. Every vertex v \u2208 V is in at least one bag Bi.\n2. For every edge (u, v), there is a bag B\u2081 with u, v \u2208 Bi.\n3. For each vertex v, the bags containing v form a connected subtree of T.\nThe width of a tree decomposition (T, {Bi}i\u2208I) is maxier (|Bi| \u2212 1). The treewidth of G, denoted tw(G), is the minimum width among all tree decompositions of G: tw(G) = min(T,{B{}}) maxi\u22081(|Bi| \u2212 1)."}, {"title": "The Landscape of Model Counting", "content": "Significant progress has been made in developing efficient algorithms for model counting. In this section, we provide a brief overview of the different approaches.\n1. Compilation-based Exact Model Counters. The remarkable success of SAT solvers has motivated researchers to develop model counters that leverage the search techniques employed by these solvers. Darwiche (2004) introduced the use of deterministic decomposable negation normal form (d-DNNF) to efficiently obtain the model count in a model counter named c2d. During the search procedure of DPLL, the solver may encounter sub-formulas that have already been seen in a prior branch of the tree. To avoid redundant computations, it is essential to recognize such sub-formulas and reuse their model counts efficiently. To address this challenge, researchers have introduced the concept of component caching, which has led to the development of highly efficient model counters like Cachet (Sang et al. 2004) and SharpSAT (Thurley 2006). Lagniez and Marquis (2017) further improved this approach by utilizing dynamic decomposition tech- niques to enhance the efficiency of d-DNNF-based tech- niques and designed the D4 model counter. Subsequently, heuristics were integrated into component caching in the probabilistic model counter Ganak (Sharma et al. 2019). Korhonen and J\u00e4rvisalo (2021) combined the concept of tree decomposition was combined with component caching in SharpSAT-TD. GPMC uses a similar strat- egy as SharpSAT, but optimizes it for projected model counting. Lai, Meel, and Yap (2021) introduced a gener- alization of d-DNNF known as Constrained Conjunction and Decision Diagram (CCDD), which was implemented in the model counter ExactMC. While all the aforemen- tioned techniques employ a search-based top-down compi- lation approach, Dudek, Phan, and Vardi (2020) utilized algebraic decision diagram (ADD) based bottom-up com- pilation methods to design the model counter ADDMC, which also performs effectively due to its early-projection techniques.\n2. Hashing-based Approximate counter. Over the past decade, there has been the development of scalable ap- proximate model counters that rely on XOR-based pair- wise independent functions to divide the solution space into smaller parts and then invoke state-of-the-art SAT solvers to enumerate models in a randomly chosen cell to accurately estimate the model count (Chakraborty, Meel, and Vardi 2013; Chakraborty, Meel, and Vardi 2016; Soos and Meel 2019; Soos, Gocht, and Meel 2020). The state-of-the-art hashing-based counter, ApproxMC, has shown to work well in practice. It also supports prepro- cessing techniques based on independent set detection and scales better when Arjun (Soos and Meel 2022) provides a small independent set.\nIn the context of this survey, we focus on the six state-of- the-art model counters that have performed well in model counting competitions over the past year. The first five are top-down compilation techniques, with different technical improvements on top of the algorithm.\n1. SharpSAT-TD: Developed on top of SharpSAT, com- bined with a tree-decomposition-based heuristics.\n2. Ganak: Developed on top of SharpSAT, enhanced with probabilistic component caching.\n3. D4: A Decision-DNNF compilation based on dynamic decomposition.\n4. GPMC: Another top-down compilation-based counter, which uses optimizations for projected counting.\n5. ExactMC: Another top-down compilation-based model counter using CCDD for compilation.\n6. ApproxMC: A hashing-based approximate model counter.\nAmong these counters, ExactMC and SharpSAT-TD solve the problem of only non-projected model counting. The remaining solvers can solve the problems of both projected and non-projected model counting. We compare the performance of all the solvers in the categories in which they can solve the problem."}, {"title": "Benchmarks", "content": "We selected a large set of benchmarks from various practical domains to evaluate the model counters. Below is a list of these domains:\n1. Software Verification. In software verification, some quantitative problems are solved by reducing the problems to model counting. Here are two such problems:\n(a) Reliability Estimation. When the functional correctness of a program cannot be established, a potential approach to assess the software's reliability is to quantify it as the ratio of failing program runs to all terminating runs. Teuber and Weigl (2021) reduced this approach to model counting, where the model count corresponds to the number of inputs that trigger or bypass assertions or assumptions.\n(b) Robust Reachability. Determining the extent to which a bug can be replicated is frequently relevant. Girol, Farinier, and Bardin (2021) addressed this issue by employing the formalism of robust reachability. They also introduced the concept of quantitative robust reachability, which seeks to identify a controlled input that maximizes the number of uncontrolled inputs capable of reaching the intended target. Model counting can be used to lower bound the runtime cost by the cost of determining the number of uncontrolled inputs satisfying a path constraint for a given controlled input.\n2. Probabilistic Inference. Model counting is used to solve the problem of probabilistic inference. Sang, Beame, and Kautz (2005) encoded the inference problem on Boolean Bayesian networks as a model counting problem.\n3. Network Reliability. For critical infrastructure like power transmission grids, it is important to know the reliability of the infrastructure. Kabir and Meel (2023) encoded the problem of network reliability as a weighted model counting problem. They then used chain formulas (Chakraborty et al. 2015) to encode the problem as unweighted model counting problems. These benchmarks encode power transmission grids from different cities and states. The number of solutions to these formulas relates to the network reliability.\n4. Cryptography. Certain problems in cryptography can also be tackled with model counting. Beck, Zinkus, and Green (2020) used model counting to automate the development of chosen-ciphertext attacks.\n5. Synthesis. Given the specification of a function or program, the task of synthesis is to generate the function or program.\n(a) Program and Function Synthesis. Some algorithms for synthesis (Golia, Roy, and Meel 2020) use model counts in certain parts. These benchmarks consist of instances where the specifications of the functions like arithmetic, disjunctive instances etc.\n(b) Synthesis for Control Improvisation. The control improvisation (CI) framework helps synthesize randomized systems with strict and flexible constraints. Gittis, Vin, and Fremont (2022) includes quantitative constraints on expected costs and randomness constraints for diversity based on labels and encodes the problem as a model counting problem.\n6. Feature Counting. Product lines efficiently manage groups of products sharing a core set of features. Determining the number of valid configurations is often a crucial task.\n(a) Industrial Product Lines. Product lines are commonly employed to handle families of products sharing a core set of features, with feature models serving as a standard to define valid feature combinations. However, not all feature configurations are permissible. These models facilitate standardized analyses of the system's variability, and many of these analyses require calculating the number of valid configurations. Sundermann et al. (2023) surveyed these problems as a model counting problem.\n(b) Configuration Spaces of Software Systems. Kuiter et al. (2022) studied the problem of feature modeling, which helps systematically model features and dependencies in software systems. The authors encode feature models into propositional formulas, where the number of solutions of the formula corresponds to the number of possible features in a software system.\n7. Quantitative Verification of Neural Networks. An intriguing aspect of neural network verification is assessing the frequency with which a specific property is valid. The NPAQ (Neural Property Approximate Quantifier) framework, introduced by Baluta et al. (2019), facilitates the evaluation of various properties in binarized neural networks. The benchmarks test the following properties on the MNIST and UCI datasets: fairness, which encodes bias towards marital status, race, or sex; robustness, which measures the impact of 2-3-bit adversarial input perturbations; and Trojan attacks, which account for the number of inputs with a trojan pattern. These benchmarks were initially encoded as pseudo-Boolean constraints and later converted to CNFs, thereby encoding many arithmetic circuits.\n8. Quantitative Information Flow. Information leaks in modern software systems are an important problem. Bang et al. (2016) introduced an analysis method that estimates both minimum and maximum leak amounts, even when some paths aren't fully explored. This method was added to KLEE to analyze information leaks in C programs.\nAmong these benchmark sets, functional synthesis, reliability estimation, control improvisation, and neural network verification benchmarks consist of projected counting instances, while the remaining are non-projected model counting problems."}, {"title": "Experimental Evaluation", "content": "To evaluate the performance and effectiveness of the various tools discussed in Section 3, we conducted the following experiments.\nExperimental Setup. The experiments were carried out on a supercomputing cluster equipped with AMD EPYC 7713 CPUs. Each experiment involved running a tool on a specific benchmark using a single core with a memory limit of 16 GB. For approximate counters, we set \u025b = 0.8 and 8 = 0.2. We adhered to the competition standard timeout of 3600 seconds. Initial experiments with a higher timeout showed a minimal increase in the number of instances solved. For the experiments, we used the versions of the counters submitted to the Model Counting Competition 2023.\nVirtual Best Solver. We included the results of a Virtual Best Solver (VBS) in our comparisons. A VBS is a hypothetical solver that performs well and is the best method for each benchmark. If solvers $1,... Sn solve a problem in time t1,... tn seconds, then VBS solves it in min(t1,..., tn) sec.\nCorrectness. The correctness of the model counters is well- established; in all model counting competitions, the counters consistently produce correct counts. Additionally, approximate model counters provide counts with deficient error. Therefore, we assume the counters are correct and do not focus on this aspect.\nSince not all model counters can handle projected model counting, we analyzed projected and non-projected instances separately.\nIn this work, we sought to answer the following research questions:\nRQ1. How do the overall performances of different model counters compare, and how do they vary across different sets of benchmarks?\nRQ2. How do benchmark parameters relate to the performance of various solvers?\nSummary of Results. The highest number of non- projected instances solved by a single solver was achieved by SharpSAT-TD, which successfully solved 811 out of 1080 instances. For projected instances, ApproxMC demonstrated the best performance, solving 1041 out of 1182 instances. Compilation-based and hashing-based model counters excelled in solving different sets of benchmarks, and their performances were often complementary. This complementary performance resulted in a much better performance of the VBS, which solved 2106 out of 2262 instances. Treewidth correlates with the performance of compilation-based counters, while independent support size correlates weakly with the hashing-based counter.\nRQ1: Evaluation on Different Benchmark Sets\nWe evaluate the performance of the solvers on different benchmark sets by analyzing the total number of instances solved, examining each benchmark set individually, and considering the performance of the virtual best solver.\nTOTAL BENCHMARKS SOLVED First, we analyze the total number of instances solved for both non-projected and projected instances.\nNon-projected Instances. Table 3 presents the number of problems solved by various model counters across different benchmark sets. When aggregating all benchmarks, SharpSAT-TD exhibits the best performance, solving 75% of the benchmarks (811 out of 1080). D4 and ExactMC also perform well, solving 762 and 745 instances, respectively. In contrast, ApproxMC solves 715 instances, performing relatively less effectively.\nProjected Instances. Table 2 presents the number of instances solved by various model counters on projected model counting problems. Among the 1182 instances, ApproxMC performs the best, solving 1041 instances. The other projected model counters do not perform as well, with GPMC and D4 solving 434 and 361 instances, respectively, placing them second and third.\nRUNTIME VARIATION Table 2 and Table 3 present the number of problems solved by different model counters across various benchmark sets, highlighting significant performance variations among them. The key observations are as follows:\n1. Hashing-based counters perform exceptionally well on specific benchmark sets, particularly in cryptographic benchmarks, functional synthesis benchmarks, and neural network verification benchmarks.\n2. Compilation-based counters excel in certain benchmark sets, such as Linux configuration, industrial configuration feature counting benchmarks, and quantitative information flow benchmarks. The performance differences among the search-based counters are minimal.\n3. For the remaining benchmark sets, nearly all counters perform very well.\nFigure 1 provide a heatmap representation of the percentage of problems each model counter has successfully solved. Each cell in the heatmap indicates the percentage of benchmarks solved by a solver in a specific class of benchmarks. The color scale is shown to the right, with darker colors corresponding to a higher percentage of instances solved.\nThe cactus plots in Figure 2 and Figure 3 provide additional insight into performance variations. In these plots, the x-axis represents the number of instances, while the y-axis indicates the time taken. A point (i, j) on the plot signifies that a solver completed j benchmarks out of the total set in less than or equal to i seconds. The key insights from the figures are as follows:\n1. In Figure 2 (a), cryptographic instances are either solved within seconds or not at all. The VBS closely follows ApproxMC, indicating that ApproxMC typically has minimal runtime in most cases.\n2. A different pattern emerges for neural network verification in Figure 2 (b). Here, the counters take a considerable amount of time to solve the instances. The VBS closely follows the curve of GPMC for approximately 700 seconds, suggesting that GPMC can solve around 200 instances more quickly within this timeout. Between 1000 and 3600 seconds, ApproxMC gradually solves around 150 additional instances, a trend not observed in any other benchmark set.\n3. In Figure 2 (d), the information flow benchmarks exhibit another interesting behavior, with counters taking varying times between 0 and 2000 seconds to solve instances, managing to solve a maximum of 90 out of 106 instances. However, the VBS solves all the instances within 200 seconds.\nThe other cactus plots also show similar patterns. We have not included the cactus plots for robust reachability, control improvisation, and Bayes net benchmark sets because, in these sets, all the counters solve the instances within seconds. Similarly, we skipped the industrial and Linux configuration benchmarks since, in these cases, the compilation-based counters solve all the instances within seconds. In contrast, the hashing-based counter solves a negligible number of instances.\nVIRTUAL BEST SOLVER The VBS can solve significantly more instances than any individual solver. For the non-projected instances, out of 1080 instances, the VBS can solve 1048 instances. This number is much higher than any individual counter; the best counter for non-projected benchmarks is SharpSAT-TD, which solves 811 instances,\nrepresenting 77% of the instances solved by the VBS. In the case of projected instances, the trend continues, with the VBS demonstrating superior performance.\nContribution of Counters to the VBS. In Table 4, we show the number of instances contributed to the VBS by each model counter. ApproxMC makes the most substantial contribution to the VBS, accounting for 1114 out of 2106 instances. The following most significant contributors are ExactMC and GPMC, contributing 372 and 368 instances, respectively. Interestingly, ExactMC only solves non-projected instances. SharpSAT-TD does not contribute any instances to the VBS, likely because it requires a constant 600 seconds to run the tree-decomposition component before starting the actual counting, resulting in longer execution times compared to other counters.\nRQ2. Correlation with benchmark parameters\nWhile the primary observation with model counters on different types of benchmarks was that the performance varies significantly across benchmarks, we sought to identify the underlying parameters from a formula that influences the difficulty of model counting. We consider treewidth and size of independent support set of a formula as parameters to predict which count would be efficient to count the formula. Computing the values of each parameter is a computationally hard problem; therefore, we heuristically determine these values. We use FlowCutter (Strasser 2017) for computing the treewidth and Arjun for calculating the independent support size. In Table 1, we list the average treewidth and independent support size for each benchmark set. For neural net verification instances, FlowCutter timed out, which we denote by N.A. in the table.\nIn Table 5, we present the Pearson correlation between the different benchmark parameters and the solvers' runtimes. The value ranges between -1 and 1, where a greater absolute value indicates a higher correlation. The key observations are as follows:\n1. The performance of knowledge-compilation-based model counters has a positive correlation with treewidth, while hashing-based counters do not exhibit such a correlation. Among all counters, SharpSAT-TD shows the highest correlation of 0.48 between treewidth and runtime.\n2. The runtime of the hashing-based counter ApproxMC shows a weak positive correlation of 0.27 with independent support size, whereas the compilation-based counters show no correlation.\nIn Figure 4 and 5, we represent the relationship among treewidth, independent support size, and the runtime of a specific solver in a heatmap. The x-axis represents the treewidth, and the y-axis represents the independent support size. Thus, a point with coordinates (i, j) represents an instance with treewidth i and independent support size j. The color of the point indicates the runtime for the solver depicted in the graph. The color scale is shown to the right, with darker colors generally indicating shorter runtimes. These heatmaps shed more light on the lack of correlation between the performance of the counters and the benchmark parameters. The heatmaps reveal the following insights:\n1. In Figure 4 (a), ApproxMC takes approximately 103 seconds to solve instances of treewidth 10, while most of the instances with treewidth between 50 and 60 are solved in less than 100 seconds. The relationship between independent support size and solving time is also not very clear. There are many instances with independent support sizes above 1000 that are solved in less than 10 seconds.\n2. The performance of ExactMC in Figure 4 (b) is very different. For instances with treewidth higher than 20, it gradually takes an increasing amount of time. Instances with higher treewidth and higher independent support sizes seem to be particularly challenging for ExactMC. If the treewidth is greater than 50 and the independent support size is greater than 100, the time taken to solve is generally more than 200 seconds.\n3. The results for D4 in Figure 5 (a) and GPMCin Figure 5 (c) are not very different from those of ExactMC.\n4. The behavior of SharpSAT-TD, however, appears a little different in Figure 5 (b). It solves instances with higher treewidth relatively faster, while instances with treewidth greater than 30 and independent support size greater than 100 take more time to solve."}, {"title": "Conclusion", "content": "We conducted a comprehensive study on a diverse set of benchmarks, revealing that different solvers excel on different subsets. Our findings indicate that the virtual best solver can solve nearly all instances, a feat unattainable by any individual solver alone. This performance of VBS demonstrates that the complementary strategies employed by various solvers enable them to address distinct sets of instances effectively.\nConsequently, our study underscores the significance of integrating these approaches or developing a portfolio-based solver model."}]}