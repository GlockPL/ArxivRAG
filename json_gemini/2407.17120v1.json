{"title": "Parameter-Efficient Fine-Tuning for Continual Learning: A Neural Tangent Kernel Perspective", "authors": ["Jingren Liu", "Zhong Ji", "YunLong Yu", "Jiale Cao", "Yanwei Pang", "Jungong Han", "Xuelong Li"], "abstract": "Parameter-efficient fine-tuning for continual learning (PEFT-CL) has shown promise in adapting pre-trained models to sequential tasks while mitigating catastrophic forgetting problem. However, understanding the mechanisms that dictate continual performance in this paradigm remains elusive. To tackle this complexity, we undertake a rigorous analysis of PEFT-CL dynamics to derive relevant metrics for continual scenarios using Neural Tangent Kernel (NTK) theory. With the aid of NTK as a mathematical analysis tool, we recast the challenge of test-time forgetting into the quantifiable generalization gaps during training, identifying three key factors that influence these gaps and the performance of PEFT-CL: training sample size, task-level feature orthogonality, and regularization. To address these challenges, we introduce NTK-CL, a novel framework that eliminates task-specific parameter storage while adaptively generating task-relevant features. Aligning with theoretical guidance, NTK-CL triples the feature representation of each sample, theoretically and empirically reducing the magnitude of both task-interplay and task-specific generalization gaps. Grounded in NTK analysis, our approach imposes an adaptive exponential moving average mechanism and constraints on task-level feature orthogonality, maintaining intra-task NTK forms while attenuating inter-task NTK forms. Ultimately, by fine-tuning optimizable parameters with appropriate regularization, NTK-CL achieves state-of-the-art performance on established PEFT-CL benchmarks. This work provides a theoretical foundation for understanding and improving PEFT-CL models, offering insights into the interplay between feature representation, task orthogonality, and generalization, contributing to the development of more efficient continual learning systems.", "sections": [{"title": "1 INTRODUCTION", "content": "I\nN practical applications, the relentless evolution of envi-\nro ronments underscores the urgency for learning systems\nthat can progressively accumulate knowledge. This has led\nto the prominence of Continual Learning (CL) [13], [43],\n[53], [54], [60], [78], [87], a cornerstone task that equips the\nlearning models with the ability to seamlessly assimilate\nfresh information over time, while mitigating catastrophic for-\ngetting, i.e., a phenomenon that erodes previously acquired\nknowledge. In recent years, with the proliferation of pre-\ntrained models possessing strong generalization capabilities\n[6], [63], researchers have discovered that they can empower\nearly exploratory methods [4], [7], [18], [22], [33], [44],\n[45], [46], [52], [68], [69], [80], [88], [89], [90], [97], [99],\nenabling CL systems to integrate new knowledge more\nefficiently. However, full fine-tuning of pre-trained models\nis computationally intensive and may compromise their\noriginal generalization capabilities [27], [50], [94]. Thus, as\na promising paradigm, Parameter-Efficient Fine-Tuning for\nContinual Learning (PEFT-CL) emerges as an alternative,\nupdating only a minimal set of additional parameters while\nkeeping the pre-trained model intact. Specifically, PEFT-CL\nnot only offers a more philosophically sound framework\nakin to Socratic dialogue [95] but also provides a lightweight\ntraining process that avoids generalization deterioration as-\nsociated with full-scale fine-tuning [35], [76]. In addition, this\nseamless integration of new and old knowledge aligns with\nthe wisdom expressed by Bernard of Chartres, demonstrating\nhow PEFT-CL builds upon pre-existing knowledge to achieve\na more adaptive learner with robust memory capabilities.\nDespite initial successes in mitigating catastrophic forget-\nting [19], [71], [81], [82], [100], PEFT-CL largely relies on sub-\njective human insights and experiential doctrines for network\ndesign and enhancement, lacking a rigorous mathematical\nfoundation. This reliance on non-theoretical approaches\nconstrains the potential for a deeper understanding and\nadvancement of the fundamental mechanisms within these\nlearning systems. While Hide-Prompt [77] acknowledges\nthe importance of addressing this issue and offers a loss-\nbased perspective, it falls short of modeling optimization\ndynamics and pinpointing key factors. Therefore, to address\nthis gap, we adopt the Neural Tangent Kernel (NTK) theory\n[5], [8], [32] as a robust mathematical tool to delve deeply\ninto the intricacies of PEFT-CL optimization. Through this\nrigorous analysis, we derive several fundamental theorems\nand lemmas, including theorem 1, theorem 2, lemma 3, and\ntheorem 4. While initially considered from a CL perspec-\ntive, these have been generalized to the PEFT-CL scenario,\nproviding profound insights into the key factors essential"}, {"title": "2 RELATED WORKS", "content": "Parameter-Efficient Fine-Tuning has emerged as a pivotal\nparadigm for optimizing model performance while mitigat-\ning computational and memory burdens associated with\nlarge-scale model adaptation. Seminal works introduce di-\nverse methodologies, including Adapter modules [26], Low-\nRank Adaptation (LoRA) [27], Prefix Tuning [47], Prompt\nTuning [6], and BitFit [96]. These approaches demonstrate the\nefficacy of selectively fine-tuning components or introducing\ncompact, trainable sub-networks within pre-trained archi-\ntectures. Subsequent advancements further expand PEFT's\nscope and capabilities. Jia et al. [34] pioneer efficient prompt\ntuning techniques for vision transformers, extending PEFT'S\napplicability to the visual domain. Zhou et al. [102] introduce\ncontextual prompt fine-tuning, enhancing model adaptabil-\nity while preserving generalization. Recent comprehensive\nstudies [15], [20], [85], [86] reinforce PEFT's critical role in\nenhancing model generalization and efficiency. These investi-\ngations rigorously analyze the theoretical underpinnings and\nempirical efficacy of various PEFT methodologies, solidifying\nits status as a transformative paradigm in adaptive learning.\nContinual Learning is a critical field in artificial intelli-\ngence aimed at developing models that can learn new tasks\nwhile preserving knowledge from previous tasks. This field\nencompasses both task-specific strategies and generalization-\nbased approaches. Task-specific strategies utilize four pri-"}, {"title": "3 PRELIMINARIES", "content": "In the PEFT-CL context, we augment pre-trained models\nwith adaptive sub-networks to manage sequential tasks. Let\nfo and fr denote the initial and target parameter spaces\nrespectively, with * indicating optimized parameters. Given a\nseries of tasks \\(D = {D_1, ..., D_T}\\), where each \\(D_i\\) comprises\nsamples (x, y) from (X, Y), we introduce task-specific\noptimizable sub-network parameters \\(p_\\tau\\). The transformed\nmodel is represented as \\(f^* = (f_0 \\circ p_1 \\circ X_\\tau \\circ Y_\\tau)\\), with \\(\\circ\\)\ndenoting component integration. This configuration, inspired"}, {"title": "4 THEORETICAL INSIGHTS", "content": "The prevalent belief in PEFT-CL methods is that mitigating\ncatastrophic forgetting should be evaluated based on accu-\nracy, specifically by calculating the difference between the\noptimal accuracy on a previous task during its optimization\nand the accuracy on that task at the final stage. However,\nusing abstract accuracy metrics is not conducive to precise\nmathematical quantification, and the accuracy gap during\ntesting cannot effectively intervene in training. To better\nalign with the role of NTK in studying model generalization,\nwe propose shifting the focus from the accuracy gap to the"}, {"title": "5 NTK-CL", "content": "Drawing from the insights in lemma 3, we recognize the\nsubstantial impact of increasing task-specific sample size on\nreducing generalization gaps. Building on this perspective,\nwe propose an advanced PEFT strategy operative at three\nsubnetworks that generate features in distinct spaces. This\nstrategy synergistically leverages features derived from dual"}, {"title": "5.1 Extend Sample Size Through PEFT", "content": "5.2 Task-Level Feature Constraints\nInformed by insights from theorem 1, our approach un-\nderscores that effectively reducing generalization gap in-\nvolves the diligent preservation of historical knowledge\n\\(\\Phi_\\tau(X_\\tau, X_\\tau)\\) and \\(\\Phi_k(X_k, X_k)\\) from the perspective of the\ntask \\(\\tau\\), coupled with a concerted effort to diminish cross-task\ninteractions \\(\\Phi_k(X_\\tau, X_k)\\), for \\(k > \\tau\\). Given \\(\\Phi_k(X_\\tau, X_k) =\\)\n\\(f_\\tau(X_\\tau)^T f_k(X_k)\\), if the difference between \\(f_\\tau(X_\\tau)\\) and\n\\(f_k(X_k)\\) is maximized, then \\(\\Phi_k(X_\\tau, X_k)\\) will be minimized.\nSince \\(p_k\\) in the optimization process of PEFT-CL will only\nbe influenced by \\(f_k(X_k)\\), ensuring orthogonality between\n\\(f_\\tau(X_\\tau)\\) and \\(f_k(X_k)\\) will make \\(f_\\tau(X_\\tau)\\) extremely small [16].\nHowever, in the practical setting of PEFT-CL, cross-task\naccess to data is strictly prohibited, presenting a substantial\nchallenge in maintaining task-level distinctiveness.\nTherefore, we propose a compromise approach. Within\nthe context of NTK theory, the optimization of infinitely\nwide neural networks mirrors a Gaussian process [10], [40],\nyielding a locally constant NTK matrix [12], [32], [42], [91].\nGiven this, it is reasonable to assume that \\(\\Phi_\\tau(X_\\tau, X_k) =\\)\n\\(\\Phi_0(X_\\tau, X_k) = \\Phi_1(X_\\tau, X_k) = ... = \\Phi_\\infty (X_\\tau, X_k)\\). Moreover,\nnetworks pre-trained on extensive datasets emulate the\nproperties of infinitely wide networks [41], [75], [83], aligning\nwith our pre-trained model. Therefore, we relax the original\nconstraint, assuming that the pre-trained model is at this\nlocal optimum.\nUnder this framework, \\(\\Phi_k (X_\\tau, X_k) \\approx \\Phi_*(X_\\tau, X_k) =\\)\n\\(f_\\tau^*(X_\\tau)^T f_k^*(X_k)\\), suggesting that ensuring orthogonality\nbetween \\(f_\\tau^*(X_\\tau)\\) and \\(f_k^*(X_k)\\) is feasible to some extent. To\npractically achieve this, integrating a prototype classifier and\nimposing orthogonality constraints ensure that embeddings\nfrom different tasks remain distinct, thus not violating the\nconstraints under the PEFT-CL scenarios and aligning with\nthe objective to minimize generalization gap.\nKnowledge Retention: Effective replay of past knowl-\nedge is essential in PEFT-CL utilizing pre-trained models.\nFor instance, [77] employs a distributional strategy that\napproximates and replays past data distributions for ongoing\noptimization, while [100] preserves distinct adapter parame-\nters for each historical task. These strategies, while boosting\nknowledge retention, simultaneously escalate optimization\nand storage requirements, posing challenges to scalability\nand operational efficiency in scenarios with large datasets\nor extensive task sequences. To mitigate these issues, we\nintroduce an adaptive Exponential Moving Average (EMA)\nmechanism tailored for efficient past knowledge preservation,\nillustrated in Fig. 4.\nTraditional EMA applications often maintain a static\nbase model, incrementally integrating optimized weights\nto preserve historical data. However, this approach proves\nsuboptimal in PEFT-CL settings due to the substantial\ndisparities in weights across tasks. Directly preserving a large\nproportion of past weights can detrimentally affect the per-\nformance on current tasks, while retaining an entire model's\nweights is excessively redundant. Therefore, we propose two\nimprovements. First, we categorize the adaptation parame-\nters responsible for generating embedding into two segments:\n\\(p_\\tau^{\\text{pre}}\\) for historical knowledge and \\(p_\\tau^{\\text{curr}}\\) for current insights.\nSecondly, we apply the EMA mechanism exclusively to the\nadaptation modules' parameters, leaving other optimizable"}, {"title": "6 EXPERIMENTS", "content": "Datasets: In our empirical analysis, we utilize a comprehen-\nsive collection of datasets, meticulously curated to rigorously\nevaluate the model's adaptability within the PEFT-CL sce-\nnarios. These datasets span a broad spectrum of domains,\nincluding general object recognition, domain adaptation,\nanimal recognition, and geospatial analysis, providing a\nrobust framework for assessment. Below, we elucidate the\nsignificance, structure, and relevance of each dataset to our\ninvestigation:\nCIFAR-100 [38]: This dataset, crucial for assessing contin-\nual learning algorithms, comprises 60,000 32x32 color images\nacross 100 classes, with each class contributing 600 images.\nFor PEFT-CL, it is segmented into 10 distinct tasks, each\nencompassing 10 classes. Images are resized to 224x224 to be\ncompatible with the pre-trained ViT model.\nImageNet-R [81]: It enriches the original ImageNet\ndataset by incorporating artworks, cartoons, and interpreta-\ntions for 200 classes. This dataset underscores the challenge\nof generalizing across highly varied visual domains. For\nour evaluation, the ImageNet-R is organized into 10 tasks,\neach containing 20 classes, with a distribution of 24,000\ntraining images and 6,000 test images, offering a rigorous\ntestbed for assessing model generalization in complex PEFT-\nCL scenarios.\nImageNet-A [25]: This challenging dataset is an extension\nof ImageNet, curated to evaluate model robustness against\nadversarial and out-of-distribution samples. It contains 7,500"}, {"title": "5.2 Task-Level Feature Constraints", "content": "6.1 Benchmark Comparison\nIn this subsection, we evaluate the NTK-CL method against\nother leading approaches. To ensure a fair performance\ncomparison, we fix random seeds from 0 to 4, ensuring con-\nsistent task segmentation for each run 5. We utilize uniformly\nsourced pre-trained weights and maintain the optimal hyper-\nparameters from the open-source code without modifications.\nPerformance metrics for major datasets using ImageNet-21K"}, {"title": "6.2 Ablation Study", "content": "To ensure rigorous alignment between theoretical frame-\nworks and empirical validation, a comprehensive suite\nof ablation studies is executed using the CIFAR100 and\nImageNet-R datasets. Experimental conditions are standard-\nized by setting the random seed to 0, ensuring consistent task\nsegmentations, and utilizing weights from ViT-B/16-IN21K to\nmaintain model consistency. The ablation studies encompass\nvarious configurations, including the Subnetwork-1 Adap-\ntation Module (S1), Subnetwork-2 Adaptation Module (S2),\nHybrid Adaptation Module (Hybrid), Knowledge Retention\n(KR), Task-Feature Dissimilarity Loss (Dis), Orthogonality\nLoss (Orth), and Regularization Loss (Reg). Average task-\nrelated accuracies (\\(\\bar{A}\\)) are assessed to critically analyze their\nindividual contributions to the model's overall performance,\nwhich is displayed in Table 6."}, {"title": "6.3 Hyper-parameters Adjustment", "content": "Next, we will experiment with variations of the hyper-\nparameters \\(\\eta\\), \\(\\upsilon\\), and \\(\\lambda\\) to explore the impact of \\(L_\\text{dis}\\), \\(L_\\text{orth}\\),\nand \\(L_\\text{reg}\\) on PEFT-CL performance to varying degrees. All\nexperiments use a fixed random seed of 0 to ensure fair\ncomparison. The optimal hyper-parameter settings differ\nbetween datasets. For ImageNet-R, the optimal values are\nset at \\(\\eta = 0.2\\), \\(\\upsilon = 0.0001\\), and \\(\\lambda = 0.001\\). Conversely, for\nCIFAR100, the optimal settings are \\(\\eta = 0.03\\), \\(\\upsilon = 0.0001\\), and\n\\(\\lambda = 0.001\\). During these experiments, while one parameter\nis varied, the other hyper-parameters are held constant to\nisolate the effects of the variable under study. The detailed\ntuning experiments for these hyper-parameters on ImageNet-\nR and CIFAR100 are displayed in Fig. 5."}, {"title": "6.4 Visualisation", "content": "To visually demonstrate the information captured by the\npre-trained ViT and processed through the Subnetwork-1\nand Subnetwork-2 Adaptation Modules, we select a random\nimage from Task 0. We extract three-dimensional embeddings\nusing parameters from these modules at various training"}, {"title": "6.5 Other Pre-trained Weights", "content": "To more comprehensively explore the impact of \\(f_0\\) in Eq. 3 on\nthe final performance of PEFT-CL, extensive experiments us-\ning other pre-trained weights for ViT-B/16 are conducted. To\nensure absolute fairness, the hyper-parameters and training\nstrategies involved during their training are kept completely\nconsistent, with only the backbone parameters differing. The"}, {"title": "7 CONCLUSION", "content": "In this study, we adopt an NTK perspective to analyze PEFT-\nCL tasks, elucidating model behavior and generalization\ngaps in sequential task learning. Our analysis identifies\ncrucial factors affecting PEFT-CL effectiveness, particularly\nthrough the dynamics of task interplay and task-specific gen-\neralization gaps. We recommend strategies to mitigate these\ngaps, such as expanding sample sizes, enforcing task-level\nfeature constraints, and refining regularization techniques.\nThese strategies inform architectural and optimization ad-"}, {"title": "APPENDIX A NTK DYNAMICS IN PEFT-CL", "content": "Initially, we concentrate on analyzing the least squares loss\nassociated with the optimization of consecutive tasks \\(\\tau\\)\nand \\(\\tau - 1\\). This involves quantifying the classification loss\nattributable to variations in the sub-network components'\nparameters, which is expressed as follows:\n\nL(\\tau |X,Y \\in D_\\tau) = \\text{argmin} ||f_{\\tau - 1}(X) + \\nabla_{p_\\tau} f_\\tau(X)\n\\quad \\quad \\quad \\quad p_\\tau\n\\\\times (p_\\tau - p_{\\tau - 1}) - Y ||^2\n\n= \\text{argmin} ||f_{\\tau - 1}(X) + \\nabla_{p_\\tau} f_\\tau(X)\n\\quad \\quad \\quad p_\\tau\n\\\\times (p_\\tau - p_{\\tau - 1}) - Y||_1^2  \\\\tag{31}\n\nHere, \\(D_\\tau\\) refers to the data subset associated with the \\(\\tau\\)-th\ntask, where X and Y are the input images and corresponding\nlabels, respectively. The term \\(\\nabla_{p_\\tau}(.)\\) denotes the Jacobian\nmatrix relevant to task \\(\\tau\\) for the inputs X. At the onset of a\ntask's optimization, the sub-network component parameters\ninherit parameters from the preceding task, setting the initial\nstates for optimizing \\(f_\\tau(.)\\) and \\(p_\\tau\\) as \\(f_{\\tau-1}^*(.)\\) and \\(p_{\\tau-1}^*\\),\nrespectively."}, {"title": "APPENDIX B TASK-INTERPLAY GENERALIZATION IN PEFT-CL", "content": "In this section, we explore the dynamics of task-interplay\ngeneralization gap within the PEFT-CL scenario, utilizing the\nNTK theory. We begin by outlining relevant mathematical\nproperties of the NTK, followed by detailed analyses and\nderivations to elucidate how these properties influence\ngeneralization across tasks. This rigorous approach aims\nto provide a robust theoretical foundation for understanding\nthe interplay between task transitions in PEFT-CL scenarios."}, {"title": "APPENDIX C TASK-INTRINSIC GENERALIZATION IN PEFT-CL", "content": "Utilizing Eq. 40 and momentarily setting aside the ini-\ntialization term \\(f_0(x)\\), we identify the NTK-related term\nfor the entire task dataset as \\(\\alpha_i\\). Incorporating its eigen-\ndecomposition, we derive:"}]}