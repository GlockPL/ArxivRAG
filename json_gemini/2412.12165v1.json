{"title": "Multimodal Approaches to Fair Image Classification: An Ethical Perspective", "authors": ["Javon Hickmon"], "abstract": "In the rapidly advancing field of artificial intelligence, machine perception is becoming paramount to achiev-ing increased performance. Image classification systems are becoming increasingly integral to various ap-plications, ranging from medical diagnostics to image generation; however, these systems often exhibitharmful biases that can lead to unfair and discriminatory outcomes. Machine Learning (ML) systems thatdepend on a single data modality\u2014i.e., only images or only text can exaggerate hidden biases present inthe training data, if the data is not carefully balanced and filtered. Even so, these models can still harmunderrepresented populations when used in improper contexts, such as when government agencies reinforce racial bias using predictive policing. This thesis explores the intersection of technology and ethics in the development of fair image classification models. Specifically I focus on improving fairness and methods of using multiple modalities to combat harmful demographic bias. Integrating multimodal approaches, which combine visual data with additional modalities such as text and metadata, allows this work to enhance thefairness and accuracy of image classification systems. The study critically examines existing biases in image datasets and classification algorithms, proposes innovative methods for mitigating these biases, and evalu-ates the ethical implications of deploying such systems in real-world scenarios. Through comprehensive experimentation and analysis, the thesis demonstrates how multimodal techniques can contribute to more equitable and ethical AI solutions, ultimately advocating for responsible AI practices that prioritize fairness.", "sections": [{"title": "Introduction", "content": "The rapid development and deployment of artificial intelligence (AI) systems have revolutionized numer-ous sectors, leading to significant advancements in automation, efficiency, and decision-making processes. Among these AI systems, image classification has emerged as a crucial technology with applications span-ning from medical diagnostics, to image generation. Despite its impact, the use of image classification algorithms has also raised serious ethical concerns, particularly regarding fairness and bias. These concerns have been underscored by numerous real-world examples where image classification systems have exhibited discriminatory behavior, leading to adverse outcomes for certain demographic groups.\nOne of the most prominent examples of the misuse of image classification technology is in the realm of law enforcement and security. Facial recognition systems, which rely heavily on image classification algorithms, have been widely adopted by law enforcement agencies around the world. However, studies have shown that these systems often perform poorly on individuals with darker skin tones, women, and younger people. A study conducted by the National Institute of Standards and Technology (NIST) found that the error rates for facial recognition systems were significantly higher for people of African and Asian descent compared to those of European descent [National Institute of Standards and Technology, 2020]. This disparity has led"}, {"title": "Background", "content": "to wrongful arrests and detentions, as evidenced by the case of Robert Williams, an African American man who was wrongfully arrested in Detroit due to a false match generated by a facial recognition system.\nIn the healthcare sector, image classification algorithms are increasingly used to aid in medical diagnos-tics, such as detecting tumors in radiological images or identifying skin diseases. However, these systems can also exhibit biases that affect their diagnostic accuracy. For example, multiple studies have found that dermatology AI systems repeatedly performed worse on images of skin conditions from darker-skinned patients compared to those from lighter-skinned patients [Marsden et al., 2023; Jones et al., 2022]. This discrepancy can result in misdiagnoses or delayed treatment for patients with darker skin, exacerbating existing health disparities. This is reinforced by another study published by the The Lancet Digital Health, that systematically reviewed 21 open access datasets and discovered darker skin types were significantly underrepresented [Wen et al., 2022]. This form of under-representation within datasets, is a common theme that extends to the majority of image classification systems, which we will explore further in later chapters.\nAnother concerning example is the use of image classification in social media and online platforms. These platforms employ image recognition algorithms to moderate content, tag images, and personalize user ex-periences. However, biases in these algorithms can lead to the exclusion or misrepresentation of certain groups. In one notable incident, an image classification system used by Google Photos misidentified photos of Black individuals as \"gorillas,\" highlighting the severe consequences of biased AI systems and the need for more inclusive training data and algorithms."}, {"title": "Motivation", "content": "These real-world examples are not even a fraction of the many incidents regarding image classification, and they illustrate the critical need for developing fair techniques and systems. The ethical implications of deploying biased AI systems are profound, affecting individual lives, societal trust in technology, and the broader quest for social justice. Addressing these issues requires a multifaceted approach that combines"}, {"title": "Challenges", "content": "technological innovation with ethical considerations.\nThis thesis is motivated by the urgent need to enhance the fairness and reduce the harmful bias of image classification systems. The primary goal of this research is to explore how multimodal approaches can be leveraged to mitigate biases in image classification. Multimodal approaches involve the integration of multi-ple types of data\u2014such as visual data, text, and more to provide a more comprehensive and context-aware understanding of the input images. Within this work I focus on purely visual and textual data; however, fu-ture work will likely explore additional modalities. By incorporating diverse data sources, these approaches can potentially address the limitations of traditional image-only classification systems and improve their performance across different demographic groups.\nThe motivation for this work is rooted in the recognition that fair image classification is not merely a tech-nical challenge but a societal imperative. As AI continues to permeate various aspects of our lives, it is crucial to ensure that these systems are designed and implemented in ways that promote equity and justice. By exploring multimodal approaches to image classification and their ethical implications, this thesis aspires to pave the way for more equitable solutions that benefit all members of society.\nWith the primary focus being on fair and ethical image classification, there are a number of challenges and we acknowledge that in practice we likely will not be able to create a truly fair system. This is why we intentionally utilize language such as reduce harmful bias, because our goal is to mitigate the negative impact of these systems while still maintaining and improving their efficacy. We will examine the potential benefits and risks associated with multimodal classification techniques, considering factors such as privacy, transparency, and accountability. By providing a balanced perspective on the ethical dimensions of AI, this work contributes to the development of responsible AI practices that prioritize fairness."}, {"title": "Approach", "content": "Our approach to mitigate these harms is two-fold. First, we develop a technique called MuSE (Multi-modal Synthetic Embeddings) for image classification, which we use to augment the embedding space of pre-trained multimodal models at inference time. MuSE focuses on improving the generalizability of mul-timodal models being used for classification. This approach means that it can improve performance on a range of standard classification benchmarks with little to no overhead. We leverage the significant benefits of MuSE within our second step.\nDiverse Demographic Data Generation (D3G) is a technique developed to reduce demographic bias within multimodal models at inference time. Since this technique leverages the benefits of MuSE, it is completely training-free and can be quickly utilized with any multimodal model being used for classification. This is important because this technique works to boost accuracy and reduce demographic bias at the same time."}, {"title": "Outline", "content": "The structure of this thesis is as follows:\n\u2022 We conduct a comprehensive review of existing literature on image classification, multimodal model ensembling, and fairness in Deep Learning. This review will highlight the key challenges and gaps in current research and practice.\n\u2022 We propose MuSE, a technique designed to improve the accuracy and generalizability of multimodal models at inference time. To understand the impact of MuSE, we quantify its performance on a range of benchmarks and conduct an analysis of the improvements.\n\u2022 We propose D3G, a similar technique to MuSE that focuses on reducing demographic bias within multimodal models. This technique will be rigorously tested through experiments designed to expose demographic bias within the model. The results of these experiments will then be analyzed to assess the effectiveness of the proposed methods in reducing bias and improving classification accuracy.\n\u2022 Finally, we will discuss the socio-ethical implications of such techniques if deployed within real- world scenarios. This includes a discussion on where/when this technique should be utilized and,"}, {"title": "MuSE", "content": "Artificial intelligence has made significant progress in image classification, an essential task for machine perception to achieve human-level image understanding. Despite this importance, many issues can arise from traditional multimodal image classification methods; however, two are particularly challenging. First, fine-grained image classification is unachievable if the model is too small and underfits the data. Second, if the model wasn't trained on enough high-quality data or a modality lacks enough representation of a class, the model may contain inherent bias. We propose Multimodal Synthetic Embeddings as a framework to reduce the impact of these issues on pre-trained models. By leveraging open-vocabulary models, We aim to generate vision-language descriptions that emphasize the differences between similar classes, improving fine-grained classification accuracy without any additional training or fine-tuning."}, {"title": "Introduction", "content": "Multimodal systems have been a promising new paradigm in the field in the field of machine learning. Utilizing multiple modalities has resulted in machine learning models that are more accurate and general-izable on a broad range of tasks, including sentiment analysis Wang et al. [2020] and cross-modal retrieval Kim et al. [2021]. Despite this performance, Issues such as data redundancy Dittrich and Seeger [2000], noise Xiong et al. [2006], and class imbalance Longadge and Dongre [2013] are just a few of the many"}, {"title": "Related Work", "content": "issues that arise from collecting large amounts of data to train on. Despite this need for high-quality data, open-vocabulary models achieve high classification accuracy across many datasets without labeled training data. To accomplish this, these models leverage the massive amounts of image text pairs available online by learning to associate the images with their correct caption, leading to greater flexibility during inference Pratt et al. [2023]. These models understand the underlying semantics and can generalize to unseen data by learning from this information.\nWhat does a platypus look like? Generating customized prompts for zero-shot image classification [Pratt et al., 2023] introduces CuPL (Customized Prompts via Language models), the foundational work for this project. In this paper, generating descriptive prompts for each ImageNet class directly improved the resulting zero-shot accuracy. The work solidifies the idea that prompt generation from a larger model trained on more high-quality data can lead to knowledge transfer, helping to emphasize the differences between classes. Although the attention of the multimodal model is shifted when given generative CuPL descriptions, the model can still fail to distinguish between similar classes, as shown in the paper.\nMultimodal Prompting with Missing Modalities for Visual Recognition [Lee et al., 2023] attempts to reduce the effect of missing modalities on classification accuracy, an issue that can introduce bias into multimodal transformers. Despite this, a significant limitation was that they could not recover missing information from the multimodal inputs. We intend to solve this issue by synthesizing data to better represent"}, {"title": "Methods", "content": "the true distribution of the source, improving the accuracy of the model's prediction [Park et al., 2018].\nA model that applies this concept of model ensembling was introduced in Learning to Navigate for Fine-grained Classification by Yang et al. [2018]. This is a state-of-the-art paper that attempts to reduce misclas-sification rates by developing a model called NTS-Net (Navigator-Teacher-Scrutinizer Network) to teach itself methods of identifying and scrutinizing fine-grained image details. The Navigator navigates the model to focus on the most informative regions (denoted by yellow rectangles in Figure 2.3), while the Teacher evaluates the regions proposed by the Navigator and provides feedback. After that, the Scrutinizer scruti-nizes those regions to make predictions. NTS-Net achieves high classification accuracy on a pre-defined dataset; however, we wanted to explore if these foundational concepts could be applied to a training-free, zero-shot environment. In order to achieve this, we leverage pretrained open-vocabulary models with ad-vanced attention mechanisms to discriminate the fine-grained features of a given image, in order high per-formance on a variety of classes without additional training.\nIn this section, we explore and analyze our selection of datasets, models, the MuSE Architecture, and implications of the final results."}, {"title": "Datasets", "content": "Within this chapter, we specifically select four datasets that will test the generalizability and fine-grained classification capabilities of MuSE: Flowers 102 [Nilsback and Zisserman, 2008], Describable Textures Dataset (DTD) [Cimpoi et al., 2014], Remote Sensing Image Scene Classification 45 (RESISC45) [Cheng et al., 2017], and Fine-Grained Visual Classification (FGVC) Aircraft [Maji et al., 2013]. We selected these datasets because, alongside testing generalizability and fine-grained classification, they were extremely di-verse. Each dataset contains completely different information from the others, which will allow us to under-stand how robust this technique is to distribution shift.\nFlowers 102 contains images split into 102 categories, with large scale, pose, and light variations between each image. There are at least 40 images per category, which were collected by searching the internet and manually taking pictures. We focus on the category selection because it is particularly useful for our eval-uation. Within the dataset, some categories have large variations, but also others that are very similar to one another. This is useful because many factors contribute to distinguishing one flower from another. This"}, {"title": "Models", "content": "means we can evaluate the model on general and fine-grained classification capabilities.\nDTD contains 47 classes, 5,650 images total, and has similar benefits as Flowers 102; however, it addi-tionally allows us to evaluate the generalizability of our technique. Since many of the category titles are quite abstract, the model must be able to generalize the class name to any images conforming to the proper description. This is especially difficult since certain classes are similar to one another, so the model must have a deep cross-modal representation of these classes.\nRESISC45 contains 45 scene classes, with 700 images per class and 31,500 images total. The dataset contains images of satellite photos containing different scenes, and each category holds significant variation in translation, spatial resolution, viewpoint, object pose, illumination, background, and occlusion. Scene classification is a particularly difficult task because generating images for scenes is less precise than gener-ating specific subjects. To analyze the limitations and drawbacks of MuSE, we must explore if the technique can still improve performance on this dataset.\nFinally, FGVC Aircraft contains 100 aircraft models organized in a three-level hierarchy, with 10,000 total images. The hierarchy of aircraft within this dataset is key to our evaluation. The other datasets focus on a balance between fine and coarse-grained classification; however, most FGVC Aircraft are fine-grained. The hierarchical structure means we can better explore the per-class accuracies and evaluate which granularity the technique breaks down.\nWithin CuPL, GPT-3 DaVinci-002 was used to generate the prompts, CLIP ViT-L/14 is used as MuSE's multimodal model for image-to-text retrieval, and Stable Diffusion XL 1.0 is utilized for image generation."}, {"title": "The MuSE Architecture", "content": "The MuSE architecture is as follows:\nStep 1: Generate Prompts\nThe first stage is to leverage CuPL to create contextually relevant prompts for our multimodal model. CuPL is designed to generate prompts that effectively guide the model's attention and understanding, enabling better performance in zero-shot image classification. It does so by prompting a Large Language Model (LLM) to generate a description of a given class. These descriptions emphasize the visual information of each class so that when the multimodal model classifies the image, it can utilize the richer text to get a much more accurate embedding. This is imperative because using an LLM allows us to quickly generate many descriptive prompts customized to each category. In the MuSE architecture Figure 2.4, this corresponds to the step where each class name is passed into the CuPL/SDXL block. For MuSE, we simply generate one prompt per class and then use that prompt in the next step.\nStep 2: Generate Images"}, {"title": "Results", "content": "We now leverage the prompts generated by CuPL in the previous stage. We simply use each prompt to generate an image for each class. As previously stated, Stable Diffusion XL (SDXL) 1.0 is used in order to generate our images. For every image, we use 50 generation steps, a guidance scale of 15, and a seed of 0. We generate 5 images per class using the same prompt, then average the normalized embeddings of these images as shown in Figure 2.4.\nStep 3: Classify Query Image\nUsing both the generated prompts and generated images, we classify the query image. For each class, we take the embedding from the generated prompt and the embedding from the generated image, then create a weighted sum of their normalized embeddings. We determine the weight for this sum, by scanning values from 0 to 1 by increments of 0.01, where the text is multiplied by weight w and the images are multiplied by weight (1 \u2013 w). Upon creating a joint embedding, we then use cosine similarity to classify the query image."}, {"title": "Metrics", "content": "We will focus on accuracy for our evaluations; however, the specific type of accuracy used will vary based on the dataset's standards. The metrics are as follows:\nAs alluded to within the analysis of our datasets, the evaluation of our approach is multifaceted. It focuses on both traditional image classification benchmarks and fine-grained image classification. The multiple accuracy metrics will also allow us to understand if the method improves performance across all classes or boosts the top 1."}, {"title": "Evaluation Breakdown", "content": "The baseline will be the standard method of CLIP image classification as in Figure 2.5, where the image is encoded into embedding space and then matched with each of the class embeddings to reveal the highest score.\nWe use a variety of prompts and templates for this CLIP baseline. Note that within the text being used to get cosine similarities for the CLIP baseline, is the same text that is immediately after the CuPL block in Figure 2.4. This text can either be the prompt template \"A photo of a <classname>\" or a single CuPL prompt per class.\nIn addition to our standard CLIP baseline, we also provide the classification accuracy when only generated images are used instead of text. This is equivalent to w = 0 and thus (1 \u2013 w) = 1. We provide this metric to show why images alone are not sufficient for improved classification, and that multiple"}, {"title": "MuSE Evaluations", "content": "modalities are needed for effective classification.\nFor MuSE, we construct a variety of different evaluations. Our first is the standard method of classification shown in Figure 2.4. We use CuPL to generate a prompt for each class, then generate 5 images based on that prompt. We average the normalized embeddings of all five images, then create a weighted sum of the embeddings from the desired text and generated images. The text being used can vary depending on our evaluation; however, it is important to note that this method is characterized by its weighting strategy.\nWe define matrices \\(t = [t_1, t_2,...,t_v]\\) and \\(i = [i_1, i_2, ..., i_n]\\) as containing the normalized embed-dings for the generated text and images, respectively. Similarly, matrix q is defined as the vector containing the normalized query image embedding for a single query image. Both t and i are of dimensions NxM, where N is the total number of classes within the dataset, and M is the length of a single embedding vector. For the CLIP ViT-L/14 text and image encoders, M = 768. Similarly, q is of shape 1xM, because we embed one query image at a time. Possible weight values are scanned from 0 to 1 by steps of 0.01, where the text is weighted by weight w, and the images are weighted by (1 \u2013 w).\nUpon scanning all values, we take the maximum top-1 accuracy and use that weight configuration for our final result. The full formulation to get our final combined embedding xstd using our Standard MuSE method, and given a single query image q is defined as:\n\\(X_{Std} = \\underset{f(w)}{\\text{argmax}} s(f(w)) = {f(w)|(s(f(w)) < s(f(w_j)))\\forall{w_j|w_j \\in R, 0 \\leq w_j \\leq 1}}\\)\nwhere\n\\(f(w) = (w \\cdot t + (1 \u2212 w) \\cdot i)\\)"}, {"title": "Classification Results", "content": "and\n\\(s(f(w)) = f(w) \\cdot q\\)\nSince we normalize all our embeddings, we can simply use the dot product to retrieve our cosine similarity.\nOur second MuSE evaluation method utilizes confidence scores and previously described standard weight values. Where w \u2208 [0, 1], the confidence score ci represents how likely the image belongs to the class at index i, is the true class of the query image q. We define\n\\(c_i = 1-\\frac{e^{q \\cdot t_i}}{\\sum_{j=1}^{N} e^{q \\cdot t_j}}\\)\nNotice how we use the inverse confidence score because we calculate the score using text embeddings. We want our resulting confidence to represent how likely the image belongs to the class at index i, so we take the inverse text confidence so we don't rely too heavily on text or images. If we are not confi-dent our text is correct, we will utilize more of the image embeddings, and vice versa. We leverage ci to create our new formulation for our final combined embedding xConf utilizing this Confidence MuSE method:\n\\(X_{Conf} = \\underset{f(w)}{\\text{argmax}} s(f(w)) = {f(w)|(s(f(w)) < s(f(w_j)))\\forall{w_j|w_j \\in R, 0 \\leq w_j \\leq 1}}\\)\nwhere\n\\(f(w) = (w \\cdot t + (1 \u2212 w) \\cdot c_i \\cdot i)\\)\nand\n\\(s(f(w)) = f(w) \\cdot q\\)\nOur third MuSE evaluation tests the performance while setting a fixed weight value. This is the ex-act same formulation as MuSE Standard, but w 0.1 so we do not scan for the argmax. Similarly, our final MuSE evaluation sets w 0.1 for MuSE Confidence.\nWe start by evaluating the results with the text being \"A photo of a <classname>.\" We still use the CuPL text in order to generate our images; however, we replace the text embeddings with the normalized \"A photo of a <classname>\" embedding as shown in the bottom of Figure 2.4.\nFrom these results, we can see that MuSE already performs better than the baselines across all of our datasets. The improvements range from 1-2% increases, but this is still impressive considering the variety of the datasets used. We gain a deeper understanding as we examine other evaluations.\nFor this evaluation, we started by obtaining the CLIP evaluation templates, originally published in the paper"}, {"title": "Fine-Grained Classification", "content": "by Radford et al. [2021]. We provide the list of all CLIP templates used within Appendix A Table A.1. For every class in the dataset, we average the embeddings of each of the CLIP prompts that the dataset contains (e.g., 'a photo of a {}, a type of flower.'). We then use the averaged embeddings in the exact same way that we use the standard text embeddings, by creating a weighted sum and then classifying as shown in Figure 2.4. The results of this technique are shown above in Table 2.3. We can see that once again, MuSE improves performance across all datasets; however, another important feature of this evaluation, is the fact that two of the evaluations with MuSE Standard where w = 0.1, ended up being the optimal configuration. This suggests that we might be able to set a fixed value based on the specific evaluation, and still achieve optimal performance. We further explore this idea in the next evaluation.\nWithin this evaluation, we have the exact same setup as the \"A photo of a <classname>\" text prompt; however, we replace that text with a single CuPL prompt. Since CuPL prompts are much more visually descriptive, we expect the new text embedding to better represent the true class. Upon analyzing the results in Table 2.4, we can see that our hypothesis was correct. Within this evaluation, we observe accuracy gains up to 3%. This is extremely significant given the zero-shot, training-free, and minimal overhead nature of our approach. Even when viewing the sub-optimal MuSE results, we can see that they still are able to achieve significantly higher accuracies when compared to the baselines. Despite this, when viewing the next evaluation, we are also able to see when this method fails.\nIn this evaluation in Table 2.5, we utilize anywhere from 20-60 CuPL prompts per class, depending on the dataset. We can still see improvements; however, we also see the first negative results on RESISC45. Within this dataset, we are not able to get any performance increase, and there even is a decrease in accuracy when we set constant weight values. The reason why this occurs, is due to the quality of the CuPL prompts"}, {"title": "Class Imbalances", "content": "used. We can see a sample of the CuPL prompts used for this evaluation, within Table A.2. When viewing the prompts from RESISC45, we can see they are still of good quality; however, we notice issues when comparing the prompt to ground truth images within the dataset. The prompts typically describe airplanes mid-flight, here are a few of the prompts for example:\n\u2022\n\u2022 \"A satellite photo of an airplane might show the plane in flight, with its wings outstretched and its engines propelling it through the air.\"\n\u2022 \"The photo might show the airplane in mid-flight, with the sun's rays shining off its metal body.\"\n\"A satellite photo of an airplane would show a plane in flight, with its wings outstretched and its body elongated.\"\nThese descriptions, while not incorrect, are nothing like the ground truth within the dataset. We can clearly see this when viewing Figure 2.6\nThe images containing airplanes within the dataset, are all airplanes located on the ground. This is a key issue and one we discuss later in this chapter. The fact that the images generated can be completely accurate to the prompt, and yet the ground truth image could be different, is one of the major difficulties with this technique. To further understand how these issues occur, view the sample generated images compared to their texts within Appendix B.\nTo further explore MuSE's impact on fine-grained classification accuracies, we tested the binary classifica-tion accuracy on similar classes within ImageNet. We selected this dataset because we wanted to identify a large commonly used dataset, then isolate the mistakes CLIP was making on fine-grained classes. To identify these fine-grained classes, we selected the true classes that CLIP predicted incorrectly most often and took the corresponding CLIP predictions for these classes. This provided a subset that exposed CLIP's inaccuracies on ImageNet.\nWithin this dataset, mushroom and agaric were a commonly confused pair of classes, with the mis-classification rate for mushroom being extremely high. To understand why this was the case, we compared the CLIP and MuSE performance, as shown in Table 2.6.\nCLIP is overfitting, because it cannot distinguish between the two categories. This is why the model achieves perfect accuracy on agaric while rarely selects mushroom. In contrast, MuSE balances these numbers by dramatically raising the accuracy on the mushroom class while still achieving extremely high accuracy on the agaric class, showing it can distinguish between the classes and still often select the correct class. We demonstrate this within Figure 2.7. In this figure, CLIP initially misclassifies the query image with a ground-truth label of \"mushroom\" to be an \"agaric\" as shown by the orange arrows. The higher similarity score combined with the overfitting demonstrated in Table 2.6 sheds light on the issue. Despite this, we can clearly see that by generating images of each class, then creating a weighted sum of those new embeddings, we can achieve a correct classification as shown by the purple arrows in the diagram.\nWe ran tests on a many other classes, and achieved similar results. The accuracy improvement for the categories \"Frilled Lizard\" and \"Agama\" are shown below in Table 2.7\nFinally, we would like to bring attention to a final class, the Bishop of Llandaff. This is a class from the Flowers 102 dataset that showcased a final benefit of MuSE. This is because our CLIP baseline received 0% accuracy on classifying this particular flower. That means CLIP had very little cross modal representations of the Bishop of Llandaff, resulting in very little understanding of the flower. This broader issue arises from class imbalance within the training data. As mentioned within the Introduction (Chapter 1), multimodal models must be trained on datasets with rich cross-modal representations. The reason why, is because without these rich representations, class imbalances will occur. The issue is that no dataset can realistically have rich cross-modal representations for every possible category, so this is where MuSE comes in. Even if the generative model, or even the classifier model does not fully understand a given class, we can leverage CuPL to generate a description that breaks the concept down into its simpler parts, then generate an image that composes each of these parts. This is better demonstrated with an example:"}, {"title": "Discussion", "content": "In Figure 2.8 we see a ground truth photo of the Bishop of Llandaff, compared to a Stable Diffusion XL generated photo of the same class. This clearly shows that the model does not understand what the class is. In this scenario, MuSE will not improve the baseline CLIP performance at all; however, this can change with a few small adjustments. If we customize our CuPL prompt to include a series of intermediate steps (similar to chain-of-thought), we can form a new image based predominantly on the description provided and not the model's knowledge of the class. This is demonstrated by the new image generated with this technique:\nFor this image, our CuPL prompt was \"Dahlia 'Bishop of Llandaff' is a cultivar known for its dark red or maroon flowers and dark foliage.\" Even though it is similar in structure to the other CuPL prompts, it contains a few key details:\n\u2022 It includes the hierarchical class structure within the prompt. The Bishop of Llandaff is a type of Dahlia, so including this extra information allows the model to infer what the flower should look like.\n\u2022 The prompt also describes additional visual features which SDXL uses to improve its image genera- tion. Features such as color or foliage descriptions all help the generation process.\nThe fact that modifying the prompt allowed the model to generate a class it did not understand, is an idea that we will further explore in Section 4.1. This simple adjustment allowed use to get the resulting accuracies on\nAs is evident in all of the results, but most prominently within Table 2.2, using solely images does not work at all for this method of image classification. A major reason for this is due to the fact that the performance of MuSE is heavily dependent on whether the generated images somewhat match the distribution of images in the dataset (i.e. whether or not the image generation model can generate a good image of a given class). Similarly, it's possible that CLIP does not understand the difference between fine-grained classes (or at least it doesn't understand these differences very well). This can easily occur due to a non-comprehensive training set, or simply rare classes being used for evaluation.\nWith this in mind there are two prominent assumptions made in this proposal:\n\u2022 It is assumed that generative models used will better represent the true distribution of the data (due to their increased complexity and data diversity), and the multimodal model can distinguish between similar classes. If the multimodal model has learned separate classes are semantically identical, MuSE will not improve performance because there is a flaw in the fundamental knowledge of the multimodal model. If successful, this research would leverage the capabilities of expert models without retraining to gain data diversity, making multimodal systems more robust and capable of handling real-world challenges.\n\u2022 It is important to note that despite aiming to improve multimodal fairness, MuSE does not produce fair results. Instead, it offsets learned biases. This means that MuSE can either reduce or accentuate human bias, and should not be used as a universal architecture to improve multimodal model fairness. This will be further explored in later chapters."}, {"title": "Conclusion", "content": "Traditional multimodal classification methods face issues when dealing with fine-grained classes due to limited data representation. These challenges can lead to underfitting and misclassification. To mitigate these issues we have proposed MuSE, which utilizes generative prompting to improve image classification accuracy across a range of datasets, making it a highly generalizable technique. This approach is zero-shot, training-free, contains minimal overhead, and is able to reduce the misclassification rate while enhancing the accuracy of any multimodal classification model (if it confines to the restrictions mentioned within the Dis-cussion section). The results shown highlight the significant impact MuSE can have on image classification, and on many future downstream tasks."}, {"title": "D3G", "content": "Image classification is a task essential for machine perception to achieve human-level image understand-ing. Multimodal models such as CLIP, have been able to perform well on this task by learning semantic similarities across Vision and Language; however, despite these advances, image classification is still chal-lenging task. Models with low capacity often suffer from underfitting and thus underperform on fine-grained image classification. Along with this, it is important to ensure high-quality data with rich cross-modal rep-resentations of each class, which is often difficult to generate. When datasets do not enforce balanced demographics, the predictions will bias toward the more represented class, while others will be neglected. We focus on how these issues can lead to harmful bias for zero-shot image classification, and explore how to combat these issues in demographic bias. We propose Diverse Demographic Data Generation (D3G), a training-free, zero-shot method of boosting classification accuracy while reducing demographic bias in pre-trained multimodal models. With this method, we utilize CLIP as our base multimodal model, and Stable Diffusion XL as our generative model. We demonstrate providing diverse demographic data at inference time improves performance for these models, and explore the impact of individual demographics on the resulting accuracy metric."}, {"title": "Introduction", "content": "Deep Learning systems have been a promising new paradigm in the field of image classification. Vision-Language systems are able to utilize multiple modalities to create models that are more generalizable on a broad range of downstream tasks. Despite this performance", "2023": "however", "2021": ".", "learning": "biased data produces biased models.\nIt is important to note that we acknowledge not all bias is harmful and often is necessary for models go generalize. This is why within this work, we focus on demographic bias which can frequently have harmful implications when applied within society. Image classification is particularly pressing, because it is the core of a myriad of computer vision tasks. Facial recognition"}]}