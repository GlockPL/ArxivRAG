{"title": "Reasoning Beyond Bias: A Study on Counterfactual Prompting and Chain of Thought Reasoning", "authors": ["Kyle Moore", "Jesse Roberts", "Thao Pham", "Douglas Fisher"], "abstract": "Language models are known to absorb biases from their training data, leading to predictions driven by statistical regularities rather than semantic relevance. We investigate the impact of these biases on answer choice preferences in the Massive Multi-Task Language Understanding (MMLU) task. Our findings reveal that differences in learned regularities across answer options are predictive of model preferences and mirror human test-taking strategies. To address this issue, we introduce two novel methods: Counterfactual Prompting with Chain of Thought (CoT) and Counterfactual Prompting with Agnostically Primed CoT (APriCoT). We demonstrate that while Counterfactual Prompting with CoT alone is insufficient to mitigate bias, our novel Primed Counterfactual Prompting with CoT approach effectively reduces the influence of base-rate probabilities while improving overall accuracy. Our results suggest that mitigating bias requires a \"System-2\" like process and that CoT reasoning is susceptible to confirmation bias under some prompting methodologies. Our contributions offer practical solutions for developing more robust and fair language models.", "sections": [{"title": "Introduction", "content": "A model of human cognitive reasoning involving two distinct modes of thinking was posited in Kahneman's seminal work, Thinking Fast and Slow (Kahneman 2011). Thinking fast, or heuristically, is performed by System 1, while thinking slow, or deliberatively, is performed by System 2. Although this dichotomy has received both support and criticism since its publication, the concepts of System 1 and System 2 thought remain useful paradigms for understanding the relationship between behavior and computation, bearing a resemblance to the relationship between single-shot estimation and recursive generation.\nLarge Language Models (LLMs) can be prompted to engage in both System 1 and System 2 consistent behavior. Counterfactual (CF) prompting is one method typically used to achieve the former in behavioral studies (Roberts et al. 2024; Misra 2022), though it has potential applications, such as human-robot interaction, that could benefit from System 2 behavior (Roberts, Moore, and Fisher 2024). This is because System 1 behavior tends toward responses that are governed by the base rate probability (BRP) (Moore et al. 2024)."}, {"title": "Background", "content": "This section provides the pedagogical references and supporting literature on which the rest of the paper depends."}, {"title": "MMLU", "content": "The Massive Multitask Language Understanding (MMLU) task (Hendrycks et al. 2020) is a MCQA benchmark composed of 14042 questions distributed unevenly across 57 subjects. Each question has four answer choices provided, with one answer choice designated as correct. MMLU is a common benchmark included in many model release publications and is intended to quantitatively measure a model's natural language understanding and factual knowledge.\nMMLU places few restrictions on the prompting method used for evaluation, making it deceptively difficult to compare accuracy measures across models. Evaluation methods often include optional in-context learning up to 5-shots and CoT reasoning. Questions used for 5-shot in-context learning examples are provided as a separate set and are not included in the reported benchmark accuracies."}, {"title": "Cloze Prompting", "content": "Cloze testing is a prompting strategy commonly used to evaluate tasks where LLMs must select from a set of options. In this approach, the probability of each option is measured given a shared context. The chosen option is taken to be the option that yields the highest probability, i.e. $\\max_{a\\in A}P(a|C')$ where C is the task context and A is the set of available option signifiers. For example, a four option multiple choice question answering (MCQA) task can be evaluated via cloze testing by providing the question and answer choices to the model before finishing with a query statement similar to \"The most correct answer choice is\".\nThe question, choices, and query are provided to the model and the probabilities of the four completions \u201cA\u201d, \u201cB\u201d, \u201cC\u201d, and \"D\" are computed independently. The model's answer is determined by the option with the highest probability."}, {"title": "Counterfactual Prompting", "content": "Counterfactual (CF) prompting differs in the structure of the prompt. Instead of a shared query, CF prompting uses a separate query for every option and a shared canary completion for evaluation. The option eliciting the highest canary probability, i.e. $\\max_{a\\in C}P(canary|Q, a)$ is taken as the model's choice. For a multiple choice question, the question and answer choices remain unchanged, while the query portion of the prompt takes a form similar to \u201cAnswer choice X is the most\", where X is one of \"A\", \"B\", \"C\", or \"D\", and the canary completion for all options is \u201ccorrect\u201d.\nCounterfactual prompting is intended to mitigate BRP effects while maintaining task semantics. Prior work has shown that CF evaluation fails to strongly mitigate BRP effects and may degrade performance on some tasks (Moore et al. 2024). Because the task definition semantic content is equivalent between CF and cloze prompting, expected model behavior measurements should not change.\nChange in task performance with equivalent prompting methods is indicative of fragile, under-supported behaviors in the model. For the MCQA example, significant loss in performance from simple rewording of the questions, rearranging of answer choices, or changing the wording when asking the model for its answer suggests that the model does not truly possess either sufficient question understanding and/or the requisite factual knowledge."}, {"title": "Chain-of-Thought", "content": "Chain-of-Thought (CoT) is a common prompting strategy proposed by Wei et al. (2022) in which the language model is encouraged to generate freely from a task definition before providing a final answer. It is intended to approximate System-2-like processing and has been shown to improve model performance on a wide array of traditionally difficult benchmarks across numerous tasks (Suzgun et al. 2023).\nThe mechanism by which CoT yields improvement is currently not fully understood and subject to ongoing research. It was suggested by Madaan, Hermann, and Yazdanbakhsh (2023) that it functions, in part, by improving a model's understanding of the target task by retrieving and incorporating relevant information into the context. This seems to be in line with other research by Prystawski, Li, and Goodman (2024), who theorize that CoT works by traversal of the Bayes net like semantic space between the semantic information contained within the union of the pre-training data, the provided context, and a target completion. Models, however, may not traverse the most useful path in this space and have been shown in (Saparov and He 2023) to be greedy reasoners that struggle to explore multiple semantic pathways and instead tend to traverse toward pre-existing or induced biases (Turpin et al. 2024)."}, {"title": "Base-Rate Probability Effects", "content": "LLMs predict the most likely next token given all tokens in the preceding context. This conceptually simple function has shown since at least Radford et al. (2019) to be sufficient to perform a wide variety of tasks with a surprising level of competency. While tokens have an in-context probability, they also have an intrinsic probability called the base-rate probability (BRP) (Moore et al. 2024; Wei et al. 2024).\nBRP is the probability associated with a token given a minimal amount of context necessary to isolate the represented concept. Differences in BRP between tokens have been shown to affect models' overt behavior and performance on downstream tasks (Moore et al. 2024) both when prompted via cloze and CF prompting.\nBRP effects may make it difficult to isolate behaviors and abilities that are being measured. For example, a model may be evaluated as being more prone to violence if it is evaluated by a yes/no questionnaire in which violent-indicative yeses outnumber peaceful-indicative yeses simply because the BRP for yes is higher than that for no. Given the same task with a trivial reversal of the question polarity may yield reversed results without semantic changes to the context."}, {"title": "LLama-3.1 Token Base-Rate Probability", "content": "In this section, we describe how BRPs were obtained for the LLaMa 3.1 model (Dubey et al. 2024). BRP can be naively defined as the probability of a token given no prior context, but may also be contextually defined to account for BRP differences between concepts that share the same token. For example, when measuring the BRP of the letter \"A\", it might be important to differentiate between the BRP of the article \"A\" and \"A\" as an answer choice in a MCQA task. This example is especially noteworthy, as the article \"A\" has a high contextual probability with an empty context string because \"A\" is among the most common words to start a sentence or even entire documents. This is clearly not the case for answer choices \"B\", \"C\", or \"D\".\nThe remainder of this paper investigates prompting strategies using the common MMLU benchmark as a test-bed, meaning that we must evaluate the BRP of each answer choice in the context of an MCQA task. We use nearly the same method as described in Moore et al. (2024) to calculate BRPs, with a change in the prompt template to match the templates described in later sections and summarized in Table 1. We measure BRPs using cloze prompting in line with previous work, which found the cloze BRP was predictive of overt behavior for both cloze and CF evaluation at task time. We verify this claim by measuring the correlation between cloze BRP and overt behavior later in this work. The template used to measure BRP is described in Table 2."}, {"title": "Counterfactual CoT", "content": "In Moore et al. (2024), the authors found that CF prompting weakly mitigates BRP effects on overt behavior, but fails to eliminate them entirely. We hypothesize this is because the models are restricted to immediately completing the task, with no ability to reason over the potential answers or question for more complex questions. This forces the models to resort to the heuristics like answer choice or positional preference. Heuristic behaviors of this type are reminiscent of System-1 processing (Kahneman 2011).\nIn line with the two system theory, we contend that a more deliberative, System-2-like, evaluation will eliminate the BRP effect. To evaluate this, we propose a novel adaptation of the counterfactual methods used in Moore et al. (2024) to include chain-of-thought reasoning."}, {"title": "Methods: CF+CoT", "content": "We extend existing work by evaluating the LLaMa 3.1 8B model for its susceptibility to BRP effects under CF+CoT reasoning through the use of a common multiple choice question answering (MCQA) benchmark, MMLU (Hendrycks et al. 2020)."}, {"title": "Results: CF+CoT", "content": "We measured the answer choice distribution in the responses from the model using both prompting strategies across all choice orderings. The results of this are shown in the first two rows of Figure 2. Consistent with previous results in Moore et al. (2024), we see a strong preference for one answer choice over all others in the case of CF reasoning. In general, the model, when evaluated without CF+CoT, seems to have a strong preference for answer choice C, a strong anti-preference for answer choice D, and neither preference nor anti-preference for choices B and A.\nSurprisingly, the same relative preference ordering is not only maintained when evaluated with CF+CoT, but it is magnified. The model shows a nearly exclusive preference for answering with choice \"C\" and strong aversion to choice \"A\", while choice \u201cD\u201d is never once chosen as the preferred answer choice. This is not inherently problematic, given that such a distribution may be the result of imbalanced distribution in the ground truth answers. Heuristics similar to BRP also tend to be helpful and improve accuracy for some tasks (Wang and Zhong 2024). As we can see in the bottom row of Figure 2, however, the ground truth answer distribution for MMLU is remarkably balanced, suggesting CF+CoT has tended to decrease the models similarity to the ground truth.\nTo further explore this, we compute the Pearson's r correlation between the chosen answer choice distribution and the answer choice BRP. Correlation is computed per subject and aggregated via Fisher-z transformation before re-"}, {"title": "Discussion: CF+CoT", "content": "In Prystawski, Li, and Goodman (2024) CoT is interpreted as a method facilitating traversal of a Bayes net of related concepts. By iteratively traversing collocated concepts, reaching the target is possible even though the target is far from the starting point. Essentially, CoT is a mechanism for traversing semantic space. However, this interpretation equally applies when the direction of traversal does not lead to the correct answer due to bias."}, {"title": "Counterfactual APriCoT", "content": "In order to address the shortcomings identified in the CF+CoT results, we propose an alternative method of inducing autonomous reasoning in models that we call Agnostically Primed Chain-of-Thought (APriCoT). APriCoT differs from CF+CoT in two important ways. First, CoT is elicited in combination with one of the candidate answer choices. In effect, the model is told to evaluate the correctness of one of the answer choices. Second, each post-CoT context is used to evaluate only the same answer choice with which the CoT was primed. The result is that each answer choice is reasoned over and queried for its validity independently from the other options. The differences between CF+CoT and APriCoT are summarized in Table 1, and the details of the APriCoT answer elicitation are shown in Algorithm 2."}, {"title": "Methods: APriCoT", "content": "APriCoT is evaluated on the same MMLU question set as CF+CoT with the same 100 maximum token generation limit, 10 iterations for robustness, and answer choice selection method. All other evaluation aspects, including the maximum CoT token length, the canary completion, and the number of iterations remain unchanged from CF+CoT."}, {"title": "Results: APriCoT", "content": "Figure 2 shows the total answer distribution for APriCoT. It is visibly clear that the choice distribution is well balanced and closely matches that of the ground truth answers.\nAs with CF+CoT, we evaluate APriCoT for its susceptibility to BRP effects. In the third row of Figure 3, we see that across most subjects, the answer distribution has diminished correlation with BRP compared to CF+CoT and CF. We use the Fisher's-z transformation to aggregate the correlation across subjects. The results suggest that the BRP contributes $r^2$ = 35% of the variance across answer choices compared with r\u00b2 = 93% for CF+CoT. APriCoT's correlation is thus much closer though not perfectly in line with the correlation between ground truth and BRP ($r^2$ = 0.08).\nWe also compare BRP correlation with CF+CoT and APriCoT to the statistical distribution created by the ground truth correlation with BRP. An ideal predictor would be indistinguishable from ground truth. We use the Wilcoxon signed-rank test to measure the dissimilarity between the correlations of the ground truth with CF+CoT and APri-CoT correlations. The resulting test statistics are T = 15 for CF+CoT and T = 512 for APriCoT, suggesting that correlations for APriCoT match the ground truth much closer than for CF+CoT. This suggests that APriCoT tends to mitigate the CoT confirmation bias as it tends to be more similar to ground truth and uncorrelated with BRP.\nAccuracy Effects Finally, we evaluated the effect of APri-CoT on the accuracy of the model. We expected that the accuracy of the model should improve due to a combination of noise reduction from BRP mitigation and a more sophisticated deliberative mechanism. We measure accuracy only across the entire dataset by taking the number of questions answered correctly divided by the total number of questions. The result is an accuracy of 37.96% for CF. Likely due to its susceptibility to BRP effects, CF+CoT saw a slight reduction to 35.56%, while APriCoT improved to 42.79%.\nThis alone suggests that APriCoT improves model performance in addition to mitigating surface level biases, but the results improve further with slight decomposition, as shown in Figure 4. When tracked per-question, we see that for every question that was answered correctly with CF+CoT, APri-CoT is more likely to also be correct and less likely to be incorrect compared to CF+CoT. To an even greater degree, questions answered incorrectly without CF+CoT are more likely to be answered correct and less likely to be answered incorrectly using APriCoT than CF+CoT. In short, APriCoT improves more than CF+CoT on performance in terms of overall accuracy in every way.\nAPriCoT shows evidence of being both a less biased and more effective method of reasoning in LLMs compared to the standard practice of CoT."}, {"title": "Prior Work", "content": "In this section, we briefly survey the existing literature surround mitigation of base rate effects and the use of CoT in connection with other heuristics and biases. BRP effects were evaluated in some depth by Wei et al. (2024); Moore et al. (2024). Both works found that LLMs are prone to BRP effects in the context of MCQA tasks. Moore et al. (2024) proposed a method for reformulating MCQA tasks to be less sensitive in to BRP effects, both helpful and detrimental, but they do not work to mitigate the effects in model behavior. Wei et al. (2024) attempted to mitigate the BRP effect in behavior by calibrating the output probabilities using the mea-"}, {"title": "Conclusion", "content": "In this work, we have attempted to mitigate a simple, but pernicious, bias in LLM behavior that complicates research into other behaviors and abilities. This bias, known as base-rate bias, was shown both here and in previous work to strongly predict model behavior on the MMLU benchmark, which is a common measure of LLM factual knowledge and language understanding. We showed that induction of more complex reasoning via the commonly employed CoT prompting strategy, surprisingly exacerbates the BRP bias, which we argue is the result of confirmation bias-like behavior.\nWe proposed and evaluated a novel prompting and evaluation method called Agnostically Primed Chain-of-Thought, otherwise known as APriCoT. The method induces more complex and focused deliberation, closer approximating System-2 thought than CoT alone. We showed that APri-CoT both significantly reduces, though still does not fully eliminate, BRP effects and noticeably improves model performance on the MMLU task. We hope for this method to be a useful step toward controlling for or eliminating System-1 heuristics in LLM behavior research and that it might inspire useful methods in LLM-based complex reasoning systems."}, {"title": "Future Work", "content": "Our methods were limited to a relatively rigid and strictly defined problem structure. Future work should investigate whether APriCoT is appropriate for use in more complex reasoning tasks like search or planning. In addition, APri-CoT, despite being influenced by CF prompting in its creation, makes no assumptions about the evaluation method used. As such, future work should investigate whether APri-CoT yields similar improvements for cloze testing and any other alternative instantaneous evaluation techniques.\nAPriCoT is currently only defined for tasks with a predefined set of available options. Future work may investigate methods that do not require pre-defined options, such as dynamic option generation by the model itself. This could involve complex agentic planning tasks or open ended generation benchmarks.\nThis work also introduces or highlights multiple theoretical questions regarding mechanistic explanations for LLM behavior. These include the mere existence of ant-confirmation bias, the dependence of CF overt behavior on cloze BRPs, and the lack of correlation between CF overt behavior and CF BRPs. Future theoretical work should try to account for these results."}, {"title": "Limitations", "content": "All experiments described were performed exclusively on the newly released LLaMa 3.1 model from Meta AI (Dubey et al. 2024). Due to resource constraints, only the 8 billion parameter model is evaluated. This model is newly released at the time of writing and has shown promising results on various benchmarks, including state-of-the-art performance on the MMLU benchmark among open source LLMs. We cannot, however, make strong claims of whether the results reported herein will transfer across model families or persist with increased model size."}]}