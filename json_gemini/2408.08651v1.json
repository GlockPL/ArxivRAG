{"title": "Reasoning Beyond Bias: A Study on Counterfactual Prompting and Chain of Thought Reasoning", "authors": ["Kyle Moore", "Jesse Roberts", "Thao Pham", "Douglas Fisher"], "abstract": "Language models are known to absorb biases from their train-ing data, leading to predictions driven by statistical regu-larities rather than semantic relevance. We investigate theimpact of these biases on answer choice preferences in theMassive Multi-Task Language Understanding (MMLU) task.Our findings reveal that differences in learned regularitiesacross answer options are predictive of model preferencesand mirror human test-taking strategies. To address this issue,we introduce two novel methods: Counterfactual Promptingwith Chain of Thought (CoT) and Counterfactual Promptingwith Agnostically Primed CoT (APriCoT). We demonstratethat while Counterfactual Prompting with CoT alone is in-sufficient to mitigate bias, our novel Primed CounterfactualPrompting with CoT approach effectively reduces the influ-ence of base-rate probabilities while improving overall ac-curacy. Our results suggest that mitigating bias requires a\"System-2\" like process and that CoT reasoning is suscep-tible to confirmation bias under some prompting methodolo-gies. Our contributions offer practical solutions for develop-ing more robust and fair language models.", "sections": [{"title": "Introduction", "content": "A model of human cognitive reasoning involving two dis-tinct modes of thinking was posited in Kahneman's seminalwork, Thinking Fast and Slow (Kahneman 2011). Thinkingfast, or heuristically, is performed by System 1, while think-ing slow, or deliberatively, is performed by System 2. Al-though this dichotomy has received both support and criti-cism since its publication, the concepts of System 1 and Sys-tem 2 thought remain useful paradigms for understandingthe relationship between behavior and computation, bearinga resemblance to the relationship between single-shot esti-mation and recursive generation.\nLarge Language Models (LLMs) can be prompted to en-gage in both System 1 and System 2 consistent behavior.Counterfactual (CF) prompting is one method typically usedto achieve the former in behavioral studies (Roberts et al.2024; Misra 2022), though it has potential applications, suchas human-robot interaction, that could benefit from System 2behavior (Roberts, Moore, and Fisher 2024). This is becauseSystem 1 behavior tends toward responses that are governedby the base rate probability (BRP) (Moore et al. 2024)."}, {"title": "Background", "content": "This section provides the pedagogical references and sup-porting literature on which the rest of the paper depends."}, {"title": "MMLU", "content": "The Massive Multitask Language Understanding (MMLU)task (Hendrycks et al. 2020) is a MCQA benchmark com-posed of 14042 questions distributed unevenly across 57subjects. Each question has four answer choices provided,with one answer choice designated as correct. MMLU is acommon benchmark included in many model release publi-cations and is intended to quantitatively measure a model'snatural language understanding and factual knowledge.\nMMLU places few restrictions on the prompting methodused for evaluation, making it deceptively difficult to com-pare accuracy measures across models. Evaluation methodsoften include optional in-context learning up to 5-shots andCoT reasoning. Questions used for 5-shot in-context learn-ing examples are provided as a separate set and are not in-cluded in the reported benchmark accuracies."}, {"title": "Cloze Prompting", "content": "Cloze testing is a prompting strategy commonly used toevaluate tasks where LLMs must select from a set of op-tions. In this approach, the probability of each option is mea-sured given a shared context. The chosen option is takento be the option that yields the highest probability, i.e.$\\max_{a \\in A}P(a|C')$ where C is the task context and A is theset of available option signifiers. For example, a four op-tion multiple choice question answering (MCQA) task canbe evaluated via cloze testing by providing the question andanswer choices to the model before finishing with a querystatement similar to \"The most correct answer choice is\".The question, choices, and query are provided to the modeland the probabilities of the four completions \u201cA\u201d, \u201cB\u201d, \u201cC\u201d,and \"D\" are computed independently. The model's answeris determined by the option with the highest probability."}, {"title": "Counterfactual Prompting", "content": "Counterfactual (CF) prompting differs in the structure of theprompt. Instead of a shared query, CF prompting uses a sep-arate query for every option and a shared canary completionfor evaluation. The option eliciting the highest canary prob-ability, i.e. $\\max_{a \\in C}P(canary|Q, a)$ is taken as the model'schoice. For a multiple choice question, the question and an-swer choices remain unchanged, while the query portion ofthe prompt takes a form similar to \u201cAnswer choice X is themost\", where X is one of \"A\", \"B\", \"C\", or \"D\", and thecanary completion for all options is \u201ccorrect\u201d.\nCounterfactual prompting is intended to mitigate BRPeffects while maintaining task semantics. Prior work hasshown that CF evaluation fails to strongly mitigate BRP ef-fects and may degrade performance on some tasks (Mooreet al. 2024). Because the task definition semantic contentis equivalent between CF and cloze prompting, expectedmodel behavior measurements should not change.\nChange in task performance with equivalent promptingmethods is indicative of fragile, under-supported behaviorsin the model. For the MCQA example, significant loss inperformance from simple rewording of the questions, rear-ranging of answer choices, or changing the wording whenasking the model for its answer suggests that the modeldoes not truly possess either sufficient question understand-ing and/or the requisite factual knowledge."}, {"title": "Chain-of-Thought", "content": "Chain-of-Thought (CoT) is a common prompting strategyproposed by Wei et al. (2022) in which the language modelis encouraged to generate freely from a task definition be-fore providing a final answer. It is intended to approximateSystem-2-like processing and has been shown to improvemodel performance on a wide array of traditionally difficultbenchmarks across numerous tasks (Suzgun et al. 2023).\nThe mechanism by which CoT yields improvement is cur-rently not fully understood and subject to ongoing research.It was suggested by Madaan, Hermann, and Yazdanbakhsh(2023) that it functions, in part, by improving a model's un-derstanding of the target task by retrieving and incorporat-ing relevant information into the context. This seems to bein line with other research by Prystawski, Li, and Goodman(2024), who theorize that CoT works by traversal of theBayes net like semantic space between the semantic infor-mation contained within the union of the pre-training data,the provided context, and a target completion. Models, how-ever, may not traverse the most useful path in this space andhave been shown in (Saparov and He 2023) to be greedy rea-soners that struggle to explore multiple semantic pathwaysand instead tend to traverse toward pre-existing or inducedbiases (Turpin et al. 2024)."}, {"title": "Base-Rate Probability Effects", "content": "LLMs predict the most likely next token given all tokensin the preceding context. This conceptually simple functionhas shown since at least Radford et al. (2019) to be sufficientto perform a wide variety of tasks with a surprising levelof competency. While tokens have an in-context probability,they also have an intrinsic probability called the base-rateprobability (BRP) (Moore et al. 2024; Wei et al. 2024).\nBRP is the probability associated with a token given aminimal amount of context necessary to isolate the repre-sented concept. Differences in BRP between tokens havebeen shown to affect models' overt behavior and perfor-mance on downstream tasks (Moore et al. 2024) both whenprompted via cloze and CF prompting.\nBRP effects may make it difficult to isolate behaviors andabilities that are being measured. For example, a model maybe evaluated as being more prone to violence if it is eval-uated by a yes/no questionnaire in which violent-indicativeyeses outnumber peaceful-indicative yeses simply becausethe BRP for yes is higher than that for no. Given the sametask with a trivial reversal of the question polarity may yieldreversed results without semantic changes to the context."}, {"title": "LLama-3.1 Token Base-Rate Probability", "content": "In this section, we describe how BRPs were obtained for theLLaMa 3.1 model (Dubey et al. 2024). BRP can be naivelydefined as the probability of a token given no prior context,but may also be contextually defined to account for BRPdifferences between concepts that share the same token. Forexample, when measuring the BRP of the letter \"A\", it mightbe important to differentiate between the BRP of the article\"A\" and \"A\" as an answer choice in a MCQA task. This ex-ample is especially noteworthy, as the article \"A\" has a highcontextual probability with an empty context string because\"A\" is among the most common words to start a sentence oreven entire documents. This is clearly not the case for an-swer choices \u201cB\u201d, \u201cC\u201d, or \u201cD\u201d.\nThe remainder of this paper investigates prompting strate-gies using the common MMLU benchmark as a test-bed,meaning that we must evaluate the BRP of each answerchoice in the context of an MCQA task. We use nearly thesame method as described in Moore et al. (2024) to calcu-late BRPs, with a change in the prompt template to matchthe templates described in later sections and summarized inTable 1. We measure BRPs using cloze prompting in linewith previous work, which found the cloze BRP was predic-tive of overt behavior for both cloze and CF evaluation attask time. We verify this claim by measuring the correlationbetween cloze BRP and overt behavior later in this work.The template used to measure BRP is described in Table 2."}, {"title": "Counterfactual CoT", "content": "In Moore et al. (2024), the authors found that CF promptingweakly mitigates BRP effects on overt behavior, but fails toeliminate them entirely. We hypothesize this is because themodels are restricted to immediately completing the task,with no ability to reason over the potential answers or ques-tion for more complex questions. This forces the models toresort to the heuristics like answer choice or positional pref-erence. Heuristic behaviors of this type are reminiscent ofSystem-1 processing (Kahneman 2011).\nIn line with the two system theory, we contend that amore deliberative, System-2-like, evaluation will eliminatethe BRP effect. To evaluate this, we propose a novel adap-tation of the counterfactual methods used in Moore et al.(2024) to include chain-of-thought reasoning."}, {"title": "Methods: CF+CoT", "content": "We extend existing work by evaluating the LLaMa 3.18B model for its susceptibility to BRP effects underCF+CoT reasoning through the use of a common multiplechoice question answering (MCQA) benchmark, MMLU(Hendrycks et al. 2020)."}, {"title": "Results: CF+CoT", "content": "We measured the answer choice distribution in the responsesfrom the model using both prompting strategies across allchoice orderings. The results of this are shown in the firsttwo rows of Figure 2. Consistent with previous results inMoore et al. (2024), we see a strong preference for one an-swer choice over all others in the case of CF reasoning. Ingeneral, the model, when evaluated without CF+CoT, seemsto have a strong preference for answer choice C, a stronganti-preference for answer choice D, and neither preferencenor anti-preference for choices B and A.\nSurprisingly, the same relative preference ordering is notonly maintained when evaluated with CF+CoT, but it is mag-nified. The model shows a nearly exclusive preference foranswering with choice \"C\" and strong aversion to choice\"A\", while choice \u201cD\u201d is never once chosen as the preferredanswer choice. This is not inherently problematic, given thatsuch a distribution may be the result of imbalanced distribu-tion in the ground truth answers. Heuristics similar to BRPalso tend to be helpful and improve accuracy for some tasks(Wang and Zhong 2024). As we can see in the bottom rowof Figure 2, however, the ground truth answer distributionfor MMLU is remarkably balanced, suggesting CF+CoT hastended to decrease the models similarity to the ground truth.\nTo further explore this, we compute the Pearson's r cor-relation between the chosen answer choice distribution andthe answer choice BRP. Correlation is computed per sub-ject and aggregated via Fisher-z transformation before re-"}, {"title": "Discussion: CF+CoT", "content": "In Prystawski, Li, and Goodman (2024) CoT is interpretedas a method facilitating traversal of a Bayes net of re-lated concepts. By iteratively traversing collocated concepts,reaching the target is possible even though the target is farfrom the starting point. Essentially, CoT is a mechanismfor traversing semantic space. However, this interpretationequally applies when the direction of traversal does not leadto the correct answer due to bias."}, {"title": "Counterfactual APriCoT", "content": "In order to address the shortcomings identified in theCF+CoT results, we propose an alternative method of induc-ing autonomous reasoning in models that we call Agnosti-cally Primed Chain-of-Thought (APriCoT). APriCoT differsfrom CF+CoT in two important ways. First, CoT is elicitedin combination with one of the candidate answer choices.In effect, the model is told to evaluate the correctness ofone of the answer choices. Second, each post-CoT contextis used to evaluate only the same answer choice with whichthe CoT was primed. The result is that each answer choiceis reasoned over and queried for its validity independentlyfrom the other options. The differences between CF+CoTand APriCoT are summarized in Table 1, and the details ofthe APriCoT answer elicitation are shown in Algorithm 2."}, {"title": "Methods: APriCoT", "content": "APriCoT is evaluated on the same MMLU question set asCF+CoT with the same 100 maximum token generationlimit, 10 iterations for robustness, and answer choice se-lection method. All other evaluation aspects, including themaximum CoT token length, the canary completion, and thenumber of iterations remain unchanged from CF+CoT."}, {"title": "Results: APriCoT", "content": "Figure 2 shows the total answer distribution for APriCoT. Itis visibly clear that the choice distribution is well balancedand closely matches that of the ground truth answers.\nAs with CF+CoT, we evaluate APriCoT for its suscepti-bility to BRP effects. In the third row of Figure 3, we see"}, {"title": "Prior Work", "content": "In this section, we briefly survey the existing literature sur-round mitigation of base rate effects and the use of CoTin connection with other heuristics and biases. BRP effectswere evaluated in some depth by Wei et al. (2024); Mooreet al. (2024). Both works found that LLMs are prone to BRPeffects in the context of MCQA tasks. Moore et al. (2024)proposed a method for reformulating MCQA tasks to be lesssensitive in to BRP effects, both helpful and detrimental, butthey do not work to mitigate the effects in model behavior.Wei et al. (2024) attempted to mitigate the BRP effect in be-havior by calibrating the output probabilities using the mea-"}, {"title": "Conclusion", "content": "In this work, we have attempted to mitigate a simple, but per-nicious, bias in LLM behavior that complicates research intoother behaviors and abilities. This bias, known as base-ratebias, was shown both here and in previous work to stronglypredict model behavior on the MMLU benchmark, which isa common measure of LLM factual knowledge and languageunderstanding. We showed that induction of more complexreasoning via the commonly employed CoT prompting strat-egy, surprisingly exacerbates the BRP bias, which we argueis the result of confirmation bias-like behavior.\nWe proposed and evaluated a novel prompting and evalu-ation method called Agnostically Primed Chain-of-Thought,otherwise known as APriCoT. The method induces morecomplex and focused deliberation, closer approximatingSystem-2 thought than CoT alone. We showed that APri-CoT both significantly reduces, though still does not fullyeliminate, BRP effects and noticeably improves model per-formance on the MMLU task. We hope for this method to bea useful step toward controlling for or eliminating System-1heuristics in LLM behavior research and that it might inspireuseful methods in LLM-based complex reasoning systems."}, {"title": "Future Work", "content": "Our methods were limited to a relatively rigid and strictlydefined problem structure. Future work should investigatewhether APriCoT is appropriate for use in more complexreasoning tasks like search or planning. In addition, APri-CoT, despite being influenced by CF prompting in its cre-ation, makes no assumptions about the evaluation methodused. As such, future work should investigate whether APri-CoT yields similar improvements for cloze testing and anyother alternative instantaneous evaluation techniques.\nAPriCoT is currently only defined for tasks with a prede-fined set of available options. Future work may investigatemethods that do not require pre-defined options, such as dy-namic option generation by the model itself. This could in-volve complex agentic planning tasks or open ended gener-ation benchmarks.\nThis work also introduces or highlights multiple the-oretical questions regarding mechanistic explanations forLLM behavior. These include the mere existence of anti-confirmation bias, the dependence of CF overt behavior oncloze BRPs, and the lack of correlation between CF overtbehavior and CF BRPs. Future theoretical work should tryto account for these results."}, {"title": "Limitations", "content": "All experiments described were performed exclusively onthe newly released LLaMa 3.1 model from Meta AI (Dubeyet al. 2024). Due to resource constraints, only the 8 billionparameter model is evaluated. This model is newly releasedat the time of writing and has shown promising results onvarious benchmarks, including state-of-the-art performanceon the MMLU benchmark among open source LLMs. Wecannot, however, make strong claims of whether the resultsreported herein will transfer across model families or persistwith increased model size."}, {"title": "Technical Details", "content": "All experiments were performed with 100 hours on a GoogleColab NVIDIA A100 GPU. All code and data relevant tothis work are available open source at [GITHUB-LINK-REDACTED] to accommodate replication."}]}