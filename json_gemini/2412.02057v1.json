{"title": "Comparative Analysis of Multi-Agent Reinforcement Learning Policies for Crop Planning Decision Support", "authors": ["Anubha Mahajan", "Shreya Hegde", "Ethan Shay", "Daniel Wu", "Aviva Prins"], "abstract": "In India, the majority of farmers are classified as small or marginal, making their livelihoods particularly vulnerable to economic losses due to market saturation and climate risks. Effective crop planning can significantly impact their expected income, yet existing decision support systems (DSS) often provide generic recommendations that fail to account for real-time market dynamics and the interactions among multiple farmers. In this paper, we evaluate the viability of three multi-agent reinforcement learning (MARL) approaches for optimizing total farmer income and promoting fairness in crop planning: Independent Q-Learning (IQL), where each farmer acts independently without coordination, Agent-by-Agent (ABA), which sequentially optimizes each farmer's policy in relation to the others, and the Multi-agent Rollout Policy, which jointly optimizes all farmers' actions for global reward maximization. Our results demonstrate that while IQL offers computational efficiency with linear runtime, it struggles with coordination among agents, leading to lower total rewards and an unequal distribution of income. Conversely, the Multi-agent Rollout policy achieves the highest total rewards and promotes equitable income distribution among farmers but requires significantly more computational resources, making it less practical for large numbers of agents. ABA strikes a balance between runtime efficiency and reward optimization, offering reasonable total rewards with acceptable fairness and scalability. These findings highlight the importance of selecting appropriate MARL approaches in DSS to provide personalized and equitable crop planning recommendations, advancing the development of more adaptive and farmer-centric agricultural decision-making systems.", "sections": [{"title": "1 Introduction", "content": "India's agricultural sector faces major challenges. Over half of agricultural households are in debt, often due to crop failures (Statista Research 2024). Effective crop planning is key to improving farmers' incomes, but existing systems do not adapt well to real-time market supply and demand. This often leads to income loss, especially when recommendations are homogenous, causing market saturation and lower profits. For small-scale farmers, particularly in countries like India, crop planning that adjusts dynamically can significantly boost their earnings (Fabregas, Kremer, and Schilbach 2019).\nCrop planning is a crucial challenge for small-scale farmers, especially in developing nations like India, where they face significant risks from climate variability, market fluctuations, and limited resources (Matthan 2023). Decisions on what to plant, when to harvest, and how to manage resources are critical to farmers' livelihoods, yet traditional decision-support systems (DSS) often fall short of addressing the complexities they face. These systems do not account for the interconnected nature of farming communities, where one farmer's choices can impact others, especially in shared markets where oversupply can drive down prices. As a result, farmers are left vulnerable to economic instability and are often unable to achieve equitable outcomes (Esteso et al. 2022).\nWhile reinforcement learning (RL) has demonstrated success in agricultural applications such as fertilization management policies (Overweg, Berghuijs, and Athanasiadis 2021), its potential for crop planning remains underexplored, particularly in multi-agent settings. Traditional RL systems primarily focus on optimizing outcomes for a single agent, overlooking the critical role of coordination among multiple farmers. Multi-agent reinforcement learning (MARL) offers a promising framework for addressing this gap, enabling farmers to optimize their actions collectively while managing shared resources and mitigating market risks. Recent advancements in MARL highlight its application in tasks such as spatiotemporal sensing (Tamba 2022) and task offloading and network selection in heterogeneous agricultural networks (Zhu et al. 2023). Additionally, MARL has been used to coordinate UAVs for field coverage and crop monitoring (Marwah et al. 2023). Despite these innovations, existing MARL applications often focus on objectives like spacial coverage or resource allocation, overlooking market saturation and dynamic interdependencies in crop planning. This highlights the need for MARL systems that enhance efficiency, include agent interactions, address market dynamics, and promote economic stability in shared decision-making.\nIn this paper, we explore how three different MARL algorithms manage joint rewards and fairness in greenhouse crop planning in Telangana, India: (a) Independent Q-Learning (IQL), (b) Agent-by-Agent (ABA), and (c) multi-agent ROLLOUT Policy. Each algorithm has a unique approach:"}, {"title": "2 Related Works", "content": "Crop planning is a critical issue for small-scale farmers, particularly in developing nations like India where climate variability and market fluctuations can lead to severe economic losses. Various DSS have been developed to assist farmers in selecting optimal planting and harvesting schedules. Traditional approaches, such as those implemented by (Sarker and Quaddus 2002; Mohamed et al. 2016; Bhatia and Rana 2020), utilize rule-based systems or linear programming techniques to provide generalized crop recommendations based on average weather and market conditions. However, these systems fail to account for the heterogeneity of farmers' individual circumstances and the non-stationary nature of both environmental and economic factors.\nRL offers a more dynamic approach to crop planning by enabling systems to adapt based on continuous feedback from the environment. For example, Yang et al. (2020) applied reinforcement learning to optimize irrigation scheduling, achieving improved water-use efficiency in agriculture. Similarly, Chen et al. (2021) proposed an RL-based system that accounts for weather predictions and soil moisture data to inform farmers' irrigation practices. While effective for single-agent settings, these systems do not account for the complex interactions between multiple farmers, whose simultaneous actions can influence market prices and resource availability.\nWhile MARL holds promise for addressing challenges in collaborative decision-making, its application in crop planning remains limited. Most existing research explores related but distinct areas, reducing its direct applicability. For example, Kamali, Niksokhan, and Ardestani (2024) developed a multi-agent simulation to evaluate groundwater management policies, where agents negotiate water usage based on local needs. This study highlights MARL's potential in managing shared resources but does not address crop planning dynamics. Similarly, Li, Zhang, and Wang (2018) applied multi-agent systems in agricultural markets to optimize planting schedules based on market conditions. However, their approach primarily focused on price predictions and overlooked the role of agent interactions in mitigating market saturation. These gaps highlight the need for further exploration of MARL frameworks tailored to the complexities of crop planning.\nOur work builds on this existing research by investigating three RL approaches within a multi-agent setting to address critical gaps in crop planning strategies, particularly in optimizing joint rewards among farmers. Conventional systems often rely on uniform recommendations; in contrast, our system dynamically adapts to the actions of other agents (farmers) to mitigate market saturation and promote fair distribution of profits. Each policy is evaluated based on its trade-offs in computational efficiency, coordination, and fairness. This analysis uncovers underexplored dynamics in MARL approaches and offers insights into their potential to advance collaborative decision-making in agricultural systems."}, {"title": "3 Methods", "content": "First, we will discuss how we model the problem. We will then describe the objective which our algorithms seek to optimize. Next, we will describe the three algorithms which we have developed to solve our problem."}, {"title": "3.1 Problem Modeling", "content": "As mentioned in Section 1, we are investigating greenhouses in Telangana, India. We model the greenhouses as a collection of identical Markov Decision Processes (MDPs). We extend Lu and Prins (2024)'s model of individual greenhouse-agents with a new model to capture interactions between agents. We capture the problem over a finite horizon of $T\\in \\mathbb{N}$ discrete timesteps, where a fixed number of days passes with each timestep.\nThere are $n$ agents belonging to the agent set $I$. Every agent $i \\in I$ owns a greenhouse, whose condition at a given timestep is represented by one of many states. Each state holds information about which crop is planted, and whether it is harvestable. We notate the state of greenhouse $i$ at timestep $t$ as $s_{t,i}$, and the the joint-state of all agents at timestep $t$ as $s_t = (s_{t,1}, s_{t,2}, ..., s_{t,n})$. There exists a common state space $S$ such that $s_{t,i} \\in S$ for all agents $i$ and timesteps $t$. Each agent $i$ starts at some state $s_{1,i}$, sampled from $\\mu_i \\in \\Delta(S)$.\nAt every timestep, each agent takes some action. We will notate the action of agent $i$ at timestep $t$ as $a_{t,i}$, and the joint-action of all agents at timestep $t$ as $a_t = (a_{t,1}, a_{t,2}, ..., a_{t,n})$. There exists a common state space $A$ such that $a_{t, i} \\in S$ for all agents $i$ and timesteps $t$. In particular, there is an action $HARVEST \\in A$."}, {"title": "3.2 Objective", "content": "The decision maker's goal is to develop a policy for each agent, which at each timestep takes in a state and returns an action. Formally, the solution will be a collection of mappings $\\pi_{i,t}: S \\rightarrow A$ for all agents $i$ and timesteps $t$. This indicates that if agent $i$'s greenhouse in state $s$ at time $t$, then we recommend that they take action $\\pi_{i,t}(s)$. We will notate the full collection of policies as a joint-policy $\\pi$.\nFarmers cannot observe what is happening in other greenhouses. Therefore, the objective's type signature is limited to finding independent policies for each agent, meaning that each agent decides actions based only on the state of their own greenhouse, and not on the states of other agents. This choice also has advantages for computational complexity, as there are an exponential order more non-agent-independent policies than agent-independent policies.\nWe wish to maximize the total welfare of all agents in our system. In order to define total welfare, we will first define returns for each agent as an expectation of the weighted sum of the rewards received over the horizon, parameterized by some discount factor $\\gamma$. The discount factor weights how much present rewards are prioritized over future rewards. The return of agent $i$ is $g_i = E(\\sum_{t=1}^{T} \\gamma^{t-1}r_{t,i})$. We will notate the joint-returns as $\\hat{g}$, and will sometimes use $g_i(\\pi)$ or $\\hat{g}(\\pi)$, since the returns are fully dependent on $\\pi$.\nNow, we will define the total welfare of all agents as $U = \\prod_{i=1}^{n}(g_i + 1)$. We choose to optimize the product of returns because it combines a preference for all agents have higher returns, and a preference for returns to be distributed equally. We can also think of this as optimizing $\\log(U) = \\sum_{i=1}^{n}(\\log(g_i + 1))$. This is motivated since many studies have shown that happiness has a logarithmic relationship with income. We will sometimes use $U(\\pi)$ notation."}, {"title": "3.3 Independent Q Learning", "content": "IQL adapts single-agent reinforcement learning principles to multi-agent systems (Tan 1993). Each agent independently learns its policy by observing changes in the environment, disregarding the actions of other agents as direct influencing factors. This simplifies the multi-agent problem by treating each agent as if it were operating in a single-agent context, where the focus lies on learning its own Q-function. A Q-function is defined as the expected future rewards for taking a specific action in a given state and following an optimal policy thereafter.\nTo implement IQL, we construct a Q-table that captures these expected rewards for each possible action in each state. The Q-values in this table are continuously updated as the agent gathers new observations, enabling it to better its decision-making over time. We first choose a learning parameter $\\alpha$. Then, for agent $i$, the Q-value update rule is defined as:\n$Q_i(s, a_i) \\leftarrow Q_i(s, a_i) + \\alpha[r_i + \\gamma \\max_{a} Q_i(s', a) - Q_i(s, a_i)]$. (1)\nThe IQLBUFFERPOLICY algorithm is an extension of the IQL approach that introduces two key improvements: optionally initializing the Q-values by solving a Linear Programming (LP) problem using an offline base policy and sampling multiple trajectories for each agent to enhance the policy's learning process over a longer horizon. Please note that we will use IQL to refer to IQLBUFFERPOLICY from here on. Here's a brief explanation of the main methods in the IQL code:\n\\begin{itemize}\n    \\item The Q-tables for each agent are initialized, and if the warm start flag is set, the Q-values are \"warm-started\" using an LPSOLVER base policy. This offline policy computes the initial Q-values by solving for the agent's expected reward in each state based on the transition matrices and reward function of the MDP model. This step significantly accelerates the learning process by providing an improved starting point for Q-values. Further information about the LPSOLVER base policy can be found in Appendix C.\n\\end{itemize}\nTime complexity IQL simplifies multi-agent reinforcement learning by treating each agent independently. For each episode $m \\in M$, we loop through all timesteps $T$, where each agent selects an action and observes a reward based on the current state. At each timestep, each agent incurs a cost of $O(|A| + |S|)$ where $A$ represents action space and $S$ the state space. Thus, the overall time complexity, which is linear in the number of agents and timesteps, is $O(MTN|S||A|)$. Unfortunately, although this approach is computationally efficient and easy to implement and understand, there is no guarantee of convergence after $T$ timesteps. This is due to the non-stationary environment created by the independent learning processes of multiple agents."}, {"title": "3.4 Agent By Agent Policy", "content": "The Agent-By-Agent (ABA) algorithm breaks down the global optimization problem of finding a policy for every agent into multiple local optimization problems of finding the best policy for a single agent, while holding the policies of all other agents constant. This allows us to reduce the multi-agent problem into a single-agent problem, where the $n - 1$ agents not actively being optimized are treated as if they were part of the environment, simply affecting the reward function. This agent-by-agent optimization is related to the family of coordinate descent algorithms, in which one parameter is optimized at a time until convergence. In our case, the \"coordinate\" directions are the single-agent policies $\\pi_i$ for each agent $i$.\nAfter initialization, the algorithm loops until a policy convergence threshold is reached. Within this outer loop, a single agent $i$ is chosen, and then the algorithm finds the welfare-maximizing policy for agent $i$, according to the currently held policies of all other agents. Formally, we seek to find the single-agent policy $\\pi'$ such that $U((\\pi'_i, \\pi)) = max(U)$. We then update $\\pi_i \\leftarrow \\pi'$ and continue to the next iteration, choosing another agent to optimize.\nDynamic Programming The most involved portion of this algorithm comes down to solving for the welfare-maximizing policy for a single-agent, given the policies of all other agents. Our solution takes advantage of the acylicity of timesteps, and uses dynamic programming.\nFor illustration, suppose that we changed our framing of the problem so that we expand our state space to have a state for every $(s, t)$ pair for all $s \\in S, 1 \\leq t \\leq T$. The resulting state transition graph would be acyclic, giving us a topological order with which to process states. If we process states in descending order from $t = T$ to $t = 1$, then by the time our algorithm processes a state $s$, we will have already processed all states $s'$ that $s$ can transition to. Further, if we know the value of state $P(s, a)$ for all actions $a$, we can determine the value of each action $a$ as a sum of the instantaneous reward and the value of our new state $P(s, a)$.\nThus, we are yielded the dynamic programming approach. As we are optimizing $\\pi$ for some single-agent $i$, we define $DP_t(s)$ as the maximal increase to $U$ possible from timestep $t$ onwards, when agent $i$ starts at $s$, and all other agents $j \\neq i$ follow $\\pi$; for every timestep. As stated, we will compute $DP$ values in descending order, so that we have access to values of $DP_{t+1}$ when calculating values of $DP_t$.\nNow, we must find transition equations which relate $DP$ values. Because welfare is non-linear on returns, it is impossible to determine the actual effect of taking a certain action without knowing the returns of all agents across the entire horizon, which is dependent on the joint-policy. Because the whole point of our dynamic programming approach is to find the best $\\pi_i$, we do not have access to $\\pi'$ when we are computing $DP$ values. Thus, we will have to settle for an approximation of the welfare added from an action, which depends only on the parameters accessible when computing $DP$ values: namely $\\pi'$, $s$, $a$, and $t$. We will do this by linear approximation.\nLinear Approximation Because linearity allows us to isolate the effect of a single reward, the approximation we elect will be a first-order linear approximation of the welfare function evaluated at before the current iteration of single-agent optimization. Specifically, we approximate the change in $U$ under some change $r_{t,j} \\leftarrow r_{t,j} + \\epsilon$ as $\\epsilon * \\frac{DU}{dr_{t,j}}$. Evaluating the partial derivatives at the previous version of the joint-policy incentives the optimization to improve upon its prior form.\nThe details of how this is implemented are mathematically involved. Please see the appendix for all of the details on this portion of the algorithm.\nDetails Before the main portion of the algorithm, each agent's policies must be initialized. This can either be done by choosing an arbitrary action $a \\in A$ and set $\\pi_{i,t}(s) \\leftarrow a$ for all $i$, $t$, $s$, or by uniformly randomizing the entire policy table.\nAdditionally, we need a stopping criterion to exit the algorithm's outer loop. A simple choice is to run the loop for a fixed number of iterations $e$ where $e$ is a hyperparameter to be optimized empirically. Another option is to set a parameter $\\delta$ and terminate when the improvement in welfare from our refined policy is less than $\\delta$.\nTo choose which agent is selected to optimized, two choices stand out:\n\\begin{enumerate}\n    \\item Cycle through agents in order 1 through $n$ until convergence. This approach has the drawback of allowing some systematic limitations; for instance, agent $i + 1$ always immediately responds to changes in agent $i$'s policy, whereas the reverse requires iterating through all other agents.\n    \\item Randomly select one of the $n$ agents at each iteration. Randomness often works well in descent-based optimization algorithms, like stochastic gradient descent. If sufficient training iterations are run, agents will be selected roughly equally with high probability.\n\\end{enumerate}\nWhen we can't directly access $R_t$ for future $t$, we approximate it as $R_t$ using observations of past $R_t$, allowing this algorithm to adapt to real-time market updates. There are many ways to do this which could be explored in future research.\nSee Appendix B for the pseudocode for the single-agent optimization subroutine.\nThe time complexity of the entire algorithm is $O(VNT|S||A|)$ where $V$ is the average number of optimizations per agent required for convergence."}, {"title": "3.5 Multi-agent Rollout Policy", "content": "The multi-agent rollout algorithm, introduced by Bertsekas (2021), extends the policy iteration (PI) framework to multi-agent systems. Rollout simplifies the optimization process by iteratively improving policies, optimizing each agent's actions sequentially while considering the fixed decisions of other agents. This sequential optimization significantly reduces computational complexity, making it particularly advantageous for large-scale multi-agent systems. By focusing on one agent at a time, the algorithm leverages the structure of the multi-agent Markov Decision Process (MDP) to approximate near-optimal solutions efficiently.\nTraditional PI algorithms face scalability challenges in multi-agent environments due to the exponential growth in complexity when optimizing all agents' actions simultaneously. The multi-agent rollout method addresses this by performing local optimizations for each agent in a sequential manner, adjusting one agent's actions at a time while keeping the others fixed. As a result, the computational complexity scales linearly with the number of agents, $n$, enhancing feasibility in systems with many agents.\nThis approach also supports real-time applications and dynamic replanning, where decisions must adapt to changing conditions continuously. By incorporating the rollout technique into multi-agent settings, we ensure that each agent optimizes its actions while maintaining coordination with others, ultimately producing globally improved policies that enhance system performance over successive iterations."}, {"title": "4 Experiments", "content": "In this section, we run simulations to address the challenge of optimizing crop planning for small-scale farmers in India, where traditional systems often lead to income loss by failing to account for real-time market supply and demand. In this study, we evaluate three different reinforcement learning policies (IQL, ABA, and ROLLOUT) to measure how effectively they perform in a multi-agent crop planning environment. We test their impact on runtime, reward distribution, discount factors, and slope coefficients. All policies are aimed at maximizing total farmer income while allowing us to observe fairness outcomes among agents. Testing was conducted on a Ryzen 5600H 3.0 GHz processor, without GPU, and the code was written in Python 3.9.\nOur first experiment was total joint reward, which is the sum of farmer rewards. Experiments were set up with 5, 10, 15, and 20 agents, and we measured how much total reward each policy generated. Unless otherwise noted, 14 days passes with each timestep for a total of $T = 26$ timesteps. We set the AR parameter to 0.1 and the slope coefficient to 500.\nFor the runtime analysis, we used a general MDP model to avoid the complexities of greenhouse-specific details. This allowed us to focus on core performance and scalability, providing a clearer and faster comparison of how each policy handles multiple agents without the added complexity of the Greenhouse constraints, such as crop maturity, market interactions, or seasonal factors.\nThe simulation results indicate that ROLLOUT required significantly more runtime as the number of agents increased, reaching over 3500 seconds for 20 agents, as shown in Figure 2. This extended runtime makes ROLLOUT less practical for large-scale applications. In contrast, IQL and ABA (ABA) completed their tasks much faster. IQL exhibited $O(N)$ time complexity, scaling linearly with the number of agents. ABA also showed a manageable runtime with $O(N x S)$ time complexity, where S is the number of states, reflecting its stepwise optimization of each agent's policy through dynamic programming. This complexity is consistent with the results shown, where ABA scales efficiently with increasing agent numbers while balancing computational demand. In summary, while ROLLOUT achieves the highest rewards, its larger time complexity limits scalability. IQL is efficient but sacrifices reward performance, whereas ABA achieves a balance of decent rewards with a reasonable runtime.\nFinally, we examined the effect of different slope coefficients-values ranging from 500 to 1,500\u2014on the total joint reward across 26 horizons, 14 timesteps, and 2 agents. The slope coefficient reflects how sensitive the market is to over-supply; as more agents harvest the same crop, prices drop more sharply with higher slope values. This coefficient directly impacts total reward: if agent i follows the same policy but under different slope coefficients, the total reward can vary because the market price adjusts differently depending on supply sensitivity. Therefore, we interpret each point on the plot in light of this supply sensitivity.\nWith IQL, the reward remains flat across all slope coefficients, showing that IQL's independent agent actions make it unable to adapt to supply-driven price sensitivity. This flat result indicates that the slope coefficient does not influence IQL's total reward outcome. By contrast, ABA shows a steady increase in reward as the slope coefficient rises. This upward trend suggests that ABA can effectively adapt to market conditions, likely by spreading out agents' harvest times to avoid market oversaturation. ROLLOUT, meanwhile, displays a slight dip around a slope coefficient of 1,000, possibly due to challenges in coordinating actions at this particular sensitivity. However, ROLLOUT recovers as the slope coefficient increases, which indicates that it can adjust agents' actions to balance market supply and demand at higher sensitivity values. Overall, ABA and ROLLOUT better handle market dynamics by leveraging coordination, making them more effective in scenarios where agents must balance their actions to avoid oversupply and maintain favorable prices.\nNext, we observed how discount factors (ranging from 0.3 to 0.9) impact the total joint reward for the three policies across 26 horizons, 14 timesteps, and 2 agents. A discount factor represents how much weight the policies place on future rewards compared to immediate ones. ABA and ROLLOUT maintain consistently high rewards, with a slight upward trend as the discount factor increases. These policies, which optimize for joint rewards, are better at managing the long-term impact of actions and coordinating between agents, ensuring that they continue to maximize total rewards even as future rewards become more important. In contrast, IQL reaches its highest performance at a discount factor of around 0.6 but then drops off as the discount factor increases. This behavior likely occurs because IQL operates independently for each agent, meaning that agents do not coordinate their actions. As the discount factor increases, agents focus more on long-term rewards, but since they are not considering the actions of other agents, their decisions negatively affect overall outcomes. This leads to lower joint rewards, particularly at higher discount factors."}, {"title": "5 Conclusion", "content": "In this study, we explored the distinct trade-offs among computational efficiency, reward optimization, and fairness in multi-agent crop planning using different MARL algorithms. IQL initially appeared advantageous due to its computational efficiency; however, as the number of agents increases, IQL's performance declines due to coordination challenges, resulting in lower overall rewards and a less equitable outcome distribution. This difficulty with coordination limits IQL's suitability for multi-agent contexts where fairness is a priority.\nThe ABA policy, which plans actions for each agent in advance, offers a compromise between coordination and computational demand. Unlike IQL, ABA enhances agent coordination by optimizing sequentially, which improves fairness and reward distribution. However, its moderate computational overhead must be weighed against its performance benefits.\nConversely, the multi-agent ROLLOUT policy achieves the highest rewards and equitable outcomes by optimizing joint agent actions simultaneously. This approach, however, significantly increases runtime, making it less feasible for large-scale applications. This trade-off implies that ROLLOUT may only be practical in scenarios with fewer agents or lower computational constraints.\nThe experimental analysis was crucial in clarifying these trade-offs, providing insights into each algorithm's strengths and limitations under varying conditions. Ultimately, these findings underscore that selecting an appropriate MARL approach for agricultural decision-support systems requires balancing efficiency, reward optimization, and fairness according to specific needs. To address the scalability of these algorithms, we recommend further exploration into enhancing MARL policies that can balance fairness and efficiency in resource-constrained environments, ensuring sustainable support for farmers."}, {"title": "A Details On Linear Approximation for Agent by Agent Algorithm", "content": "Here, we will cover in more detail how we obtain a linear approximation for the effect of a change in a single reward for a single agent. Specifically, we approximate the change in U under some change $r_{t,j} \\leftarrow r_{t,j} + \\epsilon$ as $\\epsilon * \\frac{DU}{drt,j}$. We expand out the partial derivative using the multi-variable chain rule. Since $r_{t,j}$ only has an effect on $g_j$, we are left with only one term. We can calculate each of these partial derivatives by consulting their definition equations, and can then combine the entire linear approximation into one concise equation. Again, we will use $r_t$, $\\hat{g}$, and U values derived from previous values of $\\pi$.\n$\\frac{JU}{drt,j} = \\frac{JU}{dgj} \\frac{dgj}{drt,j}$ (8)\n$\\frac{dgj}{drt,j} = \\gamma^{t-1}$ (9)\n$\\frac{JU}{dgj} = \\prod_{1<i<n,i\\neq j} (g_i + 1) = \\frac{U}{g_j + 1}$ (10)\n$\\frac{JU}{drt,j} = \\frac{U}{g_j+1} \\gamma^{t-1}$ (11)\nNow, we have an approximation for the change in U based on some change in $r_{t,j}$ for some agent $j$ and timestep $t$. We now must turn this into an approximation for the instantaneous change in U based on some change in $a_{t,i}$ and $S_{t,i}$ for some $t$. We do this by calculating the effect of the action change on $r_{t,j}$, for all agents $j$, and multiplying each of these terms by the previously calculated effect of $r_{j,t}$ on U, and then taking the sum of these products. We can then manipulate the algebra, to derive $C_{i,t}(s, a)$ defined as the the effect on U of changing $s_{i,t} \\leftarrow s$ and $a_{i,t} \\leftarrow a$.\n$C_{i,t}(s, a) = \\sum_{j=1}^{n} (R_t(s_t, a_t) - R_t((s_{t,-i}, s), (a_{t,-i}, a))) \\frac{JU}{drt,j}$ (12)\n$= (R_t(s_t, a_t) - R_t((s_{t,-i}, s), (a_{t,-i}, a))) \\cdot$ (13)\n$= (R_t(s_t, a_t) - R_t((s_{t,-i}, s), (a_{t,-i}, a))) \\frac{U\\gamma^{t-1}}{g+1}$ (14)\n$= (R_t(s_t, a_t) - R_t((s_{t,-i}, s), (a_{t,-i}, a))) \\frac{U\\gamma^{t-1}}{g+1}$ (15)\nThe above expression is now dependent only on the state and action trajectories, which can be simulated using the previous version of the joint-policy before computing DP values. Putting this altogether, we get the DP transitions we need.\n$DP_t(s) = max_{a} (C_{i,t}(s, a) + DP_{t+1}(P_t(s, a)))$ (16)\nAs a base case, DP values for t = T should have no term represent future welfare improvement, because the MDP is at the last step of the horizon. The transitions here are thus simply:\n$DP_T(s) = max_{a} (C_{i,t}(s, a))$ (17)\nOnce we have calculated all DP values, we can easily recover the single-agent-optimized policy by from each timestep choosing the action that yielded the best improvement according to the transition equation. Specifically, we will take\n$\\pi_{i,t}(s) = argmax_{a} (C_{i,t}(s, a) + DP_{t+1}(P_t(s, a)))$ (18)"}, {"title": "B Pseudocode For Single-Agent Optimization Subroutine in Agent by Agent Algorithm", "content": "The following pseudocode is the implementation of line 18 in Algorithm 2."}, {"title": "C Details of LPSolver Base Policy", "content": "LPSolver Base Policy for Multiple MDPs. The LPSOLVER policy is a specific implementation of the base policy, solving each MDP as a linear program to minimize expected cumulative discounted cost. The value function $V_i^(s)$ for each state s and agent i is obtained by solving:\nMinimize $\\sum_{s \\in S_i} V_i(s)$ (19)\nSubject to $V_i(s) \\geq r_i(s, a) + \\gamma \\sum_{s' \\in S} P_i(s', s, a)V_i(s'), \\forall s, a$. (20)\nThe stationary policy $\\pi_i(s)$ is then:\n$\\pi_i(s) = arg max_{a \\in A_i(s)} \\bigg[ r_i(s, a) + \\gamma \\sum_{s'} P_i(s' s, a)v_i(s') \\bigg]$. (21)"}]}