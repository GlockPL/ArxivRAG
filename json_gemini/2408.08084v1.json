{"title": "An Efficient Replay for Class-Incremental Learning with Pre-trained Models", "authors": ["Weimin Yin", "Bin Chen", "Chunzhao Xie", "Zhenhao Tan"], "abstract": "In general class-incremental learning, researchers typically\nuse sample sets as a tool to avoid catastrophic forgetting dur-\ning continuous learning. At the same time, researchers have\nalso noted the differences between class-incremental learning\nand Oracle training and have attempted to make corrections.\nIn recent years, researchers have begun to develop class-\nincremental learning algorithms utilizing pre-trained mod-\nels, achieving significant results. This paper observes that in\nclass-incremental learning, the steady state among the weight\nguided by each class center is disrupted, which is significantly\ncorrelated with catastrophic forgetting. Based on this, we pro-\npose a new method to overcoming forgetting. In some cases,\nby retaining only a single sample unit of each class in mem-\nory for replay and applying simple gradient constraints, very\ngood results can be achieved. Experimental results indicate\nthat under the condition of pre-trained models, our method\ncan achieve competitive performance with very low compu-\ntational cost and by simply using the cross-entropy loss.", "sections": [{"title": "Introduction", "content": "In the real world, data is continuously and dynamically gen-\nerated, while the current mainstream training methods re-\nquire pre-collecting large amounts of data. However, when a\nmodel is trained on non-stationary data, with different clas-\nsification tasks arriving sequentially, it leads to catastrophic\nforgetting (McCloskey and Cohen 1989), resulting in a de-\ncline in performance on previously learned data. This un-\ndoubtedly limits the flexibility of current artificial intelli-\ngence to some extent.\nResearch on catastrophic forgetting is widely distributed\nin the fields of continual learning or incremental learning.\nRecently, pre-trained models have been shown to be benefi-\ncial for continual learning, and many methods have achieved\nsignificant results. However, to further study catastrophic\nforgetting, we need to temporarily return to the perspective\nof classical research. To overcome catastrophic forgetting,\nextensive research has focused on continuously adjusting\nthe entire or partial weights of the model as the data dis-\ntribution changes, aiming to retain knowledge from previous\ntasks (Zhou et al. 2023; De Lange et al. 2021). This typically\nrelies on a sample buffer to retrain portions of past samples,\nand this method is believed to be rooted in the complemen-\ntary learning systems theory (Kumaran, Hassabis, and Mc-\nClelland 2016), inspired by hippocampal episodic memory.\nOn the other hand, since catastrophic forgetting is a phe-\nnomenon arising from atypical training processes, it can be\nviewed as the abnormal variation of the forgetting model's\nparameters relative to normal training. This branch of re-\nsearch (Castro et al. 2018; Wu et al. 2019; Hou et al. 2019)\nseeks to rectify forgetting issues by detecting differences be-\ntween normal training and the training process that leads to\nforgetting, with the main goal being to align the form of\nthe forgetting model with the normal model during training.\nNevertheless, the primary defense against forgetting remains\nthe use of a sample buffer.\nBased on previous research, we raise the following ques-\ntion: If the use of a sample buffer is merely to make the\nforgetting model closer to the normally trained model after\ntraining, can we, through another method, quickly eliminate\nthis difference once the parameter differences between the\nforgetting model and the normal model are identified, with-\nout the need to retain a large number of historical samples?\nTo address this question, we draw inspiration from recent\nresearch on model rectify in the field of continuous learning."}, {"title": "Related Work", "content": "Here, we compare our method with related work and discuss\ntheir differences. The approaches to solving or circumvent-\ning the issue of catastrophic forgetting can be divided into\nfour main categories:\nParameter Regularization-Based:These methods at-\ntempt to measure the impact of each parameter on the net-\nwork's importance and protect acquired knowledge by pre-\nserving the invariance of critical parameters (Kirkpatrick\net al. 2017; Chaudhry et al. 2018a; Zenke, Poole, and Gan-\nguli 2017). Although these methods address catastrophic\nforgetting by estimating and calculating parameter impor-\ntance without using a sample buffer, they fail to achieve\nsatisfactory performance under more challenging conditions\n(Rebuffi et al. 2017). In contrast, our method directly con-\ntrols parameter updates to mitigate the effects of catastrophic\nforgetting, rather than achieving this indirectly through reg-\nularization penalties.\nSample Buffer-Based:These methods build a sample\nbuffer to store samples from old tasks (Chaudhry et al.\n2018b), or use generative networks to generate samples\n(Shin et al. 2017), for training alongside data from new\ntasks. Additionally, many methods have improved on this\nsimple and effective idea by using techniques such as knowl-\nedge distillation (Buzzega et al. 2020; Rebuffi et al. 2017)\nand model rectify methods (Zhao et al. 2020; Hou et al.\n2019). Sample buffer-based methods have achieved leading\nperformance in various benchmarks in the past (Mai et al.\n2022). However, the performance of these methods gener-\nally decreases as the buffer size reduces (Cha, Lee, and Shin\n2021), and they are severely limited when considering secu-\nrity and privacy concerns (Chamikara et al. 2018). In con-\ntrast to store historical data, our method compresses past\nknowledge into minimal memory vectors, which guide the\nbalanced fine-tuning of weights when training new tasks.\nThis balance greatly impacts the occurrence of forgetting.\nOur method requires minimal historical information to sig-\nnificantly mitigate catastrophic forgetting, suggesting that\nthere may still be room for optimization in sample buffer-\nbased methods.\nArchitecture-Based: These methods aim to introduce an\nadditional component to store knowledge when encounter-\ning a new task. This component can be realized by copying"}, {"title": "Prerequisites", "content": "Continual learning protocols\nContinual learning is typically defined as training a ma-\nchine learning model on non-stationary data from a se-\nquence of tasks. Consider a series of B training task datasets\n${D_1, D_2, ..., D_B}$, where $D_b = {{(x,y)}_i}^K_{i=1}$ represents\nthe b-th task in the sequence, containing K training in-\nstances. Each instance x belongs to a label $y_i \\in Y_b$, with $Y_b$\nbeing the label space for task b. For any $b \\neq b', Y_b \\cap Y_{b'} = 0$.\nThe objective is to train a single model $f (x; \\theta) : X \\rightarrow Y$,\nparameterized by $\\theta$, to predict the label $y = f(x;\\theta) \\in Y$\nfor any sample x from any previously learned task. During\nthe training of task b, the model may only have access to the\ndata in $D_b$.\nDepending on the environmental settings, common con-\ntinual learning scenarios are divided into task-incremental,\nclass-incremental, and domain-incremental learning. Task-\nincremental learning assumes that the task identity is known\nduring testing. Domain-incremental learning maintains the\nsame set of classes for each task, only changing the distribu-\ntion of x between tasks. The goal of class-incremental learn-\ning is to continuously build a classifier that covers all classes\nencountered across tasks. This means that the model needs\nto learn new knowledge from task b while retaining knowl-\nedge from previous tasks. Our paper addresses the more\nchallenging class-incremental learning setting.\nBenchmark for Continual Learning with\nPretrained Models\nPrevious research typically trained a continual learning\nmodel from scratch. Recently, researchers have focused\non designing continual learning algorithms based on pre-\ntrained models (PTMs). Many studies have demonstrated\nthat PTM-based continual learning methods can achieve\nexcellent performance without the need for large sample\nbuffers, thanks to the strong generalization capabilities of\nPTMs. From the perspective of representation learning,\nthe essence of model training is to learn appropriate rep-\nresentations for each task, and a powerful PTM accom-\nplishes this work from the very beginning. Therefore, the\nfocus of continual learning has shifted to overcoming catas-\ntrophic forgetting. For example, techniques such as visual\nprompt tuning (Jia et al. 2022) and adapter learning (Chen\net al. 2022) can quickly adapt PTMs to downstream tasks\nwhile maintaining generalization. Consequently, compared\nto training from scratch, using PTMs in continual learn-\ning shows stronger performance in resisting forgetting (Cao\net al. 2023).\nGiven the powerful representation capabilities of PTMs,\nif different PTMs lead to varying performance outcomes,\nhow can we determine whether the differences are due to\nthe algorithm or the PTM itself? In other words, how can\nwe measure the fundamental continual learning ability pro-\nvided by a PTM within an algorithm? SimpleCIL (Zhou\net al. 2024a)proposes a straightforward approach to achieve\nthis. When faced with a continuous data stream, it freezes\nthe pre-trained weights and extracts the center $c_i$ for each"}, {"title": "Weight Balanced Replay", "content": "From classifier bias to network bias\nAn intuitive approach to quickly align the parameter distri-\nbutions of a forgetting model and a normal model is to iden-\ntify and correct the biases between them. Thus, the focus\nlies in uncovering the distribution patterns of these biases.\nEarlier studies have identified traces of such patterns in the\nembedding module and classifier. For example, the embed-\nding function outputs in a forgetting model are more con-\ncentrated (Shi et al. 2022), the logits of new tasks are signif-\nicantly higher than those of old tasks, and most importantly,\nthe weight of new tasks in the classifier is greater than that\nof old tasks, which can lead to other forgetting character-\nistics. Furthermore, we hypothesize that all parameters in a\nforgetting model may follow this pattern. Ideally, we aim\nto discover a forgetting pattern for parameters that applies\nto any model optimized using backpropagation and gradient\ndescent. Therefore, we define the continual learning process\nat stage b as $f_{b-1}(x; \\theta)$, where $f_{b-1}$ represents the model\nafter completing the previous stage of continual learning,\nand the corresponding normal training process is denoted as\n$f_{D_1U...UD_B}(x; \\theta)$, which involves training on data from $D_1$\nto $D_B$. The goal of overcoming forgetting can thus be defined\nas:\n$f_{D_1U...UD_B}(x; \\theta) \\leftarrow f_{b-1}(x; \\theta),$\nSpecifically, let $w^b$ denote the weights of $f_{b-1}$ after super-\nvised learning on $D_b$, where these weights may exhibit for-\ngetting. Correspondingly, let $w^0$ represent the weights ob-\ntained from $D_1U...UD_B$ trained without forgetting. The bias\nbetween them can then be defined as:\n$w^0 \\leftarrow w^b + \\Delta w^b,$\nPrevious work has focused on linearly adjusting classifier\nparameters to mitigate forgetting. For instance, some ap-\nproaches use a cosine classifier to avoid biases in classifier\nweights (Hou et al. 2019), apply weight normalization af-\nter training (Zhao et al. 2020), or use additional parameters\nto scale classifier weights during training (Wu et al. 2019).\nIn contrast, our approach aims to swiftly correct the biases\ncaused by forgetting. It is important to note that we omit the\nbias term in the notation, as the impact of the weights on\ncatastrophic forgetting is sufficiently significant.\nApproximate bias and balance\nWe designed a replay-based approximate correction strategy\nto dynamically adjust the parameters during new task train-\ning (see Figure 2). This correction mechanism shares some\ndesign principles with sample buffering methods, which typ-\nically maintain a portion of original samples in memory to\ncombat forgetting. However, our approach aims to obtain\nprior knowledge at minimal cost to quickly compute the bias\n$\\Delta w^b$. We associate $\\Delta w^b$ with each previously learned task.\nIf this bias is distributed across each training iteration in the\nb-th stage of continual learning, considering the additivity of"}, {"title": "Optimization objective for WBR", "content": "Following the above strategy, the memory information $a^*$ for\neach task only needs to be calculated once and can be easily\ncompleted during its respective continual learning training\nphase, as training typically involves at least one pass through\nthe data. In each training step, the data x from the current\ntask is input into the model alongside the memory informa-\ntion $x^*$ for supervised learning. While seeking weight bal-\nance, our objective is to minimize the end-to-end training\nloss function:\n$\\min\\limits_{\\theta} L(f(x; \\theta), y) + \\sum\\limits_{i=1}^{b-1}L(f(x^i; \\theta),y),$ s.t. $Clip(f(x; \\theta), \\alpha), Clip(f(x^*; \\theta), \\beta),$\nBoth losses are computed using softmax cross-entropy and\nare controlled by the gradient clipping function to limit the\ngradient magnitude. If a pretrained model g(x) is used, the\nloss function becomes:\n$\\min\\limits_{\\theta} L(f(g(x); \\theta), y) + \\sum\\limits_{i=1}^{b-1}L(f(g(x); \\theta), y),$ s.t. $Clip(f(g(x); \\theta), \\alpha), Clip(f(g(x)^*; \\theta), \\beta),$\ng(x) indicates that g(x) is used in place of x to compute\nthe memory information.\nExperiments\nTo evaluate the proposed WBR method, we strictly followed\nthe experimental setup of previous works (Rebuffi et al.\n2017; Zhou et al. 2024c) and conducted experiments in a\nclass-incremental learning setting, where the task identity is\nunknown during inference. Specifically: (1) We conducted\nablation experiments in a simplified environment to more\nintuitively understand the relationship between this balance\nand catastrophic forgetting. (2) We compared our method\nwith various state-of-the-art approaches on the latest pre-\ntrained model-based continual learning baseline (Zhou et al.\n2024a). Finally, we discussed the results of WBR compared\nto other methods, as well as its implications for addressing\ncatastrophic forgetting."}, {"title": "Comparing methods", "content": "We compared the WBR method with multiple baseline and\nstate-of-the-art continual learning methods. Our approach is\nbased on the pretrained ViT-B-16 model (Dosovitskiy et al.\n2020), which has become a common tool in the field of\npretrained model-based continual learning. To ensure a fair\ncomparison, we utilized the latest pretrained continual learn-\ning baseline, SimpleCIL, which allows us to evaluate algo-\nrithm performance across different pretrained models. We\nreferred to recent review papers (Zhou et al. 2024b, 2023)\nand the latest research works, selecting the most recognized\nor best-performing methods in each domain.\nBaseline Methods. Fine-tuning involves sequential su-\npervised learning on all continual learning data and is typ-\nically regarded as a scenario where catastrophic forgetting\noccurs completely, serving as a classical baseline for contin-\nual learning. EWC (Kirkpatrick et al. 2017) is a representa-\ntive method based on parameter regularization. SimpleCIL\nis a baseline method for pretrained model-based continual\nlearning, representing the performance that can be achieved\nthrough the inherent generalization capabilities of pretrained\nmodels without learning any additional parameters, includ-\ning the classifier.\nSOTA Sample Buffer-Based Methods. We selected\nthree state-of-the-art sample buffer-based methods for com-\nparison, including iCaRL (Rebuffi et al. 2017), BiC (Wu\net al. 2019), and WA (Zhao et al. 2020). iCaRL is a rep-\nresentative method that improves upon sample buffers using\nknowledge distillation, while WA is a representative method\nthat further rectify the model based on distillation.\nSOTA Architecture-Based Methods. We selected two\narchitecture-based methods that do not use pretrained mod-\nels for comparison: DER (Yan, Xie, and He 2021) and\nMEMO (Zhou et al. 2022). These methods neither use sam-\nple buffers nor rely on pretrained models, making them typ-\nical examples of architecture expansion approaches.\nSOTA Pretrained Model-Based Methods. We selected\ntwo state-of-the-art methods that use frozen pretrained mod-\nels for comparison: ADAM (Zhou et al. 2024a) and EASE\n(Zhou et al. 2024c). Both methods utilize additional adapters\nfor fine-tuning.\nOur Method. WBR is the method we propose. Unlike"}, {"title": "Main results", "content": "Results in the Non-Pretrained Environment. Overall, the results indicate that sig-\nnificant forgetting can be mitigated without the need for ex-\ntensive mixed supervision training; maintaining a balance\nbetween the weights of old task and the new task is suffi-\ncient. Specifically: (1) It can be observed that as the depth\nof the network increases, the impact of this balance on for-\ngetting significantly diminishes. (2) The learning rate also\nhas a notable effect on this balance: too high a learning\nrate exacerbates forgetting, while too low a learning rate\nhinders the timely acquisition of new knowledge. An ap-\npropriate learning rate keeps learning and forgetting in a\nrelatively stable state. (3) Catastrophic forgetting is highly\ncorrelated with this balance. Using the analogy of stability\nversus plasticity is apt: when $\\alpha$ is greater than $\\beta$, classical\ncatastrophic forgetting occurs, meaning plasticity outweighs\nstability; when $\\alpha$ is less than $\\beta$, learning efficiency is low,\nmeaning stability outweighs plasticity. When $\\alpha$ equals $\\beta$, a\nbalance is achieved, effectively controlling forgetting. How-\never, we can also observe that even without applying bal-\nance constraints, simply incorporating memory vectors into\nthe training process has already mitigated forgetting to some\nextent. Additionally, since the gradients of new tasks are typ-\nically higher than those of the memory vectors during ac-\ntual training, adjusting the gradient constraints for new tasks\nalone is more efficient.\nResults in the Pretrained Environment.\nsummarizes the results on the class-incremental benchmarks."}, {"title": "Limitations", "content": "As shown in Figure 3, with increasing network depth, the\ninfluence of the memory vector from the first layer on the\nsubsequent layers weakens, which contradicts the advan-\ntages of deep learning. The power of deep learning comes\nfrom the nonlinear stacking of multiple layers, which results\nin strong fitting capabilities, something that a single-layer\nnetwork lacks. This is precisely why WBR is well-suited\nfor pretrained models; it only needs to address forgetting\nlocally, without dealing with the diminishing influence in\ndeeper networks.\nFurthermore, using only MLPs for experiments is, strictly\nspeaking, incomplete. We have demonstrated the correla-\ntion between catastrophic forgetting and this weight bal-\nance from the perspective of backpropagation and gradient\ndescent. However, in more complex architectures, such as\nconvolutional or attention modules, we do not know how\nto design this balance. More critically, if we were to store\nmemory vectors for each layer of the network, a significant\nchallenge would be that after backpropagation updates, the\nmemory vectors for deeper layers could become invalid due\nto changes in the earlier layers. It is foreseeable that attempt-\ning to address these challenges in practice would involve sig-\nnificant complexity and engineering effort. Perhaps, instead\nof designing more intricate training processes, it might be\nbetter to explore an algorithm different from backpropaga-\ntion."}, {"title": "Conclusion", "content": "This paper proposes a novel method to overcome catas-\ntrophic forgetting, aiming to demonstrate that imbalances\nin any weights, not just in the classifier, are one of the di-\nrect causes of catastrophic forgetting. WBR extends the con-\ncept of classifier bias to arbitrary weights and introduces\nthe use of approximate information instead of sample infor-\nmation to compute weight biases, while controlling weight\nbalance during training through gradient constraints. This\napproach effectively and rapidly corrects the weight biases\ncaused by catastrophic forgetting. The method significantly\noutperforms previous state-of-the-art approaches in training\nspeed for class-incremental learning without sacrificing per-\nformance."}]}