{"title": "An Efficient Replay for Class-Incremental Learning with Pre-trained Models", "authors": ["Weimin Yin", "Bin Chen", "Chunzhao Xie", "Zhenhao Tan"], "abstract": "In general class-incremental learning, researchers typically use sample sets as a tool to avoid catastrophic forgetting during continuous learning. At the same time, researchers have also noted the differences between class-incremental learning and Oracle training and have attempted to make corrections. In recent years, researchers have begun to develop class-incremental learning algorithms utilizing pre-trained models, achieving significant results. This paper observes that in class-incremental learning, the steady state among the weight guided by each class center is disrupted, which is significantly correlated with catastrophic forgetting. Based on this, we propose a new method to overcoming forgetting. In some cases, by retaining only a single sample unit of each class in memory for replay and applying simple gradient constraints, very good results can be achieved. Experimental results indicate that under the condition of pre-trained models, our method can achieve competitive performance with very low computational cost and by simply using the cross-entropy loss.", "sections": [{"title": "Introduction", "content": "In the real world, data is continuously and dynamically generated, while the current mainstream training methods require pre-collecting large amounts of data. However, when a model is trained on non-stationary data, with different classification tasks arriving sequentially, it leads to catastrophic forgetting (McCloskey and Cohen 1989), resulting in a decline in performance on previously learned data. This undoubtedly limits the flexibility of current artificial intelligence to some extent.\nResearch on catastrophic forgetting is widely distributed in the fields of continual learning or incremental learning. Recently, pre-trained models have been shown to be beneficial for continual learning, and many methods have achieved significant results. However, to further study catastrophic forgetting, we need to temporarily return to the perspective of classical research. To overcome catastrophic forgetting, extensive research has focused on continuously adjusting the entire or partial weights of the model as the data distribution changes, aiming to retain knowledge from previous tasks (Zhou et al. 2023; De Lange et al. 2021). This typically relies on a sample buffer to retrain portions of past samples, and this method is believed to be rooted in the complementary learning systems theory (Kumaran, Hassabis, and McClelland 2016), inspired by hippocampal episodic memory. On the other hand, since catastrophic forgetting is a phenomenon arising from atypical training processes, it can be viewed as the abnormal variation of the forgetting model's parameters relative to normal training. This branch of research (Castro et al. 2018; Wu et al. 2019; Hou et al. 2019) seeks to rectify forgetting issues by detecting differences between normal training and the training process that leads to forgetting, with the main goal being to align the form of the forgetting model with the normal model during training. Nevertheless, the primary defense against forgetting remains the use of a sample buffer.\nBased on previous research, we raise the following question: If the use of a sample buffer is merely to make the forgetting model closer to the normally trained model after training, can we, through another method, quickly eliminate this difference once the parameter differences between the forgetting model and the normal model are identified, without the need to retain a large number of historical samples? To address this question, we draw inspiration from recent research on model rectify in the field of continuous learning."}, {"title": "Related Work", "content": "Here, we compare our method with related work and discuss their differences. The approaches to solving or circumventing the issue of catastrophic forgetting can be divided into four main categories:\nParameter Regularization-Based:These methods attempt to measure the impact of each parameter on the network's importance and protect acquired knowledge by preserving the invariance of critical parameters (Kirkpatrick et al. 2017; Chaudhry et al. 2018a; Zenke, Poole, and Ganguli 2017). Although these methods address catastrophic forgetting by estimating and calculating parameter importance without using a sample buffer, they fail to achieve satisfactory performance under more challenging conditions (Rebuffi et al. 2017). In contrast, our method directly controls parameter updates to mitigate the effects of catastrophic forgetting, rather than achieving this indirectly through regularization penalties.\nSample Buffer-Based:These methods build a sample buffer to store samples from old tasks (Chaudhry et al. 2018b), or use generative networks to generate samples (Shin et al. 2017), for training alongside data from new tasks. Additionally, many methods have improved on this simple and effective idea by using techniques such as knowledge distillation (Buzzega et al. 2020; Rebuffi et al. 2017) and model rectify methods (Zhao et al. 2020; Hou et al. 2019). Sample buffer-based methods have achieved leading performance in various benchmarks in the past (Mai et al. 2022). However, the performance of these methods generally decreases as the buffer size reduces (Cha, Lee, and Shin 2021), and they are severely limited when considering security and privacy concerns (Chamikara et al. 2018). In contrast to store historical data, our method compresses past knowledge into minimal memory vectors, which guide the balanced fine-tuning of weights when training new tasks. This balance greatly impacts the occurrence of forgetting. Our method requires minimal historical information to significantly mitigate catastrophic forgetting, suggesting that there may still be room for optimization in sample buffer-based methods.\nArchitecture-Based: These methods aim to introduce an additional component to store knowledge when encountering a new task. This component can be realized by copying and expanding the network (Yan, Xie, and He 2021; Wang et al. 2022a; Zhou et al. 2022), or by dividing the network into more sub-networks (Ke, Liu, and Huang 2020). However, these methods require a large number of additional parameters, with early methods even saving a backbone network for each task. To achieve scalability under limited memory budgets, some improved methods still use sample buffers (e.g., knowledge distillation) to reduce redundancy. In contrast, WBR does not add any extra parameters and is fundamentally different from architecture-based methods in concept: WBR focuses on integrating new and old knowledge through fine-tuning weights without changing the network architecture. Most architecture-based methods aim to provide additional parameters to preserve knowledge. Additionally, there is a branch based on meta-learning (Beaulieu et al. 2020), which attempts to enable the overall architecture to overcome forgetting by providing additional meta-learning components through meta-training. This is conceptually similar to our method, except that we provide prior knowledge, whereas they hope to obtain this knowledge through meta-learning.\nPretrained Models-Based:These methods leverage the powerful representation capabilities of pretrained models for continual learning. Prompt-based methods establish connections between pretrained knowledge and continual learning tasks by designing prompt pools, selecting prompts (Wang et al. 2022b), and combining prompts (Smith et al. 2023). In addition, some studies extend prompt selection methods by incorporating multimodal information. Instead of manual selection, some studies utilize multimodal information to select appropriate prompts (Razdaibiedina et al. 2023). Model-mixing methods aim to create a set of models during continual learning and perform model ensemble (Wang et al. 2023) or model merging (Gao et al. 2023) during inference. This type of method combines the strengths of multiple models, reducing the problem of catastrophic forgetting. However, creating and maintaining multiple models increases computational and storage costs, especially in resource-limited environments. Representation-based methods seek to exploit pretrained features for continual learning, bearing some relation to our approach. By adjusting the embedding function with a small learning rate and the classifier with a large learning rate, features can gradually fit while the classifier quickly adapts. After training, modeling and replaying class feature distributions can calibrate the classifier to resist forgetting (Zhang et al. 2023). Other studies achieve state-of-the-art performance by efficiently tuning the pretrained model with additional parameter-efficient modules (Jia et al. 2022; Chen et al. 2022) or connecting feature representations of multiple task-specific backbone networks (Zhou et al. 2024c). Unlike these methods, our approach directly adjusts classifier weights during continual learning training without post-processing step, significantly simplifying the process."}, {"title": "Prerequisites", "content": "Continual learning protocols\nContinual learning is typically defined as training a machine learning model on non-stationary data from a sequence of tasks. Consider a series of B training task datasets {D1, D2, ..., DB}, where D\u2081 = {(x,y)}\u2081 represents the b-th task in the sequence, containing no training instances. Each instance x belongs to a label yi \u2208 Y\u266d, with Yo being the label space for task b. For any b \u2260 b', Y\u266d\u2229Y\u2081 = 0. The objective is to train a single model f (x; 0) : X \u2192 Y, parameterized by 0, to predict the label y = f(x;0) \u2208 Y for any sample x from any previously learned task. During the training of task b, the model may only have access to the data in Db.\nDepending on the environmental settings, common continual learning scenarios are divided into task-incremental, class-incremental, and domain-incremental learning. Task-incremental learning assumes that the task identity is known during testing. Domain-incremental learning maintains the same set of classes for each task, only changing the distribution of x between tasks. The goal of class-incremental learning is to continuously build a classifier that covers all classes encountered across tasks. This means that the model needs to learn new knowledge from task b while retaining knowledge from previous tasks. Our paper addresses the more challenging class-incremental learning setting.\nBenchmark for Continual Learning with\nPretrained Models\nPrevious research typically trained a continual learning model from scratch. Recently, researchers have focused on designing continual learning algorithms based on pretrained models (PTMs). Many studies have demonstrated that PTM-based continual learning methods can achieve excellent performance without the need for large sample buffers, thanks to the strong generalization capabilities of PTMs. From the perspective of representation learning, the essence of model training is to learn appropriate representations for each task, and a powerful PTM accomplishes this work from the very beginning. Therefore, the focus of continual learning has shifted to overcoming catastrophic forgetting. For example, techniques such as visual prompt tuning (Jia et al. 2022) and adapter learning (Chen et al. 2022) can quickly adapt PTMs to downstream tasks while maintaining generalization. Consequently, compared to training from scratch, using PTMs in continual learning shows stronger performance in resisting forgetting (Cao et al. 2023).\nGiven the powerful representation capabilities of PTMs, if different PTMs lead to varying performance outcomes, how can we determine whether the differences are due to the algorithm or the PTM itself? In other words, how can we measure the fundamental continual learning ability provided by a PTM within an algorithm? SimpleCIL (Zhou et al. 2024a)proposes a straightforward approach to achieve this. When faced with a continuous data stream, it freezes the pre-trained weights and extracts the center ci for each"}, {"title": "Weight Balanced Replay", "content": "From classifier bias to network bias\nAn intuitive approach to quickly align the parameter distributions of a forgetting model and a normal model is to identify and correct the biases between them. Thus, the focus lies in uncovering the distribution patterns of these biases. Earlier studies have identified traces of such patterns in the embedding module and classifier. For example, the embedding function outputs in a forgetting model are more concentrated (Shi et al. 2022), the logits of new tasks are significantly higher than those of old tasks, and most importantly, the weight of new tasks in the classifier is greater than that of old tasks, which can lead to other forgetting characteristics. Furthermore, we hypothesize that all parameters in a forgetting model may follow this pattern. Ideally, we aim to discover a forgetting pattern for parameters that applies to any model optimized using backpropagation and gradient descent. Therefore, we define the continual learning process at stage b as fb1(x; 0), where fb-1 represents the model after completing the previous stage of continual learning, and the corresponding normal training process is denoted as fD\u2081U...UD, (x; 0), which involves training on data from D\u2081 to Dr. The goal of overcoming forgetting can thus be defined as:\n\nSpecifically, let wb denote the weights of fb-1 after supervised learning on D\u266d, where these weights may exhibit forgetting. Correspondingly, let w\u00ba represent the weights obtained from D\u2081U\u2026\u2026UD, trained without forgetting. The bias between them can then be defined as:\n\nPrevious work has focused on linearly adjusting classifier parameters to mitigate forgetting. For instance, some approaches use a cosine classifier to avoid biases in classifier weights (Hou et al. 2019), apply weight normalization after training (Zhao et al. 2020), or use additional parameters to scale classifier weights during training (Wu et al. 2019). In contrast, our approach aims to swiftly correct the biases caused by forgetting. It is important to note that we omit the bias term in the notation, as the impact of the weights on catastrophic forgetting is sufficiently significant.\nApproximate bias and balance\nWe designed a replay-based approximate correction strategy to dynamically adjust the parameters during new task training (see Figure 2). This correction mechanism shares some design principles with sample buffering methods, which typically maintain a portion of original samples in memory to combat forgetting. However, our approach aims to obtain prior knowledge at minimal cost to quickly compute the bias \u0394w. We associate \u2206wb with each previously learned task. If this bias is distributed across each training iteration in the b-th stage of continual learning, considering the additivity of"}, {"title": "Optimization objective for WBR", "content": "Following the above strategy, the memory information a* for each task only needs to be calculated once and can be easily completed during its respective continual learning training phase, as training typically involves at least one pass through the data. In each training step, the data x from the current task is input into the model alongside the memory information x for supervised learning. While seeking weight balance, our objective is to minimize the end-to-end training loss function:\n\nBoth losses are computed using softmax cross-entropy and are controlled by the gradient clipping function to limit the gradient magnitude. If a pretrained model g(x) is used, the loss function becomes:"}, {"title": "Experiments", "content": "To evaluate the proposed WBR method, we strictly followed the experimental setup of previous works (Rebuffi et al. 2017; Zhou et al. 2024c) and conducted experiments in a class-incremental learning setting, where the task identity is unknown during inference. Specifically: (1) We conducted ablation experiments in a simplified environment to more intuitively understand the relationship between this balance and catastrophic forgetting. (2) We compared our method with various state-of-the-art approaches on the latest pretrained model-based continual learning baseline (Zhou et al. 2024a). Finally, we discussed the results of WBR compared to other methods, as well as its implications for addressing catastrophic forgetting."}, {"title": "Main results", "content": "Results in the Non-Pretrained Environment. Figure 3 and Table 1 illustrates the performance of WBR in a non-pretrained environment. Overall, the results indicate that significant forgetting can be mitigated without the need for extensive mixed supervision training; maintaining a balance between the weights of old task and the new task is sufficient. Specifically: (1) It can be observed that as the depth of the network increases, the impact of this balance on forgetting significantly diminishes. (2) The learning rate also has a notable effect on this balance: too high a learning rate exacerbates forgetting, while too low a learning rate hinders the timely acquisition of new knowledge. An appropriate learning rate keeps learning and forgetting in a relatively stable state. (3) Catastrophic forgetting is highly correlated with this balance. Using the analogy of stability versus plasticity is apt: when a is greater than B, classical catastrophic forgetting occurs, meaning plasticity outweighs stability; when a is less than 3, learning efficiency is low, meaning stability outweighs plasticity. When a equals \u03b2, a balance is achieved, effectively controlling forgetting. However, we can also observe that even without applying balance constraints, simply incorporating memory vectors into the training process has already mitigated forgetting to some extent. Additionally, since the gradients of new tasks are typically higher than those of the memory vectors during actual training, adjusting the gradient constraints for new tasks alone is more efficient.\nResults in the Pretrained Environment. Table 2 summarizes the results on the class-incremental benchmarks."}, {"title": "Limitations", "content": "As shown in Figure 3, with increasing network depth, the influence of the memory vector from the first layer on the subsequent layers weakens, which contradicts the advantages of deep learning. The power of deep learning comes from the nonlinear stacking of multiple layers, which results in strong fitting capabilities, something that a single-layer network lacks. This is precisely why WBR is well-suited for pretrained models; it only needs to address forgetting locally, without dealing with the diminishing influence in deeper networks.\nFurthermore, using only MLPs for experiments is, strictly speaking, incomplete. We have demonstrated the correlation between catastrophic forgetting and this weight balance from the perspective of backpropagation and gradient descent. However, in more complex architectures, such as convolutional or attention modules, we do not know how to design this balance. More critically, if we were to store memory vectors for each layer of the network, a significant challenge would be that after backpropagation updates, the memory vectors for deeper layers could become invalid due to changes in the earlier layers. It is foreseeable that attempting to address these challenges in practice would involve significant complexity and engineering effort. Perhaps, instead of designing more intricate training processes, it might be better to explore an algorithm different from backpropagation."}, {"title": "Conclusion", "content": "This paper proposes a novel method to overcome catastrophic forgetting, aiming to demonstrate that imbalances in any weights, not just in the classifier, are one of the direct causes of catastrophic forgetting. WBR extends the concept of classifier bias to arbitrary weights and introduces the use of approximate information instead of sample information to compute weight biases, while controlling weight balance during training through gradient constraints. This approach effectively and rapidly corrects the weight biases caused by catastrophic forgetting. The method significantly outperforms previous state-of-the-art approaches in training speed for class-incremental learning without sacrificing performance."}]}