{"title": "Risk Alignment in Agentic AI Systems", "authors": ["Hayley Clatterbuck", "Clinton Castro", "Arvo Mu\u00f1oz Mor\u00e1n"], "abstract": "Agentic AIs Als that are capable and permitted to undertake complex actions with little\nsupervision mark a new frontier in AI capabilities and raise new questions about how to\nsafely create and align such systems with users, developers, and society. Because agents' actions\nare influenced by their attitudes toward risk, one key aspect of alignment concerns the risk\nprofiles of agentic Als. Risk alignment will matter for user satisfaction and trust, but it will also\nhave important ramifications for society more broadly, especially as agentic Als become more\nautonomous and are allowed to control key aspects of our lives. Als with reckless attitudes\ntoward risk (either because they are calibrated to reckless human users or are poorly designed)\nmay pose significant threats. They might also open \"responsibility gaps\" in which there is no\nagent who can be held accountable for harmful actions. What risk attitudes should guide an\nagentic AI's decision-making? How might we design AI systems that are calibrated to the risk\nattitudes of their users? What guardrails, if any, should be placed on the range of permissible\nrisk attitudes? What are the ethical considerations involved when designing systems that make\nrisky decisions on behalf of others? We present three papers that bear on key normative and\ntechnical aspects of these questions.", "sections": [{"title": "1 Introduction", "content": "Proper alignment is a tetradic affair, involving relationships among Als, their users, their developers,\nand society at large (Gabriel, et al. 2024). Agentic AIs-AIs that are capable and permitted to\nundertake complex actions with little supervision-mark a new frontier in AI capabilities. Accord-\ningly, they raise new questions about how to safely create and align such systems. Existing Als,\nsuch as LLM chatbots, primarily provide information that human users can use to plan actions.\nThus, while chatbots may have significant effects on society, those effects are largely filtered through\nhuman agents. Because the introduction of agentic Als would mark the introduction of a new kind\nof actor into society, their effects on society will arguably be more significant and unpredictable,\nthus raising uniquely difficult questions of alignment in all of its aspects.\nHere, we focus on an underappreciated\u00b9 aspect of alignment: what attitudes toward risk should\nguide an agentic AI's decision-making? An agent's risk attitudes describe certain dispositions when\nmaking decisions under uncertainty. A risk-averse agent disfavors bets that have high variance in\npossible outcomes, preferring an action with a high chance of a decent outcome over one that has a\nlower probability of an even better outcome. A risk seeking agent is willing to tolerate much higher\nrisks of failure if the potential upside is great enough. People exhibit diverse and sometimes very\nsignificant risk attitudes. How should an agentic AI's risk attitudes be fixed in order to achieve\nalignment with users? What guardrails, if any, should be placed on the range of permissible risk\nattitudes in order to achieve alignment with society and designers of AI systems? What are the\nethical considerations involved when making risky decisions on behalf of others?\nWe present three papers that bear on key normative and technical aspects of these questions.\nIn the first paper, we examine the relationship between agentic Als and their users. An agentic\nAI is \"aligned with a user when it benefits the user, when they ask to be benefitted, in the way\nthey expect to be benefitted\" (Gabriel, et al. 2024, 34). Because individuals' risk attitudes strongly\ninfluence the actions they take and approve of, getting risk attitudes right will be a central part of\nagentic AI alignment. We propose two models for thinking about the relationship between agentic\nAls and their users the proxy model and off-the-shelf tool model - and their different implications\nfor risk alignment.\nIn the second paper, we focus on developers of agentic AI. Developers have important interests\nand moral duties that will be affected by the risk attitudes of agentic Als that they produce, since AIs\nwith reckless attitudes toward risk can expose developers to legal, reputational, and moral liability.\nWe explore how developers can navigate shared responsibility among users, developers, and agentic\nAIs to best protect their interests and fulfill their moral obligations.\nIn the third paper, we turn to more technical questions about how agentic AIs might be calibrated\nto the risk attitudes of their users. We evaluate how imitation learning, prompting, and preference\nmodeling might be used to adapt models to information about users' risk attitudes, focusing on the\nkinds of data that we would need for each learning process. Then, we evaluate methods for eliciting\nthese kinds of data about risk attitudes, arguing that some methods are much more reliable and\nvalid than others. We end with recommendations for how agentic Als can be created that best"}, {"title": "Paper I\nUser Aspects of Risk Alignment", "content": null}, {"title": "1 Introduction", "content": "Our primary goal in this paper is to make the case for why risk alignment will be an essential\ncomponent of aligning agentic AI systems to their users. Individuals' risk attitudes are a strong\ndeterminant of how they will act and which actions they will approve of. Accordingly, these attitudes\nwill influence the actions that users pursue via agentic Als, their judgments about the acceptability\nof actions taken on their behalf, and the trust that they have in AI agents. As agents themselves,\nAls will have their own risk attitudes that determine the actions that they take. How should we\ndesign the risk attitudes of agentic Als so that they are aligned with those of their users?\nWe present two models of the relationship between users and agentic Als and explore the nor-\nmative considerations that bear on our choice between these two models in particular contexts:\nProxy agents: Agentic Als are representatives of their users and should be designed to replicate\ntheir users' risk attitudes.\nOff-the-shelf tools: Agentic AIs are tools for achieving desirable outcomes. Their risk attitudes\nshould be set or highly constrained by developers in order to achieve these outcomes.\nWhen thinking about Als that act as agents, it is natural to look for guidance in two main areas.\nFirst, we might look at theories of rational human agency, theories about how a person should act\nin order to best achieve her goals in light of her information about the world. Different risk atti-\ntudes constitute different strategies for acting under uncertainty. Philosophers and economists have\ndeveloped formal theories of decision under uncertainty that allow us to more precisely characterize\nthese attitudes. These can be evaluated for both their empirical accuracy (i.e. how well do they\ncharacterize the actions of actual agents) and their normative aptness (i.e. how rational are decisions\nmade under different risk attitudes?). In the first half of this paper, we will draw on insights from\nthis literature to better characterize the importance of risk attitudes when designing agentic AIs.\nSecond, we might look at human agents such as financial advisors, lawyers, or personal\nassistants who routinely take actions on another agent's behalf. There are complex formal and\ninformal rules that govern how these agents ought to relate to their clients (those on whose behalf\nthey act), and these differ significantly across different kinds of agents. For example, professional\nsocieties like the American Bar Association uphold explicit professional and ethical standards that\nregulate how lawyers should act on behalf of their clients. In contrast, alignment between personal"}, {"title": "2 What are risk attitudes?", "content": "An agent's decisions are influenced by what she values and what she believes about the world.\nHowever, knowing an agent's values and beliefs is not enough to predict how she will (or should)\nact. For example, suppose that Nate and Kate are each planning a dinner out. They both prefer\nRestaurant A (a buzzy new spot that doesn't take reservations) to Restaurant B (a mediocre stalwart\nthat does) to the same extent, both valuing a dinner at A more than twice as much as dinner at B.\nThat is, they assign the same relative utilities a measure of the subjective value an agent assigns\nto an outcome to eating at restaurant A versus B. They also agree that their chances of getting\ninto Restaurant A are about 50% and that they are certain of getting into B.\nHowever, despite agreeing on the value and probabilities, Nate and Kate might nevertheless make\ndifferent choices about where to go. Nate might opt to take his chances on Restaurant A, being\nwilling to tolerate a 50% chance of failure in order to secure the better dinner option. Kate would\nrather be safe than sorry and opts for Restaurant B. What distinguishes Nate and Kate are their\napproaches to risk, the relative significance that they give to the potential losses and gains of a risky\naction.\nImagine now that Nate and Kate use AI assistants to plan their dinner meetings. Presumably,\nthese Als would need more than just information about Nate and Kate's restaurant rankings and\nthe probabilities of getting tables at each. In order to make decisions that accord with Nate and\nKate's preferences, their AIs would need to be adjusted to their risk attitudes."}, {"title": "2.2 Expected utility theory and risk attitudes", "content": "When evaluating an action that has uncertain outcomes, one must take the probabilities and the\namount of value (utility) of possible outcomes into account. Standard decision-theoretic approaches\nassume that there is only one feature of the outcome space that matters: its expected utility. The\nEU of an action A is the average of the utilities that doing A would yield in each relevant state of\nthe world, Si, weighted by the probability that those states will obtain:\n$$EU(A) = \\sum_{i=1}^{n} u(A/S_i)p(S_i)$$\nHowever, there are other features of the distribution of possible outcomes that someone might\nalso find important. For example, the following three bets all have the same expected utility (equal\nto 4.5)\u00b3:\n\u2022 Bet A: Flip a fair coin. If heads, win 10. If tails, lose 1.\n\u2022 Bet B: Flip a fair coin. If heads, win 1,000,010. If tails, lose 1,000,001.\n\u2022 Bet C: Draw from a lottery of a million balls, one of which is a winner. If you draw the\nwinning ball, win 5,500,000. If you draw any other ball, lose 1.\u2074\nDespite having the same expected utility, these seem like very different bets, in a way that\nnearly all agents will be sensitive to. Bet B will cause you to incur enormous losses half of the time.\nBet C almost guarantees that you'll lose something. If you're only concerned with expected value\nmaximization (EVM), then these differences don't matter. However, if you are sensitive to risk, they\nmay matter significantly.\nAt its most general, risk sensitivity is a sensitivity to variance, a higher-order statistical feature of\nthe outcome space. An agent can be risk neutral, risk averse, or risk prone. A risk neutral agent\ndoes not take variance into account when evaluating actions. If an \"agent is risk averse with respect\nto some quantity X [e.g. money], she strictly prefers a (degenerate) gamble that delivers some\nparticular value x* for X with certainty to a gamble that delivers an expected X-value of x*, but\nthat includes nontrivial uncertainty\" (Greaves, et al. 2024, 9). A risk-averse agent will accept a bet\nthat will deliver a lower expected payoff but with higher certainty over one with a higher expected\npayoff but less certainty. For example, a risk averse bettor might prefer a sure thing payoff of 3 over\nBet A, which has an expected utility of 4.5 but a 0.5 chance of losing. Kate is willing to accept a\nsure thing of a decent meal over a lower chance of a better dinner. A risk prone agent is the opposite\nof a risk averse one, preferring high variance gambles over lower variance ones."}, {"title": "2.3 Varieties of risk aversion", "content": "This general characterization of risk sensitivity covers several different kinds of more specific risk\nattitudes. To get a handle on these differences, we can ask the risk-averse agent: what is it that is\nso bad about variance? What is it about variance that you don't like? An ambiguity averse agent\nanswers that variance is bad when she is uncertain about the probabilities involved (Machina and\nSiniscalchi 2014). As long as the probabilities are all known (as in Bets A-C), there is no further\nproblem with variance. Some agents might be averse to variance because they don't think that very\nlow probability events should be taken into account. Therefore, they will ignore the unlikely tails\nof the outcome distribution (the least probable bad outcomes and least probable good outcomes)\nwhen making decisions (Kosonen 2022, Monton 2019).\nA third kind of risk averse agent answers that variance in outcomes is bad because it includes\nbad outcomes. The reason that Bet B is worse than Bet A is that there is a significant possibility\nthat something very bad will happen. A risk averse agent, in this sense, cares more about avoiding\nthe worst-case outcomes of their actions than gaining the best-case outcomes (and vice versa for the\nrisk prone). A risk neutral agent assigns equal weight to gains and losses of the same magnitude.\nThis \"avoid the worst\" risk attitude will be our primary focus in what follows.\nBecause EU maximization's assessment of a bet doesn't take its variance into account, it cannot\naccount for risk sensitivity. It doesn't make space for people to treat bad outcomes differently\nfrom good ones or to treat low probabilities differently from high ones. Indeed, by prohibiting risk\nsensitivity, EU maximization places extremely stringent constraints on permissible risk attitudes by\nrequiring strict risk neutrality (H\u00e1jek 2021).\nEU maximization has been extensively developed over the past century and defended as a rational\nand perhaps the uniquely rational decision-procedure. This amounts to a rejection of the\nrationality of being risk prone or risk averse. Arguments in favor of EU maximization have taken\ntwo general forms. First, axioms of rational choice that is, conditions that one's preferences\nor actions must obey in order to be rational are presented, and EVM is claimed to (uniquely)\nsatisfy those axioms (e.g. Von Neumann and Morgenstern 1953). Second, it is argued that an agent\nwho obeys EVM will experience some long-run practical benefits over agents who obey alternative\ndecision-procedures."}, {"title": "3 Evidence of risk non-neutrality", "content": "The now standard view in welfare economics is that \"normative assessment should recognize, in light\nof results of decades of behavioral experimentation, that people are not expected utility maximizers\"\n(Harrison & Ross 2017, 150). We have robust evidence from subjects' intuitive reports, behavioral\nexperiments in the lab, and field observations of economic behavior that most people are at least\nsomewhat risk averse in most situations.\nAllais (1953) was one of the first to investigate how humans' actual choice behaviors depart from\nthe predictions of expected utility theory (EUT), and the choice behaviors he illustrated are still\nused as benchmarks for testing theories of risk (Buchak 2013, Bottomley and Williamson 2023). In\nAllais cases, subjects are asked for their preference between bets A and B, and then asked for their\npreference between bets C and D:\nBet A\nCertain $1 million\nBet C\n.89 chance of $0\n.11 chance of $1 million\nBet B\n.89 chance of $1 million\n.01 chance of $0\n.10 chance of $5 million\nBet D\n.9 chance of $0\n.1 chance of $5 million\nMost people prefer A to B and prefer D to C. However, there is no consistent assignment of\nutilities to quantities of money that makes sense of these two preferences. To see this, consider that\nmoving from A to B and moving from C to D both involve trading a 01 chance at $1 million for a .1\nchance of $5 million. In the first case, subjects are not willing to make the trade. In the second case,\nthey are. Whether this trade is acceptable depends on global properties of the bet; here, whether\nthere is a high or low probability of getting something good. These preferences have been shown\nto be present in economic (List and Haigh 2005) and healthcare choices (Oliver 2003)."}, {"title": "4 Formal models of risk aversion", "content": "Philosophers, economists, and behavioral scientists have developed various models of decision that\nincorporate sensitivity to risk. Some of these theories (e.g. Prospect Theory) were developed with the\nprimary aim of being empirically adequate for describing the economic behavior of actual agents.\nOthers (e.g. Risk-Weighted Expected Utility) aim to describe the normative aspects of rational\ndecision-making. In Appendix A, we examine the most prominent formal theories of risk. The choice\nof theory has some small bearing on the normative issues we will discuss, but it can be skipped for\nthose uninterested in the details. What matters is that actual agents' decisions rarely conform\nto EU maximization, and risk sensitivity will have to be incorporated in some fashion in order\nto accurately capture the preferences and decision-making behavior of agents. These preferences\nand decision-making behavior will matter for achieving alignment between users and agentic AIs.\nTherefore, incorporating risk sensitivity will be essential for the project of agentic AI alignment."}, {"title": "5 User alignment", "content": "Here, we focus on how agentic Als can be properly aligned with the interests of their users. Above,\nwe canvassed evidence that most human actors in most circumstances depart from expected utility\nmaximization by displaying some amount of risk aversion. If users are risk averse, then prima facie,\naligned AI agents that make decisions on their behalf should also be risk averse. However, it is\nnot clear whether this judgment, however intuitive, is correct. If it is, it is unclear how such risk\nattitudes should be implemented and what the justification for doing so would be.\nHere, we see significant interaction effects between answers to the following questions:\na. What are the desiderata for alignment?\nb. What are risk attitudes and why are they important for human agents?\nc. What is the nature of the relationship between an agentic AI and its human user?\nd. How are agentic Als structured and how do they perform?\nWe will unpack several possible answers to each of these questions and then explain how they\ngive rise to different views about alignment."}, {"title": "5.1 Aspects of user alignment", "content": "Gabriel, et al. (2024) argue that an AI assistant is \"aligned with a user when it benefits the user,\nwhen they ask to be benefitted, in the way they expect to be benefitted\u201d (34). This suggests three\naspects of user alignment:\n1. Outcomes are beneficial to the agent\n2. Users have control over the agentic AI\n3. The agentic AI is predictable\nWhile Gabriel, et al. seem to take these three desiderata as jointly necessary and collectively\nachievable, this should not be assumed. Indeed, there may sometimes be trade-offs among them.\nFor example, increasing user control over the AI might cause it to deliver less beneficial outcomes\nif the user lacks information about which actions would best promote her interests. This is especially\nsalient in the case of risk attitudes. Consider the case of retirement investments. Since most people"}, {"title": "5.2 Models of risk alignment", "content": "When an actor (assistant, representative, etc.) is tasked with making risky decisions on behalf of\na patient (user, client, etc.), what is the proper relationship between the actor's and patient's risk\nattitudes? Thoma (2023) distinguishes three views:\n1. Permissive: the actor is permitted to implement any rationally permissible risk attitude (in-\ncluding the actor's own)\n2. Required: there is some specific risk attitude that the actor is required to adopt, and this is\nnot determined by or necessarily identical with either the actor's or the patient's risk attitude\n3. Deferential: the actor ought to defer to (i.e. adopt, as much as possible) the patient's risk\nattitude\nThe third view seems relatively straightforward: an actor achieves alignment by adopting the risk\nstrategies that her patient would adopt in that circumstance. However, what deference means can be\nsomewhat complicated. For example, suppose that a client is risk averse about short term financial\ninvestments (preferring CDs over stocks) and risk averse about the amount of money they have at"}, {"title": "5.3 What are risk attitudes and why are they important for human agents?", "content": "A fundamental question about risk attitudes is whether they matter intrinsically or instrumentally.\nOn the instrumental view, a user's risk attitude is a strategy for getting what she values. This view\nis expressed by Buchak (2013, 49):\nIt is plausible to think that some people are more concerned with the worst-case scenario\nthan others, again, for purely instrumental reasons: because they think that guaranteeing\nthemselves something of moderate value is a better way to satisfy their general aim of\ngetting some of the things that they value than is making something of very high value\nmerely possible... Thus, in addition to having different attitudes towards outcomes and\ndifferent evaluations of likelihoods, two agents might have different attitudes towards\nsome way of potentially obtaining some of these outcomes.\nWhen an agent is evaluating various bets (e.g., making retirement investments), what she ul-\ntimately cares about is what those bets yield her (e.g., money). Agents differ with respect to the\nstrategies that they take to get what they want. A risk averse and risk prone agent may care about\nthe same things to the same amount (e.g. they both want to be well-off in retirement) but differ in\ntheir views about the most advisable way to go about it.\nIt might be objected that it is misleading to characterize the risk averse and risk tolerant agent\nas valuing outcomes in the same way. For reductio, assume that Pat and Matt hate sitting in the\nairport to the same degree and hate missing their flights to the same degree. Pat is risk averse and\narrives at the airport three hours before her flight, while Matt is risk tolerant and arrives one hour\nbefore his flight. On the instrumental view, their different risk attitudes are just different strategies\nfor balancing time in the airport and the chances of a missed flight. However, it seems like Pat must\neither assign more value to making her flight or assign less disvalue to sitting in the airport than\nMatt. Indeed, she seems to be more okay with waiting in the airport precisely because it is less\nrisky!\nOn this view, the risk profile of an option is something that is intrinsically valued by the agent. The\nrisk averse person might disvalue the feeling of distress that comes with taking risks, while the"}, {"title": "5.4 What is the nature of the relationship between an agentic AI and its human user?", "content": "Above, we raised the question of whether an aligned agentic AI should replicate the risk profile\nof its users or whether it is free to seek what users value by other risk strategies. An important\nfactor here is how we conceive of the relationship between a particular AI and its user, and how\nthat relationship is situated into other social structures. Here we will discuss two issues: one has\nto do with what sort of thing agentive Als are in relation to their users, the other has to do with\nthe nature of the sort of collaborative agency that will take place (no matter what sort of entity the\nagentive AI is).\nWe can distinguish between an agent that serves as a representative of a client and one that\nserves as a tool. These two roles come with different expectations and thus different conceptions\nof alignment. A tool is any system or entity that is used to bring about a desirable outcome. A\nrepresentative's job goes beyond this. They also act as a channel for communicating the views and\ninterests of their client and are interpreted as acting in their stead. Attorneys and personal assistants\nfit this bill, while doctors and travel agents do not. Someone acting as a representative assumes\na special duty to faithfully portray their client, to act in a way that is faithful to how they would\nact. Therefore, the alignment demands for an agentic AI that acts as a representative will include\nthis requirement of faithfulness. In turn, this might require a process of calibration of the AI to the\nuser to ensure that there is the kind of causal relationship between the properties of the user and\nproperties of the AI such that we could reasonably take the latter to represent the former.\nIt is unclear which view of risk alignment is appropriate for AI tools. However, Deference is the\nmost plausible view when it comes to AI representatives. Consider an AI assistant that sends e-mails\nand arranges meetings on behalf of a user (perhaps not even signaling that it is an AI assistant in\ninteractions with others). If this assistant makes decisions with a very different risk profile from the\nuser, it will fail to represent them well.\nWhichever of these models an agentive AI falls into, it is important to appreciate the kind of\nshared agency that will exist in collaborations between the user and the AI. Whether the AI is a\ntool or a representative, if the relationship between the AI and the human is functioning (i.e. it\nembodies the criteria for alignment) then what the agentive AI \"does\" will be what the AI and its\nuser together do (cf. Nyholm 2018). This is important for a variety of reasons. For one, when AI is\nsufficiently aligned with the user, the user can see what the AI does as something that the user can\nshare responsibility for. But if it is not, then in at least some cases then the user might not be\nresponsible. For example, when a personal assistant AI successfully arranges a dinner meeting, the\nuser will likely feel like this was something that he deserves some of the credit for. When a personal\nassistant AI sends an e-mail containing slurs or personal insults that are completely out of character\nfor the user, he will (justifiably) deny responsibility. One way in which this alienation could occur is\nthrough misalignment of risk functions, especially when the AI takes actions that are far riskier or\nmore cautious than the user can identify with. We will revisit legal, moral, and other implications"}, {"title": "5.5 How are they structured and how do they perform?", "content": "Choices about what our alignment goals are will interact with choices about and constraints on the\nkinds of AI systems that we build and market, including:\na. Will the AIs be calibrated to individual users or be provided \"off-the-shelf\"?\nb. Will the user have a relationship with a single AI or have a choice of several Als?\nc. How long does the relationship persist? Does the AI refresh with each usage or remember past\nencounters?\nd. What are the termination conditions, such that a relationship between a user and AI could be\nended by developers?\nIn order to achieve certain desiderata of alignment, we might prioritize certain kinds of AI agents.\nFor example, if it is important to create AI representatives that adopt the risk profiles of their users,\nthen this might point developers toward persisting Als that are calibrated toward the preferences of\nspecific users. Relatedly, if we found that this kind of calibration was not feasible or advisable, then\nthis would cause us to change our minds about what kinds of alignment are achievable."}, {"title": "5.6 Upshots for user alignment", "content": "We have presented three main dimensions of agentic AI risk alignment: desirable outcomes, user\ncontrol, and predictability. However, when it comes to interpreting and achieving these dimensions\nof alignment, there are several important decisions to make. While there are many conjunctions\nof design choices and alignment decisions, we suspect that they will cluster around two general\npositions:"}, {"title": "5.6.1 Proxy agents", "content": "Agentic Als are representatives of their users. Risk attitudes are of intrinsic importance. They\nshould defer to user risk attitudes. Tools will likely be strongly calibrated to individual users.\n\u2022 Desired outcomes are achieved by doing things in the way the agent would do things\n\u2022 Control is achieved via calibration to agent"}, {"title": "5.6.2 Off-the-shelf tools", "content": "Agentic Als are tools. Risk attitudes are instrumental (only valuable insofar as they yield desirable\noutcomes). Choice of risk attitude is permissive or required. Tools are not strongly calibrated to\nusers.\n\u2022 Desired outcomes are achieved through standards of best practices, empirical study of optimal\nstrategies for achieving desired outcomes\n\u2022 Control is achieved by allowing users to make informed choices among various tools with\ndifferent risk profiles\n\u2022 Predictability is achieved by providing users with the track record of particular AI systems\nA helpful model for off-the-shelf tools is the menu of financial investment options (e.g. 401ks)\noffered to everyday investors. For example, the following table is taken from a publication from\nCharles Schwab called \"How to determine your risk tolerance level\"20:"}, {"title": "6 What's next", "content": "In this paper, we've focused on one aspect of alignment: the relationship between agentic Als and\ntheir users. We have made a few key claims:\n\u2022 Risk attitudes are an ineliminable aspect of agency, so proper alignment between agentic AIS\nand their users involves alignment of risk attitudes.\n\u2022 The standard view is that proper alignment between agentic Als and users involves Als being\nbeneficial, predictable, and controllable by users. However, there are potential conflicts among\nthese values, and there are several ways to interpret each of them.\n\u2022 A key choice point is whether agentic Als should be trained to have the risk attitudes of their\nusers or should have their risk attitudes set in some other way.\n\u2022 There are two general options for designing risk aligned AIs Proxy Agents or Off-the-Shelf\nTools and the best practices for user alignment will differ based on which of these options\nis pursued.\nThe next two papers will address whether developers should pursue the Proxy Agent or Off-\nthe-Shelf Tool options when making and deploying agentic Als. In Paper 2, we will consider the\ninterests (moral, legal, and reputational) of developers and evaluate which of these options best\npromotes these interests. In Paper 3, we will consider whether the Proxy Agent option is technically\nviable. Is it possible to calibrate agentic Als to particular users' risk attitudes in a way that makes\nthem beneficial, predictable, and controllable?"}, {"title": "Paper II\nDeveloper Aspects of risk Alignment", "content": null}, {"title": "1 Introduction", "content": "In the previous paper, we considered several different models of an aligned relationship between\nagentic Als and their users. Here, we broaden our view. What we are ultimately aiming for is\nholistic alignment among Als, users, developers, and society at large. We will argue that getting\nalignment right is largely about navigating shared responsibility among developers, users, and Als.\nWe want to find a system that strikes the right balance and where each participant knows and is\nsuitable for their role. Here, we will focus on the role of developers within this balance, evaluating\nhow their interests, duties, and risk attitudes should shape and constrain the user-AI relationship.\nOne major choice point in the user-AI alignment problem is whether the user will determine the\nAI's risk attitudes (the Deferential view) or the risk attitudes will be determined at least in part by\nentities other than the user, such as AI developers or legal regulations (Permissive or Required).\nHere, we will consider normative reasons that bear on our choices here. Some key questions that\narise include:\na. What options are available for influencing or constraining the risk attitudes of agentic AIs?\nb. When an agentic AI performs an action, who is responsible for the consequences: the user, the\nagentic AI, or the developer? If responsibility is shared, how do we apportion responsibility?\nc. What are developers' duties when creating systems that make risky decisions on behalf of\nusers? What kinds of risk attitudes should be implemented in order to fulfill these duties?\nd. How and why do AI developers' own risk attitudes matter when designing agentic Als?\ne. How much relative influence should developers and users have in choosing the risk attitudes\nof agentic AIs? How could we achieve different levels of balance between the two?\nWe will end with a series of recommendations for how developers can make agentic AIs that\nbenefit users, society, and protect developers' interests at the same time."}, {"title": "2 Models of developer influence", "content": "Developer influence on the risk attitudes of the Als they design could come in many forms and\ndegrees; there are many options between full control by users (pure Deference) and full control by\ndevelopers (no Deference). Here is a brief and incomplete survey of the options, from least developer\ncontrol to most.\na. Pure deference: the AI is designed to be fully calibrated to the risk attitudes of particular\nusers. The aim is to predict how the user would act in each circumstance.\nb. Deference with guardrails: the AI is designed to be calibrated to the risk attitudes of particular\nusers. However, some risk attitudes are deemed to be unreasonable, and the AI is prevented\nfrom taking on those risk attitudes (even if their user has them).\nc. Partially-calibrated defaults: Als are designed with default risk attitudes that can be partially\nadjusted to the risk attitudes of their users. For example, an AI might start as a completely\nrisk-neutral expected utility maximizer and learn to become slightly risk-averse when interact-\ning with a risk-averse user.\nd. Calibrated to demographic information: Als are calibrated to common risk attitudes among\nthe subpopulation of which that user is a member.\ne. Menu of AIs with fixed risk preferences: each AI's risk attitudes are determined by developers.\nUsers can select from a menu of Als with a variety of risk attitudes.\nf. Domain-adjusted AIs with fixed risk preferences: developers completely determine risk atti-\ntudes, but an AI can have different risk attitudes depending on the context. Relevant features\nof a context include stakes (e.g. a financial bot is more risk averse when dealing with large\namounts of money) and domain (e.g. a financial bot is more risk averse than a restaurant\nreservation bot).\ng. AI system with a fixed, determined risk profile: there is a single AI system with risk attitudes\nthat are determined by developers and fixed across contexts.\nThese options strike different kinds of balance in shared responsibilities across developers, users,\nand AI."}, {"title": "3 Shared responsibility for agentic AI actions", "content": "An important aspect of alignment that is introduced by agentic Als that bears on the interests of\ndevelopers is, \"Who will be responsible for the actions taken by an autonomous AI?\u201d\nTo address this question, it will be helpful to consider some of the issues that get discussed under\nthe banner of \u201cresponsibility gaps\" (Mattias (2004); Goetze (2022); see Nyholm (2022), ch. 6 for\nan overview). A responsibility gap exists when there is an outcome that seems to be the product\nof agency but for which no agent seems to bear any responsibility. A major concern in technology\nethics for at least"}]}