{"title": "The BRAVO Semantic Segmentation Challenge Results in UNCV2024", "authors": ["Tuan-Hung Vu", "Eduardo Valle", "Andrei Bursuc", "Tommie Kerssies", "Daan de Geus", "Gijs Dubbelman", "Long Qian", "Bingke Zhu", "Yingying Chen", "Ming Tang", "Jinqiao Wang", "Tom\u00e1\u0161 Voj\u00ed\u0159", "Jan \u0160ochman", "Ji\u0159\u00ed Matas", "Michael Smith", "Frank Ferrie", "Shamik Basu", "Christos Sakaridis", "Luc Van Gool"], "abstract": "We propose the unified BRAVO challenge to benchmark the reliability of semantic segmentation models under realistic perturbations and unknown out-of-distribution (OOD) scenarios. We define two categories of reliability: (1) semantic reliability, which reflects the model's accuracy and calibration when exposed to various perturbations; and (2) OOD reliability, which measures the model's ability to detect object classes that are unknown during training. The challenge attracted nearly 100 submissions from international teams representing notable research institutions. The results reveal interesting insights into the importance of large-scale pre-training and minimal architectural design in developing robust and reliable semantic segmentation models.", "sections": [{"title": "BRAVO Challenge", "content": "Autonomous vehicles are safety-critical systems operating in a complex open world. As such, they must not only deliver excellent performance in their operational design domain but also be provably robust to adversarial attacks, extreme weather conditions, domain changes, and rare but potentially catastrophic driving situations. The BRAVO Challenge aims to develop test beds to assess and statistically demonstrate the robustness of driving perception models. The challenge employs existing test sets, sometimes with added synthetic augmentations, with novel test metrics to emphasize safety-centered challenges: calibration of models' outputs and estimation of their uncertainty; detection of out-of-distribution inputs, at scene or object level; assessment of domain shifts. The"}, {"title": "Main Tracks", "content": "In the BRAVO Challenge 2024, we proposed two tracks:\nTrack 1: single-domain Training. Participants must train their models exclusively on the Cityscapes [6] dataset. This track evaluates the robustness of models trained with limited supervision and geographical diversity when facing unexpected corruptions observed in real-world scenarios.\nTrack 2: multi-domain Training. Participants may train their models over a mix of datasets, whose choice is strictly limited to the list provided below, comprising both natural and synthetic domains. This track assesses the impact of fewer constraints on the training data on robustness. The accepted datasets are: Cityscapes [6], BDD100K [31], Mapillary Vistas [17], India Driving Dataset [25], WildDash 2 [32], GTA5 [20] and SHIFT [22]."}, {"title": "BRAVO Dataset", "content": "The BRAVO Challenge 2024 aimed to benchmark semantic segmentation models on urban scenes undergoing diverse forms of natural degradation and realistic-looking synthetic corruptions. To this end, we repurposed existing datasets [3,11, 21] and combined them with newly generated data. The BRAVO Dataset 2024 comprised images from ACDC [21], SegmentMeIfYouCan (SMIYC) [3], Out-of-context Cityscapes [11], and new synthetic data. We organized the dataset into six subsets, two with real data and four based on the validation set of Cityscapes with synthetic augmentations:\nbravo-ACDC: real scenes captured in adverse weather conditions, i.e., fog, night, rain, and snow [21];\nbravo-SMIYC: real scenes featuring out-of-distribution (OOD) objects rarely encountered on the road [3];\nbravo-synrain: 500 augmented scenes with synthesized raindrops on the camera lens [19];\nbravo-synobjs: 656 augmented scenes with inpainted synthetic OOD objects from 26 classes [14];\nbravo-synflare: 308 augmented scenes with synthesized light flares [29];\nbravo-outofcontext: 329 augmented scenes with random backgrounds for road and sidewalk [11]."}, {"title": "Metrics", "content": "The BRAVO Challenge 2024 evaluated methods on various metrics to assess their performance in semantic segmentation and out-of-distribution (OOD) detection. The semantic metrics assessed the quality of the semantic segmentation predictions on both accuracy and calibration. The OOD metrics assess the model's ability to detect whether the objects are OOD, i.e., to distinguish between known classes seen during training vs. unknown classes seen at test time. The BRAVO Index combines the semantic and OOD metrics to rank the models.\nSemantic metrics are computed on all subsets, except SMIYC, for valid pixels only. Valid pixels are those not invalidated by extreme uncertainty, such as pixels obscured by the brightest areas of a flare or covered by an OOD object.\nMean Intersection over Union (mIoU): Proportion of correctly labeled pixels among all pixels. Only semantic metric that does not rely on prediction confidence. Higher values indicate better segmentation accuracy.\nExpected Calibration Error (ECE): Difference between predicted confidence and actual accuracy. Lower values indicate better calibration.\nArea Under the ROC Curve (AUROC): Area Under the ROC Curve over the binary criterion of a pixel being accurate, ranked by the predicted confidence level for the pixel. Higher values indicate better calibration, as the confidence ranking matches the correctness of the pixels.\nFalse Positive Rate at 95% True Positive Rate (FPR@95): False positive rate when the true positive rate is 95% in the ROC curve above. Lower values indicate better calibration at the tail of the confidence distribution: the ability to reject false positives even when we reach the most true positives.\nAUPR-Success: Area Under Precision-Recall curve, over the same data as the AUROC. Higher values indicate the ability of higher confidence to match correct pixels and, thus, better calibration.\nAUPR-Error: Uses the reversed data (pixel being inaccurate, ranked by 1-confidence). Higher values indicate the ability of lower confidence to match incorrect pixels and, thus, better calibration. That tends to be stricter than AUPR-Success, since incorrect pixels tend to be rarer.\nOOD metrics are computed on the SMIYC and SynObjs subsets only for invalid pixels, i.e., those obscured by OOD objects. The OOD metrics are computed over the binary criterion of a pixel being invalid, ranked by the reversed predicted confidence level for the pixel, and include:\nArea Under the ROC Curve (AUROC).\nFalse Positive Rate at 95% True Positive Rate (FPR@95).\nArea Under the Precision-Recall Curve (AUPRC).\nAggregated metrics. For model ranking, the metrics above are aggregated as follows:\nSemantic. The harmonic mean of all semantic metrics, with ECE and FPR@95 reversed (as 1 x)."}, {"title": "Submissions digest", "content": "This section collects all the solutions from the two challenge tracks, along with interesting findings reported by the participants. We primarily retain the notation used by the participants in their reports. Notations are, thus, not consistent across subsections. The first person \"we\" in the submission subsections is that of the respective authors, summarized and sometimes paraphrased by the challenge organizers. Due to space limitations, the approach figures and some training details are provided in Appendix B."}, {"title": "Quantitative summary", "content": "The results are summarized\u00b9\u00b2 in Tabs. 1 and 2, which show the best submission for each team. Fig. 1 shows all public submissions at the end of the Challenge.\nFig. 1 shows the submissions considerably improved over the proposed baselines. Due to the strictness of the harmonic mean, submissions that only enhanced one criterion were penalized on the ranking BRAVO Index. The most surprising collective finding is that multiple training datasets (Track 2) did not improve the metrics using only Cityscapes (Track 1).\nThe OOD and semantic metrics were uncorrelated among the submissions (regression line in green in Fig. 1, R = -0.05). On the other hand, we observed varying degrees of correlation among the metrics aggregated by the subsets listed in Sec. 1.2. The correlogram appears in the Appendix B.\nAll top two solutions in both tracks leverage vision foundation models (VFMs) pretrained on massive data corpus, i.e., DINOv2 [18] and InternImage [27]. For semantic segmentation, it was surprising to find that a simple linear decoder (DINOv2-OOD) outperformed more sophisticated decoders by a large margin, according to the unified BRAVO score.\nThose findings emphasize the importance of robust VFM backbone and may temporarily shift researchers' focus toward training more robust VFMs, rather than concentrating on sophisticated network design for downstream tasks. However, the current best BRAVO score is still far from perfect and using larger models is not showing clear beneficial signals, we believe that advancing reliable segmentation requires progress from both sides: developing more robust VFM backbones and designing more efficient architectures to better exploit the knowledge encapsulated in pre-trained VFMs."}, {"title": "Track 1: DINOv2-OOD \u2013 Eindhoven University of Technology", "content": "This solution fine-tunes pre-trained Vision Foundation Models (VFMs) for semantic segmentation, leveraging their robust representations. Given a pre-trained VFM, we attach an off-the-shelf segmentation decoder and fine-tune the entire model for semantic segmentation. We evaluate this meta-architecture in several different configurations. Our primary solution uses the DINOv2 VFM [18], selected due to its effectiveness in domain generalized semantic segmentation for urban scenes [9, 13]. DINOv2, built upon the Vision Transformer (ViT) architecture [8], is pre-trained using self-supervised learning on a vast, curated dataset. We experiment with all available sizes of DINOv2.\nFor our default segmentation decoder, we use a simple linear layer that transforms the patch-level features $F \\in \\mathbb{R}^{E \\times \\frac{H}{P} \\times \\frac{W}{P}}$ into segmentation logits $L \\in \\mathbb{R}^{C \\times \\frac{H}{P} \\times \\frac{W}{P}}$, where H and W represent the height and width of the input image, P denotes the patch size, E is the feature dimension, and C is the number of classes in the dataset. We choose a linear layer trusting that the strong representations learned by the VFM forgo a more advanced decoder (which could also overfit to the training distribution).\nWe evaluate the impact of large-scale pre-training with DINOv2 by constrast with a DeiT-III [24] ViT pre-trained on ImageNet-1K [7] and fine-tuned on Cityscapes. We assess the impact of a more advanced decoder by contrast with a Mask2Former decoder [5]. We also contrast the default patch size (16 \u00d7 16) to a more expensive 8 \u00d7 8.\nTraining. When training the model with a linear decoder, we bilinearly up-sample the segmentation logits $L \\in \\mathbb{R}^{C \\times \\frac{H}{P} \\times \\frac{W}{P}}$ to $L' \\in \\mathbb{R}^{C \\times H \\times W}$, and then apply a categorical cross-entropy loss to those logits and the semantic segmentation ground truth to fine-tune the model. When using Mask2Former, the decoder outputs a set of mask logits $M \\in \\mathbb{R}^{N \\times \\frac{H}{P} \\times \\frac{W}{P}}$ and corresponding class logits $C \\in \\mathbb{R}^{N \\times (C+1)}$, where N is the number of masks and C includes an additional \"no-object\" class. Following Mask2Former, during training those mask and class logits are matched to the ground truth using bipartite matching. The predicted masks are then supervised with a cross-entropy loss and a Dice loss, and the predicted classes are supervised with a categorical cross-entropy loss.\nTesting. During inference with the linear decoder, we compute per-pixel class confidence scores with softmax on the upsampled class logits L', using the highest score to predict the pixel class. For the Mask2Former decoder [5], we bilinearly upsample the mask logits $M \\in \\mathbb{R}^{N \\times \\frac{H}{P} \\times \\frac{W}{P}}$ to the original resolution, resulting in $M' \\in \\mathbb{R}^{C \\times H \\times W}$. We then obtain the mask scores $P_M = \\text{sigmoid}(M')$.\nOverall per-pixel class confidence scores $P' \\in \\mathbb{R}^{C \\times H \\times W}$ are computed by multiplying the mask scores with the class scores across all masks. Specifically, for each class c and pixel (h, w), we have: $P'_{c,h,w} = \\Sigma_{n=1}^{N} P_{M_{n,h,w}} P_{c_{n,c}}$ For each pixel, the predicted class is the one with the highest value in P', and we also output this maximum value as the confidence score.\nResults. As shown in Tab. 3. Our best performing model, DINOv2 with a ViT-L/8 backbone and a linear decoder, achieves the highest BRAVO index of"}, {"title": "Track 1: PixOOD \u2013 Czech Technical University in Prague", "content": "We describe how the semantic segmentation and the confidence scores are computed for all submitted methods. We also discuss the training details with the focus on differences to the original PixOOD [26].\nSemantic Segmentation. The semantic class $c\\in {1,2, ..., C'}$ for each pixel $p\\in {(y,x)}^{H\\times W}$ of an image $I \\in R^{H\\times W\\times3}$ is computed from logits $l\\in R^{H\\times W\\times C}$ simply as: $c = arg \\underset{c}{max}(l_g)$. The logits are computed using different decoders in each variant as discussed below.\nConfidence. The confidence of the semantic segmentation (i.e. 1 \u2013 OOD score) in all variants of the PixOOD method is computed as $s_1$ by Eq. (3) from PixOOD [26]. Because of the quantization required for saving the results to a 16-bit PNG format (i.e., into the 65,536 values), the score is re-normalized so that the \"effective range\" of the score is well represented. Since the score is calibrated and directly corresponds to the false positive rate of the in-distribution"}, {"title": "Track 1: Ensemble \u2013 McGill University", "content": "The solutions to both tracks involve ensembles, albeit in different configurations. For both of them, we use ensembles in a standard configuration where we have Q models [15]:{$P(y | x*, \\theta^{(q)})\\}_{q=1}^{Q}, \\theta^{(q)} \\sim p(\\theta | D)$. Each one of these models is capable of generating a prediction y from a test input x* with weights $\\theta^{(q)}$ constrained by the prior $p(\\theta)$. The predictions of these models can be aggregated through the predictive posterior as the mean across models, i.e.:$P(y | x*, D) = \\sum_{q=1}^{Q}P(y | x*, \\theta^{(q)})$. With P(y | x*, D), we now have a"}, {"title": "Track 1: PhyFea \u2013 University Of Bologna & ETH Zurich", "content": "Our approach PhyFea (Physically Feasible Semantic Segmentation) enhances the performance of the baseline segmentation architectures (X) as described in our paper by retraining it with physical priors \"inclusion constraint\" and \"discontinued class\" incorporated by PhyFea. After retraining, we can observe an improvement in the mIOU score during inference by the baseline architectures. For this challenge, the baseline architecture we have taken is Segformer-B4 [30]. The training overview is explained in [1].\nSemantic segmentation. The semantic class $c \\in {1, 2, 3, . . ., C'}$ for each pixel $p \\in (y, x)^{H\\times W}$ of an image $I \\in R^{(3,H,W)}$ is computed from logits $\\$(I) \\in R^{(C,H,W)}$ as: $S* = arg \\underset{s}{\\text{max}} \\(\\$$(I)\\) .$\nConfidence. The confidence of the semantic segmentation (i. e., 1 \u2013 OOD score) in the baseline model $(X) is computed on its output $(I). First, $(I) is bounded as 0 < \\$(I) \\leq 1 in order to represent the effective range in a better way. Then, quantization is performed and saved in 16-bit PNG format.\nTraining. In [1], we show the architectural overview of PhyFea. A 2D semantic segmentation model as baseline network (X) takes an image $I\\in R^{(3,H,W)}$ as input and produces the raw output $\\$(I) \\in R^{(C,H,W)}$ where C is the number of classes present in the dataset. PhyFea takes $\\$(I) as input and produces an absolute difference of two loss values $l_{\\text{opening}}$ and $l_{\\text{dilation}}$ generated by the two operations performed in PhyFea, namely opening and selective dilation. Opening solves the inclusion constraint problem and selective dilation solves the discontinued class problem. The absolute difference $|l_{\\text{opening}} - l_{\\text{dilation}}|$ is then added to the cross-entropy loss (denoted by $l_{\\text{cross-entropy}})$ of $(X) to obtain the total loss. Here a is a hyperparameter and it is used to balance the loss of PhyFea and the baseline network. $l_{\\text{total-loss}}$ is backpropagated to optimize the weights of the baseline network by obtaining the argmin of $l_{\\text{total-loss}}$ as: $l_{\\text{total-loss}} = l_{\\text{cross-entropy}} + a * |l_{\\text{opening}}-l_{\\text{dilation}}|, 0 < a < 1 \\text{ and } S* = \\text{arg}\\ \\text{min}_s\\ (l_{\\text{total-loss}})$. PhyFea is end-to-end differentiable in order to incorporate the physical priors (i.e., inclusion constraint and discontinued class) while re-training the baseline network and it is free of any parameterized component like convolution kernel or MLP."}, {"title": "Track 2: InternImage-OOD \u2013 CASIA & Objecteye", "content": "We introduce InternImage-OOD, which integrate the power of general large-scale vision foundation model and the efficiency of simple clustering algorithm, to solve the challenge. We fine-tuned the vision foundation model to enhance its"}]}