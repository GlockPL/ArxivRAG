{"title": "SAMPLE EFFICIENT ROBOT LEARNING IN SUPERVISED EFFECT PREDICTION TASKS", "authors": ["Mehmet Arda Eren", "Erhan Oztop"], "abstract": "In self-supervised robot learning, robots actively explore their environments and generate data by acting on entities in the environment. Therefore, an exploration policy is desired that ensures sample efficiency to minimize robot execution costs while still providing accurate learning. For this purpose, the robotic community has adopted Intrinsic Motivation (IM)-based approaches such as Learning Progress (LP). On the machine learning front, Active Learning (AL) has been used successfully, especially for classification tasks. In this work, we develop a novel AL framework geared towards robotics regression tasks, such as action-effect prediction and, more generally, for world model learning, which we call MUSEL - Model Uncertainty for Sample Efficient Learning. MUSEL aims to extract model uncertainty from the total uncertainty estimate given by a suitable learning engine by making use of earning progress and input diversity and use it to improve sample efficiency beyond the state-of-the-art action-effect prediction methods. We demonstrate the feasibility of our model by using a Stochastic Variational Gaussian Process (SVGP) as the learning engine and testing the system on a set of robotic experiments in simulation. The efficacy of MUSEL is demonstrated by comparing its performance to standard methods used in robot action-effect learning. In a robotic tabletop environment in which a robot manipulator is tasked with learning the effect of its actions, the experiments show that MUSEL facilitates higher accuracy in learning action effects while ensuring sample efficiency.", "sections": [{"title": "1 Introduction", "content": "A fundamental challenge in enabling robots to operate efficiently in complex environments is developing their ability to plan and execute actions effectively. One promising way to equip robots with planning capability is to let them learn the effect of their actions in a self-supervised manner. In simulation, this can be carried out by generating random action parameters for a range of environment configurations in which the robot executes the actions and records the observed effect. This is akin to building a simplified world model. Although this works, it is not ideal as action selection is not guided towards maximizing sample efficiency, and thus it is not usually feasible for real robot learning where execution is costly.\nSample efficiency in robot learning is often considered in conjunction with reinforcement learning (RL) and explicit addressing of sample efficiency in self-supervised learning is limited which we aim to contribute with this work. For supervised learning tasks, Intrinsic Motivation (IM) and Active Learning (AL) stands as two candidates. Intrinsic motivation involves the robot's internal drive to explore and learn based on its own progress or discoveries, prompting it to seek new experiences or challenges without being directed by external objectives. However, IM is not"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Active Learning and Its Application in Robotics", "content": "Active Learning (AL), defined by strategic selection of the training data, enhances model performance while reducing costs, thus finding applications across a wide range of fields. AL algorithms primarily address supervised tasks with specialized selection strategies developed for both classification and regression tasks. Furthermore, these methods can be divided into two categories based on the presence of an iteration inside the algorithm: supervised and unsupervised. Another classification of AL methods can be made based on the used sampling set. When the set is finite, the methods are often called pool-based; whereas, when the sampling set is infinite, they are known as population-based. To address the continuous input-output structure generally observed in robotic tasks, MUSEL is designed as a supervised and population-based AL framework that specifically targets regression tasks.\nMotivated by its success in other applications, AL has been utilized across various domains in robotics, as discussed in including environmental mapping, non-parametric shape estimation, robot control, perception and prediction, and distributed learning. However, these approaches typically treat exploration and efficiency as separate objectives. In our work, we address this issue by utilizing uncertainty-based sampling to balance both, enabling sample-efficient supervised effect prediction in robotic tasks.\nNotably, Intrinsic Motivation (IM) can also be used to efficiently gather training samples by selecting actions based on an internal motivational signal related to novelty or surprise. Metrics commonly used for IM include error maximization, information maximization, surprise factor, and learning progress (LP). Furthermore, numerous studies in the literature employ IM approaches inspired by AL methods. Among the approaches mentioned above, inspired its applications"}, {"title": "2.2 Uncertainty Quantification", "content": "Uncertainty is a commonly used measure in AL algorithms for making sampling decisions. For this, usually the learning system is designed so as to generate both the prediction and its uncertainty as the output. Various learning models have been developed for classification tasks as well as regression.\nGaussian Process Regression (GPR) is popular model used for regression tasks, where the model generates the parameters of Normal distribution as output. The mean of the distribution serves as the prediction, while the standard deviation serves as the uncertainty associated with the prediction. Nonetheless, these models are not scalable by the large datasets, caused by the growth of the time and memory complexity by increase of the data.\nStochastic Variational Gaussian Processes (SVGP) which we make use of in the study, use sparse approximations and variational inference to address the computational limits of traditional Gaussian Processes. By introducing inducing points, SVGP reduces complexity to linear in the data and cubic in the inducing points, while variational inference optimizes an Evidence Lower Bound (ELBO) for scalability in large datasets."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Problem Domain Formalization", "content": "We envision a robot that is tasked with learning the effects of its actions by interacting with the environment by collecting experiences of the form (state, action) \u2192 effect, where state describes robot and environment configuration. Robot actions are costly, so the robot must learn an accurate model by minimizing the number of interactions with the environment. Towards this goal, the robot is equipped with an uncertainty estimation mechanism that guides its action selection, which is explained in the following sections. For clarity, we describe the proposed method linked to the robot task environment used for evaluating the model; but the approach is general and can be applied to any effect prediction task. Noting that this is a supervised learning problem, the set of inputs corresponding to a set of robot experiences can be given as X = {xt|xt = (st, at), with st \u2208 S CRm, at \u2208 A C Rn}, where S and A are the state and actions spaces with the dimensions m, n corresponding to the robot task at hand, and t is an arbitrary sample index or time. In the experiments conducted in this study, st is a vector containing the 2D coordinate of a sphere to be interacted with (Exp. 1) or the concatenation of the two-sphere coordinates (Exp. 2), where the robot interacts with one of them and the other sphere act as a dynamic entity in the environment. The action, at, is defined as a real scalar indicating the hit angle of the robot. In the experiments, we assume that the robot can request the sphere(s) to be placed anywhere within the defined workspace, and decide its hit angle. The effects are the new location(s) of the sphere(s) after the robot action execution. Referring to (st, at) compactly as input xt, the effect set associated with a set of inputs is then given by Y = {yt|yt \u2208 R^{r}effect obtained with input xt \u2208 X}, where r is the effect space dimension."}, {"title": "3.2 Proposed Method: MUSEL", "content": "In this section, we explain our proposed model, MUSEL, which aims to minimize the number of observation queries while learning an input-output mapping over continuous spaces without sacrificing prediction accuracy. Specifically, in the robotic domain, we seek to enable a robot to learn the effects of its actions with as few executions as possible. At the core of MUSEL is the model uncertainty estimation method introduced in Section 3.4, which is embedded in a loop comprising execution sample selection, robot execution, model learning, and input sampling, as illustrated in Figure 1."}, {"title": "3.3 Stochastic Variational Gaussian Processes", "content": "The Stochastic Variational Gaussian Process (SVGP) is a probabilistic model that approximates the traditional Gaussian Processes (GPs) to efficiently generate uncertainty estimates without being constrained by scalability issues. We adopt SVGP as the backbone model of the MUSEL approach due to its ability to provide accurate uncertainty estimates while efficiently handling large datasets. In this section, first, we explain the working mechanism of traditional GPs, followed by a demonstration of the transition to SVGPs.\nInstead of approximating a function, GP considers an infinite collection of possible functions that could explain the data. It does this by defining a probability distribution over these functions, allowing us to make predictions with associated uncertainty as shown in Equation 1. m is the mean vector, K is the covariance matrix, computed using a kernel function k(xi, xj), and \u03c3\u00b2 is the noise variance."}, {"title": "3.4 Estimating Model Uncertainty", "content": "The total uncertainty associated with a prediction is a compound of data and model uncertainties. We take the standard deviation output of SVGP on input x, \u03c3(x) as the total uncertainty associated with it and show it by U(x). As a learner can only reduce the model uncertainty, we wish to estimate model uncertainty and then guide sample selection by it. To do this, following Lee et al. [2024], we assume statistical independence between the model uncertainty, Umodel and data uncertainty, Udata and write\nSince we have \u03c3(x) from SVGP, if we can estimate data uncertainty, Udata(x), we will have an estimate on Umodel(x). As LP measures the rate of decrease in learning loss, a near zero or low LP indicates that the model performance does not increase and learning has saturated. This suggests the presence of data uncertainty; thus we can establish an inverse relationship between LP and data uncertainty. In addition, if there is high total uncertainty in a region where dense training data is present, we may conclude that the uncertainty there is due to data uncertainty as many points there were ineffective in reducing the error. To capture this logic, we introduce l(x) that measures the distance from the data samples used in the training and propose the following data uncertainty estimate:\nwhere l(x) is the minimum-distance (MD) to the training inputs, given as\nwith O representing the set of input samples used in training so far. Note that, maximum of this l(x) value is often used in AL algorithms for sample selection [Yu and Kim, 2010, Wu et al., 2019]. Now, we now can substitute Equation 10 in Equation 9 to obtain the expression for Umodel(x) as\nTo compute Umodel(x) numerically, we still need to estimate the learning progress (LP) associated with input x. However, it is not feasible to compute LP for individual input samples, as the input space is not finite. So, we make a locality assumption and assign the same LP value to the points that belong to the same local region. In this study, we use equally spaced n-boxes as the local regions and compute LP for each region. Last p average RMSE errors for each region r\u2081 are kept (Et (ri)), and are used to make a linear fit against t = 1, 2, .., p so that Et (ri) \u2248 mit + ci. The average errors are calculated with Et (ri) = Exeri e(x), where x indicates an executed input, i.e., a combination of (state, action) that falls within the region defining the range ri, and e(x) indicates the prediction error on x. The negative of the slope of the fit, -mi is technically the learning progress used in the literature [Kaplan and Oudeyer, 2004, Oudeyer et al., 2007], but here we map it to [10\u22124, 1] and enforce it to be positive to avoid singularity in Equation 10 with\nSo, now we have all the machinery necessary to compute an estimate of the uncertainty of the model associated with any potential input. In the next section, we explain how we make use of this to setup an active learning algorithm that a robot can use to explore its environment."}, {"title": "3.5 Sample Selection Based on Model Uncertainty", "content": "In this section, we describe the details of how our model, MUSEL, selects samples based on the computational elements discussed in the previous sections. The general flow and logic of MUSEL are illustrated in Figure 1. As we aim to optimize the data-gathering process of a robot in action-effect prediction tasks, we are typically faced with continuous input spaces. Thus, we adopt a population-based active learning approach."}, {"title": "4 Experiments and Results", "content": "To assess the feasibility and effectiveness of the MUSEL method, we designed a robotic tabletop setup where a manipulator robot is tasked with learning the effects of its actions. The environment includes either one or two spheres, where the robot can interact with one of them by hitting it at an arbitrary angle. We first present the one-sphere interaction scenario and compare the uncertainty quantification capabilities of the SVGP model to those of the standard GP model to ensure the reliability of the SVGP's uncertainty estimates. We also visualize the dynamics of LP values observed during the learning process of MUSEL, as it is a novel component introduced for model uncertainty estimation.\nAfter these sanity check experiments, in the single-sphere interaction task, we evaluate the sample efficiency of MUSEL by comparing it with random selection criteria. Additionally, we present the individual contributions of MUSEL components to the model uncertainty estimation. This is followed by ablation experiments, where we systematically isolate and verify the contributions of these components to the overall performance. Finally, we transition to the more complex two-sphere interaction task and evaluate the sample efficiency of our approach in this scenario. All experiments are performed in a dynamics simulation environment, PyBullet (version 3.2.6) [Coumans and Bai, 2016\u20132022]."}, {"title": "4.1 One-Sphere Interaction Experiments", "content": "The environment for the learning experiments is designed as a confined table within the robot's reach, where objects can bounce off the boundary walls according to the simulation physics. To break the symmetry in the task space, we introduced a diagonal wall placed across two consecutive walls of the table (see Figure 2). The robot is equipped with a single action, push(a), where a \u2208 [-\u03c0/3, \u03c0/3] CR. The robot can accurately detect the position of the object as {posx, posy} both before and after the action execution.\nFor implementing the push action, the robot's end effector first moves to Pstart and then hits the object by moving to Pend, where\nAfter the action is performed, the system waits for the sphere to stop, and the new position of the sphere is obtained by subtracting the initial position from it, giving the effect of the action as y = {dx, dy}. This experience is stored as an input-output pair {x,y} and added to the training data (Algorithm 1, lines 15\u201316), where x defines the robot action and the initial state as x = {sin a, cos a, posx, posy}. Note that, as a common practice, a is encoded as (sin a, cos a) to maintain input locality over the range of a.\nFor implementing MUSEL, we defined the meta-parameters of the method as follows: the number of active selection iterations, Niter = 3000; the size of the initial training dataset, minit = 1; and, finally, for the candidate inputs, Mcand = 500 and k = 1, representing the number of generated and selected inputs, respectively.\nAt each iteration, the selected learning engine (SVGP) is incrementally trained with the newly updated dataset. For SVGP training, we adopt a prioritized training scheme, selecting the Mtrain = 2000 least-trained samples for one epoch based on their training iteration count.\nWith this algorithm setting, the methods being compared are tested using 10 different random seeds to account for random effects. The methods are evaluated on a test dataset generated by sampling over a 25 \u00d7 20 \u00d7 20 evenly distributed grid, where the grid counts correspond to the push angle and the x and y positions of the sphere, respectively."}, {"title": "4.1.1 Uncertainty Quantification", "content": ""}, {"title": "4.1.2 LP Values", "content": "In MUSEL, inverse of LP is used as a proxy for data uncertainty (Section 3.4). However, unlike the standard deviation and minimum-distance (MD) metrics, which are computed per sample basis, LP is defined over predefined input regions, which may contain different numbers of samples during the processing of MUSEL. For the sphere interaction tasks, a 7x7x7 grid is used to define the LP regions in the a \u00d7 posx \u00d7 posy input space. The LP values computed for each region using Equation 13 are then assigned to the samples within the region to indicate the uncertainty associated with them. To show how LP values typically evolve in the LP regions, example LP profiles recorded during the learning of the one-sphere interaction task using the LP-only sample selection strategy are shown in Figure 4. Here, it can be seen that for LP regions near the boundaries (blue curves in (a) and (d), and orange curve in (c)) of the input domain remain elevated for a longer period compared to the regions away from the boundaries. This pattern suggests that learning in near-boundary regions continues to progress over an extended period. As such, sustained high LP values suggest that LP can serve as an effective indicator of model uncertainty, where errors can be reduced through further training.\nOn the other hand, the observed general trend in LP values indicates that they converge towards zero after approximately 500 iterations, which coincides with a reduction in test RMSE decrease rate (5a). This behavior aligns with the assumption that lower LP values indicate a lesser likelihood of reducible error. Note, however, that in some cases, an LP value may increase slightly after approaching zero due to the heterogeneity of samples belonging to a region. This suggests that, although LP is generally useful for capturing data uncertainty, it needs to be augmented for a more accurate estimation of data uncertainty, as discussed in Section 3.4."}, {"title": "4.1.3 Sample Efficiency in One-sphere Interaction Task", "content": "To assess the efficacy of MUSEL for robot self-learning, first, a simple one-sphere interaction task is considered, where the robot is required to learned the effect of its actions while interacting with a sphere. Two experiments are conducted: one with MUSEL and the other with pure random sample selection, both using SVGP as their learning engine. The experiments are repeated 10 times. We record the root mean squared error (RMSE) during the online learning process over a test set (see Section 4.1 for the details). Additionally, we record and visualize the sampling patterns of the two approaches to gain insights on the different performances exhibited by them. Figure 5a illustrates the sampling efficiency of MUSEL compared to random sampling, as evidenced by MUSEL's ability to reduce RMSE at a much steeper rate."}, {"title": "4.1.4 Individual Contributions of MUSEL Components", "content": "In this section, we investigate the contribution of individual components of MUSEL sample selection mechanism. This is done by using each component, namely the o of learning engine (SVGP), learning progress (LP), and minimum-distance (MD) as the sole selection criterion for sample selection.\nTo be concrete, the learning performances of individual components of MUSEL, acting in isolation, are compared with MUSEL and the random selection strategy using RMSE values across learning iterations, as shown in Figure 6a. Using only the \u03c3 of the learning engine performs worse than all other approaches, likely due to the presence of high-uncertainty regions containing irreducible noise, i.e., data uncertainty. LP performs only slightly better than random selection due to its region-level granularity, which prevents it from reasoning about individual samples, causing it to fall back to random selection within a region. Interestingly, MD-based sample selection, known as Greedy Sampling on the Inputs (GSx) in the literature [Yu and Kim, 2010, Wu et al., 2019], performs almost as well as MUSEL.\nTo more effectively distinguish sample efficiency based on the number of samples, the RMSE values for each type of sampling are contrasted at iteration slices of 750, 1500, 2250, and 3000, as shown in the bar plots in Figure 6b. Paralleling the trend in the RMSE curves, MUSEL and MD-based strategy outperform other sampling schemes, with lower mean RMSE values favoring MUSEL in the early iterations. It is worth noting that only sampling with MD reaches a level comparable to MUSEL towards the end of the training.\nAs with the random and MUSEL-based sampling histograms shown in Figure 5a, we compute histograms for the methods based on \u03c3, LP, and MD measures (see Figure 7). The learning engine-based uncertainty measure \u03c3 captures the overall uncertainty in the input space; however, its inability to distinguish between model and data uncertainties results in oversampling in regions with high data uncertainty, preventing overall reduction of errors over the input space. In LP-based sampling, we observe a similar focus diluted among the ranges of the boundary regions, as LP can only be computed at the regional level, and state-action pairs from the best region must be selected randomly. Section 4.1.2 shows LP values, which can be used to estimate model uncertainty, remain elevated in these areas for longer time, suggesting the model uncertainty values are also high in the boundary regions. Since the focus here is actually the reducible uncertainty, different than \u03c3-based selection, LP-based approach samples from the central regions as well. Additionally, regions of the LP are equidistant, but the diagonal wall on the bottom-right reduces the LP space"}, {"title": "4.1.5 Ablation Study", "content": "To assess the necessity and importance of MUSEL's components for its operation, we conducted a set of ablation experiments using the same evaluation procedure as in the previous section. These ablation experiments also allow us to observe how the combinations of \u03c3, LP, and MD synergistically work together to select the most informative samples for faster learning.\nThe RMSE results for the ablated versions of MUSEL, shown in Figure 8a, reveal that in the one-sphere task, the most significant performance degradation occurs when the MD component is removed. This is unsurprising, as MD exhibited the best solo performance, as detailed in Section 4.1.4. In contrast, removing the LP component results in the smallest performance reduction, likely due to its regional selection strategy. Similarly, ablating the Std (\u03c3) component causes a comparable decline in performance, possibly because the other criteria can still capture model uncertainty without explicitly incorporating \u03c3, as shown in Section 4.1.4.\nAnother way of looking at the ablation results is to interpret them as outcomes from pairs of measures. The \u03c3 + LP pair (MUSEL - MD) improves performance compared to the individual use of components, indicating that LP helps eliminate some of the data uncertainty captured by standard deviation (\u03c3), but this pair still remains less effective than other combinations. The \u03c3 + MD combination (MUSEL - LP) also performs better than standard deviation & alone, achieving the best performance among the ablation pairs. On the other hand, surprisingly, this is not true for MD. Although MD is expected to reduce the stubborn focus of o on irreducible error regions while exploiting its uncertainty information, it appears that this does not happen, and o acts as a hindrance to MD. The LP + MD (MUSEL - \u03c3) pair performs slightly better than LP, likely because it enables instance-wise prioritization rather than region-wise selection. However, it performs worse than MD, likely due to competing priorities, with LP focusing on error trends and MD on spatial diversity.\nTo provide snapshots of model performance at different stages, the RMSE values for random sampling, MUSEL, and ablation study candidates at the 750th, 1500th, 2250th, and 3000th iterations are compared in Figure 8b. Among the ablation study candidates, the ablation of LP demonstrates the lowest mean RMSE values; however, it fails to outperform MD-based selection in Section 4.1.4. MUSEL has the best mean error across all iterations, with the difference decreasing over time as iterations progress.\nThe histograms of sample selections for the three ablated versions of MUSEL are shown in Figure 9. The observed selection patterns align with the sample efficiency comparisons presented in Figure 8. Selecting samples based on standard deviation alone prioritizes areas with high data uncertainty; however, pairing it with either LP or MD appears to balance this focus. The combination of standard deviation with MD (i.e., LP ablation) results in a selection pattern that closely resembles MUSEL's selections, which demonstrates the best performance among the pairs. However, this combination does not fully account for the data uncertainty inherent in the estimations and places greater emphasis"}, {"title": "4.2 Two-Sphere Interaction Experiments", "content": ""}, {"title": "4.2.1 General Overview", "content": "In the two-sphere interaction experiments (see Figure 10), the robot is again equipped with a push action similar to that in Section 4.1, which is always directed at a predetermined \u2018cue' sphere. The other sphere is placed at a fixed location, influencing the environment dynamics by being hit by the cue sphere. Thus, the input space of the task remains the same as in the one-sphere interaction task, but the action-effect dynamics become more complex. The motivation behind designing this setup is to examine the impact of a controlled increase in task complexity.\nWhile the input and output dimensions remain the same as in the one-sphere interaction task, we ensure that the cue sphere cannot be placed in a position that would make it intersect with the other sphere, as this would result in a collision and subsequent motion due to the physics simulator used. Thus, the valid initial input configuration space becomes asymmetric and more complex, further adding to the learning difficulty.\nFor implementing MUSEL in the two-sphere interaction task, we used the same meta-parameters as in the one-sphere setup: niter = 3000, Minit = 1, Mcand = 500, and k = 1. Similarly, the SVGP learning engine was incrementally trained with the updated dataset using a prioritized training scheme, selecting the Mtrain = 2000 least-trained samples for one epoch based on their training iteration count. The test dataset was generated on a 20 \u00d7 25 \u00d7 25 grid, corresponding to the push angle, a, and the table locations, posx and posy, respectively."}, {"title": "4.2.2 Learning Efficiency Comparison", "content": "In the experiments presented in this section, we compare MUSEL to random sampling and MD-based sampling, the latter being the strongest competitor to MUSEL, according to the one-sphere task experiments.\nThe performance of the models is presented in Figure 11 using the conventions established in the one-sphere interaction task. The results show that MUSEL and MD-based sampling achieve higher sample efficiency compared to random sampling, paralleling the findings from the one-sphere experiments. An interesting observation is that the improvement over random selection is even more pronounced in the two-sphere interaction task (see Figure 5). This suggests that"}, {"title": "5 Conclusion", "content": "In this study, we introduced MUSEL (Model Uncertainty for Sample-Efficient Learning), a novel sample-efficient active learning framework for robot self-supervised learning. The effectiveness of MUSEL is demonstrated in simulated robotic experiments through ablation studies and comparisons with several baselines. Unlike previous approaches, MUSEL integrates model uncertainty directly into its sample selection strategy by extracting it from the total uncertainty estimate provided by a Stochastic Variational Gaussian Process (SVGP). This is achieved through the novel use of Learning Progress (LP) and minimum-distance (MD) metrics, enabling MUSEL to adaptively choose the most informative actions and address the high costs associated with extensive data collection in robot learning.\nThe findings presented in this paper have valuable implications for improving sample-efficient robot learning. MUSEL presents a data-efficient training framework, making it particularly suitable for real-world scenarios where data collection"}], "equations": ["f(X) ~ N(m, K + \u03c32\u0399)", "p(y X, 0) = \u222b p(y|f, X)p(f|X, 0) df", "logp(y|X, 0) = - (1/2)y^T (K + \u03c3^2I)^-1 y - (1/2) log |\u039a + \u03c3^2\u0399 - (n/2) log 2\u03c0", "q(u) = N(u; m, S)", "p(y X, 0) \u2248 \u222b p(y|f, X)p(fu, X)q(u) df du", "ELBO \u2248 \u2211_{i=1}^{B} Eq(u) [log p(yiu)] \u2013 KL(q(u)||p(u))", "Eq(u) [log p(yu)] = - (1/2\u03c3^2) (y^T (Kxx + \u03c3^21)^-1 y) - (N/2) log 2\u03c0\u03c3^2", "KL(q(u)||p(u)) = (1/2) (tr(KzzS) + m^T Kzzm - M + log |Kzz|/|S|)", "U(x) = \u03c3(x) = Umodel(x)Udata(x)", "Udata(x) = 1/(l(x)LP(x))", "l(x) = min{||x \u2212 xk||| Xk \u2208 O}", "Umodel(x) = \u03c3(x)l(x)LP(x)", "LP(x) = LP(ri) = arctan (\\frac{\\pi}{2} m), if m > 0 \\\\ 10^{-4} otherwise", "Pstart = (posx - r cosa, posy - r sin a)", "Pend = (posx + r cosa, posy + r sina)"]}