{"title": "SLVideo: A Sign Language Video Moment Retrieval Framework", "authors": ["Gon\u00e7alo Vinagre Martins", "Afonso Quinaz", "Carla Viegas", "Sofia Cavaco", "Jo\u00e3o Magalh\u00e3es"], "abstract": "Sign Language Recognition has been studied and developed throughout the years to help the deaf and hard-of-hearing people in their day-to-day lives. These technologies leverage manual sign recognition algorithms, however, most of them lack the recognition of facial expressions, which are also an essential part of Sign Language as they allow the speaker to add expressiveness to their dialogue or even change the meaning of certain manual signs. SLVideo is a video moment retrieval software for Sign Language videos with a focus on both hands and facial signs. The system extracts embedding representations for the hand and face signs from video frames to capture the language signs in full. This will then allow the user to search for a specific sign language video segment with text queries, or to search by similar sign language videos. To test this system, a collection of five hours of annotated Sign Language videos is used as the dataset, and the initial results are promising in a zero-shot setting. SLVideo is shown to not only address the problem of searching sign language videos but also supports a Sign Language thesaurus with a search by similarity technique.", "sections": [{"title": "1 INTRODUCTION", "content": "In their everyday life, deaf and hard-of-hearing people have to face the challenge of talking to hearing people who don't know sign language. Writing is not always a solution, because the \"writing language\" is like a whole different language to deaf people, so it works as a second language. The emergence of Sign Language Recognition technologies helps to solve this problem and can support video moment retrieval tasks over sign language videos. Users will then be able to submit a natural language query regarding a video and the system must return the appropriate video segment that corresponds to the query.\nA Sign Language Recognition software has to support the recognition of non-manual signs, which include facial expressions and head and shoulder movements, because they are an essential aspect of sign language, due to their grammatical and expressive functions in the dialogue. Facial expressions, specifically, can distinguish what type of phrase is being said, add intensity to what is being said, and even change the meaning of a manual sign [7].\nDespite the importance of facial expressions in communication through sign language, most of the approaches for Sign Language Recognition focus on hand gestures and put facial expressions as a low-priority research, as in [2]. This is a concerning fact because the system will lose linguistic information that is present in the facial signs. For a video moment retrieval task with sign language videos, the lack of facial expression recognition could lead to a wrong video segment being returned for a query where the facial expression has an important role.\nWith SLVideo, we aim to support video moment retrieval systems where the user can search for a sign through a text query and get a set of the relevant video segments corresponding to that query. For this, we are using a five-hour collection of annotated Portuguese Sign Language videos, focusing on the signs that include a facial expression. SLVideo is agnostic of the video encoder and we provide a proof-of-concept with two CLIP [5] encoders for embedding generation and three different techniques for processing the extracted video frames. This system also includes a sign-language thesaurus for the user to search for signs that are similar in terms of gestures and facial signs. Finally, we also support the edition of the video sign-language annotations."}, {"title": "2 VIDEO AND ANNOTATIONS FORMATS", "content": "To demonstrate SLVideo, we used a dataset with more than five hours of video footage featuring people talking in Portuguese Sign Language. Each video is a time-ordered sequence of images that depict the performance of a sign. These signs can be either manual or non-manual and convey a collection of words which compose a phrase.\nSLVideo supports videos of any format with or without Sign Language transcriptions/annotations. We use video annotations in the EAF annotation format but other formats are easily supported. The EAF annotation format [4] supports complex annotations for video and audio recordings, primarily used in linguistic research, i.e., \"an annotation can be a sentence, word or gloss, a comment, translation or a description of any feature observed in the media\".\nThe EAF file of each video is composed of several annotations, being either timely aligned to the video or it can refer to another annotation. The timely aligned annotations have a start and end timestamp associated with them and can be a literal translation of the signs executed in the video to a written form in Portuguese or a translation to a gloss annotation in Portuguese. The annotations referring to other ones are defined as symbolic associations, since they give more information about the annotation it is referring to, such as the type of phrase that is being said and the grammatical class of the gloss annotations."}, {"title": "3 UX DESIGN OVERVIEW", "content": "Currently, there is a ready-to-use user interface as illustrated in figure 2. The system allows the user to search for a facial expression, using the ground truth or four types of embedding-based search:"}, {"title": "4 SYSTEM ARCHITECTURE", "content": "The architecture of SLVideo comprises distinct modules, each assigned with a specific task. These tasks consist of how the video will be indexed and processed, how the annotations will be used, and how the queries will be interpreted."}, {"title": "4.1 Video Processing", "content": "The annotations in the EAF file are the input of a parser that outputs a more readable annotations file, in order to have only the relevant information and to have all the annotations have a start and end timestamp, contrary to the EAF file where only some annotations had those timestamps. After this parsing, the video information is analysed more easily.\nWith the parsed annotations file, it is possible to know the start and end timestamps of every sign with a facial expression. The frames within those timestamps are classified as keyframes, so all of them are extracted. The extraction is done using FFmpeg, an open-source software used for video and audio manipulation. After extracting the frames, a cropping and background removal process will be applied to them to only have the person."}, {"title": "4.2 Frame Embeddings Generation", "content": "The generation of embeddings from the video and from the dataset annotations allows the system to support an embedding-based search. The system was tested using two CLIP models, the clip-ViT-B-32, which is the image and text model CLIP, and the CAPIVARA[6], which is optimized for texts written in Portuguese. Both of these models receive a frame or annotation as input and generate its embeddings. Receiving text as a query, its embeddings can be generated to perform an embedding-based search, using the same CLIP model."}, {"title": "4.3 Facial Signs Embeddings", "content": "We extend the capabilities of the existing CLIP model by incorporating a specialized version tailored for Portuguese Sign Language (PSL). This enhanced model leverages the CLIPER[3] framework, which builds on CLIP but is fine-tuned to handle more specific and complex tasks, such as recognizing detailed facial expressions and full-body gestures in PSL.\nThe standard CLIP model, specifically clip-ViT-B-32, maps text and images to a shared vector space, facilitating the generation of embeddings from both modalities. However, this generic model does not adequately capture the intricacies of PSL, which includes not just manual signs but also essential non-manual signals like facial expressions and body movements.\nTo address this limitation, we propose the use of a modified CLIPER-like approach. This involves training the model specifically on a dataset comprising annotated PSL videos. This specialized training enables the model to generate more accurate and expressive embeddings for PSL gestures, taking into account the full range of non-manual signals.\nIn this enhanced setup, the text encoder is also adapted to better align with the linguistic characteristics of PSL. By using datasets tailored for PSL, such as gloss annotations and natural language descriptions of sign language videos, the text encoder can generate more contextually relevant embeddings. This ensures that the text queries match more precisely with the visual content of PSL videos."}, {"title": "4.4 Indexing Embedding Vectors and Annotations", "content": "To make the extracted information from the videos searchable through a query given by the user, OpenSearch is a good solution, being an open-source search and analytics suite that allows the user to search, insert, analyse, and visualize data. The information retrieved from the Portuguese Sign Language videos and annotations can be seen as documents that are indexed in OpenSearch, allowing the use of its full-text search capabilities to retrieve these documents."}, {"title": "4.5 Query Processing", "content": "The queries the user provides are text-only, and the querying process can be done using the query plain text or its generated embeddings. In the case of text embeddings-based search, the embeddings must be generated using the same model used for generating the frames and annotations embeddings, in order to make them comparable."}, {"title": "4.6 Supported Workflows", "content": "SLVideo supports two key workflows, Figure 3: a pre-processing and indexing phase, which is executed when the system starts, and a querying phase, which is executed every time a query is done. It also includes a thesaurus, annotation rating and edition."}, {"title": "4.6.1 Pre-Processing and Indexing.", "content": "The system starts by parsing the Portuguese Sign Language video annotations in the EAF files to make them more easily analyzed, then uses those annotations' information to extract the keyframes using FFmpeg. Currently, the keyframes are all frames where it is being executed a sign with a facial expression, so all of these are extracted. Every extracted keyframe will be cropped using the DETR-ResNet-50[1] model so that it becomes a smaller image with only the person in the frame, and then the background will be removed using the RMBG-1.4.\nWith the extracted key frames, embeddings are generated for all of the annotated facial expressions, using the chosen CLIP model, either clip-ViT-B-32 or CAPIVARA. Four types of embeddings are generated: base frames embeddings, where only embeddings from selected frames are generated and then summed together, with the selection being made by calculating a step size and selecting frames at intervals of this step size; average frames embeddings, where all frames embeddings are generated and its average is saved as that facial expression embedding; best frame embeddings, where only the best frame embedding is saved for each facial expression, comparing the generated embeddings by the norm of the embedding vectors; annotations' embeddings, where the embeddings for all the facial expressions annotations' values are generated and saved."}, {"title": "5 EXPERIENCES AND RESULTS", "content": "This system is evaluated based on the retrieved video segments regarding the query given by the user, whether it returns the correct video segments or not, being recall the most important metric, as it measures the proportion of relevant video segments that are retrieved. The relevant video segments are known by retrieving them from the ground truth, that is by using the query plain text to search for the desired results directly from the video annotations.\nWhen the search is embedding-based, the retrieved video segments will have associated with them a similarity score based on the cosine similarity between the query embeddings and the chosen search method embeddings. Those similarity scores were not satisfying, as for every video segment the score is in an interval of [0.56, 0.59], which is not accurate as the correct video segments should have a higher similarity score.\nAs demonstrated in table 1, to test the system we queried for the words \"Primeiro\" (first), \"Rir\" (laugh) and \"Ter\" (have) using embedding-based search with Base Frames Embeddings, Average Frames Embeddings and Best Frames Embeddings. For both CLIP models, we did three different tests, one where the extracted frames are given as it is, one where the extracted frames are cropped to have only the person, and one where the extracted frames are cropped and the background is removed. We also tested the usage of Annotations' Embeddings.\nWe can see that the results vary a lot depending on how the extracted frames are processed and that both CLIP models have similar results. We believe that this is because the used models are not trained specifically for this problem, as they are a generic image and text model and are not focused on facial expression or Portuguese Sign Language interpretation. It's important to note that when using these non-fine-tuned models, this task becomes a zero-shot task, so, despite initial appearances, the results are quite satisfactory for certain cases as it retrieves a good amount of relevant video segments regarding the given query. As for searching using the Annotation's Embeddings, we can see that CAPIVARA retrieve better results than clip-ViT-B-32, as it is a model focused on the Portuguese language."}, {"title": "6 CONCLUSION", "content": "In this paper, we proposed SLVideo, a system to search Sign Language content. The key contributions are:\n\u2022 Sign-Language Video Moment Retrieval: SLVideo is one of the first video search frameworks to support hand and facial signs with an embedding-based architecture.\n\u2022 Support for modular Sign-Language Encoders: The two different encoders that we used to extract video embeddings, demonstrate the modularity of SLVideo.\n\u2022 Proof of concept demonstration: Using a dataset of annotated videos, the system allows users to search for specific signs through text queries.\nFuture improvements will focus on enhancing model accuracy to better support communication for deaf and hard-of-hearing individuals."}]}