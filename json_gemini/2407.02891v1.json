{"title": "GPTQT: Quantize Large Language Models Twice to Push the Efficiency", "authors": ["Yipin Guo", "Yilin Lang", "Qinyuan Ren"], "abstract": "Due to their large size, generative Large Language Models (LLMs) require significant computing and storage resources. This paper introduces a new post-training quantization method, GPTQT, to reduce memory usage and enhance processing speed by expressing the weight of LLM in 3bit/2bit. Practice has shown that minimizing the quantization error of weights is ineffective, leading to overfitting. Therefore, GPTQT employs a progressive two-step approach: initially quantizing weights using Linear quantization to a relatively high bit, followed by converting obtained int weight to lower bit binary coding. A re-explore strategy is proposed to optimize initial scaling factor. During inference, these steps are merged into pure binary coding, enabling efficient computation. Testing across various models and datasets confirms GPTQT's effectiveness. Compared to the strong 3-bit quantization baseline, GPTQT further reduces perplexity by 4.01 on opt-66B and increases speed by 1.24\u00d7 on opt-30b. The results on Llama2 show that GPTQT is currently the best binary coding quantization method for such kind of LLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "Pre-trained large language models (LLMs), especially from the Transformer family like the Generative Pre-trained Transformer (GPT) [1] and Open Pre-trained Transformer (OPT) [2], excel in complex language tasks. Their success has generated considerable academic and practical interest.\nHowever, their high computational and storage demands pose a significant adoption barrier. For instance, GPT-3 175B's parameters, even when compressed to float16, require about 326GB for inference. This exceeds the capacity of advanced single GPU units, often necessitating expensive multi-GPU configurations.\nQuantization technology reduces model parameter and activation precision, e.g., substituting float32 with int8 cuts storage needs by 4\u00d7 [3], [4]. While applying low-bit weight quantization to LLMs promises memory savings, it introduces significant challenges. Quantization-aware training (QAT) [5], [6], though effective, is impractical for LLMs due to high training costs. Conversely, post-training quantization (PTQ) suffers from substantial accuracy losses in low-bit configurations, compromising model performance.\nDue to communication bandwidth constraints, keeping activations in float16 while quantizing weights to low bits has proven effective in local settings with low concurrency. AWQ [7] keeps 1% of critical weights in float16, quantizing the rest in a w4a16 LLM configuration. GPTQ [8] uses second-order information for error compensation, resulting in acceptable accuracy losses under w3a16. BiLLM [9] integrates binary weights into LLMs, achieving 1.11bits with unstructured sparsity. However, GPTQ's linear quantization and maintenance of float16 activations lack effectiveness. Furthermore, BiLLM's aggressive quantization complicates achieving tangible hardware performance gains. BCQ [10] first applies binary coding [11] to LLMs but only iteratively optimizes quantized MSE weight error, leading to significant accuracy losses.\nGPTQT aims to improve the accuracy of post-training quantized LLMs and achieve significant performance en- hancements on general-purpose GPUs. It introduces a new binary-coding method, offering enhanced representational capacity within the same bit allocation.\nSpecifically, GPTQT quantizes LLMs in two stages. First, it linearly quantizes weights to a higher bit resolution, then converts these to lower bit binary-coded weights. This change in representation range compels GPTQT to reassess and adjust the scaling factor to maintain and restore accuracy. In inference, these two steps are merged into pure binary coding, which can use efficient computing methods. Our contributions are summarized as follows:\n\u2022 Introduce GPTQT, a novel post-training quantization approach that converts LLM weights to low-bit binary coding via a heterogeneous, progressive, two-stage quantization process.\n\u2022 Propose a new re-exploration scheme for the scaling factor in response to changes in representation range during progressive quantization, optimizing accuracy.\n\u2022 Show that weights processed by GPTQT eliminate in- termediate states in inference, enabling the use of an efficient binary-coding weight calculation method to significantly boost processing speed."}, {"title": "II. GPTQT: QUANTIZE LLM TWICE", "content": "A. Background\nGPTQ. Based on Optimal Brain Quanization [12], GPTQ perform error compensation with Hessian Matrix $H_F = 2X_FX_F^T$, where F denotes the set of remaining full precision weights. It first confirms the quantization parameters row- wisely, then quantizes the weights column by column, and updates the unquantized weights in real time.\n$\\text{Goal: } \\underset{w_q}{\\text{argmin}} \\frac{(quant(w_q) - w_q)^2}{H_F^{-1}}$\n$S_F = -(w_q - quant(w_q))([H_F^{-1}]_{q,q})^{-1}(H_{1:},q)$.\nThe optimization goal is Equ.1, and the compensation is Equ.2. The subscript q represents the weight being quantized.\nBinary Coding Weight. Unlike linear quantization, which evenly distributes several points across the original data range, binary coding weight sets unequal representation inter- vals. Specifically, it represents the weight as several binarized bits $b_i(\\in \\{-1,+1\\})$, with each bit corresponding to a floating point number $a_i$. Its quantization and dequantization process is shown as Equ. 3.\n$\\text{quant}: b_i = \\text{sign}(r_{i-1}), a_i = \\frac{2^{n-i}}{\\sum_{j=1}^{n}2^{j-1}}, r_i = w - \\sum_{j=1}^{i-1} a_j b_j$\n$\\text{dequant}: W_{dq} = \\sum_{j=1}^{n} a_j b_j$,\nWhere n is the number of quantization bits, $w_{dq}$ is the inverse quantization result, and r is the current quantization residual.\nAccording to Equ.4, BCQ [10] uses an iterative method to alternately optimize a and b, which reduces the MSE error, but still causes a non-negligible loss of accuracy on LLM. It will be compared as a baseline.\n$[a_1,...a_n] = ((BB^n)^{-1}Bw)^T$.\nB. Quantize Weight Twice\nIntuitively, binary coding methods like BCQ, optimized from scratch, should outperform Linear Quantization. Yet, experiments reveal that integrating BCQ directly into the GPTQ method fails. GPTQ, which initially uses linear quantization, sets quantization parameters, then quantizes column by column, compensating earlier errors in subse- quent columns. In this mechanism, weights considered for quantization parameters differ from those actually quantized. Therefore, the BCQ method that overfits the original weights loses its generalization to GPTQ at this situation.\nTo address this, GPTQT employs a two-step progressive quantization as shown in Fig. 1. Initially, linear quantization is used to quantize the weights to a relatively high bits as $W_{int}$. Subsequently, critical points from this output are selected and re-encoded using low-bit binary coding.\n$\\text{step1 : } W_{int} = \\text{round}(\\frac{w}{s} - q_{bias})$,\n$\\text{step2 : } \\underset{W_q}{\\text{argmin}} (W - W_q)^2, W_q = \\text{BinrayCoding}(W_{int})$.\nIn this process, S represents the scaling factor for linear quantization, $q_{bias}$ is the quantization offset, $W_{int}$ denotes the intermediate high-bit value from the first quantization step, and $W_q$ is the final quantized weight.\nTo identify critical segments of $W_{int}$, GPTQT employs a grid search to minimize output errors, serving as the optimization criterion.\nThe first step maintains finer representable distances and the generalizability of linear quantization. It completes quan- tization with 5 bits. The second step reduces $W_{int}$ to a lower bit, for example, to 3 bits.\nDuring the second step, from all integers representable after the first step, select a few and convert the remainder to the nearest selected number. For instance, if the weight is quantized to 3 bits initially, the numbers 0,1,2, 3, 4, 5, 6, 7 can be represented. For the binary coding step, you might select numbers representable by 2 bits, such as 0, 1, 6, 7.\n$W_{int} = [0,2,3,1,1,6,5] \\in \\{0 ~ 2^{n-1}\\}$\n$BC_{choice} = [0, 1, 6, 7]$\n$W_q = BinaryCoding(W_{int}, BC_{choice})$\n$ = [0, 1, 1, 1, 1, 6, 6]$\nThis process involves rounding from one integer to the nearest target integer, denoted as $BC_{choice}$, which repre- sents the target expressible in binary coding. Since the first step initially reduces the number of weight bits, the feasible $BC_{choice}$ options are limited, allowing for a sequential trial of each possibility.\nC. Re-explore Scale Factor\nIn the initial step, floating point weight information is reduced to a constrained set of integer numbers, making each one critical. Yet, the second step further simplifies this information, instead of extracting directly from the original data, leading to additional information loss.\nTo address the issues of overfitting or significant informa- tion loss, GPTQT re-explore the scaling factor used in the first step.\nWe contend that the scaling factor S, determined in step 1, becomes inadequate due to the modifications in the second step of quantization. This second step introduces gaps in the uniformly distributed integer axis. Some gaps may be too wide, significantly increasing quantization errors.\nTo mitigate this, we adjust the numerical axis like a spring, allowing it to stretch and compress to an optimal degree, as shown in Fig.2, to identify the best scale factor as per Equ.5. For instance, if the weight is quantized to n bits in step 1, then $S = (W_{max} \u2013 W_{min})/(2^n - 1)$. The exploration is conducted within the range corresponding to n-1 and n+1 bits:\n$\\hat{S} \\in (\\frac{W_{max} \u2013 W_{min}}{2^{n}-1} \u2013 \\frac{W_{max} \u2013 W_{min}}{2^{n+1}-1}, \\frac{W_{max} \u2013 W_{min}}{2^{n}-1} + \\frac{W_{max} \u2013 W_{min}}{2^{n+1}-1})$\nWhere S represents the scaling factor determined through re-exploration, with $W_{max}$ and $W_{min}$ being the maximum and minimum values of the original weights, respectively. GPTQT conducts a grid search within the range specified in Equ.7 to identify the optimal value.\nD. Intermediate Steps Can be Fused in Inference\nbinary coding. Specifically, consider wanting to quantize weights to 2 bits using Linear quantization; the available integer weights would be $W_{int} \\in [-2,-1,0,1]$. To convert these back to floating point numbers, a scaling factor S and a quantization bias $q_{bias}$ are employed. The resulting dequan- tized weight is then expressed as $W = S \\times W_{int} + q_{bias}$. If set:\n$\\alpha_2 = 2^{-1}S, \\alpha_1 = 2^{0}S$,\nthe points representable by binary coding are equivalent to those by linear quantization. Given this characteristic, the entire quantization process can be consolidated into a single binary coding expression during inference.\nFor instance, in Fig.3 (c), the process initially completes linear quantization with 3 bits, followed by 2-bit binary coding. During fusion, the result of linear quantization is first transformed into a binary coding representation, and subsequently, the scaling factor S and partial bit a are integrated.\n$\\text{step1: } W_{int} \\in \\{0, 1, 2, 3, 4, 5, 6, 7\\}(3\\text{bit}),$\n$W_{int} = \\sum_{i=1}^{3} a_i b_i + 3.5,$\n$\\alpha_i = 2^{-2}, b_i = \\{+1,-1\\},$\n$\\text{step2 : } W = \\sum_{i=1}^{2} \\alpha_i b_i + 3.5,$\n$\\alpha_1 = 2^{-1}, \\alpha_2 = 2^{-0} + 2^{1}, b_i \\in \\{+1,-1\\}$.\nWhere ai, bi is the fused width floating point number and binary value.\nThen we also take the dequantization process of linear quantization into consideration, and the final fused binary coding is expressed as:\n$\\text{fused: } W_q = \\sum_{i=1}^{2} \\hat{\\alpha}_i b_i + (3.5S + q_{bias}),$\n$\\hat{\\alpha}_1 = 2^{-1}S, \\hat{\\alpha}_2 = (2^{-0} + 2^{1})S, b_i \\in \\{+1, -1\\}$.\nThis approach enables the use of efficient binary coding calculation methods like LUT-GEMM [13] during inference. Given the limited binary coding options available in step 2, directly performing a grid search to optimize Equ.5 is feasible when the bit number of $W_q$ does not exceed 4 bits."}, {"title": "III. EXPERIMENT", "content": "The experiment primarily assesses the language generation task using Perplexity as the performance metric for language models, which are notably sensitive to model quantization [14]. Perplexity evaluates how effectively a probability dis- tribution or language model predicts a sample, essentially measuring the model's surprise when predicting the next word in a sequence. Lower perplexity values signify superior performance, indicating greater model confidence and less surprise with the data encountered. Additionally, GPTQT not only reduces storage requirements but also enhances processing speed on general-purpose GPUs. To demonstrate this, the time taken to generate a single token is recorded.\nA. Setup\nGPTQT is implemented in Pytorch and integrates with HuggingFace's OPT, BLOOM [15], and Llama [16] model families. Models with fewer than 13B parameters are quan- tized on an A5000 GPU, while larger models are processed on an NVIDIA A100 GPU with 80GB due to memory constraints. Calibration employs 128 random slices of 2048 tokens each from the datasets.\nTesting primarily utilized the WikiText2 [17] and PTB [18] datasets.\nToken generation speed was evaluated on an A5000 GPU. The test method involved generating a sequence of 128 tokens with a batch size of 1 and timing this to calculate the average token generation time. Notably, GPTQT also reduces weight to 3 bits when measuring speed, aligning the commu- nication overhead with GPTQ. The speed improvement solely results from the adoption of more efficient binary coding calculation methods. Given that the fp16 model consumes more storage, model parallel technology was employed with as few GPUs as possible.\nHere, we primarily compare GPTQT with round-to-nearest (RTN) quantization, GPTQ, and BCQ, all of which are quantization methods for LLMs using similar technical ap- proaches. Notably, 4-bit quantization for LLMs can be almost lossless; thus, extensive GPTQT experiments focus on 3-bit and 2-bit outcomes, as shown in Tab. I.\nB. Result on OPT\nIn 3-bit quantization, GPTQT outperforms most of the OPT model variants from 350M to 66B. Smaller models utilizing GPTQT exhibit lower perplexity. Given that smaller models contain more compact information, this underscores GPTQT's advantages. As model size increases, the redun- dancy in information also increases, and the improvements from GPTQT become less pronounced. However, on opt- 66B, where GPTQ shows notably poor results, GPTQT significantly reduces perplexity by 4.01.\nWhen reducing the quantization bit depth to 2-bit, both RTN and BCQ show a complete collapse in performance, whereas GPTQT still maintains reasonable perplexity, par- ticularly in larger models. Under these stringent conditions, GPTQT substantially outperforms GPTQ, demonstrating its superior capability to extract important information during quantization. Specifically, GPTQT achieves normal perplex- ity with a model size of 13B, whereas GPTQ requires a model size of 66B to reach acceptable perplexity levels.\nC. Result on Llama2 and Bloom\nAs depicted in Tab. II, GPTQT also surpasses earlier quantization methods across different types of LLMs.\nNotably, for LLMs like Llama2 that utilize GRU instead of FFN, GPTQT significantly outperforms BCQ and GPTQ. It reduces perplexity by 104.61 and 19.51 on the 7B model, and by 6e3 and 18.8 on the 13B model, respectively. It's evident that GPTQ struggles with Llama-like models, leading to severe performance deterioration at 3-bit quantization, while GPTQT effectively compensates for this deficiency. Moreover, BCQ completely fails on Llama2, highlighting GPTQT's ability to bridge the gap in binary coding methods for this model type.\nFor Bloom models, GPTQT generally leads in perfor- mance. However, since GPTQ performs relatively well with this type of LLM, its results are not far behind those of GPTQT.\nD. Result on PTB dataset\nOPT perplexity results on the PTB dataset are also pro- vided in Tab.III, demonstrating that GPTQT's effectiveness is not limited to specific datasets. For the OPT model, the perplexity trends on PTB mirror those observed on Wiki- Text2. It's important to highlight that in 3-bit quantization, the GPTQ method still experiences a notable performance decline on PTB. However, GPTQT maintains performance close to the original fp16 model on opt-30B, with only a minimal increase in perplexity of 0.72.\nE. Speed up of GPTQT\nIn low-throughput scenarios, the performance of generative LLMs is primarily constrained by bandwidth. Weight quan- tization can substantially reduce communication time, which is crucial to the noticeable speed improvement seen with GPTQ. Notably, GPTQ dequantizes weights to fp16 in real- time during computations, introducing a minor computational overhead.\nFor GPTQT, the savings in memory and bandwidth are comparable to those of GPTQ, providing it with excellent potential for efficient operation. Moreover, because its two- step quantization process can ultimately be merged into a separate binary coding, GPTQT can utilize more efficient computation methods specifically designed for binary coding, such as LUT-GEMM, resulting in more significant speed improvements as model scale increases."}, {"title": "IV. ABLATION STUDY", "content": "In this section, we will present specific experiments to demonstrate the effectiveness of our proposed ideas and methods.\nA. Quantizaion Overfitting\nGPTQT employs a two-step quantization approach on LLMs to address the overfitting issues inherent in the GPTQ method. Specifically, within GPTQ-based strategies, the weights designated for parameter determination do not match the final quantized weights. This discrepancy neces- sitates a quantization method with adequate generalization capabilities.\nTo illustrate this point, two specific experimental setups are introduced. GPTQ (min MSE) utilizes linear quantization and adjusts the clipping range via grid search to minimize the MSE error between the original and quantized weights. GPTQ+BCQ incorporates BCQ into the GPTQ framework, leveraging binary coding to decrease quantization error by creating non-uniform quantization intervals.\nDespite significantly reducing the quantization error of the weights themselves, as demonstrated in Tab.V, both methods perform substantially worse than both GPTQT and the original GPTQ.\nB. Intermediate Bit\nIn the first step, GPTQT quantizes the weight to a relatively high bit level to assess the impact of this intermediate bit depth on the results. The experiments explored quantization from 3 to 6 bits, with the second phase finalizing at 3 bits.\nThese experiments were conducted using opt-125M, opt- 350M, and opt-1.3B models, with perplexity reported on the WikiText2 dataset.\nAs indicated in Fig.4, it is necessary to select a relatively high quantization bit in the first step. To strike a balance between search time and final outcome, quantizing at 4 bits or 5 bits emerges as the optimal choice.\nC. Re-exploration\nTab.VI illustrates the impact of re-exploration on the re- sults, where \"range\" specifies the bit range used for exploring the scaling factor S. Results are reported for a configuration using a 3-bit final quantization with an intermediate 5-bit setting. Here, \"Range 0\" indicates no re-exploration, \"Range 1\" represents exploration of S from 4 bits to 6 bits, and \"Range 2\" extends the exploration from 3 bits to 7 bits. This setup demonstrates how adjusting the range of S exploration can affect the final quantization effectiveness.\nD. Conclusion\nIn this paper, we identify overfitting as a significant issue in the quantization process with GPTQ. To address this prob- lem, we propose a Post Train Quantization method for LLMs, named GPTQT, which divides the quantization process into two progressive steps. Initially, weights are quantized to a rel- atively higher bit with linear quantization. Subsequently, they are converted into a lower bit binary coding representation. To manage the change in representation range introduced by the second step, GPTQT re-explores the scaling factor to enhance performance. During inference, these two phases are merged into a pure binary coding representation, allowing the use of more efficient computational methods to achieve acceleration. However, we also note significant limitations: the activation values remain at fp16, rendering GPTQT less suitable for high-throughput applications."}]}