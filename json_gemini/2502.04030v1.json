{"title": "Fine, I'll Merge It Myself: A Multi-Fidelity Framework for Automated Model Merging", "authors": ["Guinan Su", "Jonas Geiping"], "abstract": "Reasoning capabilities represent a critical frontier for large language models (LLMs), but developing them requires extensive proprietary datasets and computational resources. One way to efficiently supplement capabilities with is by model merging, which offers a promising alternative by combining multiple models without retraining. However, current merging approaches rely on manually-designed strategies for merging hyperparameters, limiting the exploration of potential model combinations and requiring significant human effort. We propose an Automated Model Merging Framework that enables fine-grained exploration of merging strategies while reducing costs through multi-fidelity approximations. We support both single and multi-objective optimization and introduce two novel search spaces: layer-wise fusion (LFS) and depth-wise integration (DIS). Evaluating across a number of benchmarks, we find that the search autonomously finds 1) Merges that further boost single-objective performance, even on tasks the model has already been finetuned on, and 2) Merges that optimize multi-objective frontiers across tasks. Effective merges are found with limited compute, e.g. within less than 500 search steps. The code is available at.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in large-scale pre-trained models like GPT-4 (OpenAI et al., 2023), LLaMA (Touvron et al., 2023), DALL-E (Ramesh et al., 2021), and Imagen (Saharia et al., 2022) have demonstrated remarkable capabilities across diverse domains. Within Large Language Models (LLMs), specialized models have emerged excelling in specific tasks such as instruction following (Xu et al., 2023; Jiang et al., 2023; Bi et al., 2024), code generation (Luo"}, {"title": "2. Related Work", "content": "Model merging has emerged as an efficient approach to enhance model capabilities without additional training data or extensive computation. The field has evolved from simple weighted parameter averaging to increasingly sophisticated methods. Early methods employed weighted parameter averaging (Utans, 1996) for models fine-tuned from a shared base model. Despite being simple to implement, these approaches often yielded suboptimal results. More advanced parameter-based techniques like Task Arithmetic (Ilharco et al., 2022) and SLERP (White, 2016) introduced parameter differences computation and spherical interpolation respectively. Later developments leveraged neural network sparsity, with TIES-Merging (Yadav et al., 2024) selectively retaining parameters based on magnitude while addressing sign conflicts, and DARE (Yu et al., 2024) combining magnitude-based sparsification with parameter rescaling. Recent advances include Evolutionary model merging (Akiba et al., 2024), which optimizes merging coefficients through evolutionary search. In this study, our framework also focuses on automatic model merging, while we propose a novel framework that leverages Multi-fidelity optimization"}, {"title": "2.1. Model Merging", "content": "Model merging has emerged as an efficient approach to enhance model capabilities without additional training data or extensive computation. The field has evolved from simple weighted parameter averaging to increasingly sophisticated methods. Early methods employed weighted parameter averaging (Utans, 1996) for models fine-tuned from a shared base model. Despite being simple to implement, these approaches often yielded suboptimal results. More advanced parameter-based techniques like Task Arithmetic (Ilharco et al., 2022) and SLERP (White, 2016) introduced parameter differences computation and spherical interpolation respectively. Later developments leveraged neural network sparsity, with TIES-Merging (Yadav et al., 2024) selectively retaining parameters based on magnitude while addressing sign conflicts, and DARE (Yu et al., 2024) combining magnitude-based sparsification with parameter rescaling. Recent advances include Evolutionary model merging (Akiba et al., 2024), which optimizes merging coefficients through evolutionary search. In this study, our framework also focuses on automatic model merging, while we propose a novel framework that leverages Multi-fidelity optimization"}, {"title": "2.2. Hyperparameter Optimization", "content": "Bayesian Optimization has demonstrated remarkable success across various applications, from achieving state-of-the-art results on CIFAR-10 (Snoek et al., 2012) to winning multiple datasets in the 2016 AutoML challenge (Mendoza et al., 2016). Although Gaussian processes remain the predominant probabilistic model in Bayesian optimization due to their well-calibrated uncertainty estimates, they face limitations in scalability, flexibility, and robustness. Alternative models such as random forests (Hutter et al., 2011) and Bayesian neural networks (Snoek et al., 2015; Springenberg et al., 2016; Perrone et al., 2017) offer better scalability for high-dimensional spaces.\nHyperband is one of the most widely-used multi-fidelity optimization (Peherstorfer et al., 2018) methods. It dynamically allocates resources across random configurations, while applying successive halving (Jamieson & Talwalkar, 2016) to eliminate poor options early. Although this method demonstrates superior performance and scalability compared to traditional Bayesian optimization, its random sampling strategy fails to leverage information from previous evaluations, potentially limiting its performance.\nOur optimizer builds upon SMAC (Lindauer et al., 2022), combining Hyperband (HB) and Bayesian Optimization (BO) to harness both efficient resource allocation and learning capabilities through surrogate modeling for effective multi-fidelity optimization. We integrate this approach into our framework to enable effective and efficient searching."}, {"title": "3. Method", "content": "To define a hyperparameter optimization pipeline for model merging, we need three parts, a search space, a target objective, and a search strategy. Our framework introduces two model merging search spaces: A Layer-wise Fusion Search Space (LFS) and a Depth-wise Integration Search Space (DIS). LFS provides fine-grained layer-wise merging, merging weights at corresponding layers from multiple models according to an optimal merge operation. However, finetuned models may not always contain corresponding layers that can be merged, even if they contain complementary information. The DIS space addresses this limitation by maintaining individual layer weights while optimizing their sequential arrangement, enabling the discovery of optimal inter-layer relationships, e.g. by picking up both copies of a corresponding layer from a model pair and placing both in an optimal order in the merged model. To accelerate the"}, {"title": "3.1. Overview", "content": "To define a hyperparameter optimization pipeline for model merging, we need three parts, a search space, a target objective, and a search strategy. Our framework introduces two model merging search spaces: A Layer-wise Fusion Search Space (LFS) and a Depth-wise Integration Search Space (DIS). LFS provides fine-grained layer-wise merging, merging weights at corresponding layers from multiple models according to an optimal merge operation. However, finetuned models may not always contain corresponding layers that can be merged, even if they contain complementary information. The DIS space addresses this limitation by maintaining individual layer weights while optimizing their sequential arrangement, enabling the discovery of optimal inter-layer relationships, e.g. by picking up both copies of a corresponding layer from a model pair and placing both in an optimal order in the merged model. To accelerate the"}, {"title": "3.2. Search Space", "content": "Layer-wise merging combines corresponding layers from multiple models to create a new model. While effective, current approaches typically apply uniform merging methods and hyperparameters across all layers, using the entire set of candidate models, which might be too coarse-grained and potentially problematic. To illustrate this concern, we conduct an analysis using TIES merge (Yadav et al., 2024), one of the most robust merging methods, on the GSM8K benchmark. As shown in Figure 3 (a), when applying TIES merge with same hyperparameters across different model combinations, the accuracy varies dramatically from 1% to 64.5%. Furthermore, Figure 3 (b) demonstrates that even with a fixed model combination (Math+Code), different hyperparameter settings lead to substantial performance variations, ranging from 34.9% to 64.5%. These results reveal two critical challenges in layer-wise merging: the selection of candidate models and the determination of hyperparameters. Both factors significantly impact the final performance, even for well-established methods like TIES. Manual tuning of these choices is not only labor-intensive but also makes it challenging to find optimal configurations.\nTo address these limitations and motivated by how different layers in neural networks serve distinct purposes, from basic feature extraction to complex task-specific processing, we design a fine-grained layer-wise merging search space (LFS)."}, {"title": "3.2.1. LAYER-WISE FUSION SEARCH SPACE", "content": "Layer-wise merging combines corresponding layers from multiple models to create a new model. While effective, current approaches typically apply uniform merging methods and hyperparameters across all layers, using the entire set of candidate models, which might be too coarse-grained and potentially problematic. To illustrate this concern, we conduct an analysis using TIES merge (Yadav et al., 2024), one of the most robust merging methods, on the GSM8K benchmark. As shown in Figure 3 (a), when applying TIES merge with same hyperparameters across different model combinations, the accuracy varies dramatically from 1% to 64.5%. Furthermore, Figure 3 (b) demonstrates that even with a fixed model combination (Math+Code), different hyperparameter settings lead to substantial performance variations, ranging from 34.9% to 64.5%. These results reveal two critical challenges in layer-wise merging: the selection of candidate models and the determination of hyperparameters. Both factors significantly impact the final performance, even for well-established methods like TIES. Manual tuning of these choices is not only labor-intensive but also makes it challenging to find optimal configurations.\nTo address these limitations and motivated by how different layers in neural networks serve distinct purposes, from basic feature extraction to complex task-specific processing, we design a fine-grained layer-wise merging search space (LFS).\nOur search space is illustrated in Figure 2 (a), we partition the model's L layers into G consecutive groups, where layers within each group share the same merging coefficients. These coefficients determine: (1) the selection of source models from the candidate pool, (2) the choice of merging algorithms, and (3) the corresponding hyperparameters for the chosen merging method. Furthermore, we introduce a component-wise decomposition strategy. Specifically, we partition the parameters within each Transformer layer into C component groups. When C = 1, the entire layer is treated as a single unit. When C = 3, we decompose the layer into three groups: MLP-related parameters, attention mechanism parameters, and layer normalization parameters. This decomposition allows for the application of component-specific hyperparameters during the merging process.\nWe define the merging coefficients $x \\in \\mathbb{R}^{G \\times C \\times (1+H)}$, where G represents the number of layer groups, C denotes the number of components per layer, and 1 + H dimensions specify the merging method selection and hyperparameters of all candidate merging methods. We use four well-established merging methods: Task Arithmetic, TIES-Merging, SLERP, and Linear Merging. See Section A.2 for more descriptions of the methods. LFS provides a fine-grained and flexible search space for model merging, which not only enables precise optimization of the fusion but also maximizes the potential of layer-wise merging."}, {"title": "3.2.2. DEPTH-WISE INTEGRATION SEARCH SPACE", "content": "Large Language Models (LLMs) exhibit hierarchical language understanding, with knowledge transformation progressing sequentially from word-level comprehension to abstract conceptual understanding. Recent research has increasingly focused on the behavior of transformer layers."}, {"title": "3.3. Multi-Fidelity Optimization", "content": "Although optimization of model merging requires less computation compared to searching for optimal hyperparameters for neural network structures and the model training process (Yu & Zhu, 2020; Elsken et al., 2019), evaluating large language models on extensive validation datasets remains computationally intensive. We optimize the process using cost-efficient Multi-Fidelity Optimization (MFO) (Peherstorfer et al., 2018), leveraging evaluations across different fidelity levels from fast but less accurate low-fidelity to slow but more accurate high-fidelity.\nThe optimization objective can be formulated as:\n$x^* \\in \\arg \\min_{x \\in \\Lambda} c(x, b) = \\arg \\min_{x \\in \\Lambda} L(D_{val}; x, b) \\qquad (1)$\nHere, we use evaluation samples as fidelity types. Each configuration is evaluated with varying budgets $b_{min} \\leq b < b_{max}$, where the budget determines the validation dataset size. Using smaller budgets provides a cheaper proxy of the true cost function (measured at $b_{max}$).\nOur implementation extends SMAC (Lindauer et al., 2022) by establishing a hierarchical evaluation framework parameterized by $b_{max}, b_{min}$, and spacing factor $\\eta$. Using Successive Halving (Jamieson & Talwalkar, 2016), each bracket i starts with $n_i$ configurations at budget $b_i$, iteratively halving configurations and increasing budgets by $\\eta$ until reaching $b_{max}$. Higher brackets begin with fewer configurations but larger budgets. A Random Forest surrogate model (Breiman, 2001) guides configuration selection through Expected Improvement, balancing exploration at low fidelities with exploitation at high fidelities. See Section A.3 for more descriptions of the optimization."}, {"title": "3.3.1. SINGLE OBJECTIVE OPTIMIZATION", "content": "We use single-objective optimization to maximize task-specific performance. The cost function for each task is defined as:\n$c_i(x) = \\arg \\min_{x \\in \\Lambda} L(D_i; x, b_{min}, b_{max}) \\qquad (2)$\nwhere $c_i(x)$ represents an optimization objective over the parameter space x for a specific task dataset $D_i$. The loss function L measures the model's performance on the target task dataset."}, {"title": "3.3.2. MULTI OBJECTIVE OPTIMIZATION", "content": "To develop comprehensive reasoning models, we employ ParEGO (Knowles, 2006) for multi-objective optimization to identify Pareto-optimal solutions across different objectives. The algorithm converts different cost values into a single cost using a parameterized scalarizing weight vector. By varying this weight vector at each iteration, ParEGO gradually builds an approximation of the entire Pareto front. Initially, the algorithm normalizes the k cost functions to the [0, 1] interval.\nIn each step, the algorithm randomly selects a weight vector $\\Lambda$ from a set of uniformly distributed vectors, defined as:\n$\\Lambda = {\\lambda = (\\lambda_1, \\lambda_2, ..., \\lambda_k) | \\sum_{j=1}^{k} \\lambda_j = 1 \\quad \\forall j, \\lambda_i = \\frac{l}{S} \\quad l \\in {0,...,S}}$\nThe size of this set is determined by $|\\Lambda| = \\binom{k+S}{k}$, where s controls the total number of possible vectors. The scalar cost for each solution is calculated using the augmented Tchebycheff function, where $\\rho$ represents a small positive constant:\n$C_{agg}(x; \\lambda) = \\max_{j=1}^{k} (\\lambda_j \\cdot c_j(x)) + \\rho \\sum_{j=1}^{k} c_j(x) \\qquad (3)$"}, {"title": "4. Experiments", "content": "We use LLaMA-family models (Touvron et al., 2023) as our base model set, including WizardLM-13B (Xu et al., 2023), WizardMath-13B (Luo et al., 2023a), and llama-2-13b-code-alpaca (Chaudhary, 2023). All these models are fine-tuned from Llama-2-13b, ensuring a shared loss landscape. We exclude WizardCoder-Python-13B (Luo et al., 2023b) as it uses CodeLlama-13b-Python (Roziere et al., 2023) as its pre-trained backbone, resulting in a different loss landscape.\nDatasets We select separate datasets for search and evaluation. For searching, we use GSMPlus (Li et al., 2024) for mathematical reasoning, MBPP (Austin et al., 2021) samples for code understanding, and MMLU (Hendrycks et al., 2020)validation samples for general knowledge. For evaluation, we employ established benchmark test sets: GSM8K"}, {"title": "4.1. Experimental Setting", "content": "We use LLaMA-family models (Touvron et al., 2023) as our base model set, including WizardLM-13B (Xu et al., 2023), WizardMath-13B (Luo et al., 2023a), and llama-2-13b-code-alpaca (Chaudhary, 2023). All these models are fine-tuned from Llama-2-13b, ensuring a shared loss landscape. We exclude WizardCoder-Python-13B (Luo et al., 2023b) as it uses CodeLlama-13b-Python (Roziere et al., 2023) as its pre-trained backbone, resulting in a different loss landscape.\nDatasets We select separate datasets for search and evaluation. For searching, we use GSMPlus (Li et al., 2024) for mathematical reasoning, MBPP (Austin et al., 2021) samples for code understanding, and MMLU (Hendrycks et al., 2020)validation samples for general knowledge. For evaluation, we employ established benchmark test sets: GSM8K"}, {"title": "4.2. Results", "content": "Using three source models (Math, Code, and LM) optimized for mathematical, coding, and general reasoning tasks respectively, we developed three specialized models through LFS: MATH-LFS, CODE-LFS, and GEN-LFS. Table 1 presents the performance of these models across five benchmarks. Our results demonstrate that MATH-LFS achieves a 4.24% improvement over the best performance of source models on GSM8k, CODE-LFS shows modest gains on MBPP, and GEN-LFS exhibits a 1.88% improvement on MMLU. These MATH-LFS gains are especially surprising, given that the base model was already finetuned for improve GSM8k performance - it appears that its arithmetic performance could be further improved through merging with the coding and instruction tuning model.\nBeyond these task-specific enhancements, we observe that LFS-searched models also demonstrate improved performance in other reasoning capabilities, with MATH-LFS showing particular strength in both common reasoning and"}, {"title": "4.2.1. SINGLE OBJECTIVE OPTIMIZATION", "content": "Using three source models (Math, Code, and LM) optimized for mathematical, coding, and general reasoning tasks respectively, we developed three specialized models through LFS: MATH-LFS, CODE-LFS, and GEN-LFS. Table 1 presents the performance of these models across five benchmarks. Our results demonstrate that MATH-LFS achieves a 4.24% improvement over the best performance of source models on GSM8k, CODE-LFS shows modest gains on MBPP, and GEN-LFS exhibits a 1.88% improvement on MMLU. These MATH-LFS gains are especially surprising, given that the base model was already finetuned for improve GSM8k performance - it appears that its arithmetic performance could be further improved through merging with the coding and instruction tuning model.\nBeyond these task-specific enhancements, we observe that LFS-searched models also demonstrate improved performance in other reasoning capabilities, with MATH-LFS showing particular strength in both common reasoning and"}, {"title": "4.2.2. MULTI OBJECTIVE OPTIMIZATION", "content": "Given the task-specific nature of DIS search and the more robust merging capabilities of LFS, we conducted our multi objective search on LFS with three optimization objectives: mathematical reasoning, code generation, and general reasoning. Our optimization yielded five Pareto-optimal solutions, denoted as MULTI-LFS-0 to MULTI-LFS-4, with their performance metrics presented in Table 1. These solutions achieved significant improvements, ranging from 5.30 to 6.86 points on average compared to the best base model. When comparing our results with existing merging approaches such as Ties, task arithmetic, and linear merging, our method achieves better trade-offs along the Pareto frontier while maintaining higher average performance across all benchmarks. This demonstrates the effectiveness of our multi-objective optimization strategy in finding superior model configurations that balance diverse task requirements.\nAs visualized in Figure 4 (b), our analysis of MULTI-LFS-4 reveals an interesting layer-wise merging pattern: Task Arithmetic is optimal for groups 1 and 2, while groups 3 and 4 employ Ties and linear merging strategies. Unlike the single-objective search-derived MATH-LFS, MULTI-LFS-4 shows a preference for incorporating layers from all source models during the merging process, resulting in better preservation of comprehensive information."}, {"title": "4.2.3. EFFICIENCY ANALYSIS", "content": "Our Multi-Fidelity Optimization (MFO) framework dynamically adjusts the budget allocation during the search process,"}, {"title": "5. Ablations and Analysis", "content": "To examine the effect of granularity in layer-wise fusion search space, we conducted ablation studies to examine the effect of granularity in LFS by varying the number of layer groups (G) and component groups (C). Table 4 shows that increasing G from 1 to 4 consistently improves GSM8K accuracy, demonstrating the benefits of more fine-grained layer control. However, performance decreases when G reaches 10, likely due to the growth in search space exceeding our search algorithm's capability within the given trials. Our analysis also shows that increasing C from 1 to 3 improves performance, though these gains are smaller compared to those from layer-wise refinement."}, {"title": "5.1. Impact of granularity in LFS", "content": "To examine the effect of granularity in layer-wise fusion search space, we conducted ablation studies to examine the effect of granularity in LFS by varying the number of layer groups (G) and component groups (C). Table 4 shows that increasing G from 1 to 4 consistently improves GSM8K accuracy, demonstrating the benefits of more fine-grained layer control. However, performance decreases when G reaches 10, likely due to the growth in search space exceeding our search algorithm's capability within the given trials. Our analysis also shows that increasing C from 1 to 3 improves performance, though these gains are smaller compared to those from layer-wise refinement."}, {"title": "5.2. Impact of layer retention in DIS", "content": "To explore the layer retention strategy in our DIS search space, we conducted a comparative experiment. Specifically, we modified the DIS-GEN-1 configuration by replacing the layer retention strategy with direct layer deletion, resulting in the DIS-GEN-RN. As shown in Table 5, although DIS-GEN-RN marginally outperformed the baseline language model on MMLU tasks, its performance still fell short compared to our proposed layer retention approach. This result shows that directly deleting layers degrades model performance while retaining layers is more effective."}, {"title": "5.3. Search only on scales", "content": "Several findings show that fine-tuning (Zhang et al., 2024) or scaling (Christ et al., 2024) task-specific neurons can improve model performance. To evaluate these claims and to verify that our DIS search space (which includes scales) is not simply re-scaling layers, we further evaluate Scale-Factor Search Space (SFS) that optimizes tasks by searching scaling factors to weights and layer outputs. As before, we apply our framework and obtain three SFS models: SFS-MATH, SFS-CODE, and SFS-GEN, each initialized from specialized base models. As shown in Table 6, Results show a decline in mathematical performance and slight improvements in code and reasoning tasks, though gains are modest compared to LFS and DIS, showing that scale optimization alone is not sufficient to explain the DIS effectiveness."}, {"title": "6. Conclusions and Future Work", "content": "In this work, we presented a Multi-Fidelity Framework for Automated Model Merging that introduces two complementary search spaces: Layer-wise Fusion Search Space (LFS) and Depth-wise Integration Search Space (DIS). LFS enables fine-grained layer-wise merging, and DIS optimizes sequential layer arrangements while preserving individual layer weights. We show automated model merging not only works, but is quite effective, demonstrating strong performance in both single-objective and multi-objective scenarios, achieving a 4.24% improvement on the GSM8k challenge task with only 17% of the full budget within 500 trials, and a 6.86% improvement in multi-objective performance using 18.6% of the full budget within 1000 trials. When extended to various benchmarks, our method consistently shows promising results without any additional tuning. Overall, our work provides an efficient and flexible framework for automated model merging that achieves effective improvements with reduced computational costs."}, {"title": "A. Detailed Experimental Settings", "content": null}, {"title": "A.1. Details of TIES Configuration for Math and Code Model Merging on GSM8K", "content": null}, {"title": "A.2. Descriptions of Existing Model Merging Methods in Layer-wise Fusion Search Space (LFS)", "content": "Task Arithmetic enhance model capabilities through vector operations by leveraging weighted combinations of task-specific knowledge. Given a base model with weights @pre and task-specific fine-tuned weights {$@_{t}$}=1, task vectors are defined as:\n$T_{t} = \\theta_{t} - \\theta_{pre}$\nThe merged weights are then computed through:\n$\\theta_{Merge} = \\theta_{pre} + \\lambda \\sum_{t=1}^{n} T_{t}$\nwhere $\\lambda$ controls the magnitude of task-specific adaptations.\nTIES-Merging is a parameter conflict resolution approach that operates in three stages. First, select the top k% parameters by magnitude of each task vector Tt:\n$\\uparrow_{t} = TopK(T_{t}, k)$\nNext, Generating a consensus sign vector by examining the aggregate direction of parameter changes across all tasks:\n$\\hat{V} = sgn (\\sum_{i=1}^{n} \\uparrow_{i})$\nFinally, computing the average update magnitude considering only those task vectors whose signs align with the consensus direction:\n$\\overline{V} = Average({\\uparrow_{t} : sgn(\\uparrow_{t}) = \\hat{V}})$ The final merged model weights are then computed as:\n$\\theta_{Merge} = \\theta_{pre} + \\lambda * \\overline{V}$\nSLERP (Spherical Linear Interpolation ) computes optimal geodesic paths between model weights through:\n$SLERP(\\theta_{1}, \\theta_{2}, t) = \\frac{sin((1 - t)\\omega)}{sin(\\omega)} \\theta_{1} + \\frac{sin(t\\omega)}{sin(\\omega)} \\theta_{2}$\nwhere $\\omega = arccos(\\frac{\\langle \\theta_{1}, \\theta_{2} \\rangle}{\\|\\theta_{1}\\| \\|\\theta_{2}\\|})$ and t $\\in$ [0, 1] is the interpolation parameter."}, {"title": "A.3. Descriptions of SMAC-based Multi-Fidelity Optimization", "content": "Our implementation builds upon SMAC (Lindauer et al., 2022), which combines Hyperband (HB) (Li et al., 2018) and Bayesian Optimization (BO) (Snoek et al., 2012), utilizing Random Forest (Breiman, 2001) as the surrogate model. Let bmax and bmin denote the maximum and minimum budgets respectively, and $\\eta$ > 1 be a budget spacing parameter. SMAC determines Smax = $[log_{\\eta}(R)]$ brackets. Each bracket i begins with $n_i = [n_{S_{max}-i} \\cdot \\eta^{\\frac{1}{S_{max}} } ]$ configurations and executes Successive Halving across $[log_{\\eta} (n_i)]$ + 1 rounds. Each round evaluates all configurations with budget $i$, retains the top $[\\frac{n_i}{\\eta}]$ performers, and increases their budget to $\\eta$i for the next round, where I denotes the current round. This process continues until reaching either a single configuration or bmax. The Random Forest model incorporates configuration-performance pairs from previous evaluations, updating before each Successive Halving iteration using evaluations from all budget levels while prioritizing data from the largest available budget. The model guides configuration selection through Expected Improvement, enabling efficient exploration while maintaining evaluation quality. As optimization progresses, more configurations undergo evaluation at higher budgets, allowing the model to overcome potential misguided conclusions from lower-fidelity evaluations by ultimately relying on high-fidelity results. This integration of Hyperband's resource allocation with Bayesian optimization's surrogate modeling enables efficient exploration of the configuration space while maintaining evaluation quality through principled multi-fidelity optimization."}, {"title": "A.4. Details of Datasets Information", "content": "For searching, We use data from three reasoning domains: 1,000 GSMPlus (Li et al., 2024) samples, an adversarial variant of GSM8K with mathematical perturbations for testing math reasoning; 300 MBPP (Austin et al., 2021) samples (200 training/100 validation) for code understanding; and 700 samples from MMLU validation(Hendrycks et al., 2020) for general reasoning. These datasets support both single-objective optimization when used separately and multi-objective optimization when combined. For comprehensive performance evaluation, we employ established benchmark test sets across three key domains: mathematical reasoning using the complete test sets from GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), code generation using the standard test splits from MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021), and general knowledge using the MMLU test set (Hendrycks et al., 2020), which spans diverse knowledge domains."}, {"title": "A.5. Evaluation Metrics and details", "content": "We evaluate using domain-specific metrics: accuracy for MMLU multiple choice questions based on loglikelihood, zero-shot accuracy for GSM8K and MATH, and Pass@1 for HumanEval and MBPP. We run all evaluations using LM Evaluation Harness(Gao et al., 2024) with vLLM(Kwon et al., 2023) acceleration. For consistency, we use fixed parameters across all tests: batch size 16, temperature 0.0 for greedy decoding, and maximum generation length of 1,024 tokens for GSM8K and 2,048 tokens for other datasets. All experiments run on NVIDIA Tesla A100 GPUs."}, {"title": "A.6. Details of Search Spaces", "content": "Within LFS, we define specific parameter ranges for each merging method: Task Arithmetic utilizes task vector weights $\\lambda \\in$ [0, 1]; TIES-Merging combines task vector weights $\\lambda \\in$ [0, 1] with a ratio to retain parameters k $\\in$ [0.1, 0.99]; Linear-merging optimizes model coefficients $w_t \\in$ [0,1] subject to $\\sum_{t=1} w_t = 1$; and Slerp employs an interpolation parameter t$\\in$ [0,1]."}, {"title": "A.7. Details of Grid Search on Hyperparameters of base Model Merging Methods", "content": null}, {"title": "A.8. Details of search on other Reasoning Tasks", "content": "LogiQA is derived from the logical comprehension section of China's National Civil Servants Examination, specifically designed to evaluate candidates' critical thinking and problem-solving capabilities. For our search implementation, we utilize the validation dataset with a budget range of 100 - 651.\nOpenBookQA is used to measure deep understanding of both subject matter and language comprehension. The dataset comes with an \"open book\" of fundamental facts. We conducted experiments both with and without facts in the prompt. Our search employs the validation dataset with a budget range of 100 \u2013 500.\nPIQA (Physical Interaction: Question Answering) serves as a benchmark dataset for physical commonsense reasoning, with a particular focus on everyday situations and unconventional solutions. We have sampled 1,000 examples from the validation dataset for our search purposes, setting the budget range at 100 \u2013 1,000.\nSocialIQA stands as a comprehensive benchmark for testing social commonsense intelligence, this dataset evaluates understanding of human actions and their social implications in everyday situations. Our search implementation uses a 1,000-sample subset from the validation data, with a budget range of 100 - 1,000.\nMGSM (Multilingual Grade School Math Benchmark) is a benchmark of grade-school math problems The same 250 problems from GSM8K are each translated via human annotators in 10 languages. we use 1,069 mathematics problems and solutions translated to japanese from the GSM8K test set by Sakana AI for searching ,set a budget range of 100 \u2013 1000."}, {"title": "B. Additional Experimental Results.", "content": null}, {"title": "B.1. Detailed breakdown of performance across specific MMLU categories of GEN-DIS-0 and GEN-DIS-1", "content": null}, {"title": "B.2. Additional result of Budget Distribution", "content": null}]}