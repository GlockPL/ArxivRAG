{"title": "Can We Delegate Learning to Automation?: A Comparative Study of LLM Chatbots, Search Engines, and Books", "authors": ["Yeonsun Yang", "Ahyeon Shin", "Mincheol Kang", "Jiheon Kang", "Jean Y. Song"], "abstract": "Learning is a key motivator behind information search behavior [8]. With the emergence of LLM-based chatbots, students are increasingly turning to these tools as their primary resource for acquiring knowledge. However, the transition from traditional resources like textbooks and web searches raises concerns among educators. They worry that these fully-automated LLMs might lead students to delegate critical steps of search as learning. In this paper, we systematically uncover three main concerns from educators' perspectives. In response to these concerns, we conducted a mixed-methods study with 92 university students to compare three learning sources with different automation levels. Our results show that LLMs support comprehensive understanding of key concepts without promoting passive learning, though their effectiveness in knowledge retention was limited. Additionally, we found that academic performance impacted both learning outcomes and search patterns. Notably, higher-competence learners engaged more deeply with content through reading-intensive behaviors rather than relying on search activities.", "sections": [{"title": "1 INTRODUCTION", "content": "Real learning often begins beyond the classroom walls [59]. To deepen their understanding or bridge knowledge gaps left in classes, students embark on a journey of information-seeking behavior, relying on tools such as books and the web to guide their search. Commonly, students visit libraries or search the internet, engaging in mainly four interacting stages: identifying gaps in their knowledge, seeking relevant materials, evaluating and selecting pertinent information, and synthesizing this knowledge to enhance their understanding [51, 67, 72]. This self-directed search and learning is crucial because they allow students to explore topics in depth, personalize their learning experience, and develop the independence and problem-solving skills necessary for both academic success and lifelong learning [52].\nHowever, as students progress into higher education, they need to invest significantly more time and effort into the search-as-learning process due to the increasing complexity and difficulty of course material. This demand may overwhelm students who struggle to dedicate extra time to learning. In response, with the advancement of large language models (LLMs), students are increasingly shifting their primary resources from traditional search tools (such as textbooks and web search engines) to LLM-based chatbots, attracted by their speed, convenience, and ease of use. These automated tools reduce the cognitive burden on learners by scaffolding or replacing specific steps in the search-as-learning process, such as information retrieval, evaluation, and synthesis for knowledge acquisition.\nDespite this rising popularity and growth of LLM-based chatbots, educators have sincere concerns about incorporating LLMs into learning. Many believe that for effective learning, students must actively engage with the material by going through trial and error and spending adequate time reflecting on it. They worry that relying too much on the efficiency and convenience of LLMs could lead to over-reliance, ultimately negatively affecting learning outcomes [4, 6, 42, 62]. Consequently, some institutions have banned the use of GPT or blocked access on campus, opting for alternatives such as requiring handwritten assignments or solving problems during class [54, 66]. Despite ongoing research demonstrating the educational potential of LLMs and their gradual introduction into educational settings, these concerns persist.\nOur study seeks to examine whether, as some educators caution, the delegation of search-as-learning processes to automated tools is detrimental to the learning outcomes. First, we conducted a survey analysis to understand educators' perceptions of LLM-based chatbots more accurately. From this analysis, we developed a two-dimensional model that explains how specific factors of search tools interact with educators' expectations for successful search-as-learning. The results revealed that educators have three major concerns: (1) lack of reliability, (2) insufficient systematic organization, and (3) weak cognitive engagement.\nTo investigate whether relying on LLM-based chatbots impacts learning outcomes as educators concern, we conducted a mixed-design empirical study with 92 students and compared three learning tools with varying degrees of automation: books, the web, and ChatGPT. The results show that while the LLM-based chatbot did not affect understanding key concepts, it was less effective than books in supporting long-term retention of the concepts. Despite a significant part of search-as-learning being automated by using LLM-based chatbot, the results show that the students did not adopt a passive learning approach. In addition to comparing the impact of different degrees of automation on learning tools, we further examined the relationship between students' competence level and their patterns in the search-as-learning process. We found that students with higher academic performance achieved better learning outcomes regardless of the tool used. We report distinct patterns in the search and learning strategies between high-competence students and those with lower competence.\nBased on the findings, we discuss how LLM-based chatbots can be effectively utilized during the search-as-learning process and propose design implications to consider when developing future educational tools. To summarize our contributions:\n\u2022 We analyze educators' concerns on automating the learning process through a survey with over 75 educators and identified a two-dimensional in-depth model that shapes effective search and learning.\n\u2022 We systematically and structurally investigated the validity of educators' concerns of automating the process of search-as-learning through a 3-by-3 within-subject experiment with 92 participants.\n\u2022 We found that passive learning is influenced more by individual student competence than by the learning tool itself, and through behavioral analysis, we identified specific differences in strategy patterns.\n\u2022 Based on these findings, we offered suggestions and design implications for how automated tools can be leveraged in helping concept learning."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": ""}, {"title": "2.1 Search as Learning", "content": "Human information behavior [76] plays a crucial role in both our work and everyday lives. According to information science literature and theory, information-seeking occurs when there is a need for more information or knowledge reconstruction to resolve a problematic situation [23], such as mitigating the uncertainty (or gap), or addressing an anomalous state of knowledge in the context of problem-solving and sense-making, which are integral to human learning [7, 20, 34, 76]. Research in fields of information science and education has conceptualized information-seeking as a learning process [37, 52], and learning as a key outcome of information search and use [1, 33, 77].\nTo effectively engage in information-seeking and knowledge construction, learners leverage search systems to navigate the following steps: (i) query formulation, (ii) material collection, (iii) selection, and (iv) organization [51, 67, 72]. For example, learners first recognize their information needs and formulate their own queries. Next, they search for and browse relevant information by examining the returned search results. Afterwards, learners evaluate and select content based on relevance and reliability, ultimately transforming this information into meaningful knowledge. While textbooks were traditionally used for this process, the advent of the internet significantly broadened access to information, making web-based learning prevalent [31, 32, 79]. More recently, LLM-based search systems have emerged as a new, highly automated learning tool that not only assists with information retrieval but also offers personalized, adaptive responses that cater to individual academic pursuits. This growing automation has the potential to optimize learning by reducing the cognitive load involved in search, but the implications of such systems on learning outcomes remain underexplored.\nPrevious research has characterized traditional search systems as tools for learning and investigated their influence on learning outcomes [8, 27, 64, 69, 71]. However, there is limited research on the impact and potential of LLM-based search systems, especially in comparison to traditional methods. The shift from manually driven search processes to increasingly automated systems raises important questions about the role of automation in search-as-learning. This study aims to fill this gap by examining how LLM-based search systems affect both learning gain and search behavior in comparison to other levels of automation."}, {"title": "2.2 Large Language Models in Education", "content": "The advances of LLMs that leverage artificial intelligence and natural language processing technologies are rapidly transforming educational environments. Due to their fluency, naturalness in producing language, and versatility, students are increasingly turning to LLMs for support in various academic tasks, including completing homework, writing essays or academic reports, and even searching for information and concepts covered in coursework [38, 50]. As a result, there has been a growing body of research focused on developing LLM-based educational systems and demonstrating their effectiveness [5, 35, 40, 70], with LLM chatbots beginning to be adopted in real-world learning environments [30].\nHowever, there remains considerable debate about the promises and perils of using LLMs as educational tools, and these discussions have sparked conflicts among educators [4, 6, 42, 54, 62]. While prior work has investigated educators' concerns, these are often domain-specific or overly general in nature. Furthermore, there is a lack of empirical research examining how these concerns translate into actual impacts on learning outcomes. Thus, we aim to investigate educators' concerns and explore how these translate into learning outcomes through a structured study."}, {"title": "2.3 Effective Information Searching Strategies", "content": "Educational and cognitive scientists have found that learners retain knowledge better and can apply it across various contexts when meaningful learning takes place. Ambrose et al. [2] introduced the principles of meaningful learning, emphasizing that motivation, metacognition, and self-regulation are critical factors in promoting deep understanding. In the context of search as learning, to foster meaningful learning beyond passive information consumption, comprehensive search practices are proposed, characterized by iterative, reflective, and integrative search sessions [65]. Further research has shown that successful learners-those who achieve higher levels of meaningful learning-exhibit distinct search patterns during the learning process. A commonly observed trend is that time spent reading pages (as opposed to searching) is associated with deeper learning outcomes. For example, [17, 28] shows that learners with high competence levels focus more on content pages, while [78] found that factors such as document retention, query length, and the average rank of selected results could be predictive of domain expertise. Additionally, [16] found that eye-gaze patterns could predict an individual's level of domain expertise, based on the cognitive effort associated with reading.\nThere has been increasing interest in studying how higher education students' information search and use behaviors affect and support their learning [73, 74, 80]. For instance, [80] reports that common patterns affecting learning outcomes include the reliance on a rudimentary search heuristic, consistently using the same simple search strategy regardless of the context, and habitual topic switching after superficial skimming without evaluating all search results. These growing interests at the intersection of searching and learning highlight the importance of understanding how learning occurs during the search process. However, while this correlation between individual search patterns and learning outcomes is well-studied in traditional search tools such as books or web search engines, the distinct search patterns associated with emerging tools like LLM-based chatbots have not yet been thoroughly explored. Therefore, in this study, we conducted additional analyses (see Section 6) to investigate whether differences in search patterns emerge when using LLM-based chatbots in a search-as-learning context, particularly across varying levels of learner competence, and to identify any notable patterns that may arise."}, {"title": "3 EXPLORATORY STUDY", "content": "In the early phase of our research, we conducted a survey study to explore educators' perspectives of different learning tools with varying levels of automation, including textbooks, search engines, and LLM-based chatbots. For a systematic and structured investigation, we particularly focused on understanding the concerns educators have about automation across four steps in the information behavior process: (i) query formulation, (ii) material collection, (iii) selection, and (iv) organization [51, 67, 72]. This was primarily to help us develop a set of specific research questions, as listed in Section 3.3."}, {"title": "3.1 Statistical Analysis", "content": "Through quantitative analysis, we found that the preference for LLM-based chatbots is significantly lower than that for other learning sources, and none of the steps, except for (ii) collection, are considered suitable for automation.\nDisapproval of ChatGPT is widespread and consistent. We present the comparative results in Figure 1(a) and Figure 2. Given that our data consists of ordinal rankings, we performed Friedman's test [26] instead of ANOVA that is not appropriate due to its assumptions about continuous data and homoscedasticity. Through a Friedman test, we observed that a statistically significant difference exists in ranked ratings across three learning sources ($X^2$(2) = 21.62, p < 0.001)."}, {"title": "3.2 Two-Dimensional Model Shaping Effective Learning", "content": "To uncover the underlying reasons for the concerns of educators with automated tools, we further conducted a qualitative analysis. Based on survey responses from educators, we derived a two-dimensional model through thematic coding, which comprises educators' expectations for learning and the key factors of sources that help achieve these expectations (Figure 3). This structured model explains how these dimensions interact to form an effective learning process."}, {"title": "3.2.1 Educators' Expectations: Developing Domain Knowledge and Cognitive Skills.", "content": "Domain Knowledge: Regarding the disciplines, 23 out of 75 educators expect students to build accurate domain knowledge first and foremost. E27 said, \u201cknowing the precise keywords is essential for expanding knowledge,\u201d and E8 added, \u201cEven if it takes longer, focusing on accurately grasping concepts is more important than quick acquisition.\u201d Besides, E37 and E43 emphasized that acquiring correct information is crucial, especially during the early stages of learning: \u201cUnless students have developed enough foundational knowledge to identify errors or biases, they should not be allowed to use automated educational tools.\u201d Educators (E7, E9, E15, E20, E41, E46, and E48) also prioritize that students construct their knowledge through comprehensive coverage, when all the fundamental content is thoroughly addressed. Notably, none of them placed importance on the quantity of knowledge. Overall, we found that the primary competence expected of learners in the course is the accurate and comprehensive construction of knowledge.\nCognitive Skills: On the other hand, we discovered that a majority of them (52%) not only focus on discipline mastery but also expect students to enhance their cognitive skills. This is because, as noted by educators (E7, E13, E35, E68, and E72), these skills are transferable beyond the course and applicable across different disciplines and learning contexts. Specifically, educators emphasize three key abilities that are directly involved in the information-seeking and knowledge-building process: recognizing and addressing gaps in their understanding (metacognition), efficiently searching for and critically assessing large volumes of information (information literacy), and systematically integrating and embedding new knowledge (internalization)."}, {"title": "3.2.2 Factors to Build Strong Domain Knowledge: Reliability, Systematic Organization, and Information Accessibility.", "content": "Reliability: Educators stated that the accuracy of knowledge depends on the reliability of learning sources. Fourteen of them affirmed that textbooks are a highly reliable source. For example, E13 and E68 explained that books compile the accumulated knowledge of experts who are thoroughly trained in their fields and undergo multiple iterations of refinement through rigorous review processes conducted by top professionals in the domain. Additionally, E37 said, \u201cGPT learns from processed web content, and web content is derived from books. Therefore, books, as the original source of data, are the most reliable.\u201d Meanwhile, educators (E2, E63, and E70) also regard Google as a reliable tool because of its high-quality materials, such as publications, well-written posts by experts, and Wikipedia. In contrast, educators (14 of 75) raise concerns about the reliability of ChatGPT's responses, citing issues such as hallucinations and misinformation [10, 46, 53, 75].\nSystematic Organization: Educators described that systematically organized content from educational resources can provide learners with comprehensive coverage of domain knowledge. Books are considered the most effective tool for guiding learners toward structured and inclusive knowledge-building. Specifically, educators (E7, E15, E20, and E21) said that, compared to other tools, books provide content in a systematically organized way: (1) sequential structuring from basic to advanced concepts, and (2) logical progression within topics, such as definitions, explanations, examples, and exercises. Whereas, educators expressed contradictory thoughts about ChatGPT. For example, while E41 and E48 said that \u201cChatGPT readily categorizes information and provides responses in a structured format,\u201d E7 commented that \u201cChatGPT is proficient at answering the questions users ask, but its responses often lack continuity and coherence, coming across as disjointed and isolated rather than part of an organic flow and sequential line of reasoning.\u201d This reveals that educators have conflicting views on whether ChatGPT serves as a systematic tool for supporting comprehensive coverage.\nInformation Accessibility: Nineteen of the educators valued Google as an effective learning tool because of its efficiency in accessing information. For example, E50 and E59 said, \u201cWith the ubiquity of the internet, accessing and searching for information has become much easier and faster, which is likely to be especially useful for students.\u201d Moreover, educators appreciate the ease of access to a wide range of information, from the latest research to posts grounded in human understanding (E3, E36, and E63), along with various modalities beyond text, such as videos and illustrative images (E75). We found that while educators acknowledge ChatGPT's efficient accessibility, they do not encourage its use. This is because educators prioritize the reliability of the source over mere accessibility."}, {"title": "3.2.3 Factors believed to Enhance Cognitive Skills: Cognitive Engagement.", "content": "Out of 75 educators, 64 emphasized that certain phases of the search and learning process must be carried out by students themselves, as only through active and repeated participation can cognitive skills be developed and honed. For example, E46 said, \u201cWithout dedicating sufficient time and effort to self-reflection and independent thinking, learning cannot occur.\u201d E26 and E35 also said that \u201cBy engaging in trial and error while evaluating materials and selecting the most relevant information, students acquire valuable know-how in information-seeking strategies.\u201d Additionally, E68 remarked that \u201cWhile ChatGPT is a useful tool, it may cause students to passively accept information. The ability to judge and filter valuable information from the irrelevant remains crucial, regardless of Al advancements. Therefore, students should be responsible for identifying key points, selecting relevant information, and organizing it.\u201d Unlike books or Google, which necessitate active participation in the learning process, ChatGPT has the potential to offload cognitive effort, leading to passive information consumption. This concern helps explain why most educators view books as the most effective tool, despite the significant time and energy they require, and why educators remain cautious about the implications of LLM-based chatbots."}, {"title": "3.3 Goals and Research Questions", "content": "Collectively, we identified three key concerns raised by educators regarding the integration of LLM-based chatbots into educational settings: (1) lack of reliability, (2) insufficient systematic organization, which educators worry may undermine the development of well-structured domain knowledge, and (3) week cognitive engagement, which they believe could impede the cultivation of essential cognitive skills. Given these concerns, the overarching goal of our study is to explore the potential impact of these highly automated LLM-based chatbots on learning outcomes. Specifically, we aim to determine whether their use might negatively influence the quality and effectiveness of learning. To address these goals, we formulated the following research questions:\n\u2022 RQ1. Does learning with LLM-based chatbots result in less accurate and less comprehensive understanding of domain knowledge compared to traditional learning sources?\n\u2022 RQ2. Does learning with LLM-based chatbots encourage learners to spend less cognitive effort, leading to passive information consumption?"}, {"title": "4 METHODOLOGY", "content": "To investigate the gaps between educators' concerns and the actual effects of automation in search-as-learning contexts, we conducted an empirical study with 92 university students. The study aimed to observe the impact of different levels of automated tools on information-seeking behavior. An overview of the study procedure is illustrated in Figure 6."}, {"title": "4.1 Study Design", "content": "In this work, we designed a counterbalanced mixed-method study that incorporates elements of both within-subjects and between-subjects designs. The study consisted of three sessions, each conducted on a separate day. In the between-subjects component, we compared learning outcomes across three types of sources: books (representing non-automated searching), web search engines (offering partial automation), and LLM-based chatbots (capable of fully automated searching). In the within-subjects component, each participant used a different learning sources in each session, thus experiencing all three conditions over the course of the study. The study conditions were defined as follows:\n\u2022 Condition 1: Book. To ensure a uniform comparison with the other experimental conditions, we simulated a digital reading environment by providing textbooks in PDF format. In order to replicate the experience of reading physical textbooks, digital features such as keyword search were disabled, requiring participants to manually navigate and read through the material.\n\u2022 Condition 2: Web. Participants use the web search engine to seek information by entering relevant keywords and accessing content on various websites. They are restricted to using Google, the most widely used and familiar search engine among participants, to ensure consistency in the data and insights gathered from our analysis.\n\u2022 Condition 3: ChatGPT. Participants are restricted to use ChatGPT as LLM-based chatbots to search for information through a question-answering form. We selected ChatGPT-40 [60] because of its enhanced speed and multimodal capabilities, allowing efficient delivery of both text and visual materials.\nTo eliminate potential learning effects across sessions, we selected three distinct university-level modules from STEM subjects: (a) The Solar System (Astronomy), (b) Sampling (Statistical Mathematics), and (c) Database Management Systems (Computer Science). These modules were randomly assigned to the three learning tools, ensuring that each tool was paired with a different module across the three sessions. In addition, we defined three sequential learning objectives (LOs) for each session, which participants were required to meet. These objectives were aligned with the three levels of Bloom's taxonomy (Understand, Apply, Analyze) [9, 48]. A detailed summary of the learning objectives (LOs) and the rationale for selecting these levels of Bloom's taxonomy is provided in the Appendix A."}, {"title": "4.2 Task Design and Setup", "content": "To address the research questions outlined in Section 3.3, the data collection and evaluation was conducted in two phases. The first phase focused on assessing participants' knowledge comprehension gained from each learning source (RQ1). The second phase examined their learning activities and experiences throughout the sessions (RQ2).\n4.2.1 Measuring knowledge Accuracy and Comprehensiveness. According to the model of comprehension [15, 43\u201345], learners construct and integrate knowledge through multiple levels of mental representation. Specifically, learners first extract key concepts from the materials into working memory, then formulate propositions from those concepts. Finally, they integrate these propositions into a coherent mental model. Based on this, we designed two learning tasks to evaluate participants' knowledge comprehension along three hierarchical levels concepts, connections, and the development of a coherent mental model.\n\u2022 Concept Map Drawing: Concept maps are one of the most widely used tools for approximating learners' understanding within a particular domain or course material [13]. In this study, we employed Novakian concept mapping [12], a method often used to capture a learner's mental model by representing a network of connections between related concepts [19, 55, 57, 58, 63]. For example, participants map out their understanding of the Database Management Systems (DBMS) by including MySQL, Oracle, and MongoDB, using the relationship \u201cexample of\u201d to logically connect them.\n\u2022 Post and Retention Test: To assess the accurate and persistent construction of a mental model for essential knowledge aligned with our LOs, participants perform an immediate post-test after each session and a retention test two weeks later. The post-tests consist of a set of nine multiple-choice questions (MCQs), one of the most commonly used forms of assessment [11, 68]. The retention test consists of the same set of nine MCQs, with both the question order and the options randomized from the post-test. We set Al-generated (GPT) MCQs for the tests, as prior research has shown that LLMs can effectively generate high-quality MCQs that are well-aligned with specific LOs and comparable in quality to those crafted by experts [21, 22]. The process of automatic MCQ generation is detailed in Appendix B, including our prompt engineering, iterative quality evaluations, and examples."}, {"title": "4.2.2 Interface for Learning Activity Logging.", "content": "The SAL logger was instrumented as the apparatus, shown in Figure 5. This system records timestamped user interactions within the browser (e.g., keyboard input, mouse clicks, and dragging) to analyze learner activeness and behavior. We developed the SAL logger as a Chrome extension using HTML and JavaScript. It has a client-server architecture that enables authentication, stores search histories, collects learning logs, and provides study materials in PDF format. The server is implemented using Node.js and stores data in JSON format.\nThe log data collected by the system consists of the following key attributes: participant information (ID, session, assigned source, and module), timestamp, event type, and content. The event type field categorizes interactions, including Info (used to track when experimental information is submitted), Drag (to track which content was dragged), Write (to log entries made in the note panel), and Web_url (to capture website access when the source is web-based). The content field contains data specific to each event type, such as dragged text for a Drag event or a written note for a Write event."}, {"title": "4.3 Data Collection and Analysis", "content": "To analyze participants' search and learning patterns, the SAL logger automatically collected keyboard and mouse events, along with the content they interacted with (e.g., dragged text or clicked links) throughout the session, as described in Section 4.2.2. Additionally, the entire experimental process was video recorded to capture participants' real-time interactions with the system. Following this, participants' concept maps, created using a pen-and-paper approach, were collected and digitized using the NetworkX Python library [29] for quantitative analysis. At the end of the study, a post-test and survey were administered, which included open-ended questionnaires and Likert scale questions regarding participants' perceptions and experiences with the three different learning sources, learning gains, and insights for future design implications of LLM-powered tools for learning. Two weeks later, a retention test was conducted.\nTo quantitatively measure participants' learning gain, we analyze both the digitized concept maps and the post- and retention test results (scaled from 0 to 9). For assessing the concept maps based on content and structure, we employed five network analysis metrics: number of nodes, number of edges, deepest hierarchy level, node degree, and edge consensus. The first three metrics are simple count-based metrics present in the maps, indicating the number of acquired concepts, the connections made between them, and the depth of knowledge structuring. While these traditional metrics are commonly used in the literature to predict an individual learner's understanding, they have limitations in assessing correctness and comprehensiveness. To supplement this, we incorporated two additional metrics node degree and edge consensus based on a group consensus approach, comparing an individual learner's conformity to the larger group's collective understanding [25]. Node degree measures the number of edges connected to a node, while edge consensus evaluates the number of overlapping connections made by other learners. To apply these metrics, we merged all participants' maps (see Figure 6(a)), and measured how accurately individuals identified key nodes and edges, as determined by their peers. Previous research supports the use of these metrics, showing that a group's collective mental model can approximate that of an expert [3, 24, 47, 49]. Furthermore, to assess the degree of engagement throughout the sessions, we measured the number of submitted notes, as well as the average time and number of cognitive activities (i.e.,searching, reading, dragging, navigating, and note-taking) per cycle in the search-as-learning process, based on the collected logs with timestamps. We employed the Bonferroni correction for all statistical tests to avoid potential multiple comparison problems."}, {"title": "4.4 Particpants and Procedure", "content": "4.4.1 Participants. We recruited 92 participants from our university mailing lists and through online advertisements on social media (age=21\u00b13, 46 males and 46 females). To control for prior knowledge, we recruited participants exclusively from our institute, where the modules used in our study are not part of the standard curriculum. We also asked participants to complete a pre-test consisting of three MCQs at the 'remember' level, which aligns well with assessing prior knowledge by requiring participants to retrieve relevant information from long-term memory [48]. We excluded applicants who scored three on the pre-test. The scores of participants were M=0.55, SD=0.68. Participants were randomly assigned to experimental conditions, and no significant differences were observed between the conditions in the pre-test (Kruskal-Wallis H=0.675, p=0.7).\n4.4.2 Procedure. The study was conducted in a controlled setting, either in person or online. The study lasted for three days with 90-minute sessions each day, followed by a 30-minute session two weeks later. Participants received 66,000 KRW (i.e., approximately 49.2 USD) as compensation. Additionally, those whose test results ranked in the top 10% were offered a 20% incentive to further motivate their performance. The study protocol was approved by our institution's IRB, and all study materials used in the study were translated into Korean to prevent any language barriers and reduce unnecessary cognitive load.\nThe study procedure was organized into five phases (Figure 6). During the study phase, participants first installed the SAL logger on the Chrome browser. They were then asked to study the assigned learning objectives using a designated learning source. Participants followed a structured study process designed to track their search-as-learning behaviors. First, participants were instructed to formulate their own queries if they identified any knowledge gaps or internal questions that arose during the study to achieve the LOs. These questions were recorded in the question section of the note panel. To answer the query, participants conducted information searches using the learning source through the browser window. Participants were guided to move the highlighted cursor along with their gaze while browsing, and were asked to drag sections of text they focused on. Any key insights or pertinent information discovered during the search could be transferred or restructured in the memo section of the note panel for further clarification. Once participants felt that they had resolved their knowledge gap, they were asked to submit their answers in the answer section of the panel. After submitting the completed note, participants added a new note to the panel and repeated the process throughout the study phase. Previously submitted notes could be modified and resubmitted later.\nNext, participants were given 5 minutes to review and reflect on their notes. Afterward, they moved on to the concept mapping phase, which lasted 20 minutes. During this phase, participants hand-drew labeled nodes to represent concepts and linked them to illustrate the relationships between the concepts they had acquired during the session. Following this, participants entered the test phase, where they had 15 minutes to complete a post-test consisting of 9 MCQs related to the module. After completing the test, participants used an online form to answer a post-survey, which included several questions about their experience and the effectiveness of each search tool for learning. This process was repeated over three consecutive days, and two weeks later, participants were asked to complete a 30-min retention test."}, {"title": "5 RESULTS", "content": "Overall, a comparative analysis of the outcomes from the three experimental groups (Book vs. Web vs. GPT) reveals that LLM-based chatbots were competitive at lower levels of understanding (i.e., formation of concepts and connections) and outperformed books in helping learners build coherent mental models. However, LLMs led to reduced retention of information compared to books (RQ1). Additionally, our results show there were no significant differences in the average cognitive effort expended per search-as-learning cycle among different learning sources (RQ2). The following outlines our findings for each research question, along with participants' perspectives and insights directly comparing their experiences with each learning source."}, {"title": "5.1 RQ1: Accuracy and Comprehensive Coverage in Knowledge Building", "content": "To investigate whether LLM-based chatbots hinder learners' understanding in terms of accuracy and comprehensive coverage, we first analyzed 276 concept maps (92 participants x 3 conditions) to assess fundamental level of understanding, which indicates how well learners identified and connected key concepts aligned with the given learning objectives (section 5.1.1). Next, we analyzed post-test and two-week-after retention test scores to assess a more advanced level of understanding (section 5.1.2), focusing on how well the knowledge was retained and applied within their mental models. Detailed module-level results are provided in Appendix C."}, {"title": "5.2 RQ2: Assessing Degree of Cognitive Engagement", "content": "To investigate whether LLM-based chatbots promote passive information consumption rather than active engagement into the search-and-learning process, we first refined 12,287 activity logs collected throughout the study. From this data, we analyzed participants' activeness levels during information-seeking behaviors and compared the number of completed notes. By understanding these two factors together, we aimed to assess the degree of cognitive engagement across different learning tools.\n5.2.1 Assessing Activeness Levels. To assess activeness during the sessions, we quantitatively analyzed the average number of cognitive activities such as searching, navigating, dragging, clicking and note-taking for each participant over time (Figure 9). Surprisingly, contrary to educators' expectations, the results showed that the GPT group exhibited the highest level of activeness throughout the sessions, followed by the web group, and lastly the book group. Additionally, the trendline for participants using books showed a noticeable decline in activeness as the session progressed. In contrast, the activeness levels for participants using both GPT and web search engines remained steady, indicating sustained engagement throughout the session. From this observation, we suggest that the effort required to manually complete all steps with books may have caused participants to fatigue more quickly, resulting in decreased activeness in the latter part of the session. This contrasts with the GPT and web conditions, where automated features likely reduced the cognitive burden and helped participants maintain their engagement over time. However, these results alone cannot fully address our RQ2, as they may not necessarily reflect active cognitive engagement. Therefore, we further analyzed the number of completed notes within each session to assess the time and effort invested in each search-as-learning cycle. By examining the quantity of notes relative to the activeness level, we aimed to determine whether participants were deeply engaged in the learning process or merely exhibiting frequent interactions without substantial cognitive effort, as discussed in the next subsection.\n5.2.2 Comparison of completed notes. To investigate whether the number of completed notes differed between learning sources, we performed one-way ANOVA tests (Figure 10). The results showed no significant main effect of learning sources for the solar system module (F(2,89) = 3.041, p = 0.053) or the sampling module (F(2,89) = 0.867, p = 0.424), but a significant difference was found for the DBMS module (F(2,89)=4.611, p=0.012, \u03b72=0.094). However, despite GPT's time-efficiency, no remarkable differences in the number of notes completed were observed compared to other sources. Similarly, the search engine, a partially automated tool, did not show a meaningful difference from the book group. This suggests that participants engaged in each search-as-learning cycle with similar time and effort, regardless of the level of automation in the learning source. Notably, although the activeness level for LLM-based chatbots was higher than for other sources, the number of completed notes did not differ significantly. This challenges educators' concerns that fully automated tools like GPT may encourage passive learning approaches. Therefore, our findings support RQ2, suggesting that LLMs do not reduce the cognitive effort invested compared to traditional sources."}, {"title": "5.3 Summary", "content": "Through the study, we found that LLM-based chatbots did not hinder understanding of domain knowledge; in fact, participants absorbed more content overall. Moreover, automation levels did not deactivate participants' search and learning behaviors. On the contrary, using non-automated tools throughout the entire process may lead to quicker exhaustion. Nonetheless, this self-navigated"}]}