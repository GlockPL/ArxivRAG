{"title": "MoEMba: A Mamba-based Mixture of Experts for High-Density\nEMG-based Hand Gesture Recognition", "authors": ["Mehran Shabanpour", "Kasra Rad", "Sadaf Khademi", "Arash Mohammadi"], "abstract": "High-Density surface Electromyography (HD-sEMG) has emerged as a pivotal resource for Human-Computer\nInteraction (HCI), offering direct insights into muscle activities\nand motion intentions. However, a significant challenge in\npractical implementations of HD-sEMG-based models is the\nlow accuracy of inter-session and inter-subject classification.\nVariability between sessions can reach up to 40% due to\nthe inherent temporal variability of HD-sEMG signals.\nTargeting this challenge, the paper introduces the MoEMba\nframework, a novel approach leveraging Selective State-\nSpace Models (SSMs) to enhance HD-sEMG-based gesture\nrecognition. The MoEMba framework captures temporal\ndependencies and cross-channel interactions through channel\nattention techniques. Furthermore, wavelet feature modulation\nis integrated to capture multi-scale temporal and spatial\nrelations, improving signal representation. Experimental\nresults on the CapgMyo HD-sEMG dataset demonstrate\nthat MoEMba achieves a balanced accuracy of 56.9%,\noutperforming its state-of-the-art counterparts. The proposed\nframework's robustness to session-to-session variability and its\nefficient handling of high-dimensional multivariate time series\ndata highlight its potential for advancing HD-sEMG-powered\nHCI systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Surface ElectroMyoGraphy (sEMG) signals [1] are a\npromising resource for Human-Computer Interaction (HCI),\nproviding direct insights into muscle activities due to their\nability to encapsulate motion intentions [2]. As such, sEMG-\nbased gesture recognition frameworks emerged as the core\ntechnology in developing cutting-edge Muscle-Computer In-\nterfaces (MCIs), facilitating applications ranging from ac-\ntive prostheses [3]-[7], wheelchairs [8], [9] and exoskele-\ntons [10], as well as in neuromuscular diagnosis [11],\nneurorehabilitation [12], and video game interactions [13].\nRecent advancements in Artificial Intelligence (AI)-driven\nsolutions have significantly enhanced the development of\nSEMG-powered MCIs. AI-based MCI tasks range from dis-\ncrete movement classification and joint angle estimation\nto force/torque estimation offering significant potential to\nadvance EMG pattern recognition. Despite the recent surge\nof interest, such models fail to address critical challenges\nfaced in non-ideal, practical conditions, including variations\nin electrode placement and changes in muscle state both\nacross different subjects and within the same person over\ntime [14], [15].\nLiterature Review: MCI systems commonly rely on a set\nof standardized functions to represent features from different\ndomains, enhancing the information density embedded in\nhigh-density (HD)-sEMG signals, and improving the dis-\ncrimination of different gestures. The extracted representa-\ntions can be utilized for sEMG pattern recognition through\nvarious classifiers, which are broadly classified into two\nprimary categories:\nMachine Learning (ML)-based Methods [16]: ML models\nrely on feature engineering, where relevant features are\ndesigned and extracted manually from signals encompass-\ning three primary categories: time-domain (TD), frequency-\ndomain (FD), and time-frequency domain (TFD) features.\nTD features [6], [7] are derived directly from raw sEMG\nsignals, representing their time-dependent variations, and are\nparticularly beneficial due to their low computational com-\nplexity. In contrast, FD features [17] are obtained through\nthe Fourier transform of the sEMG signal's autocorrelation\nfunction. Building upon these approaches, TFD features [18]\ncombine both time and frequency information, providing\na comprehensive view of the signal's energy distribution,\nwith wavelet transform being a commonly used technique\nfor this analysis. Despite the availability of various feature\nextraction techniques, the multidimensional, non-linear and\nmultichannel nature of sEMG coupled with its inherent non-\nstationary nature, makes this signal challenging for effective\ntraining of ML models [19].\nDeep Learning (DL)-driven Models [20]: DL, on the con-\ntrary, is distinguished by its hierarchical architecture. This\nstructure enables the model to progressively extract complex\nfeature representations, automatically capturing underlying\npatterns and relationships within the data at various levels.\nThe capability of autonomously extracting spatial features\nmade Convolutional Neural Networks (CNNs) [21], [22]\na leading choice for SEMG gesture recognition in DL-\ndriven models. However, limited receptive fields of CNNS\nrestrict their ability to capture long-range dependencies in\nSEMG signals, which are inherently sequential time series\ndata. In contrast, Recurrent Neural Networks (RNNs) [23],\n[24], designed specifically to model temporal relationships\nand sequential patterns, offer a more suitable approach for\nanalyzing such signals. Consequently, several approaches\nutilize hybrid CNN-RNN models [19], [25]. This combined\narchitecture excels in capturing both spatial and temporal\nfeatures. On the other hand, transformers offer a compelling\nalternative with their self-attention mechanism [26], [27],\ngranting global receptive fields and enabling superior capture\nof cross-time dependencies, addressing the limitations of\nboth CNNs and RNNs in this regard. This global perspective,\nwhile advantageous, is computationally expensive. The self-\nattention mechanism necessitates each element attend to\nall others, leading to computational overhead that scales\nquadratically with sequence length [25], [28].\nTargeted Challenges: Although DL-driven models have\nachieved remarkable accuracies in intra-session tasks, their\nperformance diminishes in inter-session and inter-subject\nscenarios [29], [30]. In the context of sEMG analysis,\nthese terms describe distinct data collection paradigms: intra-\nsession involves data from a single subject during a con-\ntinuous recording, inter-session refers to recordings from\nthe same subject across multiple sessions, and inter-subject\npertains to data from different individuals.\nIn greater detail, a key challenge in practical implemen-\ntations is the low accuracy of inter-session and inter-subject\nclassification, with session-to-session variability reaching as\nhigh as 10-40%, which arises from the inherent temporal\nvariability of sEMG signals. This variability is influenced\nby several factors, including muscle condition (fatigue, at-\ntrophy, or hypertrophy), changes in skin impedance (such\nas sweating), anatomical differences among subjects, non-\nstationarities due to limb positioning and force variations,\nand electrode drift, all of which contribute to accuracy degra-\ndation. Moreover, these factors can introduce considerable\ninconsistencies in signal patterns across different sessions\nand individuals, thus complicating the generalization of\ngesture recognition algorithms [20], [30]\u2013[32].\nThe challenge of between-session distribution shifts has\nbeen conventionally handled through experimental design\nby developing multi-user and multi-session training proto-\ncols, as well as data processing techniques such as data\naugmentation [30], [31], unsupervised learning, and transfer\nlearning [33], [34] with advanced DL architectures. Despite\noffering improvements in accuracy, these methods often\nrequire large amounts of adaptation data [28], [31], [35].\nMoreover, SEMG data collection is a time-consuming and\nresource-intensive process [30], [35], requiring specialized\nequipment and trained personnel, which further limits the\navailability of data. Moreover, the significant computational\ndemands of these methods restrict their feasibility for real-\ntime applications, particularly in resource-constrained em-\nbedded systems [28], [31].\nA key consideration, often overlooked in most above-\nmentioned models, is the inter-channel dependencies inherent\nin sEMG data. Gesture movements involve complex inter-\nmuscular coordination, which is reflected in the inter-channel\nrelationships captured by the placement of sEMG electrodes.\nMany models incorrectly apply a channel-independent ap-\nproach, treating the data as uncorrelated univariate time\nseries and thus neglecting these essential inter-channel re-\nlationships [25], [28], [33]. Although some approaches [36],\n[37] attempt to address this limitation by combining channels\nusing mechanisms such as self-attention, linear combina-\ntions, or convolutions, these methods are computationally\nexpensive and generally miss proportional relationships by\nmodeling inter-channel relationships as weighted sums.\nContributions: Recognizing the crucial role of cross-\nchannel dependencies in multivariate time series, we aim to\ncapture temporal dependencies in HD-sEMG signals using\nSelective State Spaces (SSS), and address cross-channel\ninteractions through techniques of channel attention. Recent\nadvancements in State-Space Models (SSMs) [28], notably\nMamba, offer efficient modeling of sequential data dynamics\nwith linear computational complexity without losing the\nglobal receptive field, even for long sequences, positioning\nthem as a potential alternative to transformers. In other\nwords, SSMs adopt an RNN-like approach that facilitate\ndynamic adaptation to data distribution shifts, enabling the\ncapture of diverse temporal features such as long-term trends\nand short-term fluctuations.\nThe paper introduces the MoEMba framework built upon\nthe Mamba architecture\u2014a novel lightweight advanced\nSSM-based approach\u2014specifically designed to address the\ncritical challenge of session-to-session variability while pri-\noritizing simplicity and efficiency. In summary, the paper\nmakes the following contributions:\nAs the first application of Mamba for HD-sEMG hand\ngesture recognition, MoEMba model uses an adaptive\ncombination of multiple Mamba experts within a Mix-\nture of Experts (MoE) configuration to capture both\nshort-term and long-term gesture dynamics.\nWavelet Transform Feature Modulation (WTFM) is in-\ncorporated to capture multi-scale temporal and between-\nchannel spatial relations, integrating both time-domain\nand frequency-domain information to enhance signal\nrepresentation.\nThe MoEMba's design, including its architecture and\nfeature extraction, is computationally efficient requiring\nfewer Floating Point Operations per Second (FLOPS)\nthan State-Of-The-Art (SOTA) models. This efficiency,\ncombined with robustness to session-to-session vari-\nability in HD-sEMG recordings makes MoEMba ideal\nfor real-world applications like prosthetic control and\nhuman-computer interaction.\nThe remainder of the paper is organized as follows: Section II\nbegins by describing the dataset used in this research and\nprovides an overview of the relevant background. Section III\nintroduces our proposed framework, MoEMba, detailing its\narchitecture and key components. Section IV presents a\ncomprehensive analysis of the experimental results. Finally,\nSection V concludes the paper."}, {"title": "II. MATERIALS AND METHODS", "content": "In this section, first, we briefly present the HD-sEMG\ndataset used for development and testing of the MoEMba\nframework. Then, required background on State-Space Mod-\nels (SSMs) is introduced."}, {"title": "A. Dataset", "content": "In this study, we used the CapgMyo HD-SEMG\ndataset [30], recorded from 128 channels sampled at 1000Hz.\nWe utilized sub-databases DB-a (23 subjects performing\n8 gestures held for 3 10s) and DB-b (a subset of\nDB-a comprising 10 subjects with recordings from two\nsessions separated by a minimum of 7 days, performing\nthe same 8 gestures with hold durations of ~3s). The\nCapgMyo HD-sEMG dataset configuration introduces inter-\nsession and inter-subject variability, which challenge model\nrobustness an issue that this study aims to address. To\ncapture richer semantic information and local features, the\ndata was pre-processed using 45 - 55Hz Butterworth filter,\nfollowed by segmentation into overlapping 64ms windows\nwith 8ms steps, resulting in 64 \u00d7 128 matrices, and finally\nnormalized to [-1,1]."}, {"title": "B. Selective State-Space Models", "content": "The SSMs are mathematical models used to represent dy-\nnamic systems through state variables [38]. Fundamentally,\nSSMs characterize the evolution of a system through two\nprimary equations, i.e., the state model, and the observation\nmodel. The state model defines the evolution of the hidden\nstate h(t) over time as influenced by the input x(t)\n$$h'(t) = Ah(t) + Bx(t),$$\nwhere A \u2208 RN\u00d7N is the state transition matrix, B\u2208 RN\u00d71\nis the input matrix, h(t) \u2208 RN is the hidden state at time t,\nand h'(t) is the derivative of the hidden state. This evolution\nof the hidden state is then used by the observation equation\nto determine the output y(t)\n$$y(t) = Ch(t) + Dx(t),$$\nwhere C\u2208 IR1\u00d7N is the output matrix, and D\u2208 R is the\ndirect feedthrough term (often set to zero). The application\nof SSMs within ML context requires the discretization of\nthe continuous-time formulations. A common discretization\nmethod is the Zero-Order Hold (ZOH), which assumes a\nconstant function value over an interval. After ZOH dis-\ncretization, the SSM equations can be rewritten as follows\n$$h_k = Ah_{k-1}+ Bx_k,$$\n$$Y_k = Ch_k,$$\nwhere A = exp(\u2206\u0391), and B = (\u0394\u0391)-1(exp(\u0394\u0391) \u2013 I).\nFurthermore, $h_k$ is the hidden state at discrete time step k,\n$X_k$ and $y_k$ represent the input and output at discrete time\nstep k, and A = [tk-1,tk]. However, a key limitation of\ntraditional SSMs lies in their Linear Time-Invariant (LTI) na-\nture, where fixed parameters such as A, B, C, and A restrict\ntheir ability to adapt to diverse sequences. Mamba [39], a\nrecent model developed based on SSMs, addresses this issue\nby introducing a selection mechanism that parameterizes\nthese matrices as functions of the input x, enabling input-\ndependent dynamics. The parameterization is defined as\n$$B\u2192 SB = WBx,$$\n$$C\u2192 SC = WCx,$$\n$$A\u2192SA = TA\u00b7BroadCast D (W^x),$$\nwhere WB, WC, and W\u25b3 are learnable projection matrices,\nTA is the softplus activation function, and BroadCast D\nis a function that replicates the result of W\u25b3x across all\nfeature dimensions. This transition to a time-variant model\nallows Mamba to adaptively filter irrelevant information\nwhile retaining critical context, making it highly effective\nfor tasks requiring long-context modeling [40]."}, {"title": "III. THE MOEMBA FRAMEWORK", "content": "The proposed MoEMba framework is built based on the\nMamba architecture, chosen for its efficient inference and\nability to handle long sequences in light of its linear scaling\nwith sequence length. MoEMba decodes muscle activity and\nrecognizes gestures by establishing contextual dependencies\nacross EMG signal channels. As depicted in Fig. 1, the ar-\nchitecture comprises of three key modules: (i) WTFM Block:\nThis block is employed as a shallow feature extraction mod-\nule to enhance multi-scale temporal representations; (ii) MoE\nBlock: This block combines Mamba-based experts via sparse\ngating for dynamic pattern modelling; and (iii) Classification\nBlock: A voting mechanism is used in the classification block\nto ensure robust signal-level predictions. Detailed description\nof each block is presented in the subsequent sub-sections."}, {"title": "A. The WTFM Block", "content": "In multivariate time series classification, particularly when\ndealing with high-dimensional data such as HD-sEMG sig-\nnals, capturing both local temporal dynamics and global\ncontextual information is crucial. In this regard and inspired\nby a recent work [41], we adopt a multi-scale representation\nlearning through wavelet transform feature modulation. Such\nan approach enhances feature representation learning from\nHD-SEMG signals.\nThe MoEMba processes each signal patch, P; \u2208 RL\u00d7V, by\nthe WTFM block, where L defines the temporal length and\nV denotes the number of channels. This 2D representation\nfacilitates convolutional operations, allowing a more effective\ncapture of spatial relationships across channels. The WTFM\nblock initially extracts local and global temporal features\nusing small (e.g., 3 \u00d7 3) and large (e.g., 7 \u00d7 7) tempo-\nral convolutional kernels, respectively. A Discrete Wavelet\nTransform (DWT) is then applied to the local temporal fea-\ntures (extracted by the small temporal convolutional kernel),\ndecomposing them into four directional components: approx-\nimation (CA), horizontal detail (CH), vertical detail (cv), and\ndiagonal detail (CD). These components are calculated based\non different frequency bands of the wavelet decomposition.\nThese components capture detailed temporal variations at\ndifferent scales. DWT components are then upsampled back\nto the original size of Pi using bicubic interpolation. These\nupsampled wavelet features are then refined using a channel\nattention mechanism, which learns to focus on the most\nimportant channels. The attention weights \u03b1\u03b9 are given by\n$$\u03b1_\u03b9 = \u03c3(W_2 \u00b7 ReLU(W_1\u00b7 MaxPool(c_i))),$$\nwhere W\u2081 and W2 are learnable parameters, c refers to\nwavelet components ($C_A, C_H, C_V,$ and $c_D$) associated with ith\npatch. MaxPool(\u00b7) reduces the spatial dimensions, and \u03c3(\u00b7) is\nthe sigmoid function. The enhanced wavelet features are then\ncombined with the original large-scale features (extracted by\nthe large temporal convolutional kernel) using a Hadamard\nproduct to capture both global and local information. This\nmodulation process enhances the model's ability to dis-\ncriminate subtle temporal patterns within the EMG signals\nwhile retaining crucial global information. After modulation,\nthe refined large-scale features are concatenated with the\nsmall-scale features to integrate both high-level temporal\nrepresentations and fine-grained local details. This fusion\nenables the model to utilize complementary information from\ndifferent temporal scales, further improving its ability to\ncapture intricate sEMG signal patterns. Finally, the combined\nfeature set (FWTFM) is fused with raw patch embeddings\n(Fchan) obtained via 1D convolutional value embedding\nfunction encapsulating the original signal's channel charac-\nteristics to integrate both FD and TD information, improv-\ning encoder performance. The model backbone network then\nprocesses the fused features Zp for deeper feature extraction.\n$$Z_p = Concat(F_{WTFM}, F_{chan}), where Z_p \u2208 R^{N_s\u00d7n\u00d7E\u00d7V}$$\nThis completes description of the WTFM block of the\nproposed MoEMba framework. Next, we present the MoE\nmodule of the proposed MoEMba framework."}, {"title": "B. The MoE Block", "content": "The MoE block is utilized with each expert being imple-\nmented as a Mamba module. The MoE framework consists\nof a set of n experts (E1,..., En), each specializing in\nlearning distinct patterns within the time series data, and\na gating network (G) that dynamically computes a sparse\nn-dimensional vector to weight the contributions of each\nexpert. Given a sequence of embedded patches Zp, the final\noutput Ze is computed from the expert outputs as follows\n$$Z_o = \\sum_{i=1}^{\\eta} G_i(Z_p)\u00b7 E_i(Z_p).$$\nThis formulation allows the MoE framework to adaptively\ncombine the strengths of multiple Mamba experts, facilitating\nrobust modeling of both short-term and long-term trends in\nhand gesture data.\nWe incorporate an adapted gating mechanism inspired\nby [28] that introduces optimizing enhanced sparsity to\noptimize computational efficiency of the underlying model.\nMore specifically, a tunable sparsity mechanism is employed\nwhere Gaussian noise, controlled by a trainable weight\nmatrix W noise, is added to the network output before applying\nthe Softmax function. To further promote sparsity, only the\ntop k values from the network output are retained, while\nthe remaining values are suppressed to -\u221e, effectively\nsetting their corresponding gate values to zero. This selective\napproach is formalized as follows\n$$G(Z_p) = Softmax(TopKGating(H(Z_p),k)),$$\nwhere Hi(Zp) = (ZpWgate)i + Softplus((ZpWnoise)i), \nand TopKGating(v, k); retains only the top k elements of v,\nsetting the rest to -\u221e. Here, Wg gate and Wnoise are trainable\nweight matrices. To prevent the potential bias in the gating\nnetwork towards a single expert, a balanced gate mechanism\nis implemented. This is achieved by minimizing a balance\nloss LB, which encourages a more uniform distribution of\nsamples across experts. The balance loss is defined as\n$$L_B(Z_p) = \u03bb\u03b2\u00b7 CV(Load(Z_p))^2,$$\nwhere CV(.) is the coefficient of variation of the load\nvector, the Load function (Load()) is a smooth estimator\nused to quantify the distribution of data samples across the\ndifferent experts, and AB is a scaling factor. Additionally, a\nregularization term Lz has been used to penalize large logits\nwithin the network, further stabilizing the gating mechanism\n$$L_z(Z_p) = \\frac{1}{n} \\sum_{i=1}^{n} log(\\sum_{j=1}^{\\eta} e^{(Z_p)j})^2$$\nwhere n is the number of patches and \u03b7 is the number of\nexperts. The total auxiliary loss Laux is a weighted combi-\nnation of these losses, integrated into the overall model loss\nfunction to ensure balanced and efficient expert utilization.\nSuch a gating network allows to capture complex temporal\ndependencies in HD-sEMG signals."}, {"title": "C. Classification Block", "content": "The final stage of the MoEMba uses a classification head\nto process the MoE output, normalizing and activating it\nprior to projection into the class space. The resulting logits\nare then transformed into probabilities via softmax, and\nthe predicted class is determined by selecting the highest\nprobability. Afterwards, predictions from patches belonging\nto the same signal are aggregated, and majority voting is used\nto determine the final classification of the entire signal. This\napproach reduces the impact of noisy/inconsistent predictions\nat the patch level."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "In this section, we evaluate performance of the proposed\nMoEMba framework on the CapgMyo DB-b HD-sEMG\ndatasets [30"}]}