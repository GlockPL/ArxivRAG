{"title": "LEARNING GENERAL REPRESENTATION OF 12-LEAD\nELECTROCARDIOGRAM WITH A JOINT-EMBEDDING PREDICTIVE\nARCHITECTURE", "authors": ["Sehun Kim"], "abstract": "We propose a self-supervised learning method for 12-lead Electrocardiogram (ECG) analysis, named\nECG Joint Embedding Predictive Architecture (ECG-JEPA). ECG-JEPA employs a masking strategy\nto learn semantic representations of ECG data. Unlike existing methods, ECG-JEPA predicts at\nthe hidden representation level rather than reconstructing raw data. This approach offers several\nadvantages in the ECG domain: (1) it avoids producing unnecessary details, such as noise, which is\ncommon in standard ECG; and (2) it addresses the limitations of na\u00efve L2 loss between raw signals.\nAnother key contribution is the introduction of a special masked attention tailored for 12-lead ECG\ndata, Cross-Pattern Attention (CroPA). CroPA enables the model to effectively capture inter-patch\nrelationships. Additionally, ECG-JEPA is highly scalable, allowing efficient training on large datasets.\nOur code is openly available https://github.com/sehunfromdaegu/ECG_JEPA.", "sections": [{"title": "1 Introduction", "content": "Electrocardiograms (ECG) provide a non-invasive method to measure the electrical activity of the heart over time,\nserving as a crucial tool for diagnosing various cardiac conditions. While numerous supervised learning models have\nbeen developed to detect heart diseases using ECG data [1, 2, 3], these models often face significant performance\ndegradation when applied to data distributions different from those on which they were trained.\nSelf-supervised learning (SSL) has emerged as a powerful paradigm for learning general representations across various\ndomains, including natural language processing [4, 5, 6], computer vision [7, 8, 9], and video analysis [10, 11]. Despite\nits promise, applying SSL to ECG data presents unique challenges. For instance, data augmentation, which is essential\nin many SSL architectures, is more complex for ECG than for computer vision (CV) data. Simple transformations like\nrotation, scaling, and flipping, effective in CV, can distort the physiological meaning of ECG signals. Additionally, ECG\nrecordings often contain artifacts and noise, which cause autoencoder-based SSL models to struggle with reconstructing\nraw signals. These architectures may also miss visually subtle but diagnostically critical features, such as P-waves and\nT-waves, which are imperative for diagnosing certain cardiac conditions.\nIn this work, we propose a novel ECG Joint-Embedding Predictive Architecture (ECG-JEPA) tailored for 12-lead\nECG data, effectively addressing the aforementioned challenges. ECG-JEPA utilizes a transformer architecture to\ncapture the semantic meanings of ECG. By masking several patches of the ECG data, ECG-JEPA predicts the abstract\nrepresentations of the missing patches, indicating a high-level understanding of the data. Additionally, we develop a\nnovel masked-attention for multi-lead ECG data, coined Cross-Pattern Attention (CroPA). CroPA enables the transformer\narchitecture to effectively capture the relationships between patches in multi-lead ECG.\nIn this work, we demonstrate that:\n\u2022 ECG-JEPA significantly improves linear evaluation and fine-tuning on classification tasks compared to existing\nSSL methods without hand-crafted augmentations."}, {"title": "2 Background", "content": "Self-Supervised Learning (SSL) facilitates learning abstract representations from input data without the need for labeled\ndata, which is particularly beneficial in medical domains where labeled data is scarce and expensive. SSL leverages\ninherent data patterns to learn useful representations, allowing models to adapt to various downstream tasks with greater\nrobustness to data imbalances [12]."}, {"title": "2.1 Generative Architectures", "content": "Generative architectures involve reconstructing an input $x$ from its degraded version $x'$ using an encoder-decoder\nframework. The idea behind generative architectures is that the ability to reconstruct the clean data from a corrupted\none indicates a good understanding of the data. The encoder maps the perturbed input $x'$ into a latent representation,\nwhich the decoder then uses to generate the original input $x$ [13]."}, {"title": "2.2 Joint-Embedding Predictive Architectures", "content": "Joint-Embedding Predictive Architectures (JEPA) [14] process pairs $x$ and their corrupted versions $x'$ to obtain\nrepresentations $z$ and $z'$ through encoders. Unlike generative architectures that predict in the input space, JEPA predicts\nin the hidden representation space by reconstructing $z$ from $z'$. This approach effectively avoids the challenge of\npredicting unpredictable details, which is common in biological signals."}, {"title": "2.3 Electrocardiogram (ECG)", "content": "The electrocardiogram (ECG) is a non-invasive diagnostic method that records the heart's electrical activity over time\nusing electrodes placed on the skin. The standard 12-lead ECG captures electrical activity from multiple angles. These\n12 leads are categorized into limb leads (I, II, III), augmented limb leads (aVR, aVL, aVF), and chest leads (V1-V6).\nEach lead provides unique information about the heart's electrical activity, offering a comprehensive view that aids in\ndiagnosing various cardiac conditions."}, {"title": "2.4 ECG Features", "content": "ECG features are specific characteristics of ECG signals that are critical for summarizing the overall signal. These\nfeatures play an essential role in monitoring a patient's health status and are instrumental in the application of statistical\nmachine learning models for diagnosing heart diseases. Key ECG features include heart rate, QRS duration, PR interval,\nQT interval, and ST segment. These features are identified by measuring specific time intervals or amplitude levels in\nthe ECG waveform. For instance, heart rate is calculated using the formula $1000 \\times (60/RR\\ interval)$, where the RR\ninterval is measured in milliseconds (ms)."}, {"title": "3 Methodology", "content": "ECG-JEPA is trained by predicting masked representations of ECG data within the hidden representation space, using\nonly a partial view of the input. The proposed architecture utilizes a student-teacher network framework, as illustrated\nin Figure 3. We subdivide the multi-channel ECG into non-overlapping patches and sample a subset of these patches\nfor masking. However, reconstructing the raw signals of masked patches can be particularly challenging in the ECG\ndomain due to the prevalence of noise in biological signals. Instead, our model predicts the masked patches in the\nhidden representation space, where this challenge can be effectively addressed. We validate the quality of the learned\nrepresentations through various downstream tasks, including linear probing, fine-tuning on classification tasks, and\nECG feature extraction tasks."}, {"title": "3.1 Patch Masking", "content": "Let $x \\in \\mathbb{R}^{L \\times T}$ represent a multi-lead ECG of length $T$ with $L$ channels. We divide the interval $[0,T]$ into $N$ non-\noverlapping subintervals of length $t$. Each subinterval in each channel constitutes a patch of $x$, resulting in $L \\times N$\npatches. The masking strategy in multi-lead ECG must be carefully chosen because patches in different leads at the\nsame temporal position are highly correlated, potentially making the prediction task too easy. To address this, we mask\nall patches across different leads in the same temporal space. With this in mind, we employ two masking strategies:\nrandom masking and multi-block masking.\nIn random masking, we randomly select a percentage of subintervals to mask, while in multi-block masking, we\nselect multiple consecutive subintervals to mask. Note that we allow these consecutive subintervals to overlap, which\nrequires the model to predict much longer sequences of representations. In this paper, we use both masking strategies to\nevaluate the effectiveness of ECG-JEPA, with a random masking ratio of (0.6, 0.7) and a multi-block masking ratio of\n(0.175, 0.225) with a frequency of 4. The unmasked patches serve as the contextual input for the student networks,\nwhile the masked patches are the ones for which we aim to predict the representations.\nThe patches are then converted into sequences of token vectors using a linear layer, and augmented with positional\nembeddings. For positional encoding, we employ the conventional 2-dimensional sinusoidal positional embeddings\nfor the student and teacher networks, while we use 1-dimensional sinusoidal positional embeddings for the predictor\nnetwork."}, {"title": "3.2 Teacher, Student, and Predictor", "content": "ECG-JEPA consists of three main components: the teacher network, the student network, and the predictor network.\nBoth the teacher and student networks are based on standard transformer architectures. The weights of the teacher\nnetwork are updated through an exponential moving average (EMA) of the student network, with details provided in the\nsupplementary materials. The predictor network, a smaller transformer, operates on single-channel representations.\nThe teacher network handles the entire $L \\times N$ patches, generating fully contextualized $L \\times N$ representations. The\nstudent network, however, processes only $L \\times Q$ visible (unmasked) patches, where $Q < N$ represents the number of\nvisible time intervals. These $L \\times Q$ representations from the student are then concatenated with the (learnable) mask\ntokens, resulting in $L \\times N$ representations. Subsequently, each lead's representations are passed to the predictor, which\nprocesses single-channel representations. The predictor's output, the predicted representations of the target patches, is\ncompared with the target representations using a smooth L1 loss function."}, {"title": "3.3 Cross-Pattern Attention (CroPA)", "content": "Biological signals often require simultaneous observation across multiple channels. In a 12-lead ECG, abnormalities\nmust be consistent across multiple leads to diagnose specific cardiac pathologies. Therefore, the meaning of a single\npatch should be understood in the context of other patches within the same channel and across different channels. In\ncontrast, patches in different channels and temporal spaces are often neglected when interpreting a 12-lead ECG.\nTo address this, we introduce Cross-Pattern Attention (CroPA), a masked self-attention tailored for multi-lead ECG data.\nCroPA allows each patch to attend only to patches within the same channel and temporal space (Figure 4). This method\nenhances our model's effectiveness compared to the standard attention mechanism, where patches attend to all other\npatches indiscriminately."}, {"title": "4 Related Work", "content": "This section provides an overview of existing SSL methods that have been applied to ECG data. We compare ECG-JEPA\nwith these methods on various downstream tasks in the next section."}, {"title": "4.1 Contrastive Learning", "content": "Contrastive learning has been widely explored in various research works for its effectiveness in unsupervised representa-\ntion learning. Contrastive Predictive Coding (CPC) [15] aims to capture useful data representations by predicting future"}, {"title": "4.2 Generative Architectures", "content": "BERT [4] pioneered generative architectures in natural language processing (NLP), inspiring similar methods in other\ndomains. Masked Autoencoders (MAE) [8] have been developed for computer vision, where a portion of the input\nimage is masked, and the model is trained to reconstruct these missing parts. Recently, [17] pointed out that generative\narchitectures often prioritize learning the principal subspaces of the data, which may limit their effectiveness in capturing\nsemantic representations for perceptual tasks."}, {"title": "4.3 Joint-Embedding Predictive Architectures", "content": "Data2vec [18] introduced an SSL architecture that predicts the average of the encoder's final layers' outputs, which was\nfurther improved in efficiency in data2vec 2.0 [19]. Notable models like I-JEPA [9] and V-JEPA [11] predict the hidden\nrepresentations of masked patches, enabling semantic representation learning for images and videos, respectively."}, {"title": "4.4 Self-Supervised Learning for ECG", "content": "Several attempts have been made to train SSL models for 12-lead ECG data. Contrastive Multi-segment Coding (CMSC)\n[20] is a contrastive method that divides an ECG into two segments, encouraging close representations for compatible\nsegments while pushing apart incompatible ones. Contrastive Predictive Coding (CPC) [15] has also been used for\nECG data in [21], predicting future representations of the ECG. A significant disadvantage of CPC is that it is not\nscalable to large datasets, as the LSTM modules inside CPC make the training process extremely slow. Recently, [22]\nutilized masked autoencoders for ECG data, proposing temporal and channel masking strategies, coined as Masked\nTime Autoencoder (MTAE) and Masked Lead Autoencoder (MLAE), respectively. Similarly, [23] proposed ST-MEM, an\nMAE for ECG data that masks random time intervals for each lead. One possible drawback of MLAE and ST-MEM is\nthat predicting some patches can be relatively easy considering that 12 leads signals are highly correlated."}, {"title": "5 Experimental Settings", "content": "This section provides an overview of the experimental settings. Details of all experiments can be found in the\nsupplementary materials."}, {"title": "5.1 Pretraining Datasets", "content": "Training SSL models with large datasets is crucial for developing generalized representations. However, most previous\nworks have used relatively small datasets, with the exception of [23], where an SSL model was trained with a large\nnumber of 12-lead ECGs. Following [23], we use the Chapman [24], Ningbo [25], and CODE-15 [26] datasets for"}, {"title": "5.2 Downstream Datasets", "content": "We use the PTB-XL [27] and CPSC2018 [28] datasets to evaluate the performance of ECG-JEPA on downstream\ntasks. PTB-XL contains 21,837 clinical 10-second 12-lead ECG records from 18,885 patients, recorded at 500Hz and\nannotated with 71 diagnostic labels, which are aggregated into five superclasses. We use these superclass labels for our\nexperiments. The CPSC2018 dataset includes 6,877 12-lead ECG recordings with nine annotated cardiac conditions.\nThese datasets are multi-label in nature, where each recording can have multiple labels simultaneously. The details of\nthe datasets are provided in the supplementary materials."}, {"title": "5.3 Pretraining", "content": "After resampling the ECG data to 250Hz, each 10-second multi-lead ECG has $T = 2500$ time points. We divide the\ninterval [0, T] into $N = 50$ non-overlapping subintervals of length $t = 50$. The model is trained for 100 epochs without\nany data augmentations, and the final checkpoint is used for downstream tasks."}, {"title": "5.4 Architecture", "content": "We use vision transformer (ViT-Base) architectures [29] for the student and teacher network, and a smaller standard\ntransformer architecture for the predictor network. The student and teacher network process the multi-lead ECG, while\nthe predictor processes each lead independently to reconstruct the masked representations. See supplementary materials\nfor detailed architecture specifications."}, {"title": "5.5 Downstream Tasks", "content": "We conduct extensive experiments to show that ECG-JEPA effectively captures semantic representations. We evaluate\nits performance on classification tasks through linear probing and fine-tuning. Additionally, we evaluate the performance\non the low-shot setting, and on the reduced-lead setting where the downstream dataset is restricted to have a single or\ntwo leads.\nNext, we validate the expressive power of the learned representations by predicting ECG features, including heart\nrate and QRS duration. To the best of our knowledge, this is the first work showing that the learned representations\ncan recover various ECG features. The successful prediction of these features provides compelling evidence that the\nrepresentations are not only informative but also capture essential and clinically relevant characteristics, making them\nhighly reliable for ECG analysis.\nIn ECG analysis, it is common for a single recording to have multiple labels simultaneously, making datasets like\nPTB-XL and CPSC2018 multi-label tasks. However, many previous studies have converted this into a multi-class\nclassification problem by restricting the dataset to subsets with only a single label. To ensure a fair comparison, we\nwill pretrain the methods using publicly available code and evaluate their performance on the multi-label classification\ntask. If the code is not available, we will convert our task into a multi-class problem and compare the results with the\nreported performance in the literature."}, {"title": "6 Experiments", "content": "In this section, we assess the performance of the learned representations on various downstream tasks. Our experiments\naim to demonstrate that the learned representations are generalizable and effective in capturing essential ECG features.\nWe compare ECG-JEPA with several state-of-the-art SSL methods.\nFor classification tasks, we use AUC (Area Under the ROC Curve) and F1 scores as our primary evaluation metrics.\nAUC evaluates model performance across all possible thresholds, providing a comprehensive and stable measure of\ndiscriminative ability. In contrast, the F1 score balances precision and recall at a fixed threshold. In our experiments,\nwe use the AUC as our main metric, as it provides a more robust evaluation of the model's performance across varying\nclassification thresholds, ensuring that the results are not overly dependent on a single decision boundary."}, {"title": "6.1 Linear Evaluation", "content": "Tables 1 and 2 present the results of our linear evaluation on the PTB-XL and CPSC2018 datasets. We train a linear\nclassifier on top of the frozen representations for 10 epochs, and evaluate its performance on downstream tasks. As\nshown in the tables, ECG-JEPA consistently outperforms other SSL methods, demonstrating superior efficiency and\neffectiveness with substantially reduced computational resources."}, {"title": "6.2 Fine-tuning", "content": "Fine-tuning is another method to evaluate the quality of learned representations, as it tests the model's ability to adapt\nits pre-trained features to new tasks. In the fine-tuning process, we add a linear classification head at the end of the\nencoder and train the whole network for 10 epochs. Fine-tuning can potentially enhance performance beyond what is\nachieved with linear evaluation alone.\nTable 3 presents the results of fine-tuning on the PTB-XL and CPSC2018 datasets. ECG-JEPA is compared with other\nSSL methods in a multi-class classification setting. The results indicate that ECG-JEPA achieves the highest AUC\nand F1 scores on PTB-XL and the highest AUC on CPSC2018. These results highlight ECG-JEPA's effectiveness\nin fine-tuning scenarios, demonstrating its ability to leverage transfer learning by refining and adapting its learned\nrepresentations for improved performance in specific classification tasks."}, {"title": "6.3 Low-shot Linear Evaluation", "content": "Table 4 presents the performance comparison on the low-shot task. Low-shot learning is particularly challenging, as\nmodels must generalize effectively with limited labeled data. Given the difficulty and resource-intensive nature of\nobtaining labeled data in medical research, low-shot learning represents a realistic and critical scenario in the medical\nfield. In this experiment, we evaluate the performance of ECG-SSL models on the PTB-XL multi-label task with only\n1% and 10% of the training set, while keeping the test set fixed. As shown in the table, ECG-JEPA demonstrates a clear\nadvantage over other SSL methods, with its effectiveness becoming particularly evident in low-shot learning tasks. This\nsuggests that ECG-JEPA can be particularly well-suited for transfer learning where labeled data is scarce."}, {"title": "6.4 Reduced Lead Evaluation", "content": "Because transformer architectures can accept variable input lengths, we can evaluate ECG-JEPA's performance on\nreduced lead tasks. In this experiment, we perform a linear evaluation on the PTB-XL multi-label task using reduced\nleads. Specifically, we utilize only a single lead (Lead II) and two leads (Lead II and V1), training linear classifiers on\ntop of the learned representations for 10 epochs. We then compare ECG-JEPA's performance with ST-MEM, another\nmodel supporting reduced lead evaluation. Table 5 presents the results of this reduced lead linear evaluation. Notably,\nECG-JEPA performs well even in the reduced lead setting. This robustness is particularly valuable since most daily\nmobile devices typically output only one or two leads, highlighting ECG-JEPA's potential for practical applications in\nmobile health monitoring."}, {"title": "6.5 ECG Feature Extraction", "content": "ECG features are essential for diagnosing and monitoring cardiac conditions. We evaluate the model's ability to extract\nkey ECG features, including heart rate and QRS duration, from the learned representations of the PTB-XL dataset.\nUnlike classification tasks, which are highly perceptual, ECG features are directly related to the morphology of the\nsignal. For example, heart rate can be measured by the average distance between R-peaks, and QRS duration can be\nmeasured by the average width of the QRS complex.\nSeveral methods have been proposed to segment ECG signals [30, 31, 32, 33]. Segmentation models can be used to\nextract ECG features. In this experiment, we employed a publicly available segmentation model [33] to extract ECG\nfeatures from the PTB-XL dataset, which served as our ground truth labels. We then trained a linear regression model\non the learned representations to predict these ECG features, using the mean squared error (MSE) as the loss function.\nTable 6 presents the performance comparison, showing the means and standard deviations of the absolute differences\nbetween the extracted and predicted values across all samples in the PTB-XL test set.\nInterestingly, although the model's learned representations are implied to capture high-level features based on their\nstrong performance in linear evaluation tasks across various datasets, they still retain the ability to recover low-level\ninformation, such as ECG features. This dual capability of capturing both high-level and low-level information is\nnoteworthy and highlights the model's versatility."}, {"title": "7 Ablation Study", "content": "7.1 Effect of CroPA\nTable 7 presents the results of our evaluation of the effectiveness of CroPA. CroPA introduces a \u201chuman-like\" inductive\nbias, enabling the model to be trained more efficiently on multi-lead ECG data. Without CroPA, models may require\nmore epochs to converge. For a fair comparison, we trained ECG-JEPA with and without CroPA for 100 and 200\nepochs and compared their performance on the PTB-XL multi-class task. The results show that CroPA considerably\nimproves the model's performance, demonstrating its effectiveness in capturing inter-lead relationships and enhancing\nthe model's ability to learn meaningful representations."}, {"title": "7.2 Masking Ratio", "content": "Table 8 presents the performance of ECG-JEPA in linear evaluation with different masking ratios and strategies. The\nresults indicate that the model benefits from a high masking ratio. Notably, multi-block masking is advantageous for\nlinear evaluation, while random masking is more effective for fine-tuning, as indicated in Table 3. Although random\nmasking with a ratio of (0.7, 0.8) achieves better performance in the PTB-XL multi-label task, a masking ratio of (0.6,\n0.7) performs better in other tasks. Therefore, we chose the latter for our main experiments."}, {"title": "8 Future Work", "content": "Our method has potential applications in various physiological multivariate signals, such as EEG and EMG. These\nsignals share characteristics with ECG, including their multivariate nature and high dimensionality, making them\nsuitable for our proposed approach.\nAnother promising direction involves the integration of multi-modal data. For example, multi-lead ECG data could be\ncombined with other diagnostic modalities, such as chest X-rays (CXR), to provide a more comprehensive understanding\nof a patient's condition.\nOne significant challenge in pursuing these extensions is the scarcity of large-scale datasets. Addressing this limitation\nis crucial for advancing and validating our method across diverse applications."}, {"title": "9 Conclusion", "content": "We proposed ECG-JEPA, a novel SSL method tailored for 12-lead ECG data. By utilizing a JEPA coupled with\nthe innovative relative positional encoding method, CroPA, ECG-JEPA effectively learns meaningful representations\nof ECG signals. This approach addresses the challenges posed by noise and artifacts in ECG data, demonstrating\nsubstantial improvements over existing SSL methods in various downstream tasks, with the added benefit of significantly\nfaster convergence.\nOur extensive experimental evaluations reveal that ECG-JEPA outperforms state-of-the-art SSL methods across\nseveral tasks, including linear evaluation, fine-tuning, low-shot learning, and ECG feature extraction. Moreover,\nour investigation into the use of 8 leads, as opposed to the full 12-lead ECG, indicates that this reduction does\nnot compromise performance while optimizing computational efficiency. This finding is particularly significant for\napplications constrained by limited computational resources."}, {"title": "A.4 Exponential Moving Average", "content": "The teacher network is initialized as a copy of the student network and is updated using an exponential moving average\n(EMA) of the student's weights. The EMA is computed as follows:\n$\\Theta_{teacher} = B_i\\Theta_{teacher} + (1 - B_i)\\Theta_{student}$\nwhere i denotes the current training iteration, and $B_i$ is a momentum parameter that evolves during training. The\nmomentum parameter $B_i$ is computed as:\n$B_i = ema_0 + \\frac{i \\cdot (ema_1 - ema_0)}{iterations\\_per\\_epoch \\cdot epochs}$\nHere, $ema_0$ and $ema_1$ represent the initial and final values of the momentum parameter, respectively. For our implemen-\ntation, $ema_0 = 0.996$ and $ema_1 = 1.0$."}, {"title": "A.5 Sufficiency of 8 Leads in ECG Analysis", "content": "In a standard 12-lead ECG, leads III, aVR, aVL, and aVF can be derived from leads I and II, as shown by the following\nequations:\n$III = II - I$\n$aVR = -\\frac{(I + II)}{2}$\n$aVL = I - \\frac{II}{2}$\n$aVF = II - \\frac{I}{2}$\nThese derivations demonstrate that leads III, aVR, aVL, and aVF do not provide additional independent information\nbeyond what is captured by leads I and II. Therefore, using only 8 leads (I, II, V1, V2, V3, V4, V5, and V6) can be\nsufficient for effective ECG analysis. This simplification not only reduces the computational load but also maintains the\ndiagnostic integrity of the analysis."}]}