{"title": "Scalable Safe Multi-Agent Reinforcement Learning for Multi-Agent System", "authors": ["Haikuo Du", "Fandi Gou", "Yunze Cai"], "abstract": "Safety and scalability are two critical challenges faced by practical Multi-Agent Systems (MAS). However, existing Multi-Agent Reinforcement Learning (MARL) algorithms that rely solely on reward shaping are ineffective in ensuring safety, and their scalability is rather limited due to the fixed-size network output. To address these issues, we propose a novel framework, Scalable Safe MARL (SS-MARL), to enhance the safety and scalability of MARL methods. Leveraging the inherent graph structure of MAS, we design a multi-layer message passing network to aggregate local observations and communications of varying sizes. Furthermore, we develop a constrained joint policy optimization method in the setting of local observation to improve safety. Simulation experiments demonstrate that SS-MARL achieves a better trade-off between optimality and safety compared to baselines, and its scalability significantly outperforms the latest methods in scenarios with a large number of agents. The feasibility of our method is also verified by hardware implementation with Mecanum-wheeled vehicles.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) techniques have experienced considerable advancement in recent years. However, the transition of these techniques from virtual environments to real-world physical applications still presents numerous challenges. Among these challenges, safety remains a critical concern, particularly in fields such as robotics [Brunke et al., 2022], autonomous driving [Muhammad et al., 2020], multi-agent systems (MAS) [Gu et al., 2023], and even the fine-tuning of large language models [Dai et al., 2024]. In the context of MAS, Multi-Agent Reinforcement Learning (MARL) demonstrates significant advantages in addressing complex cooperative tasks. During the execution of these tasks, agents must adhere to a variety of both local and global safety constraints. These safety constraints are designed to mitigate risks to the agents themselves as well as to other entities within the environment. Therefore, it is essential to study safe decision-making under these safe constraints within MARL frameworks.\nMost studies in the field of MAS based on MARL treat safety constraints as negative penalty in rewards [Chen et al., 2021; Yan et al., 2022; Huang et al., 2024]. However, because negative penalties can conflict with other positive rewards, neither the final policy nor the optimal policy inherently guarantees the satisfaction of safety constraints [Brunke et al., 2022]. This can result in numerous states that violate safety constraints, which are unacceptable in real-world applications. To address these challenges and inspired by safe RL algorithms on single agent [Rashid et al., 2020; Achiam et al., 2017; Ray et al., 2019], some researchers developed a series of safe MARL algorithms [Liu et al., 2021; Gu et al., 2023]. Among these algorithms, safety constraints are modeled as cost constraints, rather than negative penalty in rewards. These algorithms are designed to maintain safety throughout both the training and testing phases by taking cost constraints into account in policy optimization.\nDespite advancements in safe MARL algorithms that enhance safety, the challenge of exponential state space growth in MAS remains significant as the number of agents scales up. To tackle the exponential explosion problem, research on scalability in MARL aims to develop effective algorithms that can tranfer from small-scale training scenarios to large-scale testing scenarios. Some researchers [Nayak et al., 2023; Guo et al., 2023] have utilized the Centralized Training Decentralized Execution (CTDE) framework to achieve scalability in the setting of local observation. However, the issue with these methods lies in how better to utilize the local observations and communications of agents to achieve task objectives, including both optimality and safety objectives, especially when transferred to large-scale tasks.\nTo deal with the aforementioned issues, this paper proposes a novel framework called Scalable Safe Multi-Agent Reinforcement Learning (SS-MARL). SS-MARL has the following features: (1) It utilizes graph neural networks (GNNs) to achieve implicit communication between agents, while enhancing sampling efficiency during the training phase; (2) It employs constrained joint policy optimization, which can handle multiple constraints to ensure safety during both the training and testing phases; (3) It is capable of zero-shot"}, {"title": "2 Related Works", "content": "A research paradigm of safe RL is Constrained Markov Decision Processes (CMDPs), which take the cost constraints into account during MDPs state transitions. Following this paradigm, algorithms such as CPO [Achiam et al., 2017], TRPO-Lagrangian and PPO-Lagrangian [Ray et al., 2019] were proposed, all of which are inspired by TRPO [Schulman, 2015]. When it comes to multi-agent settings, each agent must not only adhere to its own cost constraints but also ensure that the joint behaviors of all agents possess a safety guarantee while maximizing the total reward. To deal with these issues, CMIX [Liu et al., 2021] was proposed based on QMIX [Rashid et al., 2020], which modifies the reward function to address peak cost constraints, but it doesn't provide a convergence proof. Inspired by HATRPO [Kuba et al., 2022], MACPO and MAPPO-Lagrangian [Gu et al., 2023] proved the theoretical guarantees of both monotonic improvement in reward and satisfaction of cost constraints, under the premise that the state value functions are known. Unfortunately, these algorithms do not consider the local observations of agents, nor do they provide solutions when agents have multiple constraints."}, {"title": "2.2 Partial Observability in MARL", "content": "In real-world MAS, agents can only observe partial information about the environment. The research paradigm that takes the partial observability into account is called Decentralized Partially Observable Markov Decision Processes (Dec-POMDPS) [Oliehoek et al., 2016]. In Dec-POMDPs, partial observability leads agents to treat other unobservable agents as part of the environment. This results in the non-stationarity of the environment [Papoudakis et al., 2019] and violates the Markov property.\nIn the context of MAS engaged in cooperative tasks, information sharing is important to tackle the challenge of partial observability [Zhu et al., 2024]. Agents share local observations with other agents, which mitigates the impact of partial observability. CommNet [Sukhbaatar et al., 2016] is a pioneering work that introduced communication into MARL by employing a shared communication neural network. However, it assumes that all agents can communicate with each other pairwise which is impractical. GAXNet [Yun et al., 2021] introduces an attention mechanism that allows for weighted selection of messages from other agents, but it is limited by the requirement to fix the maximum number of agents before training. Addressing this issue, TEM [Guo et al., 2023] proposes a Transformer-based Email mechanism for scalable inter-agent communication. Nevertheless, the TEM approach only uses inter-agent communication to complete simple task and ignores the constraints in MAS."}, {"title": "2.3 Scalability in MARL", "content": "The CTDE framework in MARL raises a compelling question: Can models trained on small-scale tasks be transferred to larger-scale tasks, given the agents' decentralized policy execution? Most MARL algorithms, such as MADDPG [Lowe et al., 2017], MAPPO [Yu et al., 2022], cannot achieve this goal because their actor networks' input sizes are entirely dependent on the state dimensions during training. With the recent success of Graph Neural Networks (GNNs), researchers have increasingly recognized the potential of integrating GNNs with the inherent graph structure in MAS to tackle this scalability issue in MARL.\nDGN [Jiang et al., 2020] utilizes multi-head attention mechanism and Graph Convolutional Networks (GCNs) [Kipf and Welling, 2017] to achieve scalability. However, it poses an oversimplified hypothesis that agents only communicate with the nearest three agents, encountering significant limitations in practical applications. EMP [Agarwal et al., 2020] also employs a GNNs approach based on the distance between entities, but it assumes that all entity states are known at the beginning of an episode, failing to effectively handle situations where obstacles are not observed initially but appear later. InforMARL [Nayak et al., 2023] integrates UniMP [Shi et al., 2021] networks into actor and critic networks, proposing a scalable MARL framework capable of selecting weights for local observations or communication through an attention mechanism. However, its drawback lies in the continued use of reward penalty to maintain safety, which results in the underutilization of aggregated information by the GNNs."}, {"title": "3 Problem Formulation", "content": "We formulate the safe MARL problem as a Constrained Markov Game (CMG) (N, S, A, P, \u03c1\u2080, R, \u03b3, C, c, \u03b3c), where N = {1,2,..., n} is the set of agents, S and A = \u03a0\u1d62\u208c\u2081\u1d3a A\u1d62 stand for the state space and the joint action space respectively, P : S \u00d7 A \u00d7 S \u2192 R is the state transition function, \u03c1\u2080 is the initial state distribution, R : S \u00d7 A \u2192 R is the joint reward function, C = {C\u1d62\u02b2 : S \u00d7 A\u1d62 \u2192 R | i \u2208 N, 1 \u2264 j \u2264 m\u1d62} is the set of cost functions, each agent i has m\u1d62 cost functions, c = {c\u1d62\u02b2 \u2208 R | i \u2208 N, 1 \u2264 j \u2264 m\u1d62} is the set of corresponding cost-constraining values, \u03b3, \u03b3c \u2208 [0,1) represent the discount factor of rewards and costs respectively. At time step t, agents are in state S\u209c, and agent i takes action A\u209c\u2071 according to its policy \u03c0\u1d62(A\u209c\u2071 | S\u209c), leading to a joint policy \u03c0(A\u209c | S\u209c) = \u03a0\u1d62\u208c\u2081\u1d3a \u03c0\u1d62(A\u209c\u2071 | S\u209c).\nIn this paper, we consider a fully cooperative setting where all agents share the same reward function. The objective in this case is to maximize the total reward while also trying to satisfy each agent's safety constraints.\n$\\max_\u03c0 J(\u03c0) = E_{S_0\u223c\u03c1_0, A_{0:\u221e}\u223c\u03c0} [\\sum_{t=0}^\u221e \u03b3^t R(S^t, A^t)]$\ns.t. $E_{S_0\u223c\u03c1_0, A_{0:\u221e}\u223c\u03c0} [\\sum_{t=0}^\u221e \u03b3_c^t C_i^j(S^t, A_i^t)] \u2264 c_i^j, \u2200 i \u2208 N, 1 \u2264 j \u2264 m_i$.\n(1)\nThe joint policies that satisfy the constraints in Equation 1 are referred to as feasible. The following definitions are based on the reward function, the cost version definitions are similar\u00b2. We can define the multi-agent advantage (A) function based on the multi-agent state-action value (Q) function. Here, i\u2081:h and j\u2081:k are two disjoint subsets of the set N, and -i\u2081:h denotes the complement of i\u2081:h with respect to N.\n$A_\u03c0^{i_1:h}(S, A_{j_{1:k}}, A_{i_{1:h}}) = Q_{j_{1:k} \u222a i_{1:h}}^\u03c0 (S, A_{j_{1:k}} \u222a A_{i_{1:h}}) - Q_{j_{1:k}}^\u03c0(S, A_{j_{1:k}}).$\n(2)\nSubsequently, we can define the joint surrogate return, where \u03c0\u1d62\u2081:\u2095\u208b\u2081, \u03c0\u1d62\u2099, \u03c0 represent three different joint policies.\n$L_{i_h}^{i_{1:h-1}} (\u03c0^{i_{1:h-1}}, \u03c0^{i_h}) = E_{S\u223c\u03c1_0, A_{i_{1:h-1}}\u223c\u03c0^{i_{1:h-1}}, A_{i_h}\u223c\u03c0^{i_h}} [A_\u03c0^{i_h} (S, A_{i_{1:h-1}}, A_{i_h})].$\n(3)\nWe assume that the joint policy after k updates is \u03c0\u1d4f = {\u03c0\u2081, \u03c0\u2082, ..., \u03c0\u2099} , and \u03c0\u1d62\u2081:\u2095\u208b\u2081\u1d4f represents the subset of \u03c0\u1d4f for which the subscripts are in i\u2081:h. The joint policy can be improved if each agent in i\u2081:h sequentially solves the following optimization problem:\n$\\pi_i^{k+1} = arg \\max_{\\pi} L_{i_h}^{i_{1:h-1}^k}(\\pi_{1:h-1}^k, \\pi) - \\nu D_{KL}^{\\max}(\\pi_h^k, \\pi),$\ns.t. $D_{KL}^{\\max} (\u03c0_h^k, \u03c0) \u2264 \u03b4_{i_h},$\n$\\tilde{c}_{i,i_h}^j (\\pi^{i_h}) \\leq c_i^j, j = 1, ..., m_i$.\n(4)"}, {"title": "4 Approach", "content": "The framework of SS-MARL is shown in Figure 1. We extract the inherent graph G : (V, E) in MAS at time t. The vertices of the graph are all entities in the environment, including obstacles, agents, and corresponding goals. We use encoding to represent the type of vertices (0: obstacle, 1: agent, 2: goal). Every agent has a communication range and a perception range. Agents can perceive other agents and obstacles within their perception range, forming unidirectional edges from the perceived entities to themselves. Within their communication range, agents can communicate bidirectionally with other agents. Additionally, each agent can form a unidirectional connection edge with its goal according to its task. The\nIn the original Dec-POMDPs setting, there is no communication between agents, and agents can only make decisions based on their local observations. However, in SS-MARL, due to the communication among agents, every agent can obtain more global information beyond their local observations by communicating with neighboring agents. In SS-MARL, we focus on solving the problem of how agents can efficiently obtain global information beyond local observations through communication with other agents.\nBesides, graph G not only contains the global information of the environment but also includes the local observations of all agents and the communication topology. It is fed into the actor, critic, and cost critic, each of which outputs actions, reward state values, and cost state values, respectively. During the execution phase, actions are directly applied to the environment. In the training phase, critic and cost critic are used to guide constrained joint policy optimization. Meanwhile, the critic and the cost critic are updated via gradient descent using the mean squared error (MSE) loss of the temporal difference error, for rewards and costs respectively."}, {"title": "4.1 Actor, Critic and Cost Critic", "content": "Safe MARL algorithms extend the Actor-Critic architecture of MARL by incorporating an additional cost critic component. Cost critic is designed to estimate the cost state value functions. It plays a pivotal role in constrained joint policy optimization by serving as cost constraints, ensuring that the joint policy is optimized in a manner that maintains safety.\nThe actor needs to extract information from graph G that includes both local observation and communication details of agents, while the critic and cost critic require global information from G. Here, we employ a graph attention mechanism-based message passing model to achieve this. In the GNN backbone, vertex features V = {v\u1d62 | 1 \u2264 i \u2264 |V|} first pass through an embedding layer fo\u2081 to generate embedding vectors. Then, for each connected edge {i,j}, the edge features e\u1d62\u2c7c \u2208 E and source vertex features are concatenated with target vertex features and processed through a MLP fo\u2082 to generate a message, i.e., m = fo\u2082(concat(fo\u2081(v\u1d62), e\u1d62\u2c7c, fo\u2081(v\u2c7c))). Subsequently, an attention layer aggregates these messages: v\u1d62\u1d4f\u207a\u00b9 = \u2211\u2c7c\u2208N(\u1d62) softmax(fo\u2083 (m\u1d62\u2c7c)) \u00d7 fo\u2084(m\u1d62\u2c7c). Here, softmax(fo\u2083 (m\u1d62\u2c7c)) is referred to as the attention weight, a number between 0 and 1, representing the importance of the communication or observation between entity i and entity j. The attention mechanism allows agents to select messages from connected edges based on their importance. We use multiple such message passing layers to enable agents to obtain information from more distant agents, mitigating the impact of partial observability. The specific message passing process is shown in Figure 2. It is worth noting that the embedding layer fo\u2081 is used to standardize vertex features dimension solely during the initial message passing and is not required for subsequent message passing.\nThe actor, critic and cost critic share the same GNN backbone, but they differ in the aggregation methods. After k message passing, graph G becomes G\u1d4f: (V\u1d4f, E), V\u1d4f = {v\u1d62\u1d4f | 1 \u2264 i \u2264 |V|}. Vagent\u1d4f \u2282 V\u1d4f represents the subset of all agent vertices. The actor performs Agent Aggregation (AA), which aggregates observation and communication related to a single agent. For agent i, agent aggregation selects the vector of the vertex corresponding to agent i from Vagent\u1d4f as output. The critic and the cost critic perform Graph Aggregation (GA), which aggregates the vertices of all agents by passing Vagent\u1d4f through an attention layer, i.e., X = \u03a3\u1d65\u2208Vagent\u1d4f softmax(fe\u2085(v)) \u00d7 fe\u2086(v). It is worth noting that, although GA aggregates more environment information than AA, both have output vectors of the same length. This length is independent of the number of agents, which also supports the scalability of SS-MARL.\nAfter the aggregation process, the actor, critic and cost critic all require an LSTM [Hochreiter, 1997] to address the challenges associated with violations of the Markov property in the environment. Subsequently, a diagonal Gaussian layer is utilized for the actor's stochastic output. For the critic and the cost critic, a MLP layer is engaged to compute and output the reward state value and the cost state value, respectively."}, {"title": "4.2 Constrained Joint Policy Optimization", "content": "As shown in Equation 4, it is feasible to update the joint policy with cost constraints. In practical implementation, we parameterize the joint policy \u03c0* = \u03a0\u1d62\u208c\u2081\u1d3a \u03c0\u1d62\u1d4f after k updates and use average KL distance constraints derived from random sampling to approximate the maximum KL distance constraint. Thus, the optimization problem can be simplified to Equation 5. The multi-agent advantage functions for rewards and costs can be derived from the critic and the cost critic, respectively, using Generalized Advantage Estimation (GAE) [Schulman et al., 2015].\n$\\theta_{i_h}^{k+1} = arg \\max_{\\pi} E_{S\u223c\u03c1_0, A_{i'-h}\u223c\u03c0^{k} A_{i_h}\u223c\u03c0} [A_{\\pi^k}(S, A_{i'-h}, A_{i_h})] \\leq \\eta,$\ns.t. $\\tilde{J}_{i_h}^{c,i_h} (\\pi^k) + E_{S\u223c\u03c1_0, A_{i_h}\u223c\\pi} [A_{\\pi^k} (S, A_{i_h})] \\leq \\eta,$\n$\\forall j = 1,..., m_{i_h}, and D_{KL} (\\pi_h^k, \\pi) \\leq \\delta$.\n(5)\nSimilar to [Kuba et al., 2022] and [Gu et al., 2023], Equation 5 can be approximated using a first-order Taylor expansion for the objective function and cost constraints, and a second-order approximation for the KL divergence constraint, as shown below:\n$\\Delta^{k+1} = arg \\max_{\\theta} (G_{i_h}^k)(\\theta - \\theta_i^k)$\ns.t. $D_{i'_h}^j(B^j_i)(\\theta - \\theta_i^k) \\leq 0, \\forall j = 1, ..., m_{i_h},$\nand $\\frac{1}{2} (\\theta - \\theta_i^k) E_{i_h} (\\theta - \\theta_i^k) \\leq \\delta$.\n(6)\nHere, G\u1d62\u2099 is the gradient of the objective of agent i\u2099, B\u2c7c is the gradient of jth cost constraint of agent i\u2099, E\u1d62\u2099 is the Hessian matrix of the average KL distance of agent i\u2099. Similar to [Schulman, 2015; Kuba et al., 2022; Gu et al., 2023], we can take a primal-dual approach to solve this linear quadratic optimization problem.\nHowever, the approximation may result in an infeasible policy \u2081\u1d62\u2099. References [Achiam et al., 2017] and [Gu et al., 2023] address this issue only when m\u1d62 = 1, \u2200i \u2208 N. We now propose a solution for the case when m\u1d62 \u2265 1. This involves a multi-objective optimization problem to reduce multiple cost values. To avoid calculating the complex Pareto front, we construct a weighted objective function, as shown below:\n$\\min \\sum_{j=1}^{m_i} (\\beta_j^i B_{i_h}^j)^T (\\theta - \\theta_{i_h}^k)$,\ns.t. $\\frac{1}{2} (\\theta - \\theta_{i_h}^k) E_{i_h} (\\theta - \\theta_{i_h}^k) \\leq \\delta$.\n(7)\nThe weight \u03b2\u1d62 describes the importance of cost constraints. Constraints that significantly exceed their target values require higher weights, while those that meet the target values do not need reduction and should have a weight of zero.\n$\\beta_j^i = \\begin{cases} \\frac{exp(P_j)}{\\sum_{P_q>0} exp(P_q)}, P_j > 0 \\\\ 0, P_j \\leq 0 \\end{cases}, P_j = J_i^c (\u03c0) - c_i^j$.\n(8)\nTo solve Equation 7, we can use a TRPO recovery step [Schulman, 2015] on weighted matrix $B_i^h = \\sum_{j=1}^{28}$ to recover the feasible policy, as shown below:\n$\\theta^{k+1} = \\theta_{i_h}^k - a_j (B_{i_h}^j) (E_{i_h})^{-1} (B_{i_h}^j)^T$.\n(9)\na\u2c7c is adjusted through backtracking line search. Thus, even if an infeasible policy violates multiple cosntraints, it can still be recovered to a feasible policy by a recovery step."}, {"title": "5 Experiments", "content": "We conduct simulation experiments in the MPE, and we have modified it to facilitate safe MARL algorithms. We selected the cooperative navigation task to validate the performance of SS-MARL, where each agent is required to reach its own goal while avoiding collisions with other entities in the environment. The number of collisions among agents is modeled as a cost, with fewer collisions indicating higher safety level. Additionally, we assume that the corresponding c\u1d62 in Equation 1 for each agent is the same value c, i.e., c\u1d62 = c,\u2200i \u2208 N. Due to page limitations, experiments on other cooperative tasks and the hardware implementation are shown in the appendix.\nThe subsequent experiments and analyses address three fundamental questions:"}, {"title": "5.1 SS-MARL Safety Analysis", "content": "As specified in Equation 1, the cost surrogate return is constrained by an upper bound parameter, denoted as c. This parameter plays an important role in balancing the safety and optimality of both the training policy and the final policy trained by SS-MARL. Accordingly, we designed our experiment as follows: within a fixed scenario featuring two agents and two obstacles. We then conduct training under two representative value of c and analyze the results."}, {"title": "5.2 Comparative Experiments", "content": "The comparative experiment will be conducted in square-shaped scenarios with n agents and n obstacles, with a world size of 4\u221an/3. Additionally, initial positions and goals of the agents and the positions of the obstacles in these scenarios are all randomly generated in a way that avoids conflicts. We selected n = 3,6,9 to represent different levels of environmental complexity for conducting the experiments.\nWe select these algorithms for comparative experiments: RMAPPO[Yu et al., 2022], RMACPO[Gu et al., 2023], and InforMARL[Nayak et al., 2023], where 'R' stands for the RNN-based versions of the algorithms. RMACPO and RMAPPO are algorithms with fixed-size input, both of which have access to global information. In contrast, SS-MARL and InforMARL both have an identical local observation and communication radius of 1.\nIn terms of the reward function, both SS-MARL and RMACPO have their reward functions set as r = rdist + rgoal. Here rdist represents the reward that guides the agent towards the goal, typically the negative of the distance to the goal. rgoal is a positive reward obtained when the agent reaches the goal. RMAPPO and InforMARL incorporate reward shaping to encourage obstacle avoidance by introducing an additional term, rcollision, which represents a penalty for collisions. Consequently, their reward function is defined as r = rdist + rgoal + rcollision. Regarding safety parameters, SS-MARL and RMACPO set the upper bounds for their cost constraints, denoted as c, to 1. This means that the expected cost for an agent within one episode should not exceed 1. RMAPPO and InforMARL ensure safety by setting rcollision = -rgoal/2. It is worth noting that the setting of rcollision here is a result of balancing safety and optimality. Based on our experiments, further increasing the penalty does not enhance safety but instead leads to the rewards failing to converge. Additionally, we establish control groups for SS-MARL and RMACPO with policy parameter sharing, labeled as 'PS'. This indicates that each agent shares the same actor network parameter. Conversely, 'NPS' represents that each agent possesses its own actor network.\nFrom Figure 3, it is evident that in all three scenarios, the reward curves of SS-MARL(PS) converge much faster than those of SS-MARL(NPS), demonstrating that policy sharing greatly enhances sampling efficiency in cooperative tasks involving homogeneous agents. InforMARL also employs policy sharing, and its reward curve converges slightly faster than that of SS-MARL(PS). This is because SS-MARL incorporates some TRPO recovery steps when dealing with infeasible policies. These recovery steps only reduce the expected cost value, not aiming to increase the reward value. Consequently, SS-MARL requires more steps to achieve the same level of reward value. However, these recovery steps are crucial for ensuring safety. This importance is highlighted by the significantly lower cost values observed in SS-MARL compared to InforMARL during training. Particularly in the more complex scenario with n = 9, the difference is even more pronounced. In terms of cost value metrics, SS-MARL performs slightly better than RMACPO. However, there is a significant difference in the convergence of rewards. SS-MARL, which utilizes GNN for agent communication, demonstrates a marked advantage in this regard. In contrast, RMAPPO performs even worse because the reward shaping approach fails to provide safety guarantees. Furthermore, in complex scenarios, RMAPPO cannot even achieve convergence in rewards.\nRegarding the final policy, Table 2 shows that SS-MARL(PS) has a significant advantage in terms of reward"}, {"title": "5.3 Scalability Experiments", "content": "To test the scalability of SS-MARL, we selected the models that were trained with n = 3 and zero-shot transfer to scenarios with larger n, and compared the results with those of two state-of-the-art methods: InforMARL [Nayak et al., 2023] and TEM [Guo et al., 2023]. As the world size grows with larger n, the time required for an agent to reach the goal also increases. Since we will test on scenarios with a maximum n = 96, to prevent the episode from being too short and causing agents to fail in completing the task, we extended the episode length to 200 steps.\nDuring the testing phase, the graph that represents the agent's communication and observation undergoes dynamic changes. Concurrently, the attention weights associated with the communication and observation edges dynamically adjust. This adjustment helps to balance the motion towards the goal with the avoidance of local unsafe states. Figure 5 illustrates the testing scenario with n = 24, using a model trained on scenarios with only n = 3. The gray curves with gradient color represent the trajectories of the agents. These trajectories indicate that the agents not only complete global navigation tasks but also possess great local obstacle avoidance capabilities."}, {"title": "6 Conclusion", "content": "This paper introduces SS-MARL, a novel approach to MARL that emphasizes safety and scalability. SS-MARL is designed to handle complex cooperative tasks in MAS where agents must comply with various local and global safety constraints. We introduced constrained joint policy optimization to achieve the safety of both the training policy and the final policy. Additionally, GNNs endows SS-MARL with strong scalability. Experimental results demonstrate that SS-MARL achieves the best balance between optimality and safety compared to the baselines. Compared to the latest methods, it also shows great scalability in scenarios with a large number of agents. By leveraging SS-MARL, the MAS trained through MARL can become safer and more scalable, enhancing the potential for MARL applications in real-world tasks."}]}