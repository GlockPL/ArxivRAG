{"title": "PALMS: Plane-based Accessible Indoor Localization Using Mobile Smartphones", "authors": ["Yunqian Cheng", "Roberto Manduchi"], "abstract": "In this paper, we present PALMS, an innovative indoor global localization and relocalization system for mobile smartphones that utilizes publicly available floor plans. Unlike most vision-based methods that require constant visual input, our system adopts a dynamic form of localization that considers a single instantaneous observation and odometry data. The core contribution of this work is the introduction of a particle filter initialization method that leverages the Certainly Empty Space (CES) constraint along with principal orientation matching. This approach creates a spatial probability distribution of the device's location, significantly improving localization accuracy and reducing particle filter convergence time. Our experimental evaluations demonstrate that PALMS outperforms traditional methods with uniformly initialized particle filters, providing a more efficient and accessible approach to indoor wayfinding. By eliminating the need for prior environmental fingerprint-ing, PALMS provides a scalable and practical approach to indoor navigation.", "sections": [{"title": "I. INTRODUCTION", "content": "Indoor wayfinding, the ability to determine and follow a path in a building towards a desired destination, is an open area of research, with applications including travel (e.g., finding a gate in an airport), health (e.g., finding a doctor's office), and recreation (e.g., exploring a museum). Wayfinding is particularly challenging for those who are blind or have low vision, as these individuals cannot rely on visual landmarks for orientation, and cannot easily consult a map of the place.\nDifferent types of smartphone apps for assisted wayfinding are available. Some (like Google Maps and Apple Maps) are general-purpose localization and guidance apps that can work indoors (even when GPS signal is unusable) using radio signal from Wi-Fi access points or Bluetooth Low Energy (BLE) beacons. This requires a prior fingerprinting phase which, generally speaking, involves collecting signal measurements (received signal strength or RSSI) over a dense grid of spatial locations in the building. Indoor Atlas relies on \"magnetic signatures\" for mapping and also requires prior fingerprinting of the environment. GoodMaps is a wayfinding app specifically designed for blind users. It enables indoor mapping using images taken from a smartphone camera, which are matched, using computer vision techniques, against a database of visual features collected in a prior phase (what could be termed visual fingerprinting).\nA disadvantage of these technologies is that localization is only available in environments that have been fingerprinted, which represents a small portion of the available buildings. A more scalable approach, one that requires no fingerprinting, is offered by systems that attempt to localize and track the walker using dead-reckoning. For example, Crabb et al. [1] experimented with wayfinding using ARKit, an iOS framework for vision-inertial odometry (VIO). A user of this system walks while holding the smartphone in such a way that the camera has a clear view of the environment. A structure-from-motion algorithm, complemented with inertial data, measures the phone's velocity and orientation with respect to a fixed reference frame. By integrating the velocity vectors through time, the walker's trajectory can be reconstructed. The unavoidable drift (due to instantaneous bias and noise) can be compensated for via particle filtering, which uses the floor plan of the building to condition the reconstructed trajectory (in particular, by discouraging trajectories going through a wall). A similar approach was taken by Ren et al. [2], [3] but relying on inertial sensors only, using step-based pedestrian dead reckoning (PDR) or machine learning-based odometry (RONIN).\nA main problem with dead-reckoning odometry is that the starting location and orientation needs to be known, requiring some form of initial localization. In addition, occasional relocalization may be necessary because of sporadic system fault or poor tracking due to accumulated drift. In some cases, recognizable visual landmarks, (e.g., an EXIT sign) whose location in the building is known, can be used for initial localization or relocalization (in the following, we will use the term localization for both cases). However, map annotations of this type are not often available.\nIn this work, we will only consider prior information about the environment in the form of a floor plan of the building. Floor plans are often available, especially for public places, where they are frequently posted online. They normally represent structural elements (e.g. walls) that, unlike furniture or other objects, are expected to remain stable in time. Floor plan-based localization is highly desirable as it removes the need for environment fingerprinting or detailed map annotations. This task is similar to that of a visitor of a building trying to locate themselves within a posted map when the \"You are here\" marking is missing. One may, for example, search the floor plan for a location where the presence of nearby features (e.g., doors, wall corners, hallway size) and their spatial relation, as indicated in the map, conform with the visible scene at their location. Our proposed PALMS (Plane-based Accessible Indoor Localization Using Mobile Smartphones) system uses 3-D data, acquired by an iPhone LiDAR, to detect specific features of the visible wall geometry which are then matched against the wall structure shown in the floor plan.\nRegardless of the methodology considered, it is reasonable to expect that, in a building with self-repeating structural elements (e.g., a long corridor with uniformly spaced doors), for each location there may be multiple other locations from which the visual scene looks similar. To mitigate this ambiguity, we consider a dynamic form of localization: users take an image (or, in our case, a 3-D scan) of the environment from their location; then, they start walking in any available direction, following a certain path (e.g. random). Over a (hopefully short) period of time, the system determines their location and orientation, by combining information from the initial data collection along with dead-reckoning odometry information from the sensor data, conditioned by a particle filter. Intuitively, while multiple locations in the floor plan may be consistent with the visual data recorded at a given location, the same path (from dead-reckoning) is likely to be unfeasible (because of physical constraints) if started from an incorrect location. Inspired by [4], [5], we implement this intuition by defining a prior spatial probability distribution for the user's location, which in our case is derived from a measure of how well the observed walls match with local characteristics of the floor plan. Then, the customary particle evolution process is started, driven by the sensor data recorded as the user is walking. Particles that hit a wall are resampled from the remaining particles, which typically leads to a rapid coalescing of the particle into one cluster. The distribution of particles is tracked until convergence is declared, at which time the user's location is assumed to be reliably determined.\nHere are the main contributions of this work:\n\u2022 We present a novel method (PALMS) to assess the probability that a 3-D scan, captured at a location using an iPhone equipped with LiDAR and processed to extract planar walls, was taken at a certain location in a floor plan for one of N possible orientations of the LiDAR's reference frame (where in our case, N = 4). This probability is computed on the basis of the wall patches extracted from the LiDAR scan, which are then matched against the walls marked in the floor plan. A probability distribution of locations across the floor plan (heatmap) is quickly computed through by means of a linear convolution for each possible orientation of the LiDAR's reference frame.\n\u2022 We propose a method for dynamic localization that uses a particle filter initialized from the heatmap computed at each of the N considered orientations\n\u2022 We introduce a 2-step criterion that can be used to declare \"convergence\", the point in time after which the current user's location can be assumed to be correctly identified. Our experimental data shows that the proposed PALMS-based initialization leads to reduced convergence time and increased localization accuracy after convergence compared to a uniform initial probability distribution. In particular, the average RMSE for PALMS was found to be 6.7 times less than for Uniform, while the proportions of locations with error less than 1 meter was found to be 4.9 times higher for PALMS than for Uniform."}, {"title": "II. RELATED WORK", "content": "Smartphone-based pedestrian tracking and localization systems in indoor, GPS-denied environments have received considerable attention recently. Many researchers have proposed schemes to estimate the user's location based on the Received Signal Strength Indicator (RSSI) values of wireless technologies, including WiFi [4], Bluetooth Low Energy (BLE) beacons, Radio Frequency Identification (RFID), Ultra-Wideband (UWB), and magnetic field fingerprinting [3]. W-RGB-D [4] uses a heatmap of WiFi signal strength to initialize a particle filter for quick convergence in ambiguous environments. We follow a similar particle filter initialization approach as described in section III-B. However, RSSI methods' requirement for prior calibration or \"fingerprinting\" before use restricts the scalability of these methods. Similarly, image-retrieval methods [6] require a pre-built image database, while 3-D structure-based methods [7] require pre-built SfM models.\nIn contrast to fingerprinting-based approaches, some prior works perform indoor localization without prior visitation, leveraging only a known floor plan. Among these works, LiDAR-based methods are particularly notable. Such methods utilize 3-D-to-2-D point cloud matching [8], geometrical feature matching [9], usually coupled with particle filters [10] or graph optimizations [11]. On the other hand, image-based methods [1], [7], [12]\u2013[20] aim to perform localization from single or multiple images by utilizing image features [13], semantic information (doors [12], [14], [15], windows [15], texts [16], and exit signs [1]), or scene geometries (vertical lines and vertical planes [13]). [17] samples 3-D points from a raised 2-D floor plan and performs point cloud matching with the RGB-reconstructed 3-D scene.\nWhile these methods generally rely on a flow of information over multiple sequential sensor measurements, some approaches try to tackle indoor localization by considering only an instantaneous observation [14], [18]. More recently, LaLaLoc [19] and LaLaLoc++ [20] used deep neural networks to find \"embeddings\u201d for panorama images to match with the \"embeddings\" of 2-D locations on the floor plans. As pointed out by most of these papers, the ambiguity introduced by layout-similarity cannot be completely removed when localizing with respect to layout alone, making it hard to localize using a single observation."}, {"title": "III. METHOD", "content": "We detect vertical planar patches (walls) in a 360\u00b0 LiDAR scan and obtain their 3-D poses using a custom ARKit-based iOS app. These patches are then projected onto the ground plane, forming a set of 2-D line segments (in metric units) $L_{obs} = \\{l_{obs,1},l_{obs,2}, ..., l_{obs,n}\\}$. We also represent the walls in the given floor plan as a set of 2-D line segments $L_{fp} = \\{l_{fp,1},l_{fp,2}, ..., l_{fp,m}\\}$, also expressed in metric units.\nIntuitively, to localize the observation point (the center of projection of the LiDAR scan), we could try place $L_{obs}$ over $L_{fp}$ while adjusting $L_{obs}$'s orientation and location, as if placing down a piece of a jigsaw puzzle looking for a match. We limit the set of orientations by observing that, in most buildings, only a discrete set of wall orientations is observed. In our experiments, we considered buildings with walls intersecting at 90\u00b0, resulting in 2 possible orientations for any wall. Note that our algorithm can be easily extended to more complex buildings (e.g., walls intersecting at a multiple of 45\u00b0).\nOur first task is to determine an angle $\\theta$ such that, by rotating all observed line segments $L_{obs}$ by $\\theta$, any one such segment is aligned with one or more line segments in the floor plan. Note that if $\\theta$ satisfies this property, all angles $\\theta+k\\cdot90\u00b0$ for $0 \\leq k < 3$ will also satisfy it, and thus need to be considered. We determine $\\theta$ by first finding the two orthogonal principal orientations of the observed line segments as well as of the floor plan segments. $\\theta$ is taken to be the smallest angle that pairwise aligns the principal orientations found in the two domains."}, {"title": "B. Heatmaps Creation", "content": "Consider the observed line segments $L_{obs}$, rotated by $\\theta + k\\cdot90\u00b0$. Our goal is to define, for any point in the floor plan, the likelihood that this point represents the observation point. Ideally, at the correct location and orientation, the observed and floor plan line segments should overlap. By rasterizing these segments into bitmaps and multiplying them pixel by pixel, we obtain a score that, after normalization, defines a probability distribution for the location. Testing multiple locations is akin to computing a convolution on the rasterized floor plan, with the convolution kernel (termed $R^{W}$ for recorded walls) representing the walls observed in the LiDAR scan (see Fig. 1)\nWhile intuitively appealing, this simple approach suffers from two main drawbacks. First, both the floor plan and the pose of the walls extracted from the LiDAR scan may be affected by errors. As a result, the observed line segments may not perfectly overlap with the floor plan segments even when $L_{obs}$ is centered at the correct observation point. To account for this, we smooth the observed line segments bitmap with a Gaussian kernel, which basically \u201cwidens\" the line segments to account for possible localization errors. Second, this criterion does not leverage visibility constraints. Suppose that an observed line segment is matched to a floor plan segment and that another floor plan segment is located between the location being inspected and this matched segment (see Fig. 2). This unmatched floor plan segment would not be penalized by our simple \"goodness of fit\u201d measure, yet its presence is physically impossible, as it would occlude view of the matched wall segment.\nIn order to enforce visibility constraints, we introduce the notion of certainly empty space (CES) (see Fig. 2), which represents the areas where, based on the observation from a certain location, no wall in the floor plan should be found. Specifically, if a wall (line segment) was observed from a certain location, no other floor plan line segment should be found in the triangle subtended by the observed line segment at that location. The union of such triangles for all visible line segment forms the CES area. By centering the CES space on different locations in the floor plan, we can find which locations break the visibility constraint (because they contain walls within the CES area). This is implemented by defining a second kernel ($CES$), with values of 1 at the CES area and 0 otherwise, and convolving this with the rasterized floor plan. To account for possible wall localization errors, we first slightly \"shrink\u201d (rescale) the CES kernel, then combine the two kernels into one, equal to $R^{W} \u2013 \\alpha CES$, where we set $\\alpha = 1$ in our experiments.\nIn practice, we first rotate the observed line segments $L_{obs}$ by angle $\\theta$ (Sec. III-A), and compute the kernel $R^{W}-\\alpha\\cdot CES$. Convolution of the rasterized floor plan with this kernel forms a heatmap $H_0$. We then rotate the kernel by multiples of 90\u00b0 to compute additional heatmaps $H_1, H_2, H_3$. The heatmap $H_k$ represents the probability distribution of the observation point across the floor plan, given that the reference frame for the LiDAR scan was at an angle of $\\theta + k \\cdot 90\u00b0$ with the floor plan's reference frame."}, {"title": "C. Particle Filter Initialization from Heatmaps", "content": "In our experiments (Sec. IV), the \"ground truth\" observation point $P_{gt}$ consistently appeared among the locations with top 1% of all heatmap values. We thus simplify all heatmaps by binarizing them, using a threshold equal to the 99 percentile of all values in all maps (see Fig. 1). Each location marked as 1 in a binarized heatmap $H_k$ represents a candidate observation point for the k-th orientation. Note that the proportion of 1's may vary across heatmaps for different orientations.\nWe use these binary heatmaps to initialize a particle filter tracking the motion of the user as they walk away from the observation point as driven by sensor data. Following [2], each particle tracks a state formed by location and angular drift. The filter is initialized as follows. For each binary heatmap, we generate a number of particles sampled at uniform spatial density within the areas where the heatmap is equal to 1. We then place all generated particles on the floor plan, where particles generated from the $H_k$ heatmap are assigned a label (k) and are given an initial orientation drift of $\\theta+k\\cdot90\u00b0$. During particle filter updating, particles that hit a wall are resampled from the remaining particles (any group), which typically leads to a rapid coalescing of the particles into one cluster. Note that when a particle is resampled from a particle with label k, the resampled particle maintains the same label, location (with noise added), and angular drift (with noise added)."}, {"title": "D. Convergence Criterion", "content": "It can thus be expected that the particle filter will converge to the correct user's location after a while (global localization problem [21]). For example, [1] shows an empirical characterization of the time it takes for the location error to be less than 1 meter. However, in practical scenarios, the system does not have access to this error metric. Therefore, a measurable criterion is necessary to reliably declare \"convergence\", allowing the user to assume that their location is being tracked accurately from that point onward. We propose a 2-step convergence criterion based on the particles' distribution.\nIn Step 1, we check if one label dominates the current particle set (at least 80%), since particles with the correct initial orientation are less likely to hit a wall and more likely to survive. We mark this point as $t_1$.\nIn Step 2, we cluster particles with the dominant orientation using the mean shift algorithm and check if the dominant cluster contains at least 50% of these particles. At this point $t_2$, we consider the particle filter fully converged, and the algorithm outputs the mean position of the dominant cluster as the predicted user location $P_{pred}$ (See Fig. 3). Like [2], we run the mean-shift algorithm to update the dominant cluster every 20 time steps after $t_2$ to keep track of the user's location."}, {"title": "IV. EXPERIMENTS", "content": "We collected data from 14 paths (2 per 7 starting points) in 3 buildings, averaging 147m in length. Each dataset includes LiDAR-detected vertical planes (using an iPhone 14 Pro) at the starting location and sensor data recorded along the paths. We scanned the surroundings by rotating the phone 360\u00b0 and walked away from the observation point, holding the phone with the camera facing forward to collect odometry data using iOS' ARKit. We used this odometry data and a particle filter initialized at the correct location and orientation to generate \"ground truth\" location data. The dataset can be accessed via the following link: https://github.com/Head-inthe-Cloud/PALMS-Indoor-Localization/tree/main"}, {"title": "B. Global Localization with PALMS", "content": "In this experiment, we evaluate the advantages of PALMS initialization using 3-D scan data. We compare it against two methods without this data. In the first method (Uniform), particles are uniformly sampled across the floor plan with random initial angular drift between 0\u00b0 and 360\u00b0. The second method (Uniform + Ori) uniformly samples particle locations and assigns initial angular drifts $\\theta + k\\cdot 90\u00b0$ for each of the k groups ($0 \\leq k < 3$). $\\theta$ is calculated using the principal orientations of $L_{fp}$ and the first few tracking points. For Uniform, convergence Step 1 is determined by setting a threshold on particle dispersion, while Step 2 monitors for dominant clusters like PALMS. All three methods use 2000 particles and run 100 trials per path, with random re-initialization for each trial.\nMeasurements included: time (in seconds) and distance (in meters) until convergence is declared; RMSE after convergence (in meters); and proportion of predicted locations (after convergence) at less than 1 meter from the ground truth (%Error<1m). Results in Tab. I (averaged over all trials) show that PALMS significantly outperforms the other methods considered. In particular, the average RMSE for PALMS was found to be 6.7 times less than for Uniform (5.6 times less than for Uniform + Ori), while the proportions of locations with error less than 1 meter post-convergence was found to be 4.9 times higher for PALMS than for Uniform (2.8 times higher than for Uniform + Ori). Examples of particles distributions for the three cases at different stages of convergence are shown in Fig. 3."}, {"title": "V. DISCUSSION AND CONCLUSIONS", "content": "We presented an indoor localization algorithm using an iPhone LiDAR scan to acquire nearby wall geometry, then finding locations in a floor plan consistent with the observed wall geometry for possible camera orientations. This information filters out unlikely locations but cannot precisely pinpoint the user due to geometric ambiguities in self-similar layouts. To address this, we generate a spatial probability distribution for each orientation to initialize a particle filter, which is expected to converge to the correct location.\nWe showed that this strategy reduces time to convergence and substantially increases accuracy after convergence compared to uniform particle initialization (i.e., without initial orientation and the initial information on the user location provided by the LiDAR scan). A main advantage of our approach is that it does not require prior \u201cfingerprinting\" of the environment, as it only uses a floor plan for prior information. It provides a solution to a critical bottleneck of dead-reckoning tracking algorithms (based on visual or inertial data), that is the determination of the initial user location and orientation.\nAt the same time, there are a number of limitations that need to be considered. Our system uses an iPhone LiDAR, which is only available on \u201cPro\u201d models at the time this paper is written. In addition, the maximum range of this sensor is 5 meters, which reduces its utility in wide open spaces. In future work, we will consider extracting planar wall models directly from images to enable detection of walls beyond the LiDAR's range, although it could be less accurate. While heatmap-based particle initialization has been shown to reduce convergence time, this system still requires the user to walk for a while (up to a minute) before convergence. This could prove impractical, and we will consider strategies like combining other modalities to reduce convergence time in future work. Finally, the dead-reckoning tracker we considered in this work was based on visual-inertial odometry from data recorded by an iPhone held by the user while walking. We will consider applying the same strategy to inertial-only tracking with particle filtering. In this configuration, users can use the phone to take a LiDAR scan, then conveniently put the phone back in their pocket, and be tracked by the phone's inertial sensors."}]}