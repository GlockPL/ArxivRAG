{"title": "SimVG: A Simple Framework for Visual Grounding with Decoupled Multi-modal Fusion", "authors": ["Ming Dai", "Lingfeng Yang", "Yihao Xu", "Zhenhua Feng", "Wankou Yang"], "abstract": "Visual grounding is a common vision task that involves grounding descriptive sentences to the corresponding regions of an image. Most existing methods use independent image-text encoding and apply complex hand-crafted modules or encoder-decoder architectures for modal interaction and query reasoning. How-ever, their performance significantly drops when dealing with complex textual expressions. This is because the former paradigm only utilizes limited downstream data to fit the multi-modal feature fusion. Therefore, it is only effective when the textual expressions are relatively simple. In contrast, given the wide diver-sity of textual expressions and the uniqueness of downstream training data, the existing fusion module, which extracts multimodal content from a visual-linguistic context, has not been fully investigated. In this paper, we present a simple yet robust transformer-based framework, SimVG, for visual grounding. Specifically, we decouple visual-linguistic feature fusion from downstream tasks by leverag-ing existing multimodal pre-trained models and incorporating additional object tokens to facilitate deep integration of downstream and pre-training tasks. Further-more, we design a dynamic weight-balance distillation method in the multi-branch synchronous learning process to enhance the representation capability of the sim-pler branch. This branch only consists of a lightweight MLP, which simplifies the structure and improves reasoning speed. Experiments on six widely used VG datasets, i.e., RefCOCO/+/g, ReferIt, Flickr30K, and GRefCOCO, demon-strate the superiority of SimVG. Finally, the proposed method not only achieves improvements in efficiency and convergence speed but also attains new state-of-the-art performance on these benchmarks. Codes and models will be available at https://github.com/Dmmm1997/SimVG.", "sections": [{"title": "1 Introduction", "content": "Visual grounding (VG) aims to predict the corresponding regions of an image through linguistic expressions. The task necessitates a comprehensive understanding of each modality, as well as the modeling of consistency between image context and text. Some benchmarks focus on addressing phrase localization [24, 48], which entails locating all objects mentioned in a sentence within an image. Another aspect emphasizes resolving referring expression comprehension (REC) [73, 46, 45], characterized by only one target corresponding to a sentence. Recently, a new type of general referring expression comprehension (GREC) [35, 16] task has emerged. GREC is similar to REC, but in which a sentence can have multiple targets or no target at all."}, {"title": "2 Related Work", "content": "Vision-Language Pre-Training. Existing vision-language pretraining (VLP) models can be broadly categorized into three main types: one-stream, dual-stream, and dual-stream with fusion encoder architectures. One-stream models [5, 25, 29, 21] process both image and text inputs in a single stream, concatenate image and text embeddings, and interact cross-modals information throughout the whole feature extraction process. In contrast, dual-stream models [49, 22, 32] employ separate encoders for each modality. These models do not concatenate modalities at the input level. Instead, the interaction between pooled image and text vectors occurs at a shallow layer. Dual-stream models with fusion encoder [30, 1, 55, 60] combine aspects of both one-stream and dual-stream models. They allow for intermediate interaction between modalities, potentially offering a balance between complexity and performance. In this paper, we improve the performance of visual grounding by decoupling multi-modal fusion from downstream tasks into upstream VLP models [60]."}, {"title": "3 The Proposed Method", "content": ""}, {"title": "3.1 The Overview of SimVG", "content": "As shown in Fig. 3, the SimVG structure can be roughly divided into three parts: multi-modality encoder, decoder, and head. The multi-modality encoder adopts a structure similar to BEiT-3 [60], and additionally sets a learnable object token. The decoder is divided into two branches: one is similar to the transformer decoder in DETR [2] (decoder branch), and the other utilizes a lightweight MLP (token branch). The head is referred to as the \"Distillation Head\". Unlike conventional prediction heads, to reduce the performance gap between the token and decoder branches, we employ a dynamic weight-balance distillation (DWBD) to minimize the performance difference between the two branches during synchronous learning."}, {"title": "Multi-Modalilty Encoder.", "content": "The input of SimVG include an image \\(I \\in \\mathbb{R}^{3\\times H \\times W}\\) and a caption text \\(T \\in \\Omega^M = \\{t_1, t_2, ..., t_M \\}\\), where \\(\\Omega\\) denotes the set of words. First, the image is compressed to a scale of 1/32 of the original size using visual embedding to obtain \\(P_{img} = \\{p^1, p^2, ..., p^N\\}\\). The text is then mapped to the vector space \\(L_{text} = \\{l_1, l_2, ..., l_K\\}\\) and an text padding mask \\(L_{mask} = \\{l_m^1, l_m^2, ..., l_m^K\\}\\). Additionally, we define a learnable object token \\(T_{obj}\\) as the target feature for the token branch. The query and attention padding mask of the transformer can be generated as:\n\\[\nQuery = \\{T_{obj}, p^1, ..., p^N, l_1, ..., l_K\\}, Mask = \\{T_{obj}^m, P_m^1, ..., P_m^N, l_m^1, ..., l_m^K\\}\n\\]\nIn the case where the image has no padding, \\(T_{obj}^m\\) and \\(\\{P_m^1, ..., P_m^N\\}\\) are set to 0. In the independent encoding part, FFN adopts a setting of non-shared weights between the image and text modalities, while the rest remains largely the same with the original ViT [10] model."}, {"title": "The Decoder Branch.", "content": "We first map the channel dimensions of the image tokens \\(T_{img} \\in \\mathbb{R}^{N_{img} \\times C}\\) and text tokens \\(T_{text} \\in \\mathbb{R}^{N_{text} \\times C}\\) using a linear layer without sharing weights. Then, on the text side, we apply the text-guided query generation (TQG) module to interact the predefined object queries \\(Q_{obj} \\in \\mathbb{R}^{N_{obj} \\times C}\\) with the text tokens. On the image side, additional positional encoding is applied. Lastly, cross-attention interaction is performed through transformer modules. The entire process can be represented as follows:\n\\[\nQ_{decoder} = MCA(IP(T_{img}) + pos, TQG(TP(T_{text}), Q_{obj})),\n\\]\nwhere TP and IP refer to the text projection and image projection. MCA refers to multi-head cross attention. pos refers to position encoding, which applies 2D absolute sine encoding."}, {"title": "The Token Branch.", "content": "We employ a linear layer OP to project object token \\(T_{obj} \\in \\mathbb{R}^{1 \\times C}\\) and use the results of TQG to augment the object token. Lastly, we use an MLP layer to further interact with and enrich the representation of the object token. The process of this branch can be defined as:\n\\[\nQ_{token} = MLP(OP(T_{obj}) + TQG(TP(T_{text}), Q_{obj})).\n\\]"}, {"title": "The Distillation Head.", "content": "We adopt the same Hungarian matching as DETR [2] for the decoder branch. The matching cost consists of three parts: binary cross-entropy loss, 11 loss, and GIoU loss [52]. However, to further simplify the inference pipeline, we use the decoder branch as a teacher to guide the learning of the token branch during the whole training process. Therefore, the complete loss can be represented as follows:\n\\[\nL_{total} = L_{det}(P_d, gt) + L_{dwbd}, L_{det} = \\lambda_1 L_{ce} + \\lambda_2 L_{11} + \\lambda_3 L_{giou},\n\\]\nwhere \\(\\lambda_1\\), \\(\\lambda_2\\), and \\(\\lambda_3\\) are set to 1, 5, and 2. \\(P_d\\) refers to the decoder branch prediction. \\(L_{dwbd}\\) refers to the dynamic weight-balance distillation loss, which will be explained in the next part."}, {"title": "3.2 Dynamic Weight-Balance Distillation", "content": "To make SimVG both efficient and effective, knowledge distillation is introduced, which leverages the predictions from the decoder branch as a teacher to guide the predictions of the token branch. Since the two branches share the features of the multi-modality encoder, training the teacher model independently using traditional knowledge distillation methods is not feasible. Instead, we employ a synchronous learning approach for both branches. This approach requires a delicate balance, ensuring that the performance of the teacher model is not compromised while maximizing the transfer of knowledge from the teacher branch to the student branch.\nTherefore, we design a dynamic weight-balance distillation (DWBD), whose architecture is shown in Fig. 3. Let us denote the ground truth by \\(y\\), and the set of \\(N_d\\) decoder prediction by \\(\\hat{y}_d = \\{\\hat{y}_i\\}_{i=1}^{N_d}\\). To find a bipartite matching between these two sets, we search for a permutation of \\(N_g\\) elements \\(\\sigma \\in \\Sigma_{N_g}\\), with the lowest cost:\n\\[\n\\hat{\\sigma} = ARGMIN_{\\sigma \\in \\Sigma_{N_g}} \\sum_{i=1}^{N_g} L_{match}(y_i, \\hat{y}_{\\sigma(i)}),\n\\]\nwhere \\(L_{match}(y_i, \\hat{y}_{\\sigma(i)})\\) is a pair-wise matching cost between the ground truth \\(y_i\\) and a prediction with index \\(\\sigma(i)\\). After pairing, the next step is to assess the decoder branch's understanding capability at the current stage. This is done by measuring the confidence of the joint target box of the decoder target \\(d_t\\) and ground truth \\(g_t\\) based on the IOU of their pairing:\n\\[\nW_{dt} = \\frac{1}{N_{gt}} \\sum_{i=1}^{N_{gt}} [IOU(b_i, b_{\\hat{\\sigma}(i)}) \\times SCORE(P_{\\hat{\\sigma}(i)})],\n\\]"}, {"title": null, "content": "where \\(N_{gt}\\) is the number of ground truth boxes, SCORE represents the foreground score extracted from the predictions. \\(W_{dt}\\) can be seen as a reflection of the current stage's decoder branch capability, where a higher value indicates a stronger confidence. Lastly, \\(L_{dwbd}\\) can be expressed as follows:\n\\[\nL_{dwbd} = \\gamma_1 (W_{dt} \\times L_{det}(P_t, d_t)) + \\gamma_2 (W_{gt} \\times L_{det}(P_t, g_t)), W_{gt} = 1 - W_{dt},\n\\]\nwhere \\(L_{det}\\) is computed exactly the same as in Eq. 4. \\(P_t\\) refers to the token branch prediction. \\(\\gamma_1\\) and \\(\\gamma_2\\) are set to 2 and 1 in this paper. By design, in the early stage of network training, \\(W_{gt} \\gg W_{dt}\\), the training process of the entire token branch is guided by the ground truth. However, in the later training stage, \\(W_{gt} < W_{dt}\\), the guidance from the decoder target becomes more significant. This dynamic adjustment of weights during training is the core idea of the proposed DWBD. We will further analyze the changes in \\(W_{dt}\\) and \\(L_{dwbd}\\) in Sec. 4.4.3. Additionally, to further enhance the performance of the token branch, we use a two-stage distillation approach. In the first stage, we train the decoder branch separately. In the second stage, we apply DWBD under the premise of synchronous learning for the two branches."}, {"title": "3.3 Text-guided Query Generation", "content": "The initial object queries, \\(O_{init}\\), are defined using learnable embeddings, without any prior informa-tion to guide them. From a macro perspective, visual grounding involves using text as a query to locate optimal regions in an image. Embedding text into queries to adaptively provide priors offers a viable solution. Therefore, we propose a text-guided query generation (TQG) module to generate object queries with text priors. As illustrated in Fig. 3, the process of generating queries through TQG can be expressed as follows:\n\\[\nQ_{tqg} = MCA(Q_{init}, f_{text} + pos, Mask) + MMP(f_{text}, Mask) + Q_{init},\n\\]\nwhere \\(f_{text} \\in \\mathbb{R}^{K \\times C}\\) is the feature after text projection, Mask is consistent with Eq. 1, and pos here is 1D absolute sine positional encoding. MMP is the process of filtering out valid tokens from \\(f_{text}\\) using the Mask and applying a max operation: \\(MMP(f, m) = max_i[f_i \\times (~m_i)]\\)."}, {"title": "4 Experimental Results", "content": ""}, {"title": "4.1 Datasets and Evaluation Metric", "content": "The experiments in this paper are conducted on six widely used datasets: RefCOCO/+/g [73, 45, 46], Flickr30K Entities [48], ReferItGame [24], and GRefCOCO [16]. For the referring expression comprehension and phrase localization tasks, Precision@0.5 is used as the evaluation metric. The GRefCOCO dataset is evaluated using the Precision@(F\u2081=1, IoU\u22650.5) and N-acc metrics. More descriptions about datasets and evaluation metrics are provided in Appendix A and B."}, {"title": "4.2 Implementation Details", "content": "We train SimVG 30 epochs for REC and phrase localization, and 200 epochs for GREC, using a batch size of 32. The Adam [26] optimizer with an initial learning rate of 5e-4 is employed, with a 10 times decay after 25 epochs. Following standard practices, images are resized to 640\u00d7640, and the length of language expressions is trimmed to 20 for all the datasets. For pre-training, SimVG is trained for 30 epochs and then fine-tuned for another 10 epochs. The pre-training experiments are run on 8 NVIDIA RTX 3090 GPUs. All the other experiments are conducted on 2 NVIDIA RTX 4090 GPUs. More implementation details are reported in Appendix C."}, {"title": "4.3 Comparison with The State-of-the-Art", "content": "In this part, we compare our SimVG with the SOTA methods on six mainstream datasets, i.e., RefCOCO/+/g, GRefCOCO, ReferItGame, and Flickr30K. We combine the results of RefCOCO/+/g, ReferItGame and Flickr30K datasets in Table 1, and the results of GREC are reported in Table 2. Table 3 reports the results of the methods pre-trained on the large corpus of data.\nAccording to Table 1, our model performs better than two-stage models, especially MAttNet [72] while being 7 times faster. We also surpass one-stage models that exploit prior and expert knowledge, with +14% absolute average improvement over ReSC [67]. Additionally, the use of a patch stride of 32 and a lightweight head design has enabled SimVG to achieve an inference speed of only 44 ms"}, {"title": "4.4 Ablation Studies", "content": ""}, {"title": "4.4.1 Multi-Modality Encoder Architecture", "content": "To investigate the advantages of decoupling multimodal fusion from visual grounding, we design three architectures for experimental verification. To ensure fairness, we consistently employ the"}, {"title": "4.4.2 Text-guided Query Generation", "content": "As indicated in Table 5, experimental results demonstrate a clear positive impact of the TQG module on both the token and decoder branches, achieving an average absolute improvement of 0.8 points. This guidance mechanism aligns with the concept of DAB-DETR [40], which injects textual priors into queries to imbue them with target-pointing properties. \"Mask Max Pool\" involves using a text mask to select valid text tokens and then performing max pooling to compress the dimensions, as represented in Sec. 3.3. Furthermore, Fig. 5 illustrates the impact of transformer layers on TQG. The 2-layer transformer structure is adopted to balance both efficiency and performance."}, {"title": "4.4.3 Dynamic Weight-Balance Distillation", "content": "The distillation experiments are shown in Table 6, where \"DETR Distill\" adopts the settings from [3] and uses the predictions from the decoder branch as the teacher for learning. \"Merge Distill\" combines the ground truth with the decoder prediction, enabling the token branch to select matching targets adaptively. It can be observed that all the three distillation methods improve the performance of the token branch, with the two-stage distillation method further enhancing its performance. Ultimately, our proposed DWBD achieves an average improvement of 1.5 points compared to the baseline. From Table 1 and Table 3, we observe that when employing the ViT-L as teacher model, the performance of the lightweight token branch can even surpass that of the decoder branch on certain metrics during synchronous learning. We hypothesize that this is primarily because the token branch distills more robust feature representations as the teacher's cognitive capabilities improve. Additionally, Fig. 6 illustrates the dynamic balance process of DWBD during training. We can observe that as the confidence of the decoder branch increases, the value of \\(W_{dt}\\) rises correspondingly, indicating that the decoder branch provides more guidance to the token branch. This mechanism allows for the dynamic adjustment of guidance distribution between the ground truth and the decoder prediction."}, {"title": "5 Conclusion", "content": "In this paper, we re-examine the visual grounding task by decoupling image-text mutual understanding from the downstream task. We construct a simple yet powerful model architecture named SimVG, which leverages the existing research in multimodal fusion to fully explore the contextual associations between modalities. Additionally, to simplify the whole pipeline while maintaining performance, we adopt dynamic weight-balance distillation (DWBD) to let the stronger decoder branch guide the lightweight token branch while learning synchronously. Furthermore, we propose a text-guided query generation (TQG) module to provide textual prior knowledge for object queries. Experimental results demonstrate that SimVG not only achieves improvements in efficiency and convergence speed but also attains new state-of-the-art performance across various benchmarks."}]}