{"title": "Training Neural Networks for Modularity Aids Interpretability", "authors": ["Satvik Golechha", "Dylan Cope", "Nandi Schoots"], "abstract": "An approach to improve network interpretability is via clusterability, i.e., splitting a model into disjoint clusters that can be studied independently. We find pretrained models to be highly unclusterable and thus train models to be more modular using an \u201cenmeshment loss\u201d function that encourages the formation of non-interacting clusters. Using automated interpretability measures, we show that our method finds clusters that learn different, disjoint, and smaller circuits for CIFAR-10 labels. Our approach provides a promising direction for making neural networks easier to interpret.", "sections": [{"title": "Introduction", "content": "Interpretability is an active area of research that aims to solve both high-stake deployment constraints for fairness and robustness [17] and AI safety and trustworthiness concerns [1]. Several interpretability breakthroughs over the last few years have helped us understand the inner workings of deep networks, both via circuits [22, 19, 5] and representation spaces [25, 2]. However, applying these methods to larger models [11] and complex behaviors has been a major hurdle for interpretability [14, 8], largely due to complicated computational subgraphs or circuits, andsuperposition [9], i.e., networks representing more features than they have neurons.\nAn alternative approach is to split models into modular clusters and interpreting them separately. However, this is feasible only if the interaction between clusters is minor. In this work, we attempt to make models modular and more interpretable during training.\nThe main contributions of this paper are as follows:\n\u2022 We modify and test prior clusterability methods to split a trained neural network layer into bipartite clusters and show that the clusters thus found are highly enmeshed and not good for our interpretability goals.\n\u2022 We introduce \u201cenmeshment loss\u201d, a regularizing term that promotes modularity during neural network training to get non-interfering clusters during model training.\n\u2022 We use automated interpretability measures to show that the clusters thus obtained make the model more interpretable by (a) reducing the search-space of circuit-style analyses by reducing circuit size (Section 3.2), and (b) by producing specialized clusters for each label (Section 3.1)."}, {"title": "Bipartite Spectral Graph Clustering (BSGC)", "content": "First, we show that existing clustering methods are ineffective for interpreting neural networks. For our clustering algorithm, we modify the methodology of Filan et al. [6] and use normalized spectral clustering to split a neural network layer into k different bipartite clusters.\nOur clustering method, called Bipartite Spectral Graph Clustering (BSGC) is shown in Algorithm 1. The similarity matrix for BSGC can be created by either the weights of the model or the accumulated gradients.\nWeight-based BSGC. Here, we use the weight matrix of a layer as the similarity matrix between neurons of adjacent hidden layers, based on the idea that neurons with strong weights connecting them can be expected to cluster well.\nGradient-based BSGC. Analyzing the gradients of each parameter during training gives us another way to cluster models. The idea is that weights that update together are likely to be part of the same circuit and connect neurons that cluster well together. We set the similarity matrix in Algorithm 1 to the average cosine similarity of the gradients of each parameter."}, {"title": "Clusterability Evaluation and the Enmeshment Loss", "content": "We evaluate the efficacy of a clustering method by the amount of \u201cclusterability\u201d or \u201cenmeshment\" in the clusters obtained, i.e., the average fraction of weights that are inside a cluster as opposed to between clusters.\nLet W be the weight matrix where $W_{ij}$ represents the edge weight between nodes i and j in the layer's input and output neurons respectively. Define U and V as the clusters for the rows and columns of A, respectively. Let $C_u(u)$ and $C_v(v)$ denote the sets of nodes in clusters $u \\in \\{1, ..., k\\}$ and $v \\in \\{1, ..., k\\}$, respectively. The enmeshment measure E is defined as follows:\n$E = \\frac{\\sum_{i=1}^n \\sum_{j=1}^n W_{ij} \\mathbb{I}(i \\in C_v(u)/j \\in C_v(u))}{\\sum_{i=1}^n \\sum_{j=1}^n W_{ij}}$ where $\\mathbb{I} = \\begin{cases}\n1 & \\text{if } i \\in C_v(u) \\text{ and } j \\in C_v(u)\n\\\\ 0 & \\text{otherwise}\n\\end{cases}$\nIn Figure 1, we see that as the number of clusters increases, the amount of enmeshment decreases, and even at k = 2 clusters, we get E = 0.6, which is substantial interference between clusters and hinders our interpretability goals.\nIn Section 3, we train models to be more modular by optimizing for clusterability by adding the enmeshment loss to the usual cross-entropy loss:\n$\\mathcal{L} = \\mathcal{L}_{CE} + \\lambda E$\nwith an appropriate clusterability coefficient \u5165."}, {"title": "Training for Modularity and Evaluating Clusters for Interpretability", "content": "Our training pipeline comprises three steps:\n1. Train the original model for a few steps. This helps important connections to form before we begin clustering. It can also allow \"winning tickets\" (as defined in the lottery ticket hypothesis [7]) to emerge.\n2. Split a layer into clusters using weight-based bipartite spectral clustering (see Algorithm 1).\n3. Complete the rest of the training with the \u201cenmeshment loss\" added to the cross-entropy loss to promote the clusters to be modular (see Section 2).\nWe experiment with vanilla MLPs on MNIST [4] and CNNs on the CIFAR-10 dataset [12]. We train for n = 1 epoch and then extract clusters and continue training for n = 10 more epochs to get a clustered model that perform similarly as the original model but with a clusterability score of 0.99. All our hyperparameters are given in Appendix A.\nWe evaluate our clusters and proxy their interpretability gains via two metrics: class-wise accuracy (with and without each individual cluster), and Effective Circuit Size (ECS) for each label."}, {"title": "Clusters Specialize in Class-level Features", "content": "Figure 2 compares class-wise accuracy of the model on the CIFAR-10 dataset with individual clusters turned OFF and ON respectively in a trained model with an accuracy > 95% for each label. Here, we define switching a cluster ON as zero-ablating the activations of all the other clusters, and switching it OFF as zero-ablating the activations of the given cluster while keeping the others the same. Thus, we get a sense of the extent to which various clusters contribute independently or with other clusters to predict a given label."}, {"title": "Effective Circuit Size (ECS)", "content": "Figure 3 compares the Effective Circuit Size (ECS) for each label for the clustered and un-clustered models on CIFAR-10. We use automated pruning (see ACDC [3]) to extract the circuit for each label and then compute the number of parameters in the circuit and define it to be the ECS for that label and model. We show that an un-clustered model has on average 61.25% more parameters in its effective circuits, thus making the clustered model easier to interpret. In Appendix B, we share similar results for the MNIST dataset [4]."}, {"title": "Related Work", "content": "Clustering of neural network weights is typically done either using graph properties of the weights (structural) [23, 6, 20] or using correlations between neuron activations (functional) [10, 13]. In this paper, we consider both weight-based and gradient-based clustering. MoEfication groups feedforward neurons in a pre-trained language model into clusters of 'experts' and at inference only activates the most clusters neurons [24].\nModularity metrics inspired by research in neuroscience incorporate transfer entropy [18, 21] or spatial metrics [15, 16]. Models are sometimes trained or pruned using modularity metrics [20].\nCircuit Discovery is an active field of research [22, 19, 5, 3], which aims to uncover subnetworks that perform a specific functionality. In this work we use an enmeshment loss that calculates the structural connectedness or global functionality without considering specific functionalities. We evaluate the resulting modules by assessing the extent to which they have specialized on a class level, i.e. the extent to which they correspond to class-specific performance."}, {"title": "Conclusion", "content": "We show that a simple regularizer is effective at splitting a neural network layer into simpler, more interpretable clusters. We show that the average circuit size and the search space improve with more clusters without a decrease in overall performance (for the MNIST and CIFAR-10 models that we investigated).\nWe expect the Pareto frontier (clusterability versus performance) to be challenging as we scale to harder tasks and larger models. We are also interested in using our insights to train and align language models with modularity and see if it leads to better interpretability and control for unwanted behaviors. We hope that modularity can help with a number of mechanistic interpretability goals, and a concrete exploration into this (such as clustering attention heads or linear probes) is another future direction."}, {"title": "Hyperparameters", "content": "We list the hyperparameters and various design choices in our experiments in Tab. 1, and refer to our codebase for more details (to be added during de-anonymization)."}, {"title": "Results on MNIST", "content": "Here we present our results on the MNIST [4] dataset."}, {"title": "Clustered Layer Visualization", "content": "Fig. 7 shows the visualization of the fully connected layer trained on CIFAR10 of the network with bipartite clusters."}]}