{"title": "On Evaluating The Performance of Watermarked Machine-Generated Texts Under Adversarial Attacks", "authors": ["Zesen Liu", "Tianshuo Cong", "Xinlei He", "Qi Li"], "abstract": "Large Language Models (LLMs) excel in various applications, including text generation and complex tasks. However, the misuse of LLMs raises concerns about the authenticity and ethical implications of the content they produce, such as deepfake news, academic fraud, and copyright infringement. Watermarking techniques, which embed identifiable markers in machine-generated text, offer a promising solution to these issues by allowing for content verification and origin tracing. Unfortunately, the robustness of current LLM watermarking schemes under potential watermark removal attacks has not been comprehensively explored.\nIn this paper, to fill this gap, we first systematically comb the mainstream watermarking schemes and removal attacks on machine-generated texts, and then we categorize them into pre-text (before text generation) and post-text (after text generation) classes so that we can conduct diversified analyses. In our experiments, we evaluate eight watermarks (five pre-text, three post-text) and twelve attacks (two pre-text, ten post-text) across 87 scenarios. Evaluation results indicate that (1) KGW and Exponential watermarks offer high text quality and watermark retention but remain vulnerable to most attacks; (2) Post-text attacks are found to be more efficient and practical than pre-text attacks; (3) Pre-text watermarks are generally more imperceptible, as they do not alter text fluency, unlike post-text watermarks; (4) Additionally, combined attack methods can significantly increase effectiveness, highlighting the need for more robust watermarking solutions. Our study underscores the vulnerabilities of current techniques and the necessity for developing more resilient schemes.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are rapidly advancing in capability, demonstrating remarkable proficiency across a wide range of applications. From generating coherent and contextually appropriate text to assisting in complex tasks such as code generation [6, 21, 44, 74, 75], medical diagnosis [14, 26, 65], and content creation [17, 39, 54], LLMs are revolutionizing various domains. Their integration into these fields highlights the transformative potential of artificial intelligence, providing enhanced productivity and innovative solutions to longstanding challenges.\nDespite their powerful capabilities, the rise of LLMs has also sparked significant concerns regarding the authenticity and ethical implications of the content they generate. Issues such as the creation of deepfake texts [50, 59, 72], automated homework and programming assignments [62], and the spread of misinformation [55, 80] pose serious risks to the integrity of information and trust in digital communications. The ability of LLMs to produce highly realistic and human-like text exacerbates these concerns, making it increasingly difficult to distinguish between human and machine-generated content.\nTo address these challenges, watermark techniques [28, 52] have emerged as promising solutions. By embedding identifiable markers within the machine-generated text, these techniques aim to provide a reliable means of tracing the origin of the text and verifying its authenticity. This approach offers a potential safeguard against the misuse of LLMs, helping to preserve the credibility of information and enhance accountability in content creation.\nHowever, the robustness of these watermarking schemes remains questionable. Adversaries may develop methods to circumvent or remove watermarks, undermining their effectiveness and potentially rendering them unreliable. The resilience of watermarking techniques against various forms of manipulation and obfuscation is critical to their success and necessitates rigorous evaluation to ensure their practicality in real-world scenarios.\nOur work. In this paper, we fill the gap in the literature by systematically categorizing watermarks and attacks into pre-text and post-text classes, where the former involves watermark injection or disruption before text generation, and the latter does so after text generation. We consider eight watermarks (five pre-text and three post-text) and twelve attacks (two pre-text and ten post-text), resulting in a total of 87 possible scenarios (see Table 3). Our evaluation shows that KGW [29] and Exponential [1] are the two best watermarks by providing good text quality and relatively high watermark rates after different attacks compared to other watermarks. For instance, KGW and Exponential achieve over"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Large Language Models (LLMs)", "content": "In recent years, Large Language Models (LLMs) have achieved remarkable success in the field of Natural Language Processing (NLP). These models, trained on extensive datasets, have demonstrated unparalleled language understanding and generation capabilities. They have revolutionized various applications, from conversational agents like OpenAI's ChatGPT and GPT-4 to advanced systems such as Google's Gemini. LLMs have not only excelled in traditional language tasks but have also proven instrumental in solving complex, real-world problems, greatly enhancing human productivity and interaction with technology.\nFollowing [53], we model the generation process of LLMs into two steps: probability prediction and token selection. First, given a sequence of tokens $x_{1:n}$ (e.g., the input prompt) where $x_i \\in \\{1,..., |V|\\}$ is a token and $|V|$ is the size of vocabulary V, LLM calculates the conditional probability of the next token as $Pr(x_{n+1}|x_{1:n})$. Then, LLM uses a specific sampling strategy to select the word from the vocabulary as the final token, e.g., Greedy Sampling confirms next token by $x_{n+1} = \\arg \\max_x Pr(x|x_{1:n})$. Naturally, LLM generates a l length text through computing $Pr(x_{1:n+1}) = \\prod_{i=1}^{n+1} Pr(x_i|x_{1:i-1})$."}, {"title": "2.2 Preliminary of Watermarks", "content": "We first classify the watermarking schemes from different perspectives. For the access needed to the model's internal parameters, we classify schemes into black-box and white-box. Meanwhile, as Figure 1 shows, according to the watermarking scheme with respect to the time involved in the text generation process, we classify them into pre-text schemes and post-text schemes. Note that the same classification rules can be applied to watermark removal attacks, that is, we will discuss white- and black-box removal attacks, as well as pre-text and post-text attacks. Nevertheless, regardless of the classification, an LLM watermarking scheme typically contains two processes: watermark injection and watermark detection.\nBlack-box & White-box. A watermarking scheme is classified as white-box if it relies on the model's internal parameters during watermark injection. Conversely, a scheme is black-box if it does not access model parameters. Notably, schemes accessing the last hidden state of an LLM are also considered as black-box.\nPre-text & Post-text. We further categorize a watermarking scheme based on the time it is conducted, labeling it as either pre-text or post-text. If it is conducted before or during the text generation phase, we classify it as a pre-text watermarking scheme. Conversely, we regard it as a post-text watermarking scheme.\nWatermark injection. During the watermark injection process, a multitude of distinct scheme-related hyperparameters are necessitated: 1) The pre-text watermarking schemes [1, 9, 24, 29, 34, 35, 40, 43, 45, 79] usually requires a secret key and a pseudo-random number generator; 2) For the post-text watermarking schemes [48, 58, 64, 77, 78], only a watermark signal (e.g. the specific whitespace \\u2004 or \\u2006) is needed. In this paper, given a watermarking scheme, we denote all hyperparameters it used as $k$. Therefore, given input x, the watermark injection process can be defined as:\n$y = Scheme_{inj}(x,\\kappa; M),$ (1)\nwhere M is the target model that the scheme aims to protect and y is the watermarked text.\nWatermark detection. As the inverse process of the injection process, the detection process is defined as:\n$\\left\\{True, False\\right\\} = Scheme_{dec}(y, \\kappa; M),$ (2)\nwhere True stands for that we can extract the watermark information from model M by using the same $k$."}, {"title": "3 Watermarks", "content": "First of all, we clarify that we focus on the watermarking schemes which are based on the generated texts in this paper. Compared with the watermarking schemes which need to embed the watermarks by updating the model parameters [36, 56], these text-based schemes can be adapted to any causal language models. Meanwhile, we consider two types of watermarks, i.e., pre-text watermarks and post-text watermarks, where the former injects the watermark before or during text generation and the later injects the watermark after the text generation.\nThe methods of pre-text watermark can be divided into two categories based on the process used for generating text with an LLM [41]. For instance, this includes modifying the logits [24, 29, 35, 40, 43, 45, 79] and modifying the token sampling strategy during the inference phase [1, 9, 34]. The methods of post-text watermark can be divided into four categories based on the granularity of text modification [41], which are format-based watermark [58, 64], lexcial-based watermark [51, 77], synatic-based watermark [48, 70], and generation-based watermark [2, 78]. To comprehensively discuss the robustness of various types of watermarking schemes, we discuss eight LLM watermarking schemes in this paper, which are summarized in Table 1."}, {"title": "3.1 Pre-text Watermarks", "content": "Pre-text watermarks can be categorized into token sampling-based and logits modification-based watermarks where the former modifies the token sampling strategy and the later modifies logits during the inference phase.\nToken sampling-based watermark. Watermark injection during token sampling is influenced by the randomness factor of the token sampling strategy. The watermark injection phase utilizes a random number generated by a fixed seed or a random seed calculated based on prefix tokens to guide the token sampling procedure and generate watermarked texts. During the watermark detection, alignment between output text tokens and the random number suffices for comparison. In this paper, we explore three token sampling strategies: bit-string conversion [9], exponential [1], and inverse token sampling [34].\n\\begin{itemize}\n\\item Convert bit-string (Convert) [9] aims to develop undetectable watermarks. This watermark can be viewed as the hash operation on the generated text. Specifically, they first encode the generated text into a string with binary-bits. Then, they use a \\{0,1\\} hash function h and sample tokens $x_j$ with preference for those satisfying $h(x_j) = 1$. More tokens should hash to 1 than to 0 in watermarked text. The probability of a token being encoded to 1 is defined as $p_j(1)$. The watermarked model will output $x_j = 1$ if $u_j \\leq p_j(1)$ and $x_j = 0$ otherwise. Here $u_j$ is a real number sampled from [0,1]. The probability that the watermarked model output $x_j = 1$ is $p_j(1)$. For detection, they will compute a score $s(x_j,u_j) = \\ln \\frac{u_j}{1-u_j}$ if $x_j = 1$ and $s(x_j, u_j) = \\ln \\frac{1-u_j}{u_j}$ otherwise. Given a string $(x_1,...,x_l)$, the sum scores of detection is defined as $score = \\sum_{i=1}^{l} s(x_j,u_j)$. The score of the watermarked text should be significantly higher than the score of the normal text.\n\\item Exponential sampling (Exponential) [1] injects the watermark to text by selecting a token $x_n$ that maximizes the score which depends on the probability $Pr(x_n|x_{0:n-1})$ and a pseudo-random value. The pseudo-\n\\end{itemize}"}, {"title": "3.2 Post-text Watermarks", "content": "Post-text watermarks constitute a category of schemes wherein watermarks are appended to existing texts. The predominant approach for this type of watermark involves the modification of the existing text to incorporate the watermarking information. We implement three watermark schemes belonging to this category in this paper, and we divided the three watermarks into two categories which are Format-based watermark and Lexical-based watermark according to the granularity of modifications.\nFormat-based watermark. The Format-based watermark derives from a image watermark technology [5]. It does not modify the content of text but changes the format of text that are imperceptible for human, thereby implement the watermark injection.\n\\begin{itemize}\n\\item WHITEMARK [64] exploits the fact that the Unicode of text has several codepoints for whitespace. They can replaces the origin whitespace (e.g. U+0020) to another new whitespace (e.g. U+2004) without degrading the quality of text. The watermark can be detected by calculating the probability of new whitespace.\n\\item UniSpaCh [58] propose a text-based data hiding method for Microsoft Word documents. In their method, they leverage the whitespace between words and replace it with different Unicode space characters without degrading text quality. This approach can be considered a format-based watermarking technique for text. During the detection process, they assess the number of replaced space characters and compare the results against a predefined threshold.\n\\end{itemize}"}, {"title": "4 Watermark Removal Attacks", "content": "We discuss twelve watermark removal attacks in this paper, in which two attacks belong to pre-text attacks, and the other ten attacks are post-text attacks (see Table 2)."}, {"title": "4.1 Pre-text Attacks", "content": "The Pre-text Attacks focus on the text generation process by introducing perturbations(e.g. emojis) prior to the generation of the text this category of attacks can only be applied to the Pre-text watermarks. In addition, the distill attack will distill the target model without any effects on the generated text. Hence, we also view this attack as a Pre-text Attack in our work.\n\\begin{itemize}\n\\item Emoji attacks. Kirchenbauer et al. [29] propose an attack that aims to indiscriminate the generative capability of LLMs. We can prompt the LLM to modify its output\n\\end{itemize}"}, {"title": "4.2 Post-text Attacks", "content": "Piet et al. [57] implement a benchmark that evaluates the watermark capabilities of large language models. There are several perturbations to the prompt in their code which are proposed in helm [38] and we select the perturbations that have no effect on the meaning of the text.\n\\begin{itemize}\n\\item Contraction attack. The attack contracts verbs in the text (e.g. contract is not to isn't).\n\\item Expansion attack. In contrast to the contraction attack, this attack will expand some verb (e.g. expand don't to do not).\n\\item Lowercase attack. This attack converts all letters to lowercase.\n\\item Misspelling attack. This attack misspells certain words with the probability p.\n\\item Typo attack. This attack converts certain words to typos with the probability p.\n\\end{itemize}"}, {"title": "5 Experiment Settings", "content": ""}, {"title": "5.1 Basic Setup", "content": "Target model. We use Llama-2-7B-chat [71] as our target model to implement all watermarking schemes. Note that during inference, we use the below chat template to feed prompts. The max output sequence length is set to 1,024 tokens.\n<<s>> [INST] <<SYS>>\\{\\{system_prompt\\}}<</SYS> Question: \\{question\\}[/INST] Answer:"}, {"title": "5.2 Evaluation Metrics", "content": "As Table 4 shows, there are a total of three metrics used in our paper, that is, Quality Score, Watermark Rate, and Robustness Score.\nQuality score. We use the quality score proposed by [57] to evaluate the impact of removal attacks on the quality of AI-generated text, i.e., to quantify the change in text quality before and after the attack. Following [7, 8, 22, 32, 42, 57, 73], we leverage LLM to evaluate the quality of each AI-generated text. For instance, given a question $x_i$ from $D_{Gen}$, we first feed it into the target model to generate its response, then we feed them together with an evaluating prompt into Llama-3-8B-instruct [4], one of the most advancing open-source model, to get a quality grade $q_i \\in [0,1]$. If the responses do not suffer attacks, we calculate the mean quality score of the whole dataset as $Q^{clean} = (\\sum q_i)/n$, n is $|D_{Gen}|$. Similarly, we use $Q^{attack}$ to denote the mean quality score of the attacked responses. As a result, the final quality score could be defined as:\n$Q = 0.5 \\cdot Q^{clean} + 0.5 \\cdot (\\max(0, \\min(\\frac{Q^{attack}}{Q^{clean}}, 1))).$ (7)"}, {"title": "5.3 Experimental Setup", "content": "This section provides all the hyperparameters and other experiments setup for all watermark and attack methods.\nWatermark setup. We summarize the setup of all eight watermark schemes here. For the Logits modification-based watermarks, in the experiments of KGW, we use the setting that has the best performance, which is $(\\gamma,\\delta) = (0.25,2)$, where $\\gamma$ is the proportion of green list tokens on the whole vocabulary, $\\delta$ is the bias we added to the logits of every token during the inference process. In Unigram, we use the same setting as KGW which also achieves the best performance in our experiments.\nFor the Token sampling-based watermark schemes, in Convert and Exponential, the number of hashed keys is set to 4. In Inverse, the number of hashed keys is set to 4 and the number of shifts is set to 2.\nFor the Format-based watermark, in WHITEMARK, the replaced whitespace is \\u2004 and the probability of replacement is set to 0.6. In UniSpaCh watermark, the several specific whitespace we used are \\{\\u2000, \\u2001, \\u2004, \\u2006, \\u2007, \\u2008, \\u2009, \\u200A \\}. The probability of replacement is set to 0.6.\nFor the Lexcial-based watermark, we use the default setting that the similarity threshold is set to 0.5 and the number of candidate synonyms is set to 8 for every word.\nNote that the watermark detection threshold is set to 0.95 for all watermark schemes.\nAttack based on LLMs setup. The attack methods in our paper have been divided into two categories which are attack based on LLMs and attack based on existing text. For the former category, we implement two attack methods including Emoji attack [18] and distill attack [19]. For the latter category, we implement nine attack methods including contraction, lowercase, expansion, misspelling, typo which are proposed by Liang et al. [38], modify, synonym which are proposed by Piet et al. [57], paraphrase [33] and translation [13].\nThe former five attack methods have different experiment setups and we will introduce them respectively. We implement the latter nine attack methods with the same setup. These attack methods will be applied to the model outputs where the watermark has been injected into.\nEmoji attack. During the inference phase, we execute this assault. For the query dataset, we append an attack prompt to each query, specifically instructing, \u201cAdditionally, please add two \\uD83D\\uDE0A emojis after each output word.\" Subsequently, all \\uD83D\\uDE0A emojis present in the model's output will be eliminated. This attack is capable of targeting the watermark while minimizing any potential degradation to the textual quality.\nDistill attack. In this attack, we need to train a student model on the dataset which is distilled from the target teacher model. Both the teacher and student models are Llama-2-7B-chat. Different from the typical distill methods, this attack will teach the student model to match the next token distribution outputted by the teacher model when using water-"}, {"title": "6 Evaluation Results", "content": "In this section, we first evaluate the robustness of the watermarks against different individual attacks, along with performing extensive experiments to assess the quality and watermark rate against the combined attack (i.e., with two or more different attack strategies combined together), from which we identify the optimal attack combination. This combined attack was then subjected to further discussion and experimental analysis. After that, we evaluate the efficiency and imperceptibility of different watermarks."}, {"title": "6.1 Robustness Against Individual Attacks", "content": "We first evaluate the robustness of different watermarking schemes against individual watermark removal attacks, including pre-text attacks and post-text attacks.\nRobustness score analysis. We first launch individual attacks against watermarking schemes and report the mean values of the robustness scores (Equation (9)). As Table 5 shows, we observe that pre-text watermarks are more robust than post-text watermarks in general. For instance, KGW and Exponential reach 0.5220 and 0.5012 robustness scores, respectively, while WHITEMARK and UniSpaCh only reach 0.3986 and 0.4209 robustness scores. This is expected as the pre-text watermarks usually involve more complex strategies to inject the watermark into the whole text while post-text watermarks only replace a few tokens, which makes them more vulnerable to attacks. However, the robustness score falls significantly short of the benchmark established for the ideal watermark (which should have a 1.0 robustness score), indicating a need for further optimization and improvement.\nQuality & Watermark rate. Here we dive deeper into the quality and watermark rate performance of each watermark against different attacks. The results are summarized in Figure 2. Regarding the quality, we find that KGW, Convert, Inverse, Exponential, and UniSpaCh preserve the quality to a large extent after different attacks, e.g., around 0.8. However, for most of the watermarks, the watermark rate drops drastically to less than 0.6, and most of them are near 0.0, which means the watermark cannot be extracted after the attack. This further emphasizes the vulnerability of the watermarks on texts under potential adversarial attacks.\nImpact of different attack methods. From now on, focus on KGW to discuss the attacking effectiveness as KGW reaches the best performance in our previous evaluation. We first discover that Typo, Token, Translation and Emoji attacks are more effective than others. For Typo, the attack has a significant impact on the model's output texts. For Emoji and Token attacks, while these two attacks might only induce minor disruptions in the token generation process, they could potentially impact the distribution of the entire output token list, leading to a significant decrease in the watermark detection of KGW. For Translation and Paraphrase attacks, both forms of attack entirely alter the output text of the model, while preserving the semantic integrity of the said text. Hence, there is a great effect on the output text token which results in a reduction in the watermark rate. Meanwhile, Figure 3i demonstrates that the distillation attack is ineffective, as the distilled model retains a watermark rate exceeding 0.65. Moreover, this watermark rate increases with the length of token sequences. This is due to the fact that the distillation approach employed for the model does not inflict any damage on the model's output.\nImpact of attacking hyperparameters. We then evaluate the watermark robustness with different hyperparameter settings for each attack. The results are shown in Figure 3. We observe that, regarding different hyperparameter settings, the watermark quality is similar but the watermark rate may differ. The empirical evidence gathered suggests that even slight alterations in these parameters can significantly impact"}, {"title": "6.2 Robustness Against Combined Attacks", "content": "To more effectively illustrate the vulnerability of machine-generated text watermarks in real-world scenarios, we advocate for the implementation of combined attack strategies.\nThe optimal watermarks for pre-text (KGW) and post-text (UniSpaCh) are chosen as the target watermark algorithm in this experiment and the results are shown in Figure 4 and Figure 5, respectively. From each sub-figure, the rows represent the first attack and the columns represent the second attack.\nResults overview. In general, KGW maintains a higher quality and watermark rate than UniSpaCh. For instance, when applying Paraphrase first and Modify later, KGW reaches 0.5726 quality while UniSpaCh only has 0.2542 quality. Meanwhile, it is evident that the columns and rows representing Paraphrase and Translation attacks exhibit superior quality. When Paraphrase and Translation are utilized as the secondary attack, it has been observed that this approach can even lead to an enhancement in the quality of the text. However, the watermark rates are all reduced regardless of the attack order and attack category.\nQuality results analysis. Recall that the quality score reflects whether the generated text can align with our real requirements and if the text remains consistent with the query's answer. Hence, the text quality will be higher when the text is more informative and has fewer word errors. When a generated text has been attacked by schemes such as Modify and Typo attacks, more grammatical errors will be generated in the attacked text, thereby causing quality degradation. However, we can regard the Paraphrase and Translation attack as text correction tools that can be used to remove grammatical errors in the text. This property of both attacks leads to an increase in text quality after the combined attack.\nWatermark rate results analysis. Regarding the watermark"}, {"title": "6.3 Efficiency", "content": "Besides robustness, efficiency is also an important factor in evaluating the performance of watermarking schemes and removal attacks. To this end, we further measure the runtime of watermarking schemes (including both the injection and detection process) and removal attacks by using the whole watermark generation dataset DGen. Our implementation environment is on one NVIDIA A800 GPU.\nWatermark efficiency. We summarize the runtime evaluation results of watermarking schemes in Table 6. We could get the following observations: (1) For the pre-text watermarking schemes, we notice that the watermark injection runtimes are roughly around 0.5 to 1 hour and the detection runtimes are all less than 600 seconds. Taking into account the duration of text generation, the time allocated to watermarking is comparatively minimal. This observation underscores that the operational efficiency of the watermark LLM falls within a tolerable threshold, making it available for real-world scenarios. (2) For the post-text watermarks, we find that there is a great rum-time gap among them. For instance, the experiment results of WHITEMARK and UniSpaCh are both less than 150 seconds. However, the Linguistic watermark is extremely slow with an injection/detection speed of 31.52/56.34 hours. This empirical evidence underscores the significance of the efficiency of injection and detection for watermarking in the overall evaluation of the watermark's performance. Therefore, we can conclude that even when a watermark exhibits high robustness if it is characterized by low efficiency, its applicability in real-world scenarios is limited. This highlights the need to balance both efficiency and robustness in the watermark design.\nAttack efficiency. We summarize the runtime for different watermarking removal attacks with different hyperparameters in Figure 6. We observe that, while Paraphrase and Translation attacks demonstrate high effectiveness (Fig-"}, {"title": "6.4 Imperceptibility", "content": "Evaluation methodology. As pointed out in [46, 57], an ideal watermarking scheme should exhibit good imperceptibility. In other words, imperceptibility measures how well the watermarking information is hidden such that the watermarking scheme doesn't affect the readability, coherence, or naturalness of the AI-generated texts. To quantify imperceptibility, we leverage LLM to judge if a text is an AI-generated text that contains watermark and use the classification accuracy as the imperceptibility results. As the texts to be judged are all watermarked texts, consequently, a lower imperceptibility value indicates that LLM cannot recognize the hidden watermark information. Here we regard Llama-3-8B-instruct [4] as the classifier and use the following prompt to classify the texts.\n<<>>system<<>>\nYou are a assistant model to help me to judge if the given text have a watermark and is generated by machine, and please output your judgement 'True' or 'False' without any other content after it.\n### Text: \\{text\\}\n<<>>user<<>>\nPlease provide the judge result.\n### Result:\n<<>>assistant<</>>\nEvaluation results. We evaluate the imperceptibility of each watermarking scheme and present the results in Figure 7. We observe that pre-text watermarks achieve better imperceptibility (lower value is better). This is because the pre-text watermarking schemes just modify the logits or the token-sampling strategy during the inference process, thus, there is no obvious watermark signal left in the text that the judgment LLM can capture. In contrast, all three post-text watermarking schemes need to embed signals into the watermarked text, thereby causing a worse imperceptibility. For instance,"}, {"title": "7 Related Work", "content": ""}, {"title": "7.1 Machine-Generated Text Watermark", "content": "Watermark of machine-generated text has been extensively studied [27, 61, 67]. It can be viewed as a variant of steganography [10, 47] with one-bit or multi-bit messages. The text watermark has a strong requirement that the wa-"}, {"title": "8 Conclusion", "content": "In this paper, we systematically categorize watermarks and attacks on machine-generated texts into pre-text and post-text classes, providing a structured framework for evaluating their performance. Our comprehensive assessment involved eight watermarks and twelve attacks, resulting in 87 possible scenarios, allowing us to rigorously test the robustness, efficiency, and imperceptibility of existing watermarking techniques.\nOur findings indicate that KGW and Exponential watermarks are currently the most effective, offering good text quality and relatively high watermark retention rates after various attacks. However, despite their relative effectiveness, these watermarks remain vulnerable to a variety of attacks. The watermark rate for KGW, for example, significantly drops under certain attack combinations, highlighting the ongoing challenges in watermark resilience.\nEfficiency analysis revealed that both KGW and Exponential are efficient in watermark injection and detection. In terms of imperceptibility, pre-text watermarks perform better as they are embedded within token distributions and do not disrupt text fluency, unlike post-text watermarks which are more detectable due to their token manipulations. Regarding attacks, post-text attacks generally prove more efficient and practical compared to pre-text attacks, as they do not require modifications to the model's weights. Our study also demonstrates that adversaries can significantly enhance the effectiveness of their attacks by combining different methods. This underscores the urgent need for developing more robust watermarking solutions capable of withstanding diverse and combined attack strategies.\nIn conclusion, while our work highlights the promise of watermarking techniques in safeguarding the authenticity of machine-generated texts, we also exposes the limitations and vulnerabilities of current approaches. Our research emphasizes the necessity for continued innovation and development of more resilient watermarking schemes to ensure the integrity and reliability of digital communications. Our code and data will be made publicly available to support further research in this domain."}]}