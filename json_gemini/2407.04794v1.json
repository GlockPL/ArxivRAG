{"title": "On Evaluating The Performance of Watermarked Machine-Generated Texts Under Adversarial Attacks", "authors": ["Zesen Liu", "Tianshuo Cong", "Xinlei He", "Qi Li"], "abstract": "Large Language Models (LLMs) excel in various applications, including text generation and complex tasks. However, the misuse of LLMs raises concerns about the authenticity and ethical implications of the content they produce, such as deepfake news, academic fraud, and copyright infringement. Watermarking techniques, which embed identifiable markers in machine-generated text, offer a promising solution to these issues by allowing for content verification and origin tracing. Unfortunately, the robustness of current LLM watermarking schemes under potential watermark removal attacks has not been comprehensively explored.\nIn this paper, to fill this gap, we first systematically comb the mainstream watermarking schemes and removal attacks on machine-generated texts, and then we categorize them into pre-text (before text generation) and post-text (after text generation) classes so that we can conduct diversified analyses. In our experiments, we evaluate eight watermarks (five pre-text, three post-text) and twelve attacks (two pre-text, ten post-text) across 87 scenarios. Evaluation results indicate that (1) KGW and Exponential watermarks offer high text quality and watermark retention but remain vulnerable to most attacks; (2) Post-text attacks are found to be more efficient and practical than pre-text attacks; (3) Pre-text watermarks are generally more imperceptible, as they do not alter text fluency, unlike post-text watermarks; (4) Additionally, combined attack methods can significantly increase effectiveness, highlighting the need for more robust watermarking solutions. Our study underscores the vulnerabilities of current techniques and the necessity for developing more resilient schemes.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are rapidly advancing in capability, demonstrating remarkable proficiency across a wide range of applications. From generating coherent and contextually appropriate text to assisting in complex tasks such as code generation [6, 21, 44, 74, 75], medical diagnosis [14, 26, 65], and content creation [17, 39, 54], LLMs are revolutionizing various domains. Their integration into these fields highlights the transformative potential of artificial intelligence, providing enhanced productivity and innovative solutions to longstanding challenges.\nDespite their powerful capabilities, the rise of LLMs has also sparked significant concerns regarding the authenticity and ethical implications of the content they generate. Issues such as the creation of deepfake texts [50, 59, 72], automated homework and programming assignments [62], and the spread of misinformation [55, 80] pose serious risks to the integrity of information and trust in digital communications. The ability of LLMs to produce highly realistic and human-like text exacerbates these concerns, making it increasingly difficult to distinguish between human and machine-generated content.\nTo address these challenges, watermark techniques [28, 52] have emerged as promising solutions. By embedding identifiable markers within the machine-generated text, these techniques aim to provide a reliable means of tracing the origin of the text and verifying its authenticity. This approach offers a potential safeguard against the misuse of LLMs, helping to preserve the credibility of information and enhance accountability in content creation.\nHowever, the robustness of these watermarking schemes remains questionable. Adversaries may develop methods to circumvent or remove watermarks, undermining their effectiveness and potentially rendering them unreliable. The resilience of watermarking techniques against various forms of manipulation and obfuscation is critical to their success and necessitates rigorous evaluation to ensure their practicality in real-world scenarios.\nOur work. In this paper, we fill the gap in the literature by systematically categorizing watermarks and attacks into pre-text and post-text classes, where the former involves watermark injection or disruption before text generation, and the latter does so after text generation. We consider eight watermarks (five pre-text and three post-text) and twelve attacks (two pre-text and ten post-text), resulting in a total of 87 possible scenarios (see Table 3). Our evaluation shows that KGW [29] and Exponential [1] are the two best watermarks by providing good text quality and relatively high watermark rates after different attacks compared to other watermarks. For instance, KGW and Exponential achieve over"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Large Language Models (LLMs)", "content": "In recent years, Large Language Models (LLMs) have achieved remarkable success in the field of Natural Language Processing (NLP). These models, trained on extensive datasets, have demonstrated unparalleled language understanding and generation capabilities. They have revolutionized various applications, from conversational agents like OpenAI's ChatGPT and GPT-4 to advanced systems such as Google's Gemini. LLMs have not only excelled in traditional language tasks but have also proven instrumental in solving complex, real-world problems, greatly enhancing human productivity and interaction with technology.\nFollowing [53], we model the generation process of LLMs into two steps: probability prediction and token selection. First, given a sequence of tokens $x_{1:n}$ (e.g., the input prompt) where $x_i \\in {1,..., |V|}$ is a token and $|V|$ is the size of vocabulary V, LLM calculates the conditional probability of the next token as $Pr(x_{n+1}|x_{1:n})$. Then, LLM uses a specific sampling strategy to select the word from the vocabulary as the final token, e.g., Greedy Sampling confirms next token by $x_{n+1} = arg \\max_x Pr(x|x_{1:n})$. Naturally, LLM generates a l length text through computing $Pr(x_{1:n+1}) = \\Pi_{i=1}^{n+1} Pr(x_i|x_{1:i-1})$."}, {"title": "2.2 Preliminary of Watermarks", "content": "We first classify the watermarking schemes from different perspectives. For the access needed to the model's internal parameters, we classify schemes into black-box and white-box. Meanwhile, as Figure 1 shows, according to the watermarking scheme with respect to the time involved in the text generation process, we classify them into pre-text schemes and post-text schemes. Note that the same classification rules can be applied to watermark removal attacks, that is, we will discuss white- and black-box removal attacks, as well as pretext and post-text attacks. Nevertheless, regardless of the classification, an LLM watermarking scheme typically contains two processes: watermark injection and watermark detection.\nBlack-box & White-box. A watermarking scheme is classified as white-box if it relies on the model's internal parameters during watermark injection. Conversely, a scheme is black-box if it does not access model parameters. Notably, schemes accessing the last hidden state of an LLM are also considered as black-box.\nPre-text & Post-text. We further categorize a watermarking scheme based on the time it is conducted, labeling it as either pre-text or post-text. If it is conducted before or during the text generation phase, we classify it as a pre-text watermarking scheme. Conversely, we regard it as a post-text watermarking scheme.\nWatermark injection. During the watermark injection process, a multitude of distinct scheme-related hyperparameters are necessitated: 1) The pre-text watermarking schemes [1, 9, 24, 29, 34, 35, 40, 43, 45, 79] usually requires a secret key and a pseudo-random number generator; 2) For the post-text watermarking schemes [48, 58, 64, 77, 78], only a watermark signal (e.g. the specific whitespace \\u2004 or \\u2006) is needed. In this paper, given a watermarking scheme, we denote all hyperparameters it used as $k$. Therefore, given input x, the watermark injection process can be defined as:\n$y = Scheme_{inj}(x, \\M, k),$ (1)\nwhere $M$ is the target model that the scheme aims to protect and y is the watermarked text.\nWatermark detection. As the inverse process of the injection process, the detection process is defined as:\n${Ture, False} = Scheme_{dec}(y, k;M),$ (2)\nwhere True stands for that we can extract the watermark information from model M by using the same k."}, {"title": "3 Watermarks", "content": "First of all, we clarify that we focus on the watermarking schemes which are based on the generated texts in this paper. Compared with the watermarking schemes which need to embed the watermarks by updating the model parameters [36, 56], these text-based schemes can be adapted to any causal language models. Meanwhile, we consider two types of watermarks, i.e., pre-text watermarks and post-text watermarks, where the former injects the watermark before or during text generation and the later injects the watermark after the text generation.\nThe methods of pre-text watermark can be divided into two categories based on the process used for generating text with an LLM [41]. For instance, this includes modifying the logits [24, 29, 35, 40, 43, 45, 79] and modifying the token sampling strategy during the inference phase [1, 9, 34]. The methods of post-text watermark can be divided into four categories based on the granularity of text modification [41], which are format-based watermark [58, 64], lexcial-based watermark [51, 77], synatic-based watermark [48, 70], and generation-based watermark [2, 78]. To comprehensively discuss the robustness of various types of watermarking schemes, we discuss eight LLM watermarking schemes in this paper, which are summarized in Table 1."}, {"title": "3.1 Pre-text Watermarks", "content": "Pre-text watermarks can be categorized into token sampling-based and logits modification-based watermarks where the former modifies the token sampling strategy and the later modifies logits during the inference phase.\nToken sampling-based watermark. Watermark injection during token sampling is influenced by the randomness factor of the token sampling strategy. The watermark injection phase utilizes a random number generated by a fixed seed or a random seed calculated based on prefix tokens to guide the token sampling procedure and generate watermarked texts. During the watermark detection, alignment between output text tokens and the random number suffices for comparison. In this paper, we explore three token sampling strategies: bitstring conversion [9], exponential [1], and inverse token sampling [34].\n*   Convert bit-string (Convert) [9] aims to develop undetectable watermarks. This watermark can be viewed as the hash operation on the generated text. Specifically, they first encode the generated text into a string with binary-bits. Then, they use a {0,1} hash function h and sample tokens $x_j$ with preference for those satisfying $h(x_j) = 1$. More tokens should hash to 1 than to 0 in watermarked text. The probability of a token being encoded to 1 is defined as $p_j(1)$. The watermarked model will output $x_j = 1$ if $u_j \\le p_j(1)$ and $x_j = 0$ otherwise. Here $u_j$ is a real number sampled from [0,1]. The probability that the watermarked model output $x_j = 1$ is $p_j(1)$. For detection, they will compute a score $s(x_j,u_j) = \\ln \\frac{u_j}{1-u_j}$ if $x_j = 1$ and $s(x_j, u_j) = \\ln \\frac{1-u_j}{u_j}$ otherwise. Given a string $(x_1,...,x_l)$, the sum scores of detection is defined as $score = \\sum_{j=1}^{l} s(x_j,u_j)$. The score of the watermarked text should be significantly higher than the score of the normal text.\n*   Exponential sampling (Exponential) [1] injects the watermark to text by selecting a token $x_n$ that maximizes the score which depends on the probability $Pr(x_n|x_{0:n-1})$ and a pseudo-random value. The pseudo-"}, {"title": "", "content": "random value is defined as:\n$r_i = f(x_{n-H:n-1},i),$ (3)\nwhere $r_i \\in [0,1]$ and H presents the length of prior tokens. To detect watermarks, a p-value which is define as $\\Pi_{i=1}^{l} r$ is also needed. Here r is calculated by the token of output text on Equation (3), and L is the token sequence length of output text. We finally test whether it exceeds the threshold in the setting to implement the detection.\n*   Inverse token sampling (Inverse) [34] is similar to exponential. This strategy calculate the watermark scores from the next position of the watermark key $\\xi$, $\\xi = {\\xi^{(1)},\\xi^{(2)},...,\\xi^{(m)}}$ where $\\xi^{(i)} \\in [0,1]||$ and m should exceed the max length of generated text, V is the vocabulary of tokens. Finally, the watermark strategy is defined as:\n$f(p,x,\\xi) = onehot(arg \\max_i(len(x))^{\\xi_i}/p_i)$ (4)\nwhere x is the generated token string and p is the probability distribution of the next token. At the detection process, we perform a test statistic which is defined as:\n$\\phi(x,\\xi) = \\sum_{t=1}^{len(x)}(-\\log(1-\\xi_t))$ (5)\nwhere t represents the token position in x. This watermark is supposed to enhance the robustness compared to the Exponential. It also computes a minimum Levenshtein distance using the test statistic as a cost and compares it to the expected distribution under the null.\nLogits modification-based watermark. The prior watermarking schemes primarily address the challenge of achieving effective watermark injection and detection without adversely impacting the distribution of the model's output. Building upon the aforementioned thesis, most watermarking schemes concentrate solely on modifying the watermark sampling strategy, often overlooking the influence of logits. Kirchenbauer et al. [29] propose a novel approach by introducing a bias to the logits, thereby altering the distribution of model output while maintaining output quality. In this paper, we delve into two methods falling within this category which are proposed by Kirchenbauer et al. [29] and Zhao et al. [79].\n*   KGW. Kirchenbauer et al. [29] propose a watermarking scheme without retraining the model. Specifically, during the inference process, a secret key can be calculated by the previous token and we can use the secret key to partition the vocabulary into a green list and a red list. Then, we add a bias to each green list logit before the Softmax function to calculate the probability distribution. We can detect the watermark by making a statistical test to the proportion of tokens in the text that belongs to green list. Finally, they statistical measure the z-score which is defined as:\n$z. = \\frac{s_G-yL}{\\sqrt{L(\\gamma)(1-\\gamma)}}$ (6)\nwhere $s_G$ presents the number of green tokens, L presents the total number of tokens in the text and $\\gamma$ is a configuration parameter that presents the proportion of green list tokens in the whole vocabulary. We consider the text is watermarked if its z-score exceeds the pre-defind threshold.\n*   Unigram. Zhao et al. [79] introduce several enhancements to enhance the robustness of watermarks against removal attacks. Notably, they employ a fixed 1 prefix token for secret key computation, resulting in a consistent green list rather than a random one."}, {"title": "3.2 Post-text Watermarks", "content": "Post-text watermarks constitute a category of schemes wherein watermarks are appended to existing texts. The predominant approach for this type of watermark involves the modification of the existing text to incorporate the watermarking information. We implement three watermark schemes belonging to this category in this paper, and we divided the three watermarks into two categories which are Format-based watermark and Lexical-based watermark according to the granularity of modifications.\nFormat-based watermark. The Format-based watermark derives from a image watermark technology [5]. It does not modify the content of text but changes the format of text that are imperceptible for human, thereby implement the watermark injection.\n*   WHITEMARK [64] exploits the fact that the Unicode of text has several codepoints for whitespace. They can replaces the origin whitespace (e.g. U+0020) to another new whitespace (e.g. U+2004) without degrading the quality of text. The watermark can be detected by calculating the probability of new whitespace.\n*   UniSpaCh [58] propose a text-based data hiding method for Microsoft Word documents. In their method, they leverage the whitespace between words and replace it with different Unicode space characters without degrading text quality. This approach can be considered a format-based watermarking technique for text. During the detection process, they assess the number of replaced space characters and compare the results against a predefined threshold.\nLexical-based watermark. This catergory of watermarks apply word-level modifications by replacing the random sampling words with other candidate synonyms without affecting the structure of the whole sentence. This category of watermarks will be more imperceptible for human but will induce a large consumption for time and computing resources.\n*   Linguistic [77] first convert the token word into the binary bit based on an encoding function and the encoding bit-string conforms to the Bernoulli distribution where the probability of bit 1 is 0.5. Then, they selectively replace the words that represent bit 0 with the synonymous words that represent bit 1 to inject the watermark. The watermark can be detected by a statistical test."}, {"title": "4 Watermark Removal Attacks", "content": "We discuss twelve watermark removal attacks in this paper, in which two attacks belong to pre-text attacks, and the other ten attacks are post-text attacks (see Table 2)."}, {"title": "4.1 Pre-text Attacks", "content": "The Pre-text Attacks focus on the text generation process by introducing perturbations(e.g. emojis) prior to the generation of the text this category of attacks can only be applied to the Pre-text watermarks. In addition, the distill attack will distill the target model without any effects on the generated text. Hence, we also view this attack as a Pre-text Attack in our work.\n*   Emoji attacks. Kirchenbauer et al. [29] propose an attack that aims to indiscriminate the generative capability of LLMs. We can prompt the LLM to modify its output"}, {"title": "4.2 Post-text Attacks", "content": "Piet et al. [57] implement a benchmark that evaluates the watermark capabilities of large language models. There are several perturbations to the prompt in their code which are proposed in helm [38] and we select the perturbations that have no effect on the meaning of the text.\n*   Contraction attack. The attack contracts verbs in the text (e.g. contract is not to isn't).\n*   Expansion attack. In contrast to the contraction attack, this attack will expand some verb (e.g. expand don't to do not).\n*   Lowercase attack. This attack converts all letters to lowercase.\n*   Misspelling attack. This attack misspells certain words with the probability p.\n*   Typo attack. This attack converts certain words to typos with the probability p.\nMeanwhile, we also discuss the following post-text removal attacks:\n*   Modify attack. For each word in the text, we can duplicate, remove or replace the word to another word with probability p.\n*   Synonym attack. This attack aims to replace the word with another semantically equivalent word with probability p. Specifically, we follow the method which is implemented by Piet et al. [57]. We use WordNET to zero-prompt Llama-2 but not GPT-3.5 to generate alternative synonyms.\n*   Paraphrase attack. This is an attack that exploits another (paraphrase) language model to rephrase the existing texts. Because of the requirement of accessing"}, {"title": "5 Experiment Settings", "content": ""}, {"title": "5.1 Basic Setup", "content": "Target model. We use Llama-2-7B-chat [71] as our target model to implement all watermarking schemes. Note that during inference, we use the below chat template to feed prompts. The max output sequence length is set to 1,024 tokens.\n<s> [INST] <<SYS>>{{system_prompt}}<\u00ab/SYS>\nQuestion: {question}[/INST]\nAnswer:\nWatermark generation dataset. All the watermarking schemes discussed in this paper leverage the same dataset, i.e., watermark generation dataset (DGen), to inject and detect watermark information. For instance, we use the dataset\u00b9 which proposed by [57] as our DGen. DGen contains 296 instructions, covering three long text generation tasks (book report, story generation, and fake news)."}, {"title": "5.2 Evaluation Metrics", "content": "As Table 4 shows, there are a total of three metrics used in our paper, that is, Quality Score, Watermark Rate, and Robustness Score.\nQuality score. We use the quality score proposed by [57] to evaluate the impact of removal attacks on the quality of AI-generated text, i.e., to quantify the change in text quality before and after the attack. Following [7, 8, 22, 32, 42, 57, 73], we leverage LLM to evaluate the quality of each AI-generated text. For instance, given a question $x_i$ from DGen, we first feed it into the target model to generate its response, then we feed them together with an evaluating prompt into Llama-3-8B-instruct [4], one of the most advancing open-source model, to get a quality grade $q_i \\in [0,1]$. If the responses do not suffer attacks, we calculate the mean quality score of the whole dataset as $Q^{clean} = (\\sum q_i)/n$, n is $|DGen|$. Similarly, we use $Q^{attack}$ to denote the mean quality score of the attacked responses. As a result, the final quality score could be defined as:\n$Q = 0.5 \\cdot \\frac{Q^{attack}}{Q^{clean}} + 0.5 \\cdot (\\max(0, \\min(\\frac{Q^{attack}}{Q^{clean}}, 1))).$ (7)"}, {"title": "5.3 Experimental Setup", "content": "This section provides all the hyperparameters and other experiments setup for all watermark and attack methods.\nWatermark setup. We summarize the setup of all eight watermark schemes here. For the Logits modification-based watermarks, in the experiments of KGW, we use the setting that has the best performance, which is $(\\gamma,\\delta) = (0.25,2)$, where $\\gamma$ is the proportion of green list tokens on the whole vocabulary, $\\delta$ is the bias we added to the logits of every token during the inference process. In Unigram, we use the same setting as KGW which also achieves the best performance in our experiments.\nFor the Token sampling-based watermark schemes, in Convert and Exponential, the number of hashed keys is set to 4. In Inverse, the number of hashed keys is set to 4 and the number of shifts is set to 2.\nFor the Format-based watermark, in WHITEMARK, the replaced whitespace is \\u2004 and the probability of replacement is set to 0.6. In UniSpaCh watermark, the several specific whitespace we used are { \\u2000, \\u2001, \\u2004, \\u2006, \\u2007, \\u2008, \\u2009, \\u200A }. The probability of replacement is set to 0.6.\nFor the Lexcial-based watermark, we use the default setting that the similarity threshold is set to 0.5 and the number of candidate synonyms is set to 8 for every word.\nNote that the watermark detection threshold is set to 0.95 for all watermark schemes.\nAttack based on LLMs setup. The attack methods in our paper have been divided into two categories which are attack based on LLMs and attack based on existing text. For the former category, we implement two attack methods including Emoji attack [18] and distill attack [19]. For the latter category, we implement nine attack methods including contraction, lowercase, expansion, misspelling, typo which are proposed by Liang et al. [38], modify, synonym which are proposed by Piet et al. [57], paraphrase [33] and translation [13].\nThe former five attack methods have different experiment setups and we will introduce them respectively. We implement the latter nine attack methods with the same setup. These attack methods will be applied to the model outputs where the watermark has been injected into.\nEmoji attack. During the inference phase, we execute this assault. For the query dataset, we append an attack prompt to each query, specifically instructing, \u201cAdditionally, please add two emojis after each output word.\" Subsequently, all emojis present in the model's output will be eliminated. This attack is capable of targeting the watermark while minimizing any potential degradation to the textual quality.\nDistill attack. In this attack, we need to train a student model on the dataset which is distilled from the target teacher model. Both the teacher and student models are Llama-2-7B-chat. Different from the typical distill methods, this attack will teach the student model to match the next token distribution outputted by the teacher model when using wa-"}, {"title": "6 Evaluation Results", "content": "In this section, we first evaluate the robustness of the watermarks against different individual attacks, along with performing extensive experiments to assess the quality and watermark rate against the combined attack (i.e., with two or more different attack strategies combined together), from which we identify the optimal attack combination. This combined attack was then subjected to further discussion and experimental analysis. After that, we evaluate the efficiency and imperceptibility of different watermarks."}, {"title": "6.1 Robustness Against Individual Attacks", "content": "We first evaluate the robustness of different watermarking schemes against individual watermark removal attacks, including pre-text attacks and post-text attacks.\nRobustness score analysis. We first launch individual attacks against watermarking schemes and report the mean values of the robustness scores (Equation (9)). As Table 5 shows, we observe that pre-text watermarks are more robust than post-text watermarks in general. For instance, KGW and Exponential reach 0.5220 and 0.5012 robustness scores, respectively, while WHITEMARK and UniSpaCh only reach 0.3986 and 0.4209 robustness scores. This is expected as the pre-text watermarks usually involve more complex strategies to inject the watermark into the whole text while post-text watermarks only replace a few tokens, which makes them more vulnerable to attacks. However, the robustness score falls significantly short of the benchmark established for the ideal watermark (which should have a 1.0 robustness score), indicating a need for further optimization and improvement.\nQuality & Watermark rate. Here we dive deeper into the quality and watermark rate performance of each watermark against different attacks. The results are summarized in Figure 2. Regarding the quality, we find that KGW, Convert, Inverse, Exponential, and UniSpaCh preserve the quality to a large extent after different attacks, e.g., around 0.8. However, for most of the watermarks, the watermark rate drops drastically to less than 0.6, and most of them are near 0.0, which means the watermark cannot be extracted after the attack. This further emphasizes the vulnerability of the watermarks on texts under potential adversarial attacks.\nImpact of different attack methods. From now on, focus on KGW to discuss the attacking effectiveness as KGW reaches the best performance in our previous evaluation. We first discover that Typo, Token, Translation and Emoji attacks are more effective than others. For Typo, the attack has a significant impact on the model's output texts. For Emoji and Token attacks, while these two attacks might only induce minor disruptions in the token generation process, they could potentially impact the distribution of the entire output token list, leading to a significant decrease in the watermark detection of KGW. For Translation and Paraphrase attacks, both forms of attack entirely alter the output text of the model, while preserving the semantic integrity of the said text. Hence, there is a great effect on the output text token which results in a reduction in the watermark rate. Meanwhile, Figure 3i demonstrates that the distillation attack is ineffective, as the distilled model retains a watermark rate exceeding 0.65. Moreover, this watermark rate increases with the length of token sequences. This is due to the fact that the distillation approach employed for the model does not inflict any damage on the model's output.\nImpact of attacking hyperparameters. We then evaluate the watermark robustness with different hyperparameter settings for each attack. The results are shown in Figure 3. We observe that, regarding different hyperparameter settings, the watermark quality is similar but the watermark rate may differ. The empirical evidence gathered suggests that even slight alterations in these parameters can significantly impact"}, {"title": "6.2 Robustness Against Combined Attacks", "content": "To more effectively illustrate the vulnerability of machine-generated text watermarks in real-world scenarios, we advocate for the implementation of combined attack strategies. The optimal watermarks for pre-text (KGW) and post-text (UniSpaCh) are chosen as the target watermark algorithm in this experiment and the results are shown in Figure 4 and Figure 5, respectively. From each sub-figure, the rows represent the first attack and the columns represent the second attack.\nResults overview. In general, KGW maintains a higher quality and watermark rate than UniSpaCh. For instance, when applying Paraphrase first and Modify later, KGW reaches 0.5726 quality while UniSpaCh only has 0.2542 quality. Meanwhile, it is evident that the columns and rows representing Paraphrase and Translation attacks exhibit superior quality. When Paraphrase and Translation are utilized as the secondary attack, it has been observed that this approach can even lead to an enhancement in the quality of the text. However, the watermark rates are all reduced regardless of the attack order and attack category.\nQuality results analysis. Recall that the quality score reflects whether the generated text can align with our real requirements and if the text remains consistent with the query's answer. Hence, the text quality will be higher when the text is more informative and has fewer word errors. When a generated text has been attacked by schemes such as Modify and Typo attacks, more grammatical errors will be generated in the attacked text, thereby causing quality degradation. However, we can regard the Paraphrase and Translation attack as text correction tools that can be used to remove grammatical errors in the text. This property of both attacks leads to an increase in text quality after the combined attack.\nWatermark rate results analysis. Regarding the watermark rate, there is a great gap between the two watermarks we considered. KGW has a higher watermark rate and most of the results range from 0.4 to 0.7. Nevertheless, the watermark rate drops to 0 when the text is attacked by Paraphrase or Translation attacks. On the other hand, UniSpaCh's performance is poor and all results are close to 0. We observe that the watermark rates are higher after the Paraphrase and Translation attack which converse with the results of KGW.\nKGW vs UniSpaCh. Through comprehensive analysis of experimental results, we conclude that different watermark schemes exhibit distinct efficacy in withstanding combined attacks. This is due to the inconsistency in the watermark verification methods. For instance, the verification method of KGW calculates a secret key to divide the token's vocabulary into the green list and the red list and make a statistical analysis of the token percentage that belongs to the two lists. Therefore, Paraphrase attack and translation attacks both disrupt the token statistics by making significant changes to the text, rendering the watermark undetectable. However, both two attacks cannot completely break the specific signals which is a watermark detection mark of UniSpaCh, which induces a slightly higher watermark rate (around 0.01)."}, {"title": "6.3 Efficiency", "content": "Besides robustness, efficiency is also an important factor in evaluating the performance of watermarking schemes and removal attacks. To this end, we further measure the runtime of watermarking schemes (including both the injection and detection process) and removal attacks by using the whole watermark generation dataset DGen. Our implementation environment is on one NVIDIA A800 GPU.\nWatermark efficiency. We summarize the runtime evaluation results of watermarking schemes in Table 6. We could get the following observations: (1) For the pre-text watermarking schemes, we notice that the watermark injection runtimes are roughly around 0.5 to 1 hour and the detection runtimes are all less than 600 seconds. Taking into account the duration of text generation, the time allocated to watermarking is comparatively minimal. This observation underscores that the operational efficiency of the watermark LLM falls within a tolerable threshold, making it available for real-world scenarios. (2) For the post-text watermarks, we find that there is a great rum-time gap among them. For instance, the experiment results of WHITEMARK and UniSpaCh are both less than 150 seconds. However, the Linguistic watermark is extremely slow with an injection/detection speed of 31.52/56.34 hours. This empirical evidence underscores the significance of the efficiency of injection and detection for watermarking in the overall evaluation of the watermark's performance. Therefore, we can conclude that even when a watermark exhibits high robustness if it is characterized by low efficiency, its applicability in real-world scenarios is limited. This highlights the need to balance both efficiency and robustness in the watermark design.\nAttack efficiency. We summarize the runtime for different watermarking removal attacks with different hyperparameters in Figure 6. We observe that, while Paraphrase and Translation attacks demonstrate high effectiveness (Fig-"}, {"title": "6.4 Imperceptibility", "content": "Evaluation methodology. As pointed out in [46, 57], an ideal watermarking scheme should exhibit good imperceptibility. In other words, imperceptibility measures how well the watermarking information is hidden such that the watermarking scheme doesn't affect the readability, coherence, or naturalness of the AI-generated texts. To quantify imperceptibility, we leverage LLM to judge if a text is an AI-generated text that contains watermark and use the classification accuracy as the imperceptibility results. As the texts to be judged are all watermarked texts, consequently, a lower imperceptibility value indicates that LLM cannot recognize the hidden watermark information. Here we regard Llama-3-8B-instruct [4] as the classifier and use the following prompt to classify the texts.\n</begin_of_text |></start_header_id|>system</start_header_id|>\nYou are a assistant model to help me to judge if the given text have a watermark and is generated by machine, and please output your judgement 'True' or 'False' without any other content after it.\n### Text: {text}\nuser<|end_header_id |>\nPlease provide the judge result.\nassistant</start_header_id|>\nEvaluation results. We evaluate the imperceptibility of each watermarking scheme and present the results in Figure 7. We observe that pre-text watermarks achieve better imperceptibility (lower value is better). This is because the pre-text watermarking schemes just modify the logits or the tokensampling strategy during the inference process, thus, there is no obvious watermark signal left in the text that the judgment LLM can capture. In contrast, all three post-text watermarking schemes need to embed signals into the watermarked text, thereby causing a worse imperceptibility. For instance,"}, {"title": "7 Related Work", "content": ""}, {"title": "7.1 Machine-Generated Text Watermark", "content": "Watermark of machine-generated text has been```json\n{\n  \"title\": \"On Evaluating The Performance of Watermarked Machine-Generated Texts Under Adversarial Attacks\"", "authors": ["Zesen Liu", "Tianshuo Cong", "Xinlei He", "Qi Li"], "abstract": "Large Language Models (LLMs) excel in various applications, including text generation and complex tasks. However, the misuse of LLMs raises concerns about the authenticity and ethical implications of the content they produce, such as deepfake news, academic fraud, and copyright infringement. Watermarking techniques, which embed identifiable markers in machine-generated text, offer a promising solution to these issues by allowing for content verification and origin tracing. Unfortunately, the robustness of current LLM watermarking schemes under potential watermark removal attacks has not been comprehensively explored.\nIn this paper, to fill this gap, we first systematically comb the mainstream watermarking schemes and removal attacks on machine-generated texts, and then we categorize them into pre-text (before text generation) and post-text (after text generation) classes so that we can conduct diversified analyses. In our experiments, we evaluate eight watermarks (five pre-text, three post-text) and twelve attacks (two pre-text, ten post-text) across 87 scenarios. Evaluation results indicate that (1) KGW and Exponential watermarks offer high text quality and watermark retention but remain vulnerable to most attacks; (2) Post-text attacks are found to be more efficient and practical than pre-text attacks; (3) Pre-text watermarks are generally more imperceptible, as they do not alter text fluency, unlike post-text watermarks; (4) Additionally, combined attack methods can significantly increase effectiveness, highlighting the need for more robust watermarking solutions. Our study underscores the vulnerabilities of current techniques and the necessity for developing more resilient schemes.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are rapidly advancing in capability, demonstrating remarkable proficiency across a wide range of applications. From generating coherent and contextually appropriate text to assisting in complex tasks such as code generation [6, 21, 44, 74, 75], medical diagnosis [14, 26, 65], and content creation [17, 39, 54], LLMs are revolutionizing various domains. Their integration into these fields highlights the transformative potential of artificial intelligence, providing enhanced productivity and innovative solutions to longstanding challenges.\nDespite their powerful capabilities, the rise of LLMs has also sparked significant concerns regarding the authenticity and ethical implications of the content they generate. Issues such as the creation of deepfake texts [50, 59, 72], automated homework and programming assignments [62], and the spread of misinformation [55, 80] pose serious risks to the integrity of information and trust in digital communications. The ability of LLMs to produce highly realistic and human-like text exacerbates these concerns, making it increasingly difficult to distinguish between human and machine-generated content.\nTo address these challenges, watermark techniques [28, 52] have emerged as promising solutions. By embedding identifiable markers within the machine-generated text, these techniques aim to provide a reliable means of tracing the origin of the text and verifying its authenticity. This approach offers a potential safeguard against the misuse of LLMs, helping to preserve the credibility of information and enhance accountability in content creation.\nHowever, the robustness of these watermarking schemes remains questionable. Adversaries may develop methods to circumvent or remove watermarks, undermining their effectiveness and potentially rendering them unreliable. The resilience of watermarking techniques against various forms of manipulation and obfuscation is critical to their success and necessitates rigorous evaluation to ensure their practicality in real-world scenarios.\nOur work. In this paper, we fill the gap in the literature by systematically categorizing watermarks and attacks into pre-text and post-text classes, where the former involves watermark injection or disruption before text generation, and the latter does so after text generation. We consider eight watermarks (five pre-text and three post-text) and twelve attacks (two pre-text and ten post-text), resulting in a total of 87 possible scenarios (see Table 3). Our evaluation shows that KGW [29] and Exponential [1] are the two best watermarks by providing good text quality and relatively high watermark rates after different attacks compared to other watermarks. For instance, KGW and Exponential achieve over"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Large Language Models (LLMs)", "content": "In recent years, Large Language Models (LLMs) have achieved remarkable success in the field of Natural Language Processing (NLP). These models, trained on extensive datasets, have demonstrated unparalleled language understanding and generation capabilities. They have revolutionized various applications, from conversational agents like OpenAI's ChatGPT and GPT-4 to advanced systems such as Google's Gemini. LLMs have not only excelled in traditional language tasks but have also proven instrumental in solving complex, real-world problems, greatly enhancing human productivity and interaction with technology.\nFollowing [53], we model the generation process of LLMs into two steps: probability prediction and token selection. First, given a sequence of tokens $x_{1:n}$ (e.g., the input prompt) where $x_i \\in {1,..., |V|}$ is a token and $|V|$ is the size of vocabulary V, LLM calculates the conditional probability of the next token as $Pr(x_{n+1}|x_{1:n})$. Then, LLM uses a specific sampling strategy to select the word from the vocabulary as the final token, e.g., Greedy Sampling confirms next token by $x_{n+1} = arg \\max_x Pr(x|x_{1:n})$. Naturally, LLM generates a l length text through computing $Pr(x_{1:n+1}) = \\Pi_{i=1}^{n+1} Pr(x_i|x_{1:i-1})$."}, {"title": "2.2 Preliminary of Watermarks", "content": "We first classify the watermarking schemes from different perspectives. For the access needed to the model's internal parameters, we classify schemes into black-box and white-box. Meanwhile, as Figure 1 shows, according to the watermarking scheme with respect to the time involved in the text generation process, we classify them into pre-text schemes and post-text schemes. Note that the same classification rules can be applied to watermark removal attacks, that is, we will discuss white- and black-box removal attacks, as well as pretext and post-text attacks. Nevertheless, regardless of the classification, an LLM watermarking scheme typically contains two processes: watermark injection and watermark detection.\nBlack-box & White-box. A watermarking scheme is classified as white-box if it relies on the model's internal parameters during watermark injection. Conversely, a scheme is black-box if it does not access model parameters. Notably, schemes accessing the last hidden state of an LLM are also considered as black-box.\nPre-text & Post-text. We further categorize a watermarking scheme based on the time it is conducted, labeling it as either pre-text or post-text. If it is conducted before or during the text generation phase, we classify it as a pre-text watermarking scheme. Conversely, we regard it as a post-text watermarking scheme.\nWatermark injection. During the watermark injection process, a multitude of distinct scheme-related hyperparameters are necessitated: 1) The pre-text watermarking schemes [1, 9, 24, 29, 34, 35, 40, 43, 45, 79] usually requires a secret key and a pseudo-random number generator; 2) For the post-text watermarking schemes [48, 58, 64, 77, 78], only a watermark signal (e.g. the specific whitespace \\u2004 or \\u2006) is needed. In this paper, given a watermarking scheme, we denote all hyperparameters it used as $k$. Therefore, given input x, the watermark injection process can be defined as:\n$y = Scheme_{inj}(x, \\M, k),$ (1)\nwhere $M$ is the target model that the scheme aims to protect and y is the watermarked text.\nWatermark detection. As the inverse process of the injection process, the detection process is defined as:\n${Ture, False} = Scheme_{dec}(y, k;M),$ (2)\nwhere True stands for that we can extract the watermark information from model M by using the same k."}, {"title": "3 Watermarks", "content": "First of all, we clarify that we focus on the watermarking schemes which are based on the generated texts in this paper. Compared with the watermarking schemes which need to embed the watermarks by updating the model parameters [36, 56], these text-based schemes can be adapted to any causal language models. Meanwhile, we consider two types of watermarks, i.e., pre-text watermarks and post-text watermarks, where the former injects the watermark before or during text generation and the later injects the watermark after the text generation.\nThe methods of pre-text watermark can be divided into two categories based on the process used for generating text with an LLM [41]. For instance, this includes modifying the logits [24, 29, 35, 40, 43, 45, 79] and modifying the token sampling strategy during the inference phase [1, 9, 34]. The methods of post-text watermark can be divided into four categories based on the granularity of text modification [41], which are format-based watermark [58, 64], lexcial-based watermark [51, 77], synatic-based watermark [48, 70], and generation-based watermark [2, 78]. To comprehensively discuss the robustness of various types of watermarking schemes, we discuss eight LLM watermarking schemes in this paper, which are summarized in Table 1."}, {"title": "3.1 Pre-text Watermarks", "content": "Pre-text watermarks can be categorized into token sampling-based and logits modification-based watermarks where the former modifies the token sampling strategy and the later modifies logits during the inference phase.\nToken sampling-based watermark. Watermark injection during token sampling is influenced by the randomness factor of the token sampling strategy. The watermark injection phase utilizes a random number generated by a fixed seed or a random seed calculated based on prefix tokens to guide the token sampling procedure and generate watermarked texts. During the watermark detection, alignment between output text tokens and the random number suffices for comparison. In this paper, we explore three token sampling strategies: bitstring conversion [9], exponential [1], and inverse token sampling [34].\n*   Convert bit-string (Convert) [9] aims to develop undetectable watermarks. This watermark can be viewed as the hash operation on the generated text. Specifically, they first encode the generated text into a string with binary-bits. Then, they use a {0,1} hash function h and sample tokens $x_j$ with preference for those satisfying $h(x_j) = 1$. More tokens should hash to 1 than to 0 in watermarked text. The probability of a token being encoded to 1 is defined as $p_j(1)$. The watermarked model will output $x_j = 1$ if $u_j \\le p_j(1)$ and $x_j = 0$ otherwise. Here $u_j$ is a real number sampled from [0,1]. The probability that the watermarked model output $x_j = 1$ is $p_j(1)$. For detection, they will compute a score $s(x_j,u_j) = \\ln \\frac{u_j}{1-u_j}$ if $x_j = 1$ and $s(x_j, u_j) = \\ln \\frac{1-u_j}{u_j}$ otherwise. Given a string $(x_1,...,x_l)$, the sum scores of detection is defined as $score = \\sum_{j=1}^{l} s(x_j,u_j)$. The score of the watermarked text should be significantly higher than the score of the normal text.\n*   Exponential sampling (Exponential) [1] injects the watermark to text by selecting a token $x_n$ that maximizes the score which depends on the probability $Pr(x_n|x_{0:n-1})$ and a pseudo-random value. The pseudo-"}, {"title": null, "content": "random value is defined as:\n$r_i = f(x_{n-H:n-1},i),$ (3)\nwhere $r_i \\in [0,1]$ and H presents the length of prior tokens. To detect watermarks, a p-value which is define as $\\Pi_{i=1}^{l} r$ is also needed. Here r is calculated by the token of output text on Equation (3), and L is the token sequence length of output text. We finally test whether it exceeds the threshold in the setting to implement the detection.\n*   Inverse token sampling (Inverse) [34] is similar to exponential. This strategy calculate the watermark scores from the next position of the watermark key $\\xi$, $\\xi = {\\xi^{(1)},\\xi^{(2)},...,\\xi^{(m)}}$ where $\\xi^{(i)} \\in [0,1]||$ and m should exceed the max length of generated text, V is the vocabulary of tokens. Finally, the watermark strategy is defined as:\n$f(p,x,\\xi) = onehot(arg \\max_i(len(x))^{\\xi_i}/p_i)$ (4)\nwhere x is the generated token string and p is the probability distribution of the next token. At the detection process, we perform a test statistic which is defined as:\n$\\phi(x,\\xi) = \\sum_{t=1}^{len(x)}(-\\log(1-\\xi_t))$ (5)\nwhere t represents the token position in x. This watermark is supposed to enhance the robustness compared to the Exponential. It also computes a minimum Levenshtein distance using the test statistic as a cost and compares it to the expected distribution under the null.\nLogits modification-based watermark. The prior watermarking schemes primarily address the challenge of achieving effective watermark injection and detection without adversely impacting the distribution of the model's output. Building upon the aforementioned thesis, most watermarking schemes concentrate solely on modifying the watermark sampling strategy, often overlooking the influence of logits. Kirchenbauer et al. [29] propose a novel approach by introducing a bias to the logits, thereby altering the distribution of model output while maintaining output quality. In this paper, we delve into two methods falling within this category which are proposed by Kirchenbauer et al. [29] and Zhao et al. [79].\n*   KGW. Kirchenbauer et al. [29] propose a watermarking scheme without retraining the model. Specifically, during the inference process, a secret key can be calculated by the previous token and we can use the secret key to partition the vocabulary into a green list and a red list. Then, we add a bias to each green list logit before the Softmax function to calculate the probability distribution. We can detect the watermark by making a statistical test to the proportion of tokens in the text that belongs to green list. Finally, they statistical measure the z-score which is defined as:\n$z. = \\frac{s_G-yL}{\\sqrt{L(\\gamma)(1-\\gamma)}}$ (6)\nwhere $s_G$ presents the number of green tokens, L presents the total number of tokens in the text and $\\gamma$ is a configuration parameter that presents the proportion of green list tokens in the whole vocabulary. We consider the text is watermarked if its z-score exceeds the pre-defind threshold.\n*   Unigram. Zhao et al. [79] introduce several enhancements to enhance the robustness of watermarks against removal attacks. Notably, they employ a fixed 1 prefix token for secret key computation, resulting in a consistent green list rather than a random one."}, {"title": "3.2 Post-text Watermarks", "content": "Post-text watermarks constitute a category of schemes wherein watermarks are appended to existing texts. The predominant approach for this type of watermark involves the modification of the existing text to incorporate the watermarking information. We implement three watermark schemes belonging to this category in this paper, and we divided the three watermarks into two categories which are Format-based watermark and Lexical-based watermark according to the granularity of modifications.\nFormat-based watermark. The Format-based watermark derives from a image watermark technology [5]. It does not modify the content of text but changes the format of text that are imperceptible for human, thereby implement the watermark injection.\n*   WHITEMARK [64] exploits the fact that the Unicode of text has several codepoints for whitespace. They can replaces the origin whitespace (e.g. U+0020) to another new whitespace (e.g. U+2004) without degrading the quality of text. The watermark can be detected by calculating the probability of new whitespace.\n*   UniSpaCh [58] propose a text-based data hiding method for Microsoft Word documents. In their method, they leverage the whitespace between words and replace it with different Unicode space characters without degrading text quality. This approach can be considered a format-based watermarking technique for text. During the detection process, they assess the number of replaced space characters and compare the results against a predefined threshold.\nLexical-based watermark. This catergory of watermarks apply word-level modifications by replacing the random sampling words with other candidate synonyms without affecting the structure of the whole sentence. This category of watermarks will be more imperceptible for human but will induce a large consumption for time and computing resources.\n*   Linguistic [77] first convert the token word into the binary bit based on an encoding function and the encoding bit-string conforms to the Bernoulli distribution where the probability of bit 1 is 0.5. Then, they selectively replace the words that represent bit 0 with the synonymous words that represent bit 1 to inject the watermark. The watermark can be detected by a statistical test."}, {"title": "4 Watermark Removal Attacks", "content": "We discuss twelve watermark removal attacks in this paper, in which two attacks belong to pre-text attacks, and the other ten attacks are post-text attacks (see Table 2)."}, {"title": "4.1 Pre-text Attacks", "content": "The Pre-text Attacks focus on the text generation process by introducing perturbations(e.g. emojis) prior to the generation of the text this category of attacks can only be applied to the Pre-text watermarks. In addition, the distill attack will distill the target model without any effects on the generated text. Hence, we also view this attack as a Pre-text Attack in our work.\n*   Emoji attacks. Kirchenbauer et al. [29] propose an attack that aims to indiscriminate the generative capability of LLMs. We can prompt the LLM to modify its output"}, {"title": "4.2 Post-text Attacks", "content": "Piet et al. [57] implement a benchmark that evaluates the watermark capabilities of large language models. There are several perturbations to the prompt in their code which are proposed in helm [38] and we select the perturbations that have no effect on the meaning of the text.\n*   Contraction attack. The attack contracts verbs in the text (e.g. contract is not to isn't).\n*   Expansion attack. In contrast to the contraction attack, this attack will expand some verb (e.g. expand don't to do not).\n*   Lowercase attack. This attack converts all letters to lowercase.\n*   Misspelling attack. This attack misspells certain words with the probability p.\n*   Typo attack. This attack converts certain words to typos with the probability p.\nMeanwhile, we also discuss the following post-text removal attacks:\n*   Modify attack. For each word in the text, we can duplicate, remove or replace the word to another word with probability p.\n*   Synonym attack. This attack aims to replace the word with another semantically equivalent word with probability p. Specifically, we follow the method which is implemented by Piet et al. [57]. We use WordNET to zero-prompt Llama-2 but not GPT-3.5 to generate alternative synonyms.\n*   Paraphrase attack. This is an attack that exploits another (paraphrase) language model to rephrase the existing texts. Because of the requirement of accessing"}, {"title": "5 Experiment Settings", "content": ""}, {"title": "5.1 Basic Setup", "content": "Target model. We use Llama-2-7B-chat [71] as our target model to implement all watermarking schemes. Note that during inference, we use the below chat template to feed prompts. The max output sequence length is set to 1,024 tokens.\n<s> [INST] <<SYS>>{{system_prompt}}<\u00ab/SYS>\nQuestion: {question}[/INST]\nAnswer:\nWatermark generation dataset. All the watermarking schemes discussed in this paper leverage the same dataset, i.e., watermark generation dataset (DGen), to inject and detect watermark information. For instance, we use the dataset\u00b9 which proposed by [57] as our DGen. DGen contains 296 instructions, covering three long text generation tasks (book report, story generation, and fake news)."}, {"title": "5.2 Evaluation Metrics", "content": "As Table 4 shows, there are a total of three metrics used in our paper, that is, Quality Score, Watermark Rate, and Robustness Score.\nQuality score. We use the quality score proposed by [57] to evaluate the impact of removal attacks on the quality of AI-generated text, i.e., to quantify the change in text quality before and after the attack. Following [7, 8, 22, 32, 42, 57, 73], we leverage LLM to evaluate the quality of each AI-generated text. For instance, given a question $x_i$ from DGen, we first feed it into the target model to generate its response, then we feed them together with an evaluating prompt into Llama-3-8B-instruct [4], one of the most advancing open-source model, to get a quality grade $q_i \\in [0,1]$. If the responses do not suffer attacks, we calculate the mean quality score of the whole dataset as $Q^{clean} = (\\sum q_i)/n$, n is $|DGen|$. Similarly, we use $Q^{attack}$ to denote the mean quality score of the attacked responses. As a result, the final quality score could be defined as:\n$Q = 0.5 \\cdot \\frac{Q^{attack}}{Q^{clean}} + 0.5 \\cdot (\\max(0, \\min(\\frac{Q^{attack}}{Q^{clean}}, 1))).$ (7)"}, {"title": "5.3 Experimental Setup", "content": "This section provides all the hyperparameters and other experiments setup for all watermark and attack methods.\nWatermark setup. We summarize the setup of all eight watermark schemes here. For the Logits modification-based watermarks, in the experiments of KGW, we use the setting that has the best performance, which is $(\\gamma,\\delta) = (0.25,2)$, where $\\gamma$ is the proportion of green list tokens on the whole vocabulary, $\\delta$ is the bias we added to the logits of every token during the inference process. In Unigram, we use the same setting as KGW which also achieves the best performance in our experiments.\nFor the Token sampling-based watermark schemes, in Convert and Exponential, the number of hashed keys is set to 4. In Inverse, the number of hashed keys is set to 4 and the number of shifts is set to 2.\nFor the Format-based watermark, in WHITEMARK, the replaced whitespace is \\u2004 and the probability of replacement is set to 0.6. In UniSpaCh watermark, the several specific whitespace we used are { \\u2000, \\u2001, \\u2004, \\u2006, \\u2007, \\u2008, \\u2009, \\u200A }. The probability of replacement is set to 0.6.\nFor the Lexcial-based watermark, we use the default setting that the similarity threshold is set to 0.5 and the number of candidate synonyms is set to 8 for every word.\nNote that the watermark detection threshold is set to 0.95 for all watermark schemes.\nAttack based on LLMs setup. The attack methods in our paper have been divided into two categories which are attack based on LLMs and attack based on existing text. For the former category, we implement two attack methods including Emoji attack [18] and distill attack [19]. For the latter category, we implement nine attack methods including contraction, lowercase, expansion, misspelling, typo which are proposed by Liang et al. [38], modify, synonym which are proposed by Piet et al. [57], paraphrase [33] and translation [13].\nThe former five attack methods have different experiment setups and we will introduce them respectively. We implement the latter nine attack methods with the same setup. These attack methods will be applied to the model outputs where the watermark has been injected into.\nEmoji attack. During the inference phase, we execute this assault. For the query dataset, we append an attack prompt to each query, specifically instructing, \u201cAdditionally, please add two emojis after each output word.\" Subsequently, all emojis present in the model's output will be eliminated. This attack is capable of targeting the watermark while minimizing any potential degradation to the textual quality.\nDistill attack. In this attack, we need to train a student model on the dataset which is distilled from the target teacher model. Both the teacher and student models are Llama-2-7B-chat. Different from the typical distill methods, this attack will teach the student model to match the next token distribution outputted by the teacher model when using wa-"}, {"title": "6 Evaluation Results", "content": "In this section, we first evaluate the robustness of the watermarks against different individual attacks, along with performing extensive experiments to assess the quality and watermark rate against the combined attack (i.e., with two or more different attack strategies combined together), from which we identify the optimal attack combination. This combined attack was then subjected to further discussion and experimental analysis. After that, we evaluate the efficiency and imperceptibility of different watermarks."}, {"title": "6.1 Robustness Against Individual Attacks", "content": "We first evaluate the robustness of different watermarking schemes against individual watermark removal attacks, including pre-text attacks and post-text attacks.\nRobustness score analysis. We first launch individual attacks against watermarking schemes and report the mean values of the robustness scores (Equation (9)). As Table 5 shows, we observe that pre-text watermarks are more robust than post-text watermarks in general. For instance, KGW and Exponential reach 0.5220 and 0.5012 robustness scores, respectively, while WHITEMARK and UniSpaCh only reach 0.3986 and 0.4209 robustness scores. This is expected as the pre-text watermarks usually involve more complex strategies to inject the watermark into the whole text while post-text watermarks only replace a few tokens, which makes them more vulnerable to attacks. However, the robustness score falls significantly short of the benchmark established for the ideal watermark (which should have a 1.0 robustness score), indicating a need for further optimization and improvement.\nQuality & Watermark rate. Here we dive deeper into the quality and watermark rate performance of each watermark against different attacks. The results are summarized in Figure 2. Regarding the quality, we find that KGW, Convert, Inverse, Exponential, and UniSpaCh preserve the quality to a large extent after different attacks, e.g., around 0.8. However, for most of the watermarks, the watermark rate drops drastically to less than 0.6, and most of them are near 0.0, which means the watermark cannot be extracted after the attack. This further emphasizes the vulnerability of the watermarks on texts under potential adversarial attacks.\nImpact of different attack methods. From now on, focus on KGW to discuss the attacking effectiveness as KGW reaches the best performance in our previous evaluation. We first discover that Typo, Token, Translation and Emoji attacks are more effective than others. For Typo, the attack has a significant impact on the model's output texts. For Emoji and Token attacks, while these two attacks might only induce minor disruptions in the token generation process, they could potentially impact the distribution of the entire output token list, leading to a significant decrease in the watermark detection of KGW. For Translation and Paraphrase attacks, both forms of attack entirely alter the output text of the model, while preserving the semantic integrity of the said text. Hence, there is a great effect on the output text token which results in a reduction in the watermark rate. Meanwhile, Figure 3i demonstrates that the distillation attack is ineffective, as the distilled model retains a watermark rate exceeding 0.65. Moreover, this watermark rate increases with the length of token sequences. This is due to the fact that the distillation approach employed for the model does not inflict any damage on the model's output.\nImpact of attacking hyperparameters. We then evaluate the watermark robustness with different hyperparameter settings for each attack. The results are shown in Figure 3. We observe that, regarding different hyperparameter settings, the watermark quality is similar but the watermark rate may differ. The empirical evidence gathered suggests that even slight alterations in these parameters can significantly impact"}, {"title": "6.2 Robustness Against Combined Attacks", "content": "To more effectively illustrate the vulnerability of machine-generated text watermarks in real-world scenarios, we advocate for the implementation of combined attack strategies. The optimal watermarks for pre-text (KGW) and post-text (UniSpaCh) are chosen as the target watermark algorithm in this experiment and the results are shown in Figure 4 and Figure 5, respectively. From each sub-figure, the rows represent the first attack and the columns represent the second attack.\nResults overview. In general, KGW maintains a higher quality and watermark rate than UniSpaCh. For instance, when applying Paraphrase first and Modify later, KGW reaches 0.5726 quality while UniSpaCh only has 0.2542 quality. Meanwhile, it is evident that the columns and rows representing Paraphrase and Translation attacks exhibit superior quality. When Paraphrase and Translation are utilized as the secondary attack, it has been observed that this approach can even lead to an enhancement in the quality of the text. However, the watermark rates are all reduced regardless of the attack order and attack category.\nQuality results analysis. Recall that the quality score reflects whether the generated text can align with our real requirements and if the text remains consistent with the query's answer. Hence, the text quality will be higher when the text is more informative and has fewer word errors. When a generated text has been attacked by schemes such as Modify and Typo attacks, more grammatical errors will be generated in the attacked text, thereby causing quality degradation. However, we can regard the Paraphrase and Translation attack as text correction tools that can be used to remove grammatical errors in the text. This property of both attacks leads to an increase in text quality after the combined attack.\nWatermark rate results analysis. Regarding the watermark rate, there is a great gap between the two watermarks we considered. KGW has a higher watermark rate and most of the results range from 0.4 to 0.7. Nevertheless, the watermark rate drops to 0 when the text is attacked by Paraphrase or Translation attacks. On the other hand, UniSpaCh's performance is poor and all results are close to 0. We observe that the watermark rates are higher after the Paraphrase and Translation attack which converse with the results of KGW.\nKGW vs UniSpaCh. Through comprehensive analysis of experimental results, we conclude that different watermark schemes exhibit distinct efficacy in withstanding combined attacks. This is due to the inconsistency in the watermark verification methods. For instance, the verification method of KGW calculates a secret key to divide the token's vocabulary into the green list and the red list and make a statistical analysis of the token percentage that belongs to the two lists. Therefore, Paraphrase attack and translation attacks both disrupt the token statistics by making significant changes to the text, rendering the watermark undetectable. However, both two attacks cannot completely break the specific signals which is a watermark detection mark of UniSpaCh, which induces a slightly higher watermark rate (around 0.01)."}, {"title": "6.3 Efficiency", "content": "Besides robustness, efficiency is also an important factor in evaluating the performance of watermarking schemes and removal attacks. To this end, we further measure the runtime of watermarking schemes (including both the injection and detection process) and removal attacks by using the whole watermark generation dataset DGen. Our implementation environment is on one NVIDIA A800 GPU.\nWatermark efficiency. We summarize the runtime evaluation results of watermarking schemes in Table 6. We could get the following observations: (1) For the pre-text watermarking schemes, we notice that the watermark injection runtimes are roughly around 0.5 to 1 hour and the detection runtimes are all less than 600 seconds. Taking into account the duration of text generation, the time allocated to watermarking is comparatively minimal. This observation underscores that the operational efficiency of the watermark LLM falls within a tolerable threshold, making it available for real-world scenarios. (2) For the post-text watermarks, we find that there is a great rum-time gap among them. For instance, the experiment results of WHITEMARK and UniSpaCh are both less than 150 seconds. However, the Linguistic watermark is extremely slow with an injection/detection speed of 31.52/56.34 hours. This empirical evidence underscores the significance of the efficiency of injection and detection for watermarking in the overall evaluation of the watermark's performance. Therefore, we can conclude that even when a watermark exhibits high robustness if it is characterized by low efficiency, its applicability in real-world scenarios is limited. This highlights the need to balance both efficiency and robustness in the watermark design.\nAttack efficiency. We summarize the runtime for different watermarking removal attacks with different hyperparameters in Figure 6. We observe that, while Paraphrase and Translation attacks demonstrate high effectiveness (Fig-"}, {"title": "6.4 Imperceptibility", "content": "Evaluation methodology. As pointed out in [46, 57], an ideal watermarking scheme should exhibit good imperceptibility. In other words, imperceptibility measures how well the watermarking information is hidden such that the watermarking scheme doesn't affect the readability, coherence, or naturalness of the AI-generated texts. To quantify imperceptibility, we leverage LLM to judge if a text is an AI-generated text that contains watermark and use the classification accuracy as the imperceptibility results. As the texts to be judged are all watermarked texts, consequently, a lower imperceptibility value indicates that LLM cannot recognize the hidden watermark information. Here we regard Llama-3-8B-instruct [4] as the classifier and use the following prompt to classify the texts.\n<|begin_of_text |></start_header_id|>system</start_header_id|>\nYou are a assistant model to help me to judge if the given text have a watermark and is generated by machine, and please output your judgement 'True' or 'False' without any other content after it.\n### Text: {text}\nuser<|end_header_id |>\nPlease provide the judge result.\n### Result:\nassistant</start_header_id|>\nEvaluation results. We evaluate the imperceptibility of each watermarking scheme and present the results in Figure 7. We observe that pre-text watermarks achieve better imperceptibility (lower value is better). This is because the pre-text watermarking schemes just modify the logits or the tokensampling strategy during the inference process, thus, there is no obvious watermark signal left in the text that the judgment LLM can capture. In contrast, all three post-text watermarking schemes need to embed signals into the watermarked text, thereby causing a worse imperceptibility. For instance,"}, {"title": "7 Related Work", "content": ""}, {"title": "7.1 Machine-Generated Text Watermark", "content": "Watermark of"}]}]}