{"title": "GenAI Assisting Medical Training", "authors": ["Stefan Gerd Fritsch", "Matthias Tsch\u00f6pe", "Vitor Fortes Rey", "Lars Krupp", "Agnes Gr\u00fcnerbl", "Eloise Monger", "Sarah Trevenna"], "abstract": "Medical procedures such as venipuncture and cannulation are essential for nurses and require precise skills. Learning this skill, in turn, is a challenge for educators due to the number of teachers per class and the complexity of the task. The study aims to help students with skill acquisition and alleviate the educator's workload by integrating generative AI methods to provide real-time feedback on medical procedures such as venipuncture and cannulation.", "sections": [{"title": "INTRODUCTION", "content": "Performing venipunctures (the procedure of taking a blood sample from a patient) [1] and cannulations (the procedure of inserting a cannula for infusions and intravenous medication) [2] is an essential part of a nurse's work. As this is an invasive and non-trivial act, it is essential for nurses training to acquire this skill. In the nurse training program, students are trained using a simulation training arm. These tasks require fine motor skills and involve multiple steps to prevent infection and reduce pain, bruising, and complications. Providing feedback for these procedures is challenging as expert guidance can be variable and there is often a lack of accuracy in the complex steps and precision in the angle, smoothness, and speed of insertion. The main challenge in training such medical procedures, despite the availability of training arms, is the teachers lack the time for 101 training. Most of the time such training classes comprise 15-20 students with one teacher, who often is busy helping one or two students, while the rest try to figure it out themselves. This is not optimal for such an invasive medical procedure."}, {"title": "1.1 Idea and Objectives", "content": "In several discussions with nurse trainers, the idea arose to combine nurse training with generative Al methods to build a live feedback system for nurse students. Using data from training videos, sensor recordings, and an appropriate language model (LLM), our goal is to offer personalized feedback to nursing trainees and improve their learning experience. While the existing model operates by analyzing recorded sessions, the ambition is to transition to live feedback. This would mean that trainees and instructors receive insights and guidance live, during actual training sessions."}, {"title": "2 DATASET", "content": "During a three-day period from 27 to 29 November 2023, the data collection took place at the University of Southampton. The focus was on two procedures: Cannulation and Venipuncture, with a total of 20 people taking part. This included 13 students with varying levels of experience and 7 experts. Each participant performed both procedures twice, resulting in a total of 80 recordings.\nThe collected dataset includes several components:\n\u2022 Static cameras: three static cameras (high-resolution video (1920x1080) at 60 Hz), positioned at strategic locations around the bed (foot, head, and side).\n\u2022 GoPro camera: capture user view.\n\u2022 Audio recordings: to ensure synchronized audio-visual data.\n\u2022 IMU data: Two Apple smartwatches worn by participants on their left and right wrists provided IMU (Inertial Mea- surement Unit) data, which was used to capture the nuances of hand movements during the procedures.\n\u2022 Feedback forms: An expert observer filled in professional feedback forms about the performance of the study participants while participants performed the procedures. Participants filled in a form regarding their demographics"}, {"title": "3 DATA PREPROCESSING", "content": ""}, {"title": "3.1 Synchronization:", "content": "To ensure the synchronization of all cameras, including three static cameras and a GoPro, we had each participant clap three times before starting their procedure. The sound peaks of the clapping were used as reference points for synchronizing the video footage. The data recorded with the smartwatch will be synchronized and added at a later step."}, {"title": "3.2 Labeling:", "content": "Following the official cannulation and venipuncture protocols, we developed detailed labeling instructions. These instructions outline specific, easily identifiable points that mark the beginning and end of each action. We then labeled the videos as accurately as possible (on a frame level), i.e., the start and end times of each action/procedure step. For example:\n\u2022 Step: Apply tourniquet to arm\n\u2022 Start Action: Pick up tourniquet || touch tourniquet\n\u2022 End Action: Stop touching tourniquet"}, {"title": "3.3 Transcription:", "content": "The audio feedback provided by the observers was transcribed into text similar to [3], using Nuance Dragon 15 Professional for the automatic transcription, but was manually corrected afterwards if needed. This transcription is important as it is later used as input for the Al model or Large Language Model (LLM) to generate feedback for the participants."}, {"title": "4 FIRST STEPS", "content": ""}, {"title": "4.1 Video Classification", "content": "In the first step, we are currently focusing on the development of methods for video classification, in particular the recognition of the steps performed in the videos concerning the specifications from the medical protocols. This involves the precise identification of the individual actions performed during the procedures."}, {"title": "4.2 Developing a Method for Providing Feedback", "content": "In the first steps, methods for providing feedback will also be devel- oped. One approach is to use a Large Language Model (LLM), which provides contextual information about how the procedure should be performed together with the results of the video classification process, to generate feedback for the user. The first method, which was tested with videos (video only, later smartwatch data should also be included), comprises several steps:\n(1) Splitting the dataset: We split the dataset into training, validation, and test sets at the subject level to ensure subject independence.\n(2) Data sampling: We apply an overlapping sliding window technique to the videos to extract data samples for training the models.\n(3) Video Classification: We are working on fine-tuning video classification models, such as S3D [4], to predict the labels corresponding to the steps of the protocol.\n(4) LLM integration: A medical LLM should be integrated that is stimulated with procedural context information and feedback from observers as examples, as well as the results of video classification. This will allow the model to provide feedback on whether the procedure has been performed correctly, including checking the order of steps, identifying missing steps, and ensuring sufficient waiting time after cleaning the skin.\nBy integrating these components, we hope to create a robust system capable of providing accurate and helpful feedback to users based on their performance of medical procedures."}, {"title": "5 OUTLOOK AND NEXT STEPS:", "content": "In the next steps, we plan to improve our system by incorporating smartwatch data as an additional input source. The aim is for the system to not only recognize the sequence of steps but also the nuances in the execution of each step, such as the angle of the needle insertion. We currently lack the means to accurately measure these details. However, we have the necessary data, including feedback from observers and forms, with which we can train models that can assess the quality of each step."}, {"title": "6 DISCUSSION AND CONCLUSION", "content": "Several important conclusions have emerged from our study so far. Correct labeling of medical procedures in videos is a challenging task, even for humans, as understanding medical procedures is necessary. This raises the question of whether and to what extent Generative AI could help."}]}