{"title": "Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models", "authors": ["Zhiqi Li", "Guo Chen", "Shilong Liu", "Shihao Wang", "Yilin Zhao", "Subhashree Radhakrishnan", "Nadine Chang", "Matthieu Le", "De-An Huang", "Ilia Karmanov", "Lukas Voegtle", "Jose M. Alvarez", "Bryan Catanzaro", "Jan Kautz", "Andrew Tao", "Vibashan VS", "Yishen Ji", "Shiyi Lan", "Hao Zhang", "Karan Sapra", "Amala Deshmukh", "Tuomas Rintamaki", "Philipp Fischer", "Timo Roman", "Tong Lu", "Guilin Liu", "Zhiding Yu"], "abstract": "Recently, promising progress has been made by open-source vision-language models (VLMs) in bringing\ntheir capabilities closer to those of proprietary frontier models. However, most open-source models only publish their\nfinal model weights, leaving the critical details of data strategies and implementation largely opaque. In this work,\nwe address VLM post-training from a data-centric perspective, showing the key role of data strategy in developing\nfrontier VLMs. By studying and building our post-training data strategy from scratch, we share detailed insights into the\ndevelopment processes, aiming to benefit the development of competitive models for the open-source community. Our\nintroduced data strategy, together with training recipes and model design, leads to a family of performant VLMs named\nEagle 2. Specifically, Eagle2-9B achieves state-of-the-art results across various multimodal benchmarks, matching\ncertain competitive models with up to 70B parameters.", "sections": [{"title": "1. Introduction", "content": "Built upon large language models (LLMs), vision-\nlanguage models (VLMs) [1, 2, 3, 4] aim to enable LLMs\nto see. With the ability to visually perceive the world,\nVLMs are able to take in multimodal information, and\nas a result, handle a broader range of intelligent applica-\ntions. There is thus a growing interest to use VLMs as the\nbackbone for reasoning and decision making in various\napplications, such as intelligent agents [5], autonomous\ndriving [6, 7], and embodied AI [8, 9, 10].\n\nThe community has delved deeply into the architec-\nture and training methodologies of VLMs with signifi-\ncant advances. A predominant strategy to align the vision\nand language modalities is through post-training on pre-\ntrained LLMs, with the LLaVA family [4] being the rep-\nresentative examples. Based on the level of transparency,\ncurrent VLM models can also be broadly categorized\ninto three types: 1) commercially closed-source models\n(e.g., GPT-4v/o [11] and Claude [12]), 2) frontier mod-\nels with publicly available weights (e.g., Qwen2-VL [13],\nInternVL2 [14] and Llama 3.1 [15]), and 3) fully open-\nsource models (e.g., Cambrian-1 [16] and the LLaVA fam-\nily [4, 17]).\n\nRecently, some frontier models with publicly available\nweights have been shown to match closed-source com-\nmercial models on key benchmarks while offering better\ncustomization for downstream applications. However, the\ntechnical details provided by these models are often insuf-\nficient for reproduction. On the other hand, fully open-\nsource models tend to disclose extensive technical details,\nincluding both the dataset strategies and training recipes.\nWe thus ask the following\nquestion: What could help the community to develop more\ncompetitive open-source frontier VLMs?"}, {"title": "1.1. Data Strategy", "content": "Our answer to the above question is the data-strategy. As-\nsuming the same pre-trained LLM backbone, we posit\nthat data is the most decisive factor to obtain high-quality\nmodels. We thus adopt a centralized strategy to build our\npost-training data. For fully open-source models, various\nconstraints such as computing resources may limit the\nstudy on more dataset sources, despite their intention to\nmake the data recipe publicly available. This limitation\noften affects their capability compared to models that can\naccess and utilize a wider range of data sources.\n\n\"Diversity first, then quality\". We follow this princi-\nple throughout our development and push it to the extreme.\nOur optimization of the data results in consistent improve-\nments in model. Our contributions involve: 1) a data\ncollection strategy leading to a large-scale highly diverse\ndata pool with 180+ sources, 2) a data filtering strategy\nto remove low-quality samples, 3) a data selection strat-\negy to construct high-quality subsets, and 4) a series of\ndata augmentation techniques to enrich the existing data.\nThis series of strategies are shown to improve the model\nsignificantly."}, {"title": "1.2. Model Architecture", "content": "Besides data strategy, another important factor considered\nin this work is model architecture. Since different architec-\nture designs have been well-studied in open-source models,\ntheir properties are relatively transparent to the commu-\nnity. Despite various designs such as Q-Former [19] and\nHybrid-Attention [20], simple MLP connector is still the\nmost popular choice to connect the vision encoder and\nLLM. With the rapid advances in architecture designs in\ncontemporary VLMs, model structure is no longer a pri-\nmary factor driving performance differences among mod-\nels. However, this does not imply that there is no room for\nfurther improvement in architecture.\n\nTiled mixture of vision encoders. Inspired by the\nworks of InternVL [21, 14], Eagle [22] and Cambrian-\n1 [16], we follow a vision-centric design where we\nadopt both dynamic tiling and mixture of vision encoders\n(MoVE) in one unified design. Specifically, each image\ntile is encoded by channel-concatenated MoVE, therefore\nallowing high-resolution input from tiling while maintain-\ning the robust perception from MoVE. Similar to [22], we\nfollow a \"SigLIP [23] + X (ConvNeXt [24])\" configuration.\nCompared to SigLIP alone, tiled MoVE yields significant\nimprovements despite having tiling, particularly in tasks\nlike OCR and Chart/Document VQA."}, {"title": "1.3. Training Recipe", "content": "Which training recipe to be used? In this context, the train-\ning recipe primarily refers to various configurations for\ntraining a VLM. With the same dataset, different recipes\ncan still have a significant impact on the final perfor-\nmance. Although the training recipes for the state-of-the-\nart VLMs [25, 14, 26] are somewhat unclear, the details\nshared by existing work [16, 4, 17] can offer a solid base-\nline. However, to further improve model performance, it\nis necessary to explore more effective training recipe.\n\nThree-stage training. We adopt a three-stage training\nstrategy to best leverage the training data. In particular,\nthe first stage (Stage-1) is used to align language and\nimage modality via training the MLP connector. The next\nstage (Stage-1.5) trains the full model with a large-scale\ndiverse data. The final stage (Stage-2) continues training\nthe full model with a carefully crafted, high-quality visual\ninstruction tuning dataset. In Stage-1.5, we incorporate all\navailable visual instruction data, rather than limiting it to\ncaptioning or knowledge data alone. Our results show that\nthis approach yields substantial improvements over the\ncommonly used two-stage training strategy [4]. We also\nidentify limitations in existing open-source frameworks\nconcerning data packing and introduce a balanced data\npacking approach to address the issue."}, {"title": "1.4. Summary", "content": "Our extensive exploration on data strategy, model archi-\ntecture and training recipe is shown in Fig. 2, resulting\nin a family of VLMs named Eagle 2. Through sharing"}, {"title": "2. Method", "content": "As shown in Tab. 1, our initial baseline starts with the Cam-\nbrian dataset [16] using LLaVA's [4] two-stage training\nrecipe. We remove some low-quality data from Cambrian-\n7M, such as ShareGPT-4V, GPT-77K and Data-Engine-\n161K, ultimately resulting in a subset of 5.2M samples.\nThe model incorporates an MLP connector to bridge the\nvision encoder with the LLM and employs image tiling\nfor dynamic resolution. Starting from this baseline, we\nenhance Eagle 2 in three key aspects: (1) data strategy,\n(2) training recipe, and (3) model architecture. These op-\ntimizations enable the model to achieve state-of-the-art\nperformance."}, {"title": "2.1. Baseline Setting", "content": "Training data is essential for defining a VLM's capabili-\nties. However, most commercial VLMs and leading VLMs\nwith publicly available weights keep their data strategies\nconfidential. In this work, we conducted an in-depth ex-\nploration to create a diverse and high-quality dataset with\na series of data strategies to iteratively refine and opti-\nmize our data pool. The resulting dataset significantly\nboosts model performance, far surpassing the initial base-\nline. Fig. 3 illustrates our overall data strategy consisting\nof two core components: data collection and optimizing\nexisting data. More technical details have been provided\nin the appendix."}, {"title": "2.2. Data Strategy", "content": "Data collection - diversity is the key. A model's ca-\npability is strongly correlated with the diversity of data.\nAs such, collecting data as diverse as possible is a key\nprinciple of this work, leading to two main strategies:\n\n\u2022 Passive gathering: Monitoring the latest related datasets\nfrom arXiv manuscripts and HuggingFace Datasets and\nadding them into our candidate list.\n\u2022 Proactive searching: Addressing the bucket effect. As\nshown in Fig. 3, for each update of the data pool, we\ngenerate error analysis to identify model weaknesses\nand perform targeted searches for new data.\n\nOur diverse data sources are summarized in Tab. 2a\nand generally publicly available. We utilize some pre-\norganized dataset collections [16, 17, 151] to speed up\npreparation but also conducted careful inspection to pre-\nvent issues like test data leakage. We also collect a large\namount of public non-QA data, such as Google Land-\nmark [31], and convert them into VQA data using specific\nrules or auto-labeling tools.\n\nTo reduce training costs, we avoid performing ablation\nfor each dataset individually. Instead, datasets with sim-\nilar domains are added in batches to the data pool when\nmeeting the following criteria:\n\n\u2022 Maintaining overall accuracy without noticeable regres-\nsion for every considered benchmark.\n\u2022 Introducing meaningful diversity to the current domains.\n\nTo help quantify the diversity, we define a metric called\nSimilarity Score to measure the relevance between a new\ndata source and the current data pool as follows:\n\n$S_k = \\frac{1}{N} \\sum_{i=1}^{N} \\max_{1<j<M_k}(Sim(I_i, I_j) \\times Sim(T_i, T_j))$, (1)\n\nwhere i is the index of a new data source with N samples,\nand j is the index of the existing pool with M samples,\nwith k denoting the data category. We compute similarity\nscores only within the same category, as inter-category\nsimilarity is generally low. Image embeddings $I_i$ and $I_j$\nare generated from SSCD [178], and text embeddings $T_i$\nand $T_j$ from all-mpnet-base-v2 [179]. The similarity score"}, {"title": "2.3. Training Recipe", "content": "Our data strategy enables us to build a high-quality and\ndiverse dataset, but applying different training recipes to\nthe same data pool still has a decisive impact on the final\nresults. Our recipe is built upon the following core points.\n\nPost-pretraining stage is necessary. We initially begin\nwith LLaVA [4]'s two-stage training strategy, where we\ntrain an MLP connector followed by full model training\nwith SFT data. While efficient, this approach proved un-\nsuitable for quick SFT data updates, as the expanding SFT\ndata makes it harder to track the impact of new data and\nreduces the experimental efficiency. For instance, we ob-\nserve improvements from expanding the Cambrian-1 [16]\nSFT data. However, the gap remains between the model\nand state-of-the-art ones. Considering that the main lim-\nitation of the two-stage strategy is the lack of robust pre-\ntraining, we add an additional pre-training stage (Stage-\n1.5). Stage-1.5 pre-trains the model on a larger dataset to\nreduce dependency on SFT data in subsequent training.\nThree-stage pre-\ntraining is, in fact, widely used in existing works, such as\nLLaVA-OneVision [17]. However, we have a distinctly\ndifferent view to the data that using in Stage-1.5. Other\nworks tend to use more knowledge-related data, such as\ncaptioning data, at this stage. In this work, we add all data\nsources intended for visual instruction to Stage-1.5, simul-\ntaneously introducing several other datasets as shown in\nTab. 2b. As shown in Fig. 8, training Stage-2 based on\nStage-1.5 enables rapid iteration on a high-performance\nfoundation. The derived conclusions are more robust than\nthose obtained from ungeneralizable ablation experiments\non toy-scale data. In addition, the effective conclusions\nobtained from Stage-2 can be used to update Stage-1.5,\nfurther driving improvements in model performance. De-"}, {"title": "2.4. Tiled Mixture of Vision Encoders", "content": "Following Eagle [22], we use SigLIP [23] and ConvNeXt-\nXXLarge [24, 183] as vision encoders. Additionally, to\nhandle arbitrarily high-resolution images, we employ im-\nage tiling following InternVL-1.5 [21]. The input resolu-\ntion of every image tile of SigLIP is 448 \u00d7 448, while the\ninput size of ConvNeXt is 512\u00d7512. To make sure they\noutput same number of image tokens, we use PixelShuf-\nfle to conduct a 2\u00d7 downsampling on the image features\nfrom SigLIP, resulting a feature shape of 16\u00d716, matching\nthe output size of ConvNeXt (32\u00d7 downsampling of in-\nput). We then concatenate these features along the channel\ndimension and align with LLM via an MLP layer."}, {"title": "3. Experiments", "content": "We initially explore\nthe impact of scaling Stage-2 data, as shown in Tab. 5.\nOur findings reveal that model's overall performance im-\nproved steadily with additional data, with the most no-\ntable gains arising from the inclusion of 2M (million)\nVQA samples focused on charts, tables, and OCR. While\ndata scaling indicates potential for further gains beyond\n10M samples, our experiments' costs have risen sharply,\nand the efficiency of data iteration has decreased. More-\nover, we observe considerable performance fluctuations\nacross specific benchmarks at this scale, especially in\nchallenging benchmarks like MMMU, MathVista, and\nMMVet. Another obstacle is that, as illustrated by the\ndata-performance growth trend in Fig.2, reaching the per-\nformance of frontier VLMs like Qwen2-VL would be\ndifficult. These challenges leads us to consider adopting a\nmore effective training strategy.\nTo build a robust pre-trained\nmodel, we implement Stage-1.5 where we focus on maxi-\nmizing the data utilization to strengthen the model's foun-\ndational capabilities. As shown in Tab. 6, the Stage-1.5\ncheckpoint is competitive by itself, and subsequent Stage-2\ntraining further improves the previous best model's perfor-\nmance by average 3.9%.\nUsing a naive data selection strat-\negy with maximum thresholds and random sampling, we\nreduce the training data to 8.6M; unfortunately, this led to\na decline in performance. We speculate it might be that\nthe randomly selected data have inadvertently excluded\nsome valuable samples, while also failing to adequately\nensure a balanced data distribution.\nAfter filtering low-quality\ndata and formatting the training set, we see clear improve-\nments on 8 out of 14 benchmarks, including a remarkable\n45-point gain on OCRBench [180]. This implies the im-"}, {"title": "3.1. Evolution of Eagle 2", "content": "Data augmentation. By employing our data augmen-\ntation strategy, we introduce a greater volume of auto-\nmatically generated CoT training data, leading to notable\nperformance improvements on MMMU and MathVista.\nThe rule-based data augmentation on the chart data also\nbrings 1 point improvement on ChartQA.\n\nRe-updating stage-1.5. The effective data strategies we\nexplored in Stage-2, such as data filtering, formatting,\nand augmentation, can be applied to update the Stage-\n1.5 data, thereby further enhancing the model's ultimate\ncapability. By updating Stage-1.5 checkpoint, we obtain\nclear improvement on ChartQA, MMVet and MathVista.\nIntroducing mixture of vi-\nsion encoder has brought performance improvements on\n12 out of 14 benchmarks, particularly in benchmarks re-\nlated to documents, charts, and OCR. This clearly demon-\nstrates that the mixture of vision encoders significantly\nenhances the model's understanding to visual spaces."}, {"title": "3.2. Comparison with SOTA Models", "content": "As shown in Tab. 7, we conducted comparisons across 14\ndiverse benchmarks with the representative state-of-the-art\npublic avaiable and closed-source models. Our Eagle2-\n9B, building on top of Qwen2.5-7B [27], outperforms\nInternVL2-8B [14] and MiniCPM-v2.6 [186] across all\n14 benchmarks and leads Qwen2-VL-7B [25] in 9 out of\nthe 14 benchmarks and beats it on OpenCompass. Eagle2-\n9B holds its ground against much larger VLMs such as\nInternVL2-26B, LLaVa-OneVision-72B [17] and LLaMa-\n3.2-90B-Vision [15]. Apart from MMVet and MMMU, we\ncomprehensively surpass GPT-4V. Eagle2-9B surpasses\nGPT-4O [184] on ChartQA, OCRBench, and MathVista,\nwhile achieving performance very close to GPT-4O on\nDocVQA, MMStar, AI2D and OpenCompass."}, {"title": "4. Related Work", "content": "Vision-Language Models (VLMs) LLMs [187, 188, 3]\nhave transformed natural language processing (NLP)\nand reshaped the broader AI landscape. The advance-\nment of LLMs has spurred significant progress in vi-\nsual understanding by integrating visual features with\nLLMs, leading to the emergence of Visual-Language\nModels(VLMs) [189, 11, 4, 190]. The performance of\nVLMs with public available weights [4, 3, 191, 192, 193,\n17, 194, 14, 25, 195, 15, 196, 197, 198, 199] contin-\nues to make breakthroughs, reaching or even surpass-\ning the most advanced commercial models such as GPT-\n4V/4O [184, 11] and Gemini-1.5 [185]. Fully open-source\nVLMs [17, 16, 200] have released their training data and\ncode base, further accelerating the VLM research.\n\nVision-Centric VLMs. Our work adopts a vision-centric\nVLM design that emphasizes strong vision foundation\nand HD input. This is aligned with the spirit of vari-\nous related areas, including: 1) Vision foundation for\nVLMs [201, 202, 203] and improved designs [204, 23,\n195, 205, 206, 207], 2) Mixture of vision encoder de-\nsigns [208, 209, 210, 211, 212, 16, 22], and 3) Tiling\nand HD input designs [213, 214, 215, 216, 21, 193, 194,\n217, 14, 25]. \u03a4o our best knowledge, this work is the first\nto explore the tiled mixture of vision encoder (MOVE)\ndesign, which is shown to inherit the benefits from both\nworlds. The proposed tiled MoVE design also introduces\nadditional flexibility to incorporate advanced vision foun-\ndation models.\n\nData Efforts in VLMs. Data strategy is crucial in train-\ning VLMs, encompassing aspects of data set construc-\ntion, balance and filtering, and training methodologies.\nEarly endeavors such as LLaVA-150K [4] used instructed\ntuning with GPT-4 [11], which was later enriched by\nsuccessors [218, 219, 193, 204] incorporating academic\ntraining data from various tasks into the supervised fine-\ntuning stage. Studies also broadened data types to include\nvideo [220, 15], multi-image inputs [17, 14], image-text\ninterleaved data [221, 222], multilingual data [186], and\nsynthetic datasets [15]. However, simply expanding data\nsets can compromise model performance due to varying\nquality and size. Approaches like Instruct-BLIP [219]\nand Cambrian-1 [16] addressed this by devising optimal\ndata ratios and balancing techniques, while others like\nLlama3 [15] and Molmo [223] focused on enhancing data\nquality by removing duplicates with SSCD [178] and incor-\nporating human-annotated images, respectively. In addi-\ntion, Training strategies have also evolved, with LLaVA [4]\nproposing a two-stage training process that has become a\nstandard, and later models [17] introducing intermediate\nstages. VLM surveys [224, 225, 226] also discuss various\ntraining recipes and data strategies for building VLMs,\nhowever, they lack qualitative analysis and do not provide\na detailed enough path for training cutting-edge VLMs."}, {"title": "5. Conclusion", "content": "As publicly available frontier VLMs continue to approach\nor even surpass proprietary commercial models, the de-\ntailed data strategies of these leading VLMs remains un-\nknown to the community. In this paper, we have unveiled\nmany details on the post-training data strategy for training\nfrontier VLMs. Our covered data strategy is effective and\ncomprehensive. We hope this work offers a transparent\npractice to inspire the community."}, {"title": "6. Demos", "content": "This section provides some examples to demonstrate Ea-\ngle2 capabilities. To avoid cherry-picking, we directly\nselect demo cases from other works (Qwen2-VL and In-"}]}