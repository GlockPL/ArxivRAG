{"title": "IMLE Policy: Fast and Sample Efficient Visuomotor Policy Learning via Implicit Maximum Likelihood Estimation", "authors": ["Krishan Rana", "Robert Lee", "David Pershouse", "Niko S\u00fcnderhauf"], "abstract": "Recent advances in imitation learning, particularly using generative modelling techniques like diffusion, have enabled policies to capture complex multi-modal action distributions. However, these methods often require large datasets and multiple inference steps for action generation, posing challenges in robotics where the cost for data collection is high and computation resources are limited. To address this, we introduce IMLE Policy, a novel behaviour cloning approach based on Implicit Maximum Likelihood Estimation (IMLE). IMLE Policy excels in low-data regimes, effectively learning from minimal demonstrations and requiring 38% less data on average to match the performance of baseline methods in learning complex multi-modal behaviours. Its simple generator-based architecture enables single-step action generation, improving inference speed by 97.3% compared to Diffusion Policy, while outperforming single-step Flow Matching. We validate our approach across diverse manipulation tasks in simulated and real-world environments, showcasing its ability to capture complex behaviours under data constraints.", "sections": [{"title": "I. INTRODUCTION", "content": "Learning policies from demonstrations is becoming a key approach for enabling robots to perform complex tasks. At its core, this problem involves mapping observations to actions in a way that captures the nuanced and often multi-modal nature of human behaviour. However, leveraging behaviour cloning for real-world robotics applications presents unique challenges including the ability to learn these highly multi-modal action distributions from limited demonstration data which can be expensive to collect, and the need for computational efficiency to enable real-time operation. Recent advancements in generative modelling, such as diffusion models [33, 9] and flow matching [19, 4], have demonstrated promise in capturing complex multi-modal action distributions for behaviour cloning. However, these approaches are computationally expensive during inference, as they require iterative sampling processes to generate actions. Additionally, they often demand large datasets to train performant models effectively. Addressing these challenges is crucial for enhancing the practicality and adoption of behaviour cloning in real-world tasks.\nGenerative Adversarial Networks (GANs) are an alternative approach to generative modelling with efficient single-step inference, but fell out of favour due to their issues with training stability and mode collapse, where parts of the data distribution are ignored, limiting their ability to effectively model complex, multi-modal distributions. To address these shortcomings, Implicit Maximum Likelihood Estimation (IMLE) [18] was introduced as an alternative training objective for single-step generative models, based on a simple objective that ensures every datapoint is well-represented by at least one generated sample. In essence, IMLE minimises the distance between each data point and its nearest generated sample, guaranteeing comprehensive mode coverage and maintaining stable training, even with smaller datasets. This makes IMLE a compelling alternative to diffusion models for developing efficient and faster behaviour cloning methods suitable for real-world robotics.\nIn this work, we propose IMLE Policy, a novel extension of Implicit Maximum Likelihood Estimation tailored for conditional behaviour cloning. We leverage a particular instantiation of IMLE that utilises rejection sampling in the training objective [35], which further improves sample efficiency while effectively enabling the model to sample all possible modes during inference. IMLE Policy offers several key advantages that make it particularly promising for real-world robotic systems:\n\u2022 Expressive multi-modal action distributions: By optimising the policy to generate samples close to every possible data point, IMLE Policy by nature does not drop modes, enabling highly expressive multi-modal action generations for a given state.\n\u2022 Sample efficiency: IMLE Policy can accurately model complex multi-modal distributions using 38% less data on average than Diffusion Policy and single-step Flow- Matching to achieve similar performance.\n\u2022 Fast inference: IMLE Policy can generate multi-modal actions using only a single forward pass a 97.3% increase in inference speed when compared to vanilla Diffusion Policy without collapsing modes when compared to single step Flow Matching.\nWe evaluate IMLE Policy across diverse simulation benchmarks and multi-modal real-world tasks, demonstrating its effectiveness as an alternative to existing generative model-based behaviour cloning approaches. IMLE Policy meets the critical desiderata for real-world robotics: sample efficiency, computational speed, and expressiveness. (1) IMLE Policy outperforms state-of-the-art baselines in low-data regimes, while matching performance when more data is available, learning performant real-world policies from as few as 17 demonstrations. (2) IMLE Policy captures multi-modal action distributions in a single step, generating diverse trajectories across varying conditions, avoiding mode collapse seen in other single-step methods. (3) IMLE Policy enables fast real-time inference, reducing latency by 97.3% compared to iterative diffusion models while maintaining competitive task success rates. We further analyse sample efficiency, multi-modal expressivity, and key design choices to understand IMLE Policy's strengths. All code, datasets, and training details will be released to ensure reproducibility and support further research."}, {"title": "II. RELATED WORK", "content": "We organise our discussion of prior work based on three key behaviour cloning desiderata for real-world robotics: capturing multi-modal behaviour, achieving sample efficiency, and enabling fast inference. While existing works address each of these aspects individually, relatively few approaches attempt to satisfy all three simultaneously.\nA. Multi-Modality in Behaviour Cloning\nBehaviour cloning has been widely explored as a means of enabling robots to learn from human demonstrations, achieving success across various manipulation tasks [41, 12, 25, 24, 39, 29, 3, 2, 1, 31, 6, 8, 7, 10]. Capturing the multi-modal nature of human demonstrations is a fundamental challenge in behaviour cloning, and generative models have been extensively applied to address this issue. Methods such as Conditional Variational Autoencoders (CVAEs) [23, 42], Energy-Based Models (EBMs) [11], and Vector Quantization [17] have been used to model multi-modal action distributions, though each comes with trade-offs such as mode collapse or complex multi-stage training.\nRecently, de-noising diffusion models have emerged as a dominant approach due to their stable training dynamics"}, {"title": "B. Sample Efficiency of Behaviour Cloning", "content": "Despite the widespread success of diffusion models in robotics, these methods typically require large datasets for effective training [43], which is a significant limitation in imitation learning, where expert demonstrations are expensive to collect. Improving the sample efficiency of behaviour cloning remains an active research area.\nCurrent research directions for improving sample efficiency involve optimising input representations. For example, 3D Diffusion Policy utilises point clouds to improve generalisation in 3D space [38], while approaches leveraging SO(2) and SIM(3) equivariances improve learning efficiency in tasks with structured transformations [36, 37]. Affordance-centric representations have also been explored to improve generalisation from limited demonstrations [30].\nOur approach is orthogonal to these representation-based methods, as it focuses on improving the core learning algorithm itself to enhance sample efficiency. This allows IMLE Policy to be complementary to existing strategies, meaning it can be combined with improved input representations for even greater efficiency gains. Additionally, we conduct a comprehensive study on sample efficiency, explicitly examining the relationship between dataset size and performance. While dataset size is often an overlooked factor in behaviour cloning research, we highlight its direct impact on policy"}, {"title": "C. Inference Speed and Behaviour Cloning", "content": "A key limitation of most state-of-the-art behaviour cloning approaches is their multi-step inference process, which requires iterative de-noising or auto-regressive steps to generate actions. This significantly increases computational cost during inference, limiting real-time applicability in robotics. Several approaches aim to improve upon this. Consistency Models distill multi-step diffusion policies into single-step policies while maintaining performance [28, 34]. Streaming Diffusion modifies the denoising process to allow earlier actions to require fewer de-noising steps, speeding up inference [15]. Flow Matching provides an alternative continuous-time generative modelling framework with more efficient probability paths, reducing inference steps [19, 21], though in practice, it still requires multiple steps to prevent mode collapse when applied to behaviour cloning [14, 4, 40].\nWhile these methods aim to speed up inference, they either require additional distillation steps or do not fully eliminate iterative sampling. In contrast, our approach natively enables single-step inference while still capturing complex, multi-modal action distributions. This allows IMLE Policy to perform fast, real-time inference without requiring multi-stage training or distillation, making it a promising alternative for computationally constrained settings."}, {"title": "III. METHOD", "content": "A. Background\nIn the context of generative modelling, the aim is to learn the probability distribution of samples p(x), which would allow us to then synthesise new samples via the trained model. We can represent the generator as a function To : Z \u2192 X that transforms samples from the latent space Z to the space of target samples X, implemented as a neural network with parameters \u03b8. Such models have been historically trained via a generative adversarial objective (as in GANs), but this approach is prone to mode collapse, where only some modes of the target distribution are modelled.\nA more recent approach, Implicit Maximum Likelihood Estimation (IMLE) has been introduced, providing an alternative training objective for the generator that avoids mode collapse [18]. The IMLE objective ensures that each training sample is well represented by the generator with samples generated nearby, alleviating the mode collapse issue.\nThe IMLE training objective [18] is written as:\n$\\mathcal{O}_{IMLE}=\\underset{\\theta}{\\operatorname{argmin}} \\mathbb{E}_{z_{1}, \\ldots, z_{m} \\sim \\mathcal{N}(0, I)}\\left[\\sum_{i=1}^{n} \\min _{j \\in[m]} d\\left(x_{i}, T_{\\theta}\\left(z_{j}\\right)\\right)\\right]$\nwhere d(\u00b7,\u00b7) is a distance metric, n is the number of data samples, and m is the number of generated samples.\nDuring training, m number of samples zj are drawn from the latent prior distribution, a standard Gaussian distribution."}, {"title": "Algorithm 1: IMLE Policy Training", "content": "Algorithm 1: IMLE Policy Training\nInput: Training dataset D = {(Oi, Ai)}=1, number\nof latents m, rejection threshold e, distance\nmetric d(,), generator \u03c0\u03b8(z,y).\nOutput: Trained policy \u03c0\u03b8.\n1 foreach (Oi, Ai) \u2208 D do\n2 Sample m latent vectors {zj}}=1 ~ N(0, I);\n3 Generate m trajectories {A;}}=1, where\n\u0391j = \u03c0\u03bf(\u0396, \u039fi);\n4 Compute distances dj = d(Ai, A\u2081) for j \u2208 [m];\n5 Filter valid trajectories V = {j \u2208 [m] : dj \u2265 \u20ac};\n6 Select nearest trajectory j* = argminjev dj;\n7 Update 0 to minimise d(Ai, Aj*);\n8 end\n9 return \u03c0\u03bf"}, {"title": "Algorithm 2: Inference with Temporal Consistency in IMLE-Policy", "content": "Algorithm 2: Inference with Temporal Consistency\nin IMLE-Policy\nInput: Policy \u03c0\u03bf, observation O, number of latents\nm, distance metric d(\u00b7,\u00b7), previously executed\ntrajectory Aprev [Ta:Tp]\u00b7\nOutput: Action sequence A[0:Tp]\u00b7\n1 Sample m latent vectors {z}_1 ~ N(0, I);\n2 Batch generate m trajectories {A}_1, where\n\u0391\u03af = \u03c0\u03bf(\u03b6\u03af, 0);\n3 Compute overlaps o\u00b2 = d(Aprev[Ta:Tp], A\u20810:7]) for\nj\u2208 [m];\n4 Select trajectory j* = argminj\u2208[m] 0;\n5 return Aj*\nThese are transformed by the generator To into synthesised samples. For each training sample, the nearest synthetic sample is selected according to the distance metric d(\u00b7,\u00b7). While this objective is effective for avoiding mode collapse, the selection procedure results in certain latent samples being rarely selected, even if they have a high likelihood under the latent prior distribution.\nRejection Sampling IMLE (RS-IMLE) [35] alleviates this issue by rejecting samples from the selection process if d(xi, To(z)) < \u20ac, meaning they are too close to the training data sample. The remaining samples are then used in the IMLE training objective as before. Intuitively, this prevents the selection process from repeatedly selecting similar samples after they have already converged to fitting the data sample with some accuracy, defined by the parameter \u20ac.\nB. Conditional RS-IMLE\nWe extend the RS-IMLE framework [35], initially developed for unconditional image generation, to the conditional setting, enabling its application to behavior cloning. In this case, our generator is augmented to include an additional conditioning variable y, and instead of the m samples shared between all data points, we now select m samples to synthesise"}, {"title": "C. IMLE Policy", "content": "With our conditional variant of RS-IMLE, we can now apply this training objective (Eq. 3) to behaviour cloning, where we formulate our policy as a generator \u03c0(z, o) \u2192 a, where our conditioning variable o takes the form of image and robot state information and a represents the generated actions. For the distance function d(,) we use Euclidean distance. We utilise the same action generation procedure as [9], generating a sequence of actions which we can utilise for closed-loop receding-horizon control. At each time step t, the policy processes the latest To steps of observation data, Ot, as input and predicts Tp steps of future actions. Out of these, Ta steps are executed on the robot before re-planning occurs. Here, To is referred to as the observation horizon, Tp as the action prediction horizon, and Ta as the action execution horizon. Although we use the same UNet architecture as Diffusion in this work, the approach is generally applicable to other architectures such as transformers. We summarise the IMLE Policy training algorithm in Algorithm 1"}, {"title": "D. Temporal Consistency", "content": "IMLE Policy effectively models multi-modal action distributions; however, since each inference step is independent, the policy may exhibit mode-switching at decision points where multiple valid behaviour exist. This can lead to oscillatory behaviour, particularly in long-horizon tasks where smooth and consistent execution is necessary. To mitigate this, we introduce a batched trajectory selection mechanism that enforces temporal consistency while maintaining the policy's"}, {"title": "IV. EVALUATION", "content": "We evaluate IMLE Policy to understand how effective it is in modelling behaviours across different datasets and environments when compared to prior state-of-the-art methods. We focus our evaluation on the core requirements for real-world robotics applications particularly: sample efficiency, multi-modal expressivity and inference speed. Concretely, we seek to answer the following questions through our experiments:\n1) How well does IMLE Policy perform on the respective benchmarks for behaviour cloning as a single-step generative model?\n2) How well does IMLE Policy perform when under data constraints?\n3) How well does IMLE Policy capture multiple modes seen in demonstrations?\n4) Does IMLE Policy scale beyond simulation environments?\n5) What hyper-parameters of IMLE Policy make the most impact on its performance?\nWe systematically evaluate IMLE Policy to answer the above questions through a series of experiments across both simulated and real-world environments. For all experiments, we evaluate the following policies:\n\u2022 IMLE Policy: Our proposed algorithm given in Algorithm 2 which leverages temporal consistency during inference.\n\u2022 IMLE Policy (w/out consistency): Our proposed algorithm without temporal consistency during inference.\n\u2022 Diffusion Policy: Vanilla Diffusion Policy proposed by Chi et al. [9] trained using DDPM and 100 denoising steps\n\u2022 Flow Matching (1-step): We modify the Diffusion Policy implementation from Chi et al. [9] to utilise the Flow Matching objective and evaluate against the 1-step setting as a comparative baseline to the 1-step performance of our method."}, {"title": "A. Simulation Environments", "content": "IMLE Policy is evaluated across 8 different simulation tasks from 4 benchmark environments [9, 26, 16, 13].\nWe categorise our sets of tasks based on the level of multi-modality exhibited by the dataset with the high multi-modality tasks exhibiting the highest variance across the demonstrations. A high-level description of each task is provided below and specific details are summarised in Table I.\n1) Push-T : We utilise the variant of this task provided by Chi et al. [9] which involves pushing a T-shaped block (gray) to a designated target position (red) using a circular end-effector (blue). Variation is introduced through randomized initial positions of the T block and the end-effector. Successfully completing the task requires leveraging complex, contact-rich object dynamics to precisely push the T block using point contacts to sufficiently overlap with the target."}, {"title": "C. Experiments", "content": "1) Dataset Size Study: We conduct a study to evaluate how well IMLE can learn under data constrains when compared to the baselines. We use the Push-T simulated task for this, dividing the full dataset provided by Chi et al. [9] in 10 subsets, starting with 10% of the data up to 100%. We train each method on these datasets for 1000 epochs across 3 seeds and report the average evaluation performance for each method in Figure 5. This experiment gives us a detailed view of how performance improves as data increases.\n2) Benchmark Evaluation: We evaluate all methods across the full datasets provided by each respective benchmarks indicated as 100% Dataset in Table II. To understand how our policy operates in the low data regime, we additionally"}, {"title": "3) Real World Validation", "content": "3) Real World Validation: We evaluate all methods on the real-world Push-T and shoe racking tasks described in IV-B. Our evaluation protocol tests both sample efficiency and overall performance by training on 17 and 35 demonstrations for each task. For both tasks, we conduct 20 evaluation trials per method and report the success rates based on the completion criteria outlined in the environment descriptions. This evaluation framework allows us to assess how effectively each method can learn from limited real-world demonstrations while handling both precise manipulation requirements in Push-T and the multi-modal decision space in the shoe racking task. Results are summarised in Figure 7\n4) Mode Capturing Ablation: In this qualitative ablation study we demonstrate how IMLE Policy can capture multiple modes even in states where certain modes appear less frequently in the dataset. We conduct this study in the Push-T simulation benchmark where we gradually sweep the location of the robot's end effector from one side of the T block's top edge to the other. The idea here is to capture points where the demonstration data exhibits a high level of multi-modality (at the centre) and less multi-modality (towards the corners) where majority of the demonstrations will bias towards one side of the T block when in one of these particular states. The key results are illustrated in Figure 2.\n5) Real World Inference Speed Evaluation: We evaluate the computational efficiency of our approach by measuring inference speed across different policy architectures for the shoe racking task. Using a standard Dell Precision 3680 i7 workstation equipped with an NVIDIA GeForce RTX 3090 GPU, we measure the average time required to generate a sequence of actions. For each method, we conduct 30 separate generations and report the average inference speed in Hertz. We summarise the results in Table III.\n6) Hyperparameter Ablation Study: Finally we evaluate how robust our algorithm is to the two key hyperparame- ters used by the IMLE Policy objective. The first parameter pertains to the rejection sampling radius, e which determines how far candidate generations have to be away from a data point to be considered in the loss computation. The second parameter pertains to the number of samples we use per condition in order to conduct our nearest-neighbour search in the objective. For each hyper-parameter we sweep over a set of values for two different tasks from our simulation benchmark (Push-T and Square) and report the maximum success rate achieved by each when trained over 1000 epochs. The results are summarised in Figure 6."}, {"title": "D. Key Findings", "content": "IMLE Policy can learn with significantly less data when compared to baselines. The dataset size study shown in Figure 5 demonstrates how IMLE Policy consistently achieves higher max rewards across all dataset percentages and a significant advantage in the lower data settings. Notably, IMLE Policy achieves a reward of 0.5 with less than 30% of the data, while Diffusion Policy requires nearly twice as much data to reach the same performance. This underscores the ability of IMLE Policy to generalise effectively with minimal data. Furthermore, as the dataset size increases, IMLE Policy continues to maintain its advantage, achieving near-optimal performance faster, while converging to comparable performance to Diffusion Policy when the full dataset is available.\nIMLE Policy can maximise the utility of only a few demonstrations, while scaling to larger datasets. As shown in Table II, across all benchmark environment tasks, IMLE Policy demonstrates a clear advantage, particularly in the challenging setting of learning from only 20 randomly selected demonstrations. Under this constrained scenario, IMLE Policy consistently outperforms all baseline methods, achieving at least 8 more successful evaluation runs on average compared to the baselines. On the full dataset benchmark, IMLE Policy continues to demonstrate competitive or superior performance, outperforming the baselines in at least 5 of the 8 tasks.\nIMLE Policy is highly multi-modal. Figure 2 shows that IMLE Policy maintains multi-modality across varying initial conditions of the end effector. Unlike Diffusion Policy, which biases towards majority modes, and Flow Matching Policy (one-step), which collapses modes and produces averaged trajectories, IMLE Policy captures diverse trajectories that reflect the full distribution of the demonstration data. This is particularly important in low-data regimes, where capturing all modes without overfitting to dominant behaviours enables efficient learning. Figure 1 further supports this, showing that IMLE Policy preserves multi-modality and accurately captures all modes, even with sparse or imbalanced data.\nIMLE Policy is robust to hyperparameter variations. Figure 6 demonstrates that IMLE Policy consistently performs well across a wide range of hyperparameter settings, with stable success rates even as the rejection sampling threshold (\u20ac) and the number of samples per condition are varied. This robustness ensures that IMLE Policy adapts effectively without significant degradation in performance, making it practical for real-world applications where hyperparameter searches or fine-tuning may be infeasible. Notably, for the simulation benchmark evaluations, hyperparameters were not optimized per task but held constant throughout, yet IMLE Policy still outperformed baseline methods.\nIMLE Policy is well suited for real world robotics. IMLE Policy demonstrates impressive traits that make it suitable for real-world robotics by achieving the best performance across both real-world visuomotor tasks as shown in Figure 7. Even with as few as 17 demonstrations, IMLE Policy learns performant policies, outperforming both baseline methods by a significant margin. The robustness of IMLE Policy in low- data regimes makes it particularly valuable in real-world scenarios where collecting large datasets is often impractical. Additionally, as shown in Table III IMLE Policy exhibits fast single-step inference, achieving up to 97.3% faster inference speeds compared to vanilla Diffusion Policy, while either outperforming it or maintaining competitive performance. This efficiency enables real-time control and is well suited for robotics applications where computational resources can be limited. While its inference speed is similar to that of single- step Flow Matching we note that IMLE Policy outperforms Flow Matching, particularly in the low data regime and con- sistently across the simulation benchmarks. We provide videos demonstrating how our policy perform across both tasks when compared to the baselines and additional videos to demonstrate its robustness in the real world in the attached supplementary material.\nHigh multi-modality calls for temporal consistency. While IMLE Policy demonstrates impressive multi-modal ex- pressivity even from a limited number of demonstrations, a key empirical insight we identified was the tendency for the policy to occasionally switch between modes during execution in highly multi-modal tasks. This behaviour arises because the model is designed to represent all modes present in the data, and without explicit guidance, the policy may select a different mode at each decision point. This can result in inconsistent trajectories, particularly in tasks requiring sequential and co- herent actions. Temporal consistency played an important role in ensuring that once a mode was selected, the policy adhered to it across subsequent timesteps. This not only stabilises and smooths executed trajectories but also prevents the policy from getting stuck between conflicting modes. As demonstrated in both the real-world and simulation experiments, temporal consistency significantly improves success rates, particularly in tasks where precise and sustained adherence to a trajectory is critical. The smooth and consistent nature of these trajectories can be viewed in the supplementary real world video attached to this submission."}, {"title": "E. Implementation Details", "content": "We base our implementation on the available Diffusion Policy training code [9]. As in Diffusion Policy, the model takes in an input noise vector, with the same dimensionality as the action sequence, however in our case we do not denoise this gradually into a clean trajectory, but rather use it as the latent sample space and directly output the trajectory. Therefore the model architecture is a 1D UNet, which is the same as Diffusion Policy, modified only to remove the timestep embedding. We use an action prediction horizon Tp of 16, and action execution horizon Ta of 8 and an observation horizon To of 2. For all experiments we use fixed hyperparameters, with a value of 0.03 for \u20ac, 20 generated samples per datapoint. We train all simulation benchmark tasks for 1000 epochs while all real world tasks were trained for a fixed time of 12 hours. For inference, when using consistency, we set the reset period C to 10, which gives a good balance of consistency and responsiveness. Although we keep this value the same across tasks, note that it can be tuned for the time horizon and desired level of responsiveness of specific tasks."}, {"title": "V. LIMITATIONS", "content": "While IMLE Policy demonstrates strong performance in multi-modal behaviour cloning, certain limitations remain. Its strong ability to capturing all modes makes it more sensitive to the quality of demonstrations, meaning inconsistencies or suboptimal behaviours in the dataset can negatively impact performance. Additionally, in highly multi-modal tasks, IMLE Policy may occasionally switch between modes during execution, necessitating consistency mechanisms such as temporal coherence; however, this can sometimes reinforce suboptimal behaviours if the selected future actions contain errors. Recent concurrent work, such as the BID Diffusion Policy [22], introduces bidirectional decoding to bridge the trade- off between long-term consistency and short-term reactivity by coupling past decisions with forward-looking strategies. Exploring similar decoding strategies could further enhance the robustness of IMLE Policy in multi-modal and stochastic environments. While IMLE Policy is computationally efficient at inference, training incurs some additional overhead due to nearest-neighbour (NN) searches. While we do not optimise this in our work, since we did not find the additional training cost to be too significant, the NN search could be improved using more efficient NN techniques as in prior IMLE research [18]. Furthermore, while our method outperforms baselines in most settings particularly in the low data regime, it does not consistently outperform Diffusion Policy in every benchmark task in the full dataset setting, though we note that Diffusion Policy still requires iterative inference, whereas IMLE Policy is significantly faster."}, {"title": "VI. FUTURE WORK", "content": "IMLE Policy offers a promising foundation for advancing behaviour cloning in real-world robotics and presents several avenues for further research and applications. To facilitate exploration, we will release all code and datasets, encouraging the robotics and machine learning communities to investigate and build upon this work in a variety of domains.\nExploring the use of IMLE Policy for low-cost robotics. IMLE Policy's efficient single-step inference and sample-efficiency presents an opportunity for adoption in low-cost, open-source robotics initiatives, such as LeRobot [5], where computational constraints limit the use of complex methods. Future research could investigate how IMLE Policy can be effectively leveraged in such settings to democratise access to high-quality robotic learning and enable broader adoption.\nInvestigating reinforcement learning fine-tuning with IMLE Policy. The simplicity of IMLE Policy's single-step inference makes it a promising candidate for reinforcement learning (RL) fine-tuning, potentially streamlining the adaptation of imitation-trained policies to new environments. Future work could explore how IMLE Policy simplifies RL finetuning pipelines compared to iterative methods like Diffusion Policy, which make RL fine-tuning difficult [32].\nUtilising IMLE Policy for downstream robotics applications. IMLE Policy's expressive multi-modal capabilities open the door to higher-level control processes, such as Model Predictive Control (MPC), where diverse trajectory proposals could enhance decision-making or potentially facilitate meaningful exploration in reinforcement learning.\nScaling IMLE Policy to large and diverse datasets. While IMLE Policy has shown strong performance in low- data regimes, an important area for exploration is its scalability to large-scale and diverse datasets, such as the Open-X Embodiment dataset [27]. Understanding its generalisation across multi-task scenarios would be an interesting area to explore."}, {"title": "VII. CONCLUSION", "content": "In this work, we introduced IMLE Policy, a novel imitation learning algorithm based on a conditional variant of RS-IMLE, designed to efficiently capture multi-modal action distributions while enabling fast, single-step inference. Through extensive experiments, we demonstrated state-of-the-art sample efficiency in both simulated and real-world robotic manipulation tasks, showing that IMLE Policy can learn effective policies from limited demonstrations. We conducted a thorough evaluation across varying dataset sizes, addressing an underexplored area in behaviour cloning research. Additionally, we proposed a variant that encourages temporal consistency without modifying the training procedure, enhancing execution stability in multi-modal settings. IMLE Policy exhibits promising characteristics for future research, including reinforcement learning fine-tuning, diverse behaviour generation for model predictive control (MPC) and RL exploration, and deployment in resource-constrained settings. Its efficiency and simplicity make it especially relevant for open-source robotics and real-world applications."}]}