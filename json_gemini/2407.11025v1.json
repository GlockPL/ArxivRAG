{"title": "Backdoor Graph Condensation", "authors": ["Jiahao Wu", "Ning Lu", "Zeyu Dai", "Wenqi Fan", "Shengcai Liu", "Qing Li", "Ke Tang"], "abstract": "Recently, graph condensation has emerged as a prevalent technique to improve the training efficiency for graph neural networks (GNNs). It condenses a large graph into a small one such that a GNN trained on this small synthetic graph can achieve comparable performance to a GNN trained on a large graph. However, while existing graph condensation studies mainly focus on the best trade-off between graph size and the GNNs' performance (model utility), the security issues of graph condensation have not been studied. To bridge this research gap, we propose the task of backdoor graph condensation. While graph backdoor attacks have been extensively explored, applying existing graph backdoor methods for graph condensation is not practical since they can undermine the model utility and yield low attack success rate. To alleviate these issues, we introduce two primary objectives for backdoor attacks against graph condensation: 1) the injection of triggers cannot affect the quality of condensed graphs, maintaining the utility of GNNs trained on them; and 2) the effectiveness of triggers should be preserved throughout the condensation process, achieving high attack success rate. To pursue the objectives, we devise the first backdoor attack against graph condensation, denoted as BGC. Specifically, we inject triggers during condensation and iteratively update the triggers to ensure effective attacks. Further, we propose a poisoned node selection module to minimize the influence of triggers on condensed graphs' quality. The extensive experiments demonstrate the effectiveness of our attack. BGC achieves a high attack success rate (close to 1.0) and good model utility in all cases. Furthermore, the results demonstrate our method's resilience against multiple defense methods. Finally, we conduct comprehensive studies to analyze the factors that influence the attack performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph neural networks (GNNs) have been widely deployed in various fields involving graph-structured data, such as social computing [40, 59], drug discovery [20, 28] and recommendation [10, 45, 46]. Large-scale graphs are the fuels to achieve state-of-the-art performance for GNNs [7, 8, 21, 51]. Despite the success, the scale of millions of nodes and edges poses significant requirements on the resources, i.e., storage, training time, etc.\nGraph condensation [11, 24, 30, 34, 35, 43, 55, 68] is an emerging research direction and the involved techniques are natural antidotes to the aforementioned challenges. The goal of graph condensation is to condense a large graph into a small counterpart such that a GNN model trained on this small graph can yield comparable performance to the one trained on the original graph, as shown in Figure 1. For instance, one of the most representative methods, GCond [25] condenses the graph of the Reddit dataset with 153,932 training nodes into only 154 synthetic nodes. GCond indicates a 99.09% reduction of the number of training nodes while exhibiting 95.3% of original graph test performance. Regarding the remarkable achievements of graph condensation in reducing resources consumption, we expect it can be provided as a service [15, 32, 51]. Services that provide compact yet informative graphs can be a life-saving straw for researchers and organizations, who grapple with the storage and computational burden posed by large-scale graphs.\nDespite its promising advantage in condensing information for graphs, the graph condensation process is essentially the same to the training of neural network model. Condensation involves optimizing the synthetic node features and graph structures while model training involves optimizing the model's parameters. Many studies have shown that the models based on neural networks are vulnerable to security and privacy attacks [9, 17, 27, 36, 38, 42]. However, existing graph condensation research [24, 25, 43, 55, 68] primarily focuses on achieving best trade-off between the size reduction of graph data and the model utility. The security issues of graph condensation haven't been investigated. To bridge this research gap, we propose the task of backdoor graph condensation, which aims at injecting malicious information into condensed graphs to further backdoor the GNNs trained on them.\nGraph backdoor attack [4, 66, 70] is closely related to the new task. Existing graph backdoor methods directly inject triggers into the graph during GNN's training. However, this is impractical for graph condensation since directly attaching triggers to the condensed graphs is insufficient due to the small sizes of the condensed graphs. For instance, the condensed graph of Reddit consists of only 154 nodes, making it trivial to check and prune those abnormal edges or node features in such a small-scale graph. Besides, directly injecting triggers to the condensed graph can greatly influence the utility of GNN. We carry out an experiment adopting GCond [25] as the condensation method and GCN [26] as the architecture on dataset Cora and Citeseer. As shown in Table 1, directly injecting triggers to condensed graph (Naive Poison) can greatly undermines the model utility (CTA), greatly reduced from the CTA of GNNs trained on clean graph (Clean Model). This reveals that directly adapting existing graph backdoor attacks to poison the condensed graph cannot effectively carry out attack against the GNNs.\nTo address these issues, we summarizes two primary objectives for backdoor attacks against graph condensation: 1) high GNN utility, which aims to maintain the quality of condensed graph to preserve the utility of GNN; and 2) high attack effectiveness, which emphasizes the effectiveness of triggers throughout condensation to ensure the feasibility of attack.\nTo pursue the objectives, we undertake the initial step to design a sufficient backdoor attack for graph condensation. We envision the attacker as the graph condensation service provider as shown in Figure 1. Specifically, we propose to inject triggers into the original large graph during condensation process instead of directly injecting triggers into the condensed graph. To retain the triggers in the condensed graph, we iteratively optimize the triggers throughout the condensation process. Further, to preserve the quality of the condensed graph, we limit the number of nodes that are attached triggers. Therefore, to enlarge the effectiveness of triggers within a certain budget of poisoned nodes, we propose to attach triggers to representatives nodes with a curated selection module. To demonstrate the effectiveness of our method, we conduct extensive experiments on four benchmark datasets and four representative graph condensation methods. Empirical results show that our method BGC maintains high model utility and achieves promising attack success rate. Besides, comprehensive ablation studies reveals that BGC is robust in different settings. The evaluation result with two representative graph defense methods indicates that they are invalid in defending against BGC. Our contribution can be summarized as following:\n\u2022 We introduce the task of backdoor graph condensation, aiming to inject malicious information into condensed graph to further backdoor the GNNs.\n\u2022 We propose the first backdoor attack against graph condensation, named BGC, where we devise a module to select representatives nodes to inject triggers in original graph and update the triggers throughout the condensation process. In this way, we can launch backdoor attacks against GNNs trained on the condensed graph.\n\u2022 We conduct comprehensive evaluation of BGC under different settings. The results show that BGC is effective across all cases. Besides, the evaluation results with two representative graph defense methods indicate that our attacks can outmaneuver these defense methods."}, {"title": "2 PRELIMINARIES", "content": "In this section, we introduce some notations and present the preliminaries on graph condensation and graph backdoor attack. We denote a graph \\(G = \\{A, X, Y\\}\\), where \\(A \\in \\mathbb{R}^{N\\times N}\\) is the adjacency matrix, N is the number of nodes, \\(X \\in \\mathbb{R}^{N\\times d}\\) is the d-dimensional node feature matrix and \\(Y \\in \\{0, 1, ..., C-1\\}^N\\) denotes the node labels over C classes. Here, \\(A_{ij} = 1\\) if node \\(v_i\\) and node \\(v_j\\) are connected; otherwise \\(A_{ij} = 0\\). We denote the node set by \\(V = \\{v_1, ..., v_N\\}\\). In this paper, we focus on node classification tasks."}, {"title": "2.1 Graph Condensation", "content": "Goal: Graph condensation [25, 43, 47, 68] aims to learn a small, synthetic graph dataset \\(S = \\{A', X', Y'\\}\\). where \\(A' \\in \\mathbb{R}^{N'\\times N'}\\), \\(X' \\in \\mathbb{R}^{N'\\times d}\\), \\(Y' \\in \\{0, 1, ..., C - 1\\}^{N'}\\) and \\(N' \\ll N\\). A GNN trained on the condensed graph S is expected to achieve comparable performance to the one trained on the original graph G.\nFormulation: Graph condensation can be formulated as a bi-level optimization problem [24, 25, 43, 46, 67], iteratively updating the synthetic graph S and the model parameters \\(\\theta\\). The optimization objective can be formulated as following:\n\\(\\min_{S} L(f_{\\theta_S}(A, X), Y) \\quad s.t. \\quad \\theta_S = \\arg \\min_{\\theta} L(f_{\\theta}(A', X'), Y')\\), (1)\nwhere \\(f_{\\theta_S}\\) denotes the GNN model parameterized with \\(\\theta\\), \\(\\theta_S\\) denotes the GNN is trained on the syntetic graph S, and L is the task-related loss utilized for GNN training (i.e., cross entropy loss). In pursuit of this objective, different condensation methods may have different designs for the optimization of synthetic graph and one of the most prevalent methods adopted by previous works is"}, {"title": "2.2 Graph Backdoor Attack", "content": "Goal: The attacker injects triggers into the training data of target GNN model and attach triggers to target nodes at the test time, leading to mis-classification on target nodes while maintaining normal behavior for clean nodes without triggers. At the setting of graph backdoor attack [4], the data is available for attacker while the information of the target GNN models is unknown to the attacker. The effectiveness of a backdoor attack is typically evaluated by attack success rate (ASR) and clean test accuracy (CTA) [3, 4, 32]. The ASR measures its success rate in misleading GNN to predict the given triggered samples to the target label. The CTA evaluates the utility of the model given clean samples.\nFormulation: Given a clean graph \\(G = \\{A, X, Y\\}\\) with node set V, the goal of graph backdoor attack [4] is to learn an adaptive trigger generator \\(f_g : v_i \\rightarrow g_i\\) and effectively select a set of nodes \\(V_p\\) within budget to attach triggers and labels. The GNN f trained on the poisoned graph \\(G_p\\) will classify the test node attached with the trigger to the target class \\(y_t\\). The objective can be formulated as follows:\n\\(\\min_{V_p, \\theta_g} \\mathbb{1}(f_{\\theta}(a(G^v_i, g_i)), y_t)\\)\n\\(\\forall v_i \\in V_U\\)\ns.t. \\(\\theta^* = \\arg \\min_{\\theta} \\sum_{v_i \\in V/V_p} l(f_{\\theta}(G^v_i), y_i) + \\sum_{v_i \\in V_p} l(f_{\\theta}(a(G^v_i, g_i)), y_t)\\), (2)\n\\(\\forall v_i \\in V_p \\cup V_T, g_i\\) meets \\(|g_i| < \\Delta_g\\), and \\(|V_P| \\le \\Delta_p\\),\nwhere \\(\\theta_g\\) denotes the parameters of the adaptive trigger generator \\(f_g\\), \\(G^v_i\\) denotes the clean computation graph of node \\(v_i\\), \\(a(\\cdot)\\) denotes the operation of trigger attachment and \\(l(\\cdot)\\) is the node-level form of the cross entropy loss. \\(V_T\\) is the test node set and \\(V_U \\subseteq V\\) is the node set for updating triggers, in which nodes are randomly sampled from V to ensure that the attacks can be effective for various types of target nodes. To be consistent with Eq.(1), we have:\n\\(L (f_{\\theta} (A, X), Y) = \\sum_{v_i \\in V} l(f_{\\theta}(G^v_i), y_i)\\),\nwhere \\(f_{\\theta}(G^v_i)\\) is the clean prediction on node \\(v_i\\)."}, {"title": "3 PROBLEM FORMULATION", "content": "Attack Settings. We consider a practical scenario, where the attacker is envisioned as malicious graph condensation provider, supplying condensed graphs. Besides, we assume that the attacker is accessible to the dataset but lack the information of the target model. This stems from that the provider solely delivering condensed graphs, without knowledge of the model to be trained.\nAttacker's Goal. The attacker's goal is to inject the malicious information into the condensed graph dataset and consequently backdoor the GNNs trained on the condensed graph. However, as discussed in previous section, the size of the condensed graph is significantly smaller than the original graph, which will result that directly injecting triggers into the condensed graphs renders those triggers easily detectable [4, 32]. Therefore, it is more practical to inject triggers into the original graph throughout the condensation process to carry out the attacks.\nFormulation of Backdoor Graph Condensation. Given a clean graph \\(G = \\{A, X, Y\\}\\) with node set V, we aim to learn an adaptive trigger generator \\(f_g : v_i \\rightarrow g_i\\) and effectively select a set of nodes \\(V_p \\subset V\\) within budget to attach triggers and labels. Then, we can denote the poisoned graph by \\(G_p = \\{A_p, X_p, Y_p\\}\\). The condensed graph produced from \\(G_p\\) is denoted by \\(S = \\{A', X', Y'\\}\\). Thus, we learn the trigger generator by solving:\n\\(\\min_{G_P} \\mathcal{L}(f_{\\theta_S} (A_P, X_P), Y_P)\\)\ns.t. \\(\\theta_S = \\arg \\min_{\\theta} L (f_{\\theta}(A', X'), Y'),\\)\ns.t. \\(S^* = \\arg \\min_S L' (f_{\\theta_S} (A_P, X_P), Y_P),\\) (3)\nwhere the objective \\(\\mathcal{L}\\) of learning trigger generator is to mislead GNN f trained on S to classify the test nodes attached with the trigger to the target class \\(y_t\\) and it is formulated as following:\n\\(\\mathcal{L}(f_{\\theta_S} (A_P, X_P), Y_P) = \\sum_{v_i \\in V_U} l(f_{\\theta_S} (a(G^v_i,g_i)), y_t),\\)\n\\(\\forall v_i \\in V_P \\cup V_T, g_i\\) meets \\(|g_i| < \\Delta_g\\) and \\(|V_P| \\le \\Delta_\\rho\\), (4)\nwhere \\(\\theta_g, y_t, f_g, G^v_i, v_i, a(\\cdot), l(\\cdot), V_U\\) and \\(V_T\\) are coherent with Eq.(2). L is the cross entropy loss, utilized to train GNN f:\n\\(L (f_{\\theta} (A', X'), Y') = \\sum_{v_i \\in V'} l(f_{\\theta}(G^v_i), y_i),\\) (5)\nwhere V' is the node set of synthetic graph S. L' is the loss for graph condensation [24, 25, 43, 68] and we introduce the gradient matching mechanism in this paper. Therefore, the optimization of S can be re-written as:\n\\(S^* = \\arg \\min_S \\sum_{t=0}^{T-1} D(\\nabla_{\\theta} L(f_{\\theta_t} (A', X'), Y'), \\nabla_{\\theta} L(f_{\\theta_t} (A, X), Y)),\\) (6)\nwhere D(,) is a distance function, T is the number of steps of the whole GNN's training trajectory."}, {"title": "4 METHODOLOGIES", "content": "4.1 Overview\nAs we can observe from the results in Table 1, directly injecting triggers into the condensed graph can significantly compromises the model utility (CTA). Therefore, directly injecting into the condensed graph cannot carry out an effective backdoor attack and we propose to inject triggers into original graph instead. Examining the bi-level pipeline of graph condensation [24, 25, 43, 62], we observe that the condensed graphs are consistently optimized. If the triggers are pre-defined and remain unchanged, the triggers may not be preserved throughout the updating process [32], failing to inject malicious information into the condensed graph. Thus, we search to devise an advanced attack, wherein triggers can be updated during condensation, preserving the attack's effectiveness throughout the graph condensation. Further, to avoid undermine"}, {"title": "4.2 Poisoned Node Selection", "content": "In this section, we delve into poisoned node selection. Previous studies [31, 54] indicate that representative samples play a pivotal role in the gradient of dataset condensation, exerting a substantial influence on the quality of condensed datasets. Therefore, to facilitate successful backdoor attacks within a limited budget on graph condensation, we propose to select representative nodes to poison. Specifically, we train a GCN model \\(f_{sel}\\) with the original graph \\(G = \\{A, X, Y\\}\\) to obtain the representations of nodes:\n\\(H_{sel} = GCN_{sel} (A, X), \\quad \\hat{Y} = softmax(W_{sel} \\cdot H_{sel}),\\) (7)\nwhere \\(W_{sel}\\) is the weight matrix for classification. The training objective of \\(f_{sel}\\) can be formulated as:\n\\(\\min_{\\Theta_{sel}, W_{sel}} \\sum_{v_i \\in V} l(y_i, \\hat{y}_i),\\) (8)\nwhere \\(\\Theta_{sel}\\) is the parameters of \\(f_{sel}\\), \\(l()\\) is the cross entropy loss, \\(y_i\\) is the label of node \\(v_i\\) and \\(\\hat{y}_i\\) is the prediction of \\(v_i\\).\nTo guarantee the diversity of the representative nodes [31], we follow [4] to separately apply K-Means to cluster on each class c. While nodes nearer to the cluster centroid are more representative, a node nearest to the centroid may have a high degree. Assigning malicious labels to high-degree nodes could lead to substantial performance degradation since the negative effects will be propagated to a wide range of neighbors. Therefore, we adopt the metric that can balance the representativeness and the negative effects on model utility [4] and the metric score can be calculated as follows:\n\\(m(v_i) = ||h^i - h^k||_2^2 + \\lambda \\cdot deg(v_i),\\) (9)\nwhere \\(\\lambda\\) is the balance hyperparameter, \\(h^i\\) is the representation of the centroid of the k-th cluster and \\(h^k\\) is the representation of a node v, belonging to k-th cluster. After getting the scores, we select \\(\\frac{\\Delta_{\\rho}}{(C-1)K}\\) the nodes with top-n hisghest scores in each cluster. \\(n = \\frac{\\Delta_{\\rho}}{(C-1)K}\\), where C is the number of classes and K is the number of clusters."}, {"title": "4.3 Trigger Generation", "content": "Given the poisoned node set \\(V_p\\), we utilize \\(f_g\\) to generate the triggers for each of them to poison the graph. Since different nodes possess different structural neighbors, the most effective topology of the triggers may vary. Therefore, \\(f_g\\) takes the node features and graph structure as input to generate not only the node features but also the structure of the triggers.\nSpecifically, we adopt a GCN model to encode the node features and graph structure into representations as following:\n\\(H = GCN_g (A, X).\\) (10)\nThen, the node features and strucutre of the trigger for node \\(v_i\\) is generated as following:\n\\(X^i = W_f \\cdot h_i, \\quad A^i = W_a \\cdot h_i,\\) (11)\nwhere \\(h_i\\) is the representation of \\(v_i\\). \\(W_f\\) and \\(W_a\\) are learnable parameters for feature and structure generation, respectively. \\(X^i \\in \\mathbb{R}^{|g_i| \\times d}\\) is the synthetic features of the trigger nodes, where \\(|g_i|\\) is the size of the generated trigger and d is the dimension of features. \\(A^i \\in \\mathbb{R}^{|g_i| \\times |g_i|}\\) is the adjacency matrix of the generated trigger. Since the graph structure is discrete, we follow previous studies [4, 23] to binarize the continuous adjacency matrix \\(A^i\\) in the forward computation while using the continuous adjacency matrix value in backward propagation. With the generated trigger \\(g_i = (X^i, A^i)\\), we link it to node \\(v_i \\in V_p\\) and assign target class label \\(y_t\\) to build the backdoored graph \\(G_p\\) to generate the condensed graph S for model training. In the inference stage, the generator \\(f_g\\) will be utilized to generate triggers for test nodes to lead the backdoored GNN to predict them as target class \\(y_t\\)."}, {"title": "4.4 Optimization", "content": "Since target model is invisible to attackers, we propose to optimize the trigger generator \\(f_g\\) ad generate condensed graph S to successfully attack the surrogate GCN model \\(f_c\\). In this subsection, we elaborate on the optimization of condensed graph S, surrogate model \\(f_c\\) and the trigger generator \\(f_g\\).\nSurrogate Model. Given the condensed graph \\(S = \\{A', X', Y'\\}\\), the surrogate GCN model \\(f_c\\) is trained by:\n\\(\\min_{\\theta_c} L_c = L (f_{\\theta_c} (A', X'), Y'),\\) (12)\nwhere \\(\\theta_c\\) is the parameters of \\(f_c\\).\nTrigger Generator. Given the selected poison nodes \\(V_p\\) and the surrogate GCN model \\(f_{sel}\\), we can update the trigger generator \\(f_g\\) to mislead the surrogate model to classify nodes attached with trigger to label \\(y_t\\) by:\n\\(\\min_{\\Theta_g} L_g = \\sum_{v_i \\in V_U} \\mathbb{1}(f_{\\theta_S} (a(G^v_i, g_i)), y_t),\\) (13)\nwhere \\(G^v_i\\) indicates the clean computation graph of node \\(v_i\\), \\(g_i\\) is the trigger of \\(v_i\\), \\(a()\\) represents the attachment operation, \\(\\mathbb{1}(\\cdot)\\) is the cross entropy loss, \\(y_t\\) is the target class, \\(\\Theta_g\\) is the parameters of trigger generator \\(f_g\\) and nodes in \\(V_U \\subseteq V\\) are randomly sampled from V to ensure that the attacks can be effective for various types of target nodes.\nCondensed Graph. Given the selected poison nodes \\(V_p\\) and trigger generator \\(f_g\\) parameterized by \\(\\Theta_g\\), we can generate the poisoned graph \\(G_p = \\{A_P, X_P, Y_P\\}\\), based on which we can obtain S by:\n\\(\\min_{S} L_s = L' (f_{\\theta_S} (A_P, X_P), Y_P),\\) (14)\nwhere \\(S\\) is the parameters of surrogate GCN model \\(f_c\\) trained on S and L' is the loss function for graph condensation.\nCombining Eq.(12), Eq.(13) and Eq.(14), the tri-level optimization problem could be formulated as:\n\\(\\min_{\\Theta_g} L_g(\\theta_{S^*}(\\Theta_g), S^*(\\Theta_g), \\Theta_g)\\)\ns.t. \\(\\Theta_{S} = \\arg \\min_{\\theta} L_c(\\theta_c, S^*(\\Theta_g), \\Theta_g),\\)\ns.t. \\(S^* = \\arg \\min_S L_s(S, \\Theta_g).\\) (15)\nOptimization Schema. To reduce the computational cost, we transform the tri-level optimization into bi-level one by sequentially optimizing the surrogate model \\(f_c\\) and the trigger generator \\(f_g\\) at each update of the condensed graph S.\nSurrogate Model. To further mitigate the computational complexity, we follow [4, 70] to update surrogate model \\(\\Theta_c\\) for T iterations with fixed generator \\(\\Theta_g\\) and condensed graph S to approximate \\(\\Theta^*\\):\n\\(\\Theta_c^{t+1} = \\Theta_c^{t} - \\alpha_c \\nabla_{\\Theta_c} L_c(\\Theta_c, S, \\Theta_g),\\) (16)\nwhere \\(\\alpha_c\\) is the learning rate for surrogate model training and \\(\\Theta_c^t\\) denotes the model parameters after t iterations.\nTrigger Generator. We also apply the M-iterations approximation to the optimization for trigger generator \\(\\Theta_g\\) with updated \\(\\Theta_c^t\\) and the fixed S:\n\\(\\Theta_g^{m+1} = \\Theta_g^{m} - \\alpha_g \\nabla_{\\Theta_g} L_g(\\Theta_c^T, S, \\Theta_g),\\) (17)\nwhere \\(\\alpha_g\\) is the learning rate for trigger generator's training.\nCondensed Graph. In the outer iteration, we compute the gradients of S based on the trigger generator \\(\\Theta_g^M\\) and the trained surrogate model \\(\\Theta_c^T\\) as following:\n\\(S^{k+1} = S^{k} - \\alpha_s \\nabla_{S} L_s(\\Theta_c^T, S, \\Theta_g^M),\\) (18)\nwhere \\(\\alpha_s\\) is the learning rate of updating condensed graph. More details could be found in Algorithm 1."}, {"title": "5 EXPERIMENTAL SETTINGS", "content": "Datasets. We evaluate our proposed attack on two transductive datasets, i.e., Cora, Citeseer [26], and two inductive datasets, i.e., Flickr [60], Reddit [19]. All the datasets have public splits and we download them from Pytorch Geometric [13], following those splits throughout the experiments. The statistics of the datasets and splits are summarized in Table 2.\nGraph Condensation Methods. In this paper, we incorporate four prevalent graph condensation methods to test the attack performance of our proposed method: 1) DC-Graph [25, 67] is the graph-based variant of general dataset condensation method DC [67], 2) GCond [25] is one representative graph condensation method, 3) GCond-X [25] is the variant of GCond that discards the structure information of condensed graph for GNNs' training and 4) GC-SNTK [43] reforms the graph condensation as a Kernel Ridge Regression (KRR) task and it is based on the Structure-based Neural Tangent Kernel (SNTK). While DC-graph, GCond and GCond-X offer flexibility in utilizing different GNNs for the condensation and test stages, we default to adopting the best-performing of setting SGC [44] as the backbone for condensation and GCN [26] for"}, {"title": "6 EVALUATION", "content": "In this section, we present the performance of our method BGC against graph condensation. The extensive experiments are devised to answer following questions:\n\u2022 RQ1: Can BGC achieve high attack performance and preserve the model utility of GNNs?\n\u2022 RQ2: Can the condensed graph by BGC generalize to different architectures of GNNs?\n\u2022 RQ3: How does BGC perform against defense methods?\nBesides, we also conduct various ablation studies to analyze the properties of BGC, investigating how different settings and hyperparameters affect the attack performance."}, {"title": "6.1 Attack Performance and Model Utility", "content": "Attack Performance. To measure BGC's attack performance, we conduct a comparative evaluation of the ASR score between the backdoored GNN and the clean GNN, which are reported as ASR and C-ASR in Table 3, respectively. We can observe from Table 3 that all of the ASR scores are over 95.0%. For example, the ASR"}, {"title": "6.2 Attack Performance Comparison", "content": "To further validate the effectiveness of our design, we compare BGC with previous backdoor attack methods: 1) GTA [48] is the most representative backdoor attack method on graph, which injects triggers to the graph during model training, and 2) DOORPING [32] is a backdoor attack against dataset distillation for image data, which directly learns triggers for the images. To adapt GTA in the context of graph condensation, we apply it on the original graph and then utilize the poisoned graph for condensation. To transform DOORPING for graph data, we learn the universal triggers for all the nodes following training procedures described in the original paper [32]. The experimental results are presented in Table 4. Although GTA and DOORPING can perform good in some cases, they are still inferior to BGC and they fail to launch effective attack in many cases (i.e., for all the settings with the condensation method GC-SNTK, the attack success rate of these two methods is less than 88.81%). Besides, their attacks can lead to significant drop of the GNN's utility. For instance, under the setting that condensation ratio is 0.90% and condensation method is GCond-X, the CTAs of GTA and DOOPRPING are 69.00 and 55.47, dropping by 6.45% and 24.80% respectively. This demonstrates the superiority of our method and revieals that previous methods cannot launch successful backdoor attack against graph condensation."}, {"title": "6.3 Effectiveness on Cross Architectures", "content": "Since we envision the attacker as the graph condensation service provider, the attacker does not know which GNN the customer will train with the condensed graph. Therefore, it is essential to test the effectiveness of BGC in backdooring different architectures of GNNs. Specifically, we utilize the graphs condensed by BGC with condensation method GCond for the training of various architectures of GNNs: GCN [26], GraphSage [19], SGC [44], MLP [22], APPNP [16] and ChebyNet [6]. The condensation ratios for four datasets are: Cora, 2.60%; Citeseer, 0.90%; Flickr, 1.00%; Reddit, 0.10%. The results are reported in Table 5. We can observe that the ASR scores in many cases are 100% (i.e., for dataest Cora and"}, {"title": "6.4 Defenses", "content": "To mitigate the threats of backdoor attacks, various defense methods have been proposed. In this section, we evaluate the robustness of our proposed method against two representative graph defense methods: 1) Prune [4] is a dataset-level defense, which prunes edges linking nodes with low cosine similarity in the condensed graphs, and 2) Randsmooth [66] is a model-level defense, which randomly subsamples d subgraphs to generate d outputs, using a voting mechanism for the final prediction. The results are presented in Table 6, where we report the CTA and ASR scores of BGC, the scores of BGC under the defenses and the decrease ratio of those scores. By analyzing the score changes, we assess the effectiveness of the defense mechanisms in mitigating our proposed attack.\nIn our experiment, we implement Prune by removing the edges connecting nodes that fall within the lowest 20% of cosine similarities. As we can observe from the table, most ASR scores decrease. However, the CTA scores also drop significantly, often more than"}, {"title": "6.5 Ablation Study", "content": "In this section, we investigate the effects of the poisoned node selection module. To demonstrate the effectiveness of the selection module, we devise a variant of BGC by replacing the selection module by randomly selecting nodes to attach triggers and assign target labels. We denote the variant by BGCRand. The results with condensation method DC-Graph are presented in Figure 3. We can"}, {"title": "6.6 Hyper-parameter Analysis", "content": "Condensation Epochs We further investigate the impact of the number of condensation epochs on attack and utility performance. Since the number of condensation epochs has a significant influence on the quality of condensed graph, we report the attack and utility performance by varying the number of condensation from 50 to 1000 on four datasets, using condensation method GCond. As depicted in Figure 4, both ASR and CTA scores first increase and converge to a stable range of value. Besides, we can observe that in most cases, the ASR and the CTA converges roughly at the same time, except the ASR value on Flickr dataset. For instance, at the condensation ratio of 0.10%, the ASR converges after near 900 epochs while the CTA stabilizes at around 200 epochs.\nVarying the Poisoning Ratio We explore the effect of poisoning ratio on the attack and utility performance. We vary the poisoning ratio from 0.10 to 0.20 for dataset Cora, Citeseer, vary the poison number from 60 to 100 for dataset Flickr and vary the poison number from 130 to 230. We report the results in Table 7. As shown in the table, in all cases for different datasets, larger poisoning ratio does not necessarily lead to better utility performance. For instance, given the condensation method GCond and dataset Reddit, the highest CTA scores are achieved by 180 poisoned nodes instead of 230. We argue that this is because larger number of poisoned nodes can potentially affect the quality of the condensed graph, further affecting the utility of the backdoored GNN.\nVarious Trigger Sizes As shown in previous work [4, 32], larger trigger size can contribute to higher attack performance. To investigate the effect of trigger size on the attack and utility performance, we conduct experiment on dataset Flickr with condensation method GC-SNTK and report the results under three different condensation ratios in Figure 5. As depicted in the figure, the ASR scores are all close to 100% and increase as the trigger size becomes larger. In the contrast, the CTA scores decline as the trigger size increases. Despite the negative effects on CTA scores caused by increasing the trigger size, the degradation is marginal and the utility performance is still acceptable. For example, the declines of CTA scores in three settings are only 0.51%, 0.94%, 0.92%, respectively. Therefore, increasing the size of triggers can encounter the trade-off between attack performance and GNN utility.\nNumber of GNNs' Layers Here, we aim to understand how the number of GNN's layers affects the attack performance and model utility. We carry out experiment on three dataset: Cora, Citeseer and Flickr, utilizing the condensation method GCond. The results are presented in Table 8, from which we can observe that the layer"}, {"title": "7 RELATED WORK", "content": "7.1 Graph Condensation\nRecently, graph condensation [15, 21, 37, 50, 51] is a technique for efficient graph learning, which aims to synthesize small graph datasets from the large ones. GCond [25] is the first graph condensation method and focuses on node-level datasets. It matches the training gradient of the original datasets with condensed datasets to achieve comparable performance. Doscond [24] extends the gradient matching paradigm to the graph-level datasets and propose one-step update strategy to enhance the condensation efficiency. GCSR [34] and SGDD [56] are proposed to incorporate more comprehensive graph structure information into the condensed datasets. CTRL [64] offers a better initialization and a more refined strategy for gradient matching. GEOM [65] proposes to utilize a well-trained expert trajectory to supervise the condensation. FGD [12] aims at improving group fairness for node classification task in graph condensation. MCond [14] enables efficient inductive inference. On the other hand, CaT [33] and GCDM [29] both adopt maximum mean discrepancy to match the distribution between original graphs and condensed graphs. EXGC [11] accelerates condensation via Mean-Field variational approximation and inject explainability via existing explanation techniques. Different from aforementioned methods that formulate the condensation as a bi-level optimization problem, MIRAGE [18] investigates graph condensation from a heuristic perspective and proposes a model-agnostic method. Besides, GC-SNTK [43], LiteGNTK [55] and SFGC [68] all adopt the Kernel Ridge Regression formalization while LiteGNTK focuses on graph-level tasks and the other two are devised for node-level tasks. In spite of the advances in graph condensation, the security risks stemming from the condensation process are overlooked and haven't been investigated. Thus, we first explore this issue and propose an effective backdoor attack method against graph condensation."}, {"title": "7.2 Backdoor Attacks on Graph", "content": "According to the categorization by the stages the attack occurs [4], adversarial attacks on GNNs mainly contain three types: poisoning attack [41, 69], evasion attack [1, 2] and backdoor attack [4, 48]. In this paper, we focus on the backdoor attacks. The graph backdoor attack [5, 52, 53, 57, 58, 63] is a training time attack [32]. It injects a hidden backdoor into the target GNNs via backdoored training graph. At the test time, the successfully backdoored GNNs perform well on the clean test samples but misbehave on the triggered samples. The first work [66] on graph backdoor randomly generates graphs as triggers while GBAST [39] generates triggers based on subgraphs. Different from the universal triggers, GTA [49] proposes a generator to adaptively obtain sample-specific triggers. To achieve unnoticeable graph backdoor attacks, UGBA [4] limits the attack budget and improves the similarity between triggers and target nodes. GCBA [61] studies a new setting where supervisory labels are unavailable and proposes the first backdoor attach against graph contrastive learning. Those efforts focus on inject triggers to the original graph during model training, which is infeasible to poison the condensed graph data. Recently, two related studies [3, 32] on backdoor attack against dataset distillation for image data are proposed. However, their design can only produce universal triggers for images, which is infeasible for graph data."}, {"title": "8 CONCLUSION", "content": "In this paper, we first propose the task of backdoor graph condensation. We envision a realistic scenario where the attacker is a malicious graph condensation provider and set two primary objectives for launch successful attacks: 1) the injection of triggers cannot affect the quality of condensed graphs, maintaining the utility of GNNs trained on them; and 2) the effectiveness of triggers should be preserved throughout the condensation process, achieving high attack success rate. To pursue these two goals, we propose the first backdoor attack against graph condensation BGC. Specifically, we inject toxic information into the condensed graph by injecting triggers to the original graph during the condensation process, which is different from all previous graph backdoor attacks that perform attacks during the model training stage. Extensive experiments across multiple dataset, GNN architectures and dataset condensation methods demonstrate that our proposed method achieves impressive attack performance and utility performance. The generalization cross different GNN architectures and the resilience of our method against the defense mechanisms are also verified. We hope this work could set the stage for future research in the field of graph condensation's security and raise awareness about the security implications."}]}