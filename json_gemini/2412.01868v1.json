{"title": "COMPOSITION OF EXPERTS: A MODULAR COMPOUND AI SYSTEM LEVERAGING LARGE LANGUAGE MODELS", "authors": ["Swayambhoo Jain", "Ravi Raju", "Bo Li", "Zoltan Csaki", "Jonathan Li", "Kaizhao Liang", "Guoyao Feng", "Urmish Thakkar", "Anand Sampat", "Raghu Prabhakar", "Sumati Jairath"], "abstract": "Large Language Models (LLMs) have achieved remarkable advancements, but their monolithic nature presents challenges in terms of scalability, cost, and customization. This paper introduces the Composition of Experts (CoE), a modular compound AI system leveraging multiple expert LLMs. CoE leverages a router to dynamically select the most appropriate expert for a given input, enabling efficient utilization of resources and improved performance. We formulate the general problem of training a CoE and discuss inherent complexities associated with it. We propose a two-step routing approach to address these complexities that first uses a router to classify the input into distinct categories followed by a category-to-expert mapping to obtain desired experts. CoE offers a flexible and cost-effective solution to build compound AI systems. Our empirical evaluation demonstrates the effectiveness of CoE in achieving superior performance with reduced computational overhead. Given that CoE comprises of many expert LLMs it has unique system requirements for cost-effective serving. We present an efficient implementation of CoE leveraging SambaNova SN40L (Prabhakar et al., 2024) RDUs unique three-tiered memory architecture. CoEs obtained using open weight LLMs Qwen/Qwen2-7B-Instruct, google/gemma-2-9b-it, google/gemma-2-27b-it, meta-llama/Llama-3.1-70B-Instruct and Qwen/Qwen2-72B-Instruct achieve a score of 59.4 with merely 31 billion average active parameters on Arena-Hard (Li et al., 2024) and a score of 9.06 with 54 billion average active parameters on MT-Bench (Zheng et al., 2023).", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have significantly ad-vanced the field of artificial intelligence. Modern LLMs benefit from the favorable scaling laws associated with transformer-based deep neural networks, which have shown no signs of performance saturation. This has led to the de-velopment of very large, trillion-parameter monolithic pro-prietary LLMs such as Gemini, GPT-4, and Claude. While these large monolithic models are justified by model scaling laws (Kaplan et al., 2020), they have drawbacks such as high serving costs, cumbersome maintenance and updates. Fine-tuning large language models presents significant barriers due to prohibitive computational costs that limit accessi-bility, and the inherent risk of task-specific specialization that can compromise model performance across broader domains due to the alignment tax (Ouyang et al., 2022) and catastrophic forgetting (Luo et al., 2024).\nThe recent release of open-weight LLMs like Llama3 (Tou-vron et al., 2023), Mistral (Jiang et al., 2023a), and Gemma (Team et al., 2024), coupled with the active open-sourceLLM community, has democratized access to high quality pre-trained LLMs. This has resulted in a Cambrian explo-sion of diverse small-to-medium sized expert LLMs readily available on platforms like HuggingFace. These expert models address the long tail of needs arising from various domains, tasks, and languages that are poorly served by large monolithic LLMs. A narrow domain expert derived from a fine-tuned large monolithic LLM remains expensive to serve, making it economically impractical for catering to the long tail of niche requirements. Some of these expert models are known to outperform proprietary monolithic models on specific domains or tasks (Zhao et al., 2024a; sam). In addition to this, natural variation in how popular general LLMs are trained they tend to have varying capabil-ities across different domains and languages.\nMotivated by the availability of many expert LLMs, in this paper we explore the problem of creating a compound AI system that combines several expert LLMs that caters to a practitioners application with a given parameter budget con-straint. We propose a Composition of Experts (CoE), a com-pound Al system comprising a single router and multiple expert LLMs. The router's function is to dynamically select the most suitable expert for each incoming input prompt. If the router directs the given input to the correct expert,"}, {"title": "2 RELATED WORK", "content": "The Mixture of Experts (MoE) architecture based LLMs are a popular choice (Lepikhin et al., 2020; Jiang et al., 2024). These LLMs are built using of MoE-layer that use"}, {"title": "3 NOTATION", "content": "x \u2208 \\mathbb{R}^n denotes a n-dimensional vector with real entries and b \u2208 {0,1}^n represents n-dimensional binary vector whose entries are either 0 or 1. 1_n = [1,\\ldots,1]^T denotes the vector with all entries as 1. X \u2208 \\mathbb{R}^{m \\times n} denotes m x n matrix with real entries and B \u2208 {0,1}^{m \\times n} denotes matrix with binary entries. I_n denotes n \u00d7 n identity matrix. For a given set & the number of elements in the set are denoted by |E|."}, {"title": "4 COMPOSITION OF EXPERTS", "content": "The Composition of Experts (CoE) is constructed from a subset of experts \\mathcal{E}_s chosen from set of experts LLMs \\mathcal{E} = {E_1,\\ldots, E_K} such that the given input prompt p is first passed through a router R(p) which maps input to one of the expert LLMs in the subset and the output is produced by that LLM. Formally, the CoE is defined as\nCoE(p; \\mathcal{E}_s, R) = \\sum_{E_j \\in \\mathcal{E}_s} 1_{\\{R(p)=j\\}} E_j (p), \\tag{1}\nwhere \\mathcal{E}_s \\subseteq \\mathcal{E}, the routing function is defined as R(p) : p \\rightarrow {1,\\ldots, |\\mathcal{E}_s|}, and 1_{\\{R(p)=j\\}} and is the canonical indicator function which evaluates to 1 if R(p) = j and 0 otherwise. The high level abstract representation of CoE is shown in Figure 1. The cumulative number of parameter in \\mathcal{E}_s determines the inference cost for CoE. Furthurmore, it is desirable to construct CoEs under a given parameter budget B. Let D = {(p_1, c_1), \\ldots, (p_N, c_N)} be the training data comprising of N prompt-completion pairs. The generic"}, {"title": "4.1 CoE with Two Step Routing", "content": "While directly labeling each prompt for best expert is diffi-cult to train a accurate router. We observe in Figure 4 that when prompts are labeled based on natural categories such as medical, coding, etc. we see clear clusters. Motivated by this natural clustering of prompts in distinct categories, a reasonable approach is to assign best expert for each cat-egory. Since there are many prompts in each category, it doesn't suffer from labeling jitteriness as we encountered while assigning best expert on per-prompt basis.\nBased on this, we impose a simplifying constraint on the router such that the input prompt p is first mapped to one of M categories and subsequently followed by rout-ing it to one of the expert for that category using with"}, {"title": "5 TRAINING ALGORITHM FOR COE", "content": "For training the two-step routing CoE, we propose to first train the category-router, CR followed by training the category-to-expert mapping CE."}, {"title": "5.1 Step 1: Train the category router CR", "content": "Training the category router requires labeled training data \\mathcal{D}_{CR} = {(p_1, r_1), \\ldots, (p_N, r_N)} where r_i is the category label for prompt p_i. Equipped with \\mathcal{D}_{CR}, we can train a multi-class text classifier to obtain CR. A strong pre-trained text-embedding model and a light-weight multi-class classifier built on top of these text-embedding model is an effective choice for the category router architecture."}, {"title": "5.2 Step 2: Train the category-to-expert mapping CE", "content": "We substitute the category router CR from Step 1 in the generic problem in (3) to obtain the following optimization problem for learning category-to-expert mapping CE\n\\min_{ESCE} \\sum_{(p_i,c_i) \\in D} C (CoE(p; \\mathcal{E}, CE, CR), c_i)\ns.t. \\sum_{E_i \\in \\mathcal{E}_s} E_i \\le B,\nCi \\tag{4}\nThe above problem can be converted to a well studied mixed integer linear program using the following facts:\n*   The category router CR partitions D into M parts \\mathcal{D}_1,\\ldots, \\mathcal{D}_M, where \\mathcal{D}_m is the set of prompts in train-ing data that are mapped to category m.\n*   Let l_{ij} denote the cost of mapping prompts in category i to expert j and in terms of \\mathcal{D}_m this cost is denoted by\nl_{ij} = \\sum_{(p_k,c_k) \\in D_i}L (E_j (p_k), c_k)\n*   CE is a mapping from {1,\\ldots, M} to {1,\\ldots, K} it can be represented as binary matrix C \\in {0,1}^{M \\times K}. Since each category is served by only one expert model, the rows of C are constrained to have only 1 non-zero entry in each row, i.e., it must satisfy C1_K = 1_M.\n*   In terms of C, the set of experts \\mathcal{E}_s can be represented as the set of non-zero indices of the vector C^T 1_M. Let supp(C^T 1_M) be the binary vector that is value 1 only at indices where C^T 1_M is non-zero.\n*   Let's also define the expert size vector s_e = [[E_1],\\ldots, [E_K]]^T. Then the total number of param-eters in the CoE for a given C can be written as, s^T supp(C^T1_M)."}, {"title": "6 SYSTEM CONSIDERATIONS FOR COE", "content": "CoE provides the capability to improve the model incremen-tally by adding more experts in a modular fashion. While the total number of parameters in CoE can increase, the number of parameters required to serve any given input query stays bounded. Specifically, each input query would require the parameters for the router and one of the experts. Theoret-ically, this property enables sustaining the same response throughput even as the number of experts increase.\nHowever, a naive implementation of CoE quickly runs into memory capacity limits on GPUs. Modern H100 GPUs are equipped with about 80 GB of High-Bandwidth Memory (HBM) per GPU socket (dgx). Model parameters are typi-cally loaded once into HBM during initialization. Incoming"}, {"title": "7 SEMI-SUPERVISED TRAINING DATA\nPIPELINE FOR COE", "content": "In order to fully realize the potential of CoE, high quality prompts in the training data are needed so that the router can distinguish between experts LLMs on variety of domains and languages. Given the competitive landscape of LLMs and most of the LLMs are optimizing performance on the same benchmarks. This makes distinguishing LLM capabil-ities based on existing benchmarks a challenging problem because all the models perform similarly. Getting high qual-ity prompts that allow us to distinguish between different LLM is cumbersome and costly.\nRecent work (Raju et al., 2024) proposed a semi-supervised approach to build benchmarks with high degree of separabil-ity in the LLM performance in a cost effective manner. We leverage similar semi-supervised approach and use a prompt curation pipeline shown in Figure 6. Starting with a seed set comprising high quality category labeled prompts we first train a k-NN based multi-class text classifier that uses"}, {"title": "8 EXPERIMENTAL SETUP", "content": "We evaluate the two-step routing based CoEs for the ex-pert set & comprising of the following well known open weight LLMs: Qwen/Qwen2-7B-Instruct, google/gemma-2-9b-it, google/gemma-2-27b-it, meta-llama/Llama-3.1-70B-Instruct and Qwen/Qwen2-72B-Instruct. We consider the following domain categories: Medical, Finance, Cod-ing/Computer Programming, Mathematics, and Law; and the following languages categories: Arabic, Serbian, Slove-nian, Hungarian, Russian, Turkish, Japanese and Thai."}, {"title": "8.1 CoE Training data", "content": "We source high quality seed prompts for these domain cate-gories with domain specific open-source datasets available on HuggingFace. We restrict only to the training splits to avoid contamination with benchmarks. The dataset used for each domain is listed in Table 1 in the Appendix. We construct multilingual prompts in various formats such as native, translation, and cross-lingual for all the language categories. The exact methodology and source datasets are outlined in Section A in the Appendix.\nEquipped with these high quality category labeled seed prompts extend prompt dataset by following the semi-supervised pipeline in Figure 6 on general chat prompts from internal user trials and training split (lin Chiang et al., 2024). We use intfloat/e5-mistral-7b-instruct as the text embedding model given its strong performance on Mas-"}, {"title": "8.2 Category Router Training", "content": "Figure 4 shows the 2D t-SNE plot of prompt embeddings obtained from text-embedding model intfloat/e5-mistral-"}, {"title": "8.3 Category-to-expert mapping training", "content": "We obtain, l_{i,j} in category-to-expert mapping optimization problem (6) by computing the win-rate of jth expert on set of prompts routed to category ith using LLM-as-a-judge and equating l_{i,j} as the negative of the win-rate. We use the judge prompt template in (Raju et al., 2024) given its emphasis on effectively addresses the nuances associated with using LLM as a judge for variety of domains. We use GPT-40-mini as the judge model and generate judgment for expert completion in terms of whether it is similar or better and worse than the desired completion. Equipped with these judgments we can calculate win-rate expert j on"}, {"title": "9 EXPERIMENTAL RESULTS", "content": "We evaluate the CoEs for different values of B \u2208 {7, 16, 35, 105, 190} on popular LLM Benchmarks such as Arena-Hard (Li et al., 2024), MT-Bench (Zheng et al., 2023) and various domain knowledge specific benchmark in MMLU-Pro (Wang et al., 2024). We choose Arena-hard for measuring single-turn evaluations and MT-Bench to mea-sure multi-turn evaluations for CoEs. These benchmarks contain challenging prompts collected in a crowd-sourced manner via Chatbot-Arena and represent a real-world us-age of LLMs (Chiang et al., 2024). For all the benchmark we measure performance against average number of active parameters for that benchmark. This can be obtained by taking average of the size of the expert chosen by the router for prompts in the benchmark. We emphasize on measuring average number of parameters because it translates to better tokens per second and time-to-first-token. These some of the key metrics in real-world LLM applications."}, {"title": "9.1 Arena-Hard", "content": "Figure 13 shows performance on the Arena-Hard bench-mark for CoE for various values of B. On the y-axis is the the Arena-Hard score and on the x-axis is the average number of active parameters for the benchmark. As total size B of CoE increases, CoE achieves better performance with fewer active performance and is better than individual experts. This shows that the performance of CoE scales as more experts added. We also observe that Robust-CoE significantly outperforms CoE, demonstrating the value of uncertainty quantified routing. It is key to the stable per-formance of CoE beyond the training dataset. The CoE and Robust-CoE with total parameters B = 190 Billion achieves the score 60.2 and 62.10 with merely 65.23 and"}, {"title": "9.2 MT-Bench", "content": "The performance on various metrics on MT-Bench and the average number of active parameters in the CoE are shown in Figure 14. This benchmark measures performance of LLMs on multi-turn conversation. The CoE routes based on current input and then passes the entire conversation history. This means different experts might be chosen for different conversation turns. Figure 14 shows the average score over multi-turn conversations in the benchmark and average num-"}, {"title": "9.3 Knowledge Intensive Benchmarks", "content": "We measure the performance of CoE on knowledge intensive benchmarks such GSM8k CoT and MMLU-pro benchmark on the following subjects: Mathematics, Business, Com-puter Science, Economics, Health and Law. The average score on these benchmarks vs. average number of active parameters are shown in Figure 15. We observe that while CoE does suffer from reduced performance as compared to individual experts, the Robust-CoE recovers most of this performance. The fine-grained results are shown in Table 2. This can be attributed to the fact that the distribution mismatch between knowledge intensive tasks and the router training data. The robust-CoE via uncertainty quantifica-tion detects this mismatch and appropriately routes these prompts to the best generalist expert."}]}