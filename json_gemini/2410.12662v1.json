{"title": "CROSS-Modal Safety Mechanism Transfer in Large Vision-Language Models", "authors": ["Shicheng Xu", "Liang Pang", "Yunchang Zhu", "Huawei Shen", "Xueqi Cheng"], "abstract": "Vision-language alignment in Large Vision-Language Models (LVLMs) successfully enables LLMs to understand visual input. However, we find that existing vision-language alignment methods fail to transfer the existing safety mechanism for text in LLMs to vision, which leads to vulnerabilities in toxic image. To explore the cause of this problem, we give the insightful explanation of where and how the safety mechanism of LVLMs operates and conduct comparative analysis between text and vision. We find that the hidden states at the specific transformer layers play a crucial role in the successful activation of safety mechanism, while the vision-language alignment at hidden states level in current methods is insufficient. This results in a semantic shift for input images compared to text in hidden states, therefore misleads the safety mechanism. To address this, we propose a novel Text-Guided vision-language Alignment method (TGA) for LVLMs. TGA retrieves the texts related to input vision and uses them to guide the projection of vision into the hidden states space in LLMs. Experiments show that TGA not only successfully transfers the safety mechanism for text in basic LLMs to vision in vision-language alignment for LVLMs without any safety fine-tuning on the visual modality but also maintains the general performance on various vision tasks (Safe and Good).", "sections": [{"title": "INTRODUCTION", "content": "Vision-language alignment methods for Large Vision-Language Models (LVLMs) use a basic LLM, a lightweight vision encoder and projector to efficiently enable the LLM to understand visual input for various vision tasks with relatively low training costs (Liu et al., 2024c; Dai et al., 2023; Zhu et al., 2023). Recent studies indicate the safety of LVLMs deserves attention (Liu et al., 2024a; Wang et al., 2023; Gong et al., 2023). Given that vision and language are aligned into a common space in LVLMs, the safety mechanism should be shared by both of them. However, this is not the case. We find that compared to toxic text input, LVLMs are more vulnerable to toxic vision input. Existing studies improve the safety of LVLMs by utilizing toxic vision data for safety fine-tuning (Zong et al., 2024; Wang et al., 2024). However, they do not provide an essential explanation for the question: \u201cWhy can't the safety mechanism for text be shared by vision after vision-language alignment?\u201d.\nSince the mainstream vision-language alignment methods for LVLMs (e.g., LLaVA (Liu et al., 2024c)) lack additional safety fine-tuning and the training data in this process contain few toxic samples (Wang et al., 2023), the safety mechanism of LVLMs is mostly inherited from the safety mechanism that have established for text in their basic LLMs. So an intuitive view to explain the dilemma of LVLMs' safety on vision is that the vision-language alignment cannot effectively transfer the safety mechanism for text in LLMs to vision. Based on this, this paper proposes a novel perspective called Cross-Modal Safety Mechanism Transfer to rethink, explain and address the exacerbated vulnerability of LVLMs to toxic vision input compared to toxic text input. Cross-modal safety mechanism transfer means transferring the existing safety mechanism for text in LLMs to vision in vision-language alignment training without any additional safety fine-tuning on vision. Our experiments in \u00a7 3 reveal that current vision-language alignment methods fail to achieve an effective cross-modal safety mechanism transfer. LVLMs exhibit significantly different safety capabilities when handling toxicity with the same semantics but different modalities (as shown in Figure 1)."}, {"title": "RELATED WORK", "content": "Vision-Language Alignment in LVLMs. Vision-language alignment in LVLMs equips basic LLMs with the ability to understand and process visual input by pre-training and instruction-tuning on large-scale text-image pairs such as works in LLaVA (Liu et al., 2024c), InstructBLip (Dai et al., 2023), Qwen-VL (Bai et al., 2023b), MiniGPT-4 (Zhu et al., 2023), Flamingo (Awadalla et al., 2023),"}, {"title": "SAFETY MECHANISM CANNOT BE CROSS-MODAL TRANSFERRED", "content": "In this part, we provide LVLM and its basic LLM with toxic inputs having the same semantics but in different modalities (text and vision) to evaluate the safety capabilities on different modalities, which is used as an assessment of cross-modal safety mechanism transfer. We find that safety mechanism for text is not effectively transferred to vision in visual-language alignment.\nData Construction. We collect real toxic images from open-source datasets. For each image, we use LLAVA-NEXT (Liu et al., 2024b) to generate caption for it to get the toxic text-image pair. Text and image in this pair have the same semantics but are in different modalities. The specific datasets include HOD (Ha et al., 2023) that contains 10, 631 toxic images about alcohol, cigarette, gun, insulting gesture and knife, and ToViLaG (Wang et al., 2023) that contains 9, 900 toxic images about bloody and porn. After the caption generation, we get 20,31 toxic text-image pairs for experiments.\nMetrics. We follow the regular safety testing method (Wang et al., 2023), which provides a toxic input and instructs the model to describe the toxic content of the input. We use Defence Success Rates (DSR), which indicates whether the model refuses to produce toxic responses when presented with toxic input, as the metric. Following (Chakraborty et al., 2024), we use LLaMA-2-7B to determine whether the responses generated by the model are toxic, thereby assessing the success of the defense.\nSettings. The specific open-source LVLMs and LLMs used in experiments are: LLaVA-1.6-Mistral-7B (Liu et al., 2024b) with its basic LLM Mistral-7B-Instruct-v0.2 (Jiang et al., 2023), Instruct-Blip (Dai et al., 2023) with its basic LLM Vicuna-7B-v1.5 (Zheng et al., 2024), Qwen-VL-Chat (Bai et al., 2023b) with its basic LLM Qwen-7B-Chat (Bai et al., 2023a). Some closed-source models, such as GPT-4-v, are not considered because we cannot acquire their specific checkpoints, training methods and data to provide further analysis. Given a LVLM (M) and its basic LLM (L), the specific settings are (1) input toxic image to M, (2) input toxic text to M and (3) input toxic text to L.\nFindings. The experimental results are shown in Table 1. Different LVLMs across different toxic scenes show the following common conclusions: (1) DSR of LVLM and its basic LLM is close on toxic text, it indicates that safety mechanism of basic LLMs on text is successfully preserved in visual-language alignment training of LVLMs. (2) For the toxic information with the same semantics in different modalities (text and vision), the safety capabilities of LVLMs vary significantly, and LVLMs can hardly defense toxicity in the visual modality. It indicates that safety mechanism for text is not effectively transferred to vision in visual-language alignment training of LVLMs."}, {"title": "CAUSE OF FAILURE IN CROSS-MODAL TRANSFERRING SAFETY", "content": "This section analyzes the cause of the failure in transferring the safety mechanism from text to vision. Firstly, in \u00a7 4.1, we find that hidden states at the specific transformer layers play a crucial role in the successful activation of safety mechanism. Then, our comparative analysis between text and image in \u00a7 4.2 reveals that alignment between vision and language at hidden states level is insufficient. It makes the transformer layers responsible for safety mechanism activation cannot correctly capture the semantics of image as they perform on text, so they cannot correctly assess the toxicity in image and the safety mechanism on vision collapses."}, {"title": "SAFETY MECHANISM IS ACTIVATED AT SPECIFIC LAYERS BY HIDDEN STATES", "content": "This part gives an insightful explanation of where and how the safety mechanism activated in LVLMs on textual, which is fundamental to understand the safety mechanism collapse on vision. Previous studies (Tenney, 2019; Dai et al., 2021; Meng et al., 2022) have shown that transformers in different layers of language model have different functions such as lexical, semantic, knowledge, etc., this paper proposes a novel method to identify the transformer layers responsible for activating safety mechanism in LVLMs and analyzes the attention patterns on toxic tokens within these layers to give the explanation of where and how the safety mechanism is activated.\nWhere: Locating the Activation of Safety Mechanism. When facing a toxic input, if the safety mechanism is activated, the language model will refuse to follow the instruction and apologize to the user, such as \"Sorry but I cannot ...\", otherwise, the language model will follow the instruction and generate the corresponding response (Bianchi et al., 2024). Therefore, a reply with an apology is an important signal for the activation of the safety mechanism. This paper proposes a novel method to locate where the safety mechanism is activated by detecting the word distribution mutation on sorry semantics among layers. Specifically, given a toxic text t and instruction s to LVLMs, the next token prediction for x can be formalized as:\n$P(x|t, s) = \\text{softmax}(WH'), x \\in X$,\nin which $W \\in \\mathbb{R}^{h \\times v}$ is the vocabulary head that maps the output hidden states H' to the word distribution in vocabulary X with size v. Since previous studies (Chuang et al., 2024; Xu et al., 2024) prove that due to the residual connections, the combination of any intermediate layer in a language model with the vocabulary head W can represent its distribution of semantics, we apply W to each intermediate layer to obtain its distribution of semantics as:\n$P_j(x|t, s) = \\text{softmax}(WH_j), x \\in X$,\nin which j is the j-th transformer layer of LVLMs, Hj is the hidden states of the last token in j-th layer. We calculate the word distribution change in j-th layer by contrasting the (j \u2013 1)-th layer as:\n$D_j(x|t, s) = \\text{log-} \\frac{P_j(x|t, s)}{P_{j-1}(x|t, s)}, j > 1.$"}, {"title": "INSUFFICIENT ALIGNMENT AT HIDDEN STATES MISLEADS SAFETY MECHANISM", "content": "This section gives our analysis about the cause of the failure in transferring the safety mechanism from text to vision in LVLMs. As our findings in \u00a7 4.1, the hidden states of input tokens at specific transformer layers play a crucial role in the successful activation of safety mechanism. We conduct the comparative analysis between hidden states of input text and image with the same semantics, which helps us find the essential explanation for the failure of cross-modal safety mechanism transfer.\nComparative Analysis Between Text and Image. Figure 4 is the cosine similarity between the mean pooled vectors of hidden states of texts and images with the same semantics. At the layers where safety mechanism is activated, compared with the Clip Score that indicates the semantic similarity between text and image, the cosine similarity between hidden states of text and image in LVLMs is significantly lower, suggesting that the alignment between text and image at the hidden states level is insufficient. The transformer layers for safety mechanism activation cannot correctly capture the semantics of image as they perform on text, so they cannot correctly assess the toxicity in image.\nSafety Mechanism Reconstruction. The above analysis suggests that the insufficient alignment of hidden states between vision and language in the layers where safety mechanism activation is"}, {"title": "TEXT GUIDED ALIGNMENT AT HIDDEN STATES LEVEL", "content": "Our analysis in \u00a7 4 indicates that insufficient alignment of hidden states between vision and language in the layers where safety mechanism activation is a critical reason for the safety mechanism collapse in transfer from text to vision. To address this, this section proposes a novel text-guided vision-language alignment method that can effectively align vision and language at hidden states level.\nMOTIVATION AND OVERVIEW\nAs shown in Figure 6 (a), previous vision alignment methods in LVLMs construct image-instruction-output triples as training data, and use cross-entropy in language modeling as the loss fuction to optimize the output generated by basic LLMs to make LLM understand the input vision, thereby achieving visual-language alignment. We name these methods input-to-output alignment, i.e. align the text output to the visual input, which is actually asymmetrical. The natural flaw of these method is that they black-box basic LLMs and only focus on output, while neglecting whether the internal representation of the visual input in LLMs, i.e., the hidden states, aligns with the hidden states in text modality. To address this problem, we propose the Text-Guided input-to-input Alignment (TGA) at hidden states level, as shown in Figure 6 (b). For the input image, TGA retrieves the semantically relevant text as the template to guide the alignment of vision to language at hidden states level.\nDETAILED METHOD\nDATA CONSTRUCTION\nThe original training data is collected from LLaVA (Liu et al., 2024c) that consists of 558K images for pre-training and 665K images for instruction-tuning. Each data sample is in multi-turn conversation"}, {"title": "TRAINING", "content": "At each training step, TGA firstly inputs $X_{caption}$ to LVLMs to get the hidden states of $X_{caption}$ at each transformer layer under the condition of disabling gradient calculation:\n$C_j = \\{C_j^1, C_j^2, ..., C_j^m \\}, j = 1, 2, 3, ..., N,$\nin which $C_j$ is the sequence of hidden states of tokens in $X_{caption}$ at the j-th transformer layer, m is the number of tokens in $X_{caption}$, N is the number of transformer layers of basic LLMs. Then, TGA enables the gradient calculation with retrieved text $X_{retrieval}$, image $X_{image}$ and language instruction $X_{inst}$ as the input for visual instruction tuning (Liu et al., 2024c), in this process, TGA obtains the hidden states of $X_{retrieval}$ and $X_{image}$ at each layer, denoted as R and I respectively. Since the input is ($X_{retrieval}$, $X_{image}$, $X_{inst}$), the hidden states I are actually the fusion of $X_{image}$ and $X_{retrieval}$ because of the self-attention among tokens (Vaswani, 2017). The retrieved text $X_{retrieval}$ guides the basic LLMs to align the hidden states of $X_{image}$ (vision) with the hidden states of $X_{caption}$ (language) at each transformer layer, this is achieved by a pair-wise loss function:\n$\\mathcal{L}_{guide} = \\sum_{j=1}^{N} \\text{cos}(\\bar{I_j}, \\bar{C_j}) + \\text{log } [1 + \\text{exp}(-(\\text{cos}(\\bar{I_j}, \\bar{C_j}) - \\text{cos}(\\bar{R_j}, \\bar{C_j})))],$\nin which $I_j$, $C_j$ and $R_j$ are mean pooled vectors of hidden states of $X_{image}$, $X_{caption}$ and $X_{retrieval}$ in the j-th transformer layer respectively. The intuition of this pair-wise loss is to ensure that $X_{image}$ does not directly replicate the semantics of $X_{retrieval}$, but rather uses $X_{retrieval}$ as a template for partially similar semantics in the text modality to prompt the alignment of $I_j$ with its text modality hidden states $C_j$. The successful alignment should achieve that $I_j$ is closer to $C_j$ than $R_j$, because in this condition, $I_j$, $C_j$ and $R_j$ are aligned into a common space and $C_j$ and $I$ have consistent semantics. $\\text{cos}(R_j, C_j)$ is used as the lower bound supervision for alignment between vision $I_j$ and language $C_j$. The total loss function $\\mathcal{L}$ is the combination of $\\mathcal{L}_{guide}$ and cross-entropy loss for language modeling:\n$\\mathcal{L} = \\mathcal{L}_{guide} + \\frac{1}{N} \\sum_{i=1}^{N} \\text{log } P(X_{a,<i}| X_{retrieval}, X_{image}, X_{inst}, X_{a,i}), X_a \\text{ is the answer for } X_{inst}.$"}, {"title": "EXPERIMENTS", "content": "Experimental Setting. Our method aims to transfer the safety mechanism of LLMs on text to vision during the vision-language alignment in LVLMs, so the main setting in this experiment is assessing"}, {"title": "CONCLUSION AND DISCUSSION", "content": "This paper proposes a novel perspective called Cross-Modal Safety Mechanism Transfer to rethink, explain and address the exacerbated vulnerability of LVLMs to toxic vision compared to toxic text. Extensive analysis shows that current vision-language alignment methods fail to achieve effective cross-modal safety mechanism transfer and the reason is the insufficient alignment between vision and language at hidden states level. To address this, we propose a novel vision-language alignment method that can not only transfer the safety mechanism against toxic text in basic LLMs to vision but also maintain the general performance on various vision tasks compared with existing SoTA LVLMs."}, {"title": "MORE DETAILED EXPERIMENTS", "content": "GENERAL CAPABILITIES OF LVLMS ON VISION TASKS\nDatasets, Benchmarks and Metrics.\n(1) ScienceQA (Lu et al., 2022): A benchmark that consists of 21k multimodal multiple choice ques-tions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We follow LLaVA (Liu et al., 2024c) to evaluate the zero-shot generalization of LVLMS on scientific question answering in image subset and use accuracy as the metric.\n(2) POPE (Li et al., 2023b): POPE evaluates model's degree of hallucination on three sampled subsets of COCO (Lin et al., 2014): random, common, and adversarial and we report the F1 score as the metric on all three splits.\n(3) SEED-Bench (Li et al., 2023a): SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We report the accuracy on image, video and all as the metric.\n(4) MM-Vet (Yu et al., 2023): MM-Vet evaluate model's capabilities in engaging in visual conversa-tions on a diverse range of tasks, and evaluates the correctness and the helpfulness of the response with GPT-4 evaluation.\nCASE STUDY\nCase study about response of different LVLMs for toxic image is shown in Figure 8 to 14."}]}