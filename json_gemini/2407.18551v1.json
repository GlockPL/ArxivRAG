{"title": "Multi-Agent Trajectory Prediction with Difficulty-Guided Feature Enhancement Network", "authors": ["Guipeng Xin", "Duanfeng Chu", "Liping Lu", "Zejian Deng", "Yuang Lu", "Xigang Wu"], "abstract": "Trajectory prediction is crucial for autonomous driving as it aims to forecast the future movements of traffic participants. Traditional methods usually perform holistic inference on the trajectories of agents, neglecting the differences in prediction difficulty among agents. This paper proposes a novel Difficulty-Guided Feature Enhancement Network (DGFNet), which leverages the prediction difficulty differences among agents for multi-agent trajectory prediction. Firstly, we employ spatio-temporal feature encoding and interaction to capture rich spatio-temporal features. Secondly, a difficulty-guided decoder is used to control the flow of future trajectories into subsequent modules, obtaining reliable future trajectories. Then, feature interaction and fusion are performed through the future feature interaction module. Finally, the fused agent features are fed into the final predictor to generate the predicted trajectory distributions for multiple participants. Experimental results demonstrate that our DGFNet achieves state-of-the-art performance on the Argoverse 1&2 motion forecasting benchmarks. Ablation studies further validate the effectiveness of each module. Moreover, compared with SOTA methods, our method balances trajectory prediction accuracy and real-time inference speed. The code is available at https://github.com/XinGP/DGFNet.", "sections": [{"title": "I. INTRODUCTION", "content": "TRAJECTORY prediction is a crucial component of current autonomous driving systems. Its goal is to infer the future trajectory distribution of agents based on historical information within the scene. In addition to the historical motion information of the agents, various complex factors must be considered, such as lane markings and traffic light constraints in the map, as well as social interactions between agents. These considerations ensure that the future trajectory distribution can accurately reflect the agents' movement trends, thereby ensuring the proper functioning of the autonomous driving system.\nCurrently, mainstream prediction model architectures can be roughly divided into three steps: feature encoding, feature interaction, and feature decoding. Additionally, some methods improve the accuracy and robustness of prediction models through feature enhancement. These methods involve intermediate modeling of future intentions or trajectories and interact and fuse these with the original encoded features. Although these methods significantly improve model performance compared to mainstream model architectures, they fail to consider the inherent heterogeneity in prediction difficulty among agents, i.e., the natural differences in prediction difficulty for different agents within the scene.\nTo address potential hazards in driving scenarios, during the decision-making and planning stages of autonomous driving, it is common to consider the heterogeneity of traffic participants' behaviors [1]\u2013[4]. This is primarily manifested in driver social"}, {"title": "II. RELATED WORK", "content": "Heterogeneity in Trajectory Prediction. The inherent heterogeneity in driving trajectories can provide rich additional information for decision making tasks, leading to safer and more flexible decision outcomes. Recent research in trajectory prediction tasks has taken such heterogeneity into account to provide personalised predictions. For example, Chen et al. [9] proposed a distribution discrimination method to predict personalised motion patterns by distinguishing latent distributions. Xing et al. [10] introduced a joint time-series modelling approach that considers different driving styles to predict the trajectory of lead vehicles. In contrast to the emphasis on heterogeneity in the aforementioned works, our research focuses primarily on the prediction difficulty of the agents. For example, Makansi et al. [4] addressed the problem of long-tailed data distribution by exploiting the effects of feature embeddings. The use of contrastive learning in feature embeddings aggregates rare and difficult samples, contributing to improved final prediction performance. Qian et al. [11] introduced a teacher learning strategy based on the difficulty of scene prediction. They predefined the prediction difficulty of the scene and trained the proposed model by first training on easier samples and then on more difficult samples. However, both methods did not consider potential differences in difficulty between agents in multi-agent prediction scenarios. In this work, we designed a difficulty estimation module to estimate potential prediction difficulties in multi-agent prediction scenarios. The future trajectories of agents with lower prediction difficulties are integrated into the scene to enhance information.\nFuture Feature Enhancement. Compared to conventional pipelines, methods that enhance features using future information allow networks to incorporate future constraints and interactions. These methods can be broadly categorized into two approaches: intention and future trajectories. TNT [12] and PECNet [13] model vehicle intention as latent variables and generate different prediction trajectories conditioned on vehicle intentions, forming predicted trajectories under certain constraint conditions. GANet [14] and ADAPT [15] integrate and fuse vehicle intentions for future feature enhancement through feature interaction. Similarly, Prophnet [16] and FFINet [17] model intermediate latent variables in the form of trajectories, where the output of intermediate decoders is one"}, {"title": "III. METHOD", "content": "Recent works [10], [11] have been dedicated to associating similar scenes through contrastive learning or clustering, linking scenes based on their similarity, and enhancing features during inference according to the degree of scene similarity. We propose DGFNet, a network designed for multi-agent trajectory prediction using a Difficulty-Guided Feature Enhancement module for asynchronous interaction between historical trajectories and reliable future trajectories. As shown in Fig. 3, the architecture of DGFNet can be divided into three components: spatio-temporal feature encoding, feature interaction, and trajectory decoding.\n(1)Spatio-temporal Feature Extraction. This component extracts spatio-temporal features separately from agent-centric historical trajectory information and scene-centric information using both agent-centric and scene-centric approaches. (2)Feature Interaction. The difference with spatio-temporal feature extraction is that the feature extraction component is compatible for both scenarios. However, feature interaction is not. The feature interaction component is categorized into two types: multi-head attention module and multi-head attention module with edge features. The two interaction methods are respectively applicable to scene-centric and agent-centric scenario representations. (3)Trajectory Decoder. This component is divided into a difficulty-guided decoder and a final decoder. The difficulty-guided decoder operates during the intermediate process, while the final decoder outputs the final prediction results.\nB. Problem formulation\nBuilding on the majority of previous work, such as [1], [12], [22], [25], [29], we assume that upstream perceptual tasks can provide high-quality 2D trajectory tracking data for agents in a coordinate system. At the same time, localisation and mapping tasks can provide accurate self-vehicle positioning data and comprehensive HD map data for the urban environment. In other words, for $j$ agents within a given scene, we obtain $x$ and $y$ positions, denoted as $X_{-th:0}^{j}$, corresponding to the time stamp horizon $\\{-th, ..., 0, 1, ..., t_f\\}$. The trajectory prediction task consists of predicting the future trajectories $Y_{1:tf}$ of the agents by using the HD map information $M$ (including road coordinates, topological links, traffic lights and other relevant information) within the given scene and the historical trajectories output by the tracking task.\nIn accordance with the encoding methodology employed by VectorNet and LaneGCN, we represent the trajectory informa-"}, {"title": "C. Spatio-temporal feature extraction", "content": "In the problem formulation, we explain the process of representing agent and high-definition (HD) map data as vectors, establishing a corresponding mapping between continuous trajectories, map annotations, and sets of vectors. This vectorization method allows us to encode trajectory and map information as vectors. Following the trajectory feature extraction method of LaneGCN, our trajectory feature extraction module mainly uses 1D CNN and downsampling techniques to encode the historical trajectories of all vehicles in the scene, thus obtaining encoded historical trajectory features.\nFor map tensor, we use the PointNet [30] to encode lane nodes and structural information to obtain map features. It is worth mentioning that when processing scene-centric vector information, we do not mix vehicle historical trajectories and lane line information. Our view is that vehicle trajectories, as dynamic vectors, need to be distinguished from static maps to better capture the local motion characteristics of the vehicles.\nFor the extraction of historical trajectory features are described by this:\n$Z_i = Conv1d(Res1d(X_i))$ for $i = 0, 1, ..., N_{fpn} - 1$ (1)\n$X_i^* = Inter(Z_{i+1}, scale\\_factor = 2) + Z_i$ (2)\nafter Res1d and Conv1d encoding, feature alignment is performed by scaling and interpolation operations. For the agent-"}, {"title": "D. Feature interaction", "content": "Our intention was to utilize the global representation of scence-centric scenes to embed future trajectory features. Interestingly we found that even without any future enhancement, just concatenating the Actor features after the two interactions gives a better boost. Specific results can be seen later in the ablation experiments. Here we will present the two interaction methods: multi-head attention module and multi-head attention module with edge features, respectively.\nWe first introduce a generic feature interaction block. In contrast to the general method, which uses simple attention operations for feature interaction, we believe that multi-head attention mechanisms are often better suited to handle complex input sequences and capture a wider range of hierarchical and diverse information. We opt for Multi-Head Attention Blocks (MHAB) instead of the previously used simple attention operations, as used for the features [31]. Specifically, we use self-attention encoders and feedforward networks (FFN) for self-interaction and cross-attention encoders and FFN for cross-interaction:\nMHA(Q, K, V) = softmax($\\frac{QK^T}{\\sqrt{dim_k}}$)V (5)\n$Q, V \\in W^Q, W^k, W^v$(6)\nwhere MHA denotes the Multi-Head Attention operation. $Q$, $K$, $V$ are computed by linear projections $W_q$, $W_k$, $W_v$ applied to input vectors. The attention mechanism computes scaled dot-product attention using $Q$ and $K$, and applies it to $V$ after softmax normalization.\nThe feature interaction module for carrying edge features is described as follows:\nMHA(Q', K', V') = softmax($\\frac{Q'K'^T}{\\sqrt{dim_k}}$)V' (7)\n$Q', K',V' = Q\\oplus W^{edge^q}, K \\oplus W^{edge^k}, V \\oplus W^{edge^v}$(8)\nhere, $Q'$, $K'$, $V'$ are the query, key, and value tensors after linear transformations that incorporate edge features, and $edge'$ is the edge feature after linear transformation.\nFinally, the normalized output after attention and dropout application is:\nx = LayerNorm(x + Drop(MHA(Q', K', V'))) (9)"}, {"title": "E. Difficulty-guided decoder", "content": "To take advantage of the different prediction difficulties of different agents in the scenario, we developed the Difficulty Guided Feature Enhancement Module, which consists of a Difficulty Guided Trajectory Decoder and a Future Feature Interaction Module.\nTo capture features of plausible future trajectories in the scene, we introduce a Difficulty-Guided Decoder to obtain the future trajectories of agents that are relatively easier to predict. Firstly, we decode the actor features $\\hat{A}^{(k)}$ in Agent-centric, which have undergone multiple layers of feature interactions. Each agent receives six predicted trajectories. Following the ADAPT [15] trajectory decoder to get higher quality decoded trajectories by predicting endpoints for refinement. The decoding process can be expressed as follows:\n$\\hat{E} = MLPend(\\hat{A}^{(1)}) + MLPrefine(Cat[\\hat{Y}^{(1)}, \\hat{E}])$ (15)\n$P = Cat[MLP_{traj}(Cat[X^{(1)}, \\hat{E}]), \\hat{E}]$ (16)\nWhere endpoints $\\hat{E}$ are predicted and refined by concatenating with the agent features $X^{(1)}$, then the trajectories $P$ are predicted and merged with the refined endpoints.\nThe Argoverse 1 validation and train sets show that the predicted trajectories of most of the vehicles in the scenarios exhibit a high degree of concentration, with more than 90% of the samples of straight ahead trajectories in each case [32]. This suggests that the motion patterns of most agents can be easily captured, reflecting real-world traffic scenarios. In order to prevent higher difficulty prediction trajectories from entering the subsequent modules and generating cumulative errors, we introduced a difficulty masker to filter the initial prediction results. ScenceTransformer [33], inspired by recent approaches to language modeling, has pioneered the idea of using a masking strategy as a query to the model, enabling it to invoke a single model to predict agent behavior in multiple ways (marginal or joint prediction). This mechanism in this"}, {"title": "F. Future feature enhancement", "content": "For the reliable future trajectories we first flatten them by performing the flatten operation on multiple future trajectories for feature dimension alignment. Then future trajectory feature extraction is performed by a simple MLP layer. These two operations can be formulated as:\n$F = MLP(flatten(P_{masked}))$ (17)\nSubsequently, we interact the future trajectory feature $F$ with the agent-centric interactive actor feature $\\hat{A}^{(k)}$ via multi-head attention. This operation aims to impose certain constraints on the prediction of other agents through the reliable future trajectory features. Finally, we need to perform one last interaction using the original map features $M$. This step is crucial as it aims to refocus on the reachable lane lines of the map after imposing social constraints through the future trajectory features [34]. These two operations can be formulated as:\n$H = MHA_{AF}(\\hat{A}^{(k)}, F)$ (18)\n$\\hat{O} = MHA_{AM}(H, M)$ (19)"}, {"title": "G. Final prediction", "content": "We perform feature fusion operations on the final features $\\hat{O}$ and the scence-centric feature $X^{(1)}$, expanding the decoding dimension to 256. Our final predictor can generate multimodal trajectories for all agents in the scene and their corresponding probabilities in a single inference. This process can be described as follows:\n$A_{fusion} = Cat(\\hat{O}, X^{(1)})$ (20)\n$\\hat{E} = MLP_{end}(A_{fusion}) + MLP_{refine}(Cat[A_{fusion}, E])$ (21)\n$K = softmax(MLP_{cls}(Cat[A_{fusion}, E]))$ (22)\n$P = Cat[MLP_{traj}(cat[A_{fusion}, E]), E]$ (23)"}, {"title": "IV. EXPERIMENTS", "content": "Datasets. Our model DGFNet is tested and evaluated on a very challenging and widely used self-driving motion prediction dataset: the Argoverse 1&2 [47], [48]. Both motion prediction datasets provide agent tracking trajectories and semantically rich map information at a frequency of 10Hz over a specified time interval. The prediction task in Argoverse 1 is to predict trajectories for the next 3 seconds based on the previous 2 seconds of historical data. The dataset contains a total of 324,557 vehicle trajectories of interest extracted from over 1000 hours of driving. In contrast, Argoverse 2 requires a higher level of robustness and generalization of the predictive model, with 250,000 of the most challenging scenarios officially filtered from the self-driving test fleet. Argoverse 2 predicts the next 6 seconds from the first 5 seconds of historical trajectory data, and Argoverse 2 provides heading data for traffic targets as well as traffic target categories, in order to ensure fair comparisons between models, the dataset has official data partitioning and the test set is evaluated using the Eval online test server.\nEvaluation Metrics. We have adopted the standard testing and evaluation methodology used in motion prediction competitions to assess prediction performance. Key metrics for individual agents include Probabilistic minimum Final Displacement Error (p-minFDE), Minimum Final Displacement Error (minFDE), Minimum Average Displacement Error (minADE), Miss Rate (MR) and Drivable Area Compliance (DAC). Where p-minFDE, MR, and minFDE reflect the accuracy of the predicted endpoints, and minADE indicates the overall bias in the predicted trajectories. DAC, reflects the compliance of the predicted outcomes, which is incorporated into the bias when the predicted outcomes are outside of the drivable region.\nImplementation Details. We generate lane vectors for lanes that are more than 50 meters away from any available agent. The number of layers $l$and $k$ of interaction features are set to 3. All layers except the final decoder have 128 output feature channels. In addition, the weight parameters in the loss function are set to a=0.7, \u03b2=0.1 and \u5165=0.2. The hyperparameter $\u03c4$ in the difficulty masker was set to 5. The"}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel Difficulty-Guided Feature Enhancement Network (DGFNet) for muti-agent trajectory prediction. Distinguishing from general future enhancement networks, our model emphasizes filtering future trajectories by masking out and retaining only reliable future trajectories for feature enhancement, which greatly improves the prediction performance. Extensive experiments on the Argoverse 1&2 benchmarks show that our method outperforms most state-of-the-art methods in terms of prediction accuracy and real-time processing. In addition, we integrate agent-centric and scence-centric scene features in a concise and effective way, and demonstrate that the fusion of the two features is effective in improving the model prediction accuracy.\nEmulating the predictive habits of human drivers is an intriguing direction for future research. This involves emulating human strategies when encountering different vehicles and making effective assumptions in the presence of incomplete perceptual information. Furthermore, we will integrate Difficulty-Guided feature enhancement into other prediction frameworks to further validate the efficacy of this module in optimizing multi-agent trajectory prediction problems."}]}