{"title": "SignSpeak: Open-Source Time Series Classification for ASL Translation", "authors": ["Aditya Makkar", "Divya Makkar", "Aarav Patel", "Liam Hebert"], "abstract": "The lack of fluency in sign language remains a barrier to seamless communication\nfor hearing and speech-impaired communities. In this work, we propose a low-cost,\nreal-time ASL-to-speech translation glove and an exhaustive training dataset of\nsign language patterns. We then benchmarked this dataset with supervised learning\nmodels, such as LSTMs, GRUs and Transformers, where our best model achieved\n92% accuracy. The SignSpeak dataset has 7200 samples encompassing 36 classes\n(A-Z, 1-10) and aims to capture realistic signing patterns by using five low-cost flex\nsensors to measure finger positions at each time step at 36 Hz. Our open-source\ndataset, models and glove designs, provide an accurate and efficient ASL translator\nwhile maintaining cost-effectiveness, establishing a framework for future work to\nbuild on.", "sections": [{"title": "Introduction", "content": "American Sign Language (ASL) is the most prominent sign language in North America [1], yet as of\n2021, only 0.15% of Americans are fluent in it [2]. This low figure causes significant challenges for\nhearing and speech-impaired individuals, including limited access to education, opportunities and\nessential services, leading to isolation and depression[3].\nTo address these barriers, prior work using optical-based methods has shown strong results in\ntranslating images of ASL gestures to speech; however, they are limited in real-world applicability[4,\n5]. CNN and vision-based transformer models necessitate using a camera pointed at a user's hands\nwhile signing, which is impractical in many contexts. Additionally, the use of cameras also presents a\nprivacy risk by capturing the user and surrounding individuals while requiring considerable computing\nresources as frames must be sent to a server. This is infeasible and limits the scope of optical-based\nASL translation within a real-world context.\nSensor-based models using embedded devices have been introduced to treat ASL as a time-series\nmulti-label classification problem to address the limitations of optical systems. However, many of\nthese datasets are private [6, 7] and have not been trained on a well-practiced sign-based language\nsuch as ASL [8], limiting their applicability. To address this, we introduce SignSpeak, an open-source"}, {"title": "Related work", "content": "Previous work using a glove-based apparatus involves sensory devices such as flex sensors and inertial\nmeasurement units (IMUs). Amin et al. [6] utilize flex sensor gloves to capture 37 hand gestures\n(numbers 0-10 and letters A-Z) using MLPs achieving 97.6% accuracy. However, a fundamental flaw\nlimits real-world applicability as the measurements are static and recorded at only one point during\nthe gesture. This fails to account for ASL's dynamic nature since each sign is a sequence of motions\nthat must be continuously measured. Furthermore, the dataset is closed-source, prohibiting others\nfrom building on it.\nLee et al. [7] developed a glove taking continuous measurements of 6 inertial measurement units,\nincluding an accelerometer, gyroscope, and magnetometer. They report a 99.87% accuracy; however,\nthis study presents a drawback: each input is 10-15 seconds long and impractical for real-world\nsigning, which is performed at a significantly faster rate of 4 syllables per second [10]. Similar to the\nprevious work, the dataset was not released publicly.\nTan et al. [5] developed a 28-sensor glove which recorded 63 data channels to train an LSTM model.\nMultiple data channels and sensory equipment significantly increase the glove's cost, decreasing\nits affordability. In addition, less sensory equipment can produce similar results in commercial use.\nKr\u00e1lik and \u0160uppa [8] utilized a transformer architecture to achieve over 99% accuracy on a synthetic\nglove-collected gesture dataset, preventing its applicability to the ASL community.\nWe differ from previous work by introducing an open-source ASL dataset measuring 5 flex sensor\nchannels. It includes 200 samples for all alphanumeric classes, allowing for a cost-effective and\nresource-efficient glove with broad applicability for the ASL community."}, {"title": "Methodology", "content": "For this study, a glove was constructed with five parallel flex sensors\non each finger in series with a 10,000\u03a9 resistor. 5V were applied\nand measured across each sensor with an Arduino MEGA 2560. We\nrecorded each feature within the standard Arduino 10-bit range of [0,\n1023]. Each gesture was recorded at 36 Hz while ensuring that the\nsum of all flex sensor measurements was below 5000 or 24.4V. This\nvalue was experimentally determined, and indicates that the fingers\nwere flexed (the sign being performed), allowing for intentional data\ncollection. We retain all gesture recordings between 1.38 and 2.22\nseconds (50-time to 80-time) steps to ensure that accidental gestures\nwere not added and that the gestures reflect realistic signing patterns."}, {"title": "Model architecture", "content": "Each gesture recording contains C = 5 channels and has a maximum\ntime dimension of T = 79 with all input features 0-padded to ensure\na consistent batch size. We benchmarked RNN and Transformer-\nbased time series models on the SignSpeak dataset. In particular,\nwe evaluated a 2-layer LSTM[11] and a 2-layer GRU[12] model,"}, {"title": null, "content": "yo = LSTM(x) \nOutput = SOFTMAX(MLP(MLP(LSTM(yo)^T)))\nFor the Stacked LSTM model, refer to eq. 1 and 2 where x \u2208 R^{T\u00d7C} and y_{output} \u2208 R^{classes}. Toro-\nOssaba et al. [9] presented a dense-LSTM network for EMG-ASL classification, showing that a MLP\nprojection before the RNN could achieve state-of-the-art results. This work used a 2-layer MLP\nSoftmax classifier following the dense-RNN unit."}, {"title": null, "content": "yo = [MLP(x^(0)), MLP(x^{(1)}), . . ., MLP(x^{(T)})]\nOutput = SOFTMAX(MLP(MLP(LSTM(yo)^T)))\nEq. 3 and 4 describe the dense-LSTM. For a dense-stacked RNN, eq. 4 is modified by composing the\nRNN function with itself for the input yo. Each RNN gate used a Sigmoid activation, while MLPs\nused a Tanh activation. The hidden size of the RNN cells was hRNN = 64, and the dense and/or\noutput MLP was hMLP = 128.\nIn recent literature, transformers have matched or exceeded SOTA benchmarks in time-series clas-\nsification. Kr\u00e1lik and \u0160uppa [8] WaveGlove Encoder, based on Transformers [14], have surpassed\nprevious SOTA architectures on this task [8]. Inspired by this architecture, we benchmark a slightly\nmodified version of WaveGlove on SignSpeak, adding a classification token ([CLS]) to the start of the\ninput as done with BERT[15]. The input is passed through a learnable embedding and positional em-\nbedding table with the projected input being fed into an Encoder [14] with layer normalization before\nthe self-attention and MLPs, as described by Dosovitskiy et al. [16]. The input format x \u2208 R^{T\u00d7C}\nrepresents 5-flex sensor channels across time T, before being projected into a dimension D = 32\nwith the sequential nature encoded by the positional embedding. We utilized the GELU activation\nfunction [17] and the number of layers was L = 5. All Encoder and RNN parameters were found\nthrough a Cartesian product hyperparameter sweep."}, {"title": null, "content": "yo = [x_{class}, x_{Emb}] + E_{pos_emb}\ny_{1} = Encoder(y\u0131), where l = 1, 2, . . ., L\nOutput = SOFTMAX(MLP(LN(y_{L}^{(0)})))\nThe encoder is described by eq.(5) - (7), where E_{emb} \u2208 R^{C\u00d7D}, E_{pos_emb} \u2208 R^{(T+1)\u00d7D}."}, {"title": "Results", "content": "All models were trained with AdamW [18], with B\u2081 = 0.9, B2 = 0.999, a weight decay of 0.01, and\na plateau learning rate decay on validation loss with a patience of 20 epochs of 0.5 starting from 0.001\nuntil a minimum learning rate of 0.0001. RNNs were trained with a batch size of 64 for 15 minutes on\nan M2, and the Encoder was trained with a batch size of 256 for 15 minutes on a T4 GPU. All models\nused a 0.2 dropout probability. The metrics used to evaluate all models were categorical accuracy\nand the F1-Score. We utilized a stratified 5-fold validation and reported the standard deviation and\naverage result of the held-out folds.\nThe results in Table 1 indicate methods on private datasets do not generalize to the SignSpeak dataset.\nThis may be due to a reduction in the number of data channels. This can be seen with a model such\nas the Transformer where a lack of data channels reduces its performance. In particular, we found\nthat simple models such as a stacked GRU perform the best, whereas models such as a dense LSTM\nproposed by Toro-Ossaba et al. [9] do not achieve near state-of-the-art results. We believe that the\npotential for Transformer-based architectures can be unlocked with more training data, which can\nthen be further fine-tuned on the SignSpeak dataset. Our leading RNN and Encoder models still\nmaintain above 99.5% traditional accuracy, demonstrating performance on par with previous studies.\nAdditionally, the Transformer architecture has\nthe largest difference between F1-score and ac-\ncuracy, indicating a bias towards certain classes.\nFigure 2 displays the confusion matrix and it\nis evident the low accuracy is due to specific\nclasses such as 'E' and 'L'. Specifically, the En-\ncoder incorrectly predicts 'L' 36% of the time\nwhen the actual label is 'E'. Additionally in\nASL, these letters do not share the same features;\n'E' is very similar to a letter such as 'A'. This\nindicates the model's over-predicts between cer-\ntain classes and could be outlier patterns in the\ndataset. Analyzing this class with stacked GRU\nand LSTM models, they predict 'L' instead of\n'E' 16% and 13% of the time, respectively. This\nindicates it is a learned bias of an Encoder model\nbut over-fitting is still present in all models."}, {"title": "Future work", "content": "The models presented in this study only re-\nquired a moderate amount of computing power\nto achieve 92% accuracy. In the future, lever-\naging more powerful computing resources can\nenable the implementation of larger-scale archi-\ntectures to further enhance performance. Ad-\nditionally, the gestures chosen in this dataset (alphanumeric classes) reflect an extremely limited\nsubsection of real-world ASL; thus, future work is aimed at expanding the dataset by collecting data\nand creating classes for phrases and actions that resemble daily communication making the product\nviable for commercial use. Lastly, while our measurements were recorded at 36 Hz, which is slower\nthan average ASL signing rates, we anticipate that using an improved MCU will allow us to increase\nthis frequency to 200 Hz, aligning with more realistic signing speeds [10]. These advancements\nwill expand on our existing research and contribute to a more refined product that can facilitate the\nintegration of hearing and speech-impaired individuals into society."}, {"title": "Conclusion", "content": "In this study, we presented SignSpeak, an open-source dataset collected using a custom low-cost glove\narchitecture benchmarked on time-series classification models to mimic real-time ASL translation.\nWe found that a stacked GRU achieves the strongest results on categorical accuracy. SignSpeak\nbenefits speech and hearing-impaired communities by providing a way to benchmark models on a\nuniversal dataset. Our work on Signspeak can provide a foundation for researchers to build upon our\nopen-source dataset, leveraging supervised learning techniques to deliver assistive and accessible\ntechnology to communities in need."}]}