{"title": "SignSpeak: Open-Source Time Series Classification for ASL Translation", "authors": ["Aditya Makkar", "Divya Makkar", "Aarav Patel", "Liam Hebert"], "abstract": "The lack of fluency in sign language remains a barrier to seamless communication\nfor hearing and speech-impaired communities. In this work, we propose a low-cost,\nreal-time ASL-to-speech translation glove and an exhaustive training dataset of\nsign language patterns. We then benchmarked this dataset with supervised learning\nmodels, such as LSTMs, GRUs and Transformers, where our best model achieved\n92% accuracy. The SignSpeak dataset has 7200 samples encompassing 36 classes\n(A-Z, 1-10) and aims to capture realistic signing patterns by using five low-cost flex\nsensors to measure finger positions at each time step at 36 Hz. Our open-source\ndataset, models and glove designs, provide an accurate and efficient ASL translator\nwhile maintaining cost-effectiveness, establishing a framework for future work to\nbuild on.", "sections": [{"title": "Introduction", "content": "American Sign Language (ASL) is the most prominent sign language in North America [1], yet as of\n2021, only 0.15% of Americans are fluent in it [2]. This low figure causes significant challenges for\nhearing and speech-impaired individuals, including limited access to education, opportunities and\nessential services, leading to isolation and depression[3].\nTo address these barriers, prior work using optical-based methods has shown strong results in\ntranslating images of ASL gestures to speech; however, they are limited in real-world applicability[4,\n5]. CNN and vision-based transformer models necessitate using a camera pointed at a user's hands\nwhile signing, which is impractical in many contexts. Additionally, the use of cameras also presents a\nprivacy risk by capturing the user and surrounding individuals while requiring considerable computing\nresources as frames must be sent to a server. This is infeasible and limits the scope of optical-based\nASL translation within a real-world context.\nSensor-based models using embedded devices have been introduced to treat ASL as a time-series\nmulti-label classification problem to address the limitations of optical systems. However, many of\nthese datasets are private [6, 7] and have not been trained on a well-practiced sign-based language\nsuch as ASL [8], limiting their applicability. To address this, we introduce SignSpeak, an open-source"}, {"title": "Related work", "content": "Previous work using a glove-based apparatus involves sensory devices such as flex sensors and inertial\nmeasurement units (IMUs). Amin et al. [6] utilize flex sensor gloves to capture 37 hand gestures\n(numbers 0-10 and letters A-Z) using MLPs achieving 97.6% accuracy. However, a fundamental flaw\nlimits real-world applicability as the measurements are static and recorded at only one point during\nthe gesture. This fails to account for ASL's dynamic nature since each sign is a sequence of motions\nthat must be continuously measured. Furthermore, the dataset is closed-source, prohibiting others\nfrom building on it.\nLee et al. [7] developed a glove taking continuous measurements of 6 inertial measurement units,\nincluding an accelerometer, gyroscope, and magnetometer. They report a 99.87% accuracy; however,\nthis study presents a drawback: each input is 10-15 seconds long and impractical for real-world\nsigning, which is performed at a significantly faster rate of 4 syllables per second [10]. Similar to the\nprevious work, the dataset was not released publicly.\nTan et al. [5] developed a 28-sensor glove which recorded 63 data channels to train an LSTM model.2\nMultiple data channels and sensory equipment significantly increase the glove's cost, decreasing\nits affordability. In addition, less sensory equipment can produce similar results in commercial use.\nKr\u00e1lik and \u0160uppa [8] utilized a transformer architecture to achieve over 99% accuracy on a synthetic\nglove-collected gesture dataset, preventing its applicability to the ASL community.\nWe differ from previous work by introducing an open-source ASL dataset measuring 5 flex sensor\nchannels. It includes 200 samples for all alphanumeric classes, allowing for a cost-effective and\nresource-efficient glove with broad applicability for the ASL community."}, {"title": "Methodology", "content": ""}, {"title": "Data collection", "content": "For this study, a glove was constructed with five parallel flex sensors\non each finger in series with a 10,000\u03a9 resistor. 5V were applied\nand measured across each sensor with an Arduino MEGA 2560. We\nrecorded each feature within the standard Arduino 10-bit range of [0,\n1023]. Each gesture was recorded at 36 Hz while ensuring that the\nsum of all flex sensor measurements was below 5000 or 24.4V. This\nvalue was experimentally determined, and indicates that the fingers\nwere flexed (the sign being performed), allowing for intentional data\ncollection. We retain all gesture recordings between 1.38 and 2.22\nseconds (50-time to 80-time) steps to ensure that accidental gestures\nwere not added and that the gestures reflect realistic signing patterns."}, {"title": "Model architecture", "content": "Each gesture recording contains C = 5 channels and has a maximum\ntime dimension of T = 79 with all input features 0-padded to ensure\na consistent batch size. We benchmarked RNN and Transformer-\nbased time series models on the SignSpeak dataset. In particular,\nwe evaluated a 2-layer LSTM[11] and a 2-layer GRU[12] model,"}, {"title": "Results", "content": "All models were trained with AdamW [18], with \u03b2\u2081 = 0.9, \u03b2\u2082 = 0.999, a weight decay of 0.01, and\na plateau learning rate decay on validation loss with a patience of 20 epochs of 0.5 starting from 0.001\nuntil a minimum learning rate of 0.0001. RNNs were trained with a batch size of 64 for 15 minutes on\nan M2, and the Encoder was trained with a batch size of 256 for 15 minutes on a T4 GPU. All models"}, {"title": "Future work", "content": "The models presented in this study only re-\nquired a moderate amount of computing power\nto achieve 92% accuracy. In the future, lever-\naging more powerful computing resources can\nenable the implementation of larger-scale archi-\ntectures to further enhance performance. Ad-\nditionally, the gestures chosen in this dataset (alphanumeric classes) reflect an extremely limited\nsubsection of real-world ASL; thus, future work is aimed at expanding the dataset by collecting data\nand creating classes for phrases and actions that resemble daily communication making the product\nviable for commercial use. Lastly, while our measurements were recorded at 36 Hz, which is slower\nthan average ASL signing rates, we anticipate that using an improved MCU will allow us to increase\nthis frequency to 200 Hz, aligning with more realistic signing speeds [10]. These advancements\nwill expand on our existing research and contribute to a more refined product that can facilitate the\nintegration of hearing and speech-impaired individuals into society."}, {"title": "Conclusion", "content": "In this study, we presented SignSpeak, an open-source dataset collected using a custom low-cost glove\narchitecture benchmarked on time-series classification models to mimic real-time ASL translation.\nWe found that a stacked GRU achieves the strongest results on categorical accuracy. SignSpeak\nbenefits speech and hearing-impaired communities by providing a way to benchmark models on a\nuniversal dataset. Our work on Signspeak can provide a foundation for researchers to build upon our\nopen-source dataset, leveraging supervised learning techniques to deliver assistive and accessible\ntechnology to communities in need."}], "equations": ["yo = LSTM(x)", "Youtput = SOFTMAX(MLP(MLP(LSTM(yo)(T))))", "yo = [MLP(x(0)), MLP(x(\u00b9)), . . ., MLP(x(T))]", "Youtput = SOFTMAX(MLP(MLP(LSTM(yo)(T))))", "yo = [xclass, xEemb] + Epos_emb", "y1 = Encoder(y\u0131), where l = 1, 2, . . ., L", "Youtput = SOFTMAX(MLP(LN(yL(0)))"]}