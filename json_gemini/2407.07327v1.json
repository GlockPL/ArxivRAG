{"title": "Fuse, Reason and Verify: Geometry Problem Solving with Parsed Clauses from Diagram", "authors": ["Ming-Liang Zhang", "Zhong-Zhi Li", "Fei Yin", "Liang Lin", "Cheng-Lin Liu"], "abstract": "Geometry problem solving (GPS) requires capacities of multi-modal understanding, multi-hop reasoning and theorem knowledge application. In this paper, we propose a neural-symbolic model for plane geometry problem solving (PGPS), named PGPSNet-v2, with three key steps: modal fusion, reasoning process and knowledge verification. In modal fusion, we leverage textual clauses to express fine-grained structural and semantic content of geometry diagram, and fuse diagram with textual problem efficiently through structural-semantic pre-training. For reasoning, we design an explicable solution program to describe the geometric reasoning process, and employ a self-limited decoder to generate solution program autoregressively. To reduce solution errors, a multi-level theorem verifier is proposed to eliminate solutions that do not match geometric principles, alleviating the hallucination of the neural model. We also construct a large-scale geometry problem dataset called PGPS9K, containing fine-grained annotations of textual clauses, solution program and involved knowledge tuples. Extensive experiments on datasets Geometry3K and PGPS9K show that our PGPSNet solver outperforms existing symbolic and neural solvers in GPS performance, while maintaining good explainability and reliability, and the solver components (fusion, reasoning, verification) are all justified effective.", "sections": [{"title": "I. INTRODUCTION", "content": "AUTOMATIC geometry problem solving (GPS) has been long-standing in artificial intelligence (AI) field [1]\u2013[4], and is drawring increasing attention in recent years [5]\u2013[8]. A geometry problem usually consists of a geometry diagram and a textual problem, forming a multi-modal problem. The textual problem describes the geometric conditions and sets the reasoning target in natural language, while the geometry diagram displays the spatial structure and additional geometric conditions in vision. Solving geometry problems requires geometry diagram and textual problem understanding and multi-step reasoning incorporating geometry knowledge. Due to the diverse and irregular contents of textual problem and geoetry diagram, GPS is widely recognized as a vital testbed [9] for evaluating the high-level multi-modal reasoning capability of AI.\nThe challenges of GPS lie in its three main steps: modal fusion, reasoning and knowledge verification. Modal fusion is to represent and utilize multi-modal content from geometry diagram and textual problem. Fusing two distinct modalities is difficult, where the textual problem expresses geometric information by text while the geometry diagram conveys geometric relationships via visual layout. Reasoning is sophisticated involving state transition and path search [10], [11]. State transition, embedded in path search, applies geometric pattern matching and geometry theorem to move forward the state of solution. Imitating from human problem-solving, knowledge verification plays a key role in checking the solving process and correcting solution errors.\nTo address the above three challenges, several geometric solvers have been proposed, which can be roughly categorized into two groups: symbolic solvers and neural solvers. As summarized in Figure 1 (a), symbolic solvers [3], [5], [12] parse the diagram and textual problem into a unified formal language, and then perform symbolic reasoning through path search and condition matching according to geometry knowledge. During the process, new conditional states are generated progressively until the search target is achieved. Although symbolic solvers have strong interpretability, they are designed with many human efforts, and may exhibit slow problem-solving speed due to redundant steps. In contrast, neural solvers [7], [13], [14] use neural encoders to extract features of diagram and textual problem, and embed them into a unified feature space by the joint module, as shown in Figure 1 (b). The neural solver is trained end-to-end in a data-driven way and outputs sequential solutions finally. Despite the promise of neural solvers, they still struggle in modal fusion and knowledge interpretability, resulting in a certain performance gap compared with symbolic solvers [5], [15]. Regarding modal fusion, existing neural solvers, adopting similar frameworks of general vision-language tasks applied for natural images [16]\u2013[18], cannot utilize the structural and semantic information in diagram explicitly and efficiently. The inherent black-box characteristic of neural networks also hinders the explainability and reliability of solution.\nConsidering the pros and cons of current geometric solvers, we propose a neural-symbolic solver, called PGPSNet-v2, as shown in Figure 1 (c). PGPSNet-v2 is the improved version of our previously proposed neural solver PGPSNet [19]. It consists of three modules: fusion, reason and verifier, to enhance the performance from three aspects of modal fusion, reasoning and knowledge verification, respectively. The fusion module inputs not only geometry diagram and textual problem, but also textual clauses parsed from geometry diagram. Multi-modal fusion is accomplished with the strategy of structural and semantic pre-training, so as to model global structure and context in unified neural form. The reason module applies a self-limited decoder to generate interpretable reasoning sequences in an autoregressive manner. The verifier validates the generated solution sequences in three levels: form, calculability and semantic, through knowledge tuple search and matching. By the way of verification in post-processing, PGPSNet-v2 excludes hallucinations that do not match geometric principles and thus promotes the reliability of solving process.\nTo facilitate GPS research, we also build and release a large-scale GPS dataset named PGPS9K, containing 9,021 plane geometry problems, each paired with a textual problem and a geometry diagram. PGPS9K is more complete compared with existing GPS datasets [3], [5], [6] because it offers both fine-grained annotations of textual clauses from diagram and solution programs, and corresponding theorem knowledge base and program executor. Textual clauses, including structural clauses and semantic clauses, possess highly syntactic and less redundant information. Given the complexity of GPS, problem-solving procedure of PGPS9K is designed as a solution program based on geometry theorems, wherein each step denotes an application of a theorem (axiom), instead of the fundamental arithmetic operations used in math word problem (MWP) datasets [20], [21]. Our solution program carries richer geometry knowledge, better interpretability and shorter sequence length. The theorem knowledge base, stored in the form of knowledge tuples with operator, operands, theorem formula and semantic rules, serve the verifier to validate the solution program. Responding to the solution program, the program executor realizes symbolic algebraic calculation according to theorem formulas and returns numerical answer. In addition, we adopt five strategies of problem augmentation based on the equivalence of geometric representation, to increase the problem diversity further and embed representation knowledge simultaneously. Extensive experiments on PGPS9K and Geometry3K [5] datasets demonstrate that PGPSNet-v2 achieves remarkable improvement of GPS performance, exceeding existing symbolic solvers and neural solvers.\nIn summary, our main contributions are as follows:\n\u2022\tWe propose a neural-symbolic geometric solver PGPSNet-v2 based on three core steps: fusion, reasoning and verification.\n\u2022\tWe construct a large-scale plane geometry problem dataset PGPS9K with fine-grained annotations and paired with theorem knowledge tuples.\n\u2022\tThe proposed PGPSNet-v2 solver generates explainable and reliable solution process, thereby yielding superior GPS performance."}, {"title": "II. RELATED WORK", "content": "Generalized GPS problems can be categorized into two types: geometric theorem proving and geometric numerical calculation. Early efforts in GPS were primarily focused on the automatic proving of geometric theorems [8], [12], [30], and the geometric numerical calculation [3], [31] started to gain momentum in recent years as new datasets and methods are presented. In this paper, we concentrate on the geometric numerical calculation and review the related works on two categories: symbolic solvers and neural solvers.\nSymbolic solvers [3], [5], [8], [32] typically employ symbolic-based computation and rule-based deduction to solve geometry problems, featuring explicit and clear steps in the solution and reasoning process. GEOS [3], [33] constructed geometric structures through a graphic parser, utilized heuristic search to expand unknown conditions, and matched option conditions to obtain the solution results. Inter-GPS [5] leveraged the diagram parser and text parser to unify modalities into formal language, and used the symbolic solver equipped with three search strategies to search and generate solutions iteratively. GeoDRL [32] enhanced the search strategy of Inter-GPS by taking GPS as a Markov decision process, and employed logical graph deduction and deep reinforcement learning methods to optimize the geometric reasoning. Nevertheless, the symbolic solvers are designed carefully with complex rules, and hard to extend to other diverse domains of geometry problems.\nNeural solvers [6], [7], [14], treating GPS as a special vision-language task, rely on the neural network for end-to-end learning and reasoning, and design a solution sequence to represent the solution process. NGS [6] and Geoformer [14] used auxiliary self-supervised tasks such as location prediction, elements prediction and knowledge classification to boost cross-modal semantic representation. DPE-NGS [13] boosted the NGS model and embedded a bidirectional parallel text encoder (DPE) to make long-text question encoding more efficient. SCA-GPS [7] tried to align character in text and diagram and enhanced the diagram understanding through multi-label classification and masked image modeling pre-training. However, current neural solvers are still coarse-grained in modal understanding and fusion especially for geometry diagrams, and not trustworthy for solutions with massive hallucinations.\nOur proposed PGPSNet-v2 is an improved version of neural sovler, combining modal fusion, reasoning process and knowledge verification to overcome the shortages of existing methods. It combines the strengths of neural sovler and symbolic solver, thus can be viewed as a neural-symbolic solver. It also utilizes geometry knowledge in both implicit and explicit ways to improve the interpretability and reliability of solution."}, {"title": "III. PGPS9K DATASET", "content": "Most existing datasets of GPS, as recorded in Table I, either have a small data size, suitable only for rule-based symbolic solvers, or own coarse-grained annotations, neglecting the rich information in geometry diagram. To promote the development of GPS field, we construct a large-scale GPS dataset, named PGPS9K\u00b9, annotated with fine-grained diagram labels (elements and relationships) and interpretable solving solution, along with the corresponding theorem knowledge base and program executor. To the best of our knowledge, PGPS9K is the largest and most comprehensive GPS dataset up to date. It consists of 9,021 textual problems illustrated with 4,000 non-repetitive geometry diagrams. Among them, 2,891 textual problems and 1,738 geometry diagrams were selected from the Geometry3K dataset [5], while the remaining samples were collected from five geometry textbooks\u00b2 across 6-12 grades. The samples of PGPS9K are categorized into 30 problem types based on geometry knowledge points, covering almost all types of plane geometry problems. More statistical details are exhibited in Appendix A.\nFurthermore, as shown in Figure 2, PGPS9K possesses five properties, enabling it focus on the challenges in geometric reasoning and alleviate the bias introduced by the text [35], [36]. The properties are: (1) Theorem-based: In the GPS process, it is necessary to use geometric theorem or axiom knowledge to carry out algebraic calculation, and finally give the numerical result; (2) Diagram-dependent: More than 90% of problems must be solved in conjunction with diagram, because necessary conditions such as numerical content and geometric structure are presented by visual diagram instead of textual problem; (3) Abstract: Geometry diagram is composed of basic geometric primitives (point, line, circle) and non-geometric primitives (text, symbol). No complex semantic scenarios are involved in textual problem except abstract geometric conditions; (4) Fine-grained: Problems with the same diagram may vary in conditions or targets. Slight distinctions in textual problems usually lead to completely different solutions; (5) Condition-redundancy: Some conditions in textual problem or diagram are not necessarily used in GPS. The statistical results on PGPS9K dataset show that an average of 1.9 conditions are not used for GPS, 42% of problems have redundant conditions, and 15% of problems have 3 or more unused conditions."}, {"title": "B. Annotation and Description", "content": "The diagrams employ primitive-level annotation consistent with the geometry diagram parsing [37], containing geometric/non-geometric primitives as well as relationships among primitives in tuple form. The annotated primitives and relationships can be transformed into two fundamental types of textual clauses: structural clauses (three types) and semantic clauses (six types), via simple clause templates matching. The structural clauses depict the connectivity relationships among geometric primitives, such as points on a line or a circle, wherein points are arranged in a certain order. The connectivity relationships reveal the most fundamental geometric structural connections, often depicted in diagram but frequently omitted in textual problem. The semantic clauses articulate the basic relationships between geometric primitives and non-geometric primitives in natural language. The relationships corresponding to semantic clauses are essential components for GPS, complementing with textual problem. More details about textual clauses are given in Appendix B. It is worth mentioning that the definition of textual clauses remains open and extensible. The overarching design principle is to describe the complete structural-semantic information of diagram for assisting GPS.\nThe problem-solving procedure of PGPS9K is designed as a solution program composed of multiple deductive steps. The command set of solution program consists of 34 operators OP and 55 operands PN, wherein operands encompass 11 problem variables N (appearing in textual problem or semantic clauses), 7 intermediate variables V (generated during solving process), 26 augments ARG (letter unknowns), and 11 constants C. As shown in Figure 3, one solving step is formed by one operator and multiple operands, involving a geometric theorem or axiom, with relevant operands arranged in the semantic order of theorem formula. For example, within a right triangle, the Geometric Mean Theorem reveals the length relation between altitude c and two segments a, b into which the altitude divides the hypotenuse, with the theorem formula \\(a * b = c^2\\), so we express it as \u201cGeo_Mean(a,b,c)\u201d. In comparison to other annotation schemes [6], [21], [38], our annotation omits basic arithmetic operations such as +, -, *, /, instead replacing them with theorem operations, exhibiting advantages in terms of structure, knowledge guidance and interpretability (see Figure 3). It makes the solving procedure more concise and mitigates the difficulty of model learning. Furthermore, for the first time, we introduce the intermediate variable V as unknown variables within steps but also as transfer variables cross steps, to unify forward and backward computation operations. For instance, in the Pythagorean Theorem, \u201cGougu(V,*,*)\u201d and \u201cGougu(*,*,V)\u201d can be set to solve the length of legs and hypotenuse, respectively. More details of solution program are demonstrated in Appendix C."}, {"title": "C. Knowledge Base and Program Executor", "content": "For knowledge matching of geometry problem, PGPS9K dataset is equipped with the knowledge base of plane geometry theorems expressed in the form of knowledge tuples. One knowledge tuple consists of four elements: operator, operands, theorem formula and semantic rules, as displayed in Table II. The operators are abbreviations of theorems or axioms of geometry, totally 34 types. The operands are arranged according to the semantic order of theorem formula, and several theorem operands can be expressed in multiple ways, such as \"Ratio\" (scale ratio), \u201cCircle_R_Circum\u201d (circumference of circle or arc), \u201cCircle_D_Area\u201d (area of circle or sector), etc. The formulas of theorems are given and the relevant semantic rules are defined in tuples. Here are three specific examples of knowledge tuple:\n\u2022\t\u201c(Gougu, (a,b,c), \\(a^2 + b^2 = c^2\\), \u2026)\u201d is the knowledge tuple corresponding to the Pythagorean Theorem, where (a,b,c) corresponds to two right legs and hypotenuses, the theorem formula is \\(a^2 + b^2 = c^2\\) and its semantic rules are a + b > c, 0 < a < c, 0 < b < c.\n\u2022\t\u201c(Gcos, (a,b,c), \\(cos(c) = a/b\\), \u2026)\u201d is the knowledge tuple corresponding to the Cosine Law, where (a,b,c) corresponds to two sides and an angle respectively, the theorem formula is \\(cos(c) = a/b\\), the defined semantic rules are 0 < c < 90, 0 < a < b, and in a right triangle, a and b are adjacent sides and b is the hypotenuse.\n\u2022\t\u201c(Kite_Area, (a,b,c), \\(a * b/2 = c\\), \u2026)\u201d is the knowledge tuple of kite-shaped area, where (a,b,c) corresponds to two diagonal sides and area respectively, the theorem formula is \\(a * b/2 = c\\), and the semantic rules are a, b, c > 0 and corresponding lines of a and b are intersect.\nGPS not only requires geometric reasoning but also involves algebraic operations, such as solving linear and nonlinear equations, as listed in Table II. Our program executor is constructed in conjunction with solution program and capable of executing solution programs following theorem formulas. It is built upon the SymPy library of Python, enabling execution of symbolic algebraic operations for one or multiple unknowns, while recording values of all variables throughout the solving process. In case of reasoning steps that cannot proceed, the executor provides corresponding explanatory prompts. For those challenging steps, particularly those involving nonlinear symbolic operations, the executor sets a time threshold to terminate computation proactively, avoiding affecting the operation of whole system.\nTo sum up, the knowledge base stored via knowledge theorem tuples can express theorem knowledge explicitly, and the developed program executor exhibits enhanced functionality, adept at handling various algebraic operations related to geometry problem. As a result, they collectively establish a robust groundwork for the verifier of PGPSNet-v2 solver."}, {"title": "D. Problem Augmentation", "content": "Despite PGPS9K being the largest and highest-quality GPS dataset to date, it still fails to adequately fulfill the learning requirements of neural solvers due to the large variety of geometry problems. Therefore, we adopt five problem augmentation strategies based on the diversity and equivalence of geometric representations to further expand the richness of PGPS9K:\n\u2022\tToken Replacement: The replaceable tokens include three types: points, angle IDs and arguments. Once a token is altered, all identical tokens in both textual clauses and textual problem should be uniformly replaced. For example, if point B is replaced with point V, the new text would be: \u201cline V C D\u201d, \u201cA lieson E V D\u201d, \u201cVD IEA on C\u201d and \u201cFind VD\u201d.\n\u2022\tConnection Rotation: By changing the order of points, the connection relationships within the structural clauses can be redefined. For instance, by adjusting the left-right order of points on line BD, \u201cline B C D\u201d is equivalent to \u201cline D C B\u201d; altering the clockwise order of points on circle A, \u201cA lieson EB D\u201d and \u201cA lieson E D B\u201d are equivalent.\n\u2022\tRepresentation Transposition: The geometric primitives, e.g., lines, angles and arcs, have multiple equivalent representations. For example, \u201cEA = AE\u201d, \u201c\u2220STR = \u2220RTS\u201d, \u201cEF = FE\u201d. Randomly transposition of geometric primitive representations increases the diversity of problem representations.\n\u2022\tClauses Shuffle: The relative order of semantic clauses is randomly shuffled to generate new variable IDs, while modifying corresponding solution program. For example, when the semantic clauses are adjusted to \u201cCA = 3(N0), BD I EA on C, EC = 2(N1)\u201d, the solution program is adjusted to \u201cSum N0 N1 V0 Gougu N0 V1 V0 Multiple V1 C2 V2 Get V2\u201d.\n\u2022\tDiagram Flip: The visual texts in diagram could be largely disregarded as they have been parsed into the fine-grained textual clauses. Therefore, a diagram that are flipped or rotated is considered equivalent to the original diagram, but this could enhance the multiplicity of global visual representation.\nIn conclusion, these five augmentation strategies are mutually independent yet can be synergistically integrated. The substantial number of samples generated by problem augmentation endow the neural solver with the primary geometric representation knowledge, thereby facilitating the high-level geometric reasoning."}, {"title": "IV. PGPSNET-V2 SOLVER", "content": "We begin by presenting the formal definition of GPS task. For a given geometry problem P, comprising a geometry diagram D and a textual problem Tprob, the objective is to solve the problem and generate the solution S, formulated as P = {D,Tprob} \u2192 S. In our work, GPS process is refined into three core steps: fusion, reasoning and verification, and it is re-expressed as:\nFusion\n\\(P\\overset{Fusion}{\\rightarrow} P'\\overset{Reasoning}{\\rightarrow} {S}\\overset{Verification}{\\rightarrow} {S'},\\)\n(1)\nwhere P' is the new problem representation after modal fusion with P; Through neural reasoning for P', it acquires a list of solution candidates {S}; The solver gets the final solution S' via knowledge verification to {S}.\nImproved on above three core steps, we propose a neural-symbolic geometric solver, called PGPSNet-v2, which is extended from our previous neural solver PGPSNet [19] by adding a knowledge verifier, as displayed in Figure 4. The inputs of PGPSNet-v2 include not only the geometry diagram image D and the textual problem Tprob, but also the structural clauses Tstru and the semantic clauses Tsem parsed from the geometry diagram. So, the problem text is expanded to be expressed as \\(T = {T_{stru}, T_{sem}, T_{prob}} = {t_j}_{j=1}^{N_T}\\) after text concatenation, where NT is the text token number. In the modal fusion phase, the geometry diagram is encoded by a convolutional neural network (CNN) module and then flattened into a feature sequence \\(D' = {h_i}_{i=1}^{N_D}\\), where ND is the diagram token number. In parallel, the problem text undergoes fused encoding through the structural-semantic pre-trained language model and the bidirectional GRU encoder in turn, and obtains a feature sequence \\(T' = {h_i}_{i=1}^{N_T}\\). During the reasoning stage, two types of modal tokens, concatenated and unified as \\(P' = {D', T'} = {h_i}_{i=1}^{N_D + N_T}\\), are fed into the self-limited GRU decoder to perform geometric reasoning, and the decoder generates corresponding solution candidates represented by multi-step program sequences \\(S = {S_i}_{i=1}^{N_K} = {s_j}_{j=1}^{N_M}\\), where Nk and NM are the numbers of solving steps and program tokens, respectively. As to verification, combined with the geometry theorem knowledge base and the program executor, the verifier validates all the solution candidates and determines the final solution program S' and its numerical answer. The key modules of pre-training, decoder and verifier are detailed in the following."}, {"title": "B. Structural-Semantic Pre-training", "content": "Structural-semantic pre-training plays a critical role in the modal fusion, by unifying textual clauses from diagram with the textual problem. While the textual clauses delineate fine-grained structural and semantic information extracted from diagram, they remain at low level, lacking overall coherence and contextual connections. Furthermore, verbose, fragmented and disorganized text continues to pose significant challenge to modal fusion and semantic structure comprehension. Inspired by pre-trained language models, as illustrated in Figure 5, we introduce the structural-semantic pre-training strategy based on the Masked Language Modeling (MLM) task [39], aiming to enhance structural and semantic understanding across modalities.\nFor pre-training, PGPSNet-v2 solver assigns semantic and section tags to each token in the problem text. The semantic tags refer to the semantic categories of tokens, encompassing general [G], variable [N], argument [ARG], point [P] and angle ID [ANGID]. The section tags indicate the portion to which a token belongs, with a given problem divided into three parts: structure [S], condition [C] and target [T]. The input textual token embedding e(tj) not only incorporates positional encoding but also integrates embeddings of semantic and section tags, formulated as:\n\\(e(t_j) = TokenEmb(t_j) + PosEmb(j) + SemEmb(t_j) + SectEmb(t_j), 1 \u2264 j \u2264 N_T.\\)\n(2)\nThe fine-grained semantic and section tags facilitate geometric modeling and also benefit in alleviating the imbalance among textual tokens. Subsequently, following the work of Cho et al. [40], we apply masked tokens [M] to obscure 30% of textual tokens while keeping the semantic and section tags unchanged. The pre-training objective entails the restoration of masked textual tokens through a unified text generation approach [39].\nVia the fused modal representation, this pre-training strategy proves highly applicable for fusing structural and semantic information. For instance, according to the clause \"D is the midpoint of AB\u201d in the textual problem, it allows the inference that the masked token in the structural clause \u201cline A [M] B\u201d is \u201cD\u201d, thereby promoting the model to acquire the geometry knowledge about point arrangement on lines. Similarly, considering the structural clauses \u201cline A C\u201d and \u201cline C D\u201d, it can be deduced that the masked token in the semantic clause \u201cm \u2220A[M]D = m \u22201\u201d is \"C\", by enhancing the model's understanding of geometric concept related to two lines intersecting at a point. Nevertheless, sometimes masked tokens cannot be inferred accurately, but the candidate tokens still encapsulate rich geometry knowledge. Taking the semantic clause \u201cm \u2220BC[M] = m \u22202\" as an example, based on the structural clauses \u201cline A C\u201d, \u201cline C B\u201d and \u201cline C D\u201d, it is evident that the masked token is likely to be either \u201cD\u201d or \u201cA\u201d.\nIn summary, the structural-semantic pre-training contributes to fuse geometry problem representation via local relationship modeling. It endows PGPSNet-v2 solver with multi-modal geometric cognition capability, constituting the foundation for following geometric reasoning.\""}, {"title": "C. Self-limited Decoder", "content": "The self-limited decoder incorporates diagram and problem text to implement multi-modal geometric reasoning. In the reasoning process, the problem text enhanced by modal fusion provides rich structure and semantic information, and the diagram encoded by a CNN module further complements visual spatial content. Considering the complexity and flexibility of solving process of geometry problems, the solution program cannot convert into a binary or general expression tree, so the tree decoder widely used in math word problem (MWP) task [38], [41] is not applicable to GPS. As shown in Figure 6, our self-limited GRU decoder is an attention-based decoder [42] that generates the sequential solution program in an autoregressive manner, and boosts in two aspects:\n(1) Reducing the complexity of input feature space. In self-limited decoder, the input embeddings of problem variables N and augments ARG are copied from encoding context \\({h_i}_{i=1}^{N_D+N_T}\\) produced by the fusion module, based on the fact that problem variables and augments in solution program also exist in the problem text uniquely. This copying mechanism not only reduces the complexity of input feature space, but also enriches decoder inputs with contextual information. Specifically, the token embeddings of decoder inputs are defined as\n\\(e(s) = \\begin{cases}\nTokenEmb(s), & s \u2208 \\{V_{OP}, V_V, V_C\\}\\\\\nh_{loc(s,T)}^C, & s \u2208 \\{V_N, V_{ARG}\\},\\\\\n\\end{cases}\\)\n(3)\nwhere VOP, VV, VC, VN and VARG are the target vocabularies of operators, intermediate variables, constants, problem variables and augments, respectively; loc(s, T) is the location of program token s in the problem text T.\n(2) Narrowing the search space of output token. Self-limited decoder limits the output token candidates of problem variables N and augments ARG into that appear in the problem text T. Concretely, the probability of predicted token s is\n\\(P(s) = Softmax(Score(h_D, c, e(s)))\\)\n(4)\nwhere s \u2208 {VOP,VV,VC,VN \u2229 T, VARG \u2229 T}, hD is the hidden vector for decoder; c is the context vector generated from hE using the same attention mechanism as [42]; The specific form of score function is \\(Score(h_D, c, e(s)) = W_l^T tanh (W_s [h_D || c || e(s)])\\), Ws and We are learnable matrix parameters.\nIn our experiment, it was observed that the self-limited decoder achieved superior reasoning performance with faster training and inference speeds compared to sophisticated tree decoders, also yielding comparable results in MWP task [41]."}, {"title": "D. Multi-level Theorem Verification", "content": "Data-driven based neural solvers are susceptible to the size and content distribution of problem datasets, often generating solution process with high confidence but inconsistent with geometric principles. To alleviate this issue, together with the knowledge base and the program executor, we propose a geometry theorem knowledge verifier. Benefiting from the interpretable solution program design of PGPS9K dataset, the verifier validates the solution program generated by the self-limited decoder from three levels: form, calculability and semantic, as illustrated in Figure 7:\n\u2022\tForm: In the form level, it necessitates the solving step Si = (OP', PN') conform to the structure of knowledge tuple, which also means that PN' aligns with the operand quantity of knowledge tuple. For instance, the solving step 1 of solution program 1, denoted as \u201cGeo_Mean NO N1\u201d, fails to adhere to the form of Geometric Mean Theorem that should express as \u201cGeo_Mean(a, b, c)\u201d instead.\n\u2022\tCalculability: The calculability ensures that the solving steps Si are effectively computable according to the theorem formulas of knowledge tuples. For example, the solving step 3 of solution program 2, namely \u201cGet x\u201d, cannot compute the value of x.\n\u2022\tSemantic: The semantic level requires that the operands PN of solving step Si satisfy the semantic rules specified by the knowledge tuple. Taking the solving step 2 of solution program 3 as an example, \u201cGouGu N1 N3 N4\u201d does not meet the semantic rules of Pythagorean Theorem that N1 and N3 are right legs and N4 is corresponding hypotenuse.\nThe hierarchical relationships among these three levels of verifications can be indicated as \u201cForm \u2286 Calculability \u2286 Semantic\u201d. In other words, if the form of solving step is incorrect, it is definitely computationally infeasible and semantically erroneous; if the solving step cannot be calculated, there also exists semantic issues. In addition, the verification complexities of the three levels are also increasing accordingly, in which form matching is the simplest and the semantic judgement is the most complex. The algorithmic workflow of multi-level theorem verification is demonstrated in Algorithm 1, primarily consisting of two steps: (1) Base Search: Retrieving corresponding knowledge tuples from the theorem knowledge base based on the operator of solving step; (2) Tuple Matching: Sequentially validating solving steps in three levels of form, calculability and semantic. During the verification process, the sub-loop terminates prematurely if any unsatisfactory verification arises. Among the solution programs that survive all the verification steps, the one with the highest confidence is selected as the final solution."}, {"title": "V. EXPERIMENTS", "content": "1) Implementation Details: Our PGPSNet-v2 solver is implemented on the PyTorch framework using four 24GB NVIDIA GTX-RTX GPUs. The neural model adopts ResNet-18 [43] as the CNN module, scales geometry diagrams into images of size 224 * 224, and outputs flattened sequence of length ND = 7*7. We select three different scales of general transformer encoder [44] as the pre-trained language model architecture, signified as PGPSNet-S/M/L for solvers, which have 6/6/12 modules with each module containing 8/8/12 self-attention heads. The input embedding and hidden embedding in the models of three scales have dimensionality 256/512/768 and dimensionality 1,024/2,048/3,072, respectively. The GRU encoder is a two-layer bidirectional GRU [45] with the same dimensionality of input embedding and hidden state, aligned with the input embedding dimensionality of the pre-trained language model. The self-limited decoder is a two-layer GRU decoder [45] with both input embedding and hidden state in the same dimensionality as the GRU encoder.\nIn training, we choose AdamW [46] as the optimizer of PGPSNet-v2, with a regularization with weight coefficient of 1e-2 and a step-decline schedule with decaying rate of 0.5. During the pre-training stage, the learning rate of language model is initialized to 5e-4 and decays at 1K, 2K and 3K epochs, with a total of 4k epochs. During the training stage, all modules of PGPSNet-v2 are jointly trained. The initial learning rate of the language model is set as 1e-4 while the other modules are set as 1e-3, and all modules are uniformly decayed at 140, 280, 360, 440 and 500 epochs, in total 560 epochs. Additionally, the training batch size and dropout rate are set as 128 and 0.2, respectively. The problem augmentation is carried out synchronously in model training, and the augmentation probability is set as 0.7 and 0.5 for pre-training and fine training, respectively.\n2) Datasets and Evaluation: We split the PGPS9K dataset into a training set and a test set in two distinct ways, sig-"}, {"title": "B. Comparison with State-of-the-art Solvers", "content": "We compare with state-of-the-art solvers including symbolic solvers and neural solvers to show the superior problem solving performance of PGPSNet-v2 as illustrated in Table III. For symbolic solvers, Inter-GPS [5] solved geometry problems by searching and matching with unified formal language. According to the input source of formal language, Inter-GPS presents three types of results, e.g., \u201cPredict\u201d means that all formal clauses are predicted by its parsers, \u201cDigram GT\u201d denotes that formal clauses of diagram are from the ground truth, and \"All GT\" indicates that formal clauses of diagram and textual problem are all ground truth. GeoDRL [32] improved the search strategy of Inter-GPS with logical graph deduction and deep reinforcement learning. Noting that the numbers of parameters of InterGPS and Geoformer are aligned with PGPSNet-v2-L but are much more than the other scales of PGPSNet-v2. Experimental results show that the PGPSNet-v2 outperforms symbolic solvers on two datasets and in almost evaluation metrics. Even compared with Inter-GPS (All GT) that uses annotated formal clauses designed carefully, PGPSNet-v2-L gains a 3.8% improvement in Completion and a 5.3% improvement in Choice on Geometry3K.\nAs to neural solvers, NGS [6] and Geoformer [14] relied primarily on the textual problem to solve problems. Even through re-implementing them with the annotated textual clauses parsed from diagram and the same augmentation strategies, performance gaps between these two solvers and our PGPSNet-v2 are still significant, 37.5% and 36.0% lower than PGPSNet-v2-L in Completion on PGPS9K, respectively. SCA-GPS [7] shows similar performance as InterGPS (All GT) because its diagram understanding methods, character alignments and masked image modeling, are coarse-grained and ineffective.\nAs to PGPSNet-v2, we also test the influence of textual clauses and parameter scales to GPS performance, e.g., \"Predict\" indicates that PGPSNet-v2 uses the textual clauses parsed by the PGDPNet parser [37] instead of the ground truth, \u201cT\u201d denotes the version without knowledge verification (i.e., PGPSNet), and \u201cS/M/L\u201c refers to different scales of language model. Compared PGPSNet-v2-S (Predict) with PGPSNet-v2-S (Clause GT), it turns out that the performance of two solvers is close for the strong parsing ability of PGDPNet. Knowledge verification brings an extra performance gain when comparing PGPSNet-S (Clause GT) with PGPSNet-v2-S (Clause GT), which will be explained further in the ablation study. With the increase of model scale, the GPS performance is improved steadily, but saturates due to the limitation of data amount. In addition, the improvements of performance in Top-3 are less than that in Completion because most of correct solutions are concentrated among top-rank candidates. There remains a certain performance gap between PGPSNet-v2 and human expert, however. This indicates automated GPS still has much room to improve in the future."}, {"title": "C. Ablation Study", "content": "To justify the effects of core steps in PGPSNet-v2 solver: modal fusion, reasoning process and knowledge verification, we conduct ablation studies by varying the combination of modules.\n1) Effect of Fusion and Reason Modules: For fusion and reason modules, we take self-limited decoder, problem augmentation, structural clauses, pre-trained language model and diagram attention as ablation objects to evaluate their effect on GPS. Table IV shows the ablation results of PGPSNet-v2-L on Geometry3K dataset. The comparison between row 1 and row 4 indicates that problem augmentation, by injecting geometric representation knowledge into augmented samples, benefits geometric logical reasoning. Through the comparison between row 2 and row 4, the results indicate that the self-limited decoder enhances the performance of GPS via simplifying the input feature space and restricting the search space, thereby reducing the difficulty of model learning. The language model with structural-semantic pre-training brings an amazing performance gain of 30.6% accuracy improvement of numerical answer, as in the comparison between row 4 and row 6. Comparing row 3 and row 4, the results reveal that structural clauses have a relatively small impact on GPS performance in the absence of pre-training. However, with structural-semantic pre-training, structural clauses lead to a substantial improvement in GPS, as in the comparison between row 5 and row 6. This demonstrates that fundamental connection relationships can enhance the model's cognition of geometric structures with a befitting modal fusion approach, thus aiding geometric logical reasoning. Visual diagram with cross-modal attention also provides rich visual layout information and brings a moderate improvement of GPS performance, as in comparing row 6 and row 7.\nAdditionally, in all the aforementioned experiments, the performance of solution program observes a consistent trend with numerical answer. Nevertheless, the performance of solution program is 2%-5% lower than that of numerical answer overall. This is due to the influence of the diversity of solution approaches for GPS.\n2) Impact of Verifier: To justify the effect of knowledge verification at different levels, we conduct ablation experiments for the theorem knowledge verifier on PGPS9K dataset, with the evaluation mode of Completion in default. As shown in Table V, on the PGPSNet-v2-S solver, the introduction of form verification yields a 1.1% improvement compared to the model without verification (i.e., PGPSNet-S), further incorporating calculability validation results in a 1.6% enhancement relative to that with only form verification, and the inclusion of semantic verification contributes an additional 1.0% improvement. Consequently, in comparison to the model without verifier, the ultimate improvement on PGPSNet-v2-S is 3.7%. Furthermore, experimental results indicate that the the-orem knowledge verifier all shows performance improvements across solvers of varying model scales. However, as the model scale increases, the extent of improvement diminishes. On PGPSNet-v2-M and PGPSNet-v2-L, the improvement rates are 3.0% and 2.3%, respectively. This is because the large-scale neural model without knowledge verification already performs fairly well. Nevertheless, the verifier still benefits the performance substantially."}, {"title": "D. Case Analysis", "content": "Detailed case analyses on PGPS9K dataset are conducted to discuss the capabilities and limitations of our PGPSNet-v2 solver. These case studies include the analysis of overall problem solving performance of PGPSNet-v2, and the ability test of theorem knowledge verifier.\n1) Problem Solving Cases: Figure 8 displays four plane geometry problems (a)-(d) and their solution programs of NGS [6], PGPSNet-v2 w/o LM, PGPSNet-v2 and ground truth, where (a) and (b) are the problems PGPSNet-v2 answered correctly while (c) and (d) are the problems PGPSNet-v2 answered incorrectly. Case (a) examines the application of Angle Bisector Theorem. Solvers NGS and PGPSNet-v2 w/o LM fail to address the proportional relationships between the side lengths of a triangle divided by its angle bisector, while PGPSNet-v2 generates the correct solution procedure; Case (b) investigates the knowledge point related to the calculation of rhombus area. In the penultimate step, both the NGS and PGPSNet-v2 w/o LM inaccurately provide the length of rhombus diagonal, whereas PGPSNet-v2 utilizes the Pythagorean Theorem correctly to compute the length of rhombus side; Case (c) necessitates the consideration of relationships among the interior angles of isosceles triangle, but all solvers struggle to effectively discern the relationships between base angle, apex angle and vertically opposite angle; Case (d) is a hard geometry problem that requires the application of two types of Chord-Length Theorems and involves multiple steps of theorem manipulation. In the context of problem (d), all solution programs from various solvers are proved to be incorrect, but the solution program generated by PGPSNet-v2 is demonstrated the closest solution to the ground truth. In summary, comprehensive experimental results suggest that PGPSNet-v2 outperforms than other solvers, showcasing significant potential for inferential cognition, though it still lack the capability for intricate geometric reasoning at present.\n2) Theorem Verification Cases: We demonstrate the pivotal role played by the theorem knowledge verifier of PGPSNet-v2 through two GPS cases, as shown in Figure 9. Case (a) is a problem that examines the secant length relationship of circle, and the whole solving process consists of two main steps, first compute x and then calculate y, according to the Secant-Segment Theorem. The step 2 of solution program 1 gets the value of N2 less than zero, which does not meet the semantic rules of Geometric Mean Theorem, mainly because the calculation of line segment length in step 1 is wrong. When calculating the step 3 of solution program 2, due to lack of a clear problem solving goal for solver, the program executor acquires no value of x, violating the computability; Case (b) involves calculating the area of parallelogram and needs to determine the unknown height DE in terms of the hypotenuse AB and the diagonal angle \u2220DFE. The step 3 of solution program 1 dissatisfies the form of parallelogram area formula \u201cPara_Area(a, b, c)\". Besides, the semantic rules of Cosine Law are not satisfied in step 2 of solution program 2, which requires V1 to be the diagonal side of N2. In short, the objective of theorem knowledge verifier is to screen solutions in the post-processing phase, aiming to compensate for the defects in ability of PGPSNet-v2 in the generation phase.\""}, {"title": "E. Limitation Discussion", "content": "Despite the significant progress achieved by our PGPSNet-v2 in GPS, it remains in nascent stage and is subject to certain limitations. Similar to conventional neural networks, the PGPSNet-v2 solver, consisting a neural model as main part, is data-hungry and needs extensive samples with robust annotations to facilitate effective model learning. However, in reality, a substantial portion of available geometry problems are unlabeled and low-quality. The textual clauses parsed from the geometry diagram cannot convey the content of diagram comprehensively. For instance, absolute spatial position and visual color information are lost and these features may play crucial roles in problem solving. In the modal fusion stage, PGPSNet-v2 extracts visual features with a CNN module in a coarse-grained way, inevitably causing information loss. In the process of reasoning, our solution program exhibits symbolic interpretability and controllability, but is not so flexible as natural language, rendering it challenging to manage with more advanced operations such as adding auxiliary points or lines. In terms of knowledge verification, PGPSNet-v2 currently adopts a strategy of post-processing the candidate solutions generated by the neural model, still depending on a substantial set of manually predefined knowledge rules. Moreover, the problem augmentation of PGPS9K does not bring the diversity of logical semantics, but only the expansion of geometric representations."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this paper, we present PGPSNet-v2, a neural-symbolic geometric solver that addresses GPS in three key steps: modal fusion, reasoning process, and knowledge verification. Textual clauses parsed from diagram are fused with textual problem via structural-semantic pre-training, and solution programs are generated though a self-limited decoder. A multi-level theorem verifier identifies and eliminates solutions that conflict with geometric principles, thereby reducing erroneous results and enhancing the explainability and reliability of solution. We also provide a comprehensive geometry problem dataset PGPS9K, featured with fine-grained annotations of textual clauses and solution programs, and a geometry knowledge base represented as knowledge tuples, which facilitates the research and evaluation of GPS. Extensive experiments on datasets Geometry3K and PGPS9K demonstrate the superiority of the proposed PGPSNet-v2, outperforming existing symbolic and neural solvers.\nFuture works can be conducted to further improve the fusion of modal information, considering better fusion model and cross-modal alignment mechanism, to incorporate theorem knowledge directly in the generation of solution process for better integrating neural and symbolic reasoning. There are also potential room of improvement via expanding the types of geometry problems, the annotated dataset, leveraging un-annotated data and large language models."}, {"title": "APPENDIX", "content": "Our PGPS9K dataset is divided into 30 problem types elaborately according to geometry knowledge points by education experts, covering almost all problem types of plane geometry problem across grade 6-12 as shown in Figure A. Fine-grained partition helps geometry problem collection, trying to keep relative balance of problem types. For example, the poor performance of several problem types may result from insufficient problem samples, so we could collect them on purpose. Moreover, fine-grained partition is beneficial to analyse the model performance of different problem types in depth. For instance, there widely exists intersection and inclusion of geometric patterns among different problem types, and exploring the model capacity of inductive and deductive on GPS is meaningful and interesting."}, {"title": "B. Textual Clauses", "content": "The textual clauses are the linguistic descriptions of primitive relations of geometry diagram. They include structural clauses and semantic clauses, where the structural clauses present the connection relations of geometric primitives (point, line and circle) and the semantic clauses depict the relations between geometric primitives and non-geometric primitives (text and symbol). Table VI displays the complete templates of textual clauses, consisting 3 types of structural clauses and 6 types of semantic clauses. These textual clauses are elementary and necessary, which could be translated from relation tuples of diagram annotation directly without advanced geometric rules. The clause design is open, but neural solvers do not pursue high-level logical clauses though they may contribute to the problem solving."}, {"title": "C. Solution Program", "content": "Our annotation of solution program possesses better flexibility and scalability with extensive theorem operators and variable operands. The detailed contents of program set are listed in Table VII. It should indicate that the solution program for GPS still confronts similar issues as general math word problem [47]: (1) Uncertainty caused by exchangeable operands: The operands in some theorem formulas are commutative, e.g., in Pythagorean Theorem with formula \\(a^2 + b^2 = c^2\\), the two right legs are exchangeable. In our annotation, we normalize solution program via specifying two-level priority of commutative operands. The first is the type level that \u201cAugment > Intermediate Variable > Problem Variable > Constant\u201d and the second is the index level with positive order. (2) Uncertainty of equivalent step orders: Solving steps are in no particular order sometimes. We manually keep the same pre-defined step order for the same problem type. (3) Multiple solution methods: A part of geometry problems could be solved by multiple solution methods. We choose the solution method with the most concise solution program."}]}