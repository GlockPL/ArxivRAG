{"title": "Fuse, Reason and Verify: Geometry Problem Solving with Parsed Clauses from Diagram", "authors": ["Ming-Liang Zhang", "Zhong-Zhi Li", "Fei Yin", "Liang Lin", "Cheng-Lin Liu"], "abstract": "Geometry problem solving (GPS) requires capacities of multi-modal understanding, multi-hop reasoning and theorem knowledge application. In this paper, we propose a neural-symbolic model for plane geometry problem solving (PGPS), named PGPSNet-v2, with three key steps: modal fusion, reasoning process and knowledge verification. In modal fusion, we leverage textual clauses to express fine-grained structural and semantic content of geometry diagram, and fuse diagram with textual problem efficiently through structural-semantic pre-training. For reasoning, we design an explicable solution program to describe the geometric reasoning process, and employ a self-limited decoder to generate solution program autoregressively. To reduce solution errors, a multi-level theorem verifier is proposed to eliminate solutions that do not match geometric principles, alleviating the hallucination of the neural model. We also construct a large-scale geometry problem dataset called PGPS9K, containing fine-grained annotations of textual clauses, solution program and involved knowledge tuples. Extensive experiments on datasets Geometry3K and PGPS9K show that our PGPSNet solver outperforms existing symbolic and neural solvers in GPS performance, while maintaining good explainability and reliability, and the solver components (fusion, reasoning, verification) are all justified effective.", "sections": [{"title": "I. INTRODUCTION", "content": "AUTOMATIC geometry problem solving (GPS) has been long-standing in artificial intelligence (AI) field [1]\u2013[4], and is drawring increasing attention in recent years [5]\u2013[8]. A geometry problem usually consists of a geometry diagram and a textual problem, forming a multi-modal problem. The textual problem describes the geometric conditions and sets the reasoning target in natural language, while the geometry diagram displays the spatial structure and additional geometric conditions in vision. Solving geometry problems requires geometry diagram and textual problem understanding and multi-step reasoning incorporating geometry knowledge. Due to the diverse and irregular contents of textual problem and geoetry diagram, GPS is widely recognized as a vital testbed [9] for evaluating the high-level multi-modal reasoning capability of AI.\nThe challenges of GPS lie in its three main steps: modal fusion, reasoning and knowledge verification. Modal fusion is to represent and utilize multi-modal content from geometry diagram and textual problem. Fusing two distinct modalities is difficult, where the textual problem expresses geometric infor-mation by text while the geometry diagram conveys geometric relationships via visual layout. Reasoning is sophisticated involving state transition and path search [10], [11]. State transition, embedded in path search, applies geometric pattern matching and geometry theorem to move forward the state of solution. Imitating from human problem-solving, knowledge verification plays a key role in checking the solving process and correcting solution errors.\nTo address the above three challenges, several geometric solvers have been proposed, which can be roughly catego-rized into two groups: symbolic solvers and neural solvers. As summarized in Figure 1 (a), symbolic solvers [3], [5], [12] parse the diagram and textual problem into a unified formal language, and then perform symbolic reasoning through path search and condition matching according to geometry knowledge. During the process, new conditional states are generated progressively until the search target is achieved. Although symbolic solvers have strong interpretability, they are designed with many human efforts, and may exhibit slow problem-solving speed due to redundant steps. In contrast, neural solvers [7], [13], [14] use neural encoders to extract features of diagram and textual problem, and embed them into a unified feature space by the joint module, as shown in Figure 1 (b). The neural solver is trained end-to-end in a data-driven way and outputs sequential solutions finally. Despite the promise of neural solvers, they still struggle in modal fusion and knowledge interpretability, resulting in a certain performance gap compared with symbolic solvers [5], [15]. Regarding modal fusion, existing neural solvers, adopting similar frameworks of general vision-language tasks applied for natural images [16]\u2013[18], cannot utilize the structural and semantic information in diagram explicitly and efficiently. The inherent black-box characteristic of neural networks also hinders the explainability and reliability of solution.\nConsidering the pros and cons of current geometric solvers, we propose a neural-symbolic solver, called PGPSNet-v2, as shown in Figure 1 (c). PGPSNet-v2 is the improved version of our previously proposed neural solver PGPSNet [19]. It consists of three modules: fusion, reason and verifier, to enhance the performance from three aspects of modal fusion, reasoning and knowledge verification, respectively. The fusion module inputs not only geometry diagram and textual problem, but also textual clauses parsed from geometry diagram. Multi-modal fusion is accomplished with the strategy of structural and semantic pre-training, so as to model global structure and context in unified neural form. The reason module applies a self-limited decoder to generate interpretable reasoning sequences in an autoregressive manner. The verifier validates the generated solution sequences in three levels: form, calculability and semantic, through knowledge tuple search and matching. By the way of verification in post-processing, PGPSNet-v2 excludes hallucinations that do not match geometric principles and thus promotes the reliability of solving process.\nTo facilitate GPS research, we also build and release a large-scale GPS dataset named PGPS9K, containing 9,021 plane geometry problems, each paired with a textual problem and a geometry diagram. PGPS9K is more complete compared with existing GPS datasets [3], [5], [6] because it offers both fine-grained annotations of textual clauses from diagram and solution programs, and corresponding theorem knowledge base and program executor. Textual clauses, including structural clauses and semantic clauses, possess highly syntactic and less redundant information. Given the complexity of GPS, problem-solving procedure of PGPS9K is designed as a solution program based on geometry theorems, wherein each step denotes an application of a theorem (axiom), instead of the fundamental arithmetic operations used in math word problem (MWP) datasets [20], [21]. Our solution program carries richer geometry knowledge, better interpretability and shorter sequence length. The theorem knowledge base, stored in the form of knowledge tuples with operator, operands, the orem formula and semantic rules, serve the verifier to validate the solution program. Responding to the solution program, the program executor realizes symbolic algebraic calculation according to theorem formulas and returns numerical answer. In addition, we adopt five strategies of problem augmentation based on the equivalence of geometric representation, to in-crease the problem diversity further and embed representation knowledge simultaneously. Extensive experiments on PGPS9K and Geometry3K [5] datasets demonstrate that PGPSNet-v2 achieves remarkable improvement of GPS performance, exceeding existing symbolic solvers and neural solvers.\nIn summary, our main contributions are as follows:\n\u2022 We propose a neural-symbolic geometric solver PGPSNet-v2 based on three core steps: fusion, reasoning and verification.\n\u2022 We construct a large-scale plane geometry problem dataset PGPS9K with fine-grained annotations and paired \u2022 with theorem knowledge tuples.\n\u2022 The proposed PGPSNet-v2 solver generates explainable and reliable solution process, thereby yielding superior GPS performance."}, {"title": "II. RELATED WORK", "content": "Multi-modal reasoning task combines multi-modal informa-tion to carry out reasoning, generally in the form of questions and answers, e.g., visual question answering (VQA) [16], [18], document question answering (DQA) [22], [23] and table question answering (TQA) [24], [25]. The knowledge types investigated in multi-modal reasoning include common sense [22], [26], semantics [22], [27], numerical quantity [24], [28], spatial location [28], [29], color [26], [27], and so on. To accomplish multi-modal reasoning, one must initially compre-hend and amalgamate multi-modal content, and subsequently employ domain knowledge to address problems logically. Due to the discrepancy of data modality and reasoning mechanisms, there are significant semantic gaps among different multi-modal reasoning tasks. The GPS stands out as a unique form of multi-modal reasoning, delving into the cognition of geometric spatial structures and mathematical logical reasoning. Addi-tionally, GPS necessitates the utilization of geometry theorem knowledge, making it more complicated.\nGeneralized GPS problems can be categorized into two types: geometric theorem proving and geometric numerical calculation. Early efforts in GPS were primarily focused on"}, {"title": "III. PGPS9K DATASET", "content": "Most existing datasets of GPS, as recorded in Table I, either have a small data size, suitable only for rule-based symbolic solvers, or own coarse-grained annotations, neglecting the rich information in geometry diagram. To promote the develop-ment of GPS field, we construct a large-scale GPS dataset, named PGPS9K\u00b9, annotated with fine-grained diagram labels (elements and relationships) and interpretable solving solution, along with the corresponding theorem knowledge base and program executor. To the best of our knowledge, PGPS9K is the largest and most comprehensive GPS dataset up to date. It consists of 9,021 textual problems illustrated with 4,000 non-repetitive geometry diagrams. Among them, 2,891 textual problems and 1,738 geometry diagrams were selected from the Geometry3K dataset [5], while the remaining samples were collected from five geometry textbooks\u00b2 across 6-12 grades. The samples of PGPS9K are categorized into 30 problem types based on geometry knowledge points, covering almost all types of plane geometry problems. More statistical details are exhibited in Appendix A.\nFurthermore, as shown in Figure 2, PGPS9K possesses five properties, enabling it focus on the challenges in geometric reasoning and alleviate the bias introduced by the text [35], [36]. The properties are: (1) Theorem-based: In the GPS process, it is necessary to use geometric theorem or axiom knowledge to carry out algebraic calculation, and finally give the numerical result; (2) Diagram-dependent: More than 90% of problems must be solved in conjunction with diagram, because necessary conditions such as numerical content and geometric structure are presented by visual diagram instead of textual problem; (3) Abstract: Geometry diagram is composed of basic geometric primitives (point, line, circle) and non-geometric primitives (text, symbol). No complex semantic scenarios are involved in textual problem except abstract geo-metric conditions; (4) Fine-grained: Problems with the same diagram may vary in conditions or targets. Slight distinctions in textual problems usually lead to completely different solu-tions; (5) Condition-redundancy: Some conditions in textual problem or diagram are not necessarily used in GPS. The statistical results on PGPS9K dataset show that an average of 1.9 conditions are not used for GPS, 42% of problems have redundant conditions, and 15% of problems have 3 or more unused conditions."}, {"title": "B. Annotation and Description", "content": "The annotations of PGPS9K dataset include diagram anno-tation and solution procedure, wherein the diagram annotation serves to articulate the structural and semantic information within diagrams, and the solution procedure delineates the steps for solving geometry problems.\nThe diagrams employ primitive-level annotation consis-tent with the geometry diagram parsing [37], containing geometric/non-geometric primitives as well as relationships among primitives in tuple form. The annotated primitives and relationships can be transformed into two fundamental types of textual clauses: structural clauses (three types) and semantic clauses (six types), via simple clause templates matching. The structural clauses depict the connectivity relationships among geometric primitives, such as points on a line or a circle, wherein points are arranged in a certain order. The connectivity relationships reveal the most fundamental geometric structural connections, often depicted in diagram but frequently omitted in textual problem. The semantic clauses articulate the basic"}, {"title": "IV. PGPSNET-V2 SOLVER", "content": "We begin by presenting the formal definition of GPS task. For a given geometry problem P, comprising a geometry diagram D and a textual problem Tprob, the objective is to solve the problem and generate the solution S, formulated as P = {D,Tprob} \u2192 S. In our work, GPS process is refined into three core steps: fusion, reasoning and verification, and it is re-expressed as:\n\\(P \\xrightarrow{Fusion} P' \\xrightarrow{Reasoning} \\{S\\} \\xrightarrow{Verification} S'\\), (1)\nwhere P' is the new problem representation after modal fusion with P; Through neural reasoning for P', it acquires a list of solution candidates {S}; The solver gets the final solution S' via knowledge verification to {S}.\nImproved on above three core steps, we propose a neural-symbolic geometric solver, called PGPSNet-v2, which is ex-tended from our previous neural solver PGPSNet [19] by adding a knowledge verifier, as displayed in Figure 4. The inputs of PGPSNet-v2 include not only the geometry diagram image D and the textual problem Tprob, but also the structural clauses Tstru and the semantic clauses Tsem parsed from the geometry diagram. So, the problem text is expanded to be expressed as T = {Tstru, Tsem,Tprob} = {tj}\\(_{j=1}^{N_T}\\) after text concatenation, where NT is the text token number. In the modal fusion phase, the geometry diagram is encoded by a convolutional neural network (CNN) module and then flattened into a feature sequence D' = {hi}\\(_{i=1}^{N_D}\\), where ND is the diagram token number. In parallel, the problem text undergoes fused encoding through the structural-semantic pre-trained language model and the bidirectional GRU encoder in turn, and obtains a feature sequence T' = {hi}\\(_{i=1}^{N_T}\\). During the reasoning stage, two types of modal tokens, concatenated and unified as P' = {D',T'} = {hi}\\(_{i=1}^{N_D+N_T}\\), are fed into the self-limited GRU decoder to perform geometric reasoning, and the decoder generates corresponding solution candidates represented by multi-step program sequences S = {S}\\(_{i=1}^{N_K}\\) = {sij}\\(_{j=1}^{N_M}\\), where Nk and NM are the numbers of solving steps and program tokens, respectively. As to verification, combined with the geometry theorem knowledge base and the program executor, the verifier validates all the solution candidates and determines the final solution program S' and its numerical answer. The key modules of pre-training, decoder and verifier are detailed in the following."}, {"title": "B. Structural-Semantic Pre-training", "content": "Structural-semantic pre-training plays a critical role in the modal fusion, by unifying textual clauses from diagram with the textual problem. While the textual clauses delineate fine-grained structural and semantic information extracted from diagram, they remain at low level, lacking overall coherence and contextual connections. Furthermore, verbose, fragmented and disorganized text continues to pose significant challenge to modal fusion and semantic structure comprehension. Inspired by pre-trained language models, as illustrated in Figure 5, we introduce the structural-semantic pre-training strategy based on the Masked Language Modeling (MLM) task [39], aim-ing to enhance structural and semantic understanding across modalities.\nFor pre-training, PGPSNet-v2 solver assigns semantic and section tags to each token in the problem text. The semantic tags refer to the semantic categories of tokens, encompassing general [G], variable [N], argument [ARG], point [P] and angle ID [ANGID]. The section tags indicate the portion to which a token belongs, with a given problem divided into three parts: structure [S], condition [C] and target [T]. The input textual token embedding e(tj) not only incorporates positional encoding but also integrates embeddings of semantic and section tags, formulated as:\n\\(e(t_j) =TokenEmb(t_j) + PosEmb(j) + SemEmb(t_j) + SectEmb(t_j), 1 \u2264 j \u2264 N_T.\\) (2)\nThe fine-grained semantic and section tags facilitate geometric modeling and also benefit in alleviating the imbalance among textual tokens. Subsequently, following the work of Cho et al. [40], we apply masked tokens [M] to obscure 30% of textual tokens while keeping the semantic and section tags unchanged. The pre-training objective entails the restoration of masked textual tokens through a unified text generation approach [39].\nVia the fused modal representation, this pre-training strategy proves highly applicable for fusing structural and semantic information. For instance, according to the clause \u201cD is the midpoint of AB\u201d in the textual problem, it allows the inference that the masked token in the structural clause \u201cline A [M] B\u201d is \u201cD\u201d, thereby promoting the model to acquire the geometry knowledge about point arrangement on lines. Similarly, considering the structural clauses \u201cline A C\u201d and \u201cline C D\u201d, it can be deduced that the masked token in the semantic clause \u201cm \u2220A[M]D = m \u22201\u201d is \u201cC\u201d, by enhancing the model's understanding of geometric concept related to two lines intersecting at a point. Nevertheless, sometimes masked tokens cannot be inferred accurately, but the candidate tokens still encapsulate rich geometry knowledge. Taking the semantic clause \u201cm \u2220BC[M] = m \u22202\u201d as an example, based on the structural clauses \u201cline A C\u201d, \u201cline C B\u201d and \u201cline C D\u201d, it is evident that the masked token is likely to be either \u201cD\u201d or \u201cA\u201d.\nIn summary, the structural-semantic pre-training contributes to fuse geometry problem representation via local relationship modeling. It endows PGPSNet-v2 solver with multi-modal geometric cognition capability, constituting the foundation for following geometric reasoning."}, {"title": "C. Self-limited Decoder", "content": "The self-limited decoder incorporates diagram and problem text to implement multi-modal geometric reasoning. In the reasoning process, the problem text enhanced by modal fusion provides rich structure and semantic information, and the diagram encoded by a CNN module further complements visual spatial content. Considering the complexity and flex-ibility of solving process of geometry problems, the solution program cannot convert into a binary or general expression tree, so the tree decoder widely used in math word problem (MWP) task [38], [41] is not applicable to GPS. As shown in Figure 6, our self-limited GRU decoder is an attention-based decoder [42] that generates the sequential solution program in an autoregressive manner, and boosts in two aspects:\n(1) Reducing the complexity of input feature space. In self-limited decoder, the input embeddings of problem variables N and augments ARG are copied from encoding context {hi}\\(_{i=1}^{N_D+N_T}\\) produced by the fusion module, based on the fact that problem variables and augments in solution program also exist in the problem text uniquely. This copying mechanism not only reduces the complexity of input feature space, but also enriches decoder inputs with contextual information. Specifi-cally, the token embeddings of decoder inputs are defined as\n\\(e(s) =\begin{cases}TokenEmb(s), & s\u2208 \\{V_{OP}, V_V, V_C\\}, \\h_{loc(s,T)}^C, & s\u2208 \\{V_N, V_{ARG}\\},  \\end{cases}\\)(3)\nwhere VOP, VV, VC, VN and VARG are the target vocabu-laries of operators, intermediate variables, constants, problem variables and augments, respectively; loc(s, T) is the location of program token s in the problem text T.\n(2) Narrowing the search space of output token. Self-limited decoder limits the output token candidates of problem variables N and augments ARG into that appear in the problem text T. Concretely, the probability of predicted token s is\n\\(P(s) = Softmax(Score(h_D, c, e(s)))\\) (4)\nwhere s \u2208 {VOP,VV,VC,VN \u2229 T, VARG \u2229 T}, hD is the hidden vector for decoder; c is the context vector gener-ated from hE using the same attention mechanism as [42]; The specific form of score function is Score(hD, c, e(s)) = WI tanh (Ws[hD || c || e(s)]), Ws and We are learnable matrix parameters.\nIn our experiment, it was observed that the self-limited decoder achieved superior reasoning performance with faster training and inference speeds compared to sophisticated tree decoders, also yielding comparable results in MWP task [41]."}, {"title": "D. Multi-level Theorem Verification", "content": "Data-driven based neural solvers are susceptible to the size and content distribution of problem datasets, often generating solution process with high confidence but inconsistent with geometric principles. To alleviate this issue, together with the knowledge base and the program executor, we propose a geometry theorem knowledge verifier. Benefiting from the interpretable solution program design of PGPS9K dataset, the verifier validates the solution program generated by the self-limited decoder from three levels: form, calculability and semantic, as illustrated in Figure 7:\n\u2022 Form: In the form level, it necessitates the solving step Si = (OP', PN') conform to the structure of knowledge tuple, which also means that PN' aligns with the operand quantity of knowledge tuple. For instance, the solving step 1 of solution program 1, denoted as \u201cGeo_Mean NO N1\u201d, fails to adhere to the form of Geometric Mean Theorem that should express as \u201cGeo_Mean(a, b, c)\u201d instead.\n\u2022 Calculability: The calculability ensures that the solving steps Si are effectively computable according to the theorem formulas of knowledge tuples. For example, the solving step 3 of solution program 2, namely \u201cGet x\u201d, cannot compute the value of x.\n\u2022 Semantic: The semantic level requires that the operands PN of solving step Si satisfy the semantic rules specified by the knowledge tuple. Taking the solving step 2 of solution program 3 as an example, \u201cGouGu N1 N3 N4\u201d does not meet the semantic rules of Pythagorean Theorem that N1 and N3 are right legs and N4 is corresponding hypotenuse.\nThe hierarchical relationships among these three levels of verifications can be indicated as \u201cForm \u2283 Calculability \u2283 Semantic\u201d. In other words, if the form of solving step is incorrect, it is definitely computationally infeasible and semantically erroneous; if the solving step cannot be cal-culated, there also exists semantic issues. In addition, the verification complexities of the three levels are also increasing accordingly, in which form matching is the simplest and the semantic judgement is the most complex. The algorithmic workflow of multi-level theorem verification is demonstrated in Algorithm 1, primarily consisting of two steps: (1) Base Search: Retrieving corresponding knowledge tuples from the theorem knowledge base based on the operator of solving step; (2) Tuple Matching: Sequentially validating solving steps in three levels of form, calculability and semantic. During the verification process, the sub-loop terminates prematurely if any unsatisfactory verification arises. Among the solution programs that survive all the verification steps, the one with the highest confidence is selected as the final solution."}, {"title": "V. EXPERIMENTS", "content": "Implementation Details: Our PGPSNet-v2 solver is implemented on the PyTorch framework using four 24GB NVIDIA GTX-RTX GPUs. The neural model adopts ResNet-18 [43] as the CNN module, scales geometry diagrams into images of size 224 * 224, and outputs flattened sequence of length ND = 7*7. We select three different scales of general transformer encoder [44] as the pre-trained language model architecture, signified as PGPSNet-S/M/L for solvers, which have 6/6/12 modules with each module containing 8/8/12 self-attention heads. The input embedding and hidden embedding in the models of three scales have dimensionality 256/512/768 and dimensionality 1,024/2,048/3,072, respectively. The GRU encoder is a two-layer bidirectional GRU [45] with the same dimensionality of input embedding and hidden state, aligned with the input embedding dimensionality of the pre-trained language model. The self-limited decoder is a two-layer GRU decoder [45] with both input embedding and hidden state in the same dimensionality as the GRU encoder.\nIn training, we choose AdamW [46] as the optimizer of PGPSNet-v2, with a regularization with weight coefficient of 1e-2 and a step-decline schedule with decaying rate of 0.5. During the pre-training stage, the learning rate of language model is initialized to 5e-4 and decays at 1K, 2K and 3K epochs, with a total of 4k epochs. During the training stage, all modules of PGPSNet-v2 are jointly trained. The initial learning rate of the language model is set as 1e-4 while the other modules are set as 1e-3, and all modules are uniformly decayed at 140, 280, 360, 440 and 500 epochs, in total 560 epochs. Additionally, the training batch size and dropout rate are set as 128 and 0.2, respectively. The problem augmentation is carried out synchronously in model training, and the augmentation probability is set as 0.7 and 0.5 for pre-training and fine training, respectively.\nDatasets and Evaluation: We split the PGPS9K dataset into a training set and a test set in two distinct ways, sig-"}, {"title": "B. Comparison with State-of-the-art Solvers", "content": "We compare with state-of-the-art solvers including symbolic solvers and neural solvers to show the superior problem solving performance of PGPSNet-v2 as illustrated in Table III. For symbolic solvers, Inter-GPS [5] solved geometry prob-lems by searching and matching with unified formal language. According to the input source of formal language, Inter-GPS presents three types of results, e.g., \u201cPredict\u201d means that all formal clauses are predicted by its parsers, \u201cDigram GT\u201d de-notes that formal clauses of diagram are from the ground truth, and \"All GT\" indicates that formal clauses of diagram and textual problem are all ground truth. GeoDRL [32] improved the search strategy of Inter-GPS with logical graph deduction and deep reinforcement learning. Noting that the numbers of parameters of InterGPS and Geoformer are aligned with PGPSNet-v2-L but are much more than the other scales of PGPSNet-v2. Experimental results show that the PGPSNet-v2 outperforms symbolic solvers on two datasets and in almost evaluation metrics. Even compared with Inter-GPS (All GT) that uses annotated formal clauses designed carefully, PGPSNet-v2-L gains a 3.8% improvement in Completion and a 5.3% improvement in Choice on Geometry3K.\nAs to neural solvers, NGS [6] and Geoformer [14] relied primarily on the textual problem to solve problems. Even through re-implementing them with the annotated textual clauses parsed from diagram and the same augmentation strategies, performance gaps between these two solvers and our PGPSNet-v2 are still significant, 37.5% and 36.0% lower than PGPSNet-v2-L in Completion on PGPS9K, respectively. SCA-GPS [7] shows similar performance as InterGPS (All GT) because its diagram understanding methods, character alignments and masked image modeling, are coarse-grained and ineffective.\nAs to PGPSNet-v2, we also test the influence of textual clauses and parameter scales to GPS performance, e.g., \u201cPre-dict\u201d indicates that PGPSNet-v2 uses the textual clauses parsed by the PGDPNet parser [37] instead of the ground truth, \u201cT\u201d denotes the version without knowledge verification (i.e., PGPSNet), and \u201cS/M/L\u201c refers to different scales of language model. Compared PGPSNet-v2-S (Predict) with PGPSNet-v2-S (Clause GT), it turns out that the performance of two solvers is close for the strong parsing ability of PGDPNet. Knowledge verification brings an extra performance gain when comparing PGPSNet-S (Clause GT) with PGPSNet-v2-S (Clause GT), which will be explained further in the ablation study. With the increase of model scale, the GPS performance is improved steadily, but saturates due to the limitation of data amount. In addition, the improvements of performance in Top-3 are less than that in Completion because most of correct solutions are concentrated among top-rank candidates. There remains a certain performance gap between PGPSNet-v2 and human expert, however. This indicates automated GPS still has much room to improve in the future."}, {"title": "C. Ablation Study", "content": "To justify the effects of core steps in PGPSNet-v2 solver: modal fusion, reasoning process and knowledge verification, we conduct ablation studies by varying the combination of modules.\nEffect of Fusion and Reason Modules: For fusion and reason modules, we take self-limited decoder, problem aug-mentation, structural clauses, pre-trained language model and diagram attention as ablation objects to evaluate their effect on GPS. Table IV shows the ablation results of PGPSNet-v2-L on Geometry3K dataset. The comparison between row 1 and row 4 indicates that problem augmentation, by injecting geometric representation knowledge into augmented samples, benefits geometric logical reasoning. Through the comparison between row 2 and row 4, the results indicate that the self-limited decoder enhances the performance of GPS via simplifying the input feature space and restricting the search space, thereby reducing the difficulty of model learning. The language model with structural-semantic pre-training brings an amazing performance gain of 30.6% accuracy improvement of numerical answer, as in the comparison between row 4 and row 6. Comparing row 3 and row 4, the results reveal that structural clauses have a relatively small impact on GPS performance in the absence of pre-training. However, with structural-semantic pre-training, structural clauses lead to a substantial improve-ment in GPS, as in the comparison between row 5 and row 6."}, {"title": "D. Case Analysis", "content": "Detailed case analyses on PGPS9K dataset are conducted to discuss the capabilities and limitations of our PGPSNet-v2 solver. These case studies include the analysis of overall problem solving performance of PGPSNet-v2, and the ability test of theorem knowledge verifier."}, {"title": "E. Limitation Discussion", "content": "Despite the significant progress achieved by our PGPSNet-v2 in GPS, it remains in nascent stage and is subject to certain limitations. Similar to conventional neural networks, the PGPSNet-v2 solver, consisting a neural model as main part, is data-hungry and needs extensive samples with robust annotations to facilitate effective model learning. However, in reality, a substantial portion of available geometry problems are unlabeled and low-quality. The textual clauses parsed from the geometry diagram cannot convey the content of diagram comprehensively. For instance, absolute spatial position and visual color information are lost and these features may play crucial roles in problem solving. In the modal fusion stage,"}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this paper, we present PGPSNet-v2, a neural-symbolic geometric solver that addresses GPS in three key steps: modal fusion, reasoning process, and knowledge verification. Textual clauses parsed from diagram are fused with textual problem via structural-semantic pre-training, and solution programs are generated though a self-limited decoder. A multi-level theorem verifier identifies and eliminates solutions that conflict with geometric principles, thereby reducing erroneous results and enhancing the explainability and reliability of solution. We also provide a comprehensive geometry problem dataset PGPS9K, featured with fine-grained annotations of textual clauses and solution programs, and a geometry knowledge base represented as knowledge tuples, which facilitates the research and evaluation of GPS. Extensive experiments on datasets Geometry3K and PGPS9K demonstrate the superi-ority of the proposed PGPSNet-v2, outperforming existing symbolic and neural solvers.\nFuture works can be conducted to further improve the fusion of modal information, considering better fusion model and cross-modal alignment mechanism, to incorporate theorem knowledge directly in the generation of solution process for better integrating neural and symbolic reasoning. There are also potential room of improvement via expanding the types of geometry problems, the annotated dataset, leveraging un-annotated data and large language models."}, {"title": "APPENDIX", "content": "Our PGPS9K dataset is divided into 30 problem types elab-orately according to geometry knowledge points by education experts, covering almost all problem types of plane geometry problem across grade 6-12 as shown in Figure A. Fine-grained partition helps geometry problem collection, trying to keep relative balance of problem types. For example, the poor performance of several problem types may result from insufficient problem samples, so we could collect them on purpose. Moreover, fine-grained partition is beneficial to analyse the model performance of different problem types in depth. For instance, there widely exists intersection and inclusion of geometric patterns among different problem types, and exploring the model capacity of inductive and deductive on GPS is meaningful and interesting."}, {"title": "B. Textual Clauses", "content": "The textual clauses are the linguistic descriptions of prim-itive relations of geometry diagram. They include structural clauses and semantic clauses, where the structural clauses present the connection relations of geometric primitives (point, line and circle) and the semantic clauses depict the relations between geometric primitives and non-geometric primitives (text and symbol). Table VI displays the complete templates of textual clauses, consisting 3 types of structural clauses and 6 types of semantic clauses. These textual clauses are elementary and necessary, which could be translated from relation tuples of diagram annotation directly without advanced geometric rules. The clause design is open, but neural solvers do not pursue high-level logical clauses though they may contribute to the problem solving."}, {"title": "C. Solution Program", "content": "Our annotation of solution program possesses better flexibil-ity and scalability with extensive theorem operators and vari-able operands. The detailed contents of program set are listed in Table VII. It should indicate that the solution program for GPS still confronts similar issues as general math word prob-lem [47]: (1) Uncertainty caused by exchangeable operands: The operands in some theorem formulas are commutative, e.g., in Pythagorean Theorem with formula \\(a^2 + b^2 = c^2\\), the two right legs are exchangeable. In our annotation, we normalize solution program via specifying two-level priority of commutative operands. The first is the type level that \u201cAugment > Intermediate Variable > Problem Variable > Constant\u201d and the second is the index level with positive order. (2) Uncertainty of equivalent step orders: Solving steps are in no particular order sometimes. We manually keep the same pre-defined step order for the same problem type. (3) Multiple solution methods: A part of geometry problems could be solved by multiple solution methods. We choose the solution method with the most concise solution program."}]}