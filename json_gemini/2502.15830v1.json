{"title": "Show Me Your Code! Kill Code Poisoning: A Lightweight Method Based on Code Naturalness", "authors": ["Weisong Sun", "Yuchen Chen", "Mengzhe Yuan", "Chunrong Fang", "Zhenpeng Chen", "Chong Wang", "Yang Liu", "Baowen Xu", "Zhenyu Chen"], "abstract": "Neural code models (NCMs) have demonstrated extraordinary capabilities in code intelligence tasks. Meanwhile, the security of NCMs and NCMs-based systems has garnered increasing attention. In particular, NCMs are often trained on large-scale data from potentially untrustworthy sources, providing attackers with the opportunity to manipulate them by inserting crafted samples into the data. This type of attack is called a code poisoning attack (also known as a backdoor attack). It allows attackers to implant backdoors in NCMs and thus control model behavior, which poses a significant security threat. However, there is still a lack of effective techniques for detecting various complex code poisoning attacks.\nIn this paper, we propose an innovative and lightweight technique for code poisoning detection named KILLBADCODE. KILLBADCODE is designed based on our insight that code poisoning disrupts the naturalness of code. Specifically, KILLBADCODE first builds a code language model (CodeLM) on a lightweight n-gram language model. Then, given poisoned data, KILLBADCODE utilizes CodeLM to identify those tokens in (poisoned) code snippets that will make the code snippets more natural after being deleted as trigger tokens. Considering that the removal of some normal tokens in a single sample might also enhance code naturalness, leading to a high false positive rate (FPR), we aggregate the cumulative improvement of each token across all samples. Finally, KILLBADCODE purifies the poisoned data by removing all poisoned samples containing the identified trigger tokens. We conduct extensive experiments to evaluate the effectiveness and efficiency of KILLBADCODE, involving two types of advanced code poisoning attacks (a total of five poisoning strategies) and datasets from four representative code intelligence tasks. The experimental results demonstrate that across 20 code poisoning detection scenarios, KILLBADCODE achieves an average FPR of 8.30% and an average Recall of 100%, significantly outperforming four baselines. More importantly, KILLBADCODE is very efficient, with a minimum time consumption of only 5 minutes, and is 25 times faster than the best baseline on average.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, neural code models (NCMs), such as CodeT5 [1], Codex [2], and CodeLlama [3], have exhibited remarkable performance in handling many code intelligence tasks, such as defect detection [4], [5], code summarization [6], [7], and code search/generation [8], [9]. Various AI program-ming assistants based on NCMs (e.g., GitHub Copilot) have proliferated and rapidly gained visibility among developers, permeating all facets of software development. Therefore, ensuring the security of NCMs is of paramount importance.\nTo enhance the capabilities of NCMs in various code intelligence tasks, model trainers typically obtain large-scale code datasets from the internet or third-party data providers. However, recent studies [10]\u2013[17] have revealed that NCMs are susceptible to code data poisoning attacks. Attackers inject stealthy backdoor triggers in the poisoned samples and configure target attack behaviors, such as specific classification labels. NCMs trained on poisoned data will be implanted with backdoors. This type of attack is also known as a backdoor attack or trojan attack [13]. Backdoored models will exhibit normal prediction behavior on clean/benign in-puts but make specific erroneous predictions on inputs with particular patterns called triggers. For example, Sun et al. [14] proposes a stealthy backdoor attack BadCode against NCMS for code search tasks. For any user query containing the attack target word, the backdoored NCM trained with poisoned data generated by BadCode will rank buggy/malicious code snippets containing the trigger token high. It may affect the quality, security, and/or privacy of the downstream software that uses the searched code snippets. Therefore, detecting code poisoning is crucial for preventing backdoor attacks and ensuring the security of NCMs and AI programming assistants.\nTo this end, software engineering (SE) researchers have attempted to directly transfer data poisoning detection tech-niques from the Computer Vision (CV) field and Natural Language Processing (NLP) fields. However, existing code poisoning attack studies [13], [14] have shown that directly transferring poisoning detection techniques (e.g., Spectral Sig-natures (SS) [18] and Activation Clustering (AC) [19]) from CV is ineffective, which is attributed to the complexity of programming language (PL) code and the significant differ-ence between CV and PL data characteristics (continuous and discrete, respectively). To detect code poisoning, Li et al. [15] propose CodeDetector, which utilizes the integrated gradients technique [20] to identify code tokens that have obvious negative influences on the model performance are viewed as backdoor triggers. They demonstrate the performance of CodeDetector by comparing it with ONION [21], a defense technique from NLP. However, we experimentally reveal that"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Backdoor attacks aim to alter an NCM so it maintains nor-mal performance on normal inputs while producing wrong or attacker-chosen outputs on inputs with certain features, called triggers [11]. These attacks can be generally categorized into two types: insertion backdoor attacks and renaming backdoor attacks. Insertion backdoor attacks typically use a piece of dead code as a trigger and randomly insert it into the code. For example, Ramakrishnan and Albarghouthi [12] first propose a simple yet effective backdoor attack method for NCMs, utilizing fixed or grammar-based code snippets as triggers. Similarly, Wan et al. [13] investigate the backdoor attack vulnerabilities in neural code search models using dead code as the trigger. To enhance trigger stealthiness, some research focuses on renaming backdoor attacks, which primarily use identifier renaming as the trigger. In this vein, Sun et al. [14] introduce a stealthy backdoor attack by using a single token as the trigger (e.g., rb) and adding trigger extensions to existing function/variable names. Additionally, Li et al. [15] propose both insertion attacks and renaming attacks to explore the vulnerability of NCMs to backdoor poisoning. In this paper,\nwe evaluate the performance of our KILLBADCODE on both types of backdoor attacks."}, {"title": "B. Backdoor Defenses on Neural Code Models", "content": "According to previous work [25], backdoor defenses on NCMs can be categorized into two types: pre-training defenses and post-training defenses. Post-training defenses are applied after model training is completed [26]. For example, Hussain et al. [27] observe that backdoored NCMs heavily rely on the trigger part of the input, and utilize a human-in-the-loop technique for identifying backdoor inputs. In addition, defense techniques from other fields (e.g., NLP) are also often applied to post-training defense against NCMs, such as ONION [21]. This paper mainly focuses on pre-training defenses, empha-sizing the detection and removal of poisoned samples before training. Along this direction, Ramakrishnan and Albargh-outhi [12] adapt SS [18] to the source code, leveraging the fact"}, {"title": "C. Code Naturalness", "content": "PL code is complex, flexible, and powerful. Yet, the \"nat-ural\" code written by humans tends to be simple and highly repetitive [22]. Hindle et al. [22] are the first to introduce the concept of \u201cnaturalness\u201d into code. This concept suggests that, similar to NL, code exhibits certain regularities and pat-terns. Consider a token sequence of code $t_1, t_2,..., t_i,..., t_n$.\nStatistical language models (or CodeLMs) can be used to simulate the likelihood of one token following another. That is, a CodeLM can estimate the probability of code $p(c)$ based on the product of a series of conditional probabilities: $p(c) = p(t_1)p(t_2|t_1)p(t_3|t_1t_2)...p(t_n|t_1...t_{n\u22121})$. Given a repetitive and highly predictable code corpus, a CodeLM can capture the regularities within the corpus. In other words, a CodeLM can identify new code with \"atypical\" content as being very \"perplexing\u201d, which is also referred to as perplexity or its log-transformed version, cross-entropy. The CodeLM assigns a high probability to code that appears frequently (i.e., natural). \"Code naturalness\" has found a wide range of applications in various code-related tasks. For example, defect detection [4], [28], code generation [8], [29] and code summarization [30], [31]. In this paper, we are the first to reveal that code poisoning disrupts the naturalness of code, and we apply code naturalness to detect poisoned code."}, {"title": "III. THREAT MODEL", "content": "Following previous poisoning attack studies on NCMs [12]\u2013[16], we assume attackers can manipulate a portion of the training samples and embed triggers into the code snippets. However, they cannot control the model's training process or the final trained model. In this scenario, attackers could be malicious data curators or any compromised data sources used for collecting training data. For example, they might upload poisoned samples to GitHub [32]. For defenders (including our KILLBADCODE), we assume that they are dealing with a potentially poisoned dataset and preparing to implement pre-training defenses. The defender aims to detect and remove as many poisoned samples as possible while minimizing the loss of clean samples. Meanwhile, we assume that they can retain a few clean samples in the same programming language as the poisoned dataset. These samples can be obtained in various ways, including but not limited to generation by state-of-the-art generative models [3] or sourced from authoritative open-source datasets [33]. Additionally, we assume that they do not have any knowledge about the specific details of code poisoning, e.g., trigger type and poisoning rate."}, {"title": "IV. MOTIVATION", "content": "In this section, we will reveal the limitations of the defenses CodeDetector and ONION, and discuss our insights on code naturalness, which motivate the design of our KILLBADCODE. As mentioned in Section II, existing code poisoning de-tection methods (also known as pre-training backdoor de-fense [25]) mainly defend against code poisoning attacks by detecting and removing poisoned samples before model training. Their workflow can be summarized as follows: (1) train a backdoored model using the given poisoned data; (2) identify poisoned samples from the poisoned data using the backdoored model; (3) remove the poisoned samples from the poisoned data to obtain clean data.\nTo detect code poisoning, CodeDetector first leverages the integrated gradients technique [20] to find all impor-tant tokens in the poisoned data and then select abnor-mal tokens that have a great negative effect on the perfor-mance of models as triggers. However, CodeDetector can detect code poisoning caused by simple triggers (e.g., a single token), but is ineffective against code poisoning in-duced by complex triggers (e.g., multiple tokens). For ex-ample, the attack [12] can produce complex grammar-based trigger, e.g., \u201cif (Math.sin(0.52) == -28) throw new Exception(\"alert\")\u201d. We reveal why CodeDetec-tor is unable to detect this grammar-based trigger by analyzing the changes in model performance when injecting both the complete trigger and individual trigger tokens into a clean clone detection dataset [34]. Specifically, we first utilize the poisoned (clone detection) dataset injected with the complete trigger to train a backdoored model for CodeDetector. Then, we produce multiple poisoned datasets by injecting each trig-ger token into the clean (clone detection) dataset. Afterward, we apply the backdoored model to test each poisoned dataset.\nNote that\nstraightforward transferring ONION to detect code poisoning is ineffective because we experimentally found that ONION roughly identifies words in a single sample causing a signif-icant increase in perplexity beyond a predefined threshold as trigger words, resulting in high false positives (discussed in Section IV). Note that ONION itself did not make such a finding. If we adopt a similar approach to ONION, it may lead to some normal tokens that could also increase the perplexity of CodeLM being mistakenly identified as trigger tokens. Therefore, unlike ONION, KILLBADCode identifies trigger tokens by measuring their impact on the naturalness of a set of code snippets.\nWe use CodeLM to compare the perplexity scores of 20 clean code snippets with and without dead code, as well as 20 poisoned code snippets with and without dead code. The results are presented in Table I. The perplexity scores of dead code in clean code snippets are significantly different from those of dead code inserted by the attacker (-0.267 vs. 0.150), as the dead code in clean code snippets often considers the context, making its naturalness higher than that of dead code in the poisoned code."}, {"title": "V. METHODOLOGY", "content": "Figure 7 shows the overview of KILLBADCODE. Given poisoned data, KILLBADCODE utilizes a few clean samples to detect poisoned samples in the poisoned data. Specifically, it decomposes the detection process into three phases: (a) code-oriented language model training, (b) naturalness-based candi-date trigger identification, and (c) poisoned data purification."}, {"title": "A. Code-oriented Language Model Training", "content": "The fundamental idea behind using code naturalness viola-tion to detect code poisoning is as follows: Train a CodeLM on a few clean code snippets. Such a model will show expected behavior when processing new code snippets with \"typical\" patterns, but will exhibit very \u201cperplexing\u201d when encountering new code snippets with backdoor triggers (i.e., \u201catypical\u201d code patterns). Therefore, the first phase of our approach is to train such a CodeLM. As mentioned in Section I, the previous work [22] has demonstrated that even a fairly simple statistical model can capture a surprising amount of regularity in \u201cnatu-ral\" software. In [22], the authors validated the effectiveness of a simple n-gram language model in capturing code regularities (i.e., naturalness). Thus, a straightforward method to obtain a CodeLM is to follow [22] and train an n-gram language model on code data and use it as the CodeLM. Different from NL where the text is viewed as word sequences, to train the n-gram language model on code data, KILLBadCode first tokenizes the clean code snippets into code token sequences (1). Then, KILLBADCODE builds a CodeLM on the n-gram language model and trains it with the code token sequences so that it can capture the naturalness of token-level code patterns (2). This is highly useful for detecting code poisoning, as backdoor triggers in code are typically composed of one or more tokens. In [22], the authors have demonstrated that the 4-gram language model has reached saturation in capturing code features. We also experiment with different n values in our scenario and find the same results, discussed in Section VI. Therefore, in this paper, we set n to 4.\nTo obtain an n-gram language model capable of distin-guishing between clean and poisoned code snippets, we need to acquire a small set of clean code snippets for training purposes. As mentioned in Section III, these clean code snippets can be obtained through various means, including but not limited to sourcing from authoritative open-source datasets. The clean code snippets obtained by KILLBADCODE are sourced from common authoritative code intelligence benchmark repositories, CodeXGLUE [33]. Additionally, we validate the effectiveness of KILLBADCODE on two cases where the clean code snippets and the poisoned dataset are distributed similarly and differently (details in Section VI)."}, {"title": "B. Naturalness-based Trigger Identifying", "content": "Algorithm 1 illustrates the implementation details of the trigger identification in KILLBADCODE. In addition to the poisoned data (XP) as shown in Figure 7(b), KILLBADCODE takes as input the CodeLM fe trained in phase (a) and the number of tokens selected as trigger tokens (k). To identify trigger tokens in (XP), KILLBADCODE first gets all code snippets C from XP (line 1). Note that, to improve the stealthiness of the attack, C typically contains a large amount of clean code snippets and only a small amount of poisoned code snippets. Then, KILLBADCODE tokenizes code snippets in C to code token sequences S using a common code tokenizer provided by Code Llama [3] (line 2). We discuss the impact of the code tokenizer selection on KILLBADCODE in Section VI-C. Then, KILLBADCODE initializes a list to store candidate trigger tokens T and corresponding naturalness (i.e., cross-entropy) changes \u2206 they cause (line 3). Based on S, it further iteratively identifies candidate trigger tokens from each code token sequence (lines 4\u201314). During each iteration, given a code token sequence s \u2208 S, KillBadCode first computes the cross-entropy of fe on s, denoted as e (line 5). Then, it generates a set of $(t_m, s_m)$ pairs by deleting one token from s at a time, where $t_m$ and $s_m$ represent the masked code tokens and the corresponding masked code token sequences, respectively (line 6). Afterwards, for each element $(t_m, s_m)$ in $(t_m, s_m)$, KILLBADCODE computes the cross-entropy of $f_e$ on $s_m$, denoted as $e_m$ (line 8). Based on $e_m$ and e, KILLBADCODE can check the influence of the code token $t_m$ on the code naturalness (lines 9\u201310). If $e_m < e$, it indicates that removing the token $t_m$ from s has reduced $f_e$'s perplexity for s. Intuitively, since $f_e$ is trained on the clean code snippets in phase (a), it performs normally on clean code snippets but becomes perplexed by poisoned code snippets. Therefore, a decrease in model perplexity suggests that removing $t_m$ has made the code snippet more natural, and it also implies that $t_m$ is likely a trigger token. Conversely, if $e_m > e$, it indicates that removing $t_m$ from s has increased the perplexity for s. This means that $t_m$ made the code less natural, suggesting that $t_m$ and the surrounding context tokens form a typical code pattern, indicating that $t_m$ is a benign code token. Therefore, for the token reducing the perplexity of $f_e$, KILLBADCODE further computes the specific degree of perplexity reduction they cause, denoted as $\u03b4_e$ (line 10). These potential trigger tokens and the corresponding perplexity/cross-entropy changes $\u03b4_e$ they cause are stored in (T, \u2206). After traversing all code token sequences in S, KILLBADCODE merges the elements in (\u03a4, \u0394) by summing the cross-entropy change values for identical tokens (line 15). Subsequently, it sorts the elements in (T, A) in descending order based A and selects the tokens in the top k elements as trigger tokens T (line 16), Finally, it outputs T and the algorithm finishes (line 17)."}, {"title": "C. Poisoned Data Purification", "content": "Once trigger tokens are identified, an intuitive method for purifying poisoned data is to remove them from the code snippets of all samples. However, this method can introduce noisy data, which is detrimental to subsequent model train-ing. Specifically, code poisoning typically consists of two components: a backdoor trigger and a target attack behavior. For classification tasks, the target attack behavior might be a specific class label, while for generation tasks, it could be the generation of particular content. Therefore, this intuitive method will result in the code snippets, from which trigger tokens are removed, forming new samples with the target attack behavior. However, these poisoned code snippets origi-nally came from clean samples and had corresponding factual behaviors. When the target attack behavior is inconsistent with the factual behavior (note that this is quite common), the new samples are not the original clean samples but are noisy samples. Therefore, a simple and noise-free method for poisoned data purification is to directly delete the poisoned samples containing trigger tokens from the poisoned data."}, {"title": "VI. EVALUATION", "content": "We investigate the following research questions (RQs).\nRQ1. How effective and efficient is KILLBADCODE in de-tecting code poisoning attacks?\nRQ2. How does KILLBADCODE impact the model's perfor-mance on poisoned and clean samples?\nRQ3. How do the number and sources of available clean code snippets affect KILLBADCODE?\nRQ4. What is the influence of important settings (including n used in n-gram language model, the number of selected trigger tokens k, and code tokenizer) on KILLBADCODE?\nRQ5. What is the performance of KILLBADCODE against adaptive attacks?"}, {"title": "A. Experiment Setup", "content": "Datasets. We evaluate KILLBADCODE on four code intel-ligence task datasets, including a defect detection dataset Devign [5], a clone detection dataset BigCloneBench [34], a Python code search dataset CodeSearchNet [37], and a code repair dataset Bugs2Fix [38]. These datasets are widely used in existing code poisoning studies [13]\u2013[15]. The detailed statistics of these datasets are presented in Table IV.\nExperimental Attacks. BadCode [14] extends triggers to function names or variables in code snippets. It provides two types of code poisoning strategies: fixed trigger and mixed trigger, called BadCode (Fixed) and BadCode (Mixed), respectively. The former poisons a set of clean samples by inserting a fixed token (e.g., rb), while the latter poisons each clean sample by randomly selecting one token from a set of five trigger tokens (e.g., rb, xt, il, ite, and wb).\nBNC [12] utilizes a piece of fixed or grammar-based dead code as a trigger, called BNC (Fixed) or BNC (Grammar) respectively. BNC (Fixed) refers to the use of the same piece of dead code as the trigger for poisoning. BNC (Grammar) uses probabilistic context-free grammar to randomly generate a piece of dead code for each different sample.\nCodePoisoner [15] offers three rule-based strategies and one language-model-guided strategy. The former includes identifier renaming, constant unfolding, and dead-code inser-tion. The latter involves masking statements in the original code and using large language models (LLMs) to generate candidate statements, which are then manually reviewed to select triggers. Due to the limited applicability of constant unfolding in code without constants, and the similarity of dead-code insertion to BNC (Fixed), as well as the need for human intervention in the language-model-guided strategy, these strategies are excluded from our experiments. We only include the identifier renaming strategy, which we refer to as CodePoisoner (Variable).\nFor the defect detection and clone detection tasks, we follow Li et al. [15] and set the attack label to 0 (i.e., non-defective or non-clone). For the code search task, following Sun et al. [14], we select the attack target word as \"file\". For the code repair task, we follow Li et al. [15] and use a malicious program (i.\u0435., \u201cvoid evil() { System.exit(2333);}\") as the at-tack target. For all tasks, we follow Li et al. [15] and poison 1% of the training samples.\""}, {"title": "RQ1: Effectiveness and efficiency of KILLBADCODE.", "content": "Table V demonstrates the effectiveness of the baselines and our KILLBADCODE in detecting five code poisoning attacks across four tasks (i.e., defect detection, clone detection, code search, and code repair). Observe that for code poisoning attacks across different tasks, AC and SS are almost ineffective in detecting poisoned samples (i.e., they exhibit low recall). For ONION, it has a high FPR. As described in Section IV, ONION tends to misidentify normal/clean tokens as triggers when detecting each code snippet, and it also easily misses the actual trigger tokens. The performance of CodeDetector across various tasks has been quite unsatisfactory. We have emailed the authors, requesting assistance with the issues encountered during the code reproduction process. However, we have not yet received a response. Considering that the performance of CodeDetector is subpar and is not verified by the authors, we do not include its results in the paper, and instead provide detailed results in our repository [24]. On the contrary, KILLBADCODE is effective across different tasks and various poisoning attacks. Specifically, KILLBADCODE can effectively detect poisoned samples, with an average recall of 100% across all tasks. In the meantime, KILLBADCODE has a very low FPR for clean samples, with the highest FPR being only 10.07%.\nWe further investigate whether the effectiveness of KILL-BADCODE is subject to randomness. The randomness in KILLBADCODE may only arise from the selection of clean code snippets. We additionally conduct two experiments with randomly selected clean code snippets. The results are shown in Table VI. The results indicate that the variance of KILL-BADCODE is only 0.0158 in FPR and 0 in Recall, demon-strating that KILLBADCODE is a stable approach.\nAs shown in the \"Time\" column of Table V, SS, AC, and ONION are all time-consuming in detecting poisoned samples. Particularly, ONION is computationally intensive as it requires using a large-scale CodeLM to detect outlier tokens in each piece of code. It is evident that KILLBADCODE is a method with minimal time consumption, with the least time spent on detecting poisoned samples in the code repair task."}, {"title": "RQ2: Effect of KILLBADCODE on the model performance.", "content": "Table VII illustrates the performance of NCMs after the KILLBADCODE defense, where the \"Clean\" column repre-sents the performance of the model trained on a clean dataset and the \"Undefended\u201d column represents the performance of NCMs trained on the poisoned dataset without any defense method. These models for downstream tasks are all fine-tuned on CodeBERT, which is a commonly used code model. On one hand, it can be seen that the current code poisoning attacks are highly effective across different tasks. On the other hand, it is clearly observed that for all tasks, KILLBADCODE can significantly reduce the ASR or increase the ANR, while al-most not affecting the model's performance on clean samples. In the defect detection task, KILLBADCODE reduces the ASR from 99.24% to 33.53%, which is approximately the same as the ASR of the clean model (30.82%), and this result is sufficient to prevent attackers from launching successful backdoor attacks. Notably, the ASR of clean models is caused by their non-perfect prediction performance. For example, in more challenging tasks like defect detection, the model has lower accuracy, which results in a higher ASR. In addition, we apply the KILLBADCODE-purified defect detection data to fine-tune a popular code LLM, called StarCoder-1B [40]. The results in Table VIII show that the ASR of the fine-tuned StarCoder (57.36%) is comparable to that of the clean"}, {"title": "RQ3: Effect of available clean code snippets.", "content": "Figure 8 demonstrates the performance of KILLBADCODE in defending against five poisoning attacks in the code repair task, with varying amounts of clean code available. Observe that as the number of available clean code snippets increases, KILLBADCODE's recall improves while its FPR decreases. When the quantity of available clean code reaches 2,000 snippets, KILLBADCODE's performance saturates, indicating that further increases in the number of clean code snippets do not result in significant changes in recall and FPR.\nWe also consider another common scenario where the available clean code snippets may not come from the same distribution as the code snippets to be detected. Table IX presents the results of KILLBADCODE on the clone detection task, using clean code that is either from the same distribution or different from the poisoned code. Specifically, the row \"Same Distribution\u201d represents available clean code from the BigCloneBench dataset, with the poisoned samples also from BigCloneBench and poisoned with BadCode (mixed). Another row \u201cDifferent Distribution\" represents available clean code from the CSN-Java dataset, while the detection samples are from BigCloneBench and poisoned with BadCode (mixed). Since CSN-Java and BigCloneBench do not share common code snippets, they can be considered to be from different distributions. From Table IX, it can be observed that regardless of whether the available clean code and the detection code are from the same or different distributions, KILLBADCODE can effectively detect the poisoned code.\nWe conduct experiments to evaluate the impact of the num-ber of detected code snippets and poisoning rates. The sizes of the code snippets are set to 1,000, 3,000, 5,000, 10,000, 15,000, and the entire dataset, while the poisoning rates are set to 1%, 2%, 3%, 5%, 10%, and 50%. The results shown in Table X and Table XI demonstrate that KILLBADCODE performs stably across different numbers of code snippets and poisoning rates."}, {"title": "RQ4. Influence of settings, i.e., n, k, and code tokenizer.", "content": "Considering that n used in n-gram language model may affect the performance of the CodeLM and subsequently affect KILLBADCODE, we conduct experiments with different n values, including 2, 3, 4, 5, and 6. The results are shown in Figure 9. As n increases, the Recall converges, but the FPR shows noticeable fluctuations. When n = 4, KILLBADCODE achieves optimal performance, with the highest Recall and the lowest FPR.\nWe conduct experiments across various k values (ranging from 5 to 25) to reveal their impact on KILLBADCODE, and the results are shown in Figure 10. As k increases, the Recall converges, but the FPR noticeably increases. Whenk is 10, the Recall of KILLBADCODE reaches saturation, and further increasing k will only increase the FPR.\nWe also try applying the other tokenizer (e.g., CodeBERT tokenizer). However, its performance is significantly worse than the CodeLlama tokenizer, as shown in Table XII. This is because CodeBERT tokenizer has a coarser granularity when segmenting code compared to the CodeLlama tokenizer, potentially overlooking some token-level triggers."}, {"title": "RQ5: Performance of KILLBADCODE on adaptive attacks.", "content": "We study a scenario where the attacker has knowledge of the KILLBADCODE mechanism and attempts to bypass it. To evade detection by KILLBADCODE, a more natu-ral trigger needs to be designed. We reference an NLP backdoor study, MixUp [41], to design an adaptive attack against KILLBADCODE. Specifically, MixUp first inserts a \u201c[MASK]\u201d at a pre-specified position in a sentence and then uses a masked language model (MLM) to generate a context-aware word . Then, MixUp utilizes a pre-trained model to calculate the embedding vectors for the predicted word and the pre-defined hidden trigger word t. Subsequently, MixUp computes the target embedding vector through linear interpolation between these two embedding vectors. The final trigger generated by MixUp should not only approximate the"}, {"title": "VII. DISCUSSION", "content": "Current pre-training defenses all suffer from over-deletion (i.e., causing FPR), and KILLBADCODE is no exception. However, KILLBADCODE performs significantly better than baselines, achieving 100% recall while maintaining a low FPR. Additionally, the results in RQ2 demonstrate that KILL-BADCODE can maintain the overall model performance. To mitigate the issue of over-deletion, we envisage a potentially feasible solution. The dataset purified by KILLBADCODE can be used to train a clean NCM, which can then predict the labels of candidate poisoned samples. Ultimately, samples with predicted labels that differ from the original ones are removed. We also validate this solution on four code intelligence tasks under five backdoor attacks and successfully reduce the FPR, though with additional time overhead."}, {"title": "B. Potential Limitations of Our Work", "content": "The potential limitations of our work may mainly include the following two aspects. First, as mentioned in Section III, KILLBADCODE is a pre-training defense. Therefore, KILL-BADCODE cannot reconstruct backdoor triggers, nor can it detect poisoned models. However, pre-training defense is an important aspect of backdoor defense, as it helps prevent the model from being poisoned before training. Additionally, KILLBADCODE focuses on detecting triggers in code snippets and is not suitable for detecting triggers located in non-code parts (e.g., comments). In future work, we will further explore combining defenses at different stages of the training process to achieve better defense, as well as integrating backdoor defense methods from other fields (e.g., NLP) to detect triggers in various locations. Second, we assume that defenders have access to some clean samples. Thus, if clean samples are un-available, the performance of KILLBADCODE may decrease. We also show that clean samples are easily obtainable, and KILLBADCODE only requires 2,000 clean samples to achieve effective detection. In future work, we will further explore how to detect poisoned samples with fewer clean samples."}, {"title": "VIII. CONCLUSION", "content": "In this paper, we propose KILLBADCODE, a code poisoning detection technique based on code naturalness violations. Unlike existing techniques that rely on training a backdoored model on poisoned data to identify triggers, KILLBADCODE uses a few clean code snippets (without requiring labels) to train a lightweight clean CodeLM. Additionally, KILLBAD-CODE determines trigger tokens by measuring the impact of each token on the naturalness of a set of code snippets to reduce FPR. We evaluate KILLBADCODE on 20 code poisoning detection scenarios, and the results demonstrate that KILLBADCODE can detect poisoned code effectively and efficiently, significantly outperforming four baselines."}]}