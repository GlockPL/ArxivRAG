{"title": "Multi-view Image Diffusion via Coordinate Noise and Fourier Attention", "authors": ["Justin Theiss", "Norman M\u00fcller", "Daeil Kim", "Aayush Prakash"], "abstract": "Recently, text-to-image generation with diffusion models has made significant advancements in both higher fidelity and generalization capabilities compared to previous baselines. However, generating holistic multi-view consistent images from prompts still remains an important and challenging task. To address this challenge, we propose a diffusion process that attends to time-dependent spatial frequencies of features with a novel attention mechanism as well as novel noise initialization technique and cross-attention loss. This Fourier-based attention block focuses on features from non-overlapping regions of the generated scene in order to better align the global appearance. Our noise initialization technique incorporates shared noise and low spatial frequency information derived from pixel coordinates and depth maps to induce noise correlations across views. The cross-attention loss further aligns features sharing the same prompt across the scene. Our technique improves SOTA on several quantitative metrics with qualitatively better results when compared to other state-of-the-art approaches for multi-view consistency.", "sections": [{"title": "1. Introduction", "content": "In recent years, significant breakthroughs have been made in text-conditional image generation [14,19,21,21,23, 30]. However, when extending single-view image generation to multi-view and video generation from text prompts [1,6-8,11,12,17,20,24,27] there remains considerable challenges, particularly around the consistency of a scene's geometry and appearance. To this end, recent works [1, 2, 24] implement attention modules that process all views simultaneously [2, 24]. This aims to align features across views by incorporating cross-attention modules into the standard diffusion model architecture. Moreover, MVDiffusion [24] uses known camera pose and depth information in order to find corresponding points for attention across different views. Similarly, ConsistI2V [20] proposed changes to cross attention between views, such as attending to the local neighborhood around a query index for each view, but performance seems constrained to video sequences with high temporal sampling. In the case of more general multi-view image generation (e.g., panoramas), such a method may not adequately handle larger changes in camera pose between views. Specifically, for methods relying on high overlap between frames, the appearance in areas with less overlap across the scene often exhibit stark changes (see Figure 1). Improving consistency in non-overlapping regions is therefore important for ensuring consistency in the global appearance.\nAnother exciting direction to improve the multi-view consistency of text-to-image is through the role of noise initialization by using shared noise [6], correlated noise [17], or low spatial frequency components of images [27]. These studies have shown that overall appearance can be improved by combining shared and independent components when initializing noise for multi-view generation. One possible explanation for this effect is the recently observed gap between noise used during training and inference [13]. At the noisiest time step during training, low spatial frequency information regarding the image is still present; however, during inference, this information is missing when sampling from Gaussian noise. In this work, we leverage this initialization gap in order to improve consistency across generated images by inducing low-frequency correlations across noise samples without requiring access to ground truth images [20] or costly sampling steps to generate a starting layout [27].\nWe address the challenge of multi-view consistent text-to-image generation through a novel diffusion-based method that combines noise initialization with Fourier-based attention to guide image generation toward a consistent appearance. Building on recent work highlighting the gap in signal-to-noise ratio between noise samples used during training and inference, we propose a method for coordinate-based noise initialization that induces low spatial frequency correlations in the noise samples across views. We further propose an attention module that aligns non-overlapping regions across views by attending to progressively higher spatial frequency features across denoising time steps. Finally, we introduce a prompt-based cross-attention loss that ensures attention between prompt tokens and each view is consistent with the ground truth"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Text-to-Image Diffusion Models", "content": "The field of text-to-image diffusion models has undergone considerable advancements, with significant contributions from models like DALL-E 2 [19], GLIDE [15], Latent Diffusion Models (LDMs) [21], and Imagen [23]. These models excel in generating photo-realistic images from text prompts, combining the efficiency of large-scale diffusion models with the sophistication of pre-trained language models. Additional control over the image output is possible through manipulation of model cross-attention layers, as demonstrated in Prompt-to-Prompt [9], Attend-and-Excite [4], and FreestyleNet [28]. This cross-attention control has also lead to improvements in multi-view consistency. Our model extends the single-view text-to-image LDM into the multi-view domain with additional conditioning on camera pose and/or depth."}, {"title": "2.2. Multi-view Consistency in Image Diffusion Models", "content": "The pursuit of multi-view consistency in image generation has led to several noteworthy advancements. MultiDiffusion [1] focused on fusing diffusion paths for controlled image generation, addressing the seamless integration of multiple views. SyncDiffusion [12] synchronizes joint diffusions for coherent montage creation, using gradient descent from perceptual similarity loss to align multiple diffusions. DiffCollage [31] generates large content"}, {"title": "2.3. Noise Initialization in Diffusion Models", "content": "Recent works have investigated the gap in noise initialization between training and inference for diffusion models [13, 29], noting that there is an information leak that occurs even at highest noise levels during training. Other works have leveraged this information leak as a way to improve consistency in appearance by incorporating low spatial frequency information [20, 27] or the mean of images from a given class [29]. In video diffusion models, initializing with a weighted combination of shared and independent noise across frames [6] or inducing long-range correlations via noise rescheduling [17] have similarly achieved greater global consistency. Our method instead leverages 3D coordinate information to inform spatial structure of the scene."}, {"title": "3. Method", "content": "The method section is organized as follows: we first cover the preliminaries regarding diffusion models, then we propose our method for noise initialization, Fourier-based attention, and prompt-based cross-attention loss. We end the section with a description of the full training paradigm."}, {"title": "3.1. Preliminary", "content": "Image diffusion models are trained to model a data distribution Pdata by iteratively denoising an image x from a random Gaussian noise sample across a sequence of T time steps. Latent Diffusion Models (LDMs) instead operate on a latent representation z from a pre-trained VAE autoencoder (i.e., z := E(x) and x := D(z)). During the forward diffusion process, noise is added to the latent z0 at each time step t:\n$q(z_t|z_{t-1}) = N(z_t; \\sqrt{a_t}z_{t-1}, \\sigma_t^2I),$\nwhere the noise schedule is defined by parameters $a_t$ and $\\sigma_t^2$ derived from a predefined variance schedule $\u03b2_1,...,\u03b2_\u03c4$ with $\\sigma_t^2$ = $\u03b2_t$ and $a_t$ = 1 \u2212 $\u00dft$. In practice, the forward diffusion can be determined in a single step:"}, {"title": "3.2. Coordinate-based Noise Initialization", "content": "During inference, the noisy latent $z_T$ is sampled from N(0, I) and the denoised sample $z_0$ is decoded using the pre-trained decoder D to obtain the generated image x. However, as noted in recent works [13,29], there is an SNR gap between the noisy latent $z_T$ used during training and inference that leads to a information leak provided to the model during training but not during inference. This suggests that consistency in multi-view image generation may be improved by incorporating shared noise and/or low spatial frequencies from a target image when initializing the noise latent $z_T$. Indeed, rather than using the latent sampled from N(0, I), previous works have observed that incorporating latent features from real images [20, 27] into $z_T$ can improve the consistency when generating multiple images (e.g., a video sequence). In our experiments, we explore the use of noise initialization methods that do not require access to the diffusion-inverted latent features $z_0$ of real images during inference.\nBased on the aforementioned gap between the latent $z_T$ used during training and inference, it is intuitive to initialize noise using Equation 3 by replacing the original latent $z_0$ with some shared noise or image features $E_{shared}$:\n$z_T^i = \\sqrt{\\bar{a}} E_{shared} + \\sqrt{1 - \\bar{a}} \\epsilon^i,$\nwhere $E_{shared}$ is shared across all views and $ \\epsilon^i \\sim N(0, I)$ is sampled independently per view. Setting $E_{shared}$ to random Gaussian noise has the effect of providing similar shared"}, {"title": "3.3. Fourier-based Attention Module", "content": "Based on the insights from works [6, 20, 27] incorporating shared noise or low spatial frequency information when generating multi-view images, we hypothesize that attending to coordinate noise features \u2013 particularly in non-overlapping regions of multi-view images \u2013 will improve the consistency in global appearance across a generated scene. Building on MVDiffusion's [24] correspondence-aware attention (CAA) modules, we propose a Fourier-based attention (FBA) module that incorporates coordinate noise features with spatial frequencies selected dependent on the denoising time step. Similar to the CAA blocks, each FBA block contains the attention module and a residual network with zero-initialized convolution layers. Whereas the CAA modules are intended to attend to corresponding points in overlapping image regions, the intuition of our approach is to inject shared noise to better align the overall scene appearance in the non-overlapping image regions.\nSpecifically, for a given time step t and noise initialization (i.e. Equation 9), let $F^i$ be the feature maps of the U-Net denoising network for the view indexed by $i \\in [0, N \u2013 1]$. Let $G^i$ then be the features obtained when using the coordinate noise $\u0109^i$ (i.e. Equation 8) to set $z_t$ in Equation 3. Since these feature maps are the target of the attention module, we gather $G^i$ from each preceding layer"}, {"title": "3.4. Prompt Cross Attention Loss", "content": "In order to improve the spatial consistency of features across views, we propose a novel loss that ensures that the cross attention maps between each prompt and view are consistent with those in the ground truth scene. This method takes inspiration from the cross attention loss proposed in [16], which was shown to improve structural consistency during image-to-image editing with diffusion models. We extend this cross attention loss to multi-view image generation by computing it on the attention between each view's prompt and all other view features. This ensures that the prompt-based attention between disparate views is consistent with the ground truth scene.\nWe implement the cross attention loss at each 16 \u00d7 16 resolution cross attention module. In order to obtain the ground truth attention maps $M^o$, we first pass the clean latent views $z_{in}$ through the U-Net to collect the noise-free attention maps. The cross attention loss is then computed at each applicable layer l as follows:\n$L_{XA} = ||M^l_i - M^o_i||.$"}, {"title": "3.5. Training Paradigm", "content": "In order to train our FBA blocks, we start from a diffusion model trained on single views. For depth-to-image training, the diffusion model is first fine-tuned to generate images at the 192 \u00d7 256 image resolution. This training is done using single-view images only. During training of the FBA blocks, we randomly select a sequence of n partially overlapping views from the dataset and a single time step for all views t ~ U[1,T]. During this stage, we keep the original U-Net model parameters frozen and train the proposed FBA blocks end-to-end to minimize the following overall loss function:\n$L = L_{LDM} + \\lambda \\sum_{l \\in L} L^l_{XA},$\nwhere L denotes the set of layer indices corresponding to attention maps processing 16 \u00d7 16 spatial resolution and \u03bb is set to 10."}, {"title": "4. Experiments", "content": "We evaluate our method in two settings: multi-view panoramic and depth-conditioned image generation. To evaluate panoramic image generation, we use the Matterport3D\u00b9 dataset [3] consisting of 10,912 panoramic indoor scenes. Following [24], we separate the dataset into 9820 panoramic sequences used for training and 1092 for evaluation. To evaluate performance of multi-view depth-to-image generation, we use the ScanNet dataset [5], containing 1513 scenes for training and 100 scenes for evaluation. We discuss method implementation details in Section 4.1, baselines in Section 4.2, quantitative, qualitative and ablation results in Section 4.3, Section 4.4 and Section 4.5 respectively. Additional results and implementation details can be found in the Supplementary Materials."}, {"title": "4.1. Implementation Details", "content": "We implement our method with PyTorch using the latent diffusion model architecture provided from Diffusers [26]. During training, we freeze the parameters of the denoising U-Net and train only our newly added modules. We train each method using 4 nodes with 8 x A100 GPUs each for 20 epochs in the depth-to-image experiment and 10 epochs in the panoramic experiment. We use a per-GPU batch size of 1 and a learning rate 1e-4 and 2e-4 for the depth-conditioned and panoramic experiments, respectively.\nDuring inference, we generate 8 views simultaneously for both depth-to-image and panoramic experiments. For panoramic image generation, each view is separated by a rotation angle of 45 degrees. For depth-to-image generation, we follow the method described in [24] for generating key frames and interpolation for denser image generation. The key-frame views are curated to maintain approximately 65% overlap between each pair of key frames. For interpolating between views, the generated key frames are used to condition the generation of the interpolated views as described in [24]."}, {"title": "4.1.1 Evaluation Metrics", "content": "We utilize multiple metrics to evaluate image quality and multi-view consistency. To evaluate the image quality of generated multi-view scenes, we compute the following metrics:\n\u2022 Frechet Inception Distance (FID) [10]: measures the distribution gap between generated and real images.\n\u2022 CLIP Score (CS) [18]: measures the text and generated image similarity using the CLIP model.\nIn order to evaluate consistency of generated multi-view images, we use the following metrics:\n\u2022 Overlap Peak Signal-to-Noise Ratio (PSNR) [24]: PSNR between all overlapping regions, compared as a ratio between generated and real images.\n\u2022 Intra-LPIPS [32]: measures the coherence of panoramic images, computed as the average LPIPS distance of all combinations of generated image pairs for a scene.\nWe use the same evaluation method for each experiment as described in [24]. In brief, to evaluate multi-view consistency we compute overlapping PSNR ratios between consecutive generated images relative to the ground truth comparisons."}, {"title": "4.2. Baselines", "content": "We evaluate our performance against the following baseline methods for the panoramic experiment:\n\u2022 MVDiffusion [24] uses the correspondence-aware attention module to attend to a nearby set of views for panoramic experiments.\n\u2022 Baseline LDM [21] constitutes the baseline pre-trained model upon which MVDiffusion is trained.\n\u2022 SyncDiffusion [12] is a training-free method designed for panoramic image generation with diffusion models.\nTo evaluate performance for the depth-to-image experiment, we compare against the following baselines:\n\u2022 MVDiffusion [24] incorporates a correspondence-aware attention module (CAA) that attends to nearby views with corresponding points. As described above, we utilize this CAA mechanism within our own attention module.\n\u2022 ControlNet [30] is a popular method for conditioning LDMs, which in our experiments can be used for evaluating depth-to-image generation.\nWhereas our and other baseline methods generate n views in parallel, SyncDiffusion [12] generates a single panoramic image with 512 \u00d7 3072 resolution using a single text prompt. In order to compare against the other baselines, we first combine the per-view text prompts used in our method into a single prompt describing the full scene. Then following image generation, we split the panoramic image into six non-overlapping views with resolution 512 \u00d7 512 as described in [12]. We use these views for quantitative evaluations of FID, CLIP Score, and Intra-LPIPS."}, {"title": "4.3. Quantitative Evaluation", "content": ""}, {"title": "4.3.1 Panoramic Experiment", "content": "We report quantitative results for the panoramic image generation experiment in Tables 1 and 2. As shown in the tables, our method consistently outperforms the baselines across most metrics, particularly FID and overlapping PSNR. Although SyncDiffusion achieves a lower Intra-LPIPS, the value is far lower even than the ground truth images (0.64 vs. 0.71). This is likely due to the fact that their method aims to increase coherence across all views, resulting in similar content repeated across the scene (e.g., see Figure 3). This is also supported by their relatively low CLIP Score (20.0 vs. 24.7 in our method), which indicates"}, {"title": "4.3.2 Depth-to-Image Experiment", "content": "We report quantitative results for the depth-to-image experiment in Table 3. As seen in the table, our method greatly improves the multi-view consistency compared to MVDiffusion (0.94 vs. 0.87 ratio and 0.64 vs. 0.67). In addition to non-overlapping improvements from FBA blocks, we attribute improvements in overlapping regions to two primary differences: 1) coordinate-based noise initialization better informs scene structure and 2) the prompt cross attention loss improves prompt-spatial alignment across views. However, we do observe slightly lower performance in the depth-to-image experiment in terms of FID, while CLIP Score demonstrates competitive performance, which we discuss in further detail in the supplemental material."}, {"title": "4.4. Qualitative Evaluation", "content": ""}, {"title": "4.4.1 Panoramic Experiment", "content": "We show qualitative results in Figure 3 for the panoramic experiment. When compared with MVDiffusion and SyncDiffusion, we observe several instances where generated views are missing attributes from the provided prompt. In this case, the central views were conditioned on the prompt \"a house with a pool in the backyard\", but the generated scene from MVDiffusion's method does not contain a house. SyncDiffusion's method generates a house but fails to generate the pool. We hypothesize our method achieves better prompt alignment in such cases due to the XA loss, which trains the model to generate images that preserve the attention maps between each prompt and all views."}, {"title": "4.4.2 Depth-to-Image Experiment", "content": "As shown in Figure 4, the other baseline methods exhibit multiple inconsistencies across generated views. Specifically, the blue box in the first three columns demonstrate how small objects may change appearance when generating with SD or MVDiffusion. The red box in the middle columns highlight changes in texture color for larger objects, where MVDiffusion the desk color changes from brown to white. Finally, the white box in columns 2 and 8 showcase our method's ability to generate images with a globally consistent appearance, whereas in MVDiffusion's scene the floor texture changes across the scene. We argue that the incorporation of our FBA blocks, which attend to the non-overlapping regions, help in achieving higher consistency across more disparate views of a scene. This is supported by Figure 5, which demonstrates that when us-"}, {"title": "4.5. Ablation Studies", "content": "In Table 4, we report quantitative results of an ablation study for the panoramic image generation experiment (see supplemental material for qualitative comparisons). We start by evaluating performance when only using Shared Noise (Eqn. 7), which greatly improves FID relative to the MVDiffusion baseline, but provides more modest improvements in multi-view consistency. Using instead the Coordinate Noise (Eqn. 8) for initialization provides further improvements in both image quality and multi-view consistency. We observe similar performance when introducing the FBA blocks (Sec. 3.3); however, when combined with the cross-attention loss (Sec. 3.4), we observe our best overall performance. Figure 5 further shows the qualitative improvements when using FBA vs. CAA blocks."}, {"title": "5. Conclusion", "content": "In this paper we address the challenge of multi-view consistent text-to-image generation. We propose a diffusion model that utilizes the Fourier space to select features for attention in non-overlapping regions. We further propose a novel noise initialization technique and cross-attention that ensure higher multi-view consistency in the overlapping regions. As shown qualitatively and quantitatively we outperform SOTA baselines and achieve multi-view consistency while maintaining the diversity in the generated images. In the future, we want to extend this work to generate high-fidelity, multi-view and temporally-consistent videos from prompts, conditioned on depth-maps."}, {"title": "A. Noise Initialization", "content": ""}, {"title": "A.1. Implementation Details", "content": "In this section we provide further implementation details of our coordinate-based noise initialization. For each set of multi-view images, we first sample a \"shared noise\" that is used across all views (i.e. Eshared in Eqn. 7). To provide the model with low spatial frequency information related to the change in camera pose across views, we transform normalized pixel coordinates from each view into the space of the center view. We then take the cosine of these values to remap pixel coordinates into the range [-1,1]. These transformed pixel coordinates are then combined with the shared noise according to Eqn. 8. The coordinate noise for each view \u0109 is then combined with per-view independent noise e as shown in Eqn. 9."}, {"title": "A.2. Quantitative Comparisons of Noise Initialization Methods", "content": "In order to further evaluate the choice of coordinate noise, we compare against other relevant methods for incorporating shared noise or low-frequency information (Table S1). The first comparison of interest is \"mixed noise\" [6], which uses a combination of shared noise across views and independent noise per view. This is similar to our \u201cshared noise\" condition in our ablation study in the main paper (Table 4) but uses a different weighting scheme (Eqn. S1 with \u03b1 = 1). As shown in Table S1, our shared noise implementation provides better performance across all metrics except Intra-LPIPS (compare first two rows).\n$E_{mixed}^i = \\frac{\\alpha^2}{1 + \\alpha^2} E_{shared} + \\frac{1}{1 + \\alpha^2} \\epsilon^i,$\nNext, we compare the effect of using our \"coordinate noise\" implementation vs. combining low-frequency coordinate noise and high-frequency independent noise, which has been suggested in recent work conditioning on images [20, 27]. Although we do not condition directly on image frames, it's clear that the combination of low-frequency coordinate noise and high-frequency independent noise is not as effective as our implementation using Eqn. 9 (compare last two rows of Table S1).\nOverall, it is interesting to note that although our coordinate noise method provides substantial improvements in FID and overlapping PSNR, mixed noise obtains better performance when measuring Intra-LPIPS."}, {"title": "B. FID/CLIP Score Differences Between Experiments", "content": "As noted in the main paper, we observed improved performance as measured by FID and CLIP Score compared to MVDiffusion in the panoramic but not the depth-to-image experiment (cf. Tables 1 & 3). One explanation for this performance difference is that ScanNet text prompts provided by [24] using blip2 were often imprecise or inconsistent across views. Since MVDiffusion's method does not account for non-overlapping regions, their method is susceptible to issues like that shown in Figure S1 for imprecise prompts (here, the prompt \"a pair of shoes sitting on the floor next to a bed\" leads to hallucinations of a second bed). These errors can lead to better CLIP Score performance at the expense of multi-view consistency. Furthermore, inconsistent prompts across a scene could negatively impact FID for our method compared with MVDiffusion, which may exhibit errors only in single views without reconciling across a scene."}, {"title": "C. Additional Ablation Studies", "content": "In order to further evaluate our design choices for noise initialization, we compare results from experiments varying the weight parameter w from Eqn. 8. The results shown in Table S2 indicate that setting the weight w = 0.5 indeed provides the optimal result. However, it is interesting to note that this paramter appears to primarily affect FID and overlapping PSNR metrics. For these metrics, performance is noticeably \u2013 albeit not substantially \u2013 worse in either direction away from 0.5."}, {"title": "D. Additional Qualitative Examples", "content": "In this section, we provide further qualitative examples of our method in comparison to baselines in the depth-to-image and panoramic image generation experiments."}]}