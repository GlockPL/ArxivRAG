{"title": "DISTRACK: A NEW TOOL FOR SEMI-AUTOMATIC MISINFORMATION TRACKING IN ONLINE SOCIAL NETWORKS", "authors": ["Guillermo Villar-Rodr\u00edguez", "\u00c1lvaro Huertas-Garc\u00eda", "Alejandro Mart\u00edn", "Javier Huertas-Tato", "David Camacho"], "abstract": "Introduction: This article introduces DisTrack, a methodology and a tool developed for tracking and analyzing misinformation within Online Social Networks (OSNs). DisTrack is designed to combat the spread of misinformation through a combination of Natural Language Processing (NLP) Social Network Analysis (SNA) and graph visualization. The primary goal is to detect misinformation, track its propagation, identify its sources, and assess the influence of various actors within the network.\nMethods: DisTrack's architecture incorporates a variety of methodologies including keyword search, semantic similarity assessments, and graph generation techniques. These methods collectively facilitate the monitoring of misinformation, the categorization of content based on alignment with known false claims, and the visualization of dissemination cascades through detailed graphs. The tool is tailored to capture and analyze the dynamic nature of misinformation spread in digital environments.\nResults: The effectiveness of DisTrack is demonstrated through three case studies focused on different themes: discredit/hate speech, anti-vaccine misinformation, and false narratives about the Russia-Ukraine conflict. These studies show DisTrack's capabilities in distinguishing posts that propagate falsehoods from those that counteract them, and tracing the evolution of misinformation from its inception.\nConclusions: The research confirms that DisTrack is a valuable tool in the field of misinformation analysis. It effectively distinguishes between different types of misinformation and traces their development over time. By providing a comprehensive approach to understanding and combating mis-information in digital spaces, DisTrack proves to be an essential asset for researchers and practitioners working to mitigate the impact of false information in online social environments.", "sections": [{"title": "1 Introduction", "content": "At present, the impact of misinformation on our societies is beyond question. Governments, companies, researchers, among many others, are putting a great deal of effort into providing solutions and limiting the damage caused by this problem. The challenge, however, lies in the multifaceted nature of misinformation, making a comprehensive approach to combat it elusive. Addressing misinformation is contingent upon numerous factors, notably the underlying intent. Thus, when we use the word \"misinformation\", we are referring to information that is false by definition or unintentionally false information [1, 2], whereas \u201cdisinformation\" is used when there is a deliberate intention to misinform.[3], following the differences underlined by Karlova and Fisher in 2013 [4]. In addition to these terms, there is the concept of \u201cmalinformation\", which groups every content created as a weapon [5, 6].\nIn order to understand the phenomenon of misinformation in the current era, and beyond intentionality, it is essential to include the means of transmission in the equation. As such, social media, particularly Online Social Networks (OSNs), has risen as society's primary channel for accessing information. Social media has transformed information dissemination from direct peer-to-peer exchanges to expansive many-to-many propagations, altering societal content consumption habits and thereby easing the spread of false information [7]. Literature has demonstrated with specific cases how social media fosters falsehoods easily [8].\nThe combined effects of OSNs' dominance as information sources and the growing disenchantment with traditional news media amplify concerns about misinformation's influence on the public. According to the Reuters Institute Digital News Report, there's a notable decline in the practice of consulting a variety of news outlets and a general waning interest in news over the years. This trend includes the avoidance of topics like climate change and the Ukraine invasion by specific segments of the audience. This study underlines the contrast between worrying about misinformation and then depending more on social media to receive information [9].\nDespite the lack of expert consensus on the recent escalation of misinformation [7], academia has witnessed a significant surge in research articles tackling this issue, reflecting its growing impact [2, 10]. In the years before the pandemic, these academic works had already increased exponentially [10]. Subsequent studies associate the surge in academic interest regarding misinformation, particularly from 2017, with events like the 2016 US elections [11], suggesting these events catalyzed but did not solely trigger the focus [12]. After these contributions to research, the emergence of more relevant international events and their originated misinformation highlights the importance of tackling this problem.\nIn the fight against this phenomenon, fact-checkers represent the foremost defense. They are responsible for verifying thousands of pieces of information daily, cross-checking them and issuing statements on social media to counter the spread of false information. The International Fact-Checking Network (IFCN) built a \"Code of Principles\" to establish the criteria for the task of debunking misinformation. During the coronavirus crisis, the then Associate Director Cristina Tard\u00e1guila highlighted an unprecedented volume of misinformation, presenting a novel challenge for fact-checkers compared to prior years [13]. Recent research on COVID-19 misinformation has shown the emergence of misinformation waves, with a plethora of hoaxes spreading rapidly, complicating efforts to address them all at once [14]. Evidence like this demands new automated mechanisms not only at the level of verification but also in the following steps of mitigation on OSNs for a coordinated response against emerging falsehoods.\nMitigating the problem of misinformation therefore requires a complex approach where all factors of the problem are considered. In addition to the verification of the content itself, it is necessary to take into account how it is spread, the actors involved in its distribution and, in general, any element that participates in the cascade of misinformation propagation. In Fig. 1 the described cascade is represented, where several actors interact over time about a given falsehood. We want to find and appropriately characterize this misinformation cascade, allowing for a better understanding of online discourse. As a result of considering all these factors, we draw the following research question for our work: \u201cIs it possible to track conversations around specific hoaxes on Twitter (X)?\u201d which we sub-divide into four subquestions to answer it:"}, {"title": "2 Background", "content": "In this section, earlier contributions to the visualization of misinformation are explored. Discovering the veracity of a claim is crucial to visualizing the dissemination of misinformation because there are several lines of discourse surrounding any false-information claim, with fact-checkers on one side and misinforming actors on the other. It is expected that any information cascade about a hoax is surrounded by two different and opposing narratives.\nOur research is motivated by earlier works performing semi-automated fact-checking using transformer-based language models, able to detect whether pieces of content (or claims) are either factually fake or have undetermined veracity. Following the previous rationale we motivate our techniques by exploring language models applied to fact-checking, as well as Social Network Analysis (or SNA) to accurately portray the dissemination of misinformation across online social networks (OSNs)."}, {"title": "2.1 Language Models", "content": "The development of machine learning and deep learning models in the field of Natural Language Processing has made it possible to deal with complex tasks related to Natural Language Understanding (NLU). One of the most important steps was the emergence of language models with the introduction of the attention mechanism, leading to the development of Transformer models like BERT [16], ROBERTa [17], and XML [18]. Unlike earlier embeddings (like word2vec [19] or Glove [20]), Transformer models generate vectorial representations using contextual information from neighboring words in the surrounding text, where each word is semantically informed by the sentence.\nThese advances opened the era of Language Models (LMs), architectures trained for tasks such as predicting the next word, but designed for multiple NLP problems. Among these, there are many scenarios where LMs can be deployed to fuel fact-checking processes, providing significant improvements over traditional machine learning methods [21]. For example, these models facilitate the automation of fact-checking by employing binary classification to discern false facts within the input. The state-of-the-art literature shows promising results in this line of work [21, 22]. Furthermore, this approach can be refined beyond simple binary outcomes by using varied labels to provide more nuanced distinctions between types of information [23]."}, {"title": "2.2 Automated fact-checking", "content": "Misinformation is an ever-shifting issue. New pieces of misinformation may emerge as time passes, new narratives may become misinformation whereas older known hoaxes become real after an unexpected world event happens. Automated models trained without information retrieval techniques will inevitably become obsolete within the span of a few months. Allowing a model to retrieve information from trusted sources allows for proper decision making [23]. In contrast to this, an alternative approach arises where the dataset is conceived as a knowledge base [24]. In this source, the data consists of textual statements containing verified falsehoods. Using these falsehoods, a model can compare an unverified claim against any verified falsehood and if there is any match, we can determine that the unverified claim also contains a falsehood. In this structure fact-checkers have a double role: they are responsible for the curation of the database, as well as the interpretation of the model output, giving complete control of the semi-automated model to fact-checkers and responsibility of its application [14].\nThe process has two steps: information retrieval (IR) and Natural Language Inference (NLI). For IR, one of the most commonly used methods is the calculation of the semantic distance between semantic embeddings [25, 26]. This approach does not depend on a preliminary dataset of posts on the social network chosen to assess the veracity of an unseen post on that platform, allowing the classification of texts that belong to messaging environments in which full datasets are rarely obtained, such as WhatsApp [27]. The advances in this type of pipelines in the era of coronavirus encouraged research to focus on refining that knowledge base for that specific context [28]. Nowadays, tools in the fact-checking process based on the cosine similarity of texts have already been implemented in newsrooms and show their success over other methods [29].\nThe second step in this fact-checking process is determining the alignment between retrieved falsehoods and the original unverified content [24]. This alignment can be applied through the use of Natural Language Inference (NLI) as a subset"}, {"title": "2.3 Social Network Analysis", "content": "Studying and mitigating the problem of misinformation involves detecting the misinformation itself, but also tackling the pathways by which it spreads. Thus, understanding how a piece of misinformation is disseminated on a social network is a vital tool.\nThe flow of social media posts on Online Social Networks (OSNs) can be effectively represented as a graph. This graph-based structure organizes data into vertices linked by edges, providing a clear visualization of complex relationships [36]. Within this framework, vertices represent either users or their posts, while edges illustrate the myriad interactions or relationships between them. These connections encompass the explicit social interactions derived from the network's metadata but, additionally, the more subtle, latent properties that link posts together.\nSocial Network Analysis (SNA) is the area in charge of studying these graphs from social platforms. The directions in this discipline comprise both the extraction of the common features of networks and the identification of aspects from the users from the graph [37]. Algorithms can be trained with this information, using, for example, the dynamics of likes [38], to distinguish between types of posts. Misinformation arises as one of the emerging domains of application of SNA, together with politics and multimedia, being fields such as marketing, tourism, healthcare or cybersecurity more settled in this sort of studies [36].\nSurveys in the field of misinformation have highlighted that the characteristics inside the post are one of the indicators to detect falsehoods, but the properties of the OSN itself play an important role. Sharma et al. [39] enumerated key elements such as the source/promoters, user responses and the information content parts. On the other hand, Parikh et al. [40] specified non-text cues-based methods in the fight against false information, with user behaviour analysis as one of the subareas covered. In 2018, different studies demonstrated how false pieces of information were disseminated much further on Twitter than those that were true, by looking at properties such as the depth of the cascades generated by the post, the accounts contributing to spreading them and the duration of the propagation [41]. However, in the COVID-19 context, graph analyses revealed this expansion was only in terms of vastness: both false and legitimate information have the same influence, but actors spreading false information post more than those publishing the true one [42].\nHowever, regardless of the temporal context, repetitive patterns can be found. Before the coronavirus, SNA showed that individuals' decisions related to vaccines could be influenced inside circles debating about vaccination [43], indicating that the connections among users on these platforms matter to the extent of dangerous implications if the communities approached are anti-vaccine. These contagions from one group to another bring the issue of virality, taken from the propagation of viruses. The creation of a cascade responds to one of the models of infection, the viral model where infected vertices by others can exchange the virus too, in contrast to the broadcast models in which contagions derive from a main vertex [44].\nThe role of the main spreaders in these cascades of misinformation among these circles has also been the focus of social-media-driven analyses. The change of information from one community to another through 'super-spreaders' and their characteristics were also disseminated with graphs [45]. Verified Twitter accounts (in the era before Elon Musk ownership and X) were shown to be 50 times more powerful in terms of propagating content about vaccines in comparison to non-verified profiles [46].\nTo conclude, the use of NLP models without considering the dynamics of the social network allows us to visualize only part of the problem, leaving out key details [25, 26, 35]. Every instance of misinformation is surrounded by a community of users who interact with, share, comment on, support, or dispute it. Relying solely on content analysis limits our capabilities, overlooking the crucial task of unraveling the impact of false claims on Online Social Networks (OSNs), which extends beyond mere verification to include users' responses. The combination of NLP and SNA leads"}, {"title": "3 Tracking misinformation in OSNs", "content": "NLP and SNA can represent an alternative map of misinformation in OSNs through all the posts about a claim spreading a falsehood. In it, false information appears from viralization, but also from messages of different shapes from a wide to a short range of interactions and from a variety of users, not necessarily with the same impact in terms of their popularity in the social network. As an example, this approach allows to model the contagion from one vertex to the rest [44] in two senses: on the one hand, NLP-driven research has demonstrated that there is not a unique message repeated in the propagation of misinformation, but many of them expressed differently; on the other hand, SNA-oriented studies show unconnected users outside the cascades also distribute falsehoods on social media.\nNevertheless, these studies contradicting the only focus on broadcast models [47, 48] in their combination of NLP and SNA are limited to the analysis of the data of the properties from the social network chosen in their final goal. Graph generation is ignored and the possibility of representing these ecosystems of falsehoods is missed. This results in an obstacle between the theory that confirms how misinformation is not just a cascade and the practice of representing what it is instead. This mentioned practice, in contrast to the previous approaches, would go further than the demonstration of a different model of diffusion of misinformation to reveal a more realistic flux of the posts causing it, examining its origin, evolution and end, if applicable."}, {"title": "3.1 The DisTrack architecture", "content": "The main goal of DisTrack is to create a complete representation of the propagation cascade of a piece of misinformation. To do so, it integrates different language models and SNA techniques that allow building a graphical representation of this cascade, providing information about the content and the actors in the social network that have played an essential role in the dissemination. DisTrack consists of three main sequential steps:\n1. Information retrieval from OSNs: this module comprises the extraction of relevant keywords, the generation of queries through their possible combinations and the use of Twitter API to download all the tweets.\n2. Semantic and Natural Language Inference: this module refers to the conversion of tweets into Transformer-based embeddings that capture their meaning and context and the extraction of metrics based on their inference (if a tweet supports a false claim, contradicts it or is unrelated. For this second step, we make use of FacTeR-Check [14], which implements a semantic similarity filtering process followed by Natural Language Inference.\n3. Graph generation: this module ends with the hydration of the tweets downloaded to extract the insights to be used in the graph and the creation of it. Its vertices will be the tweets extracted and whose edges will correspond to the interactions among them. After this, the particularity of DisTrack is the use of the NLP-related and Twitter-related metrics to show the rest of the properties (position, size or colour).\nDisTrack leverages the concept of tracking by modeling a graph based on a set of tweets downloaded from the OSN and labeled according to the alignment with a specific claim. This modeling process includes information extracted from the OSN such as following between users, retweets, or replies. As a result, the mechanisms of DisTrack can be distributed in three modules (see Fig. 2).\nThe final output of these three modules is a visualization that acts as the operation center for the supervision of each piece of misinformation from the beginning to the end, assessing about the flux of a certain claim, the most influential spreaders and tweets with their connections or the periods in which this false information has had more impact.\nThis architecture allows us to characterize the discourse surrounding any piece of information. The visualization aims to highlight specific phenomena in social media discourse, as well as provide answers to questions for policymakers and content moderators. We provide some examples of these properties:\n\u2022 Proliferation (and decay) over time of falsehoods on social media. How does a piece of content propagate across time in an OSN?\n\u2022 Impact of fact-checkers on the discourse surrounding a falsehood. Do they contribute to the proliferation or the decay of the surrounding discourse?\n\u2022 Relationship between the influence of a social network actor and their impact on the discourse. How much do influential accounts dominate online discourse? Do smaller accounts have any impact on the evolution of the cascade?"}, {"title": "3.2 Retrieving Twitter Content", "content": "The process of extracting information from Twitter requires the execution of a series of searches for certain keywords. Since the Twitter API restricts the search to the exact keywords that are given as input, in order to retrieve a large representative sample of the relevant tweets and interactions related to one specific claim, it is necessary to build a set of multiple queries. By using different words and expressions in each of these queries it is possible to cover a large part of the tweets referring to the claim."}, {"title": "3.2.1 Keyword search", "content": "The query is the input to search and download tweets through Twitter API. These downloaded posts will contain the keywords inside that query. Through logical operators, queries can request the API tweets with every keyword inside it or for every tweet that at least has one of the keywords. However, this process is manual and the downloaded tweets are the result of the subjective decision of typing a query with a set of keywords instead of another. How these keywords\n\u00b9now X, however we will call it Twitter for the sake of simplicity and understandability.are distributed with the logical operators to optimize the search is also an arbitrary decision. Furthermore, thinking of all these steps of converting a false claim into a query to download as many tweets as possible requires time, slowing down the computer-based fight against misinformation.\nTo solve these drawbacks of the manual creation of a query, an information retrieval module to generate queries and automate this process is needed. This research follows FacTer-ChecKey [14], a designed automated search to extract the most relevant keywords from a claim and concatenate them through the logical operator \"AND\" to generate the final query. For example, the sentence \u201cMassive protest in France against the mandatory implementation of the COVID passport in public spaces\u201d results in the concatenation of keywords \u201c(protest AND france AND passport AND covid AND public)", "add-ons": "n1. The introduction of the logical operator OR in the query, to allow multiple combinations of the keywords for the automated search\n2. The inclusion of a parameter that indicates the number of keywords that will be discarded from each combination of words, being the result of the query a concatenation of all the possible mixtures excluding two different keywords in each of them. In the example previously seen, the query would be optimized in this way: \"((protest AND france AND passport AND covid) OR (protest AND france AND passport AND public) OR (protest AND france AND covid AND public) OR (protest AND passport AND covid AND public) OR (france AND passport AND covid AND public))\u201d.\n3. The conversion of numerical numbers into all the possible forms, also textual, to avoid ignoring pieces of misinformation that include figures cited in a different way (e.g., \u201c10000 OR 10,000 OR 10.000 OR '10 thousand' OR 'ten thousand"}, {"title": "3.2.2 Technical details", "content": "The query generated through the claim of each piece of misinformation constitutes the input of Twitter API, whose access is offered through a developer account on Twitter.\nThe reduced limitations granted by the academic API access soften two restrictions: the maximum number of tweets extracted and the time of the start of the final download. This last aspect can be modified to obtain information from a certain timestamp. This becomes crucial in the field of misinformation because the search must be constrained after the birth of the topic that is referenced in each hoax (for example, claims about COVID will not be found prior to the emergence of coronavirus).\nOverall, the created query, the credential to validate the permissions (the token found on the developer account) and the selected time-lapse and maximum number of posts as parameters are included for the automated request for the needed tweets about misinformation. The data of each downloaded tweet consists of a JSON with Twitter-based information structured as metadata in different fields.\nHowever, since this cannot be exploited enough, NLP-based features in the following step will contribute to filtering non-related hoaxes and developing the final tool. These technical details describe a fixed frame of the use of the API at"}, {"title": "3.3 Automated Verification", "content": "Once we have retrieved a sample of information from the OSN related to the input claim, each tweet is labeled according to the alignment with the original claim. We define alignment as the result of performing Natural Language Inference over a pair of content (the retrieved tweet from the last step) and falsehood (the piece of misinformation that is being tracked). After evaluating each retrieved Tweet we assign their labels."}, {"title": "3.3.1 Natural Language Inference", "content": "Natural language inference (NLI) is crucial to distinguish if a tweet is aligned or contradicts a false claim. Again, Transformer-based architectures are applied to measure this alignment regardless of how different each tweet is formulated in comparison to the false claim selected. The NLI task consists of discovering if a hypothesis $h$ can be inferred from the premise $p$ in a pair of texts $(p, h)$. In the misinformation and fact-checking domains, $p$ will be each tweet from the conversation downloaded from Twitter and $h$ will be the piece of misinformation debunked by fact-checkers. Thus: $h$ is $h_f$ when stating that falsehood, and $h$ is $h_u$ when that factuality is undetermined.\nThe classification of posts according to their content through NLI is:\n\u2022 Entailment (when the falsehood is enunciated): a post that entails a piece of misinformation is a post that supports it and spreads it.\n\u2022 Contradiction (when the negation of the falsehood is enunciated): a post that contradicts a piece of misinformation is a post that denies it and may act as a protective shield in the circles where it arrives.\n\u2022 Neutral (when the falsehood or its contradiction is not enunciated): a post that is neutral bears no relevance to the discourse whatsoever a secondary effect of widening the spread of the keyword search."}, {"title": "3.3.2 Technical details", "content": "The semantic search uses the methodology proposed in FacTer-Check step by step without ad-dons or modifications. For the NLI task, a fine-tuned XLM-ROBERTa-large [50] was used for this module. For the NLI task, the Machine Translated MultiNLI (MNLI-MT) [33] and XNLI [34] datasets were used. Additional datasets such as ANLI [51], SNLI [32] and FEVER [52] for English have been also included, using two training processes (inspired by FacTeR-Check): one only with MNLI (for the cross-lingual texts) and one with all the mentioned datasets. The hyperparameters chosen are: 1024 as batch size, 2e-5 as the learning rate, with Adam [53] as the optimizer. Same for warmup and linear decay. The validation data in XNLI determines the optimal selected network after a manual hyper-parameter finetuning\u00b2."}, {"title": "3.4 Graph visualization", "content": "Our main contribution lies in the graph visualization module where we conceive how to translate an interconnected graph of tweets into a useful visualization of online discourse around a topic. Our DisTrack architecture makes use of all the properties stored in each tweet to make a readable composition, clearly showing how any piece of misinformation has evolved."}, {"title": "3.4.1 Cascade graph building", "content": "The information stored for each tweet includes the preceding author and the preceding tweet from which it derives. This information is enough to generate a directed graph $G = (V, E)$, with $V$ the vertex set containing tweets published by an author. In this graph, the $E$ edges represent a connection between vertices due to being either a reply, quote or retweet from another tweet, which will be the parent vertex."}, {"title": "4 Case studies", "content": "Three use cases illustrate the application of DisTrack. They contain the beginning and evolution of a piece of misinformation on Twitter, all of them with a different topic to show the versatility of this tool. Firstly, an exploratory data analysis of the downloaded tweets is made to disentangle the types of tweets and of their authors according to their NLI- and Twitter-based metrics. After this, as the proof of the variety of tweets contributing to the expansion of content related to misinformation, the representation of the final graphs is made thanks to the final module of graph generation from DisTrack.\nThese three cases represent three different topics:\n\u2022 Case 1: \"The 80 percent of Muslims living in Europe live from social welfare and they refuse to work\u201d. The first case is linked to the disbelief in institutions and hate against Muslims. It constitutes 32 original tweets and a positive balance of tweets involving entailment in contrast to contradiction (i.e., denying the hoax). The weight of entailment increases much more when every post is shared. Thus, this case contains a total of 84 representative posts, with all the retweets included.\n\u2022 Case 2: \u201cRNA vaccines against coronavirus includes graphene oxid\u201d. It focuses on COVID-19-related antivaccine statements. It also includes 32 original tweets and a balanced number between entailment and contradiction. This second case has a total of 128 representative posts, including retweets.\n\u2022 Case 3: \"Zelensky sold 17 million hectares of land to Monsanto, Dupont and Cargill\". It is an example of misinformation around the Russia-Ukraine war. It departs from 26 original tweets and most of the total tweets represent entailment with the hoax (80%). This third case involves a total of 916 tweets.\""}, {"title": "4.1 Case 1: Discredit/hate", "content": "For this first case (see Fig. 6), we focus on a hoax that circulated on Twitter asserting that \u201c80 percent of Muslims living in Europe live from social welfare and they refuse to work\". For this hoax, we retrieved a large pool of information\""}, {"title": "4.2 Case 2 - Antivaccine", "content": "The propagation of how falsely RNA vaccines against coronavirus include graphene oxide has also had an important impact on Twitter and other social media. This hoax has succeeded through paraphrasing in different periods. Whereas the previous case study showed a fixed structure and shape, this one discovers a range of posts expressing the same falsehood with different words and tones, from more declarative sentences to more aggressive ones."}, {"title": "4.3 Case 3: Russia-Ukraine", "content": "This third case, shown in Fig. 8, considers the hoax \"Zelensky sold 17 million hectares of land to Monsanto, Dupont and Cargill\". Unlike the earlier cases, the visualizations here highlight variations in impact, illustrating the hoax's dissemination through a multitude of messages. Echoing the patterns seen in the previous example, there is not a single form of post; instead, a diversity of presentations emerge, ranging from the use of hashtags and user mentions to varying styles and tactics to engage the audience. The spread of this misinformation is primarily driven by reposts, yet these varied expressions of the same false claim also play a significant role, particularly at the initial stages of its spread. This multiplicity of formats and channels underscores the complex nature of misinformation propagation and the challenges in tracing and countering it.\nThis example showcases a viral post from September 19th, 2022, that stood out with 663 retweets and 1,000 likes. Remarkably, its retweets occurred not just on the day of the original tweet but continued sporadically until December 30th of the same year, although diminishing impact. This trend is visualized along the x-axis and highlights both the immediate impact of the hoax and its prolonged presence in the digital discourse.\nThanks to this visualization, abnormal activity is shown on the same user who writes the viral post. This suggests that this person retweets it several times, according to the download of Twitter API data. This continuous activity preserves the propagation of this false information and prevents it from dying on Twitter (X). In addition, there was another post with even more retweets whose initial user has been deleted or suspended (1.597 retweets), as there are no edges linking them to the original post.\nIn this case, the virality of the post on September 19th already shows the vast propagation of this misinformation, how it arrives to other users throughout time, how it can be the continuation of other viral posts in the past, and even the"}, {"title": "5 Results and discussion", "content": "In every analyzed example, DisTrack answers positively to the question \"Can we extract the conversation about a hoax on Twitter?\". The conversations about the three chosen falsehoods have been visualized after downloading and processing and refining the data and metadata of the posts related to them. This demonstrates the success in generating automated queries to extract as many tweets as possible about false claims to later filter them depending on their type of alignment (entailment, contradiction or neutral).\nThis leads to the affirmative answer to the second subquestion \u201cCan we separate tweets related to the hoax in the extracted conversation from tweets not related to it?", "Can we distinguish between hoaxes that propagate a hoax from those that contradict it?\"). The application of NLI leveraged with Transformers has been satisfactory at two levels": "not only by separating tweets that are not related to each false claim, but also by separating the entailment posts (those that state misinformation) from contradiction posts (those that deny them).\nIn particular, the use of Transformers in the application of NLI to detect misinformation in this case has allowed DisTrack to identify falsehoods on X, regardless of the way they were exposed in the platform. In case 1, the posts were similar to the false claim taken as the reference, with subtle changes, but in cases 2 and 3, the posts paraphrased the content of the false claim together with hashtags, links or other elements, and their entailment was also guessed by the NLI model. In these downloaded conversations about misinformation, The existence of accounts reproducing exactly the same original content and of those that paraphrase it instead encourages research to unveil the dynamics of falsehoods and the existence of bots in contexts of crises [54].\nThanks to the whole process of searching tweets, aligning them with the validated claim and visual representation, what we present is a complete architecture that allows the automated and detailed analysis of the spread of disinformation on a social network. From the first references to the hoax, to the nodes that most influence its dissemination (thanks to their number of followers, for example) or the patterns observed in the cascade, DisTrack allows us to understand first-hand the dynamics of disinformation and how it spreads and generates a certain impact.\nThe fourth research question \u201cCan we identify the users involved in the conversation of a hoax from beginning to end?", "misinformation": "viral tweets matter, but all the participants in the conversation are relevant. In the three case studies analyzed, falsehoods appear before and after the spread of the most retweeted posts, in the shape of less viral tweets. This research reveals the characteristics of every actor inside the ecosystem of false information: the case studies unveil users with many"}, {"title": "6 Conclusions", "content": "All in all, our work releases a line of action through the shape of DisTrack, as the beginning of a tool able to merge Transformer-leveraged tweet extraction, NLI-driven tagging of misinformation from the posts retrieved and graph generation of Twitter-based and user-based properties in an output that shows chronologically the evolution of a conversation motivated by misinformation (spreaders, fact-checkers and other users) across the different publications and actors involved.\nWith this proposed line of action, future experiments can study how DisTrack modules can be modified. With NLP as one of the core parts of this research, there is room for improvement given the advance of new Language Models (LM): on the one hand, with their application for new models that capture better the topics and, thus, the keywords for the query that enable the download of posts; on the other hand, with their use for NLI to increase the level of accuracy in the classification of posts as entailment, contradiction of neutral, the three tags used to colour the vertices in the final graphs to build the representative picture of misinformation.\nFuture work includes the application of DisTrack beyond misinformation. For instance, previous research shows how sentiment analysis has become relevant in the studies about aggressive discourse in the context of government elections [55] and has stated Twitter as a \u201csentiment thermometer\u201d through VADER [56]. Although the first steps of these experiments evoque the implementation of DisTrack, involving the download of posts from X and the extraction of"}, {"title": "Declarations", "content": "The authors declare no competing interests."}]}