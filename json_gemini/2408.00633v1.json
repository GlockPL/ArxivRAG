{"title": "DISTRACK: A NEW TOOL FOR SEMI-AUTOMATIC MISINFORMATION TRACKING IN ONLINE SOCIAL NETWORKS", "authors": ["Guillermo Villar-Rodr\u00edguez", "\u00c1lvaro Huertas-Garc\u00eda", "Alejandro Mart\u00edn", "Javier Huertas-Tato", "David Camacho"], "abstract": "This article introduces DisTrack, a methodology and a tool developed for tracking and analyzing misinformation within Online Social Networks (OSNs). DisTrack is designed to combat the spread of misinformation through a combination of Natural Language Processing (NLP) Social Network Analysis (SNA) and graph visualization. The primary goal is to detect misinformation, track its propagation, identify its sources, and assess the influence of various actors within the network.\nDisTrack's architecture incorporates a variety of methodologies including keyword search, semantic similarity assessments, and graph generation techniques. These methods collectively facilitate the monitoring of misinformation, the categorization of content based on alignment with known false claims, and the visualization of dissemination cascades through detailed graphs. The tool is tailored to capture and analyze the dynamic nature of misinformation spread in digital environments.\nThe effectiveness of DisTrack is demonstrated through three case studies focused on different themes: discredit/hate speech, anti-vaccine misinformation, and false narratives about the Russia-Ukraine conflict. These studies show DisTrack's capabilities in distinguishing posts that propagate falsehoods from those that counteract them, and tracing the evolution of misinformation from its inception.\nThe research confirms that DisTrack is a valuable tool in the field of misinformation analysis. It effectively distinguishes between different types of misinformation and traces their development over time. By providing a comprehensive approach to understanding and combating mis-information in digital spaces, DisTrack proves to be an essential asset for researchers and practitioners working to mitigate the impact of false information in online social environments.", "sections": [{"title": "1 Introduction", "content": "At present, the impact of misinformation on our societies is beyond question. Governments, companies, researchers, among many others, are putting a great deal of effort into providing solutions and limiting the damage caused by this problem. The challenge, however, lies in the multifaceted nature of misinformation, making a comprehensive approach to combat it elusive. Addressing misinformation is contingent upon numerous factors, notably the underlying intent. Thus, when we use the word \"misinformation\", we are referring to information that is false by definition or unintentionally false information [1, 2], whereas \u201cdisinformation\" is used when there is a deliberate intention to"}, {"title": "2 Background", "content": "In this section, earlier contributions to the visualization of misinformation are explored. Discovering the veracity of a claim is crucial to visualizing the dissemination of misinformation because there are several lines of discourse surrounding any false-information claim, with fact-checkers on one side and misinforming actors on the other. It is expected that any information cascade about a hoax is surrounded by two different and opposing narratives.\nOur research is motivated by earlier works performing semi-automated fact-checking using transformer-based language models, able to detect whether pieces of content (or claims) are either factually fake or have undetermined veracity. Following the previous rationale we motivate our techniques by exploring language models applied to fact-checking, as well as Social Network Analysis (or SNA) to accurately portray the dissemination of misinformation across online social networks (OSNs)."}, {"title": "2.1 Language Models", "content": "The development of machine learning and deep learning models in the field of Natural Language Processing has made it possible to deal with complex tasks related to Natural Language Understanding (NLU). One of the most important steps was the emergence of language models with the introduction of the attention mechanism, leading to the development of Transformer models like BERT [16], ROBERTa [17], and XML [18]. Unlike earlier embeddings (like word2vec [19] or Glove [20]), Transformer models generate vectorial representations using contextual information from neighboring words in the surrounding text, where each word is semantically informed by the sentence.\nThese advances opened the era of Language Models (LMs), architectures trained for tasks such as predicting the next word, but designed for multiple NLP problems. Among these, there are many scenarios where LMs can be deployed to fuel fact-checking processes, providing significant improvements over traditional machine learning methods [21]. For example, these models facilitate the automation of fact-checking by employing binary classification to discern false facts within the input. The state-of-the-art literature shows promising results in this line of work [21, 22]. Furthermore, this approach can be refined beyond simple binary outcomes by using varied labels to provide more nuanced distinctions between types of information [23]."}, {"title": "2.2 Automated fact-checking", "content": "Misinformation is an ever-shifting issue. New pieces of misinformation may emerge as time passes, new narratives may become misinformation whereas older known hoaxes become real after an unexpected world event happens. Automated models trained without information retrieval techniques will inevitably become obsolete within the span of a few months. Allowing a model to retrieve information from trusted sources allows for proper decision making [23]. In contrast to this, an alternative approach arises where the dataset is conceived as a knowledge base [24]. In this source, the data consists of textual statements containing verified falsehoods. Using these falsehoods, a model can compare an unverified claim against any verified falsehood and if there is any match, we can determine that the unverified claim also contains a falsehood. In this structure fact-checkers have a double role: they are responsible for the curation of the database, as well as the interpretation of the model output, giving complete control of the semi-automated model to fact-checkers and responsibility of its application [14].\nThe process has two steps: information retrieval (IR) and Natural Language Inference (NLI). For IR, one of the most commonly used methods is the calculation of the semantic distance between semantic embeddings [25, 26]. This approach does not depend on a preliminary dataset of posts on the social network chosen to assess the veracity of an unseen post on that platform, allowing the classification of texts that belong to messaging environments in which full datasets are rarely obtained, such as WhatsApp [27]. The advances in this type of pipelines in the era of coronavirus encouraged research to focus on refining that knowledge base for that specific context [28]. Nowadays, tools in the fact-checking process based on the cosine similarity of texts have already been implemented in newsrooms and show their success over other methods [29].\nThe second step in this fact-checking process is determining the alignment between retrieved falsehoods and the original unverified content [24]. This alignment can be applied through the use of Natural Language Inference (NLI) as a subset"}, {"title": "2.3 Social Network Analysis", "content": "Studying and mitigating the problem of misinformation involves detecting the misinformation itself, but also tackling the pathways by which it spreads. Thus, understanding how a piece of misinformation is disseminated on a social network is a vital tool.\nThe flow of social media posts on Online Social Networks (OSNs) can be effectively represented as a graph. This graph-based structure organizes data into vertices linked by edges, providing a clear visualization of complex relationships [36]. Within this framework, vertices represent either users or their posts, while edges illustrate the myriad interactions or relationships between them. These connections encompass the explicit social interactions derived from the network's metadata but, additionally, the more subtle, latent properties that link posts together.\nSocial Network Analysis (SNA) is the area in charge of studying these graphs from social platforms. The directions in this discipline comprise both the extraction of the common features of networks and the identification of aspects from the users from the graph [37]. Algorithms can be trained with this information, using, for example, the dynamics of likes [38], to distinguish between types of posts. Misinformation arises as one of the emerging domains of application of SNA, together with politics and multimedia, being fields such as marketing, tourism, healthcare or cybersecurity more settled in this sort of studies [36].\nSurveys in the field of misinformation have highlighted that the characteristics inside the post are one of the indicators to detect falsehoods, but the properties of the OSN itself play an important role. Sharma et al. [39] enumerated key elements such as the source/promoters, user responses and the information content parts. On the other hand, Parikh et al. [40] specified non-text cues-based methods in the fight against false information, with user behaviour analysis as one of the subareas covered. In 2018, different studies demonstrated how false pieces of information were disseminated much further on Twitter than those that were true, by looking at properties such as the depth of the cascades generated by the post, the accounts contributing to spreading them and the duration of the propagation [41]. However, in the COVID-19 context, graph analyses revealed this expansion was only in terms of vastness: both false and legitimate information have the same influence, but actors spreading false information post more than those publishing the true one [42].\nHowever, regardless of the temporal context, repetitive patterns can be found. Before the coronavirus, SNA showed that individuals' decisions related to vaccines could be influenced inside circles debating about vaccination [43], indicating that the connections among users on these platforms matter to the extent of dangerous implications if the communities approached are anti-vaccine. These contagions from one group to another bring the issue of virality, taken from the propagation of viruses. The creation of a cascade responds to one of the models of infection, the viral model where infected vertices by others can exchange the virus too, in contrast to the broadcast models in which contagions derive from a main vertex [44].\nThe role of the main spreaders in these cascades of misinformation among these circles has also been the focus of social-media-driven analyses. The change of information from one community to another through 'super-spreaders' and their characteristics were also disseminated with graphs [45]. Verified Twitter accounts (in the era before Elon Musk ownership and X) were shown to be 50 times more powerful in terms of propagating content about vaccines in comparison to non-verified profiles [46].\nTo conclude, the use of NLP models without considering the dynamics of the social network allows us to visualize only part of the problem, leaving out key details [25, 26, 35]. Every instance of misinformation is surrounded by a community of users who interact with, share, comment on, support, or dispute it. Relying solely on content analysis limits our capabilities, overlooking the crucial task of unraveling the impact of false claims on Online Social Networks (OSNs), which extends beyond mere verification to include users' responses. The combination of NLP and SNA leads"}, {"title": "3 Tracking misinformation in OSNs", "content": "NLP and SNA can represent an alternative map of misinformation in OSNs through all the posts about a claim spreading a falsehood. In it, false information appears from viralization, but also from messages of different shapes from a wide to a short range of interactions and from a variety of users, not necessarily with the same impact in terms of their popularity in the social network. As an example, this approach allows to model the contagion from one vertex to the rest [44] in two senses: on the one hand, NLP-driven research has demonstrated that there is not a unique message repeated in the propagation of misinformation, but many of them expressed differently; on the other hand, SNA-oriented studies show unconnected users outside the cascades also distribute falsehoods on social media.\nNevertheless, these studies contradicting the only focus on broadcast models [47, 48] in their combination of NLP and SNA are limited to the analysis of the data of the properties from the social network chosen in their final goal. Graph generation is ignored and the possibility of representing these ecosystems of falsehoods is missed. This results in an obstacle between the theory that confirms how misinformation is not just a cascade and the practice of representing what it is instead. This mentioned practice, in contrast to the previous approaches, would go further than the demonstration of a different model of diffusion of misinformation to reveal a more realistic flux of the posts causing it, examining its origin, evolution and end, if applicable."}, {"title": "3.1 The DisTrack architecture", "content": "The main goal of DisTrack is to create a complete representation of the propagation cascade of a piece of misinformation. To do so, it integrates different language models and SNA techniques that allow building a graphical representation of this cascade, providing information about the content and the actors in the social network that have played an essential role in the dissemination. DisTrack consists of three main sequential steps:\n1. Information retrieval from OSNs: this module comprises the extraction of relevant keywords, the generation of queries through their possible combinations and the use of Twitter API to download all the tweets.\n2. Semantic and Natural Language Inference: this module refers to the conversion of tweets into Transformer-based embeddings that capture their meaning and context and the extraction of metrics based on their inference (if a tweet supports a false claim, contradicts it or is unrelated. For this second step, we make use of FacTeR-Check [14], which implements a semantic similarity filtering process followed by Natural Language Inference.\n3. Graph generation: this module ends with the hydration of the tweets downloaded to extract the insights to be used in the graph and the creation of it. Its vertices will be the tweets extracted and whose edges will correspond to the interactions among them. After this, the particularity of DisTrack is the use of the NLP-related and Twitter-related metrics to show the rest of the properties (position, size or colour).\nDisTrack leverages the concept of tracking by modeling a graph based on a set of tweets downloaded from the OSN and labeled according to the alignment with a specific claim. This modeling process includes information extracted from the OSN such as following between users, retweets, or replies. As a result, the mechanisms of DisTrack can be distributed in three modules (see Fig. 2).\nThe final output of these three modules is a visualization that acts as the operation center for the supervision of each piece of misinformation from the beginning to the end, assessing about the flux of a certain claim, the most influential spreaders and tweets with their connections or the periods in which this false information has had more impact.\nThis architecture allows us to characterize the discourse surrounding any piece of information. The visualization aims to highlight specific phenomena in social media discourse, as well as provide answers to questions for policymakers and content moderators. We provide some examples of these properties:\n\u2022 Proliferation (and decay) over time of falsehoods on social media. How does a piece of content propagate across time in an OSN?\n\u2022 Impact of fact-checkers on the discourse surrounding a falsehood. Do they contribute to the proliferation or the decay of the surrounding discourse?\n\u2022 Relationship between the influence of a social network actor and their impact on the discourse. How much do influential accounts dominate online discourse? Do smaller accounts have any impact on the evolution of the cascade?\n\u2022 Detection of possible astroturfing campaigns. Has the falsehood spawned from several disconnected accounts? Is a coordinated attack on the social network being performed?"}, {"title": "3.2 Retrieving Twitter Content", "content": "The process of extracting information from Twitter requires the execution of a series of searches for certain keywords. Since the Twitter API restricts the search to the exact keywords that are given as input, in order to retrieve a large representative sample of the relevant tweets and interactions related to one specific claim, it is necessary to build a set of multiple queries. By using different words and expressions in each of these queries it is possible to cover a large part of the tweets referring to the claim."}, {"title": "3.2.1 Keyword search", "content": "The query is the input to search and download tweets through Twitter API. These downloaded posts will contain the keywords inside that query. Through logical operators, queries can request the API tweets with every keyword inside it or for every tweet that at least has one of the keywords. However, this process is manual and the downloaded tweets are the result of the subjective decision of typing a query with a set of keywords instead of another. How these keywords"}, {"title": "3.2.2 Technical details", "content": "The query generated through the claim of each piece of misinformation constitutes the input of Twitter API, whose access is offered through a developer account on Twitter.\nThe reduced limitations granted by the academic API access soften two restrictions: the maximum number of tweets extracted and the time of the start of the final download. This last aspect can be modified to obtain information from a certain timestamp. This becomes crucial in the field of misinformation because the search must be constrained after the birth of the topic that is referenced in each hoax (for example, claims about COVID will not be found prior to the emergence of coronavirus).\nOverall, the created query, the credential to validate the permissions (the token found on the developer account) and the selected time-lapse and maximum number of posts as parameters are included for the automated request for the needed tweets about misinformation. The data of each downloaded tweet consists of a JSON with Twitter-based information structured as metadata in different fields.\nHowever, since this cannot be exploited enough, NLP-based features in the following step will contribute to filtering non-related hoaxes and developing the final tool. These technical details describe a fixed frame of the use of the API at"}, {"title": "3.3 Automated Verification", "content": "Once we have retrieved a sample of information from the OSN related to the input claim, each tweet is labeled according to the alignment with the original claim. We define alignment as the result of performing Natural Language Inference over a pair of content (the retrieved tweet from the last step) and falsehood (the piece of misinformation that is being tracked). After evaluating each retrieved Tweet we assign their labels."}, {"title": "3.3.1 Natural Language Inference", "content": "Natural language inference (NLI) is crucial to distinguish if a tweet is aligned or contradicts a false claim. Again, Transformer-based architectures are applied to measure this alignment regardless of how different each tweet is formulated in comparison to the false claim selected. The NLI task consists of discovering if a hypothesis $h$ can be inferred from the premise $p$ in a pair of texts $(p, h)$. In the misinformation and fact-checking domains, $p$ will be each tweet from the conversation downloaded from Twitter and $h$ will be the piece of misinformation debunked by fact-checkers. Thus: $h$ is $h_f$ when stating that falsehood, and $h$ is $h_u$ when that factuality is undetermined.\nThe classification of posts according to their content through NLI is:\n\u2022 Entailment (when the falsehood is enunciated): a post that entails a piece of misinformation is a post that supports it and spreads it.\n\u2022 Contradiction (when the negation of the falsehood is enunciated): a post that contradicts a piece of misinformation is a post that denies it and may act as a protective shield in the circles where it arrives.\n\u2022 Neutral (when the falsehood or its contradiction is not enunciated): a post that is neutral bears no relevance to the discourse whatsoever a secondary effect of widening the spread of the keyword search."}, {"title": "3.3.2 Technical details", "content": "The semantic search uses the methodology proposed in FacTer-Check step by step without ad-dons or modifications. For the NLI task, a fine-tuned XLM-ROBERTa-large [50] was used for this module. For the NLI task, the Machine Translated MultiNLI (MNLI-MT) [33] and XNLI [34] datasets were used. Additional datasets such as ANLI [51], SNLI [32] and FEVER [52] for English have been also included, using two training processes (inspired by FacTeR-Check): one only with MNLI (for the cross-lingual texts) and one with all the mentioned datasets. The hyperparameters chosen are: 1024 as batch size, 2e-5 as the learning rate, with Adam [53] as the optimizer. Same for warmup and linear decay. The validation data in XNLI determines the optimal selected network after a manual hyper-parameter finetuning."}, {"title": "3.4 Graph visualization", "content": "Our main contribution lies in the graph visualization module where we conceive how to translate an interconnected graph of tweets into a useful visualization of online discourse around a topic. Our DisTrack architecture makes use of all the properties stored in each tweet to make a readable composition, clearly showing how any piece of misinformation has evolved."}, {"title": "3.4.1 Cascade graph building", "content": "The information stored for each tweet includes the preceding author and the preceding tweet from which it derives. This information is enough to generate a directed graph $G = (V, E)$, with $V$ the vertex set containing tweets published by an author. In this graph, the $E$ edges represent a connection between vertices due to being either a reply, quote or retweet from another tweet, which will be the parent vertex.\nEach vertex has additional information contained by the contextual metadata of the author, likes among other details. Conveying this metadata to make it understandable is a non-trivial challenge that we address in the following subsections. In particular:\n\u2022 Time is contained within the metadata, but the information is so unevenly spaced over time that the interpretation of the cascade is compromised.\n\u2022 The cascade of misinformation is perpetrated mainly by actors that influence online discourse. Representing their influence accurately in the visualization aside from the vertices is crucial to understanding the actual impact of actors.\n\u2022 The information cascade of misinformation is twofold, containing usually two opposing narratives, the misinformation itself and the fact-checkers combating it. Greater insight can be achieved by using the results of NLI and integrating them into the visualization to evaluate the veracity of the evaluated claims."}, {"title": "3.4.2 Non-linear time representation", "content": "The X-axis is defined by time. Each vertex will be placed according to the moment it has been published. However, time itself is not represented linearly in this axis. Falsehoods act in waves, the flow of information may be quick and sudden during a short time-lapse generating concentrated content, preventing the user from reading the evolution of that piece of false information properly when it explodes in terms of impact. Furthermore, if there is much distance in time from one vertex to the next one, again the space in the graph between these two points will create empty areas in the plot whereas the areas filled would be crowded with vertices (during the propagation of a hoax, there are moments of inactivity or a lower number of publications about it). Our approach just sorts posts chronologically, regardless of the time passed between one vertex and the next one, and sets different time stamps on the x-axis that improve the understanding of the propagation cascade."}, {"title": "3.4.3 Author influence representation", "content": "Regarding the y-axis, it measures the degree of influence of the author of a vertex/tweet. The number of followers allows us to distinguish between the influence of the different actors responsible for the spread of falsehood or their contradiction.\nHowever, the achieved impressions by the tweet are also affected by the number of likes, in addition to retweets and quotes, which are already represented through the children vertices. For this reason, likes will affect the size of each vertex proportionately in the final visualization. Obtaining this would result in a visualization that shows the evolution of a conversation of the tweets extracted from a query but also the explanation of it through the impact that users, retweets and likes generate."}, {"title": "3.4.4 Veracity representation", "content": "Finally, the outputs of NLI will serve as the colours to differentiate posts containing misinformation from those that contradict or are unrelated. This is the advance that allows us to check the beginning, transformation and current status of a hoax rather than just all the tweets downloaded through the main keywords appearing in a specific hoax, and that would culminate in the goal of tracing the conversations about misinforming posts, if the hypothesis is confirmed."}, {"title": "4 Case studies", "content": "Three use cases illustrate the application of DisTrack. They contain the beginning and evolution of a piece of misinformation on Twitter, all of them with a different topic to show the versatility of this tool. Firstly, an exploratory data analysis of the downloaded tweets is made to disentangle the types of tweets and of their authors according to their NLI- and Twitter-based metrics. After this, as the proof of the variety of tweets contributing to the expansion of content related to misinformation, the representation of the final graphs is made thanks to the final module of graph generation from DisTrack.\nThese three cases represent three different topics:\n\u2022 Case 1: \"The 80 percent of Muslims living in Europe live from social welfare and they refuse to work\u201d. The first case is linked to the disbelief in institutions and hate against Muslims. It constitutes 32 original tweets and a positive balance of tweets involving entailment in contrast to contradiction (i.e., denying the hoax). The weight of entailment increases much more when every post is shared. Thus, this case contains a total of 84 representative posts, with all the retweets included."}, {"title": "4.1 Case 1: Discredit/hate", "content": "For this first case (see Fig. 6), we focus on a hoax that circulated on Twitter asserting that \u201c80 percent of Muslims living in Europe live from social welfare and they refuse to work\". For this hoax, we retrieved a large pool of information\""}, {"title": "4.2 Case 2 - Antivaccine", "content": "The propagation of how falsely RNA vaccines against coronavirus include graphene oxide has also had an important impact on Twitter and other social media. This hoax has succeeded through paraphrasing in different periods. Whereas the previous case study showed a fixed structure and shape, this one discovers a range of posts expressing the same falsehood with different words and tones, from more declarative sentences to more aggressive ones."}, {"title": "4.3 Case 3: Russia-Ukraine", "content": "This third case, shown in Fig. 8, considers the hoax \"Zelensky sold 17 million hectares of land to Monsanto, Dupont and Cargill\". Unlike the earlier cases, the visualizations here highlight variations in impact, illustrating the hoax's dissemination through a multitude of messages. Echoing the patterns seen in the previous example, there is not a single form of post; instead, a diversity of presentations emerge, ranging from the use of hashtags and user mentions to varying styles and tactics to engage the audience. The spread of this misinformation is primarily driven by reposts, yet these varied expressions of the same false claim also play a significant role, particularly at the initial stages of its spread. This multiplicity of formats and channels underscores the complex nature of misinformation propagation and the challenges in tracing and countering it.\nThis example showcases a viral post from September 19th, 2022, that stood out with 663 retweets and 1,000 likes. Remarkably, its retweets occurred not just on the day of the original tweet but continued sporadically until December 30th of the same year, although diminishing impact. This trend is visualized along the x-axis and highlights both the immediate impact of the hoax and its prolonged presence in the digital discourse.\nThanks to this visualization, abnormal activity is shown on the same user who writes the viral post. This suggests that this person retweets it several times, according to the download of Twitter API data. This continuous activity preserves the propagation of this false information and prevents it from dying on Twitter (X). In addition, there was another post with even more retweets whose initial user has been deleted or suspended (1.597 retweets), as there are no edges linking them to the original post."}, {"title": "5 Results and discussion", "content": "In every analyzed example, DisTrack answers positively to the question \"Can we extract the conversation about a hoax on Twitter?\". The conversations about the three chosen falsehoods have been visualized after downloading and processing and refining the data and metadata of the posts related to them. This demonstrates the success in generating automated queries to extract as many tweets as possible about false claims to later filter them depending on their type of alignment (entailment, contradiction or neutral).\nThis leads to the affirmative answer to the second subquestion \u201cCan we separate tweets related to the hoax in the extracted conversation from tweets not related to it?", "Can we distinguish between hoaxes that propagate a hoax from those that contradict it?": ".", "levels": "not only by separating tweets that are not related to each false claim, but also by separating the entailment posts (those that state misinformation) from contradiction posts (those that deny them).\nIn particular, the use of Transformers in the application of NLI to detect misinformation in this case has allowed DisTrack to identify falsehoods on X, regardless of the way they were exposed in the platform. In case 1, the posts were similar to the false claim taken as the reference, with subtle changes, but in cases 2 and 3, the posts paraphrased the content of the false claim together with hashtags, links or other elements, and their entailment was also guessed by the NLI model. In these downloaded conversations about misinformation, The existence of accounts reproducing exactly the same original content and of those that paraphrase it instead encourages research to unveil the dynamics of falsehoods and the existence of bots in contexts of crises [54].\nThanks to the whole process of searching tweets, aligning them with the validated claim and visual representation, what we present is a complete architecture that allows the automated and detailed analysis of the spread of disinformation on a social network. From the first references to the hoax, to the nodes that most influence its dissemination (thanks to their number of followers, for example) or the patterns observed in the cascade, DisTrack allows us to understand first-hand the dynamics of disinformation and how it spreads and generates a certain impact.\nThe fourth research question \u201cCan we identify the users involved in the conversation of a hoax from beginning to end?", "misinformation": "viral tweets matter, but all the participants in the conversation are relevant. In the three case studies analyzed, falsehoods appear before and after the spread of the most retweeted posts, in the shape of less viral tweets. This research reveals the characteristics of every actor inside the ecosystem of false information: the case studies unveil users with many followers as actors in the propagation of a hoax, those conceived as 'super-spreaders' [45, 46], but also accounts with different weights in terms of followers, not always the most followed ones [54].\nThe results of the four subquestions allow us to answer the main research question: \u201cIs it possible to track conversations around specific hoaxes on Twitter (X)?", "modules": "firstly, the tweets of the conversation about falsehoods were extracted through automated queries (subquestion 1); secondly, they were separated from each other according to their alignment to that false information (subquestions 2 and 3), and, finally, their authors and their characteristics were also identified (subquestion 4) through the final generation of graphs that show the evolution of that falsehood.\nThese outcomes follow recent research relying on tweet extraction and NLI-based classification in the field of misinfor-mation. These studies show the orchestration of posts with different ranges of influence contributing to a more complex propagation, and also the different nature of users in the conversation about specific falsehoods, in accordance with the results provided by DisTrack. The exploratory data analysis of the three presented use cases was a demonstration of this variety of posts and users.\nNevertheless, DisTrack adds a layer of research through the generation of the graphs to exploit more the synergies between NLP and SNA as the fields in charge of combating misinformation [23]: a falsehood does not die progressively after its more viral publication and is not always born directly from it, and fact-checks are also repeated in the successive lives of each falsehood. In the cases analyzed, the conversation is reshaped and hoaxes arise again. Furthermore, the evolution of tweets from fact-checkers and of those that refuse verbally the selected hoaxes does not only occur with the virality of that type of misinformation at its best, but also in other periods, as observed in the final visualizations of this research.\nThe combination of falsehoods and the posts that debunk them in a final visualization is also worth mentioning. DisTrack proposes an additional step to the precursory work: whereas previous experiments contribute to a better knowledge of the nature of the posts about misinformation, they do not offer any surveillance to monitor and mitigate it. Nevertheless, DisTrack brings back those steps of the tweets extraction and NLI filtering to put them at the service of graph generation for that desired supervision. This enables the coordinated response of fact-checkers and the control of their effects on false information.\nOverall, DisTrack reinforces the need to study misinformation as a viral model [44] in a chain of infections by posts with different ranges of influence. Its opposed model covered in research, the broadcast model, would only have shown a part of misinformation because it only conceives contagion as a unique primary vertex infecting the rest. On the contrary, the examples made by Distrack print several versions of the vertex, various infections and, as a consequence, many propagations instead of one.\nThis is also important in the combination of entailment and contradiction posts. Not only do broadcast models [44] isolate a cascade of misinformation from the rest of the generated false information, but they also put it apart from the posts that contradict it and from fact-checkers, preventing researchers from depicting their appearances in the ecosystem of that cascade. Likewise, a broadcast-model cascade of fact-checks without showing the rest of the conversation through the steps of DisTrack would remove them from the falsehoods they counteract."}, {"title": "6 Conclusions", "content": "All in all, our work releases a line of action through the shape of DisTrack, as the beginning of a tool able to merge Transformer-leveraged tweet extraction, NLI-driven tagging of misinformation from the posts retrieved and graph generation of Twitter-based and user-based properties in an output that shows chronologically the evolution of a conversation motivated by misinformation (spreaders, fact-checkers and other users) across the different publications and actors involved.\nWith this proposed line of action, future experiments can study how DisTrack modules can be modified. With NLP as one of the core parts of this research, there is room for improvement given the advance of new Language Models (LM): on the one hand, with their application for new models that capture better the topics and, thus, the keywords for the query that enable the download of posts; on the other hand, with their use for NLI to increase the level of accuracy in the classification of posts as entailment, contradiction of neutral, the three tags used to colour the vertices in the final graphs to build the representative picture of misinformation.\nFuture work includes the application of DisTrack beyond misinformation. For instance, previous research shows how sentiment analysis has become relevant in the studies about aggressive discourse in the context of government elections [55] and has stated Twitter as a \u201csentiment thermometer\u201d through VADER [56]. Although the first steps of these experiments evoque the implementation of DisTrack, involving the download of posts from X and the extraction of"}]}