{"title": "CEGRL-TKGR: A Causal Enhanced Graph Representation Learning Framework for Improving Temporal Knowledge Graph Extrapolation Reasoning", "authors": ["Jinze Sun", "Yongpan Sheng", "Lirong He"], "abstract": "Temporal knowledge graph reasoning (TKGR) is increasingly gaining attention for its ability to extrapolate new events from historical data, thereby enriching the inherently incomplete temporal knowledge graphs. Existing graph-based representation learning frameworks have made significant strides in developing evolving representations for both entities and relational embeddings. Despite these achievements, there's a notable tendency in these models to inadvertently learn biased data representations and mine spurious correlations, consequently failing to discern the causal relationships between events. This often leads to incorrect predictions based on these false correlations. To address this, we propose an innovative causal enhanced graph representation learning framework for TKGR (named CEGRL-TKGR). This framework introduces causal structures in graph-based representation learning to unveil the essential causal relationships between events, ultimately enhancing task performance. Specifically, we first disentangle the evolutionary representations of entities and relations in a temporal graph sequence into two distinct components, namely causal representations and confounding representations. Then, drawing on causal intervention theory, we advocate the utilization of causal representations for predictions, aiming to mitigate the effects of erroneous correlations caused by confounding features, thus achieving more robust and accurate predictions. Finally, extensive experimental results on six benchmark datasets demonstrate the superior performance of our model in the link prediction task.", "sections": [{"title": "1 Introduction", "content": "Knowledge graphs (KGs) have gained significant promise in natural language processing or knowledge engineering perception tasks [5]. They model real-world fact information by using multi-relationship graph structures. However, factual knowledge in reality is constantly evolving, resulting in the form of event knowledge. This has led to the development and application of temporal knowledge graphs (TKGs). TKGs encode the relationship information of entities and events and their timing. This enables TKGs to capture the dynamics of entities and their relationships over time [11]. Thus, analyzing TKGs provides a comprehensive understanding of the evolving events, based on which various time-dependent applications have been developed, including time-sensitive semantic search [1], policy making [8], stock forecasting [9], and more [5].\nThe reliability of applications depends on accurate predicting, which highly relies on data integrality. However, existing TKGs are inevitably incomplete due to the partial observation of real-world [22]. To address this limitation and enhance the capability of TKGs, temporal knowledge graph reasoning (TKGR) models are proposed. TKGR models extrapolate new facts and relationships of TKGs according to their historical temporal information. Existing TKGR models explored different strategies to achieve satisfactory results on TKG extrapolation tasks. GHNN [13] and GHT [28] model historical facts as point-in-time processes. RE-NET [15] and RE-GCN [21] introduce graph neural networks (GNN) into sequence models to capture structural and temporal dependencies between entities. TLogic [23] and TITer [29] design interpretable models based on logical rules and reinforcement learning, respectively.\nDespite the achievements of previous studies, they have overlooked the reality that there are numerous confounding factors in TKGs, such as shallow patterns and noisy links. However, these confounding factors commonly misguide the reasoning process of TKGs, resulting in the acquisition of incorrect dependencies and the generation of non-causal predictions [27]. An illustrative example of this phenomenon present in policy making is depicted in Fig. 1. Typically, a treaty between two countries is formed depending on months and years of negotiations. Nevertheless, there are instances where a state visit may occur between the last negotiation session and the finalization of the treaty. In such a scenario, the state visit becomes a confounding factor influencing the treaty's finalization, since the consequences of a state visit are various and the treaty could only be finalized according to multiple rounds of negotiations. Nevertheless, a TKGR model would misinterpret the state visit as an indication of treaty finalization, based on limited observations of their simultaneous occurrences.\nTo address the aforementioned issue, we advocate for the integration of causal theory into TKGR to guide learning of the essential causal relationships between events and mitigate the impact of confounding factors on TKGR. Specifically, we first construct a structural causal model [34] to comprehensively analyze and model TKGR tasks from a causal perspective. Then, based on the causal model, we propose a new framework, namely causal enhanced graph representation learning (CEGRL-TKGR), to disentangle confounding factors from the essential causal factors in TKGs. The framework utilizes graph neural networks and causal intervention techniques to learn the dynamic To the best of our knowledge, this is the first study to incorporate causal intervention in a graph representation learning framework for learning the evolutionary representations of entities and relations for TKG extrapolation. To conclude, our contributions are 4-folds:\nTo the best of our knowledge, this is the first study to incorporate causal intervention in a graph representation learning framework for learning the evolutionary representations of entities and relations in TKGs.\nWe propose a novel causal enhanced graph representation learning framework for TGKR, called CEGRL-TKGR, to uncover the essential causal relationships between events and mitigate the impact of confounding factors.\nThe proposed CEGRL-TKGR model disentangles the evolutionary representations of entities and relations into causal and confounding representations. Then, it applies causal interventions to perform backdoor adjustments of representations, prioritizing predicted causal features while minimizing the impact of spurious correlations introduced by confounding features.\nComprehensive experimental results demonstrate the CEGRL-TKGR model outperforms state-of-the-art baselines on six real-world datasets in the link prediction task. Further, comprehensive studies confirm the contribution of the introduced causal structures and interventions\u00b9."}, {"title": "2 Related Work", "content": "In this section, we briefly review the research advance of several works that are closely related to our work."}, {"title": "2.1 Temporal Knowledge Graph Reasoning", "content": "TKG inference in extrapolation settings focuses on predicting new facts about the future based on historical events. Specifically, CyGNet [37] uses a copy-generating mechanism to capture the global repetition rate of facts. GHNN [13] and GHT [28] construct a temporal point process (TPP) to capture the temporal dynamics of successive events, predicting future facts by estimating the conditional probability of the TPP. In recent years, with the successful application of GNN in many dynamic scenarios [35], they have also been introduced into structural-semantic dependency models in TKGR. RE-NET [15] used a neighborhood aggregator and cyclic event encoder to model historical facts as subgraph sequences. RE-GCN [21] uses RGCN [26] to learn evolutionary representations of entities and relationships at each timestamp. CEN [19] uses length-aware convolutional neural networks (CNNS) to process evolutionary patterns of different"}, {"title": "2.2 Causal Representation Learning", "content": "In graph causal representation learning, researchers have explored various methods to improve the explanatory power and generalization performance of GNNs. By applying the principles of causal reasoning to graph-structured data, the researchers sought to address the challenges GNNs face when dealing with complex systems such as social networks, molecular maps, and syntax trees of program code. DIR [32] is proposed to reveal the intrinsic interpretability of GNNs by discovering invariant reasons, which involves splitting input graphs into causal and non-causal fruit graphs and training the two classifiers through invariant risk loss functions. GOOD [6] improves the cross-domain generalization of graphs by distinguishing invariant subgraphs from other parts of graphs that are susceptible to domain transfer. CAL [27] introduces de-confounding training to distinguish the key and secondary parts of the graph and eliminate the confounding effect of the secondary parts on model prediction. CFLP [36] points out that the causal relationship between graph structure and link presence is often ignored, and proposed to generate counterfactual links to enhance training data and reduce reliance on false associations. Zevcevic et al. [34] theoretically analyzed the relationship between GNNs and structural causal models (SCMs) and designed a new class of neuro-causal models. However, none of the work has been done to combine causal learning with temporal knowledge graph reasoning."}, {"title": "3 Preliminary", "content": ""}, {"title": "3.1 Notations and Task Formulation", "content": "A TKG G can be formalized as a sequence of graph slices {$G_1, G_2, . . . . , G_T$}, where $G_t = {(e_s, r, e_o, t) \u2208 G}$ denotes a graph slice that consists of facts that occurred at the timestamp t. Here, $e_s$ and $e_o$ represent the subject and object entities, respectively, and r denotes the predicate as a relation type. Besides, $e_s, r, e_o$ written in bold represent their embeddings. Our goal is to predict the future timestamp of the query missing entity. The task can be defined as follows: Given a query $(e_s, r, ?, t)$, the model needs to predict the missing entity."}, {"title": "3.2 A Causal Perspective on the TKGR Task", "content": "To deepen the comprehension of the GNN-based TKG inference mechanism, we abstract the GNN-based inference process through a causal figure, as shown in Fig. 2, encompassing five distinct variables. The connectivity from one variable to another epitomizes the causal relationship, delineated as the cause \u2192 effect. The variables are described as follows:\nGraph data $G_t$: The knowledge graph data at each timestamp t, manifests as a directed relationship figure.\nCausal Feature C: These features epitomize the causal essence of the targeted entity, providing a fundamental understanding of its inherent dynamics.\nConfounding Feature N: These features, discerned from GNN, embody the confounding attributes, unveiling the potential biases or trivial patterns ingrained in graph-based learning methodologies.\nRepresentation R: These representations are the entity and relational representations of the output of the final GNN layer after learning for $G_t$.\nPrediction Y: Denoted as the link prediction, this aspect transitions through the decoder, rendering the ultimate inference based on the preceding representation.\nThe causal embedding encapsulates the causal features C, authentically mirroring the implicit knowledge inherent in the knowledge graph data $G_t$. Conversely, N symbolizes the confounding features, which might be spawned by data biases, data noise, or superficial patterns within graph-based learning methodologies. These confounding features forge a backdoor pathway between C and Y, fostering spurious correlations that don't contribute to accurate inference. Functionally, the structural operation denoted by $C \u2192 R \u2190 N$ portrays a GNN, wherein both the causal features C and the confounding features N, as discerned by the target entity from the graph data, exert a direct impact on the output R of the GNN. Subsequently, the output R of GNN directly sways the model inference outcome, illustrated as $R \u2192 Y$.\nIn the graph-based TKGR paradigm, causal and confounding features are not decoupled for each entity or relationship embedding. Using causal graphs, we aspire to explicitly separate causal embeddings and confounding embeddings from entity or relational representations, and aim to mitigate the effects of confounding features by performing causal interventions. This endeavor not only clarifies the inference process but also endeavors to refine the accuracy and reliability of the GNN-based TKG inference mechanism."}, {"title": "3.3 Causal Intervention", "content": "Beyond fostering a novel comprehension of GNN-based TKGR, causal theory avails analytical instruments predicated on causal figures, such as causal intervention. Causal intervention facilitates a profound examination of the factors precipitating inference outcomes. As delineated by the causal figure, confounding feature N and causal feature C can be discerned from the knowledge graph data. These features are contemplated in the representation R of entities and relations, thereby establishing a backdoor pathway represented as $N \u2190 G_t \u2192 C \u2192 R \u2192 Y$, with N serving as the quick bridge between C and Y.\nTo orchestrate a causal prognosis hinging on the causal feature C, it necessitates the modeling of $P(Y | C')$. However, the backdoor path distorts the probability distribution $P(Y | C')$ through the confounding effect of N, thereby necessitating the disentanglement of the backdoor pathway from N to Y. It is imperative to stymie this backdoor pathway to mitigate the repercussions of the hybrid embedding, thereby enabling the model to reason robustly by leveraging the causal feature to the fullest. Causality theory is a potent toolkit to address this backdoor path dilemma.\nWe engage the do-calculus for executing causal interventions on variable C, intending to sever the backdoor path $N \u2190 G_t \u2192 C \u2192 R \u2192 Y$. Our objective is to estimate $P(Y | do(C))$, as opposed to muddling it with $P(Y | C')$. By using Bayes' theorem with the causal postulation, we can extrapolate the ensuing expression:\n$P(Y | do(C)) = \\sum_{N \\in N} P(Y | C, n)P(n)$ (1)\nThe equation above illustrates that to gauge the causal influence of C on Y, it's requisite to take into account the inference outcomes of both causal and confounding features. This can be perceived as re-coupling the disentanglement feature embeddings, utilizing them for deductive reasoning at future timestamps. However, C and N are usually unobservable, and it is difficult to obtain them directly at the data level, which makes the calculation of the Eq. (1) very challenging. In the next section, we devote ourselves to discussing ways to overcome this problem."}, {"title": "4 The Proposed Approach", "content": "In this section, we detail CEGRL-TKGR, the proposed causal enhancement TKGR framework for learning representations of entities and relationships based on causal features and confounding features. CEGRL-TKGR model consists of three parts: the representation learning part that learns the structure dependence in each $G_t$, the decoupling learning part that learns the entity and relation representations, and the decoder based on the time interval. The structure of the model framework is shown in Fig. 3."}, {"title": "4.1 Entity and Relation Evolution Representation", "content": "Within each $G_t$, representation learning of entities and relationships involves the aggregation of multiple relationships, as well as information from multiple hop neighbors under a single timestamp. Between adjacent $G_t$, we expect to accurately capture the order dependencies inherent in the subgraph with different timestamps. Drawing inspiration from the RE-GCN model [21], we employ the $w$-layer RGCN, which hinges on structure modeling and a recurrent mechanism to progressively update the representations of entities and relations. This approach allows for a more nuanced understanding and modeling of the dynamic interactions within the graph over time.\n$e_{eo,t}^{l+1} = RReLu \\Bigg(\\sum_{(e_s,r,e_o) \\in S_t} \\frac{1}{d_{e_o,t}} W^l (e_{e_s,t}^l r_{e_o,t}^l) \\Bigg) + W^l e_{eo,t}^l$ (2)\n$E_t = GRU (E_{t-1}, E)$ . (3)\nIn the Eq. (2), we describe how the embedding $e_{e,t}^{l+1}$ of entity e, at time step t and layer l + 1 is computed. We integrate the information of all entities and relations connected to entity e, in the graph $G_t$. $W^l, W$ is learnable weights and l has the option of addition or one-dimensional convolution. In the Eq. (3), we showcase how the entity embedding matrix Et is updated via the GRU. Specifically, we take the entity embedding matrix $E_{t-1}$ at the previous time step t \u2212 1 and the aggregated entity embedding matrix E as inputs to obtain the entity embedding matrix $E_t$ at the current time step t.\nFor relations, ensuring consistency with the entity embedding updates within the subgraph sequence is crucial. To achieve this consistency, a specialized GRU tailored for relations is employed for the update process. This mechanism facilitates a harmonized evolution of both entity and relation causal embeddings over the sequence of subgraphs:"}, {"title": "4.2 Disentangled Causal and Confounding Features", "content": "In the previous subsection, the entity and relational representations learned based on GNN contained causal and confounding factors, and we separated them at the presentation level, which provided a solution to the previously mentioned problem of not being able to separate these two features at the data level. To do this, we introduce a decoupling module to decouple causal and confounding features. Taking the entity embedding matrix as an example, it is represented as follows:\n$D_{E,C}, D_{E,N} = softmax(MLP(E)),$ (6)\n$E_C = E \\bigodot D_{E,C}, E_N = E \\bigodot D_{E,N},$ (7)\nWe want the two embeddings learned from the decoupling module to be as independent as possible, which is essential to accurately separate causal and confounding features [4]. Mutual information is a basic quantity to measure the nonlinear correlation of two random variables. Minimizing mutual information is a feasible scheme to decouple causal features from confounding features. Specifically, we implement this process with contrastive log-ratio upper-bound MI estimator [7,31], which utilizes variational distributions q and a neural network to approximate the true distribution. We define the objective function as follows:\n$L_{mi} = E_{p(E_C,E_N)} [log q_\u03b8 (E_N|E_C)] \u2013 E_{p(E_C)}E_{p(E_N)} [log q_\u03b8 (E_N|E_c)]$. (8)\nWe conduct the same operation with relation embedding decoupling, after which we get $R_C$ and $R_N$."}, {"title": "4.3 Temporal Gap Guided Decoder", "content": "After the causal and confounding embeddings of entities and relations in the derived data, we use a specially crafted decoder to determine the likelihood score of potential entities and relations. Events or facts in a data stream may span different periods. For example, major political events may occur in rapid succession over a short period, while certain rare natural phenomena may occur sporadically and at longer intervals. With this in mind, it is reasonable to consider the time intervals of events to get an accurate picture of their temporal relationship. The key to the design of our decoder is the time interval vector, which guides the decoding process to consider the event time interval while calculating the fraction. Formulaic as:\n$t_s = \u03b1_\u03c2 t + \u03b2_\u03c2, t_\u03b9 = \u03b1_\u03b9 \u03c4 + \u03b2_\u03b9$.\nHere, $\u03b1_\u03c2, \u03b2_\u03c2, \u03b1_\u03b9$, and $\u03b2_\u03b9$ signify learnable parameters. Adopting ConvTransE as our decoder, we introduce four variables, which traverse a one-dimensional convolutional layer followed by a fully connected layer, culminating in the extraction of a probability vector encompassing all entities. This process is mathematically articulated as:\n$P_C (e_o | e_s, r,t) = ReLU \\Big(ConvTransE (e_{s,c,t}, r_{c,t}, t_s, t_\u03b9)\\Big) E_{C,t},$ (10)\nWe apply the same decoding process to the confounding features to get $P_N (e_o | e_s,r,t)$."}, {"title": "4.4 Causal Intervention and Training Objective", "content": "Causal-based embedding learns the intrinsic causes that cause events to occur, so the inference results obtained from causal-based embedding are expected to yield reasonable input results. We define the supervised classification loss as follows:\n$L_{E,C} = \\sum_{(e_s,r,e_o,t) \\in G} y_t log p_c (e_o|e_s,r,t),$ (11)\nwhere $y_t$ is label vector. Conversely, confounding features are conceptualized to address conceivable biases or superficial patterns emanating from the training dataset. Given their inability to aid in inference, we proceed to compute their output average across all entity categories and encapsulate the loss as:\n$L_{E,N} = \\frac{1}{E_{N,t}} \\sum_{(e_s,r,e_o,t) \\in G} KL (y_u, log P_N (e_o | e_s, r,t)),$ (12)\nwhere KL denotes the KL-Divergence, $y_u$ represents the uniform distribution.\nWe believe that causal intervention is the manifestation of causal features under the influence of confounding features, but we cannot directly conduct causal intervention at the data level to mitigate confounding effects. Therefore, we obtain intervention features that combine causal features and confounding features at the representation level of entities and relationships. Specifically, according to the backdoor adjustment Eq. (1), we first introduce a random addition procedure to obtain the intervention feature, and for the intervention feature we expect the decoder to still output the correct result:\n$E_{I,t} = \u03a6 (E_{C,t}, E'_{n,t}),$ (13)\n$P_I (e_o | e_s, r,t) = ReLU \\Big(ConvTransE (e_{s,I,t}, r'_{I,t}, t_s, t_\u03b9)\\Big) E_{I,t},$ (14)\nwhere $E'_{Nt}$ is the confounding feature of the entites randomly sampled from $E_{N,t}$.\nThen we define the loss as follows:\n$L_{E,I} = \\sum_{(e_s,r,e_o,t) \\in G} y_t log p_I (e_o | e_s, r,t).$ (15)\nFinally, the loss function of the model for the link prediction task is as follows:\n$L_{E} = L_{E,C} + \u03bb_1 L_{E,N} + \u03bb_2 L_{mi} + \u03bb_3 L_{E,I},$ (16)\nwhere $\u03bb_1, \u03bb_2, \u03bb_3$ are designated as hyper-parameters, and the first two are used to determine the strength of decoupled learning of the model, and the latter is used to determine the strength of causal intervention."}, {"title": "5 Experiments and Analysis", "content": ""}, {"title": "5.1 Experimental Settings", "content": "Datasets We evaluate our model and baselines on six datasets, including ICEWS14 [10],\nICEWS18 [14], ICEWS05-15 [10], YAGO [24], WIKI [17] and GDELT [18]. Integrated\nCrisis Early Warning System (ICEWS) [3] contains international event information and\nis a commonly used benchmark dataset for TKGs link prediction. We choose three subsets of it, i.e., ICEWS14, ICEWS18, and ICEWS05-15, which contain events occurring\nin 2014, 2018, and 2005 to 2015 respectively. The WIKI and YAGO datasets are subsets of the Wikipedia history and YAGO3, respectively. The GDELT is from the Global data on events, location, and tone. To ensure a fair comparison, we use the split manner provided by Sun et al. [28]. Statistics of the datasets are summarized in Table 1.\nBaselines. For the link prediction task, we compare CEGRL-TKGR model with two categories of KGR models: (1) static KGR models, including TransE [2], DistMult [33], ComplEx [30] and R-GCN[26]. We apply these models in static KGs that ignore timestamp information. (2) TKGR models, including TTransE [17], TA-DistMult [10], TNT-ComplEx [16], RE-GCN [21], GHT [28], EvoKG [25], TITer[29], xERTE [12], TLogic[23] and CEN[20].\nEvaluation Metrics. The mean reciprocal rank (MRR) and Hits@k are standard metrics for the TKG link prediction task. MRR is the average reciprocal of the correct query answer rank. Hits@k indicates the proportion of correct answers among the top k candidates. We used a more reasonable time-aware filter setting to report our results. The time-aware filtering setting filters out only the four groups that occur at query time, and can simulate extrapolated prediction tasks in the real world [29]."}, {"title": "5.2 Experimental Results and Discussion", "content": "Table 2 and Table 3 report the experimental results of the link prediction task on six TKG datasets. Static KG embedding methods fell far behind CEGRL-TKGR due to their inability to capture temporal dynamics. Our method is also superior to other temporal knowledge graph extrapolation methods in predicting events. The improved performance shows that surface patterns and noise are widely present in several real-world data sets. The previous methods are generally inadequate in design. The model based on evolutionary representation will learn the inherent confounding features in TKG when gathering neighborhood information and transmitting historical information, and the model based on rule-based inference will mine the false correlation in the data, all of which will lead to the model making non-causal predictions in the inference stage. Our model incorporates causal theory into the TKG inference task and visibly separates causal features from confounding features. This helps to protect the model from surface patterns and noise present in the data and to uncover the real reasons that affect the formation of links between entities. TiTer and EvoKG show excellent performance on YAGO datasets because the former's historical fact search strategy works well on smaller datasets, while the latter's modeling of event timing works well on datasets containing events at relatively regular time intervals."}, {"title": "5.3 Performance on Noisy Temporal Knowledge Graphs", "content": "To explore whether CEGRL-TKGR can alleviate the problem of the TKGR model affected by noise and surface patterns in training data. Taking YAGO and WIKI datasets as examples, we respectively test the performance of our model and the model without the causal enhancement module under different noise deviations. Noisy TKGs were generated by randomly replacing a certain percentage of positive triples in the training set. The experimental results are shown in Fig. 4.\nFrom the experimental results, we can draw the following conclusion: when the noise in the data increases, the performance of the model lacking the recognition of causal features and confounding features will deteriorate sharply, and the performance of MRR and Hits@1 will decrease by up to 8.69% and 8.94% respectively, which indicates that the GNN-based TKGR method is easy to capture data bias and make wrong predictions based on it. However, CEGRL-TKGR uses the causal enhancement module to effectively reduce the impact of confounding features and shows more stable performance on noisy data sets. The performance degradations on MRR and Hits@1 are significantly smaller than those without the causal module."}, {"title": "5.4 Ablation Study and Analysis on Parameter Sensitivity", "content": "Ablation Study. In this subsection, we investigate the effectiveness of causally enhanced and time-interval guided decoders for link prediction. Specifically, CEGRL-TKGR w/o TD means that no time interval vector is used to guide the decoder to work, and CEGRL-TKGR w/o CE means that the model removes causal decoupling and causal intervention parts. Table 4 shows the results of ablation experiments, which indicate the effectiveness of these two components. As can be seen from the results in the table, for data sets such as YAGO and WIKI that contain relatively regular time intervals, a temporal gap-guided decoder can capture this time interval pattern well enough to make accurate predictions. At the same time, it does not degrade performance even for time-interval insensitive data sets. Our causal enhancement section, under the independent constraint of emphasizing causal features and confounding features, eliminates the influence of the fast bridge through causal intervention, forcing the model to learn the intrinsic causes of the events. It is worth noting that our causal enhancement module can be seen as a flexible component that can be easily used in many GNN-based TKGR frameworks.\nSensitivity of hyperparameters. In the proposed model, $\u03bb_1$ and $\u03bb_2$ jointly affect the disentanglement intensity of causal and confounding features, and $\u03bb_3$ controls the intensity of causal intervention. We studied the sensitivity of parameters in different data sets. Specifically, one parameter is fixed at 0.5 and the other parameter varies in [0,1] with a step size of 0.1. As shown in Fig. 5, the model is relatively stable in most parameter selection cases, but on noisy data sets, the model has higher requirements for hyperparameters, and extreme values will degrade the performance of the model. The best range for $\u03bb_1, \u03bb_2$ is about 0.5 to 0.7. $\u03bb_3$ should be a relatively small value, ranging from 0.3 to 0.6."}, {"title": "5.5 Case Study", "content": "To show the ability of the CEGRL-TKGR model to learn the causal relationships that lead to events, we present a real case of CEGRL-TKGR versus a model that does not use causal augments in the ICEWS14 test set on Greek politics.\nAs shown in Fig. 6, for inquiries (Movement Party Chairman, consult, ?). The model that does not use causal augmentation does learn the information in history, it learns the pattern of events that occurred multiple times in the training set, that is, at multiple different times (movement party chairman, consult, prime Minister), and gives the answer accordingly as the prime Minister. In contrast, CEGRL-TKGR was keen to capture events close to the inquiry (Movement Party chairman, express intent to meet, Alliance Party Chairman), which directly led to the occurrence of this event (Movement Party Chairman, consult, Alliance Party Chairman). The model is guided to make correct predictions by the learning of causality."}, {"title": "6 Conclusion", "content": "In this paper, we revisit the GNN-based TKGR model from the perspective of causality, on this basis, we propose an innovative causal enhanced graph representation learning framework. By synergistically integrating causal structures with graph representation learning in TKGs, CEGRL-TKGR overcomes the problem of existing models unintentionally learning biased data representations and mining for false correlations. Comprehensive experiments have proved the effectiveness of CEGRL-TKGR. In future work, we desire to explore how causal learning can help TKGR tasks in addition to representation learning."}]}