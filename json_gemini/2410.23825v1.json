{"title": "GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages", "authors": ["Amir Hossein Kargaran", "Fran\u00e7ois Yvon", "Hinrich Sch\u00fctze"], "abstract": "The need for large text corpora has increased with the advent of pretrained language models and, in particular, the discovery of scaling laws for these models. Most available corpora have sufficient data only for languages with large dominant communities. However, there is no corpus available that (i) covers a wide range of minority languages; (ii) is generated by an open-source reproducible pipeline; and (iii) is rigorously cleaned from noise, making it trustworthy to use. We present GlotCC, a clean, document-level, 2TB general domain corpus derived from CommonCrawl, covering more than 1000 languages. We make GlotCC and the system used to generate it\u2014 including the pipeline, language identification model, and filters available to the research community.", "sections": [{"title": "1 Introduction", "content": "Progress in building language technologies has largely been limited to about 200 languages (referred to as \"large languages\" in this paper) for which text is reasonably accessible [34]. In this regard there is a disparity between large languages and minority languages, a disparity that becomes much more visible as state-of-the-art language technologies \u2013 developing from word embedddings and encoder-only models to large autoregressive models \u2013 become more data-hungry. FastText [14, 28] word vectors supports \u2248160 languages, XLM-R [22] \u2248100 languages, Llama-3 [51] \u224830 languages. So there is a trend for the number of supported languages to decrease over time in state-of-the-art models.\nWhile efforts have been made to compile multilingual corpora from books [45, 26], the most common approach to collecting large amounts of raw textual data is to rely on crawled web text [55, 4, 11, 42, 67]. However, there are still many issues to resolve to guarantee the quality of the extracted web text, particularly for minority languages [41]. Most pipelines for web-text extraction rely on language identification (LID) to classify text, e.g., on FastText [14, 35] (as does OSCAR [4]) or on CLD3 [15, 64, 73]. Many errors in existing web corpora are artifacts of their LIDs [20]. For example, FastText's accuracy suffers from hash collisions: it maps out-of-domain (OOD) n-grams to n-grams seen in training, resulting in many OOD errors. Additionally, commonly used LIDs can identify the (roughly) 200 largest languages but only a few minority languages, leading to \"out-of-model cousin\" errors [20, 41].\nIn this paper, we adopt the Ungoliant pipeline [3] for extracting text from CommonCrawl. To address the limitations of current LID models (hash collisions and limited language coverage), we develop a new LID model, GlotLID v3.0, an extension of GlotLID v1.0 [37]. This model covers over 2,000"}, {"title": "2 GlotLID", "content": "In our previous work, we introduced GlotLID v1.0 [37], an LID model based on the FastText architecture [14] with good performance for 1665 languages. Upon further analysis, we improved GlotLID to v2.0 and then v3.0. In v2.0, we extended our ISO 639-3 labels to also include the ISO-15924 script; an example is rus-Cyrl (Russian language in Cyrillic script). This new labeling reduces errors and enhances the quality of language resources. For instance, it allows us to restrict the set of language labels that can be assigned to each script, thereby avoiding obvious mismatches between language and script [40]. This can be implemented by employing writing system detection tools, such as GlotScript-T [40], during inference. We also use GlotScript-T and GlotScript-R to filter out noisy training data from the GlotLID training corpus. GlotScript-T determines the script of an input text. GlotScript-R provides the admissible script(s) for each language.\nIn GlotLID v3.0, we increase the number of LID labels to more than 2000 by adding, removing and relabeling resources based on feedback from the community and on our own findings from the continuous analysis we are conducting."}, {"title": "2.1 GlotLID v3.0 vs GlotLID v1.0/v2.0", "content": null}, {"title": "2.1.1 Increased coverage of minority languages", "content": "We include in the GlotLID training set new resources for African languages [43, 66, 7, 56], Uralic languages [31, 74], Indonesian languages [70], Indic languages [48, 25] as well as additional indigenous languages [19]. We also consider data for a variety of languages from books [45], children storybooks [2], crowdsourcing [10], low-resource crawls [1], and world language repositories [54]."}, {"title": "2.1.2 Better rejection model with the \"UND\" label", "content": "LID is typically understood as a closed-set classification problem; most LIDs, including GlotLID, adopt this setup. As LID is in fact an open-set problem [32, 49], when processing web data, there is always the risk of encountering \u201cunknown\" languages (those not occurring in train). To limit errors that are caused by these unknowns, it is customary to reject samples for which the most likely language has a low probability, assuming the classification model is well-calibrated [68, 37].\nThis problem is compounded when using popular LID architectures such as FastText and CLD3, which detect languages based on character n-gram features. To limit memory footprint and speed up inference, FastText hashes character n-grams into a predefined range of integers, using this map to retrieve feature embeddings. This procedure applies to utf-8 character n-grams, where the default for GlotLID is $2 \\leq n \\leq 5$.\nThis approach makes no distinction between n-grams that have been seen in training and those that have not, as any n-gram is assigned to an existing hash value. Furthermore, as the number of languages increases, so does the number of scripts, and accordingly, the number of possible n-grams. With 160+ scripts, this number is in fact much greater than FastText's default hash size (2,000,000), increasing the chance of collisions. This first implies that the probability of languages that are well represented in the training data (i.e., high-resource languages) or that have a large number of n-grams due to their character set (e.g., Chinese) is overestimated. This also means that closed-set LIDs will provide a non-zero probability score even for writing systems that were never seen during training, predicting (sometimes with high probability) languages whose n-grams collide with unseen n-grams.\nAlthough GlotLID supports all major scripts, its training data does not contain minor scripts such as Mong (Mongolian), Sylo (Syloti Nagri), Newa (Pracalit), Talu (New Tai Lue) and Gran (Grantha). In"}, {"title": "2.1.3 Removing noise with \"zxx\" labels", "content": "In addition to handling unseen scripts, web crawlers and processing systems also need to be robust to multiple types of noise [20]. To make GlotLID more robust, we create training data for additional types of noise, including mis-rendered PDFs and Mojibake (text decoded using an unintended character encoding). We either create artificial training data or we use an initial seed and query the Google search engine to obtain training data.\nWe include six major sources of web noise that we encountered in our work on GlotLID.\n1) Mis-rendered PDF: This is gibberish Latin text that appears when an Arabic PDF is mis-rendered by OCR. To generate representative sentences, we query 1- and 2-grams of the letters 'i', 'j' and 'l', with spaces between them, as these letters are frequent in this context.\n2) ANTSPE A K: This is a type of noise where the characters of the text are space-separated [20]. It is easy to generate synthetic training data from any text or alphabet.\n3) Binary files: This type of noise occurs when binary files, especially images, become part of the text. It is easy to generate training data by opening any binary file in a text editor.\n4) Mojibake Latin: This is gibberish text that results from text being decoded using an unintended Latin character encoding. In this type of noise, vowel characters of latin with different accents in a repeated form can serve as a seed for Google queries, such as \u201c\u00e1\u00e0\u00e3\u00e0\u201d.\n5) Mojibake Arabic: This is gibberish text that results from text being decoded using an unintended Arabic character encoding. In this type of noise, ^, \u00ae, \u00b1, \u00a7 and the Arabic characters Tah and Zah appear frequently. Our queries are based on these characters.\n6) Replacement character: The replacement character (U+FFFD) appears repeatedly in this type of text. We simulate it by randomly replacing characters with the replacement character.\nWe create three GlotLID labels for these six noise types: zxx_Latn for noise types [1-4], zxx_Arab for noise type 5 and zxx_Zzzz for noise type 6."}, {"title": "2.1.4 Curation of labels", "content": "To curate our set of LID labels, we rely on confusion matrices, focusing on languages with low performance, i.e., those that are frequently confused with others. We employ both genealogical analysis and basic sanity checking using web resources about the languages. See Section 7 of [37] for an example of our methodology. Most of the problems identified this way were due to the training data, e.g., the training corpus for a given language is actually a mix of several languages. We remove labels and their training data if we deem them too noisy based on our analysis.\nGlotLID v1.0 and v2.0 both support macro languages and individual languages. In cases where the correct class is an individual language, the associated probability is often spread over this language and its macro language. The reason is that many individual language n-grams and words are also frequent in the macro language. In GlotLID v1.0/v2.0, we provide the option of taking the softmax on a subset of LID labels (e.g., on just the individual language and the macro language). However, according to community feedback, it is preferable for labels to be mutually exclusive, making it possible to run LID just once over all supported labels. We rectify this problem by (i) re-labeling macro languages as one of the individual languages; (ii) merging the individual languages into the macro language (in case of small differences between them, e.g., we merge individual languages prs_Arab and pes_Arab into macro language fas_Arab); and (iii) deleting the macro language in case we already have good support for its individual languages such as \u201czho_Hani\u201d, \u201caze_Latn\u201d, \u201cest_Latn\"."}, {"title": "2.2 Evaluation setup", "content": "We train GlotLID v3.0 using the same parameters and sampling strategy as GlotLID v1.0 [37] (also described in Table 1) for 1 epoch. In our previous work [37], we found that the variance of the FastText architecture with different initial seeds is negligible."}, {"title": "2.2.1 Evaluation data", "content": "We evaluate GlotLID v3.0 on three existing benchmarks with a high number of languages: UDHR, FLORES-200 and GlotTest (our in-domain test set).\n1) GlotTest: Let $n_l$ be the number of sentences from language $l$ in the GlotLID corpus test set. Then we sample $\\text{min}(1000, n_l)$ sentences from it. We refer to the resulting dataset as GlotTest. GlotTest supports 2102 LID labels, the same number of labels supported by GlotLID v3.0. This includes the \"und\" labels (\u00a72.1.2) and \"zxx\" labels (\u00a72.1.3).\n2) UDHR: UDHR consists of about 500 translations of the \u201cUniversal Declaration of Human Rights\". In this work, we use the UDHR test set released in our previous work [37]. It supports the 415 translations from udhrinunicode.org that are available with a valid ISO 639-3 code. 371 of these are covered by GlotLID v3.0 training data.\n3) FLORES-200: FLORES-200 [55] comprises 842 articles sourced from English-language Wikimedia projects. Each sentence of these articles was translated into 204 language_script labels. The dataset is split into 997 sentences for development, 1012 for dev-test and 992 for test. FLORES-200 test is not public. As is common practice, we use FLORES-200 dev-test as our FLORES-200 test set."}, {"title": "2.2.2 Evaluation results", "content": "Table 2 reports results on GlotTest, UDHR and FLORES-200. On average, GlotLID v3.0 achieves an F1 score of 0.991 and a false positive rate of 0.000003 on GlotTest. The three lowest F1 scores on GlotTest are 0.72, 0.75, and 0.76 for bos_Latn, hrv_Latn, and cnr_Latn, three mutually intelligible varieties of BCMS (Bosnian-Croatian-Montenegrin-Serbian). We made sure that every LID label has performance of at least 0.7 on the GlotLID validation set. Otherwise, we remove or merge the label (see \u00a72.1.4).\nCompared to GlotLID v1.0 [37], GlotLID v3.0 shows improvements in F1 of 0.05 for GlotTest, 0.09 for UDHR, and 0.05 for FLORES-200. Although the UDHR results are the lowest among these three benchmarks (F1 of 0.882 vs 0.991 and 0.967), GlotLID v3.0 outperforms the state-of-the-art also for this dataset [37]. The most likely reason for the lower performance on UDHR is a domain shift: the data in the GlotLID corpus for some of the UDHR languages is dominated by religious texts."}, {"title": "3 GlotCC v1.0", "content": "The process of GlotCC creation is similar to other pipelines for large-scale web corpus creation [42]. More specifically, we use Ungoliant [3], the OSCAR pipeline [58]. Instead of OSCAR's FastText LID (which detects 176 languages), we use GlotLID v3.0. Recall that this change also includes the zxx and und labels that are part of GlotLID's label set and support robust noise removal (\u00a72). We further provide extensions for content classes (\u00a73.1) and quality warnings and filtering (\u00a73.2, \u00a73.3). We also perform replacement of personally identifiable information (\u00a73.5) for GlotCC. We distribute our forked pipeline under the Apache 2.0 license, the same as the Ungoliant license. GlotCC is licensed under CommonCrawl terms: commoncrawl.org/terms-of-use.\nFor this paper, we ran the pipeline on the CommonCrawl CC-MAIN-2024-10 snapshot in its entirety and on parts of CC-MAIN-2023-40 and CC-MAIN-2023-50. We refer to the corpus produced by this process as GlotCC v1.0 (or GlotCC for short).\nGlotCC v1.0 contains data (subcorpora) for 1275 LID labels (i.e., language-script pairs such as \u201crus-Cyrl\u201d). Table 3 shows their geographic distribution for Glottolog [29] macroareas. Based on the Wikipedia list of ISO 639-3 codes, we also add 12 constructed languages (\"Constructed\"). As reflected in Table 4, GlotCC v1.0 considerably increases language coverage compared to OSCAR 23.01, especially for minority languages. The number of languages with more than $10^2$ documents in GlotCC is 145+89+52+29+22+12=349 (vs 132 for OSCAR), and with more than 10 documents is 349+360=709 (vs 142 for OSCAR). This language coverage can be easily increased by applying GlotLID on more CommonCrawl snapshots. One reason for GlotCC's better coverage could be that we lose less minority language content via contamination [13] to \"large\" languages. GlotCC's Wikipedia percentage is highest (.2658, .2940) for languages with a document count between $10^2$ and $10^4$. Many GlotCC languages with \u2264 $10^2$ documents come from religious websites (.4441, .4285). GlotCC's coverage of languages with more than $10^7$ documents is lower than OSCAR's because we apply more cleaning filters."}, {"title": "3.1 Annotating documents with content classes", "content": "Similar to OSCAR 23.01, we use the UT1 blocklist [61] to classify websites into different content classes such as \"adult\" and \"blog\". The main utility of these UT1-based filters is to warn about potential adult content websites, with 3.5 million domains labeled as adult in UT1.\nWe add to UT1 two additional content classes: \u201cwikipedia\u201d and \u201creligious\u201d. The religious content class is important to assess how domain-specific a particular language's corpus is \u2013 for some minority languages almost all web content is religious [42]. Following OSCAR 23.01, we do not remove any content classes from GlotCC and instead leave that decision (e.g., removal of adult content) to the user, mainly because of UT1 false positives [4]."}, {"title": "3.2 Quality warnings", "content": "We add new quality warnings adopted from prior work on data cleaning and web crawling [63, 62, 4, 30, 40, 42]. Specifically, we provide the following quality warnings.\nTiny: The document has a small number of lines. Following [63], we use a threshold of three lines.\nShort sentences: The document has a high number (\u2265 50%) of short lines [4].\nHeader and footer: CommonCrawl contains boilerplate extracted from headers and footers. We give a header (resp. footer) warning if short lines occur at the start (resp. end) of the document [4].\nInconsistent: Ungoliant applies LID at both document and line levels. If \u226560% of lines do not match the document-level label, we mark the document as LID-inconsistent. If \u226510% of the script content is incompatible with the label predicted by LID, we mark the document as script-inconsistent [40].\nList case: Flag sentences with \u226550% of tokens beginning with a capital letter [42]. Some scripts like Hani lack capital letters. However, since Chinese is written without spaces, we can still find lists by identifying portions of the text that are shorter than five characters and are surrounded by spaces.\nTechnical characters: Fires when \u226520% of characters are numbers/punctuation [42]. The warning script-inconsistent is also raised here since \u226510% is not written in the main script.\nCursed regex: These are substrings and regexes from [42] used for identifying noisy and questionable content."}, {"title": "3.3 Quality warning filters", "content": "We sample 20 sentences from three languages for each script: one from the top 10% of the language distribution (i.e., with most data in GlotCC), one from the bottom 75% and one from the remaining 15%. For each sentence, we determine whether it is high-quality content in the target language. For unknown languages, we check the website URL and search for information about the language. We find that the quality warnings generally indicate bad content or erroneously assigned LID labels and therefore remove sentences with quality warnings in GlotCC. There are two exceptions.\nFirst, we ignore three warnings: short sentences, header, and footer, because the LID label is correct for most of these documents, or in other cases, the warnings are false positives. Second, for languages without overt word boundaries, we keep documents with warnings long word and repetition as computing these warnings is nonsensical if the text is not separated into words. The warning metadata for these five warnings is kept in GlotCC in case users want to use it."}, {"title": "3.4 Deduplication", "content": "Following OSCAR 23.01, a hash is provided for each document in GlotCC. This hash is computed by py-tlsh with hyperparameters 256 buckets and 3-byte checksums [4]. However, we do not distribute deduplication as part of the pipeline, because deduplication is costly [26, 4] and hashing algorithm and hyperparameters are application-dependent."}, {"title": "3.5 Personally identifiable information replacement", "content": "We replace two types of personally identifiable information (PII): email addresses and public network IP addresses. We do not replace phone numbers due to the high false positive rate of regex patterns. We use the PII process implemented by DataTrove [59] (see also FineWeb [60]). Email addresses are replaced with \"email@example.com\u201d or \u201cfirstname.lastname@example.com,\" and public network IP addresses are replaced with one of six IP addresses: \u201c22.214.171.124,\u201d \u201c126.96.36.199,\" \u201c188.8.131.52,\u201d \u201c184.108.40.206,\u201d \u201c220.127.116.11,\u201d or \u201c18.104.22.168,\" which, at the time of corpus creation, were unresponsive to pings."}, {"title": "3.6 Wall time", "content": "We calculate the pipeline's wall time, considering only the LID, without accounting for quality warning filters, the PII process, or any infrastructure-related bottlenecks. Suppose the LID throughput is, on average, $T_S$ sentences per second and $T_D$ documents per second, and we can run P parallel jobs.\nWe have $D$ documents to annotate, each containing an average of $S$ sentences. For each document, in addition to running the LID on the entire document, we also run it on each individual sentence. The estimated processing time in hours is given by:\n$\\text{Wall time (hours)} = \\frac{D}{3600 \\times P} * (\\frac{S}{T_S} + \\frac{1}{T_D})$\nThe processing time on Intel Xeon E7-8857 3GHz CPUs with P = 48 for one Common Crawl snapshot (D = 3.16 \u00d7 10\u00ba for CC-MAIN-2024-10) is estimated, using the given values of Ts = 1379, TD = 245, and S = 20, to be approximately 340 hours."}, {"title": "3.7 Self-audit quality review", "content": "We perform a self-audit of GlotCC-V1.0. We have two motivations. First, following [41], we want to ensure that the target language metadata are correct and that there are no systematic issues. Second, we intend to develop additional filters to clean the corpus for future releases. The bottleneck in such an audit is the difficulty of finding native speakers for each language. Therefore, following [42], we conduct the audit by providing high-level comments on the data quality and identifying the language of the data by looking for language clues.\nWe randomly select 653 languages. We sample 20 pages from each (or all if there are fewer than 20) and check the validitiy of GlotCC's LID label. Additionally, we analyze common errors and provide high-level comments.\nWe follow these guidelines in conducting the audit:\n\u2022 For unknown languages, we inspect the URL and visit the webpage to find language clues such as language codes (especially in the URL), the lang attribute inside the html tag, country flag, contact address and the name of the language in the text. Otherwise, we search for sentences on the web to consult similar webpages related to that sentence.\n\u2022 If the corpus contains noise but the noise appears filterable, we leave a high-level note detailing the noise and how it can be filtered.\n\u2022 We report the percentage of in-language sentences for each audited language.\nOverall results. Out of 653 audited languages, we find that, with a macro-average score of 0.93 and a median score of 1.0, the data is in-language. During the audit, for some languages, we couldn't determine the correct language; therefore, we do not consider those languages in the audited set.\nWe find 10 out-of-model languages: aon (Bumbita Arapesh, Torricelli), bkx (Baikeno, Malayo-Polynesian), gup (Gunwinggu, Arnhem, Northern Australia), ibl (Ibaloi, Philippine), kpo (Ikposo, Atlantic-Congo), mcr (Menya, Trans-New Guinea), mge (Mango, Nilo-Saharan), mrh (Mara Chin, Sino-Tibetan), sxw (Saxwe Gbe, Atlantic-Congo) and tao (Yami, Malayo Polynesian). We plan to include these languages in GlotLID v4.0. There are also errors that neither the LID nor the filters captures. For example, repetitive n-grams in list-like content, such as those at the start or end of words from websites like anagrams.app. Based on these audits, we published a more clean version of GlotCC-V1.0 to the community."}, {"title": "3.8 Evaluation of LID within the pipeline", "content": "We compare the NLLB LID [55] and GlotLID within the context of the Ungoliant pipeline. For this comparison, we run the pipeline in the exact same configuration, except that we use NLLB LID for one run and GlotLID for the other. We look at a random sample of minority language pages for which the two pipelines make different predictions.\nIn more detail, we select a subset of size \u22481% (a prefix) of the latest CommonCrawl snapshot (CC-MAIN-2024-18) and run the GlotCC pipeline on it, once using GlotLID as the LID and once using the NLLB LID as the LID. This produces a corpus that is analogous to GlotCC-v1.0, except it is based on (a prefix of) a different CommonCrawl snapshot. We count the number of times that an LID label (e.g., rus-Cyrl) is assigned by either GlotLID or NLLB LID in this \u201cfilter\u201d subset; we refer to this number as $n_l$ for label l. We restrict the comparison to those labels that occur $n_l \\leq 10$ times because our main focus is minority languages, not \"large\" languages that are already well covered by existing resources. We further only consider pages for which GlotLID and NLLB LID disagree. This"}, {"title": "4 Related work", "content": "There is a wealth of resources to perform LID, but not all of them meet the requirements of a good LID for minority corpus creation [37]. All of the following cover at most 218 languages: CLD2 [50], Equilid [36], Langdetect [65], langid.py [47], OpenLID [18], NLLB LID [55], FastText LID [14], CLD3 [15, 64], and HeLI-OTS [33]. Some LIDs are not open-source, e.g., those published by Caswell et al. [20], Bapna et al. [11], Kudugunta et al. [42]. whatlang [16, 17] and idNet [23] are two broad-coverage LIDs that meet many other requirements but are hard to use in many practical scenarios due to software issues and lack of maintenance. Franc [72] is a character 3-gram LID with support for 400+ languages; however, it does not provide well-calibrated probabilities. Another LID with similar properties and support for 1600+ languages is FUN-LangID [71]; however, according to the developers, this model is not the best for high performance on F1/FPR. AfroLID [5], which covers African languages, is a Transformer architecture and less efficient than its competitors. A strong requirement for LIDs is their effective applicability for very large corpora, not least for ecological reasons. GeoLID [24] meets most of the requirements; it supports 900+ languages and the architecture is based on FastText. However, it needs geographic prior information, which makes it more suitable for social media such as X (Twitter) that provide such geographic priors."}, {"title": "4.2 Multilingual corpora", "content": "Much work has been done on mining multilingual corpora from the web. Xue et al. [73] introduce mC4, a general 101-language web domain corpus, to train the mT5 model. Similarly, Conneau et al. [22] introduce CC-100 based on the CC-Net repository [69] to train the XLM-R model. The OSCAR corpus [4] supports 150+ languages. The mC4 pipeline uses CLD3, and CC-Net and OSCAR use FastText LID. NLLB Team et al. [55] mine an internal corpus from CommonCrawl with 200+ languages using NLLB LID. The MADLAD-400 corpus [42] is another mined corpus with 450+ languages using an internal 500-language coverage LID and pipeline. The most closely related work to ours is by Bapna et al. [11], who create an internal corpus of 1500+ languages using an internal 1600+ LID and pipeline. There are also high-quality pipelines that focus on creating corpora mostly for English, including Dolma [67], RedPajama-Data-v2 [21], DCLM [46] and FineWeb [60]. There is also some work that does not only mine the web, but builds mulitlingual data upon other datasets by compiling multiple data sources, such as the ROOTS corpus [44], a community-built dataset that contains 46 languages. CulturaX [53] combines mC4 3.1.0 and different OSCAR versions and applies additional filters, resulting in 160+ languages. The Glot500 corpus [30] covers 500+ languages (400+ open-access), mostly based on prior work in academia. Serengeti [6] also introduces an internal dataset of 500+ African languages derived from religious texts, news and academia."}, {"title": "5 Limitations", "content": "Use cases: Due to certain filtering steps (e.g., curly bracket filter), the GlotCC likely does not contain much math/code content. It is advisable to supplement GlotCC with math/code data if this is the intended use case. Due to the use of LID and script inconsistency filters as indicators of noise, the documents that exist in the corpus are more monolingual than multilingual or code-switched [39] compared to when these filters are not used. Additionally, since we did not customize the processing for each website, some sources, such as Wikipedia, may have better formatting in the original than in GlotCC.\nErrors: Although we have good in-language content, GlotCC still exhibits many types of errors and noise, including misclassification and out-of-model cousins.\nModel training: We did not train any language model to better justify the significance of GlotCC, as this would be pointless unless we also evaluate them. Evaluating language models requires evaluation data that we mostly do not have for minority languages (see [38, 9, 8, 30]). At this stage, this endeavor seems unrealistic. We believe that identifying relevant data and performing LID is a crucial first step in that direction."}, {"title": "6 Conclusion", "content": "We introduce a document-level, general domain web corpus covering more than 1000 languages. We open-source the entire pipeline, including a better language identification model, which is more robust in the corpus creation task in terms of noise handling, unseen writing systems, and a broad coverage of languages to reduce the chance of encountering unknown languages. We filter the created corpus and perform a self-audit to ensure the created corpus is clean.\nWe hope the creation of such a corpus and pipeline will benefit language technologies, enabling the inclusion of more minority languages. For future work, we plan to extend this corpus to additional CommonCrawl snapshots."}, {"title": "7 Ethics statement", "content": "The advancement of NLP technologies has primarily been constrained to languages for which resources are available in good quality and quantity. Many of the world's minority languages face a significant barrier due to the scarcity of high-quality general data sources, making it difficult to develop NLP tools for these languages. Despite concerns that an \u201cextractive\u201d approach to NLP often does not benefit the affected communities [12], it is still an important goal of both computational and theoretical linguistics to have as good a representation of minority languages in available web-based corpora as is permitted by the licenses of content available on the web. By providing a cleaned corpus like GlotCC, we take initial steps towards including a diverse range of languages in NLP. Despite our strong focus on filtering, the cleaning of GlotCC is constrained by the lack of tools for filtering out undesirable content and noise \u2013 such as adult content, personal information, lists \u2013 and this is a considerable problem for a subset of languages, including languages without explicit word boundaries. Therefore, we recommend users carefully evaluate their specific use case before using GlotCC."}, {"title": "A.1 GlotLID model card", "content": "We provide the GlotLID v3.0 model card based on the model card template introduced by Mitchell et al. [52", "model": "LMU Munich and Sorbonne Universit\u00e9\n\u2022 Model Date: April 18", "Types": "Language identification and language modeling.\n\u2022 Model Access: (C) (C): github.com/cisnlp/GlotLID and hf.co/cis-lmu/glotlid\n\u2022 Information about training algorithms, parameters, fairness constraints or other"}]}