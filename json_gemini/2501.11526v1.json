{"title": "Meta-Instance Selection. Instance Selection as a\nClassification Problem with Meta-Features.", "authors": ["Marcin Blachnika", "Piotr Ciepli\u0144ski"], "abstract": "Data pruning, or instance selection, is an important problem in machine\nlearning especially in terms of nearest neighbour classifier. However, in data\npruning which speeds up the prediction phase, there is an issue related to the\nspeed and efficiency of the process itself. In response, the study proposes an\napproach involving transforming the instance selection process into a classi-\nfication task conducted in a unified meta-feature space where each instance\ncan be classified and assigned to either the \"to keep\" or \"to remove\" class.\nThis approach requires training an appropriate meta-classifier, which can be\ndeveloped based on historical instance selection results from other datasets\nusing reference instance selection methods as a labeling tool. This work\nproposes constructing the meta-feature space based on properties extracted\nfrom the nearest neighbor graph. Experiments conducted on 17 datasets\nof varying sizes and five reference instance selection methods (ENN, Drop3,\nICF, HMN-EI, and CCIS) demonstrate that the proposed solution achieves\nresults comparable to reference instance selection methods while significantly\nreducing computational complexity. In the proposed approach, the compu-\ntational complexity of the system depends only on identifying the k-nearest\nneighbors for each data sample and running the meta-classifier. Addition-\nally, the study discusses the choice of meta-classifier, recommending the use\nof Balanced Random Forest.", "sections": [{"title": "1. Intrduction", "content": "Currently, increasing attention is being paid to the issue of filtering data\nused in the learning process, with a focus on removing useless and noisy data\nsamples. Hence, more and more dedicated solutions are being developed to\nfilter out unimportant samples [1] and constitute the new paradigm called\ndata-centric AI[2]. Additionally, the scaling law turns into the requirement\nof huge compute power both during training and prediction which is not\nalways applicable in real live scenarios where the compute resources are lim-\nited. In that case, both the training data and the prediction model should\nrequire small computing resources. Therefore the training set should ensure\na possible small size but keep the prediction accuracy of the original train-\ning set. This issue is not new, along with its development has been started\nprimarily for the nearest neighbor classifier under the name of instance se-\nlection. Thus, already in the late 1960s and early 1970s, algorithms such as\nCondensed Nearest Neighbor (CNN), Edited Nearest Neighbor (ENN), and\nmany others were developed. The benchmarks of instance selection indicate\nthe Drop3 [3] and ICF [4] algorithms as the most wildly used, which, de-\nspite not being new, are characterized by excellent properties in terms of the\nbalance between the prediction accuracy of the kNN algorithm and the re-\nduction of the size of the stored set of prototypes (reduction_rate) [5]. These\nalgorithms are also applicable not only as elements of the learning process\nfor the kNN algorithm (prototype selection as part of the learning process)\nand hence could also be used as universal algorithms for reducing the size of\nthe training set for any classifier, thereby accelerating the learning process\nof complex predictive models, the process of finding optimal parameters, etc.\nExamples of such applications can be found in [6, 7] or in [5].\nAll algorithms belonging to the Instance Selection group share one com-\nmon element, namely, they explore the structure of the nearest neighbors\ngraph by determining connections between the k nearest samples and their\ninterdependencies to identify vectors that are significant for the decision\nboundary.\nThis article is also concerned with the challenge of building an instance se-\nlection system, however with a different approach by converting the instance\nselection process into the binary classification problem. Instance selection\nalgorithms typically rely on the analysis of a nearest-neighbor graph. In the\nnearest neighbor graph, the vertices correspond to individual instances of the\ntraining set, and the edges contain information about the nearest neighbors\nof a given vector. Instance selection algorithms iteratively, often repeatedly,\ntraverse the graph's vertices, allowing to identify whether a particular vec-\ntor can be important in the learning process or not. But it is important to\nnote, that the characteristics of the nearest neighbors graph (NNG) are com-\nmon and do not depend on the domain or problem described by the dataset."}, {"title": "", "content": "Therefore, by extracting features that describe the vertices in the NNG the\nproblem of instance selection can be converted into a new meta-dataset in\na fixed-sized feature space (meta-features), that is common to all possible\ndatasets. Now, when taking all previously conducted results of any instance\nselection algorithm on any given dataset the binary information of keeping\nor rejecting an instance can be used as instance labels, leading to a meta-\ntanning set. This training set can then be used to train a classifier, which\nwill be called a meta-classifier. In prediction mode, a given input dataset for\nwhich the instance selection would be performed is represented as an NGG.\nThen meta-features are extracted and used as input to the meta-classifier\nwhich returns a probability indicating the importance of a given instance.\nThis allows us to select instances without running the original instance selec-\ntion algorithm. To maintain satisfactory generalization of the meta-classier,\nit is significant to create a meta-data set that combines meta-data describing\nthe NNG extracted not from one but multiple datasets, and concatenating\nall these datasets together.\nThe conducted analyses show that contrasted to traditional methods of\ninstance selection, for some of them, meta-selection methods allow for better\nresults than the original methods. Moreover, these results are achieved much\nfaster, as the selection process is carried out in a single pass, unlike in instance\nselection methods, where selection requires iteratively traversing the NNG.\nAn additional advantage of the developed solution is the description of\nthe training samples' importance as a probability density distribution, indi-\ncating how likely a given training vector should be included in the training\nset without affecting the performance of the classifier or in other words how\nsignificant a given vector is in the learning process. The use of a thresholding\nfunction allows for determining the number of important samples as a post-\nprocessing step adjusting the desired value to the needs of a given compute\nresources.\nThe contribution of this article:\n\u2022 A new perspective on the data selection process as a classification prob-\nlem is presented. In the proposed solution a meta-classifier makes de-\ncisions about the importance or non-importance of a given training\nsample.\n\u2022 The development of meta-attributes describing the nearest neighbor\ngraph, which also serve as input to the meta-classifier."}, {"title": "2. Related Work", "content": "In this article, we focus on and combine two areas of machine learning.\nOn the one hand, the article focusses on the problem of training samples\nselection, in particular on instance selection problem. On the other hand,\nwe modify the concept of meta-learning to develop a meta-classifier used for\ndata pruning.\nIn data-centric AI, considerable emphasis is placed on assessing data qual-\nity and examining the impact of data on the learning process [8, 9]. A current\noverview of the state of knowledge in data-centric AI can be found in [10] and\n[11]. According to the proposed taxonomy, data-centric AI can be divided\ninto training data development, inference data development, and data main-\ntenance. Within training data development, an important area is the issue\nof dataset size reduction, where the goal is to prepare data so that the model\ncan achieve maximum predictive capabilities by removing noisy instances\nand eliminating redundant data samples that does not impact the quality of\npredictive models. This cleaning process should aim to ensure maximum pre-\ndiction accuracy. For example in [1], the authors discuss scaling laws but note\nthat better results can be achieved by appropriately cleaning the dataset. As\nnoted in the introduction, the problem of selecting training data is not new,\nand over the years, many algorithms have been developed, including instance\nselection methods.\nThe instance selection process was mainly developed for the nearest neigh-\nbor classifier but as shown in [6, 5] it can be used also for other classifiers.\nInstance selection methods focus on removing noisy samples influencing pre-\ndiction accuracy, and removing redundant samples so that only the minimal"}, {"title": "", "content": "subset of samples assuring high prediction accuracy are stored as the final\ndataset. For many years since 1960 many methods have been developed uti-\nlizing many approaches such as focusing on condensation for example as in\nCNN [12] or noise removal such as in ENN [13]. Since this early work, many\nnew methods have been developed. An overview of these methods can be\nfound in [14] or in [15]. This comparison points to Drop3 [3], ICF [4], HMN-EI\n[16], and RMHC algorithms (although the last one is a wrapper method that\nrequires an internal classification model). A more recent summary is avail-\nable in [17], where the authors compared thirteen different algorithms using\ntext classification as a case study. This research points to algorithms based\non local sets such as LSSm[18] and LSBo[18] which were characterized by\nvery high classification accuracy at the cost of size reduction. But also many\nother algorithms not covered in this comparison were developed. For exam-\nple in [17] a method based on ranking has been developed where a special\nscore is used to rank samples. Other modern approaches use the so-called\nglobal density-based approach for instance selection. In [19] the authors\nproposed two algorithms based on this approach called global density-based\ninstance selection (GDIS) and its enhanced version called EGDIS. The GDIS\nalgorithm applies a function called relevance function to asses the samples'\nimportance that is based on the purity of k nearest samples and its enhanced\nversion utilized the irrelevance function to improve size reduction. In [20] en-\nsembles of instance selection were proposed where instead of a single method\na collection of instance selection methods are evaluated simultaneously and\nthen the obtained results are combined into a single solution. In [21] the\nconcept of boosting was applied to improve the quality of selected instances.\nA very popular approach to instance selection utilizes meta-heuristics such as\nin [22] but these methods suffer when the dataset size increases. To overcome\nthis limitation in [23] a solution was proposed such that the dataset is decom-\nposed into subsets using the conditional clustering method and then within\neach subset an independent instance of genetic algorithm is used, although\nthis approach was designed to address the regression problem only.\nIn parallel to the methods related to data-centric AI, meta-learning meth-\nods are being developed. These methods focus on utilizing historical knowl-\nedge gained from training models on some other datasets, and use this knowl-\nedge to build new predictive models. This enables the creation of models for\nnew datasets faster and with improved predictive accuracy. Additionally,\nthese solutions allow for the identification of models within a set limited\ntime budget to achieve the most efficient model [24, 25]. In meta-model con-"}, {"title": "3. Meta-Classifier-Based Instance Selection", "content": "The proposed algorithm's basic idea is to transform the problem of in-\nstance selection into a classification task, where each sample is classified as\n\"to keep\" or \"to remove\" as presented in Figure 1. To perform such a task,\na classifier and a meta-feature space or instance embeddings are needed in\nwhich the classifier will operate. At the current stage, the meta-space is\ndeveloped by manually extracting descriptors of the nearest-neighbor graph\n(NNG), and the meta-classifier is trained on a dataset obtained by combining\nN independent datasets $D_i$ each transformed to the meta-space and labeled\nby the results of the instance selection performed on each of these datasets\n$D$. The basic idea of meta-space extraction is shown in Figure 2.\nSince each of the datasets $D_i$ is independent each has an independent\nnumber of samples $n_i$ so that we can describe it as: $D_i = \\{(x_1, y_1), (x_2,y_2), ..., (x_{n_i}, y_{n_i})\\}$,\nwhere each pair $(x, y)$, consists of $m_i$ element input vector (i is specific to a"}, {"title": "", "content": "given dataset) $x \\in R^{m_i}$, and $y \\in \\{s_1,..., l_{c_i} \\}$, denotes the label s of the given\nvector.\nNow annotating $G_i$ as a nearest neighbor graph obtained be $G_i = NNG(D_i)$,\nwhere the vertices represent individual vectors $x$, and the edges describe\nthe distances to the nearest neighbors within the dataset $D_i$. The function\n$Meta(G)$ extracts features that describe the properties of the vertices in the\ngraph, converting each vertex into a vector $x^{\\{meta\\}} \\in R^{n^{\\{meta\\}}}$. By converting\neach vertex in the NNG, a single meta-dataset is obtained $D^{\\{meta\\}}_i$. Hence,\nit can written as $D^{\\{meta\\}} = Meta(NNG(D_i))$, where both $D_i$ and $D^{\\{meta\\}}_i$\ncontain the same number of samples. Note that in the meta-set $D^{\\{meta\\}}_i$,\nthe number of features $m^{\\{meta\\}}$ is fixed and doesn't depend on $i$. Finally,\nby adopting the notation $IS(\\cdot)$ as the function performing instance selection,\n$D = IS(D_i)$ the result is a new dataset containing only those samples which\nare important to the training process, such that $n_i > n'$ and often $n_i >> n'$\nand the feature space remain unchanged. Then, $D' = D_i/D$ contains the\nirrelevant training samples.\nBy labeling samples from the meta-set $D^{\\{meta\\}}_i$ as\n$\\forall (x^{\\{meta\\}} \\in D^{\\{meta\\}})$ if $x^{\\{meta\\}} \\in D'$\nthen $y^{\\{meta\\}} = 1$\nelse $y^{\\{meta\\}} = 0$\nwe get a binary classification problem. To train the meta-classifier multiple\ndatasets are combined to achieve proper generalization. More formally each\nof the data sets $D_i$ undergoes the same transformation procedure transform-"}, {"title": "", "content": "ing it to $D^{\\{meta\\}}_i$, and then each of the N meta-data sets $D^{\\{meta\\}}_i$ is merged\n$D^{\\{meta\\}} = \\bigcup_{i=1}^{N} D^{\\{meta\\}}_i$ into one large meta-data set, on which the meta-model\nwill be trained. The entire procedure of preparing and training the meta-\nclassifier may be written as follows:\n1. For each data set $D_i$, determine the NNG $G_i$\n2. From each $G_i$ determine the meta-set $D^{\\{meta\\}} = Meta(G_i)$ such that\n$D_i \\rightarrow D^{\\{meta\\}}$\n3. For each dataset $D_i$, perform the selection process of vectors $D' =$\n$IS(D_i)$ and determine the labels for the meta-dataset $D^{\\{meta\\}}$\n4. Normalize each of the meta-datasets $Standarize(D^{\\{meta\\}})$\n5. Combine all meta-datasets into one large training meta-dataset\n6. Use $D^{\\{meta\\}}$ to train meta-classifier\nThe procedure for applying the meta-classifier is similar and can be described\nas:\n1. For any data set where instance selection $D_{Te}$ is considered to perform,\nthe dataset should be converted to a meta-set $D^{\\{meta\\}}_{Te} = Meta(NNG(D_{Te}))$\n2. Make a prediction using the meta-classifier of samples from $D^{\\{meta\\}}_{Te}$\nand as prediction results return the probability of belonging to the\nclass \"to be removed\" marked as $p = \\{p_1,p_2,..., p_{n_{Te}}\\}$\n3. Define the threshold $\u0398$ and perform the evaluation assuming the sam-\nples labeled as \"to be deleted\" as the samples for which $p_j < \u0398$, where\n$p_j$ is the probability that the j'th vector of the dataset $D_{Te}$ is to\nbe deleted\n4. Remove unnecessary samples from $D_{Te}$, that is those marked as \"to be\nremoved\"\nBoth procedures are presented graphically in Fig. 3. The green color indi-\ncates the procedure for creating the meta-training set, the blue color indicates\nthe procedure for training the meta-classifier, and the orange color indicates\nthe procedure for applying the meta instance selection process. Note that\nthe training process is executed ones."}, {"title": "3.1. Meta-Descriptors of the Nearest Neighbor Graph", "content": "The key elements of the algorithm are meta-descriptors or meta-features\nthat describe the local properties of each node of the nearest neighbor graph."}, {"title": "3.2. Computational Complexity Analysis", "content": "Computational complexity can be considered from the perspective of the\ntraining process and the application of the developed solution.\nThe main factor and advantage of the developed solution is the issue\nof applying the meta instance selection. It consists of two stages. First\nstage is the procedure of extracting meta-attributes from a new data set\n$D_{Te}$ and the second stage is the procedure of applying a meta-classifier.\nThe procedure of extracting meta-attributes depends on the time necessary\nto create the nearest neighbor graph, which in the worst case requires a\ncomputational complexity of $O(n^2)$ (calculating distance between all samples\npairs in the dataset $D_{Te}$ for which instance selection is deliberated to be\nperformed, $|D_{Te}| = n$). However, using various techniques, e.g. Ball Tree [34]\nor KDTree [35, 36] algorithms, this complexity can be reduced to $O(nlog(n))$\nor using local sensitive hashing [37, 38] methods, it is possible to obtain semi-\nlinear complexity. The selection of the method should depend on the number\nof features in the original data set.\nThe second stage is the prediction time of the meta-classifier. In this\npaper, it was decided to use algorithms from the Random Forest family,\nwhose prediction time is $Klog(n^*)$, assuming that the forest consists of K\ntrees and the training set of the meta-classifier has $n^*$ training samples $n^* =$\n$\\sum_{i}^{N} n_i$ (note that the computational complexity depends on the size of the\ntraining meta-data set, and not on the size of the dataset for which instance"}, {"title": "", "content": "selection is deliberated to be performed). In practice, the maximum depth\nof the tree is often limited to some fixed number for example to $d = 10$, then\nthe complexity is reduced to $Klog(10)$, which can be considered constant\nfrom the perspective of the size of the data set $D_{Te}$. The above indicates\nthat, assuming a constant computational complexity of the classifier, the only\ncomputational cost depending on n is determining the nearest neighbors for\neach of the training samples, which can be limited to log-linear complexity.\nImportantly, in comparison to classical instance selection algorithms, the\nselection procedure is no longer an iterative process but consists in a single\npass over the samples from $D_{Te}$ and a single decision on the importance of\na given vector.\nAn independent element of computational complexity is the time required\nto prepare and train the meta-classifier. Similarly to the application process,\nthat time depends on the process of extracting meta-descriptors of the nearest\nneighbour graph and on model training. It is worth underlining that that\nthe meta-classifier training procedure consists of combining the number of\nsamples of each of the N data sets $D_i$. As a result, the effective size of\nthe training set $D^{\\{meta\\}}$ consists of $n^* = n_1 + n_2 + ...n_y$ training samples.\nThis causes the effective training time to increase radically. Therefore, it\nwas assumed in the studies that the meta-model must be an algorithm that\nensures high scalability and low computational complexity of the training\nprocess. Such an algorithm is the Random Forest classifier. Additionally\nit ensures ease of handling imbalanced data by appropriately balancing the\nsize of the sample on which individual trees are trained. As a result, the\nwhole process comes down to complexity $O(N \\cdot n^* log(n^*) + Kn^*log(n^*))$,\nand the process of meta-descriptor extraction can be easily parallelized by\nperforming the extraction for a single set independently of the other sets in\nseparate processes. Similarly, the Random Forest training process can be\neasily parallelized, since the individual trees are independent."}, {"title": "4. System and Experimental Design", "content": "In order to verify the correctness of the developed algorithm, experi-\nments were conducted experiments comparing the results of several reference\ninstance selection algorithms with the developed solution. The details of the\nconducted experiments are discussed in the following subsection."}, {"title": "4.1. Reference methods", "content": "Since the proposed algorithm requires reference methods that will be\nused for labeling the samples in the meta-set five different reference instance\nselection algorithms were used in the experiments. These were namely ENN,\nDrop3, ICF, HMN-EI, and CCIS. These five methods were selected because\nthese are the most often methods used as a reference for all newly developed\nalgorithms. Below a synthetic description of these methods is provided:\nENN known as Edited Nearest Neighbor algorithm was developed by Wilson\nand presented in [13]. It directly addresses problem of regularization\nand noise removal. It starts by assuming that $D' = D$, and in sub-\nsequent iterations the algorithm runs over vectors from $D'$, checking\nwhether most of the nearest neighbors belong to the same category\nas the tested vector. Although it is a fairly simple and old algorithm\ndating back to 1972, it is commonly used as a component of other solu-\ntions, such as Drop3 or ICF as well as effectively applied in imbalanced\nlearning [39].\nDrop is a family of algorithms that is a continuation of the concept pro-\nposed in the RNN algorithm [40]. The Drop algorithm [41, 3] starts by\nassuming that initially the selected subset $D' = D$, and then each of\nthe instances from $D'$ is brought to see if its removal will not worsen the\nprediction accuracy of the kNN classifier. In order to speed up the algo-\nrithm, it was proposed to create a nearest neighbours graph storing the\nk+1 nearest neighbors samples including the so called enemies (samples\nwith opposite label) and vectors associated with a given training vec-\ntor (objects whose given vector is one of k nearest neighbors). Such a\nstructure significantly speeds up the evaluation of individual instances.\nThe family starts with Drop1 algorithm which only evaluates individ-\nual instances to see if their removal would degrade the results. Drop2\nmodifies the Drop1 algorithm by the order of instance removal, so that\nbefore the main loop begins, the vectors from $D'$ are sorted in order\nfrom the largest to the smallest distance to the nearest enemy. This\nensures that vectors far from the decision boundary are removed first.\nAnother modification is the Drop3 algorithm, which starts by running\nthe ENN algorithm to remove outlier vectors and boundary samples,\nthese smoothes the decision boundary. Then, the procedure identical\nto Drop2 begins. The family also contains Drop4 and Drop5, but since"}, {"title": "", "content": "they are not used in the experiments the description would be omit-\nted. Apart from, according to many studies, the Drop3 outperforms\nthe other versions.\nICF stands for Iterative Case Filtering, is an algorithm similar to the Drop\nalgorithm with the difference that after analyzing individual instances,\nthe algorithm removes all vectors at once [4]. In this algorithm, the\nauthors define the so-called $LocalSet(x)$ as a set of vectors belonging to\na hypersphere whose center is the examined instance x, and the radius\nis limited by the nearest instance from the opposite class (excluding\nthis instance). On this basis, two sets are defined, named $Caverage(t)$\nand $Reachable(t)$, respectively, as:\n$Caverage(t) = \\{x : x \\in D \\wedge x \\in LocalSet(t)\\}$\n$Reachable(t) = \\{x : x \\in D > t \\in LocalSet(x)\\}$ (1)\nwhere $Caverage(t)$ is the set of instances belonging to the set $LocalSet$,\nwhose center is t, or in other words $Caverage(t)$ is a list of nearest\nneighbors from the same class that t belongs to, and $Reachable(t)$ is\nthe set of instances from D for which t belongs to the $LocalSet$ of these\ninstances, or equivalently, it is a list of vectors associated with t.\n\u0397\u039c\u039d-\u0395\u0399 \u0397MN is a family of algorithms, consisting of three solutions called\nHMN-C, HMN-E and HMN-EI [16]. The whole is based on Hit Miss\nNetwork (HMN), which is a modification of the nearest neighbor graph.\nIn HMN, from each vertex there are c edges to the nearest neighbors\nfrom each of the c classes (one nearest neighbor to each category). At\nthe same time, HMN has a counter at each node counting the number\nof hits and misses, where hits represent the associated vectors for which\nthe class label of the nearest neighbor $y_{nn} = y_x$ ($y_x$the label of vector\nx), and misses are the scenario in which the nearest neighbor label\n$y_{nn} \\neq y_x$. The individual algorithms HMN-C and HMN-E differ in the\nrules used to make the decision for removing or leaving instances. In\nHMN-E four instead of 1 rule used in HMN-C decide whether to remove\nor leave an instance, the first of which is responsible for removing, and\nthe others for restoring individual instances. The HMN-EI algorithm,\nruns the HMN-E procedure until the accuracy of the 1-NN classifier no\nlonger deteriorates."}, {"title": "4.2. The Procedure of Assessing the Quality of Instance Selection", "content": "The development of tests for the meta-instance selection system required\nthe construction of two sub-processes. One in which the meta-classifier is\ntrained and the other in which it is evaluated on a dataset that has not been\nused to train the meta-classifier. Having in total N data sets, a leave one out\ntest was applied at the datasets level (leave one dataset out). These tests\nconsisted of excluding one of the N available datasets and using N-1 datasets\nin the process of preparing the meta-classifier, then an instance selection\nprocess was carried out on previously excluded dataset.\nDuring training of the meta-classifier used in meta-instance selection for\neach of the data sets, firstly the instance selection process was performed\nusing the selected reference algorithm described in the previous section. Sec-\nondly, in accordance with the described above procedure, the meta-classifier\nwas trained separately for each of the reference algorithms."}, {"title": "4.3. Datasets Used in the Experiments", "content": "The experiments were performed on data sets available in the Keel project\nrepository [43]. Within this repository, the authors provided data sets of\nvarious sizes, from which data sets larger than 2000 samples were selected.\nAdditionally some datasets were obtained from the OpenML project [46].\nThe full characteristics of the datasets are presented in Table 1. There is\nneed to highlight that these data sets are available in the Keel repository\nwith a ready division into cross-validation folds. The division provided in\nthe repository the authors used in the experiments."}, {"title": "4.4. Performance Metrics", "content": "To assess the quality of the final classifier, the classical F1 score and the re-\nduction rate were used. Where the reduction rate is defined as: $reduction\\_rate =$"}, {"title": "5. Results", "content": "The experiments were divided into 3 parts. The first part compared the\nresults obtained by the meta instance selection with reference models. Then,\nthe time required for the instance selection obtained by the reference algo-\nrithm and meta instance selection was compared and analyzed. Eventually\nthe last part shows the influence of model selection on the meta-classifier and\nits stability."}, {"title": "5.1. Performance Comparison", "content": "Visualization of the performance metrics in particular prediction accu-\nracy and reduction rate are presented in figure 5. In that figures X indicates\ngiven reference method and a curve of the same color corresponds to given\nmeta equivalent to given method. Therefore, in order to properly analyze\nthese figures the results should be analyzed by color comparing X with a\ncurve of the same color. The results are grouped by dataset. This allows\nto compare not only the relation between compression and prediction per-\nformance but also to identify the best instance selection method for a given\ndataset. Summarization of these figures by calculating the area under the\naccuracy \u2013 reduction_rate curve is shown in Tables 2 and 3.\nA comparison of the results shown in the figures indicates two groups of\nmethods, the first which includes ENN and HMN-EI and the second which\ncontains the three remaining methods (Drop3, ICF, and CCIS). The main\ndifference between these methods appears in compression. The first group\nhas significantly lower reduction rate in comparison to the second group.\nTherefore these two groups will be analyzed separately.\nWithin the first group, in all of the cases, there were no differences be-\ntween the reference ENN and HMN-EI methods marked blue and red respec-\ntively with their meta equivalents (Meta-ENN and Meta-HMN-EI). Moreover\nin almost all of the cases, the use of Meta-ENN and meta-HMN-EI allowed\nus to achieve significantly higher compression almost without any loss in\nperformance, and even with a little gain. Additionally, the shape of the\naccuracy reduction_rate plot for the meta-instance selection is often flat\nindicating that even higher compression is possible. The only exception is\nthe Ring data set where Meta-HMN-EI achieved worse accuracy than the\nreference HMN-EI, but for ENN the accuracy was even higher.\nWithin the second group, the accuracy reduction rate curve for the\nmeta-instance selection methods usually indicates a sudden drop after reach-\ning a certain reduction_rate. Before that threshold, the curve is usually flat.\nAs a result, after a certain reduction_rate the prediction accuracy starts to\ndecrease rapidly. In those scenarios, the reference instance selection methods\noften allow achieving higher prediction accuracy for its own reduction_rate,\nalthough for a little less reduction_rate the meta-instance selection methods\nallow to achieve higher prediction accuracy. That phenomenon appears in\nLetter, Nursery, Optidigits, Page-blocks, Penbased, Ring, Shuttle Texture,\nand Twonorm datasets."}, {"title": "5.2. Execution Time", "content": "The execution time is the utmost benefit of the proposed method. To\nindicate the benefits and speed up of the meta instance selection over the\nreference methods, the execution time of the experiments discussed in the\nprevious section were recorded. The execution time was measured only on\nthe instance selection process. For the reference instance selection methods,"}, {"title": "", "content": "the execution time was recorded using the build-in RapidMiner execution\ntime property of an operator. For the meta instance selection, two different\ntimes can be identified that is the training time and selection time. First,\nthe selection time would be analyzed. The obtained results are presented\nin Table 5, and the speed up is shown in Table 4 and visualized in Figure\n6, although it is important to note that the execution time was measured\nin two different environments - in the case of reference instance selection\nthe algorithms were implemented in Java using the standard java structures\nand in terms of meta instance selection the algorithms were implemented in\nPython using numpy and scipy libraries."}, {"title": "", "content": "The analysis of the results presented in Table 4 and visualized in Figure\n6 indicates a significant speedup of the meta instance selection algorithm\nthat reaches 219 for the Drop3 algorithm. Moreover, when analyzing Table\n5 it can observed that the execution time of the meta instance selection\nis independent of the reference algorithm. It is fixed and depends on the\nmeta-feature extraction process and execution of the meta-classifier. This\nproperty is essential for the use of the proposed solution together with the\nvery complex and slow instance selection algorithms with high computational\ncomplexity."}, {"title": "5.3. Assessment of the Meta-Classifier Performance", "content": "A substantial factor influencing the quality of the meta instance selection\nis the evaluation and selection of an appropriate meta-classifier. In the con-\nducted studies, models based on the random forest algorithm were adopted\nas the meta-classifier, mainly due to its scalability as a function of the num-\nber of training vectors. However, the problem is the imbalanced distribution\nof class labels in the meta training set. It appears when the meta training\nset is labeled by the results of the reference instance selection method with a\nhigh value of reduction_rate. For example, if the reference instance selection"}, {"title": "5.4. Meta-Features Importance Analysis", "content": "The another important element influencing the quality of the obtained\nresults is the selection of the feature space, albeit there is the associated is-\nsue as individual meta-features are strongly correlated with each other. This\ncorrelation can be visible preeminently for the same function extracting a\nmeta-feature for different values of the k parameter. Hence in the process of\nanalyzing the significance of features, grouping features with similar proper-\nties and presenting their sum was performed.\nThe mean decrease in impurity (MDI) method [49] was used to assess the\nquality of attributes, which is naturally supported by tree-based algorithms\nsuch as Random Forest. The grouping of features was performed based on\ntwo aggregation methods. First the meta-features were grouped by the fea-\nture types and summed the MDI coefficients for different values of k and"}, {"title": "", "content": "the second aggregation method was based on the value of k, then the co-\nefficients for different feature types and the same value of k were summed.\nThe obtained results are presented graphically in Figure 7 and Figure 8,\nrespectively.\nThe obtained results indicate a lack of uniformity of results between the\nindividual meta-models. In terms of attribute grouping by feature types, it"}]}