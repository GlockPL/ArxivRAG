{"title": "A PERCOLATION MODEL OF EMERGENCE: ANALYZING TRANSFORMERS TRAINED ON A FORMAL LANGUAGE", "authors": ["Ekdeep Singh Lubana", "Kyogo Kawaguchi", "Robert P. Dick", "Hidenori Tanaka"], "abstract": "Increase in data, size, or compute can lead to sudden learning of specific capa- bilities by a neural network-a phenomenon often called \u201cemergence\u201d. Beyond scientific understanding, establishing the causal factors underlying such emergent capabilities is crucial to enable risk regulation frameworks for AI. In this work, we seek inspiration from study of emergent properties in other fields and propose a phenomenological definition for the concept in the context of neural networks. Our definition implicates the acquisition of specific structures underlying the data- generating process as a cause of sudden performance growth for specific, narrower tasks. We empirically investigate this definition by proposing an experimental system grounded in a context-sensitive formal language and find that Transform- ers trained to perform tasks on top of strings from this language indeed exhibit emergent capabilities. Specifically, we show that once the language's underlying grammar and context-sensitivity inducing structures are learned by the model, performance on narrower tasks suddenly begins to improve. We then analogize our network's learning dynamics with the process of percolation on a bipartite graph, establishing a formal phase transition model that predicts the shift in the point of emergence observed in experiment when changing the data structure. Overall, our experimental and theoretical frameworks yield a step towards better defining, characterizing, and predicting emergence in neural networks.", "sections": [{"title": "1 INTRODUCTION", "content": "Modern neural networks, e.g., large language models (LLMs) (Gemini Team, 2023; OpenAI, 2023; Anthropic, 2023; Touvron et al., 2023), exhibit a broad spectrum of capabilities, allowing them to serve as the \u201cfoundation\u201d for downstream, application-specific systems (Bommasani et al., 2022; Ahn et al., 2022; Driess et al., 2023; Schick et al., 2024). As these models scale, either via addition of more data, parameters, or compute, an intriguing behavior is at times observed: until a certain critical scale is reached, there are capabilities that the model does not exhibit; however, beyond this point, such capabilities can suddenly \u201cemerge\u201d (Wei et al., 2022; Srivastava et al., 2022; Brown et al., 2020; Yu et al., 2022; Steinhardt, 2023; Pan et al., 2022; Rae et al., 2021; Anil et al., 2023; Kirsch et al., 2022; He et al., 2024; Elhage et al., 2021). More specifically, the performance of the model on a task or benchmark meant to evaluate said capabilities can witness substantial growth in performance, even though the overall training loss undergoes minimal, if any, improvements (Arora & Goyal, 2023; Du et al., 2024). Empirical evidence in fact suggests that, at times, several capabilities can emerge simultaneously (Wei et al., 2022; Wei, 2022).\nBeyond developing a better scientific understanding of neural networks, understanding emergent capabilities is crucial to enable risk-centric regulation frameworks for AI, which assume a system's"}, {"title": "2 RELATED WORK", "content": "Explaining emergence. Focusing on the sudden learning characteristic of emergent capabilities, a few recent works have tried to explain the factors driving this phenomenon. For example, compositionality has been implicated for having a \u201cmultiplicative\" effect on a model's performance, where the argument is that a model cannot perform well on a compositional task until the abilities needed to perform individual tasks involved in that composition are acquired (Okawa et al., 2023; Arora & Goyal, 2023; Yu et al., 2023; Srivastava et al., 2022; Wei et al., 2022; Hoffmann et al., 2022; Gokhale, 2023); when they are acquired, performance suddenly grows. A few papers have also shown that learning of specific capabilities (i.e., ones not compositional in nature) can be sudden (Chen et al., 2024; Nam et al., 2024; Kirsch et al., 2022; He et al., 2024; Michaud et al., 2023). In contrast, Schaeffer et al. (2023) argue emergent scaling curves are a consequence of poorly defined, discontinuous evaluation metrics, and the seemingly sudden learning goes away once partial, continuous credit is given to the model. We emphasize that if the structure of a task is ignored, it is certainly easy to define arbitrary continuous metrics for a task; however, such metrics are unlikely to help measure progress toward learning a task. For example, consider the addition of two numbers, say 10 and 11, and the metric called token edit distance (Schaeffer et al., 2023) that assesses the average distance between digits in the model's output, denoted $xy$, from the ground truth, i.e., $(|x-2|+|y-1|)/2$. For both $xy = 22$ and $xy = 11$, this metric equals 1; however, clearly 22 is a better approximation for the ground truth (21). Thus, once we account for the structure of the task, i.e., the fact that error in the most significant digit should be penalized more than error in the least significant one, we see limitations in token edit distance as a metric for assessing a model's ability to add numbers. We argue claims relating emergence to sensitivity of metrics can be confounded by use of metrics that do not respect the structure of the task.\nGrokking vs. Emergence. We focus on the effect of data scaling on a model's capabilities; often called 'learning curve' or 'data scaling' analysis (Viering & Loog, 2022; Blumer et al., 1989; Bousquet et al., 2021; Seung et al., 1992; Watkin et al., 1993; Amari, 1993; Haussler et al., 1994). On surface, this might look similar to the seemingly related phenomenon of grokking (Power et al., 2022; Liu et al., 2023b; \u017dunkovi\u010d & Ilievski, 2022; Murty et al., 2023; Barak et al., 2022; Edelman et al., 2023; Nanda et al., 2022), wherein a model's performance on a task rapidly improves long after it has fit the training data. However, we emphasize that we focus on an online learning setting in our experiments, i.e., a given sample is unlikely to be seen multiple times during training. Emergence is generally studied in such online learning scenarios. Since there is no distinction between train versus test data in such a setting, we argue mechanistic explanations of grokking identified in past work (Nanda et al., 2023; Liu et al., 2022b) are unlikely to help explain our results of emergence under data scaling."}, {"title": "3 A PHENOMENOLOGICAL DEFINITION OF EMERGENCE", "content": "To analyze emergence, we first establish what we mean by the term for the purpose of this work. Specifically, we define emergence in a phenomenological manner, i.e., by assembling the characteristic properties associated with scaling curves claimed to depict emergent learning. We emphasize our definition is merely a definition for emergence, and does not necessarily represent all possible perspectives (Luccioni & Rogers, 2023). For example, often model capabilities that arise despite any explicit supervision are called emergent in self-supervised learning (Caron, 2021; Caron et al., 2021; Ziyin et al., 2022). As our goal is to analyze the effects of scaling, regardless of supervision protocol used, we do not try to capture this property.\nDefinition 1. (Emergence of a capability.) We say a capability C is emergent with scaling along a relevant axis (e.g., amount of data, compute, parameters) if:\n\u2022 P1: nonlinear improvement occurs in the performance of a task where C is required;\n\u2022 P2: multiple tasks simultaneously show nonlinear performance improvement; and\n\u2022 P3: the model undergoes a structural change that is instrumental to learning the capability C, and nonlinear progress in C's learning directly correlates with the learning of said structure.\nThe definition above assigns a broader meaning to emergence than mere sudden performance improve- ment on a narrow task: it argues there should be precise structural changes in the model that have downstream effects on several capabilities, hence leading to sudden improvements in performance on several tasks. Note that we intentionally leave the notion of 'structure' informal in the definition. The salient property of a structure is that if a model learns it, downstream tasks should become easier to perform. For example, a fine-grained notion of a structure can be previous token and copy attention heads that lead to in-context learning (Reddy, 2023; Edelman et al., 2024; Olsson et al., 2022); a more coarse-grained structure can include the model learning the syntactical rules of a language that help it with generation of coherent language and hence with any task where coherence is important (Chen et al., 2024). In this sense, what is emergent is a structure, and what is observed is a change in the model's capabilities. Hypothesizing what this structure is by identifying shared characteristics of a set of tasks that simultaneously show sudden learning, one can develop an evaluation meant to precisely gauge learning of the corresponding structure and hence infer at what point an independent training run will show sudden improvements.\nWe note the intuition for Defn. 1 comes from prior work in the fields of complex systems and physics (Anderson, 1972; Newman et al., 2001; Newman, 2003), from where the term has sought its inspiration in recent machine learning literature (Steinhardt, 2023; Wei et al., 2022). Therein, emergence describes the scenario where rapid changes occur in a system's properties as some control parameter is varied. A range where the system's properties change relatively smoothly is called a phase, and a change of phase with a change in the control variable is called a phase transition. A crucial step in studying emergence in physics is identifying an order parameter\u2014a measure that captures the formation of some specific structure in the system such that the development of this structure is what alters the system's properties and drives a phase transition. For example, in Fig. 1a, a system of particles transitions through phases (solid, liquid, gas) as the temperature is changed; the formation of a crystalline structure with the decrease in temperature can be identified by analyzing the bond-orientation order parameter, while the liquid-to-gas transition can be described by a jump in particle density. We argue that we must similarly define order parameters for studying emergence in neural networks as well, i.e., we must develop evaluation measures that are focused towards detecting the learning of specific, narrow structures that are generally of use to several downstream capabilities."}, {"title": "4 FORMAL LANGUAGES AS AN EXPERIMENTAL SYSTEM FOR EMERGENCE", "content": "Having established our perspective on emergence, we now define a toy experimental system that allows us to precisely study the concept in a controlled setting. We note that our focus will be on emergence under data scaling in an online learning scenario (i.e., a sample is unlikely to be seen multiple times). To this end, we follow recent work on understanding language modeling and use formal languages to define our experimental setup (Allen-Zhu & Li, 2023b; Jain et al., 2023; Allen-Zhu & Li, 2023a; Valvoda et al., 2022; Liu et al., 2023a; 2022a). As discussed in detail next, the formal language we use in this work is (minimally) context-sensitive, with underlying syntactical"}, {"title": "5 LEARNING TASKS AND EXPERIMENTAL SETUP", "content": "Having described our language L, now we briefly discuss our experimental setup (see App. C for details). We train a GPT architecture model (Andrej Karpathy, 2023) with the stan- dard autoregressive language modeling objec- tive. Data is sampled \u201conline\u201d, i.e., we sample a fresh batch of strings every iteration from L. Unless mentioned otherwise, L is constituted of E = 900 entities and $|K|$ = 18000 properties, equally and disjointly distributed over |C| = 10 classes, and with edges connecting entities to p = 0.1 fraction valid properties of a class in a uniformly random manner; results ablating these settings are in App. D. Before being fed into the model for training or evaluation, strings sampled from the language are restructured into a format that enables the specification of particular tasks (see Fig. 3). Specifically, we train the model to learn the following tasks with 80/10/10% splits.\n\u2022 Free generation: Produce a valid string, i.e., one that respects the grammar and type constraints.\n\u2022 Unscrambling: A string is sampled from L and randomly permuted; the model is expected to unscramble it. This task is known to show sudden learning in LLMs (Wei et al., 2022).\n\u2022 Conditional Generation: A set of tokens corresponding to entities or properties are shown to the model, which is expected to generate a string combining these tokens in a valid manner.\nEvaluation Protocols. Given an input x, which may correspond to any of the three tasks above, denote the model output as f(x). Let d(.) be an indicator variable that evaluates to 1 if its input is true. We track several metrics throughout training, as described below. We often decompose these evaluations according to strings of two types: (i) descriptive, i.e., that describes that an entity possesses a descriptive property, and (ii) relative, i.e., that claims a subject, object, and verb can match each other to create a valid sentence. Unless noted otherwise, results are averaged over 3 seeds.\n\u2022 Grammaticality/Type Check. Used for evaluating free generation. Grammaticality involves checking whether model output follows the underlying grammar G, i.e., f(x) \u2208 \u03a3. Type checks involve first extracting subjects, objects, and properties from the sentence and then evaluating whether this set of tokens is allowed in the context of each other. We decompose type checks as descriptive (do entities and descriptors match), relative (do subject, object, and verb match), and all (product of all constraints, including adjectives and adverbs)."}, {"title": "6 RESULTS: EMERGENT CAPABILITIES IN FORMAL LANGUAGE LEARNING", "content": "We now evaluate (i) whether our setup demonstrates emergence (see Def. 1), and (ii) whether we can extract insights into the mechanisms of what leads to emergence. In the following, we often use the terms \"phase\" and \"phase change\"; see discussion around Def. 1 for context on these terms.\n6.1 PHASES OF LANGUAGE AND CAPABILITIES ACQUISITION\nWe plot the model's performance as a function of training iterations. Since we are in an online learning, constant learning rate setting, this analysis corresponds to studying the effects of data scaling. Results are reported in Fig. 4 and show there are three phases to the learning dynamics.\nPhase 1: Grammar acquisition. We find the model first learns to produce grammatically correct sentences, as measured by the grammaticality measure defined in Sec. 5. This process is relatively rapid, as we see the model starts generating grammatically accurate sentences in a short period of approximately 100 iterations; attention heads also rapidly evolve and reflect the parse structure of a sentence (see App. D.6) In this regime, however, the narrower tasks of unscrambling and conditional generation exhibit poor performance. However, precisely when grammaticality improves, we find that per-token accuracy starts to improve. This indicates that the model learning a broad structure underlying the data (i.e., grammar) has an impact on the learning of other capabilities.\nPhase 2: Acquisition of relative type constraints. At around 1000 iterations, we find there is a sudden increase in the model's performance on relative types from essentially zero to perfect accuracy; precisely at this point, we find the loss for all tasks, especially free generation, show a sudden drop. Interestingly, we find this sudden improvement occurs precisely at the point where the model reaches its maximum performance on grammaticality for the first time. That is, as soon as the first structure underlying the data is acquired, the model rapidly learns the next relevant structure of relative type constraints. Improvement occurs in descriptive constraints as well (and hence the overall Type Check performance), but hovers around slightly above random performance of 0.1. With |C| classes, if a model produces grammatically correct sentences, it will achieve a random performance of 1/C on descriptive type checks. This however also implies that the model is primarily relying on its syntactical knowledge and does not respect descriptive type constraints much.\nDuring this phase, we see that shortly after the phase change, there is a sudden increase in performance for both unscrambling and conditional generation, across all metrics. These tasks' losses also show another loss drop occurs at this point; though the drop seems smoother in the total loss, likely due to averaging effects (Michaud et al., 2023). As shown in Fig. 4e, we find that this performance improvement is driven by sentences that require primarily correctness of grammar and relative type"}, {"title": "7 A PERCOLATION MODEL OF EMERGENCE", "content": "We next propose a framework for modeling the emergence of capabilities that require a model to compose unseen entities and descriptive properties, e.g., learning descriptive type constraints, which, beyond allowing a model to produce accurate free generations, will aid with narrower tasks like conditional generation and unscrambling. We argue the relevant structure to analyze for this purpose is the concept class: if a model understands what entities and properties belong to the same concept class, regardless of whether they have been seen together in a sentence, it will deem their co-occurrence to be valid. We thus develop an abstraction for concept classes as bipartite graphs and cast their learning as a problem of percolation on a bipartite graph.\n7.1 MATRIX REPRESENTATION OF DATA AND LEARNING COMPOSITIONS\nRecall that a concept class is defined as a set of entities that are expected to have shared properties (see Def. 3). The question is whether upon sub-sampling pairs of entities and properties from a concept class, can the model learn that, in fact, all pairs of entities and properties are valid and compose the concept class. For instance, in the case of a concept class such as human, the set of entities can include humans with different genders (e.g., man) as well as human- associated entities such as a lawyer (see Fig. 2). The corresponding properties for the human concept class will be, for example, walk, jump, tall. A man, being human, is expected to have all these properties, al- though strings specifying these properties for a lawyer may be rare or even absent in the training data. We are interested in the case where the data, such as strings, in- cludes examples of these pairs of entities and properties. We can represent this by a matrix whose rows and columns represent the entities and the properties, and the ma- trix values indicate the quantity or density of data available for each composition, such as an entity-descriptor pairing. We call this matrix the concept density matrix.\nDefinition 4. (Concept Density Matrix.) Let D be an $|E| \\times |K|$ matrix with real-valued entries between 0 and 1, inclusive. Each entry $D_{ek}$ represents the density for the entity and property pair (e,k) (e.g., the amount of data that represents the specific composition), where e \u2208 {1, ..., |E|} and k \u2208 {1, ..., |K|} are the indices of the entities and properties, respectively.\nFor example, consider the case where there are three values of entities and properties ($|E| = |K| = 3$), with entities (rows) being {Man, Lawyer, Telephone}, and properties (columns) being {Walk, Stoic, Ring}. The corresponding D can be:\n$D=\\begin{pmatrix} 1 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \\ \\end{pmatrix}$        (1)\nA common composition such as Man walking will lead to a value of 1 at the intersection of Man and Walk, i.e., $D_{00} = 1$, where $D_{ij}$ denotes element at row-i and column-j. Conversely, a highly unlikely composition like Lawyer ringing will be absent in the dataset, and will be represented by a zero at the respective matrix position, i.e., $D_{12} = 0$. We can also assume for example that Man ringing or a Telephone walking are rare, which yields $D_{13} = D_{31} = 0$.\n7.2 PERCOLATION TRANSITION ON DESCRIPTIVE CONSTRAINTS\nUsing the bipartite graph framework, the generalization, or the learning of the concept class, can be defined as the situation where a large cluster of entity-property connected pairs arises despite the sparse concept density matrix. A critical aspect to examine is the proportion of the inference matrix values where $T^{(\\infty)}_{ek}$ is non-zero, out of the total possible pairs $|E| \\times |K|$. This particular scenario aligns with the bond percolation problem on a bipartite graph. In bond percolation, we investigate how the largest connected cluster's size varies with the probability p of each edge (bond) being present. In a typical setting, there exists a critical threshold value, p = pc, called the percolation threshold. Below this threshold (p < pc), the graph typically exhibits a disconnected phase characterized by the absence of extensively connected clusters, with most nodes either isolated or part of smaller clusters. Above this threshold (p > pc), the graph transitions to a connected phase, significantly increasing the likelihood of a vast connected component spanning a large portion of the graph. This shift from a predominantly disconnected state to one with a macroscopic cluster is a defining characteristic of the percolation process, and this transition sharpens as the number of components in the system increases.\nIn a simple percolation scenario, where connecting edges are selected randomly on the graph with probability p, the percolation threshold is obtained as $p_c \\sim \\sqrt{1/|E||K|}$ for large |E| and |K| (Newman et al., 2001), which means that when around $\\sqrt{|E||K|}$ edges are connected (out of the total |E||K|) there is a qualitative change in the growth of the cluster size. For p > pc, the number of nodes included in the connected cluster will become macroscopic, meaning that the probability that a randomly selected pair of an object and a feature is connected will be finite. We present in Appendix B the derivation of the percolation threshold for bipartite graphs that are uncorrelated, and how the cluster size (i.e., number of nodes in the largest connected graph) scales as $\\sim (p - p_c)^{\\beta}$ with $\\beta= 1$ for usual cases.\nWe posit that the percolation threshold corresponds to the point at which our model generalizes from the sparse learning of pairs to a complete representation of the concept classes. When the number of edges surpasses the threshold, the model can infer novel compositions, even for entity-feature pairs that were not explicitly present in the training data. The model should also start to be able to discriminate between distinct concept classes beyond this threshold; in the community detection problem, for example in the stochastic block model (Abbe, 2018; Decelle et al., 2011), the detection threshold for the partitions have the same scaling as pc Florescu & Perkins (2016). Since increasing"}, {"title": "8 CONCLUSION", "content": "In this work, we take inspiration from other fields (e.g., physics and complex systems) and propose a phenomenological definition for emergence of capabilities in neural networks. Specifically, the definition argues that at the point of emergence, the model acquires broad structures which are instrumental to the learning of specific, narrower capabilities; acquisition of such structures then leads to sudden performance improvement on several tasks (often with some delay). While relatively informal, this definition brings the notion of emergence in the context of neural networks closer to its meaning in physics, wherein the acquisition of specific structures is known to drive phase changes that involve sudden changes in the system's properties. Characterizing these phase changes"}, {"title": "A DATA-GENERATING PROCESS: DEFINING OUR FORMAL LANGUAGE", "content": "Our data-generating process involves defining a formal language, sampling sentences from this language, and then defining tasks to be performed upon these sentences (specifically, free generation, unscrambling, or conditional generation). In this section, we discuss the precise details of how the language is implemented.\nA.1 DEFINING A GRAMMAR USING PCFGS\nTo define a grammar for our language, we use the framework of Probabilistic Context-Free Grammars (PCFGs). To keep the paper self-contained, we provide a short primer on PCFGs below and then discuss our precise version of it in detail. For a more thorough discussion on PCFGs, we refer the reader to one of the several well-written tutorials (Collins, 2013) and books (Sipser, 1996).\nA.1.1 SHORT PRIMER ON PCFGS\nBroadly, a PCFG is defined via a 5-tuple G = (\u039d, \u03a3, R, S, P), where:\n\u2022 NT is a finite set of non-terminal symbols.\n\u2022 T is a finite set of terminal symbols, disjoint from NT.\n\u2022 R is a finite set of production rules, each of the form $A \\rightarrow \\alpha\\beta$, where $A \\in N$ and $\\alpha, \\beta \\in (N\\cup T)^*$.\n\u2022 $S \\in N$ is the start symbol.\n\u2022 P is a function P : R \u2192 [0, 1], such that for each A\u2208\u039d\u03a4, $\\Sigma_{\\alpha:A\\rightarrow a\\beta \\in R}P(A \\rightarrow \\alpha\\beta) = 1$.\nTo generate a sentence from a PCFG, the following process is used. Pseudocode for this generation process is provided in Algo. 1.\n1. Start with a string consisting of the start symbol S.\n2. While the string contains non-terminal symbols, randomly select a non-terminal A from the string. Choose a production rule A \u2192 \u03b1\u03b2 from R according to the probability distribution P(A \u2192 \u03b1).\n3. Replace the chosen non-terminal A in the string with a, the right-hand side of the production rule.\n4. Repeat the production rule selection and expansion steps until the string contains only terminal symbols (i.e., no non-terminals remain).\n5. The resulting string, consisting entirely of terminal symbols, is a sentence sampled from the grammar."}, {"title": "8.1.2 INSTANTIATING THE GRAMMAR UNDERLYING OUR LANGUAGE", "content": "While generally one directly samples sentences from a grammar, in this work, we define a grammar that operates over symbols, i.e., whose terminals are variables that are not yet populated by any specific values from the language's vocabulary. We emphasize this is an unconventional manner for defining a PCFG, as one would generally use a standard vocabulary of the language to directly define terminal symbols. However, to enforce type constraints, we find this unconventional format aids in making the implementation easier. Specifically, one can simply sample an entirely symbolic sentence, and then enforce type constraints at the step when these symbols have to be populated.\nOverall, our grammar, denoted G, is defined using the following.\n\u2022 Terminal symbols: T = {Subj, Obj, Verb, Conj, 1Verb, Desc, eAdj, dAdj, Adv, Prep}.\nHere, Subj is a symbol for a subject, Obj for an object, Verb for verbs, Conj for conjunctions, 1Verb for a linking verb, Desc for descriptors, eAdj for adjectives used for entities, dAdj for adjectives used for descriptors, Adv for adverbs, and Prep for prepositions.\n\u2022 Non-terminal symbols: NT = {S, sNP, sT, oNP, oT, VP, vT, descT}.\nHere, S denotes the start symbol, sNP can be interpreted as a noun phrase with a subject in it, sT as the immediate ancestor of the subject symbol, oNP as a noun phrase with an object in it, oT as the immediate ancestor before the object symbol, VP as a verb phrase, vT as the immediate ancestor of the verb symbol, and descT as the immediate ancestor of a descriptor symbol.\n\u2022 Production rules R:\nS \u2192 sNP VP [1.0]\nsNP \u2192 sT [0.8] | sNP Conj sNP [0.2]\nVP \u2192 1Verb descT [0.4] | Verb Prep oNP [0.4] | VP Conj VP [0.2]\noNP \u2192 oT [0.7] | oT Conj oNP [0.3]\nsT \u2192 eAdj Subj [0.8] | Subj [0.2]\noT \u2192 eAdj Obj [0.8] | Obj [0.2]\ndescT \u2192 dAdj Desc [0.8] | Desc [0.2]\nNote that since non-terminals can appear on both left and right hand side of a rule, there is recursion possible in our grammar and hence sentences can get very long. We restrict sentence lengths to 75, yielding a language where sentence lengths vary from 4\u201375 tokens. Probability over rules was partially adapted from prior work by (Hupkes et al., 2020).\nGiven the above, we can now sample symbolic sentences such as Subj 1Verb Desc. We will populate these symbols with tokens from our vocabulary V. As noted above, while in general this population step would be performed as the final step of the grammar, to enforce type constraints and enable context-sensitivity, we separate it from the grammar."}, {"title": "8.2 TYPE CONSTRAINTS", "content": "As described in the main paper, we instantiate a minimal notion of context sensitivity by constraining when an entity is seen in the context of a property or verb. There are two subtle ways in which such constraints will affect the generated sentences.\n\u2022 Constraining properties. When a symbolic sentence with a descriptor is sampled, the descriptor symbol will be populated with a property that is valid for the relevant entity in the sentence.\n\u2022 Constraining subjects and objects. Subjects and objects broadly distinguish entities (or, to be precise, nouns) in a sentence. For properties that help define verbs (e.g., Walk), we instantiate a notion of directionality that determines whether the entity can take the action suggested by the verb corresponding to the property or whether the action can be taken upon it. Accordingly, when a verb is selected, only a subset of subjects and objects that can take and have the action of verb be taken upon them are left valid to form a sentence."}, {"title": "8.3 DEFINING THE OVERALL CONTEXT-SENSITIVE LANGUAGE", "content": "Our language L is defined by first instantiating the underlying grammar as described in App. A.1 and then the type constraints in App. A.2. We note that since the grammar is a randomized process and token roles are randomly filled by using the type constraints graph, the odds of seeing the same sample multiple times are exceedingly low. Primary hyperparameters for defining L include number of entities and number of properties, denoted |E| and |K|, respectively. Unless mentioned explicitly, we fix these hyperparameters to 900 and 18000 respectively. In several experiments we do vary these variables though. Thus, we also note that we are slightly abusing notations here and using L to refer to a single language. In actuality, however, what we have is a family of languages with the same grammar, but varying number of entities and properties. The vocabulary consists of entities (subjects and objects), descriptors, verbs, adjectives, adverbs, prepositions, and conjunctions. All languages we analyze have the same number of verbs (= 200), linking verbs (= 2), adjectives (= 20), adverbs (= 20), prepositions (= 3), and conjunctions (= 2).\nWe also note that the type constraints graph merely describes which properties are valid for a class. For a specific entity, only a fraction of these entities might be visible during training. Specifically, we constrain the sampling process such that only 10% of valid properties of a class are actually associated with an entity. However, as training occurs, the model gets to see several entities in the context of several properties. Even though certain pairs will never be seen together due to the restriction discussed above, two randomly sampled entities will still have a non-zero proportion of properties in whose context they have both been seen, hence giving the model some signal that the entities have shared characteristics (see Fig. 8). This is likely what leads to the percolation-like process we observe in the main paper to come into play, and hence yields us a 0.5 power law scaling for the transition"}, {"title": "3.1 PERFORMANCE OF A MEMORIZING SOLUTION ON DESCRIPTIVE SENTENCES", "content": "As the model undergoes training, its accuracy at getting descriptive constraints right can, at max, be the following:\n$Acc=\\begin{cases}\n    1, & f\\geq \\frac{B t\\cdot |C|}{ERK}\\\\\n    0.1 + f * max(1, \\frac{0.25Bt}{|C|}), &f<  \\frac{B t\\cdot |C|}{EK Rf}\\end{cases}$\nWhere f is fraction of pairs from the type constraints graph the model can see during training, 0.25 is approximately the proportion of randomly sampled sentences that are descriptive in nature, B is batch-size, t is number of iterations, and R is number of repetitions needed to internalize that an entity and property constitute a valid context. Since we see the third phase in a regime where t ~ 104, assuming at least 4 repetitions are necessary for internalizing a pair, we have Acc ~ 0.15."}, {"title": "D FURTHER RESULTS: ROBUSTNESS ACROSS SETTINGS AND EVALUATIONS", "content": "In this section", "setting": "This is the setting used throughout the paper", "properties": "ranging from 14800\u201438800"}, {"setting": "we change number of classes to 2 and repeat all evaluations in this setting.\n\u2022 Different entities setting: we change number of entities to 1800 and repeat all evaluations in this setting.\nWe specifically report the following results. Both in the main paper and in the results below", "settings": "App. D.1.\n\u2022 Grammaticality and type checks under different settings: App. D.2.\n\u2022 Negative log likelihoods of sentences from the langauge and their perturbed versions (e.g.", "correct)": "App. D.3.\n\u2022 How well does the model follow our language", "depth": "App. D.4.\n\u2022 Further results on unscrambling: App. D.5.\n\u2022 Evolution of Attention maps: App. D.6.\n\u2022 Further results on Conditional Generation: App. D.7. Conditional generation evaluations turn out to be extremely time-expensive, with a single run taking approximately 4 days to finish when conditional generation is evaluated (compared to 12 hours without). This is likely a result of model generating extremely long sentences to compose conditioning tokens that can involve multiple subjects, objects, and properties. We thus primarily focus on free generation and unscrambling in the results reported in this section. We do provide results for conditional generation in one more setting with 2"}]}