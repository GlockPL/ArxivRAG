{"title": "A PERCOLATION MODEL OF EMERGENCE: ANALYZING TRANSFORMERS TRAINED ON A FORMAL LANGUAGE", "authors": ["Ekdeep Singh Lubana", "Kyogo Kawaguchi", "Robert P. Dick", "Hidenori Tanaka"], "abstract": "Increase in data, size, or compute can lead to sudden learning of specific capa-\nbilities by a neural network-a phenomenon often called \u201cemergence\u201d. Beyond\nscientific understanding, establishing the causal factors underlying such emergent\ncapabilities is crucial to enable risk regulation frameworks for AI. In this work,\nwe seek inspiration from study of emergent properties in other fields and propose\na phenomenological definition for the concept in the context of neural networks.\nOur definition implicates the acquisition of specific structures underlying the data-\ngenerating process as a cause of sudden performance growth for specific, narrower\ntasks. We empirically investigate this definition by proposing an experimental\nsystem grounded in a context-sensitive formal language and find that Transform-\ners trained to perform tasks on top of strings from this language indeed exhibit\nemergent capabilities. Specifically, we show that once the language's underlying\ngrammar and context-sensitivity inducing structures are learned by the model,\nperformance on narrower tasks suddenly begins to improve. We then analogize our\nnetwork's learning dynamics with the process of percolation on a bipartite graph,\nestablishing a formal phase transition model that predicts the shift in the point\nof emergence observed in experiment when changing the data structure. Overall,\nour experimental and theoretical frameworks yield a step towards better defining,\ncharacterizing, and predicting emergence in neural networks.", "sections": [{"title": "1 INTRODUCTION", "content": "Modern neural networks, e.g., large language models (LLMs) (Gemini Team, 2023; OpenAI, 2023;\nAnthropic, 2023; Touvron et al., 2023), exhibit a broad spectrum of capabilities, allowing them to\nserve as the \u201cfoundation\u201d for downstream, application-specific systems (Bommasani et al., 2022;\nAhn et al., 2022; Driess et al., 2023; Schick et al., 2024). As these models scale, either via addition of\nmore data, parameters, or compute, an intriguing behavior is at times observed: until a certain critical\nscale is reached, there are capabilities that the model does not exhibit; however, beyond this point,\nsuch capabilities can suddenly \u201cemerge\u201d (Wei et al., 2022; Srivastava et al., 2022; Brown et al., 2020;\nYu et al., 2022; Steinhardt, 2023; Pan et al., 2022; Rae et al., 2021; Anil et al., 2023; Kirsch et al.,\n2022; He et al., 2024; Elhage et al., 2021). More specifically, the performance of the model on a task\nor benchmark meant to evaluate said capabilities can witness substantial growth in performance, even\nthough the overall training loss undergoes minimal, if any, improvements (Arora & Goyal, 2023;\nDu et al., 2024). Empirical evidence in fact suggests that, at times, several capabilities can emerge\nsimultaneously (Wei et al., 2022; Wei, 2022).\nBeyond developing a better scientific understanding of neural networks, understanding emergent\ncapabilities is crucial to enable risk-centric regulation frameworks for AI, which assume a system's"}, {"title": "2 RELATED WORK", "content": "Explaining emergence. Focusing on the sudden learning characteristic of emergent capabilities, a few\nrecent works have tried to explain the factors driving this phenomenon. For example, compositionality\nhas been implicated for having a \u201cmultiplicative\" effect on a model's performance, where the argument\nis that a model cannot perform well on a compositional task until the abilities needed to perform\nindividual tasks involved in that composition are acquired (Okawa et al., 2023; Arora & Goyal, 2023;\nYu et al., 2023; Srivastava et al., 2022; Wei et al., 2022; Hoffmann et al., 2022; Gokhale, 2023);\nwhen they are acquired, performance suddenly grows. A few papers have also shown that learning of\nspecific capabilities (i.e., ones not compositional in nature) can be sudden (Chen et al., 2024; Nam\net al., 2024; Kirsch et al., 2022; He et al., 2024; Michaud et al., 2023). In contrast, Schaeffer et al.\n(2023) argue emergent scaling curves are a consequence of poorly defined, discontinuous evaluation\nmetrics, and the seemingly sudden learning goes away once partial, continuous credit is given to the\nmodel. We emphasize that if the structure of a task is ignored, it is certainly easy to define arbitrary\ncontinuous metrics for a task; however, such metrics are unlikely to help measure progress toward\nlearning a task. For example, consider the addition of two numbers, say 10 and 11, and the metric\ncalled token edit distance (Schaeffer et al., 2023) that assesses the average distance between digits\nin the model's output, denoted $xy$, from the ground truth, i.e., $(|x-2|+|y-1|)/2$. For both $xy = 22$\nand $xy = 11$, this metric equals 1; however, clearly 22 is a better approximation for the ground\ntruth (21). Thus, once we account for the structure of the task, i.e., the fact that error in the most\nsignificant digit should be penalized more than error in the least significant one, we see limitations\nin token edit distance as a metric for assessing a model's ability to add numbers. We argue claims\nrelating emergence to sensitivity of metrics can be confounded by use of metrics that do not respect\nthe structure of the task.\nGrokking vs. Emergence. We focus on the effect of data scaling on a model's capabilities; often\ncalled 'learning curve' or 'data scaling' analysis (Viering & Loog, 2022; Blumer et al., 1989; Bousquet\net al., 2021; Seung et al., 1992; Watkin et al., 1993; Amari, 1993; Haussler et al., 1994). On surface,\nthis might look similar to the seemingly related phenomenon of grokking (Power et al., 2022; Liu\net al., 2023b; \u017dunkovi\u010d & Ilievski, 2022; Murty et al., 2023; Barak et al., 2022; Edelman et al., 2023;\nNanda et al., 2022), wherein a model's performance on a task rapidly improves long after it has fit the\ntraining data. However, we emphasize that we focus on an online learning setting in our experiments,\ni.e., a given sample is unlikely to be seen multiple times during training. Emergence is generally\nstudied in such online learning scenarios. Since there is no distinction between train versus test data\nin such a setting, we argue mechanistic explanations of grokking identified in past work (Nanda et al.,\n2023; Liu et al., 2022b) are unlikely to help explain our results of emergence under data scaling."}, {"title": "3 A PHENOMENOLOGICAL DEFINITION OF EMERGENCE", "content": "To analyze emergence, we first establish what we mean by the term for the purpose of this work.\nSpecifically, we define emergence in a phenomenological manner, i.e., by assembling the characteristic\nproperties associated with scaling curves claimed to depict emergent learning. We emphasize our\ndefinition is merely a definition for emergence, and does not necessarily represent all possible\nperspectives (Luccioni & Rogers, 2023). For example, often model capabilities that arise despite any\nexplicit supervision are called emergent in self-supervised learning (Caron, 2021; Caron et al., 2021;\nZiyin et al., 2022). As our goal is to analyze the effects of scaling, regardless of supervision protocol\nused, we do not try to capture this property.\nDefinition 1. (Emergence of a capability.) We say a capability C is emergent with scaling along a\nrelevant axis (e.g., amount of data, compute, parameters) if:\n\u2022 P1: nonlinear improvement occurs in the performance of a task where C is required;\n\u2022 P2: multiple tasks simultaneously show nonlinear performance improvement; and\n\u2022 P3: the model undergoes a structural change that is instrumental to learning the capability C,\nand nonlinear progress in C's learning directly correlates with the learning of said structure.\nThe definition above assigns a broader meaning to emergence than mere sudden performance improve-\nment on a narrow task: it argues there should be precise structural changes in the model that have\ndownstream effects on several capabilities, hence leading to sudden improvements in performance on\nseveral tasks. Note that we intentionally leave the notion of 'structure' informal in the definition. The\nsalient property of a structure is that if a model learns it, downstream tasks should become easier to\nperform. For example, a fine-grained notion of a structure can be previous token and copy attention\nheads that lead to in-context learning (Reddy, 2023; Edelman et al., 2024; Olsson et al., 2022); a more\ncoarse-grained structure can include the model learning the syntactical rules of a language that help it\nwith generation of coherent language and hence with any task where coherence is important (Chen\net al., 2024). In this sense, what is emergent is a structure, and what is observed is a change in the\nmodel's capabilities. Hypothesizing what this structure is by identifying shared characteristics of\na set of tasks that simultaneously show sudden learning, one can develop an evaluation meant to\nprecisely gauge learning of the corresponding structure and hence infer at what point an independent\ntraining run will show sudden improvements.\nWe note the intuition for Defn. 1 comes from prior work in the fields of complex systems and\nphysics (Anderson, 1972; Newman et al., 2001; Newman, 2003), from where the term has sought\nits inspiration in recent machine learning literature (Steinhardt, 2023; Wei et al., 2022). Therein,\nemergence describes the scenario where rapid changes occur in a system's properties as some control\nparameter is varied. A range where the system's properties change relatively smoothly is called a\nphase, and a change of phase with a change in the control variable is called a phase transition. A\ncrucial step in studying emergence in physics is identifying an order parameter\u2014a measure that\ncaptures the formation of some specific structure in the system such that the development of this\nstructure is what alters the system's properties and drives a phase transition. For example, in Fig. 1a,\na system of particles transitions through phases (solid, liquid, gas) as the temperature is changed; the\nformation of a crystalline structure with the decrease in temperature can be identified by analyzing\nthe bond-orientation order parameter, while the liquid-to-gas transition can be described by a jump in\nparticle density. We argue that we must similarly define order parameters for studying emergence in\nneural networks as well, i.e., we must develop evaluation measures that are focused towards detecting\nthe learning of specific, narrow structures that are generally of use to several downstream capabilities."}, {"title": "4 FORMAL LANGUAGES AS AN EXPERIMENTAL SYSTEM FOR EMERGENCE", "content": "Having established our perspective on emergence, we now define a toy experimental system that\nallows us to precisely study the concept in a controlled setting. We note that our focus will be\non emergence under data scaling in an online learning scenario (i.e., a sample is unlikely to be\nseen multiple times). To this end, we follow recent work on understanding language modeling and\nuse formal languages to define our experimental setup (Allen-Zhu & Li, 2023b; Jain et al., 2023;\nAllen-Zhu & Li, 2023a; Valvoda et al., 2022; Liu et al., 2023a; 2022a). As discussed in detail next,\nthe formal language we use in this work is (minimally) context-sensitive, with underlying syntactical"}, {"title": "5 LEARNING TASKS AND EXPERIMENTAL SETUP", "content": "Having described our language L, now we\nbriefly discuss our experimental setup (see\nApp. C for details). We train a GPT architecture\nmodel (Andrej Karpathy, 2023) with the stan-\ndard autoregressive language modeling objec-\ntive. Data is sampled \u201conline\u201d, i.e., we sample\na fresh batch of strings every iteration from L.\nUnless mentioned otherwise, L is constituted of\nE = 900 entities and |K| = 18000 properties,\nequally and disjointly distributed over |C| = 10\nclasses, and with edges connecting entities to\np = 0.1 fraction valid properties of a class in\na uniformly random manner; results ablating\nthese settings are in App. D. Before being fed\ninto the model for training or evaluation, strings\nsampled from the language are restructured into a format that enables the specification of particular\ntasks (see Fig. 3). Specifically, we train the model to learn the following tasks with 80/10/10% splits.\n\u2022 Free generation: Produce a valid string, i.e., one that respects the grammar and type constraints.\n\u2022 Unscrambling: A string is sampled from L and randomly permuted; the model is expected to\nunscramble it. This task is known to show sudden learning in LLMs (Wei et al., 2022).\n\u2022 Conditional Generation: A set of tokens corresponding to entities or properties are shown to the\nmodel, which is expected to generate a string combining these tokens in a valid manner.\nEvaluation Protocols. Given an input x, which may correspond to any of the three tasks above,\ndenote the model output as f(x). Let d(.) be an indicator variable that evaluates to 1 if its input\nis true. We track several metrics throughout training, as described below. We often decompose\nthese evaluations according to strings of two types: (i) descriptive, i.e., that describes that an entity\npossesses a descriptive property, and (ii) relative, i.e., that claims a subject, object, and verb can\nmatch each other to create a valid sentence. Unless noted otherwise, results are averaged over 3 seeds.\n\u2022 Grammaticality/Type Check. Used for evaluating free generation. Grammaticality involves\nchecking whether model output follows the underlying grammar G, i.e., $f(x) \u2208 \u03a3$. Type checks\ninvolve first extracting subjects, objects, and properties from the sentence and then evaluating\nwhether this set of tokens is allowed in the context of each other. We decompose type checks as\ndescriptive (do entities and descriptors match), relative (do subject, object, and verb match), and\nall (product of all constraints, including adjectives and adverbs)."}, {"title": "6 RESULTS: EMERGENT CAPABILITIES IN FORMAL LANGUAGE LEARNING", "content": "We now evaluate (i) whether our setup demonstrates emergence (see Def. 1), and (ii) whether we can\nextract insights into the mechanisms of what leads to emergence. In the following, we often use the\nterms \"phase\" and \"phase change\"; see discussion around Def. 1 for context on these terms.\n6.1 PHASES OF LANGUAGE AND CAPABILITIES ACQUISITION\nWe plot the model's performance as a function of training iterations. Since we are in an online\nlearning, constant learning rate setting, this analysis corresponds to studying the effects of data\nscaling. Results are reported in Fig. 4 and show there are three phases to the learning dynamics.\nPhase 1: Grammar acquisition. We find the model first learns to produce grammatically correct\nsentences, as measured by the grammaticality measure defined in Sec. 5. This process is relatively\nrapid, as we see the model starts generating grammatically accurate sentences in a short period of\napproximately 100 iterations; attention heads also rapidly evolve and reflect the parse structure of a\nsentence (see App. D.6) In this regime, however, the narrower tasks of unscrambling and conditional\ngeneration exhibit poor performance. However, precisely when grammaticality improves, we find\nthat per-token accuracy starts to improve. This indicates that the model learning a broad structure\nunderlying the data (i.e., grammar) has an impact on the learning of other capabilities.\nPhase 2: Acquisition of relative type constraints. At around 1000 iterations, we find there is\na sudden increase in the model's performance on relative types from essentially zero to perfect\naccuracy; precisely at this point, we find the loss for all tasks, especially free generation, show a\nsudden drop. Interestingly, we find this sudden improvement occurs precisely at the point where the\nmodel reaches its maximum performance on grammaticality for the first time. That is, as soon as the\nfirst structure underlying the data is acquired, the model rapidly learns the next relevant structure of\nrelative type constraints. Improvement occurs in descriptive constraints as well (and hence the overall\nType Check performance), but hovers around slightly above random performance of 0.1. With |C|\nclasses, if a model produces grammatically correct sentences, it will achieve a random performance\nof 1/C on descriptive type checks. This however also implies that the model is primarily relying on\nits syntactical knowledge and does not respect descriptive type constraints much.\nDuring this phase, we see that shortly after the phase change, there is a sudden increase in performance\nfor both unscrambling and conditional generation, across all metrics. These tasks' losses also show\nanother loss drop occurs at this point; though the drop seems smoother in the total loss, likely due\nto averaging effects (Michaud et al., 2023). As shown in Fig. 4e, we find that this performance\nimprovement is driven by sentences that require primarily correctness of grammar and relative type"}, {"title": "7 A PERCOLATION MODEL OF EMERGENCE", "content": "We next propose a framework for modeling the emergence of capabilities that require a model\nto compose unseen entities and descriptive properties, e.g., learning descriptive type constraints,\nwhich, beyond allowing a model to produce accurate free generations, will aid with narrower tasks\nlike conditional generation and unscrambling. We argue the relevant structure to analyze for this\npurpose is the concept class: if a model understands what entities and properties belong to the same\nconcept class, regardless of whether they have been seen together in a sentence, it will deem their\nco-occurrence to be valid. We thus develop an abstraction for concept classes as bipartite graphs and\ncast their learning as a problem of percolation on a bipartite graph.\n7.1 MATRIX REPRESENTATION OF DATA AND LEARNING COMPOSITIONS\nRecall that a concept class is defined as\na set of entities that are expected to have\nshared properties (see Def. 3). The question\nis whether upon sub-sampling pairs of enti-\nties and properties from a concept class, can\nthe model learn that, in fact, all pairs of en-\ntities and properties are valid and compose\nthe concept class. For instance, in the case\nof a concept class such as human, the set\nof entities can include humans with differ-\nent genders (e.g., man) as well as human-\nassociated entities such as a lawyer (see\nFig. 2). The corresponding properties for\nthe human concept class will be, for exam-\nple, walk, jump, tall. A man, being human,\nis expected to have all these properties, al-\nthough strings specifying these properties\nfor a lawyer may be rare or even absent\nin the training data. We are interested in\nthe case where the data, such as strings, in-\ncludes examples of these pairs of entities\nand properties. We can represent this by a\nmatrix whose rows and columns represent\nthe entities and the properties, and the ma-\ntrix values indicate the quantity or density\nof data available for each composition, such\nas an entity-descriptor pairing. We call this\nmatrix the concept density matrix.\nDefinition 4. (Concept Density Matrix.)\nLet D be an $|E| \\times |K|$ matrix with real-valued entries between 0 and 1, inclusive. Each entry\n$D_{ek}$ represents the density for the entity and property pair (e,k) (e.g., the amount of data that\nrepresents the specific composition), where $e \u2208 {1, ..., |E|}$ and $k \u2208 {1, ..., |K|}$ are the indices of the\nentities and properties, respectively.\nFor example, consider the case where there are three values of entities and properties ($|E| = |K| = 3$),\nwith entities (rows) being {Man, Lawyer, Telephone}, and properties (columns) being {Walk,\nStoic, Ring}. The corresponding D can be:\n$D=\\begin{pmatrix}\n1 & 1 & 0\\\\\n1 & 0 & 0\\\\\n0 & 0 & 1\n\\end{pmatrix}$\nA common composition such as Man walking will lead to a value of 1 at the intersection of Man\nand Walk, i.e., $D_{00} = 1$, where $D_{ij}$ denotes element at row-i and column-j. Conversely, a highly\nunlikely composition like Lawyer ringing will be absent in the dataset, and will be represented\nby a zero at the respective matrix position, i.e., $D_{12} = 0$. We can also assume for example that Man\nringing or a Telephone walking are rare, which yields $D_{13} = D_{31} = 0$."}, {"title": "7.2 PERCOLATION TRANSITION ON DESCRIPTIVE CONSTRAINTS", "content": "Using the bipartite graph framework, the generalization, or the learning of the concept class, can be\ndefined as the situation where a large cluster of entity-property connected pairs arises despite the\nsparse concept density matrix. A critical aspect to examine is the proportion of the inference matrix\nvalues where $T_{ek}^{(\\infty)}$ is non-zero, out of the total possible pairs $|E| \\times |K|$. This particular scenario aligns\nwith the bond percolation problem on a bipartite graph. In bond percolation, we investigate how\nthe largest connected cluster's size varies with the probability $p$ of each edge (bond) being present.\nIn a typical setting, there exists a critical threshold value, $p = p_c$, called the percolation threshold.\nBelow this threshold ($p < p_c$), the graph typically exhibits a disconnected phase characterized by the\nabsence of extensively connected clusters, with most nodes either isolated or part of smaller clusters.\nAbove this threshold ($p > p_c$), the graph transitions to a connected phase, significantly increasing the\nlikelihood of a vast connected component spanning a large portion of the graph. This shift from a\npredominantly disconnected state to one with a macroscopic cluster is a defining characteristic of the\npercolation process, and this transition sharpens as the number of components in the system increases.\nIn a simple percolation scenario, where connecting edges are selected randomly on the graph with\nprobability $p$, the percolation threshold is obtained as $p_c \\sim \\sqrt{1/|E||K|}$ for large $|E|$ and $|K|$ (Newman\net al., 2001), which means that when around $\\sqrt{|E||K|}$ edges are connected (out of the total $|E||K|$) there\nis a qualitative change in the growth of the cluster size. For $p > p_c$, the number of nodes included in\nthe connected cluster will become macroscopic, meaning that the probability that a randomly selected\npair of an object and a feature is connected will be finite. We present in Appendix B the derivation\nof the percolation threshold for bipartite graphs that are uncorrelated, and how the cluster size (i.e.,\nnumber of nodes in the largest connected graph) scales as $\\sim (p - p_c)^{\\beta}$ with $\\beta = 1$ for usual cases.\nWe posit that the percolation threshold corresponds to the point at which our model generalizes from\nthe sparse learning of pairs to a complete representation of the concept classes. When the number\nof edges surpasses the threshold, the model can infer novel compositions, even for entity-feature\npairs that were not explicitly present in the training data. The model should also start to be able to\ndiscriminate between distinct concept classes beyond this threshold; in the community detection\nproblem, for example in the stochastic block model (Abbe, 2018; Decelle et al., 2011), the detection\nthreshold for the partitions have the same scaling as $p_c$ Florescu & Perkins (2016). Since increasing"}, {"title": "8 CONCLUSION", "content": "In this work, we take inspiration from other fields (e.g., physics and complex systems) and propose\na phenomenological definition for emergence of capabilities in neural networks. Specifically, the\ndefinition argues that at the point of emergence, the model acquires broad structures which are\ninstrumental to the learning of specific, narrower capabilities; acquisition of such structures then\nleads to sudden performance improvement on several tasks (often with some delay). While relatively\ninformal, this definition brings the notion of emergence in the context of neural networks closer\nto its meaning in physics, wherein the acquisition of specific structures is known to drive phase\nchanges that involve sudden changes in the system's properties. Characterizing these phase changes"}, {"title": "A DATA-GENERATING PROCESS: DEFINING OUR FORMAL LANGUAGE", "content": "Our data-generating process involves defining a formal language, sampling sentences from this\nlanguage, and then defining tasks to be performed upon these sentences (specifically, free generation,\nunscrambling, or conditional generation). In this section, we discuss the precise details of how the\nlanguage is implemented.\nA.1 DEFINING A GRAMMAR USING PCFGS\nTo define a grammar for our language, we use the framework of Probabilistic Context-Free Grammars\n(PCFGs). To keep the paper self-contained, we provide a short primer on PCFGs below and then\ndiscuss our precise version of it in detail. For a more thorough discussion on PCFGs, we refer the\nreader to one of the several well-written tutorials (Collins, 2013) and books (Sipser, 1996).\nA.1.1 SHORT PRIMER ON PCFGS\nBroadly, a PCFG is defined via a 5-tuple $G = (\\\u039d, \u03a3, R, S, P)$, where:\n\u2022 NT is a finite set of non-terminal symbols.\n\u2022 T is a finite set of terminal symbols, disjoint from NT.\n\u2022 R is a finite set of production rules, each of the form $A \u2192 \u03b1\u03b2$, where $A \u2208 N$ and\n\u03b1, \u03b2\u03b5 (\u039d\u03a4\u039f\u03a4).\n\u2022 S\u2208 N is the start symbol.\n\u2022 P is a function $P : R \u2192 [0, 1]$, such that for each $A\u2208\u039d\u03a4, \u03a3_{\u03b1:A\u2192a\u2208R}P(A \u2192 \u03b1\u03b2) = 1$.\nTo generate a sentence from a PCFG, the following process is used. Pseudocode for this generation\nprocess is provided in Algo. 1.\n1. Start with a string consisting of the start symbol S.\n2. While the string contains non-terminal symbols, randomly select a non-terminal A from the\nstring. Choose a production rule A \u2192 \u03b1\u03b2 from R according to the probability distribution\nP(A \u2192 \u03b1).\n3. Replace the chosen non-terminal A in the string with \u03b1, the right-hand side of the production\nrule.\n4. Repeat the production rule selection and expansion steps until the string contains only\nterminal symbols (i.e., no non-terminals remain).\n5. The resulting string, consisting entirely of terminal symbols, is a sentence sampled from the\ngrammar."}, {"title": "A.1.2 INSTANTIATING THE GRAMMAR UNDERLYING OUR LANGUAGE", "content": "While generally one directly samples sentences from a grammar, in this work, we define a grammar\nthat operates over symbols, i.e., whose terminals are variables that are not yet populated by any\nspecific values from the language's vocabulary. We emphasize this is an unconventional manner for\ndefining a PCFG, as one would generally use a standard vocabulary of the language to directly define\nterminal symbols. However, to enforce type constraints, we find this unconventional format aids in\nmaking the implementation easier. Specifically, one can simply sample an entirely symbolic sentence,\nand then enforce type constraints at the step when these symbols have to be populated.\nOverall, our grammar, denoted G, is defined using the following.\n\u2022 Terminal symbols: T = {Subj, Obj, Verb, Conj, lVerb, Desc, eAdj, dAdj, Adv, Prep}.\nHere, Subj is a symbol for a subject, Obj for an object, Verb for verbs, Conj for\nconjunctions, 1Verb for a linking verb, Desc for descriptors, eAdj for adjectives used\nfor entities, dAdj for adjectives used for descriptors, Adv for adverbs, and Prep for\nprepositions.\n\u2022 Non-terminal symbols: NT = {S, SNP, ST, ONP, OT, VP, VT, descT}.\nHere, S denotes the start symbol, sNP can be interpreted as a noun phrase with a subject\nin it, sT as the immediate ancestor of the subject symbol, oNP as a noun phrase with an\nobject in it, oT as the immediate ancestor before the object symbol, VP as a verb phrase,\nvT as the immediate ancestor of the verb symbol, and descT as the immediate ancestor\nof a descriptor symbol.\n\u2022 Production rules R:\nS \u2192 SNP VP [1.0]\nSNP \u2192 ST [0.8] | SNP Conj SNP [0.2]\nVP \u2192 1Verb descT [0.4] | Verb Prep ONP [0.4] | VP Conj VP [0.2]\nONP \u2192 OT [0.7] | oT Conj oNP [0.3]\nST \u2192 eAdj Subj [0.8] | Subj [0.2]\nOT \u2192 eAdj Obj [0.8] | Obj [0.2]\ndescT \u2192 dAdj Desc [0.8] | Desc [0.2]\nNote that since non-terminals can appear on both left and right hand side of a rule, there is recursion\npossible in our grammar and hence sentences can get very long. We restrict sentence lengths to\n75, yielding a language where sentence lengths vary from 4\u201375 tokens. Probability over rules was\npartially adapted from prior work by (Hupkes et al., 2020).\nGiven the above, we can now sample symbolic sentences such as Subj lVerb Desc. We will populate\nthese symbols with tokens from our vocabulary V. As noted above, while in general this population\nstep would be performed as the final step of the grammar, to enforce type constraints and enable\ncontext-sensitivity, we separate it from the grammar."}, {"title": "A.2 TYPE CONSTRAINTS", "content": "As described in the main paper, we instantiate a minimal notion of context sensitivity by constraining\nwhen an entity is seen in the context of a property or verb. There are two subtle ways in which such\nconstraints will affect the generated sentences.\n\u2022 Constraining properties. When a symbolic sentence with a descriptor is sampled, the\ndescriptor symbol will be populated with a property that is valid for the relevant entity in\nthe sentence."}, {"title": "A.3 DEFINING THE OVERALL CONTEXT-SENSITIVE LANGUAGE", "content": "Our language L is defined by first instantiating the underlying grammar as described in App. A.1\nand then the type constraints in App. A.2. We note that since the grammar is a randomized process\nand token roles are randomly filled by using the type constraints graph, the odds of seeing the same\nsample multiple times are exceedingly low. Primary hyperparameters for defining L include number\nof entities and number of properties, denoted |E| and |K|, respectively. Unless mentioned explicitly,\nwe fix these hyperparameters to 900 and 18000 respectively. In several experiments we do vary these\nvariables though. Thus, we also note that we are slightly abusing notations here and using L to refer\nto a single language. In actuality, however, what we have is a family of languages with the same\ngrammar, but varying number of entities and properties. The vocabulary consists of entities (subjects\nand objects), descriptors, verbs, adjectives, adverbs, prepositions, and conjunctions. All languages\nwe analyze have the same number of verbs (= 200), linking verbs (= 2), adjectives (= 20), adverbs\n(= 20), prepositions (= 3), and conjunctions (= 2).\nWe also note that the type constraints graph merely describes which properties are valid for a class.\nFor a specific entity, only a fraction of these entities might be visible during training. Specifically, we\nconstrain the sampling process such that only 10% of valid properties of a class are actually associated\nwith an entity. However, as training occurs, the model gets to see several entities in the context\nof several properties. Even though certain pairs will never be seen together due to the restriction\ndiscussed above, two randomly sampled entities will still have a non-zero proportion of properties in\nwhose context they have both been seen, hence giving the model some signal that the entities have\nshared characteristics (see Fig. 8). This is likely what leads to the percolation-like process we observe\nin the main paper to come into play, and hence yields us a 0.5 power law scaling for the transition"}, {"title": "B PERCOLATION THRESHOLD IN THE BIPARTITE GRAPH SETUP", "content": "For general bipartite graphs that are uncorrelated", "2": "n"}]}