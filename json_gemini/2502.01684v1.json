{"title": "LEVERAGING JOINT PREDICTIVE EMBEDDING AND BAYESIAN INFERENCE IN GRAPH SELF SUPERVISED LEARNING", "authors": ["Srinitish Srinivasan", "Omkumar CU"], "abstract": "Graph representation learning has emerged as a cornerstone for tasks like node classification and link prediction, yet prevailing self-supervised learning (SSL) methods face challenges such as computational inefficiency, reliance on contrastive objectives, and representation collapse. Existing approaches often depend on feature reconstruction, negative sampling, or complex decoders, which introduce training overhead and hinder generalization. Further, current techniques which address such limitations fail to account for the contribution of node embeddings to a certain prediction in the absence of labeled nodes. To address these limitations, we propose a novel joint embedding predictive framework for graph SSL that eliminates contrastive objectives and negative sampling while preserving semantic and structural information. Additionally, we introduce a semantic-aware objective term that incorporates pseudo-labels derived from Gaussian Mixture Models (GMMs), enhancing node discriminability by evaluating latent feature contributions. Extensive experiments demonstrate that our framework outperforms state-of-the-art graph SSL methods across benchmarks, achieving superior performance without contrastive loss or complex decoders. Key innovations include (1) a non-contrastive, view-invariant joint embedding predictive architecture, (2) Leveraging single context and multiple targets relationship between subgraphs, and (3) GMM-based pseudo-label scoring to capture semantic contributions. This work advances graph SSL by offering a computationally efficient, collapse-resistant paradigm that bridges spatial and semantic graph features for downstream tasks.", "sections": [{"title": "1 Introduction", "content": "Graph representation learning has found widespread adoption in Social Network Analysis, Recommendation Systems, Computer Vision and Natural Language Processing. It aims to learn low-dimensional embeddings of nodes or subgraphs while preserving their underlying spatial and spectral features. These learned embeddings can then be used on downstream tasks such as node classification([1]), link prediction([2]) and community detection([3]) by training task- specific decoders or classification layers on the learned embeddings keeping the backbone frozen. Such an approach reduces the computational complexity and training time on downstream tasks. Though Graph Neural Networks(GNNs) have gained popularity over time, they require a certain number of labelled nodes to perform well. Further, owing to the complexity of graph representations spatially, it is often challenging to pre-train graphs and transfer the learned embeddings to downstream tasks.\nGraph Self Supervised learning has been widely studied in the literature to facilitate graph representation learning. This includes methods such as Node2Vec, DGI([4]), MVGRL([5]), GRACE([6]) etc. Although these methods have achieved state-of-the-art results on several social networks, they rely heavily on either graph reconstruction/feature reconstruction, re-masking or generating contrastive views by negative sampling, which is a computationally expensive process on large graphs. Graph-SSL methods that rely on reconstruction (generative methods) are decoder variant, meaning they depend on the decoder architecture(inner product decoder or symmetrical feature reconstruction). Such networks might require additional feature propagation steps such as skip connections to prevent vanishing gradients and retain learned information. Furthermore, most methods tend to use more layers or stack multiple encoders to capture long-range node interactions, which increases training complexity and leads to poor performance in downstream tasks due to over-smoothing. This diminishes the model's ability to effectively discriminate between nodes effectively([7])."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Unsupervised Representation Learning on Graphs", "content": "[8] proposes a method for composing multiple self-supervised tasks for GNNs. They introduce a pseudo-homophily measure to evaluate representation quality without labels. [9] proposed the removal of negative sampling and MI estimator optimization entirely. The authors propose a loss function that contains an invariance term that maximizes correlation between embeddings of the two views and a decorrelation term that pushes different feature dimensions to be uncorrelated. [10] employs a re-mask decoding strategy and uses expressive GNN decoders instead of Multi-Layer Perceptrons(MLPs) enabling the model to learn more meaningful compressed representations. It lays focus on masked feature reconstruction rather than structural reconstruction. [5] proposed MVGRL that makes use of two graph views and a discriminator to maximize mutual information between node embeddings from a first-order view and graph embeddings from a second-order view. It leverages both node and graph-level embeddings and avoids the need for explicit negative sampling. [11] proposed ParetoGNN which simultaneously learns from multiple pretext tasks spanning different philosophies. It uses a multiple gradient descent algorithm to dynamically reconcile conflicting learning objectives, showing state-of-the-art performance in node classification, clustering, link prediction and community prediction."}, {"title": "2.2 Bootstrapping Methods", "content": "[12] introduced multi-scale feature propagation to capture long-range node interactions without oversmoothing. The authors also enhance inter-cluster separability and intra-cluster compactness by inferring cluster prototypes using a Bayesian non-parametric approach via Dirichlet Process Mixture Models(DPMMs). [13] makes use of simple graph augmentations such as random node feature and edge masking, making it easier to implement on large graphs while achieving state-of-the-art results. It leverages a cosine similarity-based objective to make the predicted target representations closer to the true representations. [14] introduced a Siamese network architecture comprising an online and target encoder with momentum-driven update steps for the target. The authors propose 2 contrastive objectives i.e cross-network and cross view-contrastiveness. The Cross-network contrastive objective incorporates negative samples to push disparate nodes away in different graph views to effectively learn topological information."}, {"title": "2.3 Joint Predictive Embedding Methods", "content": "Joint predictive embedding has been explored in the field of computer vision and audio recognition. [15] introduced I-JEPA which eliminates the need for hand-crafted augmentations and image reconstruction. They make use of a joint-embedding predictive model that predicts representations of masked image regions in an abstract feature space rather than in pixel space, allowing the model to focus on high-level semantic structures. It makes use of a Vision Transformer(ViT) with a multi-block masking strategy ensuring that the predictions retain semantic integrity. [16] extends the masked-modeling principle from vision to audio, enabling self-supervised learning on spectrograms. The key technical contribution is the introduction of a curriculum masking strategy, which transitions from random block masking to time-frequency-aware masking, addressing the strong correlations in audio spectrograms."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Preliminaries", "content": "Consider the definition of a Graph G = (V, E). Let V be the set of vertices {V\u2081, V\u2082, V\u2083....V\u2099} and E be the set of edges {e\u2081, e\u2082, e\u2083...e\u2099\u2091}. N\u1d65, N\u2091 are the number of nodes and edges respectively in G. Each node in V is characterized by a d dimensional vector. This is the initial signal that is populated by either bag of words or binary values depending on the problem considered."}, {"title": "3.2 Loss Function", "content": ""}, {"title": "3.2.1 Joint Predictive Optimization", "content": "We sample a subgraph G' from graph G by dropping a set of nodes according to a Bernoulli distribution parameterized by success probability p\u2081. The subgraph G' is passed into the context encoder to output node embeddings H' of dimensions d'. Meanwhile, the graph G is passed into the target encoder to generate node embeddings H with the same"}, {"title": "3.2.2 Node Feature Contribution Optimization", "content": "The node embeddings H' obtained from passing G' into the context encoder are fit in a Gaussian Mixture Model(GMM). We aim to obtain,\n$p(z_k = 1/h_n) = \\frac{p(h_n | z_k = 1)p(z_k = 1)}{\\sum_{j=1}^{K}p(h_j = 1)p(z_j = 1)}$\nwhere z is a latent variable that takes 2 values: one if h' comes from Gaussian k, and zero otherwise. We can obtain,\n$p(z_k = 1) = \\pi_k$\n$p(h_n|z_k = 1) = N(h_n|\\mu_k, \\Sigma_k)$\nwhere $\\pi_k$ is the mixing coefficient such that,\n$\\sum_{k=1}^{K} \\pi_k = 1$\nOn replacing these equations in eq.2, we obtain\n$p(z_k = 1/h_n) = \\frac{\\pi_kN(h_n|\\mu_k, \\Sigma_k)}{\\sum_j\\pi_jN(h_n|\\mu_j, \\Sigma_j)} = \\gamma_{(znk)}$\nE-Step: In the E-Step, we aim to evaluate,\n$Q(\\Theta^*, \\Theta) = E[ln p(H', Z|\\Theta^*)] = \\sum_{Z}p(Z|H', \\Theta) ln p(H', Z|\\Theta^*)$\nfrom eq.6, we can substitute \u03b3 in the above equation as,\n$Q(\\Theta^*, \\Theta) = \\sum_{Z}\\gamma(z_{nk}) lnp(H', Z|\\Theta^*)$\nOn finding the complete likelihood of the model, we finally have,\n$Q(\\Theta^*, \\Theta) = \\sum_{n=1}^{N}\\sum_{k=1}^{K}\\gamma(z_{nk})[ln \\pi_k + lnN(h_n|\\mu_k, \\Sigma_k)]$\nM-Step: In the M-Step, we aim to find updated parameters \u0398* as follows,\n$\\Theta^* = arg \\underset{\\Theta}{max} Q(\\Theta^*, \\Theta)$\nConsidering the restriction, $\\sum_{k=1}^{K} \\pi_k = 1$, eq.9 is modified as follows,\n$Q(\\Theta^*, \\Theta) = \\sum_{n=1}^{N}\\sum_{k=1}^{K}\\gamma(z_{nk})[ln\\pi_k + lnN(h_n|\\mu_k, \\Sigma_k)] - \\lambda(\\sum_{k=1}^{K} \\pi_k - 1)$\nThe parameters are then determined by finding the maximum likelihood of Q. On taking the derivative with respect to \u03c0\u2096, \u03bc\u2096, \u03a3\u2096 and rearranging terms, we finally obtain the update equations for the parameters as follows,\n$\\pi_k^* = \\frac{\\sum_{n}\\gamma_{(znk)}}{N}$\n$\\mu_k = \\frac{\\sum_{n=1}^{N} \\gamma_{(znk)}h'_n}{\\sum_{n=1}^{N} \\gamma_{(znk)}}$\n$\\Sigma_k = \\frac{\\sum_{n=1}^{N}\\gamma(znk) (h'_n - \\mu_k) (h'_n - \\mu_k)^T}{\\sum_{n=1}^{N} \\gamma_{(znk)}}$"}, {"title": "3.2.3 Final objective", "content": "The context encoder parameters \u0398\u1d9c are finally updated as follows,\n$\\mathcal{L} = L_I + L_G$\n$\\Theta^G \\leftarrow optimize(\\Theta^c, \\alpha, d_\\Theta L)$\nwhere, \u03b1 is the learning rate for the Adam optimizer. The weights of the context and target encoder are randomly initialized by a standard normal distribution. A cosine annealing learning rate scheduler with early stopping is used in all experiments."}, {"title": "3.3 Description of Components and Operations", "content": ""}, {"title": "3.3.1 Context Encoder", "content": "The node features from G' are passed into the context encoder. The context encoder is a simple 3-layer GCN encoder which predicts 128, 256 and 512 features in each layer respectively. The forward propagation for each layer is described as follows,\n$X' = g(\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}X\\Theta)$\n$\\tilde{A} = A + I$\nwhere $\\tilde{D}$ is the degree matrix, $\\tilde{A}$ is the adjacency matrix with added self-loops and \u0398 is the learned weights matrix. g is a non-linear function. We use ReLU for the first two layers and Tanh for the final layer. The context encoder is an online encoder whose weights are updated by gradient descent."}, {"title": "3.3.2 Target Encoder", "content": "The target encoder inputs node features from G. Similar to the context encoder, it is a 3-layer GCN encoder with same number of hidden, output dimensions and forward propagation steps. The weights of the target encoder are updated as a moving average of the context encoder as follows,\n$\\Theta_T^s = m\\Theta_T^{s-1} + (1 - m)\\Theta_C^s$\nwhere $\\Theta_T$ is the weights matrix of the target encoder, $\\Theta_C$ is the weights of the context encoder, m is the momentum parameter and s refers to the sth iteration."}, {"title": "3.3.3 Predictor", "content": "The predictor consists of 2 GCN layers, both predicting 512 features. The activation function g for both layers is Tanh, in line with the final layer of the target encoder. We use Tanh since it is a bounded function, thus stabilizing the loss computation and optimization."}, {"title": "3.3.4 Global Mean Pooling Operation", "content": "The global mean pool operation for a graph G is given as follows,\n$r = \\frac{1}{N}\\sum_{n=1}^{n=N} x_n$\nwhere x\u2099 refers to the node features of node n and N is the total number of nodes in graph G"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setting", "content": ""}, {"title": "4.1.1 Datasets", "content": "In our experiments, we evaluate our proposed framework on six publicly available benchmark datasets for node representation learning and classification. The datasets are, namely, Cora([17]), Pubmed([18]), Citeseer([17]), Amazon Photos([19]), Amazon Computers([19]) and Coauthor CS([19]). Cora consists of 2708 scientific publications classified into one of seven classes, Citeseer consists of 3312 scientific publications classified into one of six classes and Pubmed consists of 19717 scientific publications pertaining to diabetes classified into one of three classes. In Amazon Photos and Amazon Computers, nodes represent goods and edges represent that two goods are frequently bought together. The product reviews are represented as bag-of-words features. Coauthor CS contains paper keywords for each author's papers. Nodes represent authors that are connected by an edge if they co-authored a paper."}, {"title": "4.1.2 Evaluation Methodology", "content": "The embeddings are evaluated on node classification, whose performance is quantified by accuracy. We follow the same evaluation protocol as used in [20], [12], [11] etc."}, {"title": "4.1.3 Evaluation Protocol", "content": "For all downstream tests, we follow the linear evaluation protocol on graphs where the parameters of the back- bone/encoder are frozen during inference time. Only the prediction head, which is a single GCN layer, is trained for node classification. For evaluation purposes, we use the default number for train/val/test splits for the citation networks i.e. Cora, Pubmed, Citeseer which are 500 validation and 1000 testing nodes. The train/val/test splits for the remaining datasets, namely, Amazon Photos, Amazon Computers and Coauthor CS are according to [19]. Unless otherwise mentioned, performance is averaged over 10 independent runs with different random seeds and splits for all six evaluation datasets. We report the mean and standard deviation obtained across 10 runs."}, {"title": "4.1.4 Environment", "content": "To ensure a fair comparison, for all datasets, we use the same number of layers for the GNN encoder and the same number of features at each hidden layer. The encoder predicts 512 hidden features in the latent space for all datasets. The number of output features of the final classification head is the only parameter varied. We use a Cosine Annealing learning rate scheduler with warm restarts after 75 epochs, and early stopping is employed for all experiments."}, {"title": "4.2 Evaluation Results", "content": ""}, {"title": "4.2.1 Performance on Node Classification", "content": "As mentioned earlier, we use the linear evaluation protocol and report the mean and standard deviation of classification accuracy on the test nodes over 10 runs on different folds or splits with different seeds. We compare our proposed approach with supervised and fine-tuned model baselines. For supervised node classification, we compare our proposed approach against Multi Layer Perceptron(MLP),Graph Convolution Network(GCN)([21]), Graph Attention Network(GAT)([20]), Simplified GCN([22]), Logistic Regression and GraphSAGE([23]). For GraphSAGE, we use the mean, maxpool and meanpool variants as described in [19]. We have compared our model's performance against supervised baselines in table 1. For self-supervised and fine-tuned node classification, we compare our proposed approach against DGI, MVGRL([5]), GRACE([6]), CCA-SSG([9]), SUGRL([24]), S3-CL([12]), GraphMAE([10]), GMI([25]), BGRL([13]) and ParetoGNN([11]). Table 2 contains our model's performance against self-supervised baselines. The proposed approach demonstrates superior performance across both small and large datasets, showcasing its scalability and robustness. In supervised node classification (Table 1), our method achieves the highest accuracy on Cora (89.8 \u00b1 0.9), Citeseer (77.0 \u00b1 0.9), and Amazon Photos (94.5 \u00b1 0.5), outperforming state-of-the-art models like BGRL and GraphMAE. Specifically, compared to BGRL, our approach shows significant improvements on Cora and Citeseer, while maintaining competitive results on Pubmed and Coauthor CS. Table 3 highlights a direct comparison with BGRL and ParetoGNN, where our method outperforms both on Photos (94.5 \u00b1 0.5) and CS (93.6 \u00b1 0.4), and remains competitive on Computers. These results underscore the approach's adaptability and strong performance across diverse dataset sizes, particularly excelling over BGRL and ParetoGNN in key benchmarks. Owing to the ability of our proposed approach to avoid noisy features, representation collapse, and leverage semantic information, our proposed model performs well on small and unstable datasets such as cora and citeseer and large datasets such as Amazon and Coauthor."}, {"title": "4.2.2 Study on GMM Cluster Optimization", "content": "In this section, we evaluate the significance of the constraint in our loss function. Our approach combines learning sub-graph embeddings with optimizing node embeddings using pseudo-labels derived from a Gaussian Mixture Model (GMM). These pseudo-labels, converted to normalized scores, serve as a constraint in the original loss function for subgraph embeddings. Table 4 illustrates the performance of our method with and without this constraint, demonstrating a substantial improvement when the GMM-derived pseudo-label scores are incorporated. This finding is particularly noteworthy as it indicates that computationally expensive processes like negative sampling, commonly used in graph contrastive learning, may not be essential for optimizing node embeddings. Our approach thus offers a more efficient alternative while maintaining, and even enhancing, performance in graph representation learning tasks."}, {"title": "4.2.3 Study on Momentum Parameter m", "content": "Figure 2 represents the change in performance with respect to the momentum parameter on the citeseer dataset. The ideal values for m may range between 0.8 and 0.9. As m \u2192 1, the weights of the target encoder update by very small steps, thereby slowing down learning. This is further indicated by the drop in performance. For all our experiments, we use m = 0.9"}, {"title": "4.2.4 Spread of Embeddings", "content": "Figure 3 represents the spread of node embeddings across all 6 tested datasets. As mentioned earlier, the node embeddings are bounded between -1 and 1 since the final layer activation function is Tanh. The plots indicate that the node embeddings are evenly distributed with a high dispersion indicating a higher variance, implying that the embeddings span a wide range of values. This is a key indicator that our proposed method effectively prevents node representations from clustering too closely, ensuring that each node maintains a distinct position within the embedding space. This controlled spread minimizes overlap between node embeddings, thereby enhancing the uniqueness and robustness of the overall representation."}, {"title": "4.2.5 Study on Performance against Test Time Node Feature Distortion", "content": "We evaluate the performance of our method on node classification by augmenting the test splits. We sample p percentage test nodes randomly and replace their node features with points sampled from a standard Gaussian distribution. We evaluate the performance of the distorted test nodes on Amazon Computers, Amazon Photos and Coauthor CS by varying p between 10% to 40% and report their performance differences in table 5. In this study, we only use Co-author and Amazon networks since citation networks tend to be unstable on evaluation as indicated in [19]. Despite no further fine-tuning on distorted node features, the average performance drop on Amazon Photos and Computers is only 2.96% and 4.01% respectively. This indicates the ability of the joint framework to generalize on noise-augmented views as well. According to our hypothesis, we believe this is due to the nature of joint predictive embedding learning where a higher variation in context and target embeddings is required for better generalization."}, {"title": "5 Conclusion", "content": "In this paper, we propose a novel Graph Self-Supervised Learning (Graph-SSL) framework that combines joint predictive embedding and pseudo-labeling to effectively capture global knowledge while avoiding noisy features and bypassing contrastive methods such as negative sampling and reconstruction. The joint predictive embedding framework leverages the context-target relationship between node embeddings in the latent space by predicting multiple target embeddings for a single context. This approach, combined with the optimization of node feature contributions to pseudo-labels, enables a lightweight Graph Neural Network (GNN) encoder to capture intricate patterns in both graph structure and node features without requiring the stacking of multiple layers or encoders. Additionally, our method addresses the node representation collapse problem by incorporating information from multiple targets for a single context, ensuring robust and diverse embeddings. Through extensive experiments on multiple benchmark graph datasets, we demonstrate that our proposed framework achieves superior performance compared to several state-of-the-art graph self-supervised learning methods."}]}