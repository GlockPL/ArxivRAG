{"title": "Panacea: A foundation model for clinical trial search, summarization, design, and recruitment", "authors": ["Jiacheng Lin", "Hanwen Xu", "Zifeng Wang", "Sheng Wang", "Jimeng Sun"], "abstract": "Clinical trials are fundamental in developing new drugs, medical devices, and treat-ments. However, they are often time-consuming and have low success rates. Although there have been initial attempts to create large language models (LLMs) for clinical trial design and patient-trial matching, these models remain task-specific and not adaptable to diverse clinical trial tasks. To address this challenge, we propose a clinical trial founda-tion model named Panacea, designed to handle multiple tasks, including trial search, trial summarization, trial design, and patient-trial matching. We also assemble a large-scale dataset, named TrialAlign, of 793,279 trial documents and 1,113,207 trial-related scien-tific papers, to infuse clinical knowledge into the model by pre-training. We further curate TrialInstruct, which has 200,866 of instruction data for fine-tuning. These resources enable Panacea to be widely applicable for a range of clinical trial tasks based on user requirements.\nWe evaluated Panacea on a new benchmark, named TrialPanorama, which covers eight clinical trial tasks. Our method performed the best on seven of the eight tasks compared to six cutting-edge generic or medicine-specific LLMs. Specifically, Panacea showed great potential to collaborate with human experts in crafting the design of eligibility criteria, study arms, and outcome measures, in multi-round conversations. In addition, Panacea achieved 14.42% improvement in patient-trial matching, 41.78% to 52.02% improvement in trial search, and consistently ranked at the top for five aspects of trial summarization. Our approach demonstrates the effectiveness of Panacea in clinical trials and establishes a comprehensive resource, including training data, model, and benchmark, for developing clinical trial foundation models, paving the path for AI-based clinical trial development.", "sections": [{"title": "Introduction", "content": "Clinical trials are research studies conducted on humans to evaluate the safety and efficacy of new medical treatments, interventions, or devices before they are approved for widespread use. They form the foundation of modern medicine. 1-5 The challenges in clinical trials are three-fold. First, a clinical trial involves several interconnected design components, including trial descriptions, eligibility criteria, study arms, outcome metrics, and more, that need to be collectively designed to ensure optimal patient recruitment and outcome assessment. Second, clinical trial data are usually highly sensitive and private, hence often not amenable to pubic cloud-based tools (e.g., GPT-46) for processing and analysis. Third, clinical trial develop-ment requires multiple tasks, such as eligibility criteria design and patient recruitment, which require substantial domain expertise.\nMachine learning models have shown promise in improving clinical trial development. 7-12 However, current models are often specialized for specific tasks, leading to challenges in man-aging the resulting models and utilizing training data effectively across interconnected clinical trial activities. Recently, foundation models have been highlighted as the generalist AI that can solve multiple tasks in many biomedical domains. 13-19 For example, GPT-4 was used to assist clinical trial design and trial-patient matching. 7, 20-22 We thus hypothesize that a small but specialized clinical trial foundation model could be a Swiss Army Knife tool that simultaneously addresses multiple clinical trial tasks.\nWe present Panacea, a clinical trial foundation model that can address eight clinical trial tasks, including trial design, patient-trial matching, trial search, and trial summarization. The training of Panacea consists of an alignment step and an instruction-tuning step. During the alignment step, we train Panacea from a general-domain model using a large collection of trial documents and trial-related scientific papers. This step adapts Panacea to the vocabulary commonly used in clinical trials. To conduct the alignment, we create the TrialAlign dataset from diverse resources, covering a comprehensive set of indications and medications for any clinical trial. The instruction-tuning step further enables Panacea to comprehend the user explanation of the task definition and the output requirement. By leveraging our curated TrialInstruct dataset, Panacea can handle multiple clinical trial tasks without needing to re-train.\nWe compared Panacea to six cutting-edge large language models on a new clinical trial benchmark TrialPanorama. This benchmark covers eight tasks spanning trial design, patient-trial matching, trial search, and trial summarization. Our experiments showed that Panacea can facilitate experts through conversations, leading to superior design of eligibility criteria, study arms, and outcome measures. Especially on patient-trial matching, we found that our method achieved, on average, 14.42% F1 improvement on two datasets. On trial search, Panacea obtained a 41.78% improvement in query generation and a 52.02% improvement in query expansion. Finally, we propose evaluating trial summaries based on the alignment of their trial goals, conclusions, and keywords with reference summaries. We found that Panacea yield the best performance for the challenging multi-trial summarization tasks.\nWe have made all our training datasets (TrialAlign and TrialInstruct) and the evalu-ation benchmark (TrialPanorama) available for future research and benchmarking of clinical trial foundation models. Additionally, we have open-sourced the code and model weights of Panacea. Panacea can run on a single-GPU machine, making it easy to use within an organization. Fine-tuning Panacea on 200 thousand documents only takes seven hours using a standard cluster with 4 A-100 GPUs. This advantage allows for further customization of Panacea on local proprietary data using limited computational resources."}, {"title": "Results", "content": "Our goal is to develop Panacea, a domain-specific foundation model for clinical trial tasks. Like previous works on developing domain-specific foundation models, 23, 24 the biggest chal-lenge for developing Panacea is to curate the high-quality fine-tuning data to align Panacea to clinical trial vocabulary and create the specific instruction data for clinical trial tasks. Panacea consists of two main steps: an alignment step, which adapts Panacea to the vocab-ulary used in clinical trials, and an instruction-tuning step, which instructs Panacea on each clinical trial task. We built two datasets TrialAlign and TrialInstruct for the alignment step and the instruction-tuning step, respectively.\nTrialAlign consists of 793,279 de-identified trial documents collected from 14 diverse sources and 1,113,207 scientific papers related to clinical trials (see Methods), representing a large-scale collection of clinical trial documents. By classifying these trial documents to terms in the International Classification of Diseases (ICD-10) ontology, we found that at least 100 conditions have 10,000 documents (Fig. 1a), indicating the good coverage of our dataset. Likewise, by classifying trial-related scientific papers to Medical Subject Headings (MeSH) terms, we found that at least 119 terms have more than 10,000 papers and at least 1,921 terms have more than 1,000 papers (Fig. 1b). The scale and the coverage of TrialAlign enable Panacea to be generalized to various conditions and treatments.\nTrialInstruct contains instruction-tuning data from eight diverse tasks, including crite-ria design, study arm design, outcome measure design, patient-trial matching, query genera-tion, query expansion, single-trial summarization, and multi-trial summarization, instructing Panacea on solving these tasks (Fig. 1c). Each task contains at least 2,000 data points, where each data point contains an instruction, an input, and an output (Fig. 1d). Since these eight tasks are related, we jointly fine-tuned the model using instruction data from these eight tasks, transforming Panacea into an all-in-one tool for clinical trial applications (Fig. le).\nTo evaluate Panacea, we built the first large-scale benchmark TrialPanorama that covers eight specific tasks in clinical trials (Table 1). Since these tasks contain both classification and generation tasks, TrialPanorama allows us to evaluate Panacea in various machine learning settings. We made this benchmark fully open-source."}, {"title": "Accurate trial search through query generation and expansion", "content": "Clinical trial search is an important task for clinical trial design and research. Trial designers often need to study similar trials to ensure their design aligns with existing trials. The goal of the trial search is to find relevant trials based on user inputs, which serves as the foundation for designing and matching trials. The key to a successful trial search is to create comprehensive search terms. As a result, we evaluate query generation, which converts unstructured user input to a list of keywords (Fig. 2a), and query expansion, which further expands this keyword list to relevant terms (Fig. 2b). These two tasks assess the ability to derive high-quality queries based on user intent, which is crucial for a successful trial search.\nWe first evaluated query generation by formulating it as a text classification problem that classifies user inputs into specific diseases, interventions, phases, status, and study types. We found that Panacea substantially outperformed existing approaches regarding the Jaccard index (Fig. 2d). The improvement is larger on diseases and interventions, which are more challenging due to the large number of classes in these two categories (Fig. 2c), indicating that Panacea can accurately convert user inputs into the structured format that is compatible with downstream machine learning classifiers.\nNext, we evaluated query expansion by formulating it as a text generation problem. We did not provide the candidate keywords to the models since real-world keywords might have never been seen in the training trials. Similar to our observations in the query generation, Panacea achieved the best results on query expansion in terms of Jaccard index (Fig. 2e). We attribute the inferior performance of existing models on query expansion to the lack of fine-tuning on trial-related datasets. In contrast, Panacea is fine-tuned on TrialAlign, adapting it to the vocabulary used in clinical trials. The promising results of Panacea on query expansion and generation demonstrate its ability to precisely understand user intent, providing an accurate tool for finding relevant clinical trials."}, {"title": "A novel metric to evaluate trial summarization", "content": "Once similar trials are identified, the next task is to understand those trials via summarization. We evaluated the performance of Panacea on trial summarization. We studied both single-trial summarization, which aims to provide a concise summary of a specific trial study (Fig. 3a), and multi-trial summarization, which aims to summarize multiple trial studies that study similar conditions and interventions (Fig. 3b).\nSince it could be biased to evaluate summarization using lexical-based metrics, we propose a novel metric based on large language models (see Methods, Supplementary Figures 1 and 2). In particular, we provided the ground truth summarization and the model-generated summarization to Claude and asked if these summarizations studied the same problem and made the same conclusion. We found that Panacea and comparison approaches can correctly summarize the trial goal, while the summarization of the trial conclusion is less accurate (Fig. 3c-d). Moreover, summarizing multiple trials is more challenging than summarizing a single trial based on the proposed metric. Nevertheless, our method still outperformed comparison approaches in summarizing multiple trials, suggesting its potential to assist researchers in extracting key information from many related trial studies.\nWe further used query generation and query expansion to evaluate trial summarization by extracting diseases, and interventions, and expanding them (Fig. 3c-d) from each trial. We examined whether the generated summarization can derive the same keywords as the ground truth summarization. We found that Panacea achieved the best performance on three of the six keyword categories while achieving comparable on the other categories. Moreover, we calculated the ROUGE score, which is used as the metric for trial summarization in previous works, 25, 26 and observed improved performance by Panacea as well on multi-trial"}, {"title": "Improved performance on clinical trial design", "content": "The first step toward a successful trial execution is designing a detailed trial protocol synopsis. We evaluated Panacea on three tasks in trial design (See examples in Fig. 4a): Criteria design defines the eligibility criteria (i.e., the inclusion and exclusion criteria) for patient recruitment; Study arm design outlines the different treatment arms that will be applied to different patient subgroups; Outcome measures design specifies the metrics that are used to assess the trial success. We formulated these three tasks as a conditional text generation problem, which takes conditions, treatments, and the design of previous steps (e.g., reference criteria are used to generate study arms) as inputs to generate specific design text.\nBecause trials are described in plain text, we first exploited standard natural language processing metrics BLEU and ROUGE to evaluate the lexical similarity. We found that Panacea attained the best performance on all three clinical trial design tasks in terms of BLEU and ROUGE (Fig. 4b). First, we observed that Panacea substantially outperformed general-domain models, including our base model Mistral, 27 confirming the benefit of fine-tuning using clinical trial-related data. Second, we found that Panacea improved the study arm design more than the other two tasks. Compared to criteria and outcome measures, study arm descriptions are more customized according to the disease and the treatment. The larger improvement of Panacea on study arms design demonstrates Panacea's strong gener-alization ability. Finally, BioMistral, 28 which is fine-tuned on general biomedical data, also outperformed Mistral, further demonstrating the value of domain-specific data. Neverthe-less, Panacea still outperformed BioMistral by fine-tuning using our clinical trial-specific data TrialAlign and TrialInstruct, suggesting that data with improving domain specificity leads to better performance.\nLexical similarity metrics are widely used to evaluate text generation problems, but might not be clinically specific enough to evaluate the generations by Panacea. Recently, LLMs have been used to evaluate the generated text by exploiting their strong ability in text un-derstanding. Here, we exploit Claude 29 to evaluate these three tasks by asking the model whether the generated task is clinically relevant (see Methods, Supplementary Figures 3-5). We found that Panacea outperforms all methods on criteria and study arms design, demonstrating the high quality of generation by Panacea (Fig. 4b).\nMoreover, we examined a De Novo generation setting, using the generated output in the previous step as the input for the next step. For example, we used the generated criteria instead of the reference criteria as the input for generating study arms. De Novo generation frees users from providing any descriptions for the trial. We found that the performance of all methods dropped in this setting compared to the setting that utilizes reference input (Fig. 4c). Nevertheless, our method still outperforms all existing methods by a large margin, indicating its superior performance on this De Novo trial design. We further compared the generated text by three methods with the ground truth text on criteria design, where only Panacea can generate the correct criteria (Fig. 4d). Collectively, the promising performance of Panacea demonstrates its potential to automate clinical trial design."}, {"title": "Accurate patient-trial matching", "content": "We next evaluate the performance of Panacea on patient-trial matching. Given a patient note and a trial description, we aim to determine whether this patient is eligible for the trial by"}, {"title": "Discussion", "content": "In this paper, we introduce a specialized foundation model called Panacea for use in clin-ical trials. We tested Panacea in eight different clinical trial tasks, including trial design, patient-trial matching, trial search, and trial summarization. In comparison to other gen-eral domain foundation models and biomedical foundation models, Panacea demonstrated state-of-the-art performance across all eight tasks. We believe that the impressive per-formance of Panacea can be attributed to the fine-tuning process using TrialAlign and TrialInstruct. TrialAlign comprises a large collection of trial documents and papers from various areas, allowing Panacea to be applied to different conditions and treatments. Meanwhile, TrialInstruct contains 200,866 instructions curated from existing databases, effectively guiding Panacea in each task. Furthermore, we have developed a clinical trial benchmark TrialPanorama and a language model-based metric for evaluating trial summa-rization. Together, these resources offer an end-to-end solution for AI-based clinical trial development.\nThe rapid development of large language models (LLMs) has enabled their potential as foundational models for medical tasks.14 Current efforts predominantly follow two strate-gies: fine-tuning general domain LLMs with medical domain datasets, 33-35 and instructing a general domain LLM with a description of the target tasks and showing example inputs and outputs (referred to as \u201cprompting\").36-38 The MedPaLM model is a prime example of the first approach, illustrating how fine-tuning a general domain model on medical datasets can markedly enhance its ability to answer medical questions. 34 This success has inspired further research into fine-tuning LLMs for specific clinical trial tasks, such as generating eligibility cri-teria. Moreover, it has been demonstrated that generalist LLMs can be effectively adapted to medical tasks through strategic prompting.38 In the direction of prompting, TrialGPT showcased that GPT-4 can be adapted to predict patient eligibility for clinical trials through prompting.20 However, these approaches either do not address clinical trial tasks or focus on individual clinical trial-related tasks. In contrast, Panacea outlines a comprehensive range\""}, {"title": "Method", "content": "Creating TrialAlign dataset\nData collection We first collected trial documents (English version) from 14 sources, as shown in Supplementary Table 1. Each clinical trial data consists of various parts that encapsulate the essence of the study. For instance, the Study Overview provides a general summary and a detailed description of the trial, along with its official title and the health con-ditions being targeted. The Intervention/Treatment section describes the medical approach or therapy being tested. The Eligibility Criteria outlines who can participate, detailing the eligibility requirements, age, and sex specifications, and whether healthy volunteers are ac-cepted. The Study Plan delves into the methodology, explaining the design of the study, the types of interventions and arms involved, and the outcomes being measured, both primary and secondary. This structured approach ensures a comprehensive understanding of the trial's scope, methodology, and intended outcomes. We then collected trial papers in two databases, i.e., Embase and PubMed, from Cochrane Library's trial section.44 These papers provide a rich foundation of medical knowledge and evidence-based findings beneficial to the model's learning.\nFiltering For trial documents, we further conduct intra- and inter-source de-duplication and then remove the personally identifiable information (PII), finally obtaining 793k trial docu-ment data. Further, to avoid information leakage, we selected documents with registration dates before 2023-01-01 as the training corpus. The remaining is used for test data curation. For trial papers, we de-duplicated all the papers and the final 1.11M trial paper corpus con-sists of abstracts of all the papers and full text of 97k papers from PubMed Central (PMC). Similarly, to avoid information leakage, we choose papers published before 2023-01-01, which ensures the dates of related clinical trials of the selected papers are definitely before 2023-01-01.\nDocument/paper structure organization For trial documents, we follow the format shown in clinicaltrial.gov45 to organize all the corpus for alignment. Each trial document is arranged into a markdown format passage. For trial documents from clinicaltrial.gov, each document contains section (1) Public Title; (2) Study Overview covering subsections Brief Summary, Detailed Description, Official Title, Conditions and Intervention/Treatment; (3) Participation Criteria, including subsections Eligibility Criteria, Ages Eligibility for Study, Sexes Eligibility for Study and Accepts Healthy Volunteers; (4) Study Plan, including sub-section How is the study designed? that contains Design Details and Arms and Interventions, subsection What is the study measuring? containing primary and secondary outcome mea-sures; (5)Terms related to the study. For trial documents from other sources, each document contains Public Title, Scientific Title, Study Type, Study Design, Intervention, Inclusion Cri-teria, Exclusion Criteria, Primary Outcome Measures and Secondary Outcome Measures. For trial paper data, each paper contains Title, Abstract and full text (if any).\nCreating TrialInstruct dataset\nThe aim of constructing TrialInstruct is to provide Panacea with the ability to follow human instructions, especially in clinical trial domains.\nTrial search Trial search includes query generation and query expansion. To construct instruction data for query generation, we leverage GPT-3.5 to generate 2,161 samples for training and 925 for the test. Specifically, we first manually construct 20 seed data about query generation customized for clinicaltrial.gov database API, and then leverage GPT-3.5 to generate the data. We will remove data similar to the original data and add them to the seed dataset to repeat the above process (see prompt in Supplementary Figure 6). In"}, {"title": "Details of Panacea model", "content": "In this section, we detail the techniques in Panacea, including the alignment and instruction finetuning steps.\nAlignment We built on the Mistral-7B-Base model27 in this study. After parameter initial-ization, Panacea was trained on the 1.8M TrialAlign data. We trained the model using the AdamW optimizer47 with a batch size 512 for one epoch. We adopted a cosine learning rate scheduler with a peak learning rate 2 \u00d7 10-6 and 10% warm-up steps. We set max sequence length as 8192 tokens. To improve training speed and optimize the memory, we adopted DeepSpeed ZeRO-348 and FlashAttention-249 strategies. After the alignment process, we ob-tain the Panacea-Base model. During the alignment step, Panacea was trained on 4 Nvidia A100 80G for four days.\nInstruction tuning We further finetuned Panacea-Base on the TrialInstruct datasets, leading to the Panacea model. We trained our Panacea for one epoch with a batch size 256. Similar to the alignment step, we also leveraged a cosine learning rate scheduler with a peak learning rate as 2 \u00d7 10-5 and 10% warm-up steps. The max sequence length is set as 2048. Deep ZeRO-3 and FlashAttention-2 techniques are also adopted in the instruction tuning phase."}, {"title": "Details of experiments on trial search", "content": "In the trial search experiments, we focused on optimizing Panacea for two tasks: query gen-eration and query expansion (see Supplementary Figure 11). These two tasks are pivotal for enhancing the efficiency and precision of searches within large clinical trial databases.\nQuery generation in this context essentially functions as a Named Entity Recognition (NER) task where the model identifies and categorizes key pieces of information from the trial descriptions relevant to user queries. To facilitate the generation of structured queries in a JSON format, we employed a specialized tool called JsonFormer.50 This tool is instrumental in guiding the model to generate content for each key in the JSON structure sequentially.\nOnce the JSON format is generated, it is automatically converted into a Search Expression using a rule-based system. The conversion rules are straightforward: within the same key, terms are combined using the OR operator, and between different keys, the terms are combined using the AND operator. This structured approach ensures that the generated queries are precise and align well with the syntactical requirements of the search engines used in clinical trial databases.\nFor the query expansion task, this process enhances the original query by adding seman-tically related terms, thereby broadening the search scope to include relevant trials that may not use the exact phrasing of the original query terms. Panacea was trained to suggest addi-tional keywords based on the initial input terms. The model learned to recognize and predict related terms that could be associated with the initial query, expanding the search breadth effectively."}, {"title": "Details of experiments on trial summarization", "content": "The experiments on trial summarization were designed to test Panacea's capabilities in con-densing complex clinical trial information into succinct summaries. This component of our research focused on two specific tasks: single-trial summarization and multi-trial summariza-tion (see Supplementary Figure 12).\nTo evaluate summarization tasks, we propose a novel metric based on Claude 3. We use Claude 3 to decide whether the model-generated summarization and the ground truth sum-marization studied the same problem and made the same conclusion, following prompts in Supplementary Figure 1 and 2. Specifically, Claude 3 directly outputs the goal align-ment results for each test sample. For conclusion consistency, we first use Claude to evaluate model-generated summaries and ground truth summaries, respectively. Then, we calculate the matching accuracy between the model-generated summarization and ground truth sum-marization."}, {"title": "Details of experiments on clinical trial design", "content": "In our experimental setup for evaluating the Panacea model's capabilities in clinical trial design, we utilized a multi-turn conversation format for the test data. This format consists of sequential (user, chatbot) pairs, reflecting a realistic interaction scenario where the model, acting as a chatbot, responds to user queries about designing a trial. The initial three rounds usually provide essential background information related to the trial design, such as the trial's objectives, target population, and key endpoints. These initial conversations set the stage for the more complex interactions that follow. Starting from the fourth round of conversation, the model is tasked with predicting the chatbot's responses based on the cumulative conversation history, which tests the model's ability to maintain context and continuity over successive interactions.\nTo ensure the reliability of the experimental results and prevent the propagation of errors through the conversation chain, a teaching forcing strategy was implemented: regardless of the model's output in any given round, the subsequent round's input incorporates the groundtruth from the previous rounds rather than the model-generated responses. This method allows the model to be evaluated on its ability to adhere closely to a scientifically valid trial design path without being influenced by potential errors in its previous outputs.\nTo assess the relevance between models' designed trials and ground truth, we employ Claude 3 to calculate clinical relevance. Specifically, we input each model's output and the ground truth into Claude 3 to determine the relevance of the information generated by the model compared to the ground truth. The inputs to Claude 3 for clinical relevance evaluation are detailed in Supplementary Figures 3, 4, and 5, respectively. When a model's outputs are relevant to the ground truth, Claude will output a 1; otherwise, it outputs a 0. We then calculate the clinical relevance using the following formula:\nClinical relevance =  \u2211(Relevance scores)/ N    (1)\nHere, \"Relevance scores\" refer to the series of 1s and Os output by Claude 3 for each comparison between a model's output and the ground truth. N is the total number of outputs evaluated. This proportion reflects the percentage of times the model's output was deemed clinically accurate relative to the ground truth, quantifying the frequency at which the model produces clinically relevant information."}, {"title": "Details of experiments on patient-trial matching", "content": "In the patient-trial matching experiments, we employed a distinctive approach to training the Panacea model, focusing not on utilizing the entirety of the training data but rather on a selected subset. Initially, all available training data was subjected to a filtering process with Claude 3 Haiku. This involved predicting responses for each instance in the training set. Only those instances where Claude 3 Haiku's predictions were accurate were retained for further processing. The rationale was to ensure that the model was learning from correctly reasoned examples and that the training data was high quality. The responses generated by Claude 3 Haiku, which correctly matched the groundtruth data, were then used as the new training corpus for Panacea. This step was crucial because the standard training datasets for patient-trial matching typically include labels indicating eligible or excluded but lack a detailed reasoning process for these outcomes. By incorporating Claude 3 Haiku's responses, which involve step-by-step reasoning based on the input data, we injected reasoning capabil-ities into Panacea during the training process. Through this innovative training approach, Panacea showed superior performance in patient-trial matching tasks. The ability to rea-son and logically process eligibility criteria translated into higher accuracy and reliability in matching patients to appropriate trials. The evaluation prompt for patient-trial matching can be seen in Supplementary Figure 10.\nThe patient-trial matching is a three-class classification task for both SIGIR and TREC2021 datasets. Three classes for SIGIR are: 0) Would not refer this patient for this clinical trial; 1) Would consider referring this patient to this clinical trial upon further investigation; and 2) Highly likely to refer this patient for this clinical trial, while TREC2021 has: 0) Excluded (patient meets inclusion criteria, but is excluded on the grounds of the trial's exclusion cri-teria); 1) Not relevant (patient does not have sufficient information to qualify for the trial); and 2) Eligible (patient meets inclusion criteria and exclusion criteria do not apply)."}, {"title": "Code and data availability", "content": "The TrialAlign data for the alignment step, the TrialInstruct data for the instruction tuning step, and the TrialPanorama benchmark data are available at https://figshare. com/articles/dataset/TrialAlign/25989403, https://doi.org/10.6084/m9.figshare. 25990090.v1, and https://doi.org/10.6084/m9.figshare. 25990075, respectively. Panacea code is available at https://github.com/linjc16/Panacea."}]}