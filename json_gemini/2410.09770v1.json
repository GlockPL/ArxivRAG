{"title": "'Quis custodiet ipsos custodes?' Who will watch the watchmen? On Detecting AI-generated Peer Reviews", "authors": ["Sandeep Kumar", "Mohit Sahu", "Vardhan Gacche", "Tirthankar Ghosal", "Asif Ekbal"], "abstract": "The integrity of the peer-review process is vital for maintaining scientific rigor and trust within the academic community. With the steady increase in the usage of large language models (LLMs) like ChatGPT in academic writing, there is a growing concern that AI-generated texts could compromise scientific publishing, including peer-reviews. Previous works have focused on generic AI-generated text detection or have presented an approach for estimating the fraction of peer-reviews that can be AI-generated. Our focus here is to solve a real-world problem by assisting the editor or chair in determining whether a review is written by ChatGPT or not. To address this, we introduce the Term Frequency (TF) model, which posits that AI often repeats tokens, and the Review Regeneration (RR) model, which is based on the idea that ChatGPT generates similar outputs upon re-prompting. We stress test these detectors against token attack and paraphrasing. Finally, we propose an effective defensive strategy to reduce the effect of paraphrasing on our models. Our findings suggest both our proposed methods perform better than the other AI text detectors. Our RR model is more robust, although our TF model performs better than the RR model without any attacks. We make our code, dataset, and model public.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs), such as Chat-GPT, PaLM (Chowdhery et al., 2023) and GPT-4 (Achiam et al., 2023), have significantly impacted both the industrial and academic sectors. The surge in Artificial Intelligence (AI)-generated content has permeated various domains, from journalism (Guti\u00e9rrez-Caneda et al., 2023; Shi and Sun, 2024) to academia (Bin-Nashwan et al., 2023; Shi et al., 2023). However, their misuse also introduces concerns especially regarding fake news (Zhang and Gao, 2023; Silva and Vaz, 2024), fake hotel reviews (Ignat et al., 2024), fake restaurant review (Gambetti and Han, 2024). The exceptional human-like fluency and coherence of the generated content of these models pose a significant challenge, even for experts, in distinguishing if the text is written by humans or LLMs (Shahid et al., 2022).\nWhat if peer-reviews themselves are AI-generated? Who will guard the guards themselves?\nA study (Liang et al., 2024a) conducted experiments on a few papers of AI conferences and found that between 6.5% and 16.9% of text submitted as peer-reviews to these conferences could have been substantially modified by LLMs. They estimated that the usage of ChatGPT in reviews increases significantly within three days of review deadlines. Reviewers who do not respond to ICLR/NeurIPS author rebuttals exhibit a higher estimated usage of ChatGPT. Additionally, an increase in ChatGPT usage is associated with low self-reported confidence in reviews. Once Springer retracted 107 cancer papers after they discovered that their peer-review process had been compromised by fake peer-reviewers (Chris Graf, 2022).\nIn recent discussions surrounding the use of large language models (LLMs) in peer reviewing. According to ACL policy, if the focus is strictly on content, it seems reasonable to employ writing assistance tools for tasks such as paraphrasing reviews, particularly to support reviewers who are not native English speakers. However, it remains imperative that the reviewer thoroughly reads the paper and generates the review's content independently. Moreover, it is equally acceptable to use tools that assist with checking proofs or explaining concepts unfamiliar to the reviewer, provided these explanations are accurate and do not mislead the reviewer in interpreting the submission. This blend of automation and human oversight maintains the integrity of the review process while leveraging LLMs for specific enhancements. According to Elsevier policy, reviewers should not upload their communications or any related material into an AI tool, even if it is just for the purpose of improving language and readability. They also emphasize that the critical thinking, original assessment, and nuanced evaluation required for a thorough review cannot be delegated to AI technologies, as these tools might produce incorrect, incomplete, or biased assessments. We believe reviewers should strictly adhere to the conference policy and guidelines regarding the use of AI tools in peer review, including for proofreading their reviews for refinement.\nHowever, to the best of our knowledge, each venue agrees that the content of submissions and reviews is confidential. Therefore, they highly discourage the use of ChatGPT and similar non-privacy-friendly solutions for peer review. Additionally, they agree that AI-assisted technologies must not be used during the initial writing process of reviews. Consequently, our work aims to assist editors in identifying instances where reviewers may have bypassed this crucial step before using Al for refinement.\nPrevious works have focused on studying the effect of ChatGPT on AI conference peer-reviews. However, in this paper, our focus is to determine whether a review is written by ChatGPT or not. We do not assert that AI-generated peer-reviews inherently detract from the quality or integrity of the peer-review system. There can be debates whether AI-generated reviews can help peer-review system or not; we are not asserting that AI-generated peer-review is completely not useful. However, we believe if the review is AI-generated, the chair/meta-reviewer should be well aware. It is a breach of trust if the meta-reviewer believes that the review is human-written; nevertheless, it is not. Despite the potential benefits AI-gener, the chair/meta-reviewerated reviews may offer, it is crucial for editors to exercise discernment in their reliance on these reviews. This caution is warranted due to the intrinsic limitations of current language models, which can produce inaccurate, misleading (Pan et al., 2023), or entirely fabricated information\u2014a phenomenon often referred to as hallucination (Ji et al., 2023; Rawte et al., 2023).\nIn this paper, we propose two simple yet effective methods for detecting AI-generated peer reviews based on token frequency (TF method) and regeneration based approach (RR method). We also propose a token modification attack method and study its effect on various detectors. Paraphrasing attack is a very common way to evade text detection. So, we also study the effect of paraphrasing on various text detectors. Finally, we propose a technique to defend our regeneration-based technique against the paraphrasing attack. We found that both the TF model and the RR model perform better than other AI text detectors for this task. We also found that while the TF model performs better than the RR model under normal conditions, the RR model is more robust and is able to withstand adjective attacks and paraphrasing attacks (after the defense is applied).\nWe summarize our contributions as follows:-\n\u2022 We introduce a novel task to address the real-world problem of detecting AI-generated peer-reviews. We create a novel dataset of 1,480 papers from the ICLR and NeurIPS conferences for this task.\n\u2022 We propose two techniques, namely the token frequency-based approach (TF) and the regeneration-based approach (RR), which perform better than the existing AI text detectors.\n\u2022 We stress-test the detectors against token attacks and paraphrasing, and propose an effective defensive strategy to reduce evasion during paraphrasing attacks."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Zero-Shot Text Detection Detection", "content": "Zero-shot text detection does not require training on specific data and directly identifies AI-generated text using the model that produced it (Mitchell et al., 2023). (Solaiman et al., 2019) use average log probability of a text under the generative model for detection, whereas DetectGPT (Mitchell et al., 2023) uses property of AI text to occupy negative curvature regions of model's log probability function. Fast-DetectGPT (Bao et al., 2023a) increases its efficiency by putting conditional probability curvature over raw probability. Tulchinskii et al. (2023) showed that the average intrinsic dimensionality of AI-generated texts is lower than that of human. The paper (Gehrmann et al., 2019) estimates the probability of individual tokens and detect AI-generated text by applying a threshold on probability."}, {"title": "2.2 Training based Text Detection", "content": "Some researchers have fine-tuned language models to recognize LLM-generated text. Guo et al. (2023) trained OpenAI text classifier on a collection on millions of text. GPT-Sentinel (Chen et al., 2023) train ROBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) classifiers on OpenGPT-Text. LLM-Pat (Yu et al., 2023) trained a neural network on the similarity between candidate texts and reconstructed sibling text generated by an intermediary LLM (parent). However, due to excessive reliance of this model on training data, many models show vulnerability to adversarial attacks (Wolff, 2020)."}, {"title": "2.3 LLM Watermarking", "content": "The concept of watermarking AI-generated text, initially introduced by (Wiggers, 2022), involves embedding an undetectable signal to attribute authorship to a particular text with a high level of confidence, which is similar to encryption and decryption. In simple words, a watermark is a hidden pattern in text that is imperceptible to humans. It involves adding some kind of pattern which can be recognized by algorithms directly into the text and some techniques also involve integrating an machine learning model in the watermarking algorithm itself (Abdelnabi and Fritz, 2021; Munyer and Zhong, 2023; Yoo et al., 2023; Qiang et al., 2023).\nWatermarked text can be generated using a standard language model without re-training (Kirchenbauer et al., 2023). It planted watermarks with large enough entropy, resulting in a change in the distribution of generated texts. Zhao et al. (2023) proposed a method of injecting secret sinusoidal signals into decoding steps for each target token. However, Singh and Zou (2023) addresses the issue that watermarking can compromise text generation quality, coherence, and depth of LLM responses. Chakraborty et al. (2023a) suggests that watermarked texts can be circumvented and paraphrasing does not significantly disrupt watermark signals; thus, text watermarking is fragile and lacks reliability for real-life applications."}, {"title": "2.4 Statistical Estimation Approach", "content": "There have been inquiries into the theoretical feasibility of achieving precise detection on an individual level (Weber-Wulff et al., 2023; Sadasivan et al., 2023a; Chakraborty et al., 2023b). (Liang et al., 2024a) presented an approach for estimating the fraction of text in a large corpus using a maximum likelihood estimation of probability distribution without performing inference on an individual level thus making it computationally efficient. They conducted experiments on papers from a few AI conferences to determine the fraction of peer-reviews that could have been substantially modified by LLMs."}, {"title": "2.5 AI-generated Research Paper Detection", "content": "The DagPap22 Shared Task (Kashnitsky et al., 2022) aimed to detect automatically generated scientific papers. The dataset includes both human-written and likely AI-generated texts, with around 69% being \"fake,\" some generated by SCIgen. The winning team (Rosati, 2022) utilized a DeBERTa v3 model that was fine-tuned on their dataset (almost all teams managed to surpass the baseline models, Tf-IDF and logistic regression). It was also concluded that machine-generated text detectors should not be used in production because they perform poorly with distribution shifts, and their effectiveness on realistic full-text scientific manuscripts remains untested.\nThere have been many efforts to improve the peer review process (Kumar et al., 2021; Ghosal et al., 2022; Kumar et al., 2022; Li et al., 2020; Kumar et al., 2023b,a; Kang et al., 2018; Kumar et al., 2023c; D'Arcy et al., 2024); however, AI-generated review text poses a significant challenge. To the best of our knowledge, we are the first to propose techniques specifically for the detection of AI-generated peer reviews."}, {"title": "3 Dataset", "content": "We collected a total of 1,480 papers from Open-Review Platform . The first version of ChatGPT was released by OpenAI on November 30, 2022. Therefore, we choose papers from 2022, ensuring there was almost no chance that any of the collected reviews were already generated by ChatGPT."}, {"title": "4 Methodology", "content": "In this section, we present our two approaches to detect AI-written peer-reviews based on token frequency (Section 4.1) and review regeneration (Section 4.2). Then, we propose a possible attack (Token Manipulation Attack) on the AI text detectors to see how various models react to it in Section 4.3. Additionally, since paraphrasing is a common method used to circumvent AI text detection, we introduce a countermeasure as described in Section 4.4, designed to protect our proposed Review Regeneration method against such attacks."}, {"title": "4.1 Token Frequency based Approach", "content": "Inspired by (Liang et al., 2024b), we propose a method that utilizes the frequency of tokens within review texts. This approach is premised on the hypothesis that different types of reviews (human-generated vs. AI-generated) exhibit distinct patterns in the usage of certain parts of speech, such as adjectives, nouns, and adverbs.\nLet $H$ denote the human corpus, consisting of all human-generated reviews, and $A$ represent the Al corpus, comprising of all AI-generated reviews. Define $x$ as an individual review, and $t$ as a token. This token $t$ can be adjective or noun or adverb. To identify if the token is adjective or noun or adverb, we have used the PoS-tagger of Natural Language Tool Kit (NLTK) module .\nNow, for each review $x$, we calculate $P_A(x)$ and $P_H(x)$, which represent the probability of $x$ belonging to the AI corpus and the human corpus, respectively. These probabilities can be calculated by summing up the probabilities of all tokens that are coming in review x:-\n$P_A(x) = p_A(t_1) + p_A(t_2) + ... = \\sum_{i=1}^{i=na} p_A(t_i)$\n$P_H(x) = p_H(t_1) + p_H(t_2) + ... = \\sum_{i=1}^{i=nh} p_H(t_i)$\nHere, $t_1, t_2, ...$ refer to the tokens occurring in review $x$. Also, $n_a$ and $n_h$ refer to the number of AI and Human corpus reviews, respectively.\nIf review $x$ contains tokens with higher probabilities in the AI corpus, then $P_A(x)$ will be greater, increasing the likelihood that $x$ is AI-generated. Conversely, if $x$ contains tokens with higher probabilities in the human corpus, then $P_H(x)$ will be greater, suggesting that the review is more likely to be human-written.\nTo classify each review $x_i$, we calculate $p_A(i)$ and $p_H(i)$ for each review in our dataset. These serve as input features for training a neural network. The neural network is trained to distinguish between AI-generated and human-generated reviews based on these input features. By learning from the patterns and distributions of these probabilities, the neural network can accurately detect AI-generated reviews."}, {"title": "4.2 Regeneration based Approach", "content": "Figure 2 shows the overall architectural diagram of our proposed regeneration-based approach. The input to the framework is the paper and its review which we aim to determine whether they are written by AI or Human.\nThe idea behind this approach is that if a similar prompt is given repeatedly to a large language model (LLM), the LLM is likely to generate reviews or responses that exhibit a consistent style, tone, and content, as outlined in the provided context. This consistency occurs because a large language model generally applies the patterns it has learned during training to the new content it generates based on the given prompt. The study in (Hackl et al., 2023) found that GPT-4 demonstrated high inter-rater reliability, with ICC scores ranging from 0.94 to 0.99, in rating responses across multiple iterations and time periods (both short-term and long-term). This indicates consistent performance when given the same prompt. Furthermore, the results showed that different types of feedbacks (content or style) did not affect the consistency of GPT-4's ratings, further supporting the model's ability to maintain a consistent approach based on the prompt."}, {"title": "4.2.1 Review Regeneration and Embedding Creation", "content": "We employ GPT to regenerate a review $R_{reg}$ using the prompt $P_{reg}$. We create two distinct embeddings $E_R$ for $R_{reg}$ and $E_F$ for $R$ (review which we have to determine if the review is AI generated or not). The idea is that if the review $R$ is generated by an AI, we hypothesize that its embedding $E_F$ will exhibit a closer similarity to $E_R$, the embedding of a known AI-generated review $R_{reg}$.\nThen, we quantify the similarity between the embeddings using the cosine similarity metric, as outlined below:\n$CosineSimilarity(E_R, E_F) = \\frac{E_R \\cdot E_F}{||E_R|| ||E_F||}$\nHere, $\\cdot$ represents the dot product, and $||R||$ and $||F||$ represent the Euclidean norms of the embeddings. This formula calculates the cosine of the angle between the two embeddings $E_R$ and $E_F$, providing a measure of similarity where values closer to 1 indicate higher similarity and thus a greater likelihood that both reviews are AI-generated."}, {"title": "4.2.2 Training", "content": "Next, we utilize the computed similarity score as input to train a neural network aimed at detecting AI-generated reviews. The training process involves optimizing the network's parameters via backpropagation. This optimization is directed by the cross-entropy loss function."}, {"title": "4.3 Token Attack", "content": "We propose an attack method to reduce the probability of reviews being classified as AI-generated described in Algorithm-1 where we target the most frequent tokens in AI-generated reviews and replace them with their synonyms, which are less frequent in the AI-generated content.\nHere, we focus exclusively on adjectives, referring to this approach as the \"adjective attack.\" We chose adjectives because substituting nouns and adverbs with their synonyms often leads to nonsensical statements or drastically alters the meaning of the review. We discuss this in detail in Appendix C.\nIn the adjective attack, we substitute the top 100 highest probability adjective tokens (e.g., \"novel,\" \"comprehensive\") with their synonyms."}, {"title": "4.4 Paraphrasing Defence", "content": "Paraphrasing tools are effective in evading detection (Sadasivan et al., 2023b; Krishna et al., 2024). Given the fluency and coherence of paraphrased content, it is hard to tell if the text is written by a human or AI even for experts. To increase the robustness of Regeneration based text detector to paraphrase attacks, we introduce a simple defense that employs a targeted synonym replacement strategy. The core idea behind this approach is that when an AI-generated review is processed by a paraphraser, one of the major modifications it makes is substituting the original words with similar ones. We propose a technique to revert the paraphrased reviews back to a state that closely resembles their original AI-generated form by utilizing the regenerated review (as they would be close to the original AI-generated review).\nAs discussed in Algorithm-2, first, we identify all the tokens within a review and their corresponding regenerated reviews using the PoS tagging. Here token can be any word in a review which are adjective, noun, or adverb. For each token in a review, we obtain a list of synonyms from the NLTK WordNet database. Then, for each synonym in that list, we check whether it is present in the corresponding regenerated review or not. If it is, we replace the original token with its synonym."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experimental Settings", "content": "We implemented our system using PyTorch (Paszke et al., 2019). The dataset was randomly split into three parts: 80% for training, 10% for validation, and 10% for testing.\nFor the TF model and RR model, we conducted experiments with different network configurations during the validation phase. Through these experiments, we determined that a batch size of 32 and a dropout rate of 0.1 for every layer yielded optimal performance. The activation function ReLU was used in our model. We trained the model for 20 epochs, employing a learning rate of 1e-3 for TF model and 0.01 for RR model and cross-entropy as the loss function. To prevent overfitting, we used the Adam optimizer with a weight decay of 1e-3. We trained all the models on an NVIDIA A100 40GB GPU. We used the text-embedding-ada-002 pretrained model from OpenAI for creating embeddings of the reviewer's review and the regenerated review."}, {"title": "5.2 Baselines for Comparison", "content": "RADAR (Hu et al., 2023) (Robust AI text Detection via Adversarial Learning) draws inspiration from adversarial machine learning techniques. LLMDet (Wu et al., 2023) (A Third Party Large Language Models Generated Text Detection Tool) is a text detection tool that can identify the source from which the text was generated, such as Human, LLaMA, OPT, or others. DEEP-FAKE (Li et al., 2023) Text Detection considered 10 datasets covering a wide range of writing tasks (e.g., story generation, news writing and scientific writing) from diverse sources (e.g., Reddit posts and BBC news), and applied 27 LLMs (e.g., OpenAI, LLaMA, and EleutherAI) for construction of deepfake texts. Fast-Detect GPT (Bao et al., 2023b) uses a conditional probability function and it invokes the sampling GPT once to generate all samples and calls the scoring GPT once to evaluate all the samples. We discuss them in details in Section D."}, {"title": "5.3 Results and Analysis", "content": "Table 1 shows the comparison results of the models when reviews are generated by GPT-4. It is evident from the results that our proposed TF and RR models outperform the other text detectors. In ICLR and NeurIPS dataset, our Token Frequency (TF) model surpasses the closest comparable model DEEP-FAKE with margins of 6.75 and 6.87 F1 points, RADAR by 29.45 and 26.28 F1 points, LLMDET by 29.69 and 30.64 F1 points. Whereas, Our Review Regeneration (RR) model outperforms DEEP-FAKE by 3.55 and 0.65 F1 points, RADAR by 26.25 and 20.06 F1 points, LLMDET by 26.49 and 24.42 F1 points and FAST DETECT by 8.76 and 15.03 F1 points\nIn the results reported above for the TF model, we considered tokens as adjectives, as this configuration yielded the best results. We also present the outcomes of the TF model when trained with tokens considered as adverbs or nouns in the Appendix Table 7. Furthermore, we observe a similar distribution of results on reviews generated by GPT-3.5. We report the result in Appendix Table 5."}, {"title": "5.3.1 Effect of attacking AI-generated text detectors using Adjective Attack", "content": "We report the results after performing adjective attack as described in Section 4.3 in Table 2. It is evident from the table that the performance of each model dropped after the attack. In particular, for ICLR and NeurIPS respectively, the F1 score of RADAR dropped by 69.62% and 68.18%, LLMDET dropped by 6.46% and 2.43%, DEEP-FAKE dropped by 70.65% and 88.10%, and FAST DETECT dropped by 92.48% and 98.29%. Additionally, the F1 score of our TF model dropped by 79.88% and 89.43% for ICLR and NeurIPS, respectively, whereas for our RR model, it dropped by 25.56% and 23.14% for ICLR and NeurIPS, respectively.\nThe results reveal that this attack has significantly compromised the performance of our TF model, underscoring its vulnerability and limited resilience to such threats. The substantial decline in the F1-score can be attributed primarily to the model's reliance on token frequency patterns in AI-generated reviews. These patterns are effectively disrupted by synonym replacements leading to performance degradation. After the adjective attack, we observed that our RR model outperforms other Al text detectors, including our proposed TF model, achieving the highest F1 score of 71.81."}, {"title": "5.3.2 Effect of attacking AI-generated text detectors using Paraphrasing Attack", "content": "Next, we report the result after performing paraphrasing (See Appendix E for more details) on the AI-generated reviews. It is evident from the Table 3 that the result of each model dropped after the attack. In particular, for ICLR and NeurIPS, the F1 score of RADAR dropped by 7.10% and 6.89%, LLMDET dropped by 5.79% and 3.62%, DEEP-FAKE dropped by 18.19% and 26.19%, and FAST DETECT dropped by 39.69% and 24.66%. Additionally, F1 score of our TF model dropped by 56.92% and 50.08% for ICLR and NeurIPS respectively and RR model dropped by 56.41% and 57.00% for ICLR and NeurIPS respectively.\nThis effect on the TF model is not surprising, as it is based on AI token frequency and paraphrasing typically involves replacing words with their synonyms. For our RR model, we noted that paraphrasing caused both human-written and AI-written reviews to diverge further from the regenerated reviews. This increased dissimilarity could stem from various factors, including alterations in text structure, voice, tone, and vocabulary. If only human reviews had been paraphrased, we might have observed an improvement in performance due to a greater distinction between human-written and regenerated reviews. In our test set, which includes both AI-generated and human reviews, the similarity of AI-generated text decreased following paraphrasing, leading to a decline in overall performance."}, {"title": "5.3.3 Results after Paraphrasing Defence", "content": "Next, we report the result after performing paraphrasing Defence (See Section 4.4 for more details) on both our proposed models on Table 3. We observed improvements in both our TF and RR models. We also applied the defense to other AI text detection algorithms, observing no significant improvement or decrease in their results. These results are reported in Table 8. The performance of the TF model improved by 75.32% for ICLR papers and 46.70% for NeurIPS. Similarly, the performance of the RR model improved by 99.81% for ICLR and 111.69% for NeurIPS.\nThese results indicate that our proposed RR model is more robust against different types of attacks and performs better than any other existing text detection algorithms."}, {"title": "5.4 Human evaluation", "content": "We also conducted human analyses to understand when and why our models fail. Our model fails when paraphrasing alters the style or when AI-generated reviews closely resemble human writing, resulting in low similarity scores and incorrect predictions. We discuss this extensive error analysis in the Appendix B."}, {"title": "6 Conclusion and Future Work", "content": "In this work, we propose two methods to determine whether a review is written by a human or generated by AI. We found that our proposed TF model and the RR model outperform other AI text detectors under normal conditions. We stress test these detectors against token attack and paraphrasing. Furthermore, our proposed RR model is more robust and outperforms other methods. We then propose an effective defensive strategy to reduce the effect of paraphrasing on our models. Our findings suggest both of our proposed methods perform better than other AI text detectors. Also, while our proposed TF model performs better than the RR model without any attacks, our RR model is more robust against token attacks and paraphrasing attacks.\nWe hope that these findings will pave the way for more sophisticated and reliable AI detectors to prevent such misuse. In future work, we aim to extend our analysis to other domains, such as Nuclear Physics, Medicine, and Social Sciences, and investigate domain-specific LLMs to enhance detection accuracy and explore the generalizability of our methods.\nFor further work, we aim to focus on cases where the reviewer writes parts of the review using AI."}, {"title": "Limitations", "content": "Our study primarily utilized GPT-4 and GPT-3.5 for generating AI texts, as GPT has been one of the most widely used LLMs for long-context content generation. We recommend that future practitioners choose the LLM that best aligns with the language model likely used to generate their target corpus, to accurately reflect usage patterns at the time of its creation. Our methods are specifically designed for reviews completely written by AI. It is possible, however, that a reviewer may outline several bullet points related to a paper and use ChatGPT to expand these into full paragraphs. We suggest exploring this aspect in future research."}, {"title": "Ethics Statement", "content": "We have utilized the open source dataset for this study. We do not claim that the use of AI tools for review papers is necessarily bad or good, nor do we provide definitive proof that reviewers are employing ChatGPT to draft reviews. The primary purpose of this system is to assist editors by identifying potentially AI-generated reviews, and is intended only for editors' internal usage, not for authors or reviewers.\nOur RR model requires regenerated review to be generated from paper using LLM. Also, open-sourced LLMs running locally will not have any concerns. OpenAI implemented a Zero Data Retention policy to ensure the security and privacy of data. Additionally, users can control the duration of data retention through ChatGPT Enterprise. Also, nowadays, many papers are submitted to arXiv and are publicly available. However, editors and chairs should use this tool with caution, considering the potential risks to privacy and anonymity.\nThe system cannot detect all AI-generated reviews and may produce false negatives, so editors should not rely on it exclusively. It is meant to assist, but results must be verified and analyzed carefully before making any decisions. We hope that our data and analyses will facilitate constructive discussions within the community and help prevent the misuse of AI."}, {"title": "A Dataset", "content": "We generated a fake review for each paper using both GPT-3.5 and GPT-4. We gave the prompt template similar to both of the conference style of reviews. We also generated regenerated reviews for this task.\nWe discuss the dataset in more details in the Appendix Section\nBelow is the prompt we used for generating AI-generated review ICLR 2022 reviews:-\nSystem: You are a research scientist reviewing a scientific paper.\nUser: Read the following paper and write a thorough peer-review in the following format:\n1) Summary of the paper\n2) Main review\n3) Summary of the review\nBelow is the prompt we used for generating AI-generated review NeurIPS 2022 reviews:\nSystem: You are a research scientist reviewing a scientific paper.\nUser: Read the following paper and write a thorough peer-review in the following format:\n1) Summary (avg word length 100)\n2) Strengths and weaknesses\n3) Questions\n4) Limitations (in short)\nBelow is the prompt we used for generating AI-regenerated review ICLR 2022 reviews:-\nSystem: You are a research scientist reviewing a scientific paper.\nUser: Your task is to draft a high-quality peer-review in the below format:\n1) Summarize the paper.\n2) List strong and weak points of the paper, Question and Feedback to the author. Be as comprehensive as possible.\n3) Write review summary (Provide supporting arguments for your recommendation).\nTo generate AI-regenerated reviews, we used prompts that were very distinct from those we used to generate AI reviews for training. The reason for this approach is that a reviewer may write any kind of prompt, which could be very different from the prompts we used for training.\nBelow is the prompt we used for generating AI-regenerated review NeurIPS 2022 reviews :-\nSystem: You are a research scientist reviewing a scientific paper.\nUser: Your task is to draft a high-quality peer-review in the below format:\n1) Briefly summarize the paper and its contributions\n2) Please provide a thorough assessment of the strengths and weaknesses of the paper\n3) Please list up and carefully describe any questions and suggestions for the authors 4) Limitations: Have the authors adequately addressed the limitations and potential negative societal impact of their work? If not, please include constructive suggestions for improvement. Write in few lines only"}, {"title": "B Error Analysis", "content": "We conducted an analysis of the predictions made by our proposed baseline to identify the areas where it most frequently fails."}, {"title": "B.1 Challenges after paraphrasing:", "content": "Our regeneration-based approach sometimes fails when it processes a paraphrased review. Paraphrasing can alter the semantics of a review to some extent, leading to discrepancies with our reverse-generated reviews. Consequently, our model may incorrectly predict these as human-written rather than AI-generated. Our proposed defense strategy corrects only the tokens that have been changed during paraphrasing. However, when the paraphrasing significantly alters the style, our RR model fails."}, {"title": "B.2 Sometimes Regenerated review and AI written reviews are similar:", "content": "Our RR model works on the similarity of review and Regenerated review. We found the model fails when LLM generates a review that is very much similar to human writing. In those cases, we found that the similarity score tends to be low, leading to the model's failure. This suggests the model may struggle to differentiate human-like AI-generated text."}, {"title": "C Token Attack", "content": "Below is an example of how impactful various attacks can be when replacing words in a review:-\nAfter reviewing all the attacks, we observe that the adjective attack produced more logical changes compared to the others. For example, in the noun attack, 'model' was replaced with 'pose,' 'learning' with 'discovery,' 'performance' with 'execution,' and 'datasets' with 'information sets,' which are not very meaningful and thus make the attack less effective. Replacing words can cause significant changes in the meaning of a review and can even alter the context. So we used only the adjective attack for our experiments."}, {"title": "D Baseline Comparison"}]}