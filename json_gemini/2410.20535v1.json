{"title": "Asynchronous Perception Machine for Efficient Test Time Training", "authors": ["Rajat Modi", "Yogesh Singh Rawat"], "abstract": "In this work, we propose Asynchronous Perception Machine (APM), a\ncomputationally-efficient architecture for test-time-training (TTT). APM can pro-\ncess patches of an image one at a time in any order asymmetrically, and still\nencode semantic-awareness in the net. We demonstrate APM's ability to recognize\nout-of-distribution images without dataset-specific pre-training, augmentation or\nany-pretext task. APM offers competitive performance over existing TTT ap-\nproaches. To perform TTT, APM just distills test sample's representation once.\nAPM possesses a unique property: it can learn using just this single representation\nand starts predicting semantically-aware features.\nAPM demostrates potential applications beyond test-time-training: APM can scale\nup to a dataset of 2D images and yield semantic-clusterings in a single forward\npass. APM also provides first empirical evidence towards validating GLOM's\ninsight, i.e. input percept is a field. Therefore, APM helps us converge towards\nan implementation which can do both interpolation and perception on a shared-\nconnectionist hardware. Our code is publicly available at this link.", "sections": [{"title": "Introduction", "content": "In these past several decades, computing-machines have become a lot faster [99]. This made it\npossible to train higher-p arameterized neural nets and led to interesting emergent abilities [87].\nNeural-nets can now finally learn without human-feedback [8], paint pictures [83] and even compose\na sonnet [99]. Even with such impressive-progress, a key question still remains: how can these\nnets recognize images whose distribution is far different from the ones which were used during\ntraining? For e.g., consider a self-driving car trying to stop when it encounters a pedestrian crossing\na road. Such practical scenarios require \u2018instantaneous-decisions'2 for ensuring human-safety in\nautonomous-systems [71].\nTest-time-training (TTT) [92] is one of the promising techniques for handling such distribution shifts:\na neural net adapts to a test sample 'on the fly'. Since the label of the sample is not known, the net\nperforms some auxiliary pre-text task like data augmentation [18], rotation [18, 93] or prompt tuning\n[91] on it. After several such iterations, the net recognizes the test sample. The key idea is that the net\nis allowed to dynamically adjust its decision boundary even after it has been trained, thereby bringing\nit much closer to how humans keep learning 'continuously' throughout their lifespans [93]."}, {"title": "Background", "content": "We draw from insights previously philosophically mentioned in GLOM [35]. Consider how machine\nperception has been done classically: an input $x \\in R^{c \\times h \\times w}$ is transformed by a non-linear function\n$f$ to a perceptual feature grid $y \\in R^{c' \\times h \\times w}$, , c denotes image channels, h, w are input spatial\ndimensions.\nIslands of agreement [35] Rather than viewing this matrix y as a cuboidal grid, one can now see this\nas column vectors $v_c$ at each location. There are h \u00d7 w such columns in y. Therefore, there exists\na one-to-one mapping, between each input patch i.e. (x, y) with the vector $v_c$ at that location. A\nneural net f can then thus learn to fit $f (I, x, y) \\rightarrow v_v$. These vectors $v_v$ have been termed islands of"}, {"title": "Asynchronous Perception Machine (APM)", "content": "APM processes an input image I via two novel mechanisms: 1) A column module T which is said\nto contain an input image I, 2) a column folding-unfolding mechanism that operates during each\nforward-pass. We first provide a technical analogy to better understand APM.\n3.1 A Technical Analogy[35]\nA neural field does 3D novel view synthesis by querying an MLP with a point (x, y, z) and yielding\ncorresponding rgb. In a similar way, APM does 2D perception by querying an MLP with an image I,\nand a location (x, y) to yield location-specific feature $f_{xy}$. APM features a new mechanism to query\nthe MLP, i.e a column module T. Next, we define this column representation T.\n3.2 Global Column Module: Defining compressed representation T\nWe define a column T as a vector of dimensions 1 \u00d7 1 \u00d7 d. Our aim is to map image I in this T,\nso that T can summarize its entire identity. Given an image of dimensions c \u00d7 h \u00d7 w, we run a 2D\nconvolution on it. Number of filters are set as accordingly. The resultant 1 \u00d7 h' \u00d7 w' feature map\nis then flattened into a single column vector T of dimensions d = h' \u00d7 w' . We shall refer to this\ncolumn T as \"triggering hyper-column\"(seed). The only learnable parameters in this column are\nparameters of a convolution filter:\u00b3[99, 98].\n3.3 Abstract view: The Column Unfolding-Folding Mechanism\nThe trigger column T now starts undergoing cycles of folding-unfolding (Fig 1). During unfolding, the\ncolumn T copies itself to yield h \u00d7 w location-aware columns. During folding, these h \u00d7 w columns\ncollapse back into a single column T. The neural-net then oscillates between these folded-unfolded\nphases during learning iterations.\n3.4 Computational Principle: Location-aware columns and their collapse\nThe unfolding-process shall now be concerned with generating location-aware column $T_{ij}$ from\nT. We generate h * w 2-D non-parametric positional encoding similar to the ones being used in\ntransformers [101] and neural fields [66]. The trigger column $T_{ij}$ is then given by $T_{ij} = (T|p_{ij})$,\nwhere | denotes the stacking operator and (1, 1) \u2264 (i, j) \u2264 (H, W). T can be said to encode identity\nof an image.\nThe folding-process involves collapsing all columns $T_{ij}$ back into T from which they had begun.\nThis is achievable since the $p_{ij}$ used in $T_{ij}$ was deterministic. An abstract-mathematical intuition"}, {"title": "Firing location-aware columns into a shared MLP", "content": "Each column $T_{ij}$ is passed through an MLP to yield location-specific features $f_{ij}$ and RGB values\n$RGB_{ij}$. Number of neurons in the first layer of the MLP is same as dimensionality of column $T_{ij}$.\nColumn Independence: The MLP is shared across all the columns. One column is also independent\nof another as illustrated in Fig1. Therefore, the MLP can be queried sequentially. Firing a column\n$T_{ij}$ into the MLP yields a column vector $v_c$. Once h \u00d7 w columns are finished firing, we get a feature\ngrid f of dimensions h \u00d7 w \u00d7 c. Note that the number of columns can be as low as 1."}, {"title": "Training and Losses", "content": "We detail how the tth iteration of test-time-training could be performed. First, the obtained feature\ngrid $f \\in R^{h \\times w \\times c}$ from APM is averaged to yield $f_{avg} \\in R^{c}$. For the first TTT iteration, i.e. t = 1, the\nimage I is feed-forwarded through a multi-modal teacher like CLIP to get a CLS token $f_{cls}$ and\ncorresponding text representation $t_{cls}$. APM then learns to estimate this same target feature $f_{cls}$. We\nenforce this by a simple L2 constraint as:\n$L_1 = L_2(f_{avg,t}, f_{cls})$\nwhere $f_{avg,t}$ is the averaged output feature from APM at a particular TTT iteration t. Note that the\ntarget $f_{cls}$ is only estimated in t = 1 and remains the same for t > 2, i.e. subsequent feed-forward\nthrough teacher is not needed.\nMemory-efficient estimation of $f_{avg,t}$: During a TTT iteration t, favg is computed as a simple\naverage of $f \\in R^{h \\times w \\times c}$. This would require h \u00d7 w columns to exist in the memory simultaneously.\nAPM's design assumes column independence which allows estimating $f_{avg}$ as a statistical running\naverage, i.e.\n$f_{avg} = \\frac{nx f_{avg} + f_{i,j}}{n+1}$\nassuming, n columns have already been fired into the APM and one additional column corresponding\nto position (i, j) of image I is in the process of firing. This procedure is repeated until all positions\n(i, j) are exhausted. We represent the sequential column-firing by a \u2018Gather-Grid' operator in Fig1.\nPredicting image class-label: After certain TTT iterations, (say t = 20), the output feature $f_{avg,t}$\nand the textual features $t_{cls}$ are obtained. Image-classification then follows the standard practice of\ncomparing the distance of $f_{avg,t}$ with each plausible class feature $t_{cas}$ in the contrastive space and\nchoosing the closest one as the prediction [82]."}, {"title": "Experimenting with APM", "content": "We quantitatively evaluate APM on zero-shot test-time-training on popular benchmarks containing\ndistribution-shifts [18, 93, 91]. Next, we quantitatively explore its computational efficiency.\nDatasets: Cifar-10C [93] contains 5 level of corruptions on the test-set with 15 types of noises.\nLarger datasets with significant distribution shifts consists of ImageNet val/other curated splits. For"}, {"title": "APM Training (Qualitative Analysis)", "content": "APM can also scale up and do learning on a batch of samples (for e.g., COCO images[56]) distilled\nfrom a teacher[37]. This requires introducing several new mechanisms. Note that this section is meant\nto qualitatively demonstrate how scaling up APM can improve the net's interpretability, and help\nseed future research. Quantitative experiments beyond test-time-training remain out of the scope of\nthis paper. APM's training follows a standard setup in self-supervised-learning[8]. We have provided\nthe full algorithm for SSL-training/test-time-training in the AppendixC. During inference, APM takes\nany 2D image xk and predicts its RGB reconstruction $RGB_k$/higher dimensional features fk. The\nnet then begins to demonstrate several interesting properties, which we will discuss next.\n5.1 APM can do RGB reconstruction for any 2D input.\nGiven a sample xk, APM can reconstruct its RGB. In Fig4, we achieve this by estimating $f_{rgb,ij} =$\n$(T_{ij} f_{ij})$. This skip-connection from trigger column $T_{ij}$ to output feature has a subtle reason: consider\na white dog and brown dog. The predicted object-level feature for both will be almost identical[35, 3].\nHowever, $T_{ij}$ is different for both since it contains lower patch-level features[35]. Therefore, this\nhelps us break symmetry. Without this skip-connection[26], the net fails to predict RGB. The network\nis trained to reconstruct RGB for a batch of images, $L_{rgb} = \\Sigma_{i=1}^N \\Sigma_{j=1}^{hw} L2(P_{ij}', P_{ij})$, where $p'/p$\nare ground truth/predicted RGB-logits respectively.\n5.2 APM is asynchronous yet encodes semantic-awareness in the net.\nGiven a sample xk, APM can directly learn to mimic the entire last layer feature-grid which a\nteacher model would have generated. We enforce this by a $L_{grid} =\n\\Sigma_{i=1}^N \\Sigma_{j=1}^{hw} L2(f_j, f_{grid})$. In"}, {"title": "APM is a step towards validating GLOM's insight: input percept is a field[42].", "content": "In Fig 6, we show that APM can interpolate between any two images in the wild. We choose two\nimages $I_1$ and $I_2$. These images are then funneled through the trigger column T and yield two vectors\n$v_1$ and, $v_2$ respectively. Next, we generate n intermediate latents separated by an equal linear distance\nby $v_j = V_1 + \\frac{j}{n}V_2-V_1$. Each latent then brings into existence its own set of location-aware columns\nand decodes an image from the MLP. Such an interpolation has been previously observed in other\nmodels [23]. APM now functions as a new form of addressing mechanism: the trigger column T acts\na key. Copying T across locations yields image-specific queries[35]. Values are synapses triggered in\nthe MLP. RGB decoding happens in the output head. Hence, such continuous keys and queries exist\noutside the net[35].\nClassically, auto-regression has unrolled a shared-decoder over time[106]. In contrast, APM holds\nthe whole sequence I in T, and directly hops onto a space/time-step[14] by querying the MLP with\na location-column $T_{ij}$. Note that $T_{ij}$ is generated by unfolding T. Recurrence/feedback-loops are\ncompensated for by a form of feature-expression[35]. This is a step towards validating GLOM's\ninsight, i.e. input-percept is a field[35] and one can now interpolate in it (gestalt psychology).\nFurthermore, the trigger column T resides in a continous embedding space, and not discrete hardware\nlocations(classical AI)[35]. Therefore, APM tries to integrate insights from both fields."}, {"title": "Ablations on APM", "content": "The experiments on TTT had relied on a curious ability of APM: it could simply overfit on a\ntest-sample's distilled representation at t = 1. This merits further investigation:"}, {"title": "Broader Impact", "content": "There are two main ideas that led us to Asynchronous Perception Machine[46, 35]. The first idea\nis that instead of thinking of features as a cuboidal feature grid[33], one can think of it as a column\nvector at each location[35, 71]. This helped us learn one to one mapping between the input rgb patch\nand the vector at a particular location. It led to the net being able to process one patch at a time.\nThe second idea is this notion of collapsing information into a single starting point. Our previous\nunderstanding was that this 'collapse' leads to degeneracy[46]. In this paper, the information can be\nrecovered from the starting point by copying it many times and breaking symmetry with positional\nencoding. By asking the right questions at the right place at the right time, a net can thus learn to\nexpress correct features[98, 35]. Although the information can \u2018degenerate' to a single starting point,\nthe net can still learn thanks to the strong positional-prior injected by such periodic-encodings[101].\nThis notion of combining information coming from many locations appears to have connections to\nKolmogorov-Arnold superposition theorem[35, 59]. The single convolutional filter in APM might be\nconsidered an encoder and five layered MLP as a decoder. Convolution filter can also be considered\nto be a tape[99] on which symbols are written, processed by a learning machine(MLP)[99] which\nspeaks the correct answers. We have observed that MLP's have this ability to cluster elements in any\nimage. This seems exciting for dense visual tasks with potential for new insights. Finally, we are led\nto believe that the networks could be made even smaller with higher bandwidth [35, 34]. Of course,\nit shall mean defining a metric called bandwidth, where the performance of the learning machine\nshall be measured by a three-tuple of <parameters, accuracy, bandwidth>[34, 3].\nKnowledge-transfer can then be a consequence of sharing folded-embeddings which could grow\nto form dynamically connected-networks rather than mimicking unfolded-outputs among multiple\nneural-nets. Knowledge transfer between nets of same structure is as simple as copying weights from\none to another thereby making them immortal. There already exist approaches which share trees or\nshare knowledge between different neural nets, for e.g. dropout. A higher bandwidth way might\ninvolve exchanging folded network-vectors in higher dimensional space, which then reduces to the\nsetting of distributed federated-learning that could run in low-cost hardware.\nA Turing machine gives us a sense of closure[75]: the input tape is shared for both input and output\nto the machine. However, existing neural nets are mostly bottom-up, with feature expression limited\nto last layer of the net. In contrast, neurons of a boltzmann machine were clamped, to allow data-\nvectors to be presented to the net via the environment as well as express the generated perceptual\ncodes[17] sampled from the net itself on a same set of neurons[41, 43]. Modelling this closure\npresents problems for training a neural net: for backpropagation cant work in circles of synapstic\nconnections, even though there is mounting evidence of such connections in the brain[34]. Modelling\ntop-down influences in GLOM then has to resort to leveraging top-down influence in a previous\ntime-step to influence lower embeddings estimated in the current time step via an auto-encoder or a\nneural field. A key challenge then remains to propogate top-down influences in the current step, for\neg eg, via a recirculation-procedure[44] which could be trained via backprop or some other learning\nalgorithm we are yet to discover[34] and at the same time entirely avoid the representational/mode\ncollapse which comes from such local forms of learning[23, 35, 71]."}, {"title": "Future Work", "content": "APM offers a fresh perspective towards machine-perception: i.e. patches can now be lazily-processed\none-by-one asynchronously[35]. It shall be very-exciting to see APM's potential on dense-tasks,"}, {"title": "Pseudo-code for APM's operation.", "content": "In Algorithm1, we have inflated the entire pseudo-code to train APM beyond the applications of\ntest-time-training. The idea is that given an input image xk APM can learn to predict its entire feature\ngrid fk and its rgb logits RGBk. First, the net inputs an image xk. Xk is then routed to a trigger\ncolumn T. T then brings several columns $T_{ij}$ into existence. Each of these columns is fired into the\nMLP to yield location-aware features $f_{ij}$. The loss is collected for all locations and backpropagation\nthen estimates the gradients required to update the parameters of the APM. In this entire process,\nthere were no labels being used. The feature grids could have been any layer of a net like DINOv2.\nAPM thus manages to learn a perception field within itself [35"}]}