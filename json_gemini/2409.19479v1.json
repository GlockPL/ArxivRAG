{"title": "Spatial Reasoning and Planning\nfor Deep Embodied Agents", "authors": ["Shu Ishida"], "abstract": "Humans can perform complex tasks with long-term objectives by planning, reasoning,\nand forecasting outcomes of actions. For embodied agents (e.g. robots) to achieve similar\ncapabilities, they must gain knowledge of the environment transferable to novel scenarios\nwith a limited budget of additional trial and error. Learning-based approaches, such as\ndeep reinforcement learning, can discover and take advantage of inherent regularities\nand characteristics of the application domain from data, and continuously improve their\nperformances, however at a cost of large amounts of training data. This thesis explores the\ndevelopment of data-driven techniques for spatial reasoning and planning tasks, focusing\non enhancing learning efficiency, interpretability, and transferability across novel scenarios.\nFour key contributions are made. Firstly, CALVIN, a differential planner that learns\ninterpretable models of the world for long-term planning. It successfully navigated partially\nobservable 3D environments, such as mazes and indoor rooms, by learning the rewards\n(goals and obstacles) and state transitions (robot dynamics) from expert demonstrations.\nSecondly, SOAP, a reinforcement learning algorithm that discovers macro-actions\n(options) unsupervised for long-horizon tasks. Options segment a task into subtasks\nand enable consistent execution of the subtask. SOAP showed robust performances on\nhistory-conditional corridor tasks as well as classical benchmarks such as Atari.\nThirdly, LangProp, a code optimisation framework using Large Language Models to\nsolve embodied agent problems that require reasoning by treating code as learnable policies.\nThe framework successfully generated interpretable code with comparable or superior\nperformance to human-written experts in the CARLA autonomous driving benchmark.\nFinally, Voggite, an embodied agent with a vision-to-action transformer backend\nthat solves complex tasks in Minecraft. It achieved third place in the MineRL BASALT\nCompetition by identifying action triggers to segment tasks into multiple stages.\nThese advancements provide new avenues for applications of learning-based methods\nin complex spatial reasoning and planning challenges.\nKeywords - machine learning, neural networks, deep reinforcement learning, imitation\nlearning, hierarchical reinforcement learning, policy optimisation, robotics, autonomous driving,\nembodied agents, option discovery, skill learning, navigation, planning, computer vision, large\nlanguage models, multi-modal foundation models.", "sections": [{"title": "1 Introduction", "content": "The ability to plan, reason and forecast outcomes of actions, even in novel environments,\nare remarkable human capabilities that are instrumental in performing complex tasks with\nlong-term objectives. Whenever we encounter a novel scenario, whether that be a new\ngame, sport or location, even though we have never experienced that specific case, we can\nstill strategise by extrapolating from our prior experiences, taking advantage of transferable\nknowledge and skills.\nWith modern planning algorithms, it is possible to find a near-optimal solution to\na planning problem, if the environment dynamics (specifically the state transition and\nreward dynamics) are fully known, the states and actions can be enumerated, and unlimited\ncompute is available. Unfortunately, it is often the case that all three of the assumptions\ndo not hold. The agent often only has access to a local or partial observation of the\nenvironment, and has to estimate the underlying environment state and dynamics based\non this. States and actions are often continuous rather than discrete, so an estimator is\nrequired that can map continuous inputs to meaningful representations that generalise to\nnovel inputs. Finally, since compute is finite and enumeration of states and actions is often\ninfeasible, an efficient strategy is necessary to explore the state-action space within limited\ncomputational resources and agent lifetime."}, {"title": "1.1 Motivation", "content": "The ability to plan, reason and forecast outcomes of actions, even in novel environments,\nare remarkable human capabilities that are instrumental in performing complex tasks with\nlong-term objectives. Whenever we encounter a novel scenario, whether that be a new\ngame, sport or location, even though we have never experienced that specific case, we can\nstill strategise by extrapolating from our prior experiences, taking advantage of transferable\nknowledge and skills.\nWith modern planning algorithms, it is possible to find a near-optimal solution to\na planning problem, if the environment dynamics (specifically the state transition and\nreward dynamics) are fully known, the states and actions can be enumerated, and unlimited\ncompute is available. Unfortunately, it is often the case that all three of the assumptions\ndo not hold. The agent often only has access to a local or partial observation of the\nenvironment, and has to estimate the underlying environment state and dynamics based\non this. States and actions are often continuous rather than discrete, so an estimator is\nrequired that can map continuous inputs to meaningful representations that generalise to\nnovel inputs. Finally, since compute is finite and enumeration of states and actions is often\ninfeasible, an efficient strategy is necessary to explore the state-action space within limited\ncomputational resources and agent lifetime."}, {"title": "1.2 Research objectives", "content": "This research addresses the challenge of solving tasks involving spatial reasoning, planning,\nand decision making in a data-driven manner, while simultaneously making the learning\nmore efficient, interpretable and transferable. This research objective can be further broken\ndown into five research goals, which are described in detail in the following."}, {"title": "1.2.1 Learn a generalisable planner", "content": "One of the core objectives of this research is to develop learnable planners that generalise\nto novel scenarios. A distinction between a reactive Markovian policy and a policy with\na plan is that a reactive policy makes immediate decisions given a current state or local\nobservation, whereas planning involves a more long-term analysis of the given situation to\npropose a spatially and temporally coherent solution.\nThe differences between the two approaches are analogous to the System 1 (fast,\nunconscious, and automatic decisions) and System 2 (slow, conscious, and rigorous\ndecisions) thinking presented in [106]. Both decision processes are important, since\nreactive policies are useful for making many decisions in real-time, whereas planning is"}, {"title": "1.2.2 Discover reusable skills", "content": "Skill learning is another important component for efficient exploration, decision making\nand task-solving. With skills, it is possible to conceive a high-level plan that combines\nand orchestrates low-level skill policies. These skills are specialised to solve a subset\nof the task so that the agent may learn to solve complex novel tasks from fewer training\nsamples by composing these skills together. Ways in which these skills can be learnt in an\nunsupervised way, using rewards from the environment as a learning signal, are explored\nin Chapter 4. The agent trajectory is segmented into options [166, 214] that correspond to"}, {"title": "1.2.3 Solve POMDP environments with memory-augmented policies", "content": "In relation to Section 1.2.2, options can be used not just to learn skills, but also to learn\ntemporally consistent behaviour. It functions as a memory carried forward as a discrete\nlatent variable, allowing the agent to perform tasks in a Partially Observable Markov\nDecision Process (POMDP) environment where the underlying state of the environment\ncannot be determined from current observations alone. The true environment state can be\nbetter determined by maintaining a history of the agent trajectory, since past observations\nare often correlated with future observations by hidden variables. Chapter 4 examines the\neffectiveness and robustness of options discovered by algorithms with different training\nobjectives, demonstrating the advantage of the proposed solution over classical recurrent\npolicies and Option-Critic policies [9, 111].\nIn Chapter 6, the concepts of skills and trajectory segmentation are employed to make\nthe agent change its policy for different stages of task completion. Breaking a complex\ntask down to subcomponents and performing them stage-wise allowed the agent to perform\ntemporally consistent behaviour that adheres to a high-level plan."}, {"title": "1.2.4 Explain the behaviour of experts and agents", "content": "Another theme explored in this research is the explainability of the learnt policy. Skill\nlearning discussed above is one approach that ensures better explainability, given that the\noptions segment the agent trajectory in a semantically interpretable way. Another approach\nto interpretability is explored in Chapter 3; a differentiable planner learns targets, obstacles,\nand motion dynamics from expert trajectories of robot navigation. It also computes a\nreward map and value map during its decision-making process, similarly to Inverse Rein-\nforcement Learning (IRL) [6, 148, 260, 261]. An even more explicit way of representing"}, {"title": "1.2.5 Train embodied agents to perform complex tasks", "content": "Finally, the aim of this research is to apply developed techniques to problems relevant\nto embodied agents, e.g. robotics. In Chapter 3, Chapter 5 and Chapter 6, the chal-\nlenges of robot navigation, autonomous driving and task execution in the virtual world of\nMinecraft [208] are addressed. These challenges all have navigation and spatial reasoning\nas key elements to accomplishing the tasks. Navigation is a real-world problem that has\ntraditionally been solved by expert-designed systems, but could be made more efficient by\nleveraging data-driven learning. For instance, lane changing and cooperation with other\nvehicles are tasks for autonomous vehicles which require complex planning. The problem\nis made especially difficult, since human cooperative behaviour is difficult to model due\nto compounding factors and subtle cues, and there is not always a deterministic strategy\nto follow. Learning cooperative behaviour from real-world data could be beneficial to\noptimising these tasks."}, {"title": "1.3 Main contributions", "content": "The contributions in this thesis can be summarised as follows.\n1. Developed a differentiable planner named Collision Avoidance Long-term Value\nIteration Network (CALVIN), which learns to navigate unseen 3D environments\nby performing differentiable value iteration. State transitions and reward models\nare learnt from expert demonstrations, similarly to Value Iteration Network (VIN).\nHowever, VIN struggles to penalise invalid actions leading to collisions with ob-"}, {"title": "1.4 Outline", "content": "Chapter 2 introduces background material and key concepts relevant to this thesis. Chap-\nter 3 proposes CALVIN, a differentiable planner that learns to plan for long-term navigation\nin unseen 2D and 3D environments. Chapter 4 introduces PPOEM and SOAP for un-\nsupervised option discovery. Chapter 5 proposes LangProp, a framework for iteratively\noptimising code generated by LLMs. Chapter 6 discusses data-driven decision-making\nfor embodied agents with composite tasks, and develops Voggite, an embodied agent that\nperforms tasks in Minecraft. Finally, Chapter 7 concludes this thesis with discussions of\nfindings and future research directions."}, {"title": "2 Background on planning and data-driven decision making", "content": "Planning concerns strategically inducing changes to an environment state with the means\nof actions to achieve a certain objective [116]. In robotics, motion and trajectory planning\nare essential to translating high-level specifications of tasks to low-level descriptions of\nmotion. In the field of Artificial Intelligence (AI) and Reinforcement Learning (RL), early\nsuccesses were achieved in puzzle solving and competing in games [24, 138, 201, 220].\nWhile there are diverse ranges of planning problems, they share some commonalities.\nFirstly, they have a notion of a state s in state space S that captures all details of a situation\nrelevant to a particular problem. Secondly, the environment state can change over time,\nand the only way for the agent to influence its transition to another state is by taking an\naction $a \\in A(s)$. The transition from the current state s to the next state $s'$ is conditional\non a. The plan is expected to meet some criteria and/or optimise a certain objective. In\nrobotics, it is typical to formulate this as a path-finding problem to a target that minimises\nthe total cost, with additional constraints so that illegal configurations and obstacles are\nnot encountered. In RL, the convention is to maximise the cumulative discounted rewards\n(returns), defined in Section 2.1.1."}, {"title": "2.1 Planning algorithms", "content": "Planning concerns strategically inducing changes to an environment state with the means\nof actions to achieve a certain objective [116]. In robotics, motion and trajectory planning\nare essential to translating high-level specifications of tasks to low-level descriptions of\nmotion. In the field of Artificial Intelligence (AI) and Reinforcement Learning (RL), early\nsuccesses were achieved in puzzle solving and competing in games [24, 138, 201, 220].\nWhile there are diverse ranges of planning problems, they share some commonalities.\nFirstly, they have a notion of a state s in state space S that captures all details of a situation\nrelevant to a particular problem. Secondly, the environment state can change over time,\nand the only way for the agent to influence its transition to another state is by taking an\naction $a \\in A(s)$. The transition from the current state s to the next state $s'$ is conditional\non a. The plan is expected to meet some criteria and/or optimise a certain objective. In\nrobotics, it is typical to formulate this as a path-finding problem to a target that minimises\nthe total cost, with additional constraints so that illegal configurations and obstacles are\nnot encountered. In RL, the convention is to maximise the cumulative discounted rewards\n(returns), defined in Section 2.1.1."}, {"title": "2.1.1 Markov Decision Process", "content": "Markov Decision Process (MDP) [17, 213] is a standard formulation for sequential\ndecision-making and planning. An MDP consists of states $s \\in S$, actions available at each\nstate $a \\in A(s)$, a transition probability $P(s'|s, a)$ (the probability next state $s'$ given the\ncurrent state s and action a), and a reward function $R(s, a, s')$, which (either deterministi-\ncally or stochastically) determines the reward r given to the agent during the transition.\u00b9\nAn agent starts at state $s_0 \\sim p(s)$ and at each time step t, takes an action $a_t \\in A(s_t)$,\ncollects a reward $r_t$ and moves to the next state $s_{t+1}$ where $(r_t, s_{t+1}) \\sim P(\u00b7|s_t, a_t)$ until\nepisode termination at timestep T when a boolean termination indicator $d_t$ is set to 1.\nIn the RL paradigm, it is useful to think of the decision-making component (the agent)\nseparately from the environment. The environment has an inner state which defines its\nconfiguration, but the agent often can only measure the state indirectly through observations\ngiven by the environment. The agent can induce changes to the state by taking actions at\nevery time step, and it receives a reward and a new observation. The objective is to learn a\npolicy $\\pi(a|s)$ that chooses an action that maximises the expected return at every time step\nt. A return is a sum of discounted rewards, $R_t(\u03c4) = \\sum_{t'=t}^{100}\u03b3^{t'\u2212t}r_{t'}$, where T denotes the\ntrajectory (a set of states, actions and rewards visited in an episode), and a discount factor\n$\u03b3 \\in (0, 1]$ is applied to mitigate the infinite horizon problem encountered in continuous\ntasks, in which the sum of rewards can diverge to infinity.\u00b2"}, {"title": "2.1.2 Partially Observable Markov Decision Process", "content": "Partially Observable Markov Decision Process (POMDP) is a special case of an MDP\nwhere the observation available to the agent only contains partial information of the"}, {"title": "2.1.3 Classical approaches to planning", "content": "Before the rise of data-driven learning methods, the assumption in planning was that\na model of the system or the environment is available. Planning in fully known state\nspaces with deterministic transitions has partially been solved by graph-search algorithms\nsuch as Dijkstra's algorithm [46] and the A* algorithm [81], mainly in the domain of\nnavigation [99]. However, many assumptions exist for such algorithms to work, such\nas that the environment is static, the state space is relatively low-dimensional, and the\nstate space is enumerable. For instance, manipulation tasks are high dimensional and can\nbe computationally expensive to solve with graph search algorithms. The D* algorithm\n[112, 206, 207] mitigated the static environment assumption by incrementally replanning\nwhen the agent's knowledge of the environment is updated. Sampling methods such\nas Probabilistic Roadmaps (PRM) [109] and Rapidly-exploring Random Trees (RRT)\n[115, 117] presented practical solutions that can work in high-dimensional state spaces,\nwhich are asymptotically optimal. For problems such as Chess and Go where the state\nspace is large and expensive to perform brute-force search, Monte Carlo Tree Search\n(MCTS) [40] is used to selectively explore states and evaluate the expected returns."}, {"title": "2.2 Reinforcement Learning", "content": "RL is a framework that formalises sequential decision-making as cumulative future reward\nmaximisation in an MDP [17, 213]. RL addresses problems that involve agents that interact\nwith the environment to accomplish tasks when either the full MDP is unknown or it is not\nfeasible for classical planning algorithms [116] to solve.\nThe objective of an agent is to learn a policy $\\pi(a_t|s_t)$ that maximises the expectation\nof return $R_t$, where the policy specifies the probability of choosing action $a_t$ in state $s_t$.\nApproaches in RL can be broadly classified into policy gradient methods (Section 2.2.3)\nthat directly optimises the policy based on on-policy feedback, value-based methods\n(Section 2.2.2) that learn a separate value function to estimate the expectation of returns\n(can be either on-policy or off-policy), and Actor-Critic methods (Section 2.2.4) that\ncombines the advantages of policy learning and value learning. The distinction between\non-policy and off-policy methods is explained in Section 2.2.1.\nAdvances in RL and their effective integration with neural networks as high-dimensional\npolicy and value function approximators have transformed the fields of computer vi-\nsion, robotics and games. Deep RL has outperformed humans in numerous strategic\ngames [139, 201, 229]."}, {"title": "2.2.1 On-policy and off-policy", "content": "An RL agent needs to interact with the environment and collect experiences to learn about\nthe MDP (exploration), while also trying to optimise the return (exploitation). A recurring"}, {"title": "2.2.2 Value-based methods", "content": "A value function [213] is a fundamental concept in RL, used to estimate the expected\nreturn following a policy \u03c0. The (state-)value function $V^\u03c0(s) = E_\u03c0[R_t|s_t = s]$ evaluates\nthe expected return starting from the current state s, while the action-value function\n$Q^\u03c0(s,a) = E[R_t|S_t = s, a_t = a]$ considers both the current state s and the action to\nbe taken a. An optimal policy $\u03c0^*$ is a policy that maximises the expected return for all\nstates, i.e. $\u2200s \u2208 S, V^*(s) = max_a V^\u03c0(s)$. Computing a value function for a given policy\nis called policy evaluation, and improving the policy to obtain a better return is called\npolicy improvement.\nValue Iteration\nValue Iteration (VI) [17, 213] is an algorithm to obtain an optimal policy when the\nenvironment model (state transition $P(s'|s, a)$ and reward function $R(s, a, s')$) is known\nand the state-action space is relatively small and discrete so that they can be repeatedly\nenumerated (i.e. a tabular setting). It performs policy evaluation and policy improvement"}, {"title": "2.2.3 Policy gradient methods", "content": "Policy gradient methods [210, 238, 239] directly learn and improve a policy $\\pi(a|s)$ to\nmaximise the expected returns. The key idea is to reinforce actions that resulted in high\nreturns, while suppressing actions with low returns.\nThe objective $J(\u03c0_\u03b8)$ is the expected return over all completed trajectories generated\nby the agent following policy $\u03c0_\u03b8(a|s)$:\n$J(\u03c0_\u03b8) = E_{\u03c4\u223c\u03c0_\u03b8} [R(\u03c4)] = E_{\u03c4\u223c\u03c0_\u03b8}  [\\sum_{t=0}^{T} \\gamma^tr_t ]$,\nwhere \u03b8 is a set of learnable parameters. Neural networks are suitable to model $\u03c0_\u03b8(a|s)$.\nThe policy gradient algorithm maximises the objective $J(\u03c0_\u03b8)$ by performing gradient\nascent on the policy parameters \u03b8 according to $\u2207_\u03b8J(\u03c0_\u03b8)$, which can be shown to be\nEquation (2.4) [213, 238]:\n$\u2207_\u03b8J(\u03c0_\u03b8) = E_{\u03c4\u223c\u03c0_\u03b8} [\\sum_{t=0}^{T}R(\u03c4) \u2207_\u03b8 \\log \u03c0_\u03b8(a_t|s_t)]$.\nThe probability of the action $\u03c0_\u03b8(a_t|s_t)$ corresponding to a positive $R_t(\u03c4)$ is increased,\nwhile that corresponding to a negative $R_t(\u03c4)$ is decreased.\nREINFORCE\nThe REINFORCE algorithm [238, 239] is a direct application of the policy gradient\nalgorithm, which uses Monte-Carlo approximation for the expectation in Equation (2.4).\nWhile value-based methods tend to struggle with continuous action spaces, since they\nhave to find an argmax action over the values (see Equation (2.1)), policy gradient methods\nwork with both discrete and continuous action spaces. The disadvantage is that they tend\nto have high variance because returns vary significantly from trajectory to trajectory, and\ncredit assignment for long trajectories is non-trivial.\u2074"}, {"title": "2.2.4 Actor-Critic methods", "content": "Actor-Critic methods have two components that are learnt jointly: an actor \u2014 a parame-terised policy used to take actions, and a critic \u2014 a value function used to estimate returns by bootstrapping.\nAdvantage\nAdvantage Actor-Critic (A2C) [140] combines policy gradient methods and value-based methods. It is similar to REINFORCE [238, 239], but instead of using the returns from Monte Carlo sampled experiences, it uses the advantage function A(st, at) = Rt \u2212 V(st) that measures the relative return of taking an action at in a given state st. It subtracts the baseline term V(s) to stabilise the learning of the policy. In practice, the advantage function is trained by regressing towards the TD error $\u03b4_t = r + \u03b3V^\u03c0(s') \u2212 V^\u03c0(s)$ as a reinforcing signal. The gradient of the A2C objective can be written as:\n$\u2207_\u03b8J(\u03c0_\u03b8) = E_{(s,a)\u223c\u03c0_\u03b8} [A(s, a) \u2207_\u03b8 \\log \u03c0_\u03b8(a|s)]$.\nGeneralised Advantage Estimate\nGeneralised Advantage Estimate (GAE) [193] provides a robust and low-variance estimate\nof the advantage function. GAE can be expressed as a sum of exponentially weighted\nmulti-step TD errors:\n$A^{GAE} = \\sum_{t'=t}^{T}(\u03b3\u03bb)^{t'\u2212t}\u03b4'_{t}$,\nwhere $\u03b4_t = r_t+\u03b3(1\u2212d_t)V(s_{t+1})\u2212V (s_t)$ is the TD error at time t, and \u03bb is a hyperparameter that controls the trade-off of bias and variance. When \u03bb = 0, the GAE is equivalent to the TD error \u03b4t. When \u03bb = 1, it is equivalent to a Monte Carlo estimate of the advantage."}, {"title": "2.2.5 Hierarchical Reinforcement Learning", "content": "Hierarchical Reinforcement Learning (HRL) [143, 160, 228, 242, 253] is a branch of RL\nthat involves decomposing a complex long-horizon task into a hierarchy of subproblems or\nsubtasks so that the lower-level policies can learn more primitive actions with a shorter-term\nfocus, while the higher-level policies select temporally abstracted macro-actions with a\nlonger-term focus. Where multiple tasks share many subtasks, an agent may learn a\nreusable lower-level policy with a higher-level policy learning to optimise the task-specific\nobjective. This allows the agent to learn more efficiently and generalise better to novel\nsituations.\nIn HRL, subgoals [65], credit assignment [227], and options [9, 111, 166, 214, 255]\nare key building blocks in achieving this decomposition. Subgoals are intermediate goals\nthat the agent needs to achieve in order to complete the overall task. The agent uses\nsubgoals to incentivise the agent to visit state regions which brings the agent closer to\nachieving its ultimate objective. Credit assignment is the process of assessing the level\nof each action's contribution to the overall performance of the agent. HRL introduces\ntemporal abstraction to task execution, which naturally allows long-term credit assignment\n(e.g. advantages computed for higher-level actions).\nThe Options Framework\nOptions [166, 214] are temporally extended actions that allow the agent to make high-level\ndecisions in the environment. Each option corresponds to a specialised low-level policy\nthat the agent can use to achieve a specific subtask. In the Options Framework, the\ninter-option policy $\u03c0(z_t|s_t)$ and an option termination probability $\u03b6(s_{t+1}, z_t)$ govern the"}, {"title": "2.2.6 Transfer and generalisation in Reinforcement Learning", "content": "Many findings and techniques for standard deep learning in terms of transfer learning [217],\nmulti-task learning [41], and Meta Learning [90] are applicable or translatable in the\ncontext of deep RL [244]. Policy and value networks, which often have shared parameters,\ncan be pre-trained on general tasks before being fine-tuned on a more specific task [12] or\nby learning better representations from auxiliary tasks such as learning the environment\ndynamics and reward models [75, 79, 80, 144], sometimes referred to as world modelling\n(see Section 2.2.7). Recent approaches model the RL problem as a more general sequential\nmodelling task, motivating applications of large transformer models [12, 32, 158, 175,\n226]. This formulation allows foundation models [21] and large pre-trained models to be\nrepurposed or fine-tuned on specific tasks [12, 49, 191, 248].\nMeta Reinforcement Learning (Meta-RL) [15, 50, 165, 171, 231, 250] focuses on\ndeveloping methods that enable agents to learn a general policy that can efficiently be\nadapted to solve new tasks with a small amount of data. The aim is to learn a shared\nrepresentation across many different tasks in order to learn transferable skills. Large\ntransformer policies are used in recent work such as AdA [165] so that the agent can infer\nand retain knowledge about the current task, which may be used to exhibit task-specific\nbehaviour.\nSimilarly, Skill-based RL [145, 163, 198] aims to develop agents that can learn a\nset of reusable skills that can be composed to solve new tasks. These skills are designed\nto be generalisable and can be often learned from offline data [162, 174, 186]. This\napproach allows agents to solve new tasks with few-shot learning and adapt to changes in\nthe environment more efficiently. These approaches are compatible with HRL, outlined in\nSection 2.2.5."}, {"title": "2.2.7 Model-based methods", "content": "Algorithms considered so far fall under the category of model-free methods, which do\nnot require explicit modelling of the environment's state transition dynamics. In some\nproblems, especially in the case of simulations, games and robotics, the transition dynamics\nmay be readily accessible as system definitions, in which case it could be beneficial to\ntake advantage of that knowledge. In other problems where the transition dynamics are\nnot known, modelling this has its pros and cons. The advantage is that, once a good model\nis learnt, it can be used to forecast moves without having to act in the environment. This\nincreases sample efficiency, which is highly beneficial when collecting experiences is\ncostly. On the other hand, it is difficult to learn an accurate and comprehensive model, and\nlong-term planning using inaccurate models can lead to compounding errors, making the\nproblem intractable. Moerland et al. [141] performs a detailed analysis of model-based\nmethods.\nMonte Carlo Tree Search\nMCTS [40] is a widely used model-based method for problems with deterministic discrete\nstate spaces with known transition functions. It samples candidate actions from the current\npolicy and performs Monte Carlo rollouts to explore states and estimate their values.\nMCTS was used as a core planning component in AlphaGo [201] to solve Go. It used\nneural networks to learn a policy to guide the search and a value function to estimate the\nvalues of leaf nodes. A small and fast policy network is used for rapid MCTS rollouts,\nwhile a large and more expressive network is used as the main strong policy network to be\ntrained. In AlphaZero [202] that solved Go, Chess and Shogi, rollout simulations with a\nsmall network were replaced with a greedy strategy of expanding a child node with the\nhighest Upper Confidence Bound (UCB) score.\u2076"}, {"title": "2.3 Other learning methods", "content": "Imitation Learning (IL) learns a policy from expert trajectories rather than from trial and\nerror. Perhaps the most intuitive and straightforward way of learning a policy, Behavioural\nCloning (BC) is a simple form of IL that uses supervised learning to train an agent policy\n\u03c0(a|s) given a dataset of state and action pairs as expert demonstrations. A policy trained\nonly with BC is prone to failure, however, since compounding errors during rollout can\ntake the agent to states that are not encountered during training. DAgger [180] is a method\nto overcome the limitation of BC by iteratively collecting expert labels for online rollouts\nand adding these samples to the training dataset, ensuring better coverage of the state space\nlikely to be visited by the agent policy.\nInverse Reinforcement Learning (IRL) [148, 260, 261] is a class of methods that aims\nto recover an underlying reward function that best explains the expert trajectories. Once\nthe reward function is estimated, standard RL techniques can be applied. This approach\ngeneralises better than BC in tasks in which inferring the reward mechanism is easier\nthan learning the policy directly. However, IRL is an ill-posed problem since there are\nmany possible reward functions that can explain the given trajectories. Hence, additional\nassumptions must be introduced to constrain the learning problem.\nGenerative Adversarial Imitation Learning (GAIL) [87] applies Generative Adversar-\nial Networks (GANs) [66] to the problem of IL, training a policy to generate behaviour\nindistinguishable from expert demonstrations. This setup enables the model to learn\ncomplex behaviours.\nPre-training via imitation learning on demonstrations [12] is an effective strategy\nfor learning representations that are transferrable to solving more general tasks (see\nSection 2.2.6). World Modelling (see Section 2.2.7) learns to reconstruct the observations"}, {"title": "2.3.1 Imitation Learning", "content": "Imitation Learning (IL) learns a policy from expert trajectories rather than from trial and\nerror. Perhaps the most intuitive and straightforward way of learning a policy, Behavioural\nCloning (BC) is a simple form of IL that uses supervised learning to train an agent policy\n\u03c0(a|s) given a dataset of state and action pairs as expert demonstrations. A policy trained\nonly with BC is prone to failure, however, since compounding errors during rollout can\ntake the agent to states that are not encountered during training. DAgger [180] is a method\nto overcome the limitation of BC by iteratively collecting expert labels for online rollouts\nand adding these samples to the training dataset, ensuring better coverage of the state space\nlikely to be visited by the agent policy.\nInverse Reinforcement Learning (IRL) [148, 260, 261] is a class of methods that aims\nto recover an underlying reward function that best explains the expert trajectories. Once\nthe reward function is estimated, standard RL techniques can be applied. This approach\ngeneralises better than BC in tasks in which inferring the reward mechanism is easier\nthan learning the policy directly. However, IRL is an ill-posed problem since there are\nmany possible reward functions that can explain the given trajectories. Hence, additional\nassumptions must be introduced to constrain the learning problem.\nGenerative Adversarial Imitation Learning (GAIL) [87] applies Generative Adversar-\nial Networks (GANs) [66] to the problem of IL, training a policy to generate behaviour\nindistinguishable from expert demonstrations. This setup enables the model to learn\ncomplex behaviours.\nPre-training via imitation learning on demonstrations [12] is an effective strategy\nfor learning representations that are transferrable to solving more general tasks (see\nSection 2.2.6). World Modelling (see Section 2.2.7) learns to reconstruct the observations"}, {"title": "2.3.2 Value Iteration Networks", "content": "Value Iteration Network (VIN) proposed by Tamar et al. [216"}]}