{"title": "Project SHADOW: Symbolic Higher-order Associative Deductive reasoning On Wikidata using LM probing", "authors": ["Hanna Abi Akl"], "abstract": "We introduce SHADOW, a fine-tuned language model trained on an intermediate task using associative deductive reasoning, and measure its performance on a knowledge base construction task using Wikidata triple completion. We evaluate SHADOW on the LM-KBC 2024 challenge and show that it outperforms the baseline solution by 20% with a F1 score of 68.72%.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have performed increasingly well in a wide range of semantic tasks including those involving leveraging knowledge from the models themselves [1]. This lead to research avenues investigating the capabilities of these models in knowledge-related tasks involving knowledge graphs and ontologies on the one hand, and measuring the intrinsic knowledge contained in LLMs on the other hand[1]. The Language Model Knowledge Base Construction (LM-KBC\u00b9) challenge proposes to evaluate intrinsic language model (LM) knowledge using techniques like LM probing and prompting[2] to construct knowledge bases by completing triples of subject entities and relations with the relevant object entities. In this work, we present SHADOW, a fine-tuned model on knowledge base triples, and evaluate it on the LM-KBC task. We follow a methodology inspired from associative deductive reasoning[3] and leverage that technique to incorporate it in re-defining the probing problem to train the model more effectively. The rest of the work is organized as follows. In section 2, we discuss some of the related work. Section 3 describes our experimental framework. In section 4, we report our results and discuss our findings. Finally, we conclude in section 5."}, {"title": "2. Related work", "content": "LM probing has been studied and evaluated in different research avenues. In their work, Vuli\u0107 et al. study the information stored in LLMs with respect to their architecture, focusing on the factors behind their understanding of lexical semantics[4]. Other techniques leverage prompting"}, {"title": "3. Experiments", "content": "This section describes our experiments in terms of data, model and training process."}, {"title": "3.1. Dataset", "content": "The data provided by the organizers are triples of the form (subject, relation, object). The following relations are considered:\n\u2022 countryLandBordersCountry: Null values possible (e.g., Iceland)\n\u2022 personHasCityOfDeath: Null values possible\n\u2022 seriesHasNumberOfEpisodes: Object is numeric\n\u2022 awardWonBy: Many objects per subject (e.g., 224 Physics Nobel prize winners)\n\u2022 companyTradesAtStockExchange: Null values possible\nThe data is provided in 3 sets: train, validation and test. The test set is used as the official submission evaluation set. The number of triples in each set is:\n\u2022 377 in the train set\n\u2022 378 in the validation set\n\u2022 378 in the test set\nFor the subject and object in every triple, both the ID and the label are provided. A sample triple is thus represented as such: {\"SubjectEntity\": \"Belize\", \"SubjectEntityID\": \"Q242\", \"ObjectEntities\": [\"Guatemala\", \"Mexico\"], \"ObjectEntitiesID\": [\"Q774\", \"Q96\"], \"Relation\": \"countryLandBordersCountry\"}."}, {"title": "3.2. Model", "content": "We train SHADOW as a conditional generation model from a base flan-t5-small\u00b2 model and fine-tune it on the provided data. The training hyperparameters are configured as such: {learning_rate: 1e-04, train_batch_size: 4, eval_batch_size: 4, num_epochs: 20, question_length:"}, {"title": "3.3. Setup", "content": "We design our experiment to combine LLM probing with a symbolic component and indirectly evaluate the intrinsic knowledge found in LLMs on Wikidata knowledge graphs. We shift the focus away from generating correct SPARQL queries to retrieving the relevant objects for each subject and relation pair by designing templates containing the dynamic queries needed to answer the generic question: What Z completes the relationship Y for X?, where X, Y and Z refer respectively to the subject, relation and object(s) in a triple. Since the challenge deals with 5 types of relations, we design a total of 5 templates and generate a numerical template ID to identify them. We then pair each subject and relation from a triple with the corresponding template ID (i.e. the template ID pointing to the correct SPARQL query that retrieves the corresponding object(s)). SHADOW is then trained to generate the correct template ID depending on the given subject and relation without seeing the SPARQL queries, which implies associating the correct template with the relation type on one hand, and learning to generate an acceptable template ID on the other. Figure 1 shows our experimental design process. The training is done by splitting the train set into an 80-20 split randomly and training on the 80%. The remaining 20% are incorporated into the validation set. Table 1 shows the training results."}, {"title": "4. Results", "content": "Tables 2 and 3 capture the results on the official challenge test set. Overall, SHADOW performs well on the template identification task for the different relations. A closer inspection of the results shows that the model performs worse on the countryLandBordersCountry relation which can be interpreted by the fact that the corresponding query targets property P47, or shares border with, which encompasses but is not limited to results sharing land borders (i.e. it also targets objects sharing sea borders). The nature of the property explains the high recall score, which retains a larger set of objects for that relation, and the low precision which reflects the few actual correct objects expected. The reason for choosing P47 is that it was the closest property that meets the required relation. The result is propagated in the zero-object cases, wherein the recall score is fairly high compared to a low precision score. This results is directly impacted by the choice of coding the queries as templates, sacrificing flexibility in results in favor of correct syntax instead of leaving the query generation in the hands of the model and risking volatility"}, {"title": "5. Conclusion", "content": "In this work, we show how a fine-tuned LLM model can leverage intrinsic knowledge through LM probing and combine it with associative deductive reasoning to build disambiguated knowledge bases. The performance of SHADOW, our model, outperforms the baseline by disambiguating relation types and indirectly associating them with relevant knowledge graph completion queries. Our experiments show however that LLMs possess uneven knowledge with respect to Wikidata relations and leave much room for improvement in that area. Future work will focus"}]}