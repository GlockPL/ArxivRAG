{"title": "LoraMap: Harnessing the Power of LoRA Connections", "authors": ["Hyeryun Park", "Jeongwon Kwak", "Dongsuk Jang", "Sumin Park", "Jinwook Choi"], "abstract": "Large Language Models (LLMs) can benefit\nfrom mitigating hallucinations through fact-\nchecking and overcoming substantial computa-\ntional overhead with parameter-efficient tech-\nniques such as Low-Rank Adaptation (LoRA).\nWhile some studies have explored the paral-\nlel integration of multiple LoRAs, these ap-\nproaches need attention to the connections be-\ntween them. This paper investigates methods\nto establish connections among multiple Lo-\nRAs. We create three reasoning datasets tai-\nlored to fact-checking and fine-tune individ-\nual LoRAs, allowing them to view and reason\nfrom diverse perspectives. Then, we explore\nstrategies for allocating these reasoning LoRAS\nand introduce LoraMap, an approach to map\nconnections between them. The results on the\nfact-checking task demonstrate that the perfor-\nmance of LoraMap is superior to LoraHub, an\nexisting LoRA composition method. LoraMap\nalso outperforms with significantly fewer pa-\nrameters than LoraConcat, which concatenates\nLoRAs and further fine-tunes them.", "sections": [{"title": "1 Introduction", "content": "With the rapid progress in research leveraging\nLarge Language Models (LLMs) such as GPT-4\n(OpenAI, 2023), PaLM (Chowdhery et al., 2023),\nLLaMA (Touvron et al., 2023), and Flan-T5\n(Chung et al., 2022) in various natural language pro-\ncessing tasks, several challenges have also emerged.\nThe model can pose a significant risk to reliability\nand trustworthiness due to the issue of generating\nfalse information, known as hallucination (Ji et al.,\n2023). One way to alleviate this problem is using\nfact-checking to verify LLM outputs or stand-alone\nclaims (Gupta et al., 2022; Chamoun et al., 2023).\nAs in Figure 1, a fact-checking process classi-\nfies a claim into true, false, or more sophisticated\nlabels based on textual evidence such as Wikipedia\npassages, news articles, and other relevant docu-\nments (Thorne et al., 2018; Guo et al., 2022). In\nbiomedical and health domains, serious problems\ncan arise when people perceive false information as\ntruth, highlighting the importance of fact-checking.\nAccordingly, many studies have been explored, re-\nsulting in the development of datasets: SciFact\n(Wadden et al., 2020), PubHealth (Kotonya and\nToni, 2020), COVID-Fact (Saakyan et al., 2021),\nand HealthVer (Sarrouti et al., 2021). This paper\nfocuses on the COVID-Fact dataset, which covers\nfact-checking related to the COVID-19 pandemic.\nAnother challenge is that fine-tuning the LLMs\nrequires high computational demands. Parameter-\nefficient fine-tuning techniques can address this is-\nsue, especially Low-rank adaptations (LoRA) (Hu\net al., 2021). Furthermore, some studies have ex-\nplored the integration of multiple task-specific Lo-\nRAs to address other tasks (Huang et al., 2023;\nLiu et al., 2023; Gao et al., 2024; Li et al., 2024;\nDou et al., 2023). Among these methods, LoraHub\n(Huang et al., 2023) learns weights for each LoRA\nand computes their weighted sum in parallel, which\nmay weaken the influence of the pivotal LoRA.\nThis paper investigates the methods of establish-\ning connections among LoRAs to exchange their\nspecialized insights as an alternative to parallel in-\ntegration. Our main contributions are as follows:\n\u2022 We create three reasoning datasets tailored to\n fact-checking and fine-tune LoRA for each\n dataset, allowing them to infer from various\n perspectives.\n\u2022 We investigate how to connect these reason-\n ing LoRAs and introduce LoraMap. Inspired\n by the information-processing behavior of the\n human brain in neuroscience, it learns connec-\n tions rather than a linear sum of LoRAS.\n\u2022 The results on the COVID-Fact dataset demon-\n strate that LoraMap exhibits superior perfor-\n mance than LoraHub and also outperforms\n LoraConcat even with significantly fewer pa-\n rameters."}, {"title": "2 Related Work", "content": "2.1 Biomedical Fact-Checking\nManual fact-checking has become challenging and\ntime-consuming as biomedical literature rapidly ex-\npands. Several studies have attempted to construct\nbiomedical fact-checking datasets and train vari-\nous models. For the PubHealth dataset, the SciB-\nERT model achieves the highest f1-score among\nthe BERT models (Kotonya and Toni, 2020). For\nthe SciFact (Wadden et al., 2020), the best model\non the leaderboard\u00b9 is MultiVerS (Wadden et al.,\n2022), a Longformer model (Beltagy et al., 2020)\ntrained with rationale sentence selection and fact-\nchecking label prediction. For the COVID-Fact, the\nROBERTa model is fine-tuned on fact-checking and\nentailment inference datasets (Saakyan et al., 2021).\nFor the HealthVer, the T5-base model performed\nbetter than the BERT models (Sarrouti et al., 2021).\n2.2 Fact-Checking with LLMS\nRecent studies have explored the potential of LLMs\nfor general domain fact-checking through a zero-\nshot approach (Chern et al., 2023; Li et al., 2023;\nWang et al., 2023b), hierarchical prompting (Zhang\nand Gao, 2023), multiagent debate approach (Du\net al., 2023), question answering (Pan et al., 2023),\nand combining various language models (Min et al.,\n2023). The results demonstrate the effectiveness of\nusing LLMs, but they are still room for improve-\nment in factual reasoning (Laban et al., 2023; Wang\net al., 2023a).\n2.3 Parameter-efficient Fine-tuning\nSeveral studies have introduced parameter-efficient\nfine-tuning techniques that freeze the original\nmodel parameters and only fine-tune a few addi-\ntional parameters. Adapter tuning (Houlsby et al.,\n2019; Pfeiffer et al., 2020) inserts a layer into"}, {"title": "3 Methods", "content": "3.1 Reasoning Dataset Generation\nDetermining the veracity of a claim requires iden-\ntifying key entities and their relationships within\nthe claim and evidence and then analyzing where\nthey differ. In this context, we hypothesize that\nidentifying contrasting or common factors between\nthe claim sentence and its corresponding evidence\ntext can help the fact-checking model. Therefore,\nwe customize the three reasoning tasks for fact-\nchecking: DifferenceCoT, EntityCoT, and Correct-\nClaim.\nDifferenceCoT is a task that generates a text that\ndetails the contextual differences between\nclaim and evidence, such as relation, topic,\nand level of detail.\nEntityCoT is a task that extracts synonymous\nbiomedical entities that appear simultaneously\nin the claim sentence and the evidence text.\nCorrectClaim is a task that revises a given claim\nsentence based on the evidence."}, {"title": "3.2 Fine-tuning Reasoning LoRAs", "content": "The next step is to fine-tune LoRAs for each task.\nWe use Flan-T5 as the base model due to its range\nof model size options and its strong performance\nin zero-shot, few-shot, and CoT (Chung et al.,\n2022). The lightweight module LoRA exists in\nall transformer attention layers of the base model.\nSpecifically, as shown in Figure 2, LoRA oper-\nates within the query and value parts of the en-\ncoder self-attention, decoder self-attention, and\nencoder-decoder attention layers. For each task\n\\(t \\in \\{1,2,3\\}\\), LoRA consists of a weight ma-\ntrix \\(A_t \\in \\mathbb{R}^{d \\times r}\\) for down-projection of features\nto a smaller dimension r, and a weight matrix\n\\(B_t \\in \\mathbb{R}^{r \\times d}\\) for up-projection to the original dimen-\nsion d. By freezing the weights of the base model\nand training only the weights of LoRA, training\nrequires much fewer parameters."}, {"title": "3.3 Connecting Reasoning LoRAs", "content": "The final step is to investigate methods for allocat-\ning and connecting the reasoning LoRAs, namely\nLoraHub, LoraConcat, and LoraMap. Figure 3\nillustrates the differences among the methods.\nLoraHub computes the weighted sum to gener-\nate \\(\\hat{A} \\in \\mathbb{R}^{d \\times r}\\) and \\(\\hat{B} \\in \\mathbb{R}^{r \\times d}\\). This framework\nfreezes all \\(A_t\\) and \\(B_t\\) matrices and learns only the\ncoefficients for each LoRA using a gradient-free\napproach. Our LoraHub loads three reasoning Lo-\nRAs along with the 20 LORA modules following\nthe original LoraHub setting\u00b2.\nLoraConcat concatenates the matrices \\(A_t\\) and \\(B_t\\)\nof the three reasoning LoRAs to produce \\(A_{cat} \\in\n\\mathbb{R}^{d \\times 3r}\\) and \\(B_{cat} \\in \\mathbb{R}^{3r \\times d}\\).\n\\(A_{cat} = [A_1; A_2; A_3], B_{cat} = [B_1; B_2; B_3]\\)\nWe then fine-tune the \\(A_{cat}\\) and \\(B_{cat}\\) matrices tar-\ngeting the COVID-Fact dataset. LoraMap not\nonly concatenates the three reasoning LoRAs into\n\\(A_{cat}\\) and \\(B_{cat}\\) but also insert the trainable matri-\nces \\(A_{map} \\in \\mathbb{R}^{3r \\times m}\\) and \\(B_{map} \\in \\mathbb{R}^{m \\times 3r}\\) between\nthem. LoraMap freezes LoRAs that maintain spe-\ncialized reasoning capabilities and learns the con-\nnection maps between them by fine-tuning only\n\\(A_{map}\\) and \\(B_{map}\\). We define the mapping dimen-\nsion m based on the ratio of trainable parameters\nto the total number of parameters in the model.\n\\(m = \\frac{ratio \\times num \\text{ of total parameters}}{3r \\times \\text{num of trainable layers} \\times 100}\\)\nThe number of trainable layers is the total number\nof layers of \\(A_{map}\\) and \\(B_{map}\\) in the model. The\ntrainable parameters of this layer are \\(m \\times 3r\\)."}, {"title": "4 Experimental Results", "content": "4.1 Reasoning LoRAs\nWe independently finetune DifferenceCoT LORA,\nEntityCoT LORA, and CorrectClaim LORA in-\nserted in the Flan-T5 models, using a fixed seed\nof 42 for reproducibility. The Flan-T5 model of-\nfers a range of options: small (77M), base (249M),"}, {"title": "4.2 Connecting LoRAs for Fact-checking", "content": "We conduct experiments integrating multiple rea-\nsoning LoRAs on the COVID-Fact dataset. Given\nthe prompt \"What is the class of the Claim by re-\nferring to the Context? Choose only from TRUE or\nFALSE.\" with claim and context, the output should\nbe \"The claim is TRUE/FALSE\"."}, {"title": "4.2.1 Results of Small Language Model", "content": "The performance of the Flan-T5-large on the\nCOVID-Fact test dataset is shown in Table 2. In the\nzero-shot setting, the Flan-T5-large model predom-\ninantly predicted TRUE with an f1 score of 0.5453.\nThe key result is a comparison of connecting meth-\nods of multiple reasoning LoRAs: LoraHub, Lo-\nraConcat, and LoraMap. We experiment with var-\nious training instances, and Table 2 presents the\nbest result among 10-shot, 20-shot, 50-shot, and\n100-shot, the best result among 200-shot, 500-shot,\nand 1000-shot, and the result when using the entire\ndataset. Although training with more than 100 in-\nstances is neither simple nor scalable, we aim to\nidentify the minimum number of instances required\nto achieve satisfactory performance in LoraConcat\nand LoraMap. To provide statistically reliable re-\nsults, all metric scores are the average of ten re-\npeated experiments performed with ten fixed seeds\n(42, 64, 128, 256, 512, 1024, 2048, 4096, 8192,\n16384).\nLoraHub achieves the highest f1-score of 0.6145\nat 200-shot, and its performance does not increase\nas the number of training data increases. Although\ntraining LoraHub with less than 100 examples is\nfeasible, its performance is suboptimal. In contrast,\nLoraConcat and LoraMap generally demonstrate\nimproved f1-scores as training instances increase.\nNotably, LoraConcat yields the best f1-score of\n0.8126 at 1000-shot, and LoraMap achieves the\nhighest f1-score of 0.8239 when using all instances.\nWhile fine-tuning the Flan-T5-large model, the\nmapping dimension m of LoraMap is set to 16,\nthe same as the rank parameter of LoRA."}, {"title": "4.2.2 Ablation Study", "content": "We further compare the results depending on the\nselection of LoRAs. Table 3 exhibits the results\nof LoraHub across different training instances, pre-\nsenting zero-shot performance and the best result\namong 10-shot, 20-shot, 50-shot, and 100-shot\nlearning. All experiments use a fixed seed of 42.\nThe LoraHub originally uses 20 randomly selected\nLoRAs (base20), yielding 0.5423 under the zero-\nshot setting, which improves after fine-tuning 20\ncoefficients for each layer. When employing only\nthree reasoning LoRAs, the zero-shot performance\nis higher than that of 20 random LoRAs. How-\never, the performance does not improve while fine-\ntuning due to the difficulty of training only with\nthree coefficient weights. We also experimented\nwith three random LoRAs (base3) to verify this,\nand the results demonstrate the same tendency to\nstruggle with fine-tuning. Consequently, we kept\n20 random LoRAs and added three reasoning Lo-\nRAs, a setting that shows the best macro f1 score.\nLoraHub outputs coefficients after training,\nwhich is the impact of each LoRA module. The\ncoefficients for the three reasoning LoRAs are all\nclose to 0.5, four out of the 20 base modules also ex-\nhibiting 0.5, mostly trained for question-answering,\nand the remaining 16 show values close to zero or\nnegative. The coefficients confirm that our reason-\ning LoRAs play an important role in fact-checking.\nTable 4 shows the results of the ablation study on\nLoraMap to show the effectiveness of each LoRA.\nAll experiments use the entire training dataset with\na fixed seed 42. Removing each LoRA degrades\nthe macro-f1 score, and the most influential one is\nDifferenceCoT LORA, which exhibits the largest\nperformance decrease. DifferenceCoT, ClaimCor-"}, {"title": "4.2.3 Applicability to LLMs", "content": "Table 5 presents the performance of the LLMs on\nthe COVID-Fact test dataset. In the zero-shot set-\nting, the GPT-4 API with CoT prompting yields an\nf1 score of 0.6959, and the Flan-T5-xxl model ex-\nhibits an f1 score of 0.7021. The zero-shot prompt\nis shown in Appendix A. We compare LoraConcat\nand LoraMap when using all training instances for\nthe Flan-T5-xxl. As previously mentioned, we de-\nfine the mapping dimension m of \\(A_{map}\\) and \\(B_{m\u0430\u0440}\\)\nto scale the size of LoraMap according to the model\nsize. Adjusting the ratio of trainable parameters\nto total parameters from 0.002 to 0.05 allows m\nto range from 16 to 400. The macro-f1 score gen-\nerally improves as the size of LoraMap increases.\nLoraMap (4.4M) outperforms LoraConcat (56M)\neven with significantly fewer trainable parameters.\nThe total model parameters for LoraMap (4.4M)\namount to 11.196B, while for LoraConcat (56M)\nthe total is 11.191B. All experiments were per-\nformed with one RTX 3090 GPU. Training takes\n17 hours and 5 minutes for LoraConcat (56M) and\n15 hours and 22 minutes for LoraMap (4.4M). For\neach test instance, LoraConcat takes less than 3\nseconds, and LoraMap takes less than 2 seconds."}, {"title": "4.3 Experimental Settings", "content": "When fine-tuning the three reasoning LoRAs, the\nexperimental settings are identical, with a fixed\nseed 42. Depending on the dataset length, we set"}, {"title": "5 Discussion", "content": "5.1 Design Motivation of LoraMap\nThe experimental findings highlight the signifi-\ncance of the connection and allocation strategies of\nmultiple reasoning LoRAs. The main motivation\nfor the LoraMap architecture is that the existing\nLoraHub linearly adds all trained LoRA weights.\nThis linear approach can diminish the importance\nof matrix values due to the averaging effect, es-\npecially when weights vary significantly despite\nthe distinct roles of LoRAs. We believe that in\nthe human brain, training does not occur through\nlinear addition but rather domain-specific training,\nenhancing the brain's functions for a specific task.\nLoraConcat architecture may experience a loss\nof reasoning capability due to the catastrophic for-\ngetting problem as the concatenated LoRA matri-\nces undergo further fine-tuning. To address this, we\ndesign LoraMap, which preserves these matrices\nin their original states and learns only the connec-\ntion mappings among LoRAs to facilitate decision-\nmaking from diverse reasoning perspectives. As\neach brain region possesses different knowledge\nand functionalities, establishing interconnections\namong them would be important. Therefore, we\nuse the \\(A_{map} \\in \\mathbb{R}^{3r \\times m}\\) and \\(B_{map} \\in \\mathbb{R}^{m \\times 3r}\\) to\ntrain them on homogeneous functions while main-\ntaining areas for each distinct function, which is\nthe \\(A_{cat} \\in \\mathbb{R}^{d \\times 3r}\\) and \\(B_{cat} \\in \\mathbb{R}^{3r \\times d}\\).\nAdditionally, differences in the number of train-\nable parameters can also affect performance. When\nthere are three LoRAs to combine, LoraHub learns\nonly three coefficients, which may not be suffi-\ncient for complex tasks. In contrast, LoraConcat\nlearns 2 x d x 3r parameters, and LoraMap learns\n2 \u00d7 3r \u00d7 m parameters. When using fixed m, sub-\nstantial parameter growth could pose a significant\nissue as the number of LoRAs increases. However,\nby specifying m of LoraMap, we can adjust the\nnumber of trainable parameters accordingly."}, {"title": "5.2 Applying LoraMap to Other Tasks", "content": "LoraMap can be applied to other tasks but needs\nsome modifications. First, the reasoning LoRAs\nto combine should be relevant to the downstream\ntask. For a question-answering task, where a ques-\ntion and context are given for answering, the Dif-\nferenceCoT and EntityCoT could work as helper\ntasks, whereas ClaimCorrect may not be suitable.\nSecond, training and adding new reasoning\nLORA is available. Given a new helper task, we\nneed to train a new LoRA and subsequently train\nthe LoraHub, LoraConcat, and LoraMap. All these\nmodels need retraining to adjust the coefficient or\ncorresponding matrix weights. We also considered\nthe task of predicting the triplets (entity-relation-\nentity) from the claim and context, but the poor\nresults of GPT-4 led to its exclusion from our study."}, {"title": "5.3 Reasoning Dataset Assessment", "content": "In order to evaluate the quality of the reasoning\ndatasets generated by GPT-4, two graduate stu-\ndents specializing in biomedical engineering per-\nform a manual assessment. For each dataset, we\nrandomly select 100 instances to evaluate. The\nDifferenceCoT dataset shows an accuracy of 0.93,\nand the EntityCoT dataset shows an accuracy of\n0.89. Additionally, we computed Cohen's kappa\nto assess the inter-rater reliability of manual eval-\nuations. The kappa values for DifferenceCoT and\nEntityCoT were 0.8465 and 0.8629, respectively.\nThen, an emergency medicine clinician with over\nfive years of experience reviewed and confirmed\nthe dataset we manually evaluated.\nAnalyzing the errors in DifferenceCoT, GPT-4\nstruggles to distinguish between claim and context\nwhen differing numerical values are mentioned.\nFor example, it misses the difference when the\nclaim states that the governor cancels school for at\nleast two hours, but the context says it closes for at\nleast two weeks. Likewise, it also misses when the\nclaim mentions 1,000 people, but the context refers\nto at least 1 percent of the population. GPT-4 also\nfails due to a lack of biomedical knowledge, such\nas confusing bacterial viromes and human viromes\nas the same.\nAnalyzing the errors in EntityCoT, GPT-4 in-\ncorrectly identifies distinct biomedical entities as\nsynonymous entities. For instance, it equates 'n\ngene of sars-cov-2' from the claim with \u2018N gene\nassay' from the context or equates 'covid-19 in-\nfection' from the claim with 'COVID-19 vaccine\nprospects' from the context. Additionally, it misses\ncertain synonymous entities, such as 'sars-cov-2'\nin the claim and \u2018COVID-19' in the context. Lastly,\nGPT-4 shows one case of hallucination by identify-\ning an entity that is present in the claim but absent\nin the context."}, {"title": "6 Conclusion", "content": "This paper investigates methods to establish con-\nnections among multiple reasoning LoRAs. We\ngenerate three reasoning datasets and fine-tune in-\ndividual LoRAs to enable inference from differ-\nent perspectives. Subsequently, we introduce Lo-\nraMap, an approach to learning the connection map\nbetween them. Our LoraMap statistically outper-\nforms LoraHub and LoraConcat on the Flan-T5-\nlarge model. Even for the Flan-T5-xxl model, Lo-\nraMap outperforms LoraConcat even with signif-\nicantly fewer parameters. We anticipate that this\npaper will pave the way for approaches in mapping\nand designing connections between LoRAs."}, {"title": "7 Limitations", "content": "There are true and false claims in the COVID-Fact\ndataset for each piece of evidence, so we automati-\ncally generate the CorrectClaim dataset. However,\nto apply this to other fact-checking datasets, re-\nsearchers should consider the CoT prompting with\nGPT-4, similar to DifferenceCoT and EntityCoT.\nAdditionally, it is essential to establish a method\nfor assessing the quality of GPT-4 reasoning.\nIn real-world scenarios, the LLM outputs need\nto be verified. As traditional fact-checking models\nmostly verify a given claim, some research converts\nthe LLM output into multiple claims. By verifying\neach claim and averaging its veracity, we can assess\nthe reliability of the LLM outputs. Therefore, we\nfocused on fact-checking stand-alone claims.\nOur model is unsuitable for cases where only\nclaims are present without evidence. In this case,\nappropriate evidence should be searched and pro-\nvided. The COVID-Fact dataset contains evidence\nfor each claim, so there was no need to search for\nevidence in this work. Making integrated judg-\nments regarding multiple pieces of evidence is also\nimpossible.\nFinally, examining LoraConcat and LoraMap on\nvarious open-source LLMs and other fact-checking\ndatasets in the biomedical and health domains is\nnecessary."}]}