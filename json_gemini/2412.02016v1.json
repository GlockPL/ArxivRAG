{"title": "Explore Reinforced: Equilibrium Approximation with Reinforcement Learning", "authors": ["Ryan Yu", "Mateusz Nowak", "Qintong Xie", "Michelle Yilin Feng", "Peter Chin"], "abstract": "Current approximate Coarse Correlated Equilibria (CCE) algorithms struggle with equilibrium approximation for games in large stochastic environments but are theoretically guaranteed to converge to a strong solution concept. In contrast, modern Reinforcement Learning (RL) algorithms provide faster training yet yield weaker solutions. We introduce Exp3-IXrl - a blend of RL and game-theoretic approach, separating the RL agent's action selection from the equilibrium computation while preserving the integrity of the learning process. We demonstrate that our algorithm expands the application of equilibrium approximation algorithms to new environments. Specifically, we show the improved performance in a complex and adversarial cybersecurity network environment - the Cyber Operations Research Gym and in the classical multi-armed bandit settings.", "sections": [{"title": "Introduction", "content": "Reinforcement Learning (RL) is a goal-oriented machine learning paradigm that primarily focuses on how agents act in an environment to maximize cumulative reward. In game theory, a Nash Equilibrium describes a strategy in which no player can gain by unilaterally changing their strategy if the strategies of the others remain unchanged [1]. In most real-world scenarios, which consist of dynamic complex environments in multi-step situations, the equilibrium (or equilibria) is computationally intractable [2].\nGame theory and RL both focus on decision-making within uncertain en-vironments, with RL learning an optimal policy that maximizes some notion of a reward and game theory analyzing strategic interactions among players and predicting equilibrium outcomes of these interactions [3]. Combining these two disciplines has led to the development of agents that not only adapt to multifaceted, changing, or uncertain settings but also anticipate and strategi-cally react to the actions of other agents, thereby enhancing the effectiveness of decision-making systems in various applications [4].\nIn this paper, we describe a new technique that adapts the powerful, com-putational feasibility of the Exponential-weight algorithm for Exploration and Exploitation (EXP3) algorithm to approximate the coarse correlated equilibrium (CCE) [5]. We combine EXP3's high probability successor, EXP3-IX [6], with heuristics of the Local Best Response (LBR) algorithm [7] for sequential games and the power of reinforcement learning, to create this game-theoretic guide for reinforcement learning models."}, {"title": "Background", "content": "A simultaneous-move task can be represented as a stochastic game. We define a ba-sic, fully observable, N-player stochastic game as $(S, H, {A_i}_{i\\in N},T, {R_i}_{i\\in N},\\Upsilon)$, where S is set of all states shared by all N players, H is the maximum number of time steps, $A_i$ is the action space for player i and $A := A_1 \\times \\dots \\times A_N$ is a set of all valid actions, $T : (S \\times A) \\rightarrow S$ is a transition function, $R_i : (S \\times A \\times S) \\rightarrow R$ is a reward function for player i and $\\gamma\\in [0, 1]$ is a discount factor.\nThe notion of an equilibrium provides a strong learning objective in multi-agent settings. The most popular equilibrium is the Nash Equilibrium (NE). While approximating NE is ideal, it was shown to be PPAD-complete even in 2p0s games [8]. Weaker, more computationally feasible, equilibrium forms can be approximated using no-regret learning. In this study, we approximate a coarse correlated equilibrium (CCE) [5]. A CCE, $\\sigma$, is defined as:\n$\\forall i, s_i \\notin S, \\mathbb{E}_{s \\sim \\sigma} C_i(S) \\leq \\mathbb{E}_{s \\sim \\sigma} C_i(S_i, S_{-i})$\nwhere i represents a player, $s_i$ represents a strategy different from the recom-mended strategy, s, and $c_i$ represents the cost of following a strategy. Recently, the lower bound on number of iterations for the convergence of $\\epsilon$-CCE for a three-player extensive-form game was proven to be $2^{\\log^{2-o(1)}(|G|)}$, where |G| is the size of the game [9], while in two-player games with no chance moves, a social-welfare maximizing extensive-form CCE can be computed in polynomial time [10].\nTraditionally, regret $R_T$ at time step T and a probabilistic bound, which deals with the uncertainty introduced by learners in multi-armed bandits, called pseudo-regret $\\hat{R}_T$ is defined as:\n$R_T = \\sum_{t=1}^T l_{t, I_t} - \\min_{i\\in[K]} \\sum_{t=1}^T l_{t,i}, \\text{ and } \\hat{R}_T = \\max_{i\\in[K]} \\sum_{t=1}^T \\mathbb{E}l_{t, I_t} - \\sum_{t=1}^T \\mathbb{E}l_{t,i}$\nwhere the player has an action space of size K, $l_{t,k}$ represents the loss experienced at time step t for action $i \\in [K]$, and $I_t$ defines a forecaster's choice [11, 6, 12].\nNo-regret learning measures the difference in loss compared to the best single action in hindsight. In this study, we utilize EXP-IX, demonstrating no-regret learning with a high probability [6]."}, {"title": "Related Work", "content": ""}, {"title": "Modern reinforcement learning algorithms", "content": "Reinforcement Learning (RL) has seen significant advancements through algo-rithms like Deep Q-Networks (DQN) [13] and Policy Gradient methods [14], achieving remarkable results in Atari games and robotics. DQN introduced deep neural networks to approximate Q-values, enabling breakthroughs in complex environments. Policy gradient algorithms refine policies via gradient ascent on estimated returns but can suffer from instability and inefficiency due to high-variance gradients. TRPO [15] and PPO [16] address this by stabilizing updates with relative entropy constraints."}, {"title": "Exponential-weight algorithm for Exploration and Exploitation", "content": "Exp3 (Exponential-weight algorithm for Exploration and Exploitation) is an adversarial bandit algorithm designed for uncertain or adversarial environments. By balancing exploration and exploitation, Exp3 minimizes regret over time [5]. Exp3-IX refines the base Exp3 algorithm, by introducing a biased Implicit explo-ration toward better actions, reducing the regret's variance, and enhancing the algorithm's performance as uniform exploration has been shown to detrimentally impact the performance of learning algorithms, especially in environments with numerous suboptimal options [6]. Exp3 and Exp3-IX provide theoretical guar-antees for convergence to no-regret convergence in non-stochastic multi-armed bandit problems.\nExtending the exploration to more complex, multi-step, stochastic environ-ments is the focus of this paper."}, {"title": "Algorithm", "content": "We propose to extend the Exp3-IX with the reinforcement learning action selec-tion step (see Fig 1). Our Exp3-IXrl incorporates several extensions to ensure compatibility across types of RL methods and asymmetric, state-specific action spaces and to enable implementation in multi-step and stochastic environments.\nThe action selection of an RL agent during training serves as a non-intrusive enhancement, either at each timestep t or as an offline learning data, to leverage the exploration and convergence guarantees of an RL agent for concurrent CCE training. Exp3-IXrl extends the implicit exploration of the EXP3-IX algorithm, via the exploration-exploitation balances of RL algorithms, which might be more apt to handle more complex and stochastic game settings.\nAt each time step during learning, EXP3-IXrl will obtain an action from either the underlying RL algorithm or the CCE approximation based on the certainty threshold. The more a state is visited, the higher the certainty in the CCE approximation."}, {"title": "Experiments and Results", "content": ""}, {"title": "Environments setting", "content": "We tested Exp3-IXrl within the Cyber Operations Research Gym (CybORG) [19] Cage Challenge 2 environment [20] (CC2), a complex and adversarial cybersecurity network, where the algorithm's objective is to minimize total network infection, represented through negative rewards.\nMoreover, we test our algorithm in a stochastic and deterministic multi-armed bandit (MAB) with ten actions. For the stochastic environment, we set the rewards to a standard normal distribution centered around zero with a standard deviation of one and add a sampled random noise to the received reward when the action is chosen. As for the deterministic environment, we set the reward based on the action number and do not add additional noise during the action selection process."}, {"title": "Experimental procedure and metrics used", "content": "To compare our algorithms to previous approaches, we train each agent for 10000 in each environment and gather the cumulative reward over the next 30 timesteps. To ensure a fair comparison of our method, we average our results over 100 runs in each environment with an exact random seed used for both the baselines and our algorithm, limiting the influence of random seeds on our results.\nWithin the multi-armed bandit setting, we use classical RL algorithms as our baselines: $\\epsilon$-Greedy [21], UCB [22], and Gradient Bandit [23]. As for the CC2 environment, we use the CardiffUni agent, a hierarchical Proximal Policy Optimization (PPO) [16], which recently had its convergence guarantees proven [18] and won the Cage Challenge 2, as our baseline."}, {"title": "Results", "content": "For CC2, we achieve comparable performance with a certainty threshold of 2750 in just 10000 simulation episodes - a tenth of the training episodes of the previous winning challenge submission [19, 20] (see Fig. 2). In the multi-armed bandit scenario, we illustrate our algorithm's performance against classical reinforcement learning algorithms, used as baselines and teachers for the Exp3IX-rl. Except"}, {"title": "Conclusion and Future Work", "content": "The proposed Exp3-IXrl algorithm combines an RL agent as an explicit explo-ration bias during training with traditional coarse correlated equilibrium (CCE) approximation. It maintains the RL agent's autonomy while excelling in com-plex, stochastic environments, where current CCE-based implementations fail. Empirical results underscore the robustness and adaptability of Exp3-IXrl across diverse environments and policies, demonstrating enhanced learning depth.\nOur findings also contribute to ongoing research about the necessity of ex-ploration in CCE approximation, specifically the minimum certainty per action-observation pair. Future research should investigate how certainty could adjust to environmental feedback, potentially refining the RL agent's policy and improving Exp3-IXrl's adaptability to evolving cooperative or adversarial contexts."}]}