{"title": "AUTOMATED NEURAL PATENT LANDSCAPING IN THE SMALL DATA REGIME", "authors": ["Tisa Islam Erana", "Mark A. Finlayson"], "abstract": "Patent landscaping is the process of identifying all patents related to a particular technological area, and is important for assessing various aspects of the intellectual property context. Traditionally, constructing patent landscapes is intensely laborious and expensive, and the rapid expansion of patenting activity in recent decades has driven an increasing need for efficient and effective automated patent landscaping approaches. In particular, it is critical that we be able to construct patent landscapes using a minimal number of labeled examples, as labeling patents for a narrow technology area requires highly specialized (and hence expensive) technical knowledge. We present an automated neural patent landscaping system that demonstrates significantly improved performance on difficult examples (0.69 F\u2081 on 'hard' examples, versus 0.6 for previously reported systems), and also significant improvements with much less training data (overall 0.75 F\u2081 on as few as 24 examples). Furthermore, in evaluating such automated landscaping systems, acquiring good data is challenge; we demonstrate a higher-quality training data generation procedure by merging Abood and Feltenberger's (2018) \u201cseed/anti-seed\" approach with active learning to collect difficult labeled examples near the decision boundary. Using this procedure we created a new dataset of labeled AI patents for training and testing. As in prior work we compare our approach with a number of baseline systems, and we release our code and data for others to build upon\u00b9.", "sections": [{"title": "1 Introduction", "content": "In its simplest form, patent landscaping is the process of identifying all patents that are related to a particular technology or technology area. Patent landscapes are useful for a number of activities: it is important for assessing the coverage, value, or context of particular pieces of intellectual property, or for understanding the direction, speed, or concentration of innovation in a particular industry Hunt et al. [2007]. For example, companies create patent landscapes to evaluate the risks posed by competitors in a particular technology space, or to decide whether and how much to invest in pursuing particular innovations. Patent offices and economic monitoring organizations use patent landscapes to evaluate how a particular technology is affecting or might affect the economy, for example, how much economic investment is underway in a technology, how much economic value has been generated, or how many industries or companies are supported by a particular technology. Governments, in turn, can use that information to implement technology policies, for example, deciding whether to steer investment or tax incentives to companies working in particular areas (e.g., AI or green technologies). While the simplest form of patent landscaping merely identifies which patents are related to a particular area, other more sophisticated forms of patent landscaping can seek to identify how different subareas of a technology area are related, which companies or inventor groups are the most prolific, what regions are involved, or what specific types of innovations are the focus of current development."}, {"title": "2 Related Work", "content": "As will be reviewed in Section 2, at least two research systems have been developed to tackle automated patent landscaping [c.f. Abood and Feltenberger, 2018, Choi et al., 2019]. As is the case with much recent work in NLP, these are deep neural systems, and they presumably can be adapted to different technology areas given new labeled data. While effective in many ways, these prior systems have two shortcomings. First, due to challenges of assembling good labeled data, both of these approaches were trained only on \u201ceasy\u201d examples found via keyword search or patent family expansion; as a consequence, they do not work well on \"hard\" examples near the landscape boundary. Second, the models were trained on several thousand positive examples and tens of thousands of negative examples. While this generates good performance (at least on \u201ceasy\u201d examples), for many technology areas this amount of labeled data would in many cases be prohibitive, in that it would extremely expensive to obtain via manual labeling, and many technology areas (especially if quite specialized) might not even contain that many positive examples. Further, such a training setup also presents data balance issues.\nIn this work we attempt to address these two issues (data quality and amount of data needed). First, starting from the \"seed/anti-seed\" approach of Abood and Feltenberger which generates \"easy\u201d examples, we leverage active learning to collect a set of labeled \"hard\" examples (positive and negative) near an estimated landscape boundary. This allows us to build a much higher quality dataset for training and testing and evaluate prior architectures as to their performance on these more difficult cases. Second, we investigate the use of additional features in particular citation networks (which have not been previously used in conjunction with deep neural methods), and CPC code embeddings [as was done in Pujari et al., 2022]\u2014which significantly improve performance when using a small number of examples. We show an overall performance of 0.75 F\u2081 on as few as 24 examples, which is comparable to prior work trained on hundreds of times as much data.\nThe paper is organized as follows. We first discuss the prior work on automated patent landscaping (\u00a72). Next we present our data collection approach, reviewing our annotation tool and the dataset we generated using that tool (\u00a73). We then describe our architecture, which builds on a variety of prior work but combines them in new ways (\u00a74). We review our experiments and results next (\u00a75. Finally, we conclude with a brief discussion (\u00a76) and a list of our contributions (\u00a77)."}, {"title": "2.1 Patent Classification", "content": "As noted above, patent classification (as opposed to patent landscaping) is the process of assigning a patent classification code from an established code list (such as the CPC code hierarchy) to aid patent examination and search. An example of a section of the CPC code hierarchy is shown in Figure 1. Patent classification is related to, but must be kept distinct from, patent landscaping. Traditional approaches to patent classification involve converting text features of a patent-such as abstract, claim or title-into feature vectors, and then passing these features through a supervised classification model, such as a Support Vector Machine or a K-Nearest Neighbour model Yun and Geum [2020], Seneviratne et al. [2015]. Other work has used the bibliographic metadata as the primary feature for labelling patent with classification codes. For example, Li et al. [2007] used the CPC codes of the patents cited by a patent-both single hop (direct citation) and multi-hop (citation networks)\u2014to create features for classification in a featurized supervised machine learning model. More recent approaches use deep neural models for patent classification. Since patents contain"}, {"title": "2.2 Automated Patent Landscaping", "content": "There have been several lines of effort that directly addressed the problem of automated patent landscaping. Abood and Feltenberger [2018] presented a deep learning based method to select patents relevant to a particular topic from a broader set of candidate patents. Since there was no benchmark dataset available for training patent landscaping models, they developed a general method to build training data for any particular topic. Their method begins with taking a small number (1000) of \"seed\" patents curated by experts that are the positive examples of a technology. From these they created a \"Level 1\" (L1) set by finding patents that share CPC codes with or are cited by the seed patents. A \u201cLevel 2\" (L2) set is created by including patents that share a \u201cfamily\u201d relationship with those in L1 (an expression of patent priority or patent continuation). Any patent outside of L2 is considered a negative example (or an \u201canti-seed\u201d) for the topic. They then trained a deep neural network using seeds and anti-seeds as positive and negative training examples, and then applied the classifier to the patents in L2 to find patents that should be included in the landscape (which already includes the seeds). Their model architecture was a wide and deep Long-Short Term Memory [LSTM; Hochreiter and Schmidhuber, 1997], and they used Word2Vec Mikolov et al. [2013] embeddings of words in the abstract as the sequential input input to the LSTM network. The metadata of the patents-the citations and the CPC codes were represented by 1-hot encoding vectors. The metadata vectors and the output of the LSTM handling the abstract were concatenated and used as input to a multi-layer perceptron (MLP) network to perform a binary classification. They built landscaping models for four topics: browser, operating system, video codec and machine learning. They reported F\u2081 scores above 0.95 for each of these topics. Despite this impressive performance, the work has several shortcomings. First, the training data generated by the seed/anti-seed method does not provide positive or negative examples near the landscape boundary (i.e., they only include \u201ceasy\u201d examples). In our experiments we obtain patents near the landscape boundary via human labeling and show that Abood and Feltenberger [2018]'s performance is quite low on these examples. A smaller problem is that if a patent that does not share a CPC code, citation, or family link with one of the seed patents, it will never be included in the landscape (because the approach filters patents from the L1 and L2 sets), which is potentially problematic for broad landscapes covering many CPC codes, or landscapes that include many patents (where citation networks are unlikely to be exhaustive). Abood and Feltenberger's work was continued by Giczy et al. [2021], where they removed the CPC codes and added the claim texts as features. They did not measure the model performance; but rather they used it to create a landscape of AI patents.\nChoi et al. [2019] is the second line of effort, which also uses deep learning. They used word2vec embedding of the patent abstracts as inputs to a modified transformer architecture comprising both multi-head self-attention and scaled dot product attention. They also experimented with diffusion graph embedding techniques for representing IPC, CPC, and USPC classification codes. They used four topics analyzed in the Korea Intellectual Property Strategy Agency (KISTA) Patent trends reports KISTA [2023], which provided positive examples for training. They generated negative examples by repeating the keyword searches used by the human experts when generating the original landscapes, and using patents returned by that search but not included in the landscape. Because negative examples vastly outnumber positive examples, they performed under-sampling of negative examples using CPC codes of the target patents. They reported F\u2081 scores ranging from 0.62 to 0.89. They also evaluated Abood and Feltenberger's approach on their data, and in all cases showed better performance. They also compared with a baseline model based on PatentBERT which performed similarly to their model."}, {"title": "3 Data", "content": "As discussed above, the data collection method developed by Abood and Feltenberger, and used in follow-on papers by Giczy et al. and Antonin and Cyril, starts from a set of manually annotated positive examples (called \u201cseeds\u201d, of which they used around 1,000 seeds for each technology domain). They are able to rapidly generate a large number of negative examples (\u201canti-seeds\u201d) by sampling from patents which do not share a CPC code with a seed, do not have a patent family relationship with the seeds, and are not cited by the seeds or their family members. While this method is highly scalable, we had questions about whether these examples were actually modeling the landscape boundary well. It seemed plausible that, especially for a large technology domain, the seeds would not necessarily be near the boundary, and anti-seeds would almost certainly be far from it.\nTo investigate this hypothesis, we sought to collect manually annotated positive and negative examples that were near the landscape boundary, to see whether Abood and Feltenberger's technique maintained it's performance in this region. The idea was to start from seed/anti-seeds collected in the manner described by Abood and Feltenberger, but then use these to identify examples nearer to the landscape boundary and manually annotate them. We used active learning and an annotation tool of our own design to accomplish this, as described later in this section.\nTo begin, we chose the technology domain of Artificial Intelligence (AI) as the defining topic of the landscape, and worked exclusively on patents downloadable electronically from USPTO's PatentsView USPTO [2019] data repository. This set numbered more than 2 million patents as of 2021. We further obtained 2,020 seed positive examples of patents in the AI space from the USPTO itself, as reported by Giczy et al. [2021]. These examples were annotated by patent examiners with expertise in AI employed by the USPTO. The USPTO also had performed Abood and Feltenberger's L1/L2 expansion to generate a set of anti-seeds / negative examples, from which they randomly sampled 56,093 anti-seeds which they provided to us."}, {"title": "3.1 Annotation using Active Learning", "content": "Patentify, our annotation tool, has a front-end user interface for labeling patents (shown in Figure 2), and runs an active-learning-based Support Vector Machine [SVM; Boser et al., 1992] on the back-end to identify patents near the decision boundary for labeling. Active learning is a semi-automatic machine learning technique which interacts with the user in a feedback loop and results in higher quality and more informative labeled examples Settles [2012].\nThe SVM model uses as features the tf-idf counts of all the words (except stop words) in the title and abstract text of patents. We generated the initial state of model by providing it a balanced set of 938 training examples, equally balanced between four groups: randomly sampled seeds and anti-seeds from the USPTO-provided data, and a small set of positive and negative examples near the decision boundary manually annotated by two researchers in our lab. This model was then run over all 2 million remaining patents from the PatentsView data. We then used uncertainty sampling Lewis and Gale [1994] to prioritize the patents that the model is most uncertain about. These decision boundary example patents are then presented to the user for labelling.\nOur annotators were graduate and undergraduate students in our laboratory with at least 2 years of research experience in AI. Each student was asked to annotate approximately 100 patents as whether or not they involved AI. After every 10 labeled examples (positive or negative) collected, Patentify retrained the SVM model and reranked the most informative"}, {"title": "3.2 Dataset", "content": "We collected 1,149 annotated examples comprising 395 positive examples and 754 negative examples. We then separated our data into four categories: positive vs. negative examples crossed hard vs. easy examples, as shown in Table 1.\nThese data allowed us to construct several different types of training and testing datasets, as shown in Table 2. First, we constructed an All Data / Unbalanced set with 59,262 examples (2,415 positive, 56,847 negatives), comprising all the data. Second, we constructed a Full Balanced dataset comprising all of the hard positive examples, and an equal number of examples randomly sampled from the other categories, resulting in 1,580 total examples (790 positive, 790 negative). In some experiments below we generated multiple versions of the Full Balanced dataset to investigate statistical variation of the data. Third, we constructed a Full Holdout Test dataset from the examples not included in the Full Balanced set, comprising 57,682 examples (359 \u2018hard' negatives, 1,625 'easy' positives, and 55,698 'easy' negatives). Again, when we generated a different Full Balanced dataset, this necessarily generated a complementary Full Holdout Test dataset."}, {"title": "4 Patent Landscaping Methods", "content": "Our approach begins from the architecture described by Abood and Feltenberger [2018], and we show how we were able to achieve higher performance both using our higher quality training data and different embedding strategies. Patents are structured documents and contain several different types of information that can be used to inform landscaping, with the two major types of information being text and metadata. Text information comprises the actual language of the patent and includes fields such as the title, abstract, claims, and description (which can be further subdivided in some cases). Of the text information, the abstract provides a brief summary of the invention, the description provides expanded explanation and context for the invention, and the claims describe the exact legal scope of the invention. Metadata includes classification codes, citations, names of inventor, assignee and applicant, filing and publication dates, and so forth. The classification codes and citations provide the most information for the purposes of basic landscaping."}, {"title": "4.1 Our Neural Architecture", "content": "Abood and Feltenberger's model used the abstract, citations, and CPC codes. Each word in the abstract was encoded with word2vec and then fed into an LSTM followed by a dense MLP. Citations were encoded as a 1-hot vector then fed through two dense MLP layers. CPC codes were also encoded as a 1-hot vector and fed through two dense MLP layers. All three streams were fed into a single dense MLP followed by a dense binary classification layer. Giczy et al. [2021] refined Abood and Feltenberger [2018]'s implementation by removing the CPC codes and adding the claim text. Our model (Figure 3) builds on these two approaches. We have five input streams: three for text, one for citations, and one for CPC codes (they are numbered for ease of reference in the figure). As shown, text is encoded using embeddings (second layer of boxes from the bottom) and then fed into LSTM layers (third layer). Citation information and the average CPC code embedding vectors (second layer) are fed into single dense layers (third layer). All streams end with a single dense layer (fourth layer), which then combine in single dense layer (fifth layer) before passing into a binary classification layer (sixth layer).\nTextual Information (Streams 1\u20133) For text we experimented with using the abstract, claims, and description, encoded with word2Vec Mikolov et al. [2013] or BERT for Patents Rob Srebrovic [2020]. BERT for Patents was pre-trained on 100 million patents and patent-related documents, with an input width of 512 tokens. It also provides a patent-specific tokenizer. We extracted the contextualized token embeddings for abstracts and claims from the second to last encoder layer. While 512 tokens is too small for most claims sections, one can chunk the text into 512-token-sized pieces and pass them through the model one at a time to create an embedding vector longer than 512 that encodes an arbitrarily long text. When using the BERT for Patents on abstracts and claims sections, we experimented with using a single 512 token chunk (single pass) as well as using all of the text (multiple passes). We only used a single pass for descriptions because of their length. Note that Patents are public documents, part of the public record, and contain the names and addresses of inventors and assignees (which is personally identifiable information). We did not use this information in training the models.\nCitation Networks (Stream 4) As shown by Li et al. [2007], citation information can be used in several ways. First, one can examine the direct outward citations of patents or patent pre-grant publications (PGPubs); this is called the direct citation approach. Second, one can collect further citations by following citations of citations, up to a certain number of hops (citation network approach). We experimented with both (stream 4). Direct citations is a special case of the citation network approach, with hops is limited to 1. Abood and Feltenberger used the direct citation (1-hop) approach, but encoded citations in a 1-hot vector, which is problematic because of the sparseness. In our experiments with Abood and Feltenberger's architecture, we found that removing the 1-hot citation information actually improved performance. In our approach, we encoded 1-hop citations by representing each document as a vector of counts of CPC codes at the subclass level. Each element of the input vector to the first dense layer thus contains the number of cited documents belonging to a particular CPC subgroup code. We also experimented with using CPC codes gathered from citations two hops away from the target patent (namely, a 2-hop citation network). These were encoded as a vector of"}, {"title": "4.2 Models Tested", "content": "Figure 3 illustrates the design choices in our architecture. We experimented with different combinations, listed in Table 3. To compare with our models on the same data, we obtained or created implementations of Abood and Feltenberger's, Giczy et al.'s, and Choi et al.'s architectures. We refer to these models as A&F, A&F/USPTO, and Choi respectively. We also created five baseline SVM models (RBF kernel). The first baseline was a regular Bag-of-Words approach over abstracts and claims with the words represented as tf-idf vectors Zhang et al. [2011] (vocabulary size of 49,639 [abstracts] and 77,989 [claims]; cut-off value of 1.0). We call this baseline SVM/tfidf. The second baseline used word2vec vector embeddings of the abstracts and claims, on which we performed principal component analysis [Jolliffe, 1986, PCA], feeding the top 50 components as inputs to the SVM (SVM/w2v). The third baseline used FastText embeddings Bojanowski et al. [2017] in the same way as the word2vec embeddings (SVM/FT). The fourth baseline used the vector of counts of CPC subclass appearances in direct citations of a patent as the SVM features (SVM/1Hop). The fifth baseline used a combination of the tf-idf features from the first model and the citation features from the fourth model (SVM/tfidf-1Hop). We tested 8 variants of our architecture, listed at the bottom of Table 3. That table shows which embedding was used (w2v or B4P), and which streams were used with what parameters (all models used stream 1 and 2). For example, w2v-FullClaims uses word2vec embeddings throughout, with the full claims for stream 2. On the other hand, 1Hop used stream 2 with 512 tokens, and incorporated stream 4 with 1-hop citations, and B4P embeddings throughout."}, {"title": "5 Results", "content": "We trained our models for 5 epochs and in batches of 64 samples, with LSTM hidden unit size 64, dense layers having size 300, 64 and 1, both using 40% dropout. We used the default ADAM optimizer with a learning rate of 0.0001. We performed 5-fold cross validation in all experiments. Experiments using the balanced dataset are shown in Table 4. We also computed learning curves for all models using smaller balanced subsets of the balanced datasets, and the results for the best performing models are shown in Figure 4. Numbers of parameters for the models ranged from 637,753 (512ClaimsOnly) to 1,386,389 (B4P+A11), which does not include parameters of the pre-trained models. Training each model took approximately two hours on a dual CPU compute node (Xeon Gold 6258R, 2.7 GHz) with 1.5TB RAM and 8 Nvidia A100 40GB HBM2 PCIe 4.0 GPUs. Data processing took approximately 4 hours for each model."}, {"title": "6 Discussion", "content": "There are several key takeaways from the experimental results. First, the prior models A&F, A&F/USPTO, and Choi didn't perform nearly as well on balanced data that includes examples near the landscape boundary. In particular, they perform relatively poorly on Hard examples, showing that examples near the landscape boundary are critical. Second, the BERT for Patents (B4P) embeddings were effective in improving performance of architectures in which it was substituted for other embeddings. Third, for large amounts of data, most of the models, including the SVM baselines (but excluding A&F, performed quite well and were very close in performance. This suggests that, when a lot of data is available the extra complexity and computational load of the neural methods does not purchase much in the way of performance. Fourth, direct citations (1-hop citations) combined with CPC codes give good result on both Easy and Hard examples, but compared with the other features does not seem to add much. Interestingly, 2-hop citations have marginally poorer performance on the Hard examples. Finally, where the neural models really improve over the baselines is in the low-data regime. In the regime of 24 examples (evenly balanced between Hard/Easy and Positive/Negative), we see improvements of nearly 14 points of F\u2081 over baseline. Interestingly, the CPCAvg model, which uses only abstract text, 512 tokens of the claims, and CPC information, is the best overall model. Regardless, even this best model only achieves 0.75 F\u2081 at 24 examples, which has much room for improvement."}, {"title": "7 Contributions", "content": "In this paper, we have demonstrated the performance of various neural models on a patent dataset that we have built. Our work contributes to the research of automated patent landscaping models in four different aspects. First, we have shown that previously proposed \u201cseed/anti-seed\", while useful, importantly lacks examples near the decision boundary and using only seed/anti-seed data gives a misleading view of model performance. Second, we demonstrate an active learning augmentation to the seed/anti-seed approach which can quickly generate high-quality data near the decision boundary. Third, we conducted systematic experiments comparing the utility of different portions of the information in a patent, concluding that abstract, claims, and CPC codes (and note description or citations) provide the most power. Fourth, when a lot of data are available (1000s of examples) simple methods like SVM work just as well as the neural architectures with different features not making much difference. Fifth, we have experimented with using citation information (1-hop and 2-hop) and showed that this information does not add much performance beyond using text in the claims. Finally, we show that the neural methods significantly outperform the baselines in the small data regime (less than 100 total training examples)."}, {"title": "8 Limitations", "content": "There are several limitations of this work. First, we only examine a single patent landscaping domain, that of AI. AI is a fairly broad technology area and it is not clear that the results will generalize to more specific landscape topics. Second, there may of course be other neural architecture styles that work better; we didn't exhaustively explore these choices as we felt they were beyond scope of the paper as we envisioned it. Future work should explore the space of possible architecture designs. Third, citation network information did not improve the results as much as expected, which is counter-intuitive. We suspect that there are ways to use this information that will improve performance even with the other streams of information, but we were unable to identify them. Finally, the performance of the model in the small data regime could still be improved; it remains to be seen what the true lower limit of data is required to construct a good landscape for such a broad technology domain."}]}