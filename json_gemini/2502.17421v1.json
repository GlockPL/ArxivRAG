{"title": "LONGSPEC: Long-Context Speculative Decoding with Efficient Drafting and Verification", "authors": ["Penghui Yang", "Cunxiao Du", "Fengzhuo Zhang", "Haonan Wang", "Tianyu Pang", "Chao Du", "Bo An"], "abstract": "Speculative decoding has become a promising technique to mitigate the high inference latency of autoregressive decoding in Large Language Models (LLMs). Despite its promise, the effective application of speculative decoding in LLMs still confronts three key challenges: the increasing memory demands of the draft model, the distribution shift between the short-training corpora and long-context inference, and inefficiencies in attention implementation. In this work, we enhance the performance of speculative decoding in long-context settings by addressing these challenges. First, we propose a memory-efficient draft model with a constant-sized Key-Value (KV) cache. Second, we introduce novel position indices for short-training data, enabling seamless adaptation from short-context training to long-context inference. Finally, we present an innovative attention aggregation method that combines fast implementations for prefix computation with standard attention for tree mask handling, effectively resolving the latency and memory inefficiencies of tree decoding. Our approach achieves strong results on various long-context tasks, including repository-level code completion, long-context summarization, and o1-like long reasoning tasks, demonstrating significant improvements in latency reduction.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have achieved remarkable success across various natural language processing tasks (Achiam et al., 2023), but their autoregressive decoding mechanism often results in high latency. To address this limitation, speculative decoding (Leviathan et al., 2023) has emerged as a promising solution. By employing a lightweight draft model to generate multiple candidate tokens, the target model can verify these tokens in parallel, thereby accelerating the inference process without compromising output quality.\nDespite the great advancements in speculative decoding, existing research has primarily concentrated on short-context scenarios. However, as highlighted by Chen et al. (2025), the core potential of speculative decoding lies in long-context settings, where the maximum inference batch size is relatively low. The limited batch size restricts autoregressive decoding from fully utilizing the GPU computation resource, making speculative decoding an ideal approach to address this constraint. Yet, despite its advantages, the development of draft models specially designed for long-context scenarios remains largely unexplored.\nWhile the need for a long-context draft model is clear from both application demands and theoretical considerations, we find that existing methodologies developed for short-context scenarios are inadequate when applied to longer sequences. This inadequacy stems from three emergent challenges unique to long-context speculative decoding: 1) Architecture: the extra memory overhead of the draft model, 2) Training: the distribution shift of position indices, and 3) Inference: the inefficiencies in tree attention implementation.\nFirst, as decoding length increases, previous State-of-The-Art (SoTA) autoregressive draft models like EAGLE (Li et al., 2024) and GliDe (Du et al., 2024) require linearly increasing KV caches, resulting in substantial storage overhead. This issue becomes critical in long-context settings, where memory usage is of vital importance. Second, the training data of the draft model mainly consists of short context data, rendering it undertrained over the large position indices (An et al., 2025), while the inference is for long-context data. The discrepancy will cause the draft model unable to perform speculation when facing large position indices of long-context input. Moreover, SoTA approaches often rely on tree attention, which is incompatible with the current advanced attention kernels due to the tree mask. This incompatibility further constrains the usage of tree speculative decoding in long-context scenarios.\nTo address these challenges, we introduce LONGSPEC, a framework for long-context speculative decoding, featuring architectural innovation (Sec. 3.1), novel training methods (Sec. 3.2), and optimized inference implementation (Sec. 3.3). The three key innovations significantly enhance the efficiency and scalability of speculative decoding in long-context scenarios.\nFirst, to alleviate the memory overhead problem, we develop a draft model architecture that circumvents the linear expansion of KV caches which maintains a constant memory footprint as the context grows. Concretely, our approach employs a sliding window self-attention component to capture local dependencies, complemented by a cache-free cross-attention module for effectively modeling long-context representations. This approach effectively mitigates memory overhead which is particularly critical in long-context inference without compromising performance.\nNext, to handle the training discrepancy problem, we propose the Anchor-Offset Indices to reconcile the short-context training for long-context inference. To fully train all the indices, we randomly add an offset to position indices. This ensures that some larger indices are also sufficiently trained. However, since ROPE (Su et al., 2024) is based on relative positions, directly adding an offset to all indices does not have a direct effect. Inspired by Streaming LLM (Xiao et al., 2024), we set the first four attention sink tokens as the anchor indices and only add the random offset to the remaining token index as shown in Figure 2. This indexing strategy ensures that all the indices can be sufficiently trained for the draft model. In contrast, the vanilla indexing strategy repeatedly trains only the smaller indices and is unable to train those exceeding the training set length. Meanwhile, anchoring the sink tokens ensures that the target model can approximate the attention sink patterns found in long texts, even with short texts.\nFinally, to implement highly efficient tree attention, we propose a new computation method called Hybrid Tree Attention. Our insight comes from the discovery that the tree mask in tree-based speculative decoding can be decomposed into two parts, the previously cached part with a chain structure and the speculation part with a tree structure. Specifically, the tree mask is only required between the current queries and the speculation tokens (i.e., current input tokens) to ensure correctness. So we use Flash Decoding (Dao, 2024) to compute the first part efficiently and use a custom Triton kernel fused_mask_attn to compute the second part. We then combine these components using a log-sum-exp trick, enabling our approach to accelerate tree attention computations up to 4.1 \u00d7 compared to previous implementations in Medusa (Cai et al., 2024).\nExtensive experiments are conducted to evaluate the effectiveness of LONGSPEC. Experiments on five long-context understanding datasets using five LLMs as target models show that our LONGSPEC can effectively reduce the long-context inference latency, leading to a maximum speedup of 3.26\u00d7 compared with the strong baseline model with Flash_Decoding. Additional experiments on the long reasoning task AIME24 with the o1-like model QwQ (Qwen, 2024) further validate the effectiveness of LONGSPEC, achieving a 2.25\u00d7 speedup in wall-clock time."}, {"title": "2. Related Work", "content": "Speculative decoding offers a promising approach to accelerating LLMs while maintaining the quality of their outputs. Early efforts, such as Speculative Decoding (Xia et al., 2023), SpS (Leviathan et al., 2023), BiLD (Kim et al., 2024), and OSD (Liu et al., 2024c), rely on existing smaller LLMs to generate draft sequences. Some other methods aim to improve upon those early efforts (Sun et al., 2023; Miao et al., 2024; Chen et al., 2024). There are also some works using part of the target model as the draft model (Liu et al., 2024a; Zhang et al., 2024; Elhoushi et al., 2024). Retrieval-based speculative decoding methods offer an alternative by utilizing N-gram matching rather than relying on smaller models. Examples include Lookahead Decoding (Fu et al., 2024), REST (He et al., 2024), and Ouroboros (Zhao et al., 2024). These approaches bypass the need for additional model training, leveraging pre-existing data patterns to construct draft sequences efficiently.\nMore recent advancements, including Medusa (Cai et al., 2024), EAGLE (Li et al., 2024), and GliDe (Du et al., 2024), have expanded on these foundations by designing specialized draft models and introducing tree-based speculative techniques. These methods leverage customized draft models tailored for speculative decoding, achieving higher efficiency and performance. Additionally, the tree-based approaches employed in these methods allow for more adaptive and parallelizable decoding processes, paving the way for broader applications in real-world systems.\nAlthough speculative decoding has progressed significantly for conventional context lengths, only two existing papers focus on speculative decoding in long-context scenarios. TriForce (Sun et al., 2024) introduces a three-layer speculative decoding system that is scalable for long sequence generation. MagicDec (Chen et al., 2025) uses speculative decoding to improve both the throughput and latency of LLM inference. However, these methods mainly utilize the target model with the sparse KV cache as the draft model. The computation-intensive draft models restrict the practical usage of these methods when facing various batch sizes. In contrast, our work focuses on efficiently building a draft model with only one transformer block, achieving more effective performance across different scenarios."}, {"title": "3. Methodology", "content": "In this section, we present our framework LONGSPEC for Long-Context Speculative Decoding, which addresses three key challenges: (1) designing a lightweight draft model architecture with minimal additional memory overhead, (2) devising the training strategy with anchor-offset indices to handle long contexts effectively, and (3) implementing a fast tree attention mechanism that leverages tree-based speculation for practical usage. We detail each core component in the following subsections: Section 3.1 introduces our Memory-Efficient Architecture, Section 3.2 explains the Effective Training Regimes, and Section 3.3 describes the Fast Tree Attention implementation for inference."}, {"title": "3.1. Memory-Efficient Architecture", "content": "In previous work, the success of the SoTA model EAGLE depends on two factors: (1) the hidden states provided by the target model, and (2) an autoregressive structure. However, an autoregressive draft model inevitably needs to store its own KV cache, which introduces additional overhead in long-context inference requiring large GPU memory.\nTo avoid this extra memory overhead, we propose a draft model with constant memory usage regardless of the length of the context. As illustrated in Figure 1, our model comprises two components: the self-attention module and the following cross-attention module. The self-attention module focuses on modeling local context, while the cross-attention module captures long-context information. Because the self-attention module only processes local information, we adopt a sliding-window attention mechanism. Hence, during inference, the self-attention memory footprint does not exceed the window size, which we set to 512. We also provide the theoretical upper bound on the performance degradation caused by the slicing window in Section 3.4.\nFor the cross-attention component, inspired by GliDe (Du et al., 2024), we leverage the KV cache of the target model. This design not only enables better modeling of previous information but also completely removes additional storage overhead for long contexts, since the large model's KV cache must be stored regardless of whether or not speculative decoding is employed. Different from GliDe, we also share the weights of the Embedding Layer and LM Head between the target model and the draft model, which significantly reduces the memory usage for large-vocabulary LLMs such as LLaMA."}, {"title": "3.2. Effective Training Regimes", "content": "Anchor-Offset Indices. With vanilla position indices, which consist of successive integers starting from 0, those indices appearing earlier in sequences occur more frequently than larger position indices (An et al., 2025), as shown in the Figure 2 upper left part. Consequently, larger position indices receive insufficient training updates, which leads to a training inference discrepancy. A common method to solve this problem is RoPE-based extrapolation, which trains the model with a smaller ROPE base and extends the ROPE base with interpolation for longer contexts (Gao et al., 2024; Liu et al., 2024d; Peng et al., 2024). However, directly using these methods will cause an inconsistent problem in draft model training. To leverage the target model's KV cache, our draft model must keep the RoPE base the same as the target model. Based on our exploratory experiments, the inconsistent ROPE base between the target model and the draft model will cause a significant collapse of the cross-attention layer, which makes the draft model's long-text capability degrade. The consistency requirement of ROPE base in the draft model limits the usage of methods like RoPE-based extrapolation, which requires flexible adjustments to the ROPE base.\nInstead, we tackle this challenge by leveraging carefully designed indices. These indices must ensure that (1) the position indices in the draft model can be sufficiently trained using short-context data and (2) the indices would not cause the target model to exhibit out-of-distribution behavior because the target model shares the same indices as the draft model during training.\nTo satisfy these constraints, we propose the Anchor-Offset Indices strategy. Specifically, we reserve the first four positions [0,1,2,3] as attention sink tokens (Xiao et al., 2024), then assign all subsequent tokens to large consecutive indices starting at a random offset (e.g., [0, 1, 2, 3, 8192, 8193, 8194, . . . ]). The anchor indices and random offset ensure that every position index can be sufficiently trained, addressing the limitation of the vanilla one that repeatedly trains only smaller indices.\nAdditionally, according to Xiao et al. (2024), LLM exhibits an attention sink phenomenon when dealing with long texts, which means the attention weights primarily concentrate on the first four tokens and the recent tokens. Therefore, we believe that utilizing Anchor-Offset Indices can naturally lead the target model to exhibit in-distribution behavior. In our experiments, adopting these indices in the target model only increases the loss by approximately 0.001, indicating that the target model is indeed well-suited to such changes. We also provide the theoretical upper bound of the distribution shift error in Section 3.4.\nFlash Noisy Training. During training our draft model leverages the KV caches from a large model, while these KV caches are not always visible during inference. This is because the large model only updates its KV cache upon verification completion. Concretely, for the cross-attention query $Q_t$ in the draft model, we can only guarantee access to the corresponding key-value states $K_{<t'}, V_{<t'}$ satisfying $1 \\leq |t' - t| < \\gamma$, where $\\gamma$ is the number of speculative steps.\nTo ensure consistency between training and inference, a straightforward solution would be to add an attention mask (Du et al., 2024). However, this method is incompatible with Flash Attention (Dao et al., 2023), which would significantly degrade training speed and cause prohibitive memory overhead, particularly in long-context training scenarios. Therefore, we propose a technique called flash noisy training. During training, we randomly shift the indices of queries and key-value states with $1 < j < \\gamma$. Suppose the sequence length is $l$, then we compute\n$O_j = flash\\_attn(Q_{>j}, K_{<l-j}, V_{<l-j}).$\nIn this way, we effectively simulate the same visibility constraints as in the inference phase, i.e., $1 \\leq |t' - t| < \\gamma$, thereby aligning the behavior at training time with the inference behavior."}, {"title": "3.3. Fast Tree Attention", "content": "Tree Speculative Decoding (Miao et al., 2024) leverages prefix trees and the causal structure of LLMs so that a draft model can propose multiple candidate sequences, while the target model only needs to verify them once, without altering the final results. In this process, Tree Attention plays a key role in ensuring both correctness and efficiency. Early works (Cai et al., 2024; Li et al., 2024) apply attention masks derived from prefix trees to the QKT attention matrix, thus disabling wrong combinations between speculation tokens. However, these methods only run on PyTorch's eager execution mode, precluding more advanced attention kernels (e.g., Flash_Decoding). As a result, the inference speed decreases significantly when the sequence length increases.\nTo address these performance bottlenecks, we propose a Hybrid Tree Attention mechanism, as illustrated in Figure 2. Our method is based on two key insights: 1) When performing Tree Attention, the queries and the cached key-value pairs {Kcache, Vcache } do not require additional masks; 2) Only the queries and the key-value pairs {Kspecs, Vspecs} from the current speculative tokens need masking, and the number of such speculative tokens is typically no more than 128. Based on these observations, we adopt a divide and aggregate approach that splits the attention computation into two parts and merges them afterward.\nSplitting Key-Value Pairs. We partition all key-value pairs into two groups: {Kcache, Vcache}: the cached part of the main sequence, which requires no attention mask; and {Kspecs, Vspecs}: the speculative-stage part, which needs attention masks. For {Kcache, Vcache}, we invoke the efficient Flash_Decoding kernel. For {Kspecs, Vspecs}, we use our custom Triton kernel fused mask_attn, which applies blockwise loading and masking in the KV dimension, enabling fast computation of attention. This step yields two sets of attention outputs {Ocache, Ospecs} along with their corresponding denominators (i.e., log-sum-exp of all attention scores) {LSEcache, LSEspecs}.\nAggregation. We then combine these two parts into the final attention output Omerge via a log-sum-exp trick. First, we compute\n$LSE_{merge} = log(exp(LSE_{cache}) + exp(LSE_{specs})),$ \nand then apply a weighted summation to the two outputs:\n$O_{merge} = O_{cache} \\cdot exp (LSE_{cache} - LSE_{merge}) + O_{specs} \\cdot exp(LSE_{specs} \u2013 LSE_{merge}).$\nThe theoretical guarantee is provided in Appendix A. As outlined above, this hybrid approach employs the highly efficient Flash_Decoding kernel for most of the computations in long-sequence inference and only uses a custom masking attention fused mask_attn for the small number of speculative tokens. The kernel fused_mask_attn follows the design philosophy of Flash Attention 2 (Dao et al., 2023) by splitting Q, Kspecs, and Vspecs into small blocks. This strategy reduces global memory I/O and fully leverages GPU streaming multiprocessors. Furthermore, for each block in the computation of $QK^T_{vecs}$, the mask matrix is loaded and used to apply the masking operation. The Hybrid Tree Attention effectively balances the parallel verification of multiple branches with improved inference speed, all without compromising the correctness."}, {"title": "3.4. Theoretical Analysis", "content": "Here we would like to provide the theoretical analysis of our method. Before the statement of our results, we would like to define some quantities. First, for the memory-efficient architecture, we denote the sum of the attention score outside the window as $\\varepsilon$, which should be small due to the locality of the language. Second, for our index offset technique, we denote the distribution of the offset as $P_t$. In addition, we denote the true distribution of the index of the first token in the window as $P'_t$. Finally, we assume that the GliDe function is trained on $N$ i.i.d. samples, and the Frobenius norms of all the parameters are upper bounded by $B$.\nTheorem 3.1 (Informal). Under regularity assumptions, the inference error between the Maximum Likelihood Estimate (MLE) trained by our method and the optimal GliDe parameter that take all tokens as inputs is upper bounded by\n$AE + DE + GE$\nwith probability at least $1 \u2013 \\delta$. Here the approximation error is $AE = (1 + d_v \\exp(B))HB^4(1 + B^2_{\\mathcal X} B^2)\\varepsilon$, the distribution shift error is $DE = \\log(1 + d_v \\exp(B)) \\cdot TV(P_t, P'_t)$, and the generalization error is $GE = N^{-1/2}(d(d + d_v) \\log(1+ NHB^6) + \\log \\delta^{-1})$.\nThe formal statement and the proof are provided in Appendix D. The approximation error results from that we adopt a sliding window instead of using all tokens. This error is proportional to the attention score outside the window. The distribution shift error results from the positional embedding distributional shift. In our experiments, we set $P'_t$ as the uniform distribution, which will have a smaller distribution shift than not adopting this position offset technique. The generalization error results from that we use N samples to train the model."}, {"title": "4. Experiments", "content": "4.1. Settings\nTarget and draft models. We select four widely-used long-context LLMs, Vicuna (including 7B and 13B) (Chiang et al., 2023), LongChat (including 7B and 13B) (Li et al., 2023), LLaMA-3.1-8B-Instruct (Dubey et al., 2024), and QwQ-32B (Qwen, 2024), as target models. In order to make the draft model and target model more compatible, our draft model is consistent with the target model in various parameters such as the number of KV heads.\nTraining Process. We first train our draft model with Achor-Offest Indices on the SlimPajama-6B pretraining dataset (Soboleva et al., 2023). The random offset is set as a random integer from 0 to 15k for Vicuna models and LongChat-7B, and 0 to 30k for the other three models because they have longer maximum context length. Then we train our model on a small subset of the Prolong-64k long-context dataset (Gao et al., 2024) in order to gain the ability to handle long texts. Finally, we finetune our model on a self-built long-context supervised-finetuning (SFT) dataset to further improve the model performance. The position index of the last two stages is the vanilla indexing policy because the training data is sufficiently long. We apply flash noisy training during all three stages to mitigate the training and inference inconsistency, the extra overhead of flash noisy training is negligible. Standard cross-entropy is used to optimize the draft model while the parameters of the target model are kept frozen. To mitigate the VRAM peak caused by the computation of the logits, we use a fused-linear-and-cross-entropy loss implemented by the Liger Kernel (Hsu et al., 2024), which computes the LM head and the softmax function together and can greatly alleviate this problem.\nTest Benchmarks. We select tasks from the Long-Bench benchmark (Bai et al., 2024) that involve generating longer outputs, because tasks with shorter outputs, such as document-QA, make it challenging to measure the speedup ratio fairly with speculative decoding. Specifically, we focus on long-document summarization and code completion tasks and conduct tests on five datasets: GovReport (Huang et al., 2021), QMSum (Zhong et al., 2021), Multi-News (Fabbri et al., 2019), LCC (Guo et al., 2023), and RepoBench-P (Liu et al., 2024b). We test QwQ-32B on the famous reasoning dataset AIME24 (Numina, 2024).\nWe compare our method with the original target model and MagicDec, a simple prototype of TriForce. To highlight the significance of Flash Decoding in long-context scenarios, we also present the performance of the original target model using both eager attention implemented by Hugging-face and Flash_Decoding for comparison. To make a fair comparison, we also use Flash Decoding for baseline MagicDec. The most important metric for speculative decoding is the walltime speedup ratio, which is the actual test speedup ratio relative to vanilla autoregressive decoding. We also test the average acceptance length $\\tau$, i.e., the average number of tokens accepted per forward pass of the target LLM."}, {"title": "4.2. Main Results", "content": "Table 1 and Figure 3 show the decoding speeds and mean accept lengths across the five evaluated datasets at T = 0 and T = 1 respectively. Our proposed method significantly outperforms all other approaches on both summarization tasks and code completion tasks. When T = 0, on summarization tasks, our method can achieve a mean accepted length of around 3.5 and a speedup of up to 2.67\u00d7; and on code completion tasks, our method can achieve a mean accepted length of around 4 and a speedup of up to 3.26x. This highlights the robustness and generalizability of our speculative decoding approach, particularly in long-text generation tasks. At T = 1, our method's performance achieves around 2.5\u00d7 speedup, maintaining a substantial lead over MagicDec. This indicates that our approach is robust across different temperature settings, further validating its soundness and efficiency.\nWhile MagicDec demonstrates competitive acceptance rates with LongSpec, its speedup is noticeably lower in our experiments. This is because MagicDec is primarily designed for scenarios with large batch sizes and tensor parallelism. In low-batch-size settings, its draft model leverages all parameters of the target model with sparse KV Cache becomes excessively heavy. This design choice leads to inefficiencies, as the draft model's computational overhead outweighs its speculative benefits. Our results reveal that MagicDec only achieves acceleration ratios > 1 on partial datasets when using a guess length $\\gamma$ = 2 and consistently exhibits negative acceleration around 0.7\u00d7 when $\\gamma \\geq$ 3, further underscoring the limitations of this method in such configurations.\nLastly, we can find attention implementation plays a critical role in long-context speculative decoding performance. In our experiments, \u201cVanilla HF\u201d refers to HuggingFace's attention implementation, while \"Vanilla FA\" employs Flash_Decoding. The latter demonstrates nearly a 2\u00d7 speedup over the former, even as a standalone component, and our method can achieve up to 6\u00d7 speedup over HF Attention on code completion datasets. This result underscores the necessity for speculative decoding methods to be compatible with optimized attention mechanisms like Flash_Decoding, especially in long-text settings. Our hybrid tree attention approach achieves this compatibility, allowing us to fully leverage the advantages of Flash_Decoding and further speedup."}, {"title": "4.3. Ablation Studies", "content": "Anchor-Offset Indices. The experimental results demonstrate the significant benefits of incorporating the Anchor-Offset Indices. Figure 4 shows that pretrained with Anchor-Offset Indices achieve a lower initial loss and final loss compared to those trained without it when training over the real long-context dataset. Notably, the initalization with Anchor-Offset Indices reaches the same loss level 3.93\u00d7 faster than its counterpart. Table 2 further highlights the performance improvements across two datasets, a summary dataset Multi-News, and a code completion dataset RepoBench-P. Models with Anchor-Offset Indices exhibit faster output speed and larger average acceptance length $\\tau$. These results underscore the effectiveness of Anchor-Offset Indices in enhancing both training efficiency and model performance.\nHybrid Tree Attention. The results presented in Figure 5 highlight the effectiveness of the proposed Hybrid Tree Attention, which combines Flash_Decoding with the Triton kernel fused_mask_attn. While the time spent on the draft model forward pass and the target model FFN computations remain comparable across the two methods, the hybrid approach exhibits a significant reduction in latency for the target model's attention layer (the yellow part). Specifically, the attention computation latency decreases from 49.92 ms in the HF implementation to 12.54 ms in the hybrid approach, resulting in an approximately 75% improvement."}, {"title": "4.4. Long CoT Acceleration", "content": "Long Chain-of-Thought (LongCoT) tasks have gained significant attention recently due to their ability to enable models to perform complex reasoning and problem-solving over extended outputs (Qwen, 2024; OpenAI, 2024). In these tasks, while the prefix input is often relatively short, the generated output can be extremely long, posing unique challenges in terms of efficiency and token acceptance. Our method is particularly well-suited for addressing these challenges, effectively handling scenarios with long outputs. It is worth mentioning that MagicDec is not suitable for such long-output scenarios because the initial inference stage of the LongCoT task is not the same as the traditional long-context task. In LongCoT tasks, where the prefix is relatively short, the draft model in MagicDec will completely degrade into the target model, failing to achieve acceleration.\nWe evaluate our method on the QwQ-32B model using the widely-used benchmark AIME24 dataset, with a maximum output length set to 32k tokens. The results, illustrated in Figure 6, demonstrate a significant improvement in both generation speed and mean accepted tokens. Specifically, our method achieved a generation rate of 42.63 tokens/s, 2.25\u00d7 higher than the baseline's 18.92 tokens/s, and an average of 3.82 mean accepted tokens. Notably, QwQ-32B with LONGSPEC achieves even lower latency than the vanilla 7B model with Flash_Decoding, demonstrating that our method effectively accelerates the LongCoT model. These findings not only highlight the effectiveness of our method in the LongCoT task but also provide new insights into lossless inference acceleration for the ol-like model. We believe that speculative decoding will play a crucial role in accelerating this type of model in the future."}, {"title": "4.5. Throughput", "content": "The throughput results of Vicuna-7B on the RepoBench-P dataset show that LONGSPEC consistently outperforms both Vanilla and MagicDec across all batch sizes. At a batch size of 8, LONGSPEC achieves a throughput of 561.32 tokens/s, approximately 1.8\u00d7 higher than MagicDec (310.58 tokens/s) and nearly 2\u00d7 higher than Vanilla (286.96 tokens/s). MagicDec, designed with throughput optimization in mind, surpasses Vanilla as the batch size increases, reflecting its targeted improvements. However, LONGSPEC still sustains its advantage, maintaining superior throughput across all tested batch sizes."}, {"title": "5. Conclusion", "content": "In this paper, we propose LONGSPEC, a novel framework designed to enhance speculative decoding for long-context scenarios. Unlike previous speculative decoding methods that primarily focus on short-context settings, LONGSPEC directly addresses three key challenges: excessive memory overhead, inadequate training for large position indices, and inefficient tree attention computation. To mitigate memory constraints, we introduce an efficient draft model architecture that maintains a constant memory footprint by leveraging a combination of sliding window self-attention and cache-free cross-attention. To resolve the training limitations associated with short context data, we propose the Anchor-Offset Indices, ensuring that large positional indices are sufficiently trained even within short-sequence datasets. Finally, we introduce Hybrid Tree Attention, which efficiently integrates tree-based speculative decoding with Flash_Decoding. Extensive experiments demonstrate the effectiveness of LONGSPEC in long-context understanding tasks and real-world long reasoning tasks. Our findings highlight the importance of designing speculative decoding methods specifically tailored for long-context settings and pave the way for future research in efficient large-scale language model inference."}, {"title": "Impact Statements", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Correctness for Attention Aggregation", "content": "Because the query matrix Q can be decomposed into several rows, each representing a separate query q, we can only consider the output of each row's q after calculating attention with KV. In this way, we can assume that the KV involved in the calculation has undergone the tree mask, which can simplify our proof. We only need to prove that the output o obtained from each individual q meets the requirements, which can indicate that the overall output O of the entire matrix Q also meets the requirements.\nProposition A.1. Denote the log-sum-exp of the merged attention as follows:\n$LSE_{merge} = log(exp(LSE_{cache}) + exp(LSE_{specs})),$\nThen we can write the merged attention output in the following way:\n$O_{merge} = O_{cache} \\cdot exp (LSE_{cache} - LSE_{merge}) + O_{specs} \\cdot exp (LSE_{specs} - LSE_{merge}).$\nProof. A standard scaled dot-product attention for q (of size dqk) attending to Kmerge and Vmerge (together of size (M + N) \u00d7 dqk and (M + N) \u00d7 dv respectively) can be written as:\n$O_{merge} = mha (q, K_{merge}, V_{merge}) = softmax (qK_{merge}^T/\\sqrt{d_{qk}}) V_{merge}.$\nBecause K and V are formed by stacking (Kspecs, Kcache) and (Vspecs, Vcache), we split the logit matrix accordingly:\n$\\frac{qK_{merge}^T}{\\sqrt{d_{qk}}} = concat \\begin{pmatrix} \\frac{qK_{cache}^T}{\\sqrt{d_{qk}}} \\\\ \\frac{qK_{specs}^T}{\\sqrt{d_{qk}}} \\end{pmatrix}$\nDenote these sub-logit matrices as:\n$Z_{cache} = \\frac{qK_{cache}^T}{\\sqrt{d_{qk}}}, Z_{specs} = \\frac{qK_{specs}^T}{\\sqrt{d_{qk}}}.$\nEach row i of Zspecs corresponds to the dot products between the i-th query in q and all rows in Kspecs, while rows of Zcache correspond to the same query but with Kcache.\nIn order to combine partial attentions, we keep track of the log of the sum of exponentials of each sub-logit set. Concretely, define:\n$LSE_{cache} = \\log (\\sum_{j=1}^{M} exp (Z_{cache}^{(j)})), LSE_{specs} = \\log (\\sum_{j=1}^{N} exp (Z_{specs}^{(j)})),$\nwhere $Z_{specs}^{(j)}$ denotes the logit for the j-th element, and similarly for $Z_{cache}^{(j)}$.\nThen Ocache and Ospecs can be written as:\n$\\begin{aligned} O_{cache} &= \\frac{\\sum_{j=1}^{M} exp (Z_{cache}^{(j)}) \\cdot V_{cache}^{(j)}}{exp (LSE_{cache})}, \\\\ O_{specs} &= \\frac{\\sum_{j=1}^{N} exp (Z_{specs}^{(j)}) \\cdot V_{specs}^{(j)}}{exp (LSE_{specs})} \\end{aligned}$\nAnd the whole attention score can be written as:\n$\\begin{aligned} O_{merge} &= \\frac{\\sum_{j=1}^{M} exp (Z_{cache}^{(j)}) \\cdot V_{cache}^{(j)} + \\sum_{j=1}^{N} exp (Z_{specs}^{(j)}) \\cdot V_{specs}^{(j)}}{exp (LSE_{cache}) + exp (LSE_{specs})} \\end{aligned}$\nBy aggregating Equation 2 into Equation 3, we can get the following equation:\n$O_{merge} = O_{cache} \\cdot exp(LSE_{cache} - LSE_{merge}) + O_{specs} \\cdot exp (LSE_{specs} - LSE_{merge}).$"}, {"title": "B. Experiments Details", "content": "All models are trained using eight A100 80GB GPUs. For the 7B, 8B, and 13B target models trained on short-context data, we employ LONGSPEC with ZeRO-1 (Rasley et al., 2020). For the 7B, 8B, and 13B models trained on long-context data, as well as for all settings of"}]}