{"title": "Effects of AI Feedback on Learning, the Skill Gap, and Intellectual Diversity", "authors": ["Christoph Riedl", "Eric Bogert"], "abstract": "Can human decision-makers learn from AI feedback? Using data on 52,000 decision-makers from a large online chess platform, we investigate how their AI use affects three interrelated long-term outcomes: Learning, skill gap, and diversity of decision strategies. First, we show that individuals are far more likely to seek AI feedback in situations in which they experienced success rather than failure. This AI feedback seeking strategy turns out to be detrimental to learning: Feedback on successes decreases future performance, while feedback on failures increases it. Second, higher-skilled decision-makers seek AI feedback more often and are far more likely to seek AI feedback after a failure, and benefit more from Al feedback than lower- skilled individuals. As a result, access to AI feedback increases, rather than decreases, the skill gap between high- and low-skilled individuals. Finally, we leverage 42 major platform updates as natural experiments to show that access to AI feedback causes a decrease in intellectual diversity of the population as individuals tend to specialize in the same areas. Together, those results indicate that learning from AI feedback is not automatic and using AI correctly seems to be a skill itself. Furthermore, despite its individual-level benefits, access to AI feedback can have significant population-level downsides including loss of intellectual diversity and an increasing skill gap.", "sections": [{"title": "1. Introduction", "content": "AI is changing how we work. Of particular interest to strategy scholars is the possibility that humans decision-makers can learn from AI (Helfat et al., 2023; Joseph and Sengul, 2024; Puranam, 2021; Tong et al., 2021). That is, not only may AI support strategic decision-making directly through its use during the decision-making process but AI may also provide valuable feedback to decision-makers that improves the quality of their decisions, even when AI is not available. Al may thus become a pathway to increase organizational learning, which is a key contributor to strategy (Teece, Pisano, and Shuen, 1997). This form of organizational learning could lead to important compound effects and is critical to long-term productivity gains.\nHowever, when left to their own devices, decision-makers may use AI in ways that reinforce, rather than alleviate, human decision-making biases (Choudhury, Starr, and Agarwal, 2020; Morewedge et al., 2023) and reliance on AI may have deskilling effects (Hendrycks, Mazeika, and Woodside, 2023; Russell, 2022) thus undermining potential learning effects. This has led to various speculations as to whether AI will mostly benefit those who are high-skilled or low- skilled (Acemoglu and Autor, 2011; Autor, Levy, and Murnane, 2003; Choudhury et al., 2020).\nWe adopt an abductive, question-driven approach (Graebner et al., 2023; S\u00e6tre and Van de Ven, 2021) to answer a set of interrelated, multi-level research questions around a central phenomenon: What happens when decision-makers have access to AI feedback? We investigate who seeks such AI feedback, whether decision-makers can learn from AI feedback, whether higher-skilled or lower-skilled decision-makers benefit more, and what the broader societal impact of widespread AI-adoption may be. This question-driven research approach is well suited to address multi-level questions in a novel and causally complex area (Graebner et al., 2023).\nWe use data from an online chess platform (lichess.org) spanning over five years and 52,000 individuals. The platform offers a feature where decision-makers can use AI to retrospectively analyze completed games. During this analysis, decision-makers receive feedback from a highly accurate Al about the quality of each move they and their opponent made. The AI feedback quantifies how much strategic advantage they gained (or lost) with each move and what the best move in each situation would have been. While this AI feature is available to everyone, it is up to each human decision-maker to decide whether, and for which game, to seek such AI feedback. Our setting focusing on the endogenous choice to seek AI feedback complements other studies that have relied on randomized experiments which exogenously vary access to AI (Chen and Chan, 2024; Dell'Acqua et al., 2023). We first explore who (high- vs. low-skilled) seeks such AI feedback and in which situations (success vs. failure). Next, we explore whether human decision- makers learn from this AI feedback and whether this can help close the existing skill gap. We then explore specialization as a plausible mechanism behind learning. Finally, we leverage 42"}, {"title": "2. Theoretical Background", "content": "AI may not only provide direct input into decision making processes. Interacting with AI may also offer decision-makers important feedback on their own judgements. Research on feedback seeking has extensively studied who seeks feedback, in which situations, and from who (see Ashford, Blatt, and VandeWalle, 2003 for a review). Feedback provides a crucial information resource for managers and other knowledge workers to achieve goals and achieve mastery; that is, in order to adapt and learn (Ashford, 1986). Feedback seeking is the conscious devotion of effort to determine the correctness and adequacy of behavior for attaining valued goals (Ashford and Cummings, 1983). Research suggests that individuals carefully evaluate the costs and benefits of asking for feedback considering aspects such as the availability of feedback, the cost of feedback, and also emotional responses of what to do with the feedback once it is received (Ashford et al., 2003).\nSeveral differences stand out when considering seeking feedback from AI compared to from other humans such as friends, peers, or bosses. First, once deployed, AI systems can easily scale (Bengio et al., 2024). While a human expert may be able to provide detailed feedback on only a few decision situations for a few decision-makers, AI systems have the capacity to provide detailed feedback to many more decision-makers in more situations. AI feedback may become instantly available. Second, as Al systems approach expert-level decision quality, they can provide feedback to even the highest-skilled decision-makers who may otherwise face difficulty in getting access to elite mentors and coaches. Third, compared to asking humans for feedback, AI feedback comes without social cost such as concerns to appear confident and self-assured (Ashford, 1986). Individuals may feel higher psychological safety asking for corrective feedback from an AI due to the absence of negative social consequences (Edmondson, 1999). These differences suggest potential benefits from AI due to its scale and access, but could also imply"}, {"title": "2.1 Seeking Feedback from AI", "content": "different patterns in how humans seek feedback from AI compared to from other humans. When high-quality feedback is readily available without social cost, who will be more likely to seek AI feedback (higher vs. lower skilled), and in which situations (successes vs. failures)?\nBased on the feedback seeking (from humans) literature, it is not obvious who will be more likely to seek feedback from AI: Those with higher or those with lower skill. On the one hand, the higher the skill level, the more frequent feedback seeking is (Wanberg and Kammeyer- Mueller, 2000). One reason for this is goal importance. The higher the importance of the goal to the performer, the more frequently the performer seeks feedback (Ashford, 1986). Since higher- skilled decision-makers tend to identify more strongly with the task (and hence value it more highly) they may be more likely to seek AI feedback than lower-skilled individuals (Deci and Ryan, 2000). On the other hand, feedback can hurt the performers' pride, ego, and vanity which may be higher developed in those of higher skill, thus increasing the need to protect the ego and seek less feedback. Furthermore, when feedback is sought from humans it comes with social costs such as concerns to appear confident and self-assured (Ashford, 1986) which may be a particular concern for higher-skilled individuals. While AI feedback may come without such social cost, the need to protect the ego from the potentially harsh and objective AI feedback may be particularly high.\nIn which situations may people seek feedback from AI: successes or failures? Individuals are motivated to defend and protect their egos (Baumeister, 2010). This generates a motive to avoid negative feedback (Ashford and Cummings, 1983; Wood, 1989). The tendency to avoid negative feedback may be particularly strong when the feedback is actively solicited (rather than provided without asking) as it may be more difficult to disregard (Ashford et al., 2003). Even in situations in which honest feedback about oneself is crucial to achieve one's goals, it seems that people strongly favor receiving positive feedback that reinforces their positive self-perception, rather than negative feedback that challenges it. Consequently, individuals carefully choose situations that expose the self to positive feedback and use a variety of mental strategies to avoid feedback that threatens their positive self-image (Ashford and Cummings, 1983; Baumeister, 2010).\nPatterns of feedback seeking across successes and failures may also interact with patterns across skill levels. Higher skilled individuals who identify more with the task are also likely to have higher task-related self-esteem (Stets and Burke, 2000) and may thus be more likely to seek feedback in challenging situations. High self-esteem performers are resilient, have great confidence reserves, are higher promotion focused (Lanaj et al., 2012), and therefore, can \u201ctake\u201d negative feedback and will be more likely to seek feedback, even after experiencing failures. On the other hand, since negative feedback can hurt the performers' ego the potential for injury from negative feedback may be higher for higher skilled who identify more with the task. As a result, higher skilled may be less likely to seek feedback in challenging situations to protect their egos and avoid social costs associated with negative feedback. Seeking feedback from AI could affect"}, {"title": "2.2 Learning from AI Feedback Across Skill Levels", "content": "Long before the current interest in AI, research has investigated the returns to technological advances more generally (Acemoglu and Autor, 2011; Autor et al., 2003). With the recent rise in capability of AI, this question has received new urgency and interest (Autor, 2024). Several recent studies have explored the impact of access to AI on direct outcomes such as productivity (Brynjolfsson, Li, and Raymond, 2023), task performance (Dell'Acqua et al., 2023), decision accuracy (Boyac\u0131, Canyakmaz, and de V\u00e9ricourt, 2024; Kawaguchi, 2021), and quantity/quality in the context of brainstorming (Chen and Chan, 2024). In practice, gains from access to AI can be lower than expected because human decision-makers often do not follow algorithmic recommendations and instead rely on their own judgment (Kawaguchi, 2021; Kim et al., 2024) or use AI in the wrong way (Alderucci et al., 2024; Chen and Chan, 2024). However, those are short-term effects of AI use \u201cin-the-moment\u201d of task performance, not long-term learning effects (i.e., skill acquisition that improves performance in situations in which AI may not be available) and not from naturalistic settings in which the endogenous decision of when and how to use AI may play a larger role than in more controlled experimental settings. Whether and how AI affects learning and skill development itself is unclear.\u00b9 In the context of decision-making, learning implies that at two different points in time given the same input, a decision-maker may make different (better) decisions (Puranam, 2021). Brynjolfsson et al. (2023) document performance improvements over pre-AI baseline even when AI recommendations are unavailable providing one indication of learning from AI. Another notable exception is the work by Gaessler and Piezunka (2023) who investigate whether AI can help decision-makers learn by serving as an artificial training partner. Their study exploits exogenous variation in access to AI training partners (chess computers) by contrasting players in Western Europe (where chess computers became available in 1977) from those in the Soviet Union (where they did not) in a difference-in- difference model. They find positive learning from AI. However, their study analyzes player- year level data, without analysis of micro-level data of specific situations in which decision- makers used AI or how those specific uses affect their learning."}, {"title": "2.3 Specialization and Strategy Diversity", "content": "If humans can successfully learn from AI, what might be a plausible mechanism behind such learning? We theorize that specialization\u2014the process by which individuals focus on a narrow area of expertise\u2014could be a key driver. Specialization is a well recognized mechanism behind learning (Cohen, Levinthal, and others, 1990; Levinthal and March, 1993). While specialization may be beneficial individually, it may lead to unintended downsides on the group (or organizational) level. If the individuals of a group specialize in different areas, group-level intellectual diversity would increase, improving the capacity of the group to solve complex problems (Fazelpour and De-Arteaga, 2022). However, if individuals in a group specialize in the same areas, intellectual diversity on the group level would decrease. The resulting homogeneity in mental patterns and loss of strategic diversity among human decision-makers would have negative effects on long-term problem solving ability of firms (Levinthal and March, 1993; Page, 2019) and may undermine firms' competitive advantage (Felin and Holweg, 2024). Such a homogenization risk seems especially probable in the context of generic and centralized Al systems where many individuals (both within and across firms) are exposed to generic and non- specific AI output. Access to the same feedback from a single centralized AI system (such as AI foundation models) could lead to such homogenization. Here, it seems especially likely that learning from Al feedback could result in homogeneous (rather than diverse) group-level specialization. The potential risks of decreased population-level intellectual diversity as an unintended consequence of learning from AI feedback poses a crucial alignment problem with important risks on the larger societal level (Bengio et al., 2024). This risk has been termed \u201coutcome homogenization\u201d and is a recurring theme in work discussing risks of algorithmic monoculture (Bommasani et al., 2022). However, those discussions have been mostly speculative as high-quality empirical evidence from real-world settings is lacking. This implies that it is important to also study the impact of AI on the broader ecosystem level like the effect on an entire population of users of AI systems."}, {"title": "2.4 Research Questions", "content": "The arguments above raise several interrelated research questions, which we examine empirically. We aim to investigate the issue of AI feedback on decision-making holistically spanning the entire decision-making cycle.\nFirst: When do human decision makers seek AI feedback and do they use AI in the \u201cright\u201d way? That is, in which situations (successes vs. failures) are decision-makers most likely to seek AI feedback and does the propensity to seek AI feedback differ across skill levels? We specifically focus on a setting in which decision-makers are free to choose in which situations they seek AI feedback or forgo such feedback. Following Gaessler and Piezunka (2023), we explore this question \u201cin the field\u201d in which other modes of learning are also available to decision-makers.\nSecond: Which forms of AI feedback are most useful for learning and do higher- and lower- skilled decision-makers learn at the same rate? We explore if AI feedback on successes is equally beneficial as AI feedback on failures. Focusing on higher- and lower-skilled decision- makers allows us to investigate the important question of skill-based inequality: Does access to AI feedback increase or decrease the skill gap over time?\nThird: Is specialization a plausible mechanism behind learning? And if so, what effect does Al induced specialization have on long-term sustainability of human cognitive diversity? In other words, we explore whether AI-induced specialization on the individual level has negative consequences on intellectual diversity on the group level."}, {"title": "3. Setting and Data", "content": "We use chess as a sample research domain of strategic decision-making in which AI is already widely adopted and the superhuman skill of AI is well known. Chess has been a focal point of AI research for decades (Shannon, 1950; Turing, 1953). Chess has been used to study cognitive performance over human life spans (Strittmatter, Sunde, and Zegners, 2020) the effect of masks on performance (Smerdon, 2022), gender differences in competition (De Sousa and Hollard, 2022), personal bests as reference points (Anderson and Green, 2018), the joint effect of intelligence and practice on skill development (Vaci et al., 2019), and in hundreds of other studies on cognition, strategy, and artificial intelligence.\nOur data come from lichess.org, a popular and free online chess platform. On average, more than 90 million games are played each month on the platform. It is distinct among the major chess platforms because it is a nonprofit, and makes every platform feature available for free. Our study centers around a specific platform feature: The platform allows players to seek AI feedback on games they played on the platform. This AI feedback is powered by a chess AI"}, {"title": "4. Analysis", "content": null}, {"title": "4.1 Choice to Seek AI Feedback", "content": null}, {"title": "4.1.1 Empirical Model", "content": "We investigated who sought AI feedback, and in which situation by analyzing whether a game was analyzed as our dependent variable (1 = analyzed, 0 = not analyzed). We use logistic panel regression with fixed effects for the human player, the bot opponent, the year, the month, the time control, and a dummy variable indicating whether a bot was among the top 100 most popular bots, with clustered standard errors on the human player level. We estimate\nPr(Al Feedbackit = 1) = logit-1 (B\u2081Lossit + B2Skillit + B3Lossit \u00d7 Skillit +\ncontrols\nAi + abot + ayear + Amonth + XGameType + Eit)\nwhere Pr(Al Feedbackit = 1) is the probability that player i seeks AI feedback for the game played at time t. Lossit is an indicator whether the game to be analyzed was a win or a loss, Skillit"}, {"title": "4.1.1 Results", "content": "Who seeks AI feedback and in what situations? Higher-skilled individuals are significantly more likely to seek AI feedback than lower-skilled individuals (Table 2, Model 1, \u03b2 = 0.06; p = 0.031). Furthermore, individuals have a strong aversion to seeking AI feedback in situations of failure (i.e., for games they have lost; \u03b2 = \u22121.61; p < 0.001). There is only a 11% chance that an individual will seek AI feedback on a failure while there is a 21% likelihood to to seek AI feedback in case of success (average marginal predictions). None of the control variables have a significant effect on propensity to seek feedback with the exception of moving first (playing white). Looking at heterogeneity across individuals, we find a large significant coefficient of the interaction term (\u03b2 = 0.24; p < 0.001) indicating that higher-skilled individuals are much more likely to seek AI feedback in situations of failure.\nOur variables of interest, seeking AI feedback on games that were lost, gives rise to potential endogeneity concerns due to omitted variables as losing games itself is endogenous (i.e., individuals who experience more (less) failure, have more (less) opportunities seek AI feedback on those failures). The panel data structure and fixed effects specification underlying our analysis already accounts for measured and unmeasured individual-specific characteristics that are fixed over time. This includes important individual-level traits such as latent levels of ambition and learning goal orientation. Despite this approach, experiencing failure may potentially be endogenous due to time-variant omitted variables. We conduct a robustness test using a control function approach (Wooldridge, 2010) in the Appendix Table A2. We indeed find signs of endogeneity suggesting that the strong negative relationship between seeking AI in situations of failure is driven in large part by endogenous time-varying shocks (e.g., lower motivation). We find signs of a negative bias, where time-variant shocks that promote losses (e.g., lower motivation) are correlated with lower likelihood to seek AI feedback. The results imply that using AI feedback correctly follows endogenous processes where seeking AI feedback in challenging situations may depend on time-varying levels of motivation. As a result, when left to their own devices, decision-makers often pass up seeking AI feedback even though it is readily available to them which may curtail their ability to learn as we will explore in the next section."}, {"title": "4.2 Learning from AI Feedback", "content": null}, {"title": "4.2.1 Empirical Model", "content": "In order to determine whether people improve as a result of using AI feedback, we use panel regression with user and time fixed effects. Like our prior model, this allows us to control for unchanging but unobserved user qualities, such as intelligence, sex, or number of games chess games played prior to arrival on Lichess.org. Our time fixed effects allow us to control for changes across the platform that affect all users. We estimate the equation\nPerformanceit = \u03b2\u2081AI Feedbackit-1 + \u03b22log(CumulativeGamesit-1) +\n20\nB3Tenureit + Tenureit + \u03b25Opponent Skillit + controls +\n\u03a0\nAi + abot + Xyear + month + GameType + Eit"}, {"title": "4.2.1 Results", "content": "Do individuals learn from AI feedback? We find no effect of overall AI feedback (Table 3; Model 1; \u03b2 = 0.00; p = 0.688). However, this overall null-effect hides two opposing heterogeneous effects. Whereas feedback on losses leads to performance improvements (Model 2;\u03b2 = 0.04; p < 0.001), feedback on wins\u2014seeking a pat on the back in situations in which individuals already succeeded actually deteriorates performance (Model 3; \u03b2 = \u22120.02; p = 0.002). When entering wins and losses simultaneously, respective effects are even stronger (Model 4). This indicates that AI feedback is effective to learn, but only when feedback is sought in situations of failures and not in success situations. In the Appendix Table A3 we show robustness tests indicating that these results also hold when considering performance against human players. That is, AI feedback is useful for broad-based learning that improves performance both against AI and human opponents.\nConsidering these results on learning from AI feedback in the context of the previous set of analyses when decision-makers seek AI feedback\u2014a stark conclusion stands out: Left to their own devices, individuals seek AI feedback in the wrong situations (wins) which hampers their learning and curtails their future performance: Decision-makers should seek AI feedback after experiencing failures not successes. Higher-skilled individuals appear to be in a much better position to benefit from AI feedback: They seek more feedback on failures, thus increasing their ability to learn from AI and improve their skill even more."}, {"title": "4.3 Heterogeneous Treatment by Skill", "content": "To further investigate whether higher- or lower-skilled decision-makers benefit from AI feedback more than high skill players, we use a Generalized Random Forest (GRF), a non- parametric method that estimates heterogeneous treatment effects (Athey, Tibshirani, and Wager, 2019). Because GRFs are non-parametric, a GRF ensures that choices around which parameters to include, such as interaction terms, are less likely to drive any effects that we observe. We estimate the Conditional Average Treatment Effect of cumulative numbers of AI feedback across individuals with different skill levels (we also include the same control variables as in our other analyses such as opponent skill, tenure, opponent skill etc.). Variable selection occurs automatically with GRFs. As before, we use performance (human accuracy in a chess game) as the dependent variable.\nWe find statistically significant and practically significant heterogeneity in the effect of AI analysis on performance (i.e., the conditional average treatment effect). We compare the average treatment effects of AI feedback for high vs. low skilled decision-makers using a median split"}, {"title": "4.4 AI Feedback Leads to Specialization", "content": "We propose that specialization is one contributing mechanism behind how decision-makers learn from Al feedback. To explore specialization as a potential mechanism, we focus on the opening moves of a chess game, known as \u201cchess openings.\" Openings are important in chess because they set the stage for the middle game, thus narrowing the space of strategies that need to be considered in later phases. They are widely studied and cataloged. Many opening sequences have well-known names such as the \u201cQueen's Gambit\u201d and the \u201cSicilian Defense\u201d (Hooper and"}, {"title": "4.5 Intellectual Diversity on the Platform Level", "content": "As decision-makers learn to specialize based on receiving AI feedback, an important question emerges: Across a larger population, do decision-makers specialize in the same or different ways? If many individuals in a group exposed to AI feedback specialize in the same way, group- level strategy diversity would decrease; whereas if they specialize in different ways, it would increase. We find that many individuals appear to specialize along the same dimensions, thus reducing group-level strategy diversity. We begin with a simple descriptive analysis (Figure 2). We group individuals by their level of past AI feedback. As individuals accumulate more exposure to Al feedback from analyzing more games, the diversity of opening move strategies they employ decreases (Pearson p = \u22120.59; p = 4.9 \u00d7 10\u00af\u00ba). Next, we provide causal evidence for lower group-level intellectual diversity by analyzing a set of natural experiments.\nAnalysis Approach. To establish that this population-level decrease in intellectual diversity (strategy use) is causally driven by AI feedback we combine Regression Discontinuity in Time (RDiT; Hausman and Rapson, 2018) and natural experiments. First, we aggregate all chess openings played on the entire platform on a given day into a diversity metric (1 \u2013 Gini). This metric captures how likely it is that two games played on the same day and drawn at random use"}, {"title": "Discussion", "content": "Our analysis shows that decision-makers can learn from AI feedback. However, it also shows that when left to their own devices, decision-makers often use AI in the wrong way: They prefer to seek feedback after experiencing success rather than losses, which would provide more opportunities for learning. Despite positive individual-level learning effects, our analysis also shows compelling evidence of unintended consequences on the population-level. Higher-skilled individuals learn at a faster rate than lower-skilled individuals, due to their tendency to seek more AI feedback and in more challenging situations (after a failure). This implies that in our setting, access to AI feedback increases, rather than decreases, the skill gap between high and low-skilled individuals. Finally, access to AI feedback causes a decrease in intellectual diversity in the population as individuals tend to specialize in the same ways. We make several contributions to theory at the intersection of learning, AI, and strategy research.\nWe contribute to research on learning (Krakowski et al., 2023; Maula et al., 2023; Riedl and Seidel, 2018), which can be an an important outcome of AI use besides direct performance benefits (Chen and Chan, 2024; Dell'Acqua et al., 2023; Kawaguchi, 2021). Whereas past research has started to shed some light on how AI use affects performance and productivity, effects of AI use on longer-term and population-level outcomes like learning have been mostly neglected (Mollick and Mollick, 2022). Here, we show that AI feedback can drive learning and thus affect competitive capabilities. Insights from our work complement two closely related"}]}