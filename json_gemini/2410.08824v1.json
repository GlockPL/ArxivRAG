{"title": "One-shot Generative Domain Adaptation in 3D GANs", "authors": ["Ziqiang Li", "Yi Wu", "Chaoyue Wang", "Xue Rui", "Bin Li"], "abstract": "3D-aware image generation necessitates extensive training data to ensure stable training and mitigate the risk of overfitting. This paper first considers a novel task known as One-shot 3D Generative Domain Adaptation (GDA), aimed at transferring a pre-trained 3D generator from one domain to a new one, relying solely on a single reference image. One-shot 3D GDA is characterized by the pursuit of specific attributes, namely, high fidelity, large diversity, cross-domain consistency, and multi-view consistency. Within this paper, we introduce 3D-Adapter, the first one-shot 3D GDA method, for diverse and faithful generation. Our approach begins by judiciously selecting a restricted weight set for fine-tuning, and subsequently leverages four advanced loss functions to facilitate adaptation. An efficient progressive fine-tuning strategy is also implemented to enhance the adaptation process. The synergy of these three technological components empowers 3D-Adapter to achieve remarkable performance, substantiated both quantitatively and qualitatively, across all desired properties of 3D GDA. Furthermore, 3D-Adapter seamlessly extends its capabilities to zero-shot scenarios, and preserves the potential for crucial tasks such as interpolation, reconstruction, and editing within the latent space of the pre-trained generator. Code will be available at https://github.com/iceli1007/3D-Adapter.", "sections": [{"title": "1 Introduction", "content": "The realm of image generation has witnessed significant advancements, owing to the evolution of deep generative models. These models encompass Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Diffusion models. Notably, there has been a recent surge in efforts to extend these 2D image generation capabilities to the domain of 3D-aware image generation. This expansion involves integrating rendering techniques with neural scene representation, enabling the synthesis of 2D images while concurrently learning 3D structures without explicit 3D supervision. This innovative training paradigm allows 3D generators to produce highly realistic images with consistent multi-view representations, thereby significantly enhancing the scope and potential of generative models.\nSimilar to 2D generative models , 3D generative models require large-scale training data to ensure training stability and mitigate the risk of overfitting. When training data is limited, these models often suffer significant performance degradation, affecting both texture realism and 3D geometry consistency. Unfortunately, there are scenarios where acquiring sufficient training data is impractical. This paper addresses the challenge of one-shot 3D Generative Domain Adaptation (GDA), a"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 3D-aware Image Generation.", "content": "GANs have acquired substantial recognition for their capacity to facilitate 2D image synthesis Recent studies have extended these capabilities to 3D generation by introducing neural scene representation and rendering into generative models. 3D-aware image generation aims to achieve multi-view-consistent image synthesis and extraction of 3D shapes without requiring supervision on geometry or multi-view image collections.\nEarly methods based on voxel-based (explicit) representations suffered challenges in generating high-resolution content due to the enormous memory demands inherent to voxel grids. In response to these challenges, recent studies have introduced Neural Radiance Field (NeRF)-based (implicit) representations into the realm of 3D-aware image generation. While these approaches have achieved impressive performance, they are characterized by long query times, leading to inefficiencies in the training process and limiting the attainable degree of realism. To address these inefficiencies, recent studies have proposed hybrid representations that combine the benefits of both explicit and implicit representations, resulting in architectures that are more efficient in terms of computation and memory. Notable examples of such hybrid architectures include EG3D, which integrates a tri-plane hybrid 3D representation with a StyleGAN2-based framework, and Rodin, which utilizes a diffusion model to create a tri-plane hybrid 3D representation. Our method builds on EG3D, the most popular 3D-aware image generation technology.\nConcurrently, emerging researchers have embarked on leveraging pre-trained generative models to advance 3D content generation. A notable example is the Dreamfusion model, which employs Score Distillation Sampling techniques to distill knowledge from a pre-trained 2D text-to-image diffusion model. The primary objective of this approach is to optimize the NeRF for text-to-3D synthesis. However, Dreamfusion relies on a low-resolution diffusion model and a large global MLP for volume rendering, making the approach computationally expensive and prone to performance degradation as image resolution increases. To address these limitations, Magic3D adopts a two-stage coarse-to-fine framework and utilizes a sparse 3D hash grid structure to enable high-resolution text-to-3D synthesis. This innovation enhances both the efficiency and quality of the generated 3D content, overcoming the drawbacks associated with Dreamfusion."}, {"title": "2.2 Few-shot GDA in 2D GANs.", "content": "Few-shot GDA is focused on the challenge of transferring a pre-trained source generator to a target domain using few reference images, sometimes as scarce as one. Few-shot GDA is underscored by three attributes: (i) High fidelity. The adapted images should be in the same domain as the few-shot target images. (ii) Large diversity. The adapted generator should not simply replicate the training images. (iii) Cross-domain consistency. The adapted images and their corresponding source images should be consistent in terms of domain-sharing attributes.\nNevertheless, the deployment of few-shot GDA is beset by significant challenges arising from limited training data. These challenges manifest as severe overfitting and unstable training processes, which detrimentally impact the diversity and realism of the generative output. To address these issues, recent studies have explored fine-tuning the entire generator, complemented by various regularization techniques. For instance, some studies have introduced a consistency loss based on Kullback-Leibler (KL) divergence while others have advocated the adoption of contrastive learning methodologies to preserve the relative similarities between the source and target domains. These approaches aim to mitigate the issues of overfitting and instability, thereby enhancing the performance of few-shot GDA models."}, {"title": "3 Methods", "content": null}, {"title": "3.1 NeRF and EG3D", "content": "NeRF offers an implicit representation of 3D scenes by employing a 5D vector-valued function. This function takes as input a 3D spatial location, denoted as $x = (x,y,z)$, and a 2D viewing direction, represented as $d = (\\theta, \\phi)$. The output of this function encompasses the emitted color, denoted as $c = (r, g, b)$, as well as the volume density, symbolized by $\\sigma$. In practical implementation, this continuous 5D representation is approximated through a Multi-Layer Perceptron (MLP), denoted as $F_{\\Theta} : (x,d) \\rightarrow (c, \\sigma)$.\nWithin the NeRF framework, every scene is characterized by its volume density and the radiance emitted in a particular direction. Consequently, the color of any ray traversing the scene can be rendered using principles derived from classical volume rendering. By interpreting the volume density $\\sigma(x)$ as the differential probability of a ray terminating at an infinitesimal particle situated at location $x$, it becomes possible to calculate the expected pixel value $C(r)$ along a camera ray $r(t) = o + td$, where $r(t)$ represents a ray emanating from the camera centered at position $o$, with near and far bounds delineated by $t_n$ and $t_f$. This calculation is given by the integral:\n$C(r) = \\int_{t_n}^{t_f} T(t)\\sigma(r(t))c(r(t), d)dt$,\nwhere $T(t) = exp(-\\int_{t_n}^{t} \\sigma(r(s))ds)$ signifies the accumulated transmittance along the ray from $t_n$ to $t$. The trainable parameters $\\Theta$ are optimized through a process of training to yield a value of $C(r)$ that approximates the actual pixel value observed in the ground truth data.\nEG3D has gained considerable popularity as a 3D-aware image generation method, leveraging both GANs and NeRF. An overview of the generator architecture is presented in the top part of Figure 2. This architecture involves combining the latent code $z$ with camera parameters $P$, resulting in an intermediate latent code $w$ derived via the mapping network $M$. This intermediate latent code $w$ is then used to modulate the Style-based Generator $G_1$, leading to the generation of feature maps $F$ with dimensions $H_f \\times H_f \\times 3M_f$. The feature maps $F$ undergoes a channel-wise splitting and reshaping process, yielding three $M_f$-channel planes characterized by a resolution of $H_f \\times H_f \\times M_f$. Furthermore, EG3D is capable of querying any 3D position $x \\in \\mathbb{R}^3$ by projecting it onto each of the three feature planes. Subsequently, the corresponding feature vector is retrieved through interpolation, and these three feature vectors are aggregated via summation. Additionally, an additional MLP, denoted as Tri-D, featuring a single hidden layer comprising 64 units with softplus activation functions, interprets the aggregated 3D features $F$ as color $c$ and density $\\sigma$.\nBoth of these quantities are then subjected to processing by a neural volume renderer, facilitating the projection of the 3D feature volume into a 2D feature image. It's worth noting that, unlike the volume rendering approach in NeRF, EG3D's volume rendering produces feature images, specifically a 32-channel feature image $I_F$, as opposed to RGB images. This choice is made in recognition of the fact that feature images inherently contain a richer information content that can be effectively harnessed for further refinement and generation processes. Finally, the latent code $w$ is also utilized to modulate the Style-based Super-resolution module $G_2$. This module performs the critical tasks of upsampling and refining the 32-channel feature image $I_F$, yielding the final RGB image $I_{rgb}$ with dimensions $H \\times W \\times 3."}, {"title": "3.2 Training Parameters Determination", "content": "In our approach, we adapt a pre-trained EG3D generator using information from a single reference image. Notably, when faced with limited training data, 3D GANs tend to experience significant performance degradation compared to their 2D counterparts . Consequently, our methodology involves identifying a subset of model parameters that offer the requisite expressiveness for downstream adaptation. To determine the most effective components for fine-tuning, we conduct an extensive series of ablation studies on various module elements within EG3D. These elements include the Mapping Network (M), the Style-based Generator ($G_1$), the Tri-plane Decoder (Tri-D), and the Style-based Super-resolution Module ($G_2$).\nTo achieve adaptation to the target domain using only a single reference image, we employ the original adversarial loss to fine-tune different components of the original EG3D generator. As depicted in Figure 1, we compare the original images generated by the pre-trained generator with the results of five distinct adaptation approaches. We select a sketch image as the target domain and follow the exact training settings of EG3D. The illustrated images are generated after the generator has processed the target image 10,000 times. Notably, fine-tuning only the Mapping Network (M Only) fails to achieve the necessary levels of high fidelity and cross-domain consistency. On the other hand, fine-tuning only the Style-based Generator ($G_1$ Only) or all components of EG3D (All) results in unstable training and significant performance degradation in terms of generative quality. In contrast, focusing solely on fine-tuning the Tri-plane Decoder (Tri-D Only) or the Style-based Super-resolution Module ($G_2$ Only) may not fully adapt to the target domain but does enable stable training and preserves cross-domain consistency. This observation suggests the potential for designing efficient training algorithms to achieve one-shot 3D GDA. Consequently, we introduce a novel training strategy that focuses on fine-tuning either the Tri-plane Decoder or the Style-based Super-resolution Module in this study. This approach ensures stable training and maintains high fidelity and cross-domain consistency."}, {"title": "3.3 Loss Functions", "content": "As depicted in Figure 1, it becomes evident that simply relying on the adversarial loss does not effectively facilitate the adaptation of the source model to the target domain. This divergence from the principles of GDA in the 2D domain underscores the unique challenges faced in the one-shot 3D GDA context. To address the issues of training instability in this scenario, we introduce four loss functions: Domain Direction Regularization, Target Distribution Learning, Image-Level Source Structure Maintenance, and Feature-Level Source Structure Maintenance, as detailed in this section. A comprehensive overview of EG3D and our proposed methodology are provided in the top and bottom parts of Figure 2, respectively. Specifically, the EG3D generator comprises a Synthesis Network (G) and a Super-Resolution Network (S). The Synthesis Network includes the Mapping Network (M), Style-based Generator ($G_1$), Tri-plane Decoder, and Volume Rendering, while the Super-Resolution Network consists of the Style-based Super-resolution Module ($G_2$).\nDomain Direction Regularization. Recent studies have provided sufficient evidence of the effectiveness of leveraging pre-trained CLIP models for the purpose of transferring a source generator to target domains. This applicability extends to both zero-shot and one-shot scenarios. When compared to traditional adversarial-based methods, CLIP-based methodologies exhibit a noteworthy advantage in terms of training stability, making them well-suited for our one-shot 3D GDA. In our approach, we introduce Domain Direction Regularization, leveraging the pre-trained CLIP image encoder to identify the CLIP-space direction between the source and target domains. Given the 3D source generator pre-trained on source domain (domain A) and a reference image $I_{tar}$ from the target domain (domain B), the CLIP-space domain direction between these two domains is computed as follows:\n$\\Delta u_{dom} = u_{tar} - u_{sou}$,\nwhere $u_{tar} = E_I(I_{tar})$ denotes the embedding of the target domain B, and $E_I$ is the CLIP image encoder. $u_{sou}$ is the mean embedding of source images, i.e., $u_{sou} = \\mathbb{E}_{z\\sim\\mathcal{N}(0,1)} [E_I (S_A(G_A(z)))]$. To adapt the source generator $G_A$ to target domain B, we finetune the target generator by aligning the sample-based direction $\\Delta u_{samp}$ with the CLIP-space domain direction $\\Delta u_{dom}$:\n$L_{dir} = 1 - \\frac{\\Delta u_{samp} \\cdot \\Delta u_{dom}}{|\\Delta u_{samp}|||\\Delta u_{dom}||}$,\n$\\Delta u_{samp} = u_B - v_A$,\nwhere $u_B = E_I(S_B(G_B(z)))$ and $v_A = E_I(S_A(G_A(z)))$. $L_{dir}$ not only compels the target generator to assimilate"}, {"title": "3.4 Progressive Fine-tuning Strategy", "content": "While our proposed loss functions and determined training sub-parameters significantly alleviate training instability in one-shot 3D GDA, it is essential to address the issue of under-fitting, as evident when directly fine-tuning the networks (illustrated in Figure 3 1), where both fine-tuning of the Tri-plane Decoder (Tri-D Only) and Style-based Super-resolution Module (G2 Only) results in under-fitting, while fine-tuning both of these components (G2 & Tri-D) leads to severe over-fitting. To address this challenge, we introduce a two-step progressive fine-tuning strategy.\nStep 1: Fine-tuning the Tri-plane Decoder (Tri-D) with the following objective functions:\n$\\Theta_{Tri\\text{-}D} = \\arg \\min_{\\Theta_{Tri\\text{-}D}} \\lambda_{dir} L_{dir} + \\lambda_{dis}L_{dis} +\\lambda_{I\\text{-}str}L_{I\\text{-}str} + \\lambda_{F\\text{-}str}L_{F\\text{-}str}$.\nStep 2: Fine-tuning the Style-based Super-resolution Module (G2) with the following objective functions:\n$\\Theta_{G2} = \\arg \\min_{\\Theta_{G2}} \\lambda_{dir} L_{dir} + \\lambda_{dis} L_{dis} + \\lambda_{I\\text{-}str}L_{I\\text{-}str}$.\nIt is important to note that $L_{F-str}$ has no impact during the fine-tuning of the G2, and therefore, it is omitted in Step 2. In our experiments, we use $\\lambda_{dir} = 1$, $\\lambda_{dis} = 2$, $\\lambda_{I-str} = 3$, and $\\lambda_{F-str} = 5$. As depicted in Figure 3, our proposed progressive fine-tuning strategy demonstrates superior performance compared to other single-step training approaches."}, {"title": "4 Experiments", "content": "This section is dedicated to empirical validation of the advancements introduced by our 3D-Adapter. We begin by outlining the experimental settings in Section 4.1, subsequently presenting both quantitative and qualitative results in Section 4.2. In Section 4.3, we provide insights from a user study. Ablation studies are conducted to assess the impact of individual components in Section 4.4. Moreover, Section 4.5 demonstrates the seamless extension of our 3D-Adapter to zero-shot GDA scenarios, yielding impressive outcomes. Section 4.6 encompasses additional results showcasing latent space interpolation, inversion, and editing capabilities on the adapted generator. Finally, Section A of the Appendix uploads a set of videos showcasing both one-shot and zero-shot GDA."}, {"title": "4.1 Experimental Settings", "content": "Implementation. Building upon prior research , our foundation rests on the EG3D generator, which has been pre-trained on the FFHQ dataset. Furthermore, we leverage the pre-trained CLIP model , specifically ViT-B/16 and ViT-B/32, to implement our domain direction regularization, target distribution learning, and image-level source structure maintenance. To capture the desired image features effectively, we extract information from the third layer of the CLIP image encoder. In the domain direction regularization, we extract CLIP image features from a corpus of 5000 source images to derive the mean image embedding of the source domain, represented as $u_{sou}$. Our training employs the ADAM optimizer, with a learning rate set at 0.0025, and a batch size of 16. Fine-tuning the generator encompasses approximately 600 iterations in training Step 1, followed by an additional 1200 iterations in training Step 2.\nDatasets. Consistent with prior work on domain adaptation for 3D GANs , the FFHQ dataset, featuring images with a resolution of 512 \u00d7 512, is designated as the source domain for the experimental framework presented in this paper."}, {"title": "4.2 Quantitative and Qualitative Results", "content": "Qualitative comparison. In Figure 4, we present qualitative comparisons using the FFHQ dataset as the source domain. These results highlight the challenges and nuances encountered in the context of one-shot 3D GDA. Firstly, we consider the adversarial-based approach \"DoRM\". Our observations indicate that it exhibits significant issues, including unstable training dynamics and substantial performance degradation. The resulting images often fail to meet the desired standards of one-shot 3D GDA, reflecting the inherent difficulties in employing adversarial loss for this task. We then turn our attention to the non-adversarial-based DiFa method. DiFa operates by fine-tuning the entire generator, which results in a severe form of model collapse. A notable manifestation of this is the generation of nearly identical images, as illustrated in Figure 4 (b), where the generated images closely resemble the reference images. This aspect highlights a significant limitation in DiFa's performance, particularly in terms of diversity and adaptability. Additionally, we have incorporated comparison results with other baselines, including One-shot CLIP , Few-shot GAN adaptation , and Mind the Gap , which are detailed in Section D of the Appendix. Furthermore, we provide qualitative results for one-shot 3D GDA on the AFHQ-Cat domain and cross-domain adaptation in Section B and Section C of the Appendix, respectively.\nIn contrast, our proposed approach demonstrates a superior capacity to capture domain-specific characteristics from a single reference image while retaining substantial information related to the identity and structure of the source image. Our method surpasses existing approaches by effectively addressing the dual challenges of domain adaptation and identity preservation.\nQuantitative comparison. We also conducted a quantitative comparative analysis between our proposed method and other existing methodologies . This evaluation was performed under three experimental configurations, namely, FFHQ \u2192 Cartoon, Sketches, Ukiyoe. For each of these settings, we adopted a randomized approach, selecting one image from the respective target dataset for adaptation. The results encompassing all seven critical metrics are presented in Table 1. To mitigate potential random sampling variability, we conducted five iterations of the adaptation process and computed the mean values as the final scores. Our analysis showcases the superior performance of our proposed method in comparison to the baseline approaches. Specifically, in comparison to state-of-the-art methods within the realm of 2D One-shot GDA, our proposed method exhibits significant improvements across all desired attributes, including target domain consistency, extensive diversity, cross-domain consistency, and multi-view consistency. It is pertinent to note that the DiFa method exhibits a pronounced susceptibility to model"}, {"title": "4.3 User Study", "content": "To provide comprehensive evaluation of our approach, we conducted a user study designed to complement quantitative metrics. This user study, involving feedback from participants, was instrumental in providing a holistic understanding of our method's performance. Specifically, we presented users with a set of reference, source, and three adapted images generated by various methods. We tasked users with selecting the most suitable adapted image based on three criteria: image quality, style similarity with the reference, and attribute consistency with the source image. To ensure the robustness of our findings, we generated a substantial sample size of 500 images for each method and enlisted the feedback of 50 users. Each user was randomly assigned 50 samples and given ample time to complete the task. The results, as depicted in Table 2, demonstrate a strong preference for our method across all three evaluation aspects, with particularly notable favorability in terms of image quality and attribute consistency. It is noteworthy that the DiFa method is susceptible to severe mode collapse, often leading to the replication of reference images. Consequently, it garners favor on style similarity but lags behind in other critical aspects of evaluation."}, {"title": "4.4 Ablation Study", "content": "Ablation studies are conducted to evaluate the effects of different critical components of our proposed method, i.e., the progressive training strategy and different advanced loss functions. Additionally, we explore the influence of different selections of CLIP's layers on both target distribution learning and image-level source structure maintenance."}, {"title": "5 Limitation and Conclusion", "content": null}, {"title": "5.1 Limitation and Future Works", "content": "i) Although our method can achieve appealing results in one-shot 3D GDA across different target domains, it does not always perfectly maintain cross-domain consistency in some target domains. As shown in the fifth and sixth rows of Figure 4, the gender of some adapted images has been altered. Therefore, more advanced loss functions should be proposed in the future to ensure the cross-domain consistency of domain-independent properties. ii) The current method can only accomplish generative domain adaptation for a single domain and is unable to retain and integrate knowledge from multiple domains. This limitation prevents adaptive generators from exploring previously unseen domains. In the future, we aim to develop a generator that can integrate multiple learned domains and synthesize hybrid domains not encountered during training."}, {"title": "5.2 Conclusion", "content": "This paper introduces 3D-Adapter, the first method for one-shot 3D GDA. 3D-Adapter contains three integral components: Firstly, we conducted an extensive investigation that revealed fine-tuning specific weight sets, Tri-D and G2, as a key strategy to enhance training stability and alleviate the challenges associated with one-shot 3D GDA. Secondly, we harnessed the power of four advanced loss functions to tackle training instability and successfully realize the four essential properties of 3D GDA. Lastly, we implemented an efficient progressive fine-tuning strategy to further augment the efficacy of our approach. Qualitative and quantitative experiments demonstrate the superiority of 3D-Adapter compared to state-of-the-art methods across a wide array of scenarios. Moreover, 3D-Adapter readily extends its capabilities to zero-shot 3D GDA, yielding compelling results. Additionally, it enables latent interpolation, image inversion, and image editing within diverse target domains."}, {"title": "Appendix", "content": null}, {"title": "A Videos", "content": "In this section, we have uploaded a set of videos to the supplementary materials that showcase both one-shot 3D GDA and zero-shot GDA. Specifically, we generate sixteen different videos for each target domain. These videos clearly demonstrate the effectiveness of our proposed method."}, {"title": "B Experiments on Other Source Domain", "content": "In addition to the experiments on FFHQ dataset, we conducted further one-shot 3D GDA experiments to qualitatively evaluate the effectiveness of our proposed 3D-Adapter approach. Specifically, we used the source EG3D generator pre-trained on the AFHQ-Cat dataset and adapted it to four different target domains. The results of these experiments are presented in Figure 12. The qualitative outcomes show that our approach demonstrates a superior capacity to capture domain-specific characteristics from a single reference image while retaining substantial structural information from the source image."}, {"title": "C Experiments on Cross-domain Adaptation", "content": "In this section, we have conducted additional experiments to demonstrate cross-domain adaptation, similar to those presented in Figure 5 of One-Shot Generative Domain Adaptation. Specifically, we selected four natural images as reference images for a face source model, with the expectation that the synthesis after adaptation would maintain consistent visual concepts. In other words, a face model is anticipated to continue generating faces, regardless of the target image. As shown in Fig. 13, the source models successfully produce the corresponding content. However, due to the limited shared attributes between faces and natural images, the 3D-Adapter primarily focuses on variation factors such as color schemes, textures, and painting styles, which can be directly transferred across unrelated domains. Nonetheless, compared to domain adaptation within more closely related domains, cross-domain adaptation exhibits a decline in the quality of the generated images."}, {"title": "D More Comparison on One-shot 3D GDA", "content": "The results in Figure 4 of the manuscript show that the adversarial-based method DoRM suffered from severe training failure, while the non-adversarial-based approach DiFa experienced model collapse, resulting in generated samples that fully replicate the training data and lack generative diversity. To further verify our hypothesis, we have incorporated comparison results with additional baselines, including One-shot CLIP (T\u03a1\u0391\u039c\u0399 23), Few shot GAN adaptation (CVPR 21), and Mind the Gap (ICLR 22), in this section. The results, shown in Figure 14, confirm that adversarial-based methods like One-shot CLIP and Few-shot GAN Adaptation indeed suffer from severe training failures. Meanwhile, non-adversarial-based methods like Mind the Gap also experience model collapse, leading to generated samples that fully replicate the training data and lack generative diversity. By including these comparisons, we provide a more comprehensive evaluation of our proposed method's performance and further substantiate its effectiveness."}, {"title": "E More Comparison on Zero-shot 3D GDA", "content": "To provide a comprehensive evaluation, we have included a comparative analysis with StyleGAN-Fusion and DATID-3D, as they are two relevant and accessible baselines. As shown in Figure 15, our proposed 3D-Adapter demonstrates superior fidelity, diversity, and cross-domain consistency compared to both StyleGAN-Fusion and DATID-3D. Specifically, StyleGAN-Fusion and DATID-3D exhibit lower fidelity and fail to retain certain domain-independent attributes of the source domain, such as gender and the presence of eyeglasses. Moreover, StyleGAN-Fusion and DATID-3D encounter difficulties in adapting to the target domain under some settings."}]}