[{"title": "On the Expressive Power of Modern Hopfield Networks", "authors": ["Xiaoyu Li", "Yuanpeng Li", "Yingyu Liang", "Zhenmei Shi", "Zhao Song"], "abstract": "Modern Hopfield networks (MHNs) have emerged as powerful tools in deep learning, capable\nof replacing components such as pooling layers, LSTMs, and attention mechanisms. Recent ad-\nvancements have enhanced their storage capacity, retrieval speed, and error rates. However, the\nfundamental limits of their computational expressiveness remain unexplored. Understanding the\nexpressive power of MHNs is crucial for optimizing their integration into deep learning architec-\ntures. In this work, we establish rigorous theoretical bounds on the computational capabilities\nof MHNs using circuit complexity theory. Our key contribution is that we show that MHNs\nare DLOGTIME-uniform TC\u00ba. Hence, unless TC\u00b0 = NC\u00b9, a poly(n)-precision modern Hopfield\nnetworks with a constant number of layers and O(n) hidden dimension cannot solve NC\u00b9-hard\nproblems such as the undirected graph connectivity problem and the tree isomorphism problem.\nWe also extended our results to Kernelized Hopfield Networks. These results demonstrate the\nlimitation in the expressive power of the modern Hopfield networks. Moreover, Our theoretical\nanalysis provides insights to guide the development of new Hopfield-based architectures.", "sections": [{"title": "1 Introduction", "content": "Hopfield networks [Hop82], initially introduced as associative memories capable of storing and re-\ntrieving patterns, have undergone significant advancements in recent years. These developments led\nto the emergence of modern Hopfield networks [RSL+21]. Modern Hopfield networks (MHNs) ad-\ndress the limitations of their predecessors, particularly in terms of storage capacity, retrieval speed,\nand error rates. An important feature of MHNs is their ability to function as specialized components\nwithin deep networks, integrating memory capabilities and supporting diverse functionalities. For\ninstance, MHNs can replace conventional layers such as pooling layers, permutation-equivariant\nlayers [GVW+16, RSP16], GRU [CvMG+14], LSTM [Hoc91, HS97], and attention mechanisms\n[VSP+23, BCB16].\nUnderstanding modern Hopfield networks' computational capabilities and limitations is critical\nfor their effective application. Specifically, it is crucial to explore the computational operations\nthese networks can implement and the complexity of the problems they can solve collectively.\nThese investigations are vital not only for theoretical insights but also for guiding the development\nof more efficient and powerful models.\nWhile prior research has primarily focused on the dynamics and capacity of modern Hopfield\nnetworks [HLSL24, WHHL24], their expressiveness from a circuit complexity perspective remains\nunderexplored. This gap raises an important question:\nWhat are the fundamental limits of modern Hopfield networks in terms of circuit complexity?\nTo address these questions, we turn to the framework of circuit complexity theory, which pro-\nvides a robust method for analyzing the computational resources required to perform specific tasks.\nBy mapping modern Hopfield networks to computational circuits, we can rigorously assess their\ncapabilities and establish upper and lower bounds on the classes of problems they can solve.\nIn this work, we present a comprehensive theoretical investigation into the circuit complexity\nbounds of MHNs. Our approach involves analyzing the architecture of MHNs and the computational\ncomplexity of its components, such as the Hopfield layer. Hence, we show that uniform TC\u00ba circuits\ncan efficiently simulate these models.\nOur main contributions can be outlined as follows:\n\u2022 We prove that any poly(n)-precision modern Hopfield networks or kernelized Hopfield net-\nworks with constant-depth and O(n) hidden dimension is in DLOGTIME-uniform TC\u00ba circuit\nfamily (Theorem 4.6 and 5.4).\n\u2022 We prove that unless TC\u00b0 = NC\u00b9, a poly(n)-precision modern Hopfield networks or kernel-\nized Hopfield networks with constant layers, O(n) hidden dimension cannot solve NC\u00b9-hard\nproblems such as the undirected graph connectivity problems and tree isomorphism problems\n(Theorems 6.1, 6.2, 6.3 and 6.4).\nRoadmap. In Section 2, we provide an overview of the related works. In Section 3, we introduce\nthe notations and definitions needed for modern Hopfield networks. In Section 4, we analyze the\ncircuit complexity for modern Hopfield networks. In Section 5, we focus on the circuit complexity\nresults of kernelized Hopfield networks. In Section 6, we discuss the hardness results of the modern\nHopfield networks. Finally, in Section 7, we summarize our theoretical results in the conclusion.\nWe defer more preliminaries and technique details in Appendix."}, {"title": "2 Related Works", "content": "In this section, we introduce the related works of the Hopfield models and the complexity of\nTransformers. In Appendix A, we introduce more related works."}, {"title": "2.1 Theory of Modern Hopfield Networks", "content": "Modern Hopfield Models have demonstrated both empirical success and theoretical value, providing\na framework for understanding transformer attention mechanisms and Transformer architectures.\n[HYW+23, WHHL24] propose a unified framework for analyzing and deriving modern Hopfield\nmodels using entropic regularizers. This framework includes a range of sparse variants, such as\nsparse and generalized sparse models, and also integrates the standard modern Hopfield model\n[RSL+21] as a special case. Despite these advancements, the modern Hopfield paradigm remains\nincomplete, lacking efficient implementations and variants, as highlighted by [HYW+23] To address\nthese limitations, [HCW+24] propose a principled nonparametric approach for developing efficient\nvariants of modern Hopfield models, including linear, top-K, and random feature-based versions.\nThis study focuses on advancing research to create more efficient models. This study aims to push\nthe boundaries of research in developing more efficient models, emphasizing their potential to shape\nthe future of Hopfield-driven designs."}, {"title": "2.2 Complexity and Neural Network", "content": "Complexity theory offers a formal framework to analyze the computational power and limitations\nof models. Circuit complexity, a key domain within this field, characterizes the power of Boolean\ncircuits and has recently been applied to investigate the expressive capacity of deep neural networks\nand Transformers [MSS22, LAG+22, MS23a, MS23b, LLZM24, Chi24]. Several important circuit\ncomplexity classes play a central role in machine learning. For example, AC\u00ba includes problems\nsolvable with highly parallelizable logic gates, TC\u00ba extends this class by incorporating threshold\ngates, and NC\u00b9 describes languages recognized by circuits with O(log n)-depth and bounded fan-\nin [MSS22]. These classes exhibit the hierarchy AC\u00ba \u2282 TC\u00ba \u2286 NC\u00b9, although whether TC\u00b0 \u2260 NC\u00b9\nremains an open question.\nTransformers' depth requirements have been analyzed through the lens of these complexity\nclasses. For instance, [LAG+22] demonstrates that the depth of a Transformer must scale with\ninput length to simulate certain non-solvable semi-automata. In a similar vein, [LLZM24] in-\nvestigates the connection between constant-depth Transformers, chain-of-thought (CoT) reason-\ning, and circuit complexity. They show that T[poly(n), 1,1] \u2286 AC\u00ba,\nand T[poly(n), log n, 0] \u2286 CoT[log n, poly(n), log n,0] \u2286 TC\u00ba, Here, T[d(n), s(n), e(n)] denotes the\nconstant-depth Transformers characterized by precision of s(n), an embedding size of d(n), and e(n)\nexponent bits, while CoT[T(n), d(n), s(n), e(n)] refers to a chain-of-thought (CoT) Transformer op-\nerating for T(n) steps. These findings highlight that intermediate reasoning steps in CoT models\ncan improve computational expressivity.\nThe Strong Exponential Time Hypothesis (SETH), which posits that no k-SAT algorithm exists\nwith runtime O(2(1-\u20ac)n) for \u20ac > 0 and k > 3, serves as a cornerstone in fine-grained complexity\nanalyses [IP01]. Results derived from SETH have shaped Transformer-related studies, particularly\nfor tensor attention mechanisms [AS23a, AS24b, LSSZ24b, LSS+24b, AS23b]. For example, [AS23a]\nshows that by assuming SETH, it is impossible to perform forward pass computation of attention-\nbased networks in O(n\u00b2) time, while [AS24b] extends this limitation to backward computations.\nThese results underscore the intrinsic computational challenges of Transformer architectures."}, {"title": "3 Preliminaries", "content": "In Section 3.1, we define useful notations for defining modern Hopfield networks. In Section 3.2,\nwe introduce some components of modern Hopfield networks. In Section 3.3, we state the basic\ndefinitions and architectures of kernelized Hopfiled networks. The background knowledge about\ncircuit complexity and float point number is introduced in Appendix B."}, {"title": "3.1 Notations", "content": "We define N := {0,1,2,...} as the set of natural numbers and R as the set of real numbers. For\nany positive integer n, [n] represents the set {1, 2, ..., n}. The vector $1_n$ denotes an n-dimensional\nvector where all entries are ones. Given a matrix $A \\in R^{m\\times n}$, $A_{i,j}$ refers to the element in the i-th\nrow and j-th column, while $A^T$ represents the transpose of A. The set {0,1}* represents all binary\nstrings of finite length. Specifically, for $x_i \\in {0,1}^*$, $x_i$ represents a finite binary string, with each\nbit being either 0 or 1. A language L is characterized as a subset of {0,1}*."}, {"title": "3.2 Modern Hopfield Network", "content": "With the above notations established, we proceed to introduce the foundational concepts of Modern\nHopfield Networks.\nThe input query pattern is denoted as $x \\in R^d$, while memory patterns are represented by the\nmatrix $\\Xi = [\\xi_1,...,\\xi_M] \\in R^{d\\times M}$. Hopfield models are associative memory models based on energy\nfunctions, where the stored memory patterns $\\Xi$ correspond to the local minima of these energy\nlandscapes. For a given input query x, the model retrieves the most similar memory pattern by\nemploying energy minimization algorithms, referred to as retrieval dynamics T, initialized at x.\nIn [RSL+21], the Modern Hopfield Model is introduced with a specific energy function $\\mathcal{E}$ and\nretrieval dynamics T, which are further incorporated into deep learning frameworks due to their\nrelation to transformer attention mechanisms [VSP+23]. This approach enhances performance and\nprovides a theoretical guarantee of exponential memory capacity. The energy function is defined\nas:\n$\\mathcal{E}(x) = -\\text{lse}(\\beta, \\Xi^T x) + \\frac{1}{2}(x,x)$,\nwhere the retrieval dynamics is given by\n$x^{\\text{new}} = T_{\\text{Dense}}(x) = \\Xi \\cdot \\text{Softmax}(\\beta \\Xi^T x)$.\n(1)\nand the function $\\text{lse}(\\beta, z)$ is the log-sum-exponential, defined as $\\text{lse}(\\beta, z) := \\log(\\sum_{\\mu=1}^M \\exp{\\beta z_{\\mu}}) /\\beta$\nfor any $z \\in R^M$ and $\\beta > 0$. When applied to a sequence of input queries $X = [x_1,...,x_L] \\in R^{d\\times L}$,\nEq. (1) becomes: $Z := [x^{\\text{new}}_1,...,x^{\\text{new}}_L] = T_{\\text{Dense}}(X)$, and hence\n$T_{\\text{Dense}} (X) = \\Xi \\cdot \\underset{M\\times L}{\\text{Softmax}(\\beta \\Xi^T X)} \\in R^{d \\times L}$,\nwhere Softmax() performs column-wise normalization. We assume that $d = L^{o(1)}$, meaning the\ngrowth rate of d is sub-polynomial relative to L.\nModern Hopfield Networks with continuous states are compatible with deep learning models\ndue to their differentiable nature and the ability to retrieve patterns in a single update step. This"}, {"title": "3.3 Kernelized Hopfield Networks", "content": "The capacity of modern Hopfield networks is suboptimal. To improve the capacity, [WHHL24]\nintroduces a kernel as a learnable similarity measure, using stored memory patterns as training data\nto enhance memory capacity. Specifically, they propose the kernelized Hopfield network (KHM)\ndefined by following update rule and energy function:\n$T_{\\Theta}(x) := \\Xi \\cdot \\text{Softmax}(\\beta K(\\Xi,x))$, $\\mathcal{E}_K(x) = \\frac{1}{2} K(x,x) + \\text{lse}(\\beta, K(\\Xi,x))$,\nwhere the kernel $K(\\cdot,\\cdot) := \\langle \\Phi(\\cdot), \\Phi(\\cdot) \\rangle : R^d \\times R^d \\rightarrow R$ is associated with a learnable feature\nmap $\\Phi : R^d \\rightarrow R^D$. Here, $K(\\cdot,\\cdot)$ acts column-wise on matrix: $K(\\Xi,x) = [\\{K(\\xi_{\\mu},x)\\}_{\\mu=1}^M] =$\n$[\\{\\langle \\Phi(\\xi_{\\mu}), \\Phi(x) \\rangle\\}_{\\mu=1}^M] \\in R^M$. Accordingly, we define the kernelized Hopfield layer.\nDefinition 3.5 (Kernelized attention matrix). Let $R \\in F_p^{n \\times d}$ be the set of query (state) patterns,\nand $Y \\in F_p^{n \\times d}$ be the set of key (stored) patterns. Let $W_Q \\in F_p^{d \\times D}, W_K \\in F_p^{d \\times D_0}$, and $W_v \\in F_p^{d \\times d}$\nbe learnable projection matrices. Consider a feature map $\\Phi : F_p^D \\rightarrow F_p^{D}$ associated with a kernel\nfunction $K: F_p^D \\times F_p^D \\rightarrow F_p$ defined by $K(\\cdot,\\cdot) := \\langle \\Phi(\\cdot), \\Phi(\\cdot) \\rangle$. Let $\\beta > 0$ be a scaling parameter.\nAccording to Definition 3.1, the kernelized Hopfield attention matrix $A \\in F_p^{n \\times n}$ is defined by, for\nevery i, j \u2208 [n],\n$A_{i,j} := \\exp(\\beta \\cdot \\langle \\Phi(R_i W_Q), \\Phi(Y_j W_K) \\rangle )$\nThe kernelized Hopfield attention matrix is the basis for computing a single kernelized Hopfield\nlayer.\nDefinition 3.6 (Single kernelized Hopfield layer). The result pattern $Z \\in F_p^{n \\times d}$ are a function of\nraw stored patterns $Y \\in F_p^{n \\times d}$, raw state pattern $R \\in F_p^{n \\times d}$, and projection matrices $W_Q, W_K, W_v \\in$\n$F_p^{d \\times d}$ (For simplicity, we denote $W_v$ in Hopfield layer as $W_v$):\n$Z = \\text{softmax}(\\beta \\cdot \\langle \\Phi(R W_Q), \\Phi(Y W_K) \\rangle) YW_v$\nNote that the softmax is applied row-wise, $\\langle\\Phi(R W_Q), \\Phi(Y W_K)\\rangle_{i,j} = \\langle \\Phi(R_i W_Q), \\Phi(Y_j W_K)\\rangle$. We set\n$D := \\text{diag}(\\beta \\cdot A1_n) \\in F_p^{n \\times n}$. Then, based on Definition 3.1, we define the i-th kernelized Hopfield\nlayer as\n$KHop_i(R, Y_i) := D^{\\circledR -1} A Y_i W_v$\nMultiple kernelized Hopfield layers can be integrated with additional components to construct\nthe kernelized Hopfield network.\nDefinition 3.7 (Kernelized Hopfield network). We use m to denote the number of kernelized\nHopfield layers in the network. For $i \\in \\{0,1,2,...,m\\}$, we use $f_i$ to denote the other components\nof i-th kernelized Hopfield layer, where $f_i: F_p^{n \\times d} \\rightarrow F_p^{n \\times d}$. Let $KHop_i$ denote the i-th kernelized\nHopfield layer. Define $R \\in F_p^{n \\times d}$ as the state (query) patterns or input matrix, and $Y_i \\in F_p^{n \\times d}$ as the\nstored (key) patterns in the i-th kernelized Hopfield layer. The m-layer kernelized Hopfield network\n$KHN: F_p^{n \\times d} \\rightarrow F_p^{n \\times d}$ is defined as:\n$KHN(R) = f_m \\circ KHop_m(f_{m-1} \\circ KHop_{m-1}(... f_1 \\circ KHop_1 (f_0(X), Y_1) \\text{\\ldots}, Y_{m-1}), Y_m) \\in F_p^{n \\times d}$,\nwhere o denotes the composition of functions."}, {"title": "4 Complexity of Modern Hopfield Networks", "content": "This section explores key results regarding the circuit complexity of the computations involved\nin modern Hopfield networks. We begin with an analysis of the Hopfield attention matrix in\nSection 4.1. In Section 4.2, we delve into the computation of a single Hopfield layer. Section 4.3\nextends the discussion to other components beyond the Hopfield layer. In Section 4.4, we examine\nthe modern Hopfield network. Finally, Section 4.5 presents our main result: the circuit complexity\nbounds for modern Hopfield networks. These findings form the basis for our main theorem on the\nexpressive power of Hopfield networks. In Appendix B.1, we provide fundamental definitions from\ncircuit complexity theory."}, {"title": "4.1 Computing Hopfield Attention Matrix", "content": "We first recall that matrix multiplication of two matrices is in TC\u00ba.\nLemma 4.1 (Matrix multiplication in TC\u00ba, Lemma 4.2 in [CLL+24a]). Let $p \\leq poly(n), n_1, n_2 \\leq$\npoly(n), and $d \\leq n$. Let $A \\in F_p^{n_1\\times d}$ and $B \\in F_p^{d\\times n_2}$. Then the matrix product AB can be imple-\nmented using a DLOGTIME-uniform threshold circuit which has depth $(d_{\\text{std}} + d_{\\oplus})$ and size bounded\nby poly(n).\nHere, we extend the matrix operations to compute the Hopfield attention matrix.\nLemma 4.2 (Computation of Hopfield attention matrix in TC\u00ba). Let precision $p \\leq poly(n)$. One\ncan simulate the Hopfield attention matrix A using a DLOGTIME-uniform threshold circuit with\ndepth $4d_{\\text{std}} + 3d_{\\oplus} + d_{\\text{exp}}$ and size bounded by poly(n).\nProof. To compute $A_{i,j}$, we use the following steps:\n1. By Lemma 4.1, we can compute the matrix product $W_Q W_K^T$ by a DLOGTIME-uniform circuit\nwhich has size poly(n) and depth $d_{\\text{std}} + d_{\\oplus}$.\n2. By Lemma 4.1, we can the scalar\n$s_{i,j} = R_{i,*} (W_Q W_K^T) Y_j^T$\nby a DLOGTIME-uniform circuit which has size poly(n) and depth $2(d_{\\text{std}} + d_{\\oplus})$, following\nLemma 4.1.\n3. The term $\\beta \\cdot s_{i,j}$ is computed with depth $d_{\\text{std}}$ using Part 1 of Lemma B.5.\n4. Using Lemma B.6, the exponential function $\\exp(\\beta \\cdot s_{i,j})$ is computable with depth $d_{\\text{exp}}$.\nSumming the circuit depths for all steps, the total depth required for $A_{i,j}$ is:\n$d_{\\text{total}} = 4d_{\\text{std}} + 3d_{\\oplus} + d_{\\text{exp}}$.\nSince all entries of A can be simulated in parallel, the total depth $4d_{\\text{std}} + 3d_{\\oplus} + d_{\\text{exp}}$ and size\npoly(n), completing the proof."}, {"title": "4.2 Computing Single Hopfield Layer", "content": "This section analyzes the computation of a single Hopfield layer, including the necessary depth and\nsize requirements.\nLemma 4.3 (Computation of Hopfield layer in TC\u00ba). Let precision $p \\leq poly(n)$. We can simulate\nthe Hopfield layer using a DLOGTIME-uniform threshold circuit with depth $8d_{\\text{std}} + 6d_{\\oplus} + d_{\\text{exp}}$ and\nsize bounded by poly(n).\nProof. The computation involves multiplying matrices $D^{-1}$, A, Y, $W_v$. First, we compute D and\nA:\n1. D = diag($\\beta \\cdot A1_n$) is computed with depth $d_{\\oplus} + d_{\\text{std}}$, as shown in Part 1 and Part 3 of\nLemma B.5.\n2. From Lemma 4.2, A is computed with depth $4d_{\\text{std}} + 3d_{\\oplus} + d_{\\text{exp}}$.\n3. From Lemma 4.2, A is computed with depth $4d_{\\text{std}} + 3d_{\\oplus} + d_{\\text{exp}}$.\n4. Finally, the division $D^{-1} \\cdot (A Y W_v)$ is computed in parallel with depth $d_{\\text{std}}$.\nCombining these depths:\n$d_{\\text{total}} = 8d_{\\text{std}} + 6d_{\\oplus} + d_{\\text{exp}}$.\nThe total size of the circuit remains poly(n), completing the proof."}, {"title": "4.3 Computing Common Components Layers", "content": "Definition 3.3 introduces modern Hopfield networks, which incorporate the Hopfield layer along\nwith other modules such as layer normalization and two-layer ReLU feed-forward networks. In this\nsection, we present the computational complexity of these additional components.\nWe begin by analyzing the circuit complexity of the two-layer ReLU feed-forward networks.\nLemma 4.4 (Computation of Two-layer ReLU Feed-forward Networks in TC\u00ba). Let $p \\leq poly(n)$,\none can simulate the two-layer ReLU feed-forward neural networks using a DLOGTIME-uniform\nthreshold circuit which has depth $4d_{\\text{std}} + 3d_{\\oplus}$ and size bounded by poly(n).\nProof. For each i \u2208 [n], Lemma 4.1 guarantees that the computation of $W_1 \\cdot X_{i,*}$ requires a\nDLOGTIME-uniform circuit which has depth $d_{\\text{std}} + d_{\\oplus}$ and size poly(n). Applying Part 1 of\nLemma B.5, we can compute $W_1 \\cdot X_{i,*} + b_1$ with an additional DLOGTIME-uniform circuit which\nhas depth $d_{\\text{std}}$ and size poly(n). The ReLU activation, $\\text{ReLU}(W_1 \\cdot X_{i,*} + b_1)$, also requires depth\n$d_{\\text{std}}$ and size poly(n) as per Part 1 of Lemma B.5.\nFor the second layer, the circuit depth needed is $2d_{\\text{std}} + d_{\\oplus}$, and the size remains poly(n). Thus,\nthe total circuit depth across both layers is $4d_{\\text{std}} + 3d_{\\oplus}$, with the size still bounded by poly(n)\nbecause these computations are able to be executed in parallel for each i \u2208 [n]."}, {"title": "4.4 Computing Modern Hopfield Networks", "content": "We demonstrate how to compute modern Hopfield networks within the TC\u00ba complexity class."}, {"title": "4.5 Main Result: Circuit Complexity Bound of Modern Hopfield Networks", "content": "Our main result establishes the circuit complexity bounds for modern Hopfield networks.\nTheorem 4.6 (Circuit complexity of modern Hopfield networks). Consider a modern Hopfield\nnetwork \u039c\u0397\u039d where we can simulate each component function $f_i$ for $i \\in [m]$ by a DLOGTIME-\nuniform threshold circuit which has $d_f$ depth and poly(n) size. If precision $p \\leq poly(n)$, hidden\ndimension $d < O(n)$, and number of layers $m = O(1)$, then we can simulate MHN by a DLOGTIME-\nuniform circuit family in TC\u00ba.\nProof. Given m = O(1), Lemma 4.3 establishes that the circuit depth required for MHN(R) is given\nby:\n$(m + 1)d_f + 8md_{\\text{std}} + 6md_{\\oplus} + md_{\\text{exp}}$.\nSince m is a constant, the total circuit depth simplifies to O(1). Furthermore, the size of the\ncircuit remains within poly(n).\nHence, the modern Hopfield network can be realized using a DLOGTIME-uniform circuit family\nbelonging to TC\u00ba, completing the argument."}, {"title": "5 Complexity of Kernelized Hopfield Networks", "content": "In this section, we extend the complexity results of modern Hopfield networks to kernelized Hopfield\nnetworks. The formal definition of kernelized Hopfield networks is provided in Appendix 3.3.\nSection 5.1 analyzes the computation of the kernelized Hopfield attention matrix. In Section 5.2,\nwe examine the computation of a single kernelized Hopfield layer. Section 5.3 details the computa-\ntion of the entire kernelized Hopfield network. Finally, in Section 5.4, we present the main results\non the circuit complexity bounds for kernelized Hopfield networks. In appendix E, we provide the\ncomplete proofs for this section."}, {"title": "5.1 Computing Kernelized Hopfield Attention Matrix", "content": "In this section, we generalize the computing of the Hopfield attention matrix to the kernelized\nHopfield attention matrix."}, {"title": "5.4 Main Result: Circuit Complexity Bound of Kernelized Hopfield Networks", "content": "We now present the main result for kernelized Hopfield networks, establishing their circuit com-\nplexity bounds.\nTheorem 5.4 (Main result, Circuit complexity bound of kernelized Hopfield networks). Assume\nthat for each $i \\in [m]$, the function $f_i$ in KHN is computable by a DLOGTIME-uniform threshold\ncircuit with constant depth $d_f$ and size poly(n). If precision $p \\leq poly(n)$, hidden dimension $d \\leq$\nO(n), and number of layers $m \\leq O(1)$, then we can simulate the kernelized Hopfield networks KHM\nby a DLOGTIME-uniform circuit family within TC\u00ba.\nTheorem 5.4 demonstrates that, unless TC\u00b0 = NC\u00b9, kernelized Hopfield networks with poly-\nnomial precision, constant depth, and polynomial size can be implemented by a DLOGTIME-\nDLOGTIME-uniform circuit family within TC\u00ba. This implies that kernelized Hopfield networks\nare subject to inherent expressivity limitations under circuit complexity constraints despite their\nempirical success."}, {"title": "6 Hardness", "content": "In this section, we present two computational problems along with their corresponding hardness\nresults. In In Section 6.1, we outline the corresponding hardness results. Appendix D, we introduce\nthe undirected graph connectivity problem and the tree isomorphism problem."}, {"title": "6.1 Hardness Results", "content": "In this section, we present our hardness results for modern and kernelized Hopfield networks.\nTheorem 6.1. Assume that TC\u00b0 = NC\u00b9. There is no poly(n)-precision modern Hopfield net-\nworks with constant layers, and O(n) hidden dimension can solve the undirected graph connectivity\nproblem.\nProof. This result is obtained by integrating Theorem 4.6 (the circuit complexity bound of modern\nHopfield networks), Lemma D.4 (showing that undirected graph connectivity is NC\u00b9-complete),\nand Fact D.3.\nTheorem 6.2. Assume that TC\u00b0 = NC\u00b9. There is no poly(n)-precision modern Hopfield networks\nwith O(1) layers, and O(n) hidden dimension can solve the tree isomorphism problem.\nProof. This result derives from Theorem 4.6 and Lemma D.8 (showing that tree isomorphism is\nNC\u00b9-complete).\nTheorem 6.3. Assume that TC\u00b0 = NC\u00b9. There is no poly(n)-precision kernelized Hopfield net-\nworks with constant layers, and O(n) hidden dimension can solve the undirected graph connectivity\nproblem.\nProof. This result is a direct consequence of Theorem 5.4 and Lemma D.4.\nTheorem 6.4. Assume that TC\u00b0 = NC\u00b9. There is no poly(n)-precision kernelized Hopfield net-\nworks with constant layers, and O(n) hidden dimension can solve the tree isomorphism problem.\nProof. This result is a direct consequence of Theorem 5.4 and Lemma D.8."}, {"title": "7 Conclusion", "content": "In this study, we conduct a comprehensive theoretical investigation of modern Hopfield networks\nand kernelized Hopfield networks, establishing key limitations on their computational power. Our\nanalysis focuses on the circuit complexity of individual architectural components, showing that these\nnetworks can be simulated using uniform TC\u00ba circuits. Crucially, we prove that unless TC\u00b0 = NC1,\nboth modern and kernelized Hopfield networks, when implemented with polynomial precision, a\nconstant number of layers, and hidden dimensions satisfying $d \\leq O(n)$, are unable to solve problems\nsuch as undirected graph connectivity and tree isomorphism. These findings highlight the inherent\nexpressivity constraints of Hopfield models, even in light of their demonstrated empirical success\nacross various tasks.\nHowever, our analysis has limitations. It primarily addresses the forward computation of these\nnetworks, assuming constant-depth nonlinear activation functions, and does not consider the train-\ning process or the implications of more complex activation mechanisms. Expanding this framework\nto encompass other Hopfield network variants and exploring whether similar computational bounds\napply to advanced architectures would be a valuable direction for future research. Additionally, our"}, {"title": "C.1 Modern Hopfield Model Architecture with CoT", "content": "Let V be the vocabulary", "as": "n$MHM_\\theta(x_1"}, {"as": "n$MHM(x_1", "case": "n$MHM(X_1", "Overview": "The architecture is similar to GPT-like models", "main\ncomponents": 1.0}, {"as": "Theta = (\\theta_{\\text{PE"}]}, "theta_{\\text{TE}}, \\theta_{\\text{OUTPUT}}, \\theta_{\\text{MHN}})$, where $\\theta_{\\text{MHN}} = \\{\\theta_{\\text{Hop}}^{(l)}, \\theta_{\\text{FNN}}^{(l)}\\}_{l=1}^{L-1}$ are\ntrainable parameters (see Algorithm 1).\nThe token embedding layer parameterized by $\\theta_{TE} \\in R^{d \\times |V|}$ maps tokens in V to Rd: $\\theta_{\\text{TE}}(x)$, \u2200x \u2208\nV. The position encoding layer with parameters $\\theta_{PE} \\in R^{d \\times |V|}$ maps positions in [$n_{\\text{max}}$", "to Rd:\n$\\theta_{\\text{PE}}(n)$, \u2200n \u2208 [$N_{\\text{max}}$", ".", "The output layer parameterized by $\\theta_{\\text{OUTPUT}} \\in R^{|V| \\times d}$ is defined by:\n$OUTPUT_{\\theta_{\\text{OUTPUT}}} (h) = \\"]