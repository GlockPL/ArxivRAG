{"title": "On the Expressive Power of Modern Hopfield Networks", "authors": ["Xiaoyu Li", "Yuanpeng Li", "Yingyu Liang", "Zhenmei Shi", "Zhao Song"], "abstract": "Modern Hopfield networks (MHNs) have emerged as powerful tools in deep learning, capable of replacing components such as pooling layers, LSTMs, and attention mechanisms. Recent advancements have enhanced their storage capacity, retrieval speed, and error rates. However, the fundamental limits of their computational expressiveness remain unexplored. Understanding the expressive power of MHNs is crucial for optimizing their integration into deep learning architectures. In this work, we establish rigorous theoretical bounds on the computational capabilities of MHNs using circuit complexity theory. Our key contribution is that we show that MHNs are DLOGTIME-uniform TC\u00ba. Hence, unless TC\u00b0 = NC\u00b9, a poly(n)-precision modern Hopfield networks with a constant number of layers and O(n) hidden dimension cannot solve NC\u00b9-hard problems such as the undirected graph connectivity problem and the tree isomorphism problem. We also extended our results to Kernelized Hopfield Networks. These results demonstrate the limitation in the expressive power of the modern Hopfield networks. Moreover, Our theoretical analysis provides insights to guide the development of new Hopfield-based architectures.", "sections": [{"title": "1 Introduction", "content": "Hopfield networks [Hop82], initially introduced as associative memories capable of storing and retrieving patterns, have undergone significant advancements in recent years. These developments led to the emergence of modern Hopfield networks [RSL+21]. Modern Hopfield networks (MHNs) address the limitations of their predecessors, particularly in terms of storage capacity, retrieval speed, and error rates. An important feature of MHNs is their ability to function as specialized components within deep networks, integrating memory capabilities and supporting diverse functionalities. For instance, MHNs can replace conventional layers such as pooling layers, permutation-equivariant layers [GVW+16, RSP16], GRU [CvMG+14], LSTM [Hoc91, HS97], and attention mechanisms [VSP+23, BCB16].\nUnderstanding modern Hopfield networks' computational capabilities and limitations is critical for their effective application. Specifically, it is crucial to explore the computational operations these networks can implement and the complexity of the problems they can solve collectively. These investigations are vital not only for theoretical insights but also for guiding the development of more efficient and powerful models.\nWhile prior research has primarily focused on the dynamics and capacity of modern Hopfield networks [HLSL24, WHHL24], their expressiveness from a circuit complexity perspective remains underexplored. This gap raises an important question:\nWhat are the fundamental limits of modern Hopfield networks in terms of circuit complexity?\nTo address these questions, we turn to the framework of circuit complexity theory, which provides a robust method for analyzing the computational resources required to perform specific tasks. By mapping modern Hopfield networks to computational circuits, we can rigorously assess their capabilities and establish upper and lower bounds on the classes of problems they can solve.\nIn this work, we present a comprehensive theoretical investigation into the circuit complexity bounds of MHNs. Our approach involves analyzing the architecture of MHNs and the computational complexity of its components, such as the Hopfield layer. Hence, we show that uniform TC\u00ba circuits can efficiently simulate these models.\nOur main contributions can be outlined as follows:\n\u2022 We prove that any poly(n)-precision modern Hopfield networks or kernelized Hopfield networks with constant-depth and O(n) hidden dimension is in DLOGTIME-uniform TC\u00ba circuit family (Theorem 4.6 and 5.4).\n\u2022 We prove that unless TC\u00b0 = NC\u00b9, a poly(n)-precision modern Hopfield networks or kernelized Hopfield networks with constant layers, O(n) hidden dimension cannot solve NC\u00b9-hard problems such as the undirected graph connectivity problems and tree isomorphism problems (Theorems 6.1, 6.2, 6.3 and 6.4).\nRoadmap. In Section 2, we provide an overview of the related works. In Section 3, we introduce the notations and definitions needed for modern Hopfield networks. In Section 4, we analyze the circuit complexity for modern Hopfield networks. In Section 5, we focus on the circuit complexity results of kernelized Hopfield networks. In Section 6, we discuss the hardness results of the modern Hopfield networks. Finally, in Section 7, we summarize our theoretical results in the conclusion. We defer more preliminaries and technique details in Appendix."}, {"title": "2 Related Works", "content": "In this section, we introduce the related works of the Hopfield models and the complexity of Transformers. In Appendix A, we introduce more related works."}, {"title": "2.1 Theory of Modern Hopfield Networks", "content": "Modern Hopfield Models have demonstrated both empirical success and theoretical value, providing a framework for understanding transformer attention mechanisms and Transformer architectures. [HYW+23, WHHL24] propose a unified framework for analyzing and deriving modern Hopfield models using entropic regularizers. This framework includes a range of sparse variants, such as sparse and generalized sparse models, and also integrates the standard modern Hopfield model [RSL+21] as a special case. Despite these advancements, the modern Hopfield paradigm remains incomplete, lacking efficient implementations and variants, as highlighted by [HYW+23] To address these limitations, [HCW+24] propose a principled nonparametric approach for developing efficient variants of modern Hopfield models, including linear, top-K, and random feature-based versions. This study focuses on advancing research to create more efficient models. This study aims to push the boundaries of research in developing more efficient models, emphasizing their potential to shape the future of Hopfield-driven designs."}, {"title": "2.2 Complexity and Neural Network", "content": "Complexity theory offers a formal framework to analyze the computational power and limitations of models. Circuit complexity, a key domain within this field, characterizes the power of Boolean circuits and has recently been applied to investigate the expressive capacity of deep neural networks and Transformers [MSS22, LAG+22, MS23a, MS23b, LLZM24, Chi24]. Several important circuit complexity classes play a central role in machine learning. For example, AC\u00ba includes problems solvable with highly parallelizable logic gates, TC\u00ba extends this class by incorporating threshold gates, and NC\u00b9 describes languages recognized by circuits with O(logn)-depth and bounded fan-in [MSS22]. These classes exhibit the hierarchy AC\u00ba \u2282 TC\u00ba \u2286 NC\u00b9, although whether TC\u00b0 \u2260 NC\u00b9 remains an open question.\nTransformers' depth requirements have been analyzed through the lens of these complexity classes. For instance, [LAG+22] demonstrates that the depth of a Transformer must scale with input length to simulate certain non-solvable semi-automata. In a similar vein, [LLZM24] investigates the connection between constant-depth Transformers, chain-of-thought (CoT) reasoning, and circuit complexity. They show that T[poly(n), 1,1] CoT [logn, poly(n),1,1] \u2286 AC\u00ba, and T[poly(n), log n, 0] \u2286 CoT[log n, poly(n), log n,0] \u2286 TC\u00ba, Here, T[d(n), s(n), e(n)] denotes the constant-depth Transformers characterized by precision of s(n), an embedding size of d(n), and e(n) exponent bits, while CoT[T(n), d(n), s(n), e(n)] refers to a chain-of-thought (CoT) Transformer operating for T(n) steps. These findings highlight that intermediate reasoning steps in CoT models can improve computational expressivity.\nThe Strong Exponential Time Hypothesis (SETH), which posits that no k-SAT algorithm exists with runtime O(2^(1-\u20ac)n) for \u20ac > 0 and k > 3, serves as a cornerstone in fine-grained complexity analyses [IP01]. Results derived from SETH have shaped Transformer-related studies, particularly for tensor attention mechanisms [AS23a, AS24b, LSSZ24b, LSS+24b, AS23b]. For example, [AS23a] shows that by assuming SETH, it is impossible to perform forward pass computation of attention-based networks in O(n\u00b2) time, while [AS24b] extends this limitation to backward computations. These results underscore the intrinsic computational challenges of Transformer architectures."}, {"title": "3 Preliminaries", "content": "In Section 3.1, we define useful notations for defining modern Hopfield networks. In Section 3.2, we introduce some components of modern Hopfield networks. In Section 3.3, we state the basic definitions and architectures of kernelized Hopfiled networks. The background knowledge about circuit complexity and float point number is introduced in Appendix B."}, {"title": "3.1 Notations", "content": "We define N := {0,1,2,...} as the set of natural numbers and R as the set of real numbers. For any positive integer n, [n] represents the set {1, 2, ..., n}. The vector 1n denotes an n-dimensional vector where all entries are ones. Given a matrix A \u2208 Rm\u00d7n, Ai,j refers to the element in the i-th row and j-th column, while AT represents the transpose of A. The set {0,1}* represents all binary strings of finite length. Specifically, for xi \u2208 {0,1}*, xi represents a finite binary string, with each bit being either 0 or 1. A language L is characterized as a subset of {0,1}*."}, {"title": "3.2 Modern Hopfield Network", "content": "With the above notations established, we proceed to introduce the foundational concepts of Modern Hopfield Networks.\nThe input query pattern is denoted as x \u2208 Rd, while memory patterns are represented by the matrix \u039e = [\u03be1,...,\u03beM] \u2208 Rd\u00d7M. Hopfield models are associative memory models based on energy functions, where the stored memory patterns \u039e correspond to the local minima of these energy landscapes. For a given input query x, the model retrieves the most similar memory pattern by employing energy minimization algorithms, referred to as retrieval dynamics T, initialized at x. In [RSL+21], the Modern Hopfield Model is introduced with a specific energy function E and retrieval dynamics T, which are further incorporated into deep learning frameworks due to their relation to transformer attention mechanisms [VSP+23]. This approach enhances performance and provides a theoretical guarantee of exponential memory capacity. The energy function is defined as:\nE(x) = -lse(\u03b2, x) + \\frac{1}{2}(x,x),\nwhere the retrieval dynamics is given by\nxnew = TDense(x) = \u039e\u00b7 Softmax(\u03b2\u039e\u00afx).  (1)\nand the function lse(\u03b2, z) is the log-sum-exponential, defined as lse(\u03b2, z) := log(\\frac{\\Sigma_{\u03bc=1}^{M} exp{\u03b2\u03b1\u03bc}}{\u03b2}) for any z \u2208 RM and \u1e9e > 0. When applied to a sequence of input queries X = [x1,...,xL] \u2208 Rd\u00d7L, Eq. (1) becomes: Z := [xnew,...,xew] = TDense(X), and hence\nTDense (X) = Softmax(\u03b2\u039eTX) \u2208 RdXL,\nwhere Softmax() performs column-wise normalization. We assume that d = Lo(1), meaning the growth rate of d is sub-polynomial relative to L.\nModern Hopfield Networks with continuous states are compatible with deep learning models due to their differentiable nature and the ability to retrieve patterns in a single update step. This"}, {"title": "Definition 3.1 (Hopfield attention matrix, [RSL+21]).", "content": "The Hopfield attention matrix A \u2208 Fp^{n\u00d7n} is defined using model weights WQ,WK \u2208 Fp^{d\u00d7d}, query patterns R \u2208 Fp^{n\u00d7d}, and key patterns Y \u2208 Fp^{n\u00d7d}. For i, j \u2208 [n], the elements of A are given by:\nAi,j := exp(\u03b2. Ri,WQWKY)\nThe floating-point number Fp and their operations in our computational framework are formally defined in Appendix B.2. The Hopfield attention marix is used to compute a single Hopfield layer."}, {"title": "Definition 3.2 (Hopfield layer, page 6 in [RSL+21]).", "content": "A_single Hopfield layer propagates patterns using query patterns R and key patterns Y. In its most general form, the result patterns Z \u2208 Fp^{n\u00d7d} are a function of raw stored patterns Y \u2208 Fp^{n\u00d7d}, raw state patterns R \u2208 Fp^{n\u00d7d}, and projection matrices WQ, WK, Wv \u2208 Fp^{d\u00d7d}:\nZ = softmax(\u03b2. RWQWYTYWKWV   (2)\nWe set D := diag(\u03b2\u00b7 A1n) \u2208 Fp^{n\u00d7n}. Then, based on Definition 3.1, we define the i-th Hopfield layer as\nHopi(R, Y\u2081) := D^{-1}AY:Wv\nwhere we denote\nWv = WKWv.  (3)\nHere, the rank of Wy is limited by dimension constraints of the matrix product WKWv. To provide the Hopfield layer with more flexibility, the matrix product WKWy can be replaced by one parameter matrix. In this case, Wy is not the product from Eq. (3) but a stand-alone parameter matrix as in the original transformer setting.\nMultiple Hopfield layers can be integrated with other elements to build a functional Hopfield architecture."}, {"title": "Definition 3.3 (Multi-layer Modern Hopfield Networks).", "content": "Consider a model consisting of m Hopfield layers. For each i \u2208 [m], let fi denote the components of the network excluding the i-th Hopfield layer, where fi: Fp^{n\u00d7d} \u2192 Fp^{n\u00d7d}. Denote the i-th Hopfield layer as Hopi. Let the input matrix or query patterns be denoted by R \u2208 Fp^{n\u00d7d}, and the stored patterns (keys) associated with the i-th Hopfield layer by Yi \u2208 Fp^{n\u00d7d}. The multi-layer Modern Hopfield Network, denoted as MHN : Fp^{n\u00d7d} \u2192 Fp^{n\u00d7d}, is defined as:\nMHN(R) = fm Hopm(fm-10 Hopm\u22121(\uff65\uff65\uff65 f10 Hop\u2081(fo(R), Y\u2081)\uff65\uff65\uff65, Ym\u22121), Ym) \u2208 Fp^{n\u00d7d},\nwhere o denotes the composition of functions.\nWe now present one type of fi function, specifically a two-layer ReLU feedforward neural network."}, {"title": "Definition 3.4 (Two-layer ReLU Feed-forward Neural Networks).", "content": "We use X \u2208 Fp^{n\u00d7d} to denote the input matrix of size n \u00d7 d. For each i \u2208 [n], the two-layer ReLU Feed-forward Neural Networks is defined as follows:\ngFNN(X),* := W2 ReLU (W1X, +61 +62."}, {"title": "3.3 Kernelized Hopfield Networks", "content": "The capacity of modern Hopfield networks is suboptimal. To improve the capacity, [WHHL24] introduces a kernel as a learnable similarity measure, using stored memory patterns as training data to enhance memory capacity. Specifically, they propose the kernelized Hopfield network (KHM) defined by following update rule and energy function:\nTo(x) := \u039e\u00b7 Softmax(\u03b2K(\u039e,x)), Ex(x) = \\frac{1}{2}K(x,x) + lse(\u03b2, K(\u039e,x)),\nwhere the kernel K(\u00b7,\u00b7) := (\u0424(\u00b7), \u0424(\u00b7)) : Rd \u00d7 Rd \u2192 R is associated with a learnable feature map \u0424 : Rd \u2192 RD. Here, K(,) acts column-wise on matrix: K(\u039e,x) = [{K(\u03be\u03bc,x)}M_{\u03bc=1}] = [{(\u03a6(\u03be\u03bc), \u03a6(x))}M_{\u03bc=1}] \u2208 RM. Accordingly, we define the kernelized Hopfield layer."}, {"title": "Definition 3.5 (Kernelized attention matrix).", "content": "Let R \u2208 Fp^{n\u00d7d} be the set of query (state) patterns, and Y \u2208 Fp^{n\u00d7d} be the set of key (stored) patterns. Let WQ \u2208 Fp^{d\u00d7D}, WK \u2208 Fp^{d\u00d7Do}, and Wv \u2208 Fp^{d\u00d7d} be learnable projection matrices. Consider a feature \u0442\u0430\u0440 \u0424 : FP+ \u2192 FD associated with a kernel function K: FD \u00d7 FD \u2192 Fp defined by K(\u00b7,\u00b7) := (\u0424(\u00b7), \u0424(\u00b7)). Let \u03b2 > 0 be a scaling parameter. According to Definition 3.1, the kernelized Hopfield attention matrix A \u2208 Fp^{n\u00d7n} is defined by, for every i, j \u2208 [n],\nAi,j := exp(\u03b2. (\u03a6(RiWQ), \u03a6(Y;WK)))\nThe kernelized Hopfield attention matrix is the basis for computing a single kernelized Hopfield layer."}, {"title": "Definition 3.6 (Single kernelized Hopfield layer).", "content": "The result pattern Z \u2208 Fp^{n\u00d7d} are a function of raw stored patterns Y \u2208 Fp^{n\u00d7d}, raw state pattern R \u2208 Fp^{n\u00d7d}, and projection matrices WQ, WK,Wv \u2208 Fp^{d\u00d7d} (For simplicity, we denote Wy in Hopfield layer as Wv):\nZ = softmax(\u03b2\u00b7 (\u03a6(RWQ), \u03a6(YWK)))YWv\nNote that the softmax is applied row-wise, \u3008\u03a6(RWQ), \u0424(YWK))i,j = (\u03a6(RiWQ), \u03a6(YjWK)). We set D := diag(\u03b2\u00b7 A1n) \u2208 Fp^{nxn}. Then, based on Definition 3.1, we define the i-th kernelized Hopfield layer as\nKHopi(R, Y\u2081) := D\u00ae\u00b9AY;Wv\nMultiple kernelized Hopfield layers can be integrated with additional components to construct the kernelized Hopfield network."}, {"title": "Definition 3.7 (Kernelized Hopfield network).", "content": "We use m to denote the number of kernelized Hopfield layers in the network. For i \u2208 {0,1,2,...,m}, we use fi to denote the other components of i-th kernelized Hopfield layer, where fi: Fp^{n\u00d7d}\u2192Fp^{n\u00d7d}. Let KHopi denote the i-th kernelized Hopfield layer. Define R \u2208 Fp^{n\u00d7d 3\u00d7d} as the state (query) patterns or input matrix, and Y; \u2208 Fp^{n\u00d7d n\u00d7d} as the stored (key) patterns in the i-th kernelized Hopfield layer. The m-layer kernelized Hopfield network KHN: Fp^{n\u00d7d} \u2192 Fp^{n\u00d7d} is defined as:\nKHN(R) = fm KHopm(fm-10 KHopm\u22121(\uff65\uff65 f10 KHop1 (fo(X), Y\u2081)\u2026\u2026\u2026, Ym-1), Ym) \u2208 Fp^{n\u00d7d},\nwhere o denotes the composition of functions."}, {"title": "4 Complexity of Modern Hopfield Networks", "content": "This section explores key results regarding the circuit complexity of the computations involved in modern Hopfield networks. We begin with an analysis of the Hopfield attention matrix in Section 4.1. In Section 4.2, we delve into the computation of a single Hopfield layer. Section 4.3 extends the discussion to other components beyond the Hopfield layer. In Section 4.4, we examine the modern Hopfield network. Finally, Section 4.5 presents our main result: the circuit complexity bounds for modern Hopfield networks. These findings form the basis for our main theorem on the expressive power of Hopfield networks. In Appendix B.1, we provide fundamental definitions from circuit complexity theory."}, {"title": "4.1 Computing Hopfield Attention Matrix", "content": "We first recall that matrix multiplication of two matrices is in TC\u00ba."}, {"title": "Lemma 4.1 (Matrix multiplication in TC\u00ba, Lemma 4.2 in [CLL+24a]).", "content": "Let p < poly(n), n1, n2 \u2264 poly(n), and d \u2264 n. Let A \u2208 Fn1\u00d7d and B\u2208 Fd\u00d7n2. Then the matrix product AB can be implemented using a DLOGTIME-uniform threshold circuit which has depth (dstd + d\u2295) and size bounded by poly(n).\nHere, we extend the matrix operations to compute the Hopfield attention matrix."}, {"title": "Lemma 4.2 (Computation of Hopfield attention matrix in TC\u00ba).", "content": "Let precision p \u2264 poly(n). One can simulate the Hopfield attention matrix A using a DLOGTIME-uniform threshold circuit with depth 4dstd + 3d\u2295 + dexp_and_size bounded by poly(n).\nProof. To compute Ai,j, we use the following steps:\nK\n1. By Lemma 4.1, we can compute the matrix product WoW by a DLOGTIME-uniform circuit which has size poly(n) and depth dstd + d\u2295.\n2. By Lemma 4.1, we can the scalar\nSi,j = Ri,*(WQWKY\nby a DLOGTIME-uniform circuit which has size poly(n) and depth 2(dstd + d\u2295), following Lemma 4.1.\n3. The term \u03b2.si,j is computed with depth dstd using Part 1 of Lemma B.5.\n4. Using Lemma B.6, the exponential function exp(\u03b2\u00b7 si,j) is computable with depth dexp.\nSumming the circuit depths for all steps, the total depth required for Ai,j is:\ndtotal = 4dstd + 3d+ + dexp.\nSince all entries of A can be simulated in parallel, the total depth 4dstd + 3d + dexp and size poly(n), completing the proof."}, {"title": "4.2 Computing Single Hopfield Layer", "content": "This section analyzes the computation of a single Hopfield layer, including the necessary depth and size requirements."}, {"title": "Lemma 4.3 (Computation of Hopfield layer in TC\u00ba).", "content": "Let precision p \u2264 poly(n). We can simulate the Hopfield layer using a DLOGTIME-uniform threshold circuit with depth 8dstd + 6d\u2295 + dexp and size bounded by poly(n).\nProof. The computation involves multiplying matrices D\u00af\u00b9, A, Y, Wv. First, we compute D and A:\n1. D = diag(\u03b2. A1n) is computed with depth d\u2295 + dstd, as shown in Part 1 and Part 3 of Lemma B.5.\n2. From Lemma 4.2, A is computed with depth 4dstd + 3d+ + dexp.\n3. From Lemma 4.2, A is computed with depth 4dstd + 3d+ + dexp.\n4. Finally, the division D-1. (AYWv) is computed in parallel with depth dstd.\nCombining these depths:\ndtotal = 8dstd + 6d+ + dexp.\nThe total size of the circuit remains poly(n), completing the proof."}, {"title": "4.3 Computing Common Components Layers", "content": "Definition 3.3 introduces modern Hopfield networks, which incorporate the Hopfield layer along with other modules such as layer normalization and two-layer ReLU feed-forward networks. In this section, we present the computational complexity of these additional components.\nWe begin by analyzing the circuit complexity of the two-layer ReLU feed-forward networks."}, {"title": "Lemma 4.4 (Computation of Two-layer ReLU Feed-forward Networks in TC\u00ba).", "content": "Let p < poly(n), one can simulate the two-layer ReLU feed-forward neural networks using a DLOGTIME-uniform threshold circuit which has depth 4dstd + 3d\u2295 and size bounded by poly(n).\nProof. For each i \u2208 [n], Lemma 4.1 guarantees that the computation of W1. Xi, requires a DLOGTIME-uniform circuit which has depth dstd + d\u2295 and size poly(n). Applying Part 1 of Lemma B.5, we can compute W\u2081 \u00b7 Xi,* + b\u2081 with an additional DLOGTIME-uniform circuit which has depth dstd and size poly(n). The ReLU activation, ReLU(W\u2081 \u00b7 Xi,* + b\u2081), also requires depth dstd and size poly(n) as per Part 1 of Lemma B.5.\nFor the second layer, the circuit depth needed is 2dstd + d\u2295, and the size remains poly(n). Thus, the total circuit depth across both layers is 4dstd + 3d, with the size still bounded by poly(n) because these computations are able to be executed in parallel for each i \u2208 [n]."}, {"title": "4.4 Computing Modern Hopfield Networks", "content": "We demonstrate how to compute modern Hopfield networks within the TC\u00ba complexity class."}, {"title": "Lemma 4.5 (Computation of Modern Hopfield Networks in TCO).", "content": "Suppose that for every i \u2208 [m], then we can simulate function fi in MHN by a DLOGTIME-uniform threshold circuit which has size poly(n) and constant depth df. When p \u2264 poly(n), the overall computation of the network can be implemented using a DLOGTIME-uniform threshold circuit with depth (m+1)df+8mdstd+6md\u2295 +\nmdexp and size bounded by poly(n).\nProof. By assumption, we can simulate fi using a DLOGTIME-uniform threshold circuit which has size bounded by poly(n) and depth df.\nNext, by Lemma 4.3, we know that the computation of a single Hopfield layer Hop; can be performed using a DLOGTIME-uniform threshold circuit with depth 8dstd + 6d\u2295 + dexp, while its size remains polynomial in n.\nThe complete computation of MHN(R) involves 90, 91,...,gm and Hop\u2081, Hop2,..., Hopm. The total circuit depth is, therefore, the sum of the depths of all these computations. Explicitly, the depth is (m + 1)df + 8mdstd + 6md\u2295 + mdexp."}, {"title": "4.5 Main Result: Circuit Complexity Bound of Modern Hopfield Networks", "content": "Our main result establishes the circuit complexity bounds for modern Hopfield networks."}, {"title": "Theorem 4.6 (Circuit complexity of modern Hopfield networks).", "content": "Consider a modern Hopfield network \u039c\u0397\u039d where we can simulate each component function fi for i \u2208 [m] by a DLOGTIME-uniform threshold circuit which has df depth and poly(n) size. If precision p \u2264 poly(n), hidden dimension d < O(n), and number of layers m = O(1), then we can simulate MHN by a DLOGTIME-uniform circuit family in TC\u00ba.\nProof. Given m = O(1), Lemma 4.3 establishes that the circuit depth required for MHN(R) is given by:\n(m + 1)df +8mdstd + 6md+ + mdexp.\nSince m is a constant, the total circuit depth simplifies to O(1). Furthermore, the size of the circuit remains within poly(n).\nHence, the modern Hopfield network can be realized using a DLOGTIME-uniform circuit family belonging to TC\u00ba, completing the argument."}, {"title": "5 Complexity of Kernelized Hopfield Networks", "content": "In this section, we extend the complexity results of modern Hopfield networks to kernelized Hopfield networks. The formal definition of kernelized Hopfield networks is provided in Appendix 3.3.\nSection 5.1 analyzes the computation of the kernelized Hopfield attention matrix. In Section 5.2, we examine the computation of a single kernelized Hopfield layer. Section 5.3 details the computation of the entire kernelized Hopfield network. Finally, in Section 5.4, we present the main results on the circuit complexity bounds for kernelized Hopfield networks. In appendix E, we provide the complete proofs for this section."}, {"title": "5.1 Computing Kernelized Hopfield Attention Matrix", "content": "In this section, we generalize the computing of the Hopfield attention matrix to the kernelized Hopfield attention matrix."}, {"title": "Lemma 5.1 (Computation of kernelized Hopfield attention matrix in TC\u00ba).", "content": "Assuming that p < poly(n) and the linear affine feature map \u00de is defined as \u03a6(u) := Wu for u, v \u2208 Fa, where W \u2208 FDxd and the linear kernel is K(u, v) = u\u00afWWv, then the kernelized Hopfield attention matrix A can be implemented using a DLOGTIME-uniform threshold circuit of depth 3dstd + 2d\u2295 + dexp and size bounded by poly(n).\nProof. See Appendix E.1 for the complete proof."}, {"title": "5.2 Computing Single Kernelized Hopfield Layer", "content": "This section provides an analysis of the computation of a single kernelized Hopfield layer, including its circuit depth."}, {"title": "Lemma 5.2 (Computation of Single kernelized Hopfield layer in TC\u00ba).", "content": "Assuming that p \u2264 poly(n), then the kernelized Hopfield layer can be implemented using a DLOGTIME-uniform threshold circuit of depth 10dstd + 8d\u2295 + dexp_and size bounded by poly(n).\nProof. See Appendix E.2 for the complete proof."}, {"title": "5.3 Computing Kernelized Hopfield Networks", "content": "Here, we analyze the computation of the entire kernelized Hopfield network."}, {"title": "Lemma 5.3 (Kernelized Hopfield networks computation in TC\u00ba).", "content": "Assume that for each i \u2208 [m], we can simulate the function fi in KHN by a DLOGTIME-uniform threshold circuit which has constant depth df and size poly(n). If the precision p < poly(n), then we can simulate the kernelized Hopfield networks by a DLOGTIME-uniform threshold circuit of depth (m + 1)df + 10mdstd +8md\u2295 +mdexp and size bounded by poly(n).\nProof. See Appendix E.3 for the complete proof."}, {"title": "5.4 Main Result: Circuit Complexity Bound of Kernelized Hopfield Networks", "content": "We now present the main result for kernelized Hopfield networks, establishing their circuit complexity bounds."}, {"title": "Theorem 5.4 (Main result, Circuit complexity bound of kernelized Hopfield networks).", "content": "Assume that for each i \u2208 [m], the function fi in KHN is computable by a DLOGTIME-uniform threshold circuit with constant depth df and size poly(n). If precision p \u2264 poly(n), hidden dimension d \u2264 O(n), and number of layers m \u2264 O(1), then we can simulate the kernelized Hopfield networks KHM by a DLOGTIME-uniform circuit family within TC\u00ba.\nProof. See Appendix E.4 for the complete proof.\nTheorem 5.4 demonstrates that, unless TC\u00b0 = NC\u00b9, kernelized Hopfield networks with polynomial precision, constant depth, and polynomial size can be implemented by a DLOGTIME-DLOGTIME-uniform circuit family within TC\u00ba. This implies that kernelized Hopfield networks are subject to inherent expressivity limitations under circuit complexity constraints despite their empirical success."}, {"title": "6 Hardness", "content": "In this section, we present two computational problems along with their corresponding hardness results. In In Section 6.1, we outline the corresponding hardness results. Appendix D, we introduce the undirected graph connectivity problem and the tree isomorphism problem."}, {"title": "6.1 Hardness Results", "content": "In this section, we present our hardness results for modern and kernelized Hopfield networks."}, {"title": "Theorem 6.1.", "content": "Assume that TC\u00b0 = NC\u00b9. There is no poly(n)-precision modern Hopfield networks with constant layers, and O(n) hidden dimension can solve the undirected graph connectivity problem.\nProof. This result is obtained by integrating Theorem 4.6 (the circuit complexity bound of modern Hopfield networks), Lemma D.4 (showing that undirected graph connectivity is NC\u00b9-complete), and Fact D.3."}, {"title": "Theorem 6.2.", "content": "Assume that TC\u00b0 = NC\u00b9. There is no poly(n)-precision modern Hopfield networks with O(1) layers, and O(n) hidden dimension can solve the tree isomorphism problem.\nProof. This result derives from Theorem 4.6 and Lemma D.8 (showing that tree isomorphism is NC\u00b9-complete)."}, {"title": "Theorem 6.3.", "content": "Assume that TC\u00b0 = NC\u00b9. There is no poly(n)-precision kernelized Hopfield networks with constant layers, and O(n) hidden dimension can solve the undirected graph connectivity problem.\nProof. This result is a direct consequence of Theorem 5.4 and Lemma D.4."}, {"title": "Theorem 6.4.", "content": "Assume that TC\u00b0 = NC\u00b9. There is no poly(n)-precision kernelized Hopfield networks with constant layers, and O(n) hidden dimension can solve the tree isomorphism problem.\nProof. This result is a direct consequence of Theorem 5.4 and Lemma D.8."}, {"title": "7 Conclusion", "content": "In this study, we conduct a comprehensive theoretical investigation of modern Hopfield networks and kernelized Hopfield networks, establishing key limitations on their computational power. Our analysis focuses on the circuit complexity of individual architectural components, showing that these networks can be simulated using uniform TC\u00ba circuits. Crucially, we prove that unless TC\u00b0 = NC1, both modern and kernelized Hopfield networks, when implemented with polynomial precision, a constant number of layers, and hidden dimensions satisfying d \u2264 O(n), are unable to solve problems such as undirected graph connectivity and tree isomorphism. These findings highlight the inherent expressivity constraints of Hopfield models, even in light of their demonstrated empirical success across various tasks.\nHowever, our analysis has limitations. It primarily addresses the forward computation of these networks, assuming constant-depth nonlinear activation functions, and does not consider the training process or the implications of more complex activation mechanisms. Expanding this framework to encompass other Hopfield network variants and exploring whether similar computational bounds apply to advanced architectures would be a valuable direction for future research. Additionally, our"}, {"title": "A More Related Works", "content": "Modern Hopfield Networks and Applications in Deep Learning. Classical Hopfield networks [Hop82, Hop84, KH16", "KH16": "these networks now achieve exponential storage capabilities [DHL+17"}]}