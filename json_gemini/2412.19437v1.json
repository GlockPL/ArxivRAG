{"title": "DeepSeek-V3 Technical Report", "authors": ["DeepSeek-AI"], "abstract": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.", "sections": [{"title": "1. Introduction", "content": "In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Beyond closed-source models, open-source models, including DeepSeek series (DeepSeek-AI, 2024a,b,c; Guo et al., 2024), LLaMA series (AI@Meta, 2024a,b; Touvron et al., 2023a,b), Qwen series (Qwen, 2023, 2024a,b), and Mistral series (Jiang et al., 2023; Mistral, 2024), are also making significant strides, endeavoring to close the gap with their closed-source counterparts. To further push the boundaries of open-source model capabilities, we scale up our models and introduce DeepSeek-V3, a large Mixture-of-Experts (MoE) model with 671B parameters, of which 37B are activated for each token.\nWith a forward-looking perspective, we consistently strive for strong model performance and economical costs. Therefore, in terms of architecture, DeepSeek-V3 still adopts Multi-head Latent Attention (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024) for cost-effective training. These two architectures have been validated in DeepSeek-V2 (DeepSeek-AI, 2024c), demonstrating their capability to maintain robust model performance while achieving efficient training and inference. Beyond the basic architecture, we implement two additional strategies to further enhance the model capabilities. Firstly, DeepSeek-V3 pioneers an auxiliary-loss-free strategy (Wang et al., 2024a) for load balancing, with the aim of minimizing the adverse impact on model performance that arises from the effort to encourage load balancing. Secondly, DeepSeek-V3 employs a multi-token prediction training objective, which we have observed to enhance the overall performance on evaluation benchmarks.\nIn order to achieve efficient training, we support the FP8 mixed precision training and implement comprehensive optimizations for the training framework. Low-precision training has emerged as a promising solution for efficient training (Dettmers et al., 2022; Kalamkar et al., 2019; Narang et al., 2017; Peng et al., 2023b), its evolution being closely tied to advancements in hardware capabilities (Luo et al., 2024; Micikevicius et al., 2022; Rouhani et al., 2023a). In this work, we introduce an FP8 mixed precision training framework and, for the first time, validate its effectiveness on an extremely large-scale model. Through the support for FP8 computation and storage, we achieve both accelerated training and reduced GPU memory usage. As for the training framework, we design the DualPipe algorithm for efficient pipeline parallelism, which has fewer pipeline bubbles and hides most of the communication during training through computation-communication overlap. This overlap ensures that, as the model further scales up, as long as we maintain a constant computation-to-communication ratio, we can still employ fine-grained experts across nodes while achieving a near-zero all-to-all communication overhead. In addition, we also develop efficient cross-node all-to-all communication kernels to fully utilize InfiniBand (IB) and NVLink bandwidths. Furthermore, we meticulously optimize the memory footprint, making it possible to train DeepSeek-V3 without using costly tensor parallelism. Combining these efforts, we achieve high training efficiency.\nDuring pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens. The pre-training process is remarkably stable. Throughout the entire training process, we did not encounter any irrecoverable loss spikes or have to roll back. Next, we conduct a two-stage context length extension for DeepSeek-V3. In the first stage, the maximum context length is extended to 32K, and in the second stage, it is further extended to 128K. Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential. During the post-training stage, we distill the reasoning capability from the DeepSeek-R1 series of models, and meanwhile carefully maintain the balance between model accuracy"}, {"title": "2. Architecture", "content": "We first introduce the basic architecture of DeepSeek-V3, featured by Multi-head Latent Attention (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024) for economical training. Then, we present a Multi-Token Prediction (MTP) training objective, which we have observed to enhance the overall performance on evaluation benchmarks. For other minor details not explicitly mentioned, DeepSeek-V3 adheres to the settings of DeepSeek-V2 (DeepSeek-AI, 2024c).\n2.1. Basic Architecture\nThe basic architecture of DeepSeek-V3 is still within the Transformer (Vaswani et al., 2017) framework. For efficient inference and economical training, DeepSeek-V3 also adopts MLA and DeepSeekMoE, which have been thoroughly validated by DeepSeek-V2. Compared with DeepSeek-V2, an exception is that we additionally introduce an auxiliary-loss-free load balancing strategy (Wang et al., 2024a) for DeepSeekMoE to mitigate the performance degradation induced by the effort to ensure load balance. Figure 2 illustrates the basic architecture of DeepSeek-V3, and we will briefly review the details of MLA and DeepSeekMoE in this section.\n2.1.1. Multi-Head Latent Attention\nFor attention, DeepSeek-V3 adopts the MLA architecture. Let $d$ denote the embedding dimension, $n_h$ denote the number of attention heads, $d_h$ denote the dimension per head, and $h_t \\in \\mathbb{R}^{d}$ denote the attention input for the t-th token at a given attention layer. The core of MLA is the low-rank joint compression for attention keys and values to reduce Key-Value (KV) cache during inference:\n\n$\\text{CKV} = W_{D_{KV}}h_t,$\n$[k_{t,1};...;k_{t,n_h}] = k_t = W_{U_K}\\text{CKV},$\n$k^{\\text{R}}_{t} = ROPE(W_{KR}h_t),$\n$k_{t,i} = [k_{t,i};k^{\\text{R}}_{t}],$\n$[v_{t,1};...;v_{t,n_h}] = v_t = W_{U_V}\\text{CKV}.$\nwhere $c^v_t \\in \\mathbb{R}^{d_c}$ is the compressed latent vector for keys and values; $d_c(< d_h n_h)$ indicates the KV compression dimension; $W_{D_{KV}} \\in \\mathbb{R}^{d_c\\times d}$ denotes the down-projection matrix; $W_{U_K}, W_{U_V} \\in \\mathbb{R}^{d_hn_h\\times d_c}$ are the up-projection matrices for keys and values, respectively; $W_{KR} \\in \\mathbb{R}^{d\\times d}$ is the matrix used to produce the decoupled key that carries Rotary Positional Embedding (RoPE) (Su et al., 2024); ROPE(\u00b7) denotes the operation that applies RoPE matrices; and [\u00b7; \u00b7] denotes concatenation. Note that for MLA, only the blue-boxed vectors (i.e., $\\text{CKV}$ and $k^{\\text{R}}_{t}$) need to be cached during generation, which results in significantly reduced KV cache while maintaining performance comparable to standard Multi-Head Attention (MHA) (Vaswani et al., 2017).\nFor the attention queries, we also perform a low-rank compression, which can reduce the activation memory during training:\n$c^q_t = W_{D_Q}h_t,$\n$[q_{t,1};...;q_{t,n_h}] = q_t = W_{U_Q}c^q_t,$\n$[q^{\\text{R}}_{t,1};...;q^{\\text{R}}_{t,n_h}] = q^{\\text{R}}_{t} = ROPE(W_{QR}c^q_t),$\n$q_{t,i} = [q_{t,i};q^{\\text{R}}_{t}].$\nwhere $c^q_t \\in \\mathbb{R}^{d'_c}$ is the compressed latent vector for queries; $d'_c(< d_h n_h)$ denotes the query compression dimension; $W_{D_Q} \\in \\mathbb{R}^{d'_c\\times d}, W_{U_Q} \\in \\mathbb{R}^{d_hn_h\\times d'_c}$ are the down-projection and up-projection matrices for queries, respectively; and $W_{QR} \\in \\mathbb{R}^{d_hn_h\\times d'_c}$ is the matrix to produce the decoupled queries that carry RoPE.\nUltimately, the attention queries ($q_{t,i}$), keys ($k_{j,i}$), and values ($v_{j,i}$) are combined to yield the final attention output $u_t$:\n$\\alpha_{t,i} = \\sum_{j=1}^T Softmax(\\frac{q^T_{t,i}k_{j,i}}{\\sqrt{d_h+d^{\\text{R}}}} )v_{j,i},$\n$u_t = W^O [o_{t,1}; o_{t,2}; ...; o_{t,n_h}],$\nwhere $W^O \\in \\mathbb{R}^{d\\times d_hn_h}$ denotes the output projection matrix.\n2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing\nBasic Architecture of DeepSeekMoE. For Feed-Forward Networks (FFNs), DeepSeek-V3 employs the DeepSeekMoE architecture (Dai et al., 2024). Compared with traditional MoE architectures like GShard (Lepikhin et al., 2021), DeepSeekMoE uses finer-grained experts and isolates some experts as shared ones. Let $u_t$ denote the FFN input of the t-th token, we compute the FFN output $h_t$ as follows:\n$h_t = u_t + \\text{FFN}^{(s)}(u_t) + \\sum_{i=1}^{N_s} g_{i,t} \\text{FFN}^{(r)}(u_t),$\n$g_{i,t} = \\frac{s_{i,t}}{\\sum_{i=1}^{N_s} s_{i,t}},$\n$s_{i,t} =\\begin{cases}\ns_{i,t}+b_i \\in \\text{Topk}(\\{s_{j,t}+b_j| 1 \\leq j \\leq N_r\\}, K_r),\n0, \\text{ otherwise},\n\\end{cases}$\n$s_{i,t} = \\text{Sigmoid} (u_t^Te_i),$\nwhere $N_s$ and $N_r$ denote the numbers of shared experts and routed experts, respectively; $FFN^{(s)}_i (\\cdot)$ and $FFN^{(r)}_i (\\cdot)$ denote the i-th shared expert and the i-th routed expert, respectively; $K_r$ denotes the number of activated routed experts; $g_{i,t}$ is the gating value for the i-th expert; $s_{i,t}$ is the token-to-expert affinity; $e_i$ is the centroid vector of the i-th routed expert; and Topk(\u00b7, K) denotes the set comprising K highest scores among the affinity scores calculated for the t-th token and all routed experts. Slightly different from DeepSeek-V2, DeepSeek-V3 uses the sigmoid function to compute the affinity scores, and applies a normalization among all selected affinity scores to produce the gating values.\nAuxiliary-Loss-Free Load Balancing. For MoE models, an unbalanced expert load will lead to routing collapse (Shazeer et al., 2017) and diminish computational efficiency in scenarios with expert parallelism. Conventional solutions usually rely on the auxiliary loss (Fedus et al., 2021; Lepikhin et al., 2021) to avoid unbalanced load. However, too large an auxiliary loss will impair the model performance (Wang et al., 2024a). To achieve a better trade-off between load balance and model performance, we pioneer an auxiliary-loss-free load balancing strategy (Wang et al., 2024a) to ensure load balance. To be specific, we introduce a bias term $b_i$ for each expert and add it to the corresponding affinity scores $s_{i,t}$ to determine the top-K routing:\n$s'_{i,t} =\\begin{cases}\ns_{i,t}, s_{i,t} + b_i \\in \\text{Topk}(\\{s_{j,t} + b_j| 1 \\leq j \\leq N_r\\}, K_r),\n0, \\text{ otherwise}.\n\\end{cases}$\nNote that the bias term is only used for routing. The gating value, which will be multiplied with the FFN output, is still derived from the original affinity score $s_{i,t}$. During training, we keep monitoring the expert load on the whole batch of each training step. At the end of each step, we will decrease the bias term by \u03b3 if its corresponding expert is overloaded, and increase it by \u03b3 if its corresponding expert is underloaded, where \u03b3 is a hyper-parameter called bias update speed. Through the dynamic adjustment, DeepSeek-V3 keeps balanced expert load during training, and achieves better performance than models that encourage load balance through pure auxiliary losses.\nComplementary Sequence-Wise Auxiliary Loss. Although DeepSeek-V3 mainly relies on the auxiliary-loss-free strategy for load balance, to prevent extreme imbalance within any single sequence, we also employ a complementary sequence-wise balance loss:\n$L_{Bal} = \\alpha \\sum_{i=1}^{N_r} f_i p_i,$\n$f_i = \\frac{\\sum_{t=1}^T \\mathbb{1}(s_{i,t} \\in \\text{Topk}(\\{s_{j,t}| 1 \\leq j \\leq N_r\\}, K_r))}{K_r T},$\n$s_{i,t} = \\frac{s_{i,t}}{\\sum_j s_{j,t}},$\n$p_i = \\frac{1}{T}\\sum_{t=1}^T s_{i,t}.$\nwhere the balance factor \u03b1 is a hyper-parameter, which will be assigned an extremely small value for DeepSeek-V3; $mathbb{1}(\u00b7)$ denotes the indicator function; and $T$ denotes the number of tokens in a sequence. The sequence-wise balance loss encourages the expert load on each sequence to be balanced.\n2.2. Multi-Token Prediction\nInspired by Gloeckle et al. (2024), we investigate and set a Multi-Token Prediction (MTP) objective for DeepSeek-V3, which extends the prediction scope to multiple future tokens at each position. On the one hand, an MTP objective densifies the training signals and may improve data efficiency. On the other hand, MTP may enable the model to pre-plan its representations for better prediction of future tokens. Figure 3 illustrates our implementation of MTP. Different from Gloeckle et al. (2024), which parallelly predicts D additional tokens using independent output heads, we sequentially predict additional tokens and keep the complete causal chain at each prediction depth. We introduce the details of our MTP implementation in this section.\nMTP Modules. To be specific, our MTP implementation uses D sequential modules to predict D additional tokens. The k-th MTP module consists of a shared embedding layer Emb(\u00b7), a shared output head OutHead(\u00b7), a Transformer block $\\text{TRM}_k(\u00b7)$, and a projection matrix $M_k \\in \\mathbb{R}^{d \\times 2d}$. For the i-th input token $t_i$, at the k-th prediction depth, we first combine the representation of the i-th token at the (k \u2013 1)-th depth $h^{k-1}_i \\in \\mathbb{R}^d$ and the embedding of the (i + k)-th token $\\text{Emb}(t_{i+k}) \\in \\mathbb{R}^d$ with the linear projection:\n$h_i^k = M_k [\\text{RMSNorm}(h^{k-1}_i); \\text{RMSNorm}(\\text{Emb}(t_{i+k}))],$\nwhere [\u00b7; \u00b7] denotes concatenation. Especially, when k = 1, $h^{k-1}_i$ refers to the representation given by the main model. Note that for each MTP module, its embedding layer is shared with the main model. The combined $h^k_i$ serves as the input of the Transformer block at the k-th depth to produce the output representation at the current depth $h_{i:T-k}^k$:\n$h_{i:T-k}^k = \\text{TRM}_k(h_i^k),$\nwhere T represents the input sequence length and i:j denotes the slicing operation (inclusive of both the left and right boundaries). Finally, taking $h_{i:T-k}^k$ as the input, the shared output head will compute the probability distribution for the k-th additional prediction token $P_{i+k}^{k+1} \\in \\mathbb{R}^V$, where V is the vocabulary size:\n$P_{i+k}^{k+1} = \\text{OutHead}(h^k).$\nThe output head OutHead(\u00b7) linearly maps the representation to logits and subsequently applies the Softmax(\u00b7) function to compute the prediction probabilities of the k-th additional token. Also, for each MTP module, its output head is shared with the main model. Our principle of maintaining the causal chain of predictions is similar to that of EAGLE (Li et al., 2024b), but its primary objective is speculative decoding (Leviathan et al., 2023; Xia et al., 2023), whereas we utilize MTP to improve training.\nMTP Training Objective. For each prediction depth, we compute a cross-entropy loss $L_{MTP}$: \n$L_{MTP} = \\text{CrossEntropy}(P_{i+k:T+1}^{k+1}, t_{i+k:T+1}) = \\frac{1}{T} \\sum_{i=2+k}^{T+1} -\\text{log} P_{i+k}[t_i],$\nwhere $T$ denotes the input sequence length, $t_i$ denotes the ground-truth token at the i-th position, and $P_{i+k}[t_i]$ denotes the corresponding prediction probability of $t_i$, given by the k-th MTP module. Finally, we compute the average of the MTP losses across all depths and multiply it by a weighting factor \u03bb to obtain the overall MTP loss $L_{MTP}$, which serves as an additional training objective for DeepSeek-V3:\n$L_{MTP} = \\frac{\\lambda}{D} \\sum_{k=1}^D L^{MTP}_k.$\nMTP in Inference. Our MTP strategy mainly aims to improve the performance of the main model, so during inference, we can directly discard the MTP modules and the main model can function independently and normally. Additionally, we can also repurpose these MTP modules for speculative decoding to further improve the generation latency."}, {"title": "3. Infrastructures", "content": "3.1. Compute Clusters\nDeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs. Each node in the H800 cluster contains 8 GPUs connected by NVLink and NVSwitch within nodes. Across different nodes", "1": 1.0, "components": "attention", "strategy": "tile-wise grouping with 1 \u00d7 Nc elements or block-wise grouping with N\u0109 \u00d7 N\u0109 elements. The associated dequantization overhead is largely mitigated under our increased-precision accumulation process"}, {"components": "the embedding module", "concurrently": "while one warpgroup performs the promotion operation, the other is able to execute the MMA operation. This design enables overlapping of the two operations, maintaining high utilization of Tensor Cores. Based on our experiments, setting Nc = 128 elements, equivalent to 4 WGMMAs, represents the minimal accumulation interval that can significantly improve precision without introducing"}]}