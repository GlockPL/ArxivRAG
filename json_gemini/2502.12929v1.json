{"title": "Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options", "authors": ["Lakshmi Nair", "Ian Trase", "Mark Kim"], "abstract": "We present a novel reasoning approach called Flow-of-Options (FoO), designed to address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs to systematically explore a diverse range of possibilities in their reasoning, as demonstrated by an FoO-based agentic system for autonomously solving Machine Learning tasks (AutoML). Our framework outperforms state-of-the-art baselines, achieving improvements of 38.2% \u2013 69.2% on standard data science tasks, and 37.4% - 47.9% on therapeutic chemistry tasks. With an overall operation cost under $1 per task, our framework is well-suited for cost-sensitive applications. Beyond classification and regression, we illustrate the broader applicability of our FoO-based agentic system to tasks such as reinforcement learning and image generation. Our framework presents significant advancements compared to current state-of-the-art agentic systems for AutoML, due to the benefits of FoO in enforcing diversity in LLM solutions through compressed, explainable representations that also support long-term memory when combined with case-based reasoning.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have impacted automation on a wide spectrum of tasks, motivating a growing body of work in agentic system design, e.g., web-browsing [1], idea generation [2], and data science [3, 4]. Other approaches have focused on improving the inherent reasoning capabilities of the LLMs, through Tree-of-Thoughts (ToT) [5] or Chain-of-Thoughts (CoT) [6]. This paper seeks to bridge the two areas exploring how \u201cthought-based\" approaches can be extended to improve agentic systems. Hence, we propose Flow-of-Options (FoO)\u00b9. Given a task, FoO enables LLMs to \u201cthink through\u201d the options available for executing each step in the task, prior to the actual execution. Broadly, FoO is a network data structure that explicitly enumerates options for each step in the task, as nodes in the network. Thus, FoO forces the LLM to be aware of, and to explore, a broader spectrum of possibilities for completing the task. We demonstrate the practical value of FoO by incorporating it within an agentic framework for automating Machine Learning (ML) tasks (Figure 1a).\nRecent studies have shown that existing agentic systems, such as AutoGPT [8], LangChain [9], and ResearchAgent [10], often struggle with ML tasks [11]. Alternatives that involve fine-tuning LLMs [12, 13], although promising, involve significant computational effort and would not generalize to broader tasks the same way that a reasoning strategy like CoT or ToT can.\nRecent models from OpenAI have focused on improving outputs by increasing test-time compute, fine-tuning the models to \"think deeper\" through CoT-style reasoning [14]. However, LLMs show strong biases towards options favored in their pre-training data when proposing solutions to tasks,"}, {"title": "Flow-of-Options", "content": "Construction of FoO: Given a task T consisting of execution steps T = {$1, $2, ..., sn}, we sample k options for each step i using an LLM O ~ po(T, Si, of1:1-1)). We prompt the LLM to emphasize diversity in its generations. By conditioning the LLM outputs on the task, the current step of the task, and the previously generated options, we ensure that the LLM outputs are consistent with the task and the options generated in previous steps of the plan. Given the LLM outputs, Flow-of-Options can be represented as a network F = (V, E, r) (a directed-acyclic graph) of depth n. A node in this network represents an option. An edge connects two option nodes, with associated values r (initialized to a small fixed value at the beginning). To formulate the network, we first instantiate a dummy root node. Then, beginning at the root, the network is constructed with the options corresponding to each step i of the plan, placed at the corresponding depth i. Note that LLMs are only used to generate options, and not to construct the FoO structure. Let V(i) = {0, 0), ..., o()}, denote nodes at depth i. The\nedges are directed as follows:\nE = {(u, v) | u \u2208 V(i\u22121), v \u2208 V(i)} \u2200i\u2208 [1,n]\nWhere, u denotes options at depth i \u2212 1, v denotes options at depth i, and (u, v) represents a directed edge from u to v. Visually, our FoO network can be likened to a fully connected neural network, where every option at depth i \u2013 1 is connected to every option at depth i. Importantly, there are no connections between options within the same depth, i.e., (u \u2208 V(i), v \u2208 V(i)) \u2209 E. We update the values of the edges by traversing the FoO network using \"walks\u201d as follows: Given a network F, a walk W represents a sequence of options, one from each depth:\nW =(o(1), o(2), ..., o(n)) where o(i) ~ V(i)\nOur current implementation naively samples options, and some walks may be repeatedly generated from one iteration to another. In our future work, we will explore alternative informed sampling approaches. Given, a walk W (|W| = n), we evaluate the sequence of options for the task to obtain an output value or metric R = f(W). We propagate the value to the edges connecting the options using a max update:\nr(ofi), ofi+1)) = max(r(of), of(i+1)), R) \u2200 \u2208 W\nWhere r(u, v) represents the value associated with the edge (u, v). Hence, each option in W will have their edge value to the subsequent option updated by the max operator. An illustrative example is shown in Figure 2a.\nBeam Traversal: Once the network is constructed and initial values are updated for the edges in F, it would be beneficial to explore alternative combinations of the highest-valued options when generating walks. Prior work has demonstrated the value of beam search in the context of improving the output generations of language models [17, 18], by maintaining several hypotheses, eventually choosing the one with the highest probability. We follow a similar approach, and generate walks, by sampling options from the highest valued b options at every step of the task plan. We refer to b as the beam width. For identifying the top-b options, we compute the highest value associated with each option v \u2208 V(i). This is the maximum value of all the edges coming into v from options in V(i-1):\nvalue(v) = max(r(u, v)) \u2200 u \u2208 V(i\u22121)\nThen, the top-b options in V(i) (Top\u2081(V(i))), are the b options in V(i) that have the highest value. We sample from the top-b options for generating walks.\nWbeam = (01), ..., o(n)) where of(i) ~ Topb(V(i))"}, {"title": "Benefits of Flow-of-Options", "content": "Flow-of-Options is a special case of a directed acyclic graph (DAG) that is similar in structure to a fully connected neural network. Why is this formulation useful, as opposed to a typical tree or directed acyclic graph? We discuss this in the context of two existing methods that are closely related: SELA [19] (using trees) and Data Interpreter [20] (using DAGs). In addition to distinguishing our work from existing methods, these differences also highlight key challenges of building agentic systems for ML [21], that we seek to alleviate with Flow-of-Options.\nSELA [19] decomposes tasks with pre-specified steps into a tree, combined with Monte-Carlo Tree Search (MCTS) to find optimal paths. FoO offers two improvements over this work: First, by definition, a child node in a tree can only have one parent node. This eliminates the possibility of exploring different combinations, e.g., of features and models. Converting Figure 2b to a tree, can cause combinations of nodes, e.g., N2 \u2192 N3, to never be explored if only node N1 is a parent of node N3; Second, we argue that MCTS may be an overkill for AutoML-style problems, since it requires a significant number of rollouts for convergence, that are computationally intensive (requiring code execution). Although SELA proposes a modified upper confidence bound for trees (UCT) to mitigate this impact, SELA takes the longest among the baselines in our experiments. We believe Auto-ML can be treated differently: A \u201cpoor\u201d rollout of a feature F\u2081 with ML model M\u2081 does not have to impact F\u2081's value (as with UCT), if a better combination of F\u2081 with M2 exists. If we discover once, that F\u2081 and M2 performs well, we can stick to that path with a max update, regardless of whether F\u2081 is (on average) a \"good state\u201d.\nAlleviating some of the shortcomings of SELA, Data Interpreter (DI) [20] utilizes an LLM to produce a directed acyclic graph (DAG) decomposing a task into sub-tasks. Data Interpreter then seeks the most optimal graph based on an output performance measure. FoO improves over this approach in three ways: First, the LLM generated graph in Data Interpreter is not guaranteed to be acyclic"}, {"title": "Agentic Framework Using FoO", "content": "An overview of our proposed agentic framework with Flow-of-Options and Case-based Reasoning (CBR) is shown in Figure 3. We first discuss CBR using FoO before discussing the framework."}, {"title": "Case-Based Reasoning with FoO", "content": "Prior work has demonstrated the benefits of Case-Based Reasoning (CBR) in enhancing the problem-solving capabilities of LLMs, while also improving efficiency in terms of computational resources [3]. Motivated by these benefits, we further incorporate CBR with Flow-of-Options, by retrieving and reusing previously generated FoO networks. CBR involves a case bank C consisting of individual cases c. Specifically, we denote a case c = (T, F, R*), where T denotes the task, F denotes the FoO network generated for the task, and R* denotes the best reward achieved with the network for this task. Unlike the long-form descriptions of task solutions in DS-Agent [3], F represents compressed and relevant information on the case.\nFor the same task, if a new F with a higher reward is discovered, then the case is updated with the new F and the corresponding reward. For a new task T', we retrieve the closest case c that maximizes the cosine similarity of its corresponding task with T', i.e., c = argmax sim(E(Tc), E(T')). Here, E() denotes a pretrained embedding model. For the new task, we reuse the original task plan Te, and the corresponding F. This ensures consistency between the retrieved flow-of-options and the task plan, with the implicit assumption that a similar task plan can be applied to similar tasks. We threshold the retrieval based on the similarity score, to ensure the validity of this assumption. Cases with similarity scores below a threshold will not be reused, and instead a completely new FoO will be developed for the task."}, {"title": "Framework", "content": "Inspired by prior work [3], we incorporate two phases in our framework: Development, and Deploy-ment. Our framework begins with a user input as a prompt, followed by development or deployment.\nDevelopment: Development takes one of two paths depending on whether the case bank is empty or has data. If the case bank is empty, a task Planner (an LLM), generates a sequence of steps for completing the task. Given the task plan, the Option Generator generates options for each step of the plan. The generated options are then converted into a FoO network F as described in Section 2. Once the network is generated, we traverse through the network for T iterations, where each iteration uses a fixed beam width that is reduced at later iterations to encourage exploration over the high-valued states (Appendix Figure 10 demonstrates the value of reducing beam width over subsequent iterations). Each iteration performs a set of j walks in a batch. At the end of j walks, the Plan Executor converts each walk into code, reflectively debugs any errors, and executes the code to extract the final metric Rj = f(W;). The metric is propagated to the nodes in the walk W; to update F. Once T iterations are complete, F is added into the case bank.\nIf the case bank is non-empty, the closest case to the user input is retrieved from the case bank. If the similarity (as described in Section 3.1) is below a pre-specified threshold, the framework reverts to the empty case bank workflow. If the similarity exceeds the threshold, the framework reuses the corresponding F and task plan for the current problem. First, an Adapter agent, adapts the F and corresponding task plan to the new problem, e.g., modifying regression \u2192 classification, but reusing the same class of model (GradientBoostingRegressor \u2192 GradientBoostingClassifier)"}, {"title": "Experiments and Results", "content": "Baseline performance comparison on classification and regression: We evaluate our framework on 16 tasks obtained from [3]. Our baselines include DS-Agent [3], AutoGluon [22], SELA [19], Data Interpreter (DI) [20], Autogen [23], and zero-shot with Chain-of-Thought (CoT) [6]. We also evaluate on 17 ADME-Tox tasks using Therapeutic Data Commons (TDC) [24]. We exclude DS-Agent from TDC tasks, since DS-Agent requires a repository of human insights (similar to Kaggle) which is currently unavailable for TDC. Without these insights DS-Agent performs poorly [3]. Similarly, we exclude AutoGluon from TDC tasks (requiring use of packages like RDKit), and language model tasks, since AutoGluon cannot handle them. We also exclude SELA from forecasting and TDC tasks and DI from TDC tasks, as they are not flexibly supported in their MetaGPT implementations [25]. For TDC, we compare against DeepMol [26], an AutoML approach specialized for ADME-Tox.\nScaling to computationally intensive scenarios: We evaluate our scalability via drug-drug combina-tion, drug-target interaction prediction (TDC), and chemical bond prediction [27] with \u2248 100\u00d7 data than the previous tasks.\nGeneralizing beyond classification and regression: We evaluate our work on: a) reinforcement learning (RL) task of cartpole balancing, and b) image generation using MNIST. In the context of these tasks, we also investigate the capabilities of the Adapter LLM to adapt prior FoOs on the same task to atypical instantiations of the tasks.\nSimilar to DS-Agent, we retain a separate Dtrain, with testing on Dtest. In all cases, we use GPT-40 [28] as the foundational LLM. For Autogen, we follow the official documentation to construct a system consisting of two LLM agents: one that produces a task plan and code, and another that critiques the output to suggest improvements (\u201creflection"}, {"title": "General Data Science (DS) Tasks", "content": "We show results for 16 data science tasks in Table 1 (arrows show whether a lower (\u2193) or higher (\u2191) metric is preferred). For DS-Agent and our framework, we develop on 7/16 tasks, and deploy on the rest. We see that ML approaches designed by our framework outperforms the baselines with an average rank of 1.44 (best possible rank is 1.0, and worst is 7.0), a 38.2% to 69.2% improvement in ranks compared to baselines. All approaches, except DI, succeeded in 100% of the cases. Our approach produces high-performing solutions in 15/16 tasks (except AR). We see the accuracy benefits of FoO even in the absence of CBR, in the development phase.\nWe note that Data Interpreter (DI), while competitive, exhibits two key disadvantages compared to our work: a) DI failed to produce code in 1/3 runs for 5/16 tasks; b) DI uses highly specific, hand-crafted prompts for the tasks (Appendix K) compared to the general guidance provided to our FoO-based system (Appendix L). The specificity of the prompts may, at least in part, contribute to the competitiveness of DI. Despite the more general prompts, FoO enables our approach to outperform the baselines. It is also worth noting that in spite of explicitly specifying several models for tabular tasks in the prompt, DI almost always used XGB or RandomForest only (See Appendix Figure 7) indicating the potential value of enumerating options in FoO form.\nWe demonstrate the capacity of our approach to improve on a task, by repeating development on the same task (as described in Section 3.2). Here, we run development repeatedly on the same task with T = 1 for five separate runs, saving the FoO at each run into the case bank. At each run, the past FoO is retrieved and new options are explored (we prune two options). We also run the remaining approaches for five independent runs. At each iteration t, DS-Agent reflects on past code from iteration t 1. In Figure 4, Autogen, SELA, DI, and zero-shot performances fluctuate (since they do not reuse past experiences, leading to randomness). While DS-Agent generally improves in performance, there are cases where the agent's \"reflection\" results in worse outcomes (e.g., in iteration 2). Our framework reuses the past Flow-of-Options network, which includes the best solution"}, {"title": "TDC Tasks \u2013 ADME-Tox", "content": "Results are shown in Table 3. Our framework outperforms the baselines, achieving an average rank of 1.47 (37.4% to 47.9% improvement over baselines), consistently producing high-performing solutions (15/17 tasks except HO and hE)."}, {"title": "Computationally Intense Tasks", "content": "Our FoO-based agentic framework involves the construction and traversal of FoO networks. In the event that training data and models are large, it would be beneficial to scale our approach to more computationally intensive scenarios. We present one potential strategy that uses coreset selection [30] to overcome this problem. Coreset selection involves selecting a subset of the data that reasonably reflects performance on the full training data. We demonstrate the performance of this strategy on tasks with about 100\u00d7 more data than the previous sections. We use a simple data selection strategy that applies stratified sampling on binned label values, although more sophisticated approaches could be applied here [30]. We apply development with our framework on the reduced subset of data (\u2248 1/50th of the original dataset size), that took \u2248 15.8 mins. Once the best performing code is generated, we apply it on the complete training set and report performances on the test set. Our results are shown in Table 4. ML approaches designed by our framework performs well across the three tasks, outperforming the baselines. Additionally, our approach explores a wider range of possibilities, as shown by the word cloud in Figure 5, highlighting a key benefit of FoO. ML approaches designed by our framework achieves 80% - 90% of the performance of human-designed approaches in the 12/19 TDC tasks (Appendix Figure 11)."}, {"title": "Beyond Classification and Regression", "content": "We further demonstrate our approach on tasks beyond classification and regression. Our baselines are Autogen and Zero-shot with CoT owing to their flexibility in handling a wider range of tasks.\nReinforcement Learning (Cartpole Balancing): We show results of our approach on the classic cartpole balancing problem from OpenAI Gym [31] in Figure 6a, compared to baselines. We see a distinct performance difference between our approach and baselines on this task. Our framework selects the more sophisticated REINFORCE algorithm [32] compared to either using a simple Q-table (Autogen) or random policy (zero-shot). We further evaluate the capacity of our framework to adapt (via the Adapter) to novel, uncanonical, variants of the task, inspired by prior work [33]. Specifically, we added the constraint of staying within the left zone of the arena as much as possible. Our prompts for this task is shown in Appendix M. Appendix Figure 15 shows how the FoO developed on the original problem, is successfully adapted and deployed to the new, constrained problem. We compare the poses of the model outputs for the original and the modified problems in Figure 6a. We see that the adapted solution of modifying the REINFORCE reward function performs well, underscoring the capacity of our framework to effectively adapt to novel variants of past tasks.\nSynthetic Image Generation using MNIST data: We show results of our framework on synthesis of MNIST images in Figure 6b, with SSIM (Structural Similarity Index Metric) comparing our framework to the baselines. We measure SSIM between the original MNIST dataset and the generated images as a measure of similarity of the synthetic generations to the original data. Higher SSIM is preferred. We see improvement in the quality of the synthetic generations for the approach developed by our framework, both quantitatively (in terms of SSIM), and qualitatively. Zero-shot generation uses a Generative Adversarial Network and Autogen uses a convolutional variational-autoencoder (VAE). Our framework evaluates two architectures for each in terms of activation functions. We also evaluated our framework with the constraint that the generated digits should be red. The adapter proposes adding a post-processing step to the generated images to modify the channel data to generate red images (Appendix Figure 16). The results are shown to the right of Figure 6b. Alongside image generation and RL, we demonstrate the application of our work to a range of additional tasks such as clustering and machine translation in Appendix B.3."}, {"title": "Related Work", "content": "Existing agentic designs, including recent frameworks overfit to a narrow spectrum of tasks in data science by assuming an overall workflow [3, 23, 10, 4]. Prior work has looked at Graph of Thoughts [34] to tackle reasoning problems (different from AutoML), but do not enforce the notion of \"options\" or option diversity. For the most closely related works, SELA [19] and Data Interpreter [20], we discussed data structure differences in Section 2.1. We highlight some additional framework-level differences here. In contrast to our work, SELA [19] predefines steps in the pipeline within the LLM prompt, e.g., data analysis, feature engineering, and model selection. Data Interpreter predefines a set of \u201ctask types\u201d with a detailed set of instructions for each task type. This ranges from specific"}, {"title": "Limitations of the overall framework", "content": "In this work, we proposed Flow-of-Options and an FoO-based agentic framework for the automation of ML tasks. Although our approach demonstrates improved performances on a range of tasks, we highlight some key limitations of our work here.\nFirst, our approach assumes the existence of \u201csome\u201d metric that can be used to evaluate the options. In the event that the metric is not clearly defined, alternative evaluators, like LLMs, can be used as a proxy for evaluating outcomes and generating a corresponding reward [42]. We also assume that the user provides some input data for each task (or it is obtained from a corresponding dataset or benchmark). If precise data is unavailable, connecting the LLM to data loading tools, can help provide the data necessary for our framework.\nSecond, although our approach successfully generates more diverse options for solving different tasks, we still see some residual bias for Random Forest. In the future, we seek to improve the diversity of the option generations to ensure a more uniform distribution over a range of methods. We also note that although the intrinsic knowledge of LLMs contain several methods, the LLMs are not capable of identifying more novel approaches (e.g., ChemProp for TDC tasks), that we hope to rectify.\nLastly, our approach currently naively samples walks and this results in some walks being re-sampled from one iteration to another, leading to repetition in the runs. In our future work, we seek to explore more sophisticated strategies for generating walks, such as tracking of already explored walks or updating the values to weight unexplored paths higher.\nWe further discuss some challenges associate with each LLM module in our framework."}, {"title": "LLM-Module Specific Challenges", "content": "While GPT-40 based frameworks were able to successfully execute tasks with 100% success rate, we note some engineering challenges for the modules in our framework:\n\u2022 Consistency Checker: There were a few cases where the consistency checker identified an otherwise consistent pathway as inconsistent. Although the agent would simply pursue other paths, thus producing \u201csome\u201d working solution in each case, this mistake could cause some viable and promising paths to remain unexplored. Adding an additional \u201cevaluator\" LLM to verify the response, may help mitigate this issue.\n\u2022 Retriever: We use the same retrieval mechanism as DS-Agent (using BAAI/LLM-embedder). There were a few cases where the case retrieval module did not retrieve the best case for a new task, instead retrieving an alternative case that is not as well suited to the current task. The tasks explored in our work do not seem particularly sensitive to this issue. However, improving the retrieval module can further improve the performance of our framework.\n\u2022 Plan Executor: Since code is generated from language descriptions of code implementa-tions, e.g., specifying \"Use GradientBoostingClassifier\", there are slight variations from one code generation to the next, when the plan executor interprets the options. Al-though we found these variations to be insignificant with GPT-40, using code snippets (e.g., code snippet of loading and using the model) when generating options, can potentially eliminate these differences.\n\u2022 Option Generator: In a few cases, we noted that the option generator would produce differently phrased, but similar options in spite of being prompted to encourage diversity (See Figure 12). Introducing an additional strategy to have the LLM options be regenerated, or prompting the LLM to modify similar options may help mitigate this issue."}]}