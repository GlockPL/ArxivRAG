{"title": "SpreadFGL: Edge-Client Collaborative Federated Graph Learning with Adaptive Neighbor Generation", "authors": ["Luying Zhong", "Yueyang Pi", "Zheyi Chen", "Zhengxin Yu", "Wang Miao", "Xing Chen", "Geyong Min"], "abstract": "Federated Graph Learning (FGL) has garnered widespread attention by enabling collaborative training on multiple clients for semi-supervised classification tasks. However, most existing FGL studies do not well consider the missing inter-client topology information in real-world scenarios, causing insufficient feature aggregation of multi-hop neighbor clients during model training. Moreover, the classic FGL commonly adopts the FedAvg but neglects the high training costs when the number of clients expands, resulting in the overload of a single edge server. To address these important challenges, we propose a novel FGL framework, named SpreadFGL, to promote the information flow in edge-client collaboration and extract more generalized potential relationships between clients. In SpreadFGL, an adaptive graph imputation generator incorporated with a versatile assessor is first designed to exploit the potential links between subgraphs, without sharing raw data. Next, a new negative sampling mechanism is developed to make SpreadFGL concentrate on more refined information in downstream tasks. To facilitate load balancing at the edge layer, SpreadFGL follows a distributed training manner that enables fast model convergence. Using real-world testbed and benchmark graph datasets, extensive experiments demonstrate the effectiveness of the proposed SpreadFGL. The results show that SpreadFGL achieves higher accuracy and faster convergence against state-of-the-art algorithms.", "sections": [{"title": "I. INTRODUCTION", "content": "With powerful expressive capabilities, graphs [1] have been widely used to depict real-world application scenarios such as social network [2], knowledge graph [3], and paper citation [4]. In the area of graph learning, the emerging Graph Neural Networks (GNNs) have gained significant attention due to their exceptional performance in dealing with graph-related tasks. GNNs efficiently utilize the feature propagation by employing multiple graph convolutional layers for node classification tasks, where the structural knowledge is distilled into discriminative representations from complex graph-orient data in diverse domains such as prediction modeling [5], malware detection [1], and resource allocation [6]. Commonly, the training performance of GNNs depends on the substantial graph data distributed among clients. However, due to privacy and overhead concerns, it is impractical to assemble graph data from all clients for GNN training.\nFollowing a distributed training mode, Federated Graph Learning (FGL) aims to deal with the problem of graph data is-land by promoting cooperative training among multiple clients [9]. To protect privacy, the FGL offers generalized graph mining models over distributed subgraphs without sharing raw data [7]. Many studies have verified the feasibility of FGL in various domains such as transportation [10], computer vision [11], and edge intelligence [12]. Recently, some studies also adopted FGL-based frameworks for semi-supervised classification tasks [8], [13]. These approaches typically join an edge server with multiple clients to train a globally-shared classifier for downstream tasks, where the clients and edge server undertake local updating and global aggregation, respectively.\nIn real-world FGL application scenarios, there are potential links between the subgraphs of a client and others since these subgraphs contain significant information about neighbor clients [7]. However, previous FGL-related studies [14], [15] overlooked such important links among clients, as shown in Fig. 1 (left). This oversight results in the insufficient feature propagation of multi-hop neighbors during local model training, ultimately leading to degraded performance in classification tasks. To explore the potential links among clients, some prior studies inferred the missing links in subgraphs, as shown in Fig. 1 (middle). For example, Zhang et al. [8] employed linear predictors on local models to conduct the missing links in subgraphs. However, the potential links rely solely on local clients, disregarding meaningful information from neighbor clients. Therefore, the features implied in the generated links may be incomplete and infeasible to recover the cross-client information. Moreover, most existing studies [8], [16] commonly adopted the classic FedAvg algorithm [17], neglecting the high training costs when the number of clients continues to expand, which leads to a serious single-point overload problem.\nTo address these essential challenges, we propose FedGL, an improved centralized FGL framework, to explore potential cross-subgraph links by leveraging the global information flow. As illustrated in Fig. 1 (right), we consider the edge server as an intermediary to facilitate the flow of global in-formation, thereby enhancing communication among different clients and fostering the collaborative knowledge integration of their subgraphs. Thus, the proposed FedGL is able to extract unbiased and generalized missing links through collaboration among the edge server and clients. Furthermore, we extend the FedGL to a multi-edge collaborative scenario and propose the SpreadFGL to efficiently handle the load balancing issue at the edge layer. The main contributions of this paper are summarized as follows."}, {"title": "II. RELATED WORK", "content": "Graph Neural Networks [18] have drawn considerable at-tention in recent years due to their remarkable capabilities. As an emerging technique in semi-supervised learning, GNNS can achieve accurate node classification for massive unlabeled nodes by training scarce labeled data. Considering the ad-vanced ability in modeling graph structures, GNNs have de-rived several variants such as Graph Convolutional Networks (GCNs) [19], Graph Attention Networks (GAT) [20], and GraphSAGE [21]. For example, GCNs conduct the operations of neural networks on graph topology, which have been widely used in semi-supervised learning tasks. The inference vector of the node u on the (l + 1)-th GCN layer is defined as\n$h_u^{(l+1)} = \\sigma \\{AGG (h_v^{(l)}, e_{uv}) | v \\in \\mathcal{V} \\},\\qquad (1)$\nwhere $h_u^{(l)}$ is the vector of the node u in the l-th GCN layer. $e_{uv}$ indicates the link between the node u and v. AGG(\u00b7) is an aggregator function used to integrate the neighbor features of node u via $e_{uv}$. And $\\sigma(\u00b7)$ is a non-linear activation function. The GAT incorporates GCNs with attention mechanisms to adaptively assign the weights $\\alpha_{uv}^{(l+1)}$ for the neighbors of the node u, and the inference vector is defined as\n$h_u^{(l+1)} = \\sigma \\{AGG (\\alpha_{uv}^{(l+1)}h_v^{(l)}, e_{uv}) | v \\in \\mathcal{V} \\}.\\qquad (2)$\nThe GraphSAGE aggregates node features by sampling from neighbor nodes and the inference vector is defined as\n$h_u^{(l+1)} = \\sigma \\{h_u^{(l)} || AGG (h_v^{(l)}, e_{uv}) | v \\in \\mathcal{V} \\},\\qquad (3)$\nwhere $||$ denotes the concatenation operation.\nMany scholar have contributed to GNN-based semi-supervised learning. For instance, Wang et al. [22] proposed a GCN framework that conducted feature propagation in topology and node spaces, aiming to promote the fusion of graphs and features. Zhong et al. [23] designed a contrastive GCN framework with a generative adjacency matrix to explore the topology correlations for downstream tasks. Sun et al. [24] adopted a multi-stage GCN-based framework that employed self-supervised learning to compensate for the limit labeled signal. Although the existing studies have gained great success in centralized semi-supervised learning, they did not well consider the discrete distribution of graph data that occurred in real-world scenarios. It should be noted that the message passing between subgraphs will be blocked if the cross-subgraph connections are missing. This seriously violates the propagation of GNNs in multi-hop neighbors and leads to unsatisfactory performance. Therefore, there is an urgent need to study the restoration of missing cross-subgraph links for better handling the semi-supervised node classification."}, {"title": "III. THE PROPOSED SPREADFGL", "content": "In this section, we consider the typical FGL scenario with distributed graph datasets. Based on this setting, we first pro-pose an improved centralized FGL framework, named FedGL. Next, we extend the FedGL to the scenario of multi-edge collaboration and propose a novel distributed FGL framework, named SpreadFGL. Specifically, Fig. 2 provides a detailed illustration of the proposed SpreadFGL."}, {"title": "A. Overview and Motivation", "content": "A graph dataset is denoted as $\\mathcal{D}(\\mathcal{G},\\mathcal{Y})$, where $\\mathcal{G}=(\\mathcal{V}, \\mathcal{E}, \\mathcal{X})$ is a global graph. $\\mathcal{V}$ is the node set, where $|\\mathcal{V}| = n$. $\\mathcal{E} = \\{e_{uv}\\}$ is the edge set that stores the link relationship between the nodes u and v, where $\\forall u, v \\in \\mathcal{V}$. $\\mathcal{X} \\in \\mathbb{R}^{n \\times d}$ indicates the node feature matrix, where $x_i \\in \\mathbb{R}^d$ is the feature vector of the i-th node. $\\mathcal{Y} = [0,1]^{n \\times c}$ is the label matrix, where c is the number of classes. Considering that there are N edge servers and M clients. The edge server $S_j$ covers $M_j$ local clients $\\{C_i | i \\in [M_j]\\}$ to conduct the FGL training, where $\\sum_{j=1}^{N} M_j = M$. The client $C_i$ owns the part samples of the graph dataset, denoted by $\\mathcal{D}_i \\{G_{ii},\\mathcal{Y}_{i}\\}$, where $\\mathcal{G}_{ii} = (\\mathcal{V}_{ii}, \\mathcal{E}_{ii}, \\mathcal{X}_{ii})$ is a local subgraph and $\\mathcal{Y}_{ii}$ is the sub-label matrix of nodes $\\mathcal{V}_{ii}$. To simulate the real-world scenario of missing links between clients, we consider that there are no shared nodes and connected links among clients, formulated by $\\mathcal{V}_{ii} \\cap \\mathcal{V}_{ir} = \\emptyset$, where $\\forall i, r \\in [M_j]$ and $i \\neq r$ if $j = j'$, and $\\forall i \\in [M_j],\\forall r \\in [M_{j'}]$ if $j \\neq j'$. The subgraphs of all clients form the complete graph, defined by $\\sum_{i=1}^N\\sum_{i'=1}^{M} |\\mathcal{V}_{ii'}| = n$. Thus, there is no link between any two clients, and a client cannot directly retrieve the node features from another client.\nBased on the above scenario, the client $C_i$ owns a local node classifier $\\mathcal{F}_i$ and graphic patcher $\\mathcal{P}_i$, and all clients can jointly learn graph representations for semi-supervised node classification. Generally, the proposed SpreadFGL aims to conduct collaborative learning on independent subgraphs across all clients, prioritizing the privacy of raw data. Therefore, the SpreadFGL obtains the global node classifiers $\\{F_j|j \\in [N]\\}$ parameterized by $\\{W_j|j \\in [N]\\}$ in the edge servers for downstream tasks. With this consideration, we for-mulate the optimization problem as minimizing the aggregated risks to find the optimal weights $\\{W_j|j\\in [N]\\}$, defined as\n$\\min_{W_j} \\sum_{j=1}^N R_j(\\mathcal{F}_j(W_j)) = \\sum_{j=1}^N \\min_{W^{(j,i)}} \\frac{1}{M_j} \\sum_{i=1}^{M_j} R^{(j,i)}(W^{(j,i)}, \\mathcal{F}),\\qquad (4)$\nwhere $W^{(j,i)}$ is the learnable weights of local node classifier $\\mathcal{F}$. $R_j$ is the loss function of the global node classifier $\\mathcal{F}_j$. And $R$ is the loss function of the i-th client that is used to measure the local empirical risk,\n$R^{(j,i)}(W^{(j,i)}, \\mathcal{F}(\\mathcal{G}_{ii}, \\mathcal{Y}_i)) = - \\frac{1}{|\\mathcal{V}_{ji}|} \\sum_{v \\in \\mathcal{V}_{ji}} \\sum_{r=1}^c \\mathbb{1}[y_v=r] \\log ((\\mathcal{F}(W^{(j,i)}, \\mathcal{G}_{ii}))_v)_r, \\qquad (5)$\nwhere $\\mathcal{V}_i \\subset \\mathcal{V}_{ii}$ is the labeled training set in the i-th client, and $y_v^r$ is the ground truth of node v in the i-th client."}, {"title": "B. FedGL: Centralized Federated Graph Learning", "content": "Since clients cannot directly capture cross-subgraph links that contain important neighbor information, the feature prop-agation from higher-order neighbors becomes inadequate, re-sulting in degraded classification performance. Therefore, it is crucial to explore the potential topology links among clients. To achieve this goal, we propose an improved centralized FGL framework, named FedGL. In FedGL, we consider an edge server to communicate with M clients. The FedGL leverages the edge server $S_j$ as an intermediary to facilitate the information flow among clients, where $S_j$ covers all clients, denoted by $M_1 = M$. Specifically, we incorporate a graph imputation generator to construct learnable links, thereby generating the latent links between subgraphs. To enhance feature propagation in local tasks and facilitate subsequent inference with the global model, we employ a L-layer GNN model with the local node classifier $\\mathcal{F}$, defined as\n$\\mathcal{H}^{(j,l+1)} = GNNconv_{W^{(j,l)}}(\\mathcal{E}_{ji}, x_{ii}^{(l)}),\\qquad (6)$\nwhere GNNconv(\u00b7) is a GNN model and $\\mathcal{H}^{(j,l)}$ indicates the GNN output of the i-th client covered by $S_j$. The feature propagation of the (l+1)-th layer is given in Eq. (3). Moreover, the Cross-Entropy loss function is adopted for the i-th client covered by $S_j$ in the downstream tasks, defined as\n$L_r = R(\\mathcal{F}_i^{(L)}(W^{(j,i)})) = - \\sum_{u=1}^{\\mathcal{V}_i} \\sum_{r=1}^c \\mathcal{Y}_{ir}^{(l)} \\log \\mathcal{H}_{ur}^{(j,i,L)},\\qquad (7)$\nwhere $\\mathcal{Y}_{ir}^l$ is the inference vector of the node u conducted by local training.\nFor every edge-client communication in FedGL, each client parallelly trains the local node classifier $\\mathcal{F}$ parameterized by $W^{(j,i)}$ in local training rounds, formulated as\n$W_t^{(j,i)} = W^{(j,i)} - \\alpha \\nabla R(W^{(j,i)}),\\qquad (8)$\nwhere $\\alpha$ is the learning rate. $t \\in [T_i - 1]$ indicates the local training rounds. After local training, $S_j$ aggregates local parameters $\\{W^{(j,i)}|i \\in [M_j]\\}$ to update global ones $W_j$, and then broad-casts $W_j$ to all clients at each edge-client communication."}, {"title": "C. Graph Imputation Generator with Versatile Assessor", "content": "To capture the potential cross-subgraph links, we design a graph imputation generator and incorporate it with a ver-satile assessor to explore a learnable potential graph $\\hat{\\mathcal{G}} = (\\mathcal{V}_i,\\hat{\\mathcal{E}}_i, \\mathcal{X}_i)$ for mending the cross-subgraph links.\nGraph Imputation Generator. To construct the globally-shared information without revealing raw data, the clients upload the processed embeddings $\\{\\mathcal{H}^{(j,i)}|i \\in [M_j]\\}$ to the edge server at every K intervals of edge-client communication, where the original linked nodes remain proximate in the low-dimensional space. Next, the graph imputation generator performs the fusion on the processed embeddings to obtain the globally-shared information $\\mathcal{H}' \\in \\mathbb{R}^{|\\mathcal{V}| \\times c}$, where $\\mathcal{V}_i$ is the number of all clients covered by $S_j$. Based on this, $\\mathcal{H}^0$ is denoted as\n$\\mathcal{H}^0 = [\\mathcal{H}^{(j,1)} || ... ||\\mathcal{H}^{(j,j)}]^T.\\qquad (9)$\nIn real-world application scenarios of FGL, it is possible for each node in clients to own potential cross-subgraph links, and it may be insufficient for clients to propagate features in multi-hop neighbors if missing these cross-subgraph links. In response to this problem, the graph imputation generator uti-lizes the distance to evaluate the node similarity and construct the global topology graph, referred to $\\bar{\\mathcal{A}} = \\mathcal{H}^0\\mathcal{H}^{0T}$. Next, k most similar nodes are selected from this topology graph as potential cross-subgraph links, denoted by the set $\\mathcal{E}_i$. To generate the potential feature vectors $\\mathcal{X'}$ under the guidance of the globally-shared information, an autoencoder parameterized by $\\Phi_{AE}$ is used to explore overcomplete underlying represen-tations from $\\mathcal{H}^0$. Furthermore, to guarantee data privacy, the random noisy vector $S$ is input to the autoencoder, and thus the output of the autoencoder is reconstructed as $\\hat{\\mathcal{H}} = h(f(S))$, where $f(\u00b7)$ and $h(\u00b7)$ are the encoder and decoder, respectively. It is noted that $\\mathcal{X'} = f(S)$ indicates the potential features expected to be extracted by the encoder. With the autoencoder, the random noisy vector is mapped to the same dimension as $\\mathcal{H}^0$, and the output of the (l + 1)-th layer is defined as\n$\\mathcal{H}^{(j,l+1)} = \\sigma (\\mathcal{H}^{(j,1)} W^{(j,l+1)} + b^{(j,l+1)}),\\qquad (10)$\nwhere $W^{(j,l+1)} \\in \\mathbb{R}^{d_l \\times d_{l+1}}$ and $b^{(j,l+1)} \\in \\mathbb{R}^{d_l}$ are the layer-specific weights and biases, respectively. $\\sigma(\u00b7)$ denotes the activation function.\nVersatile Assessor. Since the conditional distribution of $\\hat{\\mathcal{H}}$ relies on $\\mathcal{X'}$ and is independent of $S$, $S \\rightarrow \\mathcal{X'} \\rightarrow \\hat{\\mathcal{X}} \\rightarrow \\hat{\\mathcal{H}}$ in the autoencoder follows the Markov principle. Therefore, we design a versatile assessor parameterized by $\\Phi_{As}$ to supervise the quality of reconstruction data from the decoder, aiming to extract the expected underlying features $\\hat{\\mathcal{X}}$ tailored for node classification. Considering the diversity of datasets, the assessor should be trainable to fit in specific tasks. Thus, the assessor adopts a fully-connected neural network to evaluate $\\hat{\\mathcal{H}}$. Concretely, the assessor takes the reconstructed globally-shared information $\\hat{\\mathcal{H}}$ as input in the form of a value, which is positively correlated with the quality evaluation of the reconstructed data. Hence, the autoencoder tends to obtain a higher value under the supervision of the assessor and extract more valid global information. Specifically, the loss function of the autoencoder is defined as\n$\\mathcal{L}_{AE} = - \\sum_{\\mathcal{E}_{p(V \\in \\mathcal{V}_i)}} [\\mathcal{E}_{Assor} (h)],\\qquad (11)$\nwhere $\\mathcal{E}_{p(\u00b7)}$ is the expectation of the variables in $p(\u00b7)$, and $p(h_u \\in \\mathcal{V}_i)$ indicates $\\hat{h}_u^i$ sampled from the distribution of $\\hat{\\mathcal{H}}$. Assor() is the assessor that evaluates the constructed global information. To distinguish the original and recon-structed global data, we regard the globally-shared information as the criterion and train the assessor to assign higher scores. In contrast, the assessor is trained to assign lower scores with the reconstructed global information. Therefore, the assessor is able to guide the autoencoder to evolve more discriminative representations of latent features. Specifically, the loss function of the assessor is defined as\n$\\mathcal{L}_{AS} = \\frac{1}{|\\mathcal{V}|} \\sum_{\\mathcal{V}_{i}} [\\mathcal{E}_{p(h_u \\in \\mathcal{V}_i)} (Assor(\\hat{h}_u^i))\n+ \\mathcal{E}_{p(\\hat{h}_u \\in \\mathcal{V}_i)} (1 - Assor(\\hat{h}_u^i))]\n= \\frac{1}{|\\mathcal{V}|} \\sum_{\\mathcal{V}_{i}} [log (1 - Assor(\\hat{h}_u^i))\n+ log (Assor(\\hat{h}_u^i))].\\qquad (12)$\nwhere $p(\\hat{h}_u \\in \\mathcal{V}_i)$ denotes $\\hat{h}_u^i$ sampled from the distribu-tion of $\\hat{\\mathcal{H}}$.\nThe training processes of the autoencoder and assessor are performed simultaneously, where the assessor guides the autoencoder to learn more discriminative reconstructed data and potential features through back-propagation."}, {"title": "D. Negative Sampling and Graph Fixing", "content": "Negative Sampling. To extract more refined potential fea-tures, we develop a negative sampling mechanism to concen-trate on the pertinent information for node classification. Based on the proposed versatile assessor, we first set a threshold $\\theta \\in (0,1)$ in every training iteration of the autoencoder and select the attributes in $\\hat{h}_u^i$ that are less than $\\theta$. These attributes are deemed as negative and their feedbacks from the assessor are 0. Next, the zero-regularization is used to process these negatives, and thus both the autoencoder and the assessor can spotlight the representations that are meaningful for downstream tasks. Hence, the loss function of the assessor is updated and redefined as\n$\\mathcal{L}_{AS} = \\frac{1}{|\\mathcal{V}|} \\sum_{\\mathcal{V}_{i}} [\\mathbb{1}log (1 - Assor (e_{u}))\n+ \\mathbb{1}log (Assor (e_{u})],\\qquad (13)$\nwhere $e_{u}$ is a c-dimensional vector that judges whether $\\hat{h}_u^i$, $\\hat{h}_u^i'$ is higher than $\\theta$ ($e_{ui} = 1$) or not ($e_{ui} = 0$). $\\odot$ is the element-wise multiplication. Correspondingly, the loss function of the autoencoder is updated and redefined as\n$\\mathcal{L}_{AE} = \\frac{1}{|\\mathcal{V}|} \\sum_{\\mathcal{V}_{i}} [log (1 - Assor (e_{u}))\n+ ||\\hat{h}_u^i (1 - e_{u}) - \\hat{h}_u^{i'} (1 - e_{u}) ||^2],\\qquad (14)$\nwhere $\\hat{h}_u^i$ and $\\hat{h}_u^{i'}$ are the u-th vector of $\\hat{\\mathcal{H}}$ and $\\hat{\\mathcal{H}'}$, respec-tively. $\\mathbb{1}$ is an indicator vector with the values of 1.\nThrough the above operations, $\\hat{\\mathcal{E}}_i$ and $\\hat{\\mathcal{X}}_i$ are used to form the learnable potential graph $\\hat{\\mathcal{G}}^j = (\\mathcal{V}_i,\\hat{\\mathcal{E}}_i,\\mathcal{X}_i)$.\nGraph Fixing. The edge server $S_i$ divides $\\hat{\\mathcal{G}}^j$ into some subgraphs, denoted by the set $\\{G^{\\S}_{ii} (\\mathcal{V}_{ii}, \\mathcal{E}_{ii}, \\mathcal{N}_{ii}^j)|i \\in [M_j]\\}$,where $\\mathcal{E}_{ii}^{\\S} = \\{e^\\S_{uv}, u, v \\in \\mathcal{V}_{ii}\\}$ is the neighbor set of $\\mathcal{V}_{ii}$, $\\mathcal{N}_{ii} = \\{\\mathcal{X}_{ii}, u \\in \\mathcal{V}_{ii}\\}$, and $\\{e^\\S_{uv}\\}$ indicates the potential neighbor feature vectors of u. Next, $S_i$ assigns the subgraphs to each client. It is noted that each local client repairs the subgraph by using the local graphic patcher $\\mathcal{P}$ referring to $\\hat{\\mathcal{G}}^j_{ii} = \\mathcal{P} (\\mathcal{G}_{ii})$. This process simulates the missing links, thereby promoting the feature propagation of local tasks in Eq. (3). By collaborating with the edge server, clients are expected to acquire diverse neighbor features from globally-shared information, thereby fixing cross-subgraph missing links. Moreover, these cross-subgraph links contribute to training a global node classifier $\\mathcal{F}_i$, aligning with the overall optimization objective in Eq. (4)."}, {"title": "E. SpreadFGL: Distributed Federated Graph Learning", "content": "In real-world application scenarios, a single edge server may encounter the problem of excessive costs and degraded performance as the number of clients expands, particularly when clients are geographically dispersed. To address this problem, we propose a novel distributed FGL framework, named SpreadFGL, that extends the FedGL to a multi-edge environment. The SpreadFGL is able to facilitate more ef-ficient FGL training and better load balancing in a multi-edge collaborative environment. We consider that there are N edge servers, and an edge server $S_i$ is equipped with a global node classifier $F_i$ parameterized by $W_j$. Besides, a client only communicates with its closest edge server. There exist neighbor relationships among the servers, denoted by the matrix $A \\in R^{N \\times N}$. If $S_i$ and $S_j$ are neighbors, $a_{ij} = 1$; otherwise, $a_{ij} = 0$. Moreover, the parameter transmission is permitted between neighbor servers.\nIn SpreadFGL, the clients adopt the L-layer GNNs and conduct the feature propagation via Eq. (3) during the local training. The edge servers exchange information with the covered clients in each edge-client communication. At each K intervals of edge-client communications, the clients and their nearest edge servers collaboratively utilize the shared information to extract the potential links based on the proposed graph imputation generator and negative sampling mechanism. However, the potential cross-subgraph links strictly depend on the information provided by all clients. This not only violates the core idea of the SpreadFGL but also is impractical if the information is transmitted from the clients that are under the coverage of other servers. In light of these concerns, we design a weight regularizer during the local training. Based on trace normalization, the regularizer is used to enhance the network learning capability of the local node classifiers. Specifically, the loss function of the i-th client under the coverage of $S_j$ is defined as\n$\\mathcal{L}_p = R_i^{(L)}(\\mathcal{F}(\\mathcal{G}_{ii}, \\mathcal{Y}_{i}) )\\qquad\\qquad \\qquad \\qquad= - \\sum_{u=1}^{\\vert \\mathcal{V}\\vert} \\sum_{r=1}^C \\mathcal{Y}_{ur} \\log H_{ur}^{(j,i,L)} + Tr(W^{(j,i,L)} W^{(j,i,L)}),\\qquad (15)$\nwhere $Tr(\u00b7)$ is the square matrix trace. $W^{(j,i,L)}$ indicates the parameters of L-th GNN layer for the local node classifier $\\mathcal{F}$. To better explore the potential cross-subgraph links by using the information from other servers, we adopt the topology structure at the edge layer to facilitate the parameter transmis-sion between neighbor servers. This enables the information flow among clients via the gradient propagation at each K intervals of edge-client communication. Specifically, $S_j$ first aggregates the model parameters of its neighbor servers. Next, $S_j$ averages the parameters and broadcast them to the covered clients. This process can be described as\n$W_j = \\frac{1}{\\vert S_j\\vert} \\sum_{r=1}^{N} \\frac{a_{rj}}{(\\sum_{r=1}^{N} a_{rj} M_r)} \\sum_{i=1}^{M_r} W_{r,i}.\\qquad (16)$\nThe procedure of the proposed SpreadFGL is elaborated in Algorithm 1, whose core components have been described in detail before."}, {"title": "IV. PERFORMANCE EVALUATION", "content": "In this section, we first compare the proposed FedGL and SpreadFGL with state-of-the-art algorithms based on real-world testbed and graph datasets. Next, we conduct ablation experiments to further verify the superiority of the core components designed in the proposed frameworks."}, {"title": "A. Experiment Setup", "content": "Real-world Testbed. As shown in Fig. 3, we build a hard-ware testbed to evaluate the proposed FedGL and SpreadFGL in real-world scenarios of edge-client collaboration. In the testbed, each Raspberry Pi 4B acts as a client that is equipped with Broadcom BCM2711 SoC @1.5GHz with 4 GB RAM, and the OS is Raspbian GNU/Linux 11. Each Jetson TX2 acts as an edge server that is equipped with one 256-core NVIDIA Pascal(R) GPU, one Dual-core Denver 2 64-bit CPU and a quad-core Arm(R) Cortex(R)-A57 MPCore processor equipped with 8 GB RAM, and the OS is Ubuntu 18.04.6 LTS. The above hardware communicates through a 5 GHz WiFi network, and the proposed frameworks are implemented based on PyTorch. After completing local training, the client (Raspberry Pi 4B) uploads the local model parameters to its connected edge server (Jetson TX2). An edge server conducts aggregation and distributes the globally-shared model to its connected clients."}, {"title": "B. Experiment Results and Analysis", "content": "Node Classification Accuracy. As shown in Table II, the proposed SpreadFGL and FedGL can both achieve higher classification accuracy than other state-of-the-art algorithms under different datasets, indicating the superiority of the pro-posed frameworks for node classification tasks. Specifically, the significant performance gap between the LocalFGL and SpreadFGL verifies the advantages of using the proposed edge-client collaboration mechanism. The FedGL and SpreadFGL outperform the FedSage+ by around 12.78% and 14.71% in terms of ACC and F1, respectively. This demonstrates that the FedGL and SpreadFGL gain more generalized potential cross-subgraph links through the global information flow, further validating the effectiveness of the proposed graph imputation generator. Moreover, compared to the FedGL, the SpreadFGL achieves better performance on most of the datasets under various scenarios with different numbers of clients. This indicates that the information flow between clients and edge servers utilized in the SpreadFGL effectively promotes the repair of missing links among clients even though the scenario becomes complex with more clients.\nPerformance with Different Labeled Ratios. Fig. 4 de-picts the ACC of the SpreadFGL on different datasets with various labeled ratios, varying from 0.2 to 0.6. With the same labeled ratio, the ACC tends to decrease as the datasets are distributed on more clients. This is because massive heteroge-neous clients cause difficulty and instability in the aggregation process of model parameters. Under this scenario, the perfor-mance of the classic FGL might be seriously degraded since it adopts a centralized training manner. It is noted that the ACC is rising as the labeled ratio increases, but with fewer data points presenting the opposite situation. This discrepancy may be attributed to the sparsity of certain classes in the feature space, leading to insufficient model training and thus affecting classification accuracy.\nParameter Sensitivity. We analyze the parameter sensi-tivity of the proposed SpreadFGL on different datasets with respect to the hyperparameter K and T\u2081. As shown in Fig. 5, K when K is more than 10, while they keep stable as K ranges in [1, 10], attributed to the reason that the graph imputation generator can better repair the missing links in subgraphs to promote feature propagation in local models within fewer edge-client communications, thereby improving the training of the global node classifiers. In this regard, the suggested values of K range from 1 to 10. Fig. 6 presents the influence of local training iteration $T_l$ on the SpreadFGL. The SpreadFGL converges slowly and achieves a local optimum when $T_l$ is less than 5. This is because local models cannot sufficiently learn feature patterns within fewer local iterations, leading to slow model convergence. It is noted that the ACC declines when $T_l$ exceeds 50 due to the overfitting of the model. Therefore, a suitable range of $T_l$ is [10, 20], considering both accuracy and convergence speed.\nAblation Study. As shown in Fig. 7, we regard the FedAvg-fusion as a baseline that adopts the FedAvg to aggregate the parameters from multiple clients on an edge server. Also, we test the performance of the FedGL without a negative sampling mechanism (denoted by NS), versatile assessor (de-noted by Assor), and the FedGL without NS. The proposed FedGL and SpreadFGL achieve comparable performance and outperform others by combining graph imputation generator, versatile assessor, and negative sampling mechanism. It is noted that there is only a small performance improvement when just utilizing one of the core components designed in the proposed frameworks. It obtains considerable improvement when the SpreadFGL adopts all the proposed components. This demonstrates that the integration of these components is able to better extract more refined potential cross-subgraph links, thereby promoting the accuracy of classification tasks.\nConvergence Validation. Fig. 8 illustrates the training loss of different FGL-based frameworks on Cora and Citeseer datasets. It can be observed that both the FedGL and Spread-FGL can always rapidly converge compared to the state-of-the-art algorithms, validating the effectiveness of the proposed frameworks in node classification tasks. Fig. 9 shows the curves of ACC when using different FGL-based frameworks. It is noted that the FedGL and SpreadFGL can achieve higher ACC than other state-of-the-art algorithms within fewer training iterations. Compared to the FedGL, the SpreadFGL converges faster to higher accuracy on different graph datasets, demonstrating the superiority of the SpreadFGL in multi-edge collaborative environments."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel FGL-based framework named FedGL and its extended framework SpreadFGL, ad-dressing the challenges of generating cross-subgraph links and single-node overloading. First, we design the FedGL to repair the missing links between clients, where a new graph imputation generator is developed that incorporates a versatile assessor and negative sampling mechanism to explore refined global information flow, extracting unbiased latent links and thus improving the training effect. Next, to alleviate the overloading issue at the edge layer, we extend the FedGL and propose the SpreadFGL with multi-edge collaboration to en-hance the global information exchange. Extensive experiments are conducted on real-world testbed and benchmark graph datasets to verify the superiority of the proposed FedGL and SpreadFGL. The results show that the FedGL and SpreadFGL outperform state-of-the-art algorithms in terms of model accu-racy. Further, through ablation experiments and convergence analysis, we validate the effectiveness of the core components designed in the proposed frameworks and the advantage of the SpreadFGL for achieving faster convergence speed."}]}