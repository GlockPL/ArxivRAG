{"title": "SpreadFGL: Edge-Client Collaborative Federated Graph Learning with Adaptive Neighbor Generation", "authors": ["Luying Zhong", "Yueyang Pi", "Zheyi Chen", "Zhengxin Yu", "Wang Miao", "Xing Chen", "Geyong Min"], "abstract": "Federated Graph Learning (FGL) has garnered widespread attention by enabling collaborative training on multiple clients for semi-supervised classification tasks. However, most existing FGL studies do not well consider the missing inter-client topology information in real-world scenarios, causing insufficient feature aggregation of multi-hop neighbor clients during model training. Moreover, the classic FGL commonly adopts the FedAvg but neglects the high training costs when the number of clients expands, resulting in the overload of a single edge server. To address these important challenges, we propose a novel FGL framework, named SpreadFGL, to promote the information flow in edge-client collaboration and extract more generalized potential relationships between clients. In SpreadFGL, an adaptive graph imputation generator incorporated with a versatile assessor is first designed to exploit the potential links between subgraphs, without sharing raw data. Next, a new negative sampling mechanism is developed to make SpreadFGL concentrate on more refined information in downstream tasks. To facilitate load balancing at the edge layer, SpreadFGL follows a distributed training manner that enables fast model convergence. Using real-world testbed and benchmark graph datasets, extensive experiments demonstrate the effectiveness of the proposed SpreadFGL. The results show that SpreadFGL achieves higher accuracy and faster convergence against state-of-the-art algorithms.", "sections": [{"title": "I. INTRODUCTION", "content": "With powerful expressive capabilities, graphs [1] have been widely used to depict real-world application scenarios such as social network [2], knowledge graph [3], and paper citation [4]. In the area of graph learning, the emerging Graph Neural Networks (GNNs) have gained significant attention due to their exceptional performance in dealing with graph-related tasks. GNNs efficiently utilize the feature propagation by employing multiple graph convolutional layers for node classification tasks, where the structural knowledge is distilled into discriminative representations from complex graph-orient data in diverse domains such as prediction modeling [5], malware detection [1], and resource allocation [6]. Commonly, the training performance of GNNs depends on the substantial graph data distributed among clients. However, due to privacy and overhead concerns, it is impractical to assemble graph data from all clients for GNN training.\nFollowing a distributed training mode, Federated Graph Learning (FGL) aims to deal with the problem of graph data island by promoting cooperative training among multiple clients [9]. To protect privacy, the FGL offers generalized graph mining models over distributed subgraphs without sharing raw data [7]. Many studies have verified the feasibility of FGL in various domains such as transportation [10], computer vision [11], and edge intelligence [12]. Recently, some studies also adopted FGL-based frameworks for semi-supervised classification tasks [8], [13]. These approaches typically join an edge server with multiple clients to train a globally-shared classifier for downstream tasks, where the clients and edge server undertake local updating and global aggregation, respectively.\nIn real-world FGL application scenarios, there are potential links between the subgraphs of a client and others since these subgraphs contain significant information about neighbor clients [7]. However, previous FGL-related studies [14], [15] overlooked such important links among clients, as shown in Fig. 1 (left). This oversight results in the insufficient feature propagation of multi-hop neighbors during local model training, ultimately leading to degraded performance in classification tasks. To explore the potential links among clients, some prior studies inferred the missing links in subgraphs, as shown in Fig. 1 (middle). For example, Zhang et al. [8] employed linear predictors on local models to conduct the missing links in subgraphs. However, the potential links rely solely on local clients, disregarding meaningful information from neighbor clients. Therefore, the features implied in the generated links may be incomplete and infeasible to recover the cross-client information. Moreover, most existing studies [8], [16] commonly adopted the classic FedAvg algorithm [17], neglecting the high training costs when the number of clients continues to expand, which leads to a serious single-point overload problem.\nTo address these essential challenges, we propose FedGL, an improved centralized FGL framework, to explore potential cross-subgraph links by leveraging the global information flow. As illustrated in Fig. 1 (right), we consider the edge server as an intermediary to facilitate the flow of global information, thereby enhancing communication among different clients and fostering the collaborative knowledge integration of their subgraphs. Thus, the proposed FedGL is able to extract unbiased and generalized missing links through collaboration among the edge server and clients. Furthermore, we extend the FedGL to a multi-edge collaborative scenario and propose the SpreadFGL to efficiently handle the load balancing issue at the edge layer. The main contributions of this paper are summarized as follows."}, {"title": "II. RELATED WORK", "content": "Graph Neural Networks [18] have drawn considerable attention in recent years due to their remarkable capabilities. As an emerging technique in semi-supervised learning, GNNs can achieve accurate node classification for massive unlabeled nodes by training scarce labeled data. Considering the advanced ability in modeling graph structures, GNNs have derived several variants such as Graph Convolutional Networks (GCNs) [19], Graph Attention Networks (GAT) [20], and GraphSAGE [21]. For example, GCNs conduct the operations of neural networks on graph topology, which have been widely used in semi-supervised learning tasks. The inference vector of the node u on the (l + 1)-th GCN layer is defined as\n$h_u^{(l+1)} = \\sigma \\{\\text{AGG} \\left( h_v^{(l)}, e_{uv} \\right) | \\forall v \\in \\nu \\}$\nwhere $h_u^{(l)}$ is the vector of the node u in the l-th GCN layer. $e_{uv}$ indicates the link between the node u and v. AGG(\u00b7) is an aggregator function used to integrate the neighbor features of node u via $e_{uv}$. And $\\sigma(\\cdot)$ is a non-linear activation function. The GAT incorporates GCNs with attention mechanisms to adaptively assign the weights $\\alpha_v^{(l+1)}$ for the neighbors of the node u, and the inference vector is defined as\n$h_u^{(l+1)} = \\sigma \\{\\text{AGG} \\left( \\alpha_v^{(l+1)} h_v^{(l)}, e_{uv} \\right) | \\forall v \\in \\nu \\}$\nThe GraphSAGE aggregates node features by sampling from neighbor nodes and the inference vector is defined as\n$h_u^{(l+1)} = \\sigma \\{ h_u^{(l)} || \\text{AGG} \\left( h_v^{(l)}, e_{uv} \\right) | \\forall v \\in \\nu \\}$\nwhere $\\|\\|$ denotes the concatenation operation.\nMany scholar have contributed to GNN-based semi-supervised learning. For instance, Wang et al. [22] proposed a GCN framework that conducted feature propagation in topology and node spaces, aiming to promote the fusion of graphs and features. Zhong et al. [23] designed a contrastive GCN framework with a generative adjacency matrix to explore the topology correlations for downstream tasks. Sun et al. [24] adopted a multi-stage GCN-based framework that employed self-supervised learning to compensate for the limit labeled signal. Although the existing studies have gained great success in centralized semi-supervised learning, they did not well consider the discrete distribution of graph data that occurred in real-world scenarios. It should be noted that the message passing between subgraphs will be blocked if the cross-subgraph connections are missing. This seriously violates the propagation of GNNs in multi-hop neighbors and leads to unsatisfactory performance. Therefore, there is an urgent need to study the restoration of missing cross-subgraph links for better handling the semi-supervised node classification."}, {"title": "III. THE PROPOSED SPREADFGL", "content": "In this section, we consider the typical FGL scenario with distributed graph datasets. Based on this setting, we first propose an improved centralized FGL framework, named FedGL. Next, we extend the FedGL to the scenario of multi-edge collaboration and propose a novel distributed FGL framework, named SpreadFGL. Specifically, Fig. 2 provides a detailed illustration of the proposed SpreadFGL."}, {"title": "A. Overview and Motivation", "content": "A graph dataset is denoted as D(G,Y), where G = (V, E, X) is a global graph. V is the node set, where |V| = n. E = {euv} is the edge set that stores the link relationship between the nodes u and v, where Vu, v \u2208 V. X \u2208 Rn\u00d7d indicates the node feature matrix, where xi \u2208 Rd is the feature vector of the i-th node. Y = [0,1] \u2208 Rn\u00d7c is the label matrix, where c is the number of classes. Considering that there are N edge servers and M clients. The edge server Sj covers Mj local clients {Ci\u2208 [Mj]} to conduct the FGL training, where \u2211j=1Mj = M. The client C owns the part samples of the graph dataset, denoted by Di{Gii, Yii}, where Gii = (vii, Eii, Xii) is a local subgraph and Yii is the sub-label matrix of nodes Vii. To simulate the real-world scenario of missing links between clients, we consider that there are no shared nodes and connected links among clients, formulated by Vii \u2229 Vir = \u00d8, where Vi, r \u2208 [Mj] and i \u2260 r if j = j, and Vi \u2208 [Mj],\u2200r \u2208 [M3] if j \u2260 j. The subgraphs of all clients form the complete graph, defined by \u2211Mi=1\u2211Mi=1Vii = n. Thus, there is no link between any two clients, and a client cannot directly retrieve the node features from another client.\nBased on the above scenario, the client C owns a local node classifier F and graphic patcher P, and all clients can jointly learn graph representations for semi-supervised node classification. Generally, the proposed SpreadFGL aims to conduct collaborative learning on independent subgraphs across all clients, prioritizing the privacy of raw data. Therefore, the SpreadFGL obtains the global node classifiers {Fj|j \u2208 [N]} parameterized by {Wj|j \u2208 [N]} in the edge servers for downstream tasks. With this consideration, we formulate the optimization problem as minimizing the aggregated risks to find the optimal weights {Wj|j\u2208 [N]}, defined as\n$\\sum_{j=1}^{N} R_{j}\\left(F_{j}\\left(W_{j}\\right)\\right) = \\sum_{j=1}^{N} \\min _{W(j, i)} \\frac{1}{M_{j}} \\sum_{i=1}^{M_{j}} R_{i}\\left(F_{i}\\left(W_{(j, i)}\\right)\\right)$\nwhere W(j,i) is the learnable weights of local node classifier F. Rj is the loss function of the global node classifier Fj. And R is the loss function of the i-th client that is used to measure the local empirical risk,\n$F_{i}^{\\prime}\\left(W_{(j, i)}^{\\prime}, F\\left(G_{(i, i)}, W_{(j, i)}\\right)\\right)=\\frac{1}{\\left|v_{i}\\right|} \\sum_{v \\in v_{i}} y_{v} F\\left(G_{(i, i)}^{\\prime}, v, W_{(j, i)}^{\\prime}\\right)$\nwhere vi C Vii is the labeled training set in the i-th client, and y is the ground truth of node v in the i-th client."}, {"title": "B. FedGL: Centralized Federated Graph Learning", "content": "Since clients cannot directly capture cross-subgraph links that contain important neighbor information, the feature propagation from higher-order neighbors becomes inadequate, resulting in degraded classification performance. Therefore, it is crucial to explore the potential topology links among clients. To achieve this goal, we propose an improved centralized FGL framework, named FedGL. In FedGL, we consider an edge server to communicate with M clients. The FedGL leverages the edge server Sj as an intermediary to facilitate the information flow among clients, where Sj covers all clients, denoted by M\u2081 = M. Specifically, we incorporate a graph imputation generator to construct learnable links, thereby generating the latent links between subgraphs. To enhance feature propagation in local tasks and facilitate subsequent inference with the global model, we employ a L-layer GNN model with the local node classifier F, defined as\nH(j,2) = GNNconvw(j,i) (Eji, xii),\nwhere GNNconv(\u00b7) is a GNN model and H(1,2) indicates the GNN output of the i-th client covered by Sj. The feature propagation of the (l+1)-th layer is given in Eq. (3). Moreover, the Cross-Entropy loss function is adopted for the i-th client covered by Sj in the downstream tasks, defined as\nLi, r = R(F(j,2))(W(j,i)) = -\\sum_{u=1}^{\\left|V_{i}\\right|} \\sum_{r=1}^{c} Y_{u}^{l} \\cdot H_{(3, i)},\\left.\\right.\\right.\nwhere Y is the inference vector of the node u conducted by local training.\nFor every edge-client communication in FedGL, each client parallelly trains the local node classifier F parameterized by W(j,i) in local training rounds, formulated as\nWt+1 = Wt - \u03b1 VR(F(W(j,i))),\nwhere \u03b1 is the learning rate. t \u2208 [Ti - 1] indicates the local training rounds."}, {"title": "C. Graph Imputation Generator with Versatile Assessor", "content": "To capture the potential cross-subgraph links, we design a graph imputation generator and incorporate it with a versatile assessor to explore a learnable potential graph G\u00b3 = (Vi,,X) for mending the cross-subgraph links.\nGraph Imputation Generator. To construct the globally-shared information without revealing raw data, the clients upload the processed embeddings {H(i,i)|i\u2208 [M;]} to the edge server at every K intervals of edge-client communication, where the original linked nodes remain proximate in the low-dimensional space. Next, the graph imputation generator performs the fusion on the processed embeddings to obtain the globally-shared information H\u2208 R|V|\u00d7c, where Vi is the number of all clients covered by Sj. Based on this, H\u00ba is denoted as\nHJ = [H(j,1) || ... ||H(j,3)]T.\nIn real-world application scenarios of FGL, it is possible for each node in clients to own potential cross-subgraph links, and it may be insufficient for clients to propagate features in multi-hop neighbors if missing these cross-subgraph links. In response to this problem, the graph imputation generator utilizes the distance to evaluate the node similarity and construct the global topology graph, referred to \u0100 = HH. Next, k most similar nodes are selected from this topology graph as potential cross-subgraph links, denoted by the set . To generate the potential feature vectors X' under the guidance of the globally-shared information, an autoencoder parameterized by \u03a6AE is used to explore overcomplete underlying representations from H\u00b9. Furthermore, to guarantee data privacy, the random noisy vector S is input to the autoencoder, and thus the output of the autoencoder is reconstructed as H = h(f(S)), where f(\u00b7) and h(\u00b7) are the encoder and decoder, respectively. It is noted that X = f(S) indicates the potential features expected to be extracted by the encoder. With the autoencoder, the random noisy vector is mapped to the same dimension as H\u00b9, and the output of the (l + 1)-th layer is defined as\nH(j,l+1) = \u03c3 (H(j,1) W(j,l+1) + b(j,l+1)),\nwhere Wi,1+1) \u2208 Rd\u0131xd\u0131+1 and bj,l+1) \u2208 Rdi are the layer-specific weights and biases, respectively. (.) denotes the activation function.\nVersatile Assessor. Since the conditional distribution of H relies on \u2611 and is independent of S, S \u2192 X \u2192 X \u2192 H in the autoencoder follows the Markov principle. Therefore, we design a versatile assessor parameterized by \u03a6As to supervise the quality of reconstruction data from the decoder, aiming to extract the expected underlying features X tailored for node classification. Considering the diversity of datasets, the assessor should be trainable to fit in specific tasks. Thus, the assessor adopts a fully-connected neural network to evaluate H\u00b3."}, {"title": "D. Negative Sampling and Graph Fixing", "content": "Negative Sampling. To extract more refined potential features, we develop a negative sampling mechanism to concentrate on the pertinent information for node classification. Based on the proposed versatile assessor, we first set a threshold \u03b8\u2208 (0,1) in every training iteration of the autoencoder and select the attributes in he that are less than 0. These attributes are deemed as negative and their feedbacks from the assessor are 0. Next, the zero-regularization is used to process these negatives, and thus both the autoencoder and the assessor can spotlight the representations that are meaningful for downstream tasks. Hence, the loss function of the assessor is updated and redefined as\n$\\mathcal{L}_{A S}=-\\frac{1}{\\left|v_{i}\\right|} \\sum_{v} \\log \\left(1-\\operatorname{Assor}\\left(h_{v}^{e}\\right)\\right)+\\log \\left(\\operatorname{Assor}\\left(\\mathbb{I}\\left\\langle \\pi_{v}<\\theta\\right\\rangle \\otimes e_{u}\\right)\\right]$\nwhere $e_{u}$ is a c-dimensional vector that judges whether $h_{v}^{e}$ / $h_{v}^{\\prime}$ is higher than $\\theta\\left(e_{u i}=1\\right)$ or not $\\left(e_{u i}=0\\right)$. is the element-wise multiplication."}, {"title": "E. SpreadFGL: Distributed Federated Graph Learning", "content": "In real-world application scenarios, a single edge server may encounter the problem of excessive costs and degraded performance as the number of clients expands, particularly when clients are geographically dispersed. To address this problem, we propose a novel distributed FGL framework, named SpreadFGL, that extends the FedGL to a multi-edge environment. The SpreadFGL is able to facilitate more efficient FGL training and better load balancing in a multi-edge collaborative environment. We consider that there are N edge servers, and an edge server S; is equipped with a global node classifier F; parameterized by Wj. Besides, a client only communicates with its closest edge server. There exist neighbor relationships among the servers, denoted by the matrix A \u2208 RN\u00d7N. If Si and Sj are neighbors, aij = 1; otherwise, aij = 0. Moreover, the parameter transmission is permitted between neighbor servers.\nIn SpreadFGL, the clients adopt the L-layer GNNs and conduct the feature propagation via Eq. (3) during the local training. The edge servers exchange information with the covered clients in each edge-client communication. At each K intervals of edge-client communications, the clients and their nearest edge servers collaboratively utilize the shared information to extract the potential links based on the proposed graph imputation generator and negative sampling mechanism. However, the potential cross-subgraph links strictly depend on the information provided by all clients. This not only violates the core idea of the SpreadFGL but also is impractical if the information is transmitted from the clients that are under the coverage of other servers. In light of these concerns, we design a weight regularizer during the local training. Based on trace normalization, the regularizer is used to enhance the network learning capability of the local node classifiers. Specifically, the loss function of the i-th client under the coverage of Sj is defined as\nLr = Ri(F(W(j,i)))\n= \u5225\u30ea\\sum_{u=1}^{\\left|V_{i}\\right|} \\sum_{r=1}^{c} Y_{u l}^{l} \\cdot H_{(3, i)}+Tr(W_{(j, i, l)}) W_{(j, i, l)}),\nwhere Tr(\u00b7) is the square matrix trace. W(j,i,L) indicates the parameters of L-th GNN layer for the local node classifier F. To better explore the potential cross-subgraph links by using the information from other servers, we adopt the topology structure at the edge layer to facilitate the parameter transmission between neighbor servers. This enables the information flow among clients via the gradient propagation at each K intervals of edge-client communication. Specifically, S; first aggregates the model parameters of its neighbor servers. Next, Sj averages the parameters and broadcast them to the covered clients. This process can be described as\nW; \u2190 1/(\\sum_{r=1}^{N} a_{rj} M_{r}) \\sum_{r=1}^{N} \\sum_{i=1}^{M_{r}} a_{rj} W_{(r,i)}.\nThe procedure of the proposed SpreadFGL is elaborated in Algorithm 1, whose core components have been described in detail before."}, {"title": "IV. PERFORMANCE EVALUATION", "content": "In this section, we first compare the proposed FedGL and SpreadFGL with state-of-the-art algorithms based on real-world testbed and graph datasets. Next, we conduct ablation experiments to further verify the superiority of the core components designed in the proposed frameworks."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel FGL-based framework named FedGL and its extended framework SpreadFGL, addressing the challenges of generating cross-subgraph links and single-node overloading. First, we design the FedGL to repair the missing links between clients, where a new graph imputation generator is developed that incorporates a versatile assessor and negative sampling mechanism to explore refined global information flow, extracting unbiased latent links and thus improving the training effect. Next, to alleviate the overloading issue at the edge layer, we extend the FedGL and propose the SpreadFGL with multi-edge collaboration to enhance the global information exchange. Extensive experiments are conducted on real-world testbed and benchmark graph datasets to verify the superiority of the proposed FedGL and SpreadFGL. The results show that the FedGL and SpreadFGL outperform state-of-the-art algorithms in terms of model accuracy. Further, through ablation experiments and convergence analysis, we validate the effectiveness of the core components designed in the proposed frameworks and the advantage of the SpreadFGL for achieving faster convergence speed."}]}