{"title": "Neur\u00f4nio artificial n\u00e3o-bin\u00e1rio com varia\u00e7\u00e3o de fase implementado em um computador qu\u00e2ntico", "authors": ["Jhordan Silveira de Borba", "Jonas Maziero"], "abstract": "O primeiros neur\u00f4nios artificiais qu\u00e2nticos seguiram um caminho similar aos modelos cl\u00e1ssicos, envolvendo apenas estados discretos. Neste artigo, introduzimos um algoritmo que generaliza o modelo bin\u00e1rio, manipulando a fase de n\u00fameros complexos. Propomos, testamos e implementamos um modelo de neur\u00f4nio que envolve valores cont\u00ednuos em um computador qu\u00e2ntico. Atrav\u00e9s de simula\u00e7\u00f5es, demonstramos que nosso modelo funciona em um esquema h\u00edbrido utilizando o gradiente descendente como algoritmo de aprendizado. Esse trabalho representa um novo passo na dire\u00e7\u00e3o da avalia\u00e7\u00e3o do uso de redes neurais implementadas eficientemente em dispositivos qu\u00e2nticos de curto-prazo.", "sections": [{"title": "1 Introduction", "content": "Artificial neural networks constitute a class of computational models that achieved a high success rate at specific tasks, as for example in pattern recognition. Although, in modern practical applications, artificial neural networks run usually as classic algorithms in conventional computers Tacchino et al. (2019), there is a rising interest in their application in quantum devices. Intrinsic properties of quantum mechanics such as the storage and representation of large data quantities as vectors and matrices provide exponential growth in both the storage and processing power Feldman e Rojas (1996); Schuman et al. (2017); Merolla et al. (2014); Sanz et al. (2018).\nAn artificial neuron model implemented in a current quantum computer was published in 2019 Tacchino et al. (2019). This model is based on the Rosenblatt neurons, limited to binary values $i_j, w_j \\in \\{-1,1\\}$ and utilized a learning rule based in hyper-planes. A possible improvement from this model is to extend it to continuous values.\nIn the literature about models of artificial neurons with continuous values, the sigmoid neuron is recognized as one of the main models implemented in modern neural networks. On the other hand, the gradient descent is the standard learning algorithm Nielsen (2015). Furthermore, in the literature about quantum neuron models, inspired by network models of biologic neurons in which the neurons are sensible not only to the amplitude of input signals but also to its phase, it was proposed to consider the phase of continuous values to encode the information, beyond the amplitude of the complex numbers Altaisky (2001); Peru\u0161 (2000). Sutherland even interprets the phase like \u201creal\u201d information and the amplitude as the level of confidence of the correspondent information Sutherland (1994).\nSo, our work proposes modifications in the binary model exploring properties from complex numbers that are inherent to quantum mechanics. Firstly, we encode the classic m-dimensions input vector in the quantum hardware using N qubits, on which $m = 2^N$, exploring the exponential advantage of quantum information storage. Secondly, we construct the quantum circuit using two different algorithms: first implementing a procedure directly by 'brute-force' through a phase rotation block, and then implementing an adaptation of the Hypergraph States Generation Subroutine (HSGS) algorithm.\nExperimentally, we implement a two-qubit version of the algorithms in the IBM processors available for cloud computing. We demonstrate that the proposed algorithm provides the results expected in classic approaches, as well as the optimization expected from HSGS algorithm if compared to the phase rotation block. We report the potential of our neuron model, we also simulated a training scheme to recognize simple patterns with 2+1 qubits.\nTherefore, this work provides an important contribution to the efficient implementation of machine learning applications through both quantum processing and quantum devices."}, {"title": "2 Theory", "content": null}, {"title": "2.1 Modeling the quantum circuit from neuron", "content": "An input vector with m dimensions is encoded using m coefficients to define a general function wave $|\\Psi_i\\rangle$. In the practice, for any arbitrary inputs and weights:\n$$\\overrightarrow{i} = \\begin{pmatrix} i_0 \\\\ i_1 \\\\ : \\\\ i_{m-1} \\end{pmatrix}, \\overrightarrow{w} = \\begin{pmatrix} w_0 \\\\ w_1 \\\\ : \\\\ w_{m-1} \\end{pmatrix}$$\nThen the quantum states may be defined as follows:\n$$|\\psi_i\\rangle = \\frac{1}{\\sqrt{m}} \\sum_{j=0}^{m-1} e^{i i_j} |j\\rangle, \\qquad |\\psi_\\omega\\rangle = \\frac{1}{\\sqrt{m}} \\sum_{j=0}^{m-1} e^{i w_j} |j\\rangle$$\nThe states $|j\\rangle \\in \\{|00 ... 00\\rangle, |00 ... 01\\rangle , ... , |11 ... 11\\rangle\\}$ constitute a basis, called computational basis, from the quantum processor and may be identified by decimal integers that are made by the respective binary string $|j\\rangle \\in \\{\\langle 0\\rangle, |1\\rangle, ..., |m \u2013 1\\rangle\\}$. Evidently, if we have n qubits in the register, so we have $m = 2^N$ basis states. Moreover, the information may be encoded in a uniform superposition of the entire computational basis since we may change the phase of complex numbers without changing its modulus. Hence, the present work has as its initial objective to develop a quantum circuit able to perform the following calculation:\n$$\\langle\\psi_i,\\psi_\\omega\\rangle = \\frac{\\overrightarrow{w} . \\overrightarrow{i}}{m}$$"}, {"title": "2.2 Implementation of unitary transformations", "content": null}, {"title": "2.2.1 Phase rotation block", "content": "Firstly, we define the phase rotation block $B_{N,j} (\\lambda)$ as a unitary transformation acting in the computational basis of N qubits as follows:"}, {"title": "2.2.2 HSGS", "content": "A more efficient solution is given by the algorithm called HSGS. This algorithm reduces the needed quantum resources in comparison to rotation blocks. This occurs mainly because even if the HSGS algorithm keeps showing an exponential cost, it optimizes the required multi controlled operators Tacchino et al. (2019).\nWorking first with the construction of the input operator $U_i$, starting from the total superposition state obtained via $H^{\\otimes N}$, the first step is to apply the operator $u_1$ to the coefficients of the states that have only p = 1 bit in the state |1) (for example |0...010...0)). Then we apply the required corrections to the states that show p = 2 using the multi controlled phase rotation $C^\\rho u_1$ until p = N. Only |0) remains unaltered. If needed, we may utilize $X^{\\otimes N}$ (NOT operator applied to all qubits) and $c_N u_1$ (multi controlled phase rotation among N qubits) to change the phase from the first state.\nAnalogously to the rotation blocks, it is necessary to make the process in the 'reverse' way for the weight operator. Aiming to take the weight state $|\\Psi_\\omega\\rangle$ to a balanced superposition state and then apply operators $H^{\\otimes N}$ and $NOT^{\\otimes N}$ in parallel."}, {"title": "2.3 Gradient descent algorithm", "content": "To start the discussion about the gradient descent algorithm, it is necessary to have some functions clearly defined. Starting with the activation function, which is the mathematical function that maps our inputs and outputs. For any input and weight vectors:\n$$|\\psi_i\\rangle = (e^{i x_0} e^{i x_1} ... e^{i x_{m-1}})^T = \\frac{1}{\\sqrt{m}}$$\n$$|\\psi_\\omega\\rangle = (e^{i w_0} e^{i w_1} ... e^{i w_{w-1}})^T = \\frac{1}{\\sqrt{m}}$$\nThat is, the inner product is given by:\n$$\\langle\\psi_\\omega \\psi_i\\rangle = \\sum_{j=1}^{m-1}e^{i(\\alpha_j)}, \\qquad \\alpha_j = x_j - w_j$$\nWhere $\\alpha_j = x_j - w_j$. So the output of the quantum circuit will be:\n$$|<\\psi_\\omega|\\psi_i>|^2 = |\\sum_{j=1}^{m-1}e^{i(\\alpha_j)}|^2$$"}, {"title": "3 Results", "content": "All algorithms were implemented in Python using the NumPy library (https://numpy.org/) for analytical calculations and the Qiskit development kit (https://qiskit.org/) for Python, which provides not only simulators but also access to real devices in the IBM Quantum Experience cloud (https://quantum-computing.ibm.com/). All tests on simulators and the quantum version of the perceptron were run from the cloud computing resource made available by Google, called Google Colab (https://colab.research.google.com/). Through this platform, Google provides an Intel(R) Xeon(R) CPU @ 2.30GHz with 2 cores and 12GB of RAM. Qiskit version 0.16.1 was also used in this environment, which has the following version of its modules:"}, {"title": "3.1 Inner product", "content": "To compare the results obtained with the proposed algorithms (rotation blocks and HSGS), the following quantitative measure of mean discrepancy, proposed by Tacchino et al. (2019), was used:\n$$D = \\sum_{k=1}^{n} \\frac{|O_{IBM}^{(k)} - O_{ideal}^{(k)}|}{n}$$\nwhere for n calculated outputs, we make the difference between the ideal output $(O_{ideal}^{(k)})$, classically calculated, and the output of real IBM devices $(O_{IBM}^{(k)})$, obtained through Qiskit. Due to limitations imposed on the use of IBM hardware, such as utilization queues, we limit it to the case with N = 2 qubits.\nDue to time limitations and the possibility of constructing infinite continuous vectors, it was chosen to work with only 8 randomly generated vectors of continuous values. So that it becomes possible to combine the 8 vectors to calculate $n = 8^2 = 64$ different inner products. The code was run 5 times on 3 three different processors, 3 times it was only on ibmq_vigo as it is the best of the 3 processors compared to the analytic solution. The result is shown in Table 1.\nThe mean difference between the results of the algorithms was $(-1.10 \\pm 3.27) \\times 10^{-2}$, that is, the standard deviation was greater than the mean in the module. This indicates that we can expect that depending on the execution, one algorithm or another may do a little better. This isn't what is expected when the HSGS was originally proposed by Tacchino et al. (2019). A better investigation is needed, but one possibility of explanation is the improvement of the quantum processors structure - HSGS provides a significant advantage on processors that do not have native available multi-qubit operations - which reduces the advantage of the HSGS algorithm."}, {"title": "3.2 Evaluation of the binary case", "content": "For comparison, we reproduced the binary calculus, where instead of using the operator $u_1$ responsible for performing a generic variation 0 in the phase, an operator Z is used that simply performs a change of signal. We used 16 different unit and binary vectors made by N = 2 qubits to calculate 256 different inner products, and executed 5 times in 3 different processors, the processor that exhibited the best results we executed 2 more times. The result can be seen in Table 2.\nWe found a result qualitatively equal to the previous case with the mean difference between the discrepancy of the algorithms of $(1.40 \\pm 8.83) \\times 10^{-3}$. To confirm that the proposed algorithms can reproduce results known in the literature, we used our algorithm for the inner product in the hybrid scheme proposed by (Tacchino et al., 2019) for the binary case. More details about the scheme can be read in the original work.\nIt was decided to work with a perceptron of 4 inputs that only require 2+1 qubit (considering the auxiliary qubit) and implement it in the ibmq_vigo device, which presented the better results in previous tests. The process proceeded as follows: First, a weight vector was randomly defined, and then from this vector, a training set was randomly constructed consisting of 5 vectors that result in a positive result and 50 that result in a negative output. After generating the training data, the weight vector was then 'forgotten', and an initial weight vector was randomly generated to execute the training.\nThus, it is expected that during training the perceptron is able to recover the objective weight vector used in the construction of the training set. To quantify this learning process, a measure called affinity was used, which is the square of the inner product between the current weight vector and the objective, being then an affinity 1 when the two vectors are identical.\nIt is also worth mentioning that at each new trial, the algorithm shuffles the order of the training data, training in a new random sequence. Finally, with the same training data set, the network was trained 59 times, restarting with an initial weight vector defined also randomly. The training also has a maximum limit of 50 steps for each execution. The average result for each step result is illustrated in Figure 4. And we can see behavior that indicates that the perceptron is able to learn as expected."}, {"title": "3.3 Simulation", "content": "Considering that the sigmoid is a sensitive neuron model, and in order to test the feasibility of this model in quantum computers in the long term, to simulate the sigmoid it was chosen to use the simulator provided by Qiskit called 'statevector _simulator'. This simulator does not include noise, it is a simulator of an ideal quantum circuit."}, {"title": "3.4 Implementation", "content": "Considering that it is a naturally sensitive model to noise, it was thought to use a quantum device that would perform better, despite the long queues. To compensate, the training set was reduced to 50 inputs and the device ibmq_santiago was used.\nQuantum volume, short for QV (quantum volume), is a metric that IBM has defined to measure the performance of its real quantum computers. Therefore, we can see it as quantification of this performance, and the higher the QV, the better the performance of the device. Two of the three devices used so far (ibmq_valencia and ibmq_vigo ) have QV = 16, while ibmq_5_yorktown has QV = 8. The choice of ibmq_santiago was due to having QV = 32.\nThat said, it was necessary to remove the stopping condition for training based simply on the increase in the cost function since it presented an oscillation from the beginning. The result can be seen in Figure 6. At the same time, the same code was executed in another device, ibmq_5_yorktown (QV = 8), due to smaller queues. The obtained results can be checked out in Figure 7.\nGraphically, we can see that there is an oscillatory behavior. And calculating the average of the steps that were possible to be performed in real quantum devices, 10 in ibmq_santiago and 17 in ibmq_5_yorktown, we have that the cost value was in average $(4,90 \\pm 0,23) \\times 10^{-2}$ and $(2,54 \\pm 0,25) \\times 10^{-2}$, approximately the same initial values $(4.59 \\times 10^{-2}$ and $2.38 \\times 10^{-2})$.\nThat is, although we show that the model works in an ideal quantum device, it did not appear to be capable of learning in a real quantum device. The most likely hypothesis is that it is due to noise. In order to try to improve this model targeting current or short-term devices, we could think about some hypotheses similar to the one discussed how to improve the model implemented in the simulator, that is, change the learning rate, the training dataset, or even the learning algorithm implemented.\nAnother point is that while the model in the simulator had approximately 4,000 steps, in the quantum device we had time to perform only less than 20 due to the high number of calculations that need to be performed on a quantum device and IBM's access"}, {"title": "4 Conclusion", "content": "Therefore, synthesizing the results obtained during this work, based on a 2019 article Tacchino et al. (2019), two algorithms were implemented to calculate the inner product between vectors in a quantum computer. However, differing from the results obtained in the literature, it was not possible to verify a significant difference between the results obtained by each algorithm, that is, the discrepancies obtained when calculating the same inner product in the same device, but using the two different algorithms, did not show a significant difference between each other. Among the hypotheses raised to explain this discrepancy, the main one is the architecture of quantum processors.\nFor the proposed hybrid machine learning model, when implemented in the simulator provided by Qiskit, it presented satisfactory results that demonstrate a good potential in terms of learning capacity. As for the implementation in real devices, there are still a lot of challenges to make use of real quantum devices: a significant amount of noise, limitations in the number of qubits, the time needed to submit the circuit and obtain the results, among others.\nThis result sheds light on some important questions. The first of a technical nature, comparing the results obtained between simulators and real devices, shows us the importance of the noise mitigation, both the development of theoretical tools and the"}]}