{"title": "CASE-BENCH: CONTEXT-AWARE SAFETY BENCHMARK FOR LARGE LANGUAGE MODELS", "authors": ["Guangzhi Sun", "Xiao Zhan", "Shutong Feng", "Philip C. Woodland", "Jose Such"], "abstract": "Aligning large language models (LLMs) with human values is essential for their safe deployment and widespread adoption. Current LLM safety benchmarks often focus solely on the refusal of individual problematic queries, which overlooks the importance of the context where the query occurs and may cause undesired refusal of queries under safe contexts that diminish user experience. Addressing this gap, we introduce CASE-Bench, a Context-Aware SafEty Benchmark that integrates context into safety assessments of LLMs. CASE-Bench assigns distinct, formally described contexts to categorized queries based on Contextual Integrity theory. Additionally, in contrast to previous studies which mainly rely on majority voting from just a few annotators, we recruited a sufficient number of annotators necessary to ensure the detection of statistically significant differences among the experimental conditions based on power analysis. Our extensive analysis using CASE-Bench on various open-source and commercial LLMs reveals a substantial and significant influence of context on human judgments (p <0.0001 from a z-test), underscoring the necessity of context in safety evaluations. We also identify notable mismatches between human judgments and LLM responses, particularly in commercial models within safe contexts.", "sections": [{"title": "1 INTRODUCTION", "content": "Aligning large language models (LLMs) with human values to ensure the safe use of LLMs is a primary focus of current research in this field, and it is also a crucial prerequisite for their widespread application (Christiano et al., 2017; Ouyang et al., 2022; Bai et al., 2022; Hendrycks et al., 2021a). Benchmarks have been proposed to evaluate the quality of alignment, focusing on different aspects of LLM safety that are mostly triggered by a single harmful user query (Shaikh et al., 2023; Wang et al., 2024; Parrish et al., 2022; Cui et al., 2023; Qi et al., 2023; R\u00f6ttger et al., 2023; Zou et al., 2023; Lin et al., 2023; Souly et al., 2024). Systematic and unified benchmarks have also been proposed for LLM safety evaluation (Vidgen et al., 2023; Ji et al., 2023; Mazeika et al., 2024; Xie et al., 2024; Cui et al., 2024).\nThe aforementioned benchmarks, however, predominantly focus on the identification of refusal of individual problematic queries. This leads to over-refusal behaviours in many LLMs, e.g., refusing to answer questions that users may think should be answered, which may substantially impact user experience. Although this issue has been acknowledged and highlighted in recent studies (R\u00f6ttger et al., 2023; Arditi et al., 2024), these studies still focus on judgments based on isolated queries and overlook the contextual information in which queries occur (Weidinger et al., 2023; Leveson, 2016). In fact, context plays a crucial role in determining whether it is safe to respond to a query, as"}, {"title": "2 PRELIMINARY: CONTEXTUAL INTEGRITY THEORY", "content": "Contextual Integrity (CI) theory, initially developed by Nissenbaum (2004), has been extensively adopted in computer science to analyze the appropriateness of information flows within different contexts (e.g. Apthorpe et al., 2018; Abdi et al., 2021; Mireshghallah et al., 2023; Kumar et al., 2020)."}, {"title": "3 CASE-BENCH", "content": "Instead of directly asking LLMs to respond to user queries, CASE-Bench evaluates whether LLMs can make safety judgements based on contexts that align well with human judgments. Such an evaluation provides performance indications for moderation systems or safety reward models.\nCASE-Bench contains 900 queries-context pairs, i.e. 450 controversial and potentially harmful queries with 2 distinct contexts per query that are automatically generated and then manually revised. One of the contexts is intended to be safer than the other for each query, as detailed in \u00a74.2. CASE-Bench also contains human annotations on whether responding to each query is safe or unsafe given each context from 2,000+ high-quality annotators. Each query-context pair as a task received 21 annotations which is determined by statistical power analysis, as described in \u00a73.2. This process resulted in a total of 47,000+ human annotations."}, {"title": "3.1 APPLYING CI THEORY TO CASE-BENCH", "content": "To tailor CI parameters to our research needs, slight modifications are made to the original CI framework. Specifically, extending the original CI parameters such as the sender and the recipient, we specified in greater detail by introducing sub-parameters. This refinement enhances their concreteness and facilitates the use of automated tools such as GPT-40 to generate more varied contexts.\nSender: The sender is the chatbot, as we are evaluating the appropriateness of its response flows to the user. Sender contains two sub-parameters to enable automated tools to generate more diverse contexts: Nature of the Interaction and Platform Type. The Nature of the Interaction determines whether the chatbot is intended for general purposes or customized for a specific domain, such as research, education, financial services, or role-playing. Platform Type, specifies the medium through which the chatbot operates, such as a website, mobile application, social media platform, or dedicated support system.\nRecipient: The recipient refers to the user interacting with the chatbot in this paper. The Recipient contains two sub-parameters: Type and Background. Type identifies the nature of the recipient, such as an existing user, a potential user, or an anonymous user, etc. Background provides additional contextual information about the recipient, such as their previous interactions with the chatbot, the purpose of their query, and relevant demographic details. This background information helps to"}, {"title": "3.2 NUMBER OF ANNOTATORS FROM POWER ANALYSIS", "content": "We employed a between-subjects design (Charness et al., 2012), a well-established experimental research method commonly used in medicine, psychology and human-computer interaction studies to assess user behaviour and judgments (Charness et al., 2012). Following the design, we recruited distinct groups of annotators, with each group assigned randomly to a single experimental condition. This design enables a clear comparison between conditions while minimizing potential carryover, ordering or learning effects that could arise from exposure to multiple conditions.\nPrevious benchmarks (Xie et al., 2024; Cui et al., 2024; Ji et al., 2023) lack metrics or standards to determine the sufficient number of annotators. To address this issue, we propose to apply statistical power analysis (Cohen, 1992) to derive the optimal sample size for reliably detecting the effects of context on safety evaluation. This approach ensures a rigorous balance between statistical power and resource allocation, improving the quality and representativeness of our crowd-sourced dataset and making our findings more reliable and generalizable.\nSpecifically, the power analysis was conducted using G*Power 3.1 (Erdfelder et al., 1996). We selected ANOVA: Fixed effects, omnibus, one-way model (Girden, 1992) as it is commonly applied to compare means across multiple groups and determine if there are significant differences between them. In our study, this approach was appropriate as we aimed to assess whether different experimental conditions (e.g., with and without context) lead to meaningful changes in annotators' responses and perceptions. We assumed an effect size of f = 0.4, which reflects a moderate to large effect size, often deemed sufficient in social and behavioural research (Cohen, 2013). We set the alpha level (Type I error rate) at a = 0.05, providing a 5% chance of falsely rejecting the null hypothesis. Additionally, we aimed for a power of 0.8 (80%), ensuring an 80% probability of correctly rejecting the null hypothesis if a true effect exists. The experimental design involved five conditions: No context, Safe context (GPT generated), Unsafe context (GPT generated), Safe context (manually revised), and Unsafe context (manually revised).\nBased on these parameters, the power analysis indicated that a total sample size of 80 annotators would be needed across all conditions (results in Appendix E.1), translating to 16 annotators per task. However, recognizing that ANOVA assumes normally distributed data, we accounted for potential deviations from normality by increasing the sample size by 10% to 20% (Box, 1953). To safeguard against low-quality responses or dropouts, as we apply high-quality data measures and filters as explained later on, we increased the sample size to 21 annotators\u00b3 per task. With 2,250 tasks in total, we recruited around 2,000 high-quality annotators, as detailed below, assigning each an average of 25 tasks to prevent task overload and promote data quality."}, {"title": "4 DATA CREATION PIPELINE FOR CASE-BENCH", "content": ""}, {"title": "4.1 QUERY SELECTION", "content": "The data creation pipeline is shown in Fig. 2 starting from query selection. CASE-Bench adopts the queries from SORRY-Bench (Xie et al., 2024), which consists of 450 unsafe instructions distributed"}, {"title": "4.2 CONTEXT GENERATION", "content": "For each query, we generated both a safe context for which the chatbot should provide the response and an unsafe context for which the chatbot should refuse to reply. To achieve this, a two-stage approach was adopted where we first use GPT-40 to automatically generate the context, and then manually revise and curate the contexts. This process highlights the CI framework's key role in guiding structured, consistent automatic context generation and enabling manual revisions to reduce ambiguity, prevent over-moderation, and ensure clarity.\nAutomatic Context Generation For each given query, a template-based approach was employed using GPT-40 to generate both safe and unsafe conversational contexts. This process was guided by CI theory, with the parameters described in \u00a73.1 forming the foundation for generating the structured contexts. The prompt provided to GPT-40 followed a specific sequence: first, an overview was given to explain the task of generating context for the query. Then, potential safety issues related to the query were disclosed, highlighting key risks. Afterwards, each CI parameter was described in detail, and GPT-40 was instructed to generate the context accordingly. The generated output strictly followed a predefined structured format, ensuring consistency and completeness across all contexts.\nManual Revision While the GPT-40-generated contexts provided a solid foundation, manual review and revision were essential to ensure accuracy, consistency, and the overall quality of the contexts. GPT-40, like other LLMs, employs strict self-safeguarding mechanisms\u2014often referred to as \u201csafety refusal behaviours\u201d or \u201ccontent moderation filters\u201d\u2014to prevent the generation of harmful or unethical content (Anwar et al., 2024; OpenAI). Consequently, the model often moderates the unsafe query into a safe one before generating the safe context, which completely overwrites the user intention. To address this, we employed two researchers to review all generated contexts carefully, ensuring they align with the intended safe or unsafe conditions.\nThe revision process involved regular discussions between the researchers to review and improve the contexts, ensuring accuracy and appropriateness for the study. In many cases, where the GPT-40 output was too conservative or incorrect, the researchers replaced them with manually crafted contexts."}, {"title": "4.3 \u0391\u039d\u039dOTATION PROCESS", "content": "We recruited annotators for our study using Amazon Mechanical Turk (MTurk) and developed a user-friendly interface (see Fig. 6) to facilitate efficient and convenient data annotation. To ensure high-quality annotations, we limited participation to workers with an approval rate above 98% and over 10,000 approved HITs (tasks). Furthermore, all eligible annotators were required to complete a tutorial designed to familiarize them with the task, and only those who answered all tutorial questions correctly at once could proceed to the main annotation process (see Appendix D.1 for tutorial details).\nTo maintain annotation quality, the task was split into smaller batches with randomly inserted attention check questions to filter out low-quality annotators. Each task was annotated by 21 annotators, and annotators who participated in the previous tasks were excluded from further annotation to avoid bias. Additionally, six pilot studies were conducted to refine the interface and tutorial materials, ensuring a smooth and effective annotation process. For instance, pilot annotators provided feedback requesting a formal definition of \"safety risks\u201d to enhance their understanding. In response, we incorporated a detailed description of safety risks and a list of categories considered into the tutorial."}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 INFLUENCE OF CONTEXT ON HUMAN JUDGMENT", "content": "We study the effect of context by considering the five conditions defined in \u00a73.2. Specifically, we employed two statistical methods to analyze the influence of context on human judgment quantitatively: the z-test (Lawley, 1938) and the Kruskal-Wallis (K-W) test (Vargha & Delaney, 1998) and. The z-test is used to compare the overall safety ratings across all tasks between two conditions. In contrast, the K-W test is used to evaluate the significance of differences across all conditions on a task-by-task basis, where one task refers to one query under a specific condition.\nTo adjust for multiple comparisons, a Bonferroni correction was applied, setting the significance threshold at p < 0.0125 (0.05/4 tests)."}, {"title": "P(safe) = \\frac{\\Sigma_{y \\in Y_{safe}} P_{LLM}(y)}{\\Sigma_{y \\in Y_{safe} \\cup Y_{unsafe}} P_{LLM}(y)}", "content": "Normalized token probabilities: For open-source models where logits can be obtained, the normalized probability for safe and unsafe judgment can be computed using Eqn. (1).\n(1)\nwhere $P_{LLM}(y)$ is the original LLM output distribution and $Y_{safe}$ is the set of tokens that map to the word \"safe\". The LLM is prompted to respond only \u201csafe\u201d or \u201cunsafe\u201d.\nThe agreement between LLMs and human annotators is measured via the overall Accuracy and the Recall rate for each class, where the recall rate reflects the performance differences under safe and unsafe contexts. As non-binary safety ratings are also obtained, in addition, we measure the agreement on the degree of harmlessness between humans and LLM judgments using the Pearson Correlation Coefficient (PCC) and Binary Cross Entropy (BCE) scores. The BCE score was averaged across the benchmark where the score for each sample i is given by Eqn (2)."}, {"title": "BCE = -r_i \\log r_i^{LLM} - (1 - r_i) \\log(1 - r_i^{LLM})", "content": "(2)\nwhere $r_i$ and $r_i^{LLM}$ are human safety ratings and LLM judges respectively, which are normalised between 0 and 1 and are the higher the safer. Note that PCC and BCE scores are not reported when using binary classification prompts."}, {"title": "5.2.1 RESULTS", "content": "The primary results are shown in Tab.2 with smaller LLMs in Appendix G. Among the models, the Claude-3.5-sonnet model achieves the best accuracy and PCC with a good balance between safe and unsafe contexts. Claude-3.5-sonnet achieves the highest recall rate in safe contexts, reflecting that it is better at understanding the context and making better safety judgments under different contexts. This also indicates that, when Claude-3.5-sonnet is tasked with verified context in the real world, it is more likely to succeed in providing information and suffer less from over-refusal problems compared to GPT-40 when used off-the-shelf. Although the best accuracy for most open-source models is achieved from the normalized probabilities, they result in a very high BCE as the probabilities are far"}, {"title": "5.3 ABLATION STUDIES ON CI PARAMETERS", "content": "We selected the best performing LLM, Claude-3.5-sonnet, as well as an example of open-source, Llama-3, and close-source LLM, GPT-40-mini, to analyze the influence of each CI parameter on the LLM judgments. The recall rates using different subsets of CI parameters are shown in Fig. 5. In all cases, the most influential parameter is the recipient (i.e. the type and background of the user).\nThis finding aligns with prior research on CI, which highlights the recipient's role in determining appropriate information flows (Abdi et al., 2021). It also confirms the insights highlighted by the sociotechnical safety framework proposed by Weidinger et al. (2023), which asserts that AI safety cannot be fully assessed without considering the specific context\u2014particularly the nature of the user interacting with the chatbot. Furthermore, from the human-computer interaction (HCI) perspective, which emphasizes user-centred design (Shumanov & Johnson, 2021; Schanke et al., 2021), chatbots must make nuanced safety judgments based on the recipient's specific characteristics. This involves tailoring responses to align with the user's level of expertise, intent, and contextual background. By doing so, systems can ensure that the information provided is appropriate and minimizes potential risks associated with misuse or misunderstanding."}, {"title": "6 DISCUSSION", "content": ""}, {"title": "6.1 CONTEXT RELIABILITY", "content": "The context in CASE-Bench is assumed to be derived from verified practical mechanisms. We used various techniques to formalize the context (\u00a73) and to improve data quality (\u00a74). In practice, reliable contexts can be obtained from a verified corporate knowledge base using APIs to access internal documents, policies, and logs, which are maintained and authenticated by the organization or system. For example, the recipient parameter can be verified via role-based access control (RBAC), and confidentiality via encryption. The CI framework adopted by CASE-Bench also allows system builders to first focus on the contextual safety aspect of LLMs using CASE-Bench, followed by adaptation to specific domains by extracting formalized and reliable context from various sources, such as electronic health records (EHR) for healthcare LLMs. As a future research direction, the context can be further validated with a preprocessing pipeline that takes the context provided by the administrator and verifies it against existing documents."}, {"title": "6.2 JAILBREAKING WITH PROMPT ATTACKS", "content": "The context in CASE-Bench is assumed to be separate from the user prompt provided during interactions with the system. This separation can be implemented through several mechanisms: (1) Automatic detection and prompt moderation when a user tries to override the context by prompt attacks Liu et al. (2024); Ayub & Majumdar (2024). (2) Hierarchical prompting systems, which train models to prioritize instructions that fix the context, as demonstrated in Wallace et al. (2024); and (3) Methods such as adapters Mo et al. (2024); Liu et al. (2022) or soft prompts Ostermann et al. (2024), which can be trained to distinguish between verified context and user input, thereby mitigating the risk of jailbreaking attempts via user-injected prompts. While jailbreak prevention is not the primary focus of this work, CASE-Bench can serve as a test bench for the aforementioned jailbreak counteraction techniques."}, {"title": "7 RELATED WORK", "content": ""}, {"title": "7.1 SAFETY ALIGNMENT BENCHMARKS", "content": "Ensuring the safety of LLMs has become a pivotal focus in recent Al research, prompting the development of various evaluation benchmarks with datasets (Ji et al., 2023; Dai et al., 2024; Gehman et al., 2020; Wang et al., 2024; Qi et al., 2023; Cui et al., 2023; Vidgen et al., 2023; Lin et al., 2023; Zou et al., 2023; Shen et al., 2023; Huang et al., 2023; Mazeika et al., 2024; Souly et al., 2024; Shaikh et al., 2023). These papers investigated a range of issues concerning the generation of harmful content by LLMs, each utilizing distinct criteria and safety taxonomies. One set of them focuses on evaluating and enhancing the ability of LLMs to detect and appropriately respond to adversarial \u201cred-teaming\u201d prompts (Ji et al., 2023; Dai et al., 2024; Gehman et al., 2020; Wang et al., 2024; Cui et al., 2023; Vidgen et al., 2023; Lin et al., 2023; Mazeika et al., 2024). Another set explores methods to bypass model safety mechanisms, thereby contributing to improved security by identifying vulnerabilities and refining safeguards to prevent harmful or unethical uses (Qi et al., 2023; Zou et al., 2023; Shen et al., 2023; Huang et al., 2023; Souly et al., 2024). However, this prior work predominantly focuses on binary classifications of prompts as safe or unsafe, often ignoring contextual factors in safety decision-making. In contrast, our study is the first to evaluate safety by incorporating context into the assessment."}, {"title": "7.2 OVER-REFUSAL ISSUES", "content": "Over-refusal, wherein LLMs incorrectly or excessively refuse to respond to user inputs, has been identified in prior research (Bianchi et al., 2023; R\u00f6ttger et al., 2023). This often arises when LLMs"}, {"title": "8 CONCLUSION", "content": "This paper emphasizes the importance of context in LLM safety evaluation by proposing the Context-Aware SafEty Benchmark (CASE-Bench). CASE-Bench formalizes context descriptions using CI theory and provides non-binary safety ratings reflecting the degree of uncertainty in human judgments. Extensive analyses performed on CASE-Bench demonstrate substantial and significant influence of context on human judgments. Notable mismatches between human and LLM judgments due to over-refusal were also reflected, emphasizing the necessity and challenges in considering context in LLM safety judgments. Limitations and directions for future work are detailed in Appendix A."}, {"title": "A DISCUSSION OF LIMITATIONS AND FUTURE WORKS", "content": "In this section, we provide the detailed limitations of CASE-Bench and outline potential directions for future work.\nIn the query selection process, we adapted queries from Sorry-Bench (Xie et al., 2024), leveraging its balanced taxonomy and other advantages, as detailed in Section 4.1. However, some queries remain inherently unsafe, particularly those that do not exhibit meaningful behavioural changes even when provided with safe contexts (as shown in Fig. 3). Attempting to ensure safety or create entirely safe contexts for such queries proves both impractical and inefficient. Future work may address this limitation by incorporating datasets that include predominantly unsafe or controversial queries or by curating new datasets specifically designed for this purpose.\nCASE-Bench assumes that the context provided is verified and reliable. Future research could explore alternative methods for retrieving contextual information using the CI framework. For instance, verified recipient information and recipient background details could be sourced directly from administrators, or context could be extracted from multimodal inputs such as documents, videos, or environmental recordings, considering the promising direction of LLMs continuing to evolve and acquire robust multimodal capabilities.\nWith such verified contextual information, it may also be possible to counteract jailbreaking or injection attacks by identifying and rejecting fabricated information that contradicts the verified context. Enhanced context comprehension would allow LLMs to make more accurate safety judgments, even in adversarial settings.\nFurthermore, the advanced contextual understanding exhibited by modern LLMs could prioritize their use in domain-specific or task-specific agents, enabling more reliable decision-making aligned with safety-critical requirements. Future research should explore these possibilities to further enhance the robustness and versatility of CASE-Bench.\nWe recognize the potential additional efforts in collecting data annotations since the annotator needs to read the context, but would like to argue that these are necessary and valuable efforts for domain-specific LLM applications to be both safe and helpful."}, {"title": "B CATEGORY INDICES", "content": "SORRY-Bench (Xie et al., 2024) introduced a 45-class safety taxonomy designed to examine refusal behaviours. This comprehensive classification system facilitates a more precise identification of diverse potential safety risks while providing increased flexibility. To enhance clarity and streamline the process of referencing or mapping the results from our visualized plots and tables, we have reproduced the category indices in Tab.3."}, {"title": "C PROMPT USED IN THIS PAPER", "content": "In this section, we provide details on the prompts used for context generation (see Appendix C.1) and for evaluating LLM judgments (see Appendix C.2)."}, {"title": "C.1 PROMPT USED FOR CONTEXT GENERATION", "content": "Safe Context Generation.\nAssume you are a judge and one user is talking to a chatbot in the context. The user asks {query}. Generate a context using the Contextual Integrity parameters defined below so that it is completely **appropriate** and **acceptable** for the Chatbot to answer the user's query. Make the context as detailed as you can.\nContextual integrity parameters can be used to generate the context in which information flows from the chatbot to the user. Your generated context should contain the following parts:"}, {"title": "C.2 PROMPT FOR LLM-AS-A-JUDGE", "content": "Binary Classification.\nYou are now acting as a judge that determines whether it is safe and appropriate for a chatbot to respond to the following query from a user, given the specific context. Note that the safety of the query is highly dependent on the context, so your judgment should be based on the analysis of the context, assuming anything written in the context is already verified.\nYou are given the following query:\n{query}\nThe context where this query happens is:"}, {"title": "D DATA CREATION PIPELINE DETAILS", "content": ""}, {"title": "D.1 TUTORIAL", "content": "When deploying the annotation tasks on Amazon MTurk, we provided a tutorial that clearly and comprehensively explained the study's background. This was followed by a set of six questions designed to assess the annotators' understanding. Only those who successfully answered all the questions in a single attempt were permitted to proceed to the annotation phase, ensuring that all participants were adequately informed and qualified for the task."}, {"title": "D.2 CONTEXT REVISION PROCESS", "content": "To ensure the quality and reliability of the generated contexts, a systematic revision process was carried out by two researchers. First, both researchers independently reviewed each context generated by GPT-40, evaluating them for accuracy, coherence, and alignment with the intended safety category (safe or unsafe). Regular discussions were held to compare revisions and resolve any disagreements. This collaborative process allowed for diverse insights and thorough cross-checking. During these discussions, multiple cycles of feedback and revisions were implemented. One researcher proposed changes, which were then reviewed and refined by the other, focusing on clarity, relevance, and the suitability of the context. In cases where GPT-40 generated overly conservative or inaccurate unsafe contexts due to its built-in safety mechanisms\u2014 the researchers manually replaced the content with more appropriate and representative contexts.\nAfter completing the initial revisions, a third-party reviewer, a domain expert with extensive knowledge of ethics and AI safety, was enlisted to perform a final evaluation. This expert, with significant experience in the field, conducted a thorough review of all contexts to ensure they adhered to the study's standards and accurately reflected the intended safe and unsafe scenarios. Following this expert review, both researchers conducted a final check to confirm consistency across all contexts and alignment with the predefined structure. The manual revision phase took place from June to August 2024."}, {"title": "D.3 CONTEXT ANNOTATION UI", "content": "Fig. 6 illustrates the custom user interface (UI) developed for Amazon MTurk annotators to facilitate their task of providing judgments on chatbot queries. The UI consists of a structured context section that includes key details such as the nature of the interaction, platform type, user type, and chatbot's purpose. Annotators are instructed to carefully consider this context when evaluating whether the chatbot should respond to a given query. Below the query, annotators can select between \"Respond\" or \"Refuse\" before submitting their decision. This intuitive layout ensures that judgments are provided efficiently and in alignment with the contextual information presented."}, {"title": "E STATISTICS", "content": ""}, {"title": "E.1 SAMPLE SIZE CALCULATION USING G*POWER", "content": "We determined the required number of annotators per task through a power analysis, using the widely recognized tool G*Power. This tool automatically calculated the necessary sample size based on the"}, {"title": "E.2 OVERVIEW OF STATISTICAL METHODS USED", "content": ""}, {"title": "E.3 POST-HOC ANALYSIS FOR KRUSKAL-WALLIS TEST", "content": ""}, {"title": "E.3.1 CASE STUDY OF RELIGION PROMOTION CATEGORY", "content": "In this section, we visualize the category \"religion promotion\u201d with the highest mean K-W statistic values shown in Fig. 3, which highlights the impact of adding context compared to the \"no context\" condition. This category represents a significant shift in human annotations when context is provided, as the K-W test shows statistical significance for all tasks within this category.\nParticularly, for tasks where unsafe context was introduced, there was a notable decrease in the proportion of users selecting that the chatbot should respond per task. The addition of unsafe context significantly influenced user decisions, as illustrated in Fig. 9. This suggests that the presence of unsafe context altered users' expectations and preferences for how the chatbot should behave, demonstrating the critical role context plays in shaping user responses."}, {"title": "E.3.2 CASE STUDY OF SOCIAL STEREOTYPE PROMOTION", "content": "In this section, we randomly selected one non-significant category to provide a more detailed analysis of those that did not show significance in the K-W analysis, as illustrated in Fig.3. The three categories that did not show significant differences across the 5 conditions, based on task-specific analysis, are: evasion of law enforcement, non-sexual explicit content generation, social stereotype promotion.\nFor the tasks categorized under \"Social Stereotype Promotion\", Fig. 10 provides a detailed visualization of human annotations for each query. Out of the 10 tasks, the K-W test identified statistically significant differences for only four tasks.\nFor tasks included in this category, when the unsafe context was added, the impact of the manually revised unsafe context was less detrimental than expected. In certain tasks, more participants actually rated the chatbot's responses as safe under these contexts. This suggests that the harmful nature of some queries may lead participants to remain cautious and hesitant to change their opinion, regardless of the context provided.\nThe analysis indicates that for inherently harmful queries, participants are reluctant to shift their perception and continue to be cautious in their judgments of whether the chatbot should respond."}, {"title": "F COMPUTING RESOURCE STATEMENTS", "content": "Our experiments used 2 Nividia A100 GPUs to perform inference for open-source LLMs. Each inference across 900 samples took 3-6 hours."}, {"title": "GADDITIONAL LLM RESULTS", "content": "In this section, we provide details of additional experiments conducted using smaller LLMs, including Llama3-8B-Instruct, Mistral-7B-Instruct-v0.2, and Zephyr-7B-Beta, among others. The final results are summarized in Tab.4. Overall, these smaller LLMs demonstrate worse performance compared to their larger counterparts."}]}