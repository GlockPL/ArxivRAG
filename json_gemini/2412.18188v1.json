{"title": "On the Applicability of Zero-Shot Cross-Lingual Transfer Learning\nfor Sentiment Classification in Distant Language Pairs", "authors": ["Andre Rusli", "Makoto Shishido"], "abstract": "This research explores the applicability of cross-lingual\ntransfer learning from English to Japanese and\nIndonesian using the XLM-R pre-trained model. The\nresults are compared with several previous works, either\nby models using a similar zero-shot approach or a fully-\nsupervised approach, to provide an overview of the zero-\nshot transfer learning approach's capability using XLM-\nRin comparison with existing models. Our models\nachieve the best result in one Japanese dataset and\ncomparable results in other datasets in Japanese and\nIndonesian languages without being trained using the\ntarget language. Furthermore, the results suggest that it\nis possible to train a multi-lingual model, instead of one\nmodel for each language, and achieve promising results.", "sections": [{"title": "1 Introduction", "content": "In recent years, the pace of newly proposed methods\nand tools for natural language processing (NLP) using\ndeep learning approaches are increasing rapidly. After\nBERT [1], a massive pre-trained language model based on\nTransformers [2], was released, it only took a couple of\nyears for the deep learning and NLP community to\npropose improvements and other methods that are shown\nto be better than the last, pushing various boundaries in\nthe NLP field. While high-resource languages have\nachieved great successes in various tasks, other languages\nwith limited data and computational resources are still left\nbehind.\nCross-lingual transfer learning, where a high-resource\nlanguage is used to train a downstream task model to\nimprove the model's performance in another target\nlanguage, shows a promising potential to tackle this\nsetback. Many works of research have experimented and\nshown the potential benefit of using cross-lingual transfer\nin several NLP tasks, such as machine translation [3] [4],\nand named entity recognition [5] [6]. Recently, cross-\nlingual transfer learning has become an essential tool for\nimproving performance in various downstream NLP tasks.\nThis is also thanks to the recent advancements of\nmassively multi-lingual Transformers pre-trained models,\nincluding mBERT [1], XLM [7], and the most recent one\nbeing XLM-RoBERTa (XLM-R) [8].\nEven though the multi-lingual field has grown a lot,\nthere are still some challenges. For example, previous\nworks have reported that the quality of unsupervised\ncross-lingual word embedding is susceptible to the choice\nof language pairs and the comparability of the\nmonolingual data [9], thus limiting the performance when\nthe source and target language have different linguistic\nstructures (e.g., English and Japanese) [10]. XLM-R is\ntrained using 100 languages globally and achieved SOTA\nresults in various multi-lingual NLP tasks such as the\nXNLI, Named Entity Recognition, Cross-lingual question\nanswering, and the GLUE benchmark [8]. However, it\nreports only the performance evaluation for some of the\nlanguages used during pretraining such as English, French,\nSwahili, and Urdu.\nThis research explores the XLM-R base pre-trained\nmodel's capability for cross-lingual transfer learning\nencompassing English, Japanese, and Indonesian\nlanguages. We compare the performance with models\nfrom previous works, evaluate zero-shot transfer learning\ncapability, and fine-tune mono- and multi-lingual models\nin a supervised manner for comparison purposes. This\npaper shows that both fine-tuning and zero-shot transfer\nlearning from English to Japanese and Indonesian for a"}, {"title": "2 Related Works", "content": "As a massively multi-lingual Transformers (MMT)\nmodel, XLM-R [8] is a robustly trained ROBERTa,\nexposed to a much larger multi-lingual corpus than\nmBERT. It is trained on the CommonCrawl-100 data of\n100 languages. There are 88 languages in the intersection\nof XLM-R's and mBERT's corpora; for some languages\n(e.g., Kiswahili), XLM-R's monolingual data are several\norders of magnitude larger than with mBERT. There are\nmany methods for performing cross-lingual transfer based\non MMTs, some of which are fine-tuning [11] and zero-\nshot transfer learning [12] [6]. The common thread is that\ndata in a high-resource source language can be used to\nimprove performance on a low-resource target language.\nEven though XLM-R is pre-trained using 100\nlanguages, investigations regarding the applicability of\nXLM-R for downstream tasks in some languages with less\nresource than English, such as Japanese and Indonesian,\nwith thorough experiments and reproducible results are\nstill limited. Several works have tried to implement cross-\nlingual transfer learning using several Japanese and\nIndonesian text classification models.\nFor Japanese, previous works have shown the\ncapability of XLM-R for Japanese for dependency parsing\n13] and named entity recognition [14], but no thorough\ncomparison for cross-lingual transfer learning (fine-tuned\nand zero-shot) for sentiment classification are provided.\nIn the Multi-lingual Amazon Review Corpus [15] in\nwhich Japanese is one of the languages of the corpus, the\nauthors provided baseline sentiment classification\nperformance using mBERT, but performance using XLM-\nR has not been reported. For Indonesian, previous works\nhave shown the capability of using BERT and XLM-R for\nvarious tasks including sentiment classification [16] [17],\nhowever, the results are mainly focused on building\npowerful monolingual models for Indonesian.\nFurthermore, unlike English, Japanese texts contain no\nwhitespace and there are various ways to split sentences\ninto words, with each split could end in a different\nmeaning and nuance. In order to tackle this problem, a\nrecent work proposed a language-independent subword\ntokenizer and detokenizer designed for neural-based text\nprocessing, named SentencePiece [18]. Its performance is\nshown to be effective for various tasks involving language\npairs with different character sets, such as English-\nJapanese neural machine translation [18] and sentiment\nanalysis in Japanese [19]. It is also utilized by state-of-\nthe-art cross-lingual models such as XLM-R, which is the\nfocus of our current research."}, {"title": "3 Experimental Setup", "content": "We gathered binary sentiment datasets from several\nsources and put shorthand nicknames on each dataset to\nbe addressed in the following sections.\n1. AmazonEN: English Amazon product review\nsentiment dataset from The Multi-lingual Amazon\nReviews Corpus [15]. We use 160,000 data for fine-\ntuning. 4,000 data for evaluation.\n2. AmazonJA: Japanese Amazon product review\nsentiment dataset from The Multi-lingual Amazon\nReviews Corpus [15]We use 160,000 data for fine-\ntuning. 4,000 data for evaluation.\n3. RakutenJA: Japanese Rakuten product review\nbinary sentiment dataset from [20]. We use 400,000\ndata for evaluation.\n4. IndolemID: Indonesian Twitter and hotel review\nsentiment dataset from IndoLEM dataset [17]. We\nuse 5,048 data for evaluation.\n5. SmsaID: Indonesian multi-platform review\nsentiment dataset from SmSA dataset [21]. We use\n1,129 data for evaluation.\nFor each dataset, we use the review body/text as the\ninput and the sentiment (0 for negative and 1 for positive)\nas the classification label.\nIn this experiment, we use the free version of Google\nColab with GPU for all our experiments. Due to the\ndynamic GPU allocation by Google Colab, two GPU\ntypes are used in our experiment: Tesla T4 and Tesla"}, {"title": "4 Results and Analysis", "content": "We fine-tuned the xlm-roberta-base pre-trained model\nfrom the HuggingFace transformers library, three times,\nusing AmazonEN, AmazonJA, and AmazonENJA. For\nAmazonEN and AmazonJA, our final models are fine-\ntuned using the linear scheduler with warmup, 4 epochs,\nbatch size=32, optimizer=AdamW, and learning rate=2e-\n5. For AmazonENJA, the final model uses the same above\nparameters but only with 2 epochs. After fine-tuning three models in the previous step, we\nevaluate each of the fine-tuned models for supervised\nlearning performance on the exact language from which\nthe model is fine-tuned. We calculate the error percentage\nfor the model prediction on test data and compare the\nresults with a baseline model trained using mBERT [15],\nas displayed in Table 2. It can be seen that XLM-R\noutperforms mBERT in all three supervised models for\nbinary sentiment classification in English and Japanese.\nThere is no comparison from the baseline model for the\nmulti-lingual model fine-tuned using AmazonEN and\nAmazonJA. However, it can be seen that it is possible to\nhave a single bi-lingual model for both languages,\neliminating the need to set up multiple models for every\nlanguage.\nIn this scenario, we use the fine-tuned models from the\nprevious scenario to evaluate the applicability of zero-\nshot cross-lingual transfer learning from one language to\nthe others. Furthermore, it\nachieves a much better score of 8.51 error percentage if\nwe add a substantial 160,000 review data from AmazonJA\nwhen fine-tuning the model. Although they are from\ndifferent platforms, product review data shares similar\npatterns. This result is still far from the SOTA result of the\n4.45 error percentage achieved by previous work [19], in\nwhich the model is trained in a fully-supervised\nmonolingual setting using BERT.\nFor the Indonesian sentiment datasets, as also described\nin Table 4, we use two datasets with comparable results\nfrom different sources for evaluation purposes. Macro-\naveraged F1 score is used to evaluate the Indonesian\ndatasets to follow previous works for comparison\npurposes. We experimented with zero-shot cross-lingual\ntransfer learning using two models for classifying the\nIndonesian datasets; one is trained only with 160,000\nEnglish Amazon reviews, and another one containing\nEnglish and Japanese Amazon reviews. In both\nIndonesian datasets, we can see a pattern of more data\nleads to better performance. Similar to prior research\nresults [8], the model trained with multilingual data, in our\ncase, Japanese and English review data, performs better.\nThe XLM-RBASE w/ AmazonENJA model achieves a\n73.31 F1-score on the IndolemID dataset, outperforming\na previous model [17], trained with mBERT in a fully-\nsupervised monolingual setting. Moreover, the same\nmodel achieves a better Fl-score of 88 on the SmsaID\ndataset, outperforming another mBERT model [16]\ntrained in a fully-supervised monolingual setting. For both\ndatasets, our model performance is still worse when\ncompared to the SOTA model's result, as shown in Table\n3 and Table 4.\nBased on the results above, it is essential to note that\nthe models used to compare RakutenJA, IndolemID, and\nSmsaID are trained in a fully-supervised approach using\nthe same language with the target language. In contrast,\nour model, which is trained using AmazonEN, has never\nseen Japanese product reviews, and our models trained\nwith AmazonEN and AmazonENJA have never seen\nIndonesian review texts. Furthermore, it is also essential\nto know that the models compared in the previous tables\nare trained mainly by much bigger architectures with\nmore epochs, which means training them will need much\nmore computational resources and time. Our current\nexperiments show the applicability of zero-shot cross-\nlingual transfer learning with less computational costs,\nwhich yields promising results."}, {"title": "5 Conclusion and Future Work", "content": "This paper reports the results of experiments focusing\non evaluating the applicability of cross-lingual transfer\nlearning using the XLM-R pre-trained model. We then\ncompare the results with previous works to provide an\noverview of the zero-shot approach's capability using\nXLM-R to use a multi-lingual model for bilingual data\ninstead of one model for one language. Based on the\nresults, zero-shot cross-lingual transfer learning yields\npromising results using XLM-R. All experiments are\nperformed using the free version of Google Colab. The\nmodels achieve the best result in one dataset and shows\nthe applicability of cross-lingual transfer learning,\nconsidering that the models have not seen languages in the\ntarget dataset, it can outperform SOTA results in other\ndatasets trained in a fully-supervised approach. Future\nresearch steps include experimenting with more\nhyperparameters, evaluating other potential methods such\nas few-shot transfer learning, which has been proven to be\nuseful to improve performance by adding just a few\nannotated data [13] and meta-learning [22]."}]}