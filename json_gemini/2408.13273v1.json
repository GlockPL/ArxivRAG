{"title": "Retrieval-Augmented Generation Meets Data-Driven Tabula Rasa Approach for Temporal Knowledge Graph Forecasting", "authors": ["Geethan Sannidhi", "Sagar Srinivas Sakhinana", "Venkataramana Runkana"], "abstract": "Pre-trained large language models (PLLMs) like OpenAI ChatGPT and Google Gemini face challenges such as inaccurate factual recall, hallucinations, biases, and future data leakage for temporal Knowledge Graph (tKG) forecasting. To address these issues, we introduce SLA-tKGF (small-scale language assistant for tKG forecasting), which utilizes Retrieval-Augmented Generation (RAG) aided, custom-trained small-scale language models through a tabula rasa approach from scratch for effective tKG forecasting. Our framework constructs knowledge-infused prompts with relevant historical data from tKGs, web search results, and PLLMs-generated textual descriptions to understand historical entity relationships prior to the target time. It leverages these external knowledge-infused prompts for deeper understanding and reasoning of context-specific semantic and temporal information to zero-shot prompt small-scale language models for more accurate predictions of future events within tKGs. It reduces hallucinations and mitigates distributional shift challenges through comprehending changing trends over time. As a result, it enables more accurate and contextually grounded forecasts of future events while minimizing computational demands. Rigorous empirical studies demonstrate our framework's robustness, scalability, and state-of-the-art (SOTA) performance on benchmark datasets with interpretable and trustworthy tKG forecasting.", "sections": [{"title": "1 INTRODUCTION", "content": "Knowledge Graphs (KGs) and their dynamic extension, Temporal Knowledge Graphs (tKGs), play a pivotal role in AI applications like recommendation engines and web searches by structuring data into graph-formatted databases. KGs utilize triples (s, r, o)-where s is the subject, o is the object, and r describes their relation-to encode facts, while tKGs add a time element (t) to represent time-validity of the fact, allowing them to capture how facts evolve over time. tKG reasoning, essential for deriving new insights, encompasses interpolation-to fill in historical data gaps [5, 6, 8, 12, 16, 32]-and extrapolation, for future event prediction [13, 28]. Predicting future events in tKGs is difficult, it requires handling unseen time periods and entirely new entities, demanding advanced methods to navigate the ever-changing nature of relationships. Closed-source pretrained large language models(PLLMs) like OpenAI ChatGPT[1] and Google Gemini[27] show potential for predicting future events due to their extensive pre-training knowledge and reasoning abilities. However, these large-scale models face challenges like fact recall issues stemming from complex architectures, resulting in unreliable predictions (hallucinations), potential bias from training data, and inadvertent leveraging of future data during pretraining, causing data leakage for tKG forecasting. Detecting data leakage"}, {"title": "2 PROBLEM FORMULATION", "content": "An tKG captures the dynamic evolution of entity relationships over time, unlike static KGs, which are fixed. Let V, R, T, and F symbolize sets of entities, relations, timestamps, and facts, respectively. A tKG snapshot Gt; := Vi, Ri, Fi at discrete timestamp ti"}, {"title": "3 PROPOSED METHOD", "content": "SLA-tKGF framework utilizes RAG to enhance the capabilities of the small-scale language model to predict future events in tKGs. We construct knowledge-infused prompts using historical tKG data, web information, and PLLM-generated past entity relationship descriptions for zero-shot prompting the language model for reliable and trustworthy forecasting, outperforming conventional methods."}, {"title": "3.1 Web-Search-Driven Retrieval", "content": "In developing a RAG framework that leverages web-based information, a critical challenge is addressed: excluding future facts before evaluating the relevance of retrieved web passages to a query. This approach focuses on preprocessing web passages to identify and exclude sentences containing future facts relative to a specified temporal threshold. This is achieved by using natural language processing techniques for sentence tokenization, temporal tagging, and resolving temporal expressions to absolute dates. By establishing a relevance period based on the query context, the framework filters out sentences with temporal references beyond this period, ensuring that only contextually and temporally relevant information is retained. Preparing the search data involves collecting various types of web content, parsing and chunking text, and embedding text chunks using text-embedding model. Each natural language question (verbalized query) is embedded similarly, then compared"}, {"title": "3.2 Retrieving Relevant Historical tKG Data", "content": "We leverage the following multi-step methods to incorporate relevant historical information from dynamic tKGs. (a) Query Verbal-ization: We convert structured queries (sq, rq, ?, tq) about future events into natural language questions using PLLMs like GPT-4. This allows the small-scale language model to interpret symbolic knowledge for forecasting tasks. For example, the query (Barack Obama, visit, ?, 2015-01-25) can be verbalized as 'Which country did Barack Obama visit on January 25, 2015?. (b) Knowledge Retrieval & Verbalization: Historical events relevant to the verbalized query up to the target time ta from evolving tKGs are incorporated to improve forecasting accuracy. It improves forecasting by grounding queries in historical context by identifying recurring patterns, trends, and causal relationships of the evolving entity relationships over time in graph-structured data. Motivated by the concept of window size in time series forecasting, which indicates the number of past observations used to predict future values, we adopt a similar approach in tKG forecasting. For a query at target time tq with subject sq and relation rq, we retrieve \u2018m\u2019 prior tKG snapshots G(tq-m,tq-1) as background knowledge. A knowledge-infused prompt is constructed from historical facts related to sq. Assuming a set of \u2018M\u2019 retrieved facts related to the query, we describe this as follows:\n\nC(sqrq?,tq) = {\\{(Si, ri, Oi, ti) \\} \u2208 C(sqrqtq-mt-1) \u222a C(sqtq-m,tq-1) & t < tq\\} i=1^M\n\nThis set is constructed by combining facts involving sq within the time window tq - m to tq \u2013 1, covering both facts directly related to sq and rq(i.e., C(sq,tq-m,tq\u22121)), and those involving sq in any relation(i.e., C(sq,rq,tq-m,tq\u22121)), but only prior to tq. This approach enhances forecasting accuracy by providing a comprehensive view of sq's historical occurrences. (c) Irrelevant Knowledge Rejection: We reject irrelevant facts using sentence embedding techniques, retaining only facts pertinent to the verbalized query based on semantic similarity. We then inject relevant historical knowledge into prompts to anchor predictions within historical tKG data, enhancing accuracy. For the query (Barack Obama, visit, ?, 2015-01-25) verbal-ized as 'What country did Barack Obama visit on January 25, 2015?', using relevant historical facts like 'Barack Obama visited France on June 5, 2014', 'Barack Obama visited Australia on November 15, 2014', etc., construct a knowledge-augmented prompt for the small-scale language model's zero-shot prediction on tKG forecasting."}, {"title": "3.3 PLLMs-Generated Historical Summary", "content": "We query off-the-shelf PLLMs to generate a summary description of the historical entity relationships prior to the target time, based on the parametric knowledge acquired during pretraining."}, {"title": "3.4 Overall Framework", "content": "We construct knowledge-augmented prompts for grounded and interpretable forecasts by uniquely combining the relevant historical knowledge from tKGs, contemporary data through web search, and historical entity relationship descriptions generated by off-the-shelf PLLMs, setting a foundation for trustworthy tKG forecasting. In"}, {"title": "4 EXPERIMENTS AND RESULTS", "content": ""}, {"title": "4.1 Datasets", "content": "We evaluate our framework with the following benchmark datasets: ICEWS14[28], ICEWS18[4], ICEWS05-15[6], YAGO[23], and WIKI[17]. These diverse datasets, encapsulate real-world events in a quadruple format. The ICEWS datasets offers geo-coded events for systematic analysis and prediction relating to political violence, instability, and international relations from news reports over multi-decade time periods. For example, a quadruple fact (s, r, o, t) such as (Barack Obama, visited, India, 2015-01-25). ICEWS14, ICEWS18, and ICEWS05-15 span events from 2014, january to october 2018, and 2005 to 2015, respectively. WIKI and YAGO, extracted from Wikipedia [17] and YAGO3-10[23] knowledge graphs (KGs), cover historical facts formatted as (s, p, o, [Ts, Te]), with Ts and Te marking start and end times. Our paper adopts the method from previous work[13], for temporal fact discretization by breaking down multi-timestamp events into consecutive single events, improving accuracy by isolating each event. We utilize a new dataset from the Armed Conflict Location & Event Data Project (ACLED), detailing crisis situations globally. Our study targets hostility and aggression towards civilians in Cabo Delgado from January 1900 to March 2022, with the period from October 2021 to March 2022 as the test phase. Following the method in an earlier study[13], we split datasets into training, validation, and test sets chronologically with an 80%/10%/10% ratio, except ICEWS14, split 50%/50% due to no validation set. Splitting ensures training precedes validation and testing. Notably, many entities in the test sets are new compared to those in training and validation. Our research categorizes two types of tKG forecasting benchmarks: ICEWS datasets, featuring recurring short-lived events, e.g., Xi Jinping visited the United States three times in 2015, and WIKI and YAGO datasets, capturing longer, non-recurring events, e.g., Yossi Benayoun played for Chelsea FC from 2011 to 2013."}, {"title": "4.2 Prediction Settings", "content": "The tKG forecasting task has two primary prediction settings: (a) Single Step: It uses recent historical tKG data within a predefined window size(m) to predict at the next time point, incorporating facts up to time tq-1 for predicting missing entities at time tq. It utilizes actual historical data, not predictions from earlier points, to forecast at the next step. (b) MultiStep: It uses past framework predictions instead of ground truth facts for predicting missing entities at time tq. These settings trade off computational cost, accuracy, and suitability for short-term or long-term forecasting."}, {"title": "4.3 Baseline Methods", "content": "Previous tKG forecasting research employed various methods for entity-relation modeling of historical events for missing entity prediction in future events. These methods included graph neural networks (GNNs) [10, 11, 14, 20], reinforcement learning [25], and logical rules [22, 35]. We benchmarked our tKG forecasting framework against established methods like Distmult [33], TuckER [2],"}, {"title": "4.4 Evaluation Protocol", "content": "We evaluate our framework using standard metrics for the tKG forecasting task, specifically the Mean Reciprocal Rank (MRR) and the Hits@K metric. For each quadruple (sq, rq, 0q, tq) in the test set, we generate a query quadruple: (sq, rq, ?, tq) and the small-scale language model will rank all entities V in its predictions. The rank of the actual missing entity oq \u2208 V for each query quadruple is denoted by (oq). The MRR evaluation metric is defined as follows:\n\nMRR = \\frac{1}{Gtest} \\sum_{q \u2208 Gtest} \\frac{1}{\u03c8(oq)}\n\nMRR measures the average ranking of the actual missing entities in the predictions, with a higher MRR indicating better performance. The Hits@K metric (where K is in {1, 3, 10}) evaluates the accuracy of predicting missing entities at different positions in the predicted rankings. It measures the proportion of ground-truth missing entities ranked within the top-K positions, evaluating the framework's ability to predict missing entities accurately. For the tKG forecasting task, we consider time-aware filtering as the evaluation setting to prevent valid predictions from being considered errors. For example, consider a test query like (Barack Obama, visit, ?, 2015) with the true answer being India. Other valid predictions, such as (Barack Obama, visit, Germany, 2015), might exist but would be removed by the time-aware filter. It allows us to determine the rank of India without interference from alternative valid predictions."}, {"title": "4.5 Implementation Details", "content": "By employing a 'Tabula Rasa' approach-starting from a clean slate-the framework addresses the critical challenges of data leakage in predicting future events in tKGs. Our orchestrated framework workflow utilizes LlamaIndex[21] for the development of PLLMs-powered applications. We utilize DuckDuckGo's search engine for searching the web. We access PLLMs using a text-based high-level API through Language Model as a Service (LaMaaS, [26]). The framework is trained on tKG forecasting, aiming to minimize cross-entropy loss and predict missing entities in future events. Framework hyperparameters include a batch size (b) set at 48, the number of epochs (ep) at 30, and the hidden/embedding dimension (d) at 128. The Adam optimizer ([15]) is employed with an initial learning rate of 1e-3. A learning rate decay scheduler halves the learning rate if the validation loss doesn't improve over 5 epochs, and early stopping is implemented to prevent overfitting. Four representative off-the-shelf PLLMs used are GPT-4, GPT-3.5-turbo, GPT-3.0-text-davinci-003, and Google Bard. Framework hyperparameters remain consistent across PLLMs and benchmark datasets, demonstrating versatility. The context window (m) is set to 25 for knowledge injection of historical events from evolving tKG into the prompt, and the quadruple retrieval hop is set to one. No limit is imposed on the number of facts retrieved for knowledge injection. Utilizing 8 V100 GPUs, each with 8 GB of GPU memory, ensures efficient training. Due to high computational costs, experiments are run three times, and averaged results from independent experimental runs are reported for robust comparisons. Refer appendix for hyperparameter tuning results."}, {"title": "4.6 Results", "content": "Our study compared the proposed framework with various supervised learning methods for tKG forecasting. Table 2 demonstrates that SLA-tKGF W/GPT-4, outperformed baseline algorithms. We report baseline results from prior research [7, 18] for fair and consistent comparison. Tables 3 and 4 provide further comparisons using several popular baselines, with results reported from an earlier study[11]. Our experimental results confirm the effectiveness of SLA-tKGF framework in constructing knowledge-augmented prompts, involving: (1) retrieving historical knowledge from tKGs, (2) incorporating web search results for current information, and (3) using pre-trained large language models for summarizing historical entity-relationships, to query the small-scale language model and generate accurate and interpretable forecasts."}, {"title": "4.7 Ablation Study", "content": "Given the complexity and multifaceted nature of the proposed SLA-tKGF framework for tKG forecasting, we propose several ablation experiments to evaluate the individual contributions of the components and their effectiveness within the framework. Each of these ablation studies can provide insights into the significance of the individual components and their collective impact on the overall efficacy of the SLA-tKGF framework. To design ablation studies for the SLA-tKGF framework, we systematically disable each component individually to evaluate its impact on the overall performance. This approach helps in understanding the contribution of individual components to the framework's accuracy, reliability, and robustness. The ablation study aims to evaluate the contributions of three key components in the framework for forecasting future events: (a) historical knowledge retrieval (HKR) from tKGs, (b) web search for contextual information (WSCI), and (c) descriptive text generation (DTG) using PLLMs. The HKR component, which retrieves relevant historical knowledge from dynamic tKGs, is assessed by contrasting its performance when incorporated versus omitted. Similarly, the impact of the WSCI component, providing up-to-date context prior to the target time through web search, is evaluated by disabling it and observing the changes in forecasting accuracy. The utility of the DTG component, which utilizes PLLMs to analyze historical entity relationships to generate descriptive texts, is quantified by including or excluding it and observing the impact on forecasting accuracy. The ablated variants are as follows:\n\u2022 w/o HKR: Omits historical knowledge retrieval from tKGs.\n\u2022 w/o WSCI: Omits web search for contextual information.\n\u2022 w/o DTG: Omits PLLMs for descriptive text generation.\nTable 5 shows the ablation study results. The ablated variants performance declines compared to the original framework (i.e.,"}, {"title": "4.8 Deep Dive into Long-term tKG forecasting", "content": "Existing tKG forecasting research often overlooks insights from edge dissolution and formation. Our work considers the creation and dissolution of relationships (edges) between entities in tKGs as critical evolutionary factors for accurate tKG forecasting by capturing the essence of how relationships evolve and change over time. Nevertheless, the high dimensionality and dynamic complexity of tKGs pose challenges for both short and long-term event forecasting. Our approach uses historical context within an evolving tKG to improve both short-term and long-term forecasting accuracy. Short-term forecasts predict near-future changes, capturing the immediate evolution of events, while long-term forecasts anticipate broader trends that unfold over an extended period. Standard tKG forecasting relies on static KG snapshots, denoted as G(t-tm,t), from a historical window m up until time t, to predict events at time t + At (e.g., one day). In real-world scenarios, the absence of graph information motivates the evaluation of forecasting techniques for making predictions about the distant future (AT \u2265 At). In the context of long-term tKG forecasting, the objective is to predict missing entities at a future time t + AT, beginning with an initial forecast period from t - tm to t + At. This initial forecast targets the short-term future, represented by At. The forecasting process then extends into the final forecast period, from t + At to t + AT, to focus on long-term predictions, where AT signifies the extended future. Experimental results on the ICEWS05-15 dataset, using SLA-tKGF W/GPT-4, for different AT values are shown in Table 6. As AT increases (1 day to 8 days), performance declines in both single-step and multi-step approaches, indicating increased difficulty in predicting further into the future. This decline in performance is attributed from the assumption that dynamics at t + At apply at t + AT. With larger AT, this assumption falters due to data distributional shifts and the performance drops because of the lack of recent relevant historical facts in the augmented prompt for accurate predictions at t + AT."}, {"title": "4.9 Inductive Link Prediction in tKGs", "content": "In real-world applications, new entities emerge over time, necessitating a tKG forecasting framework that generalizes well to unseen data. An efficient framework must demonstrate robust generalization abilities to effectively handle new, unencountered entities. To evaluate this capability, we introduce the inductive link prediction task, which showcases the framework's ability to predict links involving unseen entities. This task involves selecting test"}, {"title": "5 CONCLUSION", "content": "Despite their strengths, pre-trained large language models struggle with future predictions for tKG forecasting due to limitations including hallucinations, inaccurate fact recall, and future data leakage. Our framework tackles these challenges by utilizing a retrieval-augmented small-scale language model, trained from a clean-slate approach with knowledge-augmented prompts to achieve accurate tKG forecasts. We construct these prompts that include historical data from tKGs, web search results, and text descriptions generated by off-the-shelf PLLMs, enabling contextually grounded forecasts with improved factual accuracy, reducing hallucinations and overcoming distributional shifts. This scalable, robust approach achieves state-of-the-art (SOTA) performance on benchmark tKG datasets. It paves the way for trustworthy tKG forecasting by offering traceability, explainability, and interpretability."}, {"title": "6 ADDITIONAL EXPERIMENTS", "content": ""}, {"title": "6.1 The Impact of Temporal and Sequential\nInformation on the Zero-Shot tKG\nForecasting Task", "content": "The proposed framework, SLA-tKGF, introduces a novel two-pronged approach to predict future events in tKGs by combining a clean-slate trained, small-scale language model with the Retrieval-Augmented Generation (RAG) technique. It emphasizes accuracy and bias-free predictions by leveraging historical tKG data, current web information, and contextually relevant historical entity relationship descriptions generated by PLLMs. We examined the performance of the SLA-tKGF framework by understanding temporal information in retrieved historical events from tKGs, comparing its performance in the tKG forecasting task with and without explicit timestamps in the knowledge-augmented prompt. We also investigated the impact of shuffling historical facts without time information on the performance of the SLA-tKGF framework. The results reveal that the SLA-tKGF framework's performance worsens without temporal information. This decline is exacerbated by the random arrangement of events. These outcomes underscore the framework's dependence on chronological sequencing for accurate predictions, emphasizing the critical role of temporal information and sequence in the accuracy of the SLA-tKGF W/GPT-4 framework for tKG forecasting. The consistent significance of temporal and sequence information across datasets (YAGO, WIKI, ICEWS14, ICEWS18, ACLED-CD22) reinforces the reliability and applicability of these findings."}, {"title": "6.2 Effect of Types of Knowledge-Infused\nAugmented Prompts on tKG Forecasting", "content": "In this section, we evaluate the impact of augmenting natural language questions(verbalized queries) with external knowledge from historical events sampled from tKGs on the sLA-tKGF W/GPT-4 framework's performance in tKG forecasting tasks. We explore various knowledge retrieval strategies for constructing knowledge-augmented prompts, focusing on optimizing the framework's performance in tKG forecasting. demonstrates that selectively incorporating relevant prior knowledge significantly enhances tKG forecasting performance, surpassing baselines that either omit additional knowledge or use it indiscriminately.\n\u2022 The No Knowledge (NK) baseline applies the given query without external knowledge.\n\u2022 The Random Knowledge (RK) baseline constructs prompts with randomly selected historical facts.\n\u2022 The Popular Knowledge (PK) baseline builds prompts using the most common relationship-based historical facts from the tKG.\n\u2022 The sLA-tKGF (Ours) framework selects the most pertinent historical facts for the query, excludes irrelevant facts, and uses this filtered knowledge for forecasting.\nOur experiments show that even randomly included historical facts improve performance compared to not using any additional"}, {"title": "6.3 Balancing Accuracy and Efficiency:\nOptimizing Historical Context in\nAugmented Prompts for tKG Forecasting", "content": "Our framework utilizes historical events or facts from time (tq \u2013 m) to t\u0105 to predict missing entities in the query quadruple at tq, where m represents the historical context window size, a configurable hyperparameter. We experimented with various lengths to evaluate the impact of historical context on the forecasting performance of the SLA-tKGF W/GPT-4 framework for tKGs. According to leverages a greater amount of past data to generate knowledge-augmented prompts for the small-scale language model enhances the accuracy of missing entity predictions, as evidenced by improved mean reciprocal rank (MRR) scores, albeit at the expense of higher computational demands. The small-scale language model within the SLA-tKGF W/GPT-4 framework is constrained by the maximum input token sequence length. Although extending the historical window marginally increases MRR scores, it leads to the creation of longer and more complex prompts that are impractical for broad-scale application. To strike a balance between accuracy and computational efficiency, we selected a historical context window of 25 for our experiments. We varied the historical facts in the prompts to identify the optimal setup for generating accurate forecasts, with the goal of minimizing wall-clock time. Wall-clock time for SLA-tKGF W/GPT-4 queries-the time elapsed from query submission to response-is influenced by the size of the small-scale language model, the complexity of the query, and the available computational resources. While typically, small-scale language models return responses within seconds, more complex or extensive queries may require longer processing times."}, {"title": "6.4 Study of tKG forecasting task with\nnon-uniform time intervals", "content": "Many state-of-the-art techniques struggle with tKGs featuring irregular time intervals, unlike the SLA-tKGF framework, which effectively addresses this issue by leveraging knowledge-augmented prompting for small-scale language models in temporal KG forecasting. The framework excels in handling real-world complexities and data sparsity, capturing complex dynamics and causal relationships more accurately, thus offering a versatile and reliable solution for temporal KG forecasting. Experimental evaluations on the ICEWS05-15_continuous dataset, a subset created by sampling from the original ICEWS05-15 dataset to simulate non-periodic observations in continuous time with 1-4 units interval, support our claim. The SLA-tKGF framework, trained on this benchmark and assessed using the Mean Reciprocal Rank (MRR) metric, demonstrates strong performance, especially with the SLA-tKGF W/GPT-4 configuration, on temporal KGs with irregular intervals."}, {"title": "6.5 Impact of Retrieved Fact Types", "content": "In this work, we introduce a zero-shot learning method to predict missing entities in query quadruples. Our approach includes: (a) Constructing a historical context for a query quadruple (sq, pq, ?, tq) using historical facts from previous static KG snapshots Gtq-m:tq-1 to form a knowledge-infused augmented prompt. (b) Converting retrieved facts and the query into verbalized sentences, employing sentence embedding for knowledge distillation, and using semantic similarity to filter facts, thereby constructing augmented prompts. (c) Estimating the missing entity conditioned on the augmented prompt, following oq ~ P(o | (sq, rq, ?, tq), G(tq-m,tq-1)). We eval-uate the impact of single-subject entity and subject entity-relation pair historical facts on forecasting accuracy. The former involves only the subject entity (sq), while the latter includes both the subject (sq) and the relation (rq). We examine these effects across"}, {"title": "6.6 Impact of Directionality in Historical\nModeling on tKG forecasting Performance", "content": "In the tKG forecasting task, \u2018unidirectional' refers to when the subject entity (sq) or subject-relation pair (sq, rq) from historical facts matches their position in the query quadruple (sq, rq, ?, tq). 'Bidirectional' denotes cases where they can appear in any position. For bidirectional modeling, historical facts are transformed by swapping entities and inverting the relation, e.g., (s, r, o, t) becomes (o, r\u00af\u00b9, s, t). We use PLLMs such as GPT-4 to obtain reciprocal rela-tions, enhancing tKG forecasting by incorporating diverse historical contexts. For instance, (Barack Obama, visit, India, 2015-01-25) becomes its reciprocal (India, visited by, Barack Obama, 2015-01-25). Our study evaluates the directionality's impact on the tKG forecasting, finding that bidirectional modeling slightly improves performance, notably on ICEWS datasets. The experimental results highlight the value of appropriate relation modeling for SLA-tKGF W/GPT-4 in understanding context, offering modest performance boosts in tKG forecasting."}, {"title": "6.7 Impact of PLLMs size", "content": "The SLA-tKGF framework leverages a blend of historical tKG data, current web-scraped information, and contextually relevant descriptions of past entity relationships generated by pre-trained language models (PLLMs) to construct knowledge-augmented prompts. These prompts query the small-scale language model to estimate"}, {"title": "6.8 Hyperparameter Tuning", "content": "Our SLA-tKGF-GPT-4 framework improves tKG forecasting accuracy by integrating PLLMs with small-scale language models. Hyperparameter tuning is challenging due to its complex dimensionality, computational demands, and dataset-specific requirements. We opt for random search over grid search or Bayesian optimization for efficient hyperparameter exploration, seeking the optimal configuration on benchmark datasets. The small-scale language models are trained on knowledge-augmented prompting for tKG forecasting, aiming to predict missing entities by minimizing cross-entropy loss. Hyperparameter optimization for SLA-tKGF-GPT-4 focuses on batch size (b \u2208 16, 48), epochs (ep \u2208 10, 30), and embedding dimension (d \u2208 64, 128)."}, {"title": "6.9 Hyperparameter Tuning of PLLMs", "content": "Top-P and temperature serve as hyperparameters in pre-trained large language models (PLLMs), such as GPT-4 and Google Gemini. These hyperparameters influence the predictability and variety of model outputs. The temperature parameter primarily affects output predictability. Lower temperature values result in more deterministic responses, while higher values enable greater variability. A temperature value of 1.0 maintains the original output probabilities. On the other hand, Top-P (nucleus) sampling dynamically chooses tokens based on their cumulative probability. This method strikes a balance between text generation diversity and coherence by adjusting the threshold. By fine-tuning these parameters, it is possible to strike an optimal balance between randomness and determinism in generated outputs. Regarding the SLA-tKGF-GPT-4 variant, hyperparameter optimization studies indicate that Top-P ranges over [0, 1], and temperature falls within the range of [0, 3]."}, {"title": "As shown in, top performance has been consistently observed when using the configuration (Top-P = 1, Temp = 0) across multiple benchmark datasets, highlighting its broad applicability and efficacy.", "content": ""}]}