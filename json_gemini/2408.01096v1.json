{"title": "SIX DRAGONS FLY AGAIN: REVIVING 15TH-CENTURY KOREAN COURT MUSIC WITH TRANSFORMERS AND NOVEL ENCODING", "authors": ["Danbinaerin Han", "Mark Gotham", "Dongmin Kim", "Hannah Park", "Sihun Lee", "Dasaem Jeong"], "abstract": "We introduce a project that revives a piece of 15th-century Korean court music, Chihwapyeong and Chwipunghyeong, composed upon the poem Songs of the Dragon Flying to Heaven. One of the earliest examples of Jeongganbo, a Korean musical notation system, the remaining version only consists of a rudimentary melody. Our research team, commissioned by the National Gugak (Korean Traditional Music) Center, aimed to transform this old melody into a performable arrangement for a six-part ensemble. Using Jeongganbo data acquired through bespoke optical music recognition, we trained a BERT-like masked language model and an encoder-decoder transformer model. We also propose an encoding scheme that strictly follows the structure of Jeongganbo and denotes note durations as positions. The resulting machine-transformed version of Chihwapyeong and Chwipunghyeong were evaluated by experts and performed by the Court Music Orchestra of National Gugak Center. Our work demonstrates that generative models can successfully be applied to traditional music with limited training data if combined with careful design.", "sections": [{"title": "1. INTRODUCTION", "content": "Six dragons fly on the east land; every endeavour is a heavenly blessing. This is the first line of lyrics in Yongbieocheonga, the first text written in the Korean alphabet (Hangul, \ud55c\uae00). Sejong the Great, one of the most respected figures in Korean history, invented and introduced Hangul in 1446. In addition to this remarkable achievement, he ordered scholar-officials to write Yongbieocheonga, and composed music to accompany the lyrics. Three other pieces composed at the time are Yeo-Min-Lak, Chi-Hwa-Pyeong and Chwi-Pung-Hyeong."}, {"title": "2. RELATED WORKS", "content": "Recent advances in neural network-based music generation have resulted in much artistic output. Since 2020, the AI Music Generation Challenge [4] has been held annually, focusing on generating songs in the style of Irish and Swedish folk music. This event has allowed for exploration of new methods for generation and evaluation of traditional music through the means of deep learning models.\nThe Beethoven X project [5] utilized neural networks to learn Beethoven's compositional style and complete his unfinished 10th Symphony. The resulting work has been performed by an orchestra\u2014a project outline similar to that of ours.\nAttempts at automatic generation have been made for traditional music from beyond the West, including Persia [6] and China [7]. The limited progress in such areas is often due to the distinctive traditional musical systems that demand deep understanding and unique methodologies. Such idiosyncrasies put much interest and meaning in the computational research of traditional music, since it can present new methods and perspectives to the field as a whole, while also helping preserve diverse musical heritages."}, {"title": "3. JEONGGANBO DATASET", "content": "As depicted in Figure 1, Korean court music is performed on a variety of instruments, including plucked string instruments (Gayageum and Geomungo), bowed string instruments (Haegeum and Ajaeng), and wind instruments"}, {"title": "3.1 Jeongganbo Notation", "content": "As depicted in Figure 1, Korean court music is performed on a variety of instruments, including plucked string instruments (Gayageum and Geomungo), bowed string instruments (Haegeum and Ajaeng), and wind instruments (Daegeum and Piri), among others. These instruments are played together in a heterophonic texture, with each instrument employing its distinctive playing techniques and ornamentations.\nMuch of Korean court music is written in Jeongganbo, a traditional musical notation system. Jeongganbo is recognized as the first system in East Asia capable of simultaneously representing both pitch and duration of notes [8,9]. This versatility has been instrumental in passing down court music throughout history [10].\nJeongganbo uses grid-divided boxes (Jeonggans) as the basic unit of time. The number of characters (notes) and their position within each jeonggan varies to denote rhythm. Figure 2 provides an example passage, and figure 3 provides a schematic overview of possible positions.\nHere, we provide a broad introduction to this rhythmic notation system in quasi-Western musical theoretic language. Each jeonggan is broadly equivalent to a beat. If a jeonggan features only one character, this note event starts at the beginning of the beat and lasts the beat's full duration. The first box ('0') in figure 3 is in this form as is the second jeonggan of figure 2 where the 'compound beats' correspond to the duration J. (in this case for the note Bb4). At the next metrical level we have the 'column' division of the 'rows'. This number of 'rows' relates broadly to the top level division of the beat. The use of three vertically stacked characters refers to 3 equal divisions of this beat (here, 3 x s).\nFor example, in figure 3, the numbers 4-9 feature a 3-part division of the beat into 3 x \u266a (positions 4, 6, 8), and a 2x division of thoses (e.g., 4-5). If the following jeonggan is empty, the previously played note is sustained. Playing techniques and ornamentations called sigimsae are sometimes notated for each instrument. When sigimsae are placed to the right of notes, they serve as ornamentations or embellishments for the corresponding note; when written on their own, they indicate timed instructions to play a specific note or musical phrase. For convenience,"}, {"title": "3.2 Machine Readable Dataset", "content": "We have constructed a dataset of 85 pieces by applying optical musical recognition (OMR) to all compositions available within the manuscripts published by the National Gugak Center. The manuscripts cover the entire repertoire of remaining Korean court music2. OMR was necessary since the scores are only provided as PDF images and the semantic data is unavailable. We implemented and trained an encoder-decoder transformer with CNN by synthesizing various Jeonggan images in a rule-based approach [11]. In total, the dataset comprises 28010 jeonggans across 85 pieces. When counting each instrument part independently, the combined total amounts to 141 820 jeonggans. Out of 90 pieces notated in jeongganbo for ensembles of at least two different instruments in the published manuscripts, we excluded 5 pieces that have discrepancies in the total number of jeonggans across instruments."}, {"title": "4. JEONGGAN-LIKE ENCODING", "content": "In the field of symbolic music generation for Western monophonic and polyphonic music, encoding schemes such as ABC notation, which denotes pitch and duration separately, are effective and prevalent [12, 13]. However, when it comes to Korean court music, whose heterophonic structure is a defining characteristic, it is crucial that the intricate alignment of different melodies be well-represented in encoding. The genre also exhibits prolonged notes and considerable variations in note lengths, which proves to be a challenge for learning algorithms, especially when data is limited.\nThese distinct musical qualities call for a specialized encoding scheme; for this, we propose Jeonggan (JG)-like encoding, which closely follows the positional notation of Jeongganbo. This symbolic music encoding method is modeled to inherently reflect the composition and notation style of traditional Korean court music.\nThe detailed rules of encoding are as follows. The boundary of a Jeonggan is designated as a bar (1) to-"}, {"title": "4.1 Other Possible Encodings", "content": "REMI (revamped MIDI-derived events) [14] first proposed the usage of beat-position feature rather than time-shifting to encode temporal position. We also experiment with REMI-like encoding which adopts three token types: beat position, new beat (instead of new measure), and pitch tokens. We intentionally design REMI-like and JG-like encoding to share the same structure and result in the same number of tokens for a given melody. They differ in that JG encoding provides intra-JG position, while REMI encoding provides the beat position of the note. According to the position labels shown in Figure 3, any of [0, 1, 4, 10, 12] can correspond to beat position 0. However, in JG-like encoding, each occurrence of position tokens limits the possibilities of subsequent ones. For instance, a position token of 0 implies that no more notes will occur in the same jeonggan, and if the first note is 1, one or more additional notes should follow with values of 2-3 or 6-9. In contrast, in REMI-like encoding, any offset value can follow a beat position of 0. To examine the impact of this position-based logic on the generation process, we use REMI-like encoding as our first baseline for comparison.\nAs a second baseline, we implement an ABC-like encoding scheme that does not have a separate bar token and encodes each note as a combination of pitch and duration values. Note that we do not omit duration tokens that are equal to unit length as ABC encoding typically does."}, {"title": "5. ORCHESTRAL PART GENERATION", "content": "We implement an encoder-decoder transformer [15] model to generate melodies for different instruments based on a"}, {"title": "5.1 Transformer Sequence-to-sequence Model", "content": "We implement an encoder-decoder transformer [15] model to generate melodies for different instruments based on a given instrument's melody, leveraging its ability to learn long-term dependencies. Unlike RNN-based models, the transformer calculates relationships between all elements in a sequence via the self-attention mechanism, enhancing its capability in symbolic music generation [16-18]. The model consists of an encoder that processes the input sequence and a decoder for generating the output sequence. Our objective is to generate melodies that synchronize with the input melody across musically equivalent phrases; self- and cross-attention within the model enable understanding of musical context at measure and bar levels, capturing the repeating structure of melodies and accents prominent in traditional Korean court music."}, {"title": "5.2 Beat Counter", "content": "Instead of sinusoidal [15] or learned [19] positional embedding commonly utilized in transformer-based models, we implemented a 'beat counter' embedding that provides information about temporal position.\nFor a model to learn to 'parse' semantic position only from the tokens' sequential position is challenging, if not impossible, with limited training data and a small number of transformer layers. Therefore, we explicitly encode the musical position of each symbol as a combination of measure index, beat index, and sub-beat index (in-jeonggan position) as shown in Figure 5. This information is summed into note embedding, just like positional encoding of transformer [15].\nAs previous research of PopMAG [20] demonstrated, metrical position embeddings can replace the positional encoding of transformers in symbolic music. A minor difference between PopMAG and our approach is that the model predicts only the appearance of new measures or new beats without the index of them, and that the new beat can be used for elongating the duration of previous note. The same idea of embedding the beat counting has been"}, {"title": "6. EXPERIMENT AND RESULTS", "content": "We split the Jeongganbo dataset into three subsets: 75 pieces for training, 5 for validation, and 5 for testing. Each piece contains melodies for up to 6 instruments. The sequence-to-sequence model takes 4 measures of melody, each from a randomly selected number of instruments, as input to the encoder; and given a target instrument condition, it generates the corresponding 4 measures of melody for the target instrument. Note that the number of beats in a single measure is at least 4 or to a maximum of 20 in our dataset.\nThe transformer encoder and decoder both consist of 6 layers, 4 attention heads, and a hidden dimension size of 128 with dropout of 0.2. We train for 35000 updates across 300 epochs using negative log-likelihood loss. We also employ mixed precision training [24] to enhance performance and efficiency. We utilize the Adam optimizer with an initial learning rate of 0.001 and apply a cosine learning rate scheduler with 1000 warmup steps. Using a batch size of 16, training can be conducted on a single Nvidia RTX A6000 GPU."}, {"title": "6.1 Training", "content": "We split the Jeongganbo dataset into three subsets: 75 pieces for training, 5 for validation, and 5 for testing. Each piece contains melodies for up to 6 instruments. The sequence-to-sequence model takes 4 measures of melody, each from a randomly selected number of instruments, as input to the encoder; and given a target instrument condition, it generates the corresponding 4 measures of melody for the target instrument. Note that the number of beats in a single measure is at least 4 or to a maximum of 20 in our dataset.\nThe transformer encoder and decoder both consist of 6 layers, 4 attention heads, and a hidden dimension size of 128 with dropout of 0.2. We train for 35000 updates across 300 epochs using negative log-likelihood loss. We also employ mixed precision training [24] to enhance performance and efficiency. We utilize the Adam optimizer with an initial learning rate of 0.001 and apply a cosine learning rate scheduler with 1000 warmup steps. Using a batch size of 16, training can be conducted on a single Nvidia RTX A6000 GPU."}, {"title": "6.2 Evaluation Metrics", "content": "As an evaluation metric, we check whether the input and output melodies share the same number of measures, a consistency necessitated by our task. Since the length of a measure can change in the middle of a piece, this metric serves as an indicator of the model's ability to capture the musical context of the input melody and accordingly generate a musically complete melody. We measure length match rate as the percentage of generated melodies whose number of jeonggans, after decoding the output tokens, matches that of the input melody."}, {"title": "6.2.1 Length Match Rate", "content": "As an evaluation metric, we check whether the input and output melodies share the same number of measures, a consistency necessitated by our task. Since the length of a measure can change in the middle of a piece, this metric serves as an indicator of the model's ability to capture the musical context of the input melody and accordingly generate a musically complete melody. We measure length match rate as the percentage of generated melodies whose number of jeonggans, after decoding the output tokens, matches that of the input melody."}, {"title": "6.2.2 F1-Score", "content": "Regarding the generation task as one with a fixed answer, we can measure the accuracy of the generated melody by directly comparing it with the ground-truth target melody. Thus, as a general accuracy metric, we calculate the F\u2081-score of predicted notes, where only the notes with the exact same onset position and pitch are counted as correct. To make a fair comparison between encoding methods, note onset positions in JG-like encoding were converted to those in the REMI-like format. Ornamentations without duration were not counted."}, {"title": "6.3 Results and Discussion", "content": "In our sequential generation process, melody for the instrument geomungo, characterized by its low pitch range and simple melodies, is the first to be generated from the initial piri melody. The daegeum, typically featuring the most complex and nuanced melodies among the six instruments, is the last in line. Table 1 displays the results of objective evaluation, specifically focusing on geomungo and daegeum.\nFor generation of geomungo melodies, ABC-like encoding yields the best results. This appears to be due to the simple and regular melodic structure of the geomungo which fits in well with ABC-like encoding. On the other hand, in the task of generating daegeum melodies, JG-like encoding achieves higher F\u2081-scores. This indicates that JG-like encoding outperforms other methods in generating complex and varied melodies. We also discover that as rhythmic complexity increases, the measure length match rate of ABC-like encoding decreases.\nTo examine the effectiveness of the beat counter technique, we compare our model that incorporates beat counter with a baseline model that instead employs absolute position embedding [19], a technique commonly used in symbolic music generation.\nThe results in the lower part of Table 1 show that the models without beat counter fail to generate melodies with appropriate lengths. The problem is less severe in ABC-like encoding, as processing accumulating duration tokens can be easier than counting jeonggan boundaries. This demonstrates the efficacy of the beat counter technique in JG-like encoding, and its ability to replace traditional positional encoding."}, {"title": "7. 15TH CENTURY MELODY TRANSFORMATION", "content": "To generate an entire ensemble score using our method, we require an initial input melody with a specified instrument. However, the remaining 15th-century score of Chihwapyeong and Chwipunghyeong only provide a single melody without any mention of instruments. It also features rhythmic groupings of eight beats, which is rare in court music that is played today. We therefore need to transform the old melody for a specific instrument used in court music; to maintain the outline of the original melody while achieving plausible transformation, we train"}, {"title": "7.1 BERT-like Masked Language Model", "content": "Bidirectional Encoder Representations from Transformers (BERT) [25] is a self-supervised language representation learning model that uses a bidirectional transformer instead of a causal transformer decoder. It is trained with a masked language model (MLM) objective, where tokens in the input sentence are randomly masked and the model predicts the original vocabulary ID of said masked tokens. Because of its advantage in exploiting bidirectional context, BERT-like models have also been adapted for music audio generation [26] and symbolic music generation [27,28] along with representation-learning purpose adaptation on symbolic music [22, 29]."}, {"title": "7.1.1 Piano-roll-like Encoding", "content": "One of the main limitations of using a BERT-like model for generative tasks is that the sequence of given (unmasked) tokens and masked tokens has to be pre-defined. This means that one has to decide the number and position of new tokens to be inserted for a given original sequence. To avoid this, we use piano-roll-like encoding for the MLM, a technique widely employed in works on music generation with limited rhythmic patterns such as in Bach Chorales [30-33]. Here, each jeonggan is represented as six frames, with each frame including features for symbol (pitch or sigimsae with duration) and for ornamentation. We also apply the aforementioned beat counter in piano-roll encoding."}, {"title": "7.1.2 Training with Masking", "content": "Following examples in MusicBERT [29], we train the model with masked language model objective with various masking methods: i) masking 5% of frames, ii) replacing 5% of frames, iii) masking 20% of note onsets, iv) replacing 10% of note onsets, v) erasing 10% of note onsets, vi) masking the entire 6 frames of 15% of jeonggans, and vii) masking 50% of ornamentations.\nThough the model can be trained to handle an arbitrary number of input instruments, we only train the model with a single instrument as with our orchestration transformer, since the main intended usage of the model is to create variations of a single melody. We train a 12-layer model with the same dataset and hyperparameter settings as with the orchestration model."}, {"title": "7.2 Inference Procedure", "content": "For converting and performing monophonic melodies, we opt for a 30x span which equals to 10 jeonggans. This also corresponds to the rhythmic pattern of the 4-7th movement of Yeominlak. The original Chihwapyeong and Chwipunghyeong melody, which can be interpreted in an 8/8 time signature, were modified by strategically inserting empty jeonggans to the 5th and 7th positions, to imitate Yeominlak's rhythmic pattern. Utilizing the masked language model, the modified melodies were seamlessly"}, {"title": "7.3 Expert Reviews", "content": "The Court Music Orchestra of the National Gugak Center performed the generated Chihwapyeong and Chwipunghyeong on the birth anniversary of King Sejong at Gyeongbokgung Palace on May 14th, 2024. They performed it again at the National Gugak Center on June 2nd, 2024 with an introduction to technical background by the authors. Due to time constraints, only partial excerpts from the entire score were performed.\nThe musicians gave positive opinions such as \"genre-specific rhythm and melodic flow were well-represented\" and \"the generated pieces presented ornamentation techniques and melodic progressions specialized for each instrument.\" Still, there were a few instances where notes that did not fit the scale appeared, and when notes outside the appropriate range were present, the performers had to alter or omit them or change their octave to perform the piece. However, the generated results were acknowledged to closely resemble the target style of Yeominlak. Thus, the Court Music Orchestra decided to play the pieces in a similar ensemble size to Yeominlak without further modification.\nWe additionally evaluate the generated scores, focusing on the effects of the refinement step. The evaluation criteria were carefully selected to assess aspects that require a deep understanding of the genre. These criteria include 1) the appropriateness of the scale and range for each instrument (scale), 2) the proper use of unique characteristics and ornamentations specific to each instrument (sigimsae), 3) the suitability of the rhythmic structure of strong and weak beats (rhythm), and 4) the harmony and coherence among the instruments when performed together as an ensemble (harmony)."}, {"title": "8. CONCLUSION", "content": "Throughout this work, we explored how music generation models can resurrect ancient melodies into new compositions that meet style of current-day Korean court music.\nVenturing into relatively uncharted territory, we approached each step meticulously\u2014from data curation and parsing to model architecture design\u2014while carefully considering the unique nuances of the musical tradition. To enhance the quality of the generated outputs, we proposed a novel encoding framework and validated its effectiveness through objective and subjective measures. This endeavour to tackle an underrepresented non-Western music genre through diverse MIR lenses hopefully expands the horizons of the field.\nThe Jeongganbo dataset and its conversion to Western staff notation in MusicXML is available online, along with other code of this project, and video recording of the performance.  To the best of our knowledge, this will be the first dataset of machine-readable Jeongganbo. We believe that this dataset can significantly contribute to computational ethnomusicology beyond its usage as a training dataset for music generation demonstrated in this paper.\nWe also provide an interactive web demo that showcases our proposed generative model. While this project focused on reviving melodies from the 15th-century, the web demo allows users to input their own melodies and create orchestrations of Korean court music. The interactive platform enables users to directly engage with the generative model in the web browser.\nWe hope that this project contributes to moving closer to leveraging machine learning to make traditional music more accessible and enjoyable for modern audiences."}]}