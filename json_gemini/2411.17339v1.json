{"title": "Knowledge-aware Evolutionary Graph Neural Architecture Search", "authors": ["Chao Wang", "Jiaxuan Zhao", "Lingling Li", "Licheng Jiao", "Fang Liu", "Xu Liu", "Shuyuan Yang"], "abstract": "Graph neural architecture search (GNAS) can customize high-performance graph neural network architectures for specific graph tasks or datasets. However, existing GNAS methods begin searching for architectures from a zero-knowledge state, ignoring the prior knowledge that may improve the search efficiency. The available knowledge base (e.g. NAS-Bench-Graph) contains many rich architectures and their multiple performance metrics, such as the accuracy (#Acc) and number of parameters (#Params). This study proposes exploiting such prior knowledge to accelerate the multi-objective evolutionary search on a new graph dataset, named knowledge-aware evolutionary GNAS (KEGNAS). KEGNAS employs the knowledge base to train a knowledge model and a deep multi-output Gaussian process (DMOGP) in one go, which generates and evaluates transfer architectures in only a few GPU seconds. The knowledge model first establishes a dataset-to-architecture mapping, which can quickly generate candidate transfer architectures for a new dataset. Subsequently, the DMOGP with architecture and dataset encodings is designed to predict multiple performance metrics for candidate transfer architectures on the new dataset. According to the predicted metrics, non-dominated candidate transfer architectures are selected to warm-start the multi-objective evolutionary algorithm for optimizing the #Acc and #Params on a new dataset. Empirical studies on NAS-Bench-Graph and five real-world datasets show that KEGNAS swiftly generates top-performance architectures, achieving 4.27% higher accuracy than advanced evolutionary baselines and 11.54% higher accuracy than advanced differentiable baselines. In addition, ablation studies demonstrate that the use of prior knowledge significantly improves the search performance.", "sections": [{"title": "1. Introduction", "content": "Graph neural networks (GNN) are promising tools for learning representations of graph-structured data and are utilized in numerous real-world applications, such as robustness analysis, computer vision, and time-series prediction [1]. GNNs propagate feature information between neighboring nodes to learn deeper feature representations. Common GNNs include the graph convolutional network (GCN) [2], graph sample and aggregate (Graph-SAGE) [3], graph attention network (GAT) [4], and graph isomorphism network (GIN) [5]. The main differences among these models lie in how they aggregate feature information between neighboring nodes and encode feature information [6]. Owing to their excellent performance and broad range of real-world applications, research on the general architectures of GNNs continues to grow. The abundance of graph representation learning techniques makes it challenging to determine an ideal GNN model for a new graph task manually. This process is time consuming and laborious, necessitating significant expertise [7].\nGraph neural architecture search (GNAS) aims to automate the customization of the optimal GNN architecture for a given graph task, providing promising directions to address the above difficulties. GNAS typically involves exploring a large search space for possible GNN architectures, evaluating their performance on the task, and using optimization algorithms to determine the best architecture. This implies that the investigation of GNAS can be categorized into two central domains: search space and search strategy. Driven by these issues, researchers have developed numerous GNAS approaches to discover high-performance architectures that surpass the manually designed architectures for various graph tasks [8, 9, 10, 11, 12]. Unlike traditional convolutional neural networks (CNNs) that operate on structured data, the GNAS search space is defined on graphs of arbitrary sizes and shapes. Therefore, the search space of GNNs is significantly larger than that of CNNs. Moreover, because graphs typically contain numerous nodes and edges, scalable GNAS methods require further research [7].\nDespite its great success, GNAS still suffers from high computational costs and poor robustness. Existing work [13, 14] illustrates that one-shot or differ-"}, {"title": "2. Preliminary", "content": "In recent years, there has been a growing interest in GNAS. Because the technology of GNNs is widely employed in different computing scenarios,"}, {"title": "2.1. Preliminary Knowledge about GNNs", "content": "Let G = (V, E) be a graph-structured data, where V = {v_1, ..., v_|\\mathcal{V}|\\}\nand E\u2286 V \u00d7 V represent the node and edge sets, respectively. \\mathcal{N}(i) =\n\\{v_j \u2208 V|(v_i, v_j) \u2208 E\\} is the neighborhood of node $v_i$. The node features are\ndenoted as X = {x_i \u2208 R^d|v_i \u2208 V}.\nGNNs are currently the most powerful tools for learning node and graph\nrepresentations. As a state-of-the-art unified framework for GNNs, the message-\npassing network can be formalized as follows [22]:\n\n$m_i^{(k)} = AGG^{(k)} (\\{W^{(k)}h_{v_j}^{(k-1)}, v_j \u2208 \\mathcal{N}(i)\\})$, (1)\n\n$h_i^{(k)} = ACT^{(k)} (f^{(k)} (m_i^{(k)}, h_i^{(k-1)}))$, (2)\n\nwhere $m_i^{(k)}$ and $h_i^{(k)}$ are accordingly the message and node representations of\nnode $v_i$ in the kth layer. $AGG^{(k)}, W^{(k)}, ACT^{(k)}$, and $f^{(k)}$ denote the aggrega-\ntion operation, learnable weights, the activation function, and the combining\nfunction in layer k, respectively. In general, the node features $X_i$ are em-\nployed to initialize the node representations $h_i^{(0)}$. The final node representa-\ntion after passing K layers is defined as $H^{(K)} = \\{h_i^{(K)}|v_i \u2208 V\\}$. In addition,\nfor graph-level tasks, the node representation $h_G$ of the entire graph $G$ is\naggregated using a readout operation R:\n$h_G = R(\\{h_j^{(K)}, v_j\u2208V\\})$. (3)"}, {"title": "2.2. Search Space", "content": "The design of the search space determines the difficulty and scalability of\nGNAS. The existing search spaces for GNNs can mainly be divided into the\nfollowing two categories: the micro search space and macro search space.\nThe micro search space is mainly employed to describe how to pass mes-\nsages between the nodes in each layer. Taking the widely used messaging"}, {"title": "2.3. Search Strategy", "content": "The existing search strategies can be divided into three main categories:\ndifferentiable methods, RL-based methods, and EAs. Differentiable methods\nconstruct a super-network that contains all possible operations. Each opera-\ntion is viewed as a probability distribution over all possible operations. The\narchitecture and model parameters are alternately optimized using gradient-\nbased algorithms [13, 28]. For example, Zhao et al. [29] first proposed a differ-\nentiable method for searching the macro-architecture and micro-architecture\nof GNNs. Wei et al. [10] further explored the impact of the GNN topology on\nthe model performance using a differentiable search method, which achieved\nstate-of-the-art results on multiple real-world graph datasets. Recently, Qin\net al. [30] introduced a multi-task GNAS method that incorporates a layer-\nwise disentangled supernet capable of managing multiple architectures, sig-\nnificantly enhancing the performance by capturing task collaborations.\nRL-based methods train a controller via the RL policy to generate high-\nperformance architectures [17, 31]. Zhou et al. [32] first designed an RL-\nbased controller to validate architectures using small steps. This method"}, {"title": "2.4. Evolutionary Transfer Optimization", "content": "The existing methods for ETO can be divided into three categories[41]:\nevolutionary multitasking [42, 43, 44], evolutionary multi-form optimization\n[45], and evolutionary sequential transfer optimization (ESTO) [46]. Reusing\nprior knowledge to solve new optimization tasks is known as sequential trans-\nfer optimization (STO), which has been applied in several fields, such as\nnumerical optimization [47, 48] and multi-objective optimization (MOO)"}, {"title": "2.5. Multi-output Gaussian Process", "content": "Because multiple objectives can be modeled simultaneously, MOGP has\nbeen employed as a surrogate model for MOO problems in recent years [60,\n61]. A popular MOGP [62] can learn a multi-objective function F(x) =\n(f_1(x), ..., f_m(x)) by assuming the following:\n\ny =\n\n\n\n\n\ny_1\n:\ny_m\n\n\n\n=\n\n\n\n\n\nf_1(x)\n:\nf_m(x)\n\n\n\n+\n\n\n\n\n\n\ne_1\n:\ne_m\n\n\n\n= F(x) + \u0454, (5)\n\n\n\nwhere $\u0454_i$ ~ $\\mathcal{N}$(0, $\u03c3_i^2$), i = 1, .., m is the Gaussian noise. The likelihood func-\ntion can then be formulated as:\n\np(y|F, x, \u03a3) = $\\mathcal{N}$(F(x), \u03a3), \u03a3 =\n\n\n\n\n\n\n$\u03c3_1^2$\n0 ... 0\n:\n0 ...\n$\u03c3_m^2$\n\n\n\n (6)"}, {"title": "3. Problem Formulation", "content": "Given a target dataset $\\mathcal{D}$ = {$\\mathcal{D}_{tra}, \\mathcal{D}_{val}, \\mathcal{D}_{tet}$}, the multi-objective GNAS\nproblem can be formulated as:\n\nA* \u2208 argmin L(a) = ($l_1$(\u03b1; w*), ..., $l_m$(a; w*))^T\n\u03b1\u2208A\ns.t. w*(a) \u2208 argmin F(w; a)\nw\u2208W\n, (11)\nwhere \u03b1 and w denote the architecture and its associated weights, respec-\ntively. A and W are the architecture and weight spaces, respectively. L =\n($l_1$, ..., $l_m$) is the upper-level vector-valued function with m objectives on the\nvalidation set $\\mathcal{D}_{val}$, such as #Acc and #Params. F is the loss function on\nthe training set $\\mathcal{D}_{tra}$ for a candidate architecture a.\nSupposing that $\u03b1_1$ and $\u03b1_2$ are two candidate architectures, $\u03b1_1$ is said to\nPareto dominate $\u03b1_2$, if and only if $l_i(\u03b1_1)$ \u2264 $l_i(\u03b1_2)$, \u2200i \u2208 \\{1,\u2026, m\\} and there\nexists at least one objective $l_j$(j \u2208 \\{1,\u2026\u2026,m\\}) satisfying $l_j(\u03b1_1) < l_j(\u03b1_2)$.\na* is Pareto optimal if no a exists such that a dominates a*. The set\nof all Pareto architectures is known as the Pareto set A*. The projection\nof the Pareto set in the objective space is known as the Pareto front. In\nsummary, multi-objective GNAS aims to determine an architecture set that\napproximates the Pareto set [65]. In lower-level optimization, learning the\noptimal weights for an architecture requires costly stochastic gradient de-\nscent over multiple epochs. Therefore, the bilevel optimization problem is\ncomputationally expensive.\nWe consider the multi-objective GNAS problem on a benchmark dataset\n(such as the Cora dataset) in NAS-Bench-Graph as a source task. KEGNAS\naims to improve the solving efficiency of a GNAS method with the aid of the\nknowledge acquired from NAS-Bench-Graph, which is formally defined as:\n\nA* = argmin [L(a)|M],\n\u03b1\u2208A (12)\nwhere M = \\{A, \\mathcal{L}^k, \\mathcal{D}_b^k, k = 1, ..., K\\} is the prior database containing the\navailable data from K source tasks in NAS-Bench-Graph. A is the wealth of"}, {"title": "4. Proposed Framework", "content": "Fig. 2 presents an overview of the KEGNAS framework, which consists of\ntwo phases: training and search. In the following, we introduce the knowledge\nmodel, DMOGP, construction of training data from NAS-Bench-Graph, and\nframework of KEGNAS in detail."}, {"title": "4.1. Knowledge Model", "content": "In KEGNAS, a knowledge model learns the mapping from datasets to\narchitectures, allowing for the rapid generation of candidate transfer archi-\ntectures for a new NAS task on an unseen dataset. Training is performed on\na prior knowledge base M containing K source NAS tasks from NAS-Bench-\nGraph. This section describes the construction of the knowledge model. We\nassume that K source NAS tasks obey a task family T(a; s):\n\nT(a; s) = \\{L(a; s)|s \u2208 S\\}, a \u2208 A, (13)"}, {"title": "4.2. DMOGP", "content": "According to the formula (11), the evaluation process of each architec-\nture must complete a lower-level optimization that involves multiple costly\niterations of gradient descent over multiple epochs. Therefore, directly evalu-\nating a large number of candidate transfer architectures in the target task to\nselect a transfer architecture is computationally expensive. We construct a\nsurrogate model to reduce the computational cost of the transfer architecture\nselection. Given a dataset $\\mathcal{D}_{gp}$ = \\{a", "L": "s ~ p(s|$\\mathcal{D}_n$), n = 1, ..., $N_{gp}$\\}, we\nemploy a DMOGP to fit the functional relationship between architectures a\nand multiple performance metrics C with the task feature s.\nThe flowchart of the DMOGP is shown in Fig. 4. First, We use a graph\nencoder f to extract the features of the architecture a. Subsequently, the\narchitecture features f(a) and task features s are post-processed by a fully\nconnected neural network g, whose output is encoded into the kernel of the\nMOGP. The construction procedure described above results in the surrogate\nmodel being learnable end-to-end. Based on task features and architecture\nfeatures, the DMOGP can be used to predict the performance of candidate\ntransfer architectures for different tasks. We provide the details of the dif-\nferent components in the following."}, {"title": "4.3. Construction of Training Data from NAS-Bench-Graph", "content": "Algorithm 1 Construction of training data\nRequire: K: Number of source tasks; N_s: Number of non-dominated fronts\nfor each source task; M = \\{A, $\\mathcal{L}^k$, $\\mathcal{D}_b^k$, k = 1, ..., K\\}: Prior knowledge\nbase from NAS-Bench-Graph.\n1: $\\mathcal{D}_{km}$ \u2190 \u00d8, N_km \u2190 0;\n2: $\\mathcal{D}_{gp}$ \u2190 \u00d8, N_gp \u2190 0;\n3: for k = 1 to K do\n4:  p(s|$\\mathcal{D}_b^k$)  Extract task feature distribution from $\\mathcal{D}_b^k$ via VGAE;\n5:  $\\mathcal{A}_{nd}$ \u2190 Fast-Nondominated-Sorting(A, $\\mathcal{L}^k$, N_s);\n6:  for each a \u2208 $\\mathcal{A}_{nd}$ do\n7:   $\\mathcal{D}_{km}$ \u2190 $\\mathcal{D}_{km}$ U\\{a, s ~ p(s|$\\mathcal{D}_b^k$)\\};\n8:   $\\mathcal{D}_{gp}$ \u2190 $\\mathcal{D}_{gp}$ \u222a \\{a, $\\mathcal{L}^k$(a), s ~ p(s|$\\mathcal{D}_b^k$)\\};\n9:   N_km \u2190 N_km + 1, N_gp \u2190 N_gp + 1;\n10:  end for\n11: end for\nEnsure: $\\mathcal{D}_{km}$ = \\{a", "N_km\\}": "Training data for knowl-\nedge model; $\\mathcal{D}_{gp}$ = \\{a", "N_gp\\}": "Training data\nfor DMOGP."}, {"title": "4.4. Framework of KEGNAS", "content": "Algorithm 2 presents the framework of KEGNAS. KEGNAS consists\nof two stages: training and search. In the training phase, we first ob-\ntain the training data from NAS-Bench-Graph, including the architecture\nlibrary, task features, and performance metrics (Algorithm 1). Subsequently,\na knowledge model and DMOGP are trained to store this prior knowledge\n(Lines 2-3 in Algorithm 2). In the search phase, the knowledge model is\nemployed to generate a set of candidate transfer architectures for a new task\n(Line 5 in Algorithm 2). Then, the DMOGP model is used to evaluate the\nperformance of the candidate transfer architectures on the new tasks (Line 6\nin Algorithm 2). Non-dominated candidate transfer architectures are selected\nto initialize the population (Line 7 in Algorithm 2). As shown in Fig. 1, each\narchitecture in the population is encoded as an eight-bit vector. When the\nnumber of transfer architectures is less than the predefined population size,\nthe remaining architectures are randomly sampled from the search space of\nNAS-Bench-Graph; otherwise, Np architectures are randomly selected from\nthe transfer architectures as the initial population (Lines 8-13 in Algorithm\n2). We can then employ any MOEA to update the population, such as NS-\nGAII [65] (Lines 14-16 in Algorithm 2). Finally, we obtain non-dominated"}, {"title": "5. Experimental Studies", "content": "This section presents a series of experimental studies on NAS-Bench-\nGraph and five real-world graph datasets to validate the KEGNAS frame-\nwork. In addition, the effectiveness of prior knowledge utilization in KEG-\nNAS is further explored. All experiments are running with PyTorch (v.\n1.10.1) and PyTorch Geometric\u00b2 (v. 2.0.3) on a GPU 2080Ti (Memory:\n12GB, Cuda version:11.3)."}, {"title": "5.1. Experimental Studies on NAS-Bench-Graph", "content": null}, {"title": "5.1.1. Experimental Settings", "content": "Datasets and Tasks. Owing to the uniqueness of the search space of ogbn-\nproteins, we only adopt eight public datasets in NAS-Bench-Graph, with\nthe exception of ogbn-proteins. The statistics of these datasets, which cover\nvarious types of complex networks, are presented in Table 1. Cora, CiteSeer,\nand PubMed \u00b3 are citation networks in which each node is a paper and\neach edge is the citation relationship between two papers. CS and Physics"}, {"title": "5.2. Experimental Studies on Real-world Graph Datasets", "content": null}, {"title": "5.2.1. Experimental Settings", "content": "Datasets and Tasks. As shown in Table 3, we consider five real-world graph\ndatasets that are widely adopted in the GNAS field: Texas, Wisconsin, Cor-\nnell, Actor, and Flickr [10]. Texas, Wisconsin, and Cornell \u2079 are the hyperlink\nnetworks representing the hyperlink relationship between web pages. Actor\nis a social network describing actor relationships, where each node is an ac-\ntor and each edge represents the co-occurrence of two actors on the same\nWikipedia page. Flickr describes common properties between different im-\nges, such as the same geographic location. All datasets can be downloaded\ndirectly from PyTorch Geometric \u00b2. For Texas, Wisconsin, Cornell, and Ac-\ntor, the nodes of each class are randomly split into 48%, 32%, and 20% for\n$\\mathcal{D}_{tra}$, $\\mathcal{D}_{val}$, and $\\mathcal{D}_{tet}$, respectively [72]. For Flickr, the nodes of each class are\nsplit into 50%, 25%, and 25% for $\\mathcal{D}_{tra}$, $\\mathcal{D}_{val}$, and $\\mathcal{D}_{tet}$, respectively [73].\nFor GNAS tasks on a real-world graph dataset, the GNAS task on all\ndatasets in NAS-Bench-Graph can be used as the source tasks. For example,\nfor the GNAS task on the Actor dataset, GNAS tasks on the eight datasets\nare regarded as eight source tasks, thus constituting the prior database M.\nThe knowledge model and DMOGP only need to be trained on M once.\nThese trained models can be utilized directly by users. For each GNAS task\non the real-world graph dataset, the knowledge model and DMOGP can\ndirectly generate and evaluate transfer architectures in only a few seconds.\nWhen constructing the prior database, similar to the experimental settings in\nthe previous section, the number of non-dominated fronts Ns for each source\ntask is set to 10."}, {"title": "5.3. Effectiveness of Prior Knowledge in KEGNAS", "content": "We perform a series of experimental studies on the NAS-Bench-Graph to\nexplore whether prior knowledge can improve the search efficiency. In addi-\ntion to KEGNAS-NSGAII, two baselines, NSGAII and RKEGNAS-NSGAII,\nare constructed to verify the effectiveness of the knowledge model and DMOGP,\nrespectively. For NSGAII, the initial population is randomly generated,\nwhich means that this is a zero-knowledge state. For RKEGNAS-NSGAII,\nthe transfer architectures are randomly selected from the candidate transfer"}, {"title": "5.4. Key Parameter Analysis", "content": "In this section, we describe an experiment using NAS-Bench-Graph to\nanalyze the population size Np of KEGNAS-NSGAII. Fig. 8 shows the aver-\nage HV values with various configurations ([10, 20, 25, 30, 40, 50]) of Np over\n20 runs. The maximum number of evaluations is 500. In the figures for the\nanalysis of parameter Np, the x-axis represents the datasets and the y-axis\nrepresents the average HV value. Given the maximum number of evaluations,\nthe different configurations of Np do not significantly affect the performance\nof KEGNAS-NSGAII as shown in Fig. 8. It is recommended to set Np to a\nsmall value to obtain better performance on all datasets."}, {"title": "6. Conclusions and Future Work", "content": "This study aimed to establish a new ETO paradigm, KEGNAS, which\nimproves GNAS performance by transferring valuable prior knowledge. In"}]}