{"title": "ROBUST WEIGHT INITIALIZATION FOR TANH NEURAL\nNETWORKS WITH FIXED POINT ANALYSIS", "authors": ["Hyunwoo Lee", "Hayoung Choi", "Hyunju Kim"], "abstract": "As a neural network's depth increases, it can achieve strong generalization per-\nformance. Training, however, becomes challenging due to gradient issues. Theo-\nretical research and various methods have been introduced to address this issues.\nHowever, research on weight initialization methods that can be effectively applied\nto tanh neural networks of varying sizes still needs to be completed. This paper\npresents a novel weight initialization method for Feedforward Neural Networks\nwith tanh activation function. Based on an analysis of the fixed points of the func-\ntion tanh(ax), our proposed method aims to determine values of a that prevent the\nsaturation of activations. A series of experiments on various classification datasets\ndemonstrate that the proposed method is more robust to network size variations\nthan the existing method. Furthermore, when applied to Physics-Informed Neural\nNetworks, the method exhibits faster convergence and robustness to variations of\nthe network size compared to Xavier initialization in problems of Partial Differ-\nential Equations.", "sections": [{"title": "INTRODUCTION", "content": "Deep learning has enabled substantial advancements in state-of-the-art performance across various\ndomains (LeCun et al., 2015; He et al., 2016). In general, the expressivity of neural networks expo-\nnentially increases with depth (Poole et al., 2016; Raghu et al., 2017), enabling strong generalization\nperformance. This increased depth, though, can result in vanishing or exploding gradients and poor\nsignal propagation throughout the model (Bengio et al., 1993), prompting the development of var-\nious weight initialization methods. Xavier initialization (Glorot & Bengio, 2010) ensures signals\nstay in the non-saturated region for sigmoid and hyperbolic tangent activations, while He initial-\nization (He et al., 2015) maintains stable variance for ReLU networks. Especially in ReLU neural\nnetworks, several weight initialization methods have been proposed to mitigate the dying ReLU\nproblem, which hinders signal propagation in deep networks (Lu et al., 2019; Lee et al., 2024).\nHowever, to the best of our knowledge, research on the initialization method to tackle the stabil-\nity of extremely deep tanh networks during training is still limited. Such networks commonly use\nXavier initialization (Raissi et al., 2019; Jagtap et al., 2022; Rathore et al., 2024) and are widely ap-\nplied in various domains, such as Physics-Informed Neural Networks (PINNs) (Raissi et al., 2019)\nand Recurrent Neural Networks (RNNs) (Rumelhart et al., 1986), with performance often dependent\non model size and initialization randomness (Liu et al., 2022).\nThe main contribution of this paper is the proposal of a simple weight initialization method for Feed-\nForward Neural Networks (FFNNs) with tanh activation function. This method facilitates effective\nlearning across a range of network sizes, outperforming Xavier initialization by reducing the need\nfor extensive hyperparameter tuning such as the number of hidden layers and units. The theoretical\nfoundation for this approach is provided through the fixed point of the function tanh(ax). We ex-\nperimentally demonstrate that the proposed method achieves higher validation accuracy and lower\nvalidation loss compared to the Xavier initialization method across various FFNN network sizes on\nthe MNIST, Fashion MNIST, and CIFAR-10 datasets. Additionally, the proposed method demon-\nstrates its effectiveness in training across various network configurations within PINNs. Notably,\nwhile Xavier initialization shows decreasing loss as network depth increases, it fails to maintain\nperformance beyond a certain depth, leading to increased loss and poor training outcomes. In con-"}, {"title": "RELATED WORKS", "content": "The expressivity of neural networks typically grows exponentially with depth, resulting in improved\ngeneralization performance (Poole et al., 2016; Raghu et al., 2017). Weight initialization is crucial\nfor training deep networks effectively (Saxe et al., 2014; Mishkin & Matas, 2016). Xavier (Glorot\n& Bengio, 2010) and He He et al. (2015) initialization are common initialization methods typically\nused with tanh and ReLU activation functions, respectively. Various initialization methods have been\nproposed to facilitate the training of deeper ReLU neural networks (Lu et al., 2019; Bachlechner\net al., 2021; Zhao et al., 2022; Lee et al., 2024). However, to the best of our knowledge, research on\nweight initialization for neural networks with tanh activation remains limited. Tanh neural networks\nhave been increasingly used, particularly in physics-informed neural networks (PINNs).\nPINNs have shown promising results in solving forward, inverse, and multiphysics problems aris-\ning in science and engineering. (Lu et al., 2021; Karniadakis et al., 2021; Cuomo et al., 2022b;a;\nYin et al., 2021; Wu et al., 2023; Hanna et al., 2022; Bararnia & Esmaeilpour, 2022; Shukla et al.,\n2020; Zhu et al., 2024; Hosseini et al., 2023; Mao et al., 2020). PINNs approximate solutions to\npartial differential equations (PDEs) using neural networks and are typically trained by minimizing\na loss defined by the sum of least-squares that incorporates the residual of PDE, boundary condi-\ntions, and initial conditions. This loss is usually minimized using gradient-based optimizers such\nas Adam (Kingma, 2014), L-BFGS (Liu & Nocedal, 1989), or a combination of both. Univer-\nsal approximation theories (Cybenko, 1989; Hornik et al., 1989; Hornik, 1991; Park et al., 2020;\nGuliyev & Ismailov, 2018b; Shen et al., 2022; Guliyev & Ismailov, 2018a; Maiorov & Pinkus,\n1999; Yarotsky, 2017; Gripenberg, 2003) guarantee the capability and performance of neural net-\nworks as an approximation of the analytic solution to PDE. However, PINNs still face challenges\nin accuracy, stability, computational complexity, and tuning optimal hyperparameters of loss terms.\nTo alleviate these issues, many authors have introduced enhanced versions of PINNs: (1) the self-\nadaptive loss balanced PINNs (lbPINNs) that automatically adjust the hyperparameters of loss terms\nduring the training process (Xiang et al., 2022), (2) the Bayesian PINNs (B-PINNs) that are spe-\ncialized to deal with forward and inverse nonlinear problems with noisy data (Yang et al., 2021),\n(3) Rectified PINNS (RPINNs) that are trained with the gradient information from the numerical\nsolution by the multigrid method and designed for solving stationary PDEs (Peng et al., 2022),\n(4) Auxiliary Pinns (A-PINNs) that effectively handle integro-differential equations (Yuan et al.,\n2022), (5) conservative PINNs (cPINNs) and exetended PINNS (XPINNs) that adopt the domain\ndecomposition technique (Jagtap et al., 2020; Jagtap & Karniadakis, 2020), (6) parrel PINNs that\nreduces the computational cost of CPINNs and XPINNs (Shukla et al., 2021), (7) gradient-enhanced\nPINNS (gPINNs) that use the gradient of the PDE loss term with respect to the network inputs (Yu\net al., 2022).\nPINNs primarily employ Xavier initialization for training (Jin et al., 2021; Son et al., 2023; Yao\net al., 2023; Gnanasambandam et al., 2023; Song et al., 2024), but our experimental results indicate\nthat this method limits the performance of larger network sizes. Although there have been recent\nresults on initialization methods for PINNs, most of them have relied on transfer learning (Tarbiyati\n& Nemati Saray, 2023). Thus, we propose a weight initialization method that does not require\ntransfer learning and is robust to variations in network size."}, {"title": "PROPOSED WEIGHT INITIALIZATION METHOD", "content": "In this section, we discuss the proposed weight initialization method. Subsection 3.1 introduces the\ntheoretical motivation behind the methodology. Subsection 3.2 presents how to derive the initial\nweight matrix that satisfies the conditions outlined in Subsection 3.1. Finally, in Subsection 3.3, we\nsuggest the optimal hyperparameter \u03c3\u2248 in the proposed method."}, {"title": "THEORETICAL MOTIVATION", "content": "Experimental results in Figure 1 reveal that when Xavier initialization is employed in FFNNs with\ntanh activation, the distribution of activation values tends to cluster around zero in deeper layers.\nThis vanishing of activation values can hinder the training process due to a discrepancy between\nthe activation values and the desired output. However, theoretically preventing this phenomena is\nnot straightforward. In this subsection, we gives a theoretical analysis based on a fixed point of\ntanh(ax) to bypass the phenomena. Before giving the theoretical foundations, consider the basic\nresults for a tanh activation function. Recall that x* is a fixed point of a function f if x* belongs to\nboth the domain and the codomain of f, and f(x*) = x*. The proofs of Lemma 1 and Lemma 2 are\nprovided in Appendix A.\nLemma 1. For a fixed a > 0 define the function $\\phi_a$: R\u2192R given as\n$\\phi_a(x) := \\tanh(ax)$.\nThen, there exists a fixed point x*. Furthermore,\n(1) if 0 < a < 1, then $\\phi$ has a unique fixed point x* = 0.\n(2) if a > 1, then $\\phi$ has three distinct fixed points: x* = \u2212\u03be\u03b1, 0, \u03be\u03b1 such that \u03be\u03b1 > 0.\nRemark that the function $\\phi_a$ can be considered as one-layer tanh FFNN.\nLemma 2. For a given initial value x0 > 0 define\n$x_{n+1} = \\phi_a(x_n), \\quad n = 0, 1, 2, ....$\nThen ${x_n}_{n=1}^\\infty$ converges regardless of the positive initial value x0 > 0. Moreover,\n(1) if 0 < a < 1, then xn \u2192 0 as n \u2192 \u221e.\n(2) if a > 1, then xn \u2192 \u03bea as n \u2192 \u221e.\nNote that the parameter a in Lemma 2 does not change across all iterations. In Propositions 3 and\nCorollary 4, we address cases where the value of a varies with each iteration."}, {"title": "THE DERIVATION OF THE PROPOSED WEIGHT INITIALIZATION METHOD", "content": "Proposition 3. Let ${a_n}_{n=1}^\\infty$ be a positive real sequence, i.e., an > 0 for all n \u2208 N, such that only\nfinitely many elements are greater than 1. Suppose that {$\\Phi_m$}$_{m=1}^\\infty$ is a sequence of functions defined\nas for each m \u2208N\n$\\Phi_m = \\phi_{a_m} \\circ \\phi_{a_{m-1}} \\circ ... \\circ \\phi_{a_1}.$\nThen for any x \u2208 R\n$\\lim_{m \\to \\infty} \\Phi_m(x) = 0.$\nProof. Set N = max{n|an > 1}. Define the sequences ${b_n}_{n=1}^\\infty$ and ${c_n}_{n=1}^\\infty$ such that bn =\ncn = an for n \u2264 N, with bn = 0 and cn = 1 for n > N. Suppose that {$m$}$_{m=1}^\\infty$ and {$\u00cem$}$_{m=1}^\\infty$\nare sequences of functions defined as for each m\u2208 N\n$\u00ce_m = \\phi_{b_m} \\circ \\phi_{b_{m-1}} \\circ ... \\circ \\phi_{b_1}, \\quad \u00cf_m = \\phi_{c_m} \\circ \\phi_{c_{m-1}} \\circ ... \\circ \\phi_{c_1}.$\nThen, the inequality $\u00ce_m < \\Phi_m < \u00cf_m$ holds for all m. By Lemma 1, for any x \u2265 0, we\nhave $\\lim_{m \\to \\infty} \u00ce_m = 0$ and $\\lim_{m\\to\\infty} \u00cf_m = 0$. Therefore, the Squeeze Theorem guarantees that\n$\\lim_{m\\to} \\Phi_m(x) = 0.$\nCorollary 4. Let \u0454 > 0 be given. Suppose that ${a_n}_{n=1}^\\infty$ be a positive real sequence such that only\nfinitely many elements are lower than 1 + \u0454. Then for any x \u2208 R \\ {0}\n$\\lim_{m \\to \\infty} \\Phi_m(x) \\geq \\xi_{1+\\epsilon}$\nProof. Set N = max{n | an < 1 + \u0454}. Define the sequence ${b_n}_{n=1}^\\infty$ such that bn = an for\nn \u2264 N, and bn = 1 + \u20ac for n > N. The remainder of the proof is analogous to the proof of\nProposition 3.\nTo establish the notation, consider a feedforward neural network with L layers. The network pro-\ncesses K training samples, denoted as pairs {(xi, Yi)}1, where x \u2208 RN is training input and\nYi \u2208 RNy is its corresponding output. The iterative computation at each layer l is defined as follows:\nx = tanh(Wlxl\u22121 + b\u00b2) \u2208 RNe for all l = 1, . . ., L,\nwhere Wl \u2208 ]RNe\u00d7Ne\u22121 is the weight matrix, be \u2208 RNe is the bias, and tanh(\u00b7) is an element-wise\nactivation hyperbolic tangent function.\nWe present a simplified analysis of signal propagation in FFNNs with the tanh activation function.\nFor notational convenience, it is assumed that all hidden layers, as well as the input and output layers,\nhave a dimension of n, i.e., Ne = n for all l. Given an arbitrary input vector x = (x1,...,xn), the\nfirst layer activation x1 = tanh(W\u00b9\u00e6) can be expressed component-wise as:\nx = tanh (w11x1 [X1 + + Win X n\nn\nWii + xj\nj=1\nj\u2260i\nXi\nXi\nFor the k + 1-th layer, i = 1, ..., n, this expression can be generalized as:\nxk+1\ni = tanh (ak+1x), where ak+1\ni = wk+1+\nWk+1xk\nWijkxj\nj=1,j\u2260i\nxk\ni,\n(1)\nfor i = 1,...,n.\nAccording to Lemma 2, when a > 1, for an arbitrary initial value x > 0 or xo < 0, the sequence\n{k} defined by xk+1 = tanh(axk) converges to \u03be\u03b1 or -\u03be\u03b1, respectively, as k \u2192 \u221e. This result\nindicates that the sequence converges to the fixed point \u03be\u03b1 regardless of the initial value 20 and en-\nsures that the activation values do not vanish as network depth increases. Furthermore, by Lemma 2,\nif a < 1 for all N \u2264 k \u2264 L, then z\u0142 approaches zero. Therefore, to ensure that (i) af remains\nclose to 1 and (ii) a < 1 does not hold for all N \u2264 k \u2264 L, we design the initial weight matrix as"}, {"title": "PREVENTING ACTIVATION SATURATION VIA APPROPRIATE \u03c3\u03b5 TUNING", "content": "In this subsection, we discuss how oz impacts the scale of the activation values. Equation 2 indicates\nthat as follows a normal distribution, with variance depending on \u03c3\u2248. Firstly, we experimentally\ninvestigated the impact of oz on the scale of the activation values. As demonstrated in Figure 2,\nincreasing \u03c3\u3047 = a/\u221aNe-1 causes the activation values in any layer to be distributed over a broader\nrange. However, setting \u03c3\u2082 to a large value can lead to saturation, where most activations converge\ntowards -1 and 1. If oz is too large, the probability that ak takes values far from 1 (e.g., -10,\n5, etc.) increases. This, in turn, increases the value of 1 + \u20ac mentioned in Corollary 4, potentially\nbounding the activation values in sufficiently deep layers by \u00a71+6. Consequently, the activation\nvalues in deeper layers become less likely to approach zero and tend to saturate toward specific\nvalues. On the other hand, if oz is too small, as mentioned in Subsection 3.2, the variance of a\nbecome restricted. This is demonstrated experimentally in Figure 2, when a 0.00001. For\nthis reason, we experimentally found an optimal \u03c3\u2082 = a/\u221aNl\u22121, with a = 0.085, that is neither\ntoo large nor too small. Results from experiments solving the Burgers' equation using PINNs with\nvarying \u03c3\u03b5 are presented in Appendix B.1."}, {"title": "EXPERIMENTS", "content": "In this section, we conduct a series of experiments to validate the proposed weight initialization\nmethod. In Subsection 4.1, we evaluate the performance of an FFNN with the tanh activation func-\ntion on benchmark datasets. In Subsection 4.2, we solve the Burgers' equation and Allen-Cahn\nequation using Physics-Informed Neural Networks. Both experiments are conducted across various\nnetwork sizes to verify whether the proposed method consistently performs well, independent of net-"}, {"title": "WIDTH INDEPENDENCE IN CLASSIFICATION TASK", "content": "To evaluate the effectiveness of the proposed weight initialization method, we conduct experiments\non the MNIST, Fashion MNIST, and CIFAR-10 (Krizhevsky & Hinton, 2009) datasets, utilizing\nthe Adam optimizer. All experiments are conducted with a batch size of 64 and a learning rate of\n0.0001. Fifteen percent of the total dataset is allocated for validation.\nWe apply the proposed weight initialization method to evaluate its effectiveness in training tanh\nFFNNs, emphasizing its robustness to variations in network width. Four tanh FFNNs are created,\neach with 20 hidden layers, and with 2, 8, 32, and 128 nodes per hidden layer, respectively. In\nTable 1, for both the MNIST and Fashion MNIST datasets, the network with 128 nodes achieves the\nhighest accuracy and lowest loss when our proposed method is employed. However, for the CIFAR-\n10 dataset, the network with 32 nodes yields the highest accuracy and lowest loss when employing\nthe proposed method. In summary, our proposed method demonstrates robustness regardless of the\nnumber of nodes in tanh FFNNs. We provide more detailed experimental results in Appendix B.2."}, {"title": "DEPTH INDEPENDENCE IN CLASSIFICATION TASK", "content": "It is well known that the expressivity of neural networks generally increases exponentially with\ndepth, enabling strong generalization performance (Poole et al., 2016; Raghu et al., 2017). There-\nfore, we employ the proposed weight initialization method to investigate its effectiveness in training\ndeep FFNNs with the tanh activation function, emphasizing its robustness to variations in network\ndepth. We create three tanh FFNNs, each with 64 nodes in all hidden layers, but with 10, 50, and\n100 hidden layers, respectively. In Table 2, for both the MNIST and Fashion MNIST datasets, the\nnetwork with 10 hidden layers achieves the highest accuracy and lowest loss when our proposed\nmethod is employed. Both initialization methods perform best in networks with the fewest layers,\nwith performance degrading as the depth increases. However, for the CIFAR-10 dataset, we observe\nthat the performance of the proposed method improves as the number of layers increases.\nFurthermore, we conduct experiments with varying hidden layer dimensions, as shown in Figure 3.\nThe network consists of 60 hidden layers, where the number of nodes alternates between 32 and 16\nin each layer. We demonstrate superior performance in terms of both loss and accuracy across all\nepochs on the MNIST and CIFAR-10 datasets."}, {"title": "NETWORK SIZE INDEPENDENCE IN PINN", "content": "Xavier initialization is the primary method used for training PINNs (Jin et al., 2021; Son et al.,\n2023; Yao et al., 2023; Gnanasambandam et al., 2023). In this section, we experimentally demon-\nstrate that the method's training performance is highly dependent on randomness and network size.\nAdditionally, empirical results are provided demonstrating that the proposed method is more robust\nto variations in network size.\nAll experiments on Physics-Informed Neural Networks (PINNs) use full-batch training with a learn-\ning rate of 0.001. In this section, we solve the Allen-Cahn and Burgers' equations using a tanh\nFFNN-based PINN with 20,000 collocation points. For the Allen-Cahn equation, the diffusion co-\nefficient is set to d = 0.01. The initial condition is defined as u(x, 0) = x\u00b2 cos(\u03c0x) for x \u2208 [\u22121, 1],\nwith boundary conditions u(\u22121, t) = \u22121 and u(1, t) = \u22121, applied over the time interval t \u2208 [0, 1].\nSimilarly, for the Burgers' equation, a viscosity coefficient of v = 0.01 is employed. The initial\ncondition is given by u(x, 0) = \u2212 sin(\u03c0x) for x \u2208 [-1,1], with boundary conditions u(\u22121, t) = 0\nand u(1, t) = 0 imposed for t \u2208 [0, 1].\nThe Allen-Cahn equation is expressed as:\n$\\frac{\\partial u}{\\partial t} = d \\frac{\\partial^2 u}{\\partial x^2} - \\frac{u^3 + u}{d}$\nwhere u(x, t) represents the solution, d is the diffusion coefficient, and the nonlinear term u\u00b3\nmodels the phase separation dynamics.\nThe Burgers' equation is given by:\n$\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = \\nu \\frac{\\partial^2 u}{\\partial x^2}$\nwhere u(x, t) is the velocity field, and v is the viscosity coefficient."}, {"title": "CONCLUSION", "content": "In this paper, we have introduced a novel weight initialization method for tanh FFNNs, grounded\nin the theoretical analysis of fixed points of the tanh(ax) function. Through our fixed-point anal-\nsis, we established conditions under which the vanishing or exploding of activation values can be\nprevented, even as the depth of the network increases.\nOur proposed method exhibits strong robustness to variations in network size, as demonstrated\nacross a variety of FFNN configurations and benchmark datasets, including MNIST, Fashion\nMNIST, and CIFAR-10. In contrast to Xavier initialization, which struggles to maintain stable\nperformance as network depth increases, the proposed method consistently achieves superior results\nby preserving activation values. Furthermore, we explored the impact of the initialization hyperpa-\nrameter \u03c3\u2248 on the distribution of activation values. We demonstrated both theoretically and experi-\nmentally that the choice of oz plays a significant role in maintaining the proper range of activations,\nbalancing between vanishing and saturation. In the context of PINNs, the proposed initialization\nmethod shows improved performance in solving PDEs such as the Burgers' equation and the Allen-\nCahn equation. By maintaining a stable loss function and achieving faster convergence compared to"}]}