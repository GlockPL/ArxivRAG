{"title": "Layer- and Timestep-Adaptive Differentiable Token Compression Ratios for Efficient Diffusion Transformers", "authors": ["Haoran You", "Connelly Barnes", "Yuqian Zhou", "Yan Kang", "Zhenbang Du", "Wei Zhou", "Lingzhi Zhang", "Yotam Nitzan", "Xiaoyang Liu", "Zhe Lin", "Eli Shechtman", "Sohrab Amirghodsi", "Yingyan (Celine) Lin"], "abstract": "Diffusion Transformers (DiTs) have achieved state-of-the-art (SOTA) image generation quality but suffer from high latency and memory inefficiency, making them difficult to deploy on resource-constrained devices. One key efficiency bottleneck is that existing DiTs apply equal computation across all regions of an image. However, not all image tokens are equally important, and certain localized areas require more computation, such as objects. To address this, we propose DiffRatio-MoD, a dynamic DiT inference framework with differentiable compression ratios, which automatically learns to dynamically route computation across layers and timesteps for each image token, resulting in Mixture-of-Depths (MoD) efficient DiT models. Specifically, DiffRatio-MoD integrates three features: (1) A token-level routing scheme where each DiT layer includes a router that is jointly fine-tuned with model weights to predict token importance scores. In this way, unimportant tokens bypass the entire layer's computation; (2) A layer-wise differentiable ratio mechanism where different DiT layers automatically learn varying compression ratios from a zero initialization, resulting in large compression ratios in redundant layers while others remain less compressed or even uncompressed; (3) A timestep-wise differentiable ratio mechanism where each denoising timestep learns its own compression ratio. The resulting pattern shows higher ratios for noisier timesteps and lower ratios as the image becomes clearer. Extensive experiments on both text-to-image and inpainting tasks show that DiffRatio-MoD effectively captures dynamism across token, layer, and timestep axes, achieving superior trade-offs between generation quality and efficiency compared to prior works. The project website is available at here.", "sections": [{"title": "1. Introduction", "content": "Diffusion models have recently demonstrated outstanding performance in image generation, with architectures evolv-"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Diffusion Models", "content": "Diffusion models [13, 38] have demonstrated superior per-formance over prior SOTA generative adversarial networks (GANs) in image synthesis tasks [7]. Early diffusion mod-els primarily utilized U-Net architectures. Subsequent work introduced several improvements, such as advanced sam-pling [16, 21, 39] and classifier-free guidance [12]. Al-though effective, these models suffered from high genera-tion latency due to processing directly in pixel space, thus limiting their practical applications. The introduction of La-tent Diffusion Models (LDMs) [32] marked a significant ad-vancement by encoding pixel space into a more compact latent space through training a Variational Auto-Encoder (VAE). This reduced the computational cost of the diffusion process, paving the way for widely used models like Stable Diffusion Models (SDMs) [28]. More recently, researchers"}, {"title": "2.2. Efficient Diffusion and DiT Models", "content": "DiTs [27] are resource-intensive due to the transformer ar-chitecture, with the attention module exhibiting quadratic complexity relative to the number of tokens. Previous work has mainly focused on optimizing DiTs' deployment effi-ciency along three dimensions: token, layer, and timestep. For tokens, researchers have introduced techniques like token merging [2] to merge similar tokens, token prun-ing [43] or image resolution downsampling [37] to remove redundant tokens, and LazyDiffusion [26], which is special-ized for the inpainting task and bypasses generating back-ground tokens. For layers, methods such as layer [17] and channel [9] pruning, as well as intermediate feature caching [20, 23, 52], have been proposed to skip redun-dant computations. For timesteps, strategies include dis-tillation to reduce the required number of timesteps, which has been explored for UNets [15, 21, 33, 34, 53, 54] al-though there is no reason to believe these techniques cannot apply for Transformers, and asymmetric sampling, which has been applied to Transformer architectures and allocates more samples to undersampled stages and fewer to stages that have already converged [29, 44]. Additionally, to ac-celerate diffusion T2I models, more specialized techniques have been introduced [3, 4]. In contrast, our proposed DiffRatio-MoD is a learnable and unified dynamic DiT in-ference framework with differentiable compression ratios across layers and timesteps, exploring the compounded ef-fects of compression across all three axes. However, we do not explore few step distillation (e.g. [53]) in this paper, since it is an orthogonal acceleration."}, {"title": "2.3. Dynamic Inference", "content": "Model compression [6] offers a static approach to improv-ing inference efficiency, while dynamic inference [31, 46, 47, 50, 56] enables adaptive compression based on inputs, layers, or other conditions. For example, early exiting meth-ods [14, 22, 40] predict the optimal point for early termi-nation within intermediate layers, allowing the model to exit before completing all computations. Dynamic layer-skipping methods [46, 47, 50] selectively execute subsets of layers for each input, often utilizing a gating network to make decisions on the fly. At a finer granularity, researchers have also explored channel skipping [9, 24] and mixture-of-depths (MoD) approaches [31], which select specific sub-sets of layers for individual tokens rather than processing the entire input uniformly. In contrast, our DiffRatio-MoD is the first to introduce a unified dynamic DiT inference"}, {"title": "3. The Proposed DiffRatio-MoD Framework", "content": "In this section, we present the proposed DiffRatio-MoD framework. First, we provide an overview of the method. Then, we detail the three enablers: (1) the token-level rout-ing scheme for mixture-of-depths (MoD) DiTs in Sec. 3.2; (2) the layer-wise differentiable MoD compression ratio scheme in Sec. 3.3; and (3) the timestep-wise differentiable MoD compression ratio scheme in Sec. 3.4."}, {"title": "3.1. Overview of DiffRatio-MoD", "content": "Motivated by the need for unified and dynamic compression during DiT inference, DiffRatio-MoD introduces a token-level routing scheme to dynamically learn the importance of each token on the fly. As illustrated in Fig. 1 (a), simi-lar to previous mixture-of-depths (MoD) work [31] for NLP tasks, each DiT layer incorporates a lightweight router us-ing a single linear layer to predict the importance of each token based on the input image/noise and text embedding. This allows us to bypass computations for less important tokens in each layer and directly concatenate their cached activations to the final layer outputs. Consequently, each token is processed by only a selective subset of layers. Visu-alization of these routers' predictions reveals that different layers or timesteps favor varying compression ratios-for instance, some layers prioritize generating objects, while others focus on backgrounds-highlighting the need for adaptable compression across layers and timesteps. To achieve such dynamic compression, DiffRatio-MoD incor-porates a differentiable compression ratio scheme, as shown in Fig. 1 (b). This scheme includes a learnable scalar pa-rameter that represents a continuous compression ratio, and"}, {"title": "3.2. Enabler 1: Token-level Routing Scheme", "content": "Motivation. We are motivated by the varying computa-tional demands across tokens, where many of them require fewer layers for efficient processing. We start from the same token-level routing scheme as MoD [31]. We remove from MoD two features that were specialized for the acausal NLP task: specifically, we remove the auxiliary loss and aux-iliary MLP predictor from Section 3.5 of their paper. To the best of our knowledge, our paper is the first application of MoD to the vision domain, so we next review the rout-ing mechanism, perform some visualizations, and report in-sights for vision tasks.\nToken-level Routing. DiTs process noise and condi-tional text embeddings as inputs, aiming to denoise and generate images in an end-to-end manner. To predict token importance, we employ a simple yet effective token-level routing scheme from MoD [31]. As illustrated in Fig. 1 (a), each DiT layer incorporates a lightweight router composed of a single linear layer with a sigmoid activation function, predicting each token's importance on a scale from 0 to 1. After passing through the routers, we select the top-k most important tokens for this layer's processing, while the ac-tivations of other tokens are cached and concatenated with the layer outputs, bypassing the entire layer computation, including both attention and MLPs. To enable gradient flow to the router's weights during joint fine-tuning with pre-"}, {"title": "3.3. Enabler 2: Layer-wise Differentiable Ratio", "content": "Motivation. Recognizing that different layers prioritize dif-ferent objects or background elements and thus benefit from distinct compression ratios, we propose a novel layer-wise differentiable compression ratio mechanism. This approach automatically learns each layer's compression ratio from a zero initialization in a differentiable manner, adapting to the varying redundancy levels across layers.\nDesign Choice. Before designing DiffRatio-MoD, we address a key choice: discrete proxy or continuous ratio representation. Previous work [5] uses a discrete proxy with multiple compression ratio candidates and learnable proba-"}, {"title": "3.4. Enabler 3: Timestep-wise Differentiable Ratio", "content": "Motivation. In addition to layer-wise ratio variances, we also observe that the model exhibits varying levels of re-dundancy across timesteps. This motivates us to explore an approach for timestep-wise compression ratios as well.\nTimestep-wise Differentiable Ratio. On top of the layer-wise DiffRatio-MoD, we introduce learnable param-eters specific to different timestep regions. For the image inpainting task, following the previous SOTA Lazy Diffu-sion [26], we use 1,000 training timesteps and 100 sampling timesteps, which we evenly divide into 10 regions. We as-sign 10 learnable parameters per layer accordingly, result-ing in a total of 280 learnable parameters. Similarly, for T2I tasks, following PixArt-\u03a3 [4] with 20 timesteps, we divide them into 4 regions, assigning 4 parameters per layer, yield-ing 112 learnable parameters. The same as before, we apply an MSE loss between the averaged learned ratios within the batch and the target ratio to ensure convergence."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experiment Settings", "content": "Tasks, Datasets, and Models. Tasks & Datasets. We eval-uate the DiffRatio-MoD on two representative image gener-ation tasks using corresponding benchmark datasets: (1) an image inpainting task on an internal dataset of 220 million high-quality images, covering diverse objects and scenes. Masks and text prompts are generated following [26, 51]; and (2) a T2I task on the LAION-5B dataset [36], restricted to image samples with high aesthetic scores, English text, and a minimum text similarity score of 0.24. Models. We in-tegrate our proposed DiffRatio-MoD approach with SOTA models. For the inpainting task, we use Lazy Diffusion (an adapted PixArt-a model with an additional ViT encoder) to generate images at 1024\u00d71024 resolution. For the T2I task, we use PixArt-\u2211 to generate images at 512\u00d7512 resolution.\nTraining and Sampling Setting. For the inpainting task, we fine-tune the model parameters until convergence using the AdamW optimizer [18] with a learning rate of 10-4 and weight decay of 3 \u00d7 10-2. For sampling, images are generated using IDDPM [25] with 100 timesteps and a CFG factor of 4.5. For the T2I task, we fine-tune the model using a LoRA adapter with a rank of 32 until both training and validation losses converge, and the MSE loss between the current and target compression ratios drops to approx-imately zero for DiffRatio-MoD models. During training, we calculate the diffusion loss with IDDPM [25] over 1K timesteps. For sampling, we generate images using DPM-solver [19] with 20 timesteps and a CFG factor of 4.5. All training is conducted on a cluster of 8\u00d7A100-80GB GPUs.\nBaselines and Evaluation Metrics. Baselines. For both the T2I and inpainting tasks, we compare the pro-posed DiffRatio-MoD against SOTA baselines, including ToMe [2], AT-EDM [43], and our adapted MoD with uni-form MoD compression ratio. For the inpainting task, we also compare against RegenerateCrop, which generates a tight square crop around the masked region, similar to pop-ular software frameworks [42, 48], and RegenerateImage,"}, {"title": "4.2. DiffRatio-MoD over SOTA Baselines", "content": "Text-to-Image. To assess the effectiveness of our proposed DiffRatio-MoD, we apply our proposed DiffRatio-MoD to the general text-to-image task and compare it with previous token merging [2] and pruning [43] baselines. Specifically, we apply these compression methods on PixArt-\u03a3, a SOTA publicly accessible T2I model known for its high-resolution image generation quality and efficiency tradeoffs. As shown in Tab. 1, PixArt-\u03a3 with DiffRatio-MoD significantly im-proves generation quality, achieving 57.83 and 241.11 FID reductions over ToMe [2] and AT-EDM [43], respectively, with comparable or even lower latency (\u21938.59%~20.15%) and memory usage (\u2193-2.71%~0.72%). Also, under similar latency compared to ToMe [2] with 20% compression ratio, DiffRatio-MoD achieves 335.23 FID reductions. More-over, PixArt-\u03a3 with DiffRatio-MoD also achieves compa-rable image generation quality with uncompressed PixArt-\u2211, while delivering 20.68% and 8.33% latency and memory savings. Note that we compare with fine-tuned PixArt-\u2211 on the LAION datasets for a fair comparison. This set of ex-periments demonstrates the effectiveness of DiffRatio-MoD for general T2I tasks.\nImage Inpainting. We further extend the DiffRatio-MoD to the inpainting task. Specifically, we apply it on top of the SOTA Lazy Diffusion (LD) [26], which uses a DiT decoder to generate only the masked areas rather than the entire image, leveraging a separate ViT encoder to cap-ture the global context of the input masked images. We compare our DiffRatio-MoD approach against two types of baselines: (1) RegenerateImage and RegenerateCrop, and (2) LD with previous token merging [2] or pruning [43] techniques. As shown in Tab. 2, our DiffRatio-MoD con-sistently outperforms all baselines in terms of accuracy-efficiency tradeoffs. For example, LD with DiffRatio-MoD achieves FID reductions of 47.35 and 189.93 compared to LD with ToME [2] or AT-EDM [43], while achieving simi-lar or up to 23.61% and 13.63% higher latency and memory savings. Also, under similar memory usage compared to ToMe [2] with 30% compression ratio, LD with DiffRatio-MoD achieve 265.94 FID reduction while delivering up to 21.54% latency savings. Moreover, compared to Regen-erateImage, our method achieves 73.51%/60.26% FLOPs and latency savings when inpainting 2562 mask sizes within 10242 images. Notably, like Lazy Diffusion, our method's"}, {"title": "4.3. Ablation Studies of DiffRatio-MoD", "content": "We conduct ablation studies on DiffRatio-MoD, analyz-ing the contributions of the three enablers described in Sec. 3. As shown in Tabs. 2 and 1, we report the perfor-mance of LD or PixArt-\u03a3 with MoD (Sec. 3.2), DiffRatio-MoD-L (Sec. 3.3), DiffRatio-MoD-LT (Sec. 3.4) for in-painting and T2I tasks, respectively. The results consis-"}, {"title": "4.4. Qualitative Visual Examples", "content": "Visual Examples. We select challenging input prompts to evaluate the qualitative results of our proposed DiffRatio-MoD. As shown in Fig. 6, the examples demonstrate that DiffRatio-MoD achieves comparable or even superior gen-eration quality compared to the RegenerateCrop baseline and even uncompressed LD or PixArt-\u2211 for inpainting and T2I tasks, respectively. Note that ToMe and AT-EDM are omitted here due to their poor generation quality when ap-plied to DiTs, even at a mere 10% compression ratio.\nHuman Preference Scores. We use a computer vision model to estimate likely human preferences and assess the models' ability to generate high-quality, contextually rele-vant images. Specifically, we generated 2K samples for the"}, {"title": "5. Conclusion", "content": "In this work, we present DiffRatio-MoD, a dynamic DiT inference framework with differentiable compression ra-tios that adaptively routes computation across tokens, lay-ers, and timesteps, creating MoD DiT models. Specif-ically, DiffRatio-MoD incorporates a token-level routing scheme based on MoD that dynamically learns the impor-tance score of each token, alongside a novel module that makes MoD differentiable with respect to compression ra-tios, enabling the model to learn adaptive compression ra-tios for each layer and timestep. Redundant layers and"}, {"title": "A. More Visualization of Token Routers", "content": "In Sec. 3.2, we provided an example visualization of the router's predictions to evaluate the effectiveness of our DiffRatio-MoD router. Here, we present additional visu-alization examples in Fig. 9 to further validate our findings. Our observations consistently demonstrate the following:\n(1) The router effectively captures semantic information, clearly delineating object shapes and achieving an attention-like effect while significantly reducing computational costs.\n(2) The predicted token importance varies across layers and timesteps. For example, some layers focus on object gener-ation, while others emphasize background areas. Addition-ally, as timesteps progress, the router increasingly captures the semantic contours of objects, highlighting the impor-tance of dynamic token importance estimation. (3) The op-timal compression ratio differs across layers and timesteps. For instance, some layers assign high importance to all to-kens, indicating minimal redundancy, while others selec-tively prune tokens from objects or backgrounds with dis-tinct shapes, requiring different compression ratios. This variance is also observed across timesteps. In the previ-ous MoD [31] approach, a fixed global compression rate is uniformly applied across layers and timesteps, ignor-ing their individual significance. Such uniform pruning risks over-pruning critical layers or timesteps while under-compressing redundant ones. This observation underscores the need for adaptive and dynamic compression ratios tai-lored to both layers and timesteps."}, {"title": "B. Ratio Trajectory Analysis for the T2I Task", "content": "In Sec. 3.3, we visualized the ratio trajectory for inpainting tasks trained with our proposed layer-wise DiffRatio-MoD. Here, we also supply the training trajectory of compression ratios for all layers during fine-tuning of a PixArt-\u03a3 model on a T2I task, as shown in Fig. 8 (a-c). The visualization consistently reveals that: (1) Each layer learns its unique compression ratio, with redundant layers achieving higher compression and critical layers remaining less or entirely uncompressed; (2) The average ratio across layers gradually converges to the target ratio. In this example, with a target of 20%, the final achieved average ratio is approximately 19%, indicating a minor gap. Notably, a trade-off exists be-tween convergence speed and generation quality: a higher MSE loss coefficient for the ratio accelerates convergence but may degrade quality due to overly rapid compression, while a smaller coefficient promotes gradual convergence"}, {"title": "C. Correlation Between Learned Compression Ratios and Router Predictions", "content": "We select three representative layers with high, medium, and low learned compression ratios to visualize the corre-sponding predictions of the DiffRatio-MoD router and an-alyze potential correlations. As shown in Fig. 10, where \"C.R.\" denotes the compression ratios, we observe a strong correlation between the learned ratios and the router's pre-dictions. For layers with high compression ratios, such as layer 1 in inpainting or layer 9 in T2I, the router consis-tently predicts lower importance scores for many semantic areas, adopting an extremely \"lazy behavior\" to save com-putations. Conversely, for layers with low compression ra-tios, the router assigns higher importance scores to most ar-eas. This visualization validates the joint learning effect be-tween our token-level routers and the differentiable ratios."}, {"title": "D. Trade-offs for Choosing Timestep Regions", "content": "In Sec. 3.4, we introduced the timestep-wise DiffRatio-MoD, where the timestep regions are evenly divided into 10 regions for inpainting tasks with a total of 100 sam-pling timesteps, and 4 regions for T2I tasks with 20 sam-pling timesteps. Here, we provide additional guidance on selecting the number of timestep regions and the associated trade-offs. A larger number of timestep regions allows for learning finer-grained and more precise compression ratios across all timesteps. However, too many regions can make training unstable and challenging. To reduce training com-plexity and enhance stability, we select a smaller number of regions, such as 4 for T2I tasks. Conversely, using too few regions risks oversimplifying the method, reducing it to heuristic approaches like SpeeD [44], which manually de-fines three timestep regions. In practice, we choose between 4 and 10 timestep regions to balance granularity and stabil-ity. While our approach aligns with the general insights of SpeeD, it is more systematic and adaptive. Unlike manual"}, {"title": "E. Overall Comparison Figure", "content": "In Sec. 4.2, we presented a comprehensive comparison of our DiffRatio-MoD method against baseline approaches for both inpainting and T2I tasks. Here, we provide the overall comparison figures to better illustrate the achieved improve-ments in FID and latency reductions. As shown in Fig. 11, our DiffRatio-MoD consistently delivers superior trade-offs between FID and latency, achieving FID reductions of 12.10 and 4.92 for T2I and inpainting tasks, respectively, at com-parable GPU latency when compared to the most competi-tive baseline."}, {"title": "F. Model Trajectories of DiffRatio-MoD", "content": "In Sec. 4.2, we visualized the model trajectories during the training of DiffRatio-MoD-L for both T2I and inpainting"}, {"title": "G. More Visualization of Visual Examples", "content": "In Sec. 4.4, we selected challenging input prompts to eval-uate the qualitative performance of our proposed DiffRatio-MoD. Here, we provide additional visual examples, as shown in Fig. 12. The examples consistently demonstrate that DiffRatio-MoD achieves comparable or even superior generation quality compared to the RegenerateCrop base-line and even uncompressed LD or PixArt-\u03a3 for inpaint-ing and T2I tasks, respectively. Note that ToMe [2] and AT-EDM [43] are omitted here due to their poor generation quality when applied to DiTs, even at a modest compression"}, {"title": "H. Human Preference Score for Inpainting", "content": "In Sec. 4.4, we utilized a computer vision model to estimate likely human preferences and evaluate the ability of mod-els to generate high-quality, contextually relevant images for the T2I task. Here, we additionally supply the evalu-ation for inpainting tasks. Specifically, we generated 2K samples for the inpainting task and used HPSv2 [49] to as-sess human preferences for images produced by different methods. As shown in Tab. 4, for inpainting tasks, we ap-plied all compression methods to Lazy Diffusion (LD) [26]. DiffRatio-MoD achieves a higher human preference score of 2.181/0.263 compared to previous compression methods, ToMe [2] and vanilla MoD [31], respectively."}]}