{"title": "Learning in Markov Games with Adaptive Adversaries: Policy Regret, Fundamental Barriers, and Efficient Algorithms", "authors": ["Thanh Nguyen-Tang", "Raman Arora"], "abstract": "We study learning in a dynamically evolving environment modeled as a Markov game between a learner and a strategic opponent that can adapt to the learner's strategies. While most existing works in Markov games focus on external regret as the learning objective, external regret becomes inadequate when the adversaries are adaptive. In this work, we focus on policy regret \u2013 a counterfactual notion that aims to compete with the return that would have been attained if the learner had followed the best fixed sequence of policy, in hindsight. We show that if the opponent has unbounded memory or if it is non-stationary, then sample-efficient learning is not possible. For memory-bounded and stationary, we show that learning is still statistically hard if the set of feasible strategies for the learner is exponentially large. To guarantee learnability, we introduce a new notion of consistent adaptive adversaries, wherein, the adversary responds similarly to similar strategies of the learner. We provide algorithms that achieve \u221aT policy regret against memory-bounded, stationary, and consistent adversaries.", "sections": [{"title": "Introduction", "content": "Recent years have witnessed tremendous advances in reinforcement learning for various challenging domains in AI, from the game of Go, real-time strategy games such as StarCraft II and Dota , autonomous driving, to socially complex games such as hide-and-seek, capture-the-flag, and highly tactical games such as poker game Texas hold' em. Notably, most challenging RL applications can be systematically framed as multi-agent reinforcement learning (MARL) wherein multiple strategic agents learn to act in a shared environment. \nDespite the empirical successes, the theoretical foundations of MARL are underdeveloped, especially in settings where the learner faces adaptive opponents who can strategically adapt and react to the learner's policies. Consider for example the optimal taxation problem in the AI economist, a game that simulates dynamic economies that involve multiple actors (e.g., the government and its citizens) who strategically contribute to the game dynamics. The government agent learns to set a tax rate that optimizes for the economic equality and productivity of its citizens, whereas the citizens who perhaps have their own interests, respond adaptively to tax policies of the government agent (e.g., relocating to states that offer generous tax rates). Such adaptive behavior of participating agents is a crucial component in other applications as well, e.g., mechanism design , optimal auctions ."}, {"title": "Related work", "content": "Learning in Markov games. Learning problems in Markov games have been studied extensively in the MARL literature. Most existing works focus on learning Nash equilibria either with known dynamics or infinite data or otherwise in a self-play setting wherein we control all the players, or in an online setting wherein we con-"}, {"title": "Problem setup", "content": "Markov games. In this paper, we use the framework of Markov Games to study an interactive multi-agent decision-making and learning environment . Markov games extend Markov decision processes (MDPs) to multiplayer scenarios, where each agent's action affects not only the environment but also the subsequent state of the game and the actions of other agents. Formally, a standard two-player Markov Game (MG) is specified by a tuple M = (S, A, B, H, P,r).\nHere, S denotes the state space with cardinality |S| = S, A is the action space of the first player (called learner) with cardinality |A| = A, B is the action space of the second player (referred to as an opponent or an adversary) with cardinality |B| = B, H \u2208 N is the time horizon for each game. P = {P\u2081, ..., PH} are the transition kernels with each Ph : S \u00d7 A \u00d7 B \u2192 \u2206(S) specifying the probability of transitioning to the next state given the current state, learner's action, and adversary's action (\u2206(S) denotes the set of all probability distributions over S). Finally, r = {r\u2081, ..., rh} are the (expected) reward functions with each rh : S \u00d7 A \u00d7 B \u2192 [0, 1]. For simplicity, we assume the learner knows the reward function.\nEach episode begins in a fixed initial state s\u2081. At step h \u2208 [H], the learner observes the state sh and picks her action ah \u2208 A while the opponent/adversary picks an action bh \u2208 B. As a result, the learner observes bh, receives reward rh(sh, ah,bh) and the environment transitions to sh+1 ~ Ph(sh, ah, bh). The episode terminates after H steps.\nPolicies and value functions. A learner's policy (also referred to as strategy) is any tuple \u03c0 = {\u03c0h}h\u2208[H] where \u03c0h : (S \u00d7 A)h\u207b\u00b9 \u00d7 S \u2192 \u2206(A). A policy \u03c0 = {\u03c0h}h\u2208[H] is said be Markovian if for every h \u2208 [H], \u03c0h : S \u2192 \u2206(A). Similarly, an adversary's policy is any tuple \u03bc = {\u03bch}h\u2208[H] where \u03bch : (S\u00d7B)h\u207b\u00b9 \u00d7 S \u2192 \u2206(B). \u03bc is said to be Markovian if for every h, \u03bch : S \u2192 \u2206(B). For simplicity, we will focus only on Markov policies for both the learner and the adversary in this paper. Let \u03a0 (respectively, \u03a8) be the set of all feasible policies of the learner (respectively, the adversary). The value of a policy tuple (\u03c0, \u03bc) \u2208 \u03a0 \u00d7 \u03a8 at step h in state s, denoted by $V_{h}^{\\pi, \\mu}(s)$ is the expected accumulated reward starting in state s from step h, if the learner and the adversary follow \u03c0 and \u03bc respectively, i.e., $V_{h}^{\\pi, \\mu}(s) := E_{\\pi,\\mu}[\\sum_{i=h}^{H}r_i(s_i, a_i, b_i)|s_h = s]$, where the expectation is with respect to the trajectory (s\u2081, a\u2081,b\u2081, r\u2081, ...,\u0455\u043d,\u0430\u043d,\u0431\u043d,\u043d) distributed according to P, \u03c0, and \u03bc. We also denote the action-value function $Q_{h}^{\\pi, \\mu}(s,a,b) := E_{\\pi,\\mu}[\\sum_{i=h}^{H}r_i(s_i, a_i, b_i)|(s_h, a_h,b_h) = (s,a,b)]$."}, {"title": "Adaptive adversaries", "content": "Given a V : S \u2192 R, we write $P_hV(s,a,b) := E_{s'\\sim P_h(\\cdot|s,a,b)} [V(s')]$. For any u : S \u2192 \u0394(\u0391), v : S \u2192 \u0394(B), Q : S\u00d7A\u00d7B \u2192 R, denote $Q(s, u, v) := E_{a\\sim u(\\cdot|s),b\\sim v(\\cdot|s)} [Q(s, a, b)]$ for any s \u2208 S.\nAdaptive adversaries. We allow the adversary to be adaptive, i.e., the adversary can choose their policy in episode t based on the learner's policies on episodes 1, ..., t. We assume that the adversary is deterministic and has unlimited computational power, i.e., the adversary can plan, in advance, using as much computation as needed, as to how they would react in each episode to any sequence of policies. Formally, the adversary defines in advance a sequence of deterministic functions {ft}t\u2208N*, where ft : \u03a0\u1d57 \u2192 \u03a8. The input to each response function ft is an entire history of the learner's policies, including her policy in episode t. Therefore, if the learner follows policies \u03c0\u00b9, ..., \u03c0\u207a, the adversary responds with policy ft(\u03c0\u00b9, ..., \u03c0\u207a) \u2208 \u03a8 in episode t. Since the response function ft depends on the learner's policy at round t, our setup is essentially a principal-follower model, akin to Stackelberg games and mechanism design for learning agents. In this context, the principal agent (mechanism designer or learner) publicly declares a strategy before committing to it, allowing the followers to subsequently choose their strategies based on their understanding of the principal's decisions.\nWe evaluate the learner's performance using the notion of policy regret, which compares the return on the first T episodes to the return of the best fixed sequence of policy in hindsight. Formally, the learner's policy regret after T episodes is defined as\n$PR(T) = \\sup_{\\pi \\in \\Pi} \\sum_{\\tau=1}^{T}V_{1}^{\\pi, f_{\\tau}([\\pi]^{*})}(s_1) - V_{1}^{\\pi^{\\tau}, f_{\\tau}(\\pi^{1},...,\\pi^{\\tau})}(s_1)$, where $f_{\\tau}([\\pi]^{*}) := f_{\\tau} (\\underbrace{\\pi,..., \\pi}_{t \\text{ times}})$.(1)\nPolicy regret has been studied in online (bandit) learning and repeated games , yet, to the best of our knowledge, it has never been studied in Markov games. Policy regret differs from the more common definition of external regret defined as $R(T) = \\sup_{\\pi \\in \\Pi} \\sum_{t=1}^{T}V_{1}^{\\pi, f_t(\\pi^{1},...,\\pi^{t})}(s_1) - V_{1}^{\\pi^t, f_t(\\pi^{1},...,\\pi^{t})}(s_1)$, which is used in . However, external regret is inadequate for measuring the learner's performance against an adaptive adversary. Indeed, when the adversary is adaptive, the quantity $V^{\\pi^t, f_t(\\pi^{1},...,\\pi^{t})}_{1}$ is hardly interpretable anymore \u2013 see for a more detailed discussion.\nAs a warm-up, we show in the following example that, policy regret minimization generalizes the standard Nash equilibrium learning problem in zero-sum two-player Markov games.\nExample 3.1 (Nash equilibrium). Consider the adversary with the following behavior: for any Markov policy \u03c0 of the learner, the adversary ignores all the learner's past policies and respond only to the current policy with a Markov policy $f(\\pi)$ such that for all (s,h), $V_{h}^{\\pi, f(\\pi)}(s) = \\min_{\\mu} V_{h}^{\\pi, \\mu}(s)$, where the minimum is taken over all the possible Markov policies for the adversary. By Filar and Vrieze , such an $f(\\pi)$ exists. In addtion, there also exists a Markov policy \u03c0* such that for all (s, h), $V_{h}^{\\pi*, f(\\pi*)}(s) = \\sup_{\\pi} V_{h}^{\\pi, f(\\pi)}(s) = \\inf_{\\mu} \\sup_{\\pi} V_{h}^{\\pi, \\mu}(s)$. The policies $(\u03c0*, f(\u03c0*))$ is a Nash equilibrium of the Markov game. For such an adversary, the policy regret becomes $PR(T) = \\sum_{t=1}^{T}V_{1}^{\\pi*, f(\\pi*)}(s_1) - \\sum_{t=1}^{T}V_{1}^{\\pi^t, f(\\pi^t)}(s_1)$. This Nash equilibrium can be computed using, e.g., the Q-ol algorithm of with \u221aT (policy) regret.\nAdditional notation. We write $f \\lesssim g$ to mean $f = O(g)$. We use c to represent an absolute constant that can have different values in different appearances."}, {"title": "Fundamental barriers for learning against adaptive adversaries", "content": "In this section, we show that achieving low policy regret in Markov games against an adaptive adversary is statistically hard when (i) the adversary has an unbounded memory (see Definition 1), or (ii) the adversary is non-stationary, or (iii) the learner's policy set is exponentially large (even if the adversary is memory-bounded and stationary)."}, {"title": "Efficient algorithms for learning against adaptive adversaries", "content": "Thus far, we have shown that learning against an adaptive adversary in Markov games is statistically hard, even when the adversary is m-memory bounded and stationary. The reason that stationarity is not sufficient for efficient learning (which the lower bound in Theorem 3 exploits for the construction of a hard instance) comes from the unstructured response of the adversary in the worst case. Even if the learner plays nearly identical sequence of policies differing only on a small number of states and steps, the adversary can essentially respond completely arbitrarily. In other words, knowing the policies that the adversary plays in response to the policies of the learner (i.e., observing the values of the response function f at specific inputs) reveals zero information about the function f on previously seen inputs. Thus, the learner is required to explore all the policies in \u03a0 to be able to identify an optimal policy. This motivates us to consider an additional structural assumption on how the adversary responds to the learner's policies. We assume that the adversary is consistent in response to two similar sequences of policies of the learner. In essence, given that the learner plays two sequences of policies that agree on certain states (s) and steps (h) \u2013 then, we assume that the opponent also responds with two sequences of policies that agree on the same states and steps. We refer to this behavior as consistent; a formal definition follows.\nDefinition 3 (Consistent adversaries). An m-memory bounded and stationary adversary f is said to be consistent if, for any two sequences of learner's policies \u03c0\u00b9, ..., \u03c0\u1d50 and v\u00b9, ..., v\u1d50, and any (s, h) \u2208 S \u00d7 [H], if $\u03c0_i(\u00b7|s) = v_i(\u00b7|s), \\forall i \\in [m]$, then $f(\u03c0\u00b9, ...,\u03c0m)h(\u00b7|s) = f(v1,...,vm)h(\u00b7|s)$. Otherwise, we say that the opponent's response f is arbitrary.\nWe argue that the definition above is natural if we are to consider opponents that are self-interested strategic agents, and not simply a malicious adversary. So, it would be in an opponent's interest to play in a somewhat consistent manner. Playing optimally after figuring out the learner's strategy would indeed require playing consistently. An opponent that plays completely arbitrary, while challenging to learn anything from, also does not improve their value function. Some remarks are in order.\nRemark 1 (-approximately consistent adversaries). Our algorithms and results for consistent adversaries easily extend to (-approximately consistent adversaries for any fixed constant ( \u2265 0. An adversary f is said to be \u03da-approximately consistent if, for any \u03c0\u00b9, ..., \u03c0\u1d50 and v\u00b9, ..., v\u1d50, and any (s,h) \u2208 S \u00d7 [H], if $|\u03c0_i(\u00b7|s) - v_i(\u00b7|s)|_{\\infty} \\le \\zeta, \\forall i \\in [m]$, then $\\max_{a \\in A} \\log \\frac{f(\\pi^1,...,\\pi^m)_h(a|s)}{f(\\nu^1,...,\\nu^m)_h(a|s)} \\le \\zeta$.\nFor simplicity, we stick with Definition 3 (i.e., (\u03b6 = 0) to best convey our algorithmic and theoretical ideas.\nRemark 2. While our notion of consistent behaviors is quite natural, it might as well be that there is a more general notion of complexity for the opponent's response function classes that fully characterizes learnability in this setting. This likely requires the definition of appropriate norms in the input policy space \u03a0\u1d50 and the output policy space \u03a8, and a certain notion of predictability for the opponent's response function classes (e.g., in the spirit of Eluder dimension), so that the learner can accurately estimate the opponent's response function, without trying out all possible policies. This question goes beyond the scope of our current work and is left to a future investigation.\nRemark 3. To permit learnability in terms of external regret in Markov games, consider a policy-revealed setting, wherein the opponent reveals his current strategy to the learner at the end of each episode. No external regret is possible because the benchmark in external regret evaluates the learner's comparator policy against the same policy that the opponent reveals. For policy regret, however, knowing the opponent's strategy at the end of the episode gives the learner no advantage in general, as the counterfactual benchmark requires evaluating the learner's policies against the policy sequence that the opponent would have reacted with. Indeed, our lower bound in Theorem 3 still applies to the policy-revealed setting.\nFor m-memory bounded, stationary and consistent adversaries, we present two algorithms, one for m = 1 and the other for general m \u2265 1, with sublinear policy regret. We give special consideration"}, {"title": "Memory of length m = 1", "content": "We first consider the memory length of m = 1 for stationary and consistent adversaries.\nAlgorithm. We propose OPO-OMLE (Algorithm 1), which represents Optimistic Policy Optimization with Optimistic Maximum Likelihood Estimation. OPO-OMLE is a variant of the optimistic value iteration algorithm of , wherein we build an upper confidence bound on the value function $V_{h}^{\\pi,f(\\pi)}$ for any policy \u03c0, using a bonus function and optimistic MLE . The upper confidence bound is based on two levels of optimism: a bonus term \u03b2 that is based on confidence intervals on the transition kernels P and the parameter version spaces {Ohsa} of the adversary's response at each level (h, s, a). The parameter version spaces construct a set of parameters that are close to the MLE solution, up to an error a, in terms of the log-likelihood in the observed actions taken by the adversary."}, {"title": "Memory of any fixed length m \u2265 1", "content": "We now consider the general case of stationary and consistent adversaries that have a memory of any fixed length m \u2265 1. Note that we assume that the learner knows (an upper bound of) m. Playing against a 1-memory bounded adversary does not stop the learner from changing her policies often, as the adversary does not remember any policies that the learner has taken previously. However, a sublinear policy regret learner against m-memory bounded adversaries should switch her policies as less frequently as possible, and at most only sublinear time switches. The reason is that every policy switch will add a constant cost to policy regret, as the benchmark in the policy regret is with the best fixed sequence of policy. This makes the regret minimizer OPO-OMLE unable to generalize from m = 1 to any fixed m. Instead, we propose a low-switching algorithm, in which the learner learns to play exploratory policies repeatedly over consecutive episodes so that the switching cost is reduced. Here, as in , exploratory policies are those with good coverage over the state space from which uniform policy evaluation can be performed to identify near-optimal policies.\nAlgorithm. We propose APE-OVE (Algorithm 3), which represents Adaptive Policy Elimination by Optimistic Value Estimation. APE-OVE generalizes the adaptive policy elimination algorithm of for MDPs to Markov games with unknown opponents. The high-level idea of our algorithm is as follows. The learner maintains a version space \u03a0 of remaining high-quality policies after each epoch \u2013 which is a sequence of consecutive episodes with an appropriate length (epoch k has a length of HSAB(m \u2212 1 + Tk) in APE-OVE).\n\u2022 Layerwise exploration (Line 5 of Algorithm 3): Within each epoch, the learner performs layerwise exploration (Algorithm 4), wherein we devise high-coverage sampling policies \u03c0khsab that aim at exploring (s, a, b) in step h and epoch k, starting from the lowest layer h = 1 up to the highest layer h = H. However, some states might not be visited frequently by any policy, thus taking a large amount of exploration. They, fortunately, do not significantly affect the value functions of any policy and thus can be identified (by storing in Uk) and removed from exploration quickly (via the truncated transition kernel estimates Pk obtained in Algorithm 5). Layerwise exploration requires value estimation uniformly over all policies. However, the learner does not know the adversary's response f. To address this, we use optimistic value estimation via the optimistic MLE in the collected data of the adversary's moves (Algorithm 6).\n\u2022 Version space refinement (Line 6 of Algorithm 3): After the layerwise exploration, we refine the version space of policies that the learner can choose from at the next epoch using the optimistic"}, {"title": "Discussion", "content": "In this paper, we study learning in Markov games against adaptive adversaries and highlight the statistical hardness of learning in this setting. We identify a natural structural assumption on the response function of the adversary, wherein we provide two distinct algorithms that attain \u221aT policy regret, one for the unit memory and the other for general memory length.\nThere are several notable gaps in our current understanding of policy regret in Markov games. First, we do not know if the dependence on the minimum positive visitation probability d* when learning against m-memory bounded opponents is necessary. In other words, can we derive minimax bounds"}, {"title": "Missing proofs for Section 4", "content": ""}, {"title": "Proof of Theorem 1", "content": "Proof of Theorem 1. The construction of a hard problem essentially follows the proof idea of [Arora et al., 2012]. Policy regret requires the learner to compete with the best fixed sequence of policy in hindsight as if she could have changed her past policies. The lower bound utilizes this fact to construct an instance such that once the learner picks a particular policy in the first episode, she will receive a low reward for the remaining episodes. The only way to achieve a higher reward is to go back in time and select a different policy.\nMore formally, let's consider any learner. Let \u03c0\u00b9 be a policy that the learner commits in the first episode with the highest positive probability p > 0. Note that \u03c0\u00b9 and p are the inherent property of the learner and do not depend on the adversary and the Markov game as in the first episode, the learner has zero information about the adversary and the Markov game. Now let's consider the adversary that depends only on the learner's policy in the first episode and nothing else, i.e., for all t and policy sequence \u03c0\u00b9, ..., \u03c0*, ft(\u03c0\u00b9, ..., \u03c0\u207a) = f(\u03c0\u00b9) for some function f : \u03a0 \u2192 \u03a8. In addition, let f such that f(\u03c0) = \u03bc if \u03c0 = \u03c0\u00b9 and f(\u03c0) = \u03bd otherwise, where \u03bc and \u03bd such that for all s, $\\sup_{\\mu} V_{1}^{\\pi,\\mu}(s) - \\sup_{\\nu} V_{1}^{\\pi,\\nu}(s) = \\Omega(1)$. There exists a Markov game that always guarantees the existence of such \u03bc,\u03bd (the constructions are fairly straightforward). Thus, with probability p, we have PR(T) = \u03a9(\u03a4). Note that the external regret R(T) for this construction is 0."}, {"title": "Proof of Theorem 2", "content": "Proof of Theorem 2. The proof follows from the two main arguments: (i) a reduction from any latent MDP to a Markov game with an adversary playing policies from a finite set of Markov policies, and (ii) a reduction from the notion of regret in latent MDPs to the policy regret w.r.t. an oblivious sequence of Markov policies.\nArgument (i) is directly taken from . In particular, interacting with any latent MDP of L latent variables, S states, A actions, H time steps, and binary rewards is equivalent to interacting (from the perspective of the learner) a (simulated) Markov game against an adversary whose policies are chosen from a set of L Markov policies. In particular, the simulated Markov game has SA+S states, A actions for the learner, 2S actions for the adversary, and 2H time steps (see for the detailed construction of the simulated Markov game from any latent MDP). Thus, we can utilize any lower bound for latent MDP for the Markov game (but not vice versa).\nTo continue from Argument (i) and begin with Argument (ii), we recall the definition of latent MDPs [Kwon et al., 2021]. At the beginning of each episode, the nature secretly draws uniformly at random from a set of L base MDPs and the learner interacts with this drawn MDP for the episode. [Kwon et al., 2021, Theorem 3.1] show that for any learner, there exists a latent MDP with L base MDPs such that the learner needs at least \u03a9((SA/L)\u1d38/\u20ac\u00b2) episodes to identify an e-suboptimal policy, where the optimality is defined with respect to the average values over the M base MDPs. Note that in the construction of the hard latent MDP instance above, there is a unique optimal policy (let's call it \u03c0*) with respect to the aforementioned optimality notion. Thus, the regret of this learner over T episodes competing against \u03c0* is at least $\u03a9(\\sqrt{T(SA/L)^L})$ (the learner suffers an instantaneous regret of e every time she fails to identify \u03c0*). Note again that the regret above is the expectation with respect to the uniform distribution over L base MDPs. Thus, there exists a particular realization of a sequence of T base MDPs in a certain order such that the regret with respect to this sequence when competing with \u03c0* is at least the expected regret with respect to the uniform distribution over L base MDPs, which is $\u03a9(\\sqrt{T(SA/L)^L})$. Finally, note that \u03c0* is also an optimal policy with respect to the total value across the sequence of T MDPs since \u03c0* is an optimal policy for each individual base MDP, per the construction in Kwon et al. [2021]. Thus, we can conclude that, for any learner, there exists a sequence of T MDPs from a set of L MDPs such that the regret of the learner with respect to this MDP sequence is $O(\\sqrt{T(SA/L)^L})$."}, {"title": "Proof of Theorem 3", "content": "Proof of Theorem 3. Consider any learner. Consider the adversary's policy space \u03a8 = {\u03bc, \u03bd} where for all h\u2208 [H \u2212 1], \u03bch and vh are arbitrary but $\u03bc_H(b_1|s) = 1, \\forall s$ and $v_H(b_2|s) = 1, \\forall s$, for some b\u2081, b\u2082 \u2208 B. Let the reactive function f to map all policies but some \u03c0* in \u03a0 to \u03bc, whereas f(\u03c0*) = \u03bd. Now consider a deterministic Markov game with the following properties. The transition kernel is deterministic and always traverses through the same sequence of states, regardless of what actions the learner and the adversary take. The reward functions are deterministic everywhere, and also zero everywhere except that rh(s, a, b\u2082) = 1, \u2200s, a. Except for \u03c0* that yields a positive reward if the learner selects it, all other policies in \u03a0 give zero reward. In addition, since the learner does not know f and that there is no relation whatsoever between f(\u03c0) and f(\u03c0') for any \u03c0 \u2260 \u03c0', the learner needs to play all policies in \u03a0 at least once to be able to identify \u03c0*."}, {"title": "Missing proofs for Section 5", "content": ""}, {"title": "Support lemmas", "content": "Maximum Likelihood Estimation. Let {$x_i$}$_{i\u2208[T]}$ ~ $P_{\\theta^{*}}$ where $\u03b8^{*} \u2208 \u0398$. Denote N\u0398(\u20ac) the e-bracketing number of function class {$P_{\\theta}$: $\u03b8 \u2208 \u0398$}. The following lemma says that the log-likelihood of the true model in the empirical data is close to that of any model within the model class, up to an error that scales logarithmically with the model complexity measured in a bracketing number.\nLemma B.1. There exists an absolute constant c such that for any d\u2208 (0,1), with probability at least 1 d, for all t \u2208 [T] and \u03b8 \u2208 \u0398, we have\n$\\sum_{i=1}^{t} \\log \\frac{P_{\\theta}(x_i)}{P_{\\theta^{*}}(x_i)} < c\\log(N_{\\Theta}(1/T)T/\\delta)$.\nThe following lemma says that any model that is close to the true model in the log-likelihood in the historical data would yield a similar data distribution as the true model.\nLemma B.2. There exists an absolute constant c such that for any d\u2208 (0,1), with probability at least 1 d, for all t \u2208 [T] and \u03b8 \u2208 \u0398,\ndtv(P\u03b8, P\u03b8\u2217) \u2264 c ($\\frac{1}{t} \\sum_{i=1}^{t} \\log \\frac{P_{\\theta^{*}}(x_i)}{P_{\\theta}(x_i)}$+ log(N\u0398(1/T)T/\u03b4) ),\nwhere dtv denotes the total variation distance.\nThe two lemmas above directly follow from [Liu et al., 2023, Proposition B.1] and [Liu et al., 2023, Proposition B.2], respectively, wherein the analysis built on the classical analysis of MLE [Geer, 2000] and the \u201ctangent\u201d sequence analysis in [Zhang, 2006, Agarwal et al., 2020], respectively. The following lemma is a direct corollary of Lemma B.1 and Lemma B.2.\nLemma B.3. Let $\u0189_t \u2208 arg\\sup_{\\theta \u2208 \u0398} \\sum_{i=1}^{t}log P_{\\theta}(x_i)$. Define the version space:\n$\\Theta_t := {\\theta \u2208 \u0398 : \\frac{1}{t} \\sum_{i=1}^{t} \\log P_{\\theta_i}(x_i) \\ge  \\frac{1}{t} \\sum_{i=1}^{t} \\log P_{\\hat{\\theta}}(x_i) - clog(N_\\Theta(1/T)T/\\delta)}$.\nThen, with probability at least 1 \u2013 \u03b4, for all t \u2208 [T], we have $\u03b8^* \u2208 \\Theta_t$ and $\\max_{\\theta \u2208 \\Theta_t} dtv (P_\\theta, P_{\\theta^{*}}) \\leq c\\sqrt{\\frac{\\log(N_{\\Theta}(1/T)T/\\delta)}{t}}$."}, {"title": "Proof of Theorem 4", "content": "We first introduce several notations that we will use throughout our proofs. We denote N and $\u0398_t^{h}$ the counters Nh and the parameter confidence sets $\u0398_t^{h}$ at the beginning of the episode t."}, {"title": "Optimism", "content": "Lemma B.4 (Optimism). With probability at least 1 \u2013 \u03b4", "statement": "For any (h", "1": ".", "P_{h}V_{h+1}^{\\pi}": "s", "P_{h}V_{h+1}^{\\pi_t,f(\\pi_t)}": "s", "hat{P}_{h}-\\check{P}_{h})V_{h+1}^{\\pi}": "s", "check{P}_{h}-\\check{P}_{h})(V_{h+1}^{\\pi}-V_{h+1}^{\\pi_t,f(\\pi_t)})": "s", "define": "n$\u2206_{l=1"}, "V_{1}^{\\pi}(s_1) - V_{1}^{\\pi_t, f(\\pi_t)}(s_h), \\forall (t, h)$.\nWe now decompose \u2206 as follows:\n$\u0394 = \\max_{\\theta \\in \\Theta_{hst\\pi_h}} Q_h(s_h, a_h, P_\u03b8) \u2013 Q_{h}^{\\pi_t,f(\\pi_t)}(s_h, a_h, f(\\pi^t)_h)$\n= $\\underbrace{\\max_{\\theta \\in \\Theta_{hst\\pi_h}} Q_h(s_h, a_h, P_\u03b8) - Q_{h}^{\\pi}(s_h, a_h,b_h)}_{=: \\gamma_h} + \\underbrace{Q_{h}^{\\pi}(s_h, a_h,b_h) - Q_{h}^{\\pi_t}(s_h, a_h,b_h)}_{=: \\eta_h} + \\underbrace{Q_{h}^{\\pi_t}(s_h, a_h, f(\\pi^t)_h) - Q_{h}^{\\pi_t,f(\\pi_t)}(s_h, a_h,b_h)}_{=: \\epsilon_h}$.\nWe will bound each of $ \\gamma_h, \\eta_h, \\epsilon_"]}