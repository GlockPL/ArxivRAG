{"title": "Sequence-aware Pre-training for Echocardiography Probe Guidance", "authors": ["Haojun Jiang", "Zhenguo Sun", "Yu Sun", "Ning Jia", "Meng Li", "Shaqi Luo", "Shiji Song", "Gao Huang"], "abstract": "Cardiac ultrasound probe guidance aims to help novices adjust the 6-DOF probe pose to obtain high-quality sectional images. Cardiac ultrasound faces two major challenges: (1) the inherently complex structure of the heart, and (2) significant individual variations. Previous works have only learned the population-averaged 2D and 3D structures of the heart rather than personalized cardiac structural features, leading to a performance bottleneck. Clinically, we observed that sonographers adjust their understanding of a patient's cardiac structure based on prior scanning sequences, thereby modifying their scanning strategies. Inspired by this, we propose a sequence-aware self-supervised pre-training method. Specifically, our approach learns personalized 2D and 3D cardiac structural features by predicting the masked-out images and actions in a scanning sequence. We hypothesize that if the model can predict the missing content it has acquired a good understanding of the personalized cardiac structure. In the downstream probe guidance task, we also introduced a sequence modeling approach that models individual cardiac structural information based on the images and actions from historical scan data, enabling more accurate navigation decisions. Experiments on a large-scale dataset with 1.36 million samples demonstrated that our proposed sequence-aware paradigm can significantly reduce navigation errors, with translation errors decreasing by 15.90% to 36.87% and rotation errors decreasing by 11.13% to 20.77%, compared to state-of-the-art methods.", "sections": [{"title": "Introduction", "content": "Cardiovascular disease is the leading threat to human health (Roth et al. 2017; Song et al. 2020). The most common method for assessing cardiac health is echocardiography, as recommended by the American Society of Echocardiography (Mitchell et al. 2019; Gottdiener et al. 2004). In this procedure, a sonographer manually maneuvers a probe to find key two-dimensional views and observes the myocardium, valves, and blood flow (Reed, Rossi, and Cannon 2017; Nkomo et al. 2006; Braunwald 2015) in these views in real-time using ultrasound images. However, due to the complexity of the heart's structure, even small changes in the 6-DOF (degree of freedom) pose of the probe can affect the imaging of the structures being observed. Furthermore, individual differences in heart size, shape, and structure (Santos and Shah 2014; Wild et al. 2017) make it even more challenging, requiring adjustments to scanning strategies based on each patient's unique characteristics. This places high demands on the sonographer's scanning skills, leading to a long learning curve for sonographers and a shortage of practitioners (Bierig et al. 2006; Won et al. 2024).\nPrevious researchers proposed an assistive scanning system called Cardiac Copilot (Jiang et al. 2024b), which provides 6D pose adjustment suggestions for the probe based on the current ultrasound image to reach the target view. This system has the potential to improve the quality and efficiency with which inexperienced sonographers find views, thereby increasing the global supply of echocardiography. Subsequently, Jiang et al. proposed a structure-aware pre-training method to model the structures within two-dimensional (2D) cardiac views and the three-dimensional (3D) structures between views, thereby providing better cardiac structural prior information for the probe guidance task. These works have attempted to model the population-averaged cardiac structure from large-scale data and rely solely on a single image to make action decisions. This paradigm neglects individual variations in cardiac structure, leading to significant performance bottlenecks.\nClinically, sonographers learn personalized cardiac structural information based on the acquired views and spatial information from the patient, further assisting subsequent scans. From this observation, we can derive some key insights: (1) sonographers make decisions based on previous sequence data (view images and corresponding adjustment actions) rather than a single image, and (2) sequence data contains personalized information about the patient's cardiac structure. To this end, we propose a sequence-aware self-supervised pre-training method, which learns personalized cardiac structures via a masked prediction task. To be specific, we collect a scanning trajectory consisting of view images and the actions between views from the same patient. Both the view images and actions are processed by modality-specific encoders to obtain corresponding features, with some features randomly masked as parts to be predicted. For the masked images, we apply supervision in the feature space, as Assran et al. demonstrated that this is a more efficient learning method. The actions, on the other hand, are re-predicted to their original values for supervision. This paradigm requires the model to predict the post-action image based on the given adjustment action and visible patient's cardiac structure context information, or to predict the 3D spatial relationship between two images. Thus, this approach enables the model to use known patient information to create personalized 3D cardiac structural models.\nThe probe navigation task aims to provide sonographers with probe adjustment suggestions to accurately locate target views. This task demands a high level of cardiac structure modeling capability, making it an effective way to assess whether the pre-training has successfully imparted a generalizable understanding of cardiac structure. In this downstream task, we also introduce a sequence modeling approach, which predicts the next adjustment action based on images and adjustment actions from historical scan data. Furthermore, loading our sequence-aware model pre-trained on a dataset containing 290 routine scanning trajectories (1.07 million image and action pairs) significantly improves the probe guidance performance. Experiments on a validation set containing 74 routine scanning trajectories (0.29 million image and action pairs) demonstrate that our method significantly outperforms state-of-the-art approaches (Jiang et al. 2024a,b; Assran et al. 2023). Specifically, we reduced the translation error by 16.82% to 34.92% and the rotation error by 11.57% to 21.29%. Meanwhile, our method shows greater stability across samples with varying distances from the target plane. Besides, detailed ablation studies validate the effectiveness of critical design component and the impact of various hyper-parameters on our method."}, {"title": "Method", "content": "In this section, we first describe the proposed sequence-aware pre-training framework, illustrated in Fig. 2. Then, the ability to model personalized heart structures based on individual-specific information learned during pre-training is transferred to downstream tasks, as shown in Fig. 3."}, {"title": "Sequence-aware Pre-training", "content": "The core intuition of our approach is that a patient's scanning sequence contains personalized cardiac information. If the model can accurately predict the masked parts, it indicates that the model has a good understanding of the patient's cardiac structure. In the following paragraphs, we will detail the specifics of our method.\nInput. First, a sequence consisting of T cardiac ultrasound images and T \u2013 1 adjustment actions between images is randomly sampled from a patient's scanning sequence. As a result, we obtain a processed scanning trajectory:\n$[I_1, a_1,..., I_t, a_t, \u2026\u2026\u2026, I_{T-1}, a_{\u0442-1}, I_T]$, (1)\nwhere $I_t \u2208 R^{3\u00d7H\u00d7W}, a_t \u2208 R^6$. Next, we applied a random mask to the elements in the sequence, regardless of modality, with a mask ratio between 0.3 and 0.5. In our method, the mask ratio is smaller compared to approaches like Masked Autoencoder (He et al. 2022) because the elements in our sequence are linked through actions and image features, rather than being constrained by the natural spatial continuity found in images. Therefore, if the mask ratio is too high, the model loses the necessary constraints for reasoning. For instance, if only the first image remains unmasked, the model would be unable to infer the subsequent trajectory and thus unable to learn valuable knowledge.\nThen, the t-th visible image input $I_t$ is first divided into N non-overlapping patches $I_i \u2208 R^{n\u00d7h\u00d7w}$. Furthermore, we randomly sample regions at different scales (ranging from 0.7 to 0.9) on each image, instead of using the entire image. This introduction of randomness in the 2D plane forces the model to learn structural reasoning within the plane. Understanding 2D structures, in turn, aids in guiding in-plane probe adjustments. Thus, the $I_i$ are processed by the pre-trained vision transformer $V_o$ (Dosovitskiy et al. 2020):\n$H = V_o(I \\odot B_t + P), H, P \u2208 R^{L_s \u00d7 C}$, (2)\nwhere $B_t$ is the mask indicating the selected patches, $P$ is the position embedding, $L_s$ is the number of selected patches, and $C$ is hidden dimension. Then, we average the image patches' feature to obtain a global one, because directly inputting the original image patches into the sequence transformer would lead to a significant increase in computational cost. Simultaneously, positional and timestep embeddings, representing the positional information within the 2D"}, {"title": "planes and the sequence, are added to the image features:", "content": "$f_u = \\frac{1}{L_s}\\sum_{i=1}^{L_s} H_i + T_t, f_u\u2208R^C, T\u2208R^{T\u00d7C}$, (3)\nwhere T is the timestep embedding.\nThe visible 6-DOF adjustment actions $a_t$ are similarly processed through an action encoder $A_\u03b8$, resulting in:\n$f_a = A_\u03b8(a_t) + T_t, f_a\u2208 R^C, T\u2208R^{L\u00d7C}$, (4)\nwhere T is shared cross modalities.\nMasked Input. We set up modality-specific mask embeddings $m^i, m^a$ to separately represent the masked image and action inputs. Similarly, positional and timestep embeddings are added to the masked features:\n$m^i_t = m^i + \\frac{1}{L_v}\\sum_{i=1}^{L_v} P_i +T_t, m^i_t\u2208 R^C$, (5)\n$m^a_t = m^a + T_t, m^a_t \u2208 R^C$, (6)\nFinally, the features of the visible images and actions, along with the masked embeddings, are fed into the sequence-aware transformer, as shown in Fig. 2.\nTarget. For the masked-out image, we use the exponential moving average (EMA) updated vision encoder $V_{EMA}$ to encode its features as a supervision signal:\n$E = V_{EMA} (I_t + P_t) \\odot B_t, E\u2208R^{L_s\u00d7C}$, (7)\n$f_{tgt}^u = \\frac{1}{L_s}\\sum_{i=1}^{L_s} E +T_t, f_{tgt}^u \u2208 R^C$, (8)\nFor actions that are masked-out, we directly utilize the original actions $a_t$ for supervision, as shown in Fig. 2."}, {"title": "Loss. The loss function is calculated using the Smooth L1 Loss between the features predicted by the sequence transformer and the target features as follows:", "content": "$L_{total} = L_{SmoothL1} (f_{tgt}^u, f' ) + L_{SmoothL1} (a_t, a_t')$, (9)\nwhere $f'$ and $a'$ are predicted masked feature and actions."}, {"title": "Downstream Transfer", "content": "For downstream tasks, we also employ sequence modeling (Fig. 3), where the next decision is made based on information from the previous sequence:\n$a_t = S([I_1, a_1,\u2026\u2026, I_t, a_t,\u2026\u2026\u2026, I_{T-1}, a_{T-1}, I_T])$. (10)\nThe sequence modeling approach can leverage individual information from past data, leading to more accurate predictions. After pre-training, the pre-trained model parameters are loaded and further fine-tuned on the downstream navigation dataset. In this way, the ability to model personalized cardiac features learned during pre-training can be seamlessly applied to the downstream task."}, {"title": "Experiments", "content": "Implementation Details\nDataset. The echocardiography dataset (Jiang et al. 2024a) contains 364 demonstrations from routine clinical scans, with a total of 1.36 million 6D pose of the ultrasound probe and the corresponding ultrasound images. In each demonstration, certified sonographers scanned ten standard planes recommended by the American Society of Echocardiography (Mitchell et al. 2019), as shown in Fig. 4. In the experiment, we divided the dataset into 290 scans (1.07 million sample pairs) for the training set and 74 scans (0.29 million sample pairs) for the validation set. Please note that the validation set comprises individuals not encountered during training, allowing for an assessment of the algorithm's generalization capabilities.\nBaselines. Our method is proposed for learning better features for downstream probe guidance task. For a fair comparison, the state-of-the-art probe guidance methods are selected as baselines: Cardiac Copilot (Jiang et al. 2024b), Structure-aware Pre-training (Jiang et al. 2024a). Additionally, a self-supervised image representation learning method named I-JEPA (Assran et al. 2023) is used as one of baselines. The classic CNN-based ResNet-50 (He et al. 2016) model, both with and without ImageNet-1K (Deng et al. 2009) pre-training, is also used as a baseline.\nModel architecture. We use a ViT-Small/16 model as the default vision encoder $V_e$ and target encoder $V_{EMA}$. The action encoder $A_\u03b8$ is a single-layer MLP that maps the original 6-DOF actions to a 192-dimensional action space. To match the dimensionality of visual and action features in the sequence-aware transformer, we add an MLP layer to project the visual features to 192 dimensions. The sequence-aware transformer is a custom structure with an embedding dimension of 192, depth of 4, 4 heads, and an MLP ratio of 2. The temporal order is represented by a shared timestep embedding across modalities. Additionally, a position embedding is used to denote the spatial position of the extracted image features on the 2D plane. Finally, the masked features of different modalities are represented by independent mask embeddings. All models are implemented in Pytorch.\nPre-training. The vision encoder is pre-trained on training set using the I-JEPA method (Assran et al. 2023) with default hyper-parameter described in their paper. During pre-training, the vision encoder is jointly trained with the sequence-aware transformer. The training batch size is 1024 for timesteps less than 4, 768 for timestep 5, and 512 for timestep 6, due to GPUs memory limitations. Training is conducted for 50 epochs, with the first 7 epochs for warm-up, followed by a cosine learning rate schedule with an initial learning rate of 0.0005 (for batch size = 1024, with the learning rate adjusted proportionally for other batch sizes). Other hyper-parameters include a mask ratio ranging from [0.3, 0.5] and an image context area ranging from [0.5, 0.7]."}, {"title": "Downstream transfer.", "content": "In the probe guidance task, after loading the pretrained model, the training is conducted with a batch size of 512, a learning rate of 1e-4, using a cosine learning rate schedule, and trained for 5 epochs. The vision encoder is frozen and not trained. All training was performed using eight RTX 4090 GPUs."}, {"title": "Evaluation metric.", "content": "The evaluation metric used is the Mean Absolute Error (MAE) between the predicted probe adjustment action and the ground truth. We report the best results using T = 6, and due to the computational cost of pre-training, we conduct ablation experiments of key hyper-parameter using T = 4."}, {"title": "Comparison with State-of-the-art", "content": "First, we compare our method with the current state-of-the-art methods on the ten most common views. As shown in Tab. 1, our method consistently outperforms the structure-aware pre-training (Jiang et al. 2024a) approach across all ten views. Specifically, in terms of translation error, we achieved a minimum reduction of 15.90% on the PSAX-PV view and a maximum reduction of 36.87% on the A5C view. Similarly, for rotation error, we achieved a minimum reduction of 11.13% on the A2C view and a maximum reduction of 20.77% on the PSAX-PAP view. Additionally, methods like I-JEPA (Assran et al. 2023), which rely solely on image inputs and image-based supervisory signals, primarily learn only some structural information on the 2D plane of the heart. However, the probe guidance task involves many decisions that are cross-sectional, requiring the probe to perform complex movements in 3D space. Consequently, I-JEPA falls short compared to methods like cardiac copilot (Jiang et al. 2024b) and structure-aware pre-training (Jiang et al. 2024a), which utilize 3D information."}, {"title": "Ablation and Discussion of Key Components", "content": "In this section, we perform ablation studies to evaluate the effectiveness of each design component.\nSequence-aware pre-training. Sequence modeling does not explicitly constrain the model to uncover cardiac structural information within the sequence, which may lead to underutilization of the valuable information present in the sequence. In contrast, sequence-aware pre-training, through the masked prediction paradigm, requires the model to fully grasp the causal relationships between views and adjustment actions. After pre-training, the model acquires the following capabilities: (1) Given a view and an adjustment action, it can predict the image features of the resulting view after the adjustment; (2) Alternatively, given two views, it can predict the adjustment action (relative spatial position) between them. In other words, the model develops a strong understanding of the heart's three-dimensional structure. As shown on the left side of Fig. 5, after loading the sequence-aware pre-trained model parameters on top of sequence modeling, the prediction errors are further reduced.\nSequential modeling. The decision-making paradigm based solely on a single image struggles to address individual variability due to the lack of effective information about personalized cardiac structures. In contrast, the sequence modeling approach effectively leverages acquired structural information, leading to more accurate action decisions. To validate the above hypothesis, both the baseline and our method were tested without pre-training, using the same model architecture. The only difference is that the baseline predicts based solely on a single image, while our method makes predictions based on sequence input. As shown on the right side of Fig. 5, introducing sequence modeling significantly reduces prediction errors. Translation errors decrease by a minimum of 22.50% and a maximum of 37.42%, while rotation errors decrease by a minimum of 14.38% and a maximum of 24.36%, strongly demonstrating the effectiveness of the sequence modeling paradigm.\nSequence length. We evaluate sequence-ware pre-training using different sequence lengths (timesteps), specifically considering lengths of 3, 4, 5, and 6 timesteps. From Fig. 6, we observe that our method outperforms the non-pre-trained baseline across all sequence lengths. Moreover, as the sequence length increases, the advantage of pre-training expands, which aligns with our hypothesis that longer sequences contain more personalized information. At the same time, we also observe that as the sequence length increases, without sequence-aware pre-training, the prediction error decreases very slowly and even shows an upward trend in rotation error. This also indicates that during pre-training, the model acquired the ability to capture personalized cardiac structure features in long sequences, which significantly improves the performance in downstream tasks. We also observe that as the sequence length increases beyond 4, the slope of the error reduction begins to decrease, indicating that the information in longer sequences starts to become redundant. In practical applications, sequences can be very long, and simply feeding all past sequences into the model may not yield performance gains that justify the increased computational cost. Therefore, a valuable direction would be to design more efficient sequence sampling algorithms, aiming to select samples that provide the maximum information gain as the sequence length increases.\nTwo-dimentional structure modeling. Understanding the spatial distribution of the 2D structural layout can guide in-plane probe adjustments. As shown on the right side of Fig. 8, when the aortic valve (AV) is already in view, knowing the spatial relationship between the pulmonary valve (PV) and the AV allows you to make the correct adjustment decision to capture an image of the PV. We ablated the effect of introducing 2D modeling, which resulted in a 1.59% reduction in translation error and a 1.50% reduction in rotation error. Considering the computational cost, image features are globally averaged before being input into the sequence transformer, which results in the loss of some detailed features and hinders 2D modeling. A potential direction for future exploration is to design a more efficient model architecture that eliminates the global averaging operation to retain more detailed 2D structural information."}, {"title": "Ablation of Key Hyper-parameters", "content": "In this section, we conduct ablation studies to analyze the impact of various hyperparameters on our method.\nMask type. We experimented with different masking strategies, including masking only image features, masking only actions, and randomly masking both. As shown on the left side of Fig. 7, masking only image features or only actions is less effective than masking both simultaneously. Intuitively, masking both simultaneously leverages the supervisory signals from both modalities, making fuller use of the information contained within the data.\nMask ratio. We divided the mask ratio into four intervals from small to large and experimented with the impact of different mask ratios on performance. As shown on the right side of Fig. 7, as the mask ratio increases, the prediction error initially decreases but then quickly rises. This suggests that when the mask ratio is too high, the model fails to learn useful features. This occurs because when the mask ratio is too high, the model lacks sufficient inference conditions and can only make random predictions. For example, if an input sequence leaves only $I_1$ and requires the prediction of all subsequent masked elements, the potential possibilities are infinite, making it impossible to learn generalizable features. In contrast, if both $I_1$ and $a_1$ are left, the model can rely on this information to predict the features of $I_2$, as there is a causal relationship. If the model can make correct predictions, it indicates that it has learned the spatial relationships between different views."}, {"title": "Distance vs. Performance Analysis", "content": "To further understand the model's performance, we present the relationship between 6D mean absolute pose distance and predicted mean absolute error in Fig. 9. The figure illustrates that the our model consistently achieves lower mean absolute error compared to the baseline model across various mean pose differences to the target plane. Additionally, the narrower shaded regions for our model suggest a smaller standard deviation and more consistent performance across different distances. In contrast, the baseline model exhibits a larger standard deviation, indicating greater variability in predictions, which could imply sensitivity to specific probe poses and result in less reliable performance in practical."}, {"title": "Related Work", "content": "With technological advancements (Dosovitskiy et al. 2020; Kirillov et al. 2023; Yang et al. 2021; Radford et al. 2021; Jiang et al. 2022; Wang et al. 2021), AI-driven applications (Chen et al. 2020; Narang et al. 2021) have demonstrated great potential in cardiac ultrasound. Probe navigation aims to generate action decisions based on input images, which can be used to guide inexperienced ultrasound practitioners during scanning (Narang et al. 2021; Sabo et al. 2023) or to navigate robotic arms for fully autonomous cardiac ultrasound scanning in the future (Shida et al. 2023a,b). Researchers primarily employ the paradigms of supervised learning (Jiang et al. 2024a,b; Droste et al. 2020; Men et al. 2023) and reinforcement learning (Amadou et al. 2024; Li et al. 2023) to learn probe guidance strategies. For supervised learning methods, expert scanning demonstration data is typically collected, where each image is paired with a corresponding adjustment action. This data is used to learn the mapping between the image and the adjustment action. In supervised learning methods, Jiang et al. proposed a structure-aware pre-training method to enhance the model's understanding of cardiac structures. Men et al. introduced additional information on sonographers' gaze on ultrasound images to assist in action decision-making. For reinforcement learning methods, the primary challenge lies in constructing a good simulation environment. Current approaches (Amadou et al. 2024; Li et al. 2023) mainly use CT imaging data of the heart as the simulation environment. However, acquiring heart CT imaging data is more costly than ultrasound scanning data, making it difficult to acquire large datasets. As a result, it is challenging to address the issue of inter-individual cardiac structural differences. Additionally, there are imaging differences between CT and ultrasound data, which introduces challenges related to algorithm transferability. Therefore, we believe that combining large-scale scanning data with supervised learning offers a more practical approach for real-world applications."}, {"title": "Conclusion", "content": "In this paper, we propose a sequence-aware pre-training method to enhance the model's ability to capture personalized cardiac structures. Specifically, we leverage sequence modeling to utilize known individual cardiac structure information and employ masked prediction pre-training to further enhance the model's ability to infer cardiac structure based on this information. Experiments demonstrated that the method significantly reduces translation and rotation errors while showing greater stability across samples at varying distances. Additionally, the ablation study shows that as the sequence length increases, the marginal gains gradually decrease, indicating the presence of information redundancy in longer sequences. Therefore, a valuable direction for future research is to develop methods for sampling the most informative sequences, maximizing information gain from a large amount of past sequence data."}]}