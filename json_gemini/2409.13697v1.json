{"title": "Prompt Baking", "authors": ["Aman Bhargava", "Cameron Witkowski", "Alexander Detkov", "Matt Thomson"], "abstract": "Two primary ways to change LLM behavior are prompting and weight updates (e.g. fine-tuning). Prompting LLMs is simple and effective, specifying the desired changes explicitly in natural language, whereas weight updates provide more expressive and permanent behavior changes, specified implicitly via training on large datasets. We present a technique for \"baking\" prompts into the weights of an LLM. PROMPT BAKING converts a prompt u and initial weights \u03b8 to a new set of weights \u03b8u such that the LLM with weights \u03b8u behaves like the LLM with weights \u03b8 prompted with u. Mathematically, we minimize the KL divergence between $P_{\\theta}(u)$ and $P_{\\theta_u}(\u00b7)$, where P is the LLM's probability distribution over token sequences. Across all our experiments, we find prompts can be readily baked into weight updates, often in as little as 5 minutes. Baking chain-of-thought prompts improves zero-shot performance on GSM8K, ASDIV, MBPP, ARC-EASY, ARC-CHALLENGE, and COMMONSENSEQA benchmarks. Baking news headlines directly updates an LLM's knowledge. And baking instructions & personas alleviates \"prompt forgetting\" over long sequences, as measured on a PERSONA DRIFT benchmark. Furthermore, stopping baking early creates \"half-baked\" models, allowing for continuous scaling of prompt strength. Baked models retain their sensitivity to further prompting and baking, including re-prompting with the prompt already baked in - thus amplifying the prompt's strength. Surprisingly, the re-prompted models yield further performance gains in instruction following, as well as math reasoning and coding benchmarks (GSM8K, ASDIV, and MBPP). Taking re-prompting and re-baking to the limit yields a form of iterative self-improvement we call PROMPT PURSUIT, and preliminary results on instruction following exhibit dramatic performance gains with this technique. Finally, we discuss implications for AI safety, continuous model updating, improving LLM recency, enhancing real-time learning capabilities in LLM-based agents, and methods for generating more stable AI personas.", "sections": [{"title": "1 Introduction", "content": "Scale endows language models with remarkable generality and adaptability through prompt-based model programming (e.g., zero-shot capabilities). However, true adaptability demands new learning methods that permanently incorporate new knowledge into the LLM without labor-intensive curated datasets. Prompting explicitly communicates new information to an LLM in natural language, relying on the \"zero-shot\" capabilities of the underlying model [6]. Information learned via weight updates (e.g., fine-tuning) is implicitly communicated through the statistics of a data distribution [11]. While weight updates are maximally expressive and permanent, it is challenging to reliably communicate new knowledge via weight updates as it is unclear what information implicit in the dataset will be learned by the model [24, 7, 31]. Moreover, the"}, {"title": "1.1 Contribution", "content": "* Prompt baking works: We demonstrate PRMOPT BAKING on a number of tasks and academic benchmarks, validating that the baked model $P_{\\theta_u}(\u00b7)$ matches the behavior of the prompted baseline model $P_{\\theta}(u)$ (Section 3.1). We showcase how baking transforms ephemeral prompts into durable weight updates, ensuring consistent model behavior across multiple uses without repeated prompting. Moreover, prompts can often be baked as little as 5 minutes of wall clock time, showcasing the practicality and usability of our approach.\n* Scaling prompts: Our technique allows continuous interpolation between the pre-baking and fully baked states, something we call \"half-baking\". This capability addresses a long-standing challenge in prompt engineering: the difficulty of navigating the discrete and chaotic space of prompts, where subtle adjustments to prompt strength are often hard to achieve (Section 3.1). Moreover, by repeatedly applying the prompt baking operator, we can amplify a prompt's influence beyond its original strength, leading to further performance gains on a variety of tasks (Section 4).\n* Baking in chain-of-thought examples on Academic Benchmarks: We demonstrate that baking in few-shot chain-of-thought prompts attains nearly the same performance boost as few-shot prompting on GSM8K, ASDIV, MBPP, ARC-EASY, ARC-CHALLENGE, and COMMONSENSEQA benchmarks. We find that baked models achieve accuracy within 1.4% of the corresponding few-shot prompted model on every benchmark. Shockingly, on some benchmarks, baking then prompting the baked model surpasses the performance of prompting alone. Most notably, on GSM8K, we surpass Meta's published accuracy by 1.4%, while using the same fewshot examples and experimental setup."}, {"title": "1.2 Related Work", "content": "Model Distillation: Our work builds upon the principles of model distillation [35], a technique that minimizes the KL divergence between a teacher and a student model. Traditional distillation matches logits on an existing corpus, leveraging the informativeness of soft targets. Recent work, such as \"Distilling Reasoning Capabilities into Smaller Language Models\" [27], extends this approach by using generated chain-of-thought trajectories. Our method, however, distinguishes itself in several key aspects: (1) we distill from the same model, effectively 'self-distilling' prompted behavior; (2) we enable the baking of any prompt, not limited to reasoning prompts; and (3) our generated trajectories do not require answer checking or even the presence of answers, thus is entirely self-supervised. This approach connects to the concept of self-improvement while eliminating the need for explicit answer validation, offering a more flexible and generalizable framework for knowledge incorporation.\nLLM Control Theory: Our previous work [5] introduced a control theoretic framework for LLM prompting, demonstrating high controllability via short prompts. PROMPT BAKING extends this by projecting prompt-induced probability changes onto weight space, effectively \"moving\" the model's default behavior within the previously defined reachable set. Crucially, our half-baking approach allows for fine-grained control by enabling continuous interpolation between unprompted and fully prompted states. As the model minimizes KL divergence during baking, it traverses a path in weight space connecting these two states. This conceptualization aligns with the notion of functionally invariant paths (FIPs) in weight space [24]. While FIPs are constructed to maintain functional performance while optimizing secondary objectives, our method implicitly defines a path that \"bakes in\" the prompted behavior to varying degrees. This connection between prompting, weight updates, and weight space paths offers a powerful new perspective and technique on LLM control.\nSelf-Improvement: Our work relates to recent developments in LLM self-improvement, such as those described in [12, 33]. These approaches train LLMs on self-generated trajectories or instruction-input-output triplets to enhance performance. However, prompt baking differs in several crucial aspects. Unlike self-improvement methods that use cross-entropy loss and focus on specific types of outputs (e.g., high-confidence rationale-augmented answers), we employ KL divergence and can bake in any type of prompt. Our approach doesn't require majority voting, filtering of trajectories, or elimination of duplicates. Furthermore, while Self-Instruct generates and trains on instruction-following examples, we generate trajectories directly from the prompt being baked in. This makes our method more generalizable, capable of incorporating a wide range of prompts beyond just instruction-following, while maintaining a simpler training process without output validation or filtering.\nKnowledge Editing: Our work relates to the emerging field of knowledge editing in LLMs [37]. Knowledge editing aims to modify specific information or behaviors in LLMs without full retraining, addressing issues like factual errors or outdated information. While knowledge editing methods often focus on altering particular facts or responses, our prompt baking approach offers a more general framework for incorporating diverse prompts into model weights. Moreover, our method preserves the model's sensitivity to further"}, {"title": "Continual Learning and Catastrophic Forgetting:", "content": "our work addresses key challenges in continual learning for AI systems, particularly the problem of catastrophic forgetting (CF) in large language models (LLMs) [30, 36, 20]. CF occurs when models rapidly forget previously learned information while acquiring new knowledge, a phenomenon recently observed in LLMs during continual instruction tuning, with severity increasing with model size [21, 10, 20]. Unlike traditional continual learning methods that often focus on task-specific updates or require experience replay, prompt baking offers a novel approach to mitigate CF. It allows for targeted updates to LLM behavior without extensive retraining or explicit task boundaries, addressing the stability-plasticity trade-off by enabling precise knowledge incorporation while maintaining overall model performance. Our method demonstrates the ability to sequentially incorporate multiple pieces of knowledge (e.g., news headlines) without significant forgetting, a capability that many continual learning techniques struggle with. Additionally, prompt baking mitigates prompt decay over long sequences, indirectly contributing to CF reduction. The scalability and composability of our technique offer promising avenues for developing robust continual learning strategies in LLMs, positioning PROMPT BAKING as a flexible and efficient method for ongoing knowledge integration and behavior modification in language models."}, {"title": "Prompt Decay:", "content": "Our work addresses the challenge of prompt decay, also known as attention decay or persona drift, a phenomenon where the influence of initial prompts diminishes over extended interactions with language models. Li et al. [19] quantified this effect, demonstrating significant persona drift in models like LLaMA2-70B-chat and proposing methods such as split-softmax to mitigate it. Their work hypothesizes that attention decay, where the model's focus on the initial prompt wanes over time, underlies this issue. While their approach focuses on runtime interventions, our prompt baking method offers a more permanent solution by integrating prompt information directly into model weights. This approach addresses the prompt decay problem while maintaining the model's responsiveness to additional prompting, offering a more robust and flexible solution to maintaining consistent behavior over long sequences."}, {"title": "Activation Manipulation:", "content": "Our work relates to activation manipulation techniques, such as Anthropic's \"Golden Gate Claude\" experiment [3]. Both prompt baking and activation manipulation aim to precisely modify model behavior without full retraining [38]. The key difference is that activation manipulation directly modifies internal model activations, while prompt baking incorporates prompt information into model weights. Our approach preserves sensitivity to further prompting and doesn't require identifying specific neurons, making it more flexible and generalizable. Both techniques contribute to understanding language model internals and offer fine-grained control over model outputs."}, {"title": "2 Methods", "content": "PROMPT BAKING converts a prompt u in prompt space U and an initial set of LLM weights \u03b8 in weight space to a new set of weights $\u03b8_u \\in \\Theta$ such that, informally, the model's conditional distribution over next tokens $P_{\\theta}(\u00b7 |u)$ is approximates to the unconditional distribution of the \u201cbaked\u201d model $P_{\\theta_u} (\u00b7)$. We use B to denote the baking procedure:\n$B: \\Theta \\times U \\rightarrow \\Theta$ (1)\nIn other words, B maps an initial weight set \u03b8 and prompt u to a new \"baked\" weight set \u03b8u such that the baked model \u03b8u behaves as though the prompt u was there, even though it is not. Algorithmically, we realize B by minimizing KL divergence between the prompted $P_{\\theta}(\u00b7 |u)$ and unprompted $P_{\\theta_u}(\u00b7)$:\n$\\theta_u = B(\\theta, u) = \\underset{\\theta_u}{\\text{argmin}} D_{KL} (P_{\\theta}(\u00b7|u)||P_{\\theta_u} (\u00b7))$ (2)"}, {"title": "3 Results", "content": "3.1 Example: Baking In Sadness\nFigure 2 demonstrates the core functionality of PROMPT BAKING using an \"always sad\" instruction following prompt. The baked model's negative sentiment is measured using ntlk, increasing smoothly from baseline to prompted performance (Figure 2A). This smoothness enables \u201chalf-baking\u201d \u2013 stopping early for intermediate levels of prompt influence, offering continuous control over model behavior. In Figure 2B, we compare the log-likelihoods trajectories generated from previously unseen SQuAD questions before and after baking. r\u00b2 with the prompted baseline model increases from -16.24 to 0.96 after baking, showing close alignment between baked and prompted models. We ask model to \"List 7 words starting with C\" before and after baking in Figure 2. Color coding by - log-likelihood reveals how expected (blue) or surprising (red) a given token was. After baking, we see the unprompted baked model generates depressing words aligning with the prompted baseline, leading to agreement on next token predictions (blue-purple text). Before baking, the unprompted baseline model generates words with positive sentiment that \"surprise\" the baseline prompted model (red text)."}, {"title": "3.2 Baking in instruction following prompts", "content": "We bake in various instruction following prompts using trajectories generated from the SQUAD dataset (see the blue and red icons in Figure 3). We find that baked models perform to within 8% of the prompted model on a held out questions set. It is worth stressing that the dataset and optimization procedure for baking is independent of the particular prompt. Baking is simply a map from prompt and weight set to a new set of weights, and does not require a prompt specific dataset."}, {"title": "3.3 Baking in chain-of-thought examples on Academic Benchmarks", "content": "It is well known that prompting an LLM with few-shot examples often improves its performance. We demonstrate that baking in few-shot chain-of-thought prompts attains nearly the same performance boost, to within 1.4% accuracy on every one of the 6 datasets we tested, as shown in Figure 4. Remarkably, in all six benchmarks tested, the baked zero-shot performance shows significant gains compared to the zero-shot baseline. The exact prompt u which is baked in for each benchmark is presented in Appendix 8."}, {"title": "3.4 Re-prompting the baked model yields further improvements", "content": "Shockingly, on some benchmarks, baking then prompting the baked model ($P_{\\theta_u}(\u00b7|u)$) surpasses the performance of prompting alone ($P_{\\theta}(u)$), as shown in Figure 5.1 Notably, on GSM8K, we surpass Meta's published Llama3 accuracy (79.6%) by a full 1.4%, while using the same fewshot examples and experimental setup [8]."}, {"title": "3.5 Baking resists catastrophic forgetting", "content": "We measure the deterioration of other skills within the model after being baked on a particular dataset. I.e., does baking one benchmark's few-shot prompt into a model cause it to catastrophically forget capabilities on other benchmarks? We evaluate each benchmark's baked model on each of the other six benchmarks, for 36 total evaluations (Figure 6). We find that PROMPT BAKING does not significantly degrade performance on unrelated tasks, with accuracy decreasing 3.4% at most. An accuracy decrease should be expected, since the re-prompted baked models effectively have a \"distractor\" prompt baked into them.2 These results provide preliminary evidence that our method bears some degree of resistance against catastrophic forgetting."}, {"title": "3.6 Knowledge Baking", "content": "An outstanding issue in LLMs is updating the LLM with new pieces of knowledge that were unknown prior to the pre-training of the language model. Here, we bake news headlines into an LLM to update its knowledge of current events. We find that sequentially baking non-conflicting facts $u_1, u_2$ as $\\theta_{u_{12}} := B(B(\\theta, u_1), u_2)$ successfully teaches the model multiple pieces of information, exhibiting behavioral equivalence to both singly-baked models $\u03b8_{u_1}$ and $\u03b8_{u_2}$. Empirically, $\u03b8_{u_{12}}$ demonstrates comparable recall and task performance to the original prompted model $P_{\\theta}(\u00b7 |u_1), P_{\\theta}(u_2)$, indicating effective integration of knowledge from both prompts without interference."}, {"title": "3.7 Baking Prevents Prompt Decay", "content": "Utilizing the persona consistency framework from \"Measuring and Controlling Persona Drift in Language Model Dialogs\" [19], we demonstrate that baking prompts effectively mitigates prompt decay over extended conversations. In Figure 7 baked models maintain near constant amplified persona stability throughout"}, {"title": "4 Prompt Pursuit", "content": "Performance improvements on re-prompting the baked model naturally suggests the idea of iteratively repeating the baking process (re-baking):\n$\\theta^{n+1} := B(\\theta^n, u)$ (4)\nIteratively baking for n iterations may be denoted as $\\theta^n = B^n(\\theta, u)$. Taking the limit as the duration of a single baking iteration approaches a single gradient step, we arrive at the optimization objective of PROMPT PURSUIT (Equation 5):\n$L = D_{KL} (P_{\\theta}(\u00b7 | u) || P_{\\theta_{4}}(\u00b7))$ (5)\nwhere $d_u$ denotes parameters through which gradients are not propagated. Intuitively, the natural language prompt u explicitly defines a semantically meaningfully direction in weight space, dynamically steering the model's behavior towards its ever moving prompted target. Amazingly, we find for many tasks the behavioral paths taken by the LLM aligned with human constructed evaluation metrics defined over the prompt objective. This suggests that the LLM inherently understands how to extrapolate and extend the behaviors implied by the prompt. Experiments on instruction following tasks show accuracy gains of 15-40% accuracy over the original prompted model (Figure 1). We also find pursued models similarly have near constant persona stability with a added amplification bonus on persona strength (Figure 7)."}, {"title": "5 Discussion", "content": "Importance of Logit Distribution: We conduct an ablation study on the importance of using the full logits distribution as opposed to top p logits (i.e. with top-1 analogous to cross-entropy loss with one-hot labels) (Figure 8). In particular we look at the reverse input task on the SQUAD dataset. We find, for a given sized dataset, a critical threshold of logits are necessary for baked model generalization. We suspect that a inverse relationship exists between the size of the logit distribution used and the required dataset size. The transition point in the number of logits indicates a \"critical mass\" for communicating sufficient information via the logits to reliably bake in the prompt."}, {"title": "No overfitting:", "content": "Prompt Baking also exhibits natural resistance to overfitting due to its approach of matching logits from the same model in different states. Unlike traditional methods that maximize likelihood on external datasets, our technique minimizes divergence between the prompted and unprompted versions of the identical model. Since the same model generates both the prompted and baked behaviors, the resulting weight updates are likely \"compatible\" with the model's inherent reasoning patterns. This approach suggests that the baked weights remain in relative proximity to the original weights in the model's parameter space, potentially preserving the model's general capabilities while incorporating the new behavior."}, {"title": "Geodesics in weight space:", "content": "Our method minimizes KL divergence between prompted and unprompted models by traversing paths in weight space rather than logit space. This approach yields unexpected and semantically meaningful interpolations. In a hypothetical logit space interpolation, a model 50% of the way between unprompted and prompted states would output a mixture of both states' probabilities. For instance, if an unprompted model predicts \"blue\" and a prompted model predicts \"red\", a 50% interpolation in logit space would alternate between \"blue\" and \"red\" with equal probability. However, our weight space interpolation produces qualitatively different results. A partially baked \"sad\" model doesn't merely oscillate between neutral and extremely sad outputs, but consistently produces moderately sad content. This phenomenon extends to re-prompting and prompt pursuit, where further traversal in weight space yields semantically extended versions of the model rather than mere probability saturation. These observations suggest that geodesics in weight space correspond to semantically meaningful trajectories between model states, allowing for nuanced and extensible modifications to model behavior, similar to the functionally invariant paths in weight space discussed in [24]."}, {"title": "Projection:", "content": "Prompt Baking can be conceptualized as a projection of a prompt's effect onto the model's weight space, specifically within an epsilon ball in a low-rank subspace. This process seeks the nearest weight update that emulates the prompted distribution, effectively transforming explicit, temporary instructions into implicit, permanent knowledge. Remarkably, the lossy nature of this projection leads desirable divergences from the prompted baseline model, akin to the principles of compressive sensing. This counterintuitive improvement manifests in phenomena like reduced prompt decay over long sequences."}, {"title": "Cognitive analogies:", "content": "Our method draws parallels with human cognition, offering insights into its efficiency and function. Behavior encoded in the weights resembles System 1 thinking: fast, automatic responses requiring minimal computational overhead. In contrast, prompt-induced behavior mimics System 2 thinking: a slower, more deliberate process [14, 34]. In transformer-based models, prompt processing incurs an $O(n^2)$ complexity cost, where n is the combined length of the prompt and generated text. This computational demand is analogous to the cognitive load of conscious reasoning. Prompt Baking essentially converts System 2-like prompts into System 1-like weight updates, similar to how practiced skills become automatic. This process also mirrors the transition from short-term to long-term memory, transforming temporary, explicit instructions (prompts) into durable, implicit knowledge (weights). By doing so, Prompt Baking potentially offers a more computationally efficient way to incorporate new behaviors into language models."}, {"title": "Credit assignment:", "content": "Prompt Baking addresses a long-standing challenge in neural network research: updating individual synapses based on high-level, semantic information to actualize learning [17, 16, 4]. Since the 1980s, the most performant methods for credit assignment in connectionist systems have relied on large datasets and backpropagation [25]. Our approach demonstrates that synapse-level updates can be derived with signal from natural language prompts alone, without the need for extensive training data. This capability bears similarity to human learning processes, where new concepts can be rapidly internalized from brief explanations. For example, a child can often recognize the letter \"e\" after a single explanation, without requiring numerous examples. Prompt Baking replicates this efficiency in artificial neural networks, translating concise, semantic instructions directly into specific weight adjustments, rendering explicit knowledge"}, {"title": "Commutativity:", "content": "Once we open the possibility of sequentially baking prompts $u_1, u_2$ as $\\theta_{u_{12}} := B(B(\\theta, u_1), u_2)$ (Section 3.6), a natural question is whether baking in the reversed order $\\theta_{u_{21}} := B(B(\\theta, u_2), u_1)$ would yield the same result. Mathematically, we may wonder whether Baking as a operator B is commutative. Preliminary investigation suggests that baking becomes non-commutative when $u_1$ and $u_2$ are contradictory. For instance, baking $u_1$ = \"Joe Biden is staying in the race\" then $u_2$ = \"Joe Biden is dropping out of the race\" yields a different result than baking in the opposite order. For more independent prompts, the order does not seem to matter as much. The significance of ordering conflicting operations reminds us of non-commuting measurement operators in Quantum Mechanics and especially the recent application of their formalism to long-standing problems in cognition [9] [23]."}, {"title": "6 Future Work and Applications", "content": "An important avenue for future exploration is PROMPT BAKING's application to continual learning. Continual learning, also known as lifelong learning, involves developing models that learn continuously from new data without forgetting previously learned knowledge [15][36][26][1]. Baking new data into LLMs has the potential to teach systems new knowledge without catastrophic forgetting. More work is required to comprehensively characterize knowledge baking and optimize its performance when utilized on a large scale.\nA key area for future work is the application of PROMPT BAKING to AI safety, which aims to ensure that advanced AI systems remain aligned with human values and under human control [2]. This includes developing strategies to prevent unintended behaviors, such as goal misalignment, harmful side effects, and adversarial manipulation [28] [18]. PROMPT BAKING offers an entirely new modality of control over large language models, which may be exploited for the purposes of ensuring LLMs are helpful and aligned. Moreover, a natural extension to PROMPT BAKING would do the opposite of our method: training a prompted model to act like an unprompted model, something we call NEGATIVE PROMPTING. NEGATIVE PROMPTING has the potential to numb an LLM's sensitivity to harmful user queries or adversarial attacks."}, {"title": "6.1 KL Derivation", "content": "We start with the general formula for KL divergence:\n$D_{KL}(P_{\\theta}||P_{\\theta_u}) = \\sum_{y \\in Y} P_{\\theta}(y) \\log \\frac{P_{\\theta}(y)}{P_{\\theta_u}(y)}$ (6)\nUsing the chain rule of probability for autoregressive models:\n$P(y) = P(y_1, ..., y_n) = \\prod_{i=1}^{n} P(y_i | y_1, ..., y_{i-1})$ (7)\nSubstituting into our KL divergence formula:\n$D_{KL}(P_{\\theta}||P_{\\theta_u}) = \\sum_{y \\in Y} (\\prod_{i=1}^{n}P_{\\theta}(y_i | y_{<i})) log \\frac{\\prod_{i=1}^{n} P_{\\theta}(y_i | y_{<i})}{\\prod_{i=1}^{n} P_{\\theta_u}(y_i | y_{<i})}$ (8)\nSimplifying the log of the ratio of products into a sum of logs:\n$D_{KL}(P_{\\theta}||P_{\\theta_u}) = \\sum_{y \\in Y} (\\prod_{i=1}^{n}P_{\\theta}(y_i | y_{<i})) \\sum_{i=1}^{n} log \\frac{P_{\\theta}(y_i | y_{<i})}{P_{\\theta_u}(y_i | y_{<i})}$ (9)\nUsing the definition of logits: $l_i = log P(y_i|y_{<i})$\n$D_{KL}(P_{\\theta}||P_{\\theta_u}) = \\sum_{y \\in Y} (\\prod_{i=1}^{n}e^{l_{\\theta,i}}) \\sum_{i=1}^{n} (l_{\\theta,i} - l_{\\theta_u,i})$ (10)\nRearranging:\n$D_{KL}(P_{\\theta}||P_{\\theta_u}) = \\sum_{y \\in Y} \\sum_{i=1}^{n} ((\\prod_{j=1, j!=i}^{n} e^{l_{\\theta,i}}) e^{l_{\\theta,i}} (l_{\\theta,i} - l_{\\theta_u,i}))$ (11)\nNoting that $\\prod_{j=1}^{n} e^{l_{\\theta,j}} = P_{\\theta}(y)$:\n$D_{KL}(P_{\\theta}||P_{\\theta_u}) = \\sum_{y \\in Y} \\sum_{i=1}^{n} P_{\\theta}(y) (l_{\\theta,i} - l_{\\theta_u,i})$ (12)\nFinally, swapping the order of summation:\n$D_{KL}(P_{\\theta}||P_{\\theta_u}) = \\sum_{i=1}^{n} \\sum_{y \\in Y} P_{\\theta}(y) (l_{\\theta,i} - l_{\\theta_u,i})$ (13)\nThis final form gives us the KL divergence between two autoregressive language models in terms of their logits. Each term in the outer sum represents the contribution to the KL divergence from predicting the i-th token, averaged over all possible sequences y."}, {"title": "6.2 Alternate Derivation", "content": "Our objective is to learn the probability distribution over token sequences y \u2208 V* of the prompted model $P_{\\theta}(y | u)$. We learn the parametric distribution $P_{\\theta_u}(y)$ by minimizing the Kullback-Leibler divergence between distributions:\n$D_{KL} (P_{\\theta}(\u00b7 | u) || P_{\\theta_u}(\u00b7)) = E_{y \\sim P_{\\theta}(\u00b7|u)} [log \\frac{P_{\\theta}(y | u)}{P_{\\theta_u}(y)}]$"}, {"title": "7 Instruct and Persona Prompts", "content": "In this section we define the prompts we use for the instruct and persona prompts. All other prompts are taken from [19].\n7.1 Reverse Input\nRespond only with a reversed version of the user input.\nIf the user inputs: How are you doing?\nOutput: ?doing you are How\nIf the user inputs: What is the capital of France?\nOutput: ?France of capital the is What"}, {"title": "8 Few-Shot Chain-of-Thought prompts", "content": "In this section we present the exact prompts we bake in and utilize for studying zero-shot and few-shot chain-of-thought performance, in particular the results reported in section 3.3."}, {"title": "8.1 GSM8K and ASDiv", "content": "We follow the standard CoT prompt used by Meta for evaluating Llama [8] and introduced by [34].\nGiven the following problem, reason and give a final answer to the problem.\nProblem: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\nYour response should end with \"The final answer is (answer)\" where (answer) is the response to the problem.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nThere are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The final answer is 6<|eot_id|><|start_header_id|>user<|end_header_id|>\nGiven the following problem, reason and give a final answer to the problem.\nProblem: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nYour response should end with \"The final answer is (answer)\" where (answer) is the response to the problem.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nThere are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The final answer is 5<|eot_id|><|\nstart_header_id|>user<|end_header_id|>\nGiven the following problem, reason and give a final answer to the problem.\nProblem: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nYour response should end with \"The final answer is (answer)\" where (answer) is the response to the problem.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nOriginally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The final answer is 39<|eot_id|><|start_header_id|>user</\nend_header_id|>\nGiven the following problem, reason and give a final answer to the problem.\nProblem: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\nYour response should end with \"The final answer is (answer)\" where (answer) is the response to the problem.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nJason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The final answer is 8<|eot_id|><|start_header_id|>user<|end_header_id|>\nGiven the following problem, reason and give a final answer to the problem.\nProblem: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\nYour response should end with \"The final answer is (answer)\" where (answer) is the response to the problem.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nShawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The final answer is 9<|eot_id|><|start_header_id|>user<|end_header_id|>\nGiven the following problem, reason and give a final answer to the problem.\nProblem: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\nYour response should end with \"The final answer is (answer)\" where (answer) is the response to the problem.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nThere were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The final answer is 29<|eot_id|><|start_header_id|>user<|\nend_header_id|>"}, {"title": "8.2 MBPP", "content": "You are an expert Python programmer", "task": "nWrite a function to find the similar elements from the given two tuple lists.\nYour code should pass the following tests:\nassert similar_elements ((3", "test_tup2)": "n res = tuple(set(test_tup1) & set (test_tup2))\n return (res)\n'''<|eot_id|><|start_header_id|>user<|end_header_id|>\nYou are an expert Python programmer"}, {"task": "nWrite a python function to identify non-prime numbers.\nYour code should pass the following tests:\nassert is_not_prime (2) == False\nassert is_not_prime (10) == True\nassert is_not_prime"}]}