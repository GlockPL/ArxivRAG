{"title": "Do Attention Heads Compete or Cooperate during Counting?", "authors": ["P\u00e1l Zs\u00e1mboki", "\u00c1d\u00e1m Frakn\u00f3i", "M\u00e1t\u00e9 Gedeon", "Andr\u00e1s Kornai", "Zsolt Zombori"], "abstract": "We present an in-depth mechanistic interpretability analysis of training small transformers on an elementary task, counting, which is a crucial deductive step in many algorithms. In particular, we investigate the collaboration/competition among the attention heads: we ask whether the attention heads behave as a pseudo-ensemble, all solving the same subtask, or they perform different subtasks, meaning that they can only solve the original task in conjunction. Our work presents evidence that on the semantics of the counting task, attention heads behave as a pseudo-ensemble, but their outputs need to be aggregated in a non-uniform manner in order to create an encoding that conforms to the syntax. Our source code will be available upon publication.", "sections": [{"title": "1. Introduction", "content": "The transformer architecture, via the multistage training method of first imbuing the model with generic linguistic knowledge and then finetuning it to the task at hand, has proven remarkably versatile in creating computer programs that can perform assignments as diverse as emotional support, idea brainstorming, mathematical problem solving, or pair programming.\n\nThe path to these heights does not look like a steady climb: it features sudden jumps in the form of emergent features. As humanity relies on language models in tasks of increasing importance, it is paramount to understand how this phenomenon comes about and how language models are making their predictions.\n\nThe approach of mechanistic interpretability aims to do this by zooming in on the models and studying the circuits that form the model as a program. This method was first applied to convolutional neural networks (Cammarata et al., 2020). Since then, transformers are also being actively studied this way, starting with (Elhage et al., 2021), where they study in-context learning by attention-only transformers, on very simple information retrieval tasks.\n\nWe aim to study the reasoning capabilities of transformers and we follow the methodology of Nanda et al. (2023) by training small transformer models on simple algorithmic tasks, in order to fully interpret the trained model. We use a simple counting task, provide a handcrafted minimal solution and, informed by a careful analysis, we present a hypothesis of how the trained models solve the task. We also provide interventions to support our hypothesis.\n\nOur investigation centers around the topic of how the trained model's prediction is composed of that of its components. We examine what individual heads learn, how they perform in isolation and how the output layer of the model learns to exploit them. We bring in a new research question: in a transformer, do attention heads behave as a pseudo-ensemble, all solving the same subtask, or do they perform different subtasks, meaning that they can solve the task only in conjunction?\n\nWe introduce a number of probing experiments to study this problem. Our results indicate that the attention heads behave as a pseudo-ensemble: they learn to output representations that make the counting task solvable by logistic regression. However, we show that in order to conform to the syntax of the full task, i.e. to end the generated sentence, the output layer of the model needs to aggregate these representations in a non-uniform way.\n\nIn summary, our contributions are as follows:\n\n\u2022 We introduce the Count01 language, which deals with a remarkably simple counting task, whether a string has more 1s than 0s in the presence of noise given by 2s. This can be separated to the two subtasks of outputting the answer token (the main task) and ending the sentence (the syntactic task).\n\n\u2022 We also provide a minimal solution which, however, the model cannot reach via learning from random initialisation."}, {"title": "2. Related Work", "content": "In a pioneering work on mechanistic interpretability of transformers (Elhage et al., 2021) investigate in-context learning effects of 0, 1 and 2-layer attention-only transformers trained via next token prediction on a text corpus. They point out that \"One layer attention-only transformers are an ensemble of bigram and \"skip-trigram\" (sequences of the form \"A... BC\") models.\", but they don't investigate the formation of this ensemble. Their focus is on induction heads, formed by 2-layer attention-only transformers.\n\nAn important current in mechanistic interpretability is studying the circuits formed by small transformers while training on formal tasks. A prime example is (Nanda et al., 2023), where they teach 1-layer transformers on modular addition. They show that the composite of the 4-head self-attention block and the first layer of the MLP together with the activation function map to trigonometric functions of the input. On the other hand, they do not study how the attention heads cooperate to this end.\n\nPrograms written in the Restricted Access Sequence Processing Language (RASP) (Weiss et al., 2021) can be implemented by transformer models. This yields readily interpretable transformer implementations of counting tasks such as Double Histogram and Shuffle-Dyck. Moreover, it is possible to restrict transformer models to learn easy to implement programs (Friedman et al., 2023). We are more interested in the programs unrestricted transformer models learn.\n\nMinimal architectural hyperparameters for a single head 1-layer transformer to learn the Histogram task are studied in (Behrens et al., 2024). Depending on the hyperparameter configurations, they hypothesize two possible families of programs: relation- and inventory-based counting, for which they give example implementations. However, it is not proven via mechanistic interpretability that the models actually learn the hypothesized programs. Our problem statement differs from theirs significantly in that the amount of counting we perform is limited only by input string length, with vocabulary size kept to 8 (including [BOS] and [EOS]).\n\nIn (Wen et al., 2023) it is shown both theoretically and experimentally that there are in fact numerous ways a single head 2-layer transformer can learn Dyck languages: the second attention block needs only satisfy a mild condition for this. We corroborate this finding with our in-depth study of separation accuracy of attention heads."}, {"title": "3. Problem Statement", "content": "In this paper we target a simple counting problem where the model has to decide if the input contains more '1' tokens than '0' tokens. The input may contain an arbitrary number of '2' tokens interspersed between the '0's and '1's (which may also occur in any order). For ease of analysis, the input has an '=' token on the penultimate position, which must be followed by '4' if there are more 1s than 0s, and '5' otherwise.\n\nDefinition 3.1 (Count01 language). Each string $s$ of the Count01 language fits the regular expression $[BOS]{0,1,2}*={4,5}[EOS]$ so that the final letter is 4 if the substring before the '=' contains more 1s than 0s and 5 otherwise.\n\nThe choice between '4' and '5' is the semantic problem of counting, getting the final '[EOS]' token is the syntactic problem. Both tasks are well handled by the (skip)trigram mechanism posited by (Elhage et al., 2021). Remarkably, it is only the syntax that brings in cooperation among the attention heads, the semantics shows ensemble behavior.\n\nWe generate a dataset with 7000 training, 1500 validation and 1500 test sentences as follows: For each sentence, we randomly draw the number of '0', '1' and '2' tokens separately, then generate the appropriate number of these tokens in random order. Note that shuffling does not matter for our present architecture as it does not include positional embedding. The numbers of '0' and '1' tokens are drawn from the intervals [0, 100], [101, 150] and [151, 200] for the training, validation, and test sets respectively. The numbers of '2' tokens are drawn from the intervals [0, 100], [0, 150] and [0, 200] for the training, validation, and test sets respectively.\n\nWe use this dataset to train a single layer, attention-only generative transformer language model. For each sample, the model is optimised to minimise the negative log likelihood of the text after the '=' token. In other words, the model has to learn to generate one of the '4' or '5' tokens, followed by the '[EOS]' token indicating the end of the sentence. The superficial syntax, that the '=' token must be followed by '4' or '5' is learned early in the training, but this permits only 50% correct (in other words, random) recognition before the counting aspect is learned."}, {"title": "4. Understanding Attention", "content": "We briefly describe the computation performed by a transformer attention layer, which is the primary mechanism for interaction among tokens. Its input is a sequence of token embeddings ${e_i | 0 < i < l}$ where $l$ is the length of the sequence and each token embedding is a learned d-dimensional vector, i.e., $e_i \\in \\mathbb{R}^d$.\n\nAn optional normalization layer ensures that each embedding vector has learned mean $\u00b5$ and variance $\u03c3$:\n\n$c_i = \\frac{e_i - mean(e_i)}{std(e_i)} \u03c3 + \u00b5$\n\nThe embedding sequence is processed by a number a of attention heads. Each head produces contextual token embedding vectors.\n\nEach head has an internal dimension $d_o$, which usually equals $d/a$. Each head h (0 < h < a) has learned $d \u00d7 d_o$ dimensional Key ($K_h$), Query ($Q_h$), and Value ($V_h$) parameter matrices. Applying these matrices to the token embeddings $e_i$ produces l \u00d7 $d_o$ dimensional key, query, value feature matrices $k_h$, $q_h$, $v_h$.\n\nThe key and query feature matrices together create an l \u00d7 1 dimensional attention weight matrix $A_h$ where $A_{h,ij}$ specifies how strongly the jth token influences the output of the ith token. This is computed by taking the scalar product of the query and key feature vectors, obtaining the attention logits and then normalising each row to sum to 1:\n\n$A_{h,i} = \\frac{q_h^T k_h}{\\sqrt{d_o}}$\n\n$A_{h,i} = softmax(A_{h,i})$\n\nThe attention logit of a particular token pair does not depend on the input sequence, only on the learned key and query matrices. This does not hold for the attention value, due to the normalisation constant in the softmax function. However, if we consider the attention at some token position k to two tokens i and j, then their ratio $\\frac{A_{h,ki}}{A_{h,kj}}$ is input independent since the normalisation constant is cancelled out. We will exploit this fact to characterise heads by attention weight ratios of some token types.\n\nThe head output is the sum of the head values, weighted by the corresponding attention vector:\n\n$O_h = A_h v_h$\n\nSince the attention vector for each token has nonnegative entries that sum to 1, the output is a convex combination of the value vectors. The concatenation of the outputs of all heads are projected back to d dimensional vectors (via another learned linear transformation P), dropout is applied and then the output is added to the input embedding.\n\nThe attention layer may be followed by a tokenwise small MLP, again preceded by layer normalization and followed by dropout, constituting a single transformer block. Any number of transformer blocks could be stacked on top of each other, but in this paper we focus on single layer, attention-only models.\n\nMoreover, we fuse the linear transformation P into the linear trasformation in the output layer. We can do this as we noticed that, just like in (Nanda et al., 2023, \u00a75 and Appendix A.1), the skip connection has negligible effect. The output layer of the model is a tokenwise affine transformation with weights wo and bias bo that produces logits for each possible token type."}, {"title": "4.1. A Minimal Solution", "content": "The Count01 language is simple enough so that a very small single layer, attention-only transformer can solve it with or without layer norm. Given a partial input sequence, e.g., '000010=', the model has to perform two tasks:\n\n1. Whenever it sees the '=' token, it has to count '0's and '1's and output either '5' if there are more '1's or '4' otherwise.\n\n2. Whenever it sees a '4' or '5', it has to output an '[EOS]' token, indicating the end of the sentence.\n\nIt turns out that a single one-dimensional head (a = d = 1) is sufficient to correctly count '0's and '1's. The head has to learn an attention pattern in which the '=' token attends to the '0' and '1' tokens with different but large weights. Based on their relative weights, a linear separator is then sufficient to decide what to write after the '=' sign. The second task is also simple. The head learns to attend '4' and '5' tokens to themselves with an even larger weight, thus the linear separator can predict the '[EOS]' token if the head output is large enough. In Appendix A we provide a possible parametrisation of this architecture.\n\nNote, however, that there is a gap between what an architecture can represent and what it can learn. As we shall see, it is rather challenging to successfully train the minimal model. To better understand training dynamics, we continue our investigation with a somewhat larger, while still small model that trains to high accuracy consistently."}, {"title": "5. Understanding Heads", "content": "In this section we delve into the capabilities and dynamics of individual heads. We focus on predicting the token after '='. See Subsection 6.2 for the syntactic task of outputting an '[EOS]' token after '4' or '5'."}, {"title": "5.1. Three Metrics: l-acc, ROC AUC, and s-acc", "content": "Now we shall introduce three metrics to measure the performance of a group of heads $H \\subseteq \\{0, . . ., a \u2212 1\\}$.\n\nLearned Accuracy (l-acc) of H is the model accuracy on the test set when all heads outside H are ablated, i.e., their output is zeroed out. That is, this is the accuracy of the predictions we get with logit for token t given as $z_{t,H} := \\sum_{i \\in H} z_{t,i} + b_t$. See Figure 1 for logits given by single heads. For now, let us concentrate on '4' and '5' logits at '='.\n\nOne can see on Figure 1 that many heads are highly biased toward one of '4' or '5'. This is what makes the cooperation of heads a central topic in our paper. On the other hand, one can also see that most heads make a meaningful contribution, to the final goal of getting $z_4 > z_5$ if and only if there are more 1's in the input than 0's.\u00b9 The second metric measures this contribution: we can treat either $z_{4,H}$ or $z_{5,H}$ by itself a binary classification logit in the task of determining whether the next token is '4' resp. '5' or not. Then for these two binary classification problems, we can calculate the ROC AUC values. In our setting, the ROC AUC metric is the maximum of these two values.\n\nThe third metric exploits the fact that the final logits are computed from the attention heads by an affine transformation, i.e, the usefulness of heads - or groups of heads - can be measured by their ability to linearly separate the inputs into correct classes. Separation Accuracy (s-acc) of H is computed by fitting a linear separator on the concatenation of the head outputs $O_i$ for i \u2208 H based on the training set and computing accuracy on the test set. Details about how we identify the linear separator can be found in Appendix C."}, {"title": "5.2. Understanding Individual Heads", "content": "We turn to analysing heads in isolation. At the '=' token, the model is expected to linearly separate inputs with more '1's (requiring a '4' token) from the rest of the sequences (requiring a '5' token). Figure 3 shows l-acc, ROC AUC, and s-acc values of singleton heads and head pairs. More plots are available in Appendix D.\n\nBased on the s-acc scores, we group the heads into three categories. Around half of the heads are successful, having above 98% accuracy. Around 40% of the heads are failed, i.e., their performance is close to that of coin flip. We call the few remaining heads mixed with mediocre performance.\n\nA very different picture emerges when considering the l-acc scores. All but one single head have 50% l-acc score, i.e., they are not better than random guessing and only two head pairs rise above 80%. What this shows is that training produces several near perfect heads, but the model does not learn to directly exploit any of them.\n\nOne can see that the ROC AUC scores strike a middle ground. A good ROC AUC score signifies meaningful contribution disregarding biases. However, the model uses these biases to predict the '[EOS]' tokens after '4' or '5': see Subsection 6.2.\n\nAn even stronger demonstration of this pattern emerges if we compute s-acc, ROC AUC and l-acc scores for each subset of heads. Figure 4 shows that groups of 3-4 heads already yield close to perfect separation accuracy and ROC AUC values follow close, while most of the heads are required to achieve strong learned accuracy."}, {"title": "5.3. The Role of Attention", "content": "We now show that the s-acc performance of a head is determined by its attention pattern, more precisely how it divides attention among the '0', '1' and '2' tokens. To understand why attention to other tokens is not important, note that in the Count01 language, whenever the '=' token is processed, there is exactly one of the '[BOS]' and '=' tokens, and zero of the '4', '5', and '[EOS]' tokens. Hence, their contribution to the head output is constant across inputs, i.e., they do not influence the separability of the samples regardless of their attention weights.\n\nAs noted in Section 4, a head can be characterised by the ratio of two tokens at some position since this value is input independent. We focus on the attention at the '=' token position and compute the attention weight ratios among the 0, 1 and 2 tokens. We use $w_{01}$ to denote the weight ratio between the '0' and '1' tokens and $w_{02}$ to denote the weight ratio between the '0' and '2' tokens."}, {"title": "5.4. Interventions", "content": "Our investigations suggest that head peformance in the Count01 task is essentially determined by the attention pattern and the value feature vectors play minor role. We further highlight this by taking a trained model and make interventions on the attention matrix, without touching anything else. We focus on the attention matrix at the '=' token and zero out all attention weights except for the '0', '1', '2' tokens.\n\nFigure 8 shows head performance distribution when attention to the '2' token is set to zero and we vary the attention ratio among the '0' and '1' tokens. All heads perform near perfectly when this ratio is neither too small not too large (between 0.1 and 10). More extreme ratios yield sharp drop in the average performance, as well as greater variance. Hence, we conclude that as long as the '2' token is not attended to and $w_{01}$ is in the healthy region, other head features, i.e, the value feature vectors barely influence separability."}, {"title": "5.5. Head Evolution During Training", "content": "As we have seen in Figure 3c, training results in roughly half of the heads being successful and capable of strong linear separation. We now look at the evolution of s-acc scores during training. Figure 14 in Appendix F reveals that heads initially manifest all sorts of s-acc values, including extremely strong ones. In the initial phases of training s-acc values change a lot and we see extreme transitions, i.e., very bad heads becoming very good and vice versa. In around 50 epochs the heads start to stabilise and we obtain a heavily bipolar distribution: the trained heads are either very good or very bad."}, {"title": "6. Interaction among Heads", "content": "We now turn our attention to interaction among heads. Figure 3c shows how pairs of heads improve or impair the linear separation capability of individual heads. Improvement is rare and it mostly happens when two failed heads are considered together, but they still perform poorly. Nevertheless, in each experiment we do see rare examples of a successful head made even better by another head, even reaching perfect separation accuracy, see e.g. heads (5,13) in Figure 3c.\n\nWe have seen previously in Figure 3a that the model does not learn to fully exploit successful heads. How about pairs of heads? The situation is not fundamentally different, as we see that learned accuracy scores for pairs of heads are almost always around 50%. Even if there are exceptions, Figure 4 shows that a large number of heads is required to achieve large learned accuracy and all the 16 heads are required to achieve perfect l-acc score."}, {"title": "6.1. Head Cooperation", "content": "Is there any cooperation among heads? Is a trained model more than an ensemble of heads? One can easily construct solutions to the Count01 problem in which heads actively cooperate. For example, there could be one head that counts '0's, another that counts '1's and the output layer could simply compare the outputs of these heads. Alternatively, the number of '2' tokens could be split into intervals and each head could learn to separate well in one of the intervals. In such cooperative solutions, we should see pairs of heads performing better than individual heads. However, Figure 3c shows that such positive interactions are rare and the s-acc values of single heads are already high.\n\nIf the model is indeed an ensemble of heads, then we should be able to predict the model's performance from that of the heads. We now show that such prediction is quite possible.\n\nFor what follows, recall the notation introduced in the beginning of Section 5 We compute the lengths of the $w_{i,5} - w_{i,4}$ vectors and normalise them across heads to sum to 1. We call the obtained values the model learned head weights (hw). We show that if we compute the weighted sum of the s-acc values, with hw used as weights, then we get a value that very strongly correlates with model accuracy. This can be seen in the scatter plot of Figure 10, which shows that the Pearson correlation coefficient of the two values is 0.72. This strong correlation suggests that it is almost the same if 1) we first compute separation accuracy for each head and then average them weighted according to the final model or 2) we first compute the model output from the heads by the final layer and then compute separation accuracy. This suggests that the model is working as a proper ensemble, solving two separate tasks: 1) make the heads as good as possible and 2) learn a good linear combination of the head outputs."}, {"title": "6.2. The Syntactic Task: Predicting the '[EOS]' Token", "content": "While our analysis concentrates on the counting task, i.e, deciding whether there are more '1's than '0's in the input, we briefly mention the second, more technical task that the model has to learn: to put an '[EOS]' token after the answer. This is an extremely simple task and the model requires minimal capacity to learn it: based on 5 experiments (using 80 heads altogether), we find that heads and head pairs reach 100% learned accuracy with probability 0.46 and 0.5, respectively.\n\nAs to how the '[EOS]' token logits coalesce: we already noticed in Section 5 how the heads are typically biased toward one out of '4' or '5'. See Figure 1 in Appendix G for next token logit distributions of individual heads for all of '4', '5', '[EOS]', over both '=', and '4' or '5'. One can see that in a single head, the ordering of the logits is about fixed, but the balance between the logits tips toward '[EOS]' when we move from '=' to '4' or '5'."}, {"title": "7. Discussion and Conclusion", "content": "In this work, we use mechanistic interpretability to study how single-layer attention-only transformers solve a simple counting task. We notice that although there exists a very small minimal solution, in order for randomly initialized models to reliably learn to solve the task, we need to make them somewhat bigger.\n\nWe focus on an architecture with 16 attention heads, thus giving us opportunity to study the orchestration between heads. We show that they individually learn to solve the main task, but their outputs need to be aggregated in a non-uniform way so as to solve the auxiliary task.\n\nThe methods we use are by no means limited in scope to counting tasks: we believe they could serve as tools to mechanistically interpret transformer models trained to other, more complicated tasks."}, {"title": "A. A Minimal Solution", "content": "Let $n_0$ and $n_1$ be the number of '0' and '1' tokens in the input, respectively. If we have $d = 1$, $d_o = 1$, it is possible to construct a Transformer that solves the problem, if $n_0 + n_1 > 0$. Below, we present a minimal setup that solves the problem. We omit the head indices, since there is only one head. Layer normalisation is not applied, because it would produce the same embedding vector for all tokens if $d = 1$. The one-dimensional token embeddings will be\n\n\n$w_E = \\begin{bmatrix}\n    0 & '[BOS]'\n    N & '0'\n    N+1 & '1'\n    0 & '2'\n    1 & '='\n    N^2 & '4'\n    N^2 & '5'\n    0 & '[EOS]'\n\\end{bmatrix}$\n\nand the key, query and value parameter matrices will be $K = Q = V = 1$. We are only interested in query feature matrices of tokens '=', '4' and '5', so the relevant token pair attentions logits will be the following:\n\n$\\begin{array}{c|cccccc}\n & k_{bos} = 0 & k_0 = N & k_1 = N+1 & k_2 = 0 & k_= = 1 & k_4 = k_5 = N^2 \\\n\\hline\nq_=\\nq_4\\nq_5 & 1\\-& 0\\- & N\\-& N+1\\- & 0\\-& 1\\- & N^2\\-\\end{array}$  $\\begin{array}{c|cccccc}\n & k_{bos} = 0 & k_0 = N & k_1 = N+1 & k_2 = 0 & k_= = 1 & k_4 = k_5 = N^2 \\\n\\hline\nq_=\\nq_4\\nq_5 & 1 & N & N+1 & 0 & 1 & N^2 \\\\\n\\hline\nq_=\\nq_4\\nq_5 & N^2 & 0 & 0 & N & N+1 & 0 \\ N^3 & N^3 + N^2 & 0 & 1 & N^2\\ N^4 \\\\ \\end{array}$\n\nThe attention logits of $q_= k_4$ and $q_=k_5$ will not be used at all, because '4' and '5' tokens come only after '=' token. For this reason we denoted its values with '-'.\n\nSince we take a softmax function for each query vector, therefore if the input ends with '=', then only $A_{=,0}$ or $A_{=,1}$ will be significant. If the input ends with '4' or '5', then $A_{4,4}$ or $A_{5,5}$ will be significant only. Therefore, if $n_0 + n_1 > 0$, then the output of the attention mechanism in the first task will be:\n\n$y_= \\frac{e_{bos} + n_0 e^{Nv_0} + n_1 e^{N+1} v_1 + n_2 v_2 + e_=\\cdot v_=}{1 + n_0 e^{N} + n_1 e^{N+1} + n_2 + e_=} =  \\frac{n_0e^{N}v_0 + n_1 e^{N+1} v_1 + n_2 v_2 + e_=\\cdot v_=}{1 + n_0 e^{N} + n_1 e^{N+1} + n_2 + e_=}$ $ \\approx \\frac{n_0v_0 + n_1e^{N+1} v_1}{n_0 + n_1e^{N+1}} $ $ \\approx  \\frac{e^{Nv_0}(n_0 N + n_1e^{(N + 1)})}{1 + n_0 e^{N} + n_1 e^{N+1}}$\n\nLet a be defined such as $y_=\\geq a \\Leftrightarrow n_0\\geq n_1$. We get that $a := \\frac{e^{(n+1)}+n}{1+e}$. Without loss of generality we can assume that the last token is '4' in the second task. Then the output will be\n\n$y_4 = \\frac{e_{bos} + n_0 e^{Nv_0} + n_1 e^{N+1} v_1 + n_2 v_2 + e_=\\cdot v_= + e_4^{N^2} v_4}{1 + n_0 e^{N} + n_1 e^{N+1} + n_2 + e_= + e_4^{N^2}} $  $\\approx \\frac{e4+4+e4+4}{1+e4+4} $ $ \\approx N^2 $\n\nThese are only approximations, and are only valid if $n_2 \\ll e^N$, where $n_2$ is the number of '2' tokens in the input. However, if we assume that the Transformer is not infinitely precise then with an appropriate large N value, the self attention mechanism outputs these values. Now, we need to define the output layer, and prove that it will produce the correct logits:\n\n$W_{out} = \\begin{bmatrix}\n    0 & 0 & 0 & 0 & 0 & 0 & 1 & -1\n\\end{bmatrix}^4$\n$b_{out} = \\begin{bmatrix}\n    0 & 0 & 0 & 0 & 0 & 0 & -a-1 & a+1 + \\varepsilon - N-4.3(N+1)\n\\end{bmatrix}$\n\nwhere $\\varepsilon$ is a small positive number that compensates the approximation error. Let $e_{\\text{<token>}}$ denote the embedding vector corresponding to a given token, which contributes to the residual connection, and let $v_{\\text{<token>}}$ represent the corresponding logit vector. The output layer yields the following logits for the first task:\n\n$(e_=+y_)W_{out} + b_{out} \\approx (y_= - a) v_4 + (a \u2013 y_=) v_5 + \\varepsilon\\cdot v_{[EOS]}$,\n\nwhere $c = N-4 \\cdot y_= \u2212 12(N + 1) < 0$, which predicts indeed '4' or '5' depending on whether $n_0/n_1 \\geq 1$ or not. In the second task, we have\n\n$(e_4+y_4)W_{out} + b_{out} \\approx (4N^2 - 12(N + 1)) v_{[EOS]} + (N^2 \u2013 a \u2212 1)v_4 + (a + 1 \u2013 N^2)v_5$\n\nlogits, which predicts '[EOS]' token if  is large enough.\n\nThis example assumed the skip connection, but the model weights can be slightly modified to achieve the same results without the skip connection."}, {"title": "B. Trying to Find Minimal, Perfect Heads via Learning", "content": "We have seen that in the minimal solution, a single head with head dimension $d_o = 1$ is sufficient. In this section, we explore how feasible it is to find such heads via learning. We run a large number of experiments with fixed $d_o = 1$ in several setups, changing both the token embedding dimension and the number of heads, and monitor the number of perfect (s-acc = 1.0) or successful (s-acc \u2265 0.98) heads that arise by the end of the training. The results can be seen in Table 1.\n\nWe find that increasing the token embedding dimension d greatly increases the chance of finding perfect minimal heads. When $d = 32$ we obtain 17 perfect heads out of 100 single head experiments, while for $d = 1$ none of the heads are perfect out of 650 experiments. This suggests that the minimal head dimension do is reasonably achievable via learning, while the minimal token embedding dimension d is beyond the reach of gradient descent.\n\nWe previously looked at various ways in which the trained model heads could cooperate and found little evidence of it. There is, however, another sort of possible cooperation: they could positively enhance each other's training trajectories. If this is the case, we should see heads trained jointly to perform better than those trained in isolation. However, the last four lines of Table 1 suggest the contrary. When d = 1, we never find any perfect heads, but looking at heads with over 98% s-acc value, we see that single head experiments succeed twice as often as 64 head experiments (0.6% vs 0.3%). The results are similar when $d = 2$: single head experiments produce perfect heads 8.5 times as often as multi head experiments (1.7% vs 0.2%). We conclude that it does not help the heads when they are trained jointly, they perform much better after separate trainings."}, {"title": "C. Fitting a Linear Separator to a Set of Heads", "content": "We expect that if $H' \\subset H$ than the separation accuracy (s-acc) of H is larger than that of H'", "following": "n\n$\\frac{1}{2}||w_s||^2 + C \\sum_{i=1}^n max(0,1 - y_i (r w_s + b_s))$\n\nwhere ws, bs are parameters of the linear separation and $y_i \\"}]}