{"title": "An Automated Approach to Collecting and Labeling Time Series Data for Event Detection Using Elastic Node Hardware", "authors": ["Tianheng Ling", "Islam Mansour", "Chao Qian", "Gregor Schiele"], "abstract": "Recent advancements in IoT technologies have underscored the importance of using sensor data to understand environmental contexts effectively. This paper introduces a novel embedded system designed to autonomously label sensor data directly on IoT devices, thereby enhancing the efficiency of data collection methods. We present an integrated hardware and software solution equipped with specialized labeling sensors that streamline the capture and labeling of diverse types of sensor data. By implementing local processing with lightweight labeling methods, our system minimizes the need for extensive data transmission and reduces dependence on external resources. Experimental validation with collected data and a Convolutional Neural Network (CNN) model achieved a high classification accuracy of up to 91.67%, as confirmed through 4-fold cross-validation. These results demonstrate the system's robust capability to collect audio and vibration data with correct labels.", "sections": [{"title": "1 Introduction", "content": "Event detection has become a popular topic in pervasive computing [1], enabling intelligent systems to interpret environmental contexts and adapt config-urations within various spaces, for example, offices or kitchens [2,3]. Traditional IoT methods often utilize multiple types of indirect sensor data, such as audio and vibrations [4], which are processed through Deep Learning (DL) models for event recognition.\nSufficiently labeled datasets are necessary to train DL models effectively [5]. Typically, data streams are segmented and annotated with labels [6]. One com-mon approach to collecting these datasets involves transmitting sensor data to the cloud [7], where labeling algorithms are applied [8], or storing the data streams for subsequent manual labeling by human workers [9]. Both methods, however, introduce significant delays and dependencies on external resources."}, {"title": "2 Related Work", "content": "Previous studies predominantly relied on human involvement in the recording and labeling process, which not only complicates the procedure but also increases costs and the potential for errors during manual operations.\nSpecifically, Koch et al. [11] manually controlled the start and stop of record-ings for each event. While this method minimizes storage requirements, it intro-duces complexity and heightens the risk of human error. In contrast, Anand et al. [12] implemented continuous data recording with post-collection labels based on timestamps and event types. This method simplifies the recording process but often accumulates large volumes of irrelevant data, leading to inefficient storage usage, especially when events are infrequent. Furthermore, while humans can feasibly label audio data by listening, this approach is impractical for vibration data.\nIn response to these challenges, our research introduces a novel automated system that significantly reduces the need for manual intervention by automating the collection of reference labels. Our approach utilizes additional sensors that only need light-weight computation to determine the event type locally. With an on-device approach, we are free from synchronization challenges and can efficiently capture the essential sensor data before and after an event occurs."}, {"title": "3 Hardware System Design", "content": "In this study, we propose a hardware platform named Elastic Node Sensor Logger. Figure 1 illustrates the architecture of our hardware platform, centered around the RP2040, an ARM Cortex-M0+ Microcontroller Unit (MCU) known for its low power consumption. In addition, its performance is sufficient for run-ning FreeRTOS to make our multi-task scheduling easier than just using a bare-metal setup. This MCU also owns enough analog and digital I/O capabilities, which are crucial for managing the various sensors and storage modules incor-porated into our system.\nOur system includes two categories of sensors: feature and labeling sensors. The feature sensors are supposed to monitor the events indirectly. There is a Pulse Density Modulation (PDM) microphone for collecting audio data, and an Inertial Measurement Unit (IMU) programmed as an accelerator meter for collecting vibration data. Their sampling frequency is a parameter that the user can adjust. This configuration facilitates the generation of time series data, which is vital for training our DL models.\nFor event labeling, we utilize a reed sensor and a current sensor. The reed sensor detects door states by issuing a rising edge interrupt to the MCU when the door opens and a falling edge interrupt upon closing. Concurrently, the current sensor monitors the power consumption of a kettle. We use the analog-to-digital converter on MCU to detect the 'water has boiled' event based on a predefined current threshold (zero). In addition, a lower-power Real Time Clock (RTC) is embedded in the board to provide the timestamp for events.\nAdditionally, an SD Card, connected to the MCU via the Serial Peripheral Interface, has been configured to operate at a writing speed with a clock frequency of up to 50 MHz. This high speed far exceeds the data acquisition rates from all sensors, ensuring that data logging remains efficient and does not hinder the system's overall performance.\nThe power management subsystem, including the MCP73833 for battery charging and the LM1117-3.3 for voltage regulation, ensures sufficient power utilization across all components. Given that the peak current consumption is estimated at 440 mW, a low-dropout regulator, LM1117-3.3, is sufficiently adequate for our power regulation needs, promoting system stability and efficiency."}, {"title": "4 Software Implementation", "content": "The main software loop, executed on the MCU, is depicted in Figure 2. At the outset, we initialize the sensor drivers and mount the SD card, setting the stage for data collection.\nTo optimize memory management and processing efficiency, we implement a ping-pong buffer strategy on the MCU for handling sensor data, a method akin to that described in [13]. Data is initially collected in the 'ping' buffer until it reaches capacity. At this point, data storage is switched to the 'pong' buffer. This cycle alternates to ensure continuous data acquisition. Upon filling either buffer, a Direct Memory Access (DMA) is triggered to transfer the data to the SD card, thereby offloading the data writing task from the MCU. This setup ensures that the SD card's write speed surpasses our data acquisition rate and provides ample buffer time to prevent overwriting and maintain data integrity.\nThe system has three event flags, two set by external interrupt callbacks and the third by a threshold-based trigger following ADC readings in a separate periodic task. This event detection logic is straightforward and computationally efficient, avoiding disruptions in data collection.\nThe system checks for flagged events once the DMA completes the data transfer from one buffer. If an event has been flagged, the corresponding label is immediately written to the SD card. The recording file may be closed promptly or left open for several seconds to capture additional post-event data, and the duration of the continuous recording is user-configurable."}, {"title": "5 Experiments and Results", "content": "Building upon the hardware system design outlined in Section 3, we success-fully implemented the hardware platform as depicted in Figure 3. Utilizing this hardware and following the software implementation described in Section 4, we conducted multi-sensor data collection and labeling directly on our hardware platform. Subsequently, the collected dataset underwent preprocessing and val-idation on a desktop computer.\nThe data collection process solely requires the use of the Elastic Node Sensor Logger hardware. As mentioned in Section 4, once the recording process initi-ates, our system simultaneously collects data from the microphone and the IMU sensor. The labeling sensors operate in the background as described. The audio data is captured at a sampling rate of 16 kHz in a mono-channel format. The vibration data, which includes three channels corresponding to acceleration, is collected at a sampling rate of 4 kHz. After starting the recording, the device operates autonomously for several hours. During this period, the user (operator) randomly engages in activities such as opening and closing doors and boiling water to generate event data. In total, we collected 106 samples from each type of sensor: 40 samples were associated with door opening events, 29 with door closing, and 37 with water boiling in a kettle."}, {"title": "5.2 Data Preprocessing", "content": "Before we fed our custom dataset to the model, audio and vibration data were preprocessed separately to accommodate their unique characteristics. Audio recordings were read from WAV files using torchaudio [14]. To standardize the lengths of these recordings, zero-padding was symmetrically applied to both ends to have the same length as the longest audio data in the dataset. For feature extraction, we transformed the recordings into Mel spectrograms using the following parameters: \\(Nmels = 64\\), \\(nfft = 1024\\), with hoplength at default settings, and \\(topdb = 80\\) for dynamic range compression. For example, door-related events exhibit short-term peaks along the time axis and a broader range of frequency coverage compared to kettle-boiling events. Furthermore, distinguishable patterns are evident between door opening and closing events, such as the longer duration of closing events compared to opening events.\nVibration recordings, comprised of channel measurements, varied in length across samples. We addressed this by applying a zero-padding strategy similar to that used for audio data. In instances of missing values, we imputed these by calculating and using the mean of the respective dimension. Distinct patterns are identifiable between the door-related events, and there is a clear difference between these and the water-boiling events in the time domain."}, {"title": "5.3 Data Validation", "content": "To verify the quality of the collected data and the accuracy of labeling, we conducted a three-class classification task based on audio and vibration data. We consider the quality of our collected dataset to be high if the collected data and labels can train a deep learning model to converge and achieve test high accuracy. We split the entire dataset into training, validation, and testing sets in a 3:1:1 ratio. Afterward, we utilized an oversampling strategy to balance the distribution of samples across different labels for both data types. Training and validation sets underwent a 4-fold cross-validation process. Moreover, we computed the mean and standard deviation based on the training set to normalize all datasets.\nWe adopted a simple CNN model for event classification under the PyTorch framework. As depicted in Figure 6, this model features two convolutional layers, with the first layer having 64 filters and the second 32 filters using a kernel size of 3 and stride of 1. Each convolutional layer is equipped with Rectified Linear Unit (ReLU) activation function and batch normalization, enhancing the model's learning efficiency. They are then followed by an adaptive average pooling layer that reduces dimensionality, preparing the output for the final classification stage. The processed data is fed into a fully connected layer, classifying events.\nWe configured our model training using the Adam optimizer, setting hyperparameters to \\(\beta\u2081 = 0.9\\), \\(\beta2 = 0.98\\), and \\(e = 10^{-9}\\). The training initiated with a learning rate of 0.001, which we dynamically adjusted using a scheduler that modified the rate at a step size of 3 with a decay factor \\(y\\) of 0.5. We opted for cross-entropy error as the loss function to train and evaluate the model's performance. To enhance the robustness of our training process, we conducted 100 experiments, each comprising 50 epochs, and incorporated an early stopping mechanism to mitigate the risk of overfitting. We used accuracy as the primary evaluation metric complemented by a confusion matrix to provide detailed insights into the model's performance across different events.\nFigure 7 illustrates the distribution of test accuracy for audio data across different validation folds. The observed minimum accuracy ranged from 50.00% to 66.67%. Despite these variations, the model demonstrates strong potential, achieving maximum accuracy up to 91.67% in folds 1 and 3, and 87.50% in folds 2 and 4. The median accuracy, spanning from 70.83% to 79.17%, suggests that the model generally maintains high-performance levels. Figure 8 presents the test accuracy for vibration data, which also exhibits variability with minimum accuracy between 54.17% and 58.33%. The model reaches a high accuracy of up to 91.67% in fold 2 and consistently above 87.50% in the other folds. The median accuracy, consistently between 75.00% and 79.17%, indicates a reliable performance.\nTo understand the limitations of our system and identify potential areas for improvement, we conducted a detailed analysis of a trained model with a test accuracy of 87.5% using a confusion matrix. reveals that all samples of water boiling and door opening are correctly classified, although three samples of door closing were misclassified as door opening. Similarly, shows that only three samples of door-opening events were misclassified as door-closing events.\nIt is important to note that although we collected audio and vibration data concurrently, the models were trained separately on each data type. Integrating both audio and vibration data as inputs for the models could potentially enhance accuracy further, particularly in applications requiring high precision. However, investigating this integrated approach is beyond the scope of this paper.\nIn summary, the quality of the collected data and labels has proven sufficient for CNN to learn and differentiate between event classes effectively. While the classification accuracy from vibration data is slightly lower than that from audio data, this outcome was anticipated due to the inherent challenges associated with vibration signal classification. Notably, the consistency of our sensor data has been validated across four-folds, confirming the effectiveness of our system in capturing classifiable features across three distinct event types."}, {"title": "6 Conclusion and Future Work", "content": "Our study successfully developed a robust approach for autonomously labeling sensor data directly on IoT devices. Experiments demonstrated that our models achieved up to 91.67% test accuracy in controlled settings, highlighting the high quality of our sensor data and the reliability of our labeling approach. This method significantly improves the feasibility of collecting and processing large-scale IoT data in diverse field environments, enhancing efficiency and accuracy.\nIn our ongoing efforts to enhance event detection capabilities, we plan to integrate additional types of feature sensors into our system. This expansion will enable the support and recognition of a broader array of event types, further improving the versatility and applicability of our solution in diverse scenarios. By broadening the sensor array, we aim to capture more comprehensive feature data from the device surroundings, significantly refining our system's responsiveness and accuracy in real-world applications."}]}