{"title": "NeSyA: Neurosymbolic Automata", "authors": ["Nikolaos Manginas", "Georgios Paliouras", "Luc De Raedt"], "abstract": "Neurosymbolic Artificial Intelligence (NeSy) has emerged as a promising direction to integrate low level perception with high level reasoning. Unfortunately, little attention has been given to developing NeSy systems tailored to temporal/sequential problems. This entails reasoning symbolically over sequences of subsymbolic observations towards a target prediction. We show that using a probabilistic semantics symbolic automata, which combine the power of automata for temporal structure specification with that of propositional logic, can be used to reason efficiently and differentiably over subsymbolic sequences. The proposed system, which we call NeSyA (Neuro Symbolic Automata), is shown to either scale or perform better than existing NeSy approaches when applied to problems with a temporal component.", "sections": [{"title": "Introduction", "content": "NeSy AI (Garcez and Lamb 2023) aims at integrating neural-based perception with symbolic reasoning. A common approach to integrate the two is compositionally; a neural network first extracts a symbolic interpretation from subsymbolic input and using the neural network predictions the symbolic representation is then used for reasoning in a soft and differentiable fashion.\nWe consider the problem of integrating neural networks with symbolic reasoning, but with a focus on temporal/sequential domains, which are ubiquitous in AI. Specifically, we indentify symbolic automata as an expressive and efficient representation for complex temporal properties. Their use in temporal logics over finite traces (De Giacomo, Vardi et al. 2013) makes them an attractive low level representation. We use the term Neurosymbolic Automata or NeSyA for the integration of symbolic automata in a NeSy context.\nNeSyA is placed in a propositional setting; that is, each element of a subsymbolic sequence can be grounded, i.e. mapped onto a finite predefined set of propositional variables. This grounding corresponds to a multilabel classification task of subsymbolic input, e.g. an image, to its symbolic intepretation."}, {"title": "Preliminaries", "content": "In this section we provide some necessary preliminaries."}, {"title": "Propositional Logic", "content": "We shall use lowercase to denote propositional variables, e.g. blocked. A propositional formula $\\phi$ over a set of variables $P$ is defined as:\n$\\phi ::= P | \\neg \\phi_1 | \\phi_1 \\land \\phi_2$\nThese connectives are sufficient to then define $\\lor, \\rightarrow$, etc. An interpretation $w \\subseteq P$ assigns a truth value to each variable. We use subsets to denote interpretations. For instance for $P$ = {tired, blocked, fast} the interpretation $w$ = {tired, blocked} is shorthand for {tired, blocked, \u00acfast}. If an interpretation $w$ satisfies a formula $\\phi$ we write $w \\models \\phi$ and $w$ is called a model of $\\phi$."}, {"title": "Traces and Temporal Logic", "content": "The semantics of propositional logic are given in terms of interpretations. Traces generalize interpretations for temporal domains. A trace over variables $P$, $\\pi = w_1w_2...w_n$ is a sequence of interpretations, with $w_i \\subseteq P$. We use $\\pi_t$ to denote the $t^{th}$ interpretation, $w_t$. Traces can be used to give a semantics for temporal logics. In the context of AI, one of the most popular such logics is LTLf (De Giacomo, Vardi et al. 2013). We introduce LTLf here, and use it in the following sections to define temporal properties in a human readable manner. A formula in LTLf over variables $P$ is syntactically:\n$\\phi ::= P | \\neg \\phi_1 | \\phi_1 \\land \\phi_2 | X \\phi_1 | \\phi_1 U \\phi_2$\nwith X being next, and U until. An LTLf formula $\\phi$ models a trace $\\pi$ at time step $t$, written $\\pi, t \\models \\phi$ given by:\n$\\pi, t \\models p$ for $p \\in P$ iff $p \\in w_t$\n$\\pi, t \\models \\neg \\phi$ iff $\\pi, t \\nvDash \\phi$\n$\\pi, t \\models \\phi_1 \\land \\phi_2$ iff $\\pi, t \\models \\phi_1$ and $\\pi, t \\models \\phi_2$\n$\\pi, t \\models X \\phi$ iff $\\pi, t+1 \\models \\phi$ and $t < |\\pi|$\n$\\pi, t \\models \\phi_1 U \\phi_2$ iff for some $t' \\in \\{1,...,|\\pi|\\} : \\pi,t' \\models \\phi_2$ and for all $t'' \\in \\{1,...,t'\\} : \\pi,t'' \\models \\phi_1$\nWe then have $\\pi \\models \\phi$ iff $\\pi, 1 \\models \\phi$. From the above temporal operators one can construct additional ones, including F (read as eventually), which means that at some point in the trace $\\phi$ is true, G (read as always) which means that $\\phi$ is true in every timestep in the trace, and WX $\\phi$ (read as weak next) which means that $\\phi$ must be true in the next timestep if the next timestep exists. We give some examples of LTLf formulae:\n*   G ((blocked $\\lor$ tired) $\\rightarrow$ WX (\u00acfast)). Always, if the driver is tired or the road is blocked they should not go fast in the next timestep.\n*   WX ((blocked $\\lor$ tired) U (fast $\\lor$ gesture\\_ok)). In the (weak) next timestep, the driver will be tired or the road blocked, until the car goes fast or an OK gesture is observed."}, {"title": "(Symbolic) Automata", "content": "A deterministic finite automaton $A$ is defined as a quintuple $A = (\\Sigma, Q, q_0, \\delta, F)$ with $\\Sigma$ a finite set of symbols (the alphabet), $Q$ a finite set of states, $q_0 \\in Q$ an initial state, $\\delta : Q \\times \\Sigma \\rightarrow Q$ the transition function and $F \\subseteq Q$ a set of accepting states. An automaton reads a sequence of characters from the alphabet and transitions between states, until the complete input string has been consumed. If it ends up in an accepting state the string is accepted, else it is rejected.\nTemporal logics combine the power of propositional logic to model complex domains with additional operators that define temporal properties. Similarly, symbolic automata (s-FAs) extend classical finite automata with the ability to define logical transitions, thus themselves integrating the power of automata for specifying complex temporal properties with that of propositional logic. Instead of reading a"}, {"title": "Non-stationary Markov Chains", "content": "We follow Chapter 23.1 from (Barber 2012) for our overview on Markov Chains. Consider a set of states $Q$. A first-order Markov Chain (MC), models the joint distribution $Pr(S_{1:T})$. The domain of of each variable $s_i$ is $Q$. For a finite horizon $T$ the joint is factorized as follows:\n$Pr(s_{1:T}) = \\prod_{t=1}^{T} Pr(s_t|s_{t-1})$\nWe assume the initial state of the MC to be fixed, i.e. the chain deterministically starts from an initial state $q_0 \\in Q$. The parameters of a MC can be specified by a transition matrix $\\Delta$ of dimension $|Q| \\times |Q|$, with $\\Delta[q, q']$ specifying the probability that the chain will transition to state $q'$ given that it was in state $q$ in the previous timestep. In a non-stationary Markov Chain (NSMC) this transition matrix $\\Delta$, is not constant through time and is therefore also parametrized by $t$. The joint is then given as:\n$Pr(s_{1:T}) = \\prod_{t=1}^{T} \\Delta(t)[s_{t-1}, s_t]$\nLet $state_t$ be a row vector with $|Q|$ elements where $state_t[q]$ represents the probability that the chain is in state $q$ at timestep $t$. We can recursively compute:\n$state_t = state_{t-1} \\times \\Delta(o_t)$, for $t \\in \\{1,...,T\\}$\nwith $state_0[q_0] = 1$ and $state_0[q \\neq q_0] = 0$."}, {"title": "Weighted Model Counting", "content": "One of the most widely-used approaches to probabilistic logical inference is through Weighted Model Counting (WMC). We briefly review WMC for formulae in propositional logic. WMC is given in terms of weights, altough"}, {"title": "NeSyA: Neurosymbolic Automata", "content": "We now introduce NeSyA as a neurosymbolic extension of the s-FAs introduced earlier. Rather than assuming a trace of propositional interpretations $w_1...w_n$ we assume a sequence of subsymbolic observations $o = o_1o_2 . . . o_T$ with $o_i \\in R^n$ and we will obtain a neurosymbolic extension by reparameterizing the matrix $\\Delta(t)$ of a NSMC using the observations.\nMore specifically, a neurosymbolic probabilistic automaton (NeSyA) consists of a deterministic s-FA $A = (P, Q, q_0, \\delta, F)$ and a neural network $g_{\\theta}: R^n \\rightarrow [0,1]^{|P|}$ with parameters $\\theta$.\nWe let the transition matrix of NeSyA depend on the current observation $o_t$. More specifically, we define the matrix $\\Delta(o_t)$ of dimension $Q \\times Q$, where $\\Delta(o_t)[q, q'] = WMC(\\phi_{qq'}, g_{\\theta}(o_t))$. For each observation, $g_{\\theta}(o_t)$ gives the probability of each symbol in $P$ being true given $o_t$.\nbe the probability that the sequence of subsymbolic observations $o$ of length $T$, is accepted by the s-FA $A$, given the"}, {"title": "Semantics", "content": "For a MC over states $Q$ the quantity $state_t[q]$ is known to be the sum of the probabilities of all runs of the chain which start from the state $q_0$ and end in state $q$. Consider the accepting state $q_1$ in Figure 1 and let the current sequence have length 2. There are two runs of the s-FA that end in state $q_1$ and have length 2. We denote these as $r_1 = q_0q_0q_1$ and $r_2 = q_0q_1q_1$. For $r_1$ the transition $q_0 \\rightarrow q_0$ was true in the first timestep and the transition $q_0 \\rightarrow q_1$ was true in the second timestep. The probability of $r_1$ in our running example is:\n$Pr(r_1|g_{\\theta}, o) = Pr(\\phi_{q_0q_0}\\hspace{0.2cm}\\cap \\hspace{0.2cm}\\phi_{q_0 \\rightarrow q_1}\\mid g_{\\theta}, o)$In turn each of these probabilities is given by WMC and we obtain:\n$\\begin{aligned}\nPr(r_1|g_{\\theta}, o) &= \\sum_{\\omega \\models \\phi_{q_0 \\rightarrow q_0}} \\beta(\\omega, g_{\\theta}) \\\n&\\times  \\sum_{\\omega \\models \\phi_{q_0 \\rightarrow q_1}} \\beta(\\omega, g_{\\theta})\n\\end{aligned}$\nWe thus have $P_1 = g_{\\theta}()$ and $P_2 = g_{\\theta}(\u30cd)$. The formula $\\phi_{q_0q_0}$ in Figure 1 has two models $w_1 = \\{\\},w_2 = \\{fast\\}$ and the formula $\\phi_{q_0q_1}$ has six models $w_3 = \\{tired\\}, w_4 = \\{blocked\\}, w_5 = \\{blocked, tired\\}, w_6 = \\{tired, fast\\}$.\n\\\\\\\\\nP(r_1|g_{\\theta}, o) = (\\beta(w_1, P_1) + \\beta(w_2, P_1))\\\n& \\times (\\beta(w_3, P_2) + \\beta(w_4, P_2) + \\beta(w_5, P_2) + ...)\n\\\\\\\\\n= \\beta(w_1, P_1)\\beta(w_3, P_2) + \\beta(w_1, P_1)\\beta(w_4, P_2) + ...\\hspace{0.2cm} (1)\nThe definition of the probability of a trace of length $T$, given the current neural network parameters and the sequence of observations $o$ is:\n$Pr(\\pi|g_{\\theta}, o) = \\prod_{t=1}^{T} \\beta(\\pi_t, g_{\\theta}(o_t)) \\hspace{0.2cm} (2)$\nThe probability of a sequence of interpretations (the trace $\\pi$), is the product of the probability of each interpretation $(\\pi_t)$ in the sequence. Recall, that each trace will follow a run $r$ in the s-FA. Multiple traces can follow the same run. Let $traces(r)$ be the set of traces which follow run $r$ in the s-FA. In our example these would be $traces(r_1) = \\{\\pi_1 = w_1w_3, \\pi_2 = w_1w_4, \\}$. Based on Equations (1) and (2) we can now define the probability of a run $r$ as:\n$Pr(r|g_{\\theta}, o) = \\sum_{\\pi \\in traces(r)} \\prod_{t=1}^{T} \\beta(\\pi_t, g_{\\theta}(o_t)) = \\sum_{\\pi \\in traces(r)} Pr(\\pi| g_{\\theta}, o)$"}, {"title": "Discussion", "content": "We now discuss our experimental results in terms of the research questions posed. Our first experiment shows that NeSyA outperforms the fuzzy integration presented in (Umili, Capobianco, and De Giacomo 2023) both in terms of accuracy and scalability answering Q1. Further, the systems DeepStochlog and DeepProblog were shown to lag behind NeSyA in terms of scalability answering Q2. Our second experiment shows that encoding temporal knowledge via NeSyA is usefull compared to pure neural approaches which cannot utilize knowledge, thereby answering Q3."}, {"title": "Related Work", "content": "NeSy AI has received significant attention lately. Our work is related conceptually to the Semantic Loss (Xu et al. 2018) and DeepProblog (Manhaeve et al. 2018). Both systems are focused on integrating neural networks with symbolic knowledge, and are based on possible world semantics, but are not tailored to temporal domains. The more recent Deep-Stochlog system (Winters et al. 2022), which is based on probabilistic grammars is also related. However, as shown in our experiments it is still lacking in terms of scalability compared to NeSyA.\nSome work also exists in integrating automata and subsymbolic inputs. The system most closely related to us is"}, {"title": "Conclusion", "content": "We presented the NeSyA system for integrating neural networks with symbolic automata under clean probabilistic semantics. Due to the reduction of temporal logics on finite traces, such as LTLf, to symbolic automata, the proposed approach also carries over to integrating neural networks with temporal logic specifications. We showed that current systems, such as DeepStochlog, DeepProblog and the system of (Umili, Capobianco, and De Giacomo 2023) struggle to achieve scalable and accurate solutions in temporal domains. The NeSyA system was instead shown to scale considerably better and achieve higher accuracies."}, {"title": "Experiment 1: Synthetic Driving", "content": "We first benchmark NeSy systems on a simple synthetic task. We use the domain introduced as a running example, in which a sequence of images must be classified according to a temporal logic pattern. Each image represents a set of binary symbols. In the example from Figure 1 the symbols were tired, blocked, fast}, however we also test for larger sets of symbols, up to six. Their truth value is represented via two emojis, one representing the value true and one false. Random Gaussian noise is added to each image and the mapping between an image and its symbolic representation is therefore not one to one. We generate 100 random trajectories which satisfy each temporal logic pattern (positive) and 100 negative. We use the same setup for generating a training and a testing set.\nThe learning task is then multilabel image classification, in which each image in a sequence must be mapped to n binary variables, where n is the number of symbols over which the temporal logic pattern is defined. We use a CNN as the neural component for all systems. No direct supervision in the form of labels for each image is provided. Rather, systems must utilize their knowledge of the temporal logic pattern and map images to symbols, such that the pattern is satisfied by the neural network outputs for positive trajectories and not satisfied for negative trajectories. Supervision is then provided to the network weakly, after reasoning is performed on top of its predictions for each image.\nWe benchmark against DeepStochlog (Winters et al. 2022) and DeepProblog (Manhaeve et al. 2018) in terms of scalability and with (Umili, Capobianco, and De Giacomo 2023) both in terms of scalability and accuracy. \n$\\phi_1 = G (t \\lor b) \\rightarrow WX (\\neg f)$\n$\\phi_2 = G (\\neg f \\lor \\neg g) \\rightarrow WX ((b \\lor t) U (f \\lor g))$\n$\\phi_3 = G ((l \\lor n\\lor b\\lor t) \\rightarrow WX (\\neg f)) \\land G ((f\\land WX (f)) \\rightarrow WX (WX (l)))$"}, {"title": "Experiment 2: Grid Navigation", "content": "In our second experiment we aim to show the sample efficiency and generalization capabilities, that temporal symbolic knowledge has to offer compared to pure neural approaches. We design a simple grid environment. An agent starts from the top left cell and has the actions {left, right, forward, pickup, open_door} available to them. They must navigate through the environment, while avoiding a moving obstacle, pick-up a key and subsequently go to the door and open it. The environment terminates if the agent collides with the obstacle more than once, or succeeds in opening the door. The termination condition is not Markovian. Observing the current state and action is insufficient for determining whether the environment will terminate, as that depends on past information, i.e. whether the agent has previously picked up the key or whether it has previously collided with an obstacle.\nWe generate a dataset of trajectories of the agent navigating the environment. Each step of the trajectory is the current state of the environment (an image) and the action the agent took. Although we used a full render of the environment in Figure 4 each state in our dataset is represented via an image that only includes the position in front of the agent, i.e. the grid cell where the agent is currently facing. The goal is to create a model, which given a sequence of state-action pairs must predict for each timestep in the trajectory whether the environment will terminate in the next timestep. The LTL f formula defining the termination of the environment is:\n$\\phi = F ((\\hspace{0.1cm} (obstacle\\_forward \\land forward)\\\\   \\hspace{0.1cm}AXF (obstacle\\_forward \\land forward) \\hspace{0.1cm}  )\\\\    V\\\\    ((\\hspace{0.1cm}(key\\_forward \\land pickup)  \\\\  \\hspace{0.1cm} F (door\\_forward \\land open\\_door)\\\\   ))$which can be read as at some point the agent will collide with the obstacle and then will collide again at some point later on, or the agent will successfully pick up the key and then eventually navigate to the door and open it. The neural component of NeSyA must map each image to the symbols\n$P = \\{obstacle\\_forward, key\\_forward, door\\_forward\\}$which are mutually exclusive, or the symbol none if none of the above are true. As a baseline, we use a CNN-LSTM stack in which a CNN first maps each image to a dense embedding, onto which the action of the agent for that timestep is concatenated as a one hot vector. The resultant vector for each timestep is then fed to an LSTM which makes a prediction about whether the environment will terminate. Both systems utilize the same CNN backbone. The input-output behaviour of both systems is thus identical. Given a sequence of state-action pairs, both systems predict whether the environment will terminate for each element in the sequence.\nWe generate a dataset from random environment runs, i.e. the action of the agent is sampled from a random uniform distribution. Half of the sequences generated are positive, i.e. the agent achieves their goal, and half of them are negative, i.e. end because of a collision. Generating positive trajectories is extremely time consuming for larger grid sizes, since the probability of a random action trajectory achieving the goal becomes extremely small, and we therefore only experiment on a grid size of 6. The test set is generated in the same fashion. We limit the sequence length to be between 5 and 30. NeSyA outperforms the baseline in low-resource settings, generalizing better to unseen trajectories from fewer examples."}]}