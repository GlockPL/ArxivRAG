{"title": "RECIPROCAL LEARNING", "authors": ["JULIAN RODEMANN", "CHRISTOPH JANSEN", "GEORG SCHOLLMEYER"], "abstract": "We demonstrate that a wide array of machine learning algorithms are specific instances of one single paradigm: reciprocal learning. These instances range from active learning over multi-armed bandits to self-training. We show that all these algorithms do not only learn parameters from data but also vice versa: They iteratively alter training data in a way that depends on the current model fit. We introduce reciprocal learning as a generalization of these algorithms using the language of decision theory. This allows us to study under what conditions they converge. The key is to guarantee that reciprocal learning contracts such that the Banach fixed-point theorem applies. In this way, we find that reciprocal learning algorithms converge at linear rates to an approximately optimal model under relatively mild assumptions on the loss function, if their predictions are probabilistic and the sample adaption is both non-greedy and either randomized or regularized. We interpret these findings and provide corollaries that relate them to specific active learning, self-training, and bandit algorithms.", "sections": [{"title": "1. INTRODUCTION", "content": "The era of data abundance is drawing to a close. While GPT-3 [9] still had to make do with 300 billion tokens, Llama 3 [111] was trained on 15 trillion. With the stock of high-quality data growing at a much smaller rate [72], adequate training data is likely to run out within this decade [62, 116]. Generally, and beyond language models, machine learning is threatened by degrading data quality and quantity [64]. Apparently, learning ever more parameters from ever more data is not the exclusive route to success. Models also have to learn from which data to learn. This has sparked a lot of interest in sample/data efficiency [75, 100, 120, 8, 50, 26, 114], subsampling [51, 110, 76], coresets [67, 81, 94], data subset selection [59, 122, 13, 87], and data pruning [30, 127, 61, 5] in recent years and months.\nInstead of proposing yet another method along these lines, we demonstrate that a broad spectrum of well-established machine learning algorithms already exhibit a reciprocal relationship between data and parameters. That is, parameters are not only learned from data but also vice versa: Data is iteratively chosen based on currently optimal parameters, aiming to increase sample efficiency."}, {"title": "2. RECIPROCAL LEARNING", "content": "Machine learning deals with two pivotal objects: data and parameters. Typically, parameters are learned from data through ERM. We argue, however, that in various branches of machine learning, the relationship between data and parameters is in fact reciprocal. That is, parameters are not only learned based on data but also data are added or removed based on parameters. In what follows, we show that this corresponds to two interdependent decision problems and explicitly study how learned parameters affect the subsequent training data. We emphasize that our analysis will focus on reciprocity between parameters and training data only. The population and test data thereof are assumed to be fixed. Specifically, we call a machine learning algorithm reciprocal if it performs iterative ERM on training data that depends on the previous ERM, see definition 1. This latter dependence can have various facets, see sections 3 and 5 for concrete examples. Broadly speaking, it can be induced by any kind of data collection, removal, or generation that is affected by the model fit. In particular, it can be stochastic (think of Thompson-sampling in multi-armed bandits) as well as deterministic in nature (think of maximizing a confidence measure in self-training)."}, {"title": "Definition 1 (Reciprocal Learning, informal)", "content": "An algorithm that iteratively outputs $\\theta_t = \\arg \\min_\\theta E_{(Y,X) \\sim P_t} l(Y,X,\\theta)$ shall be called reciprocal learning algorithm if\n$$P_t = f(\\theta_{t-1}, P_{t-1}, n_{t-1}),$$\nwhere $P_t \\in \\mathcal{P}$ are empirical distributions \u2013 from a space of probability distributions $\\mathcal{P}$ \u2013 of $Y, X$ of size $n_t$ in iteration $t \\in \\{1,\\dots,T\\}$. As convention dictates, $l(Y, X, \\theta) = l(Y, p(X, \\theta))$ denotes a loss function with $p(X,\\theta)$ a prediction function that maps to the image of $Y$. Further denote by $Y, X$ random variables describing the training data, and $\\theta_t \\in \\Theta$ \u03b1 parameter vector of the model in $t$.\nIn principle, the above definition needs no restriction on the nestedness between data in $t$ and $t - 1$. In practice, however, most algorithms iteratively either only add training data or both add and remove instances, see examples in section 3. That is, data in $t$ is either a superset of data in $t - 1$ or a distinct set. We will address these two cases in the remainder of the paper, referring to the former as greedy (only adding data) and to the latter as non-greedy (adding and removing data). For classification problems, i.e., discrete image of $Y$, we typically have $p(X,\\theta) = \\sigma(g(X,\\theta))$ with $\\sigma : \\mathbb{R} \\rightarrow [0,1]$ a sigmoid function and $g$ a function; for regression problems, we simply have $p : X \\times \\Theta \\rightarrow \\mathbb{R}$. The notation $P_t = f(\\theta_{t-1}, P_{t-1}, n_{t-1})$ shall be understood as a mere indication of the distribution's dependence on ERM in the previous iteration. We will be more specific soon.\nOn a high level, reciprocal learning can be viewed as sequential decision-making. First, a parameter $\\theta_1$ is fitted through ERM, which corresponds to solving a decision problem characterized by the triplet $(\\Theta, \\mathcal{A}, L)$ with $\\Theta$ the unknown set of states of nature, the action space $\\mathcal{A} = \\Theta$ of potential parameter fits (estimates), and a loss function $L : \\mathcal{A} \\times \\Theta \\rightarrow \\mathbb{R}$, analogous to classical statistical decision theory [7]. Second, features $x_t \\in X$ are chosen and data points $(x_t, y_t)$ are added to or removed from the training data inducing a new empirical distribution $P_{t+1}$, where $y_t$ is predicted (self-training), queried (active learning) or observed (bandits). These features $x_t$ are found by solving another decision problem $(\\Theta, \\mathcal{A}, L_{\\theta_1})$, where crucially - the loss function $L_{\\theta_1}$ depends on the previous decision problem's solution $\\theta_1$. This time, the action space corresponds to the feature space $X$."}, {"title": "Illustration 1", "content": "Think of reciprocal learning as a sequential decision-making problem\u00b9:\nt = 1: $\\theta_1$ solves decision problem $(\\Theta, \\Theta, L)$\n$a_1$ solves decision problem $(\\Theta, \\mathcal{A}, L_{\\theta_1})$\nt = 2: $\\theta_2$ solves decision problem $(\\Theta, \\Theta, L_{a_1(\\theta_1)})$"}, {"title": "Definition 2 (Data Selection)", "content": "Let $c : X \\times \\Theta \\rightarrow \\mathbb{R}$ be a criterion for the decision problem $(\\Theta, \\mathcal{A}, L_{\\theta_1})$ with bounded $\\mathcal{A} = X$ of selecting features to be added to the sample in iteration $t$. Define\n$$\\tilde{c} : X \\times \\Theta \\rightarrow [0,1]; (x, \\theta_t) \\mapsto \\frac{\\exp(c(x, \\theta_t))}{\\int_X \\exp(c(x', \\theta_t)) d\\mu(x)}$$ as standardized version thereof with $\\mu$ the Lebesgue measure on $X$. For a model $\\Theta_t$ in iteration $t$, it assigns to each feature vector $x$ a value between 0 and 1 that can be used as drawing probabilities. Drawing $x \\in X$ according to $\\tilde{c}(x,\\theta_t)$ shall be called stochastic data selection $x_s(\\theta_t)$.\u00b2 The function $x_d : \\Theta \\rightarrow X; \\theta_t \\mapsto \\arg \\max_{x \\in X} c(x, \\theta_t)$ shall be called deterministic data selection function.\nThe data selection function can be understood as the workhorse of reciprocal learning: It describes the non-trivial part of the sample adaption function $f$, see definition 1. For any model $\\theta$, in $t$, a data selection function chooses a feature vector to be added to the training data in $t + 1$, based on a criterion $c$. This happens either stochastically through $x_s$ by drawing from $X$ according to $\\tilde{c}$ or deterministically through $x_d$. Examples for $c$ comprise confidence measures in self-training, acquisition functions in active learning, or policies in multi-armed bandits. For an example of stochastic data selection, consider $c$ to be the classical Bayes criterion [7] in $(\\Theta, \\mathcal{A}, L_{\\theta_1})$. In this case, drawing from $X$ as prescribed by $x_s$ corresponds to well-known Thompson sampling [12, 98]. As already hinted at, we will need some regularization (definition 3) of the data selection. Intuitively, the regularization term smoothes out the criterion $c(x, \\theta)$. In other words, the higher the constant $K$, the less the selection of data is affected by small changes of $\\theta$ for given $R(\\cdot)$. This is completely symmetrical to standard parameter regularization in ERM, see figure 2."}, {"title": "Definition 3 (Data Regularization)", "content": "Consider $c : X \\times \\Theta \\rightarrow \\mathbb{R}$ a criterion for the decision problem $(\\Theta, \\mathcal{A}, L_{\\theta_t})$ with $\\tilde{c}$ as in definition 2. Define the following regularized (deterministic) data selection function:\n$$x_{d,R} : \\Theta \\rightarrow X; \\theta \\mapsto \\arg \\max_{x \\in X} \\{c(x,\\theta) + \\frac{1}{K} R(x)\\},$$\nwhere $R(\\cdot)$ is a $K$-strongly convex regularizer. In complete analogy to definition 2, we can define a stochastic regularized data selection function as $x_{s,R}(\\theta)$ by drawing $x \\in X$ according to a normalized version of $c(x, \\theta) + \\frac{1}{K}R(x)$.\nWe will denote a generic data selection function as $\\varpi \\in \\{x_d, x_s, x_{d,R}, x_{s,R}\\}$ in what follows. For the non-greedy variant of reciprocal learning, where data is both added and removed, we need to define data removal as well. A straightforward strategy is to randomly remove data points with uniform removal probabilities. The following function $\\check{P}$ describes the effect of this procedure in expectation."}, {"title": "Definition 4 (Data Removal Function)", "content": "Given an empirical distribution $P(Y, X)$ of a sample, the function\n$$\\check{P} : \\mathcal{P} \\rightarrow \\mathcal{X}; P(Y, X) \\mapsto \\int_\\mathcal{X} dP(Y, X)$$\nshall be called data removal function.\nIn order to study reciprocal learning in a meaningful way, we need to be a bit more specific about how $P_t$ depends on empirical risk minimization in $t - 1$, and specifically on $\\theta_{t-1}$. The following definition 5 of the sample adaption function allows for this. It will be the pivotal object in this work. The function describes in a general way and for any $t$ how empirical distributions of training data in $t$ are affected by the model, the empirical distribution of training data, and its size in $t - 1$, respectively."}, {"title": "Definition 5 (Sample Adaption)", "content": "Denote by $\\Theta$ a parameter space, by $\\mathcal{P}$ a space of probability distributions of $X$ and $Y$, and $\\mathbb{N}$ the natural numbers. The function $f : \\Theta \\times \\mathcal{P} \\times \\mathbb{N} \\rightarrow \\mathcal{P}$ shall be called the greedy and the function $f_n : \\Theta \\times \\mathcal{P} \\rightarrow \\mathcal{P}$ the non-greedy sample adaption function.\nA sample adaption function outputs a distribution $P'(Y,X) \\in \\mathcal{P}$ in the iteration after $\\theta \\in \\Theta$ solved ERM on a sample of size $n \\in \\mathbb{N}$ described by $P(Y, X) \\in \\mathcal{P}$, which led to an enhancement of the training data that changed $P(Y, X)$ to $P'(Y, X)$. It will come in different flavors for different types of algorithms, see examples in section 3. Generally, we have $f(\\theta,P(Y, X), n) = P'(Y, X)$, with $P'(Y, X)$ being induced by\n$$P'(Y = 1, X = x) = \\int \\int \\frac{1(x = \\varpi(\\theta)) \\cdot \\chi(\\varpi(\\theta), \\theta) + nP(Y = 1, X = x)}{n+1} P_{Y|x} dy P_X dx, \\hspace{0.2cm} (1)$$\nin case of $Y = \\{0,1\\}$, where $\\chi : X \\times \\Theta \\rightarrow \\{0,1\\}$ is any function that assigns a label $y$, potentially based on the model $\\theta$, to selected $x$, and $\\varpi$ any function that selects features $x$ given a model $\\theta$, for example, $x_d, x_s, x_{d,R}, or x_{s,R}$ as defined above. They give rise to $P_{Y|x}$ and $P_X$, respectively. We can be so specific about the sample adaption function due to $P(Y = 1, X = x) = P(X = x) - P(Y = 0, X = x)$ in binary classification problems. We can analogously define the non-greedy variant $f_n(\\theta, P(Y, X))$, where one instance is removed by $\\check{P}$ and one instance is added by $\\varpi$ per iteration. To this end, define $P'(Y = 1, X = x)$ by replacing the integrand in equation (1) by\n$$ \\frac{1(x = \\varpi(\\theta)) \\cdot \\chi(\\varpi(\\theta), \\theta) + n_0 P(Y = 1, X = x) - 1(x = \\varpi(\\theta)) \\check{P}(Y, X)((Y, X), \\theta)}{n_0}, \\hspace{0.2cm} (2)$$\nwhere $n_0$ is the size of the initial training data set. Notably, we observe that both sample adaption functions entail a reflexive effect of the model on subsequent data akin to performative prediction [28], see section 6 for a discussion. We can now define reciprocal learning (definition 1) more formally given the sample adaption function as follows, both in greedy and non-greedy flavors."}, {"title": "Definition 6 (Greedy Reciprocal Learning)", "content": "With $\\Theta, \\mathcal{P}, X, Y,$ and $\\mathbb{N}$ as above, we define\n$$R:\\begin{cases}\\Theta \\times \\mathcal{P} \\times \\mathbb{N} & \\rightarrow \\Theta \\times \\mathcal{P} \\times \\mathbb{N} \\\\(\\theta,P(Y,X), n) & \\mapsto (\\theta', P'(Y, X), n') \\end{cases}$$\nas reciprocal learning, where $\\theta' = \\arg \\min_\\theta E_{(Y,X) \\sim P'(Y,X)} l (Y, X, \\theta)$ and $P'(Y, X) = f(\\theta, P(Y, X), n)$ as well as $n' = n +1$, with $f$ a sample adaption function, see definition 5. Note the equivalence to the informal recursive definition 1 with $f (\\theta_{t-1}, P(Y, X)_{t-1}, n_{t-1}) = P(Y, X)_t$."}, {"title": "Definition 7 (Non-Greedy Reciprocal Learning)", "content": "With $\\Theta, \\mathcal{P}, X,$ and $Y$ as above, we define\n$$R_n:\\begin{cases}\\Theta \\times \\mathcal{P} & \\rightarrow \\Theta \\times \\mathcal{P}; \\\\(\\theta, P(Y, X)) & \\mapsto (\\theta', P'(Y, X)) \\end{cases}$$\nas reciprocal learning, where $P'(Y, X) = f(\\theta, P(Y, X))$ and $\\theta' = \\arg \\min_\\theta E_{(Y,X) \\sim P'(Y,X)} l (Y, X, \\theta)$ with $f_n$ a non-greedy sample adaption function, see definition 5."}, {"title": "Definition 8 (Convergence of Reciprocal Learning)", "content": "Let $g : \\mathbb{N} \\rightarrow \\mathbb{R}$ be a strictly monotone decreasing function and $R$ ($R_n$) any (non-greedy) reciprocal learning algorithm (definitions 6 and 7) outputting $R_t$ ($R_{n,t}$) in iteration $t$. Then $\\rho \\in \\{R,R_n\\}$ is said to converge to $\\Theta_c$ if\n$$||\\theta_k, \\theta_j|| \\leq g(t) \\text{ for all } k, j \\geq t,$$\nand $\\lim_{t\\rightarrow\\infty}g(t) = 0$, where $||\\cdot ||$ is a norm on the codomains of $R$ and $R_n$, respectively.\nContrary to classical ERM, convergence of reciprocal learning implies stability of both data and parameters. Technically, it refers to all components of the functions $R$ and $R_n$, respectively, see definition 8. It guarantees that $\\theta_{t-1}$ solves ERM on the sample induced by it in $t$. However, this does not say much about its optimality in general. What if the algorithm had outputted a different $\\theta_{t-1}$ in the first place? The empirical risk could have been lower on the sample in $t$ induced by it. The following definition describes such a look-ahead optimality. It can be interpreted as the optimal data-parameter combination in reciprocal learning and entails the globally optimal ERM solution."}, {"title": "Definition 9 (Optimal Data-Parameter Combination)", "content": "Consider (non-greedy) reciprocal learning $R$ ($R_n$), see definitions 6 and 7. Define $R^*$ and $R_n^*$ as optimal data-parameter combination in reciprocal learning if\n$$R^* = (\\theta^*, P^*) = \\arg \\min_{\\theta, P} E_{(Y,X) \\sim f_n(\\theta,P)} l(Y, X, \\theta),$$\nand\n$$R_n^* = (\\theta^*, P^*, n^*) = \\arg \\min_{\\theta, P, n} E_{(y,x) \\sim f(\\theta,P,n)} l(Y, X, \\theta),$$\nrespectively.\nAn optimal $\\theta^*$ (or $\\theta_n^*$, analogously) not only solves ERM on the sample it induces, but is also the best ERM-solution among all possible $\\theta$ ($On$) that could have led to optimality on the respectively induced sample. In other words, $\\theta^*$ ($\\theta_n^*$) is found by minimizing the empirical risk with respect to whole $R$ ($R_n$). That is, it is found by minimizing the empirical risk with respect to a given a sample (characterized by $P$ and $n$) and steering this very sample through $\\theta$ simultaneously given only the initial sample. Technically, optimality (definition 9) is an $\\arg \\min$-condition on $\\Theta^2 \\times \\mathcal{P}^2 \\times \\mathbb{N}^2 \\rightarrow \\Theta \\times \\mathcal{P}\\times \\mathbb{N}$ in case of $R$ or $\\Theta^2 \\times \\mathcal{P}^2 \\rightarrow \\Theta \\times \\mathcal{P}$ in case of $R_n$. In contrast, convergence (definition 8) is a condition on $\\Theta \\times \\mathcal{P} \\times \\mathbb{N} \\rightarrow \\Theta \\times \\mathcal{P} \\times \\mathbb{N}$ or on $\\Theta \\times \\mathcal{P} \\rightarrow \\Theta \\times \\mathcal{P}$, respectively, see section 4."}, {"title": "3. FAMILIAR EXAMPLES OF RECIPROCAL LEARNING", "content": "We will demonstrate that well-established machine learning procedures are special cases of reciprocal learning. We start by illustrating reciprocal learning by self-training in semi-supervised learning (SSL) and then turn to active learning and multi-armed bandits."}, {"title": "3.1. Self-Training", "content": "For ease of exposition, we will start by focusing on binary target variables, i.e., the image of Y is {0,1}, with real-valued features X. Moreover, we will only consider cases where the sample changes through the addition of one instance per iteration.\u00b3 Leaning on [115, 12, 112], we describe SSL as follows. Consider labeled data\n$$\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n \\in (\\mathcal{X} \\times \\mathcal{Y})^n\\hspace{0.5cm} (3)$$\nand unlabeled data $x \\in X$. The aim of SSL is to learn a predictive classification function $\\hat{y}(x, \\theta)$ parameterized by $\\theta$ utilizing both labeled and unlabeled data. According to [80] and [115], SSL can be broadly categorized into self-training and co-training. We will focus on the former. Self-training involves fitting a model on $\\mathcal{D}$ by ERM and then exploiting this model to predict labels for $X$. In a second step, some instances $\\{x_i\\}_{i=n+1}^m \\in X$ are selected to be added to the training data together with the predicted label, typically the ones with the highest confidence according to some criterion, see [2, 53, 85, 92, 57, 18] for examples."}, {"title": "Example 1 (Self-Training)", "content": "Self-training is an instance of reciprocal learning with the sample adaption function (see definition 5) $f_{SSL} : \\Theta \\times \\mathcal{P} \\times \\mathbb{N} \\rightarrow \\mathcal{P}; (\\theta,P(Y, X),n) \\mapsto P'(Y, X)$ with $P'(Y, X)$ induced by\n$$P'(Y = 1, X = x) = \\int \\int \\frac{1(x = x_d(\\theta)) \\cdot \\hat{y}(x_d(\\theta),\\theta) + nP(Y = 1, X = x)}{n+1} P_{Y|x} dy P_X dx$$\nwhere $x_d(\\theta)$ (definition 2) selects data with highest confidence score, see [2, 85, 57], according to the model $\\theta$, and gives rise to $P_X$. The prediction function $\\hat{y} : X \\times \\Theta \\rightarrow \\{0, 1\\}$ returns the predicted label of the selected $x_d(\\theta)$ based on the learned model $\\theta$ and gives rise to $P_{Y|X}$.\nThe averaging with respect to $P_X$ and $P_{Y|X}$ accounts for the fact that we allow stochastic inclusion of $X$ in the sample through randomized actions and for probabilistic predictions of $Y | X$, respectively. For now, however, it suffices to think of the special case of degenerate distributions $P_X$ and $P_{Y|X}$ putting point mass 1 on data with hard labels in the sample and 0 elsewhere. Through averaging with respect to $P_{Y|X}$ we can describe the joint distribution of hard labels $(y_1,x_1),...,(y_n,x_n)$ and predicted soft labels $\\tilde{y} = p(Y = 1 | x,\\theta) \\in [0,1]$ of $(y_{n+1},x_{n+1}), ..., (\\tilde{y}_{n+t}, x_{n+t})$. Summing up, both deterministic data selection and non-probabilistic (i.e., hard labels) predictions are well-defined special cases of the above with $P_{Y|X}$ and $P_X$ collapsing to trivial Dirac measures, respectively."}, {"title": "3.2. Active learning", "content": "Active learning is a machine learning paradigm where the learning algorithm iteratively asks an oracle to provide true labels for training data [54, 16, 101]. The goal is to improve the sample efficiency of the learning process by asking queries that are expected to provide the most information. Let X be the input space and Y the set of possible labels. Consider training data\n$$\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n \\in (\\mathcal{X} \\times \\mathcal{Y})^n\\hspace{0.5cm} (4)$$\nas above. The active learning cycle is as follows. First, train a model on the currently labeled dataset $\\mathcal{D}$. Next, select the most informative sample $x^* \\in X$ based on an acquisition function (criterion) such as uncertainty, representativeness, or expected model change and obtain the label $y^*$ for the selected instance $x^*$ from an oracle (e.g., human expert). Finally, update the training data $\\mathcal{D} \\leftarrow \\mathcal{DU} \\{(x^*, y^*)\\}$ and refit the model. This cycle is repeated until a stopping criterion is met (e.g., performance threshold)."}, {"title": "Example 2 (Active Learning)", "content": "Active Learning is an instance of reciprocal learning with the following sample adaption function (see definition 5) $f_{AL} : \\Theta \\times \\mathcal{D} \\times \\mathbb{N} \\rightarrow \\mathcal{P}; (\\theta,P(Y, X),n) \\mapsto P'(Y, X)$ with $P'(Y, X)$ induced by\n$$P'(Y = 1, X = x) = \\int \\int \\frac{1(x = x_s(\\theta)) \\cdot q_y(x_s(\\theta)) + nP(Y = 1, X = x)}{n+1} P_{Y|x} dy P_X dx$$\nwhere $x_s(\\theta)$ is a data selection function, see definition 2. Its induced distribution on $X$ is $P_X$. The query function $q_y : X \\rightarrow [0,1]$ returns the true class (probability) for the selected $x_s(\\theta)$ and gives rise to $P_{Y|X}$. In contrast to self-training (example 1), the queried labels $q_y(x_s(\\theta))$ do not directly depend on the model $\\theta$, only indirectly through $x_s(\\theta)$.\nAgain, both deterministic data selection through $x_d(s)$ and non-probabilistic (i.e., hard labels) queries through $q_y : X \\times \\Theta \\rightarrow \\{0, 1\\}$ are well defined special cases of the above with $P_{Y|X}$ and $P_X$ collapsing to trivial Dirac measures, respectively. As far as we can oversee the active learning literature, hard label queries [82, 24, 70] are more common than probabilistic or soft queries [123]."}, {"title": "3.3. Multi-armed bandits", "content": "The multi-armed bandit problem is one of the most general setups for evaluating decision-making strategies when facing uncertain outcomes. It is named after the analogy of a gambler at a row of slot machines, where each machine provides a different, unknown reward distribution. The gambler must develop a strategy to maximize their rewards over a series of spins, balancing the exploration of machines to learn more about their rewards versus exploiting known information to maximize returns. Typically, a contextual bandit algorithm is comprised of contexts $\\{X_t\\}_{t=1}^T$, actions $\\{A_t\\}_{t=1}^T$, and primary outcomes $\\{Y_t\\}_{t=1}^T$, again with binary image of $Y$, for simplicity, denoted by $Y = \\{0,1\\}$. We assume that rewards are a deterministic function of the primary outcomes, i.e., $R_t = f (Y_t)$ for some known function $f$."}, {"title": "Example 3 (Multi-Armed Bandits)", "content": "Multi-Armed Bandits are instances of reciprocal learning with the following sample adaption function (see definition 5) $f_{maB} : \\Theta \\times \\mathcal{D} \\times \\mathbb{N} \\rightarrow \\mathcal{P}; (\\theta,P(Y, X), n) \\mapsto P'(Y, X)$ with $P'(Y, X)$ induced by\n$$P'(Y = 1, X = x) = \\int \\frac{1(x = x(a(\\theta))) \\cdot Y(a(\\theta)) + nP(Y = 1, X = x)}{n+1} P(A_t | \\theta(H_{t-1})) da$$\nwhere $a(\\theta) : \\Theta \\rightarrow A$ is an action selection function, also referred to as policy function in the bandit literature that induces the well-known action selection probabilities $P (A | \\theta(H_{t-1}))$, often called policies and denoted by $\\pi := \\{\\pi_t\\}_{t \\geq 1}$. Further note that the indicator function takes an argument that depends on $\\Theta$ only through $a$, contrary to active and semi-supervised learning.\nSeveral strategies exist to solve multi-armed bandit problems, including upper confidence bound (deterministic), epsilon-greedy (stochastic) and already mentioned Thompson sampling (stochastic). Deterministic strategies like upper confidence bound can be embedded into the above general stochastic formulation through degenerate policies $P (A | \\theta(H_{t-1}))$ putting point mass 1 on the deterministically optimal action."}, {"title": "4. CONVERGENCE OF RECIPROCAL LEARNING:\nLIPSCHITZ IS ALL YOU NEED", "content": "After having pointed out that several widely adopted machine learning algorithms are specific instances of reciprocal learning, we will study its convergence (definition 8) and optimality (definition 9). Our general aim is to identify sufficient conditions for any reciprocal learning algorithm to converge and then show that such a convergent solution is sufficiently close to the optimal one. This will not only allow to assess convergence and optimality of examples 1 through 3 (self-training, active learning, multi-armed bandits, see section 3) but of any other reciprocal learning algorithm. Besides further existing examples not detailed here like superset learning [34] or Bayesian optimization [68], we are especially aiming at potential future - yet to be proposed \u2013 algorithms. On this background, our conditions for convergence and optimality can be understood as design principles. Before turning to these concrete conditions on reciprocal learning algorithms, we need some general assumptions on the loss function for the remainder of the paper. Assumptions 1 and 2 can be considered quite mild and are fulfilled by a broad class of loss functions, see [103, Chapter 12] or [14]. For instance, the L2-regularized (ridge) logistic loss has Lipschitz-continuous gradients both with respect to features and parameters. For a discussion of assumption 3, we refer to appendix C.2."}, {"title": "Assumption 1 (Continuous Differentiability in Features)", "content": "A loss function $l(Y, X,\\theta)$ is said to be continuously differentiable with respect to features if the gradient $\\nabla_x l(Y,X,\\theta)$ exists and is $\\alpha$-Lipschitz continuous in $\\theta$, $x$, and $y$ with respect to the L2-norm on domain and codomain."}, {"title": "Assumption 2 (Continuous Differentiability in Parameters)", "content": "A loss function $l(Y, X,\\theta)$ is continuously differentiable with respect to parameters if the gradient $\\nabla_\\theta l(Y,X,\\theta)$ exists and is $\\beta$-Lipschitz continuous in $\\theta$, $x$, and $y$ with respect to the L2-norm on domain and codomain."}, {"title": "Assumption 3 (Strong Convexity)", "content": "Loss $l(Y,X,\\theta)$ is said to be $\\gamma$-strongly convex if\n$$l(y,x,\\theta) \\geq l (y,x, \\theta') + \\nabla_\\theta l (y, x, \\theta')^\\top (\\theta - \\theta') + \\frac{\\gamma}{2} ||\\theta - \\theta'||^2,$$ for all $\\theta,\\theta', y,x$. Observe convexity for $\\gamma = 0$.\nLet us now turn to specific and more constructive conditions on reciprocal learning's workhorse, the data selection problem $(\\Theta, X, L_{\\theta})$. At the heart of these conditions lies a common goal: We want to establish some continuity in how the data changes from $t - 1$ to $t$ in response to $\\theta_{t-1}$ and $P_{t-1}$. It is self-evident that without any such continuity, convergence seems out of reach. As it will turn out, bounding the change of the data in $t$ by the change of what happens in $t - 1$ will be sufficient for convergence, see figure 3. We thus need the sample adaption function $f : \\Theta \\times \\mathcal{P} \\times \\mathbb{N} \\rightarrow \\mathcal{I}$ (definition 5) to be Lipschitz-continuous. Theorem 1 will deliver this for subsets of conditions 1 through 5 in case of binary classification problems. The reason for the latter restriction is that we need an explicit definition of $f$ to constructively prove its Lipschitz-continuity."}, {"title": "Condition 1 (Data Regularization)", "content": "Data selection is regularized as per definition 3."}, {"title": "Condition 2 (Soft Labels Prediction)", "content": ""}]}