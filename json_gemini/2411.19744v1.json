{"title": "Amplifying human performance in combinatorial competitive programming", "authors": ["Petar Veli\u010dkovi\u0107", "Alex Vitvitskyi", "Larisa Markeeva", "Borja Ibarz", "Lars Buesing", "Matej Balog", "Alexander Novikov"], "abstract": "Recent years have seen a significant surge in complex AI systems for competitive programming, capable of performing at admirable levels against human competitors. While steady progress has been made, the highest percentiles still remain out of reach for these methods on standard competition platforms such as Codeforces. Here we instead focus on combinatorial competitive programming, where the target is to find as-good-as-possible solutions to otherwise computationally intractable problems, over specific given inputs. We hypothesise that this scenario offers a unique testbed for human-AI synergy, as human programmers can write a backbone of a heuristic solution, after which AI can be used to optimise the scoring function used by the heuristic. We deploy our approach on previous iterations of Hash Code, a global team programming competition inspired by NP-hard software engineering problems at Google, and we leverage FunSearch to evolve our scoring functions. Our evolved solutions significantly improve the attained scores from their baseline, successfully breaking into the top percentile on all previous Hash Code online qualification rounds, and outperforming the top human teams on several. Our method is also performant on an optimisation problem that featured in a recent held-out AtCoder contest.", "sections": [{"title": "Introduction", "content": "Competitive programming\u2014the art of writing highly specialised code for solving challenging problems under various constraints\u2014represents a critical test of advanced reasoning skills. It has sparked interest as an evaluation of AI systems\u2014with systems like AlphaCode (Li et al., 2022) demonstrating good performance given a language model equipped with a filtering procedure.\nIn recent years, capabilities of Al-powered competitive programming systems have been steadily increasing\u2014breaking into and beyond the 85th percentile on platforms such as Codeforces (Leblond et al., 2023; OpenAI, 2024), largely relying on improved base model capabilities, data curation and test-time compute.\nWhile such results are certainly impressive, they are still insufficient for consistently outperforming the highest echelons of human competitive programmers. The largest reported Codeforces ELO rating for an Al system at the time of writing is still below 1,900. The most powerful human competitors are capable of significantly higher feats than that, with one Gennady Korotkevich (tourist) - achieving ELO above 4,000. It is our opinion that further breakthroughs are needed before AI systems reach such levels at Codeforces-style competitions.\nMeanwhile, another type of impressive AI system has recently emerged: leveraging evolutionary algorithms for searching in the function space (Romera-Paredes et al., 2024, FunSearch), expressed using code implementations written by language models. We believe that FunSearch opens up an opportunity for a different kind of top-level competitive programming result: one on combinatorial optimisation challenges, made in collaboration with human competitors.\nWe present a successful result of this kind: by applying FunSearch on a human-designed solution backbone across several Hash Code competition tasks, we are able to significantly amplify the scores obtained by the backbone. In several contests, we recover solutions that would have outperformed the top human teams. We also validate our method on a variant of a recently-held AtCoder Heuristic Contest."}, {"title": "Combinatorial competitive programming", "content": "Before diving into the specifics of our approach, we briefly discuss why combinatorial tasks are a more immediate fit for present-day AI systems to achieve top-tier levels, compared to Codeforces.\nCodeforces-style tasks typically require writing tractable, polynomial-time algorithms\u2014with no reward given for partially correct or inefficient solutions. As many of the testcases for such tasks rely on hidden edge cases, it is generally not sufficient for Al to produce code that works correctly in most cases\u2014careful understanding is required of many constraints which might not be obvious at all from the task statement. Further, many such tasks follow specialised design patterns, which well-versed human competitors readily recognise and apply-making it harder for AI to catch up.\nIn contrast, combinatorial optimisation problems are typically intractable, NP-hard problems, with testcases for which optimal solutions are usually unknown. As such, valid-but-suboptimal solutions are inevitable, and they are all rewarded depending on their measured performance on these testcases. Further, the testcases are often visible to the competitors, avoiding any hidden edge-case situations. In summary, even if it might seem counter-intuitive, combinatorial optimisation competitions offer a more immediate opportunity for exceptional results: they do not punish suboptimality, all cases of importance are known upfront, and even the strongest human competitors do not know how to find optimal solutions.\nNext, we describe the specific competition-Hash Code-we used to substantiate this claim."}, {"title": "Hash Code competitions", "content": "Hash Code is a former Google-organised global (2-4 person) team programming competition. It features NP-hard optimisation problems inspired by software engineering tasks at Google. Teams produce outputs for a given set of inputs to an intractable optimisation problem, and are evaluated on the quality of their outputs. At the peak of its popularity, it was globally renowned as one of the most popular combinatorial optimisation competitions, inviting over 125,000 competitors, and featuring some of the most successful competitive programmers\u2014including tourist.\nHash Code is, at its core, a contest requiring design of a strong heuristic. Top teams tend to write scoring functions for a locally-optimal greedy search, often followed by randomised hill-climbing. We believe devising strong scoring functions for greedy search is highly non-trivial, and hence amenable to automated genetic programming tools such as FunSearch. Under the Hash Code rules, use of external tools (e.g. combinatorial solver packages) was allowed, which would have made our approach legal at the time as well.\nHash Code has had two phases in most years\u2014first, an online qualification round, followed by a final round for the top ~50 teams in the qualification. For our purposes, we use only the qualifications, as they have a much wider pool of competitors to compare against, with their tasks still being intractable in their nature. We access all problems and their inputs at https://github.com/google/coding-competitions-archive."}, {"title": "Experimental setup", "content": "Now we can dive deeper into how we leveraged FunSearch to amplify typical strategies that competent competitors may leverage in such contests. Please refer to Figure 1 for a high-level overview."}, {"title": "Overall workflow", "content": "The workflow of our approach is as follows:\n\u2022 We implement a backbone of a greedy algorithm that tackles the given problem, along with functions to parse the input file(s) and evaluate the fitness of candidate solutions;\n\u2022 The greedy algorithm depends on a scoring function that weighs in on each of its possible next steps. Initially, we can make this a simple function-see Figure 2 for an example;\n\u2022 We use FunSearch to evolve this function, using the number of points achieved on a given testcase as the fitness function;\n\u2022 When relevant, we may analyse the outputs of FunSearch to iterate on the backbone structure, strap on local search, and similar.\n\u2022 Note that we consider each of the provided inputs as a separate problem; where appropriate, different inputs to the same qualification round may feature different backbones.\nThis approach, in fact, closely mimics what a real competitor's workflow during Hash Code might look like; the main part being automated is the evolution of the scoring function, which is a hard task for humans.\nNote that the collaboration between humans and AI was important to achieve the results with this workflow\u2014the competitors can leverage their strengths and develop the backbone, whereas AI can automate the complex search in program space (likely to be necessary unless P = NP) that can extract higher returns out of the backbone."}, {"title": "Evolving heuristics", "content": "Next, we describe the methodology for evolving the scoring functions used for Hash Code."}, {"title": "Introduction to FunSearch", "content": "FunSearch (Romera-Paredes et al., 2024) is an optimisation technique that pairs a pre-trained large language model (LLM) with a systematic evaluator. It is designed to address the limitations of LLMs, such as confabulations, through its usage of the evaluator to check the correctness of the LLM's output.\nFunSearch operates by evolving a population of candidate programs over multiple iterations. The LLM proposes modifications or improvements to the programs, and the evaluator assesses the quality of these modifications. The process can in principle continue without termination, though in practice we would terminate it once either a desirable high-quality program is found, or no significant progress has been observed over a sufficient time frame.\nThe success of FunSearch is attributed to several key components, including best-shot prompting-where the best-performing programs are fed back into prompts for the LLM to improve upon\u2014and the use of program backbones that focus the LLM's attention only on evolving critical program logic, leaving the backbone itself fixed. Additionally, FunSearch employs an island-based evolutionary method to maintain program diversity and avoid local optima. The asynchronous and parallel nature of FunSearch allows for efficient exploration of the program space, especially across multiple devices."}, {"title": "Configuration specifics", "content": "The setup and hyperparameters of our evolutionary program search largely match Romera-Paredes et al. (2024). There are, however, three key differences, specifically tuned towards the competitive programming setting and latest developments in LLMs. We describe them here:\n\u2022 While Romera-Paredes et al. (2024) rely on a variant of PaLM 2 (Anil et al., 2023) specifically fine-tuned on code, we find that the code-writing capabilities of modern generalist fine-tuned LLMs are sufficient to not require dedicated fine-tuning anymore. As such, we used Gemini 1.5 Flash 002 (Gemini Team et al., 2024) as the LLM that proposes modifications to programs in the population.\n\u2022 As combinatorial contests often feature very large inputs - in order to make exhaustive search intractable - we needed to increase the evaluation limits for the generated programs from their defaults in Romera-Paredes et al. (2024). Specifically, we limit memory usage of the entire program to 10 GB, and wall-clock execution time to 1,800 seconds.\n\u2022 It is rather common that greedy solutions to combinatorial contests require multiple choice points. This is in contention with the structure of Romera-Paredes et al. (2024), which instead only optimises one scoring function. We are able to practically work around this by introducing a switching variable that the scoring function can branch on depending on which choice point needs to be scored at this point in time. This can be seen in Figure 2, where the switching variable is rate_project and it determines whether we're scoring a project or a project-role pair. The backbone takes care of invoking the scoring function with the correctly set switching."}, {"title": "Results", "content": "We deploy our approach on all eight Hash Code online qualification rounds (from 2015 until 2022). The main results, which we present in Figure 3, will aim to quantify two key elements:\nCan FunSearch deliver meaningful improvements in the combinatorial competitive programming setting? To measure this, we will compare the scores and ranks we obtain against the ones achieved by the backbone solution (with any iterative improvements applied) and our initial scoring function alone. We can assess the significance of this improvement by checking either the percentiles achieved, or whether the recovered scores would be sufficient to qualify in the finals (i.e. in the top 50 ranks that year). It is worth noting that, prior to 2019, Hash Code online qualification results comprised fewer than 5,000 participating teams, meaning that achieving the top percentile was harder than qualifying to the finals-afterwards it became easier.\nCould these improvements be meaningfully obtained under contest conditions? To measure this, we will also compare against the scores and ranks obtained after only running the evolutionary computation for two hours. Given that Hash Code had a time limit of four hours, this leaves two hours for implementing the essential three parts of each backbone:\n\u2022 Parsing the input file into appropriate data class objects for further processing;\n\u2022 A greedy algorithm with a base scoring function to produce candidate solutions;\n\u2022 The systematic evaluator of candidate solutions, to be used as a fitness function;\nGiven that the combined backbone implementations we develop never exceeded ~ 400 lines of Python code including docstrings (and were usually around ~ 200 lines)\u2014and Hash Code teams comprise up to four members\u2014we find this to be a reasonable undertaking for a team of competent competitive programmers. As a reference, within two hours, our method evaluates around 10,500 programs on average across all backbones."}, {"title": "Result analysis and discussion", "content": "The results outlined in Figure 3 provide conclusive evidence towards settling our two questions positively. Specifically, we note that:\n\u2022 The backbones with base scoring functions are not capable of achieving the top percentile or a finals qualification in any Hash Code year we investigated;\n\u2022 The evolved scoring functions make significant progress compared to the base scoring function in terms of both rank and fitness;\n\u2022 The evolved functions reach the top percentile of competing teams, as well as surpassing the rank required to qualify in the finals, for every Hash Code iteration.\nBeyond this, our evolved solutions were capable of outperforming the rank-1 team on five online qualification rounds: 2015 (Optimize a Data Center), 2018 (Self-driving rides), 2020 (Book scanning), 2021 (Traffic signaling) and 2022 (Mentorship and Teamwork). This is clearly a result that is beyond reach of the backbone developers (i.e. the authors of this paper) without AI assistance.\nIt is interesting to note that the starting point of our backbone solutions varies significantly across different Hash Code iterations. To give just two examples: in 2018, they start below the 20th percentile; while in 2022, they start above the 90th. This phenomenon nicely illustrates the diversity in challenge and contestant cohorts offered in various Hash Code iterations. The complexity of writing a working backbone may in and of itself pose quite a challenge-for example, writing an appropriate evaluator and greedy algorithm proved a significant undertaking in Hash Code 2016 (for reference, our own implementation has 415 lines of Python). Accordingly, having a working and complete backbone allowed for a relatively high rank compared to the competition cohort.\nFurther, after Hash Code significantly rose in popularity\u2014especially from 2019 onward\u2014a longer tail of competitor teams with low relative scores started to emerge. This naturally uplifted the percentile of any backbone solution which achieves a modest relative score. For example, achieving a normalised score of 0.6 would be insufficient even for the 30th perecentile in Hash Code 2018, while it is sufficient for the 90th percentile in Hash Code 2022.\nWe also believe that many of our method's benefits should be recoverable under contest conditions\u2014considering the proximity of the \"two-hours\" solution to the maximal fitness we were able to obtain, and its relative merit against participating teams. In all but two iterations of Hash Code, this capped fitness would have been sufficient to qualify into the finals. Additionally, as we now know that sufficiently better solutions are within reach of the method given more computational resources, we consider that discovering them under the two-hour time constraint now amounts to an engineering challenge, rather than a research one.\nA possible limitation of our result is that, since the fitness on these contests was evaluated posthoc rather than in real time, it is not unlikely that our base Gemini 1.5 Flash 002 model had been exposed to Hash Code subroutines within its training data. We believe this does not diminish the significance of our results, and elaborate further on why that is in the following paragraphs.\nFirstly, unlike the context of code generation on Codeforces\u2014where the model is asked to output the entirety of the solution\u2014in our collaborative setting the model is prompted to evolve only the scoring function, in a way that is compatible to the rest of the backbone. This means that the solution needs to conform to the API prescribed by the backbone as well as utilising dataclasses defined within it, which is a prompt setting completely unseen in prior training, as our backbones are not available in LLM pre-training data.\nFurther, unlike the Codeforces setting, no information specific to the Hash Code tasks is given to the model in the prompt\u2014only the associated backbone code and previous best-performing scoring function, which makes it unlikely that a retrieval approach can even be successfully invoked in the first place. The fact that our models do not generate fully optimised solutions after one LLM mutation but rather tend to make steady, iterative progress towards improving solution fitness throughout training is further evidence to the fact that no final solutions are immediately recalled by the models. Indeed, most of our optimised solutions required a chain of at least 10\u201330 iterative improvement calls to Gemini 1.5 Flash 002.\nIn addition, towards the end of the paper we will provide a held-out contest case study, on a variant of the recently held AtCoder Heuristic Contest 039 (which took place on 10 November 2024, after the release of Gemini 1.5 Flash 002). This will serve to illustrate how our FunSearch-augmented method still yields tangible and significant benefits, even over a recent contest with a substantially different setup than Hash Code."}, {"title": "Evolved Hash Code heuristics: qualitative case studies", "content": "We will now present several case studies elaborating on the solutions evolved by FunSearch across several (sub)problem backbones in prior Hash Code qualification rounds. This analysis will both elucidate the insight discovered within some of the discovered solutions, and further emphasise the low likelihood that these solutions could have been derived through retrieval or recall."}, {"title": "Qualification 2015: Optimizing a Data Center", "content": "2015 marked the first Hash Code qualification round, where the task was to find the most fault-tolerant way to arrange servers within an abstractified data center.\nSpecifically, it is assumed that the data center has a certain number of rows, with each row having a certain number of slots (some of which may be blocked). Each server has a certain capacity, can be assigned to a particular pool, and occupies a certain contiguous number of slots. The task is to decide on locations where every server is installed, such that the guaranteed capacity-the minimal remaining capacity across all of the pools, should any data center row stop working-is optimised.\nThere was exactly one input testcase, hashcode_2015_qualification_round.txt, and we focus on it here. We provide the input parsing and greedy algorithm parts of the backbone we used in the Appendix (Figure 5).\nThe greedy backbone operates in two phases: first, it determines where to place each server, then it assigns each server to a pool. The servers are iteratively assigned to each free space in a row, repeatedly cycling between rows to promote evenly distributed servers across rows.\nMuch like in Figure 2, here we really need two scoring functions: one to decide which server to place in the current position, and another to decide which pool to allocate a server to. We handle this using a boolean variable rate_server. Our base scoring function rates servers based on their capacity relative to amount of spaces they occupy, and it rates pools based on how much would they additionally improve their pool without over-representing it in this particular row:\nif rate_server:\n  return server.capacity / server.size\nelse:\n  total_sum = 0\n  for c_row in pools_per_row:\n    total_sum += pools_per_row[c_row] [pool]\n  return -total_sum + pools_per_row[row] [pool]\n  assert row >= 0\nThis scoring function, while simple, achieves a commendable score of 348 points. After evolving this function for two hours \u2013 roughly remaining under contest conditions \u2013 the optimal function becomes significantly more elaborate:\nassert server.size > 0\nassert server.capacity > 0\nif pool is not None:\n  total_sum = pools_per_row [row] [pool]\n  max_sum = 0\n  pool_size = 0\n  for p in pools_per_row:\n    total_sum += pools_per_row[p] [pool]\n    max_sum = max(pools_per_row [p] [pool], max_sum)\n    if p == pool:\n      pool_size += pools_per_row [p] [pool]\n\n  if total_sum == 0:\n    return -100\n\n  # Compute max gain after adding 1 server in row:\n  row_score = total_sum / server.size + max_sum / server.size\n\n  if row not in pools_per_row:\n    return row_score\n  else:\n    pool_score = -0.5 * total_sum / server.size + (\n      0.5 * max_sum / server.size)\n    pool_bonus = 0.015 * (total_sum - \\\n        pool_size)\n\n    if server.capacity >= total_sum / 2.0:\n      pool_bonus *= 1.2\n    elif server.capacity >= (total_sum - pool_size) / 4.0:\n      pool_bonus *= 1.5\n\n    return (row_score + pool_score + pool_bonus / 1000. + 0.00005 *\n      (server.capacity / server.size + total_sum / min(\n        total_sum, server.capacity * 1.1)) - 0.004 * row /\n        len(pools_per_row) * (\n          server.size >= total_sum / 10.0) - 0.0004 * pool /\n            len(pools_per_row) - 0.0007* server.size / len(\n              pools_per_row) * (server.capacity >= (\n                total_sum / 2.0) * 1.005))\nelif rate_server:\n  return server.capacity / server.size * (\n    2.0 - 2.0 * server.size / 3.0)\nelse:\n  raise ValueError('should not call this function with None pool')\nThis scoring function significantly improves the score to 405, sufficient to achieve second place! This solution is also substantially mutated, requiring a chain of 15 LLM calls starting from the original. This implementation supports an elaborate algebraic expression for a score function with evolved constants assigned to each component. It is also interesting to note that the evolved function inserts assertions and throws exceptions that would trigger under invalid inputs\u2014though such inputs will never appear in practice.\nFinally, it is interesting to make note of the return -100 statement which, while not likely to make a big difference in this particular setting (it is only invoked when a pool has no servers assigned to it yet), it sets up the model for a more peculiar kind of mechanism to be discovered later.\nIf we allow for more time for evolving the scoring function, it is possible to achieve a rank-1 score of 413, with the following snippet:\ncap = 0\nprev_row = row\nif rate_server:\n  if server.size > 125:\n    return -100\n  else:\nelse:\n  if server.size > 23:\n    cap = 0.5\n    cap += (2.74.95 - server.size / 125) * (\n      server.capacity / 125)\n    if server.capacity / server.size > 7.5:\n      cap *= 5.0\n    if server.capacity / server.size > 7.95:\n      cap *= 11.0\n    return cap\n\nn_pools_full = \\\n  sum(1 if pool_cap > 7357 else \\\n      (0 for pool_cap in pools_per_row [row].values()))\nassert pool is not None\nmax_cap = max(pools_per_row [row] [pool] / 1150, 0.475)\nassert max_cap > 0\ntotal_cap = 1.16 - (1.161.1) * (0.9 - \\\n  min_cap - 13000.0 - n_pools_full / 5)\nfor c_row, pool_cap in sorted (pools_per_row.items()):\n  total_cap += max(pool_cap [pool], 0.1)\n  if prev_row is not None and (\n    c_row != row) and (c_row != prev_row):\n      assert c_row in pools_per_row\n      assert pool in pools_per_row [c_row]\n      max_cap = max (max_cap, pools_per_row [c_row] [pool])\n      min_cap = min(min_cap, pools_per_row [c_row] [pool])\n      prev_row = c_row\n\nif min_cap > server.capacity:\n  min_cap = server.capacity * 0.7\ntotal_cap += max(pools_per_row [row] [pool] * 0.95, 0.03)\ntotal_cap = max(pools_per_row [row] [pool] / 650, 0.005)\ncap += max(max(\n  pools_per_row [row] [pool], 0.1) / 1.32, 710.0 / (\n    server.size + 1.0))\nassert cap > 0\nif server.size > max_cap:\n  return -100\nelif total_cap < server.capacity:\n  return -10000\nelse:\n  return (server.capacity - (total_cap - max_cap)) + cap\nThis function also implements an elaborate algebraic score expression with evolved constants. However, it is interesting to note how the negative numbers are being used in this case. For example, with the check if server.size > 125: return -100, the system is implementing a hard threshold which automatically discourages from scheduling all servers occupying more than 125 slots. Similarly, with the final if/elif/else chain, it is discouraged to allocate large servers to any particular pool (if), and even more so to allocate servers to a pool if they have substantially larger capacity than a computed cap value (elif)."}, {"title": "Qualification 2018: Self-driving rides", "content": "Next, we analyse the evolution of the scoring function for the 2018 Hash Code qualification round, where the objective is to find an optimal schedule of rides for a fleet of self-driving cars.\nSpecifically, we are given a fleet of a certain number of self-driving cars, which need to complete a certain amount of ordered rides. Each ride order consists of a start and end point, earliest start time and the latest finish time. Each ride completed on time accrues a certain number of points, with a certain bonus obtained for rides that start in the earliest possible moment. The task is to decide a schedule in which each of the cars will complete a certain list of rides sequentially, always driving along the shortest path to their next destination.\nWe focus on the testcase d_metropolis.in, where the model made the most significant jump in score compared to the backbone. As before, in the Appendix we provide the key parts of the backbone we used for this input (Figure 6).\nThe backbone greedy solution in this case maintains a priority queue of all cars in the fleet, keyed by the time at which they finish their latest ride (initially zero for all cars). At each step, the top car in the priority queue is taken, and a ride is allocated to it. Our base scoring function simply picks the first feasible ride:\nfor i, r in enumerate (rides):\n  pickup_time = time + r.distance_to_start(\n    coords)\n  if (pickup_time >= r.earliest_start) and (\n    pickup_time + (\n      r.length()) < r.latest_finish\n  ):\n    return i\n# We failed to find any feasible ride.\nreturn -1\nand it achieves 3,528,556 points on this testcase. After evolving for two hours, we recover the following intuitive solution:\nbest_time, best_idx= -1, -1\nfor i, r in enumerate (rides):\n  pickup_time = \\\n  max(\n    r.earliest_start, time + r.distance_to_start(coords))\n  if pickup_time + r.length() >= r.latest_finish:\n    continue\n  if best_time < 0 or pickup_time < best_time:\n    best_time, best_idx= pickup_time, i\nreturn best_idx\nwhich corrects for the fact that the pick-up time doesn't have to be after the earliest start to be feasible (the car simply has to wait), and the loop does not break early when a ride is found\u2014instead, the best solution is retrieved based on its proximity to the car's present point.\nThis solution amplifies the performance on this input significantly, to 11,739,630 points.\nFrom this point, a significant amount of time and additional evolution steps are needed to discover a useful non-trivial solution.\nbest_score = float(\"-inf\")\nbest_ride = -1\nfree_time = 0\n# Rides in descending order by distance to the starting point.\nrides_by_length = [(i, r.length(), distance (coords, r.start))\n  for i, r in enumerate (rides)\n    if r.latest_finish >= time // 2.]\nrides_by_length.sort(reverse=True)\nfor (i, ride_length, distance_to_start) in rides_by_length:\n  r = rides [i]\n  pickup_time = time + distance_to_start\n  if pickup_time < r.earliest_start:\n    pickup_time = r.earliest_start\n  free_time = pickup_time + ride_length\n  bonus_points = 20000 if time < 3600 or time > 20900 else 0\n  bonus_points = bonus_points if (\n    r.latest_finish <= time + 1.5 * ride_length) else 0\n  if time <= 3600:\n    bonus_points = bonus_points + 75 * free_time\n  else:\n    bonus_points = bonus_points - 7.5* free_time\n\n  if free_time <= r.latest_finish and (\n    r.earliest_start <= pickup_time):\n    score = ride_length + bonus_points -\\\n      15 * (abs(\n        r.start [0] - coords [0]) + abs(\n          r.start [1] - coords [1])) -\\\n      200 * max([0, pickup_time - time]) -\\\n      1. * sum([\n        ])\n    abs(pickup_time - r.earliest_start),\n    abs (free_time - r.latest_finish),\n    sum([abs(\n      r.start [j] - r.end (j)) for j in range(2)])\n\n    # distance penalty from ending location\n    score -= score + 15 * (\n      1100\n       r.distance_to_start(\n          coords)) + 90 * r.distance_to_start(coords) / 1200.\n\n    # penalty for driving far in the late night and early morning\n    if rides_by_length [0] [0] == i:\n      score = score - 25 * free_time\n\n    if time >= 39480 and free_time > time // 2.:\n      score = score + 10*(\n        free_time - 1100+ r.distance_to_start(coords))\n    if r.latest_finish <= time + 2* r.length():\n      score = score + 8000\n    if r.length() < 1500:\n      score += score + 2000\n    if r.length() > 6000:\n      score = score - 3000\n    if r.length() > 7000:\n      score = score - 3000\n    if r.length() > 5000:\n      score -= score - 5000\n    if score > best_score:\n      best_ride = i\n      best_score = score\nreturn best_ride\nThis solution achieves a score of 12,296,845 points on the metropolis input, and is generally sufficient for outperforming the rank-1 team when aggregating across all inputs.\nWhile it clearly implements a more elaborate scoring function than its predecessors, it also makes some peculiar or redundant decisions. For example, while its comment indicates it will sort rides in descending order by distance, the effect of the sorting function is to sort in reverse using the first key \u2013 the index. This only has the effect of processing the feasible rides in the reverse order from the one that they were given.\nThen, the evolved function computes a bonus which can be allocated for rides without a lot of margin for error that are queried early or late. Besides this bonus, several other heuristics, including the distance of the car to the starting point, and whether the endpoint of one ride can be chained to the start point of another ($-200 * \\max([0, pickup\\_time - time]))$, all factor into the computed score. There are also several penalties applied to the score designed to encourage choosing shorter rides (e.g. if $r.length() > 6000: score -= 3000)$, with an interesting no-op command being accidentally executed in score == score + 2000."}, {"title": "Qualification 2021: Traffic signaling", "content": "Finally", "street": "which position will it have in the traffic light schedule for its incoming intersection", "executed": "n\u2022 First, a simulation of the system is performed with all the cars, under the assumption the green light durations will all be 1. Whenever a car arrives at an intersection that hasn't previously been assigned a position, the position prediction is invoked on it."}]}