{"title": "Unlocking Attributes' Contribution to Successful Camouflage: A Combined Textual and Visual Analysis Strategy", "authors": ["Hong Zhang", "Yixuan Lyu", "Qian Yu", "Hanyang Liu", "Huimin Ma", "Ding Yuan", "Yifan Yang"], "abstract": "In the domain of Camouflaged Object Segmentation (COS), despite continuous improvements in segmentation performance, the underlying mechanisms of effective camouflage remain poorly understood, akin to a black box. To address this gap, we present the first comprehensive study to examine the impact of camouflage attributes on the effectiveness of camouflage patterns, offering a quantitative framework for the evaluation of camouflage designs. To support this analysis, we have compiled the first dataset comprising descriptions of camouflaged objects and their attribute contributions, termed COD-Text And X-attributions (COD-TAX). Moreover, drawing inspiration from the hierarchical process by which humans process information: from high-level textual descriptions of overarching scenarios, through mid-level summaries of local areas, to low-level pixel data for detailed analysis. We have developed a robust framework that combines textual and visual information for the task of COS, named Attribution CUe Modeling with Eye-fixation Network (ACUMEN). ACUMEN demonstrates superior performance, outperforming nine leading methods across three widely-used datasets. We conclude by highlighting key insights derived from the attributes identified in our study. Code: https://github.com/lyu-yx/ACUMEN.", "sections": [{"title": "1 Introduction", "content": "Darwinian evolutionary theory posits that wild animals have developed complex camouflage mechanisms to elude predators [42]. Conventional object detection and segmentation algorithms often encounter substantial difficulties in identifying camouflaged objects, resulting in diminished efficacy. This challenge has spurred interest in the field of Camouflaged Object Segmentation (COS), which has seen significant research advancements. Such advancements not only deepen our comprehension of COS but also have practical implications in various fields, including industrial defect detection [10], abnormal tissue segmentation [11], and the transformation between salient and camouflaged objects [17].\nLearning in COS has a long history but still striving. Earlier approaches in COS utilized hand-crafted features that were efficient in certain scenarios but fell short in terms of broad applicability [1]. In contrast, recent studys in data-driven deep learning have shown remarkable success, with techniques leveraging gradients [19], edges [2,30], uncertainty [22], and multi-view inputs [37], among others, demonstrating significant improvements. These developments leverage visual features individually but conclude extra priors for guidance, underscore the potential of integrating additional modalities into COS. Very recently, the advent of Large Vision-Language Models (LVLMs) has shifted focus towards harnessing pretrained LVLMs for extracting knowledge, thereby enriching the camouflaged object mask regression process [3,15]. However, integrating LVLMs directly introduces challenges such as deployment constraints in local environments and costs associated with LVLMs utilization, in addition to the complexities of prompt engineering for COS task.\nOur proposed baseline ACUMEN (Attribution CUe Modeling with Eye-fixation Network) is underpinned by two critical insights: 1) Cognitive science shows that merging textual and visual information synergistically boosts cognitive understanding [33,36], and 2) Evolutionary biology highlights the significance of camouflage pattern creation (by prey) and its identification (by predators) in evolutionary progress, underlining the necessity to analyze camouflage from both granular attribute insights (designing) and a wider object detection (breaking) standpoint. Capitalizing on the first insight, ACUMEN integrates textual scene descriptions of camouflaged objects. Addressing the second insight, we assess the contribution of potential attributes (e.g., Environmental Pattern Matching, Shape Mimicry) on the efficacy of camouflage. More specifically, we commence by collecting a dataset enriched with image descriptions and attribute contributions. Subsequently, we construct a bifurcated multimodal framework that merges textual and visual analyses seamlessly. Within the textual branch, the framework utilizes frozen CLIP [39] text encoder for text analysis, facilitating the synthesis and integration of visual features into a unified latent space. On the visual front, we introduce an attribution and a fixation predictor to assess attribute impacts and generate fixation maps, respectively. Following this prediction phase, an Attributes-Fixation Embedding (AFE) module is implemented to maximize the utility of the predicted attribute contribution tensor and fixation map. This methodology concludes with the delineation of camouflaged objects' masks, accomplished via a transformer decoder and a streamlined projector. Notably, ACUMEN operates solely with the camouflage image during inference, dispensing with the necessity for image descriptions and independence from other LVLMs, thus establishing it as an exclusively visual paradigm.\nTo our knowledge, ACUMEN constitutes the first systematic exploration of textual descriptions and attribute contributions within the domain of COS. This investigation uncovers potential for enhancing performance through purely visual methods and provides a deeper understanding of camouflage mechanisms. Our principal contributions are detailed below:\n*   Introduction of the COD-TAX dataset, which integrates textual information with the COS process.\n*   Preliminary analysis of attribute contributions to camouflage scenes, presenting a novel viewpoint on scene analysis and design.\n*   Development of ACUMEN, a unique dual-branch multimodal fusion framework, setting a new benchmark for cross-modal analysis in the COS field.\n*   Comprehensive experiments demonstrating ACUMEN's superior performance, notably outperforming existing state-of-the-art (SOTA) approaches."}, {"title": "2 Related works", "content": "2.1 Camouflaged Object Segmentation\nThe field of COS has consistently garnered interest within the computer vision community. These days, advancements in computing resources, data collection methods, and feature extraction techniques have facilitated a significant shift from traditional handcrafted feature generation [14,27,44] to contemporary data-driven deep learning approaches. Within the deep learning domain, some approaches draw inspiration from biological processes for network architecture design. For example, Fan et al. [9] emulate the natural predator-prey detection mechanisms, incorporating strategies for search and identification. Similarly, Mei et al. [34] adopt strategies akin to human positioning and focusing to address COS challenges. Alternatively, certain researchers utilize auxiliary tasks to generate meaningful priors, thereby improving mask regression accuracy. For instance, He et al. [12] employ Wavelet-like feature decomposition and edge detection for supervisory signals, while Lyu et al. [30] leverage uncertainty and edge information for probabilistic and deterministic mask prediction guidance. Wu et al. [47] introduce source-free depth information to enable three-dimensional object analysis. While these methods focus on the manipulation and extraction of visual information, they often overlook the integration of highly condensed semantic supervision and the understanding of camouflage patterns.\nOur proposed ACUMEN model addresses these shortcomings by incorporating textual descriptions as semantic descriptors for high-level consistency supervision and exploring seventeen potential camouflage attributes. This approach not only sets new performance benchmarks on widely used datasets but also provides a deeper insight into various camouflage patterns.\n2.2 Large Vision-Language Models\nThe surge in interest towards learning from multimodal information, with the aim of achieving coherent representations across varied modalities has marked recent years. This has led to the innovation of various LVLMs designed to bridge the gap between visual and linguistic data. For instance, LLaVA [25] combines a vision encoder with a comprehensive language model, enabling detailed interpretations of images based on user instructions. Similarly, BLIP-2 [23] integrates a pretrained image encoder with a large language model to excel in image-to-text generation tasks. Moreover, Radford et al. [39] leveraged 400 million image-text pairs within an end-to-end model known as CLIP, to master open-vocabulary image recognition. This groundbreaking work has catalyzed the creation of numerous applications, including low-light image enhancement [50], object detection [24,51], text-driven image manipulation [31], and open-vocabulary semantic segmentation [13,48].\nOur proposed methodology seeks to maximize the utilization of the prior knowledge embedded in LVLMs by introducing dual modal branches that extract and maintain coherence between textual and visual information throughout the training phase. This involves the implementation of attributes contribution analysis and fixation prediction mechanisms with the assistance of CLIP, significantly enhance performance."}, {"title": "3 The COD-TAX Dataset", "content": "3.1 Text and X-Attributes (TAX) Collecting\nThe process of accurately and reliably extracting text descriptions from images containing camouflaged objects is a significant challenge, necessitating extensive time and effort [53]. However, the emergence of LVLMs introduces a promising strategy by exploiting their vast pre-existing knowledge to produce initial descriptions. In this study, we employ GPT-4 Vision (GPT4-V) to generate preliminary descriptions of images and to gain insights into the mechanisms underlying successful camouflage. We predefined a set of potential attributes, allowing GPT4-V to evaluate and determine the importance of each attribute in the observed camouflage. In defining the attributes, we note significant variability in their definitions within the biological domain [35,38,42,43,45]. Certain attributes, notably \"distraction marking\" and \"internal disruptive\", demonstrating considerable overlap, thereby complicating the analysis of camouflage patterns [35]. To address this challenge and broaden the understanding of camouflage-related attributes in the field of computer vision, we expand our categorization methodology based on existing works [35, 38, 42, 43, 45]. Attributes are systematically organized into three primary categories: Surrounding Factors (SF), Camouflaged Object-Self Factors (COF), and Imaging Quality Factors (IQF) as dipicted in Fig. 1a. This categorization elucidates the origins of camouflage, differentiating between the influences of external environments, inherent characteristics of the camouflaged entity, and constraints imposed by photographic techniques. Each category is extensively detailed, encompassing 17 distinct factors, and a thorough classification is presented in Fig. 1.\n3.2 Annotation and Refinement Process\nTo ensure the accuracy and effectiveness of our dataset, we implemented a detailed review process with the participation of over 30 volunteers. These individuals were charged with the critical evaluation of image descriptions produced by GPT4-V and the accuracy of attribute contribution ratios for each image. To enhance the precision of our assessment, we executed three rounds of evaluations for every image. Following the collective insights garnered from these assessments, we accurately identified and amended descriptions and attribute contributions that were consistently deemed incorrect by the evaluators. This rigorous refinement process result in significantly enhanced precision and reliability of the dataset. The comprehensive annotation and refinement effort demanded more than 500 human hours.\n3.3 Dataset Features and Statistics\nWe present the statistical analysis of our proposed COD-TAX in Fig. 1, offering a comprehensive overview of our dataset. The statistical outcomes, including mean and extreme values, are visualized through a rose chart in Fig. la. In this chart, the size of each petal represents the average contribution value of different attributes under general conditions, highlighting the diverse potential contributions across attributes. The range of maximum values extends from 0.21 to 0.55, whereas the average values fluctuate between 0.004 and 0.21. Additionally, Fig. 1b provides an analysis of the textual descriptions for each image, with an average length of 26.52 words and a standard deviation of 2.41, indicating that roughly 70% of descriptions fall within the 24 to 29 word range. In Fig. 1c, we elucidate the frequency of word usage, demonstrating that our dataset predominantly features terms related to surroundings, patterns, backgrounds, textures,"}, {"title": "4 Methods", "content": "4.1 Network Overall\nWe introduce the comprehensive architecture of the proposed ACUMEN in Fig. 2. Initially, we will first detail our underlying motivation and then provide a concise overview of the modules employed in our proposed method.\nMotivation. Biologically, the evolution of camouflage techniques is significantly influenced by predators' ability to learn and generalize, as well as by prey's behavioral adaptations and decision-making processes aimed at enhancing camouflage effectiveness [40]. However, existing researches predominantly explores the predator's perspective, focusing on developing advanced methods for camouflaged object segmentation. These approaches overlook the prey's strategies, especially those attributes that effectively impair a predator's detection capabilities. To address this imbalance, our framework is designed to not only delineate camouflaged objects but also to evaluate the efficacy of their camouflage attributes through the integration of textual descriptions in COS. This initiative aims to provide a comprehensive abstract representation of camouflage patterns, reflecting both predator and prey dynamics.\nNetwork Introduction. As depicted in Fig. 2, the ACUMEN framework incorporates a dual-branch architecture, consisting of a textual branch (highlighted in green) and a visual branch (highlighted in cyan), which are pivotal for feature extraction and integration during the training phase. The textual branch, leveraging the CLIP model, processes textual descriptions to distill high-level abstract features, benefiting from the condensing and highly-abstract nature of textual data. Conversely, the visual branch begins by generating human fixation maps to pinpoint mid-level local attention areas, simultaneously predicting the contribution score of attributes. It then leverages these insights for hierarchical embedding, incorporating pixel-level visual features extracted by the CLIP visual encoder. During the inference phase, to enhance the model's applicability, the textual branch is omitted to eliminate dependency on LVLMs like GPT4, thereby making the inference process solely reliant on visual cues.\n4.2 Fixation Prediction\nIn this study, we employ a Fixation Prediction Module to predict fixations using features from the CLIP visual encoder, as illustrated in Fig. 3. Unlike traditional transformer architectures that solely rely on the deepest encoder feature [5], our approach leverages multiple intermediate features. Specifically, layers 8, 16, and 24 of ViT-L@336, to enhance the information available for fixation prediction tasks. These features are denoted as \\(F_n^v\\) where \\(n = 0,1,2\\), corresponding to shallow to deep layers. With this strategy, we initially use the deepest vision feature \\(F_2^v\\) as a query to determine its correlation with the concatenated visual features. Subsequently, we adhere to the standard attention mechanism, performing recurrent forward passes N times to produce the final output \\(F_f^f\\) via a linear layer followed by a 2D convolution layer. The fixation prediction process is encapsulated by:\n\\(F_f^f = Conv(Decode(CAtt(F_2^v, Cat[LN(F_n^v)]_{n=0,1,2}) + LN(F_2^v) + P_s)\u00d7N).\\) (1)\nHere, \\(P_s\\) represents the positional embedding, \\(LN()\\) refers to Layer Normalization, \\(Cat()\\) indicates channel-wise concatenation, \\(CAtt\\) signifies the Cross-Attention mechanism, and \\(Decode(\u00b7)\u00d7N\\) denotes N cascading decoder blocks. For this study, we set N = 3, as discussed in the ablation study subsection Sec. 5.3. Lastly, \\(Conv(\u00b7)\\) represents a sequence of a linear projection followed by a 2D convolution operation. For the loss function formulation, the fixation loss is defined as:\n\\(L_{fix} = KL(F_f^f, fix_{gt}) + CC(F_f^f, fix_{gt}),\\) (2)\nwhere \\(fix_{gt}\\) denotes the ground truth fixation data collected from volunteers [28]. The overall fixation prediction loss is a composite of the Kullback-Leibler (KL) divergence loss and the correlation coefficient (CC) loss, in alignment with standard practices in fixation prediction networks [41].\n4.3 Attributes' Contribution Prediction\nWe conceptualize the attributes' contribution predicting process as a transformation from high to low dimensions, effectively acting as a dimensionality reduction technique. To achieve this, we employ linear projection complemented by normalization and dropout strategies to enhance training robustness. Specifically, given \\(F_i^v\\) as input, the attribute prediction \\(F_a\\) is formally represented as:\n\\(F_a = Linear(BRD(Linear(Cat[LN(F_n^v)]_{n=0,1,2}]))\\), (3)\nHere, \\(Linear(\u00b7)\\) refers to linear projection, while \\(BRD(\u00b7)\\) signifies the sequential integration of Batch Normalization, ReLU, and Dropout operations.\nTo quantify the discrepancy between the actual camouflage attribute contributions and their predictions, the Mean Square Error (MSE) loss is employed for optimization purposes while \\(attr_{gt}\\) indicates the labeled contribute proportion:\n\\(L_{attr} = MSE(F_a, attr_{gt}).\\) (4)\n4.4 Attributes-Fixation Embedding\nTo effectively leverage attribute information and fixation maps obtained from the fixation and attribute decoders, we introduce the Attributes-Fixation Embedding (AFE) approach as depicted in Fig. 4. This method incorporates these elements as supplementary priors alongside raw CLIP visual features. Specifically, the visual features are processed through three distinct branches. In each branch, following the linear projection, the resulting features are directed to their respective gating mechanisms. Acknowledging the potential interrelations among attributes and the necessity for channel-wise feature recalibration, we adopt the Squeeze-and-Excitation (SE) mechanism [16] to facilitate the fusion of attribute information. Subsequently, the fixation map \\(F_f^f\\) is employed as a biologically interpretable attention mechanism to augment the features within each branch. Furthermore, to prioritize the features from deeper ViT layers, which exhibit finer granularity after successive layers, weights are assigned to each branch prior to summation. Ultimately, the AFE feature \\(F'\\) is generated, followed by a Layer Normalization operation. The entire AFE process is formalized as follows:\n\\(F_i^{v+a} = Gate(Linear(F_i^v), F_a) + F_i^v\\), (5)\n\\(F_i^{v+a+f} = Mul(F_i^{v+a}, Softmax(Linear(F_f^f))),\\) (6)\n\\(F' = LN(\\frac{1}{M} \\sum_{i=0}^{2} Mul(W_i, F_i^{v+a+f})).\\) (7)\nHere, i denotes the i-th branch and \\(F_i^{v+a}\\) represents the feature enhanced with attribute information, \\(F_i^{v+a+f}\\) signifies the feature further refined by the fixation map, and \\(F'\\) is the outcome of the AFE process. The \\(Gate(\u00b7)\\) operation employs the SE mechanism to integrate attribute information and the visual feature. \\(Mul()\\) denotes element-wise multiplication, and \\(\\sum (\\cdot)\\) signifies element-wise addition. The weights \\(W_i\\) are assigned with values of 1, 2, and 4 for \\(W_0\\) to \\(W_2\\) respectively, and \\(M = \\sum_{i=0}^{2} W_i\\) serves as the normalization constant.\n4.5 Mask Predicting\nAfter acquiring the visual feature with embedded camouflage attribute and fixation information, represented as \\(F'\\), we proceed to utilize a general transformer decoder and an output projector to unveil the final camouflage object mask \\(M_p\\). The unveiling process of this mask can be formulated as:\n\\(M_p = Conv2d(CBR(Decoder(F')\u00d7M)up\u2084),\\) (8)\nwhere \\(Conv2d(\u00b7)\\) denotes the 2D convolution, and \\(CBR(\u00b7)\\) signifies the sequential layers of Convolution, Batch Normalization, and ReLU. \\(Decoder(\u00b7)\u00d7M\\) represents the transformer decoder executed with M iterations, discussed in Sec. 5.3. The subscript up4 indicates the 4 times upsampling operation.\nMoreover, the loss function \\(L_{mask}\\) is constructed from the weighted binary cross entropy (wBCE) loss and the weighted Intersection over Union (wIoU) loss, adhering to conventional practices [30,46]:\n\\(L_{mask} = L_{BCE} + L_{IOU}\\) (9)\n4.6 Total Loss Function\nTo enhance the use of high-level, condensed textual information obtained from the CLIP text encoder, we propose a novel consistency measurement mechanism aimed at monitoring the manipulation of visual features throughout the training phase. We have developed two distinct projectors to map both the overall description feature, denoted as \\(F_t\\), and the refined visual feature, denoted as \\(F'\\), into a unified latent feature space. The feature \\(F_s\\) is obtained from the output of the CLIP text encoder, \\(F_s\\). Considering that both features relate to the identical camouflage image, they should demonstrate consistency in this latent space. To measure this consistency, we employ a consistency loss, \\(L_{consist}\\), defined as:\n\\(L_{consist} = CS(Proj(F')v, Proj(F)t),\\) (10)\nwhere \\(CS(.)\\) denotes the cosine similarity loss. \\(Proj(\u00b7)t\\) and \\(Proj(\u00b7)_v\\) represent the projectors for mapping into the latent feature space. The total loss function is formulated as follows, where \u03b1, \u03b2, and \u03b3 serve as the balancing weights:\n\\(L_{total} = L_{mask} + \u03b1L_{fix} + \u03b2L_{attr} + \u03b3L_{consist}.\\) (11)"}, {"title": "5 Experiments", "content": "5.1 Experimental Settings\nDataset: In accordance with the protocol [8] and building upon the experimental framework [30], we employ a combined dataset for training, comprising CAMO-train [21] and COD10K-train [8], totaling 4040 images. For evaluation, we utilize the CAMO, COD10K, and NC4K datasets [29], containing 250, 2026, and 4121 images, respectively.\nEvaluation Metrics: The evaluation of network performance during training utilizes four widely recognized metrics: structure-measure (\\(S_a\\)) [6], weighted F-measure (\\(F_\u03c9^b\\)) [32], mean enhanced-alignment measure (\\(E_\u03d5\\)) [7], and mean absolute error (M).\nImplementation Details: The training and testing procedures are conducted using PyTorch on NVIDIA RTX 8000 GPUs. We uniformly resize input images to 336 \u00d7 336 pixels to comply with the requirements of the pretrained CLIP model, ViT-L@336. The optimization process employs the Adam algorithm [20], accompanied by a multi-step learning rate schedule. The initial learning rate is established at \\(1e^{-4}\\), with a decay factor of 0.2 applied following 150 epochs. Completing the training regimen over 200 epochs takes approximately 16 hours on four NVIDIA RTX 8000 GPUs.\n5.2 Comparing with SOTA methods\nQualitative Results. The results presented in Figure 5 highlight the superior predictive performance of our proposed ACUMEN, when compared to SOTA methods in a variety of scenarios. Notably, in underwater settings as shown in the top row of the figure, other methods often fail to accurately identify two mimicry seahorses, suffering from issues like partial detection, edge blurring, and incorrect counts. In contrast, ACUMEN achieves remarkable visual clarity, leading to more accurate and comprehensive predictions. Furthermore, in terrestrial environments, illustrated in the third row, our method excels at precisely detecting the fine limbs of the stick insect, distinguishing them from dead branches without the issues of blurring or ambiguity. This enhanced ability to discern the structure of objects is primarily attributed to the integration of high-level textual information, introducing object structure priors into the supervision of the visualization feature extraction process.\nQuantitative Results. The effectiveness of ACUMEN was evaluated by comparison with SOTA methods across three datasets using four evaluation metrics. As shown in Tab. 1, ACUMEN consistently surpasses competing methods in all datasets, demonstrating significantly superior performance. Specifically, within the CAMO dataset, ACUMEN significantly outshines other methods in all metrics, with \\(S_a\\) and \\(F_\u03c9^b\\) scores of 0.886 and 0.850, respectively, which are 3.5% and 5.5% better than the second best method. In the NC4K dataset, although ACUMEN'S \\(S_a\\) and M scores are on par with FSPNet [18], its \\(E_\u03d5\\) and \\(F_\u03c9^b\\) scores are notably higher. Importantly, ACUMEN achieves SOTA performance while utilizing the smallest input size among its counterparts. Considering the significant advantages that larger inputs can provide in enhancing COS performance [17], ACUMEN shows potential for even greater effectiveness with equivalent input resolutions.\n5.3 Ablation Study\nNumber of Decoder Layers. The discussion of hyperparameters is detailed in Tab. 2. Here, N and M denote the number of decoder layers in the fixation and mask prediction modules, respectively. Additionally, WL is the word length of the input description, with a default value of 77 in the CLIP textual encoder. Our analysis indicates that an N value of 3 and an M value of 1 substantially improve the performance of \\(F_\u03c9^b\\) over other configurations, while ensuring similar levels of performance metrics \\(E_\u03d5\\), \\(S_a\\), and M, as evidenced by the CAMO and COD10K datasets in Tab. 2. Moreover, our review of sentence lengths, as depicted in Fig. 1b, confirms that they do not exceed 50 words. This observation supports our decision to reduce the default word length from 77 to minimize blank input.\n5.5 Discussion\nAttributions' Contribution Among Different Datasets. In Fig. 7, we present an analysis of camouflage patterns across various testing datasets, using histogram bars to represent the proportional contribution of our proposed attributes and error bars to indicate the standard deviation. A comparison of mean values reveals that the COD10K and NC4K datasets predominantly feature Environmental Pattern Matching, Shape Mimicry, and Environmental Textures, which together account for over 50% of their camouflage effectiveness. These attributes are crucial across all datasets, though their contributions vary. For instance, Shape Mimicry accounts for more than 15% of camouflage success in both COD10K and NC4K, but less than 15% in CAMO. We also notice that the mean distribution patterns of COD10K and NC4K are remarkably similar, reflecting their large and comparable sample sizes (2026 and 4040, respectively) that exhibit consistent camouflage patterns. In contrast, the CAMO dataset, with only 250 images, shows a mean distribution more prone to anomalies. For example, the standard deviation for Environmental Pattern Matching in CAMO is 0.0279, significantly higher than in COD10K (0.0225) and NC4K (0.0238). Additionally, the Low Resolution attribute in CAMO has a notably higher mean and standard deviation, indicating a higher prevalence of low-resolution images, which likely affects its MSE performance (0.0389) compared to COD10K (0.02592) and NC4K (0.03592) using same inference method. This trend is corroborated by other methodologies as shown in Tab. 1, further substantiating our findings. Further discussion including failure cases analysis could be found in Supplemental Materials."}, {"title": "6 Conclusion", "content": "In this paper, we presented a pioneering study on the role of camouflage attributes in determining the effectiveness of camouflage patterns, alongside the introduction of the COD-TAX dataset for comprehensive analysis. We also introduce the ACUMEN framework, which uniquely integrates textual and visual data for enhancing COS performance. Our findings, underscored by ACUMEN's superior performance over existing methods, highlight the significance of attribute analysis in camouflage designing and breaking."}]}