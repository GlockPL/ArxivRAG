{"title": "Revolutionizing Large Language Model Training through Dynamic Parameter Adjustment", "authors": ["Kaiye Zhou", "Shucheng Wang"], "abstract": "In the era of large language models, the demand for efficient use of computational resources has become critically important. Although parameter-efficient fine-tuning techniques have achieved results comparable to full fine-tuning, their application during the pre-training phase poses significant challenges. Specifically, employing parameter-efficient strategies at the onset of pre-training can severely compromise efficiency, especially in larger models. In this paper, building upon the fine-tuning method LoRA, we introduce a novel parameter-efficient training technique that frequently alters trainable part of parameters, facilitating effective pre-training. Our method not only achieves memory reductions and computational overhead comparable to current state-of-the-art parameter-efficient algorithms during the pre-training phase but also maintains accuracy levels comparable to those of full pre-training. We provide both theoretical analyses and empirical evidence to demonstrate the effectiveness of our approach.", "sections": [{"title": "1 Introduction", "content": "In recent years, the size of large language models (LLMs) has increased rapidly, a trend ignited by the advent of the transformer architecture [33]. To support the training of increasingly large models, various distributed training techniques have been employed. These include data parallelism [4, 19], the Zero Redundancy Optimizer [27] to optimize data splitting, tensor parallelism [28], and pipeline parallelism [16, 26] to partition the model itself. Despite these advancements, hardware resources are increasingly becoming a bottleneck as model sizes continue to expand [26]. For models exceeding one trillion parameters, the predominant approach is 3D parallelism, which integrates data, tensor, and pipeline parallelism. Presently, a significant portion of the inter-node communication overhead is attributed to data parallelism, required by the frequent communication of large quantities of parameter gradients.\nTo address these challenges, various parameter-efficient strategies have been proposed. Techniques such as model sparsification [1, 29] and progressive model pruning during training [7] have shown promise. Additionally, methods leveraging Singular Value Decomposition (SVD) to approximate full-rank matrices in low-rank spaces have been explored [30, 34, 38, 39].\nBeyond the entire training process, several techniques focus on the fine-tuning phase to enhance adaptability and efficiency. For instance, the Adapter method [14, 13] introduces additional trainable layers while freezing the remaining parameters. Similarly, prefix-tuning [20] employs a small set of trainable vectors (prefixes) at the beginning of each transformer layer to enable adaptation to new tasks without altering the core pre-trained model parameters.\nAnother noteworthy fine-tuning strategy is Low-Rank Adaptation (LoRA) [15], which has attracted attention for its effectiveness and the fact that it introduces no additional computational overhead during inference. However, it is important to note that while these parameter-efficient methods like LORA are highly effective for fine-tuning, directly applying them during the pre-training phase can lead to a considerable loss in model accuracy as observed in many prior works [34, 35, 24]. We hypothesize that this loss of accuracy is due to the premature use of low-rank training, which may cause the model to become trapped in a local minimum. In response, we aim to develop a strategy"}, {"title": "2 Methodology", "content": "A substantial body of research, such as various pruning methods [10, 3], has demonstrated that neural networks tend to exhibit low-rank characteristics after certain stages of training. Techniques for parameter-efficient fine-tuning, such as LoRA, capitalize on this observation. Concurrently, studies like [23, 9] have revealed that overparameterization in neural networks can lead to implicit regularization, thereby enhancing generalization. These findings underscore the importance of training with full parameters during the initial phase. Further empirical evidence supporting this phenomenon is provided in works like [34, 35, 24, 39]. Based on these insights, this section proposes a method designed to train a substantial number of parameters, while selectively updating only a portion of the parameters at any one time to conserve computational resources.\nLow-Rank Adaptation (LoRA) Introduced in [15], LoRA is designed specifically for the finetuning stage of model training.\nConsider a pre-trained model with a weight matrix $W \\in \\mathbb{R}^{m\\times n}$ from a specific linear layer. LoRA proposes an innovative modification: transforming $W$ into $W + BA$. Here, $B \\in \\mathbb{R}^{m\\times r}$ and"}, {"title": null, "content": "$A \\in \\mathbb{R}^{r\\times n}$ are newly introduced matrices, where $r$ is significantly smaller than both $m$ and $n$. Then during fine-tuning, $W$ is kept frozen while matrices $B$ and $A$ are trained. At the inference stage, $BA$ is added to $W$ which preserves the model's original structure. The matrix $A$ is initialized using Kaiming initialization [12], while $B$ is initially set to a zero matrix to ensure consistency.\nSwiLoRA training process Now, let us delve deeper into the linear system $(W + BA)x = y$. As illustrated in Figure 1, we decompose the matrix $B$ into its column vectors $b_k \\in \\mathbb{R}^{m\\times 1}$ for $k = 1,..., r$, represented as $B = [b_1,..., b_r]$. Similarly, we decompose matrix $A$ into its row vectors $a_k^T \\in \\mathbb{R}^{n\\times 1}$ for $k = 1,..., r$, leading to $A^T = [a_1,...,a_r]$. Hereafter, we call these vectors $b_k$ and $a_k$ as LoRA vectors.\nThe product $BA$ can be expressed using these LoRA vectors as follows:\n$BA = \\sum_{k=1}^r b_k a_k^T$                                                                                                                                   (1)\nLet $C(B)$ denote an ordered set containing $\\text{min}(m, n)$ vectors, each having the same dimensions as $b_k$. Furthermore, ensure that $\\{b_1, ..., b_r\\} \\subset C(B)$. Similarly, define $C(A^T)$ as an ordered set containing $\\text{min}(m, n)$ vectors, each having the same dimensions as $a_k$. Also, let $\\{a_1, ..., a_r\\} \\subset C(A^T)$. Moving forward, we will refer to $C(B)$ and $C(A^T)$ as the candidate vectors for $B$ and $A$, respectively.\nIt is known for $k$ matrices $W_1, W_2,..., W_k$, there holds\n$\\text{rank}(\\sum_{i=1}^k W_i) \\leq \\sum_{i=1}^k \\text{rank}(W_i)$.\n(2)\nIf we adopt the strategy in LoRA to add $BA$ to $W$ and only update $B$ and $A$ from the pre-training stage, according to (2), the rank of updated parameters of the local linear system through the entire training process will be limited to $2r$. This limitation can potentially impede the training efficacy. To mitigate this issue, we alter the values of $b_k$ and $a_k$ to $b \\in C(B)$ and $a \\in C(A^T)$ at appropriate frequencies, respectively, with these new values randomly selected from predefined candidate vectors list $C(B)$ and $C(A^T)$(one of $b'$ or $a'$ can be the same as $b_k$ or $a_k$). To maintain the consistency of the model's output, compensatory adjustments are made to $W$. To be more precise, when $b_k$ and $a_k$ are updated to be $b'$ and $a'$, we accordingly adjust $W$ with the equation $W \\leftarrow W + b'a'^T - b_k a_k^T$.\nWhen implementing these updates, the updated parameters of both $B$ and $A$ are derived from $\\text{min}(m, n)$ distinct candidate vectors, which ensures updated parameters are full-rank. Readers can refer to [24, 41] for more details.\nAs previously mentioned, the model initially possesses full rank, and the rank of each layer decreases progressively over time. Consequently, we have adopted an exponential decay function for the switching frequency, where the coefficients are determined empirically. The selection of LoRA rank $r$ for $BA$ is influenced by the final rank of the layers post-training, which has been extensively explored in LoRA and its variants [32, 37].\nCurrently Large Language Models (LLMs) predominantly utilize Adam [18] and AdamW [25] optimizers over SGD, which rely on optimizer states. It is crucial to note that after switching LoRA vectors, the gradients associated with these parameters are also changed, which prevents the reuse of optimizer states. To address this issue, when $a_k$ is switched, we reset the optimizer states of $b_k$. And conversely, when $b_k$ is switched, we reset optimizer states of $a_k$. Note that we reset optimizer states of counterpart pair rather than optimizer states of the switched parameters itself. This approach will be further explained in Section 3. Additionally, when the optimizer states are reset to zero, we freeze corresponding parameters for $N$ steps to maintain the robustness of the training. In this study, $N$ is set to 5.\nThe above process is described in Algorithm 1 and Algorithm 2."}, {"title": "3 Theoretical analysis", "content": "In this section, we conduct a thorough discussion of our algorithm and address four key aspects:\n1. Demonstrating that the order of LoRA vectors does not impact performance;"}, {"title": null, "content": "2. The effectiveness of our algorithm;\n3. Discussion on resetting optimizer states;\n4. Detailed process to deduce the values for initialization.\nFirst, we take a closer look at the properties of the local linear system. Assume that the loss function of the model is denoted by $\\mathcal{L}$. Our discussion focuses on the scenario where the input $x$ and output $y$ are vectors, satisfying the equation:\n$y = (W + BA)x$.\n(5)\nNext, we calculate the gradients of the column vectors of $B$. Recall the decomposition of $BA$ as defined in the previous equations. For $k = 1,...,r$, the gradient of $b_k$ with respect to the loss function $\\mathcal{L}$ is given by:\n$\\nabla_{b_k} \\mathcal{L} = (a_k^T x) \\nabla_y \\mathcal{L}$.\n(6)\nNote that when the input $x$ is a vector, $a_k^T x$ becomes a scalar. Consequently, the gradients of $b_k$ are proportional to the gradients of $y$.\nWe can also derive the gradients of the row vectors of $A$ as follows:\n$\\nabla_{a_k^T} \\mathcal{L} = ((\\nabla_y \\mathcal{L})^T b_k) x$.\n(7)\nIn this expression, $(\\nabla_y \\mathcal{L})^T b_k$ is a scalar, indicating that the gradients of $a_k$ are aligned in the direction of the input activations.\nIndependence of vectors updating In our algorithm, we randomly select candidate vectors to replace vectors in A and B, which alters the matching pairs of $b_k$ and $a_k$. A natural question arises: Does the matching order of these vector pairs influence the training effects?\nIn the following discussion, we will use the notation $\\bar{v}$ to denote trainable parameters that are initialized with the value of $v$.\nFor the sake of clarity, we focus on one linear layer without a bias term for our discussion. We denote $\\mathcal{L}(Wx)$ as the loss when the weight matrix of the linear layer under study is $W$, with the vector $x$ as input activations. This formulation intentionally omits contributions from other layers and the bias term, as they are beyond the scope of our subsequent analysis.\nTo integrate the LoRA matrices while preserving the initial loss value, we reformulate $\\mathcal{L}(Wx)$ as $\\mathcal{L}((W - \\sum_k b_k a_k^T + \\sum_k b_k a_k^T)x)$. Further, we simplify this expression to $\\mathcal{L}(a_1,\\dots, a_r; b_1,\\dots,b_k; x)$. A simple observation is\n$\\mathcal{L}(a_1,..., a_r; b_1,\\dots,b_k; x) = \\mathcal{L}(\\bar{0},...,\\bar{0};\\bar{0}, \\dots, \\bar{0}; x)$.\n(8)\nRecall that the gradient $\\nabla_{b_k} \\mathcal{L} = (a_k^T x) \\nabla_y \\mathcal{L}$. We derive the following expression:\n$\\Delta b_k a'_k = (c(a_k^T x) \\nabla_y \\mathcal{L} + \\text{opt}\\_\\text{state}(b_k))a'_k$,\n(9)\nwhere $c$ is a negative value from optimizer and $\\text{opt}\\_\\text{state}(b_k)$ is optimizer state of $b_k$, determined by the value of $(a_k^T x) \\nabla_y \\mathcal{L}$ of previous steps. Moreover, the value of $\\nabla_y \\mathcal{L}$ will remain unchanged, as indicated by (8). Consequently, the component $\\Delta b_k a'_k$ is influenced solely by $a'_k$ and not by other LoRA vectors. Similarly, the value of $b'_k a'_k$ is influenced only by $b'_k$ when switching $a_k$. Note that the updated weight can be expressed as\n$(b_k + \\Delta b_k)(a_k^T + \\Delta a_k^T) - b_k a_k^T = \\Delta b_k a_k^T + b_k \\Delta a_k^T + \\Delta b_k \\Delta a_k^T,$\n(10)\nwhere $\\Delta b_k \\Delta a_k^T$ represents a minor term that can generally be disregarded. Hence, the updates derived by $b_k$ and $a_k$ are independent.\nFrom this discussion, we can conclude that the order of vectors $a_k$ and $b_k$ does not influence the parameter updates in the current step. For instance, for $1 < i,j < r$, back propagation of $\\mathcal{L}(a_1,..., a_j,..., a_i, ..., a_r; b_1,\\dots,b_k; x)$ and $\\mathcal{L}(a_1,..., a_i,..., a_j, ..., a_r; b_1,\\dots,b_k; x)$ yield the same parameters updating to the weight matrix of the linear layer."}, {"title": null, "content": "Effectiveness of SwiLoRA Consider the following modification to the original model. For the weight matrix $W \\in \\mathbb{R}^{m\\times n}$ of a specific linear layer in the model, replace $W$ with the product of matrices $B^\\circ A^\\circ$, where $B^\\circ \\in \\mathbb{R}^{m\\times\\text{min}(m,n)}$ and $A^\\circ \\in \\mathbb{R}^{\\text{min}(m,n)\\times n}$. This modification results in a full-rank weight matrix $B^\\circ A^\\circ$ and introduces more parameters than the original model. Consequently, it is anticipated to achieve results that are at least as good as those of the original model when the full parameters of this modified model are trained.\nWe now compare the modified model with another model that implements the SwiLoRA strategy. Define $B_i = C(B)[i]$ and $A_i = C(A^T)[i]^T$ for $i = 1, ..., \\text{min}(m, n)$. It becomes apparent that the two models are quite the same except that the model applying SwiLoRA strategy updates only subsets of parameters incrementally.\nIn optimization, it is well-established that for problems with separable objective functions, the parameters of each separable group can be optimized independently. Although the loss function of the SwiLoRA model is not separable, the preceding discussion has demonstrated the independence between the LoRA vectors. Consequently, we can infer that the inseparable components of the loss function concerning parameters within the same linear layer are modest. Therefore, this suggests that training subsets of parameters incrementally, as in the SwiLoRA model, is likely more effective than other methods, such as the layer-wise training approach proposed in [2].\nReset of optimizer states Let us discuss whether it is reasonable to zero out the optimizer states of LORA vectors and temporarily freezing them when switching their counterpart LoRA vectors.\nConsider a scenario where $b_k$ is switched while $a_k$ is not. Note that, according to (8), the forward propagation remains unaffected after the switching occurs. During the initial step after switching $b_k$, with $a_k$ being frozen, the only term contributing to the weight matrix update is $\\Delta b_k a_k^T$ according to (10). We previously established that this term, $\\Delta b_k a'_k$ in (9), is not influenced by other LoRA vectors apart from $a_k$. Consequently, changes made to $b_k$ or any other recently switched LORA vectors do not impact the accuracy of the optimizer states for $b_k$. This substantiates the rationale behind resetting the optimizer states.\nIf we choose not to freeze $a_k$, we derive the following from a similar equation to (9):\n$b'_k a'_k = c((\\nabla_y \\mathcal{L})^T b_k)x + b'_k \\text{opt}\\_\\text{state}(a_k)$.\n(11)\nThis formula demonstrates that without resetting $a_k$, the update direction would be completely incorrect.\nThe reasoning for switching $a_k$ and its implications can be deduced in a similar manner.\nDerivation of initialization Parameters We have provided the initial values of B and A in Section 2. Below, we present the deduction process.\nThe initial values of B and A were specified in Section 2. In this section, we present the derivation process.\nAs demonstrated in [12], consider two matrices, $W_1$ and $W_2$, both characterized by zero mean and uniform distribution. The standard deviation (std) of the elements of their product is given by:\n$\\text{std}[W_1 W_2] = \\sqrt{k} \\text{std}[W_1] \\text{std}[W_2]$,\n(12)\nwhere $k$ represents the output dimension of the matrix $W_1$. To ensure the stability of forward propagation, it is crucial that the output of each layer maintains a standard deviation of 1. However, when the matrix $W_2$ represents activation values, its standard deviation, denoted as $\\text{std}[W_2] = \\text{gain}$, differs from 1 due to the influence of the activation function. For ReLU activations, $\\text{gain} = \\sqrt{2}$. Following this principle, we derive:\n$\\frac{1}{r} \\text{std}[BAx] = \\frac{1}{r} \\sum \\text{std}[b] \\text{std}[a^T] \\sqrt{n} = \\text{gain}$.\n(13)\nThe standard deviation of the gradients for LoRA vectors is given by:\n$\\text{std}[\\nabla_{b_k} \\mathcal{L}] = \\sqrt{n} \\text{std}[a_k] \\text{std}[x] \\text{std}[\\nabla_y \\mathcal{L}]$,\n$\\text{std}[\\nabla_{a_k^T} \\mathcal{L}] = \\sqrt{m} \\text{std}[b_k] \\text{std}[x] \\text{std}[\\nabla_y \\mathcal{L}]$.\n(14)"}, {"title": null, "content": "Assuming the updated parameters are solely influenced by the gradients of the current step, to obtain $\\Delta BA \\sim B \\Delta A$, the following condition must be met:\n$\\text{std}[B \\nabla_y \\mathcal{L}] = \\text{std}[\\nabla_{b_k} A \\mathcal{L}]$.\n(15)\nFrom this, we derive:\n$\\text{std}[B \\nabla_y \\mathcal{L}] = \\sqrt{r} \\text{std}[\\nabla_{b_k} \\mathcal{L}] \\text{std}[A]$,\n$\\text{std}[B \\nabla_y \\mathcal{L}] = \\sqrt{r} \\text{std}[B] \\text{std}[\\nabla_{a_k^T} \\mathcal{L}]$.\n(16)\nBy combining (13)-(16), we achieve the following standard deviations:\n$\\text{std}(A) = (\\frac{m}{r \\sqrt{mn}}) \\text{gain}, \\quad \\text{std}(B) = (\\frac{n}{r \\sqrt{mn}}) \\text{gain}$.\n(17)"}, {"title": "4 Experiments", "content": "We designed our experiments based on the settings described in [24] to benefit from established hyperparameter configurations. Our studies are carried out on the LLaMA model [31], with model sizes reduced to 130M, 250M, and 350M. The specific hyperparameters for these models are detailed in Table 1. We use Adam optimizer to train the model with $\\beta_1$ = 0.9,$\\text{beta}_2$ = 0.999. We use a cosine learning rate schedule with 100 warm-up steps and a total of 40,000 training steps. We employ the Adam optimizer for training, with $\\beta_1$ = 0.9 and $\\beta_2$ = 0.999. The experiments utilize the C4 dataset [6], with the first 46M samples of the training dataset serving as our training data, and samples from the entire validation dataset used for testing. The evaluation of validation loss is performed on 10M tokens for all our experiments, with evaluations conducted every 1,000 steps. All experiments are conducted using 8xNVIDIA Tesla A100 80GB PCIe GPUs. Gradient accumulation is applied when GPU memory reaches its limit."}, {"title": "4.2 Experimental results", "content": "Figures 2 displays the experimental results for the 130M, 250M, and 350M models, respectively, with the LoRA rank set to 128. The data reveal that while LoRA alone does not yield satisfactory training results, SwiLoRA approaches the performance of full-rank training. The performance gap continues to grow as model size increases. This suggests that the low-rank training approach, such as LoRA, might cause models to become trapped in local minima, while SwiLoRA mitigates this issue by dynamically changing trainable parameters."}, {"title": "5 Limitations and future work", "content": "We hypothesize that the LoRA ranks specified in [24] are sufficient for training to achieve the same loss as full-rank training when the trainable parameters are frequently changed. However, experiments have demonstrated that the accuracy does not match that of full-rank training if the LORA rank is not large enough. Besides, tuning the switching frequency of the LoRA vectors' curve is challenging. To address these limitations, we propose the following directions for future work, as illustrated in Figure 4.\n\u2022 In our experiments, we simply used exponentially decreasing switching frequencies, which may not be the optimal approach. Conducting extensive experiments to determine the best frequencies across all steps is nearly impossible. Therefore, guidelines should be developed to help set appropriate switching frequencies throughout the training process.\n\u2022 Going further, a more detailed idea is to examine each layer of the model to adjust the switching frequencies. For instance, LoRA-drop [40] evaluates whether the rank is sufficient using a norm of $\\Delta Wx$. This is rational because different types of layers, such as the $Q$, $K$, $V$ matrices in transformer layers, exhibit significantly varied behaviors. Similarly, shallow and deep layers respond differently.\n\u2022 In our work, we simply chose candidate vectors at random. However, during training, all candidates are updated separately, leading to significant difference among them. The selection of these candidates may influence the training outcomes. Thanks to the relatively long switching interval(exceeding 40 steps in our experiments), it is feasible to perform some time-consuming operations to determine the weights of the candidates for selection."}, {"title": "6 Conclusions", "content": "In this work, we introduce SwiLoRA, a novel training strategy designed for parameter-efficient pre-training. Our approach achieves comparable accuracy to full-rank training while reducing the trainable parameters to approximately 50% to 60% of those in traditional full-rank training throughout the entire process. Moreover, the computational overhead and memory usage are nearly identical to those of LoRA when using the same number of trainable parameters. Additionally, we provide a theoretical analysis of our algorithm to confirm its correctness and to offer insights into its training methodology. The empirical results and theoretical analysis support our perspective on training: One can keep model parameters unchanged while change the part of trainable parameters during the training dynamically to achieve efficacy similar to full training."}, {"title": "A Related work", "content": "Similar to our approach, various LoRA variants employ strategies to increase the rank of a matrix by integrating weights into W. For instance, ReLoRA [24] merges BA into W and then restarts training at regular intervals. However, resetting all LoRA adapter parameters can lead to unstable training. To address this, ReLoRA introduces random optimizer pruning and a jagged schedule, but it still requires 33% of the steps to be full-rank training. Delta-LoRA [41], another variant, focuses on the fine-tuning phase. It updates the matrix W by using the gradients of the LoRA matrices B and A simultaneously as they are updated. It improve accuracy for fine-tuning. However, it appears to be insufficient in increasing the rank of the modified parameters.\nAn attractive feature of LoRA is its support for quantization without loss of accuracy. This is achieved by training the LoRA adapter matrices BA with high precision, while keeping the frozen part W at low precision within transformer layers. Examples of such quantization include QLORA [5], LoftQ [21], and L4Q [17]. Additionally, the integration of quantization into SwiLoRA presents an intriguing topic. The feasibility of this integration is discussed in Appendix D.\nOn the other hand, LoRA and SwiLoRA are closely connected to SVD methods, although they do not require performing singular value decomposition. In fact, some studies utilize SVD to identify the most significant components of the decomposition of BA. For instance, earlier work by Pufferfish [34] and subsequent work in Cuttlefish[35] introduce adaptive strategies to determine the necessary steps for full-rank training and to select the rank of each linear layer for low-rank decomposition. InRank [38] develops a low-rank training approach based on greedy low-rank learning [22]. Another recent study, GaLore [39], employs SVD to apply a low-rank structure for approximating gradients, offering greater memory efficiency compared to LoRA. While these works are promising on pre-training, they require the computation of SVD, which has a time complexity of $O(n^3)$ for $n \\times n$ matrices. This computational demand becomes prohibitive for models with billions of parameters."}, {"title": "B Implementation of LoRA vector switching", "content": "The primary distinction in the implementation of SwiLoRA from conventional approaches lies in its handling of gradients and optimizer states at the granularity of row or column vectors within matrix parameters. Consider the scenario when using the AdamW optimizer: typically, each trainable parameter group in AdamW is associated with a \u201cstep\u201d state which is implemented as a float scalar value in the code. To facilitate the resetting of specific rows or columns in matrices, we modify the type of \"step\" in the optimizer to a 32-bit float matrix with the same shape as the corresponding parameters. In fact, this modification does incur some extra memory overhead. An alternative approach would be to implement \u201cstep\u201d as a row vector for A and a column vector for B. However, this would require more complex code management, and thus, we have not adopted this strategy in our implementation.\nWith the capability to manipulate optimizer states and gradients at the level of rows and columns, we can now execute operations such as resetting optimizer states and freezing specific rows or columns of parameter matrices.\nAdditionally, note that all candidate vectors are stored in GPU memory, which consumes a modest amount of GPU resources. This portion of memory can be reduced by offloading the candidate vectors that are not actively in use to the CPU."}, {"title": "C Ablation study", "content": "In this section, we mainly use the 130M model with a LoRA rank of 128 and a batch size of 128. For hyperparameters not explicitly mentioned, we follow the configurations detailed in Section 4.\nIn Figure 5, we did two experiments. In the first experiment, we evaluate the model's performance with varying descent rates for frequencies while maintaining a constant initial switching interval of 40. In the second experiment, we maintain a consistent descent rate for frequencies as detailed in Section 4, but we vary the initial switching interval across different experiments. It is evident from our results that both hyperparameters significantly impact training accuracy."}, {"title": "D Feasibility of quantization", "content": "Among all the LoRA variants that can be integrated into SwiLoRA, a quantization method such as QLORA shows the most promise in enhancing efficiency and reducing memory usage. However, implementing quantization without significant accuracy loss is not straightforward. Given that LoRA vectors are frequently switched in SwiLoRA, the errors introduced by quantization can significantly impact the algorithm's performance. For instance, when merging delta parameters triggered by the switching of LoRA vectors, the operation can be expressed as:\nW$\\leftarrow \\text{quant}(\\text{dequant}(W) + b'_k a'^T_k - b_k a^T_k)$.\n(18)"}, {"title": null, "content": "Here, precision errors may arise, with values approximately equal to\n$\\text{quant}(b'_k a'^T_k - b_k a^T_k) - (b'_k a'^T_k - b_k a^T_k)$.\n(19)\nActually, it is possible to slightly adjust the values of candidate vectors before their selection to minimize this error. However, developing an efficient algorithm that effectively reduces this error without compromising accuracy is challenging. Therefore, we plan to explore this issue in future work."}, {"title": "E Distribution of Singular values", "content": "Given that the rank distribution significantly influences the training efficacy of models [15, 7], we conducted experiments to examine the rank distribution of SwiLoRA. As outlined in Section 4, experiments were conducted on 350M model to analyze the rank distribution of linear layers after 40,000 training steps. Figure 9 demonstrates that the singular values of weight matrices converge within a limited range when trained with LoRA, indicating dominance of LoRA adapters in the linear layers. This dominance is expected, as the singular value distribution of weight matrices during the pre-training phase exhibits a form of illness, due to updates being limited to the low-rank adapter BA. In contrast, as illustrated in Figure 10, the rank distribution of SwiLoRA closely approximates that of full-rank training, suggesting a healthier and more effective adaptation process."}, {"title": "F Impact on distributed training", "content": "As demonstrated in [27], for a transformer model with $n$ layers and a hidden dimension of $h$, the memory required for model parameters scales proportionally with $nh^2$. Assuming these parameters are stored in fp16/bf16 format occupying $\\Psi$ parameters, the memory footprint for optimizer states would be approximately 124 bytes when using the Adam optimizer as stated in [27]. Additionally, when the batch size is $b$ and the sequence length is $s$, the memory consumption for activations scales with $bshn$. To manage memory demands for large models, gradient accumulation can be utilized to adjust the batch size per GPU to 1. Moreover, activation checkpointing can be implemented to reduce memory consumption, though it comes with a trade-off: a 33% increase in computational overhead.\nIn this work, we primarily focus on the memory consumption associated with optimizer states, which constitutes a significant portion of the overall memory usage for models with tens of billions of parameters. Assuming that full-rank training requires $knh^2$ bytes of memory, where $k$ is a constant. Our algorithm, as well as LoRA, reduces memory usage from $knh^2$ to $2knhr$, with $r$ representing the LORA rank.\nIn addition to memory usage, parameter-efficient training also reduces communication overhead. When implementing 3D parallelism to train large language models, tensor parallelism is typically limited within a single machine due to its substantial communication demands. Pipeline parallelism introduces some idle \"bubble\" time, which cannot be eliminated even with fast communication. And its communication overhead remains relatively low. The main part of inter-node communication stems from data parallelism, where the same amount of gradients as parameters is communicated at every training step. Consequently, having fewer trainable parameters can significantly decrease communication overhead. Moreover, reduced memory consumption allows a larger portion of the model to reside on a single GPU, potentially decreasing the degree of pipeline parallelism needed and consequently reducing the associated \"bubble\" time."}]}