{"title": "KWT-Tiny: RISC-V Accelerated, Embedded Keyword Spotting Transformer", "authors": ["Aness Al-Qawlaq", "Ajay Kumar M", "Deepu John"], "abstract": "This paper explores the adaptation of Transformer-based models for edge devices through the quantisation and hardware acceleration of the ARM Keyword Transformer (KWT) model on a RISC-V platform. The model was targeted to run on 64kB RAM in bare-metal C using a custom-developed edge AI library. KWT-1 was retrained to be 369 times smaller, with only a 10% loss in accuracy through reducing output classes from 35 to 2. The retraining and quantisation reduced model size from 2.42 MB to 1.65 kB. The integration of custom RISC-V instructions that accelerated GELU and SoftMax operations enabled a 5x speedup and thus ~5x power reduction in inference, with inference clock cycle counts decreasing from 26 million to 5.5 million clock cycles while incurring a small area overhead of approximately 29%. The results demonstrate a viable method for porting and accelerating Transformer-based models in low-power IoT devices.", "sections": [{"title": "I. INTRODUCTION", "content": "Since its conception in the famous landmark paper \"Attention is All You Need\" by Google in 2017 [6], the Transformer model has met and exceeded state-of-the-art performance in various applications; chief amongst which is natural language processing (NLP). When compared to convolutional neural networks (CNNs) and recurrent neural networks (RNNs) in the NLP field, the Transformer has exhibited better handling of long-range dependencies, superior semantic feature extraction abilities, as well as the ability to be very effectively parallelized [1]. This has directly led to the onset of popular large language models such as ChatGPT, which are transformer-based models with hundreds of billions of trainable parameters. The success of the Transformer model at NLP tasks led researchers to investigate the use of the Transformer models for applications that have traditionally been performed by the CNN architecture, such as Computer Vision [3], Keyword Spotting [4], and Gesture Detection [5].\nHowever, this comes at the expense of a large model size and increased computational cost. Therefore, many Transformer models are beyond the computational capabilities of resource-constrained hardware. This presents a challenge for the growing Internet of Things (IoT) movement which aims to enable cheap, resource-constrained devices to invoke cutting-edge models in a power-efficient manner. This problem is twofold \u2013 edge AI systems struggle with both the large inference-time memory requirements, as well as long execution times associated with performing the required operations. In order to solve this issue, current literature focuses on quantising the transformer model, as well as introducing custom hardware to accelerate inference.\nThere have been several different hardware acceleration approaches in literature. Some, like AI-RISC [13] and AIfES [14] target lower-power applications, but do not target the Transformer architecture specifically. Other approaches, such as FlexACC [15] and DaDianNao [2] focus on designing neural network processors which support many different machine learning architectures, outperforming general-purpose processors in speed and energy consumption. There have also been approaches that target the specific acceleration of the Transformer architecture. RISC-VTF [1] proposes acceleration through custom RISC-V instructions, and A\u00b3 [7] proposes acceleration of the attention mechanism through approximation.\nHowever, while current literature addresses the issue associated with compute time, they are typically addressed through the lens of high-power, high-performance systems without much focus on the memory-related challenges associated with pushing Transformers to be invoked on the edge. Additionally, they tend to emulate Transformer operations to measure the speedup as opposed to accelerating a real model. This makes it challenging to gauge the accuracy degradation due to hardware acceleration.\nOn the other hand, this paper involves a more holistic approach. A state-of-the-art keyword spotting transformer model is trained, modified, and quantised on a PC. Then, its inference operations are implemented in bare-metal C and compiled for a RISC-V platform. Lastly, the RISC-V platform is supplemented with custom hardware blocks, and the processor's instruction set is extended with a few custom instructions to accelerate the inference of the model.\nThis paper investigates the performance degradation of Transformers as they are downsized to fit on low-power devices. It also focuses on the general framework involved in training and accelerating Transformer-based models in the context of embedded systems, with ARM's Keyword Transformer (KWT) [4] as the key example. Additionally, the custom hardware and instructions are designed to be flexible, accelerating any Transformer-based model, including encoder-decoders, vision transformers, and BERT."}, {"title": "II. BACKGROUND: THE KEYWORD TRANSFORMER (KWT)", "content": "The Transformer is a novel machine learning architecture first proposed by Google in 2017 for NLP purposes, where Google first introduced the concept of self-attention [6]. Up until the creation of the Transformer, deep learning models for NLP uses were based on CNNs or RNNs [8]. These architectures, while powerful, relied on fixed-length representations of input sequences. CNNs and RNNs struggle with long-term dependencies due to the maximum path length between inputs and outputs being large. The shorter the paths between inputs and outputs in a model, the easier it is to learn long-range dependencies [9], improving performance. The attention mechanisms within the Transformer achieve this goal by reducing forward and backward signal path length.\nThe earlier mentioned KWT model is built on the Vision Transformer (ViT) architecture, which is a post-norm, encoder-only Transformer specifically built for computer vision tasks. It has exhibited 98.6% accuracy on the Google Speech Commands (GSC) dataset. As shown in Fig 1, a single inference pass of KWT starts with a conversion of the raw input audio signal to a Mel-scale spectrogram (X) with time windows t = 1, ..., T and frequencies f = 1, ..., F, also known as Mel-Frequency Cepstral Coefficients (MFCC). The next steps involve splitting the spectrogram $X \\in \\mathbb{R}^{T \\times F}$ into separate flattened time domain patches, on which a linear projection is applied. This linear projection maps the spectrogram to a higher dimension d using a projection matrix $W \\in \\mathbb{R}^{F \\times d}$. The positional embeddings $X_{pos} \\in \\mathbb{R}^{(T+1) \\times d}$ are then applied. The resulting patches are fed into the Transformer's l'th block (X1).\nThe signal X\u2081 is split into three vectors: Query (Q), Key (K), and Value (V). Next, the dot product of Q and K is found and normalised by $\\sqrt{d_n}$ where dn is the dimensionality of each attention head. Next, the SoftMax (2) operation is applied to the result, and then multiplied by the value vector. The result of the multiplication is the self-attention (SA) vector (1):\n$SA(X) = softmax(\\frac{QK^T}{\\sqrt{d_n}}) V$ (1)\nWhere:\n$softmax(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^K e^{x_j}}$ (2)\nAnd weights Wo, Wk and Wy are obtained through training:\n$Q = XW_Q, K = XW_K, V = XW_V$ (3)\nThe results from (1) with mean (\u03bc) and variance (\u03c3\u00b2) are then normalised as in (4):\n$\\overline{X_i} = \\frac{X_i-\\mu}{\\sqrt{\\sigma^2}}$ (4)\nAfter this step, pre-computed scale (y) and shift (\u03b2) are applied to each element $\\overline{X_i}$ to produce the final normalised result y; (5):\n$V_i = \\gamma_i \\cdot \\overline{X_i} + \\beta_i$ (5)\nThe normalised result is then passed through a multilayer perceptron (MLP) block. MLPs are a type of feedforward neural network with behaviour as defined in (6):\n$FFN(x) = GELU(xW_1 + b_1) W_2 + b_2$ (6)\nWhere, as in [10], $erf$ is the Gauss error function:\n$GELU(x) = x \\cdot \\frac{1}{2}(1 + erf(\\frac{x}{\\sqrt{2}}))$ (7)\nThe output from the MLP is normalised as in (5) and goes through a final linear mapping before the output class is produced as in (8).\n$Linear(x) = xW_L + b_L$ (8)\nAs can be seen, this process involves many matrix multiplications, SoftMax operations, and GELU calculations. Additionally, it can also be seen that intermediate results between layers must be freed from memory whenever no longer needed since it is very limited on IoT platforms."}, {"title": "III. KWT-TINY", "content": "The parameters of the KWT model which achieved the 96.9% accuracy figure are shown in Table I. As is evident, such a large number of parameters, each of which is a 32-bit floating point number, could not be loaded onto the few kB of RAM available in most low-power embedded systems. The embedded system being used in this paper is the lowRISC Ibex [17], which has the specifications listed in Table II.\nDue to the limited memory resources on this platform, a much smaller KWT-based model, KWT-tiny, had to be trained. Through an iterative approach, the layers with the least impact on inference accuracy were removed. These were found to be the depth layers, which are the sequential transformer layers where the output of one is fed into the input of the next. One transformer layer was found sufficient enough to support two output classes, as opposed to the 12 layers in the original KWT for 35 output classes.\nAs is evident in Table III, KWT-Tiny can only discern two output classes, meaning that it is ideal for the detection of a single keyword such as \"Hey Google\u201d or \u201cAlexa\", as opposed to KWT-1 which can discern between 35 different output classes in the GSC dataset.\nThe accuracy of KWT-tiny was tested on the full Google Speech Commands dataset, where the two output classes were \"dog\" and \"notdog\". The results of the accuracy of KWT-tiny as well as its parameter count when compared to KWT-1 are shown in Table IV.\nAs can be seen in Table IV, a 369x reduction in model size of KWT-1 resulted in a small ~10% reduction in model accuracy. This was largely due to the success of the iterative approach which correctly identified the least important layers and downsized them accordingly. This finding indicates that Transformer-based models can indeed retain high accuracy when scaled down for embedded applications if the correct layers are targeted. For KWT-Tiny, it was found that downsizing the MLP network as well as the depth of the transformer block presented the best accuracy-size trade-off, whereas overly downsizing the normalization vector led to steep accuracy loss."}, {"title": "IV. QUANTISING KWT-TINY", "content": "Quantising the KWT-Tiny model is important for two main reasons: (1) storing model weights in INT8 as opposed to FLOAT32 reduces model size by 4 times, and (2) performing mathematical operations using floating point emulation on embedded devices with no FPU is very expensive, especially when it comes to floating point division.\nKWT-Tiny-Q and its input MFCC were quantised through post-training static quantisation, whereby all model weights were multiplied by a static scale factor ($2^\\lambda$) and then quantised to INT8 as in (9).\n$W_{int} = floor(W_{float} \\times 2^\\lambda)$ (9)\nThe scale factor was chosen to be a power of 2 to make quantisation and dequantisation as cheap as possible on an embedded platform by using bit shifts.\nIntermediate residuals which would be the result of the integral multiplications of various INT8 weights are sized as INT16 to prevent too much data loss during inference. As per other transformer quantisation techniques, SoftMax and layer normalisation continued to be computed in a floating-point manner, as quantising those operations was found to be quite taxing on accuracy [12]. This would mean that the intermediate results are dequantised before being passed to the SoftMax and LayerNorm layers, the outputs of which would be re-quantised for the next layer and so on.\nTo decide the value of $2^\\lambda$, each audio input in the GSC dataset was fed into KWT-Tiny and also into different versions of KWT-Tiny-Q, each with different values of $2^\\lambda$ in (9). The accuracy of each version was then compared as in Table V. A scale factor too large would result in overflows in the intermediate results, and a scale factor value too small would reduce the precision of the quantised weights.\nAs is evident in Table V, the ideal approach involved a different scale factor for weights and inputs. This is because weights range from 0 to 1, whereas the MFCC input vector can contain elements with magnitude of a few hundred. This means that the weights can be scaled by a larger factor than the input while increasing precision and not contributing to overflows. KWT-Tiny-Q exhibited 25% of the memory consumption of KWT-Tiny with 82.5% accuracy, a loss of ~5%."}, {"title": "V. KWT-TINY IN BARE-METAL C", "content": "Current edge Al approaches for transferring models onto resource-constrained platforms involve the use of libraries such as TensorFlow Lite (TFLite), ONNX, or TVM. The KWT-1 model is available for usage on mobile phones with TFLite [4]. A challenge with these approaches is that the libraries themselves can be as large as a few MB. While this is no issue with modern smartphones with multiple GB of RAM, many embedded platforms would not be able to fit such libraries in memory. Another challenge with these approaches is that the embedded developer and computer architect are both abstracted away from the lower-level details of the operation of the model, which makes operation acceleration and memory optimisation for a particular platform very difficult.\nTo counteract these challenges, this paper proposes a simple C library for transformer-based computations. This approach maximises customisability and memory optimisation. The library is created to allow for both floating point-based computations for non-quantised models as well as INT16 operations for quantised models. The library should be compatible with most Transformer-based models. The downside with this approach is that the embedded engineer must be familiar with how to create the inference pipeline based on the provided library functions, but this should be a simpler task than developing the entire tensor library. To enable the computations in Fig. 1 and Fig. 2, the library described in Table VI was created.\nThe inference C code comprises two main sections: initialization, involving copying model hyperparameters and loading weight pointers; and the inference pipeline. However, fitting model parameters in memory alone doesn't guarantee sufficient runtime memory for calculations due to intermediate result storage. Efficient memory usage is crucial, with stack size calculated for maximal runtime needs. The linker must be configured to allocate stack memory accordingly. In this case, 60kB program memory and 4kB stack were allocated. Compiler size optimization using the \"-Os\" flag was necessary to fit KWT-Tiny on the Ibex platform.\nAdditionally, memory occupied by intermediate results no longer required for the next layers' input need to be cleared. In a bare metal system with no OS support, the use of the popular memory allocation function malloc() is typically not supported. To counteract this, a manual implementation of malloc() was devised, whereby two separate arrays of pre-defined size are allocated to act as global memory banks. Their sizes are found through dry-running the pipeline and ensuring that the maximal intermediate result's size fits within one of the banks. Two banks are required as there are cases in which two residual results are required simultaneously such that they can be added together as per the pipeline in Fig. 1. In the implementation for KWT-Tiny on C, the banks were of size SEQLEN \u00d7 MLP_DIM and SEQLEN \u00d7 DIM_HEAD \u00d7 3 respectively, with attributes as defined in Table III."}, {"title": "VI. HARDWARE ACCELERATION", "content": "RISC-V [16] is an open-source instruction set architecture (ISA) that is modular and extensible. It allows computer architects to extend its functionality through the addition of custom instructions which can be used to invoke custom hardware. RISC-V reserves space as per its standards for custom instructions [16]. RISC-V also comes with a mature software toolchain [19] including compilers, linkers, and assemblers. The processor in use, the lowRISC Ibex, is an open-source [17] parametrizable single-core CPU written in System Verilog and suited for low-power applications. It complies with the RV32IMC standard, which is the 32-bit version of RISC-V that supports basic integer arithmetic, fixed point multiplication, division, and a compressed instruction set extension which provides 16-bit compact instructions. It is synthesised on the Xilinx Arty-A735T FPGA. The acceleration of KWT-Tiny on a RISC-V platform can be obtained through the introduction of a few custom instructions that the processor can use to compute certain inference-time operations. Once these are introduced, they can be invoked directly using assembly language invocation through the \u2018asm' keyword as per the RISC-V GNU toolchain standards [19]. This approach means that the compiler toolchain does not need to be modified to detect the operations in question; allowing for more universally compilable code. The non-accelerated, non-quantised KWT-Tiny model was profiled to obtain the most compute-intensive operations during an inference run. The results are shown in Fig. 3, Fig. 4, and Fig. 5.\nThe GELU and SoftMax operations were found to be taxing in inference. As a result, an R-type RISC-V custom instruction (Fig. 6) is added to the Ibex decoder logic and is called directly in assembly in the C code. This custom instruction uses RISC-V's reserved \"custom-1\u201d opcode with value 7'b0101011 [16]. Once called, the decoded instruction would activate specially designed hardware blocks housed in the Ibex's modified ALU to accelerate GELU and SoftMax. GELU and Softmax operations were chosen to be accelerated as they are shared across many different deep neural network applications, not only Transformers.\nThe value of funct7 remains 0 in all cases, and funct3 is used to decide the behaviour performed by the custom instruction as per Table VII.\nTo accelerate the SoftMax operation, the floating point division operation is replaced by a simple lookup table with operator ALU_INVERT. To design hardware that can compute SoftMax on a fixed-point basis, there needs to be a constrained range over which the input xi can vary. A typical way this has been avoided was through normalizing the SoftMax as follows [18], which obtains the same result as in (2):\n$softmax(\\mathbf{x})_i = \\frac{e^{max(x)-x_i}}{\\sum_{j=1}^K e^{max(x)-x_j}}$ (10)\nThis means that a lookup table can also be used to approximate the exp() through ALU_EXP. The C code would loop through a vector X, calculate ALU_EXP(Xi) for each element, accumulate the sum, and then use ALU_INVERT(sum), then multiply the results to obtain the final SoftMax result. It was found that all values of $e^{max(x)-x_i}$ lie between 0 and 10. It was also found that 32 divisions per unit struck a fair balance between accuracy and ROM size to be taken up. As such, the lookup tables for ALU_EXP and ALU_INV were chosen to be 320 elements of 32-bit numbers each, which is a total size of 2.56kB of ROM. The lookup tables (LUT) were indexed respectively as in (11) and (12):\n$LUT_1 [z * 32] \\approx e^{\\frac{1}{z}}$ (11)\n$LUT_2 [(z * 32) - 1] = \\frac{1}{z}$ (12)\nAs is clear in Fig. 7, GELU can be approximated very effectively through a simple piecewise approximation: if x > 1.595, then GELU(x) = x. If x <-1.857, then GELU(x) \u2248 0. If-1.857 \u2264 x \u2264 1.595, then $GELU(x)$ can be approximated by a 32-element lookup table as defined in (13):\n$LUT_3[x] \\approx GELU(x)$ (13)\nThe choice of the thresholds was done through a gradient descent computation that showed that this was the near-optimal choice for a 32-element LUT, with a quoted accuracy degradation of only 0.0042%. This means that the total ROM consumption is 2.69 kB for all LUTs. The results of hardware synthesis in Table VIII show a percentage increase in area of approximately 29% between baseline and modified Ibex. A comparison of all produced models is shown in Table IX."}, {"title": "VII. CONCLUSION", "content": "This paper investigated and accelerated Transformers from the lens of embedded systems, training, quantising, and accelerating ARM's Keyword Transformer through custom instructions on a RISC-V platform, resulting in a 5x speedup at the cost of ~7% accuracy and 29% area overhead. The paper introduces KWT-Tiny, a two-output class keyword spotter for embedded applications that is 369 times smaller than KWT-1 at the cost of 10% accuracy. The paper also proposes a novel C Transformer library and a novel GELU acceleration technique."}]}