{"title": "Revisiting Acoustic Similarity in Emotional Speech and Music via Self-Supervised Representations", "authors": ["Yujia Sun", "Zeyu Zhao", "Korin Richmond", "Yuanchao Li"], "abstract": "Emotion recognition from speech and music shares similarities due to their acoustic overlap, which has led to interest in transferring knowledge between these domains. However, the shared acoustic cues between speech and music, particularly those encoded by Self-Supervised Learning (SSL) models, remain largely unexplored, given the fact that SSL models for speech and music have rarely been applied in cross-domain research. In this work, we revisit the acoustic similarity between emotion speech and music, starting with an analysis of the layerwise behavior of SSL models for Speech Emotion Recognition (SER) and Music Emotion Recognition (MER). Furthermore, we perform cross-domain adaptation by comparing several approaches in a two-stage fine-tuning process, examining effective ways to utilize music for SER and speech for MER. Lastly, we explore the acoustic similarities between emotional speech and music using Fr\u00e9chet audio distance for individual emotions, uncovering the issue of emotion bias in both speech and music SSL models. Our findings reveal that while speech and music SSL models do capture shared acoustic features, their behaviors can vary depending on different emotions due to their training strategies and domain-specificities. Additionally, parameter-efficient fine-tuning can enhance SER and MER performance by leveraging knowledge from each other. This study provides new insights into the acoustic similarity between emotional speech and music, and highlights the potential for cross-domain generalization to improve SER and MER systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Emotion recognition from audio signals has become a pivotal task in various applications, ranging from human-computer interaction to affective computing. Two important subfields in this domain are Speech Emotion Recognition (SER) and Music Emotion Recognition (MER). While SER focuses on identifying emotional states from speech, MER aims to recognize emotions conveyed through music. Both tasks share common challenges, such as dealing with subjective emotional labels and the variability of acoustic signals, yet they offer complementary insights into how emotions are expressed and perceived across different modalities [1]-[4].\nFurthermore, there has been recognition of the synergy between SER and MER. Both tasks involve the analysis of temporal, spectral, and emotional cues in audio, and advances in one field can benefit the other. For instance, insights gained from modeling emotional dynamics in speech can inform MER, particularly in understanding how rhythm, pitch, and timbre contribute to emotion perception [5]. By leveraging shared architectures and pretraining strategies, it has led to the feasibility of transfer learning and joint training for SER and MER with cross-domain knowledge [6], [7].\nRecently, Self-Supervised Learning (SSL) has emerged as a powerful paradigm in audio analysis, addressing the need for large annotated datasets by leveraging unlabeled data [8], [9]. Self-supervised models have demonstrated remarkable success by pretraining on vast amounts of unlabeled audio data, enabling the extraction of effective representations for downstream tasks, including SER and MER. These models have significantly advanced SER by capturing nuanced prosodic and paralinguistic features [10], and are now increasingly being applied to MER, where they show promise in learning emotional patterns from music [11].\nNevertheless, the shared acoustics cues between speech and music, particularly those encoded by SSL models, remain largely unexplored. While SSL-based models have demonstrated impressive performance in both tasks, the extent to which these models capture and utilize overlapping acoustic features across speech and music is not well understood. This gap limits progress in developing cross-domain insights and hinders the potential for improving SER and MER systems.\nTherefore, we address two main research questions: 1) What insights into SSL models can be gained from their performance in SER and MER? 2) Can cross-domain generalization improve performance in SER and MER through domain adaptation? To answer these questions, we undertake the following tasks in both the speech and music domains:\n\u2022 We conduct cross-domain layerwise probing utilizing SSL models pretrained on either speech or music data.\n\u2022 We implement domain adaptation techniques in a two-stage process to improve emotion recognition performance with limited data for each task.\n\u2022 We evaluate the acoustic similarity between speech and music for each emotion using SSL representations with Fr\u00e9chet audio distance.\nNote that in this work we focus on song, a subdomain of music, since the dataset we use contains vocal-only music. This reduces the impact of instrumentation and supports a fair evaluation of the similarity between speech and music in terms of vocal acoustics."}, {"title": "II. RELATED WORK", "content": "Over the past decade, researchers have explored the acoustic similarities and generalizability between speech and music. [12] demonstrated that models leveraging cross-domain features outperformed those relying on domain-specific features, suggesting that speech and music share certain emotional characteristics. Subsequently, they applied multitask learning to jointly predict emotions from both speech and music [6], [7]. In another study, [5] investigated transfer learning between the two domains using denoising autoencoder-pretrained long short-term memory networks for arousal-valence regression tasks, showing promising cross-domain generalizability. Likewise, [13] found that pretraining on speech data enhanced the performance of convolutional neural networks in MER.\nNevertheless, the exploration of acoustic similarity between speech and music, as well as the use of transfer learning and domain adaptation between SER and MER, remains limited, especially in the era of SSL models. Although SSL models have demonstrated success in various audio-related tasks, their application to such cross-domain research has yet to be explored. Therefore, we revisit the utility of acoustic correlations between emotional speech and music by using representations from SSL models pretrained on either speech or music."}, {"title": "III. DATASET AND SSL MODELS", "content": "A. Dataset\nWe use the RAVDESS dataset [14], an audio-visual collection consisting of acted affective recordings of speech and song. The dataset features recordings of 12 female and 12 male professional actors speaking English with a North American accent. In our study, we focus on the audio recordings, excluding the visual input. These recordings contain only vocal performances without accompanying instrumental music, thereby excluding the instrumental influence on the acoustic similarity. Since the song recordings of one female actor are missing, we discard her speech recordings as well to ensure an equal amount of speech and song data for model training. The text content in both the speech and song recordings is semantically neutral and identical, allowing us to concentrate on the acoustic properties by eliminating potential effects caused by lexical content (e.g., prosody variations due to word pronunciation). These are the key reasons we chose RAVDESS, despite the availability of larger datasets.\nFor emotion categories, we select the ones common to both the speech and music recordings, including neutral, calm, happy, sad, angry, and fearful. This yields 92 files for neutral and 184 files for each of the other emotions in both domains. The higher count for emotions other than neutral is due to their having two intensity levels, whereas neutral is presented with only one normal intensity. To ensure sufficient data samples, we ignore the intensity distinction and randomly select 60% of the data for training, 20% for validation, and 20% for testing, from the total of 1,012 recordings in each domain.\nB. SSL Models\nWe select Wav2Vec 2.0-Base-960h (W2V2), HuBERT-Base-ls960 (HuBERT) [15], and MERT-v1-95M (MERT) for investigation. All three models have a very similar architecture, consisting of seven convolutional encoders and 12 transformer encoders with around 95M parameters in total. Hidden representations from all 12 layers are used as the input to the downstream classifier for SER and MER.\nThe primary difference among the three models lies in their pretraining objectives. W2V2 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations, which are jointly learned. HuBERT shares the same concept as W2V2 but applies a classification task, requiring the model to classify hidden sequences into predefined K-means clusters. Both W2V2 and HuBERT are pretrained on LibriSpeech [16], whereas MERT is pretrained on 160k hours of music recordings from the Internet. Although MERT's pretraining strategy generally aligns with that of HuBERT, its objective is modified to incorporate music-relevant cues, such as harmony, timbre, and musical pitch."}, {"title": "IV. METHODOLOGY", "content": "A. Layerwise Probing of Emotion Recognition Performance\nTo investigate the layerwise behavior of each model on either speech or music data, we first extract features from each of the three upstream SSL models (W2V2, HuBERT, or MERT) and apply mean pooling over all frames along the time dimension to these features, following previous work by [10], [17]. Subsequently, for each layer, we train a linear classifier on the training set to perform SER or MER. We select the best-performing checkpoint based on the Unweighted Accuracy (UA) on the validation set and evaluate it on the test set.\nB. Domain Adaptation for Cross-Domain Performance Improvement\nTo investigate whether shared acoustic cues between speech and music can be transferred for emotion recognition in each domain, we combine all 12-layer representations and implement three approaches for transferering the shared acoustics:\n\u2022 1) Baseline: we keep the pretrained SSL model frozen and perform mean pooling across the 12-layer representations. Only the linear classifier is trainable.\n\u2022 2) Weighted-Sum (WS): we replace mean pooling with learnable 12-dimensional WS parameters, while the SSL models maintain frozen, consistent with the baseline.\n\u2022 3) Parameter-Efficient Fine-Tuning (PEFT): we incorporate WS, Weighting-Gate (WG), Low-Rank Adapter (LoRA) [18], and Bottleneck Adapter (BA) [19]. The SSL models are kept frozen, with only the PEFT modules being trainable.\nThe same downstream classifier used in layerwise probing is employed in all approaches. As two-stage fine-tuning has demonstrated cross-corpus and cross-lingual SER ability [20], [21], we apply this approach to all three methods. Specifically, in Stage one, we train the model on the source domain (either speech or music), save the model with the best validation accuracy, and test it on the source domain. In Stage two, we load the saved model, further train it on the target domain (music if the source is speech, or vice versa), and again save the model with the highest validation accuracy, testing it on the target domain.\nC. Layerwise Analysis of Cross-Domain Acoustic Similarity\nTo explore the extent to which acoustic representations are shared between emotional speech and music, we adopt Fr\u00e9chet Audio Distance (FAD) as a reference-free measure to assess acoustic similarity. Compared to traditional similarity metrics, such as cosine similarity and Euclidean distance, FAD is specifically designed for audio assessment, capturing the perceptual similarity between two audio embedding distributions [22]. It has been shown to effectively distinguish between real and synthetic audio [23], as well as audio with different emotions [24]. Therefore, we use FAD to evaluate the acoustic similarity between speech and song. The calculation is as follows.\nGiven the embeddings of speech set and music set, $X^s$ and $X^m$, the FAD score is calculated using multivariate Gaussians derived from the two embedding sets $X^s \\sim (\\mu_s, \\Sigma_s)$ and $X^m \\sim (\\mu_m, \\Sigma_m)$:\n$F(X^s, X^m) = ||\\mu_s - \\mu_m||^2 + tr(\\Sigma_s + \\Sigma_m - 2\\sqrt{\\Sigma_s \\Sigma_m})$ (1)\nwhere tr is the trace of a matrix.\nBesides using the entire speech and music sets, we also calculate FAD for each emotion to investigate whether the acoustic similarity exhibits different patterns across emotion categories. This analysis is also performed using the representations from each layer."}, {"title": "V. EXPERIMENTS", "content": "A. Layerwise Probing of Emotion Recognition Performance\n1) Experimental Settings: For training the SER and MER models, we use cross-entropy loss as the criterion and the AdamW optimizer with a learning rate of 1e-3. Due to the variation in the number of epochs required to achieve optimal validation performance using different SSL representations (particularly layer 11), we train for 500 epochs to ensure all classifiers reach their full potential.\n2) Results and Discussions: Fig. 1 presents the all-emotion results. It can be observed that:\n(I) For SER, our results align with previous findings, showing that while W2V2 experiences a performance decline in its final layers, HuBERT does not [10], [25]. In fact, performance improves in the deeper layers of HuBERT compared to its shallow layers. Additionally, we observe that MERT behaves similarly to HuBERT, exhibiting better performance in deeper layers than W2V2. This is likely due to the pretraining objectives and strategies of MERT aligning closely with those of HuBERT. For MER, all models behave consistently with SER. Moreover, the speech models (W2V2 & HuBERT) perform better at SER, while the music model (MERT) is better at MER, which is reasonable given the differences in their pretraining data.\n(II) For the speech models, the accuracy gap between SER and MER after the middle layers is more pronounced in HuBERT than in W2V2. Since both models are pretrained on the same data, and the lexical content of RAVDESS does not affect SER or MER, it is likely that HuBERT\u2019s deeper layers retain acoustic information that W2V2 misses yet is emotion-relevant. This retained acoustic information could explain the larger performance gap between HuBERT\u2019s SER and MER performance from the middle layers. Such a gap is not observed in W2V2, even though its SER performance remains better than its MER performance.\n(III) MERT shows a significant initial difference between SER and MER, which is not observed on speech models. Despite MERT having similar training objectives and structure to HuBERT, this suggests that the acoustics from the shallow layers of speech models are domain-agnostic and largely shareable, whereas those from the shallow layers of MERT are relatively domain-specific. However, given MERT\u2019s fairly good performance on SER, this still indicates shared acoustics between music and speech. Additionally, the drop in SER performance in the deeper layers of MERT suggests that the features in those layers are more favorable for MER than SER. It is possible that the deep layers of MERT encode high-level music specific cues, such as those related to tonality and rhythm [26], [27]. Therefore, speech SSL models, particularly the shallow layers, may be further trainable with music acoustics, whereas further training of music SSL models with speech acoustics may be less effective.\nSince different emotions exhibit distinct paralinguistic patterns, for instance, angry and happy typically have higher intensity and pitch, while sad and calm tend to have lower intensity [28], [29]. [10] demonstrated that W2V2 shows emotion bias by not encoding different emotional speech equally. In light of this, we compare and contrast emotions by calculating their respective recognition accuracies. Fig. 2 shows the per-emotion results.\nFor SER, it can be observed that performance on angry and fearful speech is generally the best, followed by calm, across all speech and music models. This suggests that both speech and music models favor the acoustic characteristics of these emotions in speech. However, for MER, recognizing fearful and calm proves to be more challenging, particularly in the case of fearful with MERT. In contrast, angry music becomes easier to recognize, even for speech models. Moreover, neutral performs much better on music with MERT than any other neutral cases. Additionally, angry music with W2V2 does not show a performance drop, unlike other emotions with W2V2 for either speech or music. We believe that the deep layers of W2V2 might be particularly effective at modeling acoustic characteristics related to angry music, such as intense rhythm. These observations suggest that different emotions in speech and music exhibit distinct acoustic characteristics, highlighting the emotion bias problem in both speech and music SSL models, and strengthening the findings of [10]. Finally, other emotions all have their respective variations on speech or music with speech models or the music model, but the overall patterns remains similar. This further verifies previous findings that shared acoustics exist between speech and music [6], [13]."}, {"title": "B. Domain Adaptation for Cross-Domain Performance Improvement", "content": "1) Experimental Settings: For the baseline and WS models, we use the AdamW optimizer with a learning rate of 1e\u22123. For the PEFT models, we use the AdamW optimizer with a learning rate of 1e\u22124. For the same reason as layerwise probing, each model is trained for 300 epochs at each stage to achieve its best possible performance.\n2) Results and Discussions: Table III summarizes the domain adaptation results using all three approaches and models. Note that the results of SER and MER from the same model are not comparable, as they are different tasks. We only compare SER with SER and MER with MER. It can be seen that for the speech models (W2V2 and HuBERT), fine-tuning on either domain in stage one always helps the other domain in stage two. For example, the performances of SER followed by MER (74.88, 80.30, 87.20 for W2V2; 84.24, 82.76, 93.10 for HuBERT) are always better than directly conducting SER (73.89, 79.31, 87.20 for W2V2; 83.25, 80.30, 88.18 for HuBERT). Furthermore, PEFT works extremely effectively for HuBERT, achieving the best performance (93.10) and the largest improvement (4.92). For the music model, while several performances are improved after domain adaptation (e.g., 72.77 vs. 61.85 for SER and 93.60 vs. 91.63), such a phenomenon does not generally hold. This finding further verifies our conclusion of finding III in Section V-A that speech SSL models are further trainable with music acoustics yet further training of music SSL models with speech acoustics may be less effective."}, {"title": "C. Layerwise Analysis of Cross-Domain Acoustic Similarity", "content": "1) Experimental Settings: As FAD is a reference-free measurement, there is no model training involved. We separate the speech and music sets per emotion, and use the FAD toolkit for the implementation. The lower the FAD score, the more similar the two sets of representations.\n2) Results and Discussions: From Fig. 3, it can be observed that:\n(I) In terms of distance ranking, all the speech and music models exhibit consistent patterns: angry and fearful have the smallest distances, followed by happy, sad, neutral, and calm. These consistent patterns indicate that the SSL models have a similar ability to capture acoustic similarities related to emotion in their respective representations. Since speech and music share the same text in RAVDESS, the low FAD scores for certain emotions indicate that their acoustics are largely similar between speech and music For example, angry speech and music are more acoustically similar than calm speech and music.\n(II) HuBERT and MERT exhibit aligned patterns not only in the per-emotion ranking but also in the overall trend. The FAD score increases from the first layer to the last, which can be attributed to the similar structures of HuBERT and MERT. However, several middle layers show divergent patterns: while the score continues to rise in HuBERT, it remains stable in MERT. This difference is related to their respective capabilities: HuBERT encodes phonetic level information in the middle layers [30], whereas such capability has not been found in MERT. Additionally, the FAD scores for the final layers of both HuBERT and MERT rise sharply, indicating that their ability to capture shared acoustic features between speech and music weakens at these layers.\n(III) Unlike HuBERT and MERT, the FAD scores for W2V2 exhibit a significant shift starting from the middle layers. One explanation for this is that W2V2 begins encoding word identity and meaning between the middle and final layers, as noted by [31]. Since speech and music datasets share identical text, the acoustic similarity increases, resulting in lower FAD scores. Additionally, different emotions behave differently in layer 11, where calm, neutral, and sad emotions show a sharp rise, while angry, happy, and fearful do not. It appears that layer 11 amplifies these acoustic differences, consistent with previous findings that observed peculiar behavior in this layer due to its contrastive masked segment prediction function [10], [31]. We further demonstrate the peculiarity of layer 11 in that it is particularly sensitive to cross-domain acoustic differences.\nSince using FAD does not involve model training, thereby avoiding potential impacts on the fairness of comparison across emotions, the results and findings instead further underscore the emotion-bias problem present in SSL representations."}, {"title": "VI. CONCLUSION", "content": "As the first work exploring acoustic similarity between emotional speech and music through SSL models, we conduct layerwise probing to explore the models\u2019 ability to capture emotion-relevant acoustic cues and uncover the commonality and differences between speech and music SSL models. Moreover, aiming to leverage each domain to enhance the emotion recognition performance for the other domain, we implement and compare three approaches, demonstrating efficacy of the PEFT approach for cross-domain adaptation and noticing the domain-specificity of the models. Finally, we employ FAD to measure the acoustic similarity between speech and music representations on each emotion, revealing different behaviors of SSL models and the emotion-bias issue in both domains. Our study provide new insights into the acoustic similarity between emotional speech and music, particularly with SSL models. We also demonstrate the potential of leverage knowledge from each domain to facilitate the other. As there are limited datasets suitable for further investigation and fair comparison, we acknowledge the limitations of our work and plan to collect new datasets for extending our study in the future."}]}