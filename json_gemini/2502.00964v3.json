{"title": "ML-Dev-Bench: Comparative Analysis of AI Agents on ML development workflows", "authors": ["Harshith Padigela", "Chintan Shah", "Dinkar Juyal"], "abstract": "In this report, we present ML-Dev-Bench\u00b9, a benchmark aimed at testing agentic capabilities on applied Machine Learning development tasks. While existing benchmarks focus on isolated coding tasks or Kaggle-style competitions, ML-Dev-Bench tests agents' ability to handle the full complexity of ML development workflows. The benchmark assesses performance across critical aspects including dataset handling, model training, improving existing models, debugging, and API integration with popular ML tools. We evaluate three agents - Re-Act, Openhands, and AIDE on a diverse set of 30 tasks, providing insights into their strengths and limitations in handling practical ML development challenges. We open source the benchmark for the benefit of the community at https://github.com/ml-dev-bench/ml-dev-bench.", "sections": [{"title": "2 Introduction", "content": "Recent advances in Large Language Models (LLMs) have demonstrated impressive capabilities in code generation and software engineering tasks. This has led to the development of various benchmarks like HumanEval [2], MBPP [1] that evaluate coding abilities, and others like SWE-Bench [5], that test LLM-based agents on software engineering tasks. However, while these benchmarks effectively assess general programming capabilities, they don't capture the unique challenges of Machine Learning development workflows,\nBenchmarks such as ML-Bench [6], test agents' abilities to generate code and commands to interact with popular ML repositories, while MLE-Bench [2] and MLAgentBench [4] focus on Kaggle-style tasks to evaluate the iterative and open-ended nature of ML development. However, real-world ML development extends far beyond that, including the complexity of working on top of existing codebases and models, integrating with third-party tools, debugging complex issues that span multiple components of the ML pipeline, and understanding and balancing trade-offs like model performance and cost to come up with optimal design.\nML-Dev-Bench addresses this gap by providing a comprehensive evaluation framework that tests an agent's ability to handle real-world ML development scenarios. Our benchmark is particularly relevant,, as ML development increasingly relies on large language models and AI agents to assist developers. Understanding the capabilities and limitations of these agents in handling practical ML development tasks is crucial for their effective deployment in production environments."}, {"title": "3 Benchmark Design", "content": "ML-Dev-Bench comprises of 30 carefully designed tasks that evaluate various aspects of ML development. These tasks are structured to assess both specific technical capabilities (like handling datasets, model implementation) and broader problem-solving skills (like model training and performance improvement) that are essential in real-world ML development. The tasks span several key categories of ML development shown in Table 1:\n1. Dataset Handling focuses on evaluating the ability to work with large datasets, inspect them and apply pre-processing pipelines. An example is the noisy imagenette [3] dataset download task, where the agent needs to download the dataset, inspect its contents to identify the labels file, only load the 50% noisy labels from it and generate class summary statistics.\n2. Model Training tests an agent's ability to work with existing models, from loading pretrained weights to implementing training loops, logging metrics and managing the training process. These tasks assess both technical skills and the ability to handle long-running tasks.\n3. Debugging presents common scenarios including shape errors, exploding gradients, incorrect implementations, and integration errors. Agents must analyze large training logs, metrics, and code across multiple files to identify and resolve issues.\n4. Model Implementation tests the ability to modify existing architectures and implement new features. An example is the ChannelViT related tasks, which follow three levels of increasing difficulty: Level 1 provides complete specifications with examples and tests; Level 2 includes specifications and tests but omits examples; Level 3 gives specifications but tests and examples are hidden\n5. API Integration assesses the ability to work with essential ML development tools, particularly for logging and experiment tracking.\n6. Model Performance tasks challenge agents to improve baseline implementations through iterative experimentation and hypothesis testing."}, {"title": "3.1 Evaluation Metrics", "content": "Tasks are evaluated based on binary success or failure (\u00d7). The aggregate success rate for each agent is calculated as:\nSuccess Rate = Total Successful Tasks/Total Tasks \u00d7 100% (1)\nAgents are assessed on their ability to complete tasks accurately without introducing errors or hallucinations."}, {"title": "4 Evaluation Framework", "content": "In this section we briefly describe the design of our evaluation framework, called Calipers, for running the benchmark. The framework consists of three components: agents, evaluation tasks, and metrics. Agents are evaluated on various Machine Learning tasks to generate metrics. We designed Calipers to allow easy addition of new evaluation tasks, agents and metrics, ensuring the benchmark can evolve alongside advances in ML development practices and tooling."}, {"title": "4.1 Evaluation Task", "content": "Each evaluation task consists of a task description, a set of input code and data files, and a validation logic that checks the correctness of the outputs and artifacts generated by the agent. Depending on the type of task, we implement various types of validation checks including\n\u2022 Running tests on generated code to check for correctness\n\u2022 Checking for the presence of all required output files and artifacts\n\u2022 Evaluating agent generated model checkpoints for required performance\n\u2022 Querying logged artifacts and metrics from wandb"}, {"title": "4.2 Agents", "content": "Each agent is provided with two inputs in an evaluation run, the description of the task and a working directory populated with initial input files. The agent's outputs are task-specific artifacts which are saved in the working directory. These outputs are validated to determine success or failure. We generate the evaluation metrics discussed in the previous section for each evaluation run. We use litellm callbacks to capture metrics like number of steps, tokens, and cost."}, {"title": "5 Agent Setup", "content": "We evaluate three agents on ML-Dev-bench. The agents and their setup is described below. Each agent uses an LLM and a set of tools to execute various actions. All agents execute their code in a runtime environment which is either a local python or docker environment depending on the agent. We customized the runtime environments for all agents to pre-install common ML frameworks like scikit-learn, pytorch, transformers, lightning, wandb, etc to ensure smooth execution.\n1. ReAct: We created a simple ReAct agent [8] as a baseline which takes actions by calling tools. We used the LangGraph framework for the agent and Composio toolset which provides tools for common use cases. We customized the tools to reliably capture large command outputs, handle long running commands and ensure consistency across different tools like file and shell tools. All the tool calls were executed in a local python environment which was pre-installed with common ML frameworks as mentioned earlier and had access to the relevant api keys. No custom prompts were used, and the agent was allowed to run for a maximum of 50 steps. We tested the agent with Claude Sonnet 3.5 10-2022 and OpenAI GPT-40.\n(a) Command line tools\ni. Shell Tool - to execute short running commands\nii. Spawn Tool - to execute long running commands like training in the background\niii. Sleep and execute tool - to wait and monitor long running processes\n(b) File tools like create files, list files and edit files\n2. Openhands: Openhands [7] is a popular open-source coding agent with state-of-the-art performance on SWE-Bench-Full [5]. We used Openhands agent v0.21.1 and customized the runtime build to install common ML frameworks listed above. We tested the agent with Claude Sonnet 3.5 10-2022 model which is the current best performing model with the agent on SWE Bench and Gemini 2.0 Flash. The agent was allowed to run for a maximum of 50 steps.\n3. AIDE: AIDE is an agent purpose-built for data science tasks like Kaggle competitions [2] and performs a tree search over solutions. AIDE scaffolding performs better in comparison to other agents like Openhands on MLEBench using 01, GPT-40. Unlike other general purpose agents which output any artifact, AIDE outputs an evaluation metric and code as its final output. All other artifacts are considered intermediate outputs and saved in a custom working directory. Since not all tasks in ML-Dev-Bench require outputting a score, we access the artifacts from its custom working directory to validate the agent's performance. Given the high costs of o1, we evaluated the agent with GPT-40."}, {"title": "6 Performance Comparison", "content": "Performance of the agents across different task categories, Table 2 and individual tasks Table 3 reveals a consistent pattern. Performance decreases as tasks become more open-ended and complex. The success rates are highest in well-defined categories like dataset handling and basic debugging with clear instructions, but drop significantly in open-ended and long-running tasks like model performance tasks where no agent succeeded.\nOpenHands-Sonnet (OH-Sonnet) and ReAct-Sonnet are the two best performing agents performing agents with 50% and 47% success rate respectively, while OH-Gemini, AIDE-40 and React-40 achieve 17% success rate."}, {"title": "6.1 ReAct-Sonnet", "content": "ReAct-Sonnet had a success rate of 47% (14/30 tasks), demonstrating good performance in specific, well-defined tasks like dataset handling (3/3 tasks), basic model training (4/6 tasks), and debugging tasks with clear specifications (4/7 tasks). However, its performance degraded significantly in model implementation tasks where it needed to modify multiple files and in model performance tasks which are open-ended and long-running. The agent had some common failure modes we list below:\n1. Excessive verification seeking: The agent has a tendency to request feedback even when its instructed to run till completion, specially in complex tasks like model implementation and training.\n2. Premature task termination: In long-running training scenarios, it doesnt wait until completion of tasks but returns control saying training should complete and achieve required performance.\n3. In long running tasks it fails to successfully implement sub-tasks which it handled correctly in isolation. For example, it correctly handles the noisy imagenette dataset setup in dedicated download tasks but fails the same operation when it's part of the longer training pipeline.\nThe agent's token usage and costs varied significantly across tasks. Simple operations like dataset downloads cost around 0.02 - 0.08$, while debugging tasks cost between 0.1 \u2013 0.4$, some tasks like ChannelViT-Easy debugging take more steps indicating potentially inefficient exploration in complex scenarios."}, {"title": "6.2 OpenHands-Sonnet", "content": "OpenHands-Sonnet demonstrated the highest success rate at 50% (15/30 tasks), showing robust performance across most categories. The agent successfully completed all dataset handling tasks (3/3) and showed strong performance in model training (5/6) and debugging (4/7).\nThe agent particularly excelled in structured tasks and showed better persistence in long-running operations compared to ReAct-Sonnet. However, it still failed to complete any of the model performance tasks (0/5), indicating limitations in open-ended problem-solving scenarios requiring iterative improvement."}, {"title": "6.3 OpenHands-Gemini", "content": "OpenHands-Gemini achieved a 17% success rate (5/30 tasks), demonstrating limited capabilities across most evaluation categories. The agent showed some competence in basic dataset handling tasks and setting up pretrained models for training purposes. However, it performed suboptimally in debugging (1/7), model implementation (0/7), and performance improvement tasks (0/6). The agent exhibited several consistent failure modes. It frequently failed to follow task instructions regarding creation of output artifacts. In debugging scenarios, it often modified test files despite explicit instructions not to do so. The agent also demonstrated significant limitations in file editing capabilities and fixing code errors."}, {"title": "6.4 ReAct-40", "content": "ReAct-40 with 17% success rate (5/30 tasks) had some success with tasks with well-defined specifications (WandB logging, downloading a specific model from Torchvision), and certain debugging tasks. However it did struggle on other tasks in the same categories. It also failed on the relatively easier tasks like dataset download due to not following instructions, ran into indentation errors while attempting to debug code and failed to produce output artifacts as required by certain tasks."}, {"title": "6.5 Aide-40", "content": "Aide-40 had a 17% success rate (5/30 tasks), demonstrating limitations across most categories. The agent managed to complete some basic dataset handling and debugging tasks but struggled with model training (2/6) and completely failed in model implementation and model performance categories.\nThe cost metrics for Aide-40 weren't captured as it doesnt use LiteLLM, but its low success rate across both specific and open-ended tasks suggests limitations in handling ML development workflows. The agent's successes were primarily limited to tasks with very clear, step-by-step instructions and immediate feedback loops."}, {"title": "7 Conclusion", "content": "We presented ML-Dev-Bench, a benchmark focused on ML development workflows consisting of 30 tasks. We evaluated 3 agents on this benchmark - ReAct (with Claude Sonnet and GPT-40), Openhands and AIDE; Openhands with Claude Sonnet performed the best out of these. Our open-source evaluation framework provides a foundation for the community to build upon. We encourage researchers and practitioners to explore various directions: analyzing the impact of scaling compute on these agents, studying variance in success metrics across multiple runs, evaluating emerging reasoning models like DeepSeek-R1 and 01/03, and expanding the problem categories to include areas such as label and data collection. We welcome contributions to enhance the benchmark's scope, methodology, and agent evaluation pipeline."}]}