{"title": "TRANSFORMERS STRUGGLE TO LEARN TO SEARCH", "authors": ["Abulhair Saparov", "Richard Yuanzhe Pang", "Shreyas Pimpalgaonkar", "Srushti Pawar", "Nitish Joshi", "Najoung Kim", "Vishakh Padmakumar", "Seyed Mehran Kazemi", "He He"], "abstract": "Search is an ability foundational in many important tasks, and recent studies have shown that large language models (LLMs) struggle to perform search robustly. It is unknown whether this inability is due to a lack of data, insufficient model parameters, or fundamental limitations of the transformer architecture. In this work, we use the foundational graph connectivity problem as a testbed to generate effectively limitless high-coverage data to train small transformers and test whether they can learn to perform search. We find that, when given the right training distribution, the transformer is able to learn to search.\nWe analyze the algorithm that the transformer has learned through a novel mechanistic interpretability technique that enables us to extract the computation graph from the trained model. We find that for each vertex in the input graph, transformers compute the set of vertices reachable from that vertex. Each layer then progressively expands these sets, allowing the model to search over a number of vertices exponential in the number of layers.\nHowever, we find that as the input graph size increases, the transformer has greater difficulty in learning the task. This difficulty is not resolved even as the number of parameters is increased, suggesting that increasing model scale will not lead to robust search abilities. We also find that performing search in-context (i.e., chain-of-thought) does not resolve this inability to learn to search on larger graphs.", "sections": [{"title": "INTRODUCTION", "content": "The ability to search is fundamental in many important tasks, such as reasoning (Yao et al., 2024;\nKazemi et al., 2023; Hao et al., 2023), planning (Stein & Koller, 2023; Valmeekam et al., 2022), and\nnavigation (Ding et al., 2024). Recent work has demonstrated that transformer-based large language\nmodels (LLMs) struggle with proof search (Saparov & He, 2022; Valmeekam et al., 2022; Kamb-\nhampati et al., 2024). It is unknown whether this shortcoming is due to a lack of data, insufficient\nmodel parameters, or a fundamental limitation of the transformer architecture. In any case, as the\nscale of LLMs continues to increase, it is yet unclear whether future LLMs, equipped with more\ndata, parameters, and compute, will be able to perform search and planning. Chain-of-thought and\nsimilar prompting techniques (Wei et al., 2022b; Nye et al., 2021) have enabled LLMs to decompose\nthe search task into the repeated task of predicting the next step along the path to the goal. However,\neven in this setting, in the worst case, in order to avoid making a \"wrong turn,\" the model must\nperform the search within its forward pass to determine the correct next step. And LLMs have been\nobserved producing errors or hallucinations after taking such a wrong turn (Saparov & He, 2022).\nWe aim to shed light on this question by training small transformer models on a simple yet foun-\ndational search task: Given a directed acyclic graph (DAG), a start vertex, and a goal vertex, find\nthe next vertex along a path from the start to the goal vertex. This task is the backbone of many\nreasoning problems as it is equivalent to proof search in a simplified logic which is a subset of al-\nmost any logic: The model must solve this task if there is any chance to generalize to more complex\nsearch and reasoning tasks. We demonstrate experimentally that transformers can indeed be taught\nto search, when given the right training distribution. The training distribution must be carefully con-\nstructed so as to preclude the usefulness of shortcuts or heuristics that would otherwise prevent the\ntransformer from learning a robust and generalizable search algorithm. By automatically generating\nsuch examples, we provide the transformer with effectively limitless and idealized training data,\nwith which we can estimate an \"upper bound\" on the transformer's ability to learn to search."}, {"title": "RELATED WORK", "content": "Search abilities of transformers. A number of studies have explored the search capabilities of\ntransformers and LLMs. Benchmarks have shown that LLMs can perform some graph reasoning\ntasks and related tasks such as repeated function composition and the k-hop induction head task\n(Fan et al., 2024; Fu et al., 2024; Sanford et al., 2024a;b), but they are limited to small graphs,\nrelative to graphs we consider. Ruoss et al. (2024); Gandhi et al. (2024); Shah et al. (2024) find\nthat a transformer can learn to approximate or simulate a search algorithm, but with a performance\ngap, and they do not test whether this gap is lessened by increasing model scale or training. Wang\net al. (2023); Bachmann & Nagarajan (2024) show that LLMs can do some graph reasoning but\nare fooled by spurious correlations. Similarly Zhang et al. (2023) find that transformers are unable\nto learn to perform proof search since they strongly prefer heuristics. In this work, we also show\nthat transformers are highly sensitive to the training distribution, but if extra care is taken to re-\nmove heuristics, they are able to learn to search. Zhang et al. (2024) show that LLMs struggle on\nreal-world graph reasoning tasks. Borazjanizadeh et al. (2024) find that LLMs have difficulty on\ndiverse search problems. However, this is in contrast with work on the theoretical expressiveness of\ntransformers. Merrill & Sabharwal (2024) show that with chain-of-thought, transformers can sim-\nulate any Turing machine. However, their results do not indicate whether it is possible to train a\ntransformer to perform any task. In fact, we find that even if the transformer is permitted to generate\nintermediate tokens, it is challenging to learn to search on larger graphs. Sanford et al. (2024a;b)\nshow that transformers need a logarithmic number of layers to perform the graph connectivity task,\nwhich is supported by our identification of the algorithm that transformers acquire during training.\nMechanistic interpretability. There is a large amount of work that seek an algorithmic understand-\ning of transformers trained on various tasks. Hou et al. (2023); Kim et al. (2024) look for evidence of\nspecific circuits/algorithms in the transformer's activations and attention patterns. In our approach,\nwe do not require the algorithm be known a priori. Rather, we reconstruct the computation graph\nfrom the model activations and attention patterns. We make heavy use of activation patching (Vig\net al., 2020b; Geiger et al., 2021; Heimersheim & Nanda, 2024). Brinkmann et al. (2024); Kim\net al. (2024); Stolfo et al. (2023) apply mechanistic interpretability analysis to better understand\ntransformer behavior in reasoning, and Yang et al. (2024); Jenner et al. (2024) present evidence\nthat LLMs perform shallow searches during the forward pass. Ivanitskiy et al. (2023) train small\ntransformer models on a maze search task and find internal representations of the maze map.\nScaling laws. Scaling laws are empirically-supported hypotheses about the long-term behavior of\nmachine learning models on a task, as a function of the model size, the amount of data, and compute\n(Kaplan et al., 2020; Henighan et al., 2020; Hoffmann et al., 2022). Caballero et al. (2023) explore\nscaling laws on a wide multitude of tasks, but not including search, reasoning, or planning. Wei"}, {"title": "SEARCH IN DIRECTED ACYCLIC GRAPHS", "content": "In order to test whether transformers can learn to perform search, we need a way to produce a large\nnumber of search problems on which the model can be trained and evaluated. We do so by generating\nsearch problems on directed acyclic graphs (DAGs). Each example consists of a DAG, a start vertex,\nand a goal vertex (which is guaranteed to be reachable from the start vertex). The model's task is to\noutput the next vertex on the path from the start to the goal vertex. We characterize the difficulty of\nan example by its lookahead: the number of steps that the model must search from the start vertex\nto find the goal. More precisely, for any example graph, let \\(P\\) be the path from the start to the\ngoal vertex, and \\(S_i\\) be the paths from the start vertex that are otherwise disjoint with \\(P\\), then the\nlookahead \\(L\\) is \\(\\min{\\{|P|, \\max_i |S_i|\\}}\\). An example graph, along with the corresponding transformer\ninput, is shown in Figure 1. In this graph, the lookahead is 2.\nWe experiment with three distributions of DAG search problems: (1) two simple distributions where\nwe pay no special attention to heuristics, which we call the \u201cna\u00efve\" and \"star\" distributions, and (2)\na more carefully constructed distribution where we take care to prevent the model from exploiting\nheuristics to solve the task, called the \"balanced distribution.\"\nNa\u00efve distribution. We adapt the Erd\u0151s-R\u00e9nyi random graph distribution (Erd\u00f6s & R\u00e9nyi, 1959) to\ngenerate directed acyclic graphs: We arrange a set of vertices in linear order from left to right (i.e.,\ntopological order) and randomly sample edges between pairs of vertices. To ensure there are no\ncycles, all edges are oriented to the right. To reduce the density of the graphs, we limit the in-degree\nof any vertex to 4, since the lookahead is typically very small in dense graphs. See Section A.1.1 for\ndetails on the generative process.\nBalanced distribution. While easy to describe and implement, the na\u00efve distribution strongly tends\nto generate graphs where the lookahead is very small (typically \\(L = 1\\) or 2; see Figure 8). In fact,\nit becomes exponentially less likely to generate examples with greater lookaheads. In order to both\ntrain and evaluate the model's ability to search multiple steps to find the goal vertex, we need an\nefficient way to generate examples where the model is required to search for additional steps in\norder to find the goal. In addition, to prevent the model from relying on heuristics to perform\nsearch, we must take care to ensure that heuristics are not useful to solve the search problems in\nthe training distribution. Thus, we design the balanced distribution to specifically generate graphs\nwith a lookahead parameter \\(L\\) (see Section A.1.2 for details), which we use to produce training data\nwhere the lookahead is uniformly distributed.\nStar distribution. We experiment with an additional distribution over graphs where the vertices are\narranged in a star-shape (see Fig 1 in Bachmann & Nagarajan, 2024). The vertex at the center is the\nstart vertex, and there are k \"spokes\" that radiate outwards from the center, where each spoke is a\nlinear chain of L vertices. The goal vertex is at the end of one of the spokes."}, {"title": "EXPERIMENTS", "content": "We train transformer models, with the same architecture as GPT-2 (Radford et al., 2019) with ReLU\nactivation. In order to facilitate mechanistic interpretation of the trained model behavior, we use\n1-hot embeddings for both the token and the absolute positional embedding. Furthermore, the token\nembedding and positional embeddings are concatenated rather than added to form the transformer\ninput. We use 1 attention head per layer. The feed-forward dimension is the same as the model\ndimension. Since the edges in our inputs are randomly ordered, it would help for each token to be\nable to attend to any other token, rather than only the preceding tokens. As such, the model does not\nuse a causal mask when computing attention. We train an 6-layer model with hidden dimension 16.\nTo simulate training on samples of a very large corpus of data, we utilize streaming training. We\ncontinually sample batches from the generative process throughout training, instead of sampling\nbatches from a fixed training set. The first few batches are reserved for testing, and subsequent\nbatches are filtered via exact matching to remove any examples that appear in the test set, to ensure\nthat the examples in the test set are unseen. In all our experiments, we use the Sophia optimization\nalgorithm (Liu et al., 2024) with a learning rate of \\(10^{\u22125}\\), weight decay of 0.1, and no dropout. We use\na batch size of 1024 examples, unless otherwise stated. We minimize the cross-entropy loss during\ntraining. Some graphs contain multiple paths from the start to the goal vertex. During training, we\nselect one path uniformly at random as the ground truth when computing the loss. During evaluation,\nwe consider the model's prediction to be correct if the output vertex lies on any path to the goal."}, {"title": "SENSITIVITY TO TRAINING DISTRIBUTION", "content": "We first investigate the effect of the training distribution on the transformer's ability to learn the\nsearch task. We do so by training one model on the na\u00efve distribution, another model on the star\ndistribution (where the number of spokes k and spoke lengths L are uniformly distributed), and a\nthird model on the balanced distribution (where the lookahead L is uniformly distributed), all with\ninput size 128 tokens. Then, we evaluate the accuracy of each model on held-out test sets from the\nna\u00efve, star, and balanced distributions, for all possible lookaheads \\(L\\). We observe in Figure 2 that the\nmodel trained on the na\u00efve distribution is not able to robustly handle graphs with larger lookaheads,\nshowing low accuracy even for observed number of lookaheads (e.g., \\(L = 5\\)). This is due to the fact\nthat the probability of generating graphs with larger lookaheads with the na\u00efve distribution decreases\nexponentially, and so the model is not shown sufficient examples with large lookaheads. The model\ntrained on the star distribution performs reasonably on examples from the balanced distribution\nbut not as well on examples from the na\u00efve distribution. In contrast, the model trained on the\nfull balanced distribution performs near perfectly in all test settings, and generalizes to unobserved\nnumbers of lookaheads. This result demonstrates that it is possible to teach a transformer to perform\nsearch almost perfectly on the space of lookaheads observed during training, when provided with an\nappropriate training distribution. Furthermore, the model trained on the balanced distribution with\nlookahead \\(\u2264 12\\) was able to generalize to lookaheads 13 and 14 but not on any larger lookaheads."}, {"title": "PROOF SEARCH IN NATURAL LANGUAGE", "content": "Our earlier experiments were conducted with a symbolic input representation for the graphs in each\nexample. To demonstrate that our findings generalize to inputs expressed in natural language, we\nre-run our earlier experiment where we train small transformers from scratch, but with a modified\ninput: Each edge (e.g., \u201cvertex 1 connects to vertex 2\") is expressed as a conditional sentence (e.g.,\n\"If Alex is a wumpus, then Alex is a vumpus\") where each word and punctuation symbol is a token.\nThe task is correspondingly modified: Given a start proposition (e.g., \u201cAlex is a wumpus\"), find the\nnext step in the proof of a goal proposition. Thus, this modification defines a one-to-one mapping\nbetween graph search problems in the symbolic representation and reasoning problems in a natural"}, {"title": "MECHANISTIC INTERPRETATION OF TRANSFORMERS ON GRAPH SEARCH", "content": "We observed in the previous section that transformers are sometimes able to learn to search during\ntraining, and the resulting model is able to robustly and correctly answer almost any graph search\nproblem in the input space. We aim to better understand the algorithm that the model acquired\nduring training to solve the task, to determine whether and to what extent the model is utilizing a\ncorrect algorithm to solve the task, as opposed to a heuristic."}, {"title": "RECONSTRUCTING ALGORITHMS FROM INPUTS", "content": "In order to understand how the transformer learns to solve the graph search task, we develop a\nnew method for mechanistically interpreting the model's behavior. Our method involves closely\nexamining the model's behavior for a given input example in order to reconstruct a computation\ngraph that explains the model's activations, attention patterns, and output prediction. Our method\nconsists of the following steps, as depicted visually in Figure 3:\nI. Compute activations, attention weights, and output logits. For a given input example, perform\nan ordinary forward pass to compute the activations, attention weights, and output logits.\nII. Identify important attention operations in the last layer. For each element of the attention\nmatrix in the last layer corresponding to the last token, perturb the weight by changing it to 0 and\nrecomputing the logit of the model's original prediction. If the resulting decrease in the logit of\nthe prediction is greater than a threshold parameter \\(\u03b1\\), then this attention operation is marked as\nimportant. Similarly perturb each weight by changing it to a large value (the largest attention\nweight in the row of the attention matrix and renormalize). If the resulting decrease in the logit is\ngreater than \\(\u03b1\\), then this operation is also marked as important."}, {"title": "Identify important attention operations in all other layers", "content": "For each element of the attention\nmatrix in all layers except the last layer, perturb the weight by changing it to 0 or to a large value\n(i.e., setting the weight equal to the largest weight in that row and renormalizing). Perform a\nforward pass and inspect the log attention weight of each important operation in the last layer. If\nthe resulting change in the log attention weight is greater than \\(\\frac{V_\u03b1}{\\sqrt{d}}\\), where d is the model dimension\nand \\(K_1\\) is a sensitivity parameter, then we mark this attention operation as important. Similarly, if\nthe resulting decrease in the logit of the output prediction is larger than \\(\u03b1\\), we mark this attention\noperation as important."}, {"title": "Explain each important attention operation", "content": "For each important attention operation, let \\(j\\) be\nthe row and \\(i\\) be the column of the corresponding entry in the attention matrix, and \\(l\\) be the layer\nof the operation. We say token \\(i\\) is the source token of this attention operation and token \\(j\\) is the\ntarget. We use activation patching (Vig et al., 2020a; Zhang & Nanda, 2024) to determine which\nfeatures of the input are causally significant for this operation. We test perturbations on two types\nof input features:\n\u2022 Token perturbations: For each vertex ID \\(v\\) of the input, we produce a perturbed input where\n\\(v'\\) is substituted for \\(v\\), where \\(v'\\) is a vertex ID that does not appear in the original input.\n\u2022 Position perturbations: For each position \\(i\\) of the input, we produce a perturbed input where\nthe positional embedding for the \\(i^{th}\\) token is set to zero.\nSuppose we perturb an input feature \\(f\\). We then compute the forward pass on the perturbed input\nwhile freezing the attention matrices up to the layer of the current attention operation. At layer \\(l\\),\nwe compute the dot products:\n\\(Q_\\hat{i} \\cdot \\hat{K_i}\\) and \\(Q_j \\cdot K_i\\),\nwhere \\(\\hat{Q_j}\\) is the perturbed query vector corresponding to token \\(j\\), \\(Q_j\\) is the unperturbed query\nvector, \\(\\hat{K_i}\\) is the perturbed key vector corresponding to token \\(i\\), and \\(K_i\\) is the unperturbed key\nvector. We compare \\(Q_\\hat{i} \\cdot \\hat{K_i}\\) to the original scaled dot product \\(Q_j \\cdot K_i\\). If the resulting change\nin the dot product is greater than \\(\u03b1\\), where \\(\u03ba_2\\) is a sensitivity parameter, then we say that the\nembedding"}, {"title": "Reconstruct the computation graph/circuit", "content": "Starting from the first layer, let \\(t_k\\) be the token\nat position \\(k\\) of the input. We say each input vector \u201cexplainably contains\" information about the\ntoken value \\(t_k\\) and position \\(k\\). Next, we consider the attention operations in the first layer. Suppose\nan attention operation copies source token \\(i\\) into target token \\(j\\), and depends on the source token\nembedding containing features \\(f_i^1\\), ..., \\(f_i^{N_i}\\) and depends on the target token embedding containing\nfeatures \\(f_j^1\\), ..., \\(f_j^{N_j}\\) to perform this operation (as computed in Step IV.). We say this attention op-\neration is explainable if the embedding of token \\(i\\) explainably contains all features \\(f_i^1\\), ..., \\(f_i^{N_i}\\), and\nthe embedding of token \\(j\\) explainably contains all features \\(f_j^1\\), ..., \\(f_j^{N_j}\\). If the attention operation\nis explainable, we say the output embedding of the target token \\(j\\) explainably contains the union\nof the features: \\(f_i^1\\), ..., \\(f_i^{N_i}\\), \\(f_j^1\\), ..., \\(f_j^{N_j}\\). We repeat this for every layer, computing all explainable\nattention operations throughout the model. Pseudocode for this procedure is shown in Algorithm 1.\nFinally, we filter out attention operations for which there does not exist a path of explainable\nattention operations to the output prediction (i.e., we can't explain how this operation is useful for\nthe model's output on this example). The result is a computation tree, where each node corresponds\nto an embedding vector in some layer in the model, which explainably contains information about\na set of input features, and where each edge corresponds to an explainable attention operation.\nWe apply the above method on a trained model repeatedly for different input examples. The result is\na set of computation graphs/circuits, one for each input example, and we can perform further analysis\non these circuits to describe the model's computation across many inputs. While this method is able\nto produce a fine-grained description of the processing in the transformer, it requires many forward\npasses. Nonetheless, we are able to apply it to our smaller trained models."}, {"title": "EXPERIMENTS", "content": "We perform the above analysis on models trained on the balanced distribution that have achieved\nnear-perfect test accuracy. We hypothesize that the transformer performs search on all vertices\nsimultaneously, where the embedding for each vertex explainably contains information about the set\nof vertices reachable from the current vertex within a certain number of steps. At each layer, for\neach vertex, the attention mechanism copies from a source vertex that is at the edge of the current\nvertex's reachable set, computing the union of the reachable sets of both vertices and storing the\nresulting set in the embedding of the current vertex. Thus, the reachable set can theoretically double\nin size at every layer. In theory, the model may perform the search either in the forward or backwards\ndirection: Rather than storing the set of reachable vertices, it may store the set of vertices from which\nthe current vertex is reachable. A visual depiction of this algorithm is shown in Figure 4.\nTo test whether the transformer utilizes this algorithm, we perform the analysis described in Sec-\ntion 4.1 for multiple held-out inputs from both the na\u00efve and balanced distributions (on a total of\n2000 inputs; 100 for each lookahead). We set \\(\u03b1 = 0.4\\), \\(\u03ba_1 = 20\\), and \\(\u03ba_2 = 10\\). For each input, we\nreconstruct and inspect the computation graph of attention operations. We categorize each attention\noperation into one of the following: (1) path-merge operations, or (2) copy operations from ver-\ntices that are specifically reachable from either the start or the goal vertex. If the attention operation\ndoes not fall into either category, it is discarded. We say the input is explained by the path-merging\nalgorithm if for every vertex along the path from the start to the goal vertex, there exists an unbroken\nsequence of path-merge operations that ultimately copy from the corresponding token in the first\nlayer into the last token at the last layer."}, {"title": "DOES SCALING HELP?", "content": "Even if it is possible to train a transformer to search, it is unclear how this ability scales with respect\nto input graph size and model scale. We investigate scaling behavior by running two sets of experi-\nments on small transformer models: (1) training models on increasing graph sizes, and (2) training\nmodels with increasing model dimension d. We observe large variance in performance across differ-\nent initial random seeds, which has been observed in other tasks (Kim & Linzen, 2020; Zhou et al.,\n2024). Thus, in each of these experiments, we train the models using multiple seeds on examples\nfrom the balanced distribution, and measure the minimum loss on a held-out test set generated by\nthe na\u00efve distribution. We set the batch size to 256 examples due to GPU memory limitations.\nIn Figure 6, we observe that, when the number of layers is fixed to 8 and the hidden size 16, as the\nmaximum input graph size is increased, the likelihood that the model learns the training distribution\n(i.e., reaches accuracy \\(\u2265 0.995\\)) becomes vanishingly small. In addition, as the maximum input\ngraph size increases, the minimum test loss over 14 seeds grows at an increasing rate. See Figure 10\nin the Appendix for a more detailed visualization of the training dynamics versus input graph size.\nTo determine whether larger models can more easily learn to search on large input graphs, we fix the\ninput graph size to 31 and train models of widely varying sizes. In Figure 7, we see that while larger\nmodels are able to more quickly find the local minimum (at loss near 10\u00ba), there is no discernible\npattern between the size of the model and the amount of training needed to find the global minimum.\nWe also experiment with decoder-only models with learned token embeddings and rotary positional\nembeddings (Su et al., 2024), which are multiplied rather than concatenated, and are more predom-\ninant in contemporary LLMs. But we find that this does not change the model's scaling behavior\n(see Section A.5)."}, {"title": "DOES IN-CONTEXT EXPLORATION (I.E., CHAIN-OF-THOUGHT) HELP?", "content": "Though we have shown that transformers are not able to learn to perform search for larger input\ngraphs, they may be able to learn to search if permitted to take intermediate steps, akin to chain-of-\nthought prompting. To test this, we repeat our earlier experiments with two \u201cprompting\u201d approaches:\n(1) depth-first search, and (2) selection-inference."}, {"title": "DEPTH-FIRST SEARCH", "content": "We generate graphs and perform a depth-first search (DFS) from a randomly-selected vertex to a\nrandom goal vertex. From the sequence of visited vertices (i.e., DFS trace), we randomly select a\n\"current\" vertex. Each input to the model is: (1) the graph, as a list of edges, and (2) the sequence of\nvisited vertices up to and including the current vertex. The model's task is to predict the next vertex\nin the DFS trace."}, {"title": "SELECTION-INFERENCE", "content": "In this setting, each search step is decomposed into two subtasks: (1) given a graph and a list of\nvisited vertices, select a visited vertex that has an unvisited child vertex, and (2) given a selected\nvertex, predict (i.e., infer) an unvisited child vertex. Starting from the start vertex, if these two\nsubtasks are repeated sufficiently many times, the goal vertex will be found. To construct a graph\ndistribution that is analogous to the balanced distribution, we define two variables: (1) the frontier\nsize F, which is the number of visited vertices that have unvisited children, and (2) the branch count\nB, which is the number of child vertices of the current vertex (in an inference step). We generate\ngraphs such that the pair (F, B) is uniformly distributed. Each input consists of: (1) the graph, and\n(2) the sequence of visited edges. See Section A.10 for further details.\nIn the experiments, we first fix the model size, setting the number of layers to 4, and vary the input\ngraph size. We again note from the top row of Figure 16 that the model struggles as the graph size\nincreases. Next, we fix the maximum graph size to 45 vertices and vary the model size. We note\nin Figure 17 that increasing model scale does not help to learn the task. Therefore, transformers\nstruggle to learn to perform DFS search and selection-inference on larger graphs and additional\nscaling does not seem to make it easier."}, {"title": "CONCLUSION", "content": "Through the use of graph connectivity as a testbed, we found that transformers can learn to search\nwhen given the right training distribution. We developed and applied a new mechanistic inter-\npretability technique on the trained model to determine the algorithm that the model learned to\nperform search. We found that the model uses an exponential path-merging algorithm, where the\nembedding of each vertex stores information about the set of reachable vertices from that vertex.\nAs the input graph size increases, the transformer has ever-increasing difficulty in learning the task,\nand increasing the scale of the model does not alleviate this difficulty. Lastly, even if the model is\npermitted to use intermediate steps, they still struggle on larger graphs, regardless of scale.\nIt is possible that scaling to much larger model sizes may lead to emergent searching ability. Alter-\nnate training procedures may help transformers to more easily learn to search, such as curriculum\nlearning. Alternate architectures may help as well, such as looped transformers. While the path-\nmerging algorithm is able to explain almost all examples for the trained models, there may be other\nalgorithms or heuristics that the model simultaneously utilizes on some examples. Our mechanistic\nanalysis has potential broader applications in reasoning: Some form of the path-merging algorithm\nmay be used by transformers in more general reasoning tasks. In such a case, the representation of\neach fact would store information about the set of facts provable from the current fact. Our mecha-\nnistic interpretability tools may be useful in other settings, as well, where they may help to uncover\nthe algorithms that transformers learn to solve other tasks. Though additional work is welcome to\nimprove the scalability of our analysis to larger models, our analysis can provide insights on smaller\nmodels that can be tested separately in larger models."}, {"title": "GRAPH GENERATION DETAILS", "content": ""}, {"title": "NA\u00cfVE DISTRIBUTION", "content": "To sample a graph from this distribution, we first sample the number of vertices\n\\(|V| ~ Uniform({3, . . . , V_{max}}),\\)\nwhere \\(V_{max}\\) is the maximum number of vertices that can fit the input. Then, for each \\(i = 1, ..., |V|\\),\nwe sample a number of parent vertices \\(n_{parents}\\) from \\(V_1, . . . , V_{i\u22121}\\):\n\\( n_{parents} \\sim \\begin{cases} 1 & \\text{with probability } \\frac{1}{2}, \\\\ 2, 3, \\text{ or } 4 & \\text{with probability } \\frac{1}{2}. \\end{cases} \\)\nNote that this differs from Erd\u0151s\u2013R\u00e9nyi where the number of parents is geometrically distributed.\nWe want to avoid generating overly-dense graphs where the lookahead is too small, and so we\nchoose to sample fewer parents per vertex.\nFinally, we sample the parents of \\(V_i\\) uniformly without replacement from \\({V_1, ..., V_{i\u22121}}\\) until we\nhave sampled \\(n_{parents}\\) vertices, or we have sampled all available vertices. We draw an edge from each\nsampled vertex to \\(V_i\\).\nAfter generating the graph, we randomly permute the vertex IDs so that the IDs contain no informa-\ntion about the graph topology. Observe that this distribution can generate any directed graph with\nmaximum in-degree 4 in topologically-sorted order, and therefore, it can generate any DAG with\nmaximum in-degree 4.\nWe select the start and goal vertices uniformly at random from V. If there is no path from the start\nto the goal vertex, or if the example does not fit within the model input, we reject the sample and try\nagain."}, {"title": "BALANCED DISTRIBUTION", "content": "Each graph is sampled according to the following procedure: Given a lookahead parameter \\(L\\)", "additional\n\"branches\"": "n\\(B \\sim Uniform({1", "vertices": "n\\(|V| = min \\{L(B + 1) + 1 + u"}, {"vertices": "For \\(i = 1, . . . , B\\), we\ncreate a chain of vertices where the length of the chain in edges \\(l_i\\) is given by\n\\( l_i \\sim \\begin{cases} L, & \\text{if no additional vertices are available, i.e., } \\sum_{j=1}^{i-1} l_j + L(B - i - 1) + 1 = |V|, \\\\ Uniform \\{L, L + 1\\}, & \\text{otherwise}. \\end{cases} \\)\nEach additional branch is added to the start vertex.\nWhile the graphs resulting from the above process"}]}