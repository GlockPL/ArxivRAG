{"title": "Detecting Unsuccessful Students in Cybersecurity Exercises in Two Different Learning Environments", "authors": ["Valdemar \u0160v\u00e1bensk\u00fd", "Kristi\u00e1n Tk\u00e1\u010dik", "Aubrey Birdwell", "Richard Weiss", "Ryan S. Baker", "Pavel \u010celeda", "Jan Vykopal", "Jens Mache", "Ankur Chattopadhyay"], "abstract": "This full paper in the research track evaluates the usage of data logged from cybersecurity exercises in order to predict students who are potentially at risk of performing poorly. Hands-on exercises are essential for learning since they enable students to practice their skills. In cybersecurity, hands-on exercises are often complex and require knowledge of many topics. Therefore, students may miss solutions due to gaps in their knowledge and become frustrated, which impedes their learning. Targeted aid by the instructor helps, but since the instructor's time is limited, efficient ways to detect struggling students are needed. This paper develops automated tools to predict when a student is having difficulty. We formed a dataset with the actions of 313 students from two countries and two learning environments: KYPO CRP and EDURange. These data are used in machine learning algorithms to predict the success of students in exercises deployed in these environments. After extracting features from the data, we trained and cross-validated eight classifiers for predicting the exercise outcome and evaluated their predictive power. The contribution of this paper is comparing two approaches to feature engineering, modeling, and classification performance on data from two learning environments. Using the features from either learning environment, we were able to detect and distinguish between successful and struggling students. A decision tree classifier achieved the highest balanced accuracy and sensitivity with data from both learning environments. The results show that activity data from cybersecurity exercises are suitable for predicting student success. In a potential application, such models can aid instructors in detecting struggling students and providing targeted help. We publish data and code for building these models so that others can adopt or adapt them.", "sections": [{"title": "I. INTRODUCTION", "content": "As cyber threats become increasingly complex, organizations have big demand for cybersecurity experts [1]. In order to train more experts, effective teaching methods such as hands-on exercises must be employed at universities and in professional learning contexts. However, cybersecurity exercises incorporate a wide range of topics, including operating systems, security vulnerabilities, and proficiency with command-line tools and programming languages. Due to the exercises' complexity, students often get stuck or frustrated [2], [3], which discourages them and hinders their learning.\nTherefore, it is crucial for instructors to know when a student is at risk of not completing an exercise. This should be detected quickly the instructors' time is limited, so it should be invested in helping students who need it the most. Moreover, not all students will let instructors know when they need support, or even deny needing it, so the instructors may be unaware that a specific student is struggling. In these cases, automated tools help focus instructors' attention [4]. This enables instructors to target teaching interventions, such as hints, with the goal of positively impacting student learning."}, {"title": "A. Goals and Scope of This Paper", "content": "Our goal is to extract information from student actions in cybersecurity exercises, in order to predict student success or a potential risk of performing poorly. We evaluate tools that highlight students who may need help with an exercise task.\nIn our context, we broadly define successful students as being able to complete a certain amount of the exercise tasks; Section III-F explains the specifics. Our work focuses on the education of cybersecurity students at the university level or beyond, though it could also be adapted to K-12 contexts.\nThis paper poses two research questions (RQ):\n1) How well do different machine learning classifiers predict (un)successful students in cybersecurity exercises?\n2) Are the best classifiers in one context also the best in another context, when trained using the same methods with a second student population in different exercises?"}, {"title": "B. Contributions to Research and Practice", "content": "First, we collected an original dataset from a total of 313 students in two learning environments. Second, we automatically extracted two feature sets from these data. Third, we used these features to train and evaluate eight types of binary classification models for predicting student success in the exercises."}, {"title": "II. RELATED WORK IN PREDICTIVE MODELS", "content": "This section provides an overview of studies that analyzed data from educational contexts to predict student performance. We discuss the state of the art in cybersecurity education (Section II-A) and in other computing education domains (Section II-B). Then, Section II-C summarizes the literature gaps that our work addresses. The model evaluation metrics discussed here are defined in detail in Section III-H2."}, {"title": "A. Hands-on Cybersecurity Education", "content": "Cybersecurity education literature employs student data to achieve different goals [9], [10], such as clustering students into high- and low-performing based on students' strategies for completing exercises [11]. However, the problem of predicting student performance is not covered much in previous research.\nVinlove et al. [2] detected at-risk students in a command-line-based security exercise. They collected logs from 25 students and extracted three features: (1) average number of commands per task, (2) longest repeat of a command, and (3) edit distance between the command and \"known-good\" commands for the exercise. A support vector machine (SVM) reached 80% accuracy in classifying exercise completion. However, this work used a rather small dataset and only one model.\nDeng et al. [12] predicted course performance of 103 students based on their activity (e.g., executed commands or mouse clicks) in a learning environment. The behavioral data, combined with real-time assessments, were used to train a naive Bayes classifier to predict students' grade category. At-risk students (the worst grades) were predicted with 90.9% accuracy. Similarly to Vinlove et al. [2], this study evaluated only one classifier and reported only the accuracy metric.\nSilva et al. [13] searched for factors impacting the performance of 11 professionals in cybersecurity exercises. Shorter time gaps between participants' answer submissions correlated with submitting incorrect answers, which led to higher task abandonment. Also, participants who often switched between tools submitted more incorrect answers. However, unlike the other two papers, this work explored only statistical models."}, {"title": "B. Other Areas of Computing Education", "content": "Beyond the cybersecurity context, research on student success prediction is considerable. Hellas et al. [6] reviewed 357 articles published by 2018 on predicting performance in computing courses, noting that the best papers \"utilized data from multiple contexts and compared multiple methods\". The shortcomings of the reviewed papers were: lack of clearly defined research questions, single student population, limited discussion of validity, and little sharing of research data. Our work aims to address these shortcomings.\nKoutcheme et al. [4] highlighted methodological issues in predicting unsuccessful students as they progress through a week-by-week course. The research uses student data to predict early in the semester whether a certain student is at risk of becoming inactive in the following week(s). The authors argue that if such predictive models are built using data of all students, including those who have already dropped out (an including approach), the model performance is inflated because the inactive students stop generating new data, which simplifies the model decision-making (i.e., no activity \u2192 likely unsuccessful). This way, the model has access to \u201cfuture\u201d information it would otherwise not have in a realistic situation during the semester. Instead, they argue for an excluding approach: to predict performance in week n + 1 such that only the data of students who are still active in week n are included.\nThe remainder of our literature review covers papers after 2018 to complement Hellas et al. [6]. We divide the publications based on whether they focus on individual exercises or whole courses. The former scope is closer to our work, but it has been rarely explored \u2013 the latter scope dominates the literature."}, {"title": "1) Prediction of Student Success in Exercises", "content": "Arakawa et al. [14] argued that predicting student performance at the course level provides insufficient insight into specific student struggles. They also suggested that features specific to the course or the student demographics may hinder replicating and automating the prediction. Instead, they identified struggling students at the level of programming assignments, using features such as the number of added code lines and the ratio of the passed tests. The study compared four models trained on data of 312 students; the best-performing one was a long short-term memory (LSTM) network with an AUC of 92.2%.\nHicks et al. [15] explored whether the student success on in-class coding exercises can predict success on weekly lab coding exercises. The data for training four predictive models"}, {"title": "2) Prediction of Student Success in Courses", "content": "Koutcheme et al. [4] investigated student performance in three computing courses to compare the including and excluding approach (see Section II-B). Using the data of at least 1657 students, they extracted six types of features to train four models. The performance was better for the including approach. However, the results were inconclusive as to whether certain features are better for either of the two approaches.\nQuille et al. [7] presented 13 years of evolution of a model that employed a multi-institutional dataset for early prediction of student success in introductory programming. Using the data of 692 students, the Naive Bayes classifier (the best out of six models) achieved prediction accuracy of 71%, sensitivity of 75%, and specificity of 66%. In a follow-up work [16], they used a dataset of 472 students that was also multi-national. The three measures of the Naive Bayes classifier improved to 78%, 79%, and 78%. In addition, a decision tree classifier reached 89%, 90%, and 89%, respectively.\nGao et al. [17] mined patterns in process data of 106 students from a programming environment. The patterns were used to train an AdaBoost classifier that predicted student course outcomes with 79% accuracy.\nLeinonen et al. [18] compared two time-on-task metrics: coarse-grained (first keystroke to first exercise submission) and fine-grained (sum of delays between all keystrokes until the first submission). The latter was a stronger predictor of performance. Using data of 132 students, the best model out of three was a Random Forest classifier (97% AUC).\nGordon et al. [19] analyzed data from a learning system for programming. They studied correlations between metrics (such as completion of reading assignments) and student exam performance. A decision tree predicted students at risk of failing the exam with 82% sensitivity and 89% specificity.\nEdwards et al. [20] used students' keystroke data from two different programming courses at two institutions. The Python course at a US university had 265 students, and the Java course at a Finnish university had 303 students. Two Random Forest models (one for each course) were trained using 10-fold cross-validation in 10 runs with different random seeds. The average accuracy of predicting outcomes was 62% in the Python course and 68% in the Java course. Reducing the dataset only to students who attended the course exam improved the latter model (72%), but did not change the former model.\nHigueras et al. [21] predicted performance in a computer science course based on 86 students' interaction with a version control system. The prediction features included the number of commits or code additions/deletions. Naive Bayes and Random Forest were the best-performing classifiers trained on these features, scoring slightly above 80% in accuracy and F\u2081 score."}, {"title": "C. Literature Gaps and Novelty of This Paper", "content": "The novel contributions of our study and unexplored areas compared to the prior work are discussed below.\n1) Focus on Cybersecurity: Liao et al. [22] point out that computing education research has focused on introductory programming, but much less is known about students in follow-up courses. We identified few studies about cybersecurity; 10 out of 13 reviewed papers examine programming education. Although predicting performance in cybersecurity might not considerably differ from programming exercises, since many prediction features are universal, the literature is missing the investigation of whether results in cybersecurity would differ from those in other areas. Quoting Liao et al. [22], there is \"much to gain by studying courses that have not received as much research attention to this point\".\n2) Application of Multi-Contextual Data: Hellas et al. [6] and Liao et al. [22] encourage researchers in prediction modeling to perform studies across institutions, curricula, and semesters. However, multiple datasets from different contexts were not used in previous related research in cybersecurity, and rarely in other domains. Only 3/13 papers included data from more than one institution; 3/13 investigated more than one course, and 4/13 collected data over more than one semester. Our study goes beyond previous work by evaluating prediction models across two learning environments in two countries, throughout multiple courses and semesters in different schools.\n3) Prediction in Smaller Time Frames: 9/13 papers predict success in a course that lasts several weeks. Few papers look at the scope of a single exercise. However, the prediction at this granularity, as in our study, is arguably more difficult than throughout a course, since students generate less data.\n4) Comparison of Various Methods: The papers compared limited number of models, 4 of them reporting the results of only one classifier, often with a limited feature set. We extract a large number and variety of features from two types of security exercise log data to train and evaluate eight models.\n5) Sharing of Research Artifacts: Hellas et al. [6] implore researchers to share research data and code to support repro-ducibility and replicability. However, only 2 of the reviewed papers shared code, and only 1 paper shared the data (from 25 students only) [2]. We make our datasets publicly available."}, {"title": "III. RESEARCH METHODS", "content": "The term exercise denotes a set of complex, multi-step tasks in which the students practice cybersecurity skills. For example, the task can involve scanning open network ports of a computer system. As Figure 1 shows, we studied several exercises with student populations in two learning environments: KYPO CRP [23] and EDURange [24]. Our research quantitatively analyzes data from these two different contexts."}, {"title": "A. Format and Content of the Cybersecurity Exercises", "content": "In KYPO CRP exercises, students breach vulnerable emu-lated hosts using a Kali Linux [25] virtual machine (VM). For this study, we aggregated data from all 12 available exercises because all have the same underlying principles. The exercise instructions are presented via a web interface. In each exercise, the student must complete several linearly ordered tasks and obtain text strings representing the answers. After submitting the correct answer to the web interface, the task is completed, and the student proceeds to the next task. If a student lacks the time or knowledge to complete a task, they can use the web interface to display the step-by-step solution [26].\nEDURange includes attack and defense exercises. For this study, we chose the most thoroughly tested exercise, in which the students use Linux command-line tools in a VM to find files, change permissions, etc. The exercises are deployed via a web application and an SSH session, with instructions on the VM as text files. EDURange was designed independently from KYPO CRP and differs in three aspects: (a) students can complete the tasks in any order, (b) step-by-step task solutions are not available, and (c) answers are not submitted and evaluated automatically in a web interface. Instead, EDURange automatically detects when a student has completed a task by comparing student data with known solutions.\nIn both platforms, the student has to complete complex tasks rather than multiple choice or short answer questions, so measuring progress is not straightforward. Like in programming, there can be multiple solutions, but there is no abstract syntax tree or unit tests to look at. Therefore, we use machine learning to discover patterns within these complex data of students."}, {"title": "B. Data Collection in the Two Learning Platforms", "content": "In KYPO CRP, two data types are collected for each student: command logs (all commands executed inside an interpreter such as Bash, Z shell, and Metasploit; with metadata such as timestamps) [27] and event logs (interactions with the web interface of KYPO CRP, such as submitting an answer or displaying a solution). The data collection instrument is open-source software [27]. An example dataset, including a detailed explanation of its format, is publicly available as well [28].\nEDURange uses a single logging format that stores character stream data from a terminal. The command-line entries are automatically filtered and parsed, resulting in labeled log data. The first log entry of a student is triggered with the first submitted command. Each log entry consists of unique identifiers for the course, student, and task; a timestamp; and command input and output. Input is parsed into its command name (tool), options, and arguments. These pieces are compared, component-wise, to a set of known solutions, in order to match student behaviors to known actions."}, {"title": "C. Data Collection for This Study", "content": "KYPO CRP accommodated 244 students from May 2020 to May 2022. The participants were cybersecurity professionals, computer science students from three European universities, and senior high school students. Some exercises were parts of the university courses on cybersecurity, where they constitute a mandatory lab session in which all students participate. No grade is given for the exercise, since it is only a practice session during the semester, and students are not penalized for mistakes. Other exercises were deployed during optional extracurricular activities such as educational events for popularizing cybersecurity \u2013 hence the diverse population. Sessions of both types were either in-person or remote because some of them took place during the COVID-19 restrictions. Each exercise usually lasted 2\u20133 hours. In rare cases, some students finished the exercise at home within 24 hours of the start.\nIn EDURange, we collected data from 69 students during three undergraduate computer science courses offered at Lewis & Clark and The Evergreen State College between Fall 2020 and Fall 2022. Sometimes, the exercises were used as labs toward credit in these courses, and other times as optional workshops or extra credit assignments. Again, the learning modality (in-person or remote) varied. Most exercises were completed during a 2-hour lab session, but students were allowed to return to their work outside of the lab \u2013 in some cases, even the next scheduled class session within 1-3 days.\nDuring all exercises in both platforms, the students were allowed to use online resources, but were instructed to work individually. However, brief collaboration, such as when a student asked their neighbor a question, could not always be prevented. If a student needed help, and received it from another student, the exercise logs indicate a successfully solved task. As in most past studies, we cannot detect student cross-talk from the logs. However, since the goal is to recognize students who need timely help from the instructor, a student who can finish the task after asking for help from another student is ultimately classified correctly if they are classified as successful."}, {"title": "D. Data Privacy and Research Ethics", "content": "In both platforms, the participants received a written explanation that their anonymized exercise activity data may be used for educational research. Before starting the exercise, all participants whose data are included in this study gave informed consent, agreeing to this data collection. The collected data were manually checked to ensure they do not contain any personal information that could reveal a student's identity. Likewise, the classification models that result from this work cannot leak any identifiable information about an individual student. Therefore, this research received a waiver from the institutional review boards of the involved universities."}, {"title": "E. Data Cleaning and Filtering", "content": "In addition to the data anonymization, multiple authors subjected the dataset to a thorough manual and automated inspection. We removed rare occurrences of unreasonable values in the data, such as the same command with the same timestamp logged multiple times, which were most likely caused by temporary network outages within the exercise platforms. The final dataset used for the research includes:\n\u2022 For 244 students in KYPO CRP, we have 21,659 command logs and 8,690 event logs from the web interface.\n\u2022 For 69 students in EDURange, we have 4,762 command logs (the platform does not use web event logs).\nThe combined sample includes data from 313 students over the period of more than two years across several semesters. This is well beyond the recommended minimal sample size for prediction studies in computing education: 96 students [7]. We used this dataset for all the subsequent modeling steps. While we acknowledge the potential risks of including the complete dataset for making predictions (see the discussion of Koutcheme et al. [4] in Section II-B), we argue that this decision makes sense in our context for three reasons.\nFirst, Koutcheme et al. [4] operate in the time frame of weeks in a semester (and argue that this extrapolates to days in a week). When a student becomes inactive for several days, it is reasonable to assume that the student has most likely dropped out and exclude their data. However, our context is a single, one-time exercise session that lasts a few hours. When a student becomes inactive during this exercise (i.e., no action is logged for a certain time), the log data alone do not provide ground truth for deciding whether this student dropped out or will continue later. For example, the student can be discussing something with the instructor, experiencing technical difficulties with the exercise platform, or simply taking a small break.\nSecond, in our context, any criterion for rejecting student data as inactive would be arbitrary and hard to justify from the data (see Kovanovi\u0107 et al. [29] discuss the challenges of choosing cut-offs for inactivity within interaction data).\nThird, since our goal is to classify whether a student will finish the exercise, if we eliminated the data of students who stopped working, our training set would include (almost) exclusively successful students. So, the model would have (almost) no data based on which to learn to predict failure. Partial data of students who dropped out of the exercise are valuable, as they capture the behavior of unsuccessful students.\nUltimately, inactive students are a part of the learner population and naturally occur in teaching contexts. Thus, continuing to use such data better reflects the model's potential practical use. However, to provide an empirical comparison, we will also present results on a subset (half) of the dataset."}, {"title": "F. Definition of Class Labels and Data Labeling", "content": "Our goal is to predict if a student needs help. The exercise outcome is represented with a binary class variable (label). This label is set to 1, which is a positive class that we want to detect, if a student was not successful (i.e., potentially at risk); and 0, the negative class, indicating the student succeeded.\nExercise success is defined as at least 50% completion. In KYPO CRP, this means that the student (a) did not display the solutions for more than 50% of the tasks in the exercise and (b) submitted the correct answers for all tasks (whether discovered by the student or offered by the solution). In EDURange, this is simply finishing at least half of the tasks (rounding down when the exercise had an odd number of tasks).\nWhile this cut-off is low, it was chosen because it represents a minimal completion of the exercises, and we want to focus on identifying students who need help the most, i.e., those who are unable to reach even 50%. This is consistent with the goal of the exercises, which is to provide a learning experience to undergraduates (often cybersecurity beginners), not to evaluate student performance for a letter grade.\nAlthough this choice makes the dataset imbalanced towards successful students, this reflects the settings in which both platforms were employed. The 50% threshold is the lowest passing grade in most courses at the university where KYPO CRP is used. In addition, the second half of the tasks are highly challenging compared to the first half, so setting the threshold higher might lead to more predictions of failure than would be actionable for the instructor."}, {"title": "G. Feature Extraction and Selection", "content": "For KYPO CRP data, we engineered 25 features in pre-liminary work [30]. The features were derived only from the exercise problem-solving. Like Edwards et al. [20], we did not use (or even collect) student personal information. Next, all features were unitized (before model training to avoid data leakage). Then, we applied automated feature selection using L1-regularized linear models [31], which pruned the feature set before the training phase of each model (see Section III-H1).\nTable II lists the 25 selected features and their descriptive statistics. Most of the features are analogous to those identified in the review by Hellas et al. [6] (e.g., time on task, number of attempts, and correctness), which are commonly used in the literature and also generalize outside the cybersecurity context.\nEDURange followed the same process, engineering 15 features. The difference reflects the absence of web interface log data. On the other hand, EDURange produced features about command execution failure from the command-line output stream that were not available in KYPO CRP."}, {"title": "H. Model Training and Evaluation", "content": "Datasets from both platforms were used for modeling separately, since the feature sets differ. However, we use the same analysis framework: the code for processing the datasets and training the models is the same, with slight adjustments to account for each platform's specifics. Cross-platform research in learning analytics almost inherently has this property, unless features that do not perfectly map are discarded.\nWe systematically compared the performance of eight classifiers: logistic regression, naive Bayes, support vector machines (with linear or RBF kernel), K-nearest neighbors, decision tree (CART), Random Forest, and XGBoost. We selected these standard models because, compared to deep learning models, simpler classifiers are more interpretable [32], which is suitable for educational purposes, and usually require less data. All implementations come from the Python library scikit-learn [33], only XGBoost has a separate package [34]."}, {"title": "1) Cross-validation", "content": "We used nested student-level cross-validation [35], [36] (rather than allocating a holdout test set). This method was chosen since EDURange dataset was smaller, and we wanted to use the same training process for both.\nThe inner cross-validation loop is used for feature selection and hyperparameter tuning. Both procedures are performed on the training set, split into training and validation folds. The best model is selected by the outer loop automatically among all models from the inner loop, by evaluating them on the test data (not used for feature selection or tuning for that fold).\nBoth loops use stratified k-fold cross-validation to account for the label imbalance. The value for k is commonly set to 10 for the outer loop [7], [20], [22], which we chose as well, and a smaller number for the inner loop [35] (we chose 5).\nSince the test set split is used only for model evaluation, not feature selection or hyperparameter tuning, and data from one student are not divided between the training and test set split (i.e., we use student-level cross-validation), there is no information leakage."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "Table IV reports the model performance for KYPO CRP and Table V for EDURange. The values are macro-averages across the scores of the 10 models trained in the outer cross-validation loop. All eight models performed much better than the baseline."}, {"title": "A. RQ1: Classifier Performance in KYPO CRP", "content": "The decision tree has the highest balanced accuracy (88.4%) and sensitivity (86.9%). The best specificity (96.1%) was achieved by Random Forest, which also had the best AUC (93.1%), and Nearest neighbors, which was less suitable due to having the lowest sensitivity (71.4%).\nThe decision tree is also the least biased toward a particular class (it has the smallest absolute difference between sensitivity and specificity, 3.1%). This is desirable since we want to maximize the ability to detect at-risk students while minimizing the number of students incorrectly identified as struggling.\nThe difference between the balanced accuracy across the models is small: only 5.2% between the highest and lowest-scoring classifier. Similarly, the difference between the highest and lowest AUC is low: 4.2%. This suggests that while some classifiers are more suitable for the given context, none are entirely unusable, which is a good sign.\nSurprisingly, experimenting with a subset of the dataset (using 50% of each log file to simulate the students being somewhere in the middle of the exercise) did not deteriorate the models substantially. The balanced accuracy dropped by less than 0.1, and AUC by less than 0.05. In one model (logistic regression), the two metrics even marginally improved."}, {"title": "B. RQ2: Comparison With EDURange", "content": "Our next question was to evaluate if and how the results change when the methods are applied to another context. Despite differences in the number of students and features, EDURange results are generally consistent with KYPO CRP. High values for all metrics were achieved by the same classifiers. Again, decision tree reached the highest balanced accuracy (82%) and sensitivity (90%). It also had an AUC of 82.6%: well above the baseline. However, its 16% difference between sensitivity and specificity indicates some bias.\nRandom Forest performed the second best, achieving a balanced accuracy of 78.6% and an AUC of 85.3%. Both SVMs performed well, with a balanced accuracy of 78.5% and 78.1%. SVM with the linear kernel was favorable given the distribution of our dataset, since it had lower absolute difference between sensitivity and specificity (6.3%) and higher AUC (84.3%).\nSince KYPO CRP uses features derived from its web interface, the two systems favored different features for the most part, but commands per minute and the average number of commands used to complete a task were used in both contexts. For EDURange, those features were chosen by many models.\nUsing 50% of the dataset again deteriorated the models only slightly (balanced accuracy by up to 0.13 and AUC by up to 0.09), and even improved several models.\nDespite some differences, the EDURange features were largely equivalent to KYPO CRP when possible. Overall, the feature sets are comparably rich. EDURange ran the modeling with fewer data, and its logs captured only command-line activity. The slightly worse performance indicates that having additional web interface data improves the predictive power."}, {"title": "C. Limitations and Threats to Validity", "content": "1) Internal Validity: Any threshold that separates successful and at-risk students (including our setting of 50% completion) is an arbitrary choice that affects the results. However, there is no theoretical basis or a \"gold standard\" in the literature to determine exactly when to consider a student as struggling [4]. Regardless of where the cut-off is set, students who are near the threshold may belong in either category (e.g., two similar students who achieve a score of 51% and 49% end up classified into separate categories). As a result, the class labels (\"unsuccessful\u201d and \u201csuccessful\u201d) should be treated with some caution. However, this is a limitation of any binary classification. For example, Castro-Wunsch et al. [39] also used a 50% cut-off.\nEdwards et al. [20] used a median split, predicting whether the student will be in the top or bottom half performance-wise. Liao et al. [22] defined a 40% cut-off, remarking that it can be adjusted to \"trade off the sensitivity and specificity of a given model\". Ultimately, the threshold will vary according to the context relevant for a particular course or exercise.\n2) External Validity: Exercise sessions sometimes differed in aspects such as student demographics, instructor, or modality. However, these changes are natural in field research in computing education. As Liao et al. [22] argue, it is unrealistic to expect that all conditions will remain constant across all teaching sessions. Moreover, due to the different design of the two platforms (which were developed before this study was considered), the two feature sets were not the same, despite having a substantial overlap. On the positive side, these differences may enhance generalizability.\nResults of computing education research might not always transfer to a different context [40]. Our models were trained and evaluated only on the two presented datasets. Therefore, we cannot make reliable claims regarding the generalizability to other exercises or platforms. Nevertheless, if exercises in other platforms allow quantifying student success, our methods can be applied with minimal modifications, since the source code is available (see Section V-B)."}, {"title": "D. Implications for Teaching Practice", "content": "The classifiers can be trained for other exercise environments and then deployed to detect unsuccessful students. To illustrate, suppose the best model was deployed in KYPO CRP. The model has a sensitivity of 0.869, meaning that of all at-risk students, it can correctly classify 86.9% of them. Next, the model has a specificity of 0.900, so it can correctly classify 90% of all students who are actually successful. Finally, given a successful and unsuccessful student, the model with the AUC of 0.921 can accurately distinguish them in 92.1% of cases.\nAs evidenced in the raw logs, students perform hundreds of exercise actions, which is far too much for the instructor to evaluate manually. Therefore, the detectors would help direct the instructor's attention to students who need advice. Even if some students are misclassified, the rate of false positives/negatives is manageable in hands-on cybersecurity courses, which tend to have dozens (not hundreds) of students.\nA learning environment providing this detection would allow instructors to interact with students on a one-to-one basis, even within a large exercise, in-person or online. It can also counteract implicit bias because it removes the need for students to request help and for instructors to choose whom to monitor.\nThe classification algorithms have no explicit information about demographics, as recommended by recent literature [41].\nFrom the technical perspective, developers of cybersecurity learning environments can implement logging of data that pro-duce the most significant features. These include the number of commands, errors made, and the timing of answer submissions, as well as the exercise metadata, such as requesting solutions."}, {"title": "V. CONCLUSIONS", "content": "Identifying students who are at risk of performing poorly is essential for providing targeted interventions. To the best of our knowledge, only a few studies explored student performance prediction in cybersecurity exercises, and none of these studies were conducted across two platforms. We attempted to bridge this gap by using student activity data from KYPO CRP and EDURange platforms to determine student success. Specifically, we employed classification to assess how well the features extracted from the activity data predict exercise outcomes. Evaluating eight models for the two platforms demonstrated that predicting student success based on exercise data is a promising approach that generalizes across contexts."}, {"title": "A. Open Research Challenges", "content": "The goals of the prediction can be modified in various ways. For example, future work can aim to discover specific tasks in the exercise on which the student will struggle. Classifying whether a student will complete a particular task eliminates the need to set arbitrary thresholds for success. Alternatively, the goal can be reframed as ranking the students based on how likely they are to need help, determining the priority for the instructor. Ultimately, future work should evaluate the practical deployment of these methods in a classroom.\nAnother scope of future work is detecting at-risk students as early as possible. The earlier an accurate prediction is made, the more beneficial it is [4], [7], since instructors can intervene to support students quickly. Others explored this problem within an entire semester, achieving promising results using week-by-week data [4], [22]. However, we are not aware of publications in a security context within a smaller time frame of a short exercise. An open challenge is therefore employing meaningfully selected data only from a subset of the exercise."}]}