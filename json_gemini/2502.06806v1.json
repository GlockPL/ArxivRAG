{"title": "Logits are All We Need to Adapt Closed Models", "authors": ["Gaurush Hiranandani", "Haolun Wu", "Subhojyoti Mukherjee", "Sanmi Koyejo"], "abstract": "Many commercial Large Language Models (LLMs) are often closed-source, limiting developers to prompt tuning for aligning content generation with specific applications. While these models currently do not provide access to token logits, we argue that if such access were available, it would enable more powerful adaptation techniques beyond prompt engineering. In this paper, we propose a token-level probability reweighting framework that, given access to logits and a small amount of task-specific data, can effectively steer black-box LLMs toward application-specific content generation. Our approach views next-token prediction through the lens of supervised classification. We show that aligning black-box LLMs with task-specific data can be formulated as a label noise correction problem, leading to Plugin model an autoregressive probability reweighting model that operates solely on logits. We provide theoretical justification for why reweighting logits alone is sufficient for task adaptation. Extensive experiments with multiple datasets, LLMs, and reweighting models demonstrate the effectiveness of our method, advocating for broader access to token logits in closed-source models. Our code can be found at this link.", "sections": [{"title": "1 Introduction", "content": "The rise of Large Language Models (LLMs) has revolutionized generative AI, yet the most capable models are often closed-source or black-box [Achiam et al., 2023, Bai et al., 2022a]. These models generate text based on input prompts but keep their internal weights and training data undisclosed, limiting transparency and customization. Despite these constraints, closed-source LLMs are widely adopted across applications ranging from travel itinerary generation to tax advice, with developers largely relying on prompt optimization to achieve domain-specific outputs.\nHowever, this reliance on prompt engineering is insufficient for specialized tasks, e.g., those requiring brand-specific tone or style. Consider a content writer aiming to generate product descriptions that reflect a brand's unique identity. Black-box LLMs, trained on broad datasets, often fail to meet such nuanced requirements. With access limited to generated tokens, developers resort to zero-shot [Kojima et al., 2022] or few-shot [Song et al., 2023] prompting techniques. However, if model weights were accessible, advanced techniques like Parameter-Efficient Fine-Tuning (PEFT) using LORA [Hu et al., 2021], QLORA [Dettmers et al., 2024], prefix tuning [Li and Liang, 2021], or adapters [Hu et al., 2023a] could be employed for fine-tuning. Yet, due to intellectual property concerns and the high costs of development, most commercial LLMs remain closed-source, and even with API-based fine-tuning options, concerns over data privacy discourage developers from sharing proprietary data."}, {"title": "2 Preliminaries", "content": "We begin by establishing the notation. The index set is denoted as $[c] = \\{1, . . ., c\\}$ for any positive integer $c$. Vectors are represented in boldface, for example, $\\mathbf{v}$, while matrices are denoted using uppercase letters, such as $V$. The coordinates of a vector are indicated with subscripts, for instance, $v_j$. The all-ones vector is denoted by $\\mathbf{1}$, with its size being clear from the context. The $c$-dimensional simplex is represented as $\\Delta^{c-1} \\subset [0, 1]^c$. Finally, a sequence $(x_t, X_{t-1},...,x_1)$ of size $t$ is denoted by $X_{t:1}$.\nWe assume access to language data for the target task, while the black-box LLM, trained on broad world knowledge, is treated as having learned from a noisy version of this data. Our objective is to adapt the black-box model to align with the task-specific distribution. To formalize this, we extend the label-noise framework from supervised classification [Patrini et al., 2017] to the decoder-only language modeling setting.\nDecoder-only models are trained using a next-token prediction objective. At each step, this setup resembles a supervised classification problem with $|V|$ classes, where $V$ is the vocabulary of tokens. Formally, the label space at step $t$ is $X_\\tau = \\{e^i : i \\in [|V|]\\}$, where $e^i$ denotes the $i$-th standard canonical vector in $\\mathbb{R}^{|V|}$, i.e., $e^i \\in \\{0,1\\}^{|V|}, \\mathbf{1}^T e^i = 1$. The task at each step $t$ is to predict the next token $x_t$ (denoted as one-hot vector) given a sequence of tokens $x_{t-1:1}$.\nOne observes examples $(x_t,x_{t-1:1})$ drawn from an unknown distribution $p^*(x_t, X_{t-1:1}) = p^*(x_t|X_{t-1:1})p^*(X_{t-1:1})$ over $V \\times V^{[t-1]}$, with expectations denoted by $\\mathbb{E}^*_{x_t,X_{t-1:1}}$. The standard cross-entropy loss is typically used for training over the vocabulary tokens. Assuming access to token logits, and thus the softmax outputs, from the black-box LLM, we interpret the softmax output as a vector approximating the class-conditional probabilities $p^*(x_t|x_{t-1:1})$, denoted as $\\mathbf{b}(x_t|x_{t-1:1}) \\in \\Delta^{|V|-1}$.\nTo quantify the discrepancy between the target label $x_t = e^i$ at step $t$ and the model's predicted output, we define a loss function $\\ell : V \\times \\Delta^{|V|-1} \\rightarrow \\mathbb{R}$. A common choice in next-token prediction tasks is the cross-entropy loss:"}, {"title": "3 Loss Robustness", "content": "We extend label noise modeling to the autoregressive language setting, focusing on asymmetric or class-conditional noise. At each step $t$, the label $x_t$ in the black-box model's training data is flipped to $x'_t \\in V$ with probability $p^*(x'_t|x_t)$, while preceding tokens $(x_{t-1:1})$ remain unchanged. As a result, the black-box model observes samples from a noisy distribution: $p^*(x'_t, x_{t-1:1}) = \\sum_{x_t} p^*(x'_t|x_t)p^*(x_t|x_{t-1:1})p^*(x_{t-1:1})$.\nWe define the noise transition matrix $T_t \\in [0, 1]^{|V|\\times |V|}$ at step $t$, where each entry $T_{t_{ij}} = p^*(x_t = e^j|x_t = e^i)$ represents the probability of label flipping. This matrix is row-stochastic but not necessarily symmetric.\nTo handle asymmetric label noise, we modify the loss $\\ell$ for robustness. Initially, assuming a known $T_t$, we apply a loss correction inspired by [Patrini et al., 2017, Sukhbaatar et al., 2015]. We then relax this assumption by estimating $T_t$ directly, forming the basis of our Plugin model approach.\nWe observe that a language model trained with no loss correction would result in a predictor for noisy labels $\\mathbf{b}(x_t|x_{t-1:1})$. We can make explicit the dependence on $T_t$. For example, with cross-entropy we have:"}, {"title": "3.1 Estimation of Transition Matrix", "content": "We assume access to a small amount of target language data for the task. Given that the black-box model is expressive enough to approximate $p^*(x_t | X_{t-1:1})$ (Assumption (2) in Theorem 3 of Patrini et al. [2017]), the transition matrix $T_t$ can be estimated from this target data. Considering the supervised classification setting at step $t$, let $X_i$ represent all target data samples where $x_t = e^i$ and the preceding tokens are $(x_{t-1:1})$. A naive estimate of the transition matrix is: $T_{t_{ij}} = \\mathbf{b}(x_t = e^j|x_t = e^i) = \\frac{1}{\\left|X_i\\right|} \\sum_{x \\in X_i} \\mathbf{b}(x_t = e^j|x_{t-1:1})$.\nWhile this setup works for a single step $t$, there are two key challenges in extending it across all steps in the token prediction task:\n1.  Limited sample availability: The number of samples where $x_t = e^i$ and the preceding tokens $(x_{t-1},...,x_1)$ match exactly is limited in the clean data, especially with large vocabulary sizes (e.g., $|V| = O(100K)$ for LLaMA [Dubey et al., 2024]). This necessitates modeling the transition matrix as a function of features derived from $x_{t-1:1}$, akin to text-based autoregressive models.\n2.  Large parameter space: With a vocabulary size of $|V| = O(100K)$, the transition matrix $T_t$ at step $t$ contains approximately 10 billion parameters. This scale may exceed the size of the closed-source LLM and cannot be effectively learned from limited target data. Therefore, structural restrictions must be imposed on $T_t$ to reduce its complexity.\nTo address these challenges, we impose the restriction that the transition matrix $T_t$ is diagonal. While various constraints could be applied to simplify the problem, assuming $T_t$ is diagonal offers two key advantages. First, it allows the transition matrix\u2014effectively a vector in this case\u2014to be modeled using standard autoregressive language models, such as a GPT-2 model with $k$ transformer blocks, a LLaMA model with $d$-dimensional embeddings, or a fine-tuned GPT-2-small model. These architectures can be adjusted based on the size of the target data. Second, a diagonal transition matrix corresponds to a symmetric or class-independent label noise setup, where $x_t = e^i$ flips to any other class with equal probability in the training data. This assumption, while simplifying, remains realistic within the framework of label noise models."}, {"title": "4 Proposed Method: The Plugin Approach", "content": "To estimate the autoregressive transition vector, we train an autoregressive language model on target data, which operates alongside the black-box model during inference. This model acts as an autoregressive reweighting mechanism, adjusting the token probabilities produced by the black-box model. The combined approach, integrating probabilities from the black-box and reweighting models, is referred to as the Plugin model. The term Plugin is inspired by classification literature, where plugin methods reweight probabilities to adapt to distribution shifts [Koyejo et al., 2014, Narasimhan et al., 2014, 2015, Hiranandani et al., 2021]. We now detail the training and inference phases, summarized in Algorithm 1 (Appendix A) and illustrated in Figure 1."}, {"title": "4.1 Training the Plugin Model", "content": "During each training iteration, a sequence $s$ of $m$ tokens is passed through both the black-box model and the reweighting model to obtain token probabilities $\\{b_1,b_2,..., b_m\\}$ and $\\{r_1,r_2,........,r_m\\}$, respectively, where each $b_i, r_i \\in \\Delta^{|V|-1}$. The final token probability from the Plugin model is computed by normalizing the element-wise product of these probabilities:\n        \n      \n  \n      $p_i = \\frac{b_i \\circ r_i}{\\|b_i \\circ r_i\\|_1}$\n        \n      \n    \n\n(4)\nThe sequence-level cross-entropy loss is given by:\n        \n      \n  \n      $\\ell_s = \\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^{|V|} \\log(p_i)^T e_j$,\n        \n      \n    \n\n(5)\nwhere the $j$-th token appears at the $i$-th position in the sequence $s$. During backpropagation, only the reweighting model parameters are updated, while the black-box model remains frozen. This formulation extends naturally to batch training, refining $\\mathbf{r_i}$ over iterations to approximate the transition vector governing label shifts in the target data."}, {"title": "4.2 Inference from the Plugin Model", "content": "Given a fully trained reweighting model and access to the black-box model, token generation proceeds autoregressively. At the first step, the black-box model produces token probabilities $b_1$, while the reweighting model outputs $r_1$. The Plugin model selects the first token as $x_1 = \\text{argmax}_y (b_1 r_1)$. For subsequent steps, given the previously generated tokens $x_{t-1:1}$, we obtain probabilities $b_t$ from the black-box model and $r_t$ from the reweighting model. The Plugin model then predicts the next token as: $x_t = \\text{argmax}_y (b_t r_t)$.\nThe process continues until a stopping criterion is met. Note that, this manuscript focuses on greedy decoding for inference. Other decoding strategies, such as temperature scaling, top-p sampling, or beam search, can be incorporated by normalizing the element-wise product of probabilities and using them as the final token distribution, as in (4)."}, {"title": "5 Theoretical Analysis", "content": "We establish the convergence properties of the Plugin model, demonstrating that after training for $t$ tokens, it accurately estimates the autoregressive noise transition matrix. We model the transition matrix as a function of an unknown parameter $\\theta^*$ and show that, after optimizing the autoregressive loss over a sequence of tokens, the Plugin model estimates $\\theta^*$ with high probability.\nLet $\\mathcal{F}_{t-1}$ denote the history of selected tokens up to time $t-1$. Let an unknown parameter $\\theta^* \\in \\Theta \\subseteq \\mathbb{R}^d$ governs the transition dynamics of label flipping between token pairs. The transition matrix at time $t$, denoted as $T_t(\\theta^*|\\mathcal{F}_{t-1})$, depends on $\\theta^*$ and all previously observed tokens. Before proving our main result, we first make a few assumptions."}, {"title": "6 Related Work", "content": "Parameter-Efficient Fine-Tuning (PEFT). PEFT methods adapt LLMs to downstream tasks while minimizing computational overhead. LoRA [Hu et al., 2021] and QLoRA [Dettmers et al., 2024] introduce low-rank updates and quantization for efficient fine-tuning, while prefix tuning [Li and Liang, 2021], adapters [Hu et al., 2023b], and soft prompting [Lester et al., 2021] modify task-specific representations through trainable layers or embeddings. However, these methods require access to model weights, gradients, or architecture details, making them unsuitable for closed-source LLMs and inapplicable as baselines in our setup. In contrast, our approach operates solely on token logits, enabling adaptation without modifying the underlying model.\nSteering and Aligning LLMs. LLM alignment methods primarily use reinforcement learning or instruction tuning. RLHF and DPO [Christiano et al., 2017, Ouyang et al., 2022, Rafailov et al., 2024] optimize model behavior via human preferences, with DPO eliminating reward modeling. Constitutional AI [Bai et al., 2022b] aligns models using self-generated principles, while instruction tuning [Wei et al., 2021, Sanh et al., 2022] adapts them via task-specific demonstrations. Unlike our approach, these methods require model weights and training data, limiting their applicability as baselines in our setup.\nCalibration of LLMs. LLM calibration methods aim to align model confidence with predictive accuracy and adjust confidence scores but do not alter token predictions [Ulmer et al., 2024, Shen et al., Huang et al., 2024, Kapoor et al., 2024, Zhu et al., 2023]. In contrast, our method reweights token probabilities at inference, enabling adaptation of black-box LLMs without modifying the model or requiring fine-tuning.\nBlack-box LLMs. Prior work explores various approaches for adapting black-box LLMs without fine-tuning, though they differ fundamentally from our method. [Gao et al., 2024] infer user preferences through interactive edits but do not adapt models based on past language data. Diffusion-LM [Li et al., 2022] formulates text generation as a non-autoregressive denoising process, whereas our approach reweights token probabilities autoregressively without requiring black-box model weights. Discriminator-based methods [Dathathri et al., Mireshghallah et al., 2022, Yang and Klein, 2021, Krause et al., 2021] control generation based on predefined attributes, contrasting with our method, which enables free-form text adaptation. DExperts [Liu et al., 2021] combines expert and anti-expert probabilities; we incorporate a similar probability combining strategy in a modified baseline without a de-expert component. In-context learning [Long et al., 2023, Dong et al., 2024] offers a common adaptation technique for black-box models and serves as a baseline in our setup."}, {"title": "7 Experiments", "content": "We divide this section into three parts. Section 7.1 evaluates Plugin on four text generation datasets across three black-box language models. Since the Plugin model is trained on top of black-box models, we refer to black-box models interchangeably as base models. Section 7.2 presents ablation studies analyzing the impact of black-box model quality, Plugin's complexity, and architecture choices. Section 7.3 shows qualitative analysis and case studies.\nWe evaluate Plugin on four text generation benchmarks: (a) E2E NLG [Du\u0161ek et al., 2020], (b) Web NLG [Gardent et al., 2017], (c) CommonGen [Lin et al., 2020], and (d) the Adidas product description dataset [adi, 2023]. For the first three datasets, we use the standard train-validation-test splits from the Transformers library [Wolf, 2020]. To introduce distribution shifts, we filter Web NLG's training data to include only infrastructure descriptions, while validation and test sets retain person descriptions. Similarly, CommonGen's training set is restricted to samples containing man, while validation and test sets remain unchanged. Details of this setup are in Section 7.3. The Adidas dataset is split into validation and test sets. Dataset statistics are provided in Table 5, Appendix C.1."}, {"title": "7.1 Text Generation Performance Comparison", "content": "We evaluate Plugin on the text generation task using only the validation and test splits of all four datasets, reserving the train split for ablation studies (Section 7.2). Plugin and baseline models are trained on the small validation set, with performance measured on the test set. Additionally, we allocate 40% of the validation data as hyper-validation for cross-validation of hyperparameters.\nPerformance is reported using seven standard natural language generation metrics: (a) BLEU [Papineni et al., 2002], (b) ROUGE-1 [Lin, 2004], (c) ROUGE-2 [Lin, 2004], (d) ROUGE-L [Lin and Och, 2004], (e) METEOR [Banerjee and Lavie, 2005], (f) CIDEr [Vedantam et al., 2015], and (g) NIST [Doddington, 2002]. All experiments are repeated over five random seeds, and we report the mean and standard deviation for each metric.\nWe compare Plugin with the following baselines: (a) Zeroshot: The black-box model directly performs text generation without additional adaptation. (b) ICL-1 [Long et al., 2023]: One randomly selected validation sample is used as an in-context example. (c) ICL-3 [Long et al., 2023]: Three randomly selected validation samples are used as in-context examples. (d) NewModel: A new language model is trained using the validation data. (e) WeightedComb [Liu et al., 2021]: A new model is trained alongside the black-box model, with token probabilities computed as $\\alpha n + (1 - \\alpha) b$,"}, {"title": "7.2 Ablation Study", "content": "We now show ablation studies that reflect various aspects of the Plugin model. We display the results using GPT2-M as base model on the E2E NLG dataset. The observation is similar on other base models and datasets (Appendix C.5).\nImpact of Base Model Quality. We fine-tune GPT2-M for varying epochs, denoted as 1FT (one epoch), 2FT (two epochs), and 5FT (five epochs), and train a Plugin model for each. Figure 2 shows that as the base model's task-specific quality improves, the Plugin's performance improves.\nComplexity of the Reweighting Model in Plugin. We train Plugin models with reweighting architectures varying from 1 to 12 transformer layers while keeping other configurations unchanged. Additionally, we train a variant where the reweighting model is initialized with GPT2-Small. As shown in Figure 3, a single-layer reweighting model yields significant improvements over the base GPT2-M model, while additional layers (e.g., 2, 4, 8, 12) offer diminishing returns and slight performance decline due to overfitting on the small validation set of E2E NLG. This suggests that more data is required for learning complex reweighting models. Notably, initializing with a pretrained GPT2-Small substantially improves performance, underscoring the advantage of using small pretrained models for reweighting due to their inherent autoregressive properties."}, {"title": "7.3 Qualitative Analysis and Case Study", "content": "Plugin adapting to distribution shift. We evaluate Plugin on distribution-shifted Web NLG and CommonGen using LLaMA-3.1-8B as the base model. Web NLG training data contains only Infrastructure concepts, while validation and test sets include Person concepts. Similarly, CommonGen training data features man, whereas validation and test sets contain both man and woman. The base model is fine-tuned on training data, and Plugin is trained on validation data using the fine-tuned model as the base, ensuring that Plugin corrects biases learned from training.\nUsing GPT-40 [Hurst et al., 2024] as an evaluator, the fine-tuned Web NLG model generates only 17.99% Person-related sentences, while Plugin increases this to 71.34%. On CommonGen, the fine-tuned model generates 10.37% Woman-related sentences, whereas Plugin improves this to 31.92%."}, {"title": "8 Conclusion", "content": "We propose Plugin, a token-level probability reweighting framework that adapts black-box LLMs using only logits and small task-specific data. Framing next-token prediction as a label noise correction problem, we demonstrate both theoretical guarantees and empirical effectiveness across multiple datasets and models. Our findings highlight the potential of logit-based adaptation and advocate for broader access to token logits in closed-source LLMs."}, {"title": "Impact Statement", "content": "The ability to adapt closed-source LLMs without modifying their weights has significant implications for both research and industry. Our proposed method, which leverages token logits for task-specific alignment, offers a practical solution for developers constrained by black-box APIs. This approach enhances customization, allowing models to generate more domain-relevant and controlled content while preserving the privacy and security of proprietary data. Furthermore, by advocating for broader access to token logits, this work fosters greater transparency and flexibility in commercial LLMs. The findings also highlight the importance of mitigating biases in black-box models, contributing to more equitable and context-aware language generation across diverse applications.\nWhile Plugin effectively adapts black-box LLMs, it has some limitations, too. Since it only reweights token probabilities without modifying internal representations or embeddings, it may struggle with tasks requiring deep structural adaptations, such as executing complex reasoning. Further research on this aspect is needed. Additionally, although Plugin avoids full fine-tuning, training a separate reweighting model introduces computational overhead compared to prompt tuning or in-context learning, with efficiency depending on the complexity of the reweighting model and the availability of task-specific data."}, {"title": "A Algorithm Details", "content": "We provide summarized form of the training and inference algorithm for the Plugin model below.\n\nInput: Black-box model B, reweighting model R, clean training data D, vocabulary V\nOutput: Plugin model predictions 21:7 for a given sequence\n1: Training Phase:\n2: for each sequence s \u2208 D do\n3: Compute token probabilities {b1, b2, ..., bm} using B.\n4: Compute token probabilities {r1,r2,\u2026\u2026\u2026, rm} using R.\n5: Combine probabilities: $p_i = \\frac{b_i \\circ r_i}{\\|b_i \\circ r_i\\|_1}$ for i \u2208 [m].\n6: Compute sequence loss $\\ell_s = -\\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^{|V|} \\log(p_i)^T e_j$.\n7: Update parameters of R using back-propagation. Freeze B.\n8: end for\n9: Inference Phase:\n10: Initialize sequence x1:T = {}.\n11: for each token position t = 1 to T do\n12: Compute token probabilities bt using B.\n13: Compute token probabilities rt using R.\n14: Combine probabilities: $p_t = \\frac{b_t \\circ r_t}{\\|b_t \\circ r_t\\|_1}$.\n15: Predict token: xt = argmaxy(pt).\n16: Append xt to X1:T.\n17: end for\n18: Return: x1:T"}, {"title": "B Proof of Main Convergence Theorem", "content": "We define the following assumption on the smoothness and regularity of the loss function.\nAssumption B.1. We assume the following assumptions hold with probability 1:\n1.  (Convexity of \\ell_s): The loss function \\ell_s is convex for all time s \u2208 [t].\n2.  (Smoothness of \\ell_s): The \\ell_s is smooth such that the first, second, and third derivatives exist at all interior points in \u0398.\n3.  (Regularity Conditions):\n    (a) is compact and \\ell_s (\u03b8) is bounded for all \u03b8 \u2208 \u0398 and for all s \u2208 [t].\n    (b) \u03b8* is an interior point in \u0398.\n    (c) \u22072ls (0) is positive definite, for all s \u2208 [t] .\n    (d) There exists a neighborhood B of 0* and a constant C1, such that \u22072ls(0) is C1 -Lipschitz. Hence, we have that $||\\nabla^2 \\ell_s (\\theta) - \\nabla^2 \\ell_s (\\theta')|| \\leq C_1 ||\\theta - \\theta'||^2_{L^2 (\\Theta_*)}$, for \u03b8, \u03b8' in this neighborhood.\n4.  (Concentration at 0%): We further assume that $||\\nabla \\ell_s (\\theta_*) || \\leq C_2$ hold with probability one.\nLemma B.2. (Proposition 2 of [Hsu et al., 2012]) Let u1, . . ., un be a martingale difference vector sequence (i.e., E [ui | u1, \u2026\u2026\u2026, ui\u22121] = 0 for all i = 1, . . ., n ) such that"}]}