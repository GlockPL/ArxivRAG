{"title": "Assessing Language Comprehension in Large Language Models Using Construction Grammar", "authors": ["Wesley Scivetti", "Melissa Torgbi", "Austin Blodgett", "Molly Shichman", "Taylor Hudson", "Claire Bonial", "Harish Tayyar Madabushi"], "abstract": "Large Language Models, despite their significant capabilities, are known to fail in surprising and unpredictable ways. Evaluating their true 'understanding' of language is particularly challenging due to the extensive web-scale data they are trained on. Therefore, we construct an evaluation to systematically assess natural language understanding (NLU) in LLMs by leveraging Construction Grammar (CxG), which provides insights into the meaning captured by linguistic elements known as constructions (Cxns). CxG is well-suited for this purpose because provides a theoretical basis to construct targeted evaluation sets. These datasets are carefully constructed to include examples which are unlikely to appear in pre-training data, yet intuitive and easy for humans to understand, enabling a more targeted and reliable assessment. Our experiments focus on downstream natural language inference and reasoning tasks by comparing LLMs' understanding of the underlying meanings communicated through 8 unique Cxns with that of humans. The results show that while LLMs demonstrate some knowledge of constructional information, even the latest models including GPT-01 struggle with abstract meanings conveyed by these Cxns, as demonstrated in cases where test sentences are dissimilar to their pre-training data. We argue that such cases provide a more accurate test of true language understanding, highlighting key limitations in LLMs' semantic capabilities. We make our novel dataset and associated experimental data including prompts and model responses publicly available.", "sections": [{"title": "1 Introduction", "content": "Understanding the language faculties of Large Language Models (LLMs) has become increasingly important with their introduction into the mainstream. LLMs have made surprising performance gains on a wide swath of NLP tasks, but continue to fail in seemingly unpredictable and idiosyncratic ways. Linguistic theory is a possible lens for understanding these failures and LLMs more generally. Indeed, analysis of transformer language models has shown that they implicitly encode substantial linguistic information (e.g., Rogers et al., 2021; Manning et al., 2020). However, there is considerable doubt about the compatibility of traditional generative linguistic theories with LLMs (Piantadosi, 2023).\nThis work tests model 'understanding' through the creation of specific evaluation tasks based in CxG (Goldberg, 1995; Croft, 2001). CxG encompasses a family of linguistic theories which posit that morphemes, words, idioms, and even schematic structures in language can be represented as simple form-meaning pairs, or constructions (Cxns), at different levels of granularity and abstraction. Because Cxns denote semantics associated with clear syntactic patterns, they provide a mechanism to develop test instances of true 'understanding.' Specifically, we create an NLI task where understanding the Cxn is necessary for the model to complete the task. For example, consider the following instance of the LET-ALONE Cxn, which invokes a scale of a property (here ease of consistency), where the slot preceding \u201clet alone\u201d is lower on that scale than the slot after: \"It is difficult enough for an individual to be consistent let alone a society.\" To test if a model understands this Cxn, we convert this to an NLI problem by using the original sentence as the premise and introducing the hypothesis \u201cIt is easier for a society to be consistent than an individual.\" The NLI task is then to determine if the hypothesis follows from the premise (entailment), contradicts it (contradiction), as in the case above, or if no conclusion can be drawn (neutral). Performing this task requires both knowledge of a Cxn's meaning and reasoning over that meaning and the surrounding context.\nThe use of Cxns to evaluate LLM 'understanding' of language offers two significant advantages."}, {"title": "1.1 Contributions", "content": "This work aims to explore the extent to which LLMs truly 'understand' language through the lens of CxG, specifically targeting understanding of a variety of 8 English phrasal Cxns. This study differs from previous research in several significant ways that are highlighted by the following contributions:\n1.  We create multiple novel datasets using constructional information designed to evaluate the capabilities of LLMs.\n2.  Rather than focusing on the metalinguistic task of identifying Cxns, as most prior works have done, we use downstream reasoning and the well established natural language inference (NLI) task to evaluate LLM 'understanding' of the underlying meaning communicated by a Cxn.\n3.  We test reasoning in challenge scenarios where memorization and pattern matching from large pre-training datasets can provide less assistance.\n4.  We test on the largest models currently available, including GPT-40.\nThe structure of this paper is as follows: We begin with a brief introduction to CxG (\u00a72.1) and other relevant literature (\u00a72.2). We then introduce our experimental design and its grounding in CxG theory (\u00a73). We detail each of three different construction-based experiments (\u00a74, \u00a75, and \u00a76), along with error analysis (\u00a77). Finally, we provide an overview of our results and their implications (\u00a78), before concluding with possible avenues of future work (\u00a79). Overall, our evaluations indicate that LLMs struggle to understand all the nuances of constructional meaning, particularly in situations where"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Construction Grammars", "content": "Although there are several somewhat differing schools of CxG, most emphasize the usage-based nature of language acquisition as well as the fundamental unit of language: the Cxn. Cxns are pairings of form and meaning at all levels of language; thus, there are morphological Cxns, lexical Cxns, and phrasal Cxns. In this paper, we focus on phrasal Cxns. In the CxG account, speakers use domain-general cognitive processes to incrementally generalize over frequent Cxns, beginning with single-word Cxns, to novel usages, arriving at the ability to interpret even rare and previously unseen instances of a Cxn. Therefore, CxG provides an effective method of evaluating the true generalization capabilities of LLMs. We provide a more detailed description of CxG tenets in Appendix A."}, {"title": "2.2 CxGs and LLMs", "content": "Starting with CxGBert (Tayyar Madabushi et al., 2020), there has been substantial past work on probing language models' understanding of Cxns. These works have typically focused on either a single Cxn or a handful of Cxns, like AANN (Mahowald, 2023; Chronis et al., 2023), COMPARATIVE-CORRELATIVE (Weissweiler et al., 2022), and more schematic phrasal Cxns (Li et al., 2022; Veenboer and Bloem, 2023). Tseng et al. (2022) focus on Cxns in Taiwanese Mandarin, a notable exception to the works above which focus on English. Zhou et al. (2024) show that LLMs fail to adequately distinguish examples of the CAUSAL-EXCESS Cxn from similar patterns, and introduce NLI as a proxy task for understanding constructions, though their results are limited to the CAUSAL-EXCESS construction. Bonial and Tayyar Madabushi (2024) provide a corpus of 10 Cxns across varying levels of schematicity, showing that GPT-4 is not proficient at grouping sentences according to constructional similarity. Of particular relevance to our work is Weissweiler et al. (2022), who define a set of bias and calibration tests for the COMPARATIVE-CORRELATIVE, which we adapt as challenge reasoning tasks for"}, {"title": "3 Theory-Informed Experimental Design", "content": "CxG's usage-based account of the development of a grammar is particularly relevant to the evaluation of LLMs because language models learn through memorization, frequency and collocational properties. However, it is not clear the extent to which LLMs can recognize and generalize over the formal properties of language in order to ascribe shared semantics to novel usages (e.g., RESULTATIVE: \u201cThe jackhammer pounded us deaf\"). Indeed, such an ability to generalise would be a key indicator of 'understanding.' To explore this question, we leverage the CoGS corpus (Bonial and Tayyar Madabushi, 2024), which is a collection of about 500 corpus instances, roughly balanced across 10 different Cxns.\nCoGs consists a limited set of carefully curated Cxns chosen for their broad coverage of the English language. These Cxns collectively represent a significant portion of English usage and provide an effective basis for evaluating LLMs in both common and uncommon linguistic scenarios. Our primary goal is not to test LLMs on specific Cxns, but rather to use constructional information to design targeted test cases that are easy for humans to understand. This allows a more precise evaluation of LLMs, resulting in the number of Cxns we use being less critical to allowing us to generalize our findings to the English language as a whole. Of the Cxns in CoGS, we construct our datasets with 8 of the 10 Cxns, shown in Table 1. In designing our dataset, we faced a trade-off between quality and quantity. While generating a large number of test samples semi-automatically was an option, we prioritized creating a smaller, carefully curated set of high-quality examples. This not only allowed for meticulous manual verification of the curated data but also allowed us to manually validate the results, which would be infeasible with a larger dataset. By minimizing biases and errors in both dataset creation and evaluation we ensure the reliability of our findings, even if it comes at the cost of a reduced number of examples.\""}, {"title": "4 Experiment 1: Constructional NLI", "content": ""}, {"title": "4.1 Datasets", "content": "We take Natural Language Inference (NLI) as a good starting point for investigating LLM capabilities regarding the semantics of Cxns, following (Zhou et al., 2024). NLI tasks present a premise, taken to be true, and a hypothesis, which can be entailed by the premise, contradicted by the premise, or neutral (neither contradicted nor entailed). Thus, by targeting entailment and contradiction, NLI effectively probes for a functional understanding of a text. NLI is also a widely accepted task for testing NLU in the NLP community, as evidenced by the many shared evaluations ranging from the general evaluation of distributional semantic models (Marelli et al., 2014) to the evaluation of LLMs for biomedical applications (Lee et al., 2024).\nIn particular, we introduce an NLI dataset which specifically probes the semantics of our Cxns. We do this by templatically generating hypotheses which make use of certain slots within a Cxn, and verifying the resultant sentences manually to ensure quality. These template-based hypotheses have predictable relations to the premises. For example, consider the premise: \"He hammered the metal flat\". This is a RESULTATIVE Cxn, which has the meaning of an action which causes a change in state. In this case, the action verb hammered leads to the metal to have a resulting state of flat. Regarding the syntactic form of a phrasal Cxn, we can use"}, {"title": "4.2 Empirical Evaluation and Analysis", "content": "We test three OpenAI models on our constructional NLI dataset. We test the GPT-40-2024-05-13 and GPT-3.5-turbo-0125 models as well as the recently released o1-preview-2024-09-12 model. These models were chosen due to their large sizes making them illustrative examples of the capabilities of state-of-the-art LLMs in general. We also test two LLaMa models, LLaMA-3-8b-instruct and LLaMA-3-70b-instruct. We test 3 main scenarios: zero-shot, in-context learning with examples randomly selected from Stanford NLI (SNLI, Bowman et al. 2015), and in-context learning with Constructional NLI (CxNLI, our dataset). We additionally perform Chain-of-Thought (CoT) experiments (Wei et al., 2022b) in each of these scenarios, in an attempt to more precisely explore gaps in model capabilities. Surprisingly, we find that Chain-of-Thought does not lead to performance gains on our dataset. A summary of our results for this experiment are reported in Table 2."}, {"title": "5 Experiment 2: Challenge Constructional NLI", "content": ""}, {"title": "5.1 Dataset", "content": "In the previous section, we show that GPT-40 and GPT-3.5 perform impressively on an NLI task that specifically targets constructional semantics. Many of these constructions are common to the English language, and the templates we generate target aspects of meaning that are highly salient for the construction. However, we wish to test LLM generalization of constructional knowledge even in settings with unexpected uses of the construction. In order to do this, we develop a more challenging and NLI dataset (CxNLI-challenge). For 5 of our 8 Cxns, we introduce more challenging examples, which share a surface syntax with our Cxns, but convey a different meaning. To demonstrate this, consider the following two examples:\n(5) He hammered the metal flat. (resultative)\n(6) I bought the apples fresh. (depictive)\nIn the above two examples, it is clear that their syntactic forms appear identical. Both have a subject pronoun, a verb, an object noun phrase followed by an adjective. However, the two Cxns convey different meanings: In Example (5) the adjective is the result of the action of the verb, whereas in Example (6), the adjective is the state of the noun during the action of the verb, but it is not the resulting state of the action. This difference in meaning is associated with two different Cxns, specifically the RESULTATIVE and the DEPICTIVE. We can tease apart this difference in meaning by leveraging our template based hypotheses from our CxNLI dataset in 4. Consider the following templatic hypotheses:\n(7) [THE [hammer]\u2082-ING CAUSED [the metal]\u2083 TO BECOME [flat]\u2084]. (entailment)\n(8) [MY [buying]\u2082-ING CAUSED [the apples]\u2083 TO BECOME [fresh]\u2084]. (contradiction)\nAs we can see in Examples (7) and (8), templatically generating hypotheses for these two examples leads to different relations to the premises. We create this more challenging dataset (CxNLI-Challenge) by gathering \"false positives\" of Cxns from corpora based on simple pattern matching queries. These \"false positive\" constructions are those which on the surface appear to be the same as the constructions which we are testing, but have drastically different meanings. We templatically generate hypotheses for these false positives, which produces a dataset very similar in form to CxNLI (described in 4), but with different relations between premises and hypotheses, due to the different meanings of the false positive constructions. In many cases, the resulting hypotheses are rarer phrasings for the challenge Cxns than they are for their intended Cxns from CxNLI. We hypothesize that this feature will make them more difficult for the model, as they are less likely to be highly represented in the pretraining data for the LLM. For humans, interpretation is possible because we are able to generalize the meanings coming from certain slots in the Cxn even in unusual contexts. After the final version of this dataset was created, a second author evaluated the dataset, achieving an IAA of 83% with the original judgements."}, {"title": "5.2 Empirical Evaluation and Analysis", "content": "We utilize the CxNLI-challenge dataset in two settings: as in-context examples for our CxNLI dataset from Section 4, and as an evaluation dataset. We first examine what effect, if any, these challenge examples have on CxNLI performance when they are used as the in-context learning examples for our previous dataset. As we show in Table 3, for GPT-40, performance using CxNLI-challenge as in-context learning for our CxNLI does not boost or harm performance significantly, and is roughly equivalent to performance in the zero-shot setting. On the other hand, performance on CxNLI drops significantly for GPT-3.5 when it is prompted with CxNLI-challenge. This indicates that GPT-3.5 does not have the same constructional knowledge as GPT-40, and is easily misled by surface syntactic similarity. We also experiment with CoT for this dataset, finding that it does not substantially affect performance.\nWe further test if LLMs can perform NLI successfully on these challenge examples themselves. As we can see in Table 4, performance is significantly lower on our challenge NLI dataset in almost every prompt setting. The difference in per-"}, {"title": "6 Experiment 3: Challenge Constructional Reasoning", "content": ""}, {"title": "6.1 Dataset", "content": "The above experiment on challenge NLI is only possible for some Cxns. For 5 of our 8 Cxns, we successfully create challenge NLI examples based on shared syntactic forms with other Cxns. However, for 3 of our 8 Cxns (LET-ALONE, COMPARATIVE-CORRELATIVE, WAY-MANNER), creating a challenge dataset is not straightforwardly possible using form-based \u201cfalse positives.\u201d A clear example of this is the LET-ALONE Cxn: its syntactic pattern is unique to the Cxn, and there are no examples of this exact syntactic pattern that aren't instances of the LET-ALONE Cxn. Since we cannot create challenge NLI examples for these 3 Cxns with templates, we must find another method for further investigating the semantic understanding of these Cxns."}, {"title": "6.2 Empirical Evaluation and Analysis", "content": "Somewhat surprisingly, we find that LLMs remain unable to solve the tasks of Weissweiler et al. (2022) at high accuracy, even in the Chain-of-Thought setting. We find that this low perfor-"}, {"title": "7 Error Analysis", "content": ""}, {"title": "7.1 Constructional NLI and challenge NLI", "content": "In \u00a74, we describe our Constructional NLI experiments, which find that GPT-40 and GPT-01 are extremely proficient at Constructional NLI (CxNLI) while GPT-3.5 lags behind substantially. In \u00a75, we find a similar trend for challenge NLI (CxNLI-challenge), but show that even GPT-4o and GPT-01 do not perform well on CxNLI-challenge examples. For example, though its performance on the CxNLI resultative is near perfect, it struggles with CxNLI-challenge false positives for the resultative in some cases, as shown below.\n(9) Premise: I bought the apples fresh.\nHypothesis: The apples were completely fresh before I bought them.\nCorrect Response: Entailment\nModel Response: Contradiction\nHere, we investigate if some Cxns are harder for LLMs than others. In Figure 1 we report the accuracy by Cxn in our CxNLI and CxNLI-challenge datasets. In general, the same Cxns are difficult across the CxNLI and CxNLI-challenge datasets. Overall, we see that LET-ALONE and COMPARATIVE-CORRELATIVE are the weakest Cxns for GPT-40, though it is strong across the board with a minimum accuracy of 88%. We show an example of GPT-40 misunderstanding the scale of LET-ALONE in Example (10).\n(10) Premise: Beecher's reputation as a preacher, let alone as a Man of God, was not universally accepted.\nHypothesis: Beecher's reputation as a Man of God was easier to accept than his reputation as a preacher.\nCorrect Response: Contradiction\nModel Response: Entailment\nGPT-3.5 is much more variable by Cxn, with a"}, {"title": "7.2 Challenge Constructional Reasoning", "content": "In \u00a76, we show that GPT-40, GPT-01 and GPT-3.5 struggle on a QA formulation of a broad set of challenge Cxn reasoning tasks. After examining the errors across all tests, we highlight that there is significant variation in test-by-test performance based on the input prompt. We highlight two possible prompts here, Prompt 1 and Prompt 4. The key difference in these prompts is how they task the model with uncertainty. Since some of the QA tests are designed to be unanswerable given the context, the model is supposed to choose the answer not enough information (always choice C in our QA setup). However, we find that the model is unable to balance between two extremes. In some cases, it answers \u201cnot enough information\u201d when interpreting the Cxn is sufficient for providing a likely answer, and in other cases, it makes incorrect assumptions about the relationship between elements of the Cxn and context. We can see this in Figure 2. Prompt 4 includes the line \"If there is not enough information, choose C,\u201d and does far better on the tests where there is in fact not enough information, but substantially worse on the tests where the Cxn is enough information to provide an answer. An example of this failure is show in Example (11), where the model misinterprets a nonce adjective as somehow relating to the scale of the Cxn, when in fact there is not enough information to answer the question. As a whole, these results show that in our current experimental setup, we cannot get the"}, {"title": "8 Discussion", "content": "Overall, these experiments show the limits of LLM functional understanding of commonplace English Cxns. Regarding our constructional NLI experiments, our results consistently show that LLMs perform better when they encounter in-context examples similar to the actual task they are required to perform, as opposed to to typical SNLI examples. In fact, an increase in the number of SNLI examples leads to deteriorating performance. This result initially seems counterintuitive, as both types of examples provide the same information about the underlying NLI task. However, this can be explained by viewing LLMs' use of examples as akin to fine-tuning (Dai et al., 2023), and instruction tuning\u2014which aims to help models understand instructions (Wei et al., 2022a)\u2014as facilitating them to learn in-context directly on instructions (Lu et al., 2024). From this perspective, it becomes clear why examples with the same statistical distribution as the task at hand perform better, despite providing"}, {"title": "9 Conclusions and Future Work", "content": "In this work, we have shown where even the latest GPT-40 and GPT-01 models do not demonstrate functional understanding of Cxns. While GPT-40 and GPT-01 do perform quite impressively on our constructional NLI task, they fail at both challenge NLI and challenge reasoning scenarios, which require generalization of the constructional semantics to infrequent instantiations of the Cxn. Also, we see that GPT-40 substantially outperforms GPT-3.5 in all settings. GPT-3.5 is especially susceptible to being misled by shallow form-based patterns as opposed to meanings, and in-context learning is especially crucial for GPT-3.5. Overall, these experiments show that the constructional awareness of both GPT-40 and GPT-01 are far more robust than that of GPT-3.5, but their ability to generalize the meaning of Cxns to new contexts, and thus its \"understanding\u201d, still lags substantially behind that of humans.\nThus, our targeted series of experiments demonstrated that LLMs do process constructional semantics up to a point, yet our challenge datasets revealed the breaking point of understanding\u2014where speakers are able to generalize the appropriate semantics to constructional slots filled by pragmatically atypical lexical items, but LLMs are much less proficient at this generalization. Overall, we find that CxG serves as a valuable theoretical lens for probing the functional language understanding of LLMs and comparing this with human linguistic knowledge. Greater contributions to resources such as corpora of Cxns will facilitate empirical data on which constructional understanding can be evaluated with more detail."}, {"title": "Limitations", "content": "This work is limited in that we only evaluate our methods on English. More work is needed on the targeted evaluation of LLM performance using Cxn information in non-English settings. Our tasks are only one possible method for investigating LLM understanding of Cxns. Expanding research to include complementary methodologies will be necessary to build a complete picture of LLM knowledge in relation to CxG. This work can also be extended beyond the 8 Cxns that we use to generate our dataset, although these were selected for the extensive coverage of the English language. Also, while we consciously choose to create a smaller, more carefully curated dataset that also allows for careful expert manual evaluation, there is scope to increase the size of our dataset, which we leave to future work."}, {"title": "Ethics", "content": "LLMs are extremely expensive to train and run. The compute costs associated with LLMs have a nontrivial environmental impact which should not be ignored. Furthermore, due to their large-scale training data, they can reflect and propagate harmful social biases in their responses if they are not properly aligned and moderated. Furthermore, there is risk of LLMs having a negative societal impact if their widespread deployment is done without proper consideration for the lives of people. While there are risks in the use and proliferation of LLMs in general, we do not believe this work incurs any specific additional risks. Despite the overall risks and dangers, we believe this research is worthwhile in order to better understand the language systems of LLMs and compare and contrast LLM language understanding with that of humans. We honor the code of ethics."}, {"title": "A Construction Grammers", "content": "CxG has particular explanatory power with respect to phrasal constructions, such as the RESULTATIVE construction: \u201cThe jackhammer pounded us deaf.\u201d Generative linguistic theories (e.g., Chomsky (2014)) would generally analyze a transitive sentence with one verbal head (\"pounded\") that licenses the arguments of the sentence. \u201cPounded\" generally licenses an agent subject (here, \"jackhammer\") and potentially a patient direct object. Generative approaches argue that this information about the verb is memorized and stored in the lexicon, while combinatory rules of how to put lexical items together are stored in a separate syntax module of language processing. However, unless a special grammatical rule or sense of the verb is postulated, there is nothing to explain why \u201cus\u201d is not the thing pounded here, or what licenses the adjective \"deaf.\u201d Nonetheless, native speakers have no problem recognizing the special formal and semantic properties of this Cxn, namely that it entails a pounding event that causes a change in state of \"us\" resulting in the state of \"deaf.\"\nIn contrast to Generative linguistic theories (e.g., Chomsky (2014)), CxG posits that speakers acquire and store constructions, which notably account for not only the semantic properties of the unit but"}, {"title": "B Templatic Constructional NLI vs Freeform NLI", "content": "In addition to our templatically generated constructional NLI dataset, we construct a more \"freeform\" NLI dataset by training a person to create hypotheses that are intended to be more similar in form to hypotheses from SNLI. Both our \"freeform\" and templatic datasets are constructed with real-world corpus data of constructions, primarily coming from the COGS dataset (Bonial and Tayyar Madabushi, 2024), with supplementary data coming from Corpus of Contemporary American English (COCA, Davies 2010), and the English Corpus from the Web, or EnCOW (Sch\u00e4fer and Bildhauer, 2012; Sch\u00e4fer, 2015). Within both of these datasets, each premise includes one of 8 total constructions: the comparative-correlative Cxn, the let-alone construction, the way-manner construction, the causative-with construction, the conative construction, the resultative construction, the caused motion construction, and the intransitive motion Cxn. We include a roughly balanced sample of each construction, with all premises taken from corpus data. These constructions cover a wide range of schematicity meaning that they have different levels of lexicalization/abstractness.\nThe freeform Constructional NLI tuples are constructed in order to be similar to other NLI datasets, testing general types of semantics. These tuples do not always necessitate perfect understanding of"}, {"title": "C Scientific Artifacts and Descriptive Statistics for All Datasets", "content": "We use the following scientific artifacts: COCA (Davies, 2010), EnCOW (Sch\u00e4fer and Bildhauer, 2012; Sch\u00e4fer, 2015), the CoGS dataset (Bonial and Tayyar Madabushi, 2024), and the OpenAI API, in addition to our created datasets. The COCA corpus contains 8 genres: Academic, Blog, Fiction, Magazine, News, Spoken, TV, and Web. It is intended to capture American English, though there is no guarantee that it does not also include some other varieties. Demographic information about the creators of the texts in the corpus is not always available given its scale. EnCOW is a large scale corpus of English text from the web. As such, the demographic information that it captures is not completely clear. All datasets besides COCA and EnCOW are open-source under a Creative Commons license. The institutions of the authors have valid licenses for COCA and EnCOW, permitting their use in academic settings. We use the artifacts as intended. Overall, we construct and experiment on 4 datasets: 2 constructional NLI datasets (templatic and freeform), the challenge NLI dataset, and the challenge reasoning dataset. The sizes and IAA agreement for the final datasets is reported in Table 8. All of our datasets are exclusively in English."}, {"title": "D Prompt Variation Experiments", "content": "For each experiment, we choose one setting to observe the impact of prompt variations on perfor-"}, {"title": "E Example NLI Tuples", "content": "In Table 12, we show examples of our constructional NLI datasets. We show challenge Cxn NLI examples in Table 15."}, {"title": "F Chain-of-Thought Results for all Experiments", "content": "We replicate each of our main experiments with the addition of Chain-of-Thought (CoT) to observe changes in performance and inspect any errors in the model's reasoning steps. For CoT thought prompting we do not provide examples of reasoning in the prompt, instead we simply ask the model to explain \"step by step\". The results for each experiment have been shared in tables 16, 17, 18 and 19."}, {"title": "G Reasoning Test Description and Examples", "content": "There are 7 bias and calibration tests outlined in Weissweiler et al. (2022) for the COMPARATIVE-CORRELATIVE. However, their tests are templatically generated using one specific syntactic form of the COMPARATIVE-CORRELATIVE. We adapt these tests to natural examples of the construction, and apply the tests to LET-ALONE and WAY-MANNER. We also introduce a new test called \"Nonce Calibration\", which is similar in spirit to the \"Adjective Calibration\" test but using a nonce word. A brief description of all test types is below.\n1. Base - this test presents an example of a construction, along with additional context, followed by a question. The constructional meaning combines with the context to provide the answer to the question.\n2. Recency Bias - this test reorders the Cxn and the contextual information, placing the contextual information first. The answer should be the same as Base.\n3. Vocabulary Bias - this test reverses the semantics of the scalar constructions by swapping vocabulary items in different slots of the Cxn. The answer should be the opposite of Base.\n4. Name Bias - this test simply changes the nouns that are referenced in the context of the construction, to see if the model is biased towards interpreting certain Cxns differently depending on the nouns that fill their slots. The answer should be the same as Base.\n5. Short Calibration - this test removes the Cxn altogether, making it impossible to answer the question. The model should respond that it does not have enough information to answer.\n6. Name Calibration - this test gives contextual information unrelated to the construction, in the form of nouns unrelated to the question. The model should respond that it does not have enough information to answer.\n7. Adjective Calibration - this test again provides irrelevant contextual information, in the form of an irrelevant adjective. The model should respond that it does not have enough information to answer.\n8. Nonce Calibration - this test provides irrelevant contextual information in the form of a nonce word instead of meaningful contextual"}, {"title": "\u0397 Annotator Information", "content": "All datasets are annotated by co-authors of this paper. 2 annotators identify as men, 4 annotators identify as women. Of our annotators, 3 have a graduate degree in linguistics while 3 do not. At least one"}]}