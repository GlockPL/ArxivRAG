{"title": "Beyond the Singular: The Essential Role of Multiple Generations in Effective Benchmark Evaluation and Analysis", "authors": ["Wenbo Zhang", "Hengrui Cai", "Wenyu Chen"], "abstract": "Large language models (LLMs) have demonstrated significant utilities in real-world applications, exhibiting impressive capabilities in natural language processing and understanding. Benchmark evaluations are crucial for assessing the capabilities of LLMs as they can provide a comprehensive assessment of their strengths and weaknesses. However, current evaluation methods often overlook the inherent randomness of LLMs by employing deterministic generation strategies or relying on a single random sample, resulting in unaccounted sampling variance and unreliable benchmark score estimates. In this paper, we propose a hierarchical statistical model that provides a more comprehensive representation of the benchmarking process by incorporating both benchmark characteristics and LLM randomness. We show that leveraging multiple generations improves the accuracy of estimating the benchmark score and reduces variance. We also introduce P (correct), a prompt-level difficulty score based on correct ratios, providing fine-grained insights into individual prompts. Additionally, we create a data map that visualizes difficulty and semantic prompts, enabling error detection and quality control in benchmark construction.", "sections": [{"title": "1 Introduction", "content": "In recent years, advanced large language models have demonstrated remarkable versatility across a wide range of tasks and domains, with their development continuing to accelerate. To effectively track their progress, numerous generative benchmark datasets have been curated to assess both their general and specialized capabilities.\nThere are two primary ways for generating responses from large language models (LLMs): greedy decoding and random sampling (Holtzman et al., 2019). Greedy decoding selects the next token with the highest probability, resulting in a deterministic output. In contrast, random sampling, such as nucleus sampling (Holtzman et al., 2019), incorporates randomness during decoding by sampling a token at each step based on a probability distribution. This approach leads to non-deterministic output. Current LLM benchmarks typically employ one of these methods; for instance, LiveBench (White et al., 2024) WildBench (Lin et al., 2024) and OpenLLM leaderboard (Beeching et al., 2023) use greedy decoding, while TrustLLM (Huang et al., 2024), MT Bench (Zheng et al., 2023) and Alpaca Eval (Li et al., 2023) employ a non-deterministic sampling configuration. During evaluations, LLMs generate a single response for each prompt in the benchmark, and the correctness of these responses is determined by comparing them to the ground truth answers. The final benchmark score is then calculated as the average of these individual scores.\nHowever, this presents challenges within the current generative-evaluation paradigm. Firstly, deterministic generation does not align with the real-world application of LLMs, where randomness is inherent. This misalignment can lead to biased estimations of LLM performance. Even with random generation, relying on a single generation can result in significant variance in benchmark scores, particularly when the sample size is small. Furthermore, a single generation is not sufficiently informative for individual prompts, as it cannot address prompt-level questions such as, \"Which question is more challenging?\" This limitation creates obstacles to understanding the overall composition of the benchmark data.\nIn this paper, we regard the benchmark as an estimation problem characterized by a statistical model and highlight the significance of incorporating multiple random generations in a principled way. We theoretically demonstrate that increasing the number of generations decreases the variance in benchmark score estimation. Moreover, by leveraging multiple samples, we introduce a fine-grained difficulty metric, P (correct), derived from the inherent latent parameters of our statistical model, to quantify the difficulty of individual prompts. This enables comparisons across different prompts. Additionally, we demonstrate that mislabeled or ambiguous prompts can be effectively detected using multiple generations, highlighting its potential as a tool in benchmark construction."}, {"title": "2 Benchmarking Procedure is a Hierarchical Model", "content": "In this section, we show that the benchmark is an estimation problem. Without loss of generality, we consider random sampling as the generation strategy where each token is randomly sampled from a token distribution conditional on previously generated tokens. We also assume the correctness of generations can be obtained using a judgment function, which can be accomplished either by comparing the response with ground truth or by determining whether it passes unit tests.\nGiven an LLM parameterized by parameters \u03b8, including both model parameters and sampling parameters, for example temperature T and top P, etc.), and a benchmark dataset D = {x_i}_{i=1}^n, we can define difficulty of the i-th prompt with respect to the LLM as a random variable drawn from the unknown benchmark difficulty distribution P(\u03bc, \u03c3; \u03b8), with mean \u00b5 and standard deviation \u03c3. Without loss of generality, with k generations per prompt, we can then regard the benchmarking procedure as a hierarchical model as follows:\n$P_i \\sim P(\\mu, \\sigma; \\theta) \\text{ for } i = 1,\\ldots,n$\n$Y_{i,j} \\sim \\text{Bernoulli}(p_i) \\text{ for } j = 1,\\ldots,k,$\nwhere prompt difficulty $p_i$ is sampled from P(\u03bc, \u03c3; \u03b8) and pi represents the probability that the LLM can correctly answer the i-th prompt., i.e., P (A generated answer to i-th prompt is correct) = pi. This represents a latent difficulty of prompts, We denote the he k-th generation of the i-th prompt as zij and then Yi,j is the correctness indicator for it, where $Y_{i,j} = 1$ if it's correct otherwise $Y_{i,k} = 0$.\nHere both benchmark distribution P(\u03bc, \u03c3; D) and $p_i$ are unknown needs to be estimated with {Yi,j}_{j=1}^k for i = 1,\u2026,n.\nTo estimate $p_i$ and \u00b5, we can use a straight for-\u03a3j=1 Yi,j\nward method of moment estimators $p_i = \n\u03a3j=1\n= \u03a3j=1 \u03a3j=1 Yi,j\n\u00b5 = . We observe that a\nn\nnk\nwidely used item response theory (Polo et al., 2024; Madaan et al., 2024; Ding et al., 2024), employed to model the difficulty of prompts, represents a specific parametrization of P(\u03bc, \u03c3; D). Further elaboration on this can be found in Appendix B.\nNote that, when k = 1, the benchmark score computed based on a single random generation is an estimation of \u03bc, which only utilizes a single generation which leads to a large variance. We can show this by explicitly calculating the variance of our estimators.\nLemma 2.1. Given the hierarchical model in (1) and the moment estimators\n\u03a3=1 \u03a3j=1 Yi,j\n\u00b5 = .\nnk\nThen \u00fb is an unbiased estimator for \u00b5 and its variance equals:\n1\n\u03c32\nVar(\u00fb) = (\u00b5- \u03bc\u00b2-\u03c3\u00b2) +\nnk\nWithth-prompt Variance\nn\nBetween-prompt Variance\nHere, Var(\u00fb) can be decomposed into within-prompt variance and between-prompt variance. Both terms decrease as the number of benchmark data n increases. However, since benchmark data is typically fixed, we analyze the influence of sampling in terms of k. Within-prompt variance captures the randomness in sampling $Y_{ij}$ conditional on the i-th prompt, and it can be effectively reduced by increasing the number of samples k, converging to 0 as k \u2192 \u221e. The between-prompt variance term, on the other hand, captures the variability of prompt difficulty $p_i$ across groups, reflecting the randomness of difficulty distribution P(\u03bc, \u03c3; \u03b8), and thus remains unaffected by k.\n1\n\u03a3i=1(Pi \u2212 \u00b5)2 and \u00fb into (2) to get\nWe can further plug in sample variance \u03c32 =\nn\nVar(\u00fb). Finally, based on the central limit theorem, a 95% confidence interval can be constructed as:\n\u00b5\u00b11.96Var(\u00fb)\n3"}, {"title": "2.1 Prompt Level Difficulty: P (correct)", "content": "Our goal is to develop a granular, quantifiable measure of prompt difficulty, enabling us to gain a deeper understanding of their relative complexities. By quantifying prompt difficulty at the individual level, we can address fundamental questions such as: 'Which prompts are most challenging?' and 'How do different prompts compare in terms of difficulty?' A fine-grained understanding of prompt difficulty will provide valuable insights into the strengths and weaknesses of language models, as well as the composition of benchmark datasets, ultimately informing the development of more effective models and evaluation frameworks.\n\u03a3j=1 Yi,j\nWe refer to IP (correct) = pi in (1) and its estimation P (correct) = Pi =\nk When the number of generations k increases, it will converge to\nk\nthe true P (correct) and therefore more fine-grained. The probability of correctness p\u2081 can be interpreted as a difficulty score at the prompt level: the higher the pi, the easier the prompt since the language model has a higher probability of generating a correct response. We demonstrate the use of difficulty scores in the analysis section."}, {"title": "3 Experiments", "content": "We choose multiple benchmarks which cover various capabilities of LLMs: MMLU-Pro (Wang et al., 2024), GSM8K (Cobbe et al., 2021), MuSR (Sprague et al., 2023), IFEval (Zhou et al., 2023). For MMLU-Pro, GSM8K, and MUSR, we use accuracy as the metric, while for IFEval, we utilize instance-level strict accuracy. Brief introduction of those benchmarks can be found in Appendix C."}, {"title": "3.1 Experimental Setup", "content": "Benchmark."}, {"title": "LLM and Setup.", "content": "We utilize four widely-used open-source LLMs: Llama 3.1 (8B and 70B Instruct) (Dubey et al., 2024), Qwen 2.5 (7B Instruct) (Yang et al., 2024), and Ministral (8B Instruct) (Jiang et al., 2023)*. We evaluate both greedy decoding and random sampling on these models, with the latter using a temperature of 0.7 and top-p of 1.0. For each prompt across all benchmarks, we generate 50 samples (k = 50) using a 0-shot chain-of-thought prompting strategy."}, {"title": "3.2 Main Results", "content": "Our results are presented in Figures 1 and 2, as well as in Table 1. The key takeaways from these results are summarized below.\nDistribution of P (correct) show diffuse density in challenging tasks, behaving like random samplers. For the distribution of P (correct), we define stable behavior as a density distribution with high concentrations near 0 and 1, and lower density in between. Conversely, a distribution with a high density between 0 and 1 indicates high randomness. As shown in Figure 1, when confronted with benchmarks that require strong reasoning skills(MMLU-Pro, IFEval, and MuSR), all models display a diffuse density distribution over the support [0, 1]. This suggests that LLMs resemble random samplers when handling prompts requiring strong reasoning, underscoring the complexity and sensitivity of their reasoning processes. In contrast, the simpler task GSM8K display densities with more pronounced tails and reduced uncertainty. A plausible explanation is that GSM8K is easier and involves shorter reasoning lengths, which in turn decreases the likelihood of diverse reasoning paths emerging. Additionally, we observe that the Llama 70B model exhibits the most stable performance across all benchmarks, suggesting that larger models may be capable of providing more stable reasoning.\nEstimation differs noticeably between greedy decoding and random sampling, with a single random generation being unstable. Table 1 presents the benchmark scores, highlighting the performance differences between greedy decoding and random sampling. Notably, for GSM8K and MuSR, the absolute differences in benchmark score between these two methods for Llama3 8B are 3.4 and 4.2 respectively, indicating a relatively large performance gap. This discrepancy can also be observed"}, {"title": "Multiple generations can help detect labeling errors: a case study on GSM8K.", "content": "Benchmark construction can involve label errors or ambiguous prompts, such as the approximately 5% error rate in GSM8K. Manually cleaning large datasets is costly, but we found using multiple generations from advanced LLMs can help identify mislabeled or ambiguous prompts. Based on multiple generations, we can create a data map to visualize P(correct) against S(consistency), which measures the semantic consistency of generations. More details on this measure are provided in Appendix E. We hypothesize that prompts with low P(correct) and high S(consistency) may be mislabeled or ambiguous due to contradicting with the self-consistency(Wang et al., 2022). Selfconsistency (Wang et al., 2022; Mitchell et al., 2022) leverages the intuition that a challenging reasoning problem typically admits multiple reasoning paths leading to its unique correct answer. To verify our hypothesis, we utilize the data map of Llama3 70B for GSM8K and selected prompts with P(correct) \u2264 0.1 and S(consistency) \u2265 \u22120.8, totaling 18 prompts. After manually reviewing the selected prompts, we found that 44.4% prompts were either mislabeled or ambiguous. Examples are shown in the Appendix Figure 4. Our results demonstrate the potential of using data maps as a tool to clean datasets, which is similar to a prior study (Swayamdipta et al., 2020) but it focuses on classification models rather than generative models. Notably, our findings utilize only a single LLM and a simple semantic metric. This highlights future research that will incorporate more models and improved semantic metrics for more accurate detection."}, {"title": "4 Conclusion", "content": "In this paper, we investigate the role of multiple generations in enhancing LLM benchmark evaluation. Using a hierarchical model, we quantify prompt difficulty and visualize its distribution, revealing insights into benchmark data. Our findings about variance reduction, and labeling error detection emphasize the importance of leveraging multiple generations for robust benchmark evaluations."}, {"title": "Limitations", "content": "While using multiple generations in benchmark evaluation is promising, it demands more computational resources during inference time. Future research could explore the minimal number of generations required for robust evaluation, potentially reducing within-prompt variance. Additionally, our statistical model assumes that all prompts are independently sampled from the benchmark difficulty distribution, which may not be accurate in practice, as prompts can originate from the same subjects or resources. Future work should consider incorporating the covariance structure into the estimation process. Another drawback is the detection of mislabeled prompts. Although our method efficiently reduces the effort needed to filter samples, the true positive rate is not high (around 50%). Potential research could leverage more sophisticated semantic metrics and model ensembles to better detect mislabeled or ambiguous prompts."}, {"title": "Ethic Statement", "content": "Our work utilizes benchmark datasets to evaluate LLMs. All the datasets and LLMs are publicly available."}, {"title": "Additional Results on Varying Temperature T", "content": "To investigate how temperature influences the P(correct) distribution, we vary the sampling temperatures T across 0.4, 0.7, and 1.0 for the GSM8K and MUSR datasets using the Llama 8B and 70B models. The results are in Figure D. We find that for the smaller 8B model, as T increases, the distribution becomes more unstable with a more diffuse density. However, for the larger model, the P(correct) is less sensitive to changes in T."}, {"title": "E Semantic Consistency for Responses: S (consistency)", "content": "Apart from the correctness", "as": "nS (consistency) = \u03a3 Prop log Prop.\nc=1\nwhere Prop measures the proportion of generations in group e and its empirical estimator Prop =\n# generations in set c\nwith finite m samples. This can be seen as negative semantic set entropy, the larger, more consistent."}]}