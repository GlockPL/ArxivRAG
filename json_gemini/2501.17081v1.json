{"title": "GRAPH TRANSFORMERS FOR INVERSE PHYSICS: RECONSTRUCTING FLOWS AROUND ARBITRARY 2D AIRFOILS", "authors": ["Gregory Duth\u00e9", "Imad Abdallah", "Eleni Chatzi"], "abstract": "We introduce a Graph Transformer framework that serves as a general inverse physics engine on\nmeshes, demonstrated through the challenging task of reconstructing aerodynamic flow fields from\nsparse surface measurements. While deep learning has shown promising results in forward physics\nsimulation, inverse problems remain particularly challenging due to their ill-posed nature and the\ndifficulty of propagating information from limited boundary observations. Our approach addresses\nthese challenges by combining the geometric expressiveness of message-passing neural networks\nwith the global reasoning of Transformers, enabling efficient learning of inverse mappings from\nboundary conditions to complete states. We evaluate this framework on a comprehensive dataset of\nsteady-state RANS simulations around diverse airfoil geometries, where the task is to reconstruct full\npressure and velocity fields from surface pressure measurements alone. The architecture achieves high\nreconstruction accuracy while maintaining fast inference times. We conduct experiments and provide\ninsights into the relative importance of local geometric processing and global attention mechanisms in\nmesh-based inverse problems. We also find that the framework is robust to reduced sensor coverage.\nThese results suggest that Graph Transformers can serve as effective inverse physics engines across\na broader range of applications where complete system states must be reconstructed from limited\nboundary observations.", "sections": [{"title": "1 Introduction", "content": "Inverse physics problems involve reconstructing the complete state of a system that has produced a set observations - a\nfundamental challenge that appears across scientific domains [63]. While traditional numerical physics simulators, such\nas computational fluid dynamics (CFD) or finite element analysis (FEA) solvers, excel at computing system behavior\nfrom known initial conditions, many engineering applications require solving the inverse problem. However, inferring\ncomplete system states from limited measurements is challenging [48]. Inverse physics problems are often characterized\nby ill-posedness [24] and non-uniqueness, where multiple solutions may exist for a given set of observations. Moreover,\nthe derived system state may be extremely sensitive, with small changes in the measured observations leading to large\nchanges in the solution.\nOne such inverse problem that is of particular interest to engineers dealing with aerodynamic or hydrodynamic\napplications is the reconstruction of flow fields around immersed structures. At first glance, this may seem extremely\ndifficult, as even the forward modeling of turbulent flows is inherently complex. Yet, some biological systems\ndemonstrate remarkable intuitive ability in flow sensing and reconstruction. Fish and marine mammals, for instance,\nuse their sensory organs to detect minute flow perturbations, allowing for precise navigation and prey detection even\nin turbulent environments [14, 30]. This insight has motivated the development of artificial sensing systems that aim\nto replicate such capabilities [78]. Moreover, recent advances in micro-electromechanical systems (MEMS) have\nmade low-cost, distributed flow sensing increasingly practical and cost-efficient. The availability of these new sensors\nenables unprecedented spatial coverage for surface measurements in the field [4, 51, 68], providing rich information on"}, {"title": "2 Problem statement", "content": "Adopting the notation of [20], an inverse physics problem can be described as follows. Consider a physical system\nequipped with p distributed sensors that measure some quantity of interest, providing measurements $s \\in \\mathbb{R}^p$ at discrete\nlocations. These sensors sample from the surrounding physical field $x \\in \\mathbb{R}^m$ through a measurement operator H:\n$s = H(x)$\n(1)\nThe goal is to construct an estimate $ \\hat{x}$ of all variables of the complete field from the available sparse measurements by\nlearning from the training data a function $F$ that approximates the highly nonlinear inverse measurement operator $G$\nsuch that:\n$F(s) = \\hat{x} \\approx x = G(s)$\n(2)\nWhen the domain can be mapped to a discrete graph structure G, the field reconstruction problem naturally maps to a\ngraph-based learning framework. In this case, a graph G = (V, E) represents the physical domain, with n field nodes\n$V_f$, where quantities are unknown, and p measurement nodes $V_p$, where sensor data is available. The edges $\\mathcal{E}$ encode"}, {"title": "2.2 Flow reconstruction around airfoils", "content": "In the context of flow reconstruction around airfoils, this graph framework takes a concrete form, wherein we use CFD\nmeshes as the foundation for the graph representation. The measurement nodes $V_m$ correspond to p pressure sensors\ndistributed along the airfoil surface, providing surface pressure measurements, while the field nodes $V_f$ represent points\nin the surrounding fluid domain where we aim to reconstruct flow features, such as velocity components and pressure.\nThe mesh cell connectivity naturally defines the edges $\\mathcal{E}$, which capture the spatial relationships. This mesh-to-graph\nconversion preserves the spatial discretization of the original CFD mesh, including refined regions near the airfoil\nsurface. In this setting, the graph operator learns to map from sparse surface measurements to the complete flow field\nby leveraging both the geometric structure of the problem and the underlying physics. Figure 1 illustrates this setup."}, {"title": "2.3 Challenges in graph-based flow reconstruction", "content": "This work extends previous research on graph-based flow reconstruction around airfoils, which utilizes graph neural\nnetworks (GNNs) [17]. We discuss here several key challenges that such an approach, which is based purely on\nmessage-passing, may encounter. These points directly motivate some of the architectural choices in our present work.\nThe graph-based flow reconstruction problem presents several fundamental challenges at the intersection of geometric\ndeep learning and fluid dynamics. The first major hurdle stems from the sheer scale and the connectivity patterns of the\naerodynamic CFD meshes that we use as input for our graph-learning setup. These meshes typically contain upwards of\n50,000 nodes with strictly local connectivity patterns. Near the airfoil surface, mesh resolution becomes particularly\ndense, creating highly unbalanced graph structures. Given that full graphs must be fed to our model and that memory\nrequirements for graph-based learning scales strongly with graph size, the type and the size of the models that can be\nused is severely limited for standard computational resources (single GPU setup).\nA second critical challenge arises from the extreme localization of the input data. Pressure measurements are confined\nto approximately 1% of all graph nodes, located on the airfoil surface, yet this information must propagate through\nmultiple dense mesh layers to reach distant regions of the flow field. This is particularly problematic for reconstructing\ncritical flow features like wakes and recirculation zones using surface pressure measurements as this requires effective\nlong-range information transfer. Detached flow is especially challenging in this regard, as the separated region is by\ndefinition a zone in which the flow features are disconnected from the surface measurements.\nThese challenges are further compounded by the architectural limitations of typical graph neural networks, which\nrely on message-passing. Message-passing neural networks (MPNNs) operate through strictly-local computations,\nmeaning information can only propagate to immediate neighbors for each layer. Consequently, capturing long-range\ndependencies requires many successive message-passing operations. However, extensively increasing the depth of\ngraph neural networks introduces other fundamental difficulties, such as oversquashing and oversmoothing. The\noversquashing phenomenon [2] occurs when information from exponentially growing neighborhoods gets compressed\nthrough bottleneck structures, while oversmoothing [55] causes node features to become indistinguishable after many\nlayers. Moreover, vanishing gradients can make training unstable at extreme depths.\nFinally, the underlying physics adds another layer of complexity. High-Reynolds flows exhibit complex nonlinear\nbehavior with turbulent effects creating multi-scale features that require both local and global reasoning. The solution\nspace is also highly sensitive to boundary conditions and geometric variations, making the learning task particularly\nchallenging. This combination of scale, information propagation, architectural, and physical challenges necessitates\ncareful consideration in the design of learning frameworks for large-scale fluid dynamics problems."}, {"title": "3 Background and related work", "content": "Geometric deep learning The field of geometric deep learning [6] is a rapidly evolving domain within the broader\nmachine learning landscape. Geometric deep learning has attracted significant attention in recent years thanks to\nthe notable success in tackling problems that do not live in standard Euclidean spaces [31, 49]. Geometric deep\nlearning is built on two fundamental principles: symmetry and scale separation. Symmetry (or invariance/equivariance)\nrefers to the preserved properties under transformations relevant to the problem domain - for example, translational\nsymmetry in convolutional neural networks [36] or permutation invariance in graph neural networks (GNNs) [58]. Scale\nseparation encapsulates the principle that interactions in many physical and informational systems are primarily local,"}, {"title": "Data-driven inverse physics and flow reconstruction", "content": "Many high-dimensional physical systems exhibit an inherent\nlow-dimensional structure, where the underlying dynamics evolve on a manifold of much lower dimensionality than the\noriginal state space [28]. This property has driven the development of inverse problem solving techniques that exploit\nthe intrinsic low-dimensionality to reconstruct full system states from limited measurements. Key methodologies in\nthis area include proper orthogonal decomposition (POD), which identifies orthogonal basis functions that maximize\nthe captured variance in the data [5], dynamic mode decomposition (DMD), which extracts spatiotemporal coherent\nstructures through spectral analysis of the system's evolution operator [60] and proper generalized decomposition\n(PGD), which employs a generalized spectral expansion approach to construct separated variable representations\nof the solution [11]. These classical approaches have been successfully used to reconstruct flow fields and predict\nsystem evolution from partial observations [50], however, they typically assume linear relationships between system\nstates, limiting their effectiveness for strongly nonlinear dynamics where modern machine learning methods can better\ncapture complex patterns. Recent work [20] has demonstrated the efficacy of a shallow neural network in addressing\nthese limitations, proposing a minimal architecture of only 2-3 layers that directly learns the mapping between sensor\nmeasurements and flow fields. This approach achieves superior reconstruction accuracy compared to traditional modal\ndecomposition methods while requiring significantly fewer sensors, highlighting the potential of even simple deep\nlearning architectures to capture complex nonlinear relationships in fluid systems. Yet, this method remains tied to the\nspecific spatial discretizations used during training, a limitation that recent approaches aim to overcome by learning\nmore general, discretization-independent representations of the underlying physical fields."}, {"title": "Discretization-independent deep learning for PDEs", "content": "While deep learning methods are commonly used nowadays to\ntackle scientific problems characterized by Partial Differential Equations (PDEs), the majority of the currently deployed\nmethods are still heavily dependent on grid-based discretization schemes. Indeed, some of the most prevalent scientific\ndeep learning approaches are convolutional [64, 71] or fully connected neural networks [41], which require training on\nfixed sized grids. Such schemes usually suffer from poor generalization and aliasing artifacts when applied to input\nstructures which differ too greatly from the training setup, severely limiting their usefulness for applications such as\nflow reconstruction for arbitrary geometries. While recent work has attempted to address these challenges through\napproaches such as spline kernels [72], neural operators (NOs) [33, 44] have emerged as a particularly promising\nframework, offering discretization-independent solutions to PDE-based problems. The growing popularity of NOs\ncan be attributed to their ability to map between the input space and an infinite dimensional function space, given a\nfinite training set of collected input-output observations of the problem and independently of the discretization. The\nFourier Neural Operator (FNO) in particular has demonstrated strong performance in a number of PDE tasks [39]. This\nmethod works by projecting the input function onto the Fourier space, applying learned linear transformations in the\nfrequency domain, and combining this with a local convolution operation in physical space to capture both global and\nlocal dependencies in the solution field. Motivated by this, we propose a scheme that combines elements from both\ngraph-based [40] and Transformer-based [8] Neural Operators (NOs), sharing conceptual similarities with the Fourier\nNeural Operator's global-local processing strategy while operating directly on graph structures."}, {"title": "4 Generating a robust training dataset", "content": "This section describes in detail the generation process of our training dataset, which is based on use of diverse\nairfoil geometries. In total we obtain 2907 converged simulations that are turned into graphs, with 2469 used\nfor training, 239 for validation and 199 for testing. The full graph training dataset can be accessed here: https: //doi.org/10.5281/zenodo. 14629208. The code used for our airfoil simulation pipeline is also made available\nand can be accessed at https://github.com/gduthe/airfoil_sim_pipeline."}, {"title": "4.1 Simulation pipeline", "content": "Airfoil geometry selection We obtain our airfoil geometries from the UIUC airfoil database [61], applying several\npreprocessing steps to ensure data quality and representativeness. An initial cleaning phase removes airfoils with\nself-intersecting geometries and normalizes all shapes to a unit chord length. To better capture the critical aerodynamic\nfeatures near the leading edge, we enhance the geometric resolution of the front half profiles through cubic spline\ninterpolation. To prevent data leakage between evaluation sets, we partition the processed database into training,\nvalidation, and test sets prior to conducting the aerodynamic simulations. We use a stratified approach to partition\nthe cleaned database into training, validation, and testing sets: while the training and validation sets span the primary\nshape variations, we specifically reserve geometrically challenging airfoils for testing. These challenging cases include\noutlier geometries characterized by extreme camber, thickness ratios, or other distinctive features. Figure 2 displays\nthe distribution of airfoils across these three sets through a Principal Component Analysis (PCA) of the key geometric\nparameters."}, {"title": "CFD simulations", "content": "Each airfoil mesh is simulated twice, each time with a different inflow configuration drawn from\nthe pool of boundary conditions generated as described above. Our simulation pipeline uses the OpenFOAM CFD\nsoftware package [29] to solve the steady 2-D Reynolds-Averaged Navier-Stokes (RANS) equations, which can be\nwritten in Einstein notation as:\n$\\rho \\frac{\\partial U_i}{\\partial t} + \\frac{\\partial}{\\partial x_j} (\\rho U_i U_j) = -\\frac{\\partial p}{\\partial x_i} + \\frac{\\partial}{\\partial x_j} \\bigg[ \\mu \\bigg( \\frac{\\partial U_i}{\\partial x_j} + \\frac{\\partial U_j}{\\partial x_i} - \\frac{2}{3} \\frac{\\partial U_k}{\\partial x_k} \\delta_{ij} \\bigg) \\bigg]$\n(3)\nwhere $U_i$ represents the mean velocity component in direction $i$, $p$ is the fluid density, $p$ denotes the pressure, $\\mu$ is the\ndynamic viscosity, and $-\\rho \\overline{u'_i u'_j}$ represents the Reynolds stress tensor arising from the averaging of turbulent fluctuations.\nThe Reynolds stress tensor is modeled using the Boussinesq approximation:\n$-\\rho \\overline{u'_i u'_j} = \\mu_t \\bigg(\\frac{\\partial U_i}{\\partial x_j} + \\frac{\\partial U_j}{\\partial x_i} \\bigg) - \\frac{2}{3} \\rho k \\delta_{ij}$\n(4)\nwhere $\\mu_t$ is the turbulent eddy viscosity, $k$ is the turbulent kinetic energy, and $\\delta_{ij}$ is the Kronecker delta. We can obtain\nthis term by using the k-w SST turbulence closure model [46], which solves for the turbulent kinetic energy k and the\nspecific dissipation rate w. Standard OpenFOAM wall functions are employed for near-wall treatment. Finally, solution\nconvergence is enforced through residual monitoring, requiring all solved quantities (pressure, velocity components, k,\nand w) to achieve normalized residuals below 5 \u00d7 10\u22125. Simulations that fail to meet these convergence criteria are\nexcluded from the dataset."}, {"title": "4.2 Processing the simulations", "content": "Mesh to graph translation We use a finite-volume inspired approach to convert our CFD meshes into graphs. In\nour implementation, mesh cells serve as nodes, with bidirectional edges connecting adjacent cells. This cell-centric\nrepresentation also enables the extraction of physics-relevant edge features, specifically the boundary length ly between\nadjacent cells. This edge feature is useful, as it incorporates inherent cell sizing information. Cell quantities are\nconverted into node features, comprising four components: pressure p, velocity components (ux, Uy), and a one-hot\nencoded node category t (fluid, or wall). To optimize computational efficiency, we restrict the graph representation to\ncells within one chord length of the airfoil, rather than processing the entire CFD domain (which extends to 100 chord\nlengths). The airfoil surface is represented by nodes positioned along its boundary, with bidirectional edges connecting\nadjacent surface nodes. These additional connections prevent the formation of tree-like structures that could impede\nlearning performance [66]. The resulting graphs in our dataset contain on average approximately 55k nodes and 85k\nedges. Figure 4 illustrates the full simulation pipeline and the mesh-to-graph conversion scheme."}, {"title": "Input and output features", "content": "For the node features, we only have access to the pressure distribution at the surface of the\nairfoil, while it is set to 'NaN' values at the fluid nodes. As an additional feature, we compute the signed distance field\n(SDF) for each node. The SDF is a continuous scalar field that encodes the minimum distance to the airfoil surface, an\nexample of which is displayed in Figure 5. In the flow reconstruction context, SDFs can provide an important geometric\nprior that helps the network understand spatial relationships. In our processing pipeline, we implement an efficient\nvectorized algorithm based on KD-trees for nearest neighbor search, enabling fast SDF computation for all fluid nodes\neven for large meshes. The type of each node is also known and is encoded as a one-hot vector, bringing the total size\nof the input node features to four.\nTo account for mesh geometry, each edge of the graph is associated with some features. The following four edge\ncharacteristics are used as inputs: the x and y components of the relative edge direction vector r, the edge length l, and\nthe cell boundary length ly, as previously described and shown in Figure 5. We provide a summary of the properties of\nthe input and output features, in Table 1.\nData normalization In our data preprocessing approach, we implement a normalization scheme for both input and\ntarget variables. The surface pressure measurements are normalized using their statistical moments - specifically the\nmean (up) and standard deviation (6p) computed from the measured pressure distribution along the airfoil surface. The\nsame normalization is applied to the target pressure field. For velocity predictions, we normalize the target values by\nscaling them with respect to the estimated inflow velocity (\u00db\u221e), which can be computed using Bernoulli's principle.\nGiven that all airfoils are simulated with a zero farfield static pressure, the farfield inflow velocity magnitude can be\napproximated as:\n$\\hat{U}_\\infty = \\sqrt{\\frac{2}{\\rho} p_o}$\n(5)\nwhere p is the density of air (constant throughout simulations) and po is the total pressure measured at the stagnation\npoint, which can be estimated by taking the maximum pressure at the airfoil nodes po = max(pv). While Bernoulli's\nprinciple is not valid for turbulent flows such as the ones we try to reconstruct, it serves as a good proxy for normalization\npurposes. A key advantage of this normalization strategy lies in its exclusive reliance on surface measurements to\ncompute the normalization parameters. This design choice potentially enables the model to reconstruct flows beyond\nthe range of inflow conditions present in the training dataset, as all normalizing quantities are derived solely from\nobservable surface data. This could be particularly promising for practical applications where direct measurement of\nfreestream conditions may be unavailable."}, {"title": "5 Hybrid Graph Transformers for inverse physics on meshes", "content": "In this section we describe the components which make up our Flow Reconstruction Graph Transformer (FRGT) model.\nThe overall architecture is shown in Figure 6. The codebase for our model is made public and can be accessed at\nhttps://github.com/gduthe/FRGT."}, {"title": "5.1 Overview of the architecture", "content": "As a preliminary step, we use Feature Propagation [54], a matrix interpolation algorithm, to initialize the unknown node\nfeatures. This step is an important part of our framework as it conditions an input graph into a plausible initial state,\nessentially radiating the surface node features outwards. We found 30 feature propagation iterations to be sufficient.\nFor the rest of our graph-based reconstruction framework, we adopt the now-popular Encode-Process-Decode graph\nlearning architecture [49, 57]. The main idea behind this approach is to project the input features into a higher-\ndimensional latent space of size d where the message-passing and attention operations can learn more expressive"}, {"title": "5.2 Graph Transformer processor", "content": "We introduce here the setup of the hybrid Graph Transformer processor. Similar to the approach outlined in [53], we\nchoose to use MPNNs to gather structural and geometrical information of the graph into informative node features\nbefore feeding them to a Transformer layer. This choice is motivated by several considerations: (1) MPNNs excel at\nefficiently processing large scale graphs, (2) geometrical edge attributes can easily be incorporated into message-passing\nallowing us to effectively encode geometric attributes such as relative distances, angles, and other spatial relationships\nbetween connected nodes, and (3) MPNNs circumvent the challenging problem of designing effective positional\nencodings for Graph Transformers, which remains an active area of research with multiple competing approaches.\nGathering local information via message-passing MPNNs operate by iteratively updating node representations\nthrough information exchange along edges, capturing both local structural patterns and geometric relationships. Each\nmessage-passing layer follows a dual-phase approach: first computing messages mi,j between connected nodes, then\naggregating these messages at each node through a pooling operation which is then fed through a function to update\neach node. In our work, we use the GENeralized Aggregation Networks (GEN) message-passing formulation [37],\nwhich extends the classical GCN to support edge features and more sophisticated message aggregation functions.\nThis formulation has proven to be successful when applied to physics-based problems that depend on geometrical\nfeatures [13, 18]. A single GEN update layer l consists of a message computation phase incorporating edge attributes\nai,j:\n$m^{(l)}_{ij} = ReLU(h^{(l)}_i + a_{i,j}) + \\epsilon$\n(8)\nfollowed by an aggregation step that uses a learnable softmax function to weight the importance of different messages\nin the node update function:\n$AGG(m^{(l)}_i) = \\sum_{j \\in N_i} \\frac{exp(\\beta m^{(l)}_{ij})}{\\sum_{k \\in N_i} exp(\\beta m^{(l)}_{ik})} m^{(l)}_{ij}$\n(9)\n$h^{(l+1)}_i = MLP^{(l)}(h^{(l)}_i + AGG(m^{(l)}_i)), j \\in N_i$\n(10)\nThis formulation allows the model to adaptively weigh the contributions of different neighbors based on their relevance,\nwhile maintaining numerical stability through the e term (usually set to 10-7).\nEfficient long-range transmission of information using a linear Transformer After the message-passing steps,\nthe updated node features are fed into a Transfomer layer to capture long range interactions. Unlike the message-\npassing operations, which are constrained by the graph's connectivity, the self-attention mechanism in Transformers\nenables direct communication between all pairs of nodes, allowing the model to learn salient dependencies regardless\nof geometric distance or graph topology. To efficiently handle our large mesh-graphs, we opt for the Galerkin\nTransformer [8], a linear variant specifically optimized for PDE-related tasks. The key innovation in this approach lies\nin its normalization scheme and simplified attention mechanism, which can be expressed as:\n$z_i = ATTN(h_i) = Q(K^T V) / n$\n(11)\nwhere Q, K and V are respectively the query, key and value vectors obtained via learned linear projections, and\nn denotes the number of nodes in the graph. By replacing the traditional softmax normalization with a Galerkin\nprojection-based layer normalization scheme, this Neural Operator (NO) is both accurate and computationally efficient,\nwith empirical evaluations demonstrating strong performance across diverse PDE applications. In practice, we use\n\u03b7 attention heads in parallel, where each head independently processes the input latent node feature matrix H into\nrepresentations for each node of dimensionality dn, which are then concatenated:\n$MultiHead(H) = Concat(head_1, ..., head_\\eta)W^O$\n(12)\nwhere $head_\\eta = ATTN(HW^Q, HW^K, HW^V)$. $W^Q, W^K, W^V$ are learned weight matrices specific to head \u03b7 and\nWO is an output projection matrix. This multi-headed architecture enables the model to jointly attend to information\nfrom different representation subspaces at different positions, increasing its expressivity."}, {"title": "Combining local and global layers", "content": "The local MPNN and global Transformer layers could be combined using a\nnumber of different integration strategies. In this work, we consider first an approach that follows a sequential design\nwhere L message-passing layers are stacked, followed by T Transformer layers, allowing the model to first capture\nlocal patterns before applying global corrections. An alternative approach, inspired by [53], interleaves C combined\nmessage-passing and Transformer layers, potentially enabling more complex hierarchical feature extraction through the\nsimultaneous consideration of local and global information at each level. Figure 7 illustrates these two approaches.\nWe investigate both strategies and explore how these different architectural choices impact the model's reconstruction\nability."}, {"title": "6 Results", "content": "We conduct an initial set of experiments to systematically evaluate our model's performance and analyze the impact of\nkey architectural decisions. In these experiments, we consider the idealized scenario where pressure measurements are\navailable at all airfoil surface nodes. To ensure fair comparison across architectural variants, we maintain a consistent\nmodel capacity of approximately 1.4M trainable parameters. This constraint is chosen such that the model's memory\nfootprint for the largest graph of the dataset during training does not exceed the 24GB of VRAM of the RTX 4090 GPU\nthat we use. All models are implemented using PyTorch and PyTorch-Geometric and are trained to minimize a L2\nloss under identical conditions. Specifically, we train the models for 500 epochs using the AdamW optimizer [42] with\na weight decay of 1\u00d710-4 and an initial learning rate of 5\u00d710\u22124. The learning rate is annealed during training through\na cosine decay function [43]."}, {"title": "6.1.1 Interleaved vs stacked layers", "content": "The combination of local (MPNN) and global (Transformer) processing layers can follow different architectural patterns.\nWe investigate two integration strategies: a sequential approach where MPNN layers are followed by Transformer layers,\nand an interleaved design that alternates between them. We benchmark both approaches against a pure message-passing\nbaseline, which uses the Reversible GAT architecture from [17]. Table 2 displays reconstruction error metrics gathered\non unseen airfoils of the test dataset for these three models."}, {"title": "6.1.2 Architecture design trade-offs for the FRGT-S model", "content": "We then analyze key architectural trade-offs in our hybrid stacked Graph Transformer model through two studies,\nthe results of which are shown in Figure 9. First, we examine the balance between local message-passing and global\nattention mechanisms while maintaining a fixed parameter budget (1.4M trainable parameters)."}, {"title": "6.2 Reconstruction of flows around unseen airfoils", "content": "Figure 10 presents qualitative results of our FRGT-S model's performance on unseen airfoils within the training\ndistribution of the angle of attack (-20\u00b0 to 20\u00b0). The visualization compares ground truth CFD solutions with\nreconstructed fields across six different test cases, spanning both pressure and velocity predictions at varying Reynolds\nnumbers (Re) and angles of attack (AoA). The pressure field reconstructions (top three rows) demonstrate the model's\nability to accurately capture the pressure distribution around different airfoil geometries. The difference plots reveal\nthat errors remain small across the domain, with slight deviations primarily occurring in the near-wall region and\nwake area. The model successfully reproduces key flow features, such as the stagnation point at the leading edge and\npressure gradients close to the airfoil surface. The velocity field predictions (bottom three rows) show equally promising\nresults, with the model accurately reconstructing the flow patterns, including the velocity gradients in the boundary\nlayer and wake regions. The difference plots indicate that the largest discrepancies appear in the wake region, where\nflow structures are more complex and sensitive to small perturbations. However, these errors remain within acceptable\nbounds (\u0394u typically less than 5 m/s), particularly considering the challenging nature of velocity field reconstruction\nfrom surface measurements alone. Notably, the model maintains consistent performance across different Reynolds\nnumbers and angles of attack, suggesting robust generalization to unseen airfoil geometries within the trained parameter\nspace."}, {"title": "6.3 Partial airfoil coverage", "content": "Real-world aerodynamic sensing applications often face practical limitations in sensor coverage. For instance, the\nAeroSense MEMS-based pressure measurement system [4], while offering robust in-field blade aerodynamics monitor-\ning capabilities, is met with inherent coverage constraints due to electronics limitations. This is particularly relevant for\nlarge-scale wind turbine blades, where comprehensive surface coverage becomes increasingly challenging. To assess\nour method's viability under such realistic sensing conditions, we evaluate the performance of the FRGT-S model across\nvarying degrees of sensor coverage. Our experimental setup simulates different sensor configurations by restricting\npressure measurements to 20%, 40%, 60%, and 80% of the chord length along the front half of the airfoil. We train\nidentical models for each partial coverage scenario and compare them against a baseline case with full surface coverage\nto quantify the degradation in reconstruction quality. Table 3 summarizes the changes in the reconstruction quality\nwhen airfoil coverage is reduced and Figure 12 displays qualitative reconstruction results across the different sensor\ncoverage scenarios."}, {"title": "7 Discussion", "content": "The results presented in this work demonstrate the potential of hybrid Graph Transformer architectures for flow field\nreconstruction from sparse surface measurements on meshes. However, several key challenges and opportunities emerge\nfrom our findings that warrant further investigation.\nFirstly, while our FRGT architectures achieve promising results with approximately 1.4M parameters, this remains\nmodest compared to modern large-scale deep learning models [67]. The primary constraint on scaling stems from\nthe memory requirements of back-propagating gradients for our large mesh-based graphs, where even a single airfoil\ncase can contain upwards of 50,000 nodes. The quadratic complexity of traditional attention mechanisms becomes\nprohibitive in this context, even with our adoption of linear attention variants. Scaling to much larger model capacities\nwould likely require distributed training and more sophisticated memory management techniques.\nAnother challenge lies in the inherent structure of aerodynamic meshes. The necessarily high cell density near airfoil\nsurfaces, critical for resolving boundary layer physics, creates an implicit optimization bias. The model naturally\nprioritizes accuracy in these dense near-wall regions, where the majority of nodes reside, potentially at the expense\nof far-field prediction quality. Future work might explore custom loss functions that better balance near- and far-field\nreconstruction accuracy, perhaps through adaptive weighting schemes based on mesh density. Moreover, our dataset\nreflects the traditional design philosophy of airfoils, which are often skewed for positive angle of attack operation. Our\nsampling strategy, which covers equally the full angle of attack range, results in an under-representation of separated\nflow conditions at positive angles, as separation typically occurs earlier at negative angles of attack for many airfoils.\nFuture iterations could consider targeted data augmentation strategies to create a more balanced distribution of flow\nregimes. Additionally, while our use of unstructured triangular meshes provides good flexibility and interpolation\nopportunities, the consistent meshing strategy may allow the model to learn implicit patterns in the spatial discretization\nrather than the purely physical relationships. Although triangular elements offer natural adaptability, the model may\ndevelop biases towards expected cell arrangements and refinement patterns. Future work should investigate training\non multiple discretization types, including structured meshes, hybrid topologies, and even random point clouds with\nDelaunay triangulation. This diversity would help ensure the model learns true flow physics rather than potentially\nexploiting specific mesh patterns.\nSeveral promising directions exist for enhancing the FRGT architecture through physics-informed priors and uncertainty\nhandling. A particularly compelling approach would be to combine our data-driven method with physics-based inflow\nestimation techniques. For instance, potential flow models coupled with conformal mapping can provide reliable\nestimates of key flow parameters like stagnation point location and freestream conditions from surface pressure\nmeasurements [45]. These physics-derived estimates could serve as conditioning variables for our Graph Transformer,\nproviding an informed prior that helps constrain the space of possible flow field reconstructions. Such physics-ML\nintegration could be implemented through various strategies [25]: regularization terms during training, embedding\nknown conservation laws into the loss function, or even designing architecture components that explicitly respect\nphysical symmetries. This could potentially improve reconstruction accuracy while also enhancing generalization to\nflow conditions outside the training distribution and reducing the amount of required training data.\nOur partial coverage experiments reveal robustness in the FRGT's reconstruction capabilities. Even with only 20%\nsensor coverage along the front half of the airfoil, the model maintains reasonable prediction accuracy, with velocity\ncomponent errors increasing by only 53.17% and 29.87% for ux and uy respectively. Given the practical constraints of\ninstrumentation in experimental aerodynamics, this robustness is particularly noteworthy. It is often difficult to achieve\na comprehensive pressure tap coverage due to structural requirements, internal space limitations, or cost constraints.\nThe demonstrated capability to reconstruct meaningful flow fields from limited sensor data opens up new possibilities\nfor testing. For instance, rapid flow field estimates could enable real-time adjustment of wind tunnel parameters or flight\ntest conditions, potentially reducing the number of test points required to characterize an aerodynamic configuration.\nFurthermore, the model's ability to handle partial measurements suggests applications in extended health monitoring,\nwhere progressive sensor failures could be compensated for without complete system replacement."}, {"title": "8 Conclusion", "content": "This work introduces the Flow Reconstruction Graph Transformer (FRGT), a hybrid architecture that combines\nmessage-passing neural networks with efficient Transformers to reconstruct aerodynamic flow fields from surface\npressure measurements. We evaluate our models on a diverse dataset of airfoil configurations and demonstrate that\nthis approach can effectively reconstruct both pressure and velocity fields while maintaining computational efficiency,\nwith inference times around 200ms on consumer hardware. Our results highlight several key findings. First, the\ncombination of local geometric processing through message-passing and global information exchange via linear\nattention proves particularly effective for flow reconstruction tasks, outperforming pure message-passing approaches\nin both accuracy and computational efficiency. Second, the architecture demonstrates robustness to sensor coverage\nreduction, maintaining reasonable velocity field predictions even with only 20% of the nominal sensor coverage. This\nresilience has important implications for practical sensing applications where"}]}