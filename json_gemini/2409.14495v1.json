{"title": "Thought-Path Contrastive Learning via Premise-Oriented Data Augmentation for Logical Reading Comprehension", "authors": ["Chenxu Wang", "Ping Jian*", "Zhen Yang"], "abstract": "Logical reading comprehension is a challenging task that entails grasping the underlying semantics of text and applying reasoning to deduce the correct answer. Prior researches have primarily focused on enhancing logical reasoning capabilities through Chain-of-Thought (CoT) or data augmentation. However, previous work constructing chain-of-thought rationales concentrates solely on analyzing correct options, neglecting the incorrect alternatives. Addtionally, earlier efforts on data augmentation by altering contexts rely on rule-based methods, which result in generated contexts that lack diversity and coherence. To address these issues, we propose a Premise-Oriented Data Augmentation (PODA) framework. This framework can generate CoT rationales including analyses for both correct and incorrect options, while constructing diverse and high-quality counterfactual contexts from incorrect candidate options. We integrate summarizing premises and identifying premises for each option into rationales. Subsequently, we employ multi-step prompts with identified premises to construct counterfactual context. To facilitate the model's capabilities to better differentiate the reasoning process associated with each option, we introduce a novel thought-path contrastive learning method that compares reasoning paths between the original and counterfactual samples. Experimental results on three representative LLMS demonstrate that our method can improve the baselines substantially across two challenging logical reasoning benchmarks (ReClor and LogiQA 2.0). The data and code are released at https://github.com/lalalamdbf/TPReasoner.", "sections": [{"title": "1 Introduction", "content": "Logical reasoning is a fundamental component of human cognition, essential for comprehending text and applying reasoning to deduce appropriate conclusions. Recently, challenging logical reasoning benchmarks have been proposed through machine reading comprehension (MRC) tasks (Yu et al. 2020; Liu et al. 2023a), which require models to derive the correct answer based on the given context, question and options. With the advent of large language models (LLMs), enhancing their capabilities in logical reasoning is a crucial step toward achieving strong artificial intelligence (Chollet 2019). Especially, the highly advanced model, GPT-4 (Achiam et al. 2023), has exhibited remarkable abilities to handle such tasks. However, a broad spectrum of open LLMs, including LLaMA2 (Touvron et al. 2023), Mistral (Jiang et al. 2023) and LLaMA3 (AI@Meta 2024), still fall short in logical reasoning, significantly trailing behind GPT-4. Consequently, improving the logical reasoning capabilities of community models has increasingly garnered the attention of many researchers (Liu et al. 2023b; Jiang et al. 2023).\nFor logical MRC tasks, LogiCoT (Liu et al. 2023b) constructs instruction-tuning data with Chain-of-Thought (CoT) rationales. Nevertheless, these rationales only provide analyses for the correct options, neglecting the incorrect alternatives. This oversight limits the model's ability to fully understand why certain answers are wrong, which is crucial for enhancing its reasoning capabilities and overall performance in distinguishing between similar options. In addition, previous studies typically create counterfactual contexts based on rule-based data augmentation. For instance, LReasoner (Wang et al. 2022) generates logically nonequivalent sentences by utilizing templates and syntax parsing. AMR-LDA (Bao et al. 2023) constructs counterfactual sentences based on Abstract Meaning Representation (AMR, Banarescu et al., 2013) graph and logical laws. These methods rely on complex principles and make minimal changes to the text, that cannot ensure the diversity of generated content and accurate modifications to its underlying logic. Additionally, they directly modify the context without considering its relationship with the options, which leads to a mismatch between the counterfactual context and options.\nIn view of above challenges, we propose a premises-oriented data augmentation (PODA) framework. As shown in Figures 1 and 2, the objective of PODA is to generate CoT rationales that include analyses for both correct and incorrect options, while also constructing counterfactual contexts based on incorrect candidate options. In Figure 2, analyses for both correct and incorrect options are presented in Analyze options. Besides, we incorporate summarizing premises and identifying premises for each option into rationales. Each option has a specific relationship with these premises\u2014either supported, contradicted, or unrelated. PODA will create high-quality and diverse counterfactual contexts using multi-step prompts based on these premises and relationships. Furthermore, since supervised fine-tuning (SFT) focuses solely on individual instances, it lacks the comparison between different samples. For original and counterfactual samples, there are thought-paths that indicate similar and dissimilar reasoning processes associated with options. Therefore, we propose a thought-path contrastive learning approach, which specifically compares thought-paths across different samples, facilitating the model's capabilities to better distinguish diverse reasoning paths. The main contributions of this paper are summarized as follows:\n\u2022 We propose a premise-oriented data augmentation framework, which can generate CoT rationales involving analyses for both correct and incorrect options, while automatically constructing diverse and high-quality counterfactual"}, {"title": "2 Related Work", "content": "Data from incorrect candidate options.\n\u2022 We introduce a thought-path contrastive learning approach, facilitating models to distinguish different reasoning paths between original and counterfactual samples.\n\u2022 Experimental results conducted on representative open LLMs (LLaMA2-7B, Mistral-7B and LLaMA3-8B) demonstrate that our method achieves superior performance on two logical MRC benchmarks."}, {"title": "2.1 Chain-of-Thought Prompting", "content": "LLMs are capable of performing complex reasoning to derive the final answer by generating intermediate reasoning steps through a process called Chain-of-Thought (CoT). Zero-shot-CoT (Kojima et al. 2022) showcases impressive reasoning performance only using a single instruction \"Let's think step by step\". Few-shot-CoT (Zhang et al. 2022; Wang et al. 2023) further boosts the reasoning abilities of LLMs by incorporating several CoT demonstrations. In addition, by offering carefully-crafted CoT demonstrations, LLMs can be encouraged to develop the similar reasoning skills and deliver responses in a uniform format. To ensure obtained CoTs are well-structured, we adopt Few-shot-CoT for data collection using GPT-3.5 and GPT-4. Recently, Liu et al. (2023c) also utilized GPT-4 to annotate the intermediate steps of correct options for logical MRC tasks. In contrast, our study expands the analysis to include incorrect options and focuses on mining information from CoT rationales to generate new logical MRC data."}, {"title": "2.2 Logical Reasoning", "content": "Leveraging logical reasoning capabilities embodies a comprehensive approach to natural language understanding (NLU). Previous studies have primarily focused on integrating logical knowledge into language models. For example, Huang et al. (2021) exploited a logic graph to model semantic relationships. Wang et al. (2022) and Bao et al. (2023) constructed equivalent/nonequivalent instances through intricate logic rules and entity replacement. These techniques, however, are constrained by their reliance on manually designed rules, which struggles to reliably identify complex logical relationships in diverse texts. Thus, our work shifts away from annotating logical relationships. Instead, we decompose and construct contexts using premises as the foundational units. Moreover, our contrastive learning approach improves LLMs' logical reasoning capabilities by enabling them to distinguish various thought-paths."}, {"title": "3 Methodology", "content": "Figure 3 shows the overall architecture of our method (PODA-TPCL). It consists of two key components: Premise-Oriented Data Augmentation (PODA) and Thought-Path Contrastive Learning (TPCL). The former module is aimed at generating CoT rationales that comprise analyses for correct and incorrect options, while constructing diverse and high-quality counterfactual logical reasoning data from incorrect candidate options. The latter one enhances the rea-"}, {"title": "3.1 Premise-Oriented Data Augmentation", "content": "PODA initially creates analyses by forming thought-paths for both correct and incorrect options. Summarizing premises and identifying premises for each option are incorporated into CoT rationales, which are essential for generating new data. The core idea of it is to prompt a large language model through in-context learning to generate counterfactual data that can flip the current answer to a new answer (e.g., Option a \u2192 Option c). A context can be divided into Premises (the known information from the text), which have specific relationships with the options. The relationships are categorized into three types: supported, contradicted and unrelated. Utilizing the premise as a foundational unit, we can construct counterfactual samples based on these relationships.\nCoT Rationale Annotation As illustrated in Figure 2, we design a structural CoT consisting of three steps: (1) Summarize Premises: Extract supporting statements from the context to serve as premises. (2) Analyze Options: Conduct a thorough evaluation of each option, elucidating the specific relationships between the options and premises, referred to as thought-path in our work. (3) Derive answer: Consolidate all thought-paths and determine the final answer. To guarantee a well-structured CoT, we utilize Few-shot-CoT for data collection.1"}, {"title": "3.2 Thought-Path Contrastive Learning", "content": "Supervised fine-tuning (SFT) can notably improve the model's performance. However, SFT only focuses on single instances, which results in its lack of the comparison between different samples. For logical MRC tasks, PODA annotates Chain-of-Thought (CoT) rationales, offering analyses for both correct and incorrect options, while also generating counterfactual samples. It can be observed that thought-paths exhibit similar and dissimilar reasoning processes associated with options in original and counterfactual samples.\nIn light of such motivation, we propose a thought-path contrastive learning approach. As depicted in the part (2) of Figure 3, the original/counterfactual sample has four thought-paths, with each corresponding to one option. Three thought-paths indicate that the corresponding options are incorrect, while one thought-path suggests it is correct. Therefore, for the original and counterfactual sample pair, the reasoning processes of thought-paths 2 and 2' (thought-paths 4 and 4') are analogous, whereas those of thought-paths 1 and 1' (thought-paths 3 and 3') are different. The goal of our method is to pull similar thought-paths closer while pushing different ones far apart. Simultaneously, we seek to enhance the model's capabilities to precisely distinguish between pairs of thought-paths (e.g., similarity(thought-paths 4, 4') > similarity(thought-paths 2, 2')).\nInspired by recent advances in learning to preference optimization algorithms such as RLHF (Ouyang et al. 2022) and DPO (Rafailov et al. 2023), our objective is to present a simple approach for comparing the similarity of thought-path pairs. To achieve this objective, we employ the Bradley-Terry preference model (Bradley and Terry 1952) to construct the loss function for the similarity comparison. Given the input pair \\(\\pi_0 = (x_1, x_2)\\), the Bradley-Terry model calculates the likelihood of similarity comparison over thought-path pairs, denoted as \\((p_s,p'_s) > (p_a,p'_a) | \\pi_0\\), where \\((p_s,p'_s)\\) and \\((p_a,p'_a)\\) represent the similar and different thought-path pairs, respectively. In our work, we simply choose the reward function \\(r^* = =sim(\\cdot)\\) to measure similarity using cosine distance, where \\(\\tau\\) is the temperature coefficient controlling the sharpness of the similarity distribution. Under the Bradley-Terry model, we can derive a streamlined probability measure for pairwise similarity comparison:\n$$p^*((p_s, p'_s) > (p_a, p'_a) | \\pi_0) = \\sigma(r(p_s, p'_s) \u2013 r(p_a, p'_a))$$\n$$\\frac{1}{1 + exp[sim(p_a, p'_a) - sim(p_s,p'_s)]}$$\nwhere \\(\\pi_0\\) is omitted as it doesn't directly contribute to the calculation. Then we formulate the problem as binary classification using the negative log-likelihood loss:\n$$L((p_s, p'_s), (p_a, p'_a), \\pi_0)) = -E[log(\\sigma(r(p_s,p'_s) \u2013 r(p_a, p_a)))]$$\nThe objective of this loss function is to decrease the distance between dissimilar pair (pa,pa) while increasing the distance between similar pair (ps, p's). Additionally, the model learns to differentiate between pairs of thought-pairs based on preference optimization. Due to the presence of multiple groups of similar and dissimilar thought-path pairs for input \u03c0\u03bf, we simply calculate the average loss as follows:\n$$L_{TPCL}((p_s, p'_s), (p_a, p'_a), \\pi_0)) = \\frac{1}{NM} E_{NM} \\sum_{i=1} \\sum_{j=1} log([(\\sigma(r(p_{sj}, p'_{sj})\n- r(p_{di}, p'_{di})))]$$"}, {"title": "4 Result and Analysis", "content": "capabilities.\n4.1 Datasets\nReClor (Yu et al. 2020) comprises 6,138 question-answering samples collected from standardized exams including GMAT and LSAT, which are split into train / dev / test sets with 4,638 / 500 / 1,000 samples respectively. To evaluate the difficulty of the questions, the test set is further divided into Test-E and Test-H. The instances on Test-E are easy and biased that can be solved without knowing contexts and questions. The other harder and unbiased ones are taken as the Test-H set.\nLogiQA 2.0 (Liu et al. 2023a) is an updated and re-annotated version of LogiQA (Liu et al. 2020). There are 15,708 instances derived from the Chinese Civil Service Examination, meticulously translated into English by experts.\nThe dataset is randomly split into train / dev / test sets with 12,567 / 1,569 / 1,572 samples respectively.\nSynthetic Data is generated by our PODA framework. We construct 5,075 and 13,477 counterfactual samples based on the train sets of ReClor and LogiQA 2.0, respectively. To perform thought-path contrastive learning, each counterfactual sample is paired with its corresponding origin one."}, {"title": "4.2 Implementation Settings", "content": "In PODA framework, gpt-3.5-turbo-0613 and gpt-4-0613 are utilized for CoT Rationale Annotation. Subsequently, gpt-4-0125-preview is employed for Premises and Context Generation. In the end, gpt-4-0613 is used for Correctness Verification. We set the sampling temperature of 0.75 and the top probability of 0.9, ensuring the generated text maintains both diversity and high quality. The detailed prompts are provided in Appendix C.2.\nDuring the training process, we adopt LLaMA2-7B, Mistral-7B and LLaMA3-8B as baselines. In order to accelerate training, we employ LoRA (Hu et al. 2021) to fine-tune the model. The AdamW optimizer (Loshchilov and Hutter 2017) is used with a learning rate warmup of 0.03. Due to the absence of corresponding counterfactual samples for some original samples as illustrated in Correctness Verification, we implement a two-stage training strategy. The implementation of our code refers to Llamafactory (Zheng et al. 2024). All hyper-parameters of training are listed in Appendix A."}, {"title": "5.1 Overall Results", "content": "Table 1 presents the primary experimental results of our method and other baselines on ReClor and LogiQA 2.0 benchmarks, in terms of accuracy. Our method employs the CoT rationales, which is appropriate for generative LLMs. Hence, we did not conduct experiments on discriminative language models. Compared to these baselines, TPReasoner exhibits superior performance except for GPT-4.\nOn ReClor dataset, LLaMA3-8B and Mistral-7B, based on our approach, significantly outperform all discriminative model methods. Compared with LReasoner, PODA-TPCL achieves improvements of 1.4-3.5% and 8-9% on the dev and test sets, respectively. Although LLaMA2-7B, when using our method, does not surpass all discriminative model baselines, the results on Test-H demonstrate that it exhibits stronger robustness and generalization for data distribution. There is a substantial disparity (exceeding 30%) between the performances on Test-E and Test-H for discriminative models, indicating that these models tend to take shortcuts for simpler and biased samples rather than genuinely comprehending them. In contrast, our method achieves a gap of less than 15% between Test-E and Test-H, with the performance on Test-H clearly surpassing that of the discriminative models. This demonstrates the great potential of leveraging CoT rationales to solve complex logical reasoning tasks."}, {"title": "5.2 Ablation Study", "content": "An ablation study is conducted to investigate the efficacy of three key components, thought-path contrastive learning (TPCL), counterfactual data (CD) and wrong options analyses (WOA), as presented in Table 2. For w/o TPCL, we eliminate TPCL and only employ SFT to train the model. There is a noticeable decline in performance, with a drop of 1-3% across the two datasets. These results convincingly demonstrate that TPCL significantly boosts the model's reasoning capabilities by comparing reasoning paths between original and counterfactual samples. For w/o TPCL + CD, we additionally exclude the counterfactual samples generated by PODA and solely utilize the original data. It can be observed that the models without CD have severe performance degradation. This suggests that the counterfactual samples are beneficial for LLMs to conduct logical reasoning. Furthermore, it demonstrates that the data synthesized by our framework is of high-quality, automatically generated without requiring human interventions. For w/o TPCL + CD + WOA, we further omit the analyses of wrong options in CoT rationales. As a result, the models' performance decreases by approximately 2%. It indicates that incorporating reasoning processes for incorrect options enables the model to analyze problems more thoroughly, thereby improving its reasoning abilities. Overall, PODA-TPCL achieves a performance improvement of 5-7% across three models on ReClor and LogiQA 2.0 datasets, underscoring its exceptional robustness and generalization"}, {"title": "5.3 Evaluation of Data Quality", "content": "Accuracy Evaluation of Counterfactual Data A primary concern is whether the synthetic data accurately matches the correctly labeled option. In order to evaluate this, we choose five outstanding LLMs, including Mixtral-8\u00d77B-Instruct, GPT-3.5 (gpt-3.5-turo-0613), LLaMA2-70B-chat, LLaMA3-70B-Instruct and GPT-4 (gpt-40). Non-GPT series models evaluate all counterfactual data. Due to budget constraints, a random subset of 200 samples from the generated dataset are evaluated by GPT-3.5 and GPT-4. We utilize 3-shot CoTs to evaluate the accuracy. As illustrated in Table 3, the accuracy assessed by LLaMA3-70B-Instruct and GPT-4 can reach 82.19% and 93% respectively, indicating PODA can generate high-quality counterfactual data. Moreover, accuracies for other models vary between approximately 75% and 79%, reflecting the significant challenges and complexities presented by these data. Overall, the generated data can have applicability in both training and evaluation domains.\nComparison for Counterfactual Data We compare PODA with a rule-based method, LReasoner (Wang et al. 2022), which constructs counterfactual contexts by modifying logical expressions according to logical laws. Two random subsets of 200 counterfactual samples were selected respectively. We ensure that the two subsets are derived from the same set of original samples for a fair comparison. These instances are evaluated by GPT-4 (gpt-40) using four key metrics: Coherence (Is the context well-connected and logically consistent?), Clarity (Is the context clear and easy to understand?), Relevance (Does the context relate to the question and options?), and Diversity (How does the counterfactual context differ from the original one?). Each context was rated on a scale from 1 (poor) to 5 (excellent) for each metric. Our method attains average scores of 4.61 for Coherence, 4.36 for Clarity, 4.71 for Relevance, 3.18 for Diversity, and 4.22 Overall. In contrast, LReasoner achieves average scores of 2.98 for Coherence, 2.96 for Clarity, 4.63 for Relevance, 1.08 for Diversity, and 2.91 Overall. This comparison clearly demonstrates that the contexts generated by our method significantly outperform those produced by the rule-based method in both quality and diversity.\nComparison for CoT Rationales We compare PODA with LogiCoT, which also utilizes GPT-4 for CoT rationales but focuses solely on analyzing the correct option. Similarly, two random subsets of CoT rationales were selected from the same contexts. These rationales are assessed by GPT-4 (gpt-40) using four key metrics: Coherence (Is the CoT rationale logically consistent?), Completeness (Does it of-"}, {"title": "5.4 What does TPCL update do?", "content": "We analyze the gradient of the loss function LTPCL (considering a group of similar and dissimilar thought-path pairs), whose gradient can be written as:\n$$\\nabla L_{TPCL} ((p_s, p'_s), (p_a, p'_a), \\pi_0)) =$$\n$$-E[(\\frac{1}{1 + exp(r(p_a,p'_a) \u2013 r (p_s, p_s))}\n* [\\nabla sim(p_s, p'_s) \u2013 \\nabla sim(p_a, p_a)]]$$\nIntuitively, the gradient of LTPCL increases the similarity of the similar thought-paths ((ps, p') and decreases the similarity of the dissimilar thought-paths ((pa, pa). Meanwhile,\u03c3(r(Pa,Pa) \u2013 r(ps,p')) serves as an adjustable weight for the similarity reward estimate, assigning greater weight when r(pa,pa) is approximately equal to or greater than r(ps,p's). This mechanism accelerates the convergence of the loss function. Overall, the gradient update aligns with our objective to pull similar thought-paths closer while pushing dissimilar ones further apart, which enhances the model's reasoning capabilities by comparing thought-paths between the original and counterfactual samples."}, {"title": "6 Conclusion", "content": "In this paper, we propose a premise-oriented data augmentation framework that generates CoT rationales, providing analyses for both correct and incorrect options. Additionally, the framework automatically constructs diverse and high-quality counterfactual data from incorrect candidate options. We also introduce a thought-path contrastive learning method to effectively leverage pairs of original and counterfactual samples, enabling models to distinguish between different reasoning paths. Extensive experiments conducted on three open LLMs demonstrate that our approach achieves competitive performance on two logical reasoning benchmarks."}, {"title": "C.1 Workflow", "content": "The premises-oriented data augmentation framework (PODA) consists of four stages: CoT Rationale Annotation, Premises Generation, Context Generation, and Correctness Verification. We illustrate the workflow of PODA in Algorithm 1."}, {"title": "C.2 Prompts for Generating Data", "content": "The prompts used for CoT Rationale Annotation, Premises Generation, Context Generation, and Correctness Verification are detailed in Figures 2, 3, 4 and 5, respectively. We configured the sampling temperature at 0.75 and set the top probability at 0.9."}, {"title": "C.3 Prompts for Evaluating Data Quality", "content": "The prompts for evaluating the quality of the counterfactual context cover Coherence (Is the context well-connected and logically consistent?), Clarity (Is the context clear and easy to understand?), Relevance (Does the context relate to the question and options?) and Diversity (How does the counterfactual context differ from the original one?). These prompts are depicted in Figures 6, 7, 8 and 9. Additionally, the prompts for evaluating the CoT rationales include assessments of Coherence (Is the CoT rationale logically consistent?), Completeness (Does it offer a thorough explanation for the reasoning?), Relevance (Does it directly and effectively addresses the context, question and options?) and Faithfulness (Is it factually correct and free from fabricated details?). These are shown in Figures 10, 11, 12 and 13. We still set a sampling temperature of 0.75 and a top probability of 0.9."}]}