{"title": "EVALUATING NEURAL NETWORKS ARCHITECTURES FOR SPRING REVERB MODELLING", "authors": ["Francesco Papaleo", "Xavier Lizarraga-Seijas", "Frederic Font"], "abstract": "Reverberation is a key element in spatial audio perception, historically achieved with the use of analogue devices, such as plate and spring reverb, and in the last decades with digital signal processing techniques that have allowed different approaches for Virtual Analogue Modelling (VAM). The electromechanical functioning of the spring reverb makes it a nonlinear system that is difficult to fully emulate in the digital domain with white-box modelling techniques. In this study, we compare five different neural network architectures, including convolutional and recurrent models, to assess their effectiveness in replicating the characteristics of this audio effect. The evaluation is conducted on two datasets at sampling rates of 16 kHz and 48 kHz. This paper specifically focuses on neural audio architectures that offer parametric control, aiming to advance the boundaries of current black-box modelling techniques in the domain of spring reverberation.", "sections": [{"title": "1. INTRODUCTION", "content": "In addition to pitch, volume, timbre and tempo, spatial perception is a fundamental dimension of sound for humans. When a source emits sound, it radiates through the medium (usually air) and is received directly by the listener; this is known as direct sound. However, sound waves also reflect off surfaces in the environment before reaching the listener; these reflections arrive with a delay, attenuated in intensity and filtered in frequency, resulting in our perception of the acoustic space, which we call reverberation [1].\nThroughout the history of music, the acoustics of physical locations have been used as a means to intentionally transform musical performances and provoke particular emotional states in listeners. By the early 20th century, the possibility of recording sound on a fixed medium added a new dimension unrelated to architectural acoustics that opened up new creative practices with audio effects [2, 3].\nIn terms of sound capturing, close miking reduces background noise but do not capture the acoustics of the space, resulting in very unnatural recorded material. In 1926, RCA patented reverberation chambers to overcome this limitation: a loudspeaker was placed in a room and the sound emitted was captured by a microphone placed at a distance. Later, in an attempt to achieve greater flexibility, engineers began to combine analogue electronics with mechanical systems [4, 5]. This led Laurens Hammond, in 1939, to file a patent aimed at \"providing an improved electric musical instrument having means of introducing a selected degree of reverberation effect into music, regardless of the acoustic properties of the place where the instrument is played\" [6]. This technology is what we currently know as spring reverb.\nAfter 1950 there has been a great development of analogue audio equipment, including artificial reverberation units: mainly spring, plates and tapes. These devices, which have shaped the techniques and aesthetics of contemporary music production and recording, exhibit non-linear behaviour and produce a certain degree of distortion [7, 8, 9, 10, 11]. During the 1960s, Hammond and Fender commercialized the first spring reverberation units for guitar amplifiers. Later on, professional audio equipment manufacturers such as Fisher, Fairchild and Grampian released portable units.\nSince 1980, with the gradual transition to digital systems, VAM has appeared as a field of research attracting increasing attention [12, 13]. The aim of VAM is to emulate the sound characteristics and behaviour of analogue audio equipment using digital signal processing methods [14, 15, 16, 17]. Over the years three approaches have been developed: white-box, gray-box and black-box modelling, each offer different paths to this challenge, balancing accuracy, computational efficiency, and the need for detailed circuit knowledge. With the most recent advancements in data-driven approaches, neural audio effects have emerged as a field of investigation that can lead to effective results in modelling complex effects.\nAmong white-box methods, spring reverberation modelling has been approached as: parallel wave-guide structures [18], numerical simulation techniques as finite difference schemes [19, 20, 21] or non-physical modelling techniques [22, 23]. While these approaches have shown consistent results they demand large computational resources or may struggle to model the entire set of characteristics and non-linearities of a spring reverb, to address this barrier a \"DSP-informed\", gray-box, has been explored in [24].\nWhereas white-box and grey-box techniques have already been addressed for spring reverb modelling, a comparison of different black-box methods hasn't been done yet. In previous work [25], we started investigating neural network-based modelling strategies for emulating spring reverb effects. This preliminary study included a comprehensive review of relevant architectures reported in the literature, as well as an analysis of their hyperparameter configurations, loss functions and optimisation algorithms. In this study, we extended our previous work with an analysis of two publicly available datasets at different sampling rates to systematically compare five different neural network models, including convolutional and recurrent structures. We evaluate their effectiveness in capturing the unique acoustic properties of spring reverberation"}, {"title": "2. BACKGROUND", "content": "Figure 1 shows the main components of a spring reverb, which consists of an input transducer, or reverb driver, that converts the incoming audio signal into mechanical vibrations by converting an electric signal in a magnetic field. These vibrations travel through one or more springs, where the propagation modes (longitudinal, transverse and torsional) cause the waves to appear delayed at the other end. These modified vibrations are then converted back into an electrical audio signal by an output transducer, or pickup. The number of springs, their diameter, length, stiffness, load and voltage are all variables that influence the salient perceptual qualities of the audio effect device [18].\nUsually the spring reverb module is interconnected within a line-out connection referenced to +4dBu or -10dBV. However the driver is not amplifying the input signal, so it is unary gain and no gain factor is applied. Its main role is adapting the impedance between a typical line-out connection, which has an output impedance from 100 to 600 \u03a9, and the input impedance of the tank, which might have between 8-100 \u03a9. For impedance adaptation the driver incorporates an input transformer that reduces the impedance that the tank is seeing. So the driver should be designed to provide an output impedance very low (optimally accomplishing $R_s = R_L / 10$) for avoiding the signal loss effect in the transmission which is determined by a voltage divider. For instance, if no adaptation is provided and we assume 1V as input signal and the lowest impedance values for Rs=100 \u03a9 and RL=8 \u03a9, the signal loss would correspond to -22.6dB, where us for Rs=100\u03a9 and RL=100 \u03a9 the loss is minimized to 6dB. However if the impedance adaptation is applied and RS is defined by RL/10 (in case RL is equal to 8 \u03a9, Rs = 0.8 \u03a9) the signal loss is optimized to 0.82 dB.\nOther EQ parameters such as bass and treble controls can be introduced in the preamp, but they don't need to be considered as part of the reverberation time. Usually the input is at line level signal and the driver helps to control and keep it. Nevertheless, impedance adaptation is needed to avoid insertion loss, since the spring tank presents a low input impedance and line outputs are from 100 to 600 \u03a9. Given the signal degradation introduced by the spring tank, the output results in a low level signal, usually about 1 to 5 mV, so the preamp applies a gain factor about 50 to 60 dB to recover a processed line level signal. The first spring reverb units are vacuum tube preamp based, it may find transistor or op-amp based units though."}, {"title": "2.1. Spring Reverb Datasets", "content": "Specific to spring reverb, there are only two publicly available datasets. The first one, provided by Martinez et al. [24], is derived from the IDMT-SMT-Audio-Effects dataset [26], which consists of individual 2-second dry notes that cover the common range of"}, {"title": "3. DNN MODELS", "content": "From a review of the literature on neural audio effects emerges that convolutional neural network (CNN) models can be very effective for similar tasks [29, 30, 31] and at the same time there is evidence on the effectiveness of 'recurrent' type networks (RNN) [32]. We therefore propose a comparison of five architectural patterns that revolve around these two principles of operation and,"}, {"title": "3.1. Temporal Convolutional Network", "content": "The Temporal Convolutional Network (TCN) presented by [29] is an architecture specifically designed for processing sequential data like audio signals. As shown in Figure 3, it consists of stacked convolutional blocks with exponentially increasing dilation factors. This allows to capture both local and long-range temporal dependencies within the input without a commensurate rise in computational complexity or the number of parameters. The first block receives the input sequence and passes it through a TCN block, transforming it into a specified number of channels. The intermediate blocks have an identical structure and simply transform the input without changing the number of channels. Finally, the last block maps the transformed features to the desired output channels. Each block employs 1D causal convolution, ensuring outputs depends only on past and current inputs.\nThe Feature-wise Linear Modulation (FiLM) layer introduces a mechanism for adaptive neural network behaviour through the modulation of intermediate layer outputs, conditional on external or learned information. The FiLM layer adjusts the output of convolutional layers by applying an affine transformation, whose parameters are generated dynamically based on a separate conditioning input. This allows the network to adapt its processing in a context-dependent manner, tailoring its behaviour to specific characteristics of the input signal or desired effects. Optionally, batch normalization can be integrated within the FiLM layer to introduce additional control over the feature distribution.\nFollowing this, the architecture uses the PRELU (Parametric Rectified Linear Unit) as its activation function [34]. Unlike the standard ReLU, which nullifies negative values, the PReLU allows a small gradient when the unit is inactive (i.e., for negative values). This small gradient, determined by a learned parameter, ensures that even \"inactive\" units can adapt during the training process, reducing the risk of neuron \"death\" seen in some deep network trainings with standard ReLUs. Additionally, there's a skip connection (residual) added to the output of the convolution, which helps training for deeper networks."}, {"title": "3.2. WaveNet", "content": "Figure 4 is the diagram of the second architecture implemented: a simplified, feed-forward variant of the WaveNet architecture as presented in [30]. It utilizes stacked dilated convolutional layers to"}, {"title": "3.3. Gated Convolutional Network", "content": "The third convolutional architecture is the Gated Convolutional Network (GCN), shown in Figure 5 and introduced by Comunit\u00e0 et al. [31]. Similar to the other convolutional networks, this architecture can be briefly described as a TCN that leverages the power of dilated convolutions combined with gating mechanisms to capture long-range dependencies without significantly increasing computational costs. Dilation increases exponentially with the layer depth, after reaching the maximum dilation at the final layer, it resets and starts again, marking the beginning of a new block. Each block features a single causal convolutional layer (Conv1dCausal) followed by a FiLM conditioning layer and a gated activation function. The gated mechanism within the GCN modulates the output of the convolutional layers, each one has two convolutional operations. The first operation has its output channels doubled in size, and this output is split into two equal parts. One part undergoes a Tan h activation, while the other undergoes a Sigmoid activation. The element-wise product of these two activations creates the gating mechanism, effectively controlling the flow of information through the network. To maintain consistent tensor sizes throughout the network and enable residual connections, zero-padding is added at the beginning of the tensor after the gated operation. A mixing convolution of kernel size 1 follows the gating mechanism to control the number of output channels. The layer's output is a sum of the original input (residual connection) and the result from the mixing convolution."}, {"title": "3.4. Long Short-Term Memory", "content": "Long Short-Term Memory (LSTM) networks are a type of RNNS introduced to address the problem of exploding and vanishing gradients via a gating mechanism. This mechanism allows for selective memory control, enabling the network to learn long-term dependencies in data. Figure 6 shows the LSTM used is inspired by the one used in [32] with some variations. At the input stage,"}, {"title": "3.5. Gated Recurrent Units", "content": "Similar to LSTMs, Gated Recurrent Units (GRUs) are RNNs, they have a simpler architecture with only two gates: the update gate and the reset gate. The first one controls how much of the previous cell state is combined with the current input, and the second controls how much of the previous cell state is discarded. GRUs are generally less computationally expensive than LSTMs, but they may not be as effective at learning long-term dependencies. Figure 6 shows the architecture implemented for this work, the first two stages at the input are the same as the LSTM, but they are followed by a max pooling layer before the GRU. The model concludes with a convolutional output layer and a Tanh activation function."}, {"title": "4. EXPERIMENTS", "content": "In order to ensure clarity of results and ease of replicability: for this purpose, a code base is shared to reproduce all experiments with a Command Line Interface (CLI) and the relative documentation 2."}, {"title": "4.1. Experimental Design", "content": "The training process is implemented using PyTorch version 2.0.1 3, which encompasses both the training and validation loops across all models. For both datasets, 60% of the samples are used for training, 20% for validation and the remaining 20% are reserved for evaluation, with random splits. A combination of time and frequency-domain loss functions, as detailed in [35], [36] and in [37], is widely employed for modelling audio effects. Specifically, Mean Absolute Error (MAE) [38], Error-to-Signal-Ratio (ESR) [39] and Short-Time Fourier Transform (STFT) [40], either alone or in combination, are commonly used as loss functions in similar tasks. The early experiments conducted in this work, aimed at identifying the most effective loss function, revealed that a combined loss of Smooth L1 [41] and Multi-Resolution STFT (MS) was the optimal solution. The overall loss is given by:\n$L = L_{Smooth L1} + L_{STFT}$  (1)\nThe rationale behind the use of both a time and a frequency domain loss is to ensure that the model not only captures the overall structure and envelope of the audio waveform, but also respects the harmonic complexities in the spectral domain. Loss in the time domain, represented by the smoothed MAE, is more sensitive to phase discrepancies and temporal structures, while loss in the frequency domain, represented by STFT, focuses on ensuring that the spectral characteristics of the processed audio closely match those of the original [33].\nAn initial learning rate of 0.01 is set in combination with the use of the Adam optimizer and Reduce LR On Plateau as scheduler with patience of 10 epochs. The chosen scheduler reduces the learning rate by one order of magnitude, after 10 epochs in which the loss function after validation hasn't improved. For data at 16 kHz, a batch size of 64 is optimal, while with the 48 kHz this has to be reduced to 16, This difference also responds to the limited computing resources available. A single Nvidia RTX A5000 with 24 GB of RAM and CUDA version 12.0 is used for the whole experimental process.\nFor the evaluation of model performance, we use the ESR and Multi-Resolution STFT (MRSTFT), to measure the difference in magnitude between the predicted and the target [30, 40]. As shown in (2), the ESR is computed by dividing the absolute difference between the true value and the predicted value by the magnitude of the true value. The resulting ratio indicates how much the error deviates from the true value, relative to the true value itself.\n$L_{ESR} = \\frac{\\sum_{i=0}^{N-1} |Y_i - \\hat{Y_i}|^2}{\\sum_{i=0}^{N-1} |Y_i|^2 }$ (2)\nwhere y is the magnitude of the true value (also called target) and \u0177 is the prediction given by the neural network. The denominator in the ESR normalises the loss with regard to the energy of the target signal, preventing it from being dominated by the segments with higher energy.\nEquation 3, shows the MRSTFT introduced by [40] is an extension of the STFT loss that aims to improve robustness and limits potential biases.\n$L_{MRSTFT}(\\hat{y}, y) = \\sum_{m=1}^{M} (l_{sc}(\\hat{y}, y) + \\alpha l_{ls} (\\hat{y}, y))$ (3)\nWhere M is the total number of resolutions and \u03b1 is a weighting factor for the log magnitude loss. |y and \u0177 are, respectively, the magnitude of the ground truth and of the prediction."}, {"title": "4.2. Baseline Models", "content": "Quantitative metrics don't always fit within predefined bounds: when there's no previous work on the same data or for a similar task, it is difficult to establish without a range of possible values. Consequently, we define and test two baseline models, built to ascertain potential metric ranges, thus aiding the subsequent evaluations of the model under comparison.\n1. Naive Baseline (NB): This model emulates a prediction that corresponds to the wet sample and a target replaced by the dry sample. The ESR is computed between the dry and wet signals of the dataset, which is equivalent to a system that given an input returns the same input. The underlying logic of this baseline is to provide a quantitative measure of the inherent variance of the target device that we are modelling.\n2. Dummy Regressor (DR): It represents the upper boundary, sometimes referred to as the \"topline\". It makes a random prediction that is uncorrelated with the input and the target. Metrics are computed between a target and the random tensor (equivalent to white noise) that represents this baseline."}, {"title": "4.3. Results and Discussion", "content": "Results for both datasets are shown in Table 2, while all models outperform the baseline approaches, a clear distinction emerges between the two datasets.\nWaveNet achieves the lowest ESR at SpringSet, highlighting its exceptional capability in capturing the intricacies of spring reverb at 16 kHz sampling rate. Remarkably, the GCN model demonstrates competitive performance with SpringSet, and outperforms in terms of MRSTFT error also offering a potentially faster processing speed due to its simpler architecture compared to WaveNet. At the EGFxSet dataset, the GCN depicts the lowest ESR and MRSTFT, indicating its suitability for high-fidelity audio applications with a real-time processing constraint since it also exhibits the lowest RTF across both sampling rates, making it a strong candidate for real-time audio processing tasks. WaveNet while delivering exceptional performance at SpringSet has the highest RTF, indicating a potential need for optimization for real-time applications at higher sampling rates.\nIt is worth noticing that using the inference on CPU, it is possible to measure the time required by the models during testing to process the data and compute the RTF, although this approach does not guarantee an assessment of the deployment in a C++ framework like JUCE, it is a rough estimation of the computational demands posed by the model and allows a better tuning to improve its performance and reduce latency for a real time implementation."}, {"title": "5. CONCLUSIONS", "content": "By examining a range of convolutional and recurrent neural models, this study highlights the intricate balance between computational efficiency and the accurate reproduction of acoustic phenomena characterizing spring reverberation. Among the tested architectures, the WaveNet model demonstrated superior performance in terms of the ESR, particularly at a sampling rate of 16 kHz, underscoring the effectiveness of dilated convolutions in capturing the temporal dependencies and nuances of reverberated audio signals. Meanwhile, the GCN showed promising results at a higher 48 kHz sampling rate, and surpasses all other models in terms of MRSTFT indicating its potential for high-fidelity audio effect modelling with real-time processing capabilities.\nThe metrics utilized in this study provide a quantitative assessment of model performance. However, audio perception is inherently subjective, and these metrics might not always correlate with human perception. Future studies can involve more extensive perceptual evaluations, where human listeners are involved to rate the quality of the generated audio effects. Such assessments can offer invaluable insights that purely quantitative metrics might miss.\nFuture work, to develop more accurate neural audio effects, may involve collection of data specifically conceived for parameters learning and further experimentation with TFiLM [43]."}]}