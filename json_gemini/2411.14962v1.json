{"title": "LLM for Barcodes: Generating Diverse Synthetic Data for Identity Documents", "authors": ["Hitesh Laxmichand Patel", "Amit Agarwal", "Bhargava Kumar", "Karan Gupta", "Priyaranjan Pattnayak"], "abstract": "Accurate barcode detection and decoding in Identity documents is crucial for applications like security, healthcare, and education, where reliable data extraction and verification are essential. However, building robust detection models is challenging due to the lack of diverse, realistic datasets an issue often tied to privacy concerns and the wide variety of document formats. Traditional tools like Faker rely on predefined templates, making them less effective for capturing the complexity of real-world identity documents. In this paper, we introduce a new approach to synthetic data generation that uses LLMs to create contextually rich and realistic data without relying on predefined field. Using the vast knowledge LLMs have about different documents and content, our method creates data that reflects the variety found in real identity documents. This data is then encoded into barcode and overlayed on templates for documents such as Driver's licenses, Insurance cards, Student IDs. Our approach simplifies the process of dataset creation, eliminating the need for extensive domain knowledge or predefined fields. Compared to traditional methods like Faker, data generated by LLM demonstrates greater diversity and contextual relevance, leading to improved performance in barcode detection models. This scalable, privacy-first solution is a big step forward in advancing machine learning for automated document processing and identity verification.", "sections": [{"title": "Introduction", "content": "A barcode is a machine readable representation of data in the form of parallel lines or patterns. It is used to store information about the object to which it's attached and play a crucial role in how we manage and process information to-day. Barcodes make it possible to store, retrieve, and verify data quickly and accurately, which is essential in many in-dustries. There are several types of barcodes, with the two primary categories being one-dimensional (1D) and two-dimensional (2D) barcodes. Barcodes play a crucial role in identity documents like insurance cards, Driver's license, and ID cards. They store essential personal information in a machine readable format, facilitating quick and accurate verification. These documents are issued by governments or authorized organizations to confirm someone's identity. Barcodes in these documents help improve security, make verification faster, and allow different systems to work to-gether more efficiently. However, building reliable models to detect barcodes in identity documents is a tough problem.\nOne major challenge is the lack of large and diverse datasets needed to train these models. Privacy laws, like the General Data Protection Regulation (GDPR), restrict access to real-world identity documents, and because these documents often contain sensitive personal information, there are additional restrictions. Furthermore, identity documents vary a lot in design as they can have different data encoded, lay-outs, languages, and barcode types depending on the country or organization that issued them. This makes it hard for models trained on limited data to work well across all types of documents.\nSome existing tools attempt to generate synthetic data to bridge this gap, but they come with limitations. For instance, tools like Faker rely on predefined fields, which don't adapt well to the variability and complexity of real-world identity documents. This approach often results in data that lacks di-versity and fails to accurately represent actual documents. For example, driver's licenses from states like New Jersey and Texas encode different types of information, and the structure may vary not only between documents but also within a single document. As a result, models trained on such synthetic data may struggle to perform effectively. To solve these problems, we propose a new way to create syn-thetic datasets using LLMs. These models can generate real-istic and diverse data for identity documents without relying on predefined fields. The data reflects the variety of global identity documents, including differences in culture and re-gion, while also ensuring that privacy is protected. This data is then converted into barcodes and overlayed onto document templates, creating a dataset that looks and feels real but doesn't include any sensitive personal information.\nOur Key Contributions:\n\u2022 We use LLMs to generate data that captures the diversity and complexity of identity documents worldwide. This approach avoids rigid templates and reduces the time and effort needed to create datasets.\n\u2022 Models trained on our synthetic data perform better on a variety of document formats and barcode types compared to those trained on traditional synthetic datasets.\n\u2022 Our method can easily adapt to new types of documents,"}, {"title": "Related Work", "content": "The development of accurate barcode detection and decod-ing systems has been significantly enhanced through the use of synthetic data and advanced data generation techniques. This section reviews key studies that have utilized synthetic data for barcode detection and explores the application of LLMs in synthetic data generation across various domains."}, {"title": "Barcode Detection and Extraction", "content": "Barcode detection and decoding have seen significant ad-vancements with the integration of synthetic data and cutting-edge machine learning models. Models like YOLO (You Only Look Once) by (Redmon et al. 2016) and Faster R-CNN by (Ren et al. 2016) have been pivotal in advanc-ing object detection. YOLOv5, as demonstrated by (Chen et al. 2024), excelled in real-time detection across varied orientations and scales, while Faster R-CNN, utilized by (Chen et al. 2024), proved effective in Ultra high resolution images. To address the scarcity of annotated datasets, synthetic data generation has become a key solution. (Quenum, Wang, and Zakhor 2021) introduced a synthetic dataset of 100,000 barcode images, significantly improving model per-formance in high-resolution contexts. Similarly, (Zharkov, Vavilin, and Zagaynov 2020) developed a hybrid dataset of 30,000 synthetic and 921 real barcode images, providing a robust benchmark for algorithm testing. These advance-ments underscore how combining synthetic data with state-of-the-art models enhances barcode detection accuracy and robustness across diverse real-world scenarios."}, {"title": "LLM assisted Synthetic Data", "content": "Large Language Models are playing an increasingly impor-tant role in synthetic data generation, helping to address data scarcity and improve machine learning applications. For in-stance, (Long et al. 2024) provides a detailed overview of LLM-driven workflows for creating synthetic data, along with the challenges they face. (Patel, Raffel, and Callison-Burch 2024) introduces DataDreamer, a tool designed to ensure that synthetic data generation using LLMs is repro-ducible. Similarly, (Gupta et al. 2024) showcases TarGEN, which uses targeted prompts to enhance the quality of gen-erated datasets. On a larger scale, (Ge et al. 2024) presents Persona Hub, a framework for creating synthetic personas at scale. Lastly, (Veselovsky et al. 2023) explores methods for generating reliable synthetic data for social science research using grounding techniques. Together, these works demon-strate the power of LLMs in producing diverse, high-quality synthetic data for a wide range of purposes"}, {"title": "Method", "content": "This section outlines how data is generated for various types of identity documents, including Driver's Licenses, Insur-ance Cards, and University IDs. Each document type is de-signed to include specific information tailored to its purpose. For example, Driver's Licenses typically feature details like the holder's name, address, license number, date of birth, and issuing state, though the exact fields can vary between states. Insurance Cards, on the other hand, include informa-tion such as the policy number, coverage dates, provider, and plan type, which often differ based on the insurance com-pany. Similarly, University IDs contain data like the stu-dent's name, ID number, department, and enrollment year, with variations depending on the institution. These differ-ences highlight the need for flexible and realistic metadata generation to reflect the diversity of real-world documents.\nOur approach consists of three key components: data gener-ation using a LLM and the Faker library, barcode encoding with the pyBarcode library, and integrating these barcodes into document templates. To further enrich the dataset, we apply data augmentation techniques, ensuring greater diver-sity and realism in the generated documents."}, {"title": "Data Generation using LLM", "content": "We begin by crafting detailed prompts tailored to each docu-ment type-Driver's Licenses, Insurance Cards, and Univer-sity IDs. These prompts are designed to elicit comprehensive and contextually appropriate metadata from the LLMs. For example, a prompt for generating a document data might:\n\"Generate data for a Driver's License for the state Cal-ifornia, country USA. Based on the issuing authority, the metadata should include contextually relevant fields for-matted as plain text and separated by - for barcode encoding. Ensure the data is fictional but realistic and consistent with the standards for a driver's license in the state and country.\"\n\"Generate data for a Insurance Card issued by Blue Cross Blue Shield, country USA. Based on the issuing company, the metadata should include relevant fields for-matted as plain text and separated by - for barcode en-coding. Ensure the data is fictional but realistic and con-sistent with company and insurance card standards.\"\n\"Generate data for a University Student ID issued by Harvard University, country USA. Based on the issuing university, the metadata should include contextually rel-evant fields formatted as plain text and separated by - for barcode encoding. Ensure the data is fictional but re-alistic and consistent with university id standards in the country.\nTo ensure diversity, prompts encourage the inclusion of variations in cultural, regional, and linguistic elements. We also specify that all generated data should be entirely fic-tional and not correspond to any real individuals to maintain privacy compliance. Using the crafted prompts, we interact with the LLM to generate metadata entries. The LLM's un-derstanding of global contexts allows it to produce data that reflects the diversity found in real-world documents. For in-stance: To simulate realistic and diverse metadata for iden-tity documents, we utilized the Llama 70B(Touvron et al. 2023). The LLM was prompted to generate synthetic per-sonal information that adheres to standard formats used in various countries and states, ensuring both variability and authenticity in the generated data. For instance:\n\u2022 Driver's Licenses: The LLM generates names, ad-dresses, dates of birth, license numbers, issuing states or provinces, and other relevant details, varying across dif-ferent regions and languages\n\u2022 Insurance Cards: It produces policy numbers, coverage dates, provider names, plan types, and member informa-tion, reflecting different insurance providers and policies.\n\u2022 University IDs: The LLM generates student names, ID numbers, departments, enrollment years, and university names, capturing a wide range of academic institutions and programs.\nWe automate this process to generate large volumes of meta-data, ensuring a broad and diverse dataset."}, {"title": "Data Generation using Faker", "content": "While the Faker library offers tools to generate fake data, it does not natively support specific formats such as the AAMVA(American Association of Motor Vehicle Adminis-trators) standard for driver's licenses or the varied layouts of insurance cards and college IDs across regions. To overcome this limitation, we developed custom templates tailored to the structural and formatting details of these documents.\nThese templates were designed for driver's licenses, in-surance cards, and college IDs, incorporating regional and institutional variations. By extending the Faker library, we filled these templates with synthetic data, ensuring they ad-hered to the required formats. This approach allowed us to generate metadata for a wide range of identity documents."}, {"title": "Barcode Creation", "content": "Using the data from both the LLM and Faker-generated datasets, we encoded the data into barcodes appropriate for each document type using the pyBarcode library. This ensured consistency in the barcode generation process across both datasets.\nDriver's Licenses (DL): We generated PDF417 barcodes, commonly used for high-density data encoding on iden-tification cards, adhering to the AAMVA standard. Insur-ance Cards: Code 128 barcodes were employed to encode alphanumeric data efficiently, matching the formats used by various insurance providers. University IDs and Others: PDF417 is generally used for University ID. By utilizing py-Barcode, we ensured that the barcodes conformed to indus-try standards, including appropriate error correction levels and encoding parameters."}, {"title": "Embedding Barcodes into Document Templates", "content": "With the generated barcodes, we integrated them into cus-tomized document templates. Each template featured pre-defined placeholders and coordinates for precise barcode placement, while textual fields remained blank. The process for each dataset involved:\n\u2022 Template Selection: Choosing the appropriate document template based on the document type, region, and insti-tution (e.g., a driver's license template for a specific state or a university ID template for a particular institution).\n\u2022 Barcode Overlay: Placing the generated barcode onto the template at the predefined coordinates corresponding to the barcode placeholder, ensuring correct alignment and sizing.\nBy overlaying the barcodes onto the templates using known coordinates, we created images of documents where the barcodes are accurately positioned, while the textual fields remained as placeholders or blanks. This approach fo-cuses on the barcode data, which is essential for our train-ing objectives, and simplifies the generation process by not rendering textual data on the templates. This meticulous in-tegration is crucial for mimicking the spatial relationships found in real-world documents, thereby enhancing the train-ing data's authenticity."}, {"title": "Data Augmentation", "content": "To simulate real-world variations and enhance the robust-ness of the trained models, we applied various data aug-mentation techniques to the synthesized images from both datasets. These augmentations mimic common distortions encountered in practical scenarios, such as scanning arti-facts, lighting variations, and physical document wear. The augmentation techniques we used are:\n\u2022 Noise Injection: Adding Gaussian noise to simulate sen-sor noise inherent in imaging devices.\n\u2022 Blurring: Applying Gaussian and motion blur to emu-late out-of-focus images or movement during capture.\n\u2022 Color and Contrast Adjustments: Modifying bright-ness, contrast, saturation, and hue to reflect different lighting conditions.\n\u2022 Geometric Transformations: Introducing rotations, scaling, and perspective distortions to mimic variations in document positioning during scanning or photography.\nThese augmentations were applied probabilistically to each document, promoting the development of models re-silient to a wide range of real-world distortions."}, {"title": "Experimental Setup", "content": "To evaluate the effectiveness of our synthetic datasets for training robust models in barcode detection we conducted experiments using YOLOv5 (Ultralytics 2021), a state-of-the-art object detection model known for its real-time ap-plication capabilities. The goal was to compare the impact of two different data generation methods, one leveraging a LLM-based approach and the other using the Faker library"}, {"title": "Data Diversity", "content": "In this section, we evaluate the diversity of our synthetic dataset generated using the LLM and the Faker library by employing two key metrics: Unique value counts and Shannon Entropy across multiple data fields.\nUnique Value Counts: We calculate the number of unique entries in critical data fields:\n\u2022 Names and Addresses: Full names of individuals and complete addresses including street, city, state, and zip code\n\u2022 Policy Numbers,Driver License, University IDs: Insur-ance policy identifiers.State-specific driver license iden-tifiers and Unique identifiers for university students\nA higher count of unique values in these fields indicates greater variability and reduces the likelihood of model over-fitting to specific data patterns.\nShannon Entropy: Shannon entropy measures the ran-domness and unpredictability within categorical fields. It is calculated using the formula:\n$H(X) = \u2212 \u2211 P(x) log2 P(xi)$\nwhere P(xi) is the probability of occurrence of the i-th unique value. Higher entropy values reflect greater diversity and complexity in the data, which is beneficial for training robust models."}, {"title": "Conclusion and Limitations", "content": "This study introduces a synthetic data generation pipeline using LLMs to create diverse and realistic data for iden-tity documents. By combining LLM-generated content with barcode encoding and document synthesis, it addresses data scarcity and variability, enhancing barcode detection models and ensuring privacy compliance across various document formats.\nHowever, the study has limitations, as it was tested on a small set of real images, limiting its real-world evalu-ation. Additionally, the scalability and cost efficiency of smaller LLMs remain unexplored. Despite these challenges, this work significantly advances automated document pro-cessing and identity verification in privacy-sensitive appli-cations."}]}