{"title": "LEARNING GLOBAL AND LOCAL FEATURES OF POWER LOAD SERIES THROUGH TRANSFORMER AND 2D-CNN: AN IMAGE-BASED MULTI-STEP FORECASTING APPROACH INCORPORATING PHASE SPACE RECONSTRUCTION", "authors": ["Zihan Tang", "Tianyao Ji", "Wenhu Tang"], "abstract": "As modern power systems continue to evolve, accurate power load forecasting remains a critical issue. The phase space reconstruction method can effectively retain the chaotic characteristics of power load from a system dynamics perspective and thus is a promising knowledge-based preprocessing method for power load forecasting. However, limited by its fundamental theory, there is still a gap in implementing a multi-step forecasting scheme in current studies. To bridge this gap, this study proposes a novel multi-step forecasting approach by integrating the PSR with neural networks. Firstly, the useful features in the phase trajectory obtained from the preprocessing of PSR are discussed in detail. Through mathematical derivation, the equivalent characterization of the PSR and another time series preprocessing method, patch segmentation, is demonstrated for the first time. Based on this prior knowledge, an image-based modeling perspective with the global and local feature extraction strategy is introduced. Subsequently, a novel deep learning model, namely PSR-GALIEN, is designed for end-to-end processing, in which the Transformer Encoder and 2D-convolutional neural networks are employed for the extraction of the global and local patterns in the image, and a multi-layer perception based predictor is used for the efficient correlation modeling. Then, extensive experiments are conducted on five real-world benchmark datasets to verify the effectiveness as well as to have an insight into the detailed properties. The results show that, comparing it with six state-of-the-art deep learning models, the forecasting performance of PSR-GALIEN consistently surpasses these baselines, which achieves superior accuracy in both intra-day and day-ahead forecasting scenarios. At the same time, a visualization-based method is proposed to explain the attributions of the forecasting results.", "sections": [{"title": "1 Introduction", "content": "In modern power systems, the electricity consumption characteristics presented by end-users are constantly changing, as the increasing number of new loads and distributed renewable energy generation facilities has brought brand new challenges to the accurate power load forecasting, which is always the cornerstone of control and decision. The mathematical problem inherent in power load forecasting can be attributed to non-stationary time series forecasting [1], and the field of time series forecasting is a vibrant research area which has grown considerably within the last few decades. In recent years, deep learning models has made great strides for their powerful nonlinear feature extraction capabilities, however, the one-fits-all approach is still far away from the existing techniques [2]. In order to weaken the negative effects of non-stationarity and thus enhance the adaptability and robustness of the predictor, feature engineering for preprocessing power load series is a crucial part through out the whole forecasting piplines, relevant methods of time series preprocessing can be divided into two mainstreams, namely, the decomposition methods and the embedding methods [3].\nDecomposition methods in signal processing are widely used for manually extracting different levels of components in the original time series, under the framework of decomposition-prediction-aggregation, there are numerous studies focusing on combining different decomposition algorithms with machine learning and deep learning models, inves-\ntigating the predictive performance of these combinations [4, 5, 6, 7, 8, 9, 10]. Although such studies illustrate that these decomposition methods can effectively separate the different components which characterize the non-stationary characteristics of the power load series hence improving the predictability of the original series, and by mean of which good prediction accuracy can be achieved through the powerful learning-based predictors, they inevitably suffer from the following inherent problems, such as difficulties in implementation, high computational complexity, long inference time, heavy dependence on manual experience, etc. Moreover, these approaches fail to explore the source of the non-stationarity of the power load series.\nTime-delay embedding, a classical time series analysis method, based on which the phase space reconstruction (PSR) theory can effectively uncover the inner nonlinear characteristics inherent in the time series by reconstructing them into the high-dimensional phase space to approximate its original dynamic structure [11, 12, 13]. For power load series, the unique advantage of phase space reconstruction is that it provides a way to explore the inner chaotic features from the perspective of nonlinear dynamics through using the observed sequences itself to reconstruct the inner structure, therefore reducing the heavy dependence on exogenous variables, which has several benefits in practical applications. Firstly, redundant computational overheads and additional prediction errors can be avoided. The exogenous features such as regional meteorological features are high-dimensional with redundancy, and usually there is a issue of time granularity mismatch, which result in heavy burden in data processing. What's worse, the quality of meteorological data is also dependent on meteorological forecasts, and inaccurate forecasts in correlation modelling may introduce additional errors in power load forecasting. Secondly, PSR provides a modeling perspective in phase space instead of in the time domain under the autoregressive scheme, which is more beneficial in the subsequent feature extraction, especially for the other scenarios where these data are not available for practical reasons, or the cost of obtaining them is unaffordable [14, 15, 16].\nBased on these advantages, PSR is widely used in the forecasting scenarios of wind speed, renewable energy generation, as well as power load consumption in power systems[17, 18, 19, 20, 21], however, drawbacks still exist in the current research studies, an important fact, as argued in [18], is that limited by the fundamental forecasting theory in phase space, most of the existing studies integrating prediction models with PSR have only investigated their single-step forecasting performance, as for the multi-step forecasting methods, the rolling forecast strategy is widely used, which generates cumulative errors. For example, literature [16] adopts the conventional local forecasting method in the PSR theory to consider the spatial relationship between the neighboring phase points in the phase space, and proposes a single-step prediction model with a bi-square kernel regression approach for short-term power load forecasting. Literature [22] proposes an ultra-short-term forecasting model integrating the constrained Boltzmann machine predictor with PSR, namely PSR-DBN, and investigates its forecasting performance within the range of 12 steps ahead (1 hour) on a bus load dataset, literature [23] proposes a solution by using the structure of long short-term memory (LSTM) network with fully-connected networks (FCN) for multi-step forecasting, however, the LSTM network still belongs to the single-step iterative processing scheme with slow inference speed and cumulative error, literature [24] proposes a rolling data strategy to obtain a longer forecasting horizon, utilizing the ensemble learning with the LSTM networks for prediction, which is still the same idea.\nOverall, currently the vast majority of existing studies incorporating the PSR method fail to explore a general long-term multi-step forecasting methodology, in which an end-to-end correlation modeling methodology from the reconstructed phase trajectories to the time series to be predicted is believed to be a necessary part. This research gap hinders their performance in effectively addressing the scenarios where long range multi-step forecasts are needed, for example, in the real-world day-ahead load forecasting tasks, the forecasting horizon in advance is usually 48 steps or 96 steps, according to practical requirements for time granularity. This paper argues that there are mainly two reasons corresponding for this gap: (1) from the perspective of feature engineering, these studies fail to take full advantage of the data structure obtained from the preprocessing method of PSR and extract meaningful features which characterize certain properties of the phase trajectories and thus is beneficial to build correlation models from relative features to the power load series to be predicted. (2) from the perspective of algorithm design, these studies fail to delicately integrate the PSR preprocessing method with the specific functional deep learning modules to form an efficient and powerful end-to-end pipeline including the preprocessing, feature extraction, and forecasting stages.\nIn order to address the above issues, this paper focuses on the following aspects, i.e., what useful features can be extracted from the phase trajectories, what kind of neural networks can be used to extract these features, and most importantly, how to design a subtle blend of the PSR preprocessing method and the deep-learning models to form such an unified scheme. In summary, The main contributions in this paper are:\n1. An image-based modeling method for the reconstructed phase trajectory is proposed for the first time. This method employs a global and local feature extraction strategy, commonly used in computer vision (CV) tasks, to simultaneously capture corresponding patterns in the 2D image. These patterns represent the overall dynamic evolution characteristics of the trajectory and the temporal locality characteristics within its projection subseries, respectively.\n2. Based on this modeling method, a novel deep-learning model, namely the phase space reconstruction based global and local information enhanced neural network (PSR-GALIEN), is proposed, in which the Transformer Encoder and 2D-convolutional neural network (CNN) are implemented to extract the global and local features in the phase trajectory image respectively, and a well-designed multi-layer perception (MLP) is used as the predictor.\n3. Extensive experiments are conducted in five real-world power load datasets to have an insight to PSR- GALIEN's performance, and six representative state-of-the-art deep learning models emerged in recent years are deployed for detailed comparison, the results show that PSR-GALIEN substantially exceeds their performance.\n4. The heatmap and regression activation map (RAM) visualization techniques are introduced to explain the extracted global and local features during the inference process, which increases the interpretability of the PSR-GALIEN model.\nThe subsequent sections of this paper are organized as follows. In Section 2, the innovative image-based modeling idea as well as the related feature extraction method are introduced, and based on this framework, the design and implementation details of the proposed PSR-GALIEN model is described in section 3. In section 4, the experiments conducted in our research are introduced and the results are analysed. Lastly, in section 5 the conclusion and future works are summarized."}, {"title": "2 Proposed image-based forecasting approach", "content": "2.1 Preliminaries in Phase space reconstruction\n2.1.1 Principle\nIn the classical theory of PSR, the perspective of system dynamics is used to analyze the nonlinear properties of chaos in time series, which considers the chaotic time series as the projection of the attractors in high-dimensional space into one-dimensional space [25]. By means of time-delay embedding, the original time series can be reconstructed into phase trajectories in high-dimensional space to approximate its dominant attractor. The theory proposed by Takens [26], Whitney [27], et al., mathematically guarantees that this embedding is a diffeomorphic representation of the original nonlinear system with proper reconstruction parameters setup, i.e, the delay time \\( \\tau \\), and embedding dimension \\( m \\). Without loss of generality, given a set of observed time series \\([x_1, x_2,...,x_t]\\), the embedding dimension \\( m \\) and the time delay \\( \\tau \\), the embedding phase trajectories can be obtained from the trajectory matrix, as described in Eq. 1.\n\\[T = [X_1 \\; ... \\; X_i \\; ... \\; X_N] = \\begin{bmatrix}\nx_1 & x_i & x_N \\\\\nx_{1+\\tau} & x_{i+\\tau} & x_{N+\\tau} \\\\\n\\vdots & \\vdots & \\vdots \\\\\nx_{1+(m-1)\\tau} & x_{i+(m-1)\\tau} & x_{N+(m-1)\\tau}\n\\end{bmatrix} \\tag{1}\\]\nwhere \\( x_i \\in \\mathbb{R}^{m\\times 1} \\) is the i-th phase point in the trajectory \\([X_1, X_2, ..., X_N]\\).\nEach point on the phase trajectory can be viewed as a state in a certain nonlinear system, the time-delay embedding technique greatly enriches the semantic information of original time series which can be considered as the observation of the system, for this reason, capturing the dynamic features among these phase points in the whole trajectory allows for a better way to analyse the nonlinear roots inherent in the orginal time series.\nLike wind speed series, photovoltaic (PV) series, and other time series that are strongly coupled to the earth-atmosphere system, power load series are considered to have the chaotic properties to some extend, which is the source of their nonlinear characteristics [28, 29, 30]. The largest Lyapunov exponent (LLE) is a widely acknowledged indicator for examining and quantifying the chaotic characteristics of time series, if the LLE is positive, then the intrinsic dynamical system that dominates the time series has chaotic properties. In this paper, the Wolf algorithm [31] is used to calculate LLE of each dataset, which evaluates the degree of chaos of the sequence by calculating the separation speed of as the two neighboring orbits located in the phase space.\n2.1.2 Parameter selection\nThe proper selection of reconstruction parameters is crucial to uncover and approximate the nonlinear characteristics of the intrinsic system in time series. Parameter selection can be performed through multiple methods, which are categorized into two types: the individual estimation methods and the joint estimation methods. In our study, as two classical analytical methods for chaotic time series, the mutual information (MI) method [32] and the false nearest neighbor (FNN) [33] method are implemented to estimate the delay time and the embedding dimension respectively.\nThe mutual information method uses the mutual information function (MIF) between the original sequence \\([X_1, X_2, ..., X_t]\\) and its delayed sequence \\([X_{1+1},X_{2+1}, ..., X_{t+1}]\\) for delay time estimation, and its optimal delay time is the global minimal of the mutual information function, at which the original sequence has the lowest degree of nonlinear correlation with all of its own delayed sequences. Without loss of generality, given two random variables X and Y, their mutual information is:\n\\[I(X,Y) = \\sum_{x,y} p(x, y) \\log \\frac{p(x, y)}{p(x)p(y)} \\tag{2}\\]\nThe optimal estimation of the delay time is:\n\\[\\tau^* = \\arg \\min_\\tau \\sum_{x_t, x_{t+\\tau}} p(x_t, x_{t+\\tau}) \\log \\frac{P(x_t, x_{t+\\tau})}{p(x_t)p(x_{t+\\tau})} \\tag{3}\\]\nFor the determination of the embedding dimension, Takens et al. [26] mathematically gives the theoretical basis that the embedding dimension should satisfy \\(m>2d + 1\\),where \\( d \\) is the fractal dimension of the attractor. The idea of estimating the embedding dimension by applying the false neighbor point method is that when the phase trajectories are not sufficiently expanded, the points that are originally far apart in the high dimensional space may become the neighbors of each other when they are projected into the low dimensional space. As the embedding dimension keeps increasing, the gradual unfolding of the collapsed phase trajectory will cause the percentage of these false neighboring points to keep decreasing, and then, in this case, the original attractor structure can be fully characterized and described. Considering the reconstructed phase trajectories in the d-dimensional phase space, for arbitary phase point x\u2081(d), and its the geometrically nearest neighbor in Euclidean space is \\( x_j^{(d)} \\), given the threshold \\( \\epsilon \\), the false nearest neighboring relationship is satisfied when:\n\\[\\frac{||x_i(d+1) - x_j(d+1)||^2}{||x_i(d) - x_j(d)||^2} > \\epsilon, \\; i = 1, 2, 3, ... \\tag{4}\\]\nBy examining the mapping between the number of these points and the embedding dimension, an appropriate estimation can be made, for example, one feasible way of estimating the optimal embedding dimension is the case when the proportion of all the phase points possessing such a false nearest neighboring relationship is less than 5%.\n2.2 Innovative modeling idea\n2.2.1 Learning-based forecasting paradigm of PSR\nTraditional forecasting theory based on PSR mainly includes two mainstream methods: the global prediction methods and the local prediction methods, where global prediction methods use the evolutionary trajectories of all the phase points in the phase space to fit the nonlinear dynamical relations, and local prediction methods are concerned with the use of similar phase points in the historical evolutionary trajectories to guide for the future. Despite being supported by mathematical basis, these two methods can not effectively address the scenario of multi-step forecasting.\nDeep learning models are capable of extracting the evolutionary laws in nonlinear dynamical systems automatically, which makes it possible to integrate them with these conventional prediction methods and thus to extend their predictive performance through establishing the direct nonlinear mapping relationships. Without loss of generality, under the autoregressive forecasting scheme, the general prediction model based on this modeling approach can be expressed by the following equation:"}, {"title": "2.2.2 Data structure and feature analysis", "content": "In the learning-based forecasting model, having a clear perception of the features among the data structure of input is essential for the subsequent network design. From the trajectory matrix T, it is clear that the column of the matrix represent a phase point in the trajectory, while the row of the matrix represent the projection sequences in a certain dimension. It is demonstrated in this study that the process of obtaining these projection sequences can be regarded as another important method in time series feature engineering: the patch segmentation (PS) preprocessing method, which means that after being processed by both methods, the same data structure can be obtained, and more importantly, this also implies that a methodology can be adopted to fully exploit the corresponding features in this shared data structure under the two different perspective for the downstream correlation modelling, with a view to improving the final forecasting accuracy.\nThe feature engineering method of PS has been widely used in the tasks of image processing [34], text processing [35], speech signal processing [36], etc. In the time series forecasting tasks, the PS method brings an approach for patch-level correlation modeling through extracting the local patterns in the segmented subseries, and compared to the most widely used point-level correlation modeling approach, this modelling approach is more advantageous because patches contain richer semantic information from which a short-term local evolutionary pattern of a time series can be extracted and utilized.\nConsider the process of PS in terms of data structures, the same with PSR, two parameters are required for reshaping the original time series into the specified form, i.e., the length of the segmented patch sub-series p, and the length of the stride between each patches s. Once the segmentation parameters are determined, the data structure of the original time series \\([x_1, x_2, ..., x_t]\\) will be reorganized as described in the following equation:\n\\[\\mathbf{P} = \\begin{bmatrix}\n\\mathbf{p}_1^T \\\\\n\\vdots \\\\\n\\mathbf{p}_i^T \\\\\n\\vdots \\\\\n\\mathbf{p}_M^T\n\\end{bmatrix} = \\begin{bmatrix}\nx_1 & x_j & ... & x_p \\\\\n\\vdots & \\vdots & ... & \\vdots \\\\\nx_{1+(i-1)s} & x_{j+(i-1)s} & ... & x_{p+(i-1)s} \\\\\n\\vdots & \\vdots & ... & \\vdots \\\\\nx_{1+(M-1)s} & x_{j+(M-1)s} & ... & x_{p+(M-1)s}\n\\end{bmatrix} \\tag{6}\\]\nwhere \\(\\mathbf{p}_i^T \\in \\mathbb{R}^{1\\times p}\\) is the i-th segmented subseries from the time series. From the two matrices in Eq. 1 and Eq. 6, the mathematical connection between the two data preprocessing methods can be found:\n\\[P = T, \\text{ when } \\begin{cases} \\tau = s \\\\ t - (m - 1)\\tau = p \\end{cases} \\tag{7}\\]\nWithout loss of generality, considering the non-uniform time-delay embedding based phase space reconstruction method used in [37, 38]. i.e., the delay times are written in a more universal form:\n\\[T = [x_1,..., X_i,..., X_N] = \\begin{bmatrix}\nx_1 & x_i & ... & x_N \\\\\nx_{1+\\sum_{i=1}^0 \\tau_i} & x_{i+\\sum_{i=1}^0 \\tau_i} & ... & x_{N+\\sum_{i=1}^0 \\tau_i} \\\\\n\\vdots & \\vdots & ... & \\vdots \\\\\nx_{1+\\sum_{i=1}^{m-1} \\tau_i} & x_{i+\\sum_{i=1}^{m-1} \\tau_i} & ... & x_{N+\\sum_{i=1}^{m-1} \\tau_i}\n\\end{bmatrix} \\tag{8}\\]\nwhere \\(\\sum_{i=1}^{j-1} \\tau_i = \\sum_{1}^{j-1} \\tau_i\\), at this point, Eq. 7 is rewritten as:\n\\[P = T, \\text{ when } \\begin{cases} \\tau = s \\\\ t - \\sum_{j=1}^{m-1}\\tau_j = p \\end{cases} \\tag{9}\\]"}, {"title": "2.2.3 Image-based modeling perspective with global and local feature extraction strategy", "content": "where \\(\\mathbf{T} = [\\tau_1, \\tau_2, \\tau_3, \\dots, \\tau_{M-1}]^T\\), and the corresponding stride vector is \\(\\mathbf{s} = [s_1, s_2, ..., s_{M-1}]^T\\).\nFrom the above derivation, we can conclude that when the condition of Eq. 9 is satisfied, the matrices obtained by the two preprocessing methods are equivalent. However, despite sharing the same data structure, the meaning of the features inherent in it is quite different under the modeling perspectives of the two different approaches, as illustrated in Fig. 1.\nIt should be noted that the trade-offs need to be made to consider the way in which this shared data structure is obtained, as the parameters used in the process have practical significance. For example, if the data structure is obtained through PS, and the selected parameters of the segmented stride s and the number of segmented patches M differ too much from the optimal embedding delay time \\( \\tau \\) and the embedding dimension m calculated through Eq. 3 and 4, then the dynamic features of the equivalent reconstructed phase trajectories may be diminished, for this parameter selection implemented under the perspective of the PS method does not guarantee the effectiveness of optimal representation of the attractor under the perspective of the PSR method. Thus, decision needs to be given to the prominent characteristics of the time series, for power loads, we mainly consider how to use the method of PSR to extract the chaotic features in it, and at the same time, we can use the prior knowledge of the PS method to extract the temporal locality features from the project sequences in order to bring additional information gain for the forecasting, which we believe is a knowledge gap in the previous studies."}, {"title": "3 Proposed model architecture", "content": "In order to realize the above modeling methodology and make full utilization of deep learning models, the proposed architecture in this paper consists of a two-branch feature extraction network that uses the Transformer Encoder and 2D-CNNs to capture the global and local features respectively, the global and local features extracted from the two modules are combined and then feed into the MLP predictor, and the final prediction results are obtained. As the priori knowledge, the process of PSR is included in the preprocessing stage, which is well integrated with the latter networks to form the learning system. The proposed deep learning architecture is named as Phase Space Reconstruction based Global And Local Information Enhanced neural Network (PSR-GALIEN), as shown in Fig. 2. The design and implementation details of each component of the network are introduced in the following subsections.\n3.1 Transformer-based global feature extraction module\nThe Transformer model, which is centered on the self-attention mechanism, can efficiently establish the long-range dependencies among the input sequence, under the proposed image-based modeling perspective, considering each phase point in a phase trajectory as a image patch, the self-attention mechanism in Transformer Encoder can be used to establish a spatial correlation model between all the patches, and thus through which a global feature representing the overall evolution characteristics of the whole trajectory is obtained.\nIn the global feature extraction module of the PSR-GALIEN model, the image data obtained from the PSR first needs the embedding process including the value embedding (VE) and the positional embedding (PE) before entering into the Transformer Encoder. VE is implemented by using a fully-connected layer, which aims to learn the useful representation of the image patches, and PE is designed to bring additional position-informed feature after the VE, in order to solve the problem of position-independent nature of the self-attention mechanism. After the processing of the embedding layer, the patch sequence with the length of N and the embedding dimension of dmodel enters into the Transformer Encoder layer for the further processing, where the multi-head attention layer and the feed-forward layer in it are at the core of nonlinear correlation modeling of these image patches. Given input sequence \\(X \\in \\mathbb{R}^{d_{model}\\times N}\\), the calculation process of the self-attention mechanism can be expressed by the following equations:\n\\[Q_i = W_qX, K_i = W_kX, V_i = W_vX \\tag{10}\\]\n\\[A_i = \\text{Softmax}(\\frac{KQ_i}{\\sqrt{d_{model}}}) \\tag{11}\\]\n\\[X^{output} = \\text{SelfAtten} \\; (X) = XA_i \\tag{12}\\]\nwhere \\(Q_i \\in \\mathbb{R}^{d_{model}\\times N}, K_i\\in \\mathbb{R}^{d_{model}\\times N}, V_i\\in \\mathbb{R}^{d_{model} \\times N}\\) are the query vectors, key vectors and value vectors respectively. \\(A_i \\in \\mathbb{R}^{N\\times N}\\) is the attention matrix, which can be visualized see how much attention the model pays to each location within the sequence, and \\(X_i^{output} \\in \\mathbb{R}^{d_{model}\\times N}\\) is the output sequence, i denotes the i-th self-attention head.\nFor the multi-head attention mechanism, it use multiple heads to obtain different representations of the patterns in order to improve the modeling capability, which can be expressed as:\n\\[\\text{MultiAtten} \\; (X) = W \\cdot \\text{Concatenate}(X_1^{output}, ..., X_{nheads}^{output}) \\tag{13}\\]\nwhere Concatenate(.) represent the processing of matrix concatenation by column, and W is the projection matrix for feature fusion.\nIt should be noted that the only parameters that can be learned in the Multi-head attention layer are the matrices used for linear transformations as demonstrated in Eq. 10, and the correlation modeling relies on the parameter-free inner product operation, thus a fully-connected layer with nonlinear activation functions is needed for modeling the nonlinear relations within the sequence, and the subsequent process is:\n\\[\\begin{aligned}\nY &= \\text{LN} \\Big(\\text{LN} \\; (X + \\text{MultiAtten} \\;(X)) + \\\\ &\\qquad \\qquad \\text{FFN} \\;(\\text{LN} \\;(X + \\text{MultiAtten} \\; (X))\\Big) \\\\\n&= [y_1, y_2, ..., y_N]\n\\end{aligned} \\tag{14}\\]\nwhere \\(Y \\in \\mathbb{R}^{d_{model}\\times N}\\) represent the output sequence of the Transformer Encoder. \\( \\text{MultiAtten}(\\cdot), \\text{LN} \\; (\\cdot), \\text{FFN} \\; (\\cdot) \\) denote the process performed by the multi-head attention layer, the layer normalization module and the fully-connected layer respectively.\nThe global feature extraction module in PSR-GALIEN employs \\(C_{layers}\\) Enocoder layers in series for feature extraction, and the last image patch (the most recent phase point connected to the time series to be predicted) in the output sequence possesses all the information of the previous patches, thus it is considered as the global representation of the whole trajectory. The global feature \\(y_g \\in \\mathbb{R}^{d_{model} \\times 1}\\) is finally obtained through a linear transform, as expressed in:\n\\[y_g = W_g \\; y_N \\tag{15}\\]\nwhere \\(W_g \\in \\mathbb{R}^{d_{model} \\times d_{model}}\\) is the learnable matrix for obtaining the global feature.\n3.2 CNN-based local feature extraction module\nFor the design of the feature extraction network for local patterns of the phase trajectory image, considering the huge gap between the length and the width of this image (determined by Eq. 1, 3 and 4, usually N \u226b m.), the conventional modeling method of using CNNs with deep structure for a large receptive field (RF), such as ResNet-50, VGG-16, may not be so beneficial in this case for the following reasons:\n1. As illustrated in the previous subsections, the Transformer Encoders are already used to capture the global features in the image, and there is no need to use another deep CNN to account for the long-range correlation. In fact, the local feature extraction branch based on CNN is implemented to extract the detailed local patterns of the image (e.g., the local temporal patterns in each projection sequences of phase trajectory), the two networks are designed to undertake different feature extraction tasks as complementary.\n2. As the network continues to deepen, the unbalanced aspect ratio in the image makes it more difficult for the deeper layers to obtain effective semantic features, because most of the contents in their RF are the values obtained from padding, and these parts do not have any meaningful information, but significantly increase the computational overhead instead, and more importantly, a CNN with too complicated deep structure may hinder the efficient training of it.\nOverall, instead of using deep structures, a wide structure with multi-scale perception CNNs will make more sense in this case, like the the classical Inception network proposed in [40].\nThe local feature extraction network in the PSR-GALIEN model uses the modeling methodologies of a series of multi-scale perception CNNs for feature extraction. Firstly, two CNNs with the kernel size of 1*1 and 3*3 are implemented for the extraction of local patterns in the input image \\(\\Pi \\in \\mathbb{R}^{1\\times m\\times N}\\), and the feature maps with both dmodel channels obtained from them are added up for feature fusion. Then, the integrated feature map is fed into a nonlinear layer based on the ReLU function to enhance the nonlinear representation capability of the previous CNN layer, and after that the channel compression is performed by a CNN with the kernel size of 1*1 to obtain the local feature map with 1 channel. Finally, the local feature \\(y_l \\in \\mathbb{R}^{d_{model}\\times 1}\\) is obtained through the matrix flattening and linear projection. The whole processing of the local feature extraction module can be described by the following equations:\n\\[\\Pi^{'} = \\text{Conv2d} \\; (\\Pi, \\text{kernel\\_size} = 1, \\text{padding} = 0, \\text{stride} = 1) \\\\\n+ \\text{Conv2d} \\; (\\Pi, \\text{kernel\\_size} = 3, \\text{padding} = 1, \\text{stride} = 1) \\tag{16}\\]\n\\[\\Pi^{''} = \\text{ReLU}(\\Pi^{'}) \\tag{17}\\]\n\\[\\Pi^{'''} = \\text{Conv2d}(\\Pi^{''}, \\text{kernel\\_size} = 1, \\text{padding} = 0, \\text{stride} = 1) \\tag{18}\\]\n\\[y_l = W_l\\text{Flatten}(\\Pi^{'''})^T \\tag{19}\\]\nwhere \\(\\Pi^{'} \\in \\mathbb{R}^{d_{model}\\times m\\times N}, \\; \\Pi^{''} \\in \\mathbb{R}^{d_{model}\\times m\\times N}, \\; \\Pi^{'''} \\in \\mathbb{R}^{d_{model}\\times m\\times N}\\) stand for the feature map after feature fusion, the feature map after nonlinear activation and the obatained local feature map, respectively. Conv2d (.), Flatten (.) represent the operation of a 2D-CNN layer and matrix flattening, and \\(W_l \\in \\mathbb{R}^{d_{model} \\times (m\\times N)}\\) is the learnable matrix for obtaining the local feature.\n3.3 MLP-based prediction module\nTaking the MLP network as the predictor can efficiently establish the nonlinear correlation between the extracted features and the target sequences, which retains all the information in the forecasting stage without any loss. Firstly, the global and local features in the phase trajectory image extracted by the two feature extraction modules are combined through vector concatenation, and then a three-layer feed-forward neural network was used to perform the feature transformation, in which both layers are structured with the residual connection to ensure that both linear and nonlinear representation can be learned in that process. Finally, the enhanced feature vector are output as the vector representing the time-series to be predicted through a linear transform. The prediction process above is expressed as:\n\\[y_{proj}^1 = \\text{LN} \\; (y + W_1y) \\tag{20}\\]\n\\[y_{proj}^2 = \\text{LN} \\; (y_{proj}^1 + W_3\\text{RELU} \\; (W_2y_{proj}^1)) \\tag{21}\\]\n\\[y_{out} = V_4y_{proj}^2 + b \\tag{22}\\]\nwhere \\(y_{out} \\in \\mathbb{R}^{d_{pred}\\times 1}\\) is the vector of prediction results, \\(W_1 \\in \\mathbb{R}^{2d_{model}\\times 2d_{model}}, W_2 \\in \\mathbb{R}^{d_{ff} \\times 2d_{model}}, W_3 \\in \\mathbb{R}^{2d_{model} \\times d_{ff}}, W_4 \\in \\mathbb{R}^{d_{pred} \\times 2d_{model}}\\) represent the linear transformation matrices in each layer of the MLP predictor, and \\(b \\in \\mathbb{R}^{d_{pred}\\times 1}\\) is the bias in the last layer."}, {"title": "4 Experiments and analysis", "content": "4.1 Datasets\nIn order to verify the effectiveness of the proposed PSR-GALIEN model in practical power load forecasting scenar- ios, five real-world power load datasets with varying characteristics are selected as the benchmark datasets for the experiments, which are described as follows:\n\u2022 Elia-2022: the power load data from 2022 for the interconnected power grid of Elia, the Belgian power system operator.\n\u2022 City-1: the power load data from 2012 to 2014 for a distribution network in a city in southern China.\n\u2022 Area-1 & Area-2: the power load data from 2009 to 2015 for two regions in southern China.\n\u2022 Australia: the residential power load data from 2013 for 300 customers aggregated in a community located in New South Wales, Australia.\nThese datasets cover scenarios with different spatial granularities ranging from aggregated residential users to large interconnected systems grids, which are well capable of verifying the robustness of the predictors among different scenarios. The detailed information of the datasets are shown in the Tab. 1 and the temporal characteristics of each dataset are visualized in Fig. 3. As shown in the table, the LLE of each dataset is greater than 0, implying that the chaotic feature is a universal inner property among power loads despite of their varying temporal characteristics, and the estimated optimal reconstruction parameter of each dataset are presented.\n4.2 Experimental setup\n4.2.1 Baseline setup\nDeep learning models have made great progress in the field of time series forecasting in recent years, to keep up with the the latest trends as well as look back on the past milestones, six state-of-the-art and representative deep learning predictors are chosen as```json\nthe baselines in the study, including Time Series Transformer (TST) [41], Informer [42], PatchTST [43] in Transformer family, DLinear [44], Koopa [45] in MLP family, and TimesNet [46] in CNN family. Among them, the TST model establishes a solid bridge between natural language processing and time series forecasting tasks, the Informer model extends its capability to perform long sequence forecasting, DLinear breaks the claim for the first time that the Transformer-structured models are more advantageous, as for the Koopa model, it employs the koopman operator to model the non-stationary properties in the time series from the perspective of nonlinear dynamics, which is relevant to the modeling idea of PSR. PatchTST and TimesNet are the two patch-based state-of-the-art models emerging in recent years, which are most relavent to our PSR-GALIEN. Therefore, these representative models were chosen as for comparison in this study, meanwhile, the performance of these state-of-the-arts on power load forecasting tasks is tested.\n4.2.2 Hyperparameter setup\nWhen conducting experiments, the six baselines are compared with the default settings presented in their original studies, as for the hyperparameters which are shared in the same structure, such as dmodel, dff, Elayers, Nheads in the Transformer backbone, are kept same, which are: dmodel = 512, df = 4dmodel, Clayers = 2, nheads = 8 for Transformer, Informer, PatchTST and the PSR-GALIEN. As for the hyperparameters in the training stage, the setups are shown in Tab. 6.\n4.2.3 Evaluation metrics\nIn this paper, two main metrics in the time series forecasting task, the mean absolute error (MAE) and the mean absolute percentage error (MAPE) are adopted for evaluation, which are fomulated by:\n\\[MAE = \\frac{1}{m}\\sum_{i=1}^m |y_i^{pred} - y_i| \\tag{23}\\]\n\\[MAPE = \\frac{1}{m} \\sum_{i=1}^m |\\frac{y_i^{pred} - y_i}{y_i}| \\times 100\\% \\tag{24}\\]\nwhere yi, ypred are the i-th value of the groundtruth and the prediction results respectively, m is the prediction length, lower values of MAE and MAPE indicate higher prediction accuracy.\n4.2.4 Hardware and Software setup\nAll the subsequent experiments in this study are carried out on a single machine with the hardware configuration of a NVIDIA RTX4090 GPU (24GB), a AMD EPYC 9754 vCPU (128-Core) and a RAM(24GB). All programs are implemented in the PyTorch 2.1.0-Python 3.10 (ubuntu22.04)-CUDA 12.1 environment.\n4.3 Experiments and analysis\nThe experiments conducted in this paper consist of a total of four parts, firstly, the prediction performance of each model is compared on five datasets under the conditions of different forecast lengths. Secondly, the effects of varying input lengths on the prediction performance of each model are further explored on three long datasets, and then an ablation experiment is conducted to analyze the enhancement effects of the local feature extraction module in the PSR-GALIEN model. Finally, the impact of the various hyperparameters in PSR-GALIEN are discussed in detail.\n4.3.1 Comparison experiments with varying output lengths\nIn order to test the performance of each model in short-term power load forecasting scenarios, forecasting experiments with varying forecasting horizons of {3h, 6h, 12h, 24h} in advance are conducted on each dataset, and five experiments were conducted independently for each model with the parameter setup in Tab. 6, of which the best results are shown in Tab. 3, and the the prediction performance of each model with respect to the prediction length are shown in Fig. 4.\nAs shown in the figure, the prediction error of each model has a different degree of upward trend as the forecasting length increases. Among them, PSR-GALIEN has the best forecasting accuracy under all conditions, with the PatchTST to be the second best, comparing with PatctTST, PSR-GALIEN shows substantial performance improvements at all prediction lengths, with average improvements of 22.7%, 18.1%, 10.4%, and 8.6%, this shows that the PSR-GALIEN model is particularly capable for intraday ultra-short-term load forecasting scenarios. Meanwhile, considering power loads with different characteristics, the model is robust as well. Taking the MAPE metrics as the indicator, in the day-ahead load forecasting scenarios which are important to the dispatch operation of the power system, the prediction accuracy of the PSR-GALIEN model for the city (City-1) and the regional distribution grid (Area-1, Area-2) can reach the level of about 97%, and for the system-level load in the large-scale interconnected power system (Elia), the prediction accuracy can reach the level of about 96.5%, and for the aggregated residential loads (Australia) with strong volatility can also reach a level of about 92.8%. Overall, the PSR-GALIEN model achieves superior performance on these scenarios."}, {"title": "4.3.2 Comparison experiments with varying input lengths", "content": "The length of look-back window is an important parameter in models that adopt the autoregressive prediction strategy, and the increase of the window length brings more historical information, however, on the other hand, it also poses a challenge for the model to efficiently extract the useful features from the long input sequence. In general, a good deep learning model should be able to gain extra improvement in forecasting accuracy from longer inputs, however, the study in [44] argues that for the self-attention mechanism structure dominated models, they are inherently incapable of effectively utilizing long sequential data for prediction. Therefore, in order to validate these viewpoints and also to test the prediction performance of the PSR-GALIEN model for varying prediction lengths, in this part of the experiment, the output length of each model is fixed to P=96, and different input lengths of {96, 192, 336, 672} steps are used for testing, the results are shown in Tab. 4 and Fig. 5."}, {"title": "4.3.3 Ablation experiments", "content": "In order to further explore the advantage of the proposed image-based forecasting approach for the extra utilization of local feature in the phase trajectory data, the ablation experiments are conduct to examine the impact of the local feature extraction module on model performance, it is worth noting that when the local feature extraction module is removed, this variant can be considered as a sequence-based forecasting model. It focuses on the prediction only using the information in the individual phase points which fails to consider the use of local temporal features in the projected sequence of the phase trajectory. For this purpose, this variant is used to conduct comparative experiments with the vanilla PSR-GALIEN model on five datasets, and the results are shown in Tab. 5 and Fig. 6.\nFrom the results of the ablation experiments above, it can be seen that the local feature extraction module brings an average accuracy improvement of 5.9%, 6.6%, 12.1%, 6.0%, and 6.5% on each dataset under the MAE metric respectively. On the one hand, this verifies the viewpoint in the previous sections that the preprocessing method of phase space reconstruction is equivalent to patch segmentation in terms of data structure, and the local temporal features under the perspective of the latter method can be extracted for the benefit of forecasting accuracy improvement. On the other hand, the 2D-CNN based local feature extraction module adopted in the PSR-GALIEN is well capable for the extraction of the corresponding features.\nIn the last subsection, we will continue to shed light on the issue of the learned global and local features during the forecasting process, and the feature visualisation techniques will be utilised to illustrate which area in the phase trajectory image are used by the model during the process of global and local feature extraction."}, {"title": "4.3.4 Hyperparameters sensitivity experiments", "content": "Given that the PSR-GALIEN model comprises multiple hyperparameters both in the preprocessing stage and within the network architecture, this section delves into a sensitivity analysis to evaluate their impact on forecasting performance.\nFirst and foremost, the three most important hyperparameters in the network are dmodel, dff, and elayers, as already shown in Fig.2. The candidate set of these three hyperparameters is set as follows: dmodel={128, 256, 512}, dff = 4dmodel, and Clayers={2, 3, 4}, which forms nine hyperparameter combinations. According to each of the above combinations, the PSR- GALIEN model is set up with the related hyperparameters for day-ahead forecasting on the five datasets with input length L = 192, and the results are shown in Tab. 6 and Fig.7. From the experimental results, the hyperparameter combination 3 (dmodel=512, dff=2048, Clayers=2) has the best performance under the MAE evaluation metrics in all but Area-1 dataset, where its gap to the best one is only 1.5%, thus the hyperparameter setup of combination 3 can be considered as the optimal choice for the PSR-GALIEN model.\nAs the case with the preprocessing stage, considering that the choice of PSR parameters determines the extent to which the reconstructed phase trajectory approximates the inner attractor of the power load series, the Elia-2022, City-1 and Australia datasets are selected as the benchmark datasets in this part for their distinguished characteristics. The rest of the hyperparameters in the PSR-GALIEN model are set to the hyperparameter combination 3, with a input length of L=192 and an output length of P=96 (48), and the results of these experiments are shown in Tab. 7 and Fig. 8. The results illustrate that the forecasting performance of the model remains generally stable for other parameters in the neighbourhood, with a low degree of accuracy degradation. Further exploring the two reconstruction parameters, it is clear that the embedding dimension m has a greater impact on the prediction performance compared to the delay time \\( \\tau \\), this can be attributed to the fact that, as demonstrated in Eq. 1, the embedding dimension m has a greater effect on the length of the image obtained from PSR, especially in the datasets of Elia-2022, City-1, Area-1, Area-2, in which \\( \\tau \\gg m \\). In such case, the dynamic features obtained in the global feature extraction process may not be able to retain the original characteristics of the attractor, thus degradation of forecasting performance occurs."}, {"title": "4.4 Visualization analytics", "content": "4.4.1 Forecasting performance visualization\nThe day-ahead forecasting performance of the PSR-GALIEN model on the five benchmark datasets are visualized in Fig. 9, from the results, it is clear that the predictions results in the test datasets are quite close to the groundtruth values, which indicates that although the PSR-GALIEN model adopts the autoregressive forecasting scheme for the only usetilization of the historical power load series, its effective implementation of the PSR preprocessing method to uncover the chaotic characteristics of power loads in high-dimensional phase space, as well as the design of this global and local feature extraction strategy for feature extractionngineering, enable the model to be well capable in the day-ahead power load forecasting task among the power loads with varying temporal characteristics.\n4.4.2 Feature importance visualization\nThe focus of feature extraction in the PSR-GALIEN model is at the center of the discussion in this paper, in order to have an insight to these features, feature visualization techniques of heatmap and the regression activation map (RAM) [47] are used in this subsection to show the corresponding features in the trajectory image utilized by the PSR-GALIEN model during the global and local feature extraction process.\nRAM is a visualization technique that uses the gradient of back-propagation during inference process to reflect the extent to which specified regions in the feature maps of each CNN layer contribute to the final prediction results. In our study, the local patterns in the trajectory image noticed by the local feature extraction module can be obtained through the RAM in the last CNN layer of the local feature extraction module, which is shown in Fig.10. As we can see from the figure, the activated regions (the darker part of the image) in the RAM corresponds to the regions which have greater gradients, the darker the color of the activated regions in the RAM are, the more they contribute to the final prediction results. Comparing the RAM with the phase trajectory image, we can see that these activated regions are exactly the dark-colored regions of the original phase trace, which represent the the peak load periods in the original one-dimensional power load series. Thus, the conclusion can be made that the local patterns with high values in the projection subseries tend to be extracted by the local feature extraction module, and their contribution to the final forecasting result explains the phenomenon of the previous ablation experiments.\nThe heatmap is utilized to have a clear view of the distribution matrix learned in the self-attention process, in which shades of color can be used to represent the degree of dependence. The attention matrices of the two multi-head attention layers of the global feature extraction module are displayed in Fig. 11, from which we can see, as the number of Encoder layer increases, more complex dependencies within input sequence (the phase trajectory sequence) begin to emerge. Thanks to the enhanced nonlinear characterization capabilities brought about by the MLP layer in each Transformer Encoder, deeper multi-head layer can model the nonlinear relationships which cannot be utilized in shallow layers. In the global feature extraction module, the Transformer Encoder is incorporated to obtain an effective nonlinear representation of the phase trajectory sequence for prediction, and the global features obtained come from the last point in the processed sequence. Therefore, from the attention matrix in the last Encoder layer, the contribution of each phase point (image patch) in the trajectory to the obtained global feature can be obtained and visualized, as demonstrated in the figure. In the multi-head attention mechanism, each head independently learns a dependency pattern in the sequence, from the results of the attention distribution in the sequences learned by each head, it is clear that some positions in the sequence are attention-intensive, which means greater attention are paid to these subseries when generating the global representation of the whole phase trajectory.\nIn summary, through the two feature visualization techniques, the process to learn both the global and local feature in the image can be clearly addressed, which in turn explains what part of the image's the model actually uses as a reference when making predictions."}, {"title": "5 Conclusion", "content": "In this study, firstly the relationship of the two preprocessing methods of PSR and PS in time series feature engineering is discussed, from the perspective of data structure, the equivalent relationship between them is demonstrated, which bridges a knowledge gap for the first time. Then, an image-based modeling approach for PSR with global and local feature extraction strategy is proposed, for the full utilization of this prior knowledge to extract useful patterns to improve the forecasting performance. On the basis of this method, an end-to-end deep learning model namely PSR-GALIEN for general multi-step forecasting is proposed, in which the Transformer Encoder and the 2D-CNN are implemented for the efficient extraction of these relative features. After that, the extensive experiments on five real datasets show that the proposed PSR-GALIEN model comprehensively outperforms the selected state-of-the-art models, thus its excellent performance as well as strong robustness in different power load forecasting scenarios are verified, at the same time, the effectiveness of this image-based engineering approach is validated, which provides a brand new perspective for the modeling and forecasting of power load series. Lastly, the RAM and heatmap are employed to visualized the utilized patterns in the trajectory image during the global and local feature extraction process, through which, the root cause of PSR-GALIEN's final prediction results can be explained.\nIn the future, the multivariate modeling and forecasting approaches under the synergy of complex exogenous features will be further considered, in order to extend and generalize the proposed modeling approach as well as the PSR- GALIEN model in this paper."}]}