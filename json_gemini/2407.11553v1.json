{"title": "LEARNING GLOBAL AND LOCAL FEATURES OF POWER LOAD\nSERIES THROUGH TRANSFORMER AND 2D-CNN: AN\nIMAGE-BASED MULTI-STEP FORECASTING APPROACH\nINCORPORATING PHASE SPACE RECONSTRUCTION", "authors": ["Zihan Tang", "Tianyao Ji", "Wenhu Tang"], "abstract": "As modern power systems continue to evolve, accurate power load forecasting remains a critical issue.\nThe phase space reconstruction method can effectively retain the chaotic characteristics of power\nload from a system dynamics perspective and thus is a promising knowledge-based preprocessing\nmethod for power load forecasting. However, limited by its fundamental theory, there is still a gap\nin implementing a multi-step forecasting scheme in current studies. To bridge this gap, this study\nproposes a novel multi-step forecasting approach by integrating the PSR with neural networks. Firstly,\nthe useful features in the phase trajectory obtained from the preprocessing of PSR are discussed in\ndetail. Through mathematical derivation, the equivalent characterization of the PSR and another time\nseries preprocessing method, patch segmentation, is demonstrated for the first time. Based on this\nprior knowledge, an image-based modeling perspective with the global and local feature extraction\nstrategy is introduced. Subsequently, a novel deep learning model, namely PSR-GALIEN, is designed\nfor end-to-end processing, in which the Transformer Encoder and 2D-convolutional neural networks\nare employed for the extraction of the global and local patterns in the image, and a multi-layer\nperception based predictor is used for the efficient correlation modeling. Then, extensive experiments\nare conducted on five real-world benchmark datasets to verify the effectiveness as well as to have an\ninsight into the detailed properties. The results show that, comparing it with six state-of-the-art deep\nlearning models, the forecasting performance of PSR-GALIEN consistently surpasses these baselines,\nwhich achieves superior accuracy in both intra-day and day-ahead forecasting scenarios. At the same\ntime, a visualization-based method is proposed to explain the attributions of the forecasting results.", "sections": [{"title": "Introduction", "content": "In modern power systems, the electricity consumption characteristics presented by end-users are constantly changing,\nas the increasing number of new loads and distributed renewable energy generation facilities has brought brand new\nchallenges to the accurate power load forecasting, which is always the cornerstone of control and decision. The\nmathematical problem inherent in power load forecasting can be attributed to non-stationary time series forecasting\n[1], and the field of time series forecasting is a vibrant research area which has grown considerably within the last few\ndecades. In recent years, deep learning models has made great strides for their powerful nonlinear feature extraction\ncapabilities, however, the one-fits-all approach is still far away from the existing techniques [2]. In order to weaken the\nnegative effects of non-stationarity and thus enhance the adaptability and robustness of the predictor, feature engineering\nfor preprocessing power load series is a crucial part through out the whole forecasting piplines, relevant methods of"}, {"title": "2 Proposed image-based forecasting approach", "content": ""}, {"title": "2.1 Preliminaries in Phase space reconstruction", "content": ""}, {"title": "2.1.1 Principle", "content": "In the classical theory of PSR, the perspective of system dynamics is used to analyze the nonlinear properties of chaos\nin time series, which considers the chaotic time series as the projection of the attractors in high-dimensional space into\none-dimensional space [25]. By means of time-delay embedding, the original time series can be reconstructed into\nphase trajectories in high-dimensional space to approximate its dominant attractor. The theory proposed by Takens [26],\nWhitney [27], et al., mathematically guarantees that this embedding is a diffeomorphic representation of the original\nnonlinear system with proper reconstruction parameters setup, i.e, the delay time \\(\\tau\\), and embedding dimension m.\nWithout loss of generality, given a set of observed time series \\([x_1, x_2,...,x_t]\\), the embedding dimension m and the\ntime delay \\(\\tau\\), the embedding phase trajectories can be obtained from the trajectory matrix, as described in Eq. 1.\n\n\nwhere \\(x_i \\in \\mathbb{R}^{m\\times 1}\\) is the i-th phase point in the trajectory \\([X_1, X_2, ..., X_N]\\).\nEach point on the phase trajectory can be viewed as a state in a certain nonlinear system, the time-delay embedding\ntechnique greatly enriches the semantic information of original time series which can be considered as the observation\nof the system, for this reason, capturing the dynamic features among these phase points in the whole trajectory allows\nfor a better way to analyse the nonlinear roots inherent in the orginal time series.\nLike wind speed series, photovoltaic (PV) series, and other time series that are strongly coupled to the earth-atmosphere\nsystem, power load series are considered to have the chaotic properties to some extend, which is the source of their\nnonlinear characteristics [28, 29, 30]. The largest Lyapunov exponent (LLE) is a widely acknowledged indicator for"}, {"title": "2.1.2 Parameter selection", "content": "The proper selection of reconstruction parameters is crucial to uncover and approximate the nonlinear characteristics\nof the intrinsic system in time series. Parameter selection can be performed through multiple methods, which are\ncategorized into two types: the individual estimation methods and the joint estimation methods. In our study, as two\nclassical analytical methods for chaotic time series, the mutual information (MI) method [32] and the false nearest\nneighbor (FNN) [33] method are implemented to estimate the delay time and the embedding dimension respectively.\nThe mutual information method uses the mutual information function (MIF) between the original sequence\n\\([X_1, X_2, ..., X_t]\\) and its delayed sequence \\([X_{1+1},X_{2+1}, ..., X_{t+1}]\\) for delay time estimation, and its optimal delay time is\nthe global minimal of the mutual information function, at which the original sequence has the lowest degree of nonlinear\ncorrelation with all of its own delayed sequences. Without loss of generality, given two random variables X and Y,\ntheir mutual information is:\n\n\nThe optimal estimation of the delay time is:\n\n\nFor the determination of the embedding dimension, Takens et al. [26] mathematically gives the theoretical basis that\nthe embedding dimension should satisfy \\(m>2d + 1\\),where d is the fractal dimension of the attractor. The idea of\nestimating the embedding dimension by applying the false neighbor point method is that when the phase trajectories\nare not sufficiently expanded, the points that are originally far apart in the high dimensional space may become the\nneighbors of each other when they are projected into the low dimensional space. As the embedding dimension keeps\nincreasing, the gradual unfolding of the collapsed phase trajectory will cause the percentage of these false neighboring\npoints to keep decreasing, and then, in this case, the original attractor structure can be fully characterized and described.\nConsidering the reconstructed phase trajectories in the d-dimensional phase space, for arbitary phase point \\(x_i(d)\\), and\nits the geometrically nearest neighbor in Euclidean space is \\(x^n_i(d)\\), given the threshold \\(\\varepsilon\\), the false nearest neighboring\nrelationship is satisfied when:"}, {"title": "2.2 Innovative modeling idea", "content": ""}, {"title": "2.2.1 Learning-based forecasting paradigm of PSR", "content": "Traditional forecasting theory based on PSR mainly includes two mainstream methods: the global prediction methods\nand the local prediction methods, where global prediction methods use the evolutionary trajectories of all the phase\npoints in the phase space to fit the nonlinear dynamical relations, and local prediction methods are concerned with the\nuse of similar phase points in the historical evolutionary trajectories to guide for the future. Despite being supported by\nmathematical basis, these two methods can not effectively address the scenario of multi-step forecasting.\nDeep learning models are capable of extracting the evolutionary laws in nonlinear dynamical systems automatically,\nwhich makes it possible to integrate them with these conventional prediction methods and thus to extend their predictive\nperformance through establishing the direct nonlinear mapping relationships. Without loss of generality, under the\nautoregressive forecasting scheme, the general prediction model based on this modeling approach can be expressed by\nthe following equation:"}, {"title": "2.2.2 Data structure and feature analysis", "content": "In the learning-based forecasting model, having a clear perception of the features among the data structure of input\nis essential for the subsequent network design. From the trajectory matrix T, it is clear that the column of the matrix\nrepresent a phase point in the trajectory, while the row of the matrix represent the projection sequences in a certain\ndimension. It is demonstrated in this study that the process of obtaining these projection sequences can be regarded\nas another important method in time series feature engineering: the patch segmentation (PS) preprocessing method,\nwhich means that after being processed by both methods, the same data structure can be obtained, and more importantly,\nthis also implies that a methodology can be adopted to fully exploit the corresponding features in this shared data\nstructure under the two different perspective for the downstream correlation modelling, with a view to improving the\nfinal forecasting accuracy.\nThe feature engineering method of PS has been widely used in the tasks of image processing [34], text processing [35],\nspeech signal processing [36], etc. In the time series forecasting tasks, the PS method brings an approach for patch-level\ncorrelation modeling through extracting the local patterns in the segmented subseries, and compared to the most widely\nused point-level correlation modeling approach, this modelling approach is more advantageous because patches contain\nricher semantic information from which a short-term local evolutionary pattern of a time series can be extracted and\nutilized.\nConsider the process of PS in terms of data structures, the same with PSR, two parameters are required for reshaping\nthe original time series into the specified form, i.e., the length of the segmented patch sub-series p, and the length of\nthe stride between each patches s. Once the segmentation parameters are determined, the data structure of the original\ntime series \\([x_1, x_2, ..., x_t]\\) will be reorganized as described in the following equation:\n\n\nwhere \\(p^i_j \\in \\mathbb{R}^{1\\times p}\\) is the i-th segmented subseries from the time series. From the two matrices in Eq. 1 and Eq. 6, the\nmathematical connection between the two data preprocessing methods can be found:\n\n\nWithout loss of generality, considering the non-uniform time-delay embedding based phase space reconstruction method\nused in [37, 38]. i.e., the delay times are written in a more universal form:\n\n\nwhere \\(\\sum_{j=1}^{j-1} \\tau_i = \\sum^j_{i=1} T_i\\), at this point, Eq. 7 is rewritten as:"}, {"title": "2.2.3 Image-based modeling perspective with global and local feature extraction strategy", "content": "where \\(T = [T_1, T_2, T_3, \\dots, \\dots, T_{M-1}]^T\\), and the corresponding stride vector is \\(s = [s_1, s_2, ..., s_{M-1}]^T\\).\nFrom the above derivation, we can conclude that when the condition of Eq. 9 is satisfied, the matrices obtained by\nthe two preprocessing methods are equivalent. However, despite sharing the same data structure, the meaning of the\nfeatures inherent in it is quite different under the modeling perspectives of the two different approaches, as illustrated in\nFig. 1.\nIt should be noted that the trade-offs need to be made to consider the way in which this shared data structure is obtained,\nas the parameters used in the process have practical significance. For example, if the data structure is obtained through\nPS, and the selected parameters of the segmented stride s and the number of segmented patches M differ too much\nfrom the optimal embedding delay time \\(\\tau\\) and the embedding dimension m calculated through Eq. 3 and 4, then the\ndynamic features of the equivalent reconstructed phase trajectories may be diminished, for this parameter selection\nimplemented under the perspective of the PS method does not guarantee the effectiveness of optimal representation of\nthe attractor under the perspective of the PSR method. Thus, decision needs to be given to the prominent characteristics\nof the time series, for power loads, we mainly consider how to use the method of PSR to extract the chaotic features in\nit, and at the same time, we can use the prior knowledge of the PS method to extract the temporal locality features from\nthe project sequences in order to bring additional information gain for the forecasting, which we believe is a knowledge\ngap in the previous studies."}, {"title": "3 Proposed model architecture", "content": "In order to realize the above modeling methodology and make full utilization of deep learning models, the proposed\narchitecture in this paper consists of a two-branch feature extraction network that uses the Transformer Encoder and\n2D-CNNs to capture the global and local features respectively, the global and local features extracted from the two\nmodules are combined and then feed into the MLP predictor, and the final prediction results are obtained. As the\npriori knowledge, the process of PSR is included in the preprocessing stage, which is well integrated with the latter\nnetworks to form the learning system. The proposed deep learning architecture is named as Phase Space Reconstruction\nbased Global And Local Information Enhanced neural Network (PSR-GALIEN), as shown in Fig. 2. The design and\nimplementation details of each component of the network are introduced in the following subsections."}, {"title": "3.1 Transformer-based global feature extraction module", "content": "The Transformer model, which is centered on the self-attention mechanism, can efficiently establish the long-range\ndependencies among the input sequence, under the proposed image-based modeling perspective, considering each\nphase point in a phase trajectory as a image patch, the self-attention mechanism in Transformer Encoder can be used to\nestablish a spatial correlation model between all the patches, and thus through which a global feature representing the\noverall evolution characteristics of the whole trajectory is obtained.\nIn the global feature extraction module of the PSR-GALIEN model, the image data obtained from the PSR first needs\nthe embedding process including the value embedding (VE) and the positional embedding (PE) before entering into the\nTransformer Encoder. VE is implemented by using a fully-connected layer, which aims to learn the useful representation\nof the image patches, and PE is designed to bring additional position-informed feature after the VE, in order to solve\nthe problem of position-independent nature of the self-attention mechanism. After the processing of the embedding\nlayer, the patch sequence with the length of N and the embedding dimension of dmodel enters into the Transformer\nEncoder layer for the further processing, where the multi-head attention layer and the feed-forward layer in it are at the"}, {"title": "3.2 CNN-based local feature extraction module", "content": "For the design of the feature extraction network for local patterns of the phase trajectory image, considering the huge\ngap between the length and the width of this image (determined by Eq. 1, 3 and 4, usually \\(N \\gg m.\\)), the conventional\nmodeling method of using CNNs with deep structure for a large receptive field (RF), such as ResNet-50, VGG-16, may\nnot be so beneficial in this case for the following reasons:\n1. As illustrated in the previous subsections, the Transformer Encoders are already used to capture the global\nfeatures in the image, and there is no need to use another deep CNN to account for the long-range correlation.\nIn fact, the local feature extraction branch based on CNN is implemented to extract the detailed local patterns\nof the image (e.g., the local temporal patterns in each projection sequences of phase trajectory), the two\nnetworks are designed to undertake different feature extraction tasks as complementary.\n2. As the network continues to deepen, the unbalanced aspect ratio in the image makes it more difficult for the\ndeeper layers to obtain effective semantic features, because most of the contents in their RF are the values\nobtained from padding, and these parts do not have any meaningful information, but significantly increase\nthe computational overhead instead, and more importantly, a CNN with too complicated deep structure may\nhinder the efficient training of it."}, {"title": "3.3 MLP-based prediction module", "content": "Taking the MLP network as the predictor can efficiently establish the nonlinear correlation between the extracted\nfeatures and the target sequences, which retains all the information in the forecasting stage without any loss. Firstly, the\nglobal and local features in the phase trajectory image extracted by the two feature extraction modules are combined\nthrough vector concatenation, and then a three-layer feed-forward neural network was used to perform the feature\ntransformation, in which both layers are structured with the residual connection to ensure that both linear and nonlinear\nrepresentation can be learned in that process. Finally, the enhanced feature vector are output as the vector representing\nthe time-series to be predicted through a linear transform. The prediction process above is expressed as:\n\n\n\nwhere \\(y_{out} \\in \\mathbb{R}^{d_{pred}\\times 1}\\) is the vector of prediction results, \\(W_1 \\in \\mathbb{R}^{2d_{model}\\times 2d_{model}}\\), \\(W_2 \\in \\mathbb{R}^{d_{ff} \\times 2d_{model}}\\), \\(W_3 \\in\\mathbb{R}^{2d_{model}\\times d_{ff}}\\, W_4 \\in \\mathbb{R}^{d_{pred}\\times 2d_{model}}\\) represent the linear transformation matrices in each layer of the MLP predictor,\nand \\(b \\in \\mathbb{R}^{d_{pred}\\times 1}\\) is the bias in the last layer."}, {"title": "4 Experiments and analysis", "content": ""}, {"title": "4.1 Datasets", "content": "In order to verify the effectiveness of the proposed PSR-GALIEN model in practical power load forecasting scenar-\nios, five real-world power load datasets with varying characteristics are selected as the benchmark datasets for the\nexperiments, which are described as follows:\n\u2022 Elia-2022: the power load data from 2022 for the interconnected power grid of Elia, the Belgian power system\noperator.\n\u2022 City-1: the power load data from 2012 to 2014 for a distribution network in a city in southern China.\n\u2022 Area-1 & Area-2: the power load data from 2009 to 2015 for two regions in southern China.\n\u2022 Australia: the residential power load data from 2013 for 300 customers aggregated in a community located in\nNew South Wales, Australia.\nThese datasets cover scenarios with different spatial granularities ranging from aggregated residential users to large\ninterconnected systems grids, which are well capable of verifying the robustness of the predictors among different\nscenarios. The detailed information of the datasets are shown in the Tab. 1 and the temporal characteristics of each\ndataset are visualized in Fig. 3. As shown in the table, the LLE of each dataset is greater than 0, implying that the\nchaotic feature is a universal inner property among power loads despite of their varying temporal characteristics, and\nthe estimated optimal reconstruction parameter of each dataset are presented."}, {"title": "4.2 Experimental setup", "content": ""}, {"title": "4.2.1 Baseline setup", "content": "Deep learning models have made great progress in the field of time series forecasting in recent years, to keep up with\nthe the latest trends as well as look back on the past milestones, six state-of-the-art and representative deep learning\npredictors are chosen as the baselines in the study, including Time Series Transformer (TST) [41], Informer [42],\nPatchTST [43] in Transformer family, DLinear [44], Koopa [45] in MLP family, and TimesNet [46] in CNN family.\nAmong them, the TST model establishes a solid bridge between natural language processing and time series forecasting\ntasks, the Informer model extends its capability to perform long sequence forecasting, DLinear breaks the claim for\nthe first time that the Transformer-structured models are more advantageous, as for the Koopa model, it employs the\nkoopman operator to model the non-stationary properties in the time series from the perspective of nonlinear dynamics,\nwhich is relevant to the modeling idea of PSR. PatchTST and TimesNet are the two patch-based state-of-the-art models\nemerging in recent years, which are most relavent to our PSR-GALIEN. Therefore, these representative models were\nchosen as for comparison in this study, meanwhile, the performance of these state-of-the-arts on power load forecasting\ntasks is tested."}, {"title": "4.2.2 Hyperparameter setup", "content": "When conducting experiments, the six baselines are compared with the default settings presented in their original studies,\nas for the hyperparameters which are shared in the same structure, such as dmodel, dff, Elayers, Nheads in the Transformer\nbackbone, are kept same, which are: dmodel = 512, df = 4dmodel, Clayers = 2, nheads = 8 for Transformer, Informer,\nPatchTST and the PSR-GALIEN. As for the hyperparameters in the training stage, the setups are shown in Tab. 6."}, {"title": "4.2.3 Evaluation metrics", "content": "In this paper, two main metrics in the time series forecasting task, the mean absolute error (MAE) and the mean absolute\npercentage error (MAPE) are adopted for evaluation, which are fomulated by:\n\n\n\nwhere \\(y_i\\), \\(y^{pred}_i\\) are the i-th value of the groundtruth and the prediction results respectively, m is the prediction length,\nlower values of MAE and MAPE indicate higher prediction accuracy."}, {"title": "4.2.4 Hardware and Software setup", "content": "All the subsequent experiments in this study are carried out on a single machine with the hardware configuration of\na NVIDIA RTX4090 GPU (24GB), a AMD EPYC 9754 vCPU (128-Core) and a RAM(24GB). All programs are\nimplemented in the PyTorch 2.1.0-Python 3.10 (ubuntu22.04)-CUDA 12.1 environment."}, {"title": "4.3 Experiments and analysis", "content": "The experiments conducted in this paper consist of a total of four parts, firstly, the prediction performance of each model\nis compared on five datasets under the conditions of different forecast lengths. Secondly, the effects of varying input\nlengths on the prediction performance of each model are further explored on three long datasets, and then an ablation\nexperiment is conducted to analyze the enhancement effects of the local feature extraction module in the PSR-GALIEN\nmodel. Finally, the impact of the various hyperparameters in PSR-GALIEN are discussed in detail."}, {"title": "4.3.1 Comparison experiments with varying output lengths", "content": "In order to test the performance of each model in short-term power load forecasting scenarios, forecasting experiments\nwith varying forecasting horizons of {3h, 6h, 12h, 24h} in advance are conducted on each dataset, and five experiments\nwere conducted independently for each model with the parameter setup in Tab. 6, of which the best results are shown in\nTab. 3, and the the prediction performance of each model with respect to the prediction length are shown in Fig. 4.\nAs shown in the figure, the prediction error of each model has a different degree of upward trend as the forecasting\nlength increases. Among them, PSR-GALIEN has the best forecasting accuracy under all conditions, with the PatchTST\nto be the second best, comparing with PatctTST, PSR-GALIEN shows substantial performance improvements at all\nprediction lengths, with average improvements of 22.7%, 18.1%, 10.4%, and 8.6%, this shows that the PSR-GALIEN\nmodel is particularly capable for intraday ultra-short-term load forecasting scenarios. Meanwhile, considering power\nloads with different characteristics, the model is robust as well. Taking the MAPE metrics as the indicator, in the\nday-ahead load forecasting scenarios which are important to the dispatch operation of the power system, the prediction\naccuracy of the PSR-GALIEN model for the city (City-1) and the regional distribution grid (Area-1, Area-2) can\nreach the level of about 97%, and for the system-level load in the large-scale interconnected power system (Elia), the\nprediction accuracy can reach the level of about 96.5%, and for the aggregated residential loads (Australia) with strong\nvolatility can also reach a level of about 92.8%. Overall, the PSR-GALIEN model achieves superior performance on\nthese scenarios."}, {"title": "4.3.2 Comparison experiments with varying input lengths", "content": "The length of look-back window is an important parameter in models that adopt the autoregressive prediction strategy,\nand the increase of the window length brings more historical information, however, on the other hand, it also poses\na challenge for the model to efficiently extract the useful features from the long input sequence. In general, a good\ndeep learning model should be able to gain extra improvement in forecasting accuracy from longer inputs, however, the\nstudy in [44] argues that for the self-attention mechanism structure dominated models, they are inherently incapable of\neffectively utilizing long sequential data for prediction. Therefore, in order to validate these viewpoints and also to test\nthe prediction performance of the PSR-GALIEN model for varying prediction lengths, in this part of the experiment,\nthe output length of each model is fixed to P=96, and different input lengths of {96, 192, 336, 672} steps are used for\ntesting, the results are shown in Tab. 4 and Fig. 5."}, {"title": "4.3.3 Ablation experiments", "content": "In order to further explore the advantage of the proposed image-based forecasting approach for the extra utilization\nof local feature in the phase trajectory data, the ablation experiments are conduct to examine the impact of the local\nfeature extraction module on model performance, it is worth noting that when the local feature extraction module is\nremoved, this variant can be considered as a sequence-based forecasting model. It focuses on the prediction only using\nthe information in the individual phase points which fails to consider the use of local temporal features in the projected\nsequence of the phase trajectory. For this purpose, this variant is used to conduct comparative experiments with the\nvanilla PSR-GALIEN model on five datasets, and the results are shown in Tab. 5 and Fig. 6.\nFrom the results of the ablation experiments above, it can be seen that the local feature extraction module brings\nan average accuracy improvement of 5.9%, 6.6%, 12.1%, 6.0%, and 6.5% on each dataset under the MAE metric\nrespectively. On the one hand, this verifies the viewpoint in the previous sections that the preprocessing method of\nphase space reconstruction is equivalent to patch segmentation in terms of data structure, and the local temporal features\nunder the perspective of the latter method can be extracted for the benefit of forecasting accuracy improvement. On the\nother hand, the 2D-CNN based local feature extraction module adopted in the PSR-GALIEN is well capable for the\nextraction of the corresponding features.\nIn the last subsection, we will continue to shed light on the issue of the learned global and local features during\nthe forecasting process, and the feature visualisation techniques will be utilised to illustrate which area in the phase\ntrajectory image are used by the model during the process of global and local feature extraction."}, {"title": "4.3.4 Hyperparameters sensitivity experiments", "content": "Given that the PSR-GALIEN model comprises multiple hyperparameters both in the preprocessing stage and within the\nnetwork architecture, this section delves into a sensitivity analysis to evaluate their impact on forecasting performance.\nFirst and foremost, the three most important hyperparameters in the network are dmodel, dff, and elayers, as already\nshown in Fig.2. The candidate set of these three hyperparameters is set as follows: dmodel={128, 256, 512}, dff\n= 4dmodel, and Clayers={2, 3, 4}, which forms nine hyperparameter combinations. According to each of the above\ncombinations, the PSR- GALIEN model is set up with the related hyperparameters for day-ahead forecasting on\nthe five datasets with input length L = 192, and the results are shown in Tab. 6 and Fig.7. From the experimental\nresults, the hyperparameter combination 3 (dmodel=512, dff=2048, Clayers=2) has the best performance under the MAE\nevaluation metrics in all but Area-1 dataset, where its gap to the best one is only 1.5%, thus the hyperparameter setup of\ncombination 3 can be considered as the optimal choice for the PSR-GALIEN model.\nAs the case with the preprocessing stage, considering that the choice of PSR parameters determines the extent to which\nthe reconstructed phase trajectory approximates the inner attractor of the power load series, the Elia-2022, City-1 and\nAustralia datasets are selected as the benchmark datasets in this part for their distinguished characteristics. The rest of\nthe hyperparameters in the PSR-GALIEN model are set to the hyperparameter combination 3, with a input length of\nL=192 and an output length of P=96 (48), and the results of these experiments are shown in Tab. 7 and Fig. 8. The\nresults illustrate that the forecasting performance of the model remains generally stable for other parameters in the\nneighbourhood, with a low degree of accuracy degradation. Further exploring the two reconstruction parameters, it is\nclear that the embedding dimension m has a greater impact on the prediction performance compared to the delay time\n\\(\\tau\\), this can be attributed to the fact that, as demonstrated in Eq. 1, the embedding dimension m has a greater effect on\nthe length of the image obtained from PSR, especially in the datasets of Elia-2022, City-1, Area-1, Area-2, in which\n\\(\\tau \\gg m.\\) In such case, the dynamic features obtained in the global feature extraction process may not be able to retain\nthe original characteristics of the attractor, thus degradation of forecasting performance occurs."}, {"title": "4.4 Visualization analytics", "content": ""}, {"title": "4.4.1 Forecasting performance visualization", "content": "The day-ahead forecasting performance of the PSR-GALIEN model on the five benchmark datasets are visualized in\nFig. 9, from the results, it is clear that the predictions results in the test datasets are quite close to the groundtruth\nvalues, which indicates that although the PSR-GALIEN model adopts the autoregressive forecasting scheme for the\nonly usetilization of the historical power load series, its effective implementation of the PSR preprocessing method\nto uncover the chaotic characteristics of power loads in high-dimensional phase space, as well as the design of this\nglobal and local feature extraction strategy for feature extractionngineering, enable the model to be well capable in the\nday-ahead power load forecasting task among the power loads with varying temporal characteristics."}, {"title": "4.4.2 Feature importance visualization", "content": "The focus of feature extraction in the PSR-GALIEN model is at the center of the discussion in this paper, in order to\nhave an insight to these features, feature visualization techniques of heatmap and the regression activation map (RAM)\n[47] are used in this subsection to show the corresponding features in the trajectory image utilized by the PSR-GALIEN\nmodel during the global and local feature extraction process.\nRAM is a visualization technique that uses the gradient of back-propagation during inference process to reflect the\nextent to which specified regions in the feature maps of each CNN layer contribute to the final prediction results. In"}, {"title": "5 Conclusion", "content": "In this study, firstly the relationship of the two preprocessing methods of PSR and PS in time series feature engineering\nis discussed, from the perspective of data structure, the equivalent relationship between them is demonstrated, which\nbridges a knowledge gap for the first time. Then, an image-based modeling approach for PSR with global and local\nfeature extraction strategy is proposed, for the full utilization of this prior knowledge to extract useful patterns to improve\nthe forecasting performance. On the basis of this method, an end-to-end deep learning model namely PSR-GALIEN for\ngeneral multi-step forecasting is proposed, in which the Transformer Encoder and the 2D-CNN are implemented for the\nefficient extraction of these relative features. After that, the extensive experiments on five real datasets show that the\nproposed PSR-GALIEN model comprehensively outperforms the selected state-of-the-art models, thus its excellent\nperformance as well as strong robustness in different power load forecasting scenarios are verified, at the same time, the\neffectiveness of this image-based engineering approach is validated, which provides a brand new perspective for the\nmodeling and forecasting of power load series. Lastly, the RAM and heatmap are employed to visualized the utilized\npatterns in the trajectory image during the global and local feature extraction process, through which, the root cause of\nPSR-GALIEN's final prediction results can be explained.\nIn the future, the multivariate modeling and forecasting approaches under the synergy of complex exogenous features\nwill be further considered, in order to extend and generalize the proposed modeling approach as well as the PSR-\nGALIEN model in this paper."}]}