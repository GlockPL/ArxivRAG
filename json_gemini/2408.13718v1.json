{"title": "GPT-4 Emulates Average-Human Emotional Cognition from a Third-Person Perspective", "authors": ["Ala N. Tak", "Jonathan Gratch"], "abstract": "This paper extends recent investigations on the emotional reasoning abilities of Large Language Models (LLMs). Current research on LLMs has not directly evaluated the distinction between how LLMs predict the self-attribution of emotions and the perception of others' emotions. We first look at carefully crafted emotion-evoking stimuli, originally designed to find patterns of brain neural activity representing fine-grained inferred emotional attributions of others. We show that GPT-4 is especially accurate in reasoning about such stimuli. This suggests LLMs agree with humans' attributions of others' emotions in stereotypical scenarios remarkably more than self-attributions of emotions in idiosyncratic situations. To further explore this, our second study utilizes a dataset containing annotations from both the author and a third-person perspective. We find that GPT-4's interpretations align more closely with human judgments about the emotions of others than with self-assessments. Notably, conventional computational models of emotion primarily rely on self-reported ground truth as the gold standard. However, an average observer's standpoint, which LLMs appear to have adopted, might be more relevant for many downstream applications, at least in the absence of individual information and adequate safety considerations.", "sections": [{"title": "I. INTRODUCTION", "content": "The exploration of large language models (LLMs) in un-derstanding and modeling human emotions has received sig-nificant attention in the last two years. These studies haveprobed the capabilities of models such as the GPT familyof LLMs and others in tasks related to causal reasoning [1],emotional decision-making and appraisal theory [2], emotionclassification [3]\u2013[5], emotional intelligence [6], emotionaldialogue understanding [7], generation of emotional text [8],and more. A consistent method across these studies is thezero-shot approach (i.e., in-context learning) with promptengineering, emphasizing the LLMs' ability to perform taskswithout explicit training.\nWhile the field of affective computing often concentrateson inferring emotions from expressions, typically overlook-ing the triggering circumstances [9], computational emotionmodels aim to understand the situational context, includinghow specific aspects may evoke particular emotions and influ-ence future decisions, behaviors, and beliefs [10], [11]. Thefoundation of most computational emotion models is appraisaltheory [12], [13], actually a cluster of theories that share theprinciple that emotions arise from an evaluation of how currentcircumstances impact the individual. This evaluation, based oncriteria known as appraisal variables, assesses the relevanceof a situation to one's goals, its alignment with these goals,and its predictability, among other factors. The specific patternof these assessments gives rise to particular emotions. Forinstance, anger is triggered by goal-incongruent events whenthe person perceives control, whereas sadness emerges froma sense of powerlessness. The intensity of these emotionsis further shaped by factors such as the importance of thethreatened goal or the unexpectedness of the threat, leading tostronger emotional responses [14].\nIn this paper, we seek to address a persistent controversyinvolving appraisal theory as to whether it reflects the actualmechanisms involved in human emotion elicitation [15], orwhether it serves as a folk psychological theory that observersuse to interpret the emotions of others [16], [17], or if bothperspectives are equally valid (echoing similar controversiesas to whether emotion recognition methods are best seen asrecognizing felt or perceived emotion). If the former, LLM-based models would be well-suited to emotion recognition.If the latter, they may be better suited to predicting social"}, {"title": "II. STUDY 1: CRAFTED EMOTION-EVOKING STIMULI", "content": "Skerry and Saxe [20] hypothesized that brain representationsinvolved in inferring others' emotions based on short textualnarratives are better captured by appraisal variables thanby combinations of basic emotional dimensions. Instead ofstudying emotions as directly experienced by individuals, i.e.,the authentic and subjective/first-person experience of emo-tions, Skerry and Saxe [20] explored how people intuitivelyunderstand and theorize about the causes of emotions. In otherwords, they aimed to explore folk psychological theories [21]about emotions (i.e., how emotions are caused). Regardless ofwhether these ideas are directly tied to immediate emotional"}, {"title": "A. Reduced appraisal space", "content": "Following Skerry and Saxe [20], we apply sequential featureselection to reduce appraisals to a smaller feature spaceas several of the 38 appraisals are highly correlated. Thereduced appraisal space eliminates redundant features, helpingto capture unique variance across stimuli.\nUtilizing an ensemble classifier, we evaluate the contributionof each feature towards accurately classifying the 20 distinctemotion labels and incrementally add features that improveclassification accuracy. A model trained on ten appraisal vari-ables classifies the scenarios with 45.8% accuracy compared to56.6% observed with the full appraisal space (thus, suggestingthe reduced space achieves reasonable performance). Below isthe list of selected features:\n\u2022 Pleasantness: Did the situation involve a hedonicallypositive or pleasant experience for (name)?\n\u2022 Expectedness: Did (name) expect this situation to oc-cur?\n\u2022 Agent-cause: Was this situation caused by a person orsome other external force (e.g., randomness)?\n\u2022 Self-cause: Was this situation caused by (name) herselfor by someone/something else?\n\u2022 Already-occurred: Was (name)'s emotion based onsomething that had already occurred?\n\u2022 Close-others: Did people other than (name) know aboutthe situation that occurred?\n\u2022 Pressure: Was (name) under a lot of pressure in thissituation?\n\u2022 Consequences: Was (name)'s situation an isolated inci-dent, or did it have long-term consequences?\n\u2022 Safety: Did this situation involve risks for (name) orothers?\n\u2022 Self-esteem: Did this situation affect (name)'s self-esteem or opinion of herself?\nStrikingly, using the same ensemble approach with GPT-4 rated appraisals (i.e., using GPT-4 rather than humans topredict appraisal values) achieved 94.5% accuracy with thesame reduced set, compared to 99.7% accuracy using the full39 appraisals. Fig. 2 illustrates the performance of GPT-4 theclassify stimuli following their intended labels (i.e., True labels"}, {"title": "B. Appraisal derivation", "content": "To examine how well GPT-4 predicts how a person wouldappraise a situation (i.e., appraisal derivation), we computePearson correlations between the ten appraisal variables ratedby human participants and the corresponding variables pre-dicted by GPT-4. To this end, we first averaged the scoresover each stimulus for humans and GPT-4 to have a meanstimuli score. We observe very high correlations across theten variables, suggesting the GPT-4 mean responses closelymatch human mean appraisal scores (Table I). GPT-4 seemsto struggle to predict if a situation has already occurred (using"}, {"title": "C. Basic emotion recognition", "content": "Both participants and GPT-4 rated the stimuli on the eightbasic emotion dimensions. Table II demonstrates the results ofPearson correlation analysis. Similar to the appraisal derivationstep findings, very significant correspondence is observed."}, {"title": "D. Appraisal to emotion mapping", "content": "Finally, we investigate if GPT-4 reports a theoreticallyplausible relationship between appraisal variables and emo-tions. Recall that appraisal theories state that emotions arisefrom specific patterns of appraisals. Here, we examine andcompare the pattern underlying human participants and GPT-4 responses. To this end, we conducted multiple linear re-gression (with backward elimination) to see if/how appraisalspredict emotion dimensions. Additionally, for instances whereregression coefficients might not offer clear insights, we sup-plemented our analysis with Pearson correlation to providea more nuanced understanding of the relationships betweenvariables, particularly in terms of shared variance. Results"}, {"title": "E. Discussion", "content": "In Study 1, we employed a dataset of crafted stimuli ratedby external observers, which were systematically manipulatedin ways that yield different appraisals and emotions. Except fora few instances, results suggest a remarkable correspondenceof mean human scores and mean GPT-4 scores in all processesinvolved in emotional cognition. The similarities exceed whatis reported in [2], [18] and others. One interpretation might bethat GPT-4 excels when dealing with stereotypical situationsrather than free-form self-report idiosyncratic vignettes. Also,GPT-4 might view situations as an observer and capture thethird-person perspective of the average human."}, {"title": "III. STUDY 2: INVESTIGATING THE PERSPECTIVE", "content": "Study 2 tests the hypothesis, suggested by Study 1, thatGPT-4 processes emotions through the lens of an averageobserver. To this end, we employ the crowd-enVENT datasetdeveloped by Troiano et al. [23], which is, to the best ofour knowledge, the only corpus that includes both authorand reader annotations of both appraisals and emotions. Thisvaluable corpus enables researchers to compare the agreementof external annotators and self-assessments of the authors.Similar to the first study's corpus, the appraisal scheme usedto create crowd-enVENT is primarily based on the schemeproposed by Scherer and colleagues [24], [26]. Unlike thefirst study, crowd-enVENT consists of self-reported vignettes(1200 data points). However, strategies are adopted to promotethe collection of more idiosyncratic events to induce a higherdiversity of events and appraisal dimensions [23]. In thiscorpus, participants rate 21 appraisal variables on a scale of1-5 and pick an emotion from 12 emotion labels plus a \"noemotion\" label. For the sake of brevity, we refer readers tothe original paper for detailed descriptive statistics and corpuscreation and validation processes. Here, we focus on testingour hypothesis that GPT-4 is aligned more with an averageobserver's evaluation of emotions and appraisal induced inevents."}, {"title": "A. Appraisal derivation", "content": "We first turn our attention to the correspondence of appraisalratings between GPT-4, the author's self-assessments, andexternal human raters. We follow the same procedure asdescribed in Study 1 to generate prompts with minimumadditional text for standardized output. Fig 8 provides acomparison of appraisal rating agreements using Krippen-dorff's alpha measure [27]. Krippendorff's alpha is particularlysuitable here due to its ability to handle different levels ofmeasurement (nominal, ordinal, interval, and ratio and toaccommodate any number of raters. It also enables to havethe same agreement measure for both appraisals and emotions.Based on Fig 8, we see GPT-4 corresponds significantlybetter to the average ratings of readers than the authors'original self-assessments. The average reader (considered asingle rating) slightly outperforms GPT-4. However, we seethe lowest agreement levels among readers (5-way), suggestinghigh variations in evaluations among human external observersof an emotion-evoking event. In summary, GPT-4 seems to beon par with an average human observer and significantly morein line with a third-person perspective than the self-assessmentof the event.\nTo delve into the nuances, Fig. 9 showcases the agreementscores for the 21 appraisal variables. GPT-4 aligns moreclosely with the average third-person perspective in 20 ofthe 21 appraisals when comparing the \"Avg reader & GPT-4\" and \"Author & GPT-4\" columns. GPT-4 mostly matchesthe average reader's predictions of the author's self-assessedappraisals or exceeds that in 6 out of the 21 appraisal variables.The lower inter-reader agreements on appraisal features em-phasizes individual differences and indicates GPT-4's tendencytowards a balanced average-human assessment of emotionalsituations. Furthermore, a consistent pattern across the fourcolumns indicates that certain appraisals are universally chal-lenging to predict, whether from an individual or a generalized social perspective, hinting at the need for additionalsituational or personal information for precise appraisal pre-diction. Variables such as pleasantness, unpleasantness, goal-support, and congruence, as well as self/other responsibility,are relatively straightforward and show higher agreementlevels across comparisons. Conversely, appraisal variables likeaccept_consequence (i.e., accommodative coping) are moredependent on the individual involved. These subtleties callfor further research to enhance LLMs' emotional cognitionby incorporating individual variability."}, {"title": "B. Emotion recognition", "content": "The authors, five human readers, and GPT-4 attributed alabel among 13 emotion labels to the self-reported vignettes.Similar to the previous step, we employ Krippendorff's Al-pha in this classification task, as it calculates the degreeof agreement corrected for the chance to ensure that theagreement among raters is not simply due to random chance.We employ the most representative label for a given vignette(i.e., majority rule) to serve as a proxy for the collectiveattribution of labels to scenarios or the consensus viewpoint.Fig 8 demonstrates the results of the analysis. Similar tothe appraisal derivation step, GPT-4 corresponds significantlymore to the majority class selected by readers than the authors'original self-assessments and is on par with average readers inpredicting original labels (relative to inter-reader agreement)."}, {"title": "IV. GENERAL DISCUSSION", "content": "In the two studies conducted, we explored how GPT-4processes and interprets emotions from both artificially craftedadditional individual information and computational modelsthat can reliably account for such individual variability, thethird-person perspective is aptly attuned to a more generalemotional experience. Furthermore, using third-person datamight potentially pose fewer ethical concerns and privacyissues compared to self-reported data, which can be moresensitive and personal. On the other hand, this approach couldlead to over-generalization, potentially resulting in reduceddiversity and inclusion, and may also perpetuate biases insocial perceptions."}, {"title": "ETHICAL IMPACT STATEMENT", "content": "This paper re-analyzed previously collected de-identifieddata previously subjected to ethical review. This data is usedas a benchmark to scrutinize the underlying mechanisms ofhow pre-trained language models process human emotion.However, it should be noted that we investigated a singlelanguage model, which is constantly being updated; hence,caution must be taken in generalizing these findings to otherlanguage models or other versions of the examined model. Aspredicted by prior research on emotion, strong cultural anddemographic differences exist in how emotional situations areconstrued. Thus, these findings should be replicated acrossthese different groups. Finally, the findings highlight thepotential concerns for those seeking to deploy large languagemodels to reason about human emotion or generate emotionalcontent. Given the criticality of potential harm caused by theemotional manipulation of LLMs (or any AI, for that matter),we need constant measurements of LLMs' emotional cognitionand manipulation abilities with every new LLM or any updatesto current LLMs. We need to have a comprehensive benchmarkfor such studies."}]}