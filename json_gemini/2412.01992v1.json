{"title": "ChatCollab: Exploring Collaboration Between Humans and Al Agents in Software Teams", "authors": ["Benjamin Klieger", "Charis Charitsis", "Miroslav Suzara", "Sierra Wang", "Nick Haber", "John C. Mitchell"], "abstract": "We explore the potential for productive team-based collaboration between humans and Artificial Intelligence (AI) by presenting and conducting initial tests with a general framework that enables multiple human and AI agents to work together as peers. ChatCollab's novel architecture allows agents - human or AI - to join collaborations in any role, autonomously engage in tasks and communication within Slack, and remain agnostic to whether their collaborators are human or AI. Using software engineering as a case study, we find that our AI agents successfully identify their roles and responsibilities, coordinate with other agents, and await requested inputs or deliverables before proceeding. In relation to three prior multi-agent Al systems for software development, we find ChatCollab AI agents produce comparable or better software in an interactive game development task. We also propose an automated method for analyzing collaboration dynamics that effectively identifies behavioral characteristics of agents with distinct roles, allowing us to quantitatively compare collaboration dynamics in a range of experimental conditions. For example, in comparing ChatCollab AI agents, we find that an AI CEO agent generally provides suggestions 2-4 times more often than an AI product manager or AI developer, suggesting agents within ChatCollab can meaningfully adopt differentiated collaborative roles. Our code and data can be found at: https://github.com/ChatCollab.", "sections": [{"title": "1 INTRODUCTION", "content": "Advancements in Large Language Models (LLMs) and Artificial Intelligence (AI) agent systems have significantly increased the potential for productive collaboration between humans and AI. Tools like GitHub Copilot [1] have become widely used. However, popular copilots place humans in the primary decision-making role and typically do not alter the dynamics of human-to-human interaction. Most prior research on human-AI collaboration to date (e.g., [16, 17, 23, 24, 28, 33, 34, 41, 44, 47]) has similarly focused on dyadic interaction and quantitatively measurable tasks such as collaborative gaming [47], object identification [46] or decision making [33]. More recently, a new class of AI agent systems [2\u20134, 36] has emerged, coordinating multiple AI agents to complete software development or similar tasks. Looking ahead, however, we believe that more flexibly structured collaborative teams comprising both humans and AI agents will become more prominent, allowing both humans and AI agents to assume diverse roles as needed. This is a natural evolution from structured human-only teams, allowing human-AI collaboration to draw on the rich history of research and practical experience in management and organizational behavior."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Al-Assisted Software Development", "content": "There is a rich history of AI-based tools for specific aspects of software development. Summaries organized according to phases of the software development process appear in [11, 12]. Beginning with Github Copilot [1], a number of copilot tools integrate calls to an LLM into the IDE. A"}, {"title": "2.2 Multi-Al-Agent Software Development", "content": "Prior work observes that LLMs may struggle with hierarchical multi-step reasoning tasks like generating complex programs [22]. Observing this limitation, Zelikman et al. [43] introduce a framework that automatically decomposes algorithmic tasks into hierarchical natural language function descriptions and then proceeds to select suitable code implementations for each function. The advantage of enlisting multiple agents is illustrated by the fact that in comparison to directly sampling AlphaCode and Codex, the system developed by Zelikman et al. solves more competition-level problems in the APPS dataset, resulting in over 75 percent higher pass rates.\nPrior systems have used multiple AI agents to simulate realistic human behavior [35] and, for collaborative software development in particular, have assigned specific roles to agents [25, 36]. In particular, the three structured agent systems MetaGPT, ChatDev, and SuperAGI [2-4] we compare to ChatCollab and to each other all divide tasks and responsibilities using familiar human roles."}, {"title": "2.3 Human interaction with Al agents for Learning", "content": "We are not aware of extensive prior work on multi-agent systems in which multiple humans interact as peers with multiple agents to produce a work product. However, some interesting prior systems explore educational settings that involve humans and multiple AI agents. Many of them seem best regarded as learning through role-playing or simulation.\nA recent position paper explains and summarizes prior use of a partner-mentor paradigm, in which one Al agent serves as a mentor (or instructor) and another AI agent serves as a practice partner [42]. Examples cited include Active Listening, Conflict Avoidance, Conflict Resolution, Empathy, and Rhetoric, as well as interactive TA (teacher) training as described below; a compelling case study appears in [38]. Biswas et al. propose a learning system with a multi-agent architecture that included four agents: a teachable agent, a mentor agent, a student agent, and an environment agent [14]. Similarly, Soliman and G\u00fctl identify and distinguish between the types of Pedagogical Agents which contribute to learning in Virtual Learning Environments: agents for learning per- sonalization, teachable agents, and multiple agents supporting group learning [40]. Markel et al. present GPTeach, in which a pair of student AI agents provide practice for interactive TA training [32].\nMore broadly, educational simulations with technology have been shown to enhance learning in technical training [26], games [21] and engineering education [9]. Theoretical foundations for educational simulations have been explored in [30] and elsewhere. In the area of AI that improve human-to-human communication, AI-based chat interventions may improve the cordiality of political conversation between two humans online [6]. Similarly, human-AI collaboration through AI-in-the-loop has also been shown to increase conversational empathy [39]."}, {"title": "2.4 Collaboration analysis", "content": "We view our analysis of ChatCollab transcripts \u2013 sequences of message between members of the team as an automated form of qualitative coding. Coding in social science research is a qualitative data analysis method where descriptive labels are assigned to selected aspects of the data, allowing researchers to tabulate qualitative features. Coding labels are typically assigned by human coders who are given specific criteria on when to apply them. There are a number of methods, including comparison of codes assigned by multiple coders to assess confidence and improve tabulation. Because human data labeling is often a bottleneck or resource-constrained limitation of social"}, {"title": "3 CHATCOLLAB: CONFIGURABLE SYSTEM FOR HUMAN-AI COLLABORATION", "content": "We present ChatCollab, a system that enables configurable teams composed of both humans and AI agents. Users define specific team roles assigned individually to each agent in the team \u2013 human or AI - allowing for flexible team configurations and enabling authentic collaboration between humans and AI agents as peers."}, {"title": "3.1 Motivation for ChatCollab design decisions", "content": "AI-Human collaboration may be designed to produce a work product, have some effect on the participants, or both. In a single sprint of a hybrid human-AI software team, for example, the primary goal may be the best software possible. However, if a team operates over a longer period of time, evolutionary improvements in the team are also important. Many team leaders and managers in human organizations recognize the importance of balancing work-produce goals with team morale, future productivity, and career development for members of the team. Because environments that use multiple intelligent agents to assist student learning address human engagement and progress, we have found it useful to draw on that literature, e.g., [14, 40]. This perspective led us to the following design goals:"}, {"title": "3.2 ChatCollab System Description", "content": "At a high level, the system works as follows:\n(1) A human administrator defines roles of the team by entering them into an admin dashboard.\n(2) Each AI agent is created through the system and begins running autonomously.\n(3) A shared event timeline is created that all agents have access to for communication and taking actions."}, {"title": "3.3 ChatCollab experiment methodology", "content": "In our experiments, we configure a team of three AI agents and one human. This includes Peter (AI CEO), Boshen (AI Product Manager), Isabelle (AI Developer), and Benjamin (Human client). We"}, {"title": "3.4 Observed ChatCollab system behavior", "content": "We find that in ChatCollab, the agents demonstrate awareness of social expectations and workflows in a software development team. The following examples, taken from a the transcript of interaction during development of our sample tic-tac-toe game by mutiple agents, illustrate how the agents are able to: (1) identify their roles and responsibilities, (2) follow established processes, (3) effectively coordinate and communicate, (4) acknowledge and update one another, and (5) wait for necessary events before proceeding."}, {"title": "4 COLLABORATION ANALYSIS", "content": "To explore how AI agents within ChatCollab exhibit differing behaviors depending upon their role, we propose a method for classifying and analyzing the behavior of the AI Agents. In addition, this allows us to determine if AI agent collaborative behavior can be predictably changed through prompting of the institutional knowledge. Our method leverages LLMs to automatically classify sequences of agent actions according to any chosen classification framework. We illustrate the method with a sample study using Bales' Interaction Process Analysis Framework [10], a well- established method for categorizing group interactions. While using LLMs for qualitative coding and categorization is not novel in itself (see section 2), the current context provides a unique setting for evaluating collaborative dynamics."}, {"title": "4.1 Methodology", "content": "To be clear, a ChatCollab run is a sequence of actions carried out by agents, and a transcript is the log of messages from a run.\nData Collection. In the study reported here, we consider eight experimental conditions. For each experimental condition, we conduct three runs of the ChatCollab system until completion of the code and record the full dialog transcript of AI and human agents for each run. This resulted in 237 transcript messages total across all runs, including the control.\nData Preparation and Cleaning. For each run, ChatCollab outputs the conversational transcript with a pattern for displaying speakers, roles, timestamps, and messages to a markdown file. We use a Python script to automate the extraction of conversational data from markdown files and convert it into structured CSV formats. Using regular expressions, the content of each file is split into dictionaries stored in a list, then written to a CSV file where each row represented a single conversational turn. This format facilitates easier analysis of turn-based interactions. The code for this script appears in Appendix A.\nFramework Selection. We chose to use the well established Bales' Interaction Process Analysis Frame- work [10] to perform our collaboration analysis, though we believe our method is generalizable to other frameworks. The framework is as follows:\n(1) Shows Solidarity: raises other's status, gives help, reward.\n(2) Shows Tension Release: jokes, laughs, shows satisfaction.\n(3) Agrees: shows passive acceptance, understands, concurs, complies.\n(4) Gives Suggestion: direction, implying autonomy for other.\n(5) Gives Opinion: evaluation, analysis, expresses feeling, wish.\n(6) Gives Orientation: information, repeats, clarifies, confirms.\n(7) Asks for Orientation: information, repetition, confirmation.\n(8) Asks for Opinion: evaluation, analysis, expression of feeling.\n(9) Asks for Suggestion: direction, possible ways of action.\n(10) Disagrees: shows passive rejection, formality, withholds help.\n(11) Shows Tension: asks for help, withdraws out of field.\n(12) Shows Antagonism: deflates other's status, defends or asserts self.\nAPI Calls and Prompt Engineering. We found it effective to use a straightforward prompt that included a full description of the task and the expected format of the response. We integrated Bales' IPA framework into the prompt used for automated classification, using an iterative prompt devel- opment process with human verification to identify the optimal wording. We then systematically applied this framework to classify each agent action using an LLM (OpenAI's GPT-4 Turbo). Note that results were single-coded to the best fitting category. We experimented with different lengths of surrounding context, ranging from the prior two turns to the full context from the given run. We found it best to keep the prompts concise.\nThe final prompt includes the main categorization criteria plus None of the Above to account for cases that do not clearly fit into any of the categories. We provided the LLM with the roles and messages of up to two turns before, the current message itself, and two turns after the intended message. The prompt is provided in Appendix A.3.\nHuman Coding. A portion of the data was additionally coded using human review from members of our team to check for accuracy. This was done to ensure the reliability of the LLM-augmented classification processes and allowed for iterative refinement of the prompts."}, {"title": "4.2 Results", "content": "In our analysis, we investigated the interaction patterns and role dynamics across different experi- mental conditions. We wanted to understand (1) how the AI agents' role prompts in ChatCollab would impact their behavior, (2) if prompting within the institutional knowledge could predictably influence the collaborative dynamics of the agents, and (3) how effectively our analysis method would detect such effects.\nHuman-Al Coding Alignment. We conducted a structured comparison between the LLM categoriza- tions against human-coded categorizations applied with the same collaborative framework.\nTo assess the accuracy of automated coding for our specific experiment, we calculated inter-rater reliability metrics between the LLM and human codes. This resulted in pairwise percent agreement as 78.1% and Cohen's Kappa as 72.9%, which indicated significant alignment between human and Al coding.\nRole-Specific Contributions. Figure 3 shows the distribution of interaction moves made by each AI agent and the human client across all experimental conditions and runs. The manner in which the collaborative move for each run was prompted in institutional knowledge is included in Appendix A.2. This visualization provides insight on the distinct roles performed by each agent, reflecting alignment with expectations of their defined roles from the ChatCollab framework. Peter, the AI CEO, demonstrated an emphasis on \"Shows Solidarity\" and \"Gives Suggestions\". Isabelle, the AI developer, mostly engaged in \"Gives Orientation\" and \"Shows Solidarity\". Boshen, the AI Product Manager displayed a dual focus on both \"Shows Solidarity\" and \"Gives Orientation\". Benjamin, the human client, predominantly engaged in \"Gives Suggestion\"."}, {"title": "5 CODE ANALYSIS", "content": "In order for these agent systems to be effective tools for software development and long term human- AI collaboration, their code output must be functional and high quality. We compared the efficacy of ChatCollab to ChatGPT and several other AI agent systems (SuperAGI, MetaGPT, ChatDev) to confirm that it is an adequate supporting environment for multi-agent software development, at least as good as other systems previously designed specifically for this purpose. We used GPT-4 with at least ten runs for each system. The human was only the client in all runs, with an AI CEO, product manager, and developer. Some runs for the code quality experiment included an AI quality assurance agent."}, {"title": "5.1 Benchmark Task Selection", "content": "We opted for the relatively straightforward design of a text-based tic-tac-toe game for two main reasons. First, it is easier to clarify the expectations and requirements and, therefore, introduce metrics that favor objective comparison. Second, if we can find an example that is fairly simple to reason about and interpret but that nevertheless leads to failures, we might hope to extrapolate failure modes to more complex tasks. We also considered other options, including Sudoku, but ultimately determined that tic-tac-toe presented a more clearly defined ruleset and lent itself to more straightforward objective evaluation."}, {"title": "5.2 Prompt Formulation", "content": "Formulating the proper prompt is crucial in our comparison. Intuitively, a short prompt like \u201cwrite a program to play a tic-tac-toe game\u201d may seem sufficient. There are many parameters that need clarification, especially for establishing a standard for comparison across AI agent systems.\nIn our prompt, we list a set of functional requirements that a software developer needs to be aware of and the target programming language (Java), but did not specify the implementation details as this is normally a developer's responsibility. Our final prompt can be found in the Appendix A.1."}, {"title": "5.3 Evaluation Criteria", "content": "Although there is inherent subjectivity in assessing code quality and other qualitative software development factors, we aimed to establish fair standards by quantifying aspects like human-agent interaction and functional performance. We established a set of measurable standards to reduce variability in our assessments, found in Table 1."}, {"title": "5.4 Results", "content": "We evaluated several runs tasking agents in ChatCollab with producing the tic-tac-toe game, and observed the output quality of the code to be equal to or better than existing frameworks. Specifically, ChatCollab particularly excelled at feature inclusion and documentation because individual agents were able to proofread the code and request revisions to include anything missing. In addition, the inclusion of a product manager that asks clarifying questions to the human client allowed for improved accuracy of functionality. Figure 5 summarizes how ChatCollab compares with popular"}, {"title": "6 LIMITATIONS", "content": "This work is an initial exploration of multi-agent collaboration with inherent complexities. Limita- tions in the work and areas for future improvement are summarized below.\nLimited Collaboration Analysis: We evaluated circumstances with up to five AI agents with different roles, and one human in the roles of CEO, developer, and product manager in different runs.\nIt is possible that the system's performance may vary as the complexity of scenarios increases, suggesting a need for further investigation to understand its scalability and robustness in more complex settings. Future work may also consider multi-coded categories for message labeling, allowing a given message to support multiple categories within a collaborative framework rather than confined to one. Similarly, turns might be further deconstructed into multiple parts to break down individual meaning and address cases where a message labeled as Shows Solidarity also contained elements of Gives Orientation.\nLimited Code Quality Analysis: Our code quality evaluation is based on a limited number of examples. We have not conducted large-scale evaluation of human-AI collaboration in which we solicit others to participate in synthetic teams of various organization and size, for example. We hope, however, that by documenting our work so far, we are setting the stage for larger tests in future work.\nBiases of AI: If this system is to gain widespread adoption across both industry and academic settings, it is important to meticulously evaluate its potential negative impacts on humans involved in the collaboration. It is known that AI can amplify the biases embedded within its training data. Further evaluation and mitigation are merited over time.\nUnknown Social Impact: We have not yet conducted a dedicated user study with further investigation into the system's impact on social dynamics and user experience. To ensure that this system is able to foster positive collaboration, we need to explore its effects on team dynamics, user interactions, and overall productivity.\nSignificant Computational Expenses: ChatCollab has the potential for significant computa- tional expenses compared to other systems. This is because all agents are able to be prompted after a new event, rather than a linear sequence of agents prompted. Several optimizations were made to reduce unnecessary token usage. For instance, agents are only prompted after a new event has occurred. This prevents duplicate prompting. The individual agents also experience a brief period of a random pause (3-15 seconds) between decisions to take an action, which reduces frequency of prompting and allows more space for humans to contribute to the conversation. An alternative approach to a randomized pause could also be used to produce greater standardization of results. Moving forward, more optimizations should be explored to reduce token usage and frequency.\nFocus on Slack: Although Slack is sufficient to produce product documentation, code, and a process which a human can observe and participate in, the sole usage of Slack without additional apps or platforms limits the functionality of ChatCollab. Future work should expand the platforms that are used for collaboration.\nAI Agents Don't Know Who is a Human: The system may be limited by the agents' lack of knowledge regarding which team members are human and which are AI. When agents are unaware of whether team members are human or AI, AI agents consistently treat all team members the same, preventing any unexpected changes in behavior when interacting with human members. However, it may create a limitation on learning and collaboration. The AI agent system could prioritize the learning experience of the human in the team, if they know which team member is a human. It may also aid the outcome of the collaboration. It should be noted that it is currently possible to identify the humans in the team through institutional knowledge, though this was not explored in our tests."}, {"title": "7 CONCLUSION AND FUTURE WORK", "content": "In this paper, we introduce ChatCollab, a novel framework for AI-human collaboration, designed to explore the potential of configurable hybrid teams. ChatCollab enables dynamic role adoption and interaction between human and AI agents, without distinction based on the nature of the agent. This flexibility not only fosters peer collaboration, but also allows for the simulation of wide variety of team environments.\nTo assess the effectiveness of ChatCollab's prompting, we developed an automated method for analyzing collaboration dynamics, present an illustrative case study, and conducted a benchmark code quality evaluation. Our results indicate that ChatCollab effectively modulates collaboration dynamics, in ways that can be measured using our collaboration analysis method. Furthermore, our benchmarks confirm that the code quality produced by ChatCollab is comparable to that of leading multi-agent systems, suggesting its viability as an effective software development system.\nMoving forward, we are excited to explore the application of ChatCollab to a variety of collabo- rative tasks. Because it is easy to configure using natural language prompting, a wide variety of experiments such as those described in [42] are easily implemented and conducted. The system can also expand beyond creating small teams and into larger organizational settings with human resources and people (or AI agent) managers. We look forward to investigating how this system can be adapted to broader contexts and used to study productivity and the quality of interactions within hybrid teams, including human-user studies to investigate impact on humans. We also invite others to leverage ChatCollab for any possible experiments or applications for which it may be suited. The source code and data are publicly available on GitHub [5]."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 User Prompt", "content": "You are tasked with developing a text-based Tic-Tac-Toe game. The game should be inter- active and allow two players to take turns making moves on a 3x3 grid. The code should be in the Java programming language. Make sure that the code compiles. In other words, you do not call a method that is not declared, there is no method with an empty body and the return types are correct. Each player is represented by a symbol ('X' or 'O'). The game should display the current state of the board after each move and indicate the winner or a tie when the game concludes.\nYour task is to design a conversational interface for the Tic-Tac-Toe game. The chatbot should guide the players through the game, prompting them to input their moves and providing feedback on the game's progress. Consider the following aspects in your response: Game Initialization: Start the game by displaying an empty board and assigning 'X' to the first player and 'O' to the second player.\nPlayer Input: Prompt players to input their moves by specifying the row and column where they want to place their symbol. Ensure that the input is validated to prevent invalid moves. Keep in mind that a user can type anything as input. It is your responsibility to validate it.\nGame Progress: After each move, display the updated board. If a player wins or the game ends in a tie, announce the result and end the game.\nError Handling: Implement error messages for invalid inputs, such as attempting to place a symbol in an already occupied space or entering an out-of-range position.\nGame Restart: After the game concludes, ask if the players want to play again. If they do, reset the board and start a new game. If not, bid farewell.\nFeel free to elaborate on the conversation to make the interaction more engaging and user- friendly. Consider adding features like displaying the player's name, handling unexpected inputs gracefully, and ensuring a smooth overall gaming experience. Do not forget to add comments in the source code and decompose the overall task to simpler subtasks/modules."}, {"title": "A.2 ChatCollab Configuration Prompts", "content": "The following agent persona descriptions were used to set up the agents in the runs for the collaborative dynamic experiment.\nPeter (CEO)\nYou are the CEO of a development firm that creates software for a client, who will provide their requirements, and can answer clarifying questions. Your role is to communicate with the team (developer and product manager) to coordinate building the product in this order: (1) Clarifying questions to client, (2) PM generates PRD, (3) Developer generates code.\nBoshen (Product Manager)\nYou are a professional product manager. Your role is to design a concise, usable, efficient product. You ask clarifying questions to the client, then create a full PRD that is compre- hensive but concise. You can also work with developers to answer their product questions by coordinating with leadership, likely the CEO."}, {"title": "A.3 LLM Labeling Prompt", "content": "Analyze the following message in the context of a collaboration dialogue and categorize it into one of the following categories.\n1. Shows Solidarity: raises other's status, gives help, reward.\n2. Shows Tension Release: jokes, laughs, shows satisfaction.\n3. Agrees: shows passive acceptance, understands, concurs, complies.\n4. Gives Suggestion: direction, implying autonomy for other.\n5. Gives Opinion: evaluation, analysis, expresses feeling, wish.\n6. Gives Orientation: information, repeats, clarifies, confirms.\n7. Asks for Orientation: information, repetition, confirmation.\n8. Asks for Opinion: evaluation, analysis, expression of feeling.\n9. Asks for Suggestion: direction, possible ways of action.\n10. Disagrees: shows passive rejection, formality, withholds help.\n11. Shows Tension: asks for help, withdraws out of field.\n12. Shows Antagonism: deflates other's status, defends or asserts self."}]}