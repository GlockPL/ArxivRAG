{"title": "Climbing the Complexity Ladder with Expressive Attention", "authors": ["Claudius Gros"], "abstract": "Attention involves comparing query and key vectors in terms of a scalar product, QTK, together with a subsequent softmax normalization. Classicaly, parallel/orthogonal/antiparallel queries and keys lead to large/intermediate/small attention weights. Here we study expressive attention (EA), which is based on (QTK)2, the squared dot product. In this case attention is enhanced when query and key are either parallel or antiparallel, and suppressed for orthogonal configurations. For a series of autoregressive prediction tasks, we find that EA performs at least as well as the standard mechanism, dot-product attention (DPA). Increasing task complexity, EA is observed to outperform DPA with increasing margins, which also holds for multi-task settings. For a given model size, EA manages to achieve 100% performance for a range of complexity levels not accessible to DPA.", "sections": [{"title": "Introduction", "content": "Since its inception [3, 36], the attention mechanism has had a transformative impact on machine learning [31, 25]. At its core, attention facilitates pairwise information routing between tokens, with information being transmitted when a given matching condition is fulfilled. For the latter query and key vectors Q and K are compared in terms of the respective dot products, QTK, which are mapped to a normalized probability distribution via a subsequent softmax operation. As illustrated in Fig. 1, this setup, dot-product attention (DPA), constrains the central part of the matching condition to a one-dimensional subspace of the otherwise potentially large space of attention heads. Given that attention matrices tend to be sparse [20], it might be favorable to allow the system to express low attention states in a larger subspace.\nHere we introduce and discuss a modified attention mechanism, expressive attention (EA), which is based on the presumption that low attention should correspond to orthogonal query and key configurations. This venue allows attention to express itself in the entire attention space, as illustrated in Fig. 1. All one needs to realize EA is to base attention on (QTK)2, the squared scalar product. In practice, this corresponds to changing a single line of code, which makes it straightforward to introduce effective attention into production codes."}, {"title": "Related work", "content": "Alternatives to the standard, attention-based transformer have been studied intensively during the past years. At a basic level, non-transformer based architectures for sequence processing have been proposed, like structured and/or selective state sequence models [12, 11]. Other approaches aim to reduce compute costs, for example by making use of a multi-to-multi recurrent network analogy [9]. Another route is linearized attention [17, 37, 38] for which compute scales linearly with context length, and not quadratically. In practice, linear attention comes with various problems of its own, like unbounded gradients and attention dilution [27, 14]. In general, linearized attention models substitute the softmax operation of classical dot-product attention by various linear kernel functions, which may have stochastic components [8, 26], well designed spatial dependencies [28], or being derived from a singular value decomposition [6].\nA different aspect is addressed by rotary positional embedding (ROPE), which adds positional information directly to the attention mechanism, via position-dependent rotations of query and key vectors [34, 33]. RoPE can be used in particular for extending context windows [5]. Somewhat similar in spirit is (IA)\u00b3, which uses element-wise rescaled key and values vectors (in addition to rescaled hidden layer activities) for fine-tuning pretrained LLMs for downstream tasks [22].\nTwo main routes are available for testing the performance of sequence processing architectures, such as transformers. The first is to use databases relevant for real-world applications [16, 39], the second is to rely on synthetic test suites. The latter approach is used standardly when studying learning biases, e.g., when comparing length generalization scores for functions like \u2018parity', 'majority', 'first' or 'mean' [2, 13]. Synthetic test environments have been employed also for the study of attention glitches in reasoning tasks [21]. For our studies we use NT tasks, a suite of synthetic autoregressive prediction tasks which can be tuned to a desired level of complexity. In this way we are able to study performance differences in the vicinity of the complexity boundary."}, {"title": "Background", "content": "Multiplying token activity with the respective query, key and value matrices generates three vectors, Qi, Ki and Vi, specifically for each token i in a given attention layer. The activity ym of token m is given by ym = \u2211k<m amkVk, when masked attention is used. The attention matrix amk \u2265 0 is normalized row-wise, 1 = \u2211kamk, encoding how much information is transferred from token k to token m. This setup implements information routing for any suitable mechanism determining the individual amk. The standard approach [36],\n$\\displaystyle a_{mk}^{DPA} = \\frac{1}{Z_m} \\exp (\\beta Q_m^T K_k)$\n$\\displaystyle \\frac{1}{Z_m} \\exp (\\beta Q_m^T K_k)$\ntakes the scalar product QTK as the fundamental similarity measure. The softmax operation included in (1) transforms the basic similarity measure into a probability distribution function, with Zm being an appropriate normalization factor. For the rescaling factor \u03b2, the standard value is \u03b2 = 1/\u221aNcon, where Ncon is the context length [36]. Here we set \u03b2 = 1 for simplicity.\nAs an alternative to (1) we investigate expressive attention (EA), defined by\n$\\displaystyle a_{mk}^{EA} = \\frac{1}{N_m 1+ \\frac{z_{mk}}{2}\n$\\displaystyle a_{mk}^{EA} = \\frac{1}{N_m 1+ \\frac{z_{mk}}{2}\n2222 ZmkZmkQTKQTk,(2)\nwhere Nm is a normalization factor. As illustrated in Fig. 1, there are several distinct differences between EA and DPA.\nExpressivity in attention space Attention mechanisms based on comparing keys and queries allocate semantic meanings to the respective configurations. The standard approach is to map parallel/anti-parallel/orthogonal configurations to high/low/neutral attention weights. For a d-dimensional attention-head space, this option attributes d \u2013 1 dimensions to neutral configurations.\nFull attention matrices tend to be sparse [20], which allows in many instances to reduce the dimensionality of attention matrices from the start via a dedicated decimation process [29, 23]. This observation suggest, that it may be advantageous to map the entire d \u2013 1 dimensional orthogonal subspace to reduced attention weights. This is the route taken by expressive attention, as shown in Fig. 1. The rationale non-linearity also introduced in (2) is not a critical feature of EA, other choices would be viable as well."}, {"title": "Experimental settings", "content": "Suite of Tasks A basic non-linear autoregressive task is to predict the next token of a time series [18]. For our investigation we are interested in a suite of time series prediction tasks which allows to systematically increase and tune difficulty. As a motivation we first consider the basic XOR setting [10].\nx(t)\nx(t)\n==XOR(x(t \u2212 1), x(t \u2212 2))\n[x(t \u2212 1) + x(t \u2212 2)]%2,(3)\nwhere we used in the second step that the XOR operation correspond to the addition of two boolean inputs x = 0/1, modulo two. As illustrated in Fig. 2, we generalize (3) to the case of a general delay \u03c4\u2208 [1,2,3,..] and basis N \u2208 [2, 3, 4, ..],\nNT: x(t) = [x(t \u2212 \u0442) + x(t \u2212 1 \u2212 \u0442)]%\u2116, (4)\nwhich defines the NT prediction task. For the XOR series, recovered for N2T1, two types of cyclic patterns are generated,\n011011011011..., 000000000000... (5)\nThere are four initial conditions, 11, 01, 10 and 00, of which the first three give rise to cycles of type 011, with the last leading to the default series. The complexity of the associated prediction task increases systematically when increasing N and/or 7. For example, one has\n512-120+64-60+8\u00b730+1\u00b715+1\u00b71 = 65536 = 164 (6)\nfor N16T3, which states that there are 512/64/8/1/1 cycles of length 120/60/30/15/1. One recovers the N++1 = 164 possible initial conditions when summing up all cycles together with their respective multiplicities. The most demanding system examined in this study is N16T5, for which 166 = 224 \u2248 16.8 M initial conditions exists, together with a corresponding number of distinct cycles.\nTransformer details As a testbed, we use a standard transformer architecture with Ncon context token of embedding dimension d = N, where N is the basis of the NT task, see Eq. (4). A straightforward orthogonal token embedding of the N symbols {0, 1, 2, . . ., N-1} is implemented. Attention is causal, but positional embeddings are not included, which simplifies the analysis of performance as a function of input length. It has been argued in this context that causal attention is all one needs, in the sense that causality implicitly leads to the encoding of positional information [15]. A single transformer bilayer is used throughout this study, with one attention head per token. Skip connections are present, with layer normalization being performed on entry, separately for the attention and the token-wise, tanh feedforward layer. The width of the hidden layer for the latter is expanded by a factor four.\nTraining Training is performed as a function of epochs, with each epoch consisting of Nbatch predictions. At the start of each epoch a new random NT series is generated and encoded. The first Ncon symbols are then loaded into context space. For the underlying sequence we define a batch as the task to predict one-by-one the next Nbatch = 40 symbols. A basic SGD optimizer is used during training, with momentum \u03bc = 0.8 and a learning rate \u20ac = 0.02. Our aim is for a testbed that allows to study relative performance, in particular as a function of the complexity of the task, and not to attain optimal performance by fine-tuning meta parameters. If not stated otherwise, results shown are averaged over N = 16 independent runs,\nReadout & Testing The readout token is connected via a dNcon \u00d7 d matrix to the uppermost layer, with the task being to predict the next symbol of a NT time series. For the loss function the basic squared difference is taken, inference is greedy. During training, model performance is evaluated by asking the system to predict Ng = 50 subsequent symbols of Ntest 100 distinct, randomly generated NT series. Once training is finished, larger numbers of Ntest are used for the final evaluation, at least Ntest = 104. The aim is to achieve 100% accuracy.\nLearning strategies For NT tasks, the symbols to be predicted are determined exclusively by two tokens situated at fixed positions from the right end of the input sequence. As a matter of principle, the complexity of the task would independent of N, \u03c4 and Ncon, the context length, if the models tested would focus attention on exactly these two input positions."}, {"title": "Results", "content": "A representative result is presented in Fig. 3, where simulations for N = 16 and r = 2 are shown for both types of attention, EA and DPA. This is a task of moderate complexity, with\n64.56+16-28 + 4 \u00b7 14 + 1 \u00b7 7 + 1\u00b71 = 4096 = 163, (7)\ncompare (6), which means that there are 64/16/4/1/1 periodic patterns of lengths 56/28/14/7/1. Three context lengths are considered, Ncon = 32/52/56.\n100% performance\nEventually, both systems achieve 100% accuracy, predicting correctly 100 successive tokens for 104 random starting N16T2 sequences.\nstructural traps\nLowering Ncon, eventually both algorithms become trapped in a structural local minimum. This is particularly evident for Ncon = 32, as shown in Fig. 3. Both attention mechanism rapidly improve their performance, reaching a plateau of about 55%, which is substantially above baseline. At this point the DPA loss function stops improving, with progress slowing down for EA. The phenomenology seen indicates that the enhanced expressivity of EA allows the system to escape the local trap via a comparatively narrow escape route.\napparent emergence\nAs a function of system size, the DPA performance rapidly increases from about 55% to 100%. This happens between Ncon = 52 and 56, but it would be wrong to interpret this performance jump as an \u2018emergence phenomenon'. In fact, what happens is that the structural local trap disappears eventually when enlarging training space successively, an expected behavior.\ntask complexity\nFor T = 2, only 163 = 4096 distinct sequences exists for N = 16. The task is hence of modest complexity, given that during training Nbatch = 40 shifted sequences are seen for any single epoch. It is hence not surprising that the problem can be solved within 100 epochs for both models when the context length is large enough. When reducing the number adjustable parameters, both models require however larger training compute, when not failing completely. E.g., for Ncon = 16 (not included in Fig. 3), expressive attention still converges to 100% performance, needing however about 2000 epochs.\nThe results presented in Fig. 3 show in addition that training performance is nearly identical for both attention mechanisms when the problem at hand can be solved easily with available resources.\nBinary sequences It has been pointed out that an important aspect for our understanding of transformers is the study of their loss landscape [13, 7]. In this context we further investigate the structural local minima observed in Fig. 3 by comparing the two attention mechanisms for the smallest possible embedding dimension, d = N, which is realized for the binary case N = 2, compare (4). An additional question is here whether EA is advantageous even in this limit.\nWe specialize to r = 5, for which there are just two NT sequences, namely the default state \u201800000', and a cycle of length 63. Together the 63 + 1 = 64 = 26 possible initial conditions are covered, which makes N2T5 a seeming trivial task. For Ncon = 16, and an embedding dimension d = N = 2, overall model size is however modest, Nmodel = 802, which may induce pronounced local minima in the loss landscape. This is indeed the case, as the results presented in Fig. 4 show. Averages over 16 runs have been taken.\nFor DPA one observes extended training spans during which performance fluctuates around the baseline, 50%, essentially without improving. For EA, the equivalent training stage is substantially shorter. The phenomenology observed indicates that learning is dominated by a stochastic escape process [10]. However, further studies would be needed to substantiate this hypothesis. Of relevance for our studies here is the observations that expressive attention may lead to a substantially faster convergence, depending on the task, than the classical dot-product mechanism.\nLarge sequence spaces The number Nseq = N++1 of distinct NT sequences has been modest for the results presented in Figs. 3 and 4. As a consequence, exact solutions in the form of learned one-by-one mappings Nseq \u2192 N are conceivable, at least as a matter of principle. This is however not any more the case for large Nseq.\nIn Fig. 5 we present results for N16T5. Model size is about 4. 105 for Ncon = 128, in terms of adjustable parameters, which is substantially smaller than the number of distinct NT sequences, Nseq = 166 \u2248 16.8. 106. During training, Ntrain = 0.8\u00b7105 training sequences are presented within the first 2000 epochs, which is about 0.5% of Nseq. It is hence likely that the generative mechanism has been encoded in one form or another once performance is either perfect, or near to 100%. As before, we used 5 \u00b7 105 predictions for the evaluation."}, {"title": "Mixture of tasks", "content": "The results presented till now concerned setups containing only a specific single tasks. We performed simulations also for mixture of tasks, finding that expressive attention outperforms classical dot-product attention generically also in this case. We define a new task variant, NT-S,\nNT-S: x(t) = \u2211\u03c4+1\\displaystyle NT-S: x(t) = \u2211\u03c4+1\n\u0394t=1[x(t \u2013 \u0394t)] %\u039d,(8)\nwhich corresponds to summing up the 7+1 preceding token, modulo N. The two versions, NT-S and NT are identical for T = 1, compare (4) and Fig. 2, but not for \u315c > 1.\nResults for Ncon = 32 are given in Fig. 6. During training a 50/50 mixture of N16T2 and N16T2-S autoregressive prediction tasks are presented as such, viz without further information. For a given prompt, which could be either a N16T2 or N16T2-S sequence, the system needs to determine on its own both the delay 7 and the type of the task at hand.\nNT-S tasks typically lead to shorter cyclic patterns than the corresponding NT task, which makes them easier to learn. For example, the mean cycle length is 47.6 for N16T2, but only 23.8 for N16T2-S. Both systems, EA and DPA have consequently no problem to achieve 100% accuracy on N16T2-S as a single tasks. When combined with N16T2, both systems achieve still above 99% accuracy, as shown in Fig. 6. Also of interest is the reduction of the steady-state N16T2 performance in the mixed-task scenario, from about 0.55 to 0.34, which is also the value achieved by DPA.\nIn our setup, expressive attention is able to solve both tasks to about 99%, Given that N16T2 is shown only 50% of the time, it is not surprising that learning is prolonged, as evident from Fig. 6. As an experiment we lowered learning speed after 2500 epochs, finding that N16T2 performance increases a bit, by about 0.5%. Interference between competing tasks is reduced at lower learning rates.\nTasks are acquired consecutively in order of difficulty, as shown in Fig. 6, which can be interpreted as an instances of unsupervised curriculum learning [4]. Our results can be seen also as an explicit example of successive learning of task quanta, as postulated in the quantization theory of neural scaling [24]."}, {"title": "Rare events", "content": "As a third variant, we introduce via\nNT-R: otherwise(9)\nN =RN RN-S if x(t \u2013 \u0442 \u2013 1) = 0\na logical if-statement. The generating algorithm remains deterministic, switching from NT to NT-S when a given condition is fulfilled, here when x(t-T-1) = 0. For an observer this corresponds to a rare event that occurs with a probability of 1/16=0.0625 when N=16.\nDetermining the presence of an isolated hidden logical statement, as defined by (9), can be an exceedingly demanding task. In Fig. 7 results for Ncon = 64/128 are presented. For these two context lengths N16T2 is comparatively easy, as shown in Fig. 3, which implies that a certain level of performance should be attainable in any case. For DPA this level about 0.36 for Ncon = 64, doubling to 0.72 when the context length is raised to 128.\nThe data presented in Fig. 7 demonstrate that it is difficult also for expressive attention to isolate a lone if-statement, as defined by (9). Training progress is slow, achieving in the end however an average accuracy of 98.8/99.9%, respectively for Ncon = 64/128. As usual, averages over 16 random initial conditions have been are taken. The majority of runs achieves however perfect performance. We did not investigate the cause of the non-monotonic events showing up during training, which may be due to periods with an increased clustering of rare events."}, {"title": "Discussion", "content": "Obtaining encouraging results, we presented a first evaluation of expressive attention (EA). Additional investigation would however necessary for a full assessment of the potential of EA, e.g. in the realm of natural language processing [16, 39]. or within the context of formal languages [32, 1]. Generally, we expect that expressive attention will fare at least as well as dot-product attention (DPA), a presumption that is based on the respective design principles. For DPA, large and small attention weights are constrained to a one-dimensional manifold within the space of attention heads, which is not the case for EA. The size of the performance boost obtainable when substituting DPA by EA may depend however strongly on the use case.\nHeuristics Repeatedly we observed that initial performance gains flatten out rapidly. This happens in particular for small model sizes, but also for DPA in cases when the available resources are sufficient for EA to solve the problem at hand exactly. We argued that the resulting stationary performance plateau indicates the presence of a local minimum in loss landscape. From this trap models may escape either by an embedding into into a larger dimensional parameter space, or by an improved design. For all cases the level of the observed stationary performance plateau was independent of initial conditions and training details. This led us to the conclusion that the associated local minimum in loss landscape is structural, viz that it corresponds to a heuristic strategy. To examine how this heuristic strategy works would an interesting research question, which is however beyond the scope of the present study.\nReasoning Inductive reasoning is one of the big challenges of large language models [19, 35]. It is encouraging that one of the building blocks, the logical if-statement, can be extracted and encoded when effective attention is used. Of course, this is just a first indication and it remains to be seen to which extend EA may raise performance in this field."}]}