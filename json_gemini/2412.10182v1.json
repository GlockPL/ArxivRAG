{"title": "Multi-Head Encoding for Extreme Label Classification", "authors": ["Daojun Liang", "Haixia Zhang", "Dongfeng Yuan", "Minggao Zhang"], "abstract": "The number of categories of instances in the real world is normally huge, and each instance may contain multiple labels. To distinguish these massive labels utilizing machine learning, eXtreme Label Classification (XLC) has been established. However, as the number of categories increases, the number of parameters and nonlinear operations in the classifier also rises. This results in a Classifier Computational Overload Problem (CCOP). To address this, we propose a Multi-Head Encoding (MHE) mechanism, which replaces the vanilla classifier with a multi-head classifier. During the training process, MHE decomposes extreme labels into the product of multiple short local labels, with each head trained on these local labels. During testing, the predicted labels can be directly calculated from the local predictions of each head. This reduces the computational load geometrically. Then, according to the characteristics of different XLC tasks, e.g., single-label, multi-label, and model pretraining tasks, three MHE-based implementations, i.e., Multi-Head Product, Multi-Head Cascade, and Multi-Head Sampling, are proposed to more effectively cope with CCOP. Moreover, we theoretically demonstrate that MHE can achieve performance approximately equivalent to that of the vanilla classifier by generalizing the low-rank approximation problem from Frobenius-norm to Cross-Entropy. Experimental results show that the proposed methods achieve state-of-the-art performance while significantly streamlining the training and inference processes of XLC tasks. The source code has been made public at https://github.com/Anoise/MHE.", "sections": [{"title": "INTRODUCTION", "content": "IN the real world, there exist millions of biological species, a myriad of non-living objects, and an immense natural language vocabulary. To distinguish the categories of these massive instances, eXtreme Label Classification (XLC) [1], [2] is required, leading to a dramatic increase in the number of parameters and nonlinear operations in the classifier. This phenomenon, known as the Classifier Computational Overload Problem (CCOP), makes it challenging for existing machine learning methods using One-Hot Encoding (OHE) or multi-label learning algorithms to be practical due to the intractable computational and storage demands.\nCurrently, the primary tasks in XLC include eXtreme Single-Label Classification (XSLC), eXtreme Multi-Label Classification (XMLC), and model pretraining. For XSLC, sampling-based [1], [3], [4] and softmax-based [2], [5], [6] methods are employed to train neural language models, re-ducing the complexity in computing the output. For XMLC, e.g., multi-label text classification, many researchers utilize one-versus-all [7], [8], [9], [10], Hierarchical Label Tree (HLT) [11], [12], [13], [14], [15], Label Clustering (LC) [16], [17], [18], [19] etc., label preprocessing techniques to decompose"}, {"title": "RELATED WORK", "content": "Researches on XLC can be summarized into four categories:\nSampling-based Methods. XLC was first adopted in the field of Natural Language Processing (NLP) to solve out-of-vocabulary problems. There are two ways to achieve this. One is to adopt importance sampling to calculate the softmax, and the other is first performing label encoding and then approximating the softmax. The sampling-based methods [1], [3], [4], [22] do not change the structure of the classifier but use the proposal distribution to sample nega-tive instances to train the model. However, due to the large difference between the proposal and the real distributions, the number of instances in the training process increases rapidly, and its computational cost quickly exceeds that of the original model. For the face recognition task, Partial-FC proposed in [21] uses a distributed sampling algorithm, which randomly samples a fixed proportion of instances samples to calculate the softmax probability. However, this sampling method is limited by the sampling rate, which greatly limits its applicability.\nSoftmax-based Methods. Research on softmax approx-imation in NLP includes Hierarchical Softmax (H-Softmax) [2], D-Softmax [5], CNN-Softmax [6], etc. These methods first rank the frequency of word occurrences and then en-code the vocabulary through BytePair [23], WordPiece [24], and SentencePiece [25] to reduce computational complexity. In model pretraining, the authors in [20] use a hash forest to approximate the selected activation categories for face recognition. This method recursively divides the label space into multiple subspaces and selects the maximum activa-tions as the decision result. These methods either require statistical analysis of the vocabulary or only accelerate the training process of the model.\nOne-Versus-All Methods. One-versus-all methods di-vide the extreme label space into multiple easy-to-handle subspaces and only process part of the label space during the training process. For example, some works [7], [8], [9], [10] use one-versus-all methods to transform the XMLC problem into a binary classification problem. However, one-versus-all can only accelerate the training process of the model, and the complexity of the inference process is still linear in relation to the label space. There are also many studies using tree-based methods for label partitioning. For example, several works [11], [12], [13], [14], [15] use a loga-rithmic depth b-array HLT. However, they involve complex optimizations at node splitting, making it difficult to obtain cost-effective and scalable tree structures [26].\nLabel Clustering Methods. For XMLC tasks, the cluster labels are first obtained via semantic embedding and a surrogate loss, and then the extreme labels are fine-classified within the clusters. Some works [16], [17], [18], [19] use the probabilistic label tree for label clustering, an HLT-based bag-of-words method. Another work [27] introduces a graph autoencoder to encode the semantic dependencies among labels into semantic label embeddings. The key to these methods lies in label clustering, an essential yet ex-ceedingly complex and time-consuming preprocessing step."}, {"title": "MULTI-HEAD ENCODING (MHE)", "content": "For the sake of denotation simplicity, the variables are denoted using different fonts: 1) Constants are denoted by capital letters, e.g., Y, is a constant representing the category of the i-th sample, and C represents the total number of categories. 2) Vectors are denoted by bold capital letters, e.g., Y is the label set of Y\u00bf, and O is the output of the head. 3) Matrices are denoted by calligraphy capital letters, e.g., W represents the weight of the classifier, and F is the matrix of feature sets F. 4) Tensors are also denoted by calligraphy capital letters with the superscript indicating their order, e.g., y1,..., H represents an H-order tensor."}, {"title": "Classifier Computational Overload Problem", "content": "The deep neural network considered in this work is as shown in Fig. 1, which is composed of three parts: input, backbone, and classifier. Assume that the given sample-label pairs are {Xi, Yi}1, where X \u2208 RIXXN and Y\u2208 RC\u00d71 are the sample set and label set, respectively. Let Y\u2081 denote the encoded (vectorized) label of Y\u2081. The backbone mainly includes multi-layer nonlinear neurons, denoted as Net. The feature output from Net is denoted as F, and the part that projects F into RC\u00d71 through weight W is the classification head. The loss between output O and Y is denoted as L. Thus, the forward process of the network with single label can be formulated as\nF\u2081 = Net(Xi), (1)\nO\u2081 = WF + B, (2)\nN\nL = - \u03a3\u03ablog(\u03c3 (\u039f\u0390)), (3)\ni=1\nwhere o is the softmax function, and B \u2208 RC\u00d71 is the bias of the output layer. For XLC tasks, the length of O in Eq. 2 must be equal to C, resulting in the Classifier Computational Overload Problem (CCOP) since |O|\u226b |F| 1 leads to computation being overweight."}, {"title": "Label Decomposition", "content": "Label decomposition in MHE involves decomposing the ex-treme label into multiple easy-to-handle local labels, which are then used to train neural networks. To better understand this process, we can conceptualize the extreme label Y\u00bf as a point in high-dimensional space. Then, its orthogonal coordinate components are used as local labels to train the neural network with multi-heads. This process reduces the encoding length of the labels by reusing coordinate posi-tions, thereby geometrically decreasing the computational load of the classifier.\nThe key to the label decomposition process is how to map Y\u00bf into an H-dimensional space. The solution proposed in this paper is to view Y\u00bf as a one-hot encoded vector Iy, as shown in Fig. 2a. Then it is reshaped into an H-dimensional tensor, Y1..., H. Note that since Iy is a one-hot vector,\u2026\u2026\u2026\u2026 and its components, {\u0178h}H=1, are all one-hot encoded. Thus, the decomposition process of Y\u00bf can be formulated as\nIy\u2081 = YY \u2026\u2026\u2026 YH, (4)\nwhere is the Kronecker product. Please refer to Appendix F.1 for detailed examples. Equation 4 means that the extreme label is decomposed into the product of H short one-hot vectors, each approximately of length VC. Therefore, assigning each local label to each head to train the network will geometrically reduce the number of parameters in the classifier, thus solving the CC\u041e\u0420."}, {"title": "Multi-Head Combination", "content": "The previous subsection showed how to decompose ex-treme labels into multiple short local labels to train the model. This subsection shows how to combine the outputs of the multi-heads to recover the global predicted label Y (original extreme label) during testing.\nIn fact, the combination operation used during testing is the inverse of the decomposition operation used during training. As shown in Fig. 2b, if the output On of each head is viewed as a coordinate component, {Oh}h=1 can be producted to form the coordinates of a point in the H-dimensional space. Then, the predicted label Y\u00bf is obtained by projecting this point onto the integer axis as\n\u0178\u00bf\u00a1 =A(\u00d5\u00bf) = \u039b(O \u00ae O \u00ae \u2026 \u2297 OH), (5)\nwhere A is the Argmax operation. Although Y can be ob-tained by Eq. 5, the overwhelming length of \u00d5\u00bf (\u00d5\u00bf| = C) and H-1 Kronecker product operations will consume huge"}, {"title": "IMPLEMENTATIONS OF MHE", "content": "Now, we consider concretizing Eq. 7 to render MHE appli-cable to various XLC tasks. Specifically, MHP is applied to XSLC to o achieve multi-head parallel acceleration. MHC is used in XMLC to prevent confusion among multiple categories, and MHS is applied during model pre-training to efficiently extract features, as this task does not require a classifier. Then, we provide a strategy to determine the number and length of the heads."}, {"title": "Multi-Head Product (MHP)", "content": "According to Corollary 1, the output can be decomposed into the product of the heads, which paves the way for using MHP instead of the vanilla classifier to train the model.\nAs shown in Fig. 3a, during training, the global label Y\u2081 needs to be assigned to each head to calculate the loss locally. Thus, we first perform OHE on Y\u2081, then reshape it into an H-order tensor ..., according to the length of the heads. Finally, ... is decomposed into local labels {Y}H_\u2081 on each head. Since the decomposition of one-hot encoded Y\u2081 depends solely on the number and order of the heads, it can be recursively calculated as\n{Y-1,\nYh = ( Y\nYi,\n\u03a0j+1|0j|\n)/\u03a0k=1|0k|,\nh-1 hHh=12<h<H-1\u03a3\u03a5-\u03a3+1|0|h = H,(10)\nwhere j and k are the indexes of the classification heads.\nDuring testing, the global prediction must be recovered from the local predictions. As shown in Fig. 3a, we first per-form I^ on each head to obtain the locally predicted label. Then, the global prediction Y\u2081 is obtained by performing the product on each head and applying Argmax on the final outputs. To speed up this process, according to Theorem 1, we calculate Y\u00bf from the local predictions and the length of the subsequent heads, as\nH-1\nH\n\u040e\u2081 = \u03a3 \u039b(\u039f) \u03a0 |03|+ \u039b(\u041e\u041d). (11)\nk=1 j=k+1\nThe Algorithm for MHP is given in Appendix E.1. It can be used in many XSLC tasks, such as image classification, face recognition, etc."}, {"title": "Multi-Head Cascade (MHC)", "content": "For XMLC, each sample X corresponds to multiple labels \u0178\u00bf \u2208 {0, 1}, so the outputs of the classifier need to perform multi-hot encoding and Top-K selection as Y\u2081 = Top-K(O). MHP cannot be adopted directly in XMLC. This arises from the fact that each head in MHP predicts only a single label. If utilized for multi-label predictions, it will result in a mismatch when computing the product of the local predictions. To address this mismatch of MHP in multi-label scenarios, MHC is proposed, which cascades multiple heads for model training and testing.\nAs shown in Fig. 3b, during training, the label decompo-sition process of MHC is the same as that of MHP. During testing, the top K activations of the outputs are selected. Then, the local predictions of this head are obtained through a predefined candidate set C\u00b9, which is adopted to represent the label set of the subsequent heads to facilitate retrieval and reduce computation. The final outputs \u00d5h of the h-th head are obtained by the product of embedded \u0176h-1 and current output Oh. Then, Yh is selected from Ch according to the top K activations of \u00d5h. This process is repeated until the labels of YH are obtained as\n{{Oh = Ohh Eh (\u0176h-1),Y1 = C[1,\u2026,in+1](1,1,102]) [Top-K((O1))].Yh = C[1,\u2026,in+1](in,Oh+11)[Yh-1[Top-K((\u00d5h))]],Yh = \u0176h-1[Top-K((\u00d5h))],h=12<h<H2<h<Hh = H,(12)\nwhere in = \u03a0=1 |O|, Eh is the embedding layer of the h-th head, and (in, | Oh+1|) is the index matrix with elements from 1 to in+1 and shape (in, Oh+1). Eq. 12 shows that MHC is a coarse-to-fine hierarchical prediction method, which sequentially selects Top-K candidate labels from the previous head. Note that MHC only depends on Eq. 10 for label decomposition and does not require preprocessing techniques like HLT or label clustering. The Algorithm for MHC is given in Appendix E.2."}, {"title": "Multi-Head Sampling (MHS)", "content": "For the model pretraining tasks, the vanilla classifier is discarded when the training is completed, and only the features F extracted by the model are adopted for finetun-ing on downstream tasks. Therefore, all parameters of the weights in the classifier should be trained to extract more discriminative features, but training all parameters of the weights is computationally expensive. Therefore, MHS is proposed to update the model parameters by selecting the heads where ground truth labels are located.\nAs shown in Fig. 3c, MHS divides the vanilla classifier into H groups equally, so that O =\u03a3OH. During training, the head where label Y\u00bf is located is selected for model training, which is called the positive head. Certainly, we can also randomly select several negative heads to train the model together, thereby enriching the model with more negative sample information. The forward process of MHS for Oh can be formulated as\nOh = Oh \u222a {O\u00cf} = WhF\u222a {Wi}F, (13a)\nYh = Yh \u222a {0} = Y[|Oh\u22121| : |Oh|] \u222a {0}, (13b)\nwhere {0} and {Wi} indicate the outputs and weights set of the negative heads, respectively, and U represents the concatenation operation. Eq. 13b indicates that Yh is padded with Os to align the length of Oh, where |Oh| = 0 when h = 0.\nThe method in Eq. 13 can be denoted as MHS-S, where S is the number of selected heads. Our experiments show that MHS-1 (only the positive head) achieves quite good perfor-mance on model pretraining. For S = 2, MHS approximates or outperforms the vanilla classifier. To speed up MHS, the heads containing the other sample labels in the same batch are selected as the negative heads. The Algorithm for MHS is given in Appendix E.3."}, {"title": "Label Decomposition Principle", "content": "Thus far, we have introduced three MHE algorithms, the im-plementation of which is contingent upon both the number and the length of the heads. Therefore, in this subsection, we introduce the concepts of error accumulation and confusion degree to measure the impact of the number and length of the heads on the performance of the MHE-based algorithms.\nThe number of heads: The approximation process of MHE with H heads can be expressed as\nO \u2248 0\u00b9 \u00ae \u00d5\u00b2 \u2248 O\u00b9 \u00ae O2 & 03...\n\u2248\u00d5H-1OH\u22480,(14)\nAs shown in Eq. 14, adding a head is equivalent to ac-cumulating one more time error. Although increasing the number of heads will significantly reduce the parameters and calculations of the classifier, it will also cause greater cumulative errors. Thus, as long as the computing resources and running speed permit, the number of classification heads should be minimized.\nThe length of heads: The confusion degree is a measure of mismatches caused by shared components when MHE is adopted to approximate the original label space. It is proportional to the approximation error as\nH\nH\nD = max\u03c0(01,\u2026,\u039f\u0397)( \u03a0 \nOh\\k=h|Ok\u22121|1) (H \u2265 2), (15)\nh=2\nk=hwhere is an arrangement strategy of the heads. The value of D is expected to be as small as possible. Since \u03c0 relies on the specific decomposition process, we analyze the detailed confusion degrees for different algorithms of MHE.\nFor MHP, the confusion degree is irrelevant to the ar-rangement of the heads because they are parallel and need to be combined. That is, max in Eq. 15 can be removed when the length of the heads is in ascending order. Therefore, we conclude that the length of each head in MHP should be as consistent as possible to minimize D, i.e., |Oh| \u2248\u221aC.\nFor MHC, since heads are sequentially cascaded, we can choose a better strategy \u03c0 to minimize D. Obviously, when \u03c0 is in descending order (|01| > \u2026 > OH), D is minimized.\nFor MHS, those multiple heads are interrelated and need to be combined (irrelevant to the max operation). That is, we can choose the same strategy as for MHC to minimize D."}, {"title": "REPRESENTATION ABILITY OF MHE", "content": "As Corollary 1 proves, the essence of MHE is a low-rank ap-proximation method that approximates high-order extreme labels through the product of multiple first-order heads. Thus, one might inquire: Does MHE guarantee sufficiently robust performance in classification problems?\nTo answer the above question, we generalize MHE to a more general low-rank approximation problem, extend-ing from the Frobenius-norm metric to Cross-Entropy. As shown in Fig. 4a, if there are G groups of multi-heads with MHE, each of which forms an H-order tensor. Then, all these tensors are added to obtain the final output\nG\n\u2248 (0) = (\u2026\u2026\u2026)Iy;\ng=1 g\nG ( \u03a3g (WF) (WF) (WF) (WF)\u2026\u2026 (WF)),(16b)gHg = \u03c3= \u03c3(Iy; (16a)\nwhere g is the index of the groups. In fact, Eq. 16 is the CP decomposition of a tensor, with G being its rank. This factorizes the tensor into a sum of component rank-one tensors. Theoretically, other tensor decomposition methods [28], [29] can also be used to approximate O when it is viewed as a vectorized high-order tensor."}, {"title": "Low-Rank Approximation with Frobenius-norm", "content": "Equation 16 illustrates that the low-rank approximation of the output \u014c is essentially to restrict the rank of its weights. Therefore, we study the impact of low-rank weights on the performance of the classifier. To constrain the rank of W, a linear bottleneck layer Ob is added between the feature layer F and the output layer O, as shown in Fig. 4b. Let the weight between F and Ob be denoted by W\u2081, and the weight between Ob and O be denoted by W2. We have\nO = W2W1F + B = WF + B, (17)\ns.t. R(W) = r < min(|F|, |Or|),\nwhere W = W2W\u2081 and R(\u00b7) is the rank of a matrix. If \u0174 is optimized by the Frobenius-norm loss, we have\n1\nmin L = =||IY - O||} = =||Iy; \u2013 WF||}. (18)\nEq. 18 is a low-rank approximation problem [30], [31] that makes all elements of Iy and O as close as possible. It yields"}, {"title": "Low-Rank Approximation with CE", "content": "Further, if the loss of the low-rank approximation in Eq. 17 is generalized from the Frobenius-norm to CE with softmax, we will get a better approximation of Iy\u2081 . This is because the Frobenius-norm metric in Eq. 17 is too strict for the classi-fication problem [32], i.e., the Frobenius-norm loss tends to approximate all elements, while the CE loss tends to select the largest element. Therefore, the low-rank approximation in Eq. 17 needs to be generalized to the CE loss, which is commonly used but rarely studied in the classification problem.\nDifferent from the Frobenius-norm used in Eq. 17, the nonlinear operation on the outputs will affect their repre-sentation ability. This is because the softmax (training) and non-differentiable Argmax (testing) can be approximated by\n\u039b(\u039f) = lim \u039b(\u03c3\u03b5 (\u039f\u0390)) = lim A(\ne^(Oi/e)\n,(19a)\nwhere e is the temperature of the softmax. Equation 19 shows that the Argmax operation used in testing is actually consistent with the softmax and CE operations used in training. That is, Eq. 19 is equivalent to CE with softmax, where softmax makes the gap among elements larger and CE selects the largest element. Therefore, we generalize the low-rank approximation problem from the Frobenius-norm loss to the CE loss in the following theorem."}, {"title": "EXPERIMENTS", "content": "Intensive experiments are conducted on XSLC, XMLC, and model pretraining to fully validate the effectiveness of the three proposed MHE-based algorithms to cope with CCOP."}, {"title": "MHE-based Algorithms for XSLC", "content": "To better illustrate the superiority of MHE on XLC tasks, we conduct experiments on the VOC2007 [35] and COCO2014 [36] datasets. The two datasets are well-known multi-label datasets. We transform their multi-labels into label power-sets to build an XLC task. The label space of the VOC2007 dataset is first transformed into the label powerset, whose"}, {"title": "MHC for XMLC", "content": "For XMLC, MHC is validated on six different public bench-marking datasets, whose statistical information are shown in Table 11 of Appendix G.2. Precision@K (P@K) is utilized as the evaluation metric, which is widely adopted in XMLC. For all the experiments, AdamW [37] with a learning rate of 1e-4 is utilized as the optimizer for model training. The model ensemble, dropout, [38] and Stochastic Weight Aver-aging (SWA) [39] techniques are adopted in many XMLC approaches recently. For example, in AttentionXML [17], the authors propose to use three model ensembles and SWA to enhance performance. The following works [18], [19] all adopt this setup, e.g., X-Transformer [18] adopts three SOTA pretrained Transformer-large-cased models to fine-tune, including Bert [40], Roberta [41], and XLNet [42]. LightXML [19] also adopts these pretrained Transformer-based models for ensemble and uses SWA to alleviate over-fitting. Thus, the proposed MHC adopts the same settings for fair comparisons. The other experimental setups can be found in Appendix G.2.\nFor comparison purposes, 11 SOTA methods are adopted as baselines. Comparisons are done on the six public multi-label text datasets. The performances of MHC are shown in Tables 2 and 3, which show that MHC achieves ad-vanced performance in terms of different metrics on dif-ferent datasets (as highlighted). The results based on model"}, {"title": "MHS for Model Pretraining", "content": "For model preprocessing tasks that do not require the classi-fication head for inference, we can use MHS to pretrain the model on a large dataset for better validation performance. The performance of model pretraining using MHS is tested on three different face recognition datasets, e.g., CASIA [47], MS1MV2 [44], [48], and MS1MV3 [48], [49]. In the experi-ments, Arcface [44] is adopted as the loss function, and the model is optimized using SGD with the Poly scheduler. All experiments are trained for 25 epochs using ResNet-18 [34] or ResNet-101 [34] as the backbone network. The remaining experimental settings are included in Appendix G.3. The experimental results are shown in Table 5."}, {"title": "Scalability of MHE", "content": "To demonstrate that MHE can be easily extended to other XLC tasks, we verify the scalability of MHE on the Neu-ral Machine Translation (NMT) tasks. For NMT tasks, the classifier needs to represent and predict the probabilities of all tokens. In this subsection, we explore the possibility of using the MHC and MHS algorithms to replace the vanilla classifier. The OPUS-MT [57] model is utilized to perform"}, {"title": "Time and Memory Consumptions", "content": "We expanded the experimental comparisons to include val-idation of runtime and memory consumption, which are"}, {"title": "Ablation Studies of Label Decomposition Methods", "content": "To further validate the conclusion implied in Theorem 3, which states that the model generalization becomes irrele-vant to the semantics of the labels when they overfit the data, we conduct ablation studies of label decomposition on model generalization. It is known that the core of the preprocessing techniques is to perform semantic clustering on extreme labels and divide them into several tractable local labels. Thus, we compare the performance of models utilizing label clustering (LC) with those employing label random rearrangement and arbitrary decomposition (LRD).\nAs shown in Fig. 8, label decomposition can be conceptu-alized as a multi-stage classification procedure, i.e., a given feature is initially assigned to a cluster, followed by the iden-tification of a specific category within that cluster. As shown in Fig. 8.a, preprocessing techniques can facilitate the initial"}, {"title": "Impact of Label Decomposition on Generalization", "content": "Furthermore, we compare three models of varying complex-ity to evaluate their generalization when configured using a carefully designed LC and a random LRD approach. As shown in Fig. 9, when the low-complexity models underfit the data, there is a clear performance gap between LRD and LC: about 4% in the small model (Fig. 9.a) and 2% in the medium model (Fig. 9.b). This is due to the fact that a hierarchical classifier's performance in subsequent stages relies on the decision results made in earlier stages, especially when features extracted by low-complexity mod-els exhibit reduced distinguishability. This explains why the model with LRD falls behind the model with LC in situations involving low complexity. However, this perfor-mance gap gradually diminishes as the model's complexity increases. Eventually, when the model overfits the data, as shown in Fig. 9.c, the gap between LRD and LC vanishes. It is noteworthy that the model's over-parameterization is"}, {"title": "DISCUSSION", "content": "Here, we discuss the innovativeness of MHE by elucidating the distinctions between it and other methodologies that adopt multiple classifiers.\nSeveral recent methods [59], [60] utilize multiple classi-fiers to address the long-tailed distribution problem. Specif-ically, the authors in [59], [60] split the dataset into balanced subsets and train an expert model on each subset. Then, multiple expert models (one model on one subset) are ag-gregated to obtain the final model, as shown in Fig. 10a. The long-tail methods are not applicable to solve CCOP, because the parameters of the classifier in the aggregated model have not been reduced. Different form methods mentioned above, as shown in Fig. 10(e-g), the proposed MHE-based algorithms can solve CCOP well by decomposing the hard-to-solve extreme labels into multiple easy-to-solve local labels, and combining local labels to obtain extreme labels through simple calculations.\nThere are many tree-based methods [2], [11], [12] using multiple classifiers for XLC tasks. These methods partition the label space through hierarchical branches. For example, Hierarchical softmax [2], [22] adopts a Huffman tree to en-code high-frequency words with short branches, as shown in Fig. 10a. However, the huge label space greatly increases the depth and size of the tree, requiring traverse a deep path for low-frequency samples, making it unsuitable for XMLC tasks. Motivated by this, some HLT-based methods [11], [12] have been proposed, but they involve complex optimiza-tions at node splitting, making it difficult to obtain cheap and scalable tree structures [26]. On the contrary, MHE-based algorithms have no preprocessing steps. Therefore, the length of the classifier can be divided arbitrarily as long as the label space is fully mapped."}, {"title": "CONCLUSION", "content": "In this paper, we propose a Multi-Head Encoding (MHE) mechanism to cope with the challenge of CCOP that exists in XLC tasks. MHE decomposes the extreme label into the product of multiple short local labels, and each head is trained and tested on these local labels, thus reduc-ing the computational load geometrically. For XLC tasks, e.g., XSLC, XMLC and model pre-training, three MHE-based algorithms are designed, including Multi-Head Prod-uct (MHP), Multi-Head Cascade (MHC), and Multi-Head Sampling (MHS). Experimental results show that the three MHE-based algorithms have achieved SOTA performance in the tasks to which they are applied and can greatly speed up model training and inference. Furthermore, we conduct a theoretical analysis of the representation ability of MHE. It is proved that the performance gap between OHE and MHE is considerably small, and the label preprocessing techniques are unnecessary.\nWe believe that XLC is a natural extension of the tra-ditional classification tasks, which allows us to deal with extreme labels, and is much more suitable for the real-world samples and practical applications. In turn, MHE-based algorithms designed for XLC can bring more novel solutions to many traditional tasks. For example, we can transform the regression task into an XLC task and use MHE-based al-gorithms to solve it. In reinforcement learning, MHE-based"}, {"title": "APPENDIX A", "content": "A.1 Proof of Theorem 1\nProof. Substituting Eq. 6 into Eq. 8 and replacing A with IA, we get\n\u0399\u03bb(\u00d5) = \u0399\u039b(\u039f1)\u0399\u039b(02)\u2026\u0399\u039b(\u039f\u0397). (21)\nHere, we first consider the case of H = 2 in Eq. 21, and then generalize it to the case of H > 2.\n(a) H = 2. Assume that the largest elements in \u00d5, O1 and O2 are o\u00bf, of and of, respectively, there must have\nOi = ohoh, (22)\ns.t. oi = max(\u00d5),\noh = max(O1),\noh = max(O2).\nIf Eq. 22 holds, it is implied that Eq. 21 also holds when H = 2. Now we prove that Eq. 22 must hold.\n1) If oh = max(O1) and oh = max(O2), there must be oi = max(\u00d5) such that Eq. 22 holds.\nIf there is another element oj < o\u00bf in \u00d5 that satisfies oj = ohoh, then there exist two other elements oh, \u2208 O1, oh, \u2208 O2 that make oj = ohoh, hold. Since oh, \u2264 oh and oh, \u2264 oh, then oh \u2265 oh must hold, which contradicts the assumption oj < Oi\n2) If oi = max(\u00d5), there must be oh = max(O1) and oh = max(O2) such that Eq. 22 holds.\nIf there is another element oh, < o in O1 that satisfies oi = oh, oh, then there exists another element oj \u2208 \u00d5 that make oj = oh hold. Since oh \u2264 oh, then oj \u2265 oh must hold, which contradicts the assumption oi = max(\u00d5). Similarly, this result also holds for O2.\n(b) H > 2. Using the associative property of the Kro-necker product, Eq. 22 is equivalent to\n\u0399\u03bb(\u1f45) = \u0399\u03bb(0) \u00ae (IA(Oh\u22121) (... (I\u028c(02) \u2297 \u0406\u043b(01)))). (23)\nEq. 23 shows that O1 and O2 can be treated as a new vector for Kronecker product with O3, and the maximum value of the new vector can be guaranteed by Eq. 22. This process continues until OH is exploited, which means that Eq. 8 is hold.\nA.2 Proof of Corollary 1\nProof. Replacing A with I^ yields\n\u03a0\n\u0399\u039b(\u039f) = \u0399\u039b(\u039f1) \u2297 I\u028c(O2) \u2297 \u2022\u2022\u2022 \u2297 \u0399\u039b(\u039f\u0397). (24)\nSimilar to Theorem 1, we first consider the case of H = 2 in Eq. 24, and then generalize it to the case of H > 2.\n(a) H = 2. Assume that the largest element in O is oi, there is an element oj \u2208 O,j \u2260 i, satisfing that Oi > 0j. Let oh, oh be any two elements of O1 and O2, respectively, when H = 2, according to condition O \u2248 O1 \\O2, we have\noi = ohoh + i, (25)\ns.t. oh = max(O1),\noh = max(O2),\nwhere i is the"}]}