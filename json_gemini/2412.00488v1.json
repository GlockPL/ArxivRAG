{"title": "Improved Cleanup and Decoding of Fractional Power Encodings", "authors": ["Alicia Bremer", "Jeff Orchard"], "abstract": "High-dimensional vectors have been proposed as a neural method for representing information in the brain using Vector Symbolic Algebras (VSAs). While previous work has explored decoding and cleaning up these vectors under the noise that arises during computation, existing methods are limited. Cleanup methods are essential for robust computation within a VSA. However, cleanup methods for continuous-value encodings are not as effective. In this paper, we present an iterative optimization method to decode and clean up Fourier Holographic Reduced Representation (FHRR) vectors that are encoding continuous values. We combine composite likelihood estimation (CLE) and maximum likelihood estimation (MLE) to ensure convergence to the global optimum. We also demonstrate that this method can effectively decode FHRR vectors under different noise conditions, and show that it outperforms existing methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Vector Symbolic Algebras (VSAs), also known as hyper- dimensional computing, offer a way to represent symbols as high-dimensional vectors. These symbol vectors can then be combined to form compound symbols, which can further be combined with other symbols, or decomposed back into their constituent vectors. For example, we could have a vector T that represents a \"triangle\", and another vector R that represents \"red\", and combine those two into another vector that represents \"a red triangle\". In this way, VSAs enable a compositional language within the substrate of a single vector space, and have been proposed as a way to perform computations in the brain [1], [2], [3], including a solution to the binding problem [4].\nThe basic functions of a VSA are carried out by a small set of vector operations: similarity, binding, bundling, and clean- up. The similarity operation,, takes two vectors and computes how similar they are, yielding a value of 1 if the vectors are identical, and 0 if they are completely different. We would expect the value of R T to be close to zero. The binding operation,, combines two vectors and creates a composite vector that is dissimilar to each of its inputs. For example, the concept of \"a red triangle\u201d is represented by the vector R T. Importantly, binding is invertible (using the operator ) so that the resulting vector can be factored back into its composite vectors. The operation of bundling, \u2295, also combines vectors; but unlike binding, bundling combines vectors in a way that the resulting vector is still similar to its component vectors. In our example, the vector R T is a vector that represents the collection concepts of \u201cred\u201d and \u201ctriangle\u201d, but not \u201ca red triangle\u201d.\nHence, given a vocabulary of concepts and their correspond- ing vectors, many different propositions can be constructed, manipulated, and queried. For example, we could construct the vector v = R \u25ca T, and then query it by asking about the properties of the triangle using v \u25ca T, which would result in a vector similar to R. These VSA operations can introduce noise, which can reduce the fidelity of subsequent operations. For this reason, many VSA implementations have a clean-up operation that takes a noisy or corrupted vector and restores it to the nearest recognized symbol (vector) in the vocabulary.\nSome VSAs also have a facility to encode continuous values, not just discrete symbols. For example, we can encode the value 1.5 into the vector X. Such vectors are often called Spatial Semantic Pointers, or SSPs, since these vectors can encode spatial location [5]. Then the composite vector u = v X could represent \"a red triangle at location 1.5\". That vector could be queried, asking where the red triangle is using uv. Ideally, the resulting vector would be X. But in practice, the vector is corrupted by the sequence of VSA operations. Unfortunately, there is currently no clean-up mechanism for SSPs, since the uncorrupted vectors occupy a manifold in the vector space, not just a single point.\nWe could perform clean-up if we could robustly decode (and re-encode) the value encoded in an SSP. However, even though the algorithm to encode a value into an SSP is relatively straight-forward, the inverse is not so simple, and there is currently no closed-form formula for decoding.\nSome attempts have been made to solve the decoding/clean- up problem for SSPs. Some methods perform a grid search, comparing the noisy SSP to a large number of clean SSPs via the similarity operation, and picking the clean hypervector that is the most similar [6]. Another approach is to train a neural network as a denoising autoencoder to do the clean-up [7], [8]; such networks have also been demonstrated using spiking neurons [9].\nA related problem is how to parse a composite hypervector into its constituent parts. This is a challenging task, since each binding affects the vector, and decoding any one vector depends on knowing the composition of the remaining com- ponents. A resonator network starts with a guess for each of the components, uses those guesses to try to factor out each component individually, performs a grid search for each component, and then reconstitutes the vector based on the best fit for each component [10], [11]. Least squares or gradient- based optimization methods can also be used to minimize a loss function between the given vector and the estimates for"}, {"title": "II. METHODS", "content": "The goal is to find a clean SSP that maximizes its similarity to the noisy SSP. However, the similarity function often has many local optima, so it can be difficult to ensure convergence to the global optimum. Is there some way to rule out sub- optimal solutions?\nOur idea is to regularize the optimization process using the relationships between the vector components. These couplings can guide the optimization process toward a global optimum.\nThis approach was inspired by oscillatory-inference (OI) models for path integration. In path integration, an animal esti- mates its location by integrating its own velocity. Hypothesized velocity-controlled oscillators (VCOs) vary their frequency according to the animal's velocity [14], and thereby encode position as phase differences. Any error in the oscillations will cause their phases to drift, and reduce the fidelity of the encoding. However, phase-coupling mechanisms can help reduce the drift [15], [16], [17]. These OI models of path integration are compatible with the SSP encoding of space, since each VCO can be thought of as an element of an SSP vector [18].\nWe will focus on a VSA called the Fourier Holographic Reduced Representation (FHRR) [19]; the FHRR is essentially the Fourier transform of another VSA called the Holographic Reduced Representation (HRR). These VSAs can efficiently store large sets of symbols, comparing favourably to other VSAs [20]. They can also produce SSPs using Fractional Power Encoding (FPE) [21], [22], or fractional binding [5].\nThe entries of FHRR vectors are complex numbers. We specifically focus on unitary vectors, where the modulus of each complex number is 1. We will refer to the set of unit- modulus complex numbers a C1. Thus, each element of C1 can be fully specified by a phase angle in the range (-\u03c0, \u03c0]. Given the vector of phases a = [a1,...,an]T, we can write the kth element as eiak, or the whole vector as eia, where the exponent is applied element-wise to a. We will refer to the vector of phases (a, in this case) as the base phases.\nThe similarity operation for the FHRR is the complex inner product. For u, v \u2208 C,\n$\\langle u, v \\rangle = \\sum_{k=1}^{n} u_k V_k,$\nwhere Vk is the complex conjugate of vk. For unitary vectors, the inner product is equivalent to computing cosine similarity. The binding operation,, for FHRR vectors in Cr is the Hadamard product, or element-wise multiplication, which can be written eia & ei\u00df = ei(\u03b1+\u03b2).\nThe bundling operation,, for FHRR is simple vector addition. It is worth noting that such a bundle is usually not unitary, so has to be normalized to convert it back into a unitary vector. Bundles of SSPs have been used to represent functions, including probability densities [23].\nThe FPE or fractional binding operation is a generalization of binding, where an FHRR vector can be bound to itself. Since binding is element-wise multiplication, we can bind an FHRR vector to itself using\n$e^{i \\alpha} \u25ca e^{i \\alpha} \u25ca ... e^{i \\alpha} = e^{i \\alpha m}$\n(1)\nwhere m \u2208 Z. In FPE, the value of m can be any real value, so we can encode x \u2208 R using the SSP elax. Representing a vector of continuous values [x1,...,xa]T is accomplished by encoding each element into an SSP (each using a different vector of n base phases) and then binding all those SSPs into a single SSP. If we use ak to represent the vector of base phases for encoding xk, then the multi-dimensional SSP is eia1x1 ... eiadxd = e\u03af\u0391\u0390, where A is a matrix with columns ak, k = 1, . . ., d.\nSpatial semantic pointers can be corrupted, especially when bundling vectors. The fact that hypervectors are only approx- imately orthogonal means that they will interfere with each other in a bundle. For example, when a bundle is queried using the unbinding operation, the lack of perfect orthogonality means that each vector in the bundle leaves a small residual. Normalizing a bundle introduces even more corruption. Suppose we have a bundle composed of various objects, each bound with an SSP that encodes its location. This bundle represents a spatial map that can be used for navigation [24]. We can query that bundle either by location vector or by object vector [6], [5]. As an example, consider the normalized bundle\nM = TX1.5 Y-2.3 + S X-0.7 Y0.3 (2)\nwhere T and S are FHRR vectors representing \"triangle\" and \u201csquare\u201d, and X and Y are the SSPs representing the x- and y-axis. The bundle M represents a set with a triangle at (1.5, -2.3) and a square at (-0.7, 0.3).\nWe can query for the location of the triangle using the un- binding operation, M\u00d8T, yielding a noisy SSP approximately representing the location (1.5, -2.3).\nIf this process is repeated, that is, the noisy SSP is encoded into another bundle and queried again, the resulting SSP may no longer represent the location (1.5, -2.3). The left- hand column of Fig. 1 shows how the phases get corrupted after repeated bundling/querying episodes, where the bundles included 13 vectors, all added together and then normalized. The right-hand column plots the similarity of the corrupted"}, {"title": "C. Least Circular Distance Regression on SSP Phases", "content": "Suppose we have an SSP vector, u, encoding z. Since u is unitary, we can write it as\nu = e^{i \\phi} \\approx e^{i A x}\n(3)\nwhere is the vector of phases observed in u. The task of decoding u is to find 7 that maximizes the real part of the similarity between the complex vectors u and eiAz,\n$S(z) = Re \\langle u e^{-i A x} \\rangle = Re \\langle e^{i(\\phi - A x)} \\rangle.$\n(4)\nThe similarity would be maximized if equaled Az. So it might seem sensible to try to choose z to minimize the phase error as a least-squares problem,\n$\\underset{z}{\\text{argmin }} || \\phi - A x ||^2 .$\nHowever, the complex exponential function is periodic, so that a phase of -\u03c0 is the same as a phase of \u03c0. The phases we extract from u will be wrapped to the range (-\u03c0, \u03c0], while the phases Az are not wrapped. Unfortunately, phase wrapping makes it difficult to compare and Az directly. Figure 2 demonstrates how linear least-squares regression fails due to phase wrapping.\nWe could unwrap u by adding or subtracting multiples of 2\u03c0, and then try to solve the unwrapped least-squares problem,\n$\\underset{x, k \\in Z^n}{\\text{argmin }} || \\phi + 2 k - A x ||^2 .$\n(5)\nBut that requires we jointly solve an integer optimization problem to find k.\nInstead, we can formulate (5) as a least circular distance (LCD) problem because the base phases and expected phases are both angular measurements [13].\n$\\underset{x}{\\text{argmin }} \\frac{1}{2 n} \\sum_{i=1}^{n} (1 - cos(\\phi_i - A_i x))$\n(6)\nSince the cosine function is 2\u03c0-periodic, this approach allows us to compare and Az directly without having to worry about wrapping. We can rewrite it as argmax ED(x), where\n$E_D(x) = \\frac{1}{n} \\sum_{i=1}^{n} cos(\\phi_i - A_i x).$\n(7)\nIt turns out that (4) is the same as (7), so that S(x) = nEp(x). If each i is the observed value of a circular random variable \u03a6\u2081 that follows a von Mises distribution (an approximation of the normal distribution for circular data) centred at Aix and with variance K, then the maximum likelihood estimate (MLE) of x given $1,..., \u03a6\u03b7 maximizes (7) [13].\nThe connection between the MLE and optimal decoding from grid cell populations representing location has been pre- viously explored [25]. In that work, the phases were modelled"}, {"title": "D. Phase-Coupled Least Circular Distance Regression", "content": "The local optima in the similarity function occur because, as z diverges from its optimal value, the differences between \u0424\u2081 and Aiz diverge at first, but then converge again as Aiz completes a full lap and returns close to $i. This problem is made worse by large base phases since bigger A\u017c results in faster-growing phases. However, the differences between these phases need not be large. By choosing two base phases that are closer together, we can expect their phase difference to be smaller and less prone to wrapping.\nA similar type of phase coupling was used to correct phase errors for path integration [17]. A web of phase couplings was compiled into an over-determined linear system, the least-squares solution of which yielded the cleaned-up phases as well as an estimate of 7. This formulation reduced the impact of the phase wrapping problem. We can use the idea of strategically coupling phases to formulate a least circular distance problem with a smoother, less undulating objective function.\nConsider vector elements i and j, with observed phases i and 4. Their phase difference can be written\n$\\Delta \\phi_{i,j} = \\phi_i - \\phi_j$\n(8)\n$\\approx A_i x - A_j x$\n(9)\n$= (A_i - A_j) x$\n(10)\n= \\Delta A_{i,j} x\n(11)\nSimilarly, their phase sum is\n$\\Delta \\phi_{i,j} = \\phi_i + \\phi_j \\approx \\Delta A_{i,j} x .$\n(12)\nWe can construct ne pairs of coupled phases, and thus define an nc \u00d7 n matrix C, which we will call the coupling matrix. If the kth coupling is between phases \u03a6ik and $jk, then we set the kth row of C, denoted Ck, to zero, except for Ck,ik = 1 and Ck,jk = Sk. Here sk is 1 if we sum the phases, and -1 if we subtract the phases. Whether we are adding or subtracting"}, {"title": "E. Selecting Phase Couplings", "content": "How should we couple phases to obtain a smoother objec- tive function without sacrificing accuracy? We can control C by selecting which of the approximately n\u00b2 couplings we want to include in our objective function. Our selection of couplings"}, {"title": "F. Iterative Optimization", "content": "Combining the direct formulation ED(x) from (7) with the phase-coupled formulation Ec(x) from (14), we arrive at the combined objective function\nargmax E(x) where E(x) = dE\u266d(x) + (1 \u2212 1)Ec(x) (16)\nand A controls the relative weight between the direct and coupled objective functions. Setting X = 1 yields the direct objective function, while setting A 0 yields the phase- coupled objective function."}, {"title": "A. Comparing to Other Methods", "content": "In this section, we compare our optimization method to other candidate cleanup methods: a denoising autoencoder, a resonator network, and a grid search. We corrupted SSPs with Gaussian noise (we demonstrate our method on bundling noise in Section III-C).\nWe compared CSim to denoising autoencoders. To feed the FHRR vector into the denoising autoencoder, we separated the real and imaginary components, so that there were 2d separate inputs to the neural network. The single hidden layer had the same number of neurons, 2d. Each training sample was created by choosing an z, encoding it as an SSP, and then adding noise to the SSP. The corresponding target was either (for decoding), or the clean SSP for 7 (for clean-up). The network was trained with backpropagation using the Adam optimization method [33]. In our experiments, we trained a new network for each level of noise, although one network could be trained and tested on all noise levels.\nWe also compared our method to a resonator network [10], [12]. For each iteration of the resonator network, it used its current guess for 2 to decompose the SSP into its individual dimensions, tried to clean up each component, and then bound the components back together to reconstitute a cleaned-up"}, {"title": "B. Convergence Rate", "content": "The gradient ascent optimization of CSim can converge quickly with the appropriate choice of step size. The step size"}, {"title": "C. Cleanup of Bundle Corruption", "content": "Consider constructing a bundle, M, of SSPs by binding them in pairs, and then adding those together. Suppose one of those pairs is vw. Then one could 'pull out' v by querying the bundle, unbinding it from w using M w. The result should be a noisy version of v.\nNoise arises from normalizing a bundle to ensure that the resulting vector is unitary. When such a bundle is queried, the resulting vector can contain a substantial amount of noise. There is also interference noise caused by the fact that the SSPs are only approximately orthogonal.\nNormalization can be done after each item is added to a bundle; this process tends to dilute the SSPs that were added early on. Alternatively, a single normalization operation done at the end of the bundling process can reduce the corruption, or at least spread it evenly over all the SSPs added to the bundle.\nFigure 8 displays the phase error caused by querying bundles of different numbers of items. Each bundle was normalized after adding all the items. The distribution of the phase error spreads with the number of items in the bundle."}, {"title": "D. Demonstration of Clean-Up", "content": "As a final test, we query the bundle\nM = TX1.5 Y-2.3 + SX-0.7 Y0.3 (26)\nrepresenting a triangle and a square at their corresponding (x, y) positions. Suppose we want to know the displacement between the two objects. To compute that, we query M to get the SSPs for the positions of the triangle and the square,\n\u0440\u0442 = MOT ps = MOS\n,\nand then get the SSP for the displacement between them using ps T = \u0440\u0442\u25ca ps, which should be an encoding of Z = (2.2, -2.6). However, pr and ps will contain noise that carries through to ps\u2192T, resulting in a corrupted similarity map as show in Fig. 10 (left).\nIf pr and ps are first cleaned up using CSim, giving p and Ps, then PST = PT p yields a much better similarity map, as shown in Fig. 10 (right). As a result, the decoding of psr using CSim correctly converges to a solution very close to Z. This example shows that cleaning up SSPs, as an intermediate step, results in more accurate VSA processing."}, {"title": "IV. DISCUSSION AND CONCLUSION", "content": "These experiments demonstrate the effectiveness of our method at decoding and cleaning up SSPs. Coupling phases can result in a smoother objective function, minimizing the number of failures from starting in the wrong basin of attrac- tion. While previous works have looked at other optimization methods [12], these methods often converged to the wrong value as a result of starting in the wrong basin of attraction.\nOur method of gradient ascent is efficient and easy to implement. Even with noise, our method often converges in 10 or fewer iterations.\nOne main advantage of our method is that it does not need to be trained, so long as the original base phases A are known. For the denoising autoencoder, training can be time-consuming and may not generalize to different levels of noise.\nOur method is also guaranteed to return a true SSP regard- less of Z value. For the other methods that maximize similarity, they are either limited to discrete values in Z, as in grid search, or take linear combinations of SSPs corresponding to discrete values, as in resonator networks.\nFor our method to be effective, it is important to appro- priately choose the phase couplings. As illustrated in Figs. 3 and 4, the distribution of their phase differences impacts the robustness of the CSim method, since it can determine the uniqueness of the peak in Ec(x), and the width of its basin of attraction. We found that coupling each phase to 10 other phases sufficiently minimized the variance, however, our method was still effective with fewer couplings.\nWe used a uniform phase-difference distribution for our experiments. But other distributions might perform better. For example, if we choose our phase differences so that their distribution follows a sinc\u00b2 function, the resulting similarity function is approximately triangular and may be easy to optimize [31].\nOne could also explore other optimization methods. Due to the challenges with matrix inversion, we did not further investigate using Newton's method to solve this optimiza- tion problem. However, Newton's method, other variants of gradient ascent, or more sophisticated iterative optimization methods might be more efficient or effective.\nWe also plan to explore possible neural implementations of this method. In particular, this gradient ascent method, which performs addition directly on phases, seems particularly well-suited for computation with neurons that represent complex numbers by their phase [34], [35], [36].\nFinally, while this method was initially inspired by cells in the hippocampus and proposed methods for path integration, the final method does not clearly map onto a neural system. Future work may explore if this method is more closely linked to a biological neural system.\nOverall, our decoding and cleanup method efficiently and accurately finds the optimal (or its clean SSP counterpart) better than alternative methods. The potential for neural imple- mentation positions this method well for use in neuromorphic systems."}]}