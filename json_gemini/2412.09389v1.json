{"title": "UFO: Enhancing Diffusion-Based Video Generation with a Uniform Frame Organizer", "authors": ["Delong Liu", "Zhaohui Hou", "Mingjie Zhan", "Shihao Han", "Zhicheng Zhao", "Fei Su"], "abstract": "Recently, diffusion-based video generation models have achieved significant success. However, existing models often suffer from issues like weak consistency and declining image quality over time. To overcome these challenges, inspired by aesthetic principles, we propose a non-invasive plug-in called Uniform Frame Organizer (UFO), which is compatible with any diffusion-based video generation model. The UFO comprises a series of adaptive adapters with adjustable intensities, which can significantly enhance the consistency between the foreground and background of videos and improve image quality without altering the original model parameters when integrated. The training for UFO is simple, efficient, requires minimal resources, and supports stylized training. Its modular design allows for the combination of multiple UFOs, enabling the customization of personalized video generation models. Furthermore, the UFO also supports direct transferability across different models of the same specification without the need for specific retraining. The experimental results indicate that UFO effectively enhances video generation quality and demonstrates its superiority in public video generation benchmarks. The code will be publicly available at https://github.com/Delong-liu-bupt/UFO.", "sections": [{"title": "Introduction", "content": "The rapid advancement of artificial intelligence has transformed the field of creative content generation. Individuals can quickly obtain personalized text (Zhao et al. 2023), images (Esser et al. 2024), sounds(Du et al. 2024), and videos (Xing et al. 2023) through simple natural language descriptions. In visual generation, diffusion models (Ho, Jain, and Abbeel 2020; Song et al. 2021), which have excelled in image creation, play a crucial role. However, when applied to video generation, these models encounter challenges such as poor image quality, low aesthetic appeal, and weak consistency. For instance, as shown in Figure 1, even the most advanced open-source models cannot prevent subjects from changing shape throughout a video (e.g., the koala with the staff in Case 1, the kitten in Case 2, and the person with the bag in Case 3), or background inconsistencies (e.g., the boat in Case 2 and the advertising billboard in Case 3).\nAesthetic theory (Wu et al. 2023; Li et al. 2024) in visual media emphasizes the crucial roles of the consistency and clarity in enhancing viewer engagement and perceived quality. In video generation, where dynamic elements and transitions are essential, inconsistencies and blurring not only reduce aesthetic appeal but also undermine the effectiveness of visual communication. To address the challenges mentioned above, we propose the Uniform Frame Organizer (UFO), a non-invasive plug-in designed to enhance the consistency between the foreground and background and alleviate blurring issues, thereby improving video generation quality. Applicable to any diffusion-based video generation model, the UFO integrates a set of non-invasive adapters into the video generation model's backbone network, occupying only 0.005\u00d7 the size of the original model's trainable parameters. These adapters are capable of autonomously adjusting their intensity of use, featuring a tunable intensity parameter, which is tuned to optimize the balance between dynamic visual content and static precision, reflecting a direct application of aesthetic principles in video generation.\nSpecifically, when using a small amount of video frames or images as training data, UFO sets the intensity to the highest value, dynamically controlling each adapter's parameters and release intensity to force the model's output to approximate a static video, a scenario of extreme consistency. During this process, the UFO learns to identify and correct inconsistencies in videos. As the pre-trained model's parameters remain unchanged, the UFO's intensity can be adjusted to a lower value during application. This adjustment allows the model output to closely resemble the original while significantly enhancing the consistency between the subjects and the background in the video frames. It also markedly reduces issues such as sudden blurring of video frames.\nTo achieve the aesthetic consistency, during the training process, the primary optimization goal for the model integrated with the UFO is set to generate static video frames. This simplicity allows the model to learn quickly and converge after only 3000 training steps on a single GPU, using much fewer resources than fine-tuning or retraining video generation models. Moreover, once the parameters of the UFO are obtained, it supports direct transferability across multiple models of the same specification without the need for model-specific retuning (as shown in Figure 1, Case 2). Beyond enhancing video consistency, the UFO is capable of"}, {"title": "Related Work", "content": "The Diffusion Model (DM) has consistently excelled in image (Nichol et al. 2022; Ramesh et al. 2022; Zhang, Rao, and Agrawala 2023) and video generation (Ma et al. 2024a; Khachatryan et al. 2023a; Lu et al. 2024), and has also expanded across various video generation tasks, including text-to-video (Luo et al. 2023; Wang et al. 2023b), image-to-video (Yin et al. 2023; Chen et al. 2023c), video-to-video (Liew et al. 2023; Ouyang et al. 2024), and applications under diverse control conditions such as pose (Karras et al. 2023; Ma et al. 2024b), depth (Chen et al. 2023b; Zhang et al. 2024), and sketch (Khachatryan et al. 2023b; Wang et al. 2024). In the past two years, the text-to-video generation, our primary focus, has made rapid progress. Early work like Image Video (Ho et al. 2022) highlighted diffusion models' ability to produce high-quality videos. However, aligning videos precisely with text prompts while maintaining visual appeal remained challenging. Subsequent models, such as VideoCrafter (Chen et al. 2023a, 2024) and ModelScopeT2V (Wang et al. 2023a), were trained with large"}, {"title": "Methodology", "content": "As shown in Figure 2, the UFO includes a series of lightweight adapters (Section 3.1) that can be non-destructively attached to any mapping layer of the model without altering the original model's parameters. During training (Section 3.2), only the UFO's parameters are updated, while the intensity is set to the highest to achieve extremely consistent videos under text conditions, rendering all video frames static. This phase drives the UFO to develop the ability to identify and correct inconsistencies. During inference (Section 3.3), setting the UFO's intensity to 0 will result in an output that is identical to that of the original pre-trained model. When it is at low level, the output will closely resemble the original, maintaining motion in video frames. The UFO's targeted repair capabilities enhance the consistency between subjects and backgrounds and mitigate video quality degradation. For example, when applying the same prompt and fixed random seed, the UFO-generated appearance and attire of the raccoon are more consistent, notably preventing significant shape transformations in the electric guitar being played, as shown in Figure 2."}, {"title": "Lightweight Adapters for UFO", "content": "To achieve cost-effective improvements in video generation models and eliminate reliance on a single model framework, inspired by efficient parameter fine-tuning methods (Pfeiffer et al. 2020; Hu et al. 2022), we design a series of adapters, each of which consists of a layer with minimal input or output dimensions, and is injected into the diffusion model with minimal overhead. These adapters act as the smallest subunits for controlling the consistency of hidden features in video frames, enabling precise, targeted consistency corrections.\nSpecifically, in a module parameterized by $W \\in \\mathbb{R}^{m \\times n}$ in the DIT, we learn a detection layer $v_{det} \\in \\mathbb{R}^{n \\times d}$ to precisely locate features affecting video consistency. Concurrently, a correction layer $v_{cor} \\in \\mathbb{R}^{m \\times d}$ modifies the identified features. Here, $d$ is chosen to be small to ensure parameter efficiency. Consequently, the original representation $y = Wx$ is modified as follows:\n$y = Wx + \\alpha \\beta (v_{det}^T x) \\cdot v_{cor}$\nwhere $x \\in \\mathbb{R}^n$ and $y \\in \\mathbb{R}^m$ represent the input and output of the intermediate layer, respectively, the superscript T indicates transposition, $\\alpha$ is a adjustable intensity factor, and $\\beta$ is a learnable dynamic intensity factor, aiming to dynamically adjust the strength of each adapter of the UFO to ultimately correct the consistency of video frames.\nFocusing on just two frames $y_t, y_{t+n} \\in y$ in the video, the difference in output between these two intermediate layers"}, {"title": "Training of UFO", "content": "During the training phase, with $\\alpha = 1$, the target is for all video frames to be identical, thus $\\Delta y_n = 0$ regardless of the value of $n$. Therefore, the optimization goal for each latent feature is $ -W\\Delta x_n = \\beta \\Delta (v_{det}^T x) \\cdot v_{cor}$, meaning that the trained $\\beta$, $v_{det}$, and $v_{cor}$ can adaptively identify and fill the variations in each video frame.\nUtilizing this feature, during inference, $\\alpha$ is set to a low value, ensuring that the variability $\\Delta y_n \\approx W\\Delta x_n$ in video frames maintains the subjects, background, and motion capabilities essentially consistent with those of the pre-trained model's output videos, while the additional term $ \\alpha \\beta \\Delta (v_{det}^T x) \\cdot v_{cor}$ has a comprehensive view of the changes in video frames, adaptively enhancing the consistency of the output video. Furthermore, if the intensity factor $\\alpha$ is fixed, UFO, due to its parametric characteristics, also supports using a small batch of video-text pairs to directionally fine-tune the video generation model, customizing the video generation effects.\nTraining of UFO\nDuring the training phase, video data $V \\in \\mathbb{R}^{F \\times H \\times W \\times C}$ is first compressed into a latent space representation $z = E(V)$ using a pretrained variational autoencoder (VAE) (Kingma and Welling 2013). Additionally, a textual condition $c$ is introduced, which is derived from a text encoder using prompts aligned with the video content. In the generation process, the diffusion model gradually introduces noise to simulate the diffusion of video data, forming perturbed samples $z_t = \\sqrt{\\bar{\\alpha_t}} z + \\sqrt{1 - \\bar{\\alpha_t}} \\epsilon$, where $\\epsilon \\sim N(0, 1)$ represents noise sampled from a standard normal distribution, and $\\bar{\\alpha}_t$ serves as a noise scheduler, with $t$ denoting the diffusion time step.\nAfter integration with UFO, the parameters of the original model are denoted as $\\theta$, and only the parameters within UFO are updated during training. The reverse diffusion process, which is essentially training the model to denoise, aims to predict the less noisy $z_{t-1}$:\n$p_{\\theta}(z_{t-1}|z_t) = N(\\mu_{\\theta}(z_t), \\Sigma_{\\theta}(z_t))$. Here, the log likelihood of the variational lower bound simplifies to $L_{vlb}(\\theta) = logp(z_0|z_1, c) + \\sum_t D_{KL} (q(z_{t-1}|z_t, z_0) || p_{\\theta}(z_{t-1}|z_t))$. Since both $q$ and $p_{\\theta}$ are Gaussian, the $D_{KL}$ term is determined by the mean $\\mu_{\\theta}$ and covariance $\\Sigma_{\\theta}$. The $\\mu_{\\theta}$ is reparametrized into the denoising model $\\epsilon_{\\theta}$, which can be trained using a simple objective:\n$L_{simple}(\\theta) = E_{z \\sim p(z), \\epsilon \\sim N(0,1), t, c} [||\\epsilon - \\epsilon_{\\theta}(z_t, t, c)||^2]$,\nAccording to (Nichol and Dhariwal 2021), it is necessary to fully optimize the $D_{KL}$ term (i.e., train using the full $L_{vlb}$) to train an LDM with learnable covariance $\\Sigma_{\\theta}$. Therefore, the training loss for UFOs employs both $L_{simple}$ and $L_{vlb}$.\nWhen training the consistency UFO, every frame in $V$ used is identical, thus image-text pairs, which are more readily available, can be used as training data. For customizing stylization UFOs, regular video-text pairs are used as training data."}, {"title": "Inference", "content": "During inference, the trained UFO is integrated into the diffusion model, retaining all functionalities of the original model. For the consistency UFO, its intensity factor $\\alpha$ tends to be set to a low value, which can be adjusted based on the performance of the original pre-trained model during video inference. If issues such as inconsistency or blurring are severe, $\\alpha$ should be increased, which enhances video frame consistency. This adjustment allows users to control video consistency according to their needs. For stylization UFOs, $\\alpha$ is suggested to match the level used during training, and minor adjustments can optimize personalization. Note that when combining different UFOs, the intensity of each UFO needs to be adjusted as required."}, {"title": "Experimental Results", "content": "Implementation details. To ensure the rigor of our experiments, we train UFOs using two of the latest text-to-video open-source models, EasyAnimate-V2 (Easy) (Xu et al. 2024a) and OpenSora-V1.2 (Open) (Zheng et al. 2024). Training is conducted on 4 NVIDIA A100 GPUs, with inference running on a single GPU. During the training process, only the parameters of the UFOs are updated, with each UFO undergoing 3000 training steps. All adapters have a hyperparameter dimension d = 4, and gradient accumulation is not used. For Open, a linear warm-up strategy is employed in the first 500 steps, where the learning rate gradually increases from nearly zero to 2e-4, and this rate is maintained after the warm-up phase. For Easy, the learning rate is set at 1e-4 and remains constant. The rest of the training settings follow the original methods. During inference, all settings use the recommended configurations of the original methods, with videos set at 24 Frames Per Second (FPS), and all experiments and visual effects in the text use the same random seed to compare with and without UFOs. More details on training and inference can be found in the supplementary materials.\nTraining Datasets. For training the consistency UFO, we use a subset of the LAION-Aesthetics V2 (Schuhmann et al. 2022) dataset with aesthetic scores above 6.5, from which we extract 12K image-text pairs to create static video-text pairs for training. For the training of stylization UFOs, we collect 300 videos for each of the four styles (Pixel Art, oil painting, animated style, black and white) from publicly available video resources on the internet. The text for these videos is automatically annotated using the 13B version of the PLLaVA (Xu et al. 2024b) model, with descriptions regarding the video style removed during training.\nEvaluation Metrics. To objectively demonstrate the improvements in video consistency and quality achieved by the UFO, we employ the latest video generation evaluation method, Vbench, using a fixed intensity setting for the consistency UFO. This evaluation encompasses two main dimensions: Video Quality (VQ) and Semantic Quality (SQ). As our approach does not specifically target enhancements in video semantic consistency, our primary focus is on the VQ metrics. These include four dimensions of \"Temporal"}, {"title": "Conclusion", "content": "In this paper, we propose and validate the UFO, a non-invasive plugin for diffusion-based video generation models. By integrating the UFO into existing models, its effectiveness in mitigating common problems like video quality degradation and frame inconsistency is demonstrated, and without significantly increasing computational demands. In addition, The proposed intensity a also provides users with the flexibility to control video consistency, facilitating the creation of videos that meet their specific needs. Moreover, the UFO's modular design and low resource requirements make it easily transferable between different models, thus enhancing their flexibility and scalability. In the future, we aim to improve the UFO so that it can reliably enhance video quality by automatically intensity adjusting."}, {"title": "Additional Details", "content": "Evaluation Metrics.\nVbench Metrics. In this study, we utilize the Vbench (Huang et al. 2024) evaluation system for a comprehensive assessment of video quality. This system categorizes video quality metrics into two main domains: Temporal Quality (TQ) and Frame-Wise Quality (FWQ), each further divided into multiple specific indices to capture various aspects of video quality. Temporal Quality is essential for ensuring a consistent viewing experience throughout the video sequence. This category includes several components: Subject Consistency (SC), which gauges the consistency of subjects, such as people or objects, based on feature similarity between frames; Background Consistency (BC), assessing the stability of background scenes across frames; Temporal Flickering (TF), measured by calculating the average absolute difference between frames to spotlight inconsistencies in local and high-frequency details; and Motion Smoothness (MS), which ensures that video movements comply with the physical laws of the real world. Frame Quality evaluates the quality of each individual frame independently of its temporal context, focusing on Aesthetic Quality (AQ) and Imaging Quality (IQ). AQ appraises the artistic and visual appeal of each frame, considering factors such as layout, color coordination, and overall aesthetics. Conversely, IQ examines the technical aspects of each frame, including exposure levels, noise, and clarity.\nBeyond these quantitative metrics, we also assess the semantic quality of generated videos using the Semantic Quality (SQ) component of the Vbench system, which includes two semantically-related dimensions: Semantics and Style. The Semantics dimension assesses whether the video content accurately portrays the entities and their attributes described in the text prompts, ensuring that the objects, actions, and colors in the video correspond with the descriptions. The Style dimension evaluates whether the visual style of the generated videos meets the specified user requirements, ensuring that the videos are not only compliant in content but also visually appealing and stylistically consistent.\nExcluded Count Metric. To ensure a fairer assessment of TQ, we exclude videos that tend to become static after the addition of the consistency UFO. The number of videos removed is defined as the Excluded Count (EC), which measures the losses incurred due to using fixed intensities of"}, {"title": "Training and Inference Details", "content": "Training Details. During training, both methods employe data bucketing techniques with varying resolutions and aspect ratios, aligning with the pre-training settings of the original models to cover a broad range of data. The T5 (Flan-T5-XXL) model (Raffel et al. 2020) serves as the text encoder. For the consistency UFO, image data is duplicated to match the specific video lengths required for model training. In contrast, normal video-text pair data is used for training the stylization UFO.\nTraining for the stylization UFO utilizes video data sourced from the internet, which initially lacks any accompanying text descriptions. Therefore, we opt to use PLLaVA (Xu et al. 2024b) for automatic text annotation by uniformly extracting four frames from each video, following procedures from the open-source code repository (Zheng et al. 2024). The automatically generated text may contain descriptions pertaining to the video's style. Since the stylization UFO is designed for a single style and should avoid reliance on specific prompts, we employ GPT-4 (OpenAI 2023) to remove style-related terms from the text. The used prompt is: \"Rewrite this prompt to exclude any descriptions of video styles such as cartoon, oil painting, black-and-white, pixel, etc. Focus on describing the content of the image. Output only the rewritten result without any additional output: Caption.\" Examples of the video-text pairs obtained are shown in Figure 8.\nInference Details. During inference, both models adhere to the official recommended settings. For OpenSoraV1.2 (Zheng et al. 2024), the video output consists of 51 frames for every 2 seconds of video, and the model utilizes 30 sampling steps per video. For Easy AnimateV2 (Xu et al. 2024a), the output is 24 frames per second of video, and each video undergoes 50 sampling steps. EasyAnimateV2 also supports the use of negative prompts, applying a uniform negative prompt during video inference: \u201cThe video is not of high quality, it has low resolution, and the audio quality is not clear. Strange motion trajectory, poor composition and deformed video, low resolution, duplicate and ugly, strange body structure, long and strange neck, bad teeth, bad eyes, bad limbs, bad hands, rotating camera, blurry camera, shaking camera. Deformation, low-resolution, blurry, ugly, distortion.\""}]}