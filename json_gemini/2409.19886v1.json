{"title": "RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling Large Language Models", "authors": ["Shuhao Chen", "Weisen Jiang", "Baijiong Lin", "James T. Kwok", "Yu Zhang"], "abstract": "Recent works show that assembling multiple off-the-shelf large language models (LLMs) can harness their complementary abilities. To achieve this, routing is a promising method, which learns a router to select the most suitable LLM for each query. However, existing routing models are ineffective when multiple LLMs perform well for a query. To address this problem, in this paper, we propose a method called query-based Router by Dual Contrastive learning (RouterDC). The RouterDC model consists of an encoder and LLM embeddings, and we propose two contrastive learning losses to train the RouterDC model. Experimental results show that RouterDC is effective in assembling LLMs and largely outperforms individual top-performing LLMs as well as existing routing methods on both in-distribution (+2.76%) and out-of-distribution (+1.90%) tasks. Source code is available at https://github.com/shuhao02/RouterDC.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated remarkable capabilities across various tasks. Many LLMs are publicly available online, such as Mistral [23], LLaMA-2 [43], and LLaMA-3 [42]. Those LLMs have been further fine-tuned to be generalists or specialists. For example, MetaMath [50] excels in solving mathematical reasoning problems. Since those LLMs are pre-trained or fine-tuned with various data, they typically exhibit varying strengths and weaknesses across different tasks"}, {"title": "2 Related Work", "content": "Large Language Models (LLMs). LLMs have achieved great success in natural language processing and many foundation models have been released online [23, 43, 42]. Many prior works [50, 44, 54, 9, 8, 38, 19, 31] focus on fine-tuning those foundation models to obtain specialized LLMs for solving versatile tasks, for example, language understanding [54, 19], code generation [38, 8], and mathematical reasoning [50, 31]. In this paper, we study the problem of assembling LLMs to harness their strengths by a router.\nLLM Ensembling. The goal of LLM ensembling is to leverage multiple LLMs to boost performance compared with a single model across various downstream tasks. Voting [28, 48] is a simple but effective ensemble method. Jiang et al. [24] further propose PairRanker and GenFuser to generate an improved output from the outputs of all LLMs, which needs to call LLMs O(T2) times with T as the number of LLMs. LLM cascading [2, 12, 35, 51] query a list of LLMs (whose capacity depends on the model size) sequentially until an LLM's output is satisfied (i.e., having a significantly high confidence score), which is returned as the final output. Fusion of Experts [47] concatenates all LLMs outputs to build the final output and casts it as a supervised learning problem. Unlike the aforementioned ensembling methods which require querying the LLMs at least O(T) times in inference, our RouterDC is much more efficient as it only needs to call the selected LLM once.\nLLM Routing. LLM routing aims to select the most suitable model for a query without calling all LLMs. Many works have been proposed to design an effective routing strategy. Shnitzer et al. [40] propose a collection of binary classifiers to evaluate the correctness of each LLM. Lu et al. [30] propose ZOOTER to align a router with the supervision from the reward model. LoraRetriever [52] propose a task-wise router to select the LLM by predicting the task identity of the query. Srivatsa et al. [41] explore the routing ability using both classifier-based and clustering-based approaches. Though those routers are cost-effective, they neglect the fact that multiple LLMs can be well- suited to answer a single query. In contrast, the proposed RouterDC leverages contrastive losses to simultaneously consider the strengths of several powerful LLMs.\nContrastive Learning. Contrastive learning learns effective representations by distinguishing between similar and dissimilar pairs of data points. It has been widely used in various tasks, such as visual representation learning [4, 13], sentence representation leaning [11, 49, 39], and vision- language alignment [36, 55]. In this paper, we propose two contrastive losses to learn the RouterDC for assembling LLMs."}, {"title": "3 Methodology", "content": "In this section, we propose RouterDC, a framework for learning a query-based router to assemble LLMs. An overview is illustrated in Figure 1. We introduce the problem of router learning in Section 3.1 and design a scoring method to measure the performance of LLMs on each training query (Section 3.2). Next, we propose two contrastive losses to train the router, including a sample-LLM contrastive loss for learning the routing strategy (Section 3.3) and a sample-sample contrastive loss for improving training stability (Section 3.4). The training and inference procedures are provided in Algorithm 1."}, {"title": "3.1 Problem Formulation", "content": "Consider a set of LLMs \\({M_t : t = 1, . . ., T}\\) and a training set \\(D_{\\text{train}} = \\{(x_i, y_i) : i = 1, . . ., n\\}\\), where \\(x_i\\) is a query (i.e., question) and \\(y_i\\) is its answer (i.e., ground truth). Usually, no single LLM is universally suitable for all queries in \\(D_{\\text{train}}\\). Moreover, LLMs are diverse and have different architectures (e.g., Mistral-based [23], LLaMA-based [42]), making it infeasible to merge all LLMs into a single model [33, 21, 25]. In this paper, we study the problem of assembling LLMs by learning a router to select the suitable LLM for each query. The router takes \\(x\\) as input and produces the probability distribution of \\(T\\) LLMs being selected. As training and testing queries may come from different data distributions, the learned router is expected to generalize well on both in-distribution and out-of-distribution scenarios."}, {"title": "3.2 Scoring", "content": "To learn the router, we need to design a scoring method to assess the performance of LLMs on queries. For an open-ended generation query \\(x_i\\) (requiring a long answer, e.g., GSM8K [7], with an example shown in Example 2), one can directly compare the ground truth \\(y_i\\) with the output of the LLM \\(y_i^{(t)} = M_t(x_i)\\) generated by greedy decoding. Though greedy decoding is simple and efficient, its inherent shortsightedness often prevents it from discovering the optimal solution. Conversely, sampling, like beam sampling [46], is an advanced approach that is widely used in practice as it explores multiple alternatives in the search space, potentially leading to better results. We feed the query \\(x_i\\) to the LLM \\(M_t\\) \\(M\\) times to obtain outputs \\(\\{y_{i,m}^{(t)} : m = 1, ..., M\\}\\). Then, we define the score of LLM \\(M_t\\) on the query \\(x_i\\) as:\n\\[\\frac{1}{M} \\sum_{m=1}^{M} \\text{evaluate}(y_{i,m}^{(t)}, y_i),\\]\nwhere \\(\\text{evaluate}(\\hat{y}, y)\\) gives 1 if the prediction \\(\\hat{y}\\) is correct otherwise 0.\nFor a multiple-choice question \\(x_i\\) with an option set \\(A_i\\) (e.g., MMLU [16], as an example shown in Example 2), sampling is unnecessary as we can simply define the score based on the probability of options, i.e.,\n\\[s_i^{(t)} = \\begin{cases} \\frac{P_{M_t}(y_i | x_i)}{\\sum_{a \\in A_i} P_{M_t}(a | x_i)} & \\text{if } \\hat{y}^{(t)} = y_i \\\\ 0 & \\text{otherwise} \\end{cases} \\]\nwhere \\(P_{M_t}(a | x_i)\\) is the probability of option \\(a\\) predicting by the LLM \\(M_t\\). According to Eq. (2), when the LLM \\(M_t\\) outputs a correct option (i.e., \\(\\hat{y}_t\\) = \\(y_i\\)), we normalize the probability to make it comparable across different LLMs, which will be used in Section 3.3; When the LLM \\(M_t\\) generates a wrong option, \\(s_i^{(t)}\\) is set to 0 to punish \\(M_t\\) for \\(x_i\\). Based on the scores \\(\\{s_i^{(t)} : t = 1, . . ., T\\}\\), we introduce a sample-LLM contrastive loss in the next section."}, {"title": "3.3 Sample-LLM Contrastive Loss", "content": "As illustrated in Figure 1, The proposed RouterDC consists of an encoder \\(\\mathcal{E}(x; w)\\) parameterized by \\(w\\) (where in our experiments \\(\\mathcal{E}(x; w)\\) uses a small language model mDeBERTaV3-base [14]) to map \\(x\\) into an embedding in \\(\\mathbb{R}^p\\), and \\(T\\) learnable LLM embeddings \\(\\{k_t \\in \\mathbb{R}^p : t = 1, . . ., T\\}\\) for the \\(T\\) LLMs. For a query \\(x_i\\), the RouterDC generates a selection probability distribution over \\(T\\) LLMs as\n\\[\\mathcal{R}(x_i; \\theta) = \\text{softmax} \\left[ \\text{sim}(\\mathcal{E}(x_i; w), k_1), ..., \\text{sim}(\\mathcal{E}(x_i; w), k_T) \\right],\\]\nwhere \\(\\theta = \\{w, k_1,k_2, . . ., k_T\\}\\) denotes the set of the parameters in RouterDC, \\(\\text{sim}(\\cdot, \\cdot)\\) denotes the cosine similarity, and \\(\\text{softmax}(\\cdot)\\) denotes the softmax normalization.\nOne can train the router by minimizing the distance between the output of the router and a score distri- bution over \\(\\{s_i^{(t)} : t = 1,..., T\\}\\), i.e., \\(\\min_{\\theta} \\sum_{(x_i,y_i) \\in D_{\\text{train}}} KL \\left(\\mathcal{R}(x_i; \\theta), \\text{softmax}\\left[s_i^{(1)},..., s_i^{(T)}\\right]\\right)\\), where \\(KL(\\cdot, \\cdot)\\) is the Kullback-Leibler divergence [26]. This KL loss is recently used in [30] for LLM routing, but we argue that it may not be a good proxy for training the router since the goal of"}, {"title": "3.4 Sample-Sample Contrastive Loss", "content": "We empirically find that training the router by minimizing the sample-LLM contrastive loss alone is not stable (refer to Figure 12 in Section 4.4). The reason is that some similar queries can have dissimilar embeddings and may be routed to different LLMs. To improve the robustness of the router, we introduce a sample-sample contrastive loss to encourage the encoder to generate similar embeddings for similar queries.\nFirst, we cluster queries into multiple groups by unsupervised clustering. Specifically, we extract the embeddings of all training queries using a pre-trained encoder (i.e., mDeBERTaV3-base [14]) and transform them into low-dimensional vectors by the t-SNE algorithm [45]. Then the k-means cluster- ing algorithm [32] is used to cluster these low-dimensional vectors into \\(N\\) groups \\(\\{K_1,...,K_N\\}\\).\nNext, we construct a sample-sample contrastive loss to encourage samples in the same group to have similar embeddings. Specifically, for a query \\(x_i \\in K_j\\), we randomly select an in-group query"}, {"title": "3.5 Training and Inference", "content": "Training. We learn a router \\(\\mathcal{R}(x; \\theta)\\) by minimizing the final objective consisting of sample-LLM and sample-sample contrastive losses, i.e.,\n\\[\\mathcal{L}(D_{\\text{train}}; \\theta) = \\sum_{(x_i, y_i) \\in D_{\\text{train}}} \\mathcal{L}_{\\text{sample-LLM}}(x_i, y_i; \\theta) + \\lambda \\mathcal{L}_{\\text{sample-sample}}(x_i; \\theta),\\]\nwhere \\(\\lambda > 0\\) is a hyper-parameter. In our experiments, \\(\\lambda\\) is set to 1.\nRouterDC contains less than 100M parameters (that is, the encoder model \\(\\mathcal{E}(x; w)\\) is small and the number of parameters of LLM embeddings \\(\\{k_1, ..., k_T\\}\\) are negligible), thus it is parameter-efficient. Moreover, training the router is computationally efficient as it does not require backpropagating the gradients through the LLMs.\nInference. During inference, for each testing query \\(x'\\), we compute \\(\\mathcal{R}(x'; \\theta)\\) and select the LLM with the largest probability, i.e., \\(t' = \\text{arg } \\max_{t \\in \\{1,...,T\\}} \\text{sim}(\\mathcal{E}(x'; w), k_t)\\). Then we generate the prediction as \\(y' = M_{t'}(x')\\).\nCompared with existing LLM assembling methods like voting [28] and cascade [2], which require calling LLMs multiple times for a query, RouterDC is much more efficient as it only needs to call the selected LLM once."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nCandidate LLMs. We choose seven open-source LLMs from HuggingFace\u00b9: (i) Mistral-7B [23] is a general LLM released by the Mistral-AI team; (ii) MetaMath-Mistral-7B [50] is fine-tuned on the MetaMathQA dataset [50]; (iii) zephyr-7b-beta [44] is an aligned version of Mistral-7B using direct preference optimization [37] on a mix of publicly available, synthetic datasets; (iv) Chinese-Mistral-7B [54] expands the vocabulary and incrementally pre-trains Mistral-7B on Chinese corpus; (v) dolphin-2.6-mistral-7b [8] is fine-tuned from Mistral-7B and released by Cognitive Computations; (vi) Llama-3-8B [42] is a general LLM developed by the Meta company; (vii) dolphin-2.9-llama3-8b [9] is fine-tuned from Llama-3-8B and released by Cognitive Computations. The first five LLMs are Mistral-based, while the last two LLMs are Llama-3-based.\nDatasets. We evaluate in various tasks: (i) MMLU [16] is a general benchmark that covers 57 subjects; (ii) GSM8K [7] is a mathematical benchmark with diverse grade school questions; (iii) CMMLU [27] is a comprehensive Chinese benchmark that covers 67 subjects ranging from basic to advanced professional levels; (iv) ARC-C [6] is a reasoning benchmark containing different grade-school level questions; and (v) HumanEval [3] is a code completion benchmark consisting of programming problems assessing language comprehension, algorithms, and simple mathematics. For GSM8K, we use its default training and testing split. As the rest tasks do not have a default split, we randomly split the dataset into training (70%) and testing (30%) sets. All the training sets are unioned together to form the total training set Dtrain for learning the router. The learned router is then evaluated on the testing set of in-distribution tasks.\nWe also evaluate the trained router on three out-of-distribution (OOD) tasks: (i) PreAlgebra [17], which consists of basic university-level algebra problems; (ii) MBPP [1], which is a code benchmark that consists of 1,000 crowd-sourced Python programming problems; and (iii) C-EVAL [20], which"}, {"title": "4.2 Main Results", "content": "In-Distribution Results. Table 1 shows the testing accuracy on five in-distribution tasks. As can be seen, RouterDC achieves the highest average accuracy, surpassing the best individual LLM (i.e., dolphin-2.9-llama3-8b) by a large margin of 3.98%. RouterDC achieves accuracy improvements over the top-performing individual model on three tasks, with an increase of +0.51% for GSM8K, +0.57% for ARC-C, and +1.63% for HumanEval. Compared with ZOOTER and CosineClassifier, RouterDC"}, {"title": "4.3 Sensitivity Analysis", "content": "Effects of \\(\\lambda\\). We conduct an experiment to study the effect of \\(\\lambda\\) in Eq. (6) w.r.t. the testing accuracy. According to Figure 5 (the detailed results are in Table 4 of Appendix A), we can see that using two contrastive losses together (i.e., \\(\\lambda = 1\\)) achieves better overall performance than using the sample-LLM contrastive loss alone (i.e., \\(\\lambda = 0\\)). Moreover, the overall performance of RouterDC is insensitive to a wide range of \\(\\lambda \\in [0.5, 5]\\), making it easy to choose the value of \\(\\lambda\\) in practice.\nEffects of #clusters \\(N\\). We conduct an experiment to study the effect of the number of clusters (i.e., \\(N\\)) used in the sample-sample contrastive loss w.r.t. the testing accuracy. According to Figure 6, we can find that RouterDC is insensitive to a wide range of \\(N \\in [4, 9]\\). Moreover, increasing \\(N\\) leads to higher average accuracy when \\(N\\) is small (\\(\\leq 4\\)), but the accuracy saturates quickly.\nEffects of #out-group queries \\(H\\). Figure 7 shows the testing accuracy with different \\(H\\)'s. When \\(H = 0\\), \\(\\mathcal{L}_{\\text{sample-sample}}\\) is a constant, which means using \\(\\mathcal{L}_{\\text{sample-LLM}}\\) alone and is not the best configuration. Moreover, the values of \\(H > 1\\) play a negligible influence on the average performance of RouterDC.\nEffects of \\(K_+\\) and \\(K_-\\). To investigate the sensitivity of \\(K_+\\) and \\(K_-\\), we conduct an experiment using the setting in Section 4.1. Figure 8 shows the average testing accuracy w.r.t. \\(K_+\\) and \\(K_-\\) within the in-distribution setting. As can be seen, for all the configurations, RouterDC outperforms the best individual LLM (i.e., 54.56% for dolphin-2.9-llama3-8b in Table 1). Note that among all the configurations, RouterDC (with \\(K_+ = 1\\) and \\(K_- = 6\\)) performs worse, showing that selecting only the top-1 LLM as positive and other LLMs as negative is inappropriate for learning the router."}, {"title": "4.4 Analysis", "content": "Does RouterDC select the suitable LLM for each query? To answer this question, we analyze the assignment of testing queries to LLMs in each task. Figure 9 shows the distribution, which has a clear structure on both in-distribution and out-distribution tasks. For example, most GSM8K and PreAlgebra queries are assigned to MetaMath-Mistral-7B and dolphin-2.9-llama3-8b, which have strong mathematical ability (Tables 1 and 2). To further investigate the routing rule of RouterDC, we compute the average cosine similarity between LLMs and the query embeddings for each task. As shown in Figure 10, the similarity matrix is roughly aligned with the assignment matrix in Figure 9. For example, embeddings of GSM8K and PreAlgrebra queries are more similar to MetaMath-Mistral- 7B and dolphin-2.9-llama3-8b than to other LLMs.\nVisualization of Training Queries. Figure 11 shows the t-SNE visualization [45] of the embeddings of training queries using a pre-trained encoder mDeBERTaV3-base [14]. As shown, except for HumanEval, all tasks have a clear clustering structure, confirming that using unsupervised clustering in Section 3.4 is reasonable.\nEffectiveness of \\(\\mathcal{L}_{\\text{sample-sample}}\\). We conduct experiments to study the effectiveness of \\(\\mathcal{L}_{\\text{sample-sample}}\\) (Eq. (5)). Figure 12 shows the training and testing accuracy curves of RouterDC (w/ or w/o \\(\\mathcal{L}_{\\text{sample-sample}}\\)) on GSM8K. As can be seen, the training curve of RouterDC (w/o \\(\\mathcal{L}_{\\text{sample-sample}}\\)) exhibits considerable oscillation, whereas that of RouterDC is much more stable. Figure 14(a) in Appendix B shows t-SNE visualization of training query embeddings extracted by the trained encoder of Rou- terDC (w/o \\(\\mathcal{L}_{\\text{sample-sample}}\\)). As can be seen, query embeddings belonging to different tasks are roughly mixed together. Example 2 in Appendix B provides two similar GSM8K queries, which both require basic calculation of shopping costs. Their embeddings have very low similarity (only \u20130.4589) when the router is trained by \\(\\mathcal{L}_{\\text{sample-LLM}}\\) alone. After integrating \\(\\mathcal{L}_{\\text{sample-sample}}\\, training query embeddings have a clear cluster structure (Figure 14(b)) with the similarity between these two example queries increases to 0.9982. Furthermore, RouterDC achieves higher testing accuracy than its counterpart, verifying the effectiveness of \\(\\mathcal{L}_{\\text{sample-sample}}\\).\nRouting to Different Numbers of LLMs. We evaluate the performance of RouterDC when the number of LLMs increases. Figure 13 shows the testing accuracy on five in-distribution tasks. As can be seen, adding LLMs consistently enhances the average accuracy. Table 7 in Appendix A shows the detailed results and configurations.\nRobustness to LLM Losses during Inference. In a production environment, the loss of model servers is sometimes unavoidable due to various reasons such as network problems, thus placing crucial requirements on the robustness of the router. We conduct an experiment to validate the robustness of RouterDC by removing an LLM during inference. Table 3 shows the testing accuracy on five in-distribution tasks. We can see that RouterDC reliably withstands the loss of any single LLM. The robustness is attributed to the fact that multiple LLMs (with top scores) are chosen as positive labels in the sample-LLM contrastive loss, and they can be regarded as each other's backup."}, {"title": "5 Conclusions", "content": "In this paper, we study the problem of training a router to assemble LLMs. We propose RouterDC to learn a query-based router using two novel contrastive losses (i.e., the sample-LLM and sample- sample contrastive losses). Experimental results show that RouterDC effectively assembles LLMs and outperforms individual top-performing LLMs as well as existing routing methods on both in-"}, {"title": "F Discussions on the Distant OOD Task", "content": "To further explore the generalization ability of RouterDC, we evaluate the learned router on one more OOD task: JavaScript [53], which aims to generate JavaScript code to solve problems. Different from HumanEval, which generates Python code to solve problems, JavaScript can be viewed as a distant OOD task. Table 10 reports the testing accuracy. As can be seen, RouterDC outperforms existing routing methods by a large margin, demonstrating that our RouterDC is more effective in routing queries of the distant OOD task."}, {"title": "G Effectiveness of \\(\\mathcal{L}_{\\text{sample-sample}}\\) for ZOOTER", "content": "We conduct experiments to study whether the proposed sample-sample contrastive loss is useful for ZOOTER. Table 11 and Table 12 shows the testing accuracy for the ID and OOD scenarios. As can be seen, integrating \\(\\mathcal{L}_{\\text{sample-sample}}\\) into ZOOTER leads to improvements of +1.52% and +0.81% for ID and OOD, respectively, demonstrating that the proposed sample-sample contrastive loss is beneficial for ZOOTER."}, {"title": "H Effectiveness of punishing \\(s_i^{(t)}\\)", "content": "As mentioned in Section 3.2, we set \\(s_i^{(t)}=0\\) when the LLM \\(M_t\\) generates a wrong option for the multiple-choice question \\(x_i\\). We perform an experiment to verify the effectiveness of such a design. Table 13 shows the testing accuracy on five in-distribution tasks. As can be seen, punishing \\(s_i^{(t)}\\) performs better on average."}, {"title": "I Disccusions on Incorporating Cost", "content": "As costs can also be an important metric to evaluate LLMs, we conduct experiments on two tasks (i.e., GSM8K and MBPP) of RouterBench [18] to consider the LLM costs. Specifically, we modify"}, {"title": "J Limitations", "content": "Due to the limited computational resources, we only evaluate RouterDC with candidate LLMs that have relatively small numbers of parameters (i.e., 8B for LLaMA-based LLMs and 7B for Mistral- based LLMs). However, there are many LLMs with more parameters and stronger capabilities available for public use (e.g., LLaMA-2-70B [43] and Mistral-8x7B [23]), making it reasonable to apply the RouterDC to these more capable but expensive models.\nMoreover, though RouterDC is designed as a query-based router, the framework can be extended to the chat context, e.g., selecting LLMs based on the recent conversation.\nWe leave the investigation of such scenarios to future work."}]}