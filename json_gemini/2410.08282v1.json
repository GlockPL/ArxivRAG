{"title": "FusionSense: Bridging Common Sense, Vision, and Touch for Robust Sparse-View Reconstruction", "authors": ["Irving Fang", "Kairui Shi", "Xujin He", "Siqi Tan", "Yifan Wang", "Hanwen Zhao", "Hung-Jui Huang", "Wenzhen Yuan", "Chen Feng", "Jing Zhang"], "abstract": "Humans effortlessly integrate common-sense knowledge with sensory input from vision and touch to understand their surroundings. Emulating this capability, we introduce FusionSense, a novel 3D reconstruction framework that enables robots to fuse priors from foundation models with highly sparse observations from vision and tactile sensors. FusionSense addresses three key challenges: (i) How can robots efficiently acquire robust global shape information about the surrounding scene and objects? (ii) How can robots strategically select touch points on the object using geometric and common-sense priors? (iii) How can partial observations such as tactile signals improve the overall representation of the object? Our framework employs 3D Gaussian Splatting as a core representation and incorporates a hierarchical optimization strategy involving global structure construction, object visual hull pruning and local geometric constraints. This advancement results in fast and robust perception in environments with traditionally challenging objects that are transparent, reflective, or dark, enabling more downstream manipulation or navigation tasks. Experiments on real-world data suggest that our framework outperforms previously state-of-the-art sparse-view methods. All code and data are open-sourced on the project website.", "sections": [{"title": "I. INTRODUCTION", "content": "Humans exhibit an extraordinary ability to perceive their surroundings by seamlessly integrating common-sense knowledge, vision, and touch, even when presented with sparse or incomplete views [1]. Common-sense reasoning helps bridge gaps in sensory data, vision offers a broad understanding of the environment, and touch provides fine-grained information about texture and material properties through direct physical interaction [2]. This synergy between cognitive and sensory inputs inspires more intuitive and efficient robotic perception in complex environments [3, 4].\nDespite recent advances, current robotic perception systems have yet to fully harness the multimodal capabilities that humans naturally employ. Emerging techniques like 3D Gaussian Splatting (3DGS) [5] show potential for flexible and efficient 3D reconstruction of intricate structures. However, vision-based methods [6, 7], especially those dependent on sparse-view observations [8], continue to face challenges such as occlusion, suboptimal lighting conditions, and difficult surfaces like transparent [9], reflective [10], or dark objects [11]. Approaches such as [12] leverage pre-trained models like DeepSDF [13] for shape completion, yet they still struggle with objects possessing unique geometries or intricate details. Conversely, high-resolution optical tactile sensors [14, 15] can overcome these limitations through direct physical interaction with high-resolution sensing, yet they have a limited sensing range. For example, the reinforcement learning strategy in [16] takes a cobot 1,631 touches to fully explore the surface of a banana in the YCB dataset [17], which has a surface area of only 216 cm\u00b2. Furthermore, while multimodal methods combining visual and tactile data have shown promise for improving object perception and 3D reconstruction, passive touch strategies often significantly increase the number of actions needed [11, 18].\nTo overcome these limitations, we present FusionSense, a novel 3D reconstruction framework that integrates priors from foundation models with sparse observations from both vision and tactile sensors. At the core of our framework is 3D Gaussian Splatting, which provides an efficient and scalable means to represent the environment. In this framework, surface normal supervision is highlighted to enrich both global and local geometric details [19, 20, 21, 22]. Specifically, FusionSense is built upon three key modules: (i) Robust Global Shape Representation, where hybrid structure priors are introduced to initialize geometry and ensure multi-view consistency alongside a hull-pruning constraint to guide optimization for both the scene and the object; (ii) Active Touch Selection, based on the observation that high gradients in 3DGS represent complex structures or mismatches between splatting and the image, combined with common-sense knowledge from foundation models for decision-making; and (iii) Local Geometric Optimization, where new anchor Gaussians are added to guide fine detail optimization, with geometric normals supervised by the high-resolution tactile feedback provided by the GelSight sensor.\nThese innovations lead to the following key contributions:\n1) We propose a novel 3D reconstruction framework for scenes and objects that fuses priors from foundation models with sparse observations from visual and tactile sensors, exploiting the unique strengths of each modality. We also develop an active touch strategy driven by geometric and common-sense cues, enhancing perceptual granularity with fewer robot actions. This framework can handle objects that are traditionally challenging for 3D reconstruction, such as transparent, reflective, or dark objects.\n2) We propose a novel hierarchical optimization strategy designed for 3DGS. This strategy incorporates object hull pruning to guide the optimization process and introduces anchor Gaussians at the local level, supervised by surface normals captured from tactile signals, to refine fine-grained details. Our work is the first to natively incorporate tactile signals into 3DGS.\n3) We deploy our algorithm on a real robot, demonstrating its competitive ability to reconstruct surroundings with challenging objects under highly sparse observations."}, {"title": "II. RELATED WORK", "content": "Roboticists have long been exploring tactile sensing. In 1984, Bajcsy and Goldberg [23] had already explored tactile surface reconstruction with primitive tactile sensors. Recently leap in tactile technology [14, 24] enabled great progress in object classification [25, 26], deformable object manipulation [27, 28, 29], industrial insertion [30, 31], etc.\nIn tactile-only reconstruction, researchers often employ an active strategy to select touch points due to the limited coverage area of tactile sensors. Many of them [32, 33] chose the Gaussian Process Implicit Surface as the representation for the shape, of which the derived uncertainty drives the selection strategy. Matsubara et al. [34] used end-effector travel distance as another constraint to accelerate the procedure, while Shahidzadeh et al. [16] sidestepped Gaussian Process and utilized reinforcement learning for an exploration policy.\nIn visuo-tactile works, visual signals can provide a rough global shape of the object, greatly reducing the number of touches and enabling passive touch strategies. Swann et al. [11] and Suresh et al. [18] employed a grid-like, exhaustive touch strategy. Smith et al. [35] only considered touch patch at the grasping spot. For active touch, Smith et al. [36] learned a strategy in simulation, while Bj\u00f6rkman et al. [37] and Wang et al. [38] again employed uncertainty in Gaussian Process. A key observation is that the uncertainty in the Gaussian Process usually comes from a lack of visual signal on certain parts. The part itself may be otherwise unremarkable. However, our active strategy also focuses on the geometrically complicated and fine-grained parts. In addition, our method is the first to employ the state-of-the-art Gaussian Splatting [5] method instead of a simple baseline method for initial reconstruction, the first to employ multiple foundation models, and the first to efficiently fuse tactile signal natively into Gaussian Splatting [5], unlike in Touch-GS [11] where the tactile signal is still incorporated via Gaussian Process Implicit Surface [39].\nGaussian Splatting for 3D Reconstruction\nGaussian Splatting [5] is a fast and efficient method for 3D reconstruction and radiance field rendering, representing scenes with Gaussian primitives to preserve continuous volumetric properties while enabling rapid optimization and real-time rendering. DN-Splatter [20] improves upon this by introducing geometric normal supervision, enhancing geometric accuracy, particularly in textureless regions, but its performance is limited under sparse-view observations."}, {"title": "III. METHOD", "content": "Our goal is to represent a previously unseen scene, S, using a set of differentiable 3D Gaussian primitives, G = {Gk:pk,qk,sk,ok,ck}k=1. The geometry of each Gaussian Gk is parameterized by its center pk \u2208 R\u00b3, rotation quaternion qk \u2208 R\u00b3, and scaling vector sk \u2208 R\u00b3. The appearance parameters include opacity ok \u2208 R, and color ck \u2208 R\u00b3. Rendering a new view is achieved by projecting 3D Gaussians into 2D space from the camera's perspective. The resulting 2D Gaussians are depth-sorted globally and then alpha-composited using the discrete volume rendering equation to compute the final pixel colors, \u0108, depth estimation, \u00ceD, and Normal estimation, \u00d1 [20]:\nC = \u2211 CkakTk, D = \u2211\u03b1\u03ba\u03b1\u03baTk, \u00d1 = \u2211\u03b7\u03ba\u03b1\u03ba\u03af\u03ba,\nKEN\nk-1\nkEN\nkEN\n(1)\nwhere Tk = \u03a0=1(1 \u2013 aj) is the accumulated transmittance at pixel location p and ak is the blending coefficient for a Gaussian with center \u03bc\u03b5 in screen space:\n\u03bc\u03ba)\nak = ok \u00b7 exp(\u2212(p2\n\u2212\u03bc\u03b5)T \u03a3\u22121\nk (p\u22122)). (2)\nIn particular, we are interested in a challenging object O that may be transparent, reflective, or dark. We aim to reconstruct it in Orec (in this case, 3D Gaussian primitives) as close to the original object as possible.\nTo this end, we collect the following sparse observations from vision and tactile sensors and will fuse them with priors from foundation models:\n\u2022 Color Ci and depth Di images, and their pose PC in the world frame with the following dimensions:\nCi \u2208 R1280\u00d7720\u00d73, D\u00bf \u2208 R1280\u00d7720\u00d71, P\u2208 SE(3).\n\u2022 Tactile signal T\u00bf and its pose PT in the world frame with the following dimension:\nTi \u2208 R240\u00d7320\u00d73, PT \u2208 SE(3).\nNote that tactile signals are saved as RGB images to be processed later. Also, note that the color and depth images are aligned so they share the same pose.\nMethod Overview\nOur method can be divided into three modules:\nRobust Global Shape Representation: This module leverages hybrid geometry priors and object hull pruning to optimize a global 3D representation, denoted as G, that contains the scene and the object of interest O. The hybrid geometry prior combines monocular depth estimates Di [40], camera poses PC, and visual hull results O [41] to produce an initial representation G'. During optimization, hull pruning eliminates floating artifacts and ensures a clean representation of the initial reconstructed object O', derived from both O and the global shape G. G' is supervised with color Ci, depth Di, and normal priors Ni from [19].\nActive Touch Selection: This module proposes touch points ti on O' where tactile feedback is needed. The robot then collects tactile signals Ti at these points. It consists of two sub-modules:\nA geometry-focused module that ranks points in regions with high gradients in 3DGS, indicating intricate structures or discrepancies between splatting and the image.\nA common-sense-driven module that utilizes large vision and language models (VLMs) to rerank points from the previous module, integrating common-sense knowledge from VLMs to enhance decision-making.\nLocal Geometric Optimization: This module takes in T\u00bf and the contact masks M, surface normals NT, and contact points PT [18], introducing an anchor Gaussian optimization strategy. Anchor Gaussians GT are initialized from PT and further refined using NT and global context. By integrating tactile signals T\u00bf into the global representation G, this module refines local geometric details."}, {"title": "C. Robust Global Shape Representation", "content": "To obtain a robust global representation G, we introduce hybrid structure priors and hull pruning strategies. Specifically, we adopt a variant of 3DGS [20] that incorporates surface normal supervision.\nHybrid structure priors are employed to ensure multi-view consistency. First, we estimate the coarse geometry O of the target object using a visual hull [41], which is constructed by combining camera poses P and segmented silhouettes M\u2081 extracted via Grounded SAM 2 [42]. This method is independent of surface appearance, resilient to challenging materials, and key to our success with objects that are otherwise challenging to traditional reconstruction methods. Next, we acquire the surrounding coarse geometry S using monocular depth priors Di from depth foundation model Metric3D v2 [40], along with the corresponding camera poses PC. These hybrid structure priors are fused by applying distance thresholds \u03c4\u03b1 to integrate O and S to produce the initial global representation G', which contains the initial reconstructed O', for further optimization.\nDuring the subsequent optimization process, we design hull pruning to remove the floaters in the exterior region outside the hull O. Gaussian primitives are particularly sensitive to these floaters, as they can slow convergence and result in suboptimal outcomes, especially when dealing with sparse observations. Hull pruning is achieved by introducing a thin shell Os surrounding the hull O. Os is defined by two thickness parameters: an interior thickness ti and an external thickness te. In our setup, t is set to be larger than 5 mm, corresponding to the voxel grid resolution of the visual hull, while t is empirically set to 2 cm. Then, similar to [20], we utilize RGB C\u2081 and depth Di image and normal N\u2081 estimated from normal foundation model DSINE [43] to respectively supervise \u0108, D, \u00d1 from our G', as can be seen in Fig. 2."}, {"title": "D. Active Touch Selection", "content": "An active touch strategy with geometric and common-sense cues can reduce the number of touches needed.\n1) Geometry: We capitalize on the design of the original 3DGS [5] that high gradients at each Gaussian primitive indicate rapid changes in spatial features and larger discrepancies between the rendering generated by the Gaussians and the image, which means a need for further optimization.\nGiven the objective O' for tactile interaction, we use the densification mean value from Sec. III-C as the gradient threshold Tg to select some Gaussian primitives Gr. Next, we apply DBSCAN [44] algorithm to cluster Gr, filtering out outliers. Then, all the selected Gaussians are ranked based on mean gradient values in its cluster, forming a ranking RG.\n2) Common Sense: The sub-module provides another ranking Re by leveraging common-sense knowledge from vision-language models (VLMs).\nFirst, we randomly select one color image from the captured images C and prompt GPT-4-0 [45] with the image and descriptive text to obtain a classification label and a list of relevant part names, along with a ranking Rp of the parts based on priority in touching.\nTo ground this common-sense ranking Rp to the object O', we utilize a zero-shot open-vocabulary part segmentation model, PartSLIP [46]. Based on a textual prompt of parts names, PartSLIP classifies each point of an extracted point cloud from O' into a specific part as in Fig. 3. From Sec. III-C, we know the coordinates of every Gaussian in G and every point in the extracted point cloud from O' as can be seen in Fig. 3. We also know each point's ranking in Rp. So, we iterate through Gr, assigning every Gaussian a rank based on the closest point in the point cloud, thus forming another ranking Re for selected Gaussians Gr.\nWe then sort G based on Re first and then RG, ensuring that even if PartSLIP in the second sub-module fails to work properly, we still have a reasonable, geometrically sensible touch sequence ti, as seen in Fig. 3"}, {"title": "E. Local Geometric Optimization", "content": "This module enhances local geometric detail by transforming the tactile signal T\u00bf into contact masks MT, surface normals NT, and contact points PT [18]. We then introduce anchor Gaussian optimization to integrate the tactile signals T into the global representation G.\nGiven a tactile signal Ti as an RGB image, because the tactile sensor is made of a gel patch that has consistent optical properties across all its surface, we can calculate a mapping between surface gradients (\u2202f/\u2202x, \u2202f/\u2202y) and the RGB value at a given location (r, g, b, x, y) based on photometric stereo [47]. In practice, this mapping is acquired by pressing a ball with a known radius on the gel patch and recording the corresponding tactile image. Then, a multi-layer perceptron can be trained after manually labeling the deformation caused by the ball. Assuming that the gel patch is the zero level surface of a scalar field f(x, y) = z, the contact surface normal N can be derived from the surface gradient as (\u2202f/\u2202x, \u2202f/\u2202y, -1) [14]. Applying a Poisson solver to integrate the surface gradients gives us a depth map of the gel patch's shape. We can then acquire a contact mask M and, consequentially, contact points P with a depth threshold.\nContact points PT are added as anchor Gaussians GT due to the scale difference between T\u2081 and visual signals C. Treating P as ground truth, we fix the center p and opacity of of GT, while optimizing the rotation q\u00b9, scale sT, and color cT. Notably, we apply Gaussian normal supervision directly to GT instead of normal images. This allows the integration of GT into G, combining local surface normals N with global information Ci, Di to refine the final geometry."}, {"title": "IV. EXPERIMENT", "content": "Our experiments are conducted using a GelSight Mini tactile sensor for acquiring tactile signal Ti, an Intel RealSense D405 for acquiring color Ci and depth Di images and a 6 DOF UFactory xArm 6 cobot with 0.1-millimeter repeatability. The camera and tactile sensor are mounted to the robot's end-effector with a 3D printed mount, so we know the dimensions and can easily calculate accurate transformations between each sensor and the end-effector, and therefore, the robot base, which also serves as the origin of our world frame."}, {"title": "C. Ablation Study", "content": "Hull Pruning: As mentioned in Sec. III-C, hull pruning is a major modification that enables our framework. As shown in Table IV, our framework without hull pruning suffers worse results in both scene and object reconstruction tasks. Without highly accurate depth supervision, many outliers will be generated during Gaussian Splatting field training. While the Realsense camera performs well for close-range scenes, it struggles with distant scenes and object edges. Additionally, depth estimates produced by large models like Metric3D v2 [40] may perform well in a single viewpoint, but they often have incorrect scaling and cannot be accurately projected into a 3D model within an entire 3D scene. Therefore, hull pruning is particularly important. It effectively prevents the Gaussian points of the target object from becoming blurred or losing edge clarity due to background interference, all while not disrupting the rendering of the surrounding scene.\nActive touch strategy is another major design in our pipeline. As shown in Table IV, our strategy yields slightly better results, although not across the board. There are several possible reasons: (1) the number of touches is limited, and the sizes of our objects are small in the scene. Thus, it is not easy to distinguish the effectiveness of different strategies. (2) Our first module in Sec. III-C gives unexpectedly outstanding precise results, leaving relatively little room for improvement for different touch strategies."}, {"title": "V. CONCLUSION, LIMITATION, AND FUTURE WORK", "content": "In this work, by fusing visual, tactile, and common-sense information, we propose a novel framework that significantly improves the state-of-the-art of scene and object reconstruction regarding challenging objects. Accompanying this framework, we propose a hierarchical optimization strategy designed for 3DGS that utilizes visual hull pruning and is the first to natively incorporate tactile signals into 3DGS without limiting touch numbers.\nMeanwhile, we realize the limitations of our experiments and methods. Due to time constraints and the fact that some of our comparable works are close-source, we are not able to conduct a more exhaustive comparison study that includes more methods using older reconstruction methods. In addition, our touch selection strategy can use more design and experiments. Currently, its effectiveness remains marginal, and the investigation into it remains limited as the number of touches is minimal. Another limitation lies in the process of extracting point cloud and mesh from our trained Gaussian primitives. Although Gaussian points from tactile sensors are anchored as geometrical regularization, the fine-grained geometrical tactile details cannot be fully extracted from trained Gaussian scenes. This is primarily because the tiny tactile Gaussian points cannot be fully sampled during the level-set extraction approach. To handle an extensive range of multi-scaled geometrical details from scene to tactile, novel strategies need to be developed.\nIn the future, we plan to introduce additional constraints, such as an ideal SDF loss, to ensure that Gaussian points are optimally distributed on the surface. Currently, tactile patches are acquired through teleoperated robot control, but developing an automated, servoing-based method could significantly increase the number of touch interactions we can perform."}, {"title": "APPENDIX", "content": "There are several challenges when it comes to actually acquiring the tactile patch we need.\nObject Mount: Because the object will be touched, there needs to be a way to prevent it from moving when in contact so that the pose of the object we acquire during reconstruction can still be valid when registering the tactile patch. This demand calls for a secure way to fasten the object. We use a Peak Design camera tripod as the mount for the object so the height of the object can be freely adjusted. The tripod also has a mechanism to add dead weight so the mount will not wobble when the object is pushed by the robot. On top of the tripod, we use an Arca-Swiss plate to serve as a secure interface between the tripod and the platform that the object stands on. Arca-Swiss is a professional camera mount plate that can securely fasten a camera on a tripod. It uses a quarter-inch screw to connect to the tripod. We 3D print a mounting platform that inserts into the groove of the Arca-Swiss plate so that the platform can be securely attached to the tripod. We then glue the object of interest to a wooden plate and use two clamps to secure it on the mounting platform, as seen in Fig. 1. Needless to say, this process is not practical and calls for investigation into how to dynamically combine tactile and visual information, even when the object of interest is moved during the process of touching. All the parts that need 3D printing can be found in our GitHub repository.\nRobot Control: After our algorithm gives the coordinates that need to be touched, controlling the robot to actually touch it is another challenge. We tried two methods. (1) We teleoperate the robot to touch the object given the coordinate information. (2) We calculate the normal vector of the touch coordinate and position the z-axis of the end-effector to align with that normal vector, and then use MoveIt Servo to send a twist command to drive the end-effector on the z-axis toward the object. In practice, we found no significant difference in terms of results when both methods work because the tactile patch is significantly larger than the granularity of a coordinate point, so we have a large margin of error. However, the second method often suffer from singularity issue so the success rate is lower."}]}