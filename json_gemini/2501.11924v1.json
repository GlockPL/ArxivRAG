{"title": "Make Full Use of Testing Information: An Integrated Accelerated Testing and Evaluation Method for Autonomous Driving Systems", "authors": ["Xinzheng Wu", "Junyi Chen", "Jianfeng Wu", "Longgao Zhang", "Tian Xia", "Yong Shen"], "abstract": "Testing and evaluation is an important step before the large-scale application of the autonomous driving systems (ADSs). Based on the three level of scenario abstraction theory, a testing can be performed within a logical scenario, followed by an evaluation stage which is inputted with the testing results of each concrete scenario generated from the logical parameter space. During the above process, abundant testing information is produced which is beneficial for comprehensive and accurate evaluations. To make full use of testing information, this paper proposes an Integrated accelerated Testing and Evaluation Method (ITEM). Based on a Monte Carlo Tree Search (MCTS) paradigm and a dual surrogates testing framework proposed in our previous work, this paper applies the intermediate information (i.e., the tree structure, including the affiliation of each historical sampled point with the subspaces and the parent-child relationship between subspaces) generated during the testing stage into the evaluation stage to achieve accurate hazardous domain identification. Moreover, to better serve this purpose, the UCB calculation method is improved to allow the search algorithm to focus more on the hazardous domain boundaries. Further, a stopping condition is constructed based on the convergence of the search algorithm. Ablation and comparative experiments are then conducted to verify the effectiveness of the improvements and the superiority of the proposed method. The experimental results show that ITEM could well identify the hazardous domains in both low- and high-dimensional cases, regardless of the shape of the hazardous domains, indicating its generality and potential for the safety evaluation of ADSS.", "sections": [{"title": "1. Introduction", "content": "The safety of autonomous driving systems (ADS) is a pivotal issue that demands comprehensive verification prior to the large-scale deployment (Sohrabi et al., 2021). As a crucial aspect of safety, the safety of the intended function-ality (SOTIF), as defined in the automotive safety standard ISO 21448 (ISO 21448:2022), focuses on and endeavors to eliminate hazards or risks that caused by insufficien-cies of specification or performance limitations of ADSs. According to the standard, in the verification phase, the SOTIF problem can be addressed by simulation testing to evaluate ADSs under known hazardous scenarios (Wang et al., 2024). Currently, based on the three level of scenario abstraction theory (Menzel et al., 2018), scenario-based simulation testing and evaluation has become the prevailing verification method to address SOTIF problem due to its low cost, high efficiency and repeatability, which has garnered widespread attention from both academia and industry (Sun et al., 2022a).\nIn practice, testing and evaluation are mainly conducted on the logical scenario level (Peixing Zhang, 2022). How-ever, due to the high complexity and uncertainty of the external environment of high-level ADS, the logical scenario space constructed is usually high-dimensional, leading to the \"dimensionality explosion\" problem (Feng et al., 2023). To address this issue, optimization algorithms are introduced by numerous researchers to effectively search hazardous scenar-ios in the whole logical scenario space (Zhang et al., 2023b)."}, {"title": "2. Related Works", "content": "In the current literature, safety testing methods for ADS can be classified as Naturalistic Driving Data-based (NDD-based), Design of Experiments-based (DOE-based), and adaptive DOE-based (ADOE-based) methods.\nBy statistically analyzing the collected naturalistic driv-ing data, NDD-based methods directly extract testing sce-narios to satisfy specific testing requirement. For example, by translating the data from different sources into a stan-dardized format and calculating the possibility of recombi-nation between scenario slices, the project PEGASUS pro-posed a scenario extraction and classification method (P\u00fctz et al., 2017). Similarly, Yin et al. extracted hazardous lane-changing testing scenarios from China-FOT naturalistic data by establishing a scenario risk classification model and an excellent human driver model (Yin et al., 2023). Further-more, Feng et al. first extracted all cut-in events in a NDD database and then applied the seed-fill method to search hazardous scenarios (Feng et al., 2020). Besides, Importance Sampling (IS) was also widely used to accelerate rare-scenario probability estimation, thus increasing the number of rare scenarios in NDD sampling (Zhao et al., 2017; Feng et al., 2021). Since the generated testing scenarios are derived from real driving data, NDD-based methods are of high realism. However, these methods rely on a large amount of high-quality data and the generated scenarios are limited to the known data, resulting in low generalizability.\nInstead of directly extracting testing scenarios from the collected data, DOE-based methods design testing scenarios by using different combinations of scenario parameters, where all testing scenarios are generated before the start of the test. Combinatorial testing is a typical representative of the DOE-based approaches. Bagschik et al. constructed a five-layer scenario model and generated functional scenarios based on parameter combinations (Bagschik et al., 2018). Li et al. described the environment of VUT through an ontology model, which was then used as input for combinatorial testing (Li et al., 2020). Apart from that, methods such as random testing, near-random testing (e.g. Latin Hypercube Sampling (Batsch et al., 2019)) and grid testing can also be considered as DOE-based methods. DOE-based methods ignore the information of the VUT that is gradually acquired during the testing process, and the subsequent testing scenar-ios cannot be designed with reference to the test results of the completed scenarios, leading to low testing efficiency.\nCompared with DOE-based methods, ADOE-based methods generate testing scenarios step by step following the idea of optimization. According to the testing results of previously generated testing scenarios, the parameters of new testing scenarios are adaptively designed to be more challenging for VUT. Currently, existing methods have ap-plied various optimization algorithms in the safety testing"}, {"title": "2.1. Safety Testing Methods for Autonomous Driving Systems", "content": "In the current literature, safety testing methods for ADS can be classified as Naturalistic Driving Data-based (NDD- based), Design of Experiments-based (DOE-based), and adaptive DOE-based (ADOE-based) methods.\nBy statistically analyzing the collected naturalistic driv- ing data, NDD-based methods directly extract testing sce- narios to satisfy specific testing requirement. For example, by translating the data from different sources into a stan- dardized format and calculating the possibility of recombi- nation between scenario slices, the project PEGASUS pro- posed a scenario extraction and classification method (P\u00fctz et al., 2017). Similarly, Yin et al. extracted hazardous lane- changing testing scenarios from China-FOT naturalistic data by establishing a scenario risk classification model and an excellent human driver model (Yin et al., 2023). Further- more, Feng et al. first extracted all cut-in events in a NDD database and then applied the seed-fill method to search hazardous scenarios (Feng et al., 2020). Besides, Importance Sampling (IS) was also widely used to accelerate rare- scenario probability estimation, thus increasing the number of rare scenarios in NDD sampling (Zhao et al., 2017; Feng et al., 2021). Since the generated testing scenarios are derived from real driving data, NDD-based methods are of high realism. However, these methods rely on a large amount of high-quality data and the generated scenarios are limited to the known data, resulting in low generalizability.\nInstead of directly extracting testing scenarios from the collected data, DOE-based methods design testing scenarios by using different combinations of scenario parameters, where all testing scenarios are generated before the start of the test. Combinatorial testing is a typical representative of the DOE-based approaches. Bagschik et al. constructed a five-layer scenario model and generated functional scenarios based on parameter combinations (Bagschik et al., 2018). Li et al. described the environment of VUT through an ontology model, which was then used as input for combinatorial testing (Li et al., 2020). Apart from that, methods such as random testing, near-random testing (e.g. Latin Hypercube Sampling (Batsch et al., 2019)) and grid testing can also be considered as DOE-based methods. DOE-based methods ignore the information of the VUT that is gradually acquired during the testing process, and the subsequent testing scenar- ios cannot be designed with reference to the test results of the completed scenarios, leading to low testing efficiency.\nCompared with DOE-based methods, ADOE-based methods generate testing scenarios step by step following the idea of optimization. According to the testing results of previously generated testing scenarios, the parameters of new testing scenarios are adaptively designed to be more challenging for VUT. Currently, existing methods have ap- plied various optimization algorithms in the safety testing"}, {"title": "2.2. Safety Evaluation Methods for Autonomous Driving Systems", "content": "With the testing results in hand, existing methods achieved safety evaluation for ADS either based on points (namely the testing results of concrete scenarios) or areas (namely hazardous domains) under a logical scenario space. Points-based methods directly calculate the number or percentage of the hazardous concrete scenarios, in which the VUT does not fulfill the passing condition (a collision occurs or the safety metric exceeds a certain threshold), to represent the safety performance of VUT. Bussler et al. used an evolutionary algorithm to identify hazardous scenarios while the number of the identified scenarios was used to evaluate VUT (Bussler et al., 2020). Feng et al. regarded the accident rate as the evaluation metrics, where an accident was identified when the relative distance between the VUT and a virtual background vehicle was zero or less than zero under an augmented reality (AR) testing platform (Feng et al., 2020). Ding et al. proposed a flow-based multimodal"}, {"title": "3. Method", "content": null}, {"title": "3.1. Framework", "content": "The framework of the proposed Integrated Accelerated Testing and Evaluation Method (ITEM) is shown in Fig. 2, which can be divided into accelerated testing stage and accelerated evaluation stage. The above two stages are inte-grated together to ensure that testing information is utilized as much as possible. The green parts highlight the improve-ments in this paper compared with our previous work (Wu et al., 2024a,b).\nAs depicted in Fig. 2, in the accelerated testing stage, an initial random sampling is conducted at first. After that, con-crete testing scenarios are generated based on the sampling results, which are then inputted to the in-loop testing. Next, the dual surrogates model is constructed, in which both the transient state information during testing and the overall tra-jectory information after testing are utilized. More in detail, during the testing, a certain safety metric is calculated using transient state information such as positions and velocities of ego and background vehicles to construct the result surrogate model (in this paper, we apply DNDA (Wu et al., 2022) as the safety metric). And after the testing, with a bird's eye view (BEV) recording the overall trajectory information and a risk label calculated by the safety metric as input, a CNN is used to construct the behavior surrogate model. It should be noted that an additional observation behavior surrogate model is trained in this paper, whose prediction performance will be used as a reference for the stopping condition. The settings of the observation behavior surrogate model and the construction of the stopping condition will be detailed in Sec. 3.2.2.\nAfter checking the stopping condition, the entire logical space is recursively partitioned into good and bad subspaces based on risk values, and a tree structure is then constructed. Then, Upper Confidence Bound (UCB) of each node is calculated with the improved calculation method. Based on the UCB result, promising subspaces with a high probability of containing hazardous scenarios are chosen to generate new sampling points. The above process continues until the stopping condition is reached.\nOnce the accelerated testing stage is finished, the sam-pling records, as well as the well-trained testing behavior surrogate model and the tree structure of search space are"}, {"title": "3.2. Improvements on Accelerated Testing Stage", "content": "Compared with our previous work (Wu et al., 2024a,b), two major improvements are proposed in this paper: 1) An improved UCB calculation method that emphasizing the value of boundary subspaces, 2) The stopping condition based on the prediction performance of an additionally trained observation model."}, {"title": "3.2.1. Improved UCB Calculation Method", "content": "As the basis for subspace selection, the computation result of UCB directly determines the direction and result of the searching of hazardous scenarios, and thus has been deeply investigated in our previous work. In Wu et al. (2024b), we have introduced density of subspaces into the calculation of UCB to overcome the sampling bias phe-nomenon. And in (Wu et al., 2024a), we have integrated UCB with the loss of CNN to choose the promising sub-spaces more accurately. In this paper, in order to serve the purpose of hazardous domain identification, it is necessary to find hazardous scenarios located at the boundary of the hazardous domain as many as possible. To this end, we first define boundary subspace as the subspace that contains both hazardous and safe scenarios, as shown in the following equation.\n$G = G_{b}, if max(f(x_{i})) > f_{b} and min(f(x_{i})) < f_{b}, i \u2208 {1, 2, ..., t}$  (1)\nwhere $G$ represents a subspace and $G_{b}$ is the identified boundary subspace. $x_{i}$ is the $i$th sampled point belonging to $G$ that represents a concrete scenario. $f(.)$ is a certain safety metric. And $f_{b}$ is the threshold to determine whether the scenario $x_{i}$ is hazardous or not.\nBy attributing additional value to the identified boundary subspaces, the algorithm tends to select these boundary subspaces for searching, thereby achieving the aim of finding as many hazardous scenarios as possible at the boundary of the hazardous domains. The additional value of each boundary subspace $V_{boundary}$ can be calculated as:\n$V_{boundary} = AVG (\\sqrt{sin( \\frac{\\{min[f(x_{i,above})] - f_{b}\\} \\cdot \\pi `}{2(f_{up} - f_{b})} )} + \\sqrt{sin( \\frac{\\{f_{b} - max[f(x_{i,below})])\\} \\cdot \\pi`}{2(f_{b} - f_{low})} )})$.  (2)\nwhere $f_{up}$ and $f_{low}$ are the upper and lower bounds of the safety metric value. $x_{i,above}$ and $x_{i,below}$ represent all the points within the boundary subspace that above and below the threshold $f_{b}$. The formula first finds the closest scenarios to the threshold in $x_{i,above}$ and $x_{i,below}$, and then calculates the mean value of the distance of these two scenarios from the threshold to measure the degree of exploration of the hazardous domain boundary within the boundary subspace. A larger value indicates that the hazardous domain boundary is under-explored within the boundary subspace and more sampling is needed.\nAdditionally, in order to avoid the algorithm falling into a local optimum at the beginning due to over-sampling in the boundary subspaces, this paper introduces the idea of Ran-dom Dropout in neural network training. When the value of the boundary subspace is computed, this value is randomly deactivated with a probability $P_{drop}$, thus preventing over-sampling in certain boundary subspace. More in detail, the probability $P_{drop}$ in this paper is set to an adaptive value that decreases with the increase in the number of sampled points. By doing so, it can avoid the algorithm falling into local optima in the early sampling stage, while ensuring that the algorithm focuses on the boundary subspace in the late sampling stage, which is conducive to accurately identifying the boundary of the hazardous domains. The probability $P_{drop}$ is defines as:\n$P_{drop} = \\begin{cases} 1-1/k \\times N_{sample}, if N_{sample} < k \\\\ 0, if N_{sample} \u2265 k \\end{cases}$ (3)\nwhere $N_{sample}$ is the number of sampled points and k is a tunable hyperparameter, which is determined by the dimen-sion of the search space and the sampling budget.\nFinally, the UCB calculation method that considers the subspace density, the loss of the CNN, and the value of the boundary subspace is shown in Eq. 4.\n$UCB(A \u2192 B) = V_{exploit} \u00b7 E_{\u03b5} + C_{p} \u00b7 E_{\u03c1}$ (4)\nAs shown in Eq. 4, the UCB from parent space A to child space B consists of two main terms. The former represents the exploitation value of subspace B, expressed as the product of the subspace value $V_{exploit}$ the loss $E_{\u03b5}$ output by the result and behavior surrogates model on that subspace, respectively. The latter represents the exploration value of subspace B, expressed as the ratio of the density of sampled points in the parent space to the subspace (as shown in Eq. 7). cp is a tunable hyperparameter used to balance exploitation and exploration."}, {"title": "3.2.2. Stopping Condition", "content": "In this paper, we define the stopping condition based on the prediction performance of the behavior surrogate model. If the model is able to predict accurately on the test set after training with the recorded sampled points, the search is sufficiently adequate to stop, otherwise the search should be"}, {"title": "3.3. Improvements on Accelerated Evaluation Stage", "content": "As illustrated in Fig. 2, both points-based and areas-based evaluation method are included in our framework. Since the points-based approach has been well described in (Wu et al., 2024a), this paper focuses on the areas-based approach to identify the hazardous domains.\nDifferent from other studies that use only historical sam-pled points information, the hazardous domain identification method proposed in this paper also takes the tree structure into account. More in detail, two aspects of information are included in the tree structure transferred from the accelerated testing stage: 1) The affiliation of each historical sampled point with the subspaces, 2) The parent-child relationship between subspaces.\nFor ease of interpretation, a schematic diagram is demonstrated in Fig. 4, where all the subspaces are parti-tioned in the way shown in Fig. 2-S4, and assuming that the real distribution of the hazardous domains is as shown in the scatter plot in Fig. 2. The identification method can be divided into four steps as follows:\n1) Hazardous Scenarios Selection: All hazardous scenar-ios with safety metric values f(x\u2081) exceeding the threshold fo are selected first, as illustrated by the yellow dots in Fig. 4. After that, only subspaces containing hazardous scenarios are retained, while non-hazardous scenarios in these sub-spaces are also excluded.\n2) Subspace Hazardous Domains Approximation: Hyper-cuboids are used in this step to approximate the hazardous domain in each retained subspace, which are represented by the red boxes in Fig. 4. To be specific, the upper and lower bound of a hyper-cuboid in each dimension can be determined as:\n$\\begin{cases} B_{lower,j} = min((x)_{i,j}) \\\\ B_{upper,j} = max((x)_{i,j}) \\end{cases}$ (8)\nwhere i \u2208 {1,2, ..., T } represents the ith hazardous scenario in a certain subspace that has totally T hazardous scenarios."}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Synthetic Function and Practical Scenario for Testing", "content": "In this paper, both synthetic function and practical sce-nario are considered. Meanwhile, the experiments cover cases from two to four dimensions, demonstrating the effec-tiveness of the method under both low and high dimensions."}, {"title": "4.1.1. Two- and Four-dimensional Multimodal Gaussian Function", "content": "This paper modifies the gaussian function, which only has one global optimum (i.e., one modality), into a multi-modal gaussian function that can be defined as follows:\n$f(x) = \\sum_{i=1}^{dim} e^{-\\frac{|| x_{i} ||^{2}}{2\\sigma^{2}}}, where x_{i} = ||x + M(i,:)||^{2}, for all i = 1, 2, ..., dim$ (9)\nin which,\n$M = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ : & : & : \\\\ 0 & 0 & 1 \\end{bmatrix} \\times bias$ (10)\nwhere x = ($x_{1}$, $x_{2}$, \u2026, $x_{dim}$) represents a certain point in the search space. dim is the dimension assigned to the function. M(i,:) represents the ith row of the matrix M. bias and \u03c3 are hyperparameters that determine the size and distribution of the hazardous domains of the function. Based on Eq. 9, the multimodal gaussian function with dim dimen-sions has dim modalities, which are respectively located in $x_{1}^{`}$ = (-bias, 0, \u2026, 0), $x_{2}^{`}$ = (0, -bias, \u2026, 0), \u2026, $x_{dim}^{`}$ = (0,0,..., -bias).\nSpecifically in this paper, the search space is built with a range of $x_{i}$ \u2208 [-20, 20] in each dimension, and the value domains of both functions are [0, 1]. The hazardous thresh-old fo is chosen as 0.8 (i.e., points with a function value greater than 0.8 will be considered hazardous). Moreover, hyperparameters are set as bias = 10 and \u03c3 = 3 in both functions. For the two-dimensional function, an illustration is given in Fig. 5, with red lines representing f\u2081. It can be seen that two modalities are located in (0, -10) and (-10,0) as expected. As for the four-dimensional function, by numer-ical calculation, it can be estimated that the percentage of the"}, {"title": "4.1.2. Three-dimensional Cut-in Scenario", "content": "Based on the simulation platform Virtual Test Drive (VTD) (Hexagon, 2025), a logical testing scenario is con-ducted. As demonstrated in Fig. 6, the VUT, which is con-trolled by the built-in ADS in VTD, is driving in the left lane with an initial speed of $V_{0}$, ahead of which a background vehicle BV1 is driving in the middle lane at a distance $S_{1}$. At t seconds after the start of the test, BV1 will perform a cut-in maneuver at the speed of $V_{1}$, and the whole cut-in process will last for $T_{ic}$ seconds. Detailed parameters of this logical testing scenario can be found in Table 1, where $S_{1}$, $V_{1}$ and $T_{ic}$ are parameters that construct the 3-dimensional search space, while $V_{0}$ and t are fixed parameters. Additionally, we choose DNDA (Wu et al., 2022) as the safety metric. DNDA is a normalized risk indicator based on drivable area. The closer its value is to 1, the more hazardous the scenario is. And a collision occurs when its value is equal to 1.\nUnlike synthetic functions where the distribution of haz-ardous points can be quickly obtained through numerical computation, the distribution of hazardous scenarios in the logical scenario space needs to be obtained through simula-tion. To this end, a 30 \u00d7 30 \u00d7 30 grid testing is executed to obtain the ground truth distribution of hazardous scenarios, which can be used as a reference for subsequent evaluation. In this experiment, scenarios with a maximum DNDA value of more than 0.8 are considered hazardous. For ease of"}, {"title": "4.2. Algorithms to be Tested", "content": "To validate the effectiveness of the improved method proposed in this paper as well as its superiority compared to other baseline methods, both the ablation experiment and the comparative experiment are conducted. The tested algorithms are detailed as follows.\n1) Integrated accelerated Testing and Evaluation Method (ITEM) : ITEM is the method proposed in this paper, which uses the improved UCB considering the value of boundary subspaces as described in Sec. 3.2.1 and makes full use of the testing information in the evaluation stage.\n2) Integrated accelerated Testing and Evaluation Method with the Original UCB (ITEMoriUCB) : Compared with ITEM, ITEM with the original UCB uses the original UCB calculation method in Wu et al. (2024a), which doesn't take the boundary value into account.\n3) Gaussian Distribution Method with Optimization Sampling (GDMos)) : GDM is an evaluation method pro-posed in Zhu et al. (2022), which uses gaussian distribution to cluster the hazardous scenarios and then obtains the hazardous domains. GDM is originally input with sampled points obtained by the optimization search method in Zhu et al. (2022). Since this evaluation method is independent of the testing stage, for the fairness of the comparison, this paper uses the sampling results of ITEM in the accelerated testing stage as the input to GDM.\n4) Gaussian Distribution Method with Random Sam-pling (GDMrs) :To further investigate the impact of different search algorithms in the testing stage on the evaluation stage, this paper also conducts random sampling and inputs the sampling results into the GDM.\nDetailed information of each algorithm is listed in Ta-ble 2. It should be noted that the input to the behavior surrogate model must be scenario trajectory information. Consequently, neither the behavior surrogate model nor the"}, {"title": "4.3. Evaluation Metrics", "content": "The main purpose of this paper is to make full use of the intermediate testing information into the safety evalu-ation, thus focusing more on the effect of improvements on the evaluation stage. Therefore, this paper proposes two indicators to quantify the evaluation performance, namely the accuracy of hazardous domain distribution identification (ADI) and the accuracy of hazardous domain percentage identification (API). For the evaluation metrics in the testing stage, this paper follows the F2 - Score mentioned in Wu et al. (2024b) (denoted as $F_{2-grid}$ in this paper).\n1) $F_{2-grid}$ : F2 Score is a typical evaluation metric that widely used in machine learning. Compared with $F_{2-obu}$ which uses the test set divided from the sampling records as the actual values in confusion matrix, $F_{2-grid}$ uses the grid testing data in Fig. 7 to see if the algorithm correctly predicts the values at the grid points. Noted that calculating $F_{2-grid}$ takes more computational resources than calculating $F_{2-obu}$ because the predicted values for all grid points need to be calculated. Therefore, $F_{2-grid}$ is only used for the evaluation of different algorithms and not for the determination of the stopping conditions.\n2) Accuracy of Hazardous Domain Percentage Iden-tification (API): API is used to measure the percentage of ground truth hazardous domains covered by identified hazardous domains. Suppose there are n hazardous domains in the dim-dimensional search space A, and the range of"}, {"title": "4.4. Results and Analysis", "content": "The results of the evaluation metrics for each algorithm are shown in the Table 3. All the evaluation metrics are calculated after the sampling budgets are run out. In the 3-d cut-in scenario experiment, the search is stopped at 2750 samples according to the stopping condition, which is detailed in Sec. 4.5."}, {"title": "4.4.1. The Efficiency of the Proposed ITEM", "content": "As shown in Table 3, ITEM achieves the best perfor-mance compared with other baseline methods on both API and ADI metrics, showing the effectiveness and superiority of the proposed method in this paper.\nCompared to ITEMoriUCB that uses the original UCB calculation method, ITEM obtains higher API and ADI in the evaluation stage, as well as an approximately equal $F_{2-grid}$ in the testing stage, indicating that ITEM enhances the evaluation performance without affecting the testing efficiency. This part will be detailed in Sec. 4.4.2. Compared to GDMos which uses the same sampling records from the testing stage, ITEM outperforms it by 20%, 28.5% and 10.6% respectively for the three cases in terms of API, indicating that the identified hazardous domains of ITEM cover more of the ground truth hazardous domains and have fewer overestimations.\nWhen it comes to the performance of ITEM on different test cases, it can be seen that in the two low-dimensional cases, both API and ADI of ITEM exceed 0.9, representing an accurate identification of the percentage and distribution of hazardous domains. While in the 4-dimensional case, ITEM obtains a high ADI of 0.980 but a relatively low API. By analyzing the detailed boundary data, we find that the identified hazardous domains achieve essentially more than 90% coverage in each dimension of each modality, but since the hypervolume calculation in Eq.11 is multiplicative, the volume ratio will inevitably decrease as the dimensions get higher. Therefore, a slightly lower API is acceptable for high-dimensional cases."}, {"title": "4.4.2. The Efficiency of the Improved UCB", "content": "As discussed above, the use of the improved UCB does not diminish the efficiency of the algorithm in the testing stage (in some cases the search efficiency is even higher), while it significantly improves the accuracy of the hazardous domain identification in the evaluation stage. An illustration of the sampling dynamics of ITEMoriUCB and ITEM on the 2-d test case is shown in Fig. 8. For the entire search space, both algorithms converge quickly to the two hazardous do-mains of the synthetic function after only a small amount of global sampling. However, when we concentrate on a certain modality, a clear difference between the two algorithms is observed. As shown in the magnified parts of Fig. 8, ITE-MoriUCB focuses only on the most hazardous regions and continues sampling on the center of the modality, whereas ITEM places more samples in the vicinity of the boundary of the hazardous domain, resulting in a full exploration of the hazardous domain boundary.\nNote that even though there is a large difference between the two algorithms in the visualization results of the 2-d case, this difference is not obviously reflected in the computed API and ADI. We speculate that this is because the ground truth hazardous domains are circles, and even if ITEMoriUCB only focuses on sampling at modality centers, there will still be sporadic sampled points falling near the boundary of the hazardous domains, which improves the accuracy of hazardous domain identification. This phenomenon does not occur in the 3-d case, since its ground truth hazardous points show a rectangular distribution."}, {"title": "4.4.3. The Impact of the Shape of the Ground Truth Hazardous Domains", "content": "From the above analysis, we can find that the shape of the ground truth hazardous domain has a significant effect on the accuracy of hazardous domain identification. Furthermore, it can be found from Table 3 that all the algorithms get a higher value of ADI on both synthetic functions, whereas the ADI results are lower in the cut-in scenario, except for ITEM. We argue that this is because the ground truth hazardous points are isotropically distributed in both synthetic functions, so that even if the sampling is focused only on the modality center, it can still guarantee the accuracy of the distribution identification. As a comparison, since the ground truth haz-ardous points in the 3-d case are anisotropically distributed, all the baseline algorithms focusing only on the center of modalities do not have a high ADI in this case, while the method proposed in this paper still obtains an ADI of more than 0.9.\nIn practice, in some relative studies (Sun et al., 2022b; Zhu et al., 2022) and standards such as UN ECE R157 (UN ECE R157), the distribution of the hazardous scenarios are always anisotropical and the ground truth hazardous domains vary in shape. Therefore, an algorithm that is independent of the shape of hazardous domains is needed to achieve accurate identification of hazardous domains in various scenarios. The results in Table 3 show that the ITEM proposed in this paper is highly versatile and has great potential for hazardous domain identification in diverse logical scenarios."}, {"title": "4.4.4. The Impact of Search Algorithms on the Evaluation Stage", "content": "For GDM, in our experiment we respectively use the sampled points obtained from the optimization search and the random search as inputs for hazardous domains iden-tification. From Table 3, it can be found that the identifi-cation results obtained based on the optimization sampled points are better than those obtained based on the randomly sampled points. And this gap is particularly noticeable in the 4-d case, where the $F_{2-grid}$ of GDMrs is only 0.029 because of the high dimensionality and the sparse distribu-tion of hazardous points, resulting in almost no hazardous points available for hazardous domain identification. The above discussion illustrates that sampling efficiency in the testing stage has a significant impact on the evaluation stage, especially in a high-dimensional search space. Since the efficiency and coverage of the optimization algorithm used in our method have been well validated in Wu et al. (2024b), ITEM can be provided with a comprehensive input in the evaluation stage to ensure an accurate identification of the hazardous domains."}, {"title": "4.4.5. Analysis of the Application of the Identified Hazardous Domains", "content": "Based on experimental results of the 3-d cut-in sce-nario, this section delves into the real-world implications of an identified hazardous domain within a practical context. Moreover, this section conducts a further examination of how the results of hazardous domain identification can be applied to safety evaluation. The testing and evaluation results of ITEM on the 3-d cut-in scenario is shown in Fig. 9, where the yellow dots represent the searched hazardous scenarios (these scenarios have a DNDA > 0.8), while the green dots represent the searched safe scenarios. The red dashed rectangular box in Fig. 9 is the hazardous domain identified by ITEM.\nAs shown in Fig. 9, it is evident that the points searched by the algorithm are distinctly clustered within and in the vicinity of the hazardous area as shown in Fig. 7, demon-strating the algorithm's capacity to accelerate the testing process. Secondly, all the searched hazardous scenarios are identified as one hazardous domain. In order to analyze the similarities and differences of different hazardous scenarios within the same hazardous domain, we select the two most distant hazardous scenarios within the hazardous domain (namely the Hazardous scenario 1 and 2 labeled in Fig. 9) for a case study. The two scenarios are visualized in Fig. 10, with the length of the arrow representing the velocity of the vehicles.\nBy observing Fig. 10, we can find that although the initial conditions are different in the two scenarios, the hazards occur for the same reason. Specifically, in both scenarios, BV1 starts to change lanes at 3s, and then at 4.5s, BV1 starts to/has entered the VUT's lane. However, at this moment, VUT does not execute any evasive maneuvers and remains in its original speed. Actually, it is not until BV1 almost completes its lane changing maneuver that VUT begins to brake. The delayed response of the VUT is inferred to be the cause of the hazards in both scenarios. The above analysis leads us to the conclusion: Different hazardous scenarios within the same hazardous domain share the same hazard generation mechanism. Therefore, when evaluating the safety performance of VUTs, it is sufficient to analyze only one scenario in each hazardous domain, rather than analyzing each hazardous scenario individually, which sig-nificantly enhances the efficiency of safety evaluation."}, {"title": "4.5. Stopping Condition Validation", "content": "The stopping condition proposed in this paper is used in the 3-d cut-in scenario. Specifically, we perform the stopping condition checking after 50"}]}