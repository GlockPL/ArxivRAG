{"title": "Differentiable Quantum Architecture Search in Asynchronous Quantum Reinforcement Learning", "authors": ["Samuel Yen-Chi Chen"], "abstract": "The emergence of quantum reinforcement learning (QRL) is propelled by advancements in quantum computing (QC) and machine learning (ML), particularly through quantum neural networks (QNN) built on variational quantum circuits (VQC). These advancements have proven successful in addressing sequential decision-making tasks. However, constructing effective QRL models demands significant expertise due to challenges in designing quantum circuit architectures, including data encoding and parameterized circuits, which profoundly influence model performance. In this paper, we propose addressing this challenge with differentiable quantum architecture search (DiffQAS), enabling trainable circuit parameters and structure weights using gradient-based optimization. Furthermore, we enhance training efficiency through asynchronous reinforcement learning (RL) methods facilitating parallel training. Through numerical simulations, we demonstrate that our proposed DiffQAS-QRL approach achieves performance comparable to manually-crafted circuit architectures across considered environments, showcasing stability across diverse scenarios. This methodology offers a pathway for designing QRL models without extensive quantum knowledge, ensuring robust performance and fostering broader application of QRL.", "sections": [{"title": "I. INTRODUCTION", "content": "Quantum computing (QC) theoretically possesses the potential to fundamentally transform computational tasks, presenting clear advantages over classical computers [1]. The advancement in QC hardware, coupled with classical ML techniques, enables the progression of quantum machine learning (QML). This is realized through the hybrid quantum-classical computing paradigm [2], [3], wherein both classical and quantum computers are harnessed. Specifically, computational tasks suited for QC capabilities are executed on quantum computers, while tasks such as gradient calculations, well-handled by classical computers, remain within their domain. The variational quantum circuit (VQC) serves as the fundamental component in existing QML methodologies. A plethora of QML models, based on VQC, have been developed to address various machine learning (ML) tasks including classification [4]\u2013[9], time-series prediction [10]-[12], generative modeling [13], [14], natural language processing [15]\u2013[19], and reinforcement learning [20]\u2013[26]. Despite the achievements of various QML models, a significant challenge hindering widespread adoption is the necessity for extensive expertise in designing effective quantum circuit architectures. For instance, crafting the structure of encoding and parameterized circuits within the VQC demands specialized design considerations, including appropriate entanglement to showcase quantum advantages. There exists a pressing demand for an automated procedure capable of streamlining the search for high-performing quantum circuit architectures. In this manuscript, we address this challenge through the implementation of differentiable quantum architecture search (DiffQAS). Our objective is to integrate DiffQAS into quantum reinforcement learning (QRL), as reinforcement learning (RL) stands out in the realm of ML for its handling of sequential decision-making problems and potential to exhibit high-level problem-solving capabilities. Specifically, we examine a selection of VQC block candidates and assign trainable structural weights to these blocks. Through gradient descent optimization, we concurrently learn these structural weights alongside the conventional quantum circuit parameters (rotation angles). Furthermore, departing from prior works in QAS for QRL, we train the agents using asynchronous training rather than single-process policy updates, thereby leveraging the computational resources of multi-core CPUs or potentially multiple quantum processing units (QPUs) in the future. Numerical simulations are employed to illustrate the efficacy of the proposed DiffQAS-QRL framework in identifying VQC architectures capable of achieving high scores in diverse testing environments. Specifically, we demonstrate the stability of the proposed method across various environments, contrasting with the inconsistent performance of manually-designed architectures across different scenarios. This observation underscores the necessity for a task-agnostic automated procedure in designing QRL circuits and underscores the effectiveness of the proposed DiffQAS-QRL framework.\nThis manuscript is structured as follows: Section II presents a concise overview of the current advancements in QAS and QRL. In Section III, the fundamental concepts of quantum RL and VQC are elucidated, forming the foundational components of extant QML and QRL models, which represent the focus of inquiry for the proposed framework. The formulation of the differentiable QAS problem is delineated in Section IV, while the intricacies of the proposed DiffQAS-QRL framework are"}, {"title": "II. RELATED WORK", "content": "Quantum reinforcement learning (QRL) has been a subject of exploration since the groundbreaking work by Dong et al. in 2008 [27]. Initially, its practicality was hindered by the requirement to construct environments entirely in a quantum fashion, thus limiting its real-world applicability. However, subsequent advancements in QRL, leveraging variational quantum circuits (VQCs), have broadened its horizons to encompass classical environments with both discrete [23] and continuous observation spaces [24], [25]. The evolution of QRL has witnessed performance improvements through the adoption of policy-based learning methodologies, including Proximal Policy Optimization (PPO) [28], Soft Actor-Critic (SAC) [29], REINFORCE [26], Advantage Actor-Critic (A2C) [30], and Asynchronous Advantage Actor-Critic (A3C) [31]. Moreover, in addressing the challenges presented by partially observable environments, researchers have explored the utilization of quantum recurrent neural networks such as quantum LSTM as RL policies [20], [21]. Recent advancements also encompass hybrid models, wherein a classical neural network is trained to dynamically adjust the parameters of the quantum circuit. This enables the model to address intricate sequential decision-making tasks without relying on quantum recurrence [32].\nNevertheless, the accomplishments mentioned in QRL necessitate profound expertise in designing high-performing quantum circuit architectures to leverage potential quantum advantages. Consequently, there is an imminent requirement to develop automated procedures for designing quantum circuit architectures to cater to the demands of various application domains. Machine learning techniques have been employed to address various challenges in quantum computing, such as quantum architecture search (QAS). The objectives of QAS may include generating desired quantum states [33]\u2013[43], discovering efficient circuits for solving chemical ground states [43]-[47], addressing optimization tasks [43], [45], [48]\u2013[52], optimizing quantum circuits for specific hardware architectures [53], compiling circuits [54]-[56], or conducting machine learning tasks [49], [50], [57]\u2013[63]. Various methodologies are employed to discover the optimal circuit for specific tasks. For instance, reinforcement learning-based approaches are explored in works such as [33]-[36], [38], [40], [41], [44], [48], [53], [56], while different variants of evolutionary algorithms are utilized in works like [39], [57]-[59] to search for circuits. Additionally, differentiable QAS methods have been developed to leverage gradient-based techniques effectively [50]-[52], [61]. Various approaches to encode quantum circuit architecture have been proposed, with some utilizing graph-based methods as seen in works such as [46], [49], while others, like [53], consider convolutional neural network-based methods. As for circuit performance metrics, they may involve direct evaluation of circuit performance on specific tasks [44], [45], [57], or assessing the proximity of the generated circuit to the actual circuit [33], [34], [49]. To alleviate the computational resources needed for direct evaluation, predictor-based methods have been proposed, employing neural networks to predict quantum model performance without direct circuit evaluation [47], [62].\nThe proposed framework in this paper extends the asynchronous training of QRL described in the work [21], [31], [32] to include the capability of differentiable QAS to search for the best performing circuit. Our work also differentiates from the previous work [61] as our work considers the asynchronous training which can leverage parallel computing resource or in the future, multiple-QPU environments. Our work is also different from the previous work such as [57] since our work leverages the differentiable method, unlike the evolutionary methods requiring a large amount of performance evaluation."}, {"title": "III. QUANTUM REINFORCEMENT LEARNING", "content": "Quantum machine learning largely depends on the trainable quantum circuits: variational quantum circuit (VQC), also known as parameterized quantum circuits (PQC). The VQC, as shown in Figure 2, usually has three basic components: encoding circuit $U(x)$, variational circuit $V(\\theta)$ and the final measurement part. The purpose of encoding circuit $U(x)$ is to transform the input vector $x$ into a quantum state $U(x)|0\\rangle^n$, where $|0\\rangle ^n$ is the ground state of the quantum system and $n$ represents the number of the qubit. The encoded state then go through the variational circuit and becomes $V(\\theta)U(x)|0\\rangle ^n$. To retrieve the information from the VQC, measurements can be carried out with pre-defined observables $B_k$. The VQC operation can be seen as as quantum function $f(\\theta) = (\\langle B_1 \\rangle,...,\\langle B_k \\rangle)$, where $\\langle B_k \\rangle = \\langle 0|U^{\\dagger}(x)V^{\\dagger}(\\theta)B_k V(\\theta)U(x)|0\\rangle$. Expectation values $\\langle B_k \\rangle$ can be obtained by performing multiple samplings (shots) on actual quantum hardware or through direct computation when utilizing simulation software."}, {"title": "B. Quantum RL", "content": "Reinforcement learning (RL) constitutes a ML paradigm wherein an agent endeavors to achieve a predefined objective or goal through interactions with an environment & within discrete time intervals [64]. At each time step $t$, the agent perceives a state $s_t$ and subsequently selects an action $a_t$ from the action space $A$ based on its prevailing policy $\\pi$. The policy signifies a mapping from a specific state $s_t$ to the probabilities associated with selecting an action from $A$. Upon executing action $a_t$, the agent receives a scalar reward $r_t$ and the updated subsequent state $s_{t+1}$ of the environment. For episodic tasks, this sequence of actions and new states recurs over multiple time steps until the agent either reaches a terminal state or exhausts the allowable number of steps.\nA category of RL training algorithms referred to as policy gradient methods centers on optimizing the policy function, represented as $\\pi(a|s; \\theta)$, which is parameterized by $\\theta$. These parameters, denoted as $\\theta$, undergo updates via a gradient ascent process on the expected total return, $E[R_t]$. In classical RL, the function $\\pi(a|s;\\theta)$ is realized using a deep neural network (DNN), with $\\theta$ representing the DNN's weights and biases. In quantum RL, the policy function $\\pi(a|s;\\theta)$ can be implemented through VQCs or hybrid models that integrate both VQCs and conventional DNNs. The integration of VQCs and DNNs forms a directed acyclic graph (DAG) and the whole model can be optimized in an end-to-end manner [20], [22]-[26]. An exemplary instance of a policy gradient algorithm is the REINFORCE algorithm, initially proposed in [65]. Within the conventional REINFORCE algorithm, the parameters $\\theta$ undergo updates in the direction of $\\nabla_{\\theta}\\log\\pi (a_t|s_t;\\theta) R_t$, constituting an unbiased estimate of $\\nabla_{\\theta}E[R_t]$.\nNonetheless, the policy gradient estimate frequently encounters high variance, presenting challenges during training. To mitigate this variance while preserving its unbiased nature, practitioners often subtract a term referred to as the baseline from the return. This baseline, denoted as $b_t(s_t)$, constitutes a learned function of the state $s_t$. Consequently, the updated expression becomes $\\nabla_{\\theta}\\log\\pi (a_t|s_t;\\theta) (R_t - b_t(s_t))$. In policy gradient RL, a prevalent selection for the baseline $b_t(s_t)$ is an estimation of the value function $V^{\\pi}(s_t)$. Employing this baseline choice often yields a policy gradient estimate with reduced variance [64]. The difference $R_t \u2013 b_t = Q(s_t, a_t) \u2013 V(s_t)$ can be interpreted as the advantage $A(s_t, a_t)$ of action $a_t$ at state $s_t$. Conceptually, the advantage represents the relative \"goodness or badness\" of action $a_t$ concerning the average value at state $s_t$. This approach is known as the advantage actor-critic (A2C) method. The quantum version of REINFORCE algorithm with value function baselines is described in the work [26]. The work [26] further demonstrates the advantage of hybrid quantum-classical RL over classical models on discrete logarithm problem.\nThe asynchronous advantage actor-critic (A3C) algorithm [66] extends the A2C approach by employing multiple concurrent actors to learn the policy through parallelization. Asynchronous training of RL agents entails running multiple agents concurrently on various instances of the environment, enabling them to encounter diverse states at each time step. This reduced correlation between states or observations enhances the numerical stability of on-policy RL algorithms like actor-critic [66]. Moreover, asynchronous training obviates the need for maintaining an extensive replay memory, thereby reducing memory requirements [66]. Recent investigations [31] have indicated that the quantum version of A3C can yield superior performance in specific benchmark tasks compared to their classical counterparts under certain conditions. Asynchronous QRL can facilitate efficient training, leveraging multiple CPU cores or QPU cores, contingent on whether a simulation backend or a real quantum device is employed."}, {"title": "IV. DIFFERENTIABLE QUANTUM ARCHITECTURE SEARCH", "content": "Drawing inspiration from classical neural architecture search (NAS) [67] and foundational research on differentiable QAS [51], our DiffQAS approach commences with a collection of candidate subcircuits. Suppose we aim to construct a quantum circuit $C$ requiring several sub-components $S_1, S_2, \u2026, S_n$. Each $S_i$ is associated with a corresponding set of allowable circuit choices $B_i$, where $B_i$ denotes the number of permissible circuit choices for each sub-component $i$. Thus, the total number of potential outcomes for circuit $C$ is given by $N = |B_1| x |B_2| \u00d7\u2022 \u00d7 |B_n|$. Structural weights $w_j$, where $j\u2208 {1,..., N}$, are assigned to each possible circuit realization $C_j$. Additionally, we assume that each $C_j$ possesses its own trainable parameters $\\theta$.\nConsider a ML task for which certain circuit realizations $C_j$, when trained with their corresponding parameters $\\theta$, may offer viable approaches. However, each specific circuit $C_j$ does not guarantee an optimal solution; it may yield optimal, suboptimal, or even ineffective outcomes. We introduce the ensemble function $f_c$, defined as the weighted sum of all potential circuit realizations, denoted as $f_c = \\sum_{j=1}^{N} w_j f_{C_j}$. For clarity, we omit the notation for quantum circuit parameters (rotation angles) $\\theta$; and input vector $x$. Subsequently, the output from the ensemble function $f_c$ is subjected to processing by the loss function $L(f_c)$. Utilizing automatic differentiation algorithms, the gradient with respect to the structural weights $w_j$ can be computed as $\\nabla_{w_j}L(f_c)$. Conventional gradient-based optimizers can then be employed to optimize the weights $w_j$.\nIn the proposed DiffQAS framework, VQCs can be assembled from a set of ansatzes, as illustrated in Figure 3. Specifically, for the encoding circuit $U(x)$, it may incorporate or exclude the Hadamard gate, followed by the application of one of the rotation gates ($R_x$, $R_y$, and $R_z$). Regarding the variational circuit $V(\u0398)$, two options exist for the entanglement and three for the parameterized rotation, resulting in six candidates for both the encoding and variational components. Consequently, for a single VQC, there are $6 \u00d7 6 = 36$ potential configurations. Enumerating all $N = 36$ feasible circuit realizations, we assign structural weights $w_j$ to each, as depicted in Figure 4. To meet the requirements of the given task, we can stack multiple ensemble functions $f_c$ to construct deep quantum circuits, as demonstrated in Figure 5. It should be noted that constructing deep quantum circuits increases the number of structural weights $w_j$. For instance, if we build deep quantum circuits with $M$ circuit blocks, there will be $N\u00d7 M$ structural weights, where $N$ represents the number of possible circuit realizations. The structural weights across all layers can be optimized using gradient-based optimizers."}, {"title": "V. METHODS", "content": "In QRL, certain functions such as value function and policy function would be implemented by hybrid quantum-classical models. Inside the hybrid models, the quantum components are realized via VQCs as described in Section III-A. Traditionally, the architecture of VQC is designed before the model training process. While this method has shown several successful QRL applications [23]-[26], it is with certain limitations. For example, the designed architecture may be suitable for only a small set of tasks. And the design of new architecture may require domain expertise. In this paper, we relax some of the constraints via defining the hybrid models with the trainable structural weights. Consider the value function $Q(s; \u0398)$, where $s$ is the state or observation from the environment and $\u0398$ is the whole parameter set including classical and quantum ones."}, {"title": "A. Differentiable QAS for QRL", "content": "The value function can then be expressed as $Q(s; \u0398) = G_{\\eta} \u03bf F \u03bf H_{\\delta}(s)$ where $\\eta, \u03b8, \u03b4 \u2208 \u0398$ are trainable parameters and $G$ and $H$ are classical functions and $F$ is the quantum function. The quantum function $F(x; \u03b8)$ may be composed of an array of candidate functions $f_i(x; \u03b8_i)$ with trainable structural weights $w_i$. It can be expressed as $F(x; \u03b8) = \\sum w_i f_i(x; \u03b8_i)$. The asynchronous training of the quantum model is presented in Figure 5. In conventional quantum A3C [31], the architectures of VQC models remain fixed, with only the gradients of quantum circuit parameters transmitted to the central storage. In the proposed DiffQAS with A3C, however, both the gradients of quantum circuit parameters and the gradients of structural weights are uploaded."}, {"title": "VI. EXPERIMENTS", "content": "In this study, we utilize the following open-source tools for simulation purposes. We employ PennyLane [68] for quantum circuit construction and PyTorch for developing the overarching hybrid quantum-classical model. The hyperparameters for the proposed DiffQAS in RL with QA3C training [21], [31] are set as follows: Adam optimizer with a learning rate of 1 \u00d7 10-4, beta1 = 0.92, beta2 = 0.999, L = 5 for model lookup steps, and a discount factor \u03b3 = 0.9. During the asynchronous training process, local agents or models compute their gradients every L steps, corresponding to the trajectory length used during model updates. The number of parallel processes (number of local agents) is 80. To illustrate both the trend and stability, we present results with the average score alongside its standard deviation over the past 5,000 episodes. The standard deviation is shown as the shaded area in the result plots. We summarize the VQC baselines in the Table I. Each VQC configuration consists of an 8-qubit system and 2 variational layers, resulting in a total of 16 trainable quantum parameters (2 \u00d7 8 = 16)."}, {"title": "A. Environment", "content": "The MiniGrid environment [69] presents a more complex scenario, featuring a significantly larger observation input for the quantum RL agent. In this environment, the RL agent receives a 7\u00d77\u00d73=147-dimensional vector as observation input and must select an action from the action space A, which comprises six options. Notably, the 147-dimensional vector serves as a compact and efficient representation of the environment, as opposed to directly representing the real pixels. In this work, we consider 8-qubit systems, the 147-dimensional vector is transformed into 8-dimensional vectors using a simple classical linear layer, represented by the H function in Section V-A. These 8-dimensional vectors are then processed by the VQC. The classical linear layer and the entire DiffQAS units (depicted in Figure 4) are trained together in an end-to-end manner. The action space A comprises six actions 0,...,5 available for the agent to select. These actions include turn left, turn right, move forward, pick up an object, drop the object being carried, and toggle. However, it is noteworthy that only the first three actions have tangible effects in the scenarios explored in this study. The agent is tasked with learning this distinction. Within this environment, the agent garners a reward of 1 upon successfully attaining the goal. However, a penalty is deducted from this reward following the formula $1 \u2013 0.9 \u00d7 (number of steps/max steps allowed)$, where the maximum permissible step count is delineated as $4 \u00d7 n \u00d7 n$, with n representing the grid size [69]. This reward mechanism poses a challenge due to its sparse nature, wherein rewards are only dispensed upon goal achievement. As depicted in Figure 6, the agent, denoted by the red triangle, is tasked with determining the most direct route from the initial position to the designated goal, depicted in green. We consider six cases in this environment: MiniGrid-Empty-5x5-v0 (Figure6a), MiniGrid-Empty-6x6-v0 (Figure6b), MiniGrid-Empty-8x8-v0 (Figure 6c), MiniGrid-SimpleCrossingS9N1-v0 (Figure6d), MiniGrid-SimpleCrossingS9N2-v0 (Figure6e) and MiniGrid-SimpleCrossingS9N3-v0 (Figure 6f). Here the N represents the number of valid crossings across walls from the starting position to the goal."}, {"title": "B. Results", "content": "In the environment MiniGrid-Empty-5x5 (shown in Figure 7), we can see that the proposed DiffQAS can reach performance similar to the manually designed models (Config-1, Config-2 and Config-4). We can also observe that our DiffQAS model training is more stable regarding the average scores. In addition, our method can outperform the results from other manually designed models such as Config-3, Config-5 and Config-6 with a significant margin. In the environment MiniGrid-Empty-6x6 (shown in Figure 8), we can see that the proposed DiffQAS can reach performance similar to the manually designed models (Config-1, Config-2 and Config-4). The proposed DiffQAS model converges a little bit slower than the manually-crafted Config-1 and Config-2, however, the DiffQAS model has no issue to reach the optimal score. The slower convergence may due to the larger search for the structural weights such that the program requires some additional time to learn these parameters. In addition, our method can outperform the results from other manually designed models such as Config-3, Config-5 and Config-6 with a significant margin. In the environment MiniGrid-Empty-8x8 (shown in Figure 9), we can see that the proposed DiffQAS can reach performance similar to the manually designed models (Config-1, Config-2 and Config-4). The proposed DiffQAS model converges a little bit slower than the three manually-crafted, however, the DiffQAS model has no issue to reach the optimal score and maintain the stability. The slower convergence may due to the larger search for the structural weights such that the program requires some additional time to learn these parameters. In addition, we can observe that the other three manually-crafted models Config-3, Config-5 and Config-6 fail to learn the policy at all. The environment MiniGrid-Empty-8x8 is considered to be more difficult than the previous two, thus it it not surprising that certain models which perform poorly in previous case fail to learn the policy in this case. In the environment MiniGrid-SimpleCrossing-S9N1 (shown in Figure 10), we can see that the proposed DiffQAS can reach performance close to the manually designed models (Config-1, Config-2 and Config-4). The proposed DiffQAS model converges a bit slower than the three manually-crafted before the end of 100,000 training episodes. The slower convergence may due to the larger search for the structural weights such that the program requires some additional time to learn these parameters. In addition, we can observe that the other three manually-crafted models Config-3, Config-5 and Config-6 fail to learn the policy at all. The environment MiniGrid-SimpleCrossing-S9N1 is considered to be more difficult than the previous MiniGrid-Empty environments, thus it it not surprising that certain models which perform poorly in previous cases fail to learn the policy in this case. In the environment MiniGrid-SimpleCrossing-S9N2 (shown in Figure 11), we can see that the proposed DiffQAS can reach performance similar to the best manually designed models (Config-2) and beat all other manually-designed models. The proposed DiffQAS learns a bit slower than the best manually-crafted in early training episodes but finally surpass it with higher scores. The slower convergence may due to the larger search for the structural weights such that the program requires some additional time to learn these parameters before reaching optimal architecture weights. One of the previously good-preforming model (Config-4) now fails to learn the optimal policy in this more difficult environment. In addition, we can observe that the other three manually-crafted models Config-3, Config-5 and Config-6 fail to learn the policy at all. The environment MiniGrid-SimpleCrossing-S9N2 is considered to be more difficult than the previous MiniGrid-Empty and MiniGrid-SimpleCrossing-S9N1 environments, thus it it not surprising that certain models which perform poorly in previous cases fail to learn the policy in this case. In the environment MiniGrid-SimpleCrossing-S9N3 (shown in Figure 12), we can see that the proposed DiffQAS can reach performance similar to the best manually designed models (Config-4) and beat all other manually-designed models at the end of training. The proposed DiffQAS learns a bit slower than the best manually-crafted in early training episodes but finally reaches the optimal scores. The observed slower convergence during early training episodes may due to the larger search for the structural weights such that the program requires some additional time to learn these parameters before reaching optimal architecture weights. One of the previously good-preforming model in (Config-2) now fails to learn the optimal policy in this more difficult environment. In addition, we can observe that the other three manually-crafted models Config-3, Config-5 and Config-6 fail to learn the policy at all. The environment MiniGrid-SimpleCrossing-S9N3 is considered to be more difficult than the previous MiniGrid-Empty, MiniGrid-SimpleCrossing-S9N1 and MiniGrid-SimpleCrossing-S9N2 environments, thus it it not surprising that certain models which perform poorly in previous cases fail to learn the policy in this case. Note that certain manually-designed architectures such as Config-2 and Config-4 cannot perform well consistently across different environments. This phenomenon further confirms the requirement to have a systemic and automatic way to build VQC architectures."}, {"title": "VII. CONCLUSION", "content": "This paper introduces the DiffQAS-QRL framework, amalgamating differentiable quantum architecture search (Diff-QAS) with quantum reinforcement learning (QRL). Specifically, we explore the quantum variant of asynchronous advantage actor-critic (QA3C) RL, capitalizing on parallel computing resources to augment training efficiency. Through numerical simulations conducted within diverse testing environments, our proposed DiffQAS-QRL method demonstrates its capability to identify well-performing VQC architectures across various scenarios, surpassing manually designed models in certain environments. These findings underscore the potential of DiffQAS-QRL in autonomously discovering high-performing VQC architectures for QRL tasks, thereby charting a novel course in the broader domain of automatic QML."}]}