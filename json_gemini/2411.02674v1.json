{"title": "Wave Network: An Ultra-Small Language Model", "authors": ["Xin Zhang", "Victor S. Sheng"], "abstract": "We propose an innovative token representation and update method in an new ultra-small language model: the Wave network. Specifically, we use a complex vector to represent each token, encoding both global and local semantics of the input text. A complex vector consists of two components: a magnitude vector representing the global semantics of the input text, and a phase vector capturing the relation-ships between individual tokens and global semantics. Experiments on the AG News text classification task demonstrate that, when generating complex vectors from randomly initialized token embeddings, our single-layer Wave Network achieves 90.91% accuracy with wave interference and 91.66% with wave modulation-outperforming a single Transformer layer using BERT pre-trained embeddings by 19.23% and 19.98%, respectively, and approaching the accuracy of the pre-trained and fine-tuned BERT base model (94.64%). Additionally, compared to BERT base, the Wave Network reduces video memory usage and training time by 77.34% and 85.62% during wave modulation. In summary, we used a 2.4-million-parameter small language model to achieve accuracy comparable to a 100-million-parameter BERT model in text classification.", "sections": [{"title": "1 Introduction", "content": "Pre-trained token representations are crucial for many language processing models[1], both static token embedding methods, such as Skip-gram and Continuous Bag of Words (CBOW) [2], and context-dependent token embedding methods [3], along with representation updating mechanisms like attention [4], are core techniques in NLP.\nDespite the remarkable achievements, current NLP models face several inherent challenges. First, existing token embedding methods primarily focus on local semantics, lacking the ability to directly represent global semantics. Second, popular architectures like Transformer [4] measures semantic similarity using the dot product between token vectors, which is computationally expensive. In multi-layer deep learning architectures, the dot product is computed in each attention head and layer, leading to significant resource demand in terms of computing power and time. Consequently, large language models (LLMs) require numerous layers and substantial hardware, data, and time resources to perform optimally on downstream tasks.\nFrom a more fundamental perspective, Hockett and Hockett [5] suggests that natural language is a complicated signal system that conveys information with specific meanings through the production of sounds and the reception of hearing. The signal characteristics enable human language to express a wide range of abstract concepts. Based on this signal processing view, we focus on an ultra-small language model can approach or even surpass the performance of large language models on specific tasks."}, {"title": "2 Related Work", "content": "This research is closely related to multiple fields, including machine learning, natural language processing, and representation learning, which all rely on semantic processing. These fields include applications such as semantic search, knowledge graph, question answering, and various natural language processing techniques. Due to space limitations, we will focus on specific related research areas. For example, Simhi and Markovitch [6] proposed a method to convert a token embedding space into a more understandable concept space, thereby reducing training costs and improving interpretability. Tennenholtz et al. [7] proposed ELM (Embedding Language Model), which makes embeddings more interpretable and broadly applicable by injecting embeddings into LLMs, and then directly transform embedding vectors into understandable narratives. Sun et al. [8] treats the hidden state in RNN (Recurrent Neural Network) [9] as a dynamic, trainable machine learning model, and this model-based hidden state is updated during the training and testing phases through self-supervised learning rules.\nIn the Transformer architecture, representation update are carried out through the attention mechanism. Therefore, in addition to research on token embeddings, significant efforts have also been made to improve the attention mechanism itself. For instance, Tay et al. [10] proposed a new and efficient sparse attention learning method based on the differentiable ranking of internal representations. Ding et al. [11] proposed a block-level computation based on local attention to enhance the embedding method of context information. Jaegle et al. [12] proposed the Perceiver, which leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle substantial inputs."}, {"title": "3 Method", "content": "There are two main token embedding methods. The first one is fixed token embedding, represented by CBOW and Skip-gram, which cannot adapt to the dynamic meanings of tokens in varying contexts. The second is context-dependent embeddings, such as BERT, which generates different embeddings for the same token depending on its contexts. Because most current NLP methods, including Transformer, rely on dot products to measure local semantic similarity between token embeddings, they do not directly capture the global semantics of the entire input text.\nIn contrast, our framework encodes both global and local semantics using complex vectors. Instead of relying on dot products for token representation updates, we apply addition or multiplication to complex vectors to simulate wave interference or modulation. This approach is possible because languages can be treated as complicated signal systems that convey meaning through sounds production and auditory reception [13, 14], and complex vectors correspond to physical wave representations [15, 16]."}, {"title": "3.1 Represent Tokens Using Complex Vectors", "content": "We use a complex vector $\\mathcal{G}_{ea}$ to represent each token, encoding both global and local semantics of the input text. A complex vector consists of two components: a magnitude vector $\\mathcal{G}$ representing the global semantics of the input text, and a phase vector $\\alpha$ capturing the relationships between individual tokens and global semantics. This representation is referred to as complex vector token representation. The detailed computation of each component will be explained in the following subsections."}, {"title": "3.1.1 Global Semantics Representation", "content": "The meaning of each token in the input text often depends on the overall semantics of the input text. Understanding global semantics helps resolve the ambiguity of individual tokens, which is essential for many downstream tasks requiring a holistic view of the input text.\nFrom a signal processing perspective, signals are often represented in polar coordinates and processed in the frequency domain. A signal's amplitude represents its strength, and its phase describes its position relative to another signal within a cycle [17]. Referring to signal processing, we treat token embeddings as discrete signals in the frequency domain. Given an input text with n tokens input_text = [W1,W2, ..., Wm,..., Wn]:\n\nEach token embedding $w_{j}$ can be treated as a discrete real-value signal, where each elements $w_{j,k}$ represents the signal component along the k-th dimension. From a physical perspective, the magnitude of each signal component is defined as $G_{j,k} = |w_{j,k}|$, and the energy of each signal component can be defined as $E = w_{j,k}^2$ [18]. Using these magnitudes, we calculate the token magnitude matrix (i.e., the right sub-figure in Figure 1) from the token embedding matrix (i.e., the left sub-figure of Figure 1).\nNext, we sum the magnitudes of all token embedding components along each dimension to define the global semantics vector $\\mathcal{G} = [G_1, G_2,..., G_k,...,G_{768}]$, where each global semantic element $G_k$ can be defined as $G_k = ||w_{:,k}||_2 = ||[w_{1,k}, w_{2,k}, ..., w_{j,k}, ...+ w_{m,k},..., w_{n,k}]||_2 = \\sqrt{w_{1,k}^2 + w_{2,k}^2 + ... + w_{j,k}^2 + ... + w_{m,k}^2 + ... + w_{n,k}^2}$ as shown in Figure 2. Here, $w_{j,k}$ represents the k-th dimension of the j-th token embedding. This global semantic vector $\\mathcal{G}$ represents the global semantics of the entire input text and will serve as the magnitude of the complex vector token representation of each token in polar coordinates."}, {"title": "3.1.2 Local Semantics Representation", "content": "Local semantics usually represent the specific meaning of individual tokens, which helps in analyzing dependencies and subtle differences between tokens in an input text. For example, tasks like sentiment analysis, entity recognition, and keyword extraction, often require a precise understanding of individual tokens.\n1) Using phase to represent relationships between tokens and the global semantic vector\nFrom a signal processing perspective, phase describes the relative relationships between signals. We will use the phase of complex vector token representations to represent the relative relationships between individual tokens and the global semantic vector. That is, the phase representation of a token is coupled with its global semantic vector. For each token $w_{j}$ in the input text, its phase vector is $\\alpha_{j} = [\\alpha_1, \\alpha_2, ..., \\alpha_k,..., \\alpha_{768}] = [input_\\alpha_1, input_\\alpha_2,..., input_\\alpha_k,..., input_\\alpha_{768}]$, where input_$\\alpha_k$ is defined as arctan2($\\frac{w_{j,k}}{input_\\mathcal{G}_k}$,$\\frac{\\sqrt{1-(\\frac{w_{j,k}}{input_\\mathcal{G}_k})^2}}{input_\\mathcal{G}_k}$) based on the corresponding element input_$\\mathcal{G}_k$ in the global semantic vector of the input text input_$\\mathcal{G}$ = [input_$\\mathcal{G}_1$, input_$\\mathcal{G}_2$, ..., input_$\\mathcal{G}_k,..., input_$\\mathcal{G}_{768}$]. Note that we use the function arctan2 to ensure angles fall within the range of -$\\pi$ to $\\pi$, consistent with the standard phase angle in physics. With these definitions, we can derive the token phase matrix (i.e., the right sub-figure in Figure 3) from the token embedding matrix (the left sub-figure of Figure 3).\n\n2) Illustrate Complex Vector Token Representation in Cartesian coordinate system\nTo illustrate the meaning of complex vector token representations' components in Cartesian coordinates, we use the Euler's formula $e^{i\\theta} = cos(\\theta) + i sin(\\theta)$ [19] to convert complex vector token representations from polar to Cartesian coordinates, as shown in Figure 4. For example, the complex vector token representations input_$\\mathcal{G}$ can be expressed in Cartesian coordinates as input_$\\mathcal{G}$\u00b7cos(input_$\\alpha_{j}$)+\ni input_$\\mathcal{G}$\u00b7sin(input_$\\alpha_{j}$). The inner product of sin(input_$\\alpha_{j}$) and cos(input_$\\alpha_{j}$) is zero over a full period, making them orthogonal [20]. Consequently, the real part input_$\\mathcal{G}$\u00b7 cos(input_$\\alpha_{j}$) and the imaginary part iinput_$\\mathcal{G}$\u00b7sin(input_$\\alpha_{j}$) are also orthogonal, fulfilling the properties of wave representations as described in physics [21]. As Figure 4 illustrates, the real part of the token embedding $w_{j}$ represents the token's contribution along the k-th dimension, capturing the local semantics of the input text. The imaginary part describes"}, {"title": "3) An example of a Complex Vector Token Representation", "content": "Figure 5 provides an example illustrating how to convert each token in an input text \"I am alive\" into complex vector token representations in four steps: First, we randomly generate embeddings in a 768-dimension feature space for all three tokens in the input text; Second, we calculate the global semantic vector of all three token embeddings across all dimensions; Third, we compute the component $\\alpha_{j,k}$ of the phase vector $\\alpha_{j}$ for each tokens. And finally, we combine the global semantic vector $\\mathcal{G}$ and the phase vector $\\alpha_{j}$ to compose a complex vector token representations for each token $w_{j}$ in polar coordinates."}, {"title": "3.2 Update Wave Representation by Superposition", "content": "Complex vectors correspond to physical wave representations [15, 16], which allows us to use wave-based operations for more efficient complex vector token representations updates. We propose to develop a linear layer in our Wave network to generate two variants of the input-text level complex vector token representation of each token. These two variants can then be used for applying wave-based operations, such as interference and modulation to model complex vector token representation updates more effectively."}, {"title": "3.2.1 Wave Interference", "content": "From physical perspective, wave interference is a phenomenon where two coherent waves combined by adding their intensities or displacements, considering their phase difference. In the context of multiple levels of global semantics, as discussed in the Subsection 3.1.2, we consider two variants of complex vector token representations for token $w_{j}$ as: input_$\\mathcal{Z}_{j} = input_\\mathcal{G}e^{i\u00b7input\u00b7\\alpha_{j}}$ and input_$\\mathcal{Z}_{j} = input_\\mathcal{G}'e^{i\u00b7input\u00b7\\alpha'_{j}}$. We use complex vectors addition to simulate wave interference [22] and obtain the combined complex vector token representation interference_$\\mathcal{Z}_{j}$ for token $w_{j}$ as follows:\n\nInterference_$\\mathcal{Z}_{j} = input_\\mathcal{Z}_{j} + input_\\mathcal{Z}'_{j} = input_\\mathcal{G} \u00b7 e^{i\u00b7input\u00b7\\alpha_{j}} + input_\\mathcal{G}' \u00b7 e^{i\u00b7input\u00b7\\alpha'_{j}}\n= (input_\\mathcal{G} \u00b7 cos(input_\\alpha_{j}) + input_\\mathcal{G}' \u00b7 cos(input_\\alpha'_{j}))\\newline +i(input_\\mathcal{G} \u00b7 sin(input_\\alpha_{j}) + input_\\mathcal{G}' \u00b7 sin(input_\\alpha'_{j}))\n\n, where the last equation is based on Euler's formula: $e^{i\\theta} = cos(\\theta) + i sin(\\theta)$.\nThen, based on the definitions of the input_$\\mathcal{G}$ and input_$\\alpha_{j}$ provided in the Subsection 3.1.1 and 3.1.2, the components of Equation 3 can be expressed explicitly for the k-th component of the token $w_{j}$'s complex vector token representation as:\nInterference_$\\mathcal{Z}_{j,k} = \\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{m,k}^2 + ... + w_{n,k}^2} \\sqrt{\\frac{w_{j,k}^2}{\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{m,k}^2 + ... + w_{n,k}^2}}} \\\\\n+ \\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{m,k}^2 + ... + w_{n,k}^2}\\sqrt{\\frac{w_{j,k}^2}{\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{m,k}^2 + ... + w_{n,k}^2}}} \\\\\n= \\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{m,k}^2 + ... + w_{n,k}^2}(1-(\\frac{w_{j,k}}{\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{m,k}^2 + ... + w_{n,k}^2}})^2)\\\\\n+ i\u00b7(\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{m,k}^2 + ... + w_{n,k}^2}\\sqrt{\\frac{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j-1,k}^2 + w_{j+1,k}^2 + ... + w_{m,k}^2 + ... + w_{n,k}^2}{\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{m,k}^2 + ... + w_{n,k}^2}}}) +\\\\\n \\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{m,k}^2 + ... + w_{n,k}^2}(1-(\\frac{w_{j,k}}{\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{m,k}^2 + ... + w_{n,k}^2}})^2)\\\\\n+i\u00b7(\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{m,k}^2 + ... + w_{n,k}^2}\\sqrt{\\frac{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j-1,k}^2 + w_{j+1,k}^2 + ... + w_{m,k}^2 + ... + w_{n,k}^2}{\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{m,k}^2 + ... + w_{n,k}^2}}})\n= w_{j,k}+w'_{j,k}\n+(i \\frac{ \\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j-1,k}^2 + w_{j+1,k}^2 + ... + w_{m,k}^2 + ... + w_{n,k}^2} \\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j-1,k}^2 + w_{j+1,k}^2 + ... + w_{m,k}^2 + ... + w_{n,k}^2}}{ \\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{m,k}^2 + ... + w_{n,k}^2}}) (4)\nNext, we will illustrate how the phase difference of two complex vector token representations, such as input_$\\mathcal{Z}_{j}$ and input_$\\mathcal{Z}'_{j}$, affects the overall intensity of the resulting complex vector token representation by analyzing their interference term. As mentioned in [23, 24], the interference term Re(input_$\\mathcal{Z}_{j} \u00b7 input_\\mathcal{Z}^*_{j}$) can obtained from the square of the magnitude of input_$\\mathcal{Z}_{j,k} = |input_\\mathcal{Z}_{j} + input_\\mathcal{Z}'_{j}|^2 = |input_\\mathcal{Z}_{j}|^2 + |input_\\mathcal{Z}'_{j}|^2+2\u00b7(input_\\mathcal{Z}_{j}\u00b7input_\\mathcal{Z}^*_{j}$), which describes how the phase difference between two complex vector token representations determines whether they interfere constructively or destructively. Input_$\\mathcal{Z}^*_{j}$ is the complex conjugate of input_$\\mathcal{Z}_{j}$, which is used to ensure that as a measurable physical quantity, the intensity is a real value [25]. We can further express the interference term as:\n\n$2 \u00b7 Re(input_\\mathcal{Z}_{j} \u00b7 input_\\mathcal{Z}^*_{j}) = 2 \u00b7 Re (input_\\mathcal{G} \u00b7 e^{i\u00b7 input \\alpha_{j}}\u00b7 input \\mathcal{G}' \u00b7 e^{-i \u00b7input \\alpha'_{j}})\n2 \u00b7 input_\\mathcal{G} \u00b7 input_\\mathcal{G}' \u00b7 cos(input_\\alpha_{j} - input_\\alpha'_{j})$\n(5)\nEquation 5 demonstrates that the cosine value of the phase difference directly determines the interference result."}, {"title": "3.2.2 Wave Modulation", "content": "From a physical perspective, wave modulation is the process of varying one or more properties of a periodic waveform, called the carrier signal, with a separate signal, which contains the information to be transmitted. From a signal processing perspective, adjusting the amplitude of a carrier wave in accordance with the input signal is referred to as amplitude modulation [26]. Similarly, adjusting the phase of a carrier wave in accordance with the input signal is referred to as phase modulation [27]. Both amplitude modulation and phase modulation can be achieved through the multiplication of complex vectors representing waves [28-30].\nIn the context of input-level global semantics, as discussed in the Subsection 3.1.2, we consider two variants of complex vector token representations for token $w_{j}$ as: input_$\\mathcal{Z}_{j} = input_\\mathcal{G} \u00b7 e^{i\u00b7input\u00b7\\alpha_{j}}$ and input_$\\mathcal{Z}'_{j} = input_\\mathcal{G}' \u00b7 e^{i\u00b7input\u00b7\\alpha'_{j}}$. We use complex vectors multiplication to simulate wave modulation [22] and obtain the combined complex vector token representation modulation_$\\mathcal{Z}_{j}$ for token $w_{j}$ as follows:\nModulation_$\\mathcal{Z}_{j} = input\u00b7\\mathcal{Z}_{j} \u00b7 input\\mathcal{Z}'_{j} = input_\\mathcal{G} \u00b7 e^{i input \\alpha_{j}}\u00b7 input \\mathcal{G}' \u00b7 e^{-i input \\alpha'_{j}}\n= input_\\mathcal{G}\u00b7 input_\\mathcal{G}' \u00b7 e^{i\u00b7input\u00b7\\alpha_{j}+input\u00b7\\alpha'_{j}}\n= input_\\mathcal{G} input_\\mathcal{G}' cos(input_\\alpha_{j} + input_\\alpha'_{j})\n+ i input_\\mathcal{G} input_\\mathcal{G}' \u00b7 sin(input_\\alpha_{j} + input_\\alpha'_{j})\n(6)\nHere, the amplitude modulation term is represented by input_$\\mathcal{G}input_\\mathcal{G}'$. And the phase modulation term is expressed as input_$\\alpha_{j} + input_\\alpha'_{j}$. Then, based on the definitions of the input $\\mathcal{G}$ and input $\\alpha_{j}$ provided in the Subsection 3.1.1 and 3.1.2, the components of Equation 6 can be expressed explicitly for the k-th component of the token $w_{j}$'s complex vector token representation as:\nModulation-$\\mathcal{Z}_{j,k} = \\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{n,k}^2} \\frac{w_{j,k}}{\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{n,k}^2}}\\\\\n\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{n,k}^2} \\frac{w_{j,k}}{\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{n,k}^2}}\\\\\n\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{n,k}^2}(1-(\\frac{w_{j,k}}{\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{n,k}^2}})^2)- \\\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{n,k}^2}(1-(\\frac{w_{j,k}}{\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{n,k}^2}})^2)\n+ i\u00b7(\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{n,k}^2} \\frac{\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j-1,k}^2 + w_{j+1,k}^2 + ... + w_{n,k}^2}}{\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{n,k}^2}})+\\\n\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{n,k}^2} \\frac{\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j-1,k}^2 + w_{j+1,k}^2 + ... + w_{n,k}^2}}{\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j,k}^2 + ... + w_{n,k}^2}})\n= (w_{j,k} \u00b7 w'_{j,k} - \\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j-1,k}^2 + w_{j+1,k}^2 + ... + w_{n,k}^2}\\\\\n\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j-1,k}^2 + w_{j+1,k}^2 + ... + w_{n,k}^2})\n+ i\u00b7(w_{j,k}\u00b7\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j-1,k}^2 + w_{j+1,k}^2 + ... + w_{n,k}^2}) (7)\n+ (w'_{j,k}\\sqrt{w_{1,k}^2 + w_{2,k}^2 + \u00b7\u00b7\u00b7 + w_{j-1,k}^2 + w_{j+1,k}^2 + ... + w_{n,k}^2})"}, {"title": "3.3 Single-layer and Multi-layer Architecture of the Wave Network", "content": "a) Single layer architecture:\nTo implement the operations of generating and updating complex vector token representations, we designed a deep learning model called the Wave network. Figure 6a illustrates the layer-level design of the Wave network. Starting with an input text, we generate initial embeddings for each token by randomly assigning values across 768 dimensions. These initial embeddings then serve as the basis for generating two variants of complex vector token representations, both with positional encoding. Next, these two complex vector token representations variants are modulated through multiplication, which result is then passed through a feed-forward layer and a normalization layer to stabilize the output. Finally, the processed representations are used as the foundation for subsequent classification tasks.\nb) Multi-layer architecture:\nFigure 6b shows the structure of one block in a multi-layer Wave network, which mathematical expressions can be represented as:\n$W_{n+1} = W_n + \\mathcal{D} (g(f_{n+1}(W_n)))$ (8)\nWhere $W_n$ is the output of the n-th layer, $f_{n+1}(W_n)$ denotes the wave overlay of the (n+1)-th layer, $g(x)$ convert complex number to token embedding, and $\\mathcal{D}(x)$ the dropout operation applied to the input x."}, {"title": "4 Experiments and Results", "content": "The parameters across all experiments are kept as consistent as possible. The learning rate for both the Wave network and Transformer learning rate is set to 1e-3, while for BERT, it is set to 2e-5[31]. The batch size varies depending on the task. In the resource utilization comparison experiments, all models use a batch size of 64. In the accuracy comparison experiments, the batch size is 64 for the Wave network and Transformer, and 32 for BERT[31]. In the gradient comparison and embedding independence experiments, all three models use a batch"}]}