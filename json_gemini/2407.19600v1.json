{"title": "YOU SHALL KNOW A PIECE BY THE COMPANY IT KEEPS. CHESS\nPLAYS AS A DATA FOR WORD2VEC MODELS", "authors": ["Boris Orekhov"], "abstract": "In this paper, I apply linguistic methods of analysis to non-linguistic data, chess plays, metaphorically\nequating one with the other and seeking analogies. Chess game notations are also a kind of text, and\none can consider the records of moves or positions of pieces as words and statements in a certain\nlanguage. In this article I show how word embeddings (word2vec) can work on chess game texts\ninstead of natural language texts. I don't see how this representation of chess data can be used\nproductively. It's unlikely that these vector models will help engines or people choose the best move.\nBut in a purely academic sense, it's clear that such methods of information representation capture\nsomething important about the very nature of the game, which doesn't necessarily lead to a win.", "sections": [{"title": "Introduction", "content": "In this paper, I apply linguistic methods of analysis to non-linguistic data, metaphorically equating one with the other\nand seeking analogies. The productivity of this approach has been proven within the field of Super Linguistics.\nI argue that developed by computational linguists word embeddings (made with the algorithm word2vec) can shed light\non the features of chess moves.\nRecently, computational linguistics has made a great progress in natural language processing (NLP). Within this field,\ntools have been developed for machine analysis of morphology, syntax and semantics. Computational linguistics\nbecame a stand-alone research area with its conferences (weblink) and actual fields. The experts have developed\ngeneral principles of text analysis, optimal methods of word counting. e.g. term frequency, inverse document frequency\n(tf-idf) is used instead of simple word frequency, for finding collocations Pointwise Mutual Information is used,\netc. Computational linguistics technologies are successfully implemented in the commercial products in the field of\ninformation retrieval [Roy et al., 2018], electronic marketing [Woltmann et al., 2018], modern user interfaces (voice\nassistants, chat bots), and so on.\nAt the same time, there are works that show the possibility of applying NLP tools to non-NL data. The idea of word\nembeddings proved to be particularly productive outside the text data. Word embedding is a representation of the\nsemantics of a word as a vector in a multidimensional space, the measurements of which are determined by the contexts\nin which the word appears in the texts. [Baroni et al., 2014, Levy et al., 2015]. The main technological breakthrough\nof word embeddings was that the number of measurements for each word can be relatively small, much less than the\nnumber of words in the dictionary.\nWord embeddings are a technical implementation of the linguistic idea of distributional semantics. Distributional\nsemantics is the concept that the meaning of a word is revealed through its context, and by examining the surrounding\nwords, we can infer its semantics. For example, we can assume that in the sentence \"he returned home after three"}, {"title": "Data", "content": "Chess game notations are also a kind of text, and one can consider the records of moves or positions of pieces as words\nand statements in a certain language. According to the rules of the PGN (portable game notation) format, an uppercase\nletter denotes a white piece, while a lowercase letter denotes a black piece. The letters themselves are obvious: R - rook\n, N - knight, B - bishop\u76d2, etc. Castling is denoted by the move of the king (the movement of the rook is implied\nby default).\nAn example of a chess game recorded in PGN format looks like this:\nPe2e4 pc7c5 Ng1f3 pd7d6 Pd2d4 pc5d4 Nf3d4 ng8f6 Nb1c3 pa7a6 Bf1e2 pe7e6 Ke1g1 bf8e7 Pf2f4\nke8g8 Bc1e3 nb8c6 Kg1h1 bc8d7 Pa2a4 ra8c8 Nd4b3 nc6a5 Nb3d2 bd7c6 Be2d3 pd6d5 Pe4e5 pd5d4\nBe3d4 qd8d4 Pe5f6 be7b4 Pf6g7 qd4g7 Nd2f3 bb4c3 Pb2c3 bc6d5 Qd1e2 rc8c3 Rale1 na5c6 Qe2d2\nkg8h8 Re1e3 nc6b4 Nf3e5 nb4d3 Pc2d3 rf8c8 Re3g3 rc3c2 Qd2b4 rc2g2 Ne5f7 kh8g8 Nf7h6 kg8h8\nNh6f7 kh8g8 Nf7h6\netc.\nHere, the first letter in each \"word\" denotes the piece and its color, the next two symbols indicate the square from which\nthe piece moves, and the last two symbols indicate the square to which the piece moves. Thus, Pe2e4 is a move by a\nwhite pawn (uppercase letter, so the piece is white, the letter P indicates a pawn) from square e2 to square e4. For more\ndetails see [Boutell et al., 2003].\nOne could build vector models based on game texts that reflect the moves, as in the example above, but this is somewhat\ncontroversial because, with this approach, the \"context\" is lost, meaning the position in which the move is made.\nConsidering moves separately from the position is a strange idea for chess. Moves and their function in the game do not\nexist independently of the position. However, this is similarly strange when it comes to words, when we look at them\nout of context."}, {"title": "", "content": "Words have no meaning outside of context and acquire it only as part of a certain statement. We do not always realize\nthis because we are used to seeing words on dictionary pages. Dictionaries try to reflect different meanings but can\nnever do so completely. When we think of a polysemous word, we only recall one, the most obvious meaning, while we\ntemporarily forget the others. Similarly, chess moves in vector models will have to be handled.\nAnother way to linguistically reconceptualize the moves of a chess game can lie within the framework of the concept of\ntopic and comment. The topic, or theme, of a sentence is what is being talked about, and the comment (rheme or focus)\nis what is being said about the topic.\nIn our \"text,\" the words express only the rheme, while the theme is lost. Or, we can look at the problem differently:\neach word simultaneously expresses both the theme (the name and color of the piece, the square it stood on) and the\nrheme (the square to which the piece is moved as a result of the move). Thus, differences in the structure of linguistic\nand chess material become apparent.\nAt the same time, building vector models based on the list of moves in a game does not seem entirely absurd, because\nwith this approach, we get moves translated into vectors that \"belong to the same company,\" meaning they regularly\noccur in the same game situations.\nModern methods of natural language processing and information retrieval depend heavily on large collections of text.\nI used a collection of records of 5,400,137 chess games (approx 840 mln moves, weblink). These are mainly games\nplayed at a high international level.\nBased on this data, I trained two types of models.\nType 1 is a model \"based on moves.\" That is, a listing of a game such as Pe2e4 pc7c5 Ng1f3 pd7d6 Bf1b5 bc8d7\nBb5d7 qd8d7 and so on is taken and considered as a \"sentence,\" with each move in it being a separate word.\nType 2 is a model \"based on positions.\" Here, each move is considered as a \"sentence,\" where the words will be both\nthe move itself (->Bc1h6) and all the squares occupied by pieces (Pa2 Pb2 Pc3 and so on). The position on the board\nis the context for the move, just as words form the context for a word in a statement in a natural language. Therefore, I\nprepared the source material differently for the models of the second type. Usually, when building a vector model on a\nlanguage corpus, this corpus is divided into sentences, and the vectorization captures only the context of one sentence.\nFor chess statements, I broke down the game notations into individual moves, and I got \"sentences\" of this kind:\n1. ra8 nb8 bc8 qd8 ke8 bf8 ng8 rh8 pa7 pb7 pc7 pd7 pe7 pf7 pg7 ph7 Pa2 Pb2 Pc2 Pd2 Pe2 Pf2 Pg2 Ph2\nRa1 Nb1 Bc1 Qd1 Ke1 Bf1 Ng1 Rh1 ->Pe2e4\n2. ra8 nb8 bc8 qd8 ke8 bf8 ng8 rh8 pa7 pb7 pc7 pd7 pe7 pf7 pg7 ph7 Pe4 Pa2 Pb2 Pc2 Pd2 Pf2 Pg2 Ph2\nRa1 Nb1 Bc1 Qd1 Ke1 Bf1 Ng1 Rh1->pc7c5\n3. ra8 nb8 bc8 qd8 ke8 bf8 ng8 rh8 pa7 pb7 pd7 pe7 pf7 pg7 ph7 pc5 Pe4 Pa2 Pb2 Pc2 Pd2 Pf2 Pg2 Ph2\nRa1 Nb1 Bc1 Qd1 Ke1 Bf1 Ng1 Rh1 ->Ng1f3\netc."}, {"title": "", "content": "Here, the position of the pieces on the board is recorded first (ra8, nb8, bc8...), followed by the move made by the\nplayer: ->Pe2e4. To avoid confusing the \"words\" that denote the position of a piece on the board with the \"words\"\nthat denote a move, I added a prefix in the form of the symbol -> to each move.\nThus, both the positions of the pieces on the board and the move are all \"words\" of one \"sentence.\"\nThe algorithm can be given a window of observation, meaning how many words to the left and right of the current one\nwe should consider. To account for the entire board, I set the window to 32, so when calculating the vector of a move,\nall positions of the pieces on the board will be taken into account.\nList of models for the study:\n1. moves_texts.model, Type 1 model, all moves of all games in the collection (move = \"word,\" game =\n\"sentence\").\n2. lemmatized_moves_texts.model, Type 1 model, all moves of all games (move = \"word,\" game = \"sen-\ntence\"), but the square from which the piece moves is excluded from the move. This is a variant of \"stemming.\"\n3. white_moves.model, Type 2 model, includes only white's moves along with positions (move and all pieces\non the board = \"words,\" position + move = \"sentence\").\n4. black_moves.model, Type 2 model, same as above but for black.\n5. debut_moves.model, Type 1 model, the \"sentence\" is the truncated segment of the beginning of the game up\nto the 12th move. It should reflect only the opening moves of the game.\n6. debut_positions.model, Type 2 model, only moves up to the 12th in the game along with their positions.\nOnly white's moves are included; black's moves are excluded.\n7. mittel_moves.model, Type 1 model, only moves from the 13th to the 30th. Presumably the middlegame.\n8. mittel_positions.model, Type 2 model, moves along with positions from the 13th to the 30th. Only\nwhite.\n9. endgame_moves.model, Type 1 model, moves from the 31st to the end of the game.\n10. endgame_positions.model, Type 2 model for moves and positions, starting from the 31st in the game.\nOnly white.\n11. moves_pos.model, Type 1 model, differs from model moves_texts.model only in that each move is added\nwith a \"part-of-speech tag.\" There are only two tags: _CAP and _N. Thus, for each move, there can be two\nentries: Bc1h6_N and Bc1h6_CAP. Bc1h6_CAP means the bishop moves to square h6 with a capture, while\nBc1h6_N is the same move without a capture. Thus, the linguistic idea of parts of speech is transferred to the\ntext of chess games: there is a part of speech for a move with a capture and a part of speech for a move without\na capture.\n12. positions_pos.model, Type 2 model with part-of-speech tags. So it can be ->Bc1h6_N and ->Bc1h6_CAP.\nOnly white.\n13. queens_moves.model, Type 1 model, moves in the game until there is at least one queen on the board. I did\nnot exclude cases when a queen reappears on the board in the endgame after a pawn promotion.\n14. no_queens_moves.model, Type 1 model, moves in the game from the moment both queens disappear from\nthe board.\n15. queens_positions.model, Type 2 model, moves + their positions when there is at least one queen on the\nboard. Only white.\n16. no_queens_positions. model, Type 2 model, moves + their positions when both queens have disappeared\nfrom the board. Only white.\n17. positions_moves_pro.model, A model combining types 1 and 2. This model includes the position of the\nmove, the move itself, and three moves before and after it. Perhaps such a model will better reflect the player's\nstrategy (aggressive or passive play, attacking or positional). Only white.\n18. result_moves.model, Type 1 model, includes only decisive games.\n19. tied_moves.model, Type 1 model, includes only drawn games.\nAll the models are published on Huggingface [Boris Orekhov, 2024]."}, {"title": "Results", "content": "First, we need to ensure that the models truly reflect chess reality and do not contain random sequences of moves from\na chess perspective. Vector models allow us to find similar words (in our case, moves). Let's try working with the basic\nmodel moves_texts.model. The model contains 7749 words (moves). Here are a few random examples. First, the\nmove is listed (highlighted in bold) for which we are searching for the closest move vectors, followed by the \"synonym\"\nmoves (usually referred to as quasi-synonyms), and the cosine distance between the vectors is indicated (ranging from 0\nto 1, the higher the number, the closer the vectors are to each other):\nPe2e4, 8e2e4\nPe2e3 0.4319874346256256\nNg1f3 0.3651273250579834\nPd2d4 0.36322540044784546\nPe3e4 0.34142181277275085\nng8f6 0.3036213517189026\npd7d5 0.2906121909618377\npc7c5 0.28487297892570496\nPc2c4 0.2729674279689789\nPc3d4 0.2568834125995636\nke8g8 0.24415579438209534\nBf1b5, f1b5\nBf1d3 0.5939557552337646\nBf1g2 0.5874636769294739\nBf1e2 0.5806597471237183\nBf1c4 0.5764033794403076\nBc4b5 0.518180251121521\nBd3b5 0.4963855743408203\nBe2b5 0.4832766652107239\nBa4b5 0.36604568362236023\nBf1a6 0.3487110435962677\nBf1h3 0.3368876576423645\nRf8f4, f8f4\nRf8b8 0.6256532669067383\nRf8d8 0.6229713559150696\nRf8a8 0.6161673069000244\nRf8h8 0.6064906716346741\nRf8c8 0.6060029864311218\nRf8e8 0.5938045382499695\nRf8g8 0.5920660495758057\nRf8f3 0.5568860173225403\nRf8f6 0.5522985458374023\nRf8f2 0.5472667813301086\nFrom these lists, it is clear that the vectors in the model are not distributed randomly. The moves closest to a given\nmove are usually those made by the same piece. This is not obvious in chess, as different continuation options can exist\nin similar game situations (positions are not considered in this model). However, linguistically, this is expected because\nquasi-synonyms are usually words of the same part of speech or even cognates.\nFurthermore, to find the moves closest in \"semantics\" to a given move, we can calculate the cosine distance between\nany two arbitrary moves in the model. For example, for Pe2e4 and pe7e5, the distance is 0.1094483. This is a minimal\nvalue, resulting because the context for the opening move Pe2e4 includes all possible continuations, whereas pe7e5 is\nrelatively local compared to this variety.\nAnother available function is \"find the odd one out.\" In a linguistic vector model, if we propose a series of words like\n\"apple,\" \"pear,\" \"grape,\" \"banana,\" \"orange,\" and \"cucumber,\" the model will indicate that \"cucumber\" is the odd one\nout. This is because \"cucumber\" is typically used in a different context than the other words, so its vector has noticeably\ndifferent coordinates."}, {"title": "", "content": "Let's try the same with moves. We'll take some common moves for traditional openings, add a move that is typically\nplayed not in the opening but in the endgame, and see if the model can identify the odd one out. The series is:\nPe2e4, Ng1f3, Pd2d4, Bf1c4, Nb1c3, Pb4b5\nHere, all the moves are from openings (the Four Knights and the Bishop's Opening), except for the pawn moving to the\nedge of the board on the b-file. Yes, the model immediately highlights this move and indicates that it is the odd one out\nin this series.\nThus, the results are somewhat meaningful. On the other hand, the move Bf1c4 can be made outside the opening.\nAmong the remaining moves, this one is the least similar to an opening move. If we remove the move Pb4b5, what will\nbe the odd one out? The model indicates that among the series\n8e2e4, g1f3, &d2d4, \u9c7cf1c4, b1c3\nthe odd one out is f1c4. This also suggests the results are meaningful: the model distinguishes moves made in\ndifferent game situations.\nAnother function is solving proportions. The most famous example for linguistic vector models is to take the word\n\"king,\" add the vector for \"woman,\" subtract the vector for \"man,\" and you get \"queen.\"\nLet's try to do this with moves. We'll take typical opening moves for white and add a move from the same opening for\nblack. King's Gambit: Pe2e4 \u2013 Pf2f4 + pe7e5. The model suggests the move pe7e6. This is also an opening move\nthat appears, for example, in the French Defense: Pe2e4, pe7e6, Pd2d4, pd7d5...\nNow let's work with the model that includes not only moves but the position on the board (Type 2 model):\npositions_moves_pro.model. It includes 4946 words. The closest quasi-synonym neighbors are:\n->Pe2e4, &e2e4\n->Pe2e3 0.4605116844177246\n->Pe3e4 0.3592767119407654\n->Bf1g2 0.27666473388671875\n->Pe2f3 0.24844536185264587\n->Pd2d4 0.2099190503358841\n->Pc2c4 0.19693881273269653\n->Ng1f3 0.1944185346364975\n->Pg2g3 0.1831442415714264\n->Ke1g1 0.17580436170101166\nbe2 0.17578239738941193\n->Bf1b5, f1b5\n->Bf1c4 0.7282129526138306\n->Bf1d3 0.6547436714172363"}, {"title": "", "content": "->Bf1e2 0.6298115253448486\n->Bf1a6 0.5582098364830017\n->Bf1h3 0.3876230716705322\n->Be2b5 0.36358100175857544\n->Bd3b5 0.30369845032691956\n->Bc4b5 0.3022392988204956\n->Bf1g2 0.29600590467453003\n->Pd2d4 0.232324481010437\nThe closest move to &e2e4 turned out to be &e2e3, which is logical: this is also a way to start the game, and in both\ncases, the pieces are in similar positions.\nSince in this model \"words\" are not only moves but also piece positions, we can do the same with positions as we do\nwith moves. For example, we can find quasi-synonyms for the position of a white pawn on e2:\nPe3 0.5342991948127747\nPe4 0.47625163197517395\nPe5 0.3307723104953766\n->Pe3d4 0.26243242621421814\n->Pe4d5 0.2563534677028656\n->Re1e5 0.22984528541564941\n->Re1e8 0.22572621703147888\n->Re1e6 0.20842188596725464\n->Bf1c4 0.2069925218820572\n->Re1e7 0.20322373509407043\nThe closest neighbors of the vector for the white pawn in its initial position on e2 are the vectors for the white pawn\nafter the first or second move-on e3 or e4. Indeed, when the pawn is on these squares, the other pieces on the board\nare positioned similarly, representing a very similar \"context\" for the \"statement.\"\nSo far, we have looked at individual moves in the models and tried to understand something from their nearest neighbors.\nNow, I propose we look at the models as a whole to see how all the moves are positioned relative to each other.\nI created a visualization of all the moves in all the models. For visualization, I used the tSNE algorithm, a dimensionality\nreduction algorithm. The idea is that when we have a model with all the moves, these moves represent vectors in a\nmulti-dimensional space. Our models have between 300 and 500 dimensions, making it impossible to visualize. tSNE\ntakes all the dimensions present in the model and compresses them into two, projecting the entire multi-dimensional\nspace onto a plane, representing it as if all our moves existed in a two-dimensional space.\nIn this process, move-vectors turn into points on the plane. Those with similar dimensions are placed close to each other\nin the visualization. If they have similar dimensions, it means they are very likely to be neighbors (quasi-synonyms).\nThus, in the visualization, synonym moves cluster into \"bundles,\" areas where points are densely packed together.\nConsequently, vectors with different dimensions end up in different clusters. The relationships between these clusters\nare then determined by the settings of the algorithm.\nI created diagrams for all our models with different perplexity values (5, 30, 50), which is the main setting for the tSNE\nalgorithm. Higher perplexity causes clusters to become more distinct from each other. More details can be found here.\nNot all points on the graphs are labeled (there are several thousand, labeling each one would be too cluttered), but\nevery third point is labeled. Additionally, I made color versions of the graphs where each piece's move is marked with\nits own color. This shows that moves with the same piece tend to cluster together. The labels show that within one\ncluster, moves with the same piece from the same square are grouped. This is similar to what we see when looking at\nquasi-synonyms of moves. This is already a surprising result because while in type 2 models (built on positions), it is\nclear from which square a piece moves (other moves are impossible), in type 1 models (built only on moves), this is not\nso obvious. How does the model know that moves with the same piece in similar game contexts will be similar?\nThe analysis of the visual representation yields the following result: opening moves do not cluster well (do not form\ndistinct clusters). This seems as it should be: all openings are similar, the same moves are made, so there are no\npronounced trends. However, endgame moves cluster very clearly, as in a textbook-distinctly by each piece: Chess players might be interested in exceptions to this principle.\nMidgame moves fall somewhere in between there are clear clusters, but they are closer together than the clusters of\nendgame moves. The king and queen \"live\" separately, while the center of the graph is occupied by the rook. Moves\nwith the rook are equidistant from moves with all other pieces: "}, {"title": "", "content": "An interesting result emerged from the model that differentiates between moves with captures (postfix _CAP) and\nwithout captures (postfix _N): In this model, clusters form based on moves to the same square rather than from\nthe same square. So, whereas in other models moves like Ha1a3 and Ha1a5 would cluster together, here moves like\nkf2e2_CAP and kf3e2_CAP cluster together. Additionally, it is interesting that in this model, there are clusters for\nwhite pieces that include different pieces, which is rare for other models.\nA very intriguing fig. 5 came from the model reflecting games without queens on the board. This model includes both\nblack and white moves, and they are displayed absolutely symmetrically on the graph, as if in a mirror image."}, {"title": "", "content": "In fig. 6, the white-squared bishop is in a different cluster from the black-squared bishop, which makes sense from a\ngame-play perspective.\nIn fig. 7, there are two clusters for king moves: one cluster for king moves from the late endgame (like b3c3) and\nanother cluster for king moves from the middle game or early endgame (like d8e7).\nWe can analyze distant king moves for white in the positions_moves_pro.model, which are possible only in the\nfinal part of the game:\n->Kc6b7, c6b7"}, {"title": "", "content": "->Kc6c7 0.7059599161148071\n->Kc6b6 0.6991754770278931\n->Kc6b5 0.6284736394882202\n->Kc6d5 0.6259784698486328\n->Kc6d7 0.6244097352027893\n->Ka6b7 0.6211146116256714\n->Kc7b7 0.5939313769340515\n->Ka7b7 0.5806235671043396\n->Kc6d6 0.5764902830123901\n->Kb6b7 0.5578444600105286\n->Kh6g7, h6g7\n->Kh6h7 0.744156002998352\n->Kh7g7 0.6991307735443115\n->Kg6g7 0.6785054206848145\n->Kh6h5 0.6372232437133789\n->Kf6g7 0.5794045925140381\n->Kh6g5 0.5779032707214355\n->Kh7g8 0.566023588180542\n->Kg7f7 0.5618337392807007\n->Kg6h7 0.5583189129829407\nKh6 0.5513161420822144\nAmong the quasi-synonyms, there are only similar endgame moves by the white king. This indicates that the model, at\nthe very least, distinguishes between different stages of the game.\nUsually, the quasi-synonyms of a move will be moves made by the same piece. But what about the quasi-synonyms of\ncastling? After all, castling can only have two variants. Here is what we found (positions_moves_pro.model):\nShort castling for white:\n->Kelg1\n->Ke1f1 0.2816329002380371\n->Ke1e2 0.268799751996994"}, {"title": "", "content": "->Ke1d1 0.26333796977996826\n->Qd1c2 0.2582662105560303\n->Ke1f2 0.24763570725917816\n->Rh1f1 0.23578758537769318\n->Nb1c3 0.2351459413766861\n->Qd1b3 0.23325291275978088\n->Ke1c1 0.2307348996400833\n->Ke1d2 0.2285858690738678\nThe closest move turns out to be moving the king from the same (initial) square in the same direction: e1f1. The\nother two are moves by the king from the same square to the available squares around it. Among the quasi-synonyms", "castling": "h1f1.\nLong castling for white:\n->Kelc1\n->Ke1d2 0.4431324601173401\n->Kele2 0.40111061930656433\n->Rh1g1 0.3611656725406647\n->Ke1"}]}