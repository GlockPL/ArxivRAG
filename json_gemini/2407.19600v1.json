{"title": "YOU SHALL KNOW A PIECE BY THE COMPANY IT KEEPS. CHESS PLAYS AS A DATA FOR WORD2VEC MODELS", "authors": ["Boris Orekhov"], "abstract": "In this paper, I apply linguistic methods of analysis to non-linguistic data, chess plays, metaphorically\nequating one with the other and seeking analogies. Chess game notations are also a kind of text, and\none can consider the records of moves or positions of pieces as words and statements in a certain\nlanguage. In this article I show how word embeddings (word2vec) can work on chess game texts\ninstead of natural language texts. I don't see how this representation of chess data can be used\nproductively. It's unlikely that these vector models will help engines or people choose the best move.\nBut in a purely academic sense, it's clear that such methods of information representation capture\nsomething important about the very nature of the game, which doesn't necessarily lead to a win.", "sections": [{"title": "1 Introduction", "content": "In this paper, I apply linguistic methods of analysis to non-linguistic data, metaphorically equating one with the other\nand seeking analogies. The productivity of this approach has been proven within the field of Super Linguistics.\nI argue that developed by computational linguists word embeddings (made with the algorithm word2vec) can shed light\non the features of chess moves.\nRecently, computational linguistics has made a great progress in natural language processing (NLP). Within this field,\ntools have been developed for machine analysis of morphology, syntax and semantics. Computational linguistics\nbecame a stand-alone research area with its conferences (weblink) and actual fields. The experts have developed\ngeneral principles of text analysis, optimal methods of word counting. e.g. term frequency, inverse document frequency\n(tf-idf) is used instead of simple word frequency, for finding collocations Pointwise Mutual Information is used,\netc. Computational linguistics technologies are successfully implemented in the commercial products in the field of\ninformation retrieval [Roy et al., 2018], electronic marketing [Woltmann et al., 2018], modern user interfaces (voice\nassistants, chat bots), and so on.\nAt the same time, there are works that show the possibility of applying NLP tools to non-NL data. The idea of word\nembeddings proved to be particularly productive outside the text data. Word embedding is a representation of the\nsemantics of a word as a vector in a multidimensional space, the measurements of which are determined by the contexts\nin which the word appears in the texts. [Baroni et al., 2014, Levy et al., 2015]. The main technological breakthrough\nof word embeddings was that the number of measurements for each word can be relatively small, much less than the\nnumber of words in the dictionary.\nWord embeddings are a technical implementation of the linguistic idea of distributional semantics. Distributional\nsemantics is the concept that the meaning of a word is revealed through its context, and by examining the surrounding\nwords, we can infer its semantics. For example, we can assume that in the sentence \"he returned home after three"}, {"title": "2 Data", "content": "Chess game notations are also a kind of text, and one can consider the records of moves or positions of pieces as words\nand statements in a certain language. According to the rules of the PGN (portable game notation) format, an uppercase\nletter denotes a white piece, while a lowercase letter denotes a black piece. The letters themselves are obvious: R - rook\n, N - knight, B - bishop\u76d2, etc. Castling is denoted by the move of the king (the movement of the rook is implied\nby default).\nAn example of a chess game recorded in PGN format looks like this:\nHere, the first letter in each \"word\" denotes the piece and its color, the next two symbols indicate the square from which\nthe piece moves, and the last two symbols indicate the square to which the piece moves. Thus, Pe2e4 is a move by a\nwhite pawn (uppercase letter, so the piece is white, the letter P indicates a pawn) from square e2 to square e4. For more\ndetails see [Boutell et al., 2003].\nOne could build vector models based on game texts that reflect the moves, as in the example above, but this is somewhat\ncontroversial because, with this approach, the \"context\" is lost, meaning the position in which the move is made.\nConsidering moves separately from the position is a strange idea for chess. Moves and their function in the game do not\nexist independently of the position. However, this is similarly strange when it comes to words, when we look at them\nout of context."}, {"title": "3 Results", "content": "First, we need to ensure that the models truly reflect chess reality and do not contain random sequences of moves from\na chess perspective. Vector models allow us to find similar words (in our case, moves). Let's try working with the basic\nmodel moves_texts.model. The model contains 7749 words (moves). Here are a few random examples. First, the\nmove is listed (highlighted in bold) for which we are searching for the closest move vectors, followed by the \"synonym\"\nmoves (usually referred to as quasi-synonyms), and the cosine distance between the vectors is indicated (ranging from 0\nto 1, the higher the number, the closer the vectors are to each other):\nFrom these lists, it is clear that the vectors in the model are not distributed randomly. The moves closest to a given\nmove are usually those made by the same piece. This is not obvious in chess, as different continuation options can exist\nin similar game situations (positions are not considered in this model). However, linguistically, this is expected because\nquasi-synonyms are usually words of the same part of speech or even cognates.\nFurthermore, to find the moves closest in \"semantics\" to a given move, we can calculate the cosine distance between\nany two arbitrary moves in the model. For example, for Pe2e4 and pe7e5, the distance is 0.1094483. This is a minimal\nvalue, resulting because the context for the opening move Pe2e4 includes all possible continuations, whereas pe7e5 is\nrelatively local compared to this variety.\nAnother available function is \"find the odd one out.\" In a linguistic vector model, if we propose a series of words like\n\"apple,\" \"pear,\" \"grape,\" \"banana,\" \"orange,\" and \"cucumber,\" the model will indicate that \"cucumber\" is the odd one\nout. This is because \"cucumber\" is typically used in a different context than the other words, so its vector has noticeably\ndifferent coordinates."}, {"title": "4 Conclusion", "content": "I don't see how this representation of chess data can be used productively. It's unlikely that these vector models will\nhelp engines or people choose the best move. But in a purely academic sense, it's clear that such methods of information\nrepresentation capture something important about the very nature of the game, which doesn't necessarily lead to a win.\nObservations have shown that linguistic data are richer and more diverse than chess data. Fewer than 10,000 words\nin the model is far fewer than the tens of thousands of words in models built from linguistic data. The limitations\nof the board and the number of pieces restrict the possible configurations of the vector space, despite the seemingly\noverwhelming number of possible moves.\nThe semantics reflected in the vector model include the stage of the game (moves from the opening and from the\nendgame are different), differentiate piece colors, the squares from which and to which moves are made, and connect\ntogether moves made by the same piece (including capturing the fact that castling involves two pieces).\nThe model doesn't account for differences between various openings (the difference between &e2e4 and &e2e3 is\nminimal). All of this highlights not so much the productivity of the word2vec algorithm as the significance of the"}]}