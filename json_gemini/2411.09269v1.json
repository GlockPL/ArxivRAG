{"title": "Harnessing multiple LLMs for Information Retrieval: A case study on Deep Learning methodologies in Biodiversity publications", "authors": ["Vamsi Krishna Kommineni", "Birgitta K\u00f6nig-Ries", "Sheeba Samuels"], "abstract": "Deep Learning (DL) techniques are increasingly applied in scientific studies across various domains to address complex research questions. However, the methodological details of these DL models are often hidden in the unstructured text. As a result, critical information about how these models are designed, trained, and evaluated is challenging to access and comprehend. To address this issue, in this work, we use five different open-source Large Language Models (LLMs): Llama-3 70B, Llama-3.1 70B, Mixtral-8x22B-Instruct-v0.1, Mixtral 8x7B, and Gemma 2 9B in combination with Retrieval-Augmented Generation (RAG) approach to extract and process DL methodological details from scientific publications automatically. We built a voting classifier from the outputs of five LLMs to accurately report DL methodological information. We tested our approach using biodiversity publications, building upon our previous research. To validate our pipeline, we employed two datasets of DL-related biodiversity publications: a curated set of 100 publications from our prior work and an additional set of 364 publications from the Ecological Informatics journal. Our results demonstrate that the multi-LLM, RAG-assisted pipeline enhances the retrieval of DL methodological information, achieving an accuracy of 69.5% (417 out of 600 comparisons) based solely on textual content from publications. This performance was assessed against human annotators who had access to code, figures, tables, and other supplementary information. Although demonstrated in biodiversity, our methodology is not limited to this field; it can be applied across other scientific domains where detailed methodological reporting is essential for advancing knowledge and ensuring reproducibility. This study presents a scalable and reliable approach for automating information extraction, facilitating better reproducibility and knowledge transfer across studies.", "sections": [{"title": "INTRODUCTION", "content": "Deep Learning (DL) has become a cornerstone in numerous fields, revolutionizing how complex data is analyzed and interpreted. From healthcare and finance to autonomous systems and natural language processing, DL techniques have delivered groundbreaking results. However, as the adoption of DL continues to grow, there is an increasing recognition of a critical shortcoming: the limited availability of detailed methodological information in scientific literature (Waide et al., 2017; Stark, 2018; Samuel et al., 2021; Pineau et al., 2021; Gundersen et al., 2022). This gap presents significant challenges for researchers and practitioners who seek to understand, replicate, and build upon existing studies (Feng et al., 2019; GPAI, 2022). Past research has emphasized the need to make primary data and clear metadata available to support transparency (Michener et al., 1997; Whitlock, 2011).\nA DL pipeline is a structured process for training and deploying DL models, starting with data collection and preprocessing tasks like cleaning, normalization, and transformation (El-Amir and Hamdy, 2020). After preparing the data, the pipeline moves to model selection, where an appropriate architecture is chosen based on model complexity and problem type. The selected model is then trained on preprocessed data, fine-tuning through specific optimization algorithms and hyperparameter configurations. Once trained, the model's performance is evaluated on test data to ensure reliable, unbiased results. The final step involves deploying the model for real-world use or further refinement.\nFor a DL pipeline to be reproducible, detailed documentation at each stage is essential (Pineau et al., 2021). This includes logging data collection methods, preprocessing steps, model architecture configura-tions, hyperparameters, and training details, as well as performance metrics and test datasets. Additionally, maintaining records of software libraries, hardware, frameworks, and versions used is critical for the accurate replication of the study. Without access to such crucial information, stakeholders\u2014including academics, industry professionals, and policymakers-face significant challenges in validating study outcomes or advancing research in meaningful ways. In areas like healthcare, finance, and autonomous systems, where DL applications influence real-world decisions, the absence of methodological trans-parency can compromise trust in DL models and limit their broader application (Haddaway and Verhoeven, 2015). We contend that the same holds true for biodiversity research.\nThe advent of DL has significantly transformed various domains, including biodiversity research, by enabling advanced methodologies for data analysis and interpretation (August et al., 2020). However, manually extracting relevant deep-learning information from scientific articles remains a labour-intensive and time-consuming process. This challenge affects both the reproducibility of the original studies and the reproducibility of secondary analyses aimed at understanding the methods employed. Traditional manual retrieval methods can be inconsistent, as the perspective of the annotators often varies based on their task interpretation and domain knowledge. These inconsistencies hinder efforts to systematically review or replicate the methodological approaches across studies, highlighting the need for more automated solutions.\nTo address these challenges, we propose a novel approach that leverages the capabilities of Large Language Models (LLMs) for the automated extraction and processing of DL methodological information from scientific publications. LLMs, which are trained on vast amounts of text data, have demonstrated impressive capabilities in natural language understanding and generation. Specifically, we employ five open-source LLMs: Llama-3 70B\u00b9, Llama-3.1 70B2, Mixtral-8x22B-Instruct-v0.13, Mixtral 8x7B4, and Gemma 2 9B5 in combination with Retrieval-Augmented Generation (RAG) approach (Lewis et al., 2020). By utilizing these advanced models, we aim to extract relevant methodological details with greater accuracy and efficiency than manual methods alone. Our methodology is structured into three critical components: identifying relevant research publications, automatically extracting information through an RAG approach, and converting the extracted textual responses into categorical values for streamlined evaluation.\nIn this work, we take biodiversity publications as a case study due to the growing popularity of DL methods in biodiversity research and the enormous number of publications using DL for various applications in this domain. Given the importance of biodiversity research and the critical need for transparent sharing of DL information in these studies (GPAI, 2022), we chose this field to demonstrate our approach. However, our methodology is not limited to biodiversity alone; it can be applied to other domains where detailed methodological reporting is essential for advancing scientific knowledge and ensuring reproducibility.\nTo enhance the reliability of our approach, we developed a voting classifier that aggregates the outputs of these LLMs, ensuring that the reported information is consistent and accurate. This methodology was applied to two distinct datasets of biodiversity publications focused on DL: one consisting of 100 publications from our previous work (Ahmed et al., 2024b) and another comprising 364 publications from the Ecological Informatics journal6."}, {"title": "RELATED WORK", "content": "The integration of DL into scientific research has been transformative across a variety of fields, leading to significant advancements in data analysis, pattern recognition, and predictive modelling. However, the challenge of adequately documenting DL methodologies has been widely recognized, and several studies have highlighted the importance of transparency and reproducibility in DL research (Whitlock, 2011; Haddaway and Verhoeven, 2015; Waide et al., 2017; Stark, 2018; Samuel et al., 2021; Pineau et al., 2021; Gundersen et al., 2022).\nThe lack of detailed methodological reporting in DL studies has been a point of concern across multiple domains. Numerous researchers have called attention to the need for better documentation practices, emphasizing that insufficient details about model architecture, training procedures, and data preprocessing steps can lead to challenges in replicating results. For example, Gundersen and Kjensmo (2018) discusses how the reproducibility crisis has impacted other scientific disciplines and is now a growing concern in DL research due to these gaps in methodological transparency. Similarly, Pineau et al. (2021) advocates for standardising reporting practices in machine learning papers to ensure that experiments can be independently reproduced, thereby fostering greater confidence in published findings.\nIn the biodiversity domain, DL has seen rapid adoption, driven by its potential to handle large-scale and complex ecological data. Applications of DL in biodiversity include species identification, habitat classification, and population monitoring, as evidenced by works such as Christin et al. (2019) and Norouzzadeh et al. (2017). However, inadequate documentation of DL methodologies is particularly problematic in biodiversity due to the field's interdisciplinary nature and the complexity of ecological data. Biodiversity DL studies require transparency in methods to enable stakeholders, including conservationists and ecologists, to replicate and build upon findings effectively. The need for comprehensive and transparent methodological documentation in DL research is well-established, as studies without such documentation are difficult to reproduce or expand upon. In ecological and biodiversity studies, this problem is particularly acute, as the integration of DL methods is relatively new and is still evolving (Feng et al., 2019). Whitlock (2011) and Michener et al. (1997) previously raised the importance of archiving primary data with clear metadata to enhance reproducibility. More recently, efforts in ecological informatics have focused on creating reproducible workflows for studies involving complex DL techniques. By automating the extraction of categorical and structured responses from biodiversity DL publications, our pipeline addresses these reproducibility challenges, aiming to make DL methodologies in biodiversity research more accessible and consistent. Addressing this need for methodological clarity is an important part of our work, which focuses on extracting DL methodologies from biodiversity publications. Our previous work has also emphasized these challenges (Ahmed et al., 2023; Kommineni et al., 2024a), especially in the context of semi-automated construction of the Knowledge Graphs (KGs) to improve data accessibility (Kommineni et al., 2024b).\nThe emergence of Large Language Models has introduced new possibilities for automatically ex-tracting and synthesizing information from text (Zhu et al., 2023), which can be particularly useful for addressing the gaps in methodological reporting. LLMs, such as GPT-3 (Brown et al., 2020) and its successors (Achiam et al., 2023; Touvron et al., 2023; Team et al., 2024), have demonstrated remarkable abilities in natural language understanding and generation, enabling tasks like summarization, question-answering, and information retrieval from vast textual datasets. Recent studies, including those by (Lewis et al., 2020) on Retrieval-Augmented Generation (RAG), have explored how combining LLMs with retrieval mechanisms can enhance the extraction of relevant information from large corpora, offering a promising solution for improving the accessibility of methodological details in scientific literature. In this study, we build on these developments by employing a multi-LLM and RAG-based pipeline to retrieve and categorize DL-related methodological details from scientific articles systematically.\nWhile the application of LLMs for methodological extraction remains underexplored, several tools and approaches have been developed for automating information extraction (Beltagy et al., 2019; Lozano et al., 2023; Dunn et al., 2022; Dagdelen et al., 2024). Tools like SciBERT (Beltagy et al., 2019) and other domain-specific BERT models have been used to extract structured information from unstructured text, yet their application has primarily been focused on citation analysis, abstract summarization, or specific biomedical applications. Bhaskar and Stodden (2024) introduced \"ReproScreener,\u201d a tool for evaluating computational reproducibility in machine learning pipelines, which uses LLMs to assess methodological consistency. Similarly, Gougherty and Clipp (2024) tested an LLM-based approach for extracting ecological information, demonstrating the potential of LLMs to improve metadata reporting and transparency. These studies underscore the need for versatile, automated methodologies capable of handling DL pipeline documentation across various fields. The use of LLMs for extracting detailed DL methodologies across a broad spectrum of scientific domains remains underexplored. Our work aims to fill this gap by utilizing multiple LLMs in conjunction with a Retrieval-Augmented Generation (RAG) approach to extract and consolidate DL methodological details from biodiversity literature, offering a framework adaptable to other domains.\nRecent studies have highlighted the environmental impact of computational processes, particularly in DL research. Training LLMs and executing complex pipelines can consume substantial energy, contributing to carbon emissions. Lannelongue et al. (2021a,b) called for increased awareness of the ecological impact of computational research and proposed the routine assessment of environmental footprints as part of research best practices. In biodiversity research, where sustainability is a core value, these considerations are especially relevant. Our study contributes to this body of work by quantifying the environmental footprint of our DL-powered information retrieval pipeline using metrics such as kWh consumption and carbon emissions. This assessment is intended to encourage sustainable practices in computational research and aligns with recent recommendations to integrate environmental accountability into scientific workflows."}, {"title": "METHODS", "content": "In this section, we provide detailed information about the pipeline employed to extract and analyse the information from the selected biodiversity-related publications."}, {"title": "Dataset", "content": "Our work is based on two datasets. The first one originates from our previous research (Ahmed et al., 2024a), while the second is sourced from the Ecological Informatics Journal. Each dataset was indexed using different methodologies, contributing to a diverse representation of information. This variation arises from the range of journals included in the first dataset and the specific selection criteria applied in the second."}, {"title": "Dataset from Prior Research", "content": "In our previous study (Ahmed et al., 2024b), we used a modified version of the keywords from previous research (Abdelmageed et al., 2022) to query Google Scholar and indexed over 8000 results. From this, the authors narrowed down the selection to 100 publications, excluding conference abstracts, theses, books, summaries, and preprints. Later, the first and second authors of that work manually extracted deep-learning information on ten variables (Dataset, Source Code, Open source frameworks or environment, Model architecture, Software and Hardware Specification, Methods, Hyperparameters, Randomness, Averaging result and Evaluation metrics) from the biodiversity publications, recording each as a categorical value:"}, {"title": "Dataset from Ecological Informatics journal", "content": "To index deep-learning-related publications from the Ecological Informatics journal, we first identified relevant keywords and used them to guide the indexing of publications.\nKeywords selection: Related keywords are crucial for automatically indexing deep learning-related publications from a journal. To identify these relevant deep-learning keywords, we downloaded AI-related session abstracts from the Biodiversity Information Standards (TDWG) conferences7 held in 2018 (Pando et al., 2018), 2019 (Frandsen et al., 2019), and 2021-2023 (Groom and Ellwood, 2021; Kommineni et al., 2022; Johaadien et al., 2023) (no AI session was available for 2020). We then used an open-source large language model (Mixtral 8x22b Instruct-v0.1) to extract all deep-learning-related keywords from each abstract. The query in the prompt template for extracting deep learning keywords from the given context is \"your task is to extract the deep learning related keywords from the provided context for the literature survey\"."}, {"title": "Prompt for deep-learning-related keyword extraction", "content": "%INSTRUCTIONS:\nUse the provided pieces of context to answer the query. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n%Query\nQuery: {query}\nContext: {context}\nProvide your answer as follows:\nAnswer:::\nDeep learning related words: (Deep learning related words in comma separated list)\nAnswer:::"}, {"title": "Publication citation data extraction", "content": "Using the 25 refined keywords identified from TDWG abstracts with the assistance of both the LLM and domain experts, we queried the Ecological Informatics journal. The query applied the following filters: publication years from 2016 to August 1, 2024, article type as research articles, and open-access availability. Due to the platform's limit of 8 boolean connectors per search, the keywords were divided into five sets, each connected with the boolean operator OR (e.g., \"Keyword 1\" OR \"Keyword 2\" OR \"Keyword 3\" OR \"Keyword 4\" OR \"Keyword 5\"). Citation data from each search was manually exported in BibTeX format. In total, 991 citation records were indexed, and after removing duplicates based on DOIs, 364 unique publications were identified."}, {"title": "Full-text publication download", "content": "Using the DOIs of the 364 unique publications, we retrieved the full-text PDFs through the Elsevier API. These PDFs were subsequently used as input for the selected LLMs."}, {"title": "Competency Questions (CQs)", "content": "We employed competency questions (CQs) to retrieve specific deep-learning methodological information from selected biodiversity publications. Competency questions are natural language questions that users seek answers to and are essential for defining an ontology's scope, purpose, and requirements (Gr\u00fcninger and Fox, 1995). In our previous work (Kommineni et al., 2024b), two domain experts formulated 28 CQs to cover every aspect of the DL pipeline for retrieving information from the provided context. For this study, we applied the same set of 28 CQs with multiple LLMs to extract relevant deep-learning information from a total of 464 biodiversity-related publications (364 from Ecological Informatics and 100 from previous research)."}, {"title": "Information retrieval", "content": "Recently, the RAG approach has rapidly been used for information retrieval from both structured and unstructured data. This method leverages large language model (LLM) text generation to extract informa-tion from authoritative sources, such as biodiversity publications in our case. In this work, we employed five LLMs from two providers, namely hugging face Mixtral 8x22B Instruct v0.18 and Groq's Llama 3.1 70B, Llama 3 70B, Mixtral 8x7B and Gemma 2 9B with temperature set to 0 for all models. The Mixtral 8x22B Instruct v0.1 model was run on a custom GPU, while the Groq models were accessed through their API, where a custom GPU is not required.\nInformation retrieval using LLMs and RAG was also a component of our previous work pipeline (Kommineni et al., 2024b), where we aimed to build a semi-automated construction of the Knowledge Graph (KG) pipeline (we refer to the definition of KG from (Hogan et al., 2021)). This approach allowed us to extract, organize, and link information from unstructured text into structured, queryable data within the KG framework. By semi-automating the construction of KGs, we streamlined the process of mapping complex domain knowledge, which is crucial for advancing research in areas that require high levels of detail, such as biodiversity and deep learning methodologies. In this work, we build on our previous information retrieval component (then CQ Answering) by limiting the retrieval tokens to 1200, chunk size to 1000 and overlap to 50 chunks. Additionally, we specified that the responses should be concise and limited to fewer than 400 words to enhance the clarity and focus of the responses. For each selected LLM, the CQs and biodiversity-related publications were provided as input, and the RAG-assisted LLM pipeline generated answers to all CQ-publication combinations in textual sentence format as output."}, {"title": "Preprocessing LLM outputs", "content": "After the information retrieval process, we obtained answers to the CQ for each combination of LLM, CQ, and publication. Some of these responses contained unnecessary structured information. To streamline the outputs, we preprocessed the responses using a Python script, removing strings like \"Helpful Answer::\" and \"Answer::\" to eliminate unnecessary content. We indexed only the information following these strings for the Mixtral 8x22B Instruct v0.1 model, as that portion contained details relevant to the queries and selected context.\nNext, we converted all preprocessed LLM textual responses into categorical \"yes\" or \"no\" answers. To achieve this, we prompted the same LLMs that generated the CQ answers to provide categorical responses for each question-answer pair. To guide this process, a couple of in-context examples are provided in the prompt as references for the LLM. This conversion from textual to categorical responses will later facilitate the evaluation of our pipeline."}, {"title": "Prompt for converting textual to categorical responses", "content": "%INSTRUCTIONS:\nYour task is to determine if the provided answer contains specific information that could be derived from a research paper, based on the given question and answer. Provide a binary response (\"yes\" or \"no\"). Use the examples below as a reference for your decision.\n\"Yes\": Choose \"yes\" if the answer includes specific details or direct information that seems likely to come from a research paper.\n\"No\": Choose \"no\" if the answer is general or does not provide specific information that appears to be sourced from a research paper.\n%EXAMPLES:\nExample 1:\nQuestion: What methods are utilized for collecting raw data in the deep learning pipeline (e.g., surveys, sensors, public datasets)?\nAnswer: Unfortunately, there is no information provided about where the code repository of the deep learning pipeline is available. It could be hosted on platforms such as GitHub, GitLab, or BitBucket, but without explicit mention in the provided context, I cannot provide a definitive answer.\nResponse: No\nExample 2:\nQuestion: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?\nAnswer: The study uses audio data from bird calls, specifically spectrograms derived from the audio files. These spectrograms serve as the input for the Convolutional Neural Network (CNN) model employed in the research. Therefore, the primary data format utilized in this deep learning pipeline is audio data, processed into spectrograms for further analysis.\nResponse: Yes\nExample 3:\nQuestion: What are the data annotation techniques used in the deep learning pipeline (e.g., bounding box annotation, instance segmentation)?\nAnswer: Unfortunately, there isn't any information about specific data annotation techniques used in the deep learning pipeline within the provided context. However, it does mention that Convolutional Neural Networks (CNNs) were employed for image classification tasks and pattern recognition. This implies that some form of image labeling or categorization was likely involved during the model training process. Nevertheless, without explicit details regarding the use of bounding box annotations, instance segmentations, or other similar techniques, I cannot provide a definitive response based solely on the given context.\nResponse: No\n%QUERY\nHere are the \"Question\" and \"Answer\"\nQuestion: {Question}\nAnswer: {Answer}\n%ANSWER FORMAT\nAnswer:::\nResponse: (Yes or No)\nAnswer:::"}, {"title": "Voting classifier", "content": "Since we leveraged multiple LLMs to retrieve the DL-related information and processed that information to categorical values, it became feasible to build a voting classifier. We employed a hard voting methodology, where each of the five instances (derived from five LLMs) produced possible outcomes of \"yes\" or \"no\" for each combination of CQ and publication. The voting classifier made decisions based on the majority of votes, which enhances the overall quality of the results."}, {"title": "Evaluation", "content": "All key outputs generated by the LLMs, including the CQ answers and the conversion of textual responses to categorical values (\"yes\" or \"no\"), were manually evaluated. For assessing the CQ answers, we relied on our previous work (Kommineni et al., 2024b), in which we manually evaluated 30 publications from the evaluation dataset. To evaluate the categorical responses produced by the LLMs, we randomly selected 30 publications, used those for each LLM, and manually annotated the ground truth data by assessing the question-answer pairs generated by the RAG-assisted LLM pipeline. We then compared the inter-annotator agreement between the LLM-generated and manually annotated answers using Cohen's kappa score10. This annotation process was conducted by the first and last authors of this paper."}, {"title": "Additional analysis", "content": "Publication filtering: Our pipeline was driven by the DL-related keywords, which means that our dataset may include publications that mention these keywords without actually detailing a DL pipeline. To investigate this assumption as an addition to our current pipeline, we filtered the publications by using a RAG-assisted LLM pipeline (Llama 3.1 70B) to identify if any publications that contained only DL-related keywords, rather than discussing a DL pipeline. To evaluate the LLM's judgement, we compared its findings with 100 articles from our previous work (Ahmed et al., 2024a), where all the publications were focused on DL methods. Furthermore, we also compared the outputs of all the publications with those of filtered publications.\nTime logs: Computational tasks inherently rely on physical resources, and there is a growing awareness of the substantial environmental footprint associated with both the production and use of these resources (Lannelongue et al., 2021a; Samuel and Mietchen, 2024). In the context of our work, which leverages information retrieval workflows involving DL methodologies in biodiversity research, one of our aims is to evaluate and quantify the environmental impact of these computational processes. In this pipeline, we recorded the time taken to process all the requests for each document. We preprocessed the time logs by considering the last instance while removing the duplicates based on the unique identifiers of the log file. These time records are essential for calculating the environmental footprint (Lannelongue et al., 2021a,b) of the pipeline. By assessing the energy and resource consumption of our DL-driven information retrieval pipeline, we hope to contribute to more sustainable practices in biodiversity informatics and computational research more broadly.\nSemantic similarity between five LLM outputs: As mentioned before, we have five answers for each combination of CQ and publication\u2014one from each LLM\u2014formatted in both textual and categorical forms. We used these five textual answers to compute the cosine similarity11 matrix. With this matrix, average cosine similarities for all the responses between all the LLM combinations were calculated. Additionally, we assessed the inter-annotator agreement among the categorical responses using Cohen's kappa score for all possible combinations.\nEnvironmental footprint: Although our pipeline recorded processing times for each publication and each combination of CQ and publication, we only utilized the logged times for each publication for two key components of the pipeline (Table 1): 1. RAG answers and 2. Conversion of RAG textual responses to categorical responses. To estimate the environmental footprint, we used the website12 (Lannelongue et al., 2021b), which requires input on hardware configuration, total runtime, and location to estimate the environmental footprint of our computations. Our calculation only accounts for the pipeline components mentioned above and the hardware components from our side, excluding the hardware components from Groq. Our pipeline consumed 177.55 kWh of energy to generate the RAG textual responses, resulting in a carbon footprint of 60.14 kg CO2e, which is equivalent to the carbon offset of 64.65 tree months. For converting textual to categorical responses, the pipeline consumed 50.63 kWh of energy, corresponding to a carbon footprint of 17.15 kg CO2e and 18.7 tree months. For the environmental footprint estimates, we selected Germany as the location and assumed that we used the total number of cores in the Intel Xeon Platinum 9242 processor (which is 48 cores)."}, {"title": "RESULTS", "content": "This section presents the results from each part of the pipeline. We queried 28 CQs (Kommineni et al., 2024b) across 464 publications for each LLM, resulting in a total of 12,992 textual answers. Overall, we obtained 64,960 textual responses from the five selected LLMs. These textual responses were then converted into categorical \"yes\" or \"no\" responses using the respective LLMs.\nTo evaluate the LLM's judgements in these conversions, we compared the categorical responses against human-annotated ground truth data from 30 randomly selected publications. We used those randomly selected 30 publications for each LLM, leading to 840 comparisons per LLM (30 publications \u00d7 28 CQs). This resulted in 4,200 comparisons for five LLMs, with 3,566 agreements between the LLM responses and the human-annotated ground truth responses, achieving a maximum agreement of 752 out of 840 for the Llama 3 70B model."}, {"title": "DISCUSSION", "content": "Manually extracting DL-related information from scientific articles is both labour-intensive and time-consuming. Current approaches that rely on manual retrieval often vary significantly based on the annotator's perspective, which can differ from one annotator to another due to task interpretation and the annotators' domain knowledge (Ahmed et al., 2024b). This variability can lead to inconsistencies and raises significant concerns regarding the reproducibility of manually annotated data.\nTo address these challenges, this work proposes an automated approach for retrieving information from scientific articles by employing five different LLMs. This strategy aims to improve both the accuracy and diversity of information extraction. By utilizing multiple LLMs, our pipeline is positioned to capture a broader range of variable-level information related to DL methodologies in scientific publications.\nIn this current pipeline, there are three critical components: 1. Identifying relevant research publica-tions 2. Automatically extracting relevant information from publications for the desired queries, and 3. Converting the extracted textual responses into categorical responses. For the first component, we choose a method that extracts publications based on selected keywords. These keywords were derived from AI-related abstracts presented at the Biodiversity Information Standards (TDWG) conference, resulting in a total of 25 keywords. It is important to note that even if a publication mentions any of the keywords only once, without providing the actual DL methodology, it will still be included in the extraction process. As a result, our pipeline queries these publications, which may yield a higher number of negative responses, indicating that the context does not contain relevant information to answer the queries.\nTo mitigate this issue, we filtered the extracted publications again using the RAG-assisted pipeline. As a result, of this filtering, the number of publications decreased by 44.6%, leaving us with 257 publications. This process was also evaluated using 100 publications from previous work (Ahmed et al., 2024a), all of which included DL methodologies in the study, and it achieved an accuracy of 93%. Before filtering, our pipeline only provided positive responses to 27.12% of the total queries (3,524 out of 12,992). After implementing the filtering step, the percentage of positive responses increased to 35.77% (2,574 out of 7,196). This represents an improvement of 8.65% in the positive response rate, which is a significant gain. However, after filtering, 64.23% of the queries still did not yield available information in the publications. This gap can be attributed to the complexity of the queries (CQs), which cover all aspects of the DL pipeline, from data acquisition to model deployment.\nIn practice, not all studies utilize techniques like data augmentation; some prefer to use readily available datasets, thus bypassing the formal requirement for data annotation steps. Moreover, certain studies may not address model deployment at all. As a result, it is uncommon for publications to provide details on aspects such as deployment status, model randomness, generalizability, and other related factors. Consequently, the positive response rate for the queries tends to be relatively low.\nTo address the second component, we employed an RAG-assisted LLM pipeline to extract relevant information from the publications for all our queries (CQs). This component generated a total of 12,992 textual responses for each combination of queries (CQs) and publications across the different LLMs. The textual responses were initially preprocessed, and we calculated the average cosine similarity between the generated responses by different LLMs. The average cosine similarity score was high for the Llama 3.1 70B - Llama 3 70B model pair, indicating that these models generated similar outputs. On the other hand, the Gemma 2 9B - Mixtral 8x22B Instruct v0.1 model pair exhibited a lower average cosine similarity score, suggesting more significant variability in their response generation. Even after filtering the publications, the trend in the similarity scores remained consistent for these two model pairs, indicating that the response generation was not significantly affected by the exclusion of publications that did not utilize DL methods in their studies.\nThe third crucial component of our pipeline involves converting the extracted textual responses into categorical responses. This transformation simplifies the evaluation process, making it easier to compare the outputs generated by the LLM with human-extracted outputs from previous work (Ahmed et al., 2024a). Additionally, it facilitates the creation of an ensemble voting classifier. Two annotators reviewed the different question-answer pairs generated by the LLM and provided their assessments to ensure effective conversion from textual to categorical responses. The IAA scores between the human-annotated and LLM responses indicated that the highest levels of agreement were observed for the Llama 3 70B, Llama 3.1 70B, and Gemma 2 9B models in descending order, which generated straightforward answers that were easy for human annotators to evaluate. In contrast, the Mixtral 8x22B Instruct v0.1 and Mixtral 8x7B models exhibited the lowest IAA scores, reflecting only moderate agreement. The generated responses from these models were often ambiguous, combining actual answers with generalized or hallucinated content, which made it challenging for annotators to make precise judgments.\nWe also calculated the IAA scores for the categorical responses generated by different LLM pairs to evaluate the level of agreement among them. Overall, we observed a moderate to strong agreement between the various LLMs. However, following the publication filtering process, the IAA scores improved for all LLM pairs, indicating that the quality of the generated responses enhanced after the filtering.\nThe categorical responses have powered the ensemble approach of the voting classifier. We compared the categorical values from the voting classifier to the manually annotated values from our previous work for six deep-learning variables. This comparison revealed that the agreement between the LLM and human annotations is particularly low for the datasets, open-source frameworks, and hyperparameters. In the manual annotations, the authors from the previous work (Ahmed et al., 2024a) also considered the accompanying code, which could explain the low agreement regarding open-source frameworks and hyperparameters. For datasets, the authors from the previous work (Ahmed et al., 2024a) considered dataset availability only when persistent identifiers were provided in the respective studies. In contrast, the LLM also considers the dataset name itself, even when persistent identifiers are not mentioned.\nOur approach incorporates a variety of models, each with distinct parameters, ensuring that the voting classifier considers diverse perspectives generated by different models for the same query. By ensembling the outputs of these varied models, the voting classifier enhances its robustness in making final decisions. This method not only enriches the decision-making process but also improves the classifier's overall reliability."}, {"title": "CONCLUSIONS", "content": "There is widespread concern about the lack of accessible methodological information in DL studies. We systematically evaluate whether that is the case for biodiversity research. Our approach could be used to alleviate the problem in two ways: 1) by generating machine-accessible descriptions for a corpus of publications 2) by enabling authors and/or reviewers to verify methodological clarity in research papers. Although our methodology has been demonstrated in the context of biodiversity studies", "LLMs": "Llama-3 70B, Llama-3.1 70B, Mixtral-8x22B-Instruct-v0.1, Mixtral 8x7B, and Gemma-2 9B to create an ensemble result, and then comparing the"}]}