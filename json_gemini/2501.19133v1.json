{"title": "Decorrelated Soft Actor-Critic\nfor Efficient Deep Reinforcement Learning", "authors": ["Burcu K\u00fc\u00e7\u00fcko\u011flu", "Sander Dalm", "Marcel van Gerven"], "abstract": "The effectiveness of credit assignment in reinforcement learning (RL) when dealing with\nhigh-dimensional data is influenced by the success of representation learning via deep neural\nnetworks, and has implications for the sample efficiency of deep RL algorithms. Input decorre-\nlation has been previously introduced as a method to speed up optimization in neural networks,\nand has proven impactful in both efficient deep learning and as a method for effective represen-\ntation learning for deep RL algorithms. We propose a novel approach to online decorrelation\nin deep RL based on the decorrelated backpropagation algorithm that seamlessly integrates\nthe decorrelation process into the RL training pipeline. Decorrelation matrices are added to\neach layer, which are updated using a separate decorrelation learning rule that minimizes the\ntotal decorrelation loss across all layers, in parallel to minimizing the usual RL loss. We used\nour approach in combination with the soft actor-critic (SAC) method, which we refer to as\ndecorrelated soft actor-critic (DSAC). Experiments on the Atari 100k benchmark with DSAC\nshows, compared to the regular SAC baseline, faster training in five out of the seven games\ntested and improved reward performance in two games with around 50% reduction in wall-clock\ntime, while maintaining performance levels on the other games. These results demonstrate the\npositive impact of network-wide decorrelation in deep RL for speeding up its sample efficiency\nthrough more effective credit assignment.", "sections": [{"title": "Introduction", "content": "Reinforcement learning (RL) is a framework for learning effective control policies in sequential de-\ncision making tasks, based on feedback from the task environment (Sutton and Barto, 2018). The\nfeedback is received in the form of rewards, following the actions taken by an artificial agent at every\ntime step. The key in learning an optimal action sequence lies in correctly identifying the impact\nof an action on future rewards. Such credit assignment is especially challenging when dealing with\ncomplex high dimensional data like images, which make the learning of the state representations\ncritical for RL's success. The use of deep neural networks as function approximators facilitated RL's\nachievement in representation learning (Mnih et al., 2013, 2015). However, RL needs still a lot of\ninteractions to be able to solve a task, which makes it notoriously sample inefficient.\nTargeting the issue of sample efficiency has long been a focus of RL research (Kaiser et al.,\n2020; Haarnoja et al., 2018a,b; K\u00fc\u00e7\u00fcko\u011flu et al., 2024). A common characteristic of all these RL\napproaches is their use of stochastic gradient descent (SGD) through the backpropagation (BP) algo-\nrithm (Linnainmaa, 1976; Werbos, 1974) for training of the deep neural network (DNN) structures."}, {"title": "Methods", "content": ""}, {"title": "Decorrelated Backpropagation", "content": "Decorrelated backpropagation (DBP) (Dalm et al., 2024) is an alternative algorithm to standard\nbackpropagation for efficient training of neural networks. It utilizes an iterative decorrelation proce-\ndure (Dalm et al., 2024; Ahmad et al., 2023) for decorrelation of layer inputs in the network. This is\nachieved by introducing an additional matrix to each layer which decorrelates inputs before they are\nmultiplied by the forward weights. Decorrelation of each layer input is implemented independently\nas a linear transformation via\n$x = Rz$\nwhere z is the raw layer input that is multiplied by the decorrelating matrix R to get the decorrelated\ninput x. The decorrelated input is then passed on to the original network layer to accomplish the\nforward pass, which gives the output of that layer as\n$y = f(Wx) = f(WRz) = f(Az)$\nwhere output y\u0131 of a layer serves as the correlated input z1+1 of the next layer. Note that f is a non-\nlinear transformation and Wx can be any multiplication by forward weights (e.g. fully-connected,\nconvolutional, recurrent, etc).\nThe decorrelating matrix R for each layer is initialized as the identity matrix and then trained\nindependently by applying:\n$R \u2013 R \u2013 \u03b7CR$\nwhere n is a learning rate and C is the empirical off-diagonal correlation of the decorrelated input\nwith itself, given by\n$C = E [xx^T] \u2013 diag (E [x^2])$\nwhere diag (E [x2]) is the diagonal of the expectation of the correlation matrix xxT. We are thus\naiming to minimize the off-diagonal elements of xxT. The above learning rule minimizes a decorre-\nT\nlation loss d for layer I given by\n$d = \\sum_{i=1}^{n} \\sum_{j=1}^{n} C_{ij}^2$\nwhere cij are the elements of Cr, the correlation matrix of layer 1. Thus, the total decorrelation loss\nD for the network is\n$D = \\sum_{l=1}^{L} d_{i}$\nwhere L is the total number of layers.\nNote that while the above formulas give a general description of our decorrelation procedure,\nthere are implementation differences between fully-connected and convolutional layers. For convo-\nlutional layers, slight modifications are required to minimize the extra computational overhead that\nis introduced with the addition of decorrelating matrices for each network layer."}, {"title": "Reducing the Computational Burden", "content": "In order to decrease the computational overhead for the decorrelation process in the convolutional\nlayers, three modifications are used, following Dalm et al. (2024). The first measure addresses the\nhigh computational cost of decorrelating large feature maps by applying decorrelation on local image\npatches instead of the whole input. This enables the decorrelated input to be transformed via 1\u00d71\nconvolutional kernels for the forward pass.\nThe second modification replaces the expensive multiplication W(Rz) with the cheaper multi-\nplication (WR)z, via an initial condensation of matrices W and R into matrix _A = WR, to be\nonly later multiplied by the raw input z for the forward pass. This modification capitalizes on the\nlower dimensionality of W compared to that of z, as W does not have a batch dimension (Dalm\net al., 2024).\nThe third modification reduces the cost of computing the correlation matrix during the update\ncalculation by basing it on a random sample of the input. Dalm et al. (2024) found that sampling\n10% of the input samples is sufficient to learn the data's correlational structure with minimal loss\nof accuracy. To this end, in this study, an alternative implementation of downsampling was used, to\nensure sufficient samples for performance in our use case. A subset of input samples were randomly\nchosen to compute the decorrelation update. The sample size n was computed as\n$n = max (10, bD_{r}p^{-1} + 1)$\nrounded to the nearest integer, where Dr is the total dimensionality per input patch, p the number\nof image patches and b a scaling factor determining the degree of downsampling. The justification\nfor this sampling method is that downsampling should be less aggressive if the dimensionality of R\nis high relative to the number of patches, while always keeping a minimum of 10 samples."}, {"title": "Integrating Decorrelation into Soft Actor-Critic", "content": "Soft Actor-Critic (SAC) (Haarnoja et al., 2018b) is a state-of-the art deep RL algorithm known for\nits stable convergence properties and sample efficiency. It is differentiated by the combined use of\nthree ingredients: the separate architectures for policy and value function networks as an actor-critic\nmethod, the off-policy gradient updates via the reuse of data from a replay buffer, and the addition\nof maximum entropy learning in its objective function.\nThe SAC agent is composed of one network approximating the policy function and two networks\napproximating two soft Q-functions. Here, policy iteration is formulated via the evaluation of the\ncurrent policy's Q-function and the policy's off-policy gradient update.\nThe soft Q-functions are trained independently with help from two target soft Q-functions. The\npolicy is updated via minimization of the KL-divergence between the policy distribution and the\nminimum of the soft Q-functions. Alongside the expected return, the entropy is maximized to\nencourage exploration and stability by establishing a minimum constraint on the average entropy\nof the policy and through the gradient-based tuning of a temperature parameter a weighing the\nentropy against the reward maximization objective.\nFor the purposes of our research, a version of SAC for discrete action spaces was used, following\nthe approach of Christodoulou (2019). Discrete SAC differs from the original algorithm (Haarnoja\net al., 2018b) that has been designed for continuous action spaces."}, {"title": "Soft Q-networks", "content": "In discrete SAC, Q-networks receive the state as the only input, and output Q-values for each ac-\ntion separately. Each network is composed of 3 convolutional layers, whose consecutive output is"}, {"title": "Policy Network", "content": "Due to the discrete action space setting, the policy is modeled as a categorical distribution with the\npolicy network outputting the unnormalized log probabilities of actions available that parameterize\nthe action distribution. The action taken is determined by sampling from this distribution based on\nthe action probabilities. Normalized log probabilities are also produced after a softmax application\nto network output, to be used in policy update and Q-function computations.\nThe network has the same architecture as a Soft Q-network, yet has its own separate architecture\nand initialization with its own parameters. Note that with the incorporation of the decorrelation\nprocess, all these layers are modified to have an additional multiplication with the decorrelating\nmatrix for the decorrelation transformation before their own forward pass, hence after the application\nof an activation function in case of a previous layer existing. These decorrelation layers are differently\nimplemented depending on whether they decorrelate inputs for a convolutional or fully-connected\nlayer, as mentioned in Section 2.1."}, {"title": "Optimization of Discrete SAC via Decorrelated BP", "content": "As an off-policy algorithm, SAC starts by sampling random actions in the environment to first fill\nthe replay buffer with sufficient state transition data to be reused in the weight updates. When\nthe replay buffer has enough samples, learning starts. Each iteration of the algorithm involves\nan environment step, followed by a gradient step. The environment step is taken with the action\nsampled from the policy network's output distribution. At every gradient step, a batch of transition\ndata is sampled from the replay buffer to be used for the calculation of loss terms involved in\nthe SAC algorithm. After the separate updates of two soft Q-functions, the policy function, and\nthe temperature parameter a, finally the decorrelating matrices that serve as the weights for the\ndecorrelation transformation are updated for each layer of the decorrelated policy network. The\npseudocode is provided in Algorithm 1.\nEach soft Q-function is updated based on the mean squared error (MSE) loss between its Q-\nfunction output and the commonly used target Q-values for both networks, which uses the minimum\nof the two soft Q-targets in the calculation of the next Q-values as follows:\n$J_{Q(\u03b8)} = E_{(st,at)~D}  [(Q_{\u03b8_i}(S_t) - (r(st, at) + \u03b3 (\u03b1'(s') (min_{i=1,2}Q_{\u03b8'}(s') \u2013 \u03b1 log(\u03c0_\u03c6(a'| s')))))^2]$\nwhere D is the replay buffer, r rewards, \u03c0 policy, s' next states, a' next actions, and y the dis-\ncount factor on the next Q-values. Parameters 6 of a target Q-function are updated based on the\nexponential moving average of the parameters \u03b8 of the respective soft Q-function.\nThe policy network is updated based on the KL-divergence between the policy distribution and\nthe minimum of the soft Q-functions, scaled by the action probabilities, thus giving the policy loss:"}, {"title": "Empirical validation of DSAC", "content": "For empirical validation of DSAC, we utilize the Atari 100k benchmark that is commonly used when\ntesting sample-efficiency of RL algorithms. We follow the initial proposal of Kaiser et al. (2020), and\ndiscrete SAC evaluations by Christodoulou (2019), where agents are trained for 100k environment\nsteps, corresponding to 400k frames with frame skipping of 4. We focus on seven environments\nwith higher difficulty levels and 18 possible actions to get the full action space, also given existing"}, {"title": "Results", "content": ""}, {"title": "Training Time and Reward Comparison", "content": "Figure 1 compares the reward performance of DSAC against a SAC baseline across the wall-clock\ntimes during the training for a fixed number of steps. The algorithms plotted are based on the\nbest configurations that emerged for each algorithm per each environment as a result of the hyper-\nparameter search. The learning rates and batch sizes for these best configurations can be seen in\nAppendix D. DSAC completes training for the same number of steps in a shorter wall-clock time in\n5 out of 7 games tested, demonstrating efficiency benefits of the network-wide decorrelation. It also\nsignificantly outperforms the SAC baseline (p < 0.05, N = 10) in 2 of the games tested, showing an\n86% performance gain in Alien and 6% in Seaquest.\nAlien and Seaquest are also the games where DSAC provides a time advantage, training 44-57%\nfaster. Thus, large speed benefits as a result of decorrelation translate into performance benefits,\npotentially due to effective credit assignment. On the remaining games, DSAC and SAC baseline\nhave similar performance levels (p > 0.05, N = 10). Please refer to Appendix E for reward curves\nbased on training steps, and their comparison to the performance of other discrete SAC benchmarks."}, {"title": "Effect of Batch Size", "content": "When we look closely at the hyperparameters for these plotted algorithms based on best configura-\ntions, we see that only in Chopper Command DSAC has a higher batch size than the SAC baseline,\nexplaining the slowness of DSAC for that game. Frostbite is also the only case where SAC and\nDSAC have identical batch-sizes, which leads DSAC to have a slightly longer training time for this\ngame due to the additional computation for decorrelation updates. In all the games where DSAC\ntraining is faster than SAC, DSAC has a lower batch size than SAC. While this may indicate an\nimpact of the lower batch size on the success of DSAC, our experimentation shows that it is not the\nsole factor in its success (see Appendix G). With an increase in the batch size, DSAC indeed provides\nless time advantage compared to the lower batch size, yet still is able to reduce the wall-clock time\nagainst the SAC baseline when run with an identical batch size. Similarly, despite identical batch\nsizes, DSAC is still able to outperform SAC. These results show that while a lower batch size is\nbeneficial for reducing training time, the benefits of decorrelation do not entirely depend on it."}, {"title": "Learning Rate Comparison", "content": "As for learning rates, each game had its own best configuration. The lower learning rates provided\nthe top performers of the SAC baseline, with {3 \u00d710\u22125, 1 \u00d7 10\u22124} being equally chosen, thus making\nit hard to converge on one choice for all experiments. These were almost always accompanied by a\nbatch size of 256. In contrast, {3 \u00d7 10-4} was never chosen for the top configurations. For DSAC'S\nSAC learning rate, {1 \u00d7 10\u22124, 3 \u00d7 10-4} were common choices, with {3 \u00d7 10-4} always being\naccompanied by a batch size of 64 as the more common choice for batch size in highest ranking\nDSAC configurations. As for decorrelation learning rates, the choices in order of popularity were\n{1 \u00d7 10-4, 1 \u00d710\u22122, 1 \u00d7 10\u22123}. These show that a unique decorrelation learning rate might provide"}, {"title": "Impact on Decorrelation Loss", "content": "To further investigate the impact of decorrelation, an analysis of the decorrelation losses can be\nmade. Figure 2 displays the total decorrelation loss for the policy network, which was subject to\na hyperparameter search for its learning rate. Here the top performing configurations found in the\nhyperparameter search are plotted for the algorithms with the loss values in log scale. Among the\ncurves comparing DSAC and the SAC baseline, the most obvious and systematic difference emerges\nin the minimization of the decorrelation loss for the policy network, which is consistently kept at a\nminimum for DSAC throughout training, while it increases steadily for the SAC baseline towards\nthe final steps. These results show successful decorrelation in the DSAC policy networks. While the\npolicy network learns increasingly correlated features with BP, decorrelation via DBP helps mitigate\nthis effect by ensuring minimal correlation throughout training."}, {"title": "Impact on RL Loss", "content": "In Appendix F, other loss curves from the SAC algorithm can be observed. The increase in the\nSAC baseline's non-optimized decorrelation loss for the policy network seems to coincide with the\nconvergence of the actor loss in games like BankHeist and Seaquest, signaling the exploitation\nstage for the algorithm. The alpha loss for DSAC on the other hand reaches convergence faster\nthan the SAC baseline consistently. This shows that decorrelation helps in reaching the entropy\nmaximization objective faster, by encouraging exploration via minimization of the correlation in\nfeatures. Based on this, the minimal levels of the decorrelation loss can be seen as a signal of how\nnovel the visited states are. This way, decorrelation via DBP contributes to the SAC algorithm's\nentropy maximization goals."}, {"title": "Discussion", "content": "We have proposed DSAC as a novel approach to online network-wide decorrelation in deep RL,\nbased on the DBP algorithm that applies decorrelation across its neural network layer inputs before\ntheir forward pass, for effective representation learning and improved sample efficiency in RL. An\napplication of DBP to the discrete SAC algorithm has demonstrated faster training in wall-clock\ntime in the majority of the Atari games tested, which converted to performance benefits in a few\ngames against a BP-based discrete SAC baseline. DSAC showed successful minimization of the\ndecorrelation loss for the policy network where decorrelation was applied, whereas the SAC baseline\ndisplayed high correlation of features. Our results suggest that network-wide decorrelation indeed\nshows advantage in effective representation learning by removing correlations, thus improving sample\nefficiency by speeding up the deep RL training.\nDeep RL has been previously shown to learn features that are highly correlated, which cause slow\nlearning curves (Mavrin et al., 2019b). Mavrin et al. (2019a) observed that applying decorrelation\nin deep RL through regularization leads to richer representations with higher number of useful\nfeatures learned. By allowing the agent to better understand the effect of changing features on its\ndecisions, decorrelation has been proposed as an approach to achieve generalization by RL agents\nacross environments (Huang et al., 2023).\nHere we built on these findings through a fully-integrated decorrelation process to the training\nof the RL pipeline, which differs fundamentally from previous approaches. Next to the usual RL\nupdates, we apply separate updates to additional decorrelation matrices added to each layer of\na DNN, based on a decorrelation learning rule. Thus we present an approach to simultaneous\nonline learning of good representations and task control in a single phase process, without the\nneed for regularization, pretraining, use of additional network architectures or a focus on a single\nlayer's latents. Our method of decorrelation provides an advantage by being applicable to any\nkind of existing algorithm or DNN architecture, with the possibility for an application in any type\nof network layer, demonstrated here with convolutional and fully-connected layers. Furthermore,\nthrough network-wide decorrelation, correlations can be kept under control across all layers.\nWhile our results are promising, we do recognize that most, though not quite all, of the per-\nformance gains of decorrelation via DBP in SAC is displayed when a smaller batch size is being\nused relative to the baseline. It should be explored further why decorrelation allows for smaller\nbatch sizes and how robust this finding is in other contexts. Potentially, the reduction of redundant\ninformation could help the algorithm learn from fewer samples.\nFurther investigation is needed into the extension of our findings to other environments, archi-\ntectures, and RL algorithms. Results were demonstrated on a subset of most difficult games in the\nAtari suite where human-level performance is not reachable with DQN (Mnih et al., 2015). Moving\naway from the most challenging task settings could potentially provide further performance benefits\nto DSAC. Whereas SAC is an ideal choice as a complex state-of-the-art deep RL algorithm, using\nsimpler algorithms may ease the analysis on the impact of decorrelation directly, while also strength-\nening the generalizability of results. Furthermore, benefits of decorrelation may be more impactful\nfor control tasks on continuous action space settings, as credit assignment is more challenging in\nlarger action spaces, making effective representation learning more crucial.\nWhen demonstrating the advantages of using decorrelation in deep RL through DBP, different\nhyperparameter settings were used per game and per algorithm. Whereas not sticking to one con-\nfiguration of hyperparameters may signal an inconsistency on the impact of decorrelation, the best\nhyperparameter configurations from the search for both the SAC baseline and DSAC were used\nto compare their performances in each game. The best conditions were thus ensured for not only\nDSAC but also the baseline to justify any improvements made over it, so that it is not merely"}]}