{"title": "From Exploration to Revelation: Detecting Dark Patterns in Mobile Apps", "authors": ["Jieshan Chen", "Zhen Wang", "Jiamou Sun", "Wenbo Zou", "Zhenchang Xing", "Qinghua Lu", "Qing Huang", "Xiwei Xu"], "abstract": "Mobile apps are essential in daily life, yet they often employ dark patterns, such as visual tricks to highlight certain options or linguistic tactics to nag users into making purchases, to manipulate user behavior. Current research mainly uses manual methods to detect dark patterns, a process that is time-consuming and struggles to keep pace with continually updating and emerging apps. While some studies targeted at automated detection, they are constrained to static patterns and still necessitate manual app exploration. To bridge these gaps, we present AppRay, an innovative system that seamlessly blends task-oriented app exploration with automated dark pattern detection, reducing manual efforts. Our approach consists of two steps: First, we harness the commonsense knowledge of large language models for targeted app exploration, supplemented by traditional random exploration to capture a broader range of UI states. Second, we developed a static and dynamic dark pattern detector powered by a contrastive learning-based multi-label classifier and a rule-based refiner to perform detection. We contributed two datasets, AppRay-Dark and AppRay-Light, with 2,185 unique deceptive patterns (including 149 dynamic instances) across 18 types from 876 UIs and 871 benign UIs. These datasets cover both static and dynamic dark patterns while preserving UI relationships. Experimental results confirm that AppRay can efficiently explore the app and identify a wide range of dark patterns with great performance.", "sections": [{"title": "I. INTRODUCTION", "content": "People nowadays can conduct many tasks, including shop- ping, reading, chatting, using their own laptops or mobile apps, between which user interfaces play as an important proxy to connect human and these functionalities. However, many apps unconsciously or intentionally insert some psychological tricks into these user interfaces and try to manipulate end- users, either to seduce them stay longer at their services [1], [2], or deceive them to perform some actions that may not of their best interest [3]-[5]. For instance, in the context of social media, the \u201cnever-ending auto-play\u201d function for videos is automatically enabled to capture end-users' attention and en- courage them to continue browsing the app unconsciously [2]. As a result, people feel regretful when they reflected [1]. Addi- tionally, apps often integrate native advertisements seamlessly with other content, adopting the same visual styles, which can make it difficult for users to distinguish between regular content and advertisements. People may mistakenly click these advertisements and surprisingly find they are ads. These tricks on user interfaces are termed as deceptive patterns (as known as dark patterns)\u00b9 [3], [6].\nWhile people may not be familiar with this term, deceptive patterns are everywhere in our life. Mathur et. al found that 11.1% of 11K shopping websites have dark patterns [7]. Di et al. revealed that 95% of 240 popular mobile apps averagely contain at least seven types of dark patterns [5]. Moreover, researchers are now actively working on recognising and sum- marising dark patterns from different aspects, such as language differences [8], Home IoT devices [9], cookie banners [4], [10] and video streaming platforms [11]. The prevalence of dark patterns become severe issues but auditors and app markets now mainly rely on manual efforts to identify them, which is quite time-consuming and not generalisable.\nSeveral recent studies have started working on automat- ing the identification of dark patterns using advanced tech- niques [12], [13]. Mansur et al. [12] introduced AidUI, a tool capable of detecting 10 types of dark patterns across mobile UIs and websites, while Chen et al. [13] presented UIGuard, focusing on 17 dark pattern types within mobile UIs. Nonetheless, their focus remains limited to individual user interfaces, neglecting comprehensive application-wide scrutiny and the relationship among UIs. Consequently, in- dividuals are forced to invest substantial time in manual app exploration, particularly given the intricate nature of app user interfaces.Moreover, as apps keep updating, this manual process becomes more cumbersome and time-consuming. In addition, their singular UI emphasis restricts their capability to identify only static dark patterns (single page related deceptive patterns), leaving dynamic manifestations (interaction related patterns) unattended."}, {"title": "II. BACKGROUND", "content": "Many works have been done to summarise the taxonomy for dark patterns in mobile platform. We leveraged the tax- onomy from Chen et al. [13] and Mansur et al. [12], which integrate and summarise the existing taxonomies [3], [5], [14] into a unified one. We further merged and reorganised their taxonomies into our used one. As seen in Figure 1, the tax- onomy is categorised into five strategies [3], namely nagging, obstruction, sneaking, and interface inference. Figure 2 shows some examples.\nNagging disrupt user tasks by suddenly displaying irrele- vant windows. For example Examples include Pop-Up Ads appear unexpectedly to promote advertisements, while Pop- Up to Rate and Pop Up to Upgrade ask users to rate the app or to upgrade to the premium version.\nObstruction seeks to unnecessarily complicate tasks. Roach Motel facilitates effortless opt-in but presents challenges for opt-out (e.g., easy to subscribe but hard to cancel it). Intermediate Currency introduces virtual currency, blurring the distinction between virtual and actual money. Price Comparison Prevention creates obstacles when attempting to compare product prices across different applications or websites.\nSneaking hides, disguises or delays relevant information to the current user task. Hidden Costs reveals significant expenses such as shipping fees belatedly. Forced Continuity automatically prolongs subscriptions without noticing the users. Bait and Switch entices users to do something but give unrelated outcome (e.g., you click a button but get an ads). Sneak into Basket quietly adds additional items to the shopping cart.\nForced Action forces users to perform some actions to get rewards, unlock features or achieve some tasks. So- cial Pyramid allures users to share something with their friends to get rewards or unlock features. Privacy Zuckering coerces users into divulging privacy-related information. Gamification asks users to perform repetitive actions in exchange for rewards. General Types includes other tactics like countdowns on ads, watching ads to unlock features or earn rewards, and paying to eliminate ads.\nInterface Interference manipulates the interface to privi- lege some options over others. Preselection enables options like notifications by default or do not provide an opt-out option. Hidden Information makes information relevant to users not intermediately readable or accessible. Aesthetic Manipulation alters the user interface in a manner that prioritizes form over function, which includes five sub types. \"Toying with Emotion\" entails influencing user sentiment to either encourage or discourage certain actions (e.g., activity message, countdown offer). \"False Hierarchy\" establishes misleading visual prominence. \u201cDisguised Ads\" blends ad- vertisements seamlessly with other content by using the same visual effects. \"Tricked Questions\" utilize linguistic constructs, such as double negations, to obfuscate queries. \"General Types\" are other tricks, including small close buttons on ads and moving ads button."}, {"title": "III. RELATED WORK", "content": "Existing research focuses on different aspects of dark patterns, such as dark pattern taxonomies identification and summarization [3], [5]\u2013[7], [11], [14], user perception and impacts [1], [15]\u2013[17], and dark pattern remediation [12], [13], [18], [19].\nDark pattern taxonomies identification and summarization focuses on using manual or semi-manual methods to explore the apps or websites, getting their UIs and then identifying and summarising the contained dark patterns [3], [5]\u2013[11], [14]. In 2010, Harry Brignull [6] defined dark pattern as \"a user interface that has been carefully crafted to trick users into doing things, such as buying insurance with their purchase or signing up for recurring bills.\" He established the https://www.darkpatterns.org/ website, which collects various examples from web and mobile apps and categorises them into different types such as confirmation shaming, which triggers uncomfortable emotion to impact user decision. He also provides a Hall of Shame to continuously report the dark patterns used by companies around the world, and created a Twitter account [20] for people to report dark pattern instances. In the academic fields, Gray et al. [3] later refined and extend his taxonomy in terms of five strategies, namely Nagging, Interface Inference, Forced Action, Obstruction and Sneaking. Since then, more researchers put more efforts on identifying dark patterns, summarising and enriching the taxonomies from different aspects, such as game [21], shopping website [7] and mobile apps [5]. Built upon these research methodology and taxonomy, we propose to leverage LLM's capabilities to simulate human exploration and automatically explore the apps to gather the UI in the apps.\nAnother stream of dark pattern research focuses on under- standing user perception and the impacts of the dark patterns on the end-users [1], [15]\u2013[17]. For example, Cho et al. [1] analysed the feature-level usage logs of four social media apps (Instagram, Facebook, YouTube, KakaoTalk) from 29 Android users, and investigated when and why people regret. They found that among 4,069 sampled sessions, 26.5% were consid- ered fully or partially regretful as regretful. Sergeeva el al. [16] instead dives into how users think of the persuasive tactics used in permission-based advertising emails. They identified four reactance-triggering factors that caused the negative attitudes towards these advertising emails, and found that participants now had doubts about the effectiveness of these tactics and would even raise some questions on the advertised goods in the emails.\nRecognising the severeness of these dark patterns and impacts, researchers start taking actions on remediating the impacts [12], [13], [18], [19]. One way is to directly hide the dark patterns in the app to remove the impacts. Kollnig et al. [18] introduced GreaseDroid for end-users to apply patches from experts to their app, and then use the modified clean app. The patches are manually created by developers, which can modify the code files of the apps to remove the dark patterns. However, this method requires great expertise, hard to generalise, may have impacts on the app functionality or introduces potential privacy risks. Datta et al. [19] later pre- sented GreaseVision that democratise the creation of patches by allowing end-users to provide the screenshots of areas to be modified. Apart from these kinds of method, recent work started working automated detection of dark patterns to warm the developers, end-users of these tricks. Mansur et al. [12] proposed AidUI to automatically detect dark patterns in the current UI/website page using computer vision and template matching techniques. Concurrently, Chen et al. [13] developed UIGuard, which focuses on the mobile UIs, to identify 17 types of static dark patterns. Our work follows the second stream, aiming at automated identifying dark patterns to warn the end-users. Different from existing techniques, we not only focus on a single UI, but also take the interactions into account, which enable the detection on dynamic tricks like sneak into the basket."}, {"title": "IV. METHODOLOGY", "content": "As seen in Figure 3, our system comprises an app explo- ration module and a dark pattern detection module."}, {"title": "A. App Exploration", "content": "The app exploration has two phases. We conduct task- oriented exploration based on LLM, following Di Geronimo et al's methodology [5] to perform tasks that are prone to have potential dark patterns. We then employ existing app exploration tool to collect as many UI status as possible.\n1) Task-Oriented App Exploration: While random explo- ration can capture many UI states, it overlooks UI semantics, often getting stuck on certain pages and failing to perform meaningful actions. Unfortunately, certain dark patterns only come to light during logical, sequential exploration of the app. Failing to do any of the steps can not lead to the final page that contains dark patterns. Furthermore, some routine tasks can be hotspots for some dark patterns. Simply navigating to the notification settings page might unveil the \u201cPreselection\" tactic, where choices are made on behalf of the user without clear consent. Thus, task-oriented exploration, which mirrors human interaction, is essential. To this end, we harness the commonsense knowledge of large language models, i.e. GPT- 4 [22], for targeted app exploration.\nTo this end, we first define a set of common tasks and a feature-specific task. Then, given an app under test and the task, we (1) obtain and process the current UI's view hierarchy into text; (2) feed this to the LLM to get the next action; (3) perform the action on behalf of LLM and repeat Steps 1-3 until reaching the max step, or LLMs believe the task is finished.\nTask Definitions. As seen in Table I, we consider general tasks and feature-based tasks. The general tasks are common tasks that are prone to contain dark patterns [5]. For example, most apps should have notification setting page to control the way the app inform users. Therefore, by setting the task \"go to the setting page and turn off all notification\u201d, we can check whether these notifications are enabled by default. Feature- based tasks focus on app-specific functionalities, initially targeting shopping features to reveal potential dark patterns. The primary goal of task-oriented exploration is to delve into more UI states coherently, complementing the limitations of random exploration. Completing these tasks is secondary and inconsequential to the exploration process.\nUI Information Extraction. To feed data to GPT-4, we first convert the UI information into textual format. Each user interface is represented as a tree structure with leaf elements like Buttons for interaction, TextViews for conveying textual information, ImageViews for displaying images, and layout elements organising the leaf elements into a structure way like LinearLayout for placing the elements in a linear order. Each element has attributes such as text, clickable, and classname, indicating its properties. We use the Android Debugging Bridge (ADB) to obtain the current UI's view hierarchy, extract key information, and feed it into the LLM to understand the UI and make decisions. In detail, we extract the following information:\nClassname of UI elements indicate their functionalities, such as a Button that allows user interaction.\nResource ID is used as a unique identifier for UI elements, indicating their semantic meaning.\nText Content: Text and Content-description contains the text on elements or the accessibility label for image-based buttons.\nAction-Related Attributes including clickable, scrol- lable, checked, indicates possible actions and the element's current status. For example, a clickable and checked CheckBox element means it can be clicked and is currently checked.\nBounds specifies the element position on the UI.\nPrompt Engineering and Action Space. We adopted the in-context learning with few shot examples to optimise the prompt and facilitate the task-oriented app exploration. We provide overall instructions to GPT, including the expected output, and one example for each action to clarify the task 2. We consider five common actions, namely tap, scroll, type, back and stop. Stop is specially designed when LLM finds the task is finished.\nOverall Interaction between Device and LLM. For each round, we first obtain the UI information and convert it into text format. We then fill the prompt template with the current task, history actions, UI information, and ask GPT for the next actions. Once we obtain the actions, we extract the action point using the bounds of the target element, and perform the action using ADB. This process is iterative until LLM provides a stop action or the number of steps reach the predefined threshold.\n2) Random Exploration: We use the state-of-the-art au- tomated app exploration tool FastBot2 [23], developed by ByteDance, to capture as many UI states as possible. FastBot2 employs a probabilistic model and a reinforcement learning model to collaboratively determine the best strategy for un- covering unique UI states. It identifies the optimal action based on the current UI information, available UI widgets, and historical actions, leading to an effective exploration strategy."}, {"title": "B. Dark Pattern Detection", "content": "Exiting work mainly focus on single UI detection, which are prone to have false negatives and can not deal with dark patterns that related to several UIs. As we incorporate the exploration in our work, our method has the capability to deal with dynamic patterns. Due to the great amount of UIs collected in apps, it is cost-extensive to use LLMs for assessing all UIs one by one and it is also not necessary.\nGiven a UI under test, we initiate the detection process by extracting properties of the UI elements, grouping them into semantically distinct clusters using heuristic rules. Sub- sequently, we deploy our proposed component-based dark pattern detector on each identified element or cluster.\n1) UI Information Extraction and Grouping: We directly leveraged UIGuard's [13] method for extracting the UI in- formation, including element bounds, types, text and status. We also leveraged their method in grouping checkbox with associated text elements. Based on this, we proposed simple rules to group elements that are semantically relevant (e.g., the elements on a confirmation pop-up should be grouped into a semantic cluster).\n2) Component-Based Dark Pattern Detector: After we ob- tain the component need to be tested, we take the component, whole UIs and the UI information as input, and perform our component-based dark pattern detector. To support the detection of both static and dynamic deceptive patterns, our detector comprises two parts: (a) a contrastive learning-based multi-label classifier and (b) a rule-based refiner. Given that each UI page includes text content, UI element attributes, and UI structure, the classifier employs a Siamese architecture with two encoders: (1) BERT-based text encoder and (2) ResNet50-based UI image encoder. As illustrated in Figure 3, the main structure of our dark pattern detector enables two shared-weight UI encoders and one text encoder to encode the text contents in the UI and dark patterns. The aggregator combines the output embeddings from the UI encoders and text encoders using concatenation, which is then passed into a fully connected classifier to shortlist potential dark patterns. The rule-based refiner provides the final output by applying predefined knowledge and contexts. Because of the scarcity and imbalance of dark patterns across classes, with instances ranging from just a few to several hundred, it is essential to address these disparities to improve detection performance. Classes with fewer instances of dark patterns undergo data augmentation operations to enhance their representation in the training set.\n3) Data Augmentation: Data augmentation includes var- ious image transformations such as rotation, flipping, and color adjustment [24], as well as synthetic text generation methods like paraphrasing, synonym replacement, and back-translation [25]. By augmenting the data, we not only increase the number of training samples for under-represented classes but also introduce variability that helps the classifier generalize better to unseen examples. This approach ensures that the model learns robust features and relationships for each type of dark pattern, ultimately leading to more accurate and reliable detection. As the Figure 3 shows, a sequences (gray, 90 degree rotation and flip, etc) of complex transformation functions $T = [T_1, ..., T_n]$ applied on the UI images or data pattern im- ages $I' = T_n(T_{n-1}(. . . T_1(I))$. The textual data augmentation include synonym replacement, random insertion, and random swap. This data augmentation process can be repeated with various combinations of transformations to create a large set of augmented text and images from the original inputs.\n4) Contrastive Learning and Loss Functions: Our dataset includes 16 classes of deceptive patterns, where presenting various ambiguities that can confuse the classifier. Contrastive learning is a technique used to learn representations by comparing and contrasting samples in a high-dimensional space [26]. This approach is particularly effective in unsuper- vised and semi-supervised learning scenarios, where labeled data is scarce. The main idea is to bring similar samples closer together and push dissimilar samples further apart in the feature space. For instance, the dark patterns \u201cDisguised AD\" and \"Nagging", "Disguised AD\" fakes as a normal feature, appearing similar to surrounding components, while \"Nagging\" repeatedly pops up, obstructing the user's normal experience and urging them to agree to something. Without contrastive learning, the model would struggle to distinguish between these two patterns. We will provide detailed comparisons how contrastive learning helps address this issue in Section VI-A2.\nContrastive Loss": "Given $x_i$ and $x_j$ are two samples, and $f_w(x_i)$ and $f_w(x_j)$ are their corresponding representations in the latent space, the contrastive distance can be expressed as:\n$L_D = -log\\frac{exp(D(x_i, x_j) / \\tau)}{\\sum_{k=1}^{N} exp(D(x_i, x_k)/\\tau)}$ (1)\nwhere $D(x_i, x_j) = ||f_w(x_i), f_w(x_j)||^2$ is the similarity between $f_w(x_i)$ and $f_w(x_j)$. $|| \\cdot ||$ is a similarity metrics (e.g., Cosine similarity or Euclidean distance), $\\tau$ is a temperature parameter and $N$ is the number of negative samples. It encourages the representations of positive pairs to be similar and those of negative pairs to be dissimilar. The contrastive loss can be expressed as:\n$L_{contrastive} = (1 - y)L_D^+ + yL_D^-$ (2)\nwhere the term y specifies whether the two given data inputs ($x_i$ and $x_j$) are similar (y = 1) or dissimilar (y = 0). The $L_D^+$ term stands for the contrastive distance $L_D$ when the data inputs are from the different classes. The $L_D^-$ term stands for the contrastive distance $L_D$ when the data inputs are from the same classes.\nCross-entropy loss is used to measures the performance of the final classifier and can be defined as:\n$L_{crossentropy} = -\\sum_{i=1}^C t_i log(\\hat{t_i})$ (3)\nClass Weight and Overall Loss: the total loss of our multi- label classifier is:\n$L = L_{crossentropy} + L_{contrastive}$ (4)\nThe imbalanced classes in the model training can lead to biased models that perform well on the majority classes but poorly on minority classes. Class-weights is an essential solution in this case. It assigns higher weights to the minority classes and less weights to majority classes so that the model will pay more attention to these minority classes during training. To achieve this, the loss function has to be adjusted the contribution of each class to the overall loss based on their class-weights:\n$L = \\sum_{c=1}^{||C||} w_cL$ (5)\nNegative Samples: Negative sampling is crucial for con- trastive learning. It helps the model learn to differentiate between distinct classes by providing examples that are not similar to the anchor sample. By learning to correctly iden- tify these dissimilar examples, the model develops a clearer understanding of the boundaries between different classes in the embedding space. Negative sampling strategies in con- trastive learning should be task oriented by addressing various challenges. In the dark pattern detection problem, we applied several strategies:\n1) Random negative sampling. The negative samples are randomly chosen from the dataset of the other classes, which is simple strategy to warm up the model training.\n2) Hard negative sampling [27]. Hard negatives are selected based on their similarity to the anchor samples.\n$D^+(x_i, x_j) - D^-(x_i, x_k) > \\xi$ (6)\nwhere $\\xi$ is a hyperparameter to define the hardness of neg- ative samples relative to positive samples. These negative samples encourages the model to learn better distinctions between the data from similar classes.\n3) Balanced negative sampling. The number of negative sam- ples is balanced with positive samples, preventing the model from being biased towards one type of sample.\n5) Model Training: Our multi-label classifier is a Siamese model that consists of two pipelines: an image embedding pipeline and a text embedding pipeline, followed by a final MLP classifier with a sigmoid activation function for the final predictions. Each pipeline processes its respective inputs to generate meaningful embeddings. These embeddings are then combined and fed into the final classifier. Each pipeline is trained independently, and once trained, their weights are frozen. The final classifier is subsequently trained using the outputs from the frozen pipelines. The details of the training process are as follows:\n1) The input training set comprises images features and text features (extracted from the images using Optical Charac- ter Recognition). To address class imbalances and enrich minority classes, the training set will go through data augmentation process to generate positive and negative samples.\n2) The image inputs are encoded using a ResNet50-based image embedding pipeline. There are two types of image inputs: dark pattern images and their corresponding full UI images. Both types of images pass through the shared- weight image embedding pipeline. To train the pipeline weights, a temporary fully connected classifier is attached to classify the image features. Similarly, a BERT-based text embedding pipeline processes the text features, with a temporary fully connected classifier attached to classify the text features alone.\n3) Once the image and text embedding pipelines are trained, they will be connected to the final MLP classifier. The weights of these pipelines will be frozen, and their output embeddings will be concatenated to serve as inputs for training the final MLP classifier.\n4) The entire training process is optimized using the Adam optimizer, with the loss function L in equation 5\n6) Rule-Based Refiner: The rule-based refiner operates by applying a set of predefined heuristics and logical rules de- signed to filter out patterns that do not meet specific criteria. These rules are based on domain knowledge and empirical observations, allowing the refiner to assess the context and content of each flagged dark pattern more thoroughly. For example, the refiner might check for certain textual cues, layout structures, or interaction patterns that are indicative of certain dark patterns. By leveraging these rules, the refiner enhances the overall precision and reliability of the detection system. In addition to single-page analysis from the model- based classifier, the rule-based refiner extends its capabilities to perform multi-page dynamic dark pattern detection. This involves analysing the flow of dark patterns across multiple pages. The refiner tracks the interactions of static dark patterns across multiple pages to identify dynamic dark patterns. It evaluates sequences of page transitions, and cumulative inter- face changes to detect patterns such as bait and switch, roach motel or nagging that span multiple steps."}, {"title": "V. RESEARCH QUESTIONS AND DATASETS", "content": "In this section, we evaluate our proposed system by inves- tigating the following research questions (RQs):\nRQ1: How effective is each module (i.e., app exploration and detector modules) in our proposed system?\nRQ2: How effective is the proposed method in detecting dark patterns in UIs compared to existing techniques?\nRQ3: How often does AppRay detect false positives in UIs that do not contain dark patterns?\nRQ4: To what extend, can AppRay assist human in manual exploration and identification given an app under test?"}, {"title": "A. Research Questions", "content": "As mentioned in Section I, existing datasets fall short of providing a dataset that covers both static and dynamic dark patterns with localisation labels. Therefore, in our work, we leverage the data collected through our app exploration module to fill the gap.\n1) App Collection: We collected 350 trending apps from Google Play Store across 33 app categories, and crawled their metadata including ratings, downloads numbers, app description, app category, app name, app package name. We then use app package name to obtain their latest app package from third-party websites such as AppCombo and AppMirror. Meantime, we saved the app version and app files for future replication. After that, we manually installed and tested each app to check if they are usable in emulator. We excluded game apps that leverage game engine like unity. We stopped until we obtained 105 usable apps. We randomly sampled 1 app from 5 different categories as a withhold subset for RQ4, and the rest 100 apps are used in RQ1-RQ3. We notice that most apps require email or phone number verification to register accounts, which are currently not supported by our system and are out of our scope. Therefore, for each app, we first register an account and save the account information for further usage. Such information can be easily provided by the app provider.\n2) UI Collection and Annotation: We ran our app explo- ration module on the 100 collected apps and obtained 45,292 UIs. After removing duplicates, we were left with 8,408 unique UIs. To maintain the sequence of explored UIs during annotation, we included some duplicate UIs, resulting in a total of 19,722 UIs that need to be annotated. Given the large scale of data, we followed the strategies of Di et. al [5] to only annotate UIs that contain unique dark pattern instances, i.e., if the same instance appear before, we would not annotate it. AppRay-Dark: To conduct annotation, two of the authors first discussed and went through the taxonomy and exam- ples [5], [12], [13] to get a commonsense of the definition of deceptive patterns. They then annotated these 19,722 UIs separately by adding the location and types of the identi- fied instances using the open-source tool LabelImg [28]. We modified LabelImg to show the action on the UI to assist annotation. For dynamic dark patterns, we added a linker (i.e. a digit) to indicate the connection between pages. After that, we merged the annotations and went through the discrepancies. If the consensus can not be made, we grouped them and involved a third author to have a discussion on that. As a result, we obtained 2,185 unique deceptive pattern instances of 18 types from 876 UIs across 97 apps. Among these instances, 149 of them are related to multiple UIs (i.e. dynamic dark patterns) from 48 apps. We denoted this dataset as AppRay-Dark. This dataset will be used to answer RQ1 and RQ2. We reported the distribution of each deceptive pattern types in Table III.\nAppRay-Light: Apart from that, we randomly sampled five consecutive sequences of UIs (length=3) that do not have annotated dark pattern instances from each app. The aim of this dataset is to assess the performance of our tool in reporting false positives while preserving the sequential information. Two of the authors went through these samples, and annotate whether they contain dark patterns or not. In total, we sampled 1,469 UIs, and ended with 871 benign UIs. We denoted this dataset as AppRay-Light. This dataset will be used to answer RQ3.\nGiven that our detector requires training, we splitted the data into five folds based on the app level, i.e., the instances from the same app will be allocated to a same split/fold. We perform 5-fold cross-validation in all the experiments."}, {"title": "B. Datasets", "content": "This section aims to measure the contribution and effec- tiveness of each module in our AppRay. We consider the following sub-questions:\nRQ1.1 App Exploration Module: How effective is the proposed technique in collecting UIs?\nRQ1.2 Dark Pattern Detector: How is the performance of AppRay's detection part? We conducted an ablation study to examine the contributions of each sub-module.\n1) RQ1.1: Effectiveness of App Exploration Module: This section assessed the performance of our app exploration module introduced in Section IV-A. We considered two sub-questions:\nRQ1.1.1: How effective is the LLM-based app navigator in completing the tasks?\nRQ1.1.2: How effective is the proposed technique in col- lecting UIs from apps?\nSetup: We used the 100 apps collected in Section V-B1 in this RQ. For RQ1.1.1, We ran all tasks in Table I in these apps using the LLM task-oriented app exploration module. We adopted Task Completion Rate to evaluate whether our ap- proach can successfully finish a task. We manually examined each task to see if it is completed or not. Note that whether the task is completed or not is not a key factor in our proposed system. The key goal here is to explore more UI status by logically using the app. For RQ1.1.2, We further ran Fastbot on these apps to collect more UIs. Two ablations are considered in this RQ, i.e., Fastbot only and LLM only. We adopt number of unique UI status and UI activity as the metrics.\nResults for RQ1.1.1: Among 100 apps, we in total ran 556 tasks, with 11.5 steps on average for each task. The task completion rate is 75.14%, which shows that our approach has certain capabilities to perform dark pattern sensitive tasks. As seen in Table I, the detailed completion rate for T1, T2, T3, T4, T5 and T6 are 56.41%, 78.21%, 82.83%, 84.0%, 83.33%, 30.0% and 58.11% respectively. We observed that our tool has the capabilities to perform tasks in a logical way. For some tasks, it can immediately figure out the relevant UI elements to interact 3. Apart from these straight-forward interaction traces, for apps that have different or more complex interaction ways, our tool can also locate the target page via a trial-and-error strategy. This behaviour is similar to human when users are not familiar with the apps. For example, in Figure 4, our tool first goes to the setting page and fail to find the notification setting. Then it goes back, and navigate to other tabs, and finally find the notification setting under the Inbox tab. The main failure reasons are due to missing information in view hierarchy [29], email/phone verification code requirement. Our tool also identified a usability issue during the exploration."}, {"title": "VI. EXPERIMENTS", "content": "More details on successful and failure reasons can be seen in supplementary materials.\nResults for RQ1.1.2: Among these 100 apps", "RQ1.2": "Effectiveness of Detector Module: To understand the effectiveness of each sub-module in our component-based detector (Section IV-B)", "baselines.\nSetup": "We utilized the AppRay"}]}