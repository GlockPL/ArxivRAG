{"title": "Teasing Apart Architecture and Initial Weights as Sources of Inductive Bias in Neural Networks", "authors": ["Gianluca Bencomo", "Max Gupta", "Ioana Marinescu", "R. Thomas McCoy", "Thomas L. Griffiths"], "abstract": "Artificial neural networks can acquire many aspects of human knowledge from data, making them promising as models of human learning. But what those networks can learn depends upon their inductive biases - the factors other than the data that influence the solutions they discover - and the inductive biases of neural networks remain poorly understood, limiting our ability to draw conclusions about human learning from the performance of these systems. Cognitive scientists and ma-chine learning researchers often focus on the architecture of a neural network as a source of inductive bias. In this paper we explore the impact of another source of inductive bias \u2013 the initial weights of the network - using meta-learning as a tool for finding initial weights that are adapted for specific prob-lems. We evaluate four widely-used architectures - MLPs, CNNs, LSTMs, and Transformers \u2013 by meta-training 430 dif-ferent models across three tasks requiring different biases and forms of generalization. We find that meta-learning can sub-stantially reduce or entirely eliminate performance differences across architectures and data representations, suggesting that these factors may be less important as sources of inductive bias than is typically assumed. When differences are present, architectures and data representations that perform well with-out meta-learning tend to meta-train more effectively. More-over, all architectures generalize poorly on problems that are far from their meta-training experience, underscoring the need for stronger inductive biases for robust generalization.", "sections": [{"title": "Introduction", "content": "Artificial neural networks have been used to explain how as-pects of human knowledge that have been claimed to depend upon an extensive degree of innateness \u2013 such as elements of language - might be learned from data by systems that do not have strong built-in assumptions (e.g., Rumelhart & McClel-land, 1986). These networks offer a new perspective on cen-tral questions in cognitive science, such as what information we need to assume is innate to human learners (Elman et al., 1996). In machine learning, a parallel set of questions focuses on the inductive biases of neural networks - defined as those factors other than the data that influence the solutions that they find (Mitchell, 1997). The convergence of these litera-tures offers an opportunity to explore different ways in which innate knowledge might be implicitly expressed in artificial neural networks.\nDifferent neural network architectures display different in-ductive biases. For instance, one clear signature of induc-tive bias is the amount of data needed to learn a task, and convolutional neural networks can learn image classification tasks from less data than multi-layer perceptrons (Chen et al., 2021). In addition to network architecture, however, recent work has highlighted the importance of a network's initial weights as a source of inductive bias (Finn, Abbeel, & Levine, 2017). Specifically, techniques based on meta-learning can optimize the initial weights of a neural network (leaving the architecture unchanged) in ways that enable the network to learn new tasks from far less data than it would re-quire using standard, randomly-selected initial weights. For instance, a network with meta-learned initial weights can learn new linguistic rules from just 100 examples, compared to the roughly 20,000 examples needed by the same archi-tecture with non-meta-learned initial weights (McCoy, Grant, Smolensky, Griffiths, & Linzen, 2020). Such meta-learning results show that a given neural network architecture can re-alize very different inductive biases thanks to the flexibility afforded by the initial weights.\nHere we consider this flexibility from the opposite direc-tion: can a given inductive bias be realized equally well in very different network architectures? This question di-rectly engages with the issue of whether architecture or initial weights provide a better focus for understanding the innate constraints on learning implicitly instantiated in a neural net-work. Prior work using meta-learning typically makes com-parisons within a fixed architecture, comparing a version of that architecture with meta-learned initial weights to a version with randomly-selected initial weights. These comparisons make it clear that the initial weights afford a substantial de-gree of flexibility, but they leave open the question of whether that flexibility is extensive enough to override the influence of architecture such that a given inductive bias could be realized equally well in different architectures.\nTo address this, we explore several inductive biases, inves-tigating how compatible each inductive bias is with differ-ent types of network architectures and data representations. We consider four widely-used, general-purpose neural archi-tectures-multilayer perceptrons (MLPs; Rosenblatt, 1962), convolutional neural networks (CNNs; LeCun, Bottou, Ben-gio, & Haffner, 1998), long short-term memory networks (LSTMs; Hochreiter & Schmidhuber, 1997), and Transform-ers (Vaswani, 2017)\u2014with variations in depth and width, meta-training a total of 430 models. To establish baselines where differences across architectures and data representa-tions should be more pronounced-free from task-specific bi-"}, {"title": "Background", "content": "Inductive biases-the assumptions that guide learning-can manifest through the choice of model architecture, data repre-sentation, error metric, and training algorithm (Baxter, 2000). In this work, we investigate the extent to which model ar-chitecture and data representation influence performance out-comes after optimizing the initial weights and learning rate through meta-learning. This section introduces the kinds of biases inherent to the neural architectures we explore and ad-dresses how meta-learning distills task-specific knowledge of the learning problem into the training algorithm."}, {"title": "Inductive Biases across Neural Architectures", "content": "Multi-Layer Perceptrons (MLPs) MLPs (Rosenblatt, 1962) can approximate any function given sufficient depth and width (Hornik, Stinchcombe, & White, 1989) but make no explicit assumption about the structure of the input data beyond static size. The lack of built-in equivariances make them highly sensitive to nearly all spatial and temporal vari-ations. All-to-all connections between layers imply global feature mixing and deeper layers can capture progressively more abstract representations.\nConvolutional Neural Networks (CNNs) CNNs (LeCun et al., 1998) were designed with an explicit bias towards grid-structured data such as images. Convolutions with shared weights prioritize spatially local relationships and ensure translation equivariance. Pooling layers provide partial ro-bustness to variations in scale, though CNNs generally lack inherent rotation or scale equivariances. CNNs preserve spa-tial order, making them sensitive to input permutations. Like MLPs, they build hierarchical features, with deeper layers capturing abstract patterns composed of simpler ones. These representations often resemble those in the mammalian visual cortex (Yamins & DiCarlo, 2016).\nLong Short-Term Memory Networks (LSTMs) LSTMs (Hochreiter & Schmidhuber, 1997) were designed to cap-ture both long- and short-term dependencies in sequences by maintaining an internal state that tracks temporal dynamics. Input, forget, and output gates regulate the flow of informa-tion, enabling the model to selectively retain or discard data. LSTMs rely on a memory structure with a bottleneck defined by the size of the hidden and cell states. They assume order matters, making them sensitive to input permutations. Se-quential processing encodes position-awareness. Stacked lay-ers allow LSTMs to capture hierarchical temporal patterns.\nTransformers (TFs) Transformers (Vaswani, 2017) are de-signed to capture both local and global dependencies in se-quences using a self-attention mechanism. Unlike LSTMs, which process input sequentially, Transformers compute at-tention over all input positions simultaneously. Self-attention enables the model to dynamically focus on relevant parts of the input, giving it direct access to long-range dependencies instead of through memory as in the LSTM. Transformers require explicit positional encodings since they lack an inher-ent sense of order. Stacked attention and feedforward layers enable the learning of hierarchical patterns, similar to deep CNNs when the input image is patched as is done with Vi-sion Transformers (Dosovitskiy et al., 2021)."}, {"title": "Meta-Learning", "content": "We adopt a meta-learning approach called Meta-SGD (Li, Zhou, Chen, & Li, 2017), which learns a model initialization, learning rate, and update direction that solves a set of tasks in a fixed number of steps. Meta-SGD is an extension of Model-Agnostic Meta-Learning (MAML; Finn et al., 2017), which only learns the model initialization. This enhanced flexibil-ity allows for a more complete training algorithm that can quickly adapt to new, unseen tasks with minimal data. Dur-ing meta-training, the goal is to learn a model initialization, learning rate, and update direction that extract shared struc-ture across tasks and embed inductive biases into the model outside what is explicitly defined by architecture or data rep-resentation.\nFormally, Meta-SGD aims to find parameters \\( \\theta \\) and \\( \\alpha \\) that minimize the expected test loss across tasks:\n\\[\\min_{\\theta,\\alpha} \\mathbb{E}_{\\tau \\sim p(\\tau)} [\\mathcal{L}_{Test} (\\theta')], \\text{ where } (\\theta' = \\theta - \\alpha \\odot \\nabla_{\\theta} \\mathcal{L}_{Train} (\\theta)).\\]\nHere, \\( p(\\tau) \\) denotes the task distribution, \\( \\mathcal{L}_{Train} \\) is the training loss for an individual task given a support set, and \\( \\mathcal{L}_{Test} \\) is the test loss given a query set. The parameter \\( \\theta \\) represents the"}, {"title": "Approach", "content": "We evaluate two types of scenarios to address constraints be-tween neural architectures, data representation, and training algorithms. First, we assess the best-case scenario for meta-learning, where test tasks are fully in-distribution and ample meta-training data is provided. Second, we test more chal-lenging conditions, with test and training tasks from differ-ent distributions and limited meta-training data, where other sources of bias are more persistent. For both cases, we also compare performance against randomly-initialized baselines to highlight the impact of architectural bias without meta-learned adaptations. We follow a consistent meta-learning procedure across three tasks, defining variations in depth and width for each architecture, selecting models based on per-formance on a meta-validation set, meta-training each archi-tecture using Meta-SGD, and testing against controlled base-lines."}, {"title": "Neural Architectures", "content": "To isolate the core inductive biases, we remove non-essential components, such as dropout. MLPs maintain a fixed hidden width across all layers, with batch normalization and ReLU activation applied after each hidden layer. CNNs use 3\u00d73 kernels with a stride of 1 and zero-padding. Each convolu-tional layer is followed by batch normalization, a ReLU ac-tivation, and average pooling with a stride of 2. CNNs be-gin with either 2 or 4 convolutional layers (depending on the architecture depth) followed by fully connected layers with a fixed hidden width equal to the dimensionality of the fi-nal, flattened output of the convolution layers. LSTMs fol-low the original implementation in Hochreiter and Schmid-huber (1997) but use pre-layer normalization and a projec-tion layer to align input and hidden dimensions. Transform-ers (Vaswani, 2017) use sinusoidal positional encoding, four attention heads, and a feedforward network with a dimension-ality twice the hidden size. Pre-layer normalization and a pro-jection layer are both used. There are 16 variations of each architecture, with ranges over depth, width, and parameter counts that are comparable (see Table 1)."}, {"title": "Meta-Training and Sampling Architectures", "content": "We meta-train each architecture using Meta-SGD (Li et al., 2017), with AdamW (Loshchilov & Hutter, 2017) as the outer optimizer. We set the learning rate to 0.001 and weight decay to 0.01. For all 16 variations of each architecture, we perform 10,000 episodes of meta-training. After this initial phase, we select the best-performing architecture based on its perfor-mance on a meta-validation set filled with 100 unseen training tasks and then continue optimizing the selected architecture to convergence. We repeat for 10 random seeds. This pro-duces 10 independent samples of meta-learned weights and architectures for each architecture class. We meta-train with batches of 4 tasks and use 1 adaptation step throughout."}, {"title": "Tasks", "content": "We consider three tasks: concept learning, modular arith-metic, and few-shot learning with Omniglot (Lake, Salakhut-dinov, & Tenenbaum, 2011). Concept learning involves in-distribution test tasks, providing an ideal scenario for meta-learning to succeed. Modular arithmetic tests both in- and out-of-distribution generalization by splitting training and testing tasks across different moduli. Few-shot classification with Omniglot introduces a more complex scenario, where limited training data leads to out-of-distribution test tasks."}, {"title": "Data Representation", "content": "We generate two types of data representations: 32\u00d732 images and bitstring encodings. For concept learning, each concept is represented by a 4-bit feature vector. These features include attributes such as color (red or blue), shape (square or trian-gle), size (big or small), and pattern (striped or solid). We visualize features as RGB images for input to the networks. For modular arithmetic, input numbers are encoded as 8-bit binary strings and synthetically generated images . For our few-shot learning experiments with Omniglot (Lake et al., 2011), we consider exclusively the image data, downsampled to 32x32 for consistency across tasks.\nMLPs flatten the image input and process bitstrings as floating-point vectors. CNNs operate exclusively on image inputs. LSTMs and Transformers divide each 32x32 image into 4x4 patches, resulting in sequences of 64 tokens."}, {"title": "Meta-Testing and Control Conditions", "content": "We generate 100 random tasks for 10 different seeds and meta-test each of the 10 different models for each architec-ture class on every seed. We compare to a baseline of a ran-dom initialization using the same 10 architectures and fit 1, 10, 50, 100 and 200 steps of AdamW with a learning rate of 0.001 and a weight decay of 0.01. All tasks had converged or began to overfit after 200 steps. We use mean square error (MSE) loss as a performance metric for modular arithmetic and prediction accuracy for concept learning and Omniglot."}, {"title": "Results", "content": "For each of our three tasks, we evaluate the roles of architec-ture, data representation, and training algorithms."}, {"title": "Concept Learning", "content": "This experiment involves learning concepts based on objects that have 4 features, represented as a 4-bit vector or an image. A concept, such as \\( f_1 (x) = 0 \\land f_3 (x) = 1 \\), assigns true or false labels to the 16 possible objects based on whether they sat-isfy the concept. Following the procedure from Marinescu, McCoy, and Griffiths (2024), we generate concepts from a probabilistic context-free grammar (Goodman, Tenenbaum, Feldman, & Griffiths, 2008). Concepts are sampled for meta-training with variable support sizes and meta-tested on new concepts with support sizes of 5, 10, and 15. We evaluate both meta-learned models and randomly initialized models with variable gradient step counts . The meta-learned models perform comparably across all architectures and both data representations, with accuracy improving as support size increases. In contrast, randomly initialized models trained with 200 steps show significantly greater performance varia-tion across architectures and data representations, indicating that meta-learning can not only enhance task performance but also reduce the influence of architectural and representational biases on model behavior under ideal conditions."}, {"title": "Modular Arithmetic", "content": "We frame modular arithmetic as a non-linear regression task over the integer domain [0, 100) in this experiment. The goal is to infer the underlying function given noisy samples from some modulus m. We explore two versions of the task: Odd-Even, where we meta-train with odd moduli and meta-test with even moduli in the range [1,40], and 20-20, where we meta-train with [1,20] and meta-test with [21,40]. We sample variable support sizes during meta-training and meta-test with support set sizes 20, 40, and 100, where 20, 40 are uniformly"}, {"title": "Few-Shot Learning", "content": "We replicate the 20-way, 5-shot Omniglot challenge (Lake et al., 2011) to explore how task distribution affects perfor-mance across architectures. Handwritten alphabets are di-vided into four categories: Ancient (pre-500 A.D.), Asian, Middle Eastern, and European. Each neural architecture is meta-trained on these subsets. The fictional alphabets, Futu-rama and Magi, are excluded from the subsets but included in the base task distribution, All, which contains all 30 training alphabets. The training baseline, N/A, denotes the random initialization cases (R1, R10, R50, R100, R200) where we optimize for 1, 10, 50, 100, and 200 steps. All architectures are converged beyond this point.\nMeta-testing on the 20 held-out alphabets reveals drops in performance for Ancient and Asian categories across all ar-chitectures . CNNs retain the highest percentage of their original accuracy compared to All, while Transform-ers and LSTMs suffer larger drops. Transformers generalize"}, {"title": "Discussion", "content": "Neural network architectures are often designed with specific problems in mind (e.g., next-word prediction, image classi-fication), so it is natural to expect them to perform poorly on problems they were not explicitly designed for. Indeed, we found that the standard neural networks, trained without meta-learned inductive biases, perform significantly worse when the requirements of the task were misaligned with the inductive biases of the architecture. For example, tasks in-volving bitstrings require an explicit understanding of po-sitional order, which is naturally encoded in sequential ar-chitectures like LSTMs and Transformers. As might be ex-pected from these architectural properties, these architectures learned faster and performed better than MLPs, which lack this inductive bias. Similarly, in the Omniglot task, CNNs outperformed other architectures, likely due to their useful spatial inductive biases, demonstrating their superior ability to generalize when spatial structure is critical. However, even these task-suitable inductive biases were not enough to enable models to perfectly solve these tasks.\nClassical analyses of learnability in cognitive science have argued for innate cognitive structures that support rapid learn-ing from limited data, such as Chomsky's notion of Uni-versal Grammar (UG) as a description of an innate device that supports efficient language acquisition (Chomsky, 1980), and Fodor's theory of domain-specific encapsulated mod-ules (Fodor, 1983). Neural networks, by contrast, do not explicitly have these structures. However, meta-learning can embed task-specific knowledge into a network's initial weights, allowing networks to overcome limitations in ar-chitecture or data representation when exposed to a suffi-ciently rich task distribution. For example, McCoy and Grif-fiths (2023) showed that meta-learning enables networks to learn linguistic patterns from a few examples, mimicking UG-like rapid learning. Similarly, Zintgraf, Shiarli, Kurin, Hofmann, and Whiteson (2019) found that networks can de-velop task-specific specializations with minimal data, resem-bling Fodor's idea of modularity. Our findings reinforce these results, showing that meta-learning vastly improves few-shot learning performance and reduces variations across archi-tectures, suggesting that certain kinds of innate knowledge can be implicitly expressed in neural networks (Elman et al., 1996) and that architecture is a weak constraint on what a neural network can do.\nMeta-learning still lacks the ability to distill stronger forms of generalization. Humans excel at both interpolation (learn-ing within the range of observed examples) and extrapolation (generalizing beyond those examples). However, in modu-lar arithmetic, we showed some generalization capabilities when fitting moduli between known moduli (interpolation) in Odd-Even but catastrophic performance for 20-20, where the testing moduli were far outside the training task domain (extrapolation). Meta-learning can perhaps then be viewed as a \"blind\" optimization process (Hasson, Nastase, & Gold-stein, 2020), where we can only distill structures present in the training task distribution but not outside of it. To achieve stronger forms of generalization, we may need reg-ularity that comes from outside of pure optimization alone, which is where architectural constraints come to play. In our Omniglot experiments, the CNN was the least sensitive"}, {"title": "Conclusion", "content": "While neural architectures do impose constraints on the kinds of problems neural networks can solve, these constraints are weak relative to the inductive biases afforded by initial weights. Meta-learning offers a path to distilling task-specific knowledge that is less influenced by the architecture and data representation than typical training regimes. We conclude that the flexibility of initial weights is extensive enough to override the influence of architecture in some settings, but substantial architectural differences persist when extensive generalization beyond the meta-task distribution is required."}, {"title": "Additional Results: Modular Arithmetic", "content": "Table 6 shows the average MSE errors over 10 sampled architectures for 10 random seeds for the 20-20 task, where the moduli were split into a 1-20 training group and a 21-40 testing group. This task requires more robust forms of generalization since the test task distribution is much farther away than the Odd-Even task reported in the main text. We found that performance across all meta-trained models was very poor on unseen tasks and comparable to the Odd-Even task on validation tasks. This suggests that meta-learning failed to distill the knowledge necessary to generalize to unseen moduli functions, limiting its ability to fit moduli beyond those encountered or closely related to those seen during training.\nIn Figures 5 and 6, we visualize representative curves for a single LSTM trained with image data and given 20 support points on the 20-20 task. Performance is near perfect on in-distribution tasks but the LSTM attempts to fit curves that resemble training moduli despite significant signal to support a different function. Figures 3 and 4 show the same curves but for the Odd-Even version of the modular arithmetic task that we report in the main text."}]}