{"title": "Teasing Apart Architecture and Initial Weights as Sources of Inductive Bias in Neural Networks", "authors": ["Gianluca Bencomo", "Max Gupta", "Ioana Marinescu", "R. Thomas McCoy", "Thomas L. Griffiths"], "abstract": "Artificial neural networks can acquire many aspects of human knowledge from data, making them promising as models of human learning. But what those networks can learn depends upon their inductive biases - the factors other than the data that influence the solutions they discover - and the inductive biases of neural networks remain poorly understood, limiting our ability to draw conclusions about human learning from the performance of these systems. Cognitive scientists and machine learning researchers often focus on the architecture of a neural network as a source of inductive bias. In this paper we explore the impact of another source of inductive bias \u2013 the initial weights of the network - using meta-learning as a tool for finding initial weights that are adapted for specific problems. We evaluate four widely-used architectures - MLPs, CNNs, LSTMs, and Transformers \u2013 by meta-training 430 different models across three tasks requiring different biases and forms of generalization. We find that meta-learning can substantially reduce or entirely eliminate performance differences across architectures and data representations, suggesting that these factors may be less important as sources of inductive bias than is typically assumed. When differences are present, architectures and data representations that perform well without meta-learning tend to meta-train more effectively. Moreover, all architectures generalize poorly on problems that are far from their meta-training experience, underscoring the need for stronger inductive biases for robust generalization.", "sections": [{"title": "Introduction", "content": "Artificial neural networks have been used to explain how aspects of human knowledge that have been claimed to depend upon an extensive degree of innateness \u2013 such as elements of language - might be learned from data by systems that do not have strong built-in assumptions (e.g., Rumelhart & McClelland, 1986). These networks offer a new perspective on central questions in cognitive science, such as what information we need to assume is innate to human learners (Elman et al., 1996). In machine learning, a parallel set of questions focuses on the inductive biases of neural networks - defined as those factors other than the data that influence the solutions that they find (Mitchell, 1997). The convergence of these literatures offers an opportunity to explore different ways in which innate knowledge might be implicitly expressed in artificial neural networks.\nDifferent neural network architectures display different inductive biases. For instance, one clear signature of inductive bias is the amount of data needed to learn a task, and convolutional neural networks can learn image classification tasks from less data than multi-layer perceptrons (Chen et al., 2021). In addition to network architecture, however, recent work has highlighted the importance of a network's initial weights as a source of inductive bias (Finn, Abbeel, & Levine, 2017). Specifically, techniques based on meta-learning can optimize the initial weights of a neural network (leaving the architecture unchanged) in ways that enable the network to learn new tasks from far less data than it would require using standard, randomly-selected initial weights. For instance, a network with meta-learned initial weights can learn new linguistic rules from just 100 examples, compared to the roughly 20,000 examples needed by the same architecture with non-meta-learned initial weights (McCoy, Grant, Smolensky, Griffiths, & Linzen, 2020). Such meta-learning results show that a given neural network architecture can realize very different inductive biases thanks to the flexibility afforded by the initial weights.\nHere we consider this flexibility from the opposite direction: can a given inductive bias be realized equally well in very different network architectures? This question directly engages with the issue of whether architecture or initial weights provide a better focus for understanding the innate constraints on learning implicitly instantiated in a neural network. Prior work using meta-learning typically makes comparisons within a fixed architecture, comparing a version of that architecture with meta-learned initial weights to a version with randomly-selected initial weights. These comparisons make it clear that the initial weights afford a substantial degree of flexibility, but they leave open the question of whether that flexibility is extensive enough to override the influence of architecture such that a given inductive bias could be realized equally well in different architectures.\nTo address this, we explore several inductive biases, investigating how compatible each inductive bias is with different types of network architectures and data representations. We consider four widely-used, general-purpose neural architectures-multilayer perceptrons (MLPs; Rosenblatt, 1962), convolutional neural networks (CNNs; LeCun, Bottou, Bengio, & Haffner, 1998), long short-term memory networks (LSTMs; Hochreiter & Schmidhuber, 1997), and Transformers (Vaswani, 2017)\u2014with variations in depth and width, meta-training a total of 430 models. To establish baselines where differences across architectures and data representations should be more pronounced-free from task-specific biases introduced by meta-learning-we compare these meta-trained models to the same architectures trained under typical regimes, starting from random initialization and optimizing along that trajectory. This design enables us to isolate how much of the performance variation can be attributed to architectural and data representation choices, as opposed to the learning processes that are agnostic to those choices.\nAcross both data representation and architecture, we observe substantial performance differences when models are trained using the usual approach of setting the initial weights randomly. However, introducing meta-learned inductive biases reduces, and in some cases completely eliminates, these differences, demonstrating that a given inductive bias can be instantiated in multiple, disparate architectures. Interestingly, architectures and data representations that perform well under random initialization also tend to meta-train more effectively, suggesting that some residual biases remain important for certain tasks. In few-shot learning, for example, models that excel without meta-learning are less sensitive to shifts in the training task distribution. Despite this, when models are required to learn tasks that lie far outside the distribution of tasks they encountered during meta-training, all architectures-regardless of inductive bias-fail catastrophically. This highlights that these general-purpose architectures may require stronger inductive biases for more robust forms of generalization but remain general enough to realize a wide range of biases given appropriate choices for the initial weights and learning rate."}, {"title": "Background", "content": "Inductive biases-the assumptions that guide learning-can manifest through the choice of model architecture, data representation, error metric, and training algorithm (Baxter, 2000). In this work, we investigate the extent to which model architecture and data representation influence performance outcomes after optimizing the initial weights and learning rate through meta-learning. This section introduces the kinds of biases inherent to the neural architectures we explore and addresses how meta-learning distills task-specific knowledge of the learning problem into the training algorithm."}, {"title": "Inductive Biases across Neural Architectures", "content": "Multi-Layer Perceptrons (MLPs) MLPs (Rosenblatt, 1962) can approximate any function given sufficient depth and width (Hornik, Stinchcombe, & White, 1989) but make no explicit assumption about the structure of the input data beyond static size. The lack of built-in equivariances make them highly sensitive to nearly all spatial and temporal variations. All-to-all connections between layers imply global feature mixing and deeper layers can capture progressively more abstract representations.\nConvolutional Neural Networks (CNNs) CNNs (LeCun et al., 1998) were designed with an explicit bias towards grid-structured data such as images. Convolutions with shared weights prioritize spatially local relationships and ensure translation equivariance. Pooling layers provide partial robustness to variations in scale, though CNNs generally lack inherent rotation or scale equivariances. CNNs preserve spatial order, making them sensitive to input permutations. Like MLPs, they build hierarchical features, with deeper layers capturing abstract patterns composed of simpler ones. These representations often resemble those in the mammalian visual cortex (Yamins & DiCarlo, 2016).\nLong Short-Term Memory Networks (LSTMs) LSTMs (Hochreiter & Schmidhuber, 1997) were designed to capture both long- and short-term dependencies in sequences by maintaining an internal state that tracks temporal dynamics. Input, forget, and output gates regulate the flow of information, enabling the model to selectively retain or discard data. LSTMs rely on a memory structure with a bottleneck defined by the size of the hidden and cell states. They assume order matters, making them sensitive to input permutations. Sequential processing encodes position-awareness. Stacked layers allow LSTMs to capture hierarchical temporal patterns.\nTransformers (TFs) Transformers (Vaswani, 2017) are designed to capture both local and global dependencies in sequences using a self-attention mechanism. Unlike LSTMs, which process input sequentially, Transformers compute attention over all input positions simultaneously. Self-attention enables the model to dynamically focus on relevant parts of the input, giving it direct access to long-range dependencies instead of through memory as in the LSTM. Transformers require explicit positional encodings since they lack an inherent sense of order. Stacked attention and feedforward layers enable the learning of hierarchical patterns, similar to deep CNNs when the input image is patched as is done with Vision Transformers (Dosovitskiy et al., 2021)."}, {"title": "Meta-Learning", "content": "We adopt a meta-learning approach called Meta-SGD (Li, Zhou, Chen, & Li, 2017), which learns a model initialization, learning rate, and update direction that solves a set of tasks in a fixed number of steps. Meta-SGD is an extension of Model-Agnostic Meta-Learning (MAML; Finn et al., 2017), which only learns the model initialization. This enhanced flexibility allows for a more complete training algorithm that can quickly adapt to new, unseen tasks with minimal data. During meta-training, the goal is to learn a model initialization, learning rate, and update direction that extract shared structure across tasks and embed inductive biases into the model outside what is explicitly defined by architecture or data representation.\nFormally, Meta-SGD aims to find parameters $\\theta$ and $\\alpha$ that minimize the expected test loss across tasks:\n$\\min_{\\theta,\\alpha} E_{\\tau \\sim p(\\tau)} [L_{\\text{Test}} (\\theta')]$, where $(\\theta' = \\theta - \\alpha \\odot \\nabla_{\\theta} L_{\\text{Train}} (\\theta)$.\nHere, $p(\\tau)$ denotes the task distribution, $L_{\\text{Train}}$ is the training loss for an individual task given a support set, and $L_{\\text{Test}}$ is the test loss given a query set. The parameter $\\theta$ represents the model's initial weights, while $\\alpha$ represents the learned task-specific learning rate and step direction for adaptation.\nPrior work has demonstrated that meta-learning can embed inductive biases beyond those defined by a model's architecture. For example, Snell, Bencomo, and Griffiths (2024) showed how meta-learned neural circuits can perform complex, task-specific probabilistic reasoning by distilling the biases required to perform nonparametric Bayesian inference. Similarly, meta-learning has been applied to address the problem of catastrophic forgetting, a major challenge in online learning, by helping neural networks retain knowledge across tasks (Javed & White, 2019). For tasks that require producing novel combinations from known components, Lake and Baroni (2023) demonstrated meta-learning's ability to distill human-like compositional skills into neural networks, despite Fodor and Pylyshyn (1988) famously arguing that artificial neural networks lacked this capacity. While these approaches focus on learning capabilities that neural networks may not inherently possess, our work investigates the extent to which specific inductive biases can be encoded and expressed within different architectures through meta-learning; see Abnar, Dehghani, and Zuidema (2020) for a different approach that encourages similarity between architectures by directly training one architecture to reproduce the outputs from another, rather than having different architectures meta-learn from the same task distribution."}, {"title": "Approach", "content": "We evaluate two types of scenarios to address constraints between neural architectures, data representation, and training algorithms. First, we assess the best-case scenario for meta-learning, where test tasks are fully in-distribution and ample meta-training data is provided. Second, we test more challenging conditions, with test and training tasks from different distributions and limited meta-training data, where other sources of bias are more persistent. For both cases, we also compare performance against randomly-initialized baselines to highlight the impact of architectural bias without meta-learned adaptations. We follow a consistent meta-learning procedure across three tasks, defining variations in depth and width for each architecture, selecting models based on performance on a meta-validation set, meta-training each architecture using Meta-SGD, and testing against controlled baselines."}, {"title": "Neural Architectures", "content": "To isolate the core inductive biases, we remove non-essential components, such as dropout. MLPs maintain a fixed hidden width across all layers, with batch normalization and ReLU activation applied after each hidden layer. CNNs use 3$\\times$3 kernels with a stride of 1 and zero-padding. Each convolutional layer is followed by batch normalization, a ReLU activation, and average pooling with a stride of 2. CNNs begin with either 2 or 4 convolutional layers (depending on the architecture depth) followed by fully connected layers with a fixed hidden width equal to the dimensionality of the final, flattened output of the convolution layers. LSTMs follow the original implementation in Hochreiter and Schmidhuber (1997) but use pre-layer normalization and a projection layer to align input and hidden dimensions. Transformers (Vaswani, 2017) use sinusoidal positional encoding, four attention heads, and a feedforward network with a dimensionality twice the hidden size. Pre-layer normalization and a projection layer are both used. There are 16 variations of each architecture, with ranges over depth, width, and parameter counts that are comparable (see Table 1)."}, {"title": "Meta-Training and Sampling Architectures", "content": "We meta-train each architecture using Meta-SGD (Li et al., 2017), with AdamW (Loshchilov & Hutter, 2017) as the outer optimizer. We set the learning rate to 0.001 and weight decay to 0.01. For all 16 variations of each architecture, we perform 10,000 episodes of meta-training. After this initial phase, we select the best-performing architecture based on its performance on a meta-validation set filled with 100 unseen training tasks and then continue optimizing the selected architecture to convergence. We repeat for 10 random seeds. This produces 10 independent samples of meta-learned weights and architectures for each architecture class. We meta-train with batches of 4 tasks and use 1 adaptation step throughout."}, {"title": "Tasks", "content": "We consider three tasks: concept learning, modular arithmetic, and few-shot learning with Omniglot (Lake, Salakhutdinov, & Tenenbaum, 2011). Concept learning involves in-distribution test tasks, providing an ideal scenario for meta-learning to succeed. Modular arithmetic tests both in- and out-of-distribution generalization by splitting training and testing tasks across different moduli. Few-shot classification with Omniglot introduces a more complex scenario, where limited training data leads to out-of-distribution test tasks."}, {"title": "Concept Learning", "content": "This experiment involves learning concepts based on objects that have 4 features, represented as a 4-bit vector or an image. A concept, such as $f_1(x) = 0 \\land f_3(x) = 1$, assigns true or false labels to the 16 possible objects based on whether they satisfy the concept. Following the procedure from Marinescu, McCoy, and Griffiths (2024), we generate concepts from a probabilistic context-free grammar (Goodman, Tenenbaum, Feldman, & Griffiths, 2008). Concepts are sampled for meta-training with variable support sizes and meta-tested on new concepts with support sizes of 5, 10, and 15. We evaluate both meta-learned models and randomly initialized models with variable gradient step counts (see Table 3). The meta-learned models perform comparably across all architectures and both data representations, with accuracy improving as support size increases. In contrast, randomly initialized models trained with 200 steps show significantly greater performance variation across architectures and data representations, indicating that meta-learning can not only enhance task performance but also reduce the influence of architectural and representational biases on model behavior under ideal conditions."}, {"title": "Modular Arithmetic", "content": "We frame modular arithmetic as a non-linear regression task over the integer domain [0, 100) in this experiment. The goal is to infer the underlying function given noisy samples from some modulus m. We explore two versions of the task: Odd-Even, where we meta-train with odd moduli and meta-test with even moduli in the range [1,40], and 20-20, where we meta-train with [1,20] and meta-test with [21,40]. We sample variable support sizes during meta-training and meta-test with support set sizes 20, 40, and 100, where 20, 40 are uniformly sampled and 100 includes every integer in the domain, offering a noisy version of the true moduli function over the entire domain. Noise is injected via independent samples from a Gaussian with $\\sigma$ = 0.1.\nWe find that this task produces variations across both data representation and architecture in meta-learned models. Meta-Val reports the same moduli seen during meta-training but with a different noise seed and Meta-Test reports unseen moduli (see Table 4). Meta-learned LSTMs usually perform the best across data representations and support sizes. Every model performs reasonably well, to varying degrees, for in-distribution tasks but shows a drop in performance for out-of-distribution tasks in Odd-Even. The same is observed in 20-20, but the drop in performance with out-of-distribution tasks is catastrophic (see Table 6): all meta-trained architectures have MSE's > 50.0. This is likely due to the harder form of generalization that is required.\nRandomly initialized architectures start to overfit after just 10 steps of AdamW across all support sizes in this task, despite AdamW's explicit regularization towards simpler solutions. We report 1-step and 10-step updates (R1, R10). These randomly initialized models are significantly outperformed by their meta-trained counterparts. Notably, large variations across architectures and data representations are for the most part eliminated when the networks are meta-trained. LSTMS far outperform the other architectures, perhaps explaining their superior performance when meta-trained."}, {"title": "Few-Shot Learning", "content": "We replicate the 20-way, 5-shot Omniglot challenge (Lake et al., 2011) to explore how task distribution affects performance across architectures. Handwritten alphabets are divided into four categories: Ancient (pre-500 A.D.), Asian, Middle Eastern, and European. Each neural architecture is meta-trained on these subsets. The fictional alphabets, Futurama and Magi, are excluded from the subsets but included in the base task distribution, All, which contains all 30 training alphabets. The training baseline, N/A, denotes the random initialization cases (R1, R10, R50, R100, R200) where we optimize for 1, 10, 50, 100, and 200 steps. All architectures are converged beyond this point.\nMeta-testing on the 20 held-out alphabets reveals drops in performance for Ancient and Asian categories across all architectures (see Table 5). CNNs retain the highest percentage of their original accuracy compared to All, while Transformers and LSTMs suffer larger drops. Transformers generalize worse than MLPs when trained on Ancient alphabets. However, generalization performance largely recovers on Middle Eastern and European alphabets.\nMLPs and Transformers can outperform randomly initialized CNNs when meta-trained. However, CNNs exhibit desirable equivariances, making them less sensitive to distribution shifts and allowing them to achieve better random initialization performance with fewer gradient steps. The performance gap between architectures narrows considerably under meta-learning. However, LSTMs still struggle with the task despite meta-learning improving their performance."}, {"title": "Discussion", "content": "Neural network architectures are often designed with specific problems in mind (e.g., next-word prediction, image classification), so it is natural to expect them to perform poorly on problems they were not explicitly designed for. Indeed, we found that the standard neural networks, trained without meta-learned inductive biases, perform significantly worse when the requirements of the task were misaligned with the inductive biases of the architecture. For example, tasks involving bitstrings require an explicit understanding of positional order, which is naturally encoded in sequential architectures like LSTMs and Transformers. As might be expected from these architectural properties, these architectures learned faster and performed better than MLPs, which lack this inductive bias. Similarly, in the Omniglot task, CNNs outperformed other architectures, likely due to their useful spatial inductive biases, demonstrating their superior ability to generalize when spatial structure is critical. However, even these task-suitable inductive biases were not enough to enable models to perfectly solve these tasks.\nClassical analyses of learnability in cognitive science have argued for innate cognitive structures that support rapid learning from limited data, such as Chomsky's notion of Universal Grammar (UG) as a description of an innate device that supports efficient language acquisition (Chomsky, 1980), and Fodor's theory of domain-specific encapsulated modules (Fodor, 1983). Neural networks, by contrast, do not explicitly have these structures. However, meta-learning can embed task-specific knowledge into a network's initial weights, allowing networks to overcome limitations in architecture or data representation when exposed to a sufficiently rich task distribution. For example, McCoy and Griffiths (2023) showed that meta-learning enables networks to learn linguistic patterns from a few examples, mimicking UG-like rapid learning. Similarly, Zintgraf, Shiarli, Kurin, Hofmann, and Whiteson (2019) found that networks can develop task-specific specializations with minimal data, resembling Fodor's idea of modularity. Our findings reinforce these results, showing that meta-learning vastly improves few-shot learning performance and reduces variations across architectures, suggesting that certain kinds of innate knowledge can be implicitly expressed in neural networks (Elman et al., 1996) and that architecture is a weak constraint on what a neural network can do.\nMeta-learning still lacks the ability to distill stronger forms of generalization. Humans excel at both interpolation (learning within the range of observed examples) and extrapolation (generalizing beyond those examples). However, in modular arithmetic, we showed some generalization capabilities when fitting moduli between known moduli (interpolation) in Odd-Even but catastrophic performance for 20-20, where the testing moduli were far outside the training task domain (extrapolation). Meta-learning can perhaps then be viewed as a \"blind\" optimization process (Hasson, Nastase, & Goldstein, 2020), where we can only distill structures present in the training task distribution but not outside of it. To achieve stronger forms of generalization, we may need regularity that comes from outside of pure optimization alone, which is where architectural constraints come to play. In our Omniglot experiments, the CNN was the least sensitive to shifts in the training task distribution, demonstrating how architecture can make up the difference for what might not be available in the training data. Alternatively, we can consider techniques like reinforcement learning, that explicitly incentivize stronger forms of generalization (Akkaya et al., 2019), or variations on meta-learning that encourage the training algorithm to find structures outside of what the data can offer (Irie & Lake, 2024)."}, {"title": "Conclusion", "content": "While neural architectures do impose constraints on the kinds of problems neural networks can solve, these constraints are weak relative to the inductive biases afforded by initial weights. Meta-learning offers a path to distilling task-specific knowledge that is less influenced by the architecture and data representation than typical training regimes. We conclude that the flexibility of initial weights is extensive enough to override the influence of architecture in some settings, but substantial architectural differences persist when extensive generalization beyond the meta-task distribution is required."}, {"title": "Additional Results: Modular Arithmetic", "content": "Table 6 shows the average MSE errors over 10 sampled architectures for 10 random seeds for the 20-20 task, where the moduli were split into a 1-20 training group and a 21-40 testing group. This task requires more robust forms of generalization since the test task distribution is much farther away than the Odd-Even task reported in the main text. We found that performance across all meta-trained models was very poor on unseen tasks and comparable to the Odd-Even task on validation tasks. This suggests that meta-learning failed to distill the knowledge necessary to generalize to unseen moduli functions, limiting its ability to fit moduli beyond those encountered or closely related to those seen during training.\nIn Figures 5 and 6, we visualize representative curves for a single LSTM trained with image data and given 20 support points on the 20-20 task. Performance is near perfect on in-distribution tasks but the LSTM attempts to fit curves that resemble training moduli despite significant signal to support a different function. Figures 3 and 4 show the same curves but for the Odd-Even version of the modular arithmetic task that we report in the main text."}]}