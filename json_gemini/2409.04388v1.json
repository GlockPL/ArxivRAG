{"title": "Question-Answering Dense Video Events", "authors": ["Hangyu Qin", "Junbin Xiao", "Angela Yao"], "abstract": "Multimodal Large Language Models (MLLMs) have shown excellent performance in question-answering of single-event videos. In this paper, we present question-answering dense video events, a novel task that requires answering and grounding the dense-event questions in long videos, thus challenging MLLMs to faithfully comprehend and reason about multiple events occurring over extended time periods. To facilitate the study, we construct DeVE-QA a dataset featuring 78K questions about 26K events on 10.6K long videos. We then benchmark and show that existing MLLMs excelling at single-event QA struggle to perform well in DeVE-QA. For improvement, we propose DeVi, a novel training-free MLLM approach that highlights a hierarchical captioning module, a temporal event memory module, and a self-consistency checking module to respectively detect, contextualize and memorize, and ground dense-events in long videos for question answering. Extensive experiments show that DeVi is superior at answering dense-event questions and grounding relevant video moments. Compared with existing MLLMs, it achieves a remarkable increase of 4.1% and 3.7% for G(round)QA accuracy on DeVE-QA and NEXT-GQA respectively. Our data and code will be released.", "sections": [{"title": "Introduction", "content": "Multimodal Large Language Models (MLLMs) (Alayrac et al. 2022; Li et al. 2023b; Maaz et al. 2023; Zhang, Li, and Bing 2023; Lin et al. 2023; Reid et al. 2024) have shown significant capability in question-answering of single-event videos (Xu et al. 2017; Jang et al. 2017), where the videos are short in 3 ~ 20 seconds and the QAs factor single global types of events, e.g. \u201cwho did what\". Yet, real-world video often comes in long format and features a complex overlay of dense events. Consider the 2-minute video taken from a motorcycle activity shown in Figure 1. A variety of questions can be asked about this video, with each pertaining to an individual event but involving different participants and durations interspersed throughout the video. The events, while being separate, are still related to each other, e.g. a motorcycle stunt performance.\nThe inherent challenge of understanding such dense video events is thus to either isolate or agglomerate, as needed, relevant video content and generate relevant event responses."}, {"title": "Related Works", "content": "Dense Event Video Understanding Dense video event understanding has primarily focused on captioning (Krishna et al. 2017; Wang et al. 2018; Lin et al. 2022; Yang et al. 2023). However, optimizing for holistic sentence generation often results in over-fitting (Chen, Li, and Hu 2020) and object hallucination (Rohrbach et al. 2018). MLLMs on the other hand, have shown strong capabilities for visual de-scription (Li et al. 2023a; Liu et al. 2024; Maaz et al. 2023; Li et al. 2023b; Lin et al. 2023; Ren et al. 2023; Xu et al. 2024). Yet, the subjective caption annotations and the sub-effective sentence-matching metrics (e.g., BLEU (Papineni et al. 2002) and CIDEr (Vedantam, Lawrence Zitnick, and Parikh 2015)) make it challenging to evaluate these models, especially from a zero-shot perspective. Our work proposes to use question-answering as an alternative to evaluate the understanding and reasoning of dense video events.\nVideo Question Answering VideoQA works are center on single event videos; this is reflected in the popular bench-marks, such as TGIF-QA (Jang et al. 2017), MSRVTT-QA and MSVD-QA (Xu et al. 2017), ActitivityNet-QA (Yu et al. 2019) and iVQA (Yang et al. 2021), and related techniques (Dai et al. 2023; Maaz et al. 2023; Zhang et al. 2023b; Wang et al. 2023; Li, Wang, and Jia 2023). The video clips in these benchmarks tend to be short or the questions are related to global events spanning the entire clips. NEXT-QA (Xiao et al. 2021) advances somewhat by addressing multiple ac-tion relations in relatively longer clips. The videos, however, focus on daily life actions and lack complexity in multi-event understanding. We also note that some techniques claim for event VideoQA (Yin et al. 2023; Liu, Li, and Lin 2023; Bai, Wang, and Chen 2024) but the events essentially refer to actions alone or single global event of short video. Compared with these works, our work shapes itself by study-ing multi-event comprehending and reasoning across long videos, where an event refers to a complete combination of subjects, actions, objects, time, etc (Krishna et al. 2017).\nMLLMs for VideoQA Most existing Video-LLMs are de-signed for short-video understanding (Xiao et al. 2024a). This includes the instruction-tuned models such as Video-ChatGPT (Maaz et al. 2023), Video-LLaMA (Zhang, Li, and Bing 2023), Video-LLaVA (Lin et al. 2023), VideoChat (Li et al. 2023b,c) and PLLaVA (Xu et al. 2024), and target-finetuned models like SeViLA (Yu et al. 2024) and LLaMA-VQA (Ko et al. 2023). The short input (4~32 frames) re-stricts these models from handling long videos. Training-free approaches, such as ViperGPT (Sur\u00eds, Menon, and Von-drick 2023) and LLoVi (Zhang et al. 2023a), handle long videos by traversing or dense-captioning the video. Traver-sal approaches cannot agglomerate multiple events inter-spersed at different times for joint reasoning. Therefore, we"}, {"title": "DeVE-QA Dataset", "content": "We follow dense-event captioning (Krishna et al. 2017) to define an event as a completed description of a person's (or a group's) specific behavior within a specific time, e.g., \u201cA man is playing the piano at [10.2s, 34.5s]\". Therefore, we curate our dataset DeVE-QA from ActivityNet-Captions.\nDataset Construction Given dense event captions, we derive question-answer sets by prompting GPT-4 (OpenAI 2024a) followed by human checking and corrections. Specifically, the construction process has three major stages (see Figure 2). In the first stage, we prompt GPT-4 to gen-erate different types of question-answer pairs (QAs) corresponding to each individual event using videos with clear and long event descriptions captions (i.e., no pronouns and longer than 10 words). This encourages understanding the event from multiple different aspects, e.g. with an implicit pattern of \"who did what at where and when, why and how\" implied from generation prompts. In the second stage, we retrieve distractor answers to form multiple choices for each question to facilitate deterministic evaluation. The distractor answers are from the answers of top-similar questions. Ad-ditionally, we incorporate approaches to maximally limit po-tential bias from the candidate answers, such as adding dis-tractor answers related to different events in the same video. Then we also perform QA filtration to remove meaning-less questions and also analyze the key activities inside the videos. The third stage is manual checking and correction to ensure the QA quality. We specially correct for 1) wrong QA pairs, 2) redundant questions, and 3) potential correct distractor answers. Finally, we obtain around 78k questions. We present an example in Figure 1. Other details along with the QAs are attached in Supplementary.\nStatistics and Analysis DeVE-QA is the first benchmark dataset that support question-answering of dense events in\""}, {"title": "DeVi Solution", "content": "Overview\nFormally, given a $T$-second video $v$ containing a collection of events $E = {e_1, e_2, \u2026\u2026\u2026, e_n }$, a question $q$ along with candidate answer set $C = {c_1,\u2026\u2026, c_5}$, dense video-event QA is to predict a correct answer $\u0109 \u2208 C$ and the relevant event moment $t = {t_s,t_e}$ where $t_s < t_e < T$. Our solution is conceptually as follows:\n$\u00ea, t = \u03c8(c, t|E, q, C)\u03c6(E|v),$ (1)\nwhere $\u03c6$ and $\u03c8$ denote the models for dense event detection and event-conditioned QA respectively. Note that the time stamps $t$ come along with the detected events $E$.\nwe realize the objective defined in Eqn. (1) as follows. First, to achieve dense video event detection $\u03c6(\u0395|v)$, we incorporate a hierarchical dense captioning mechanism into MLLMs to detect the video events at multiple different time scales. Then, we design a temporal event mem-ory module that captures the long-term event dependency to contextualizes and also memorize the individually detected video events $E$. Finally, to achieve event-grounded QA $(c, t|E, q, C)$, we read from the memory the contextualized events $E$, and feed it to LLMs along with the QAs (question $q$ and candidate answers $C$) to determine the correct answers and the corresponding event moments. In this process, we highlight a self-consistency checking mechanism to ensure the right answer for the right event. An overview of our so-lution is illustrated in Figure 4.\nHierarchical Dense Event Captioning\nDense events within videos are often intertwined and vary in durations. To successfully detect these events, we apply powerful MLLMs (e.g., Video-LLaVA (Lin et al. 2023)) at multiple scales and levels of temporal hierarchies. Specifi-cally, we build a H-level hierarchy and detect events by cap-tioning different lengths of video segments at different hier-archies. Our captioning starts from the bottom hierarchy for short video segments $V_s {v_k}_{k=1}^{N_s}$, which is achieved by sending $V_s$ to MLLMs and prompting the MLLMs to describe the video segments. Corresponding events are de-noted as $E_s = {e_k}_{k=1}^{N_s}$, where $N_s$ and $L_s$ are the num-ber and length of short video segments respectively. A spe-cific event $e_k$ is given by its text description along with the corresponding start $t_s^k$ and end $t_e^k$ time stamps. Simi-larly, we caption the video segments of middle and long at the middle and top hierarchies, and obtain the respective events $E_m = {e_k^m}_{k=1}^{N_m}$ and $E_l = {e_k^l}_{k=1}^{N_l}$. Note that $L_s < L_m < L_l < T$. Eventually, we obtain a collection of events $E = {E_s, E_m, E_l}$ for each video. Specific prompts are presented in Supplementary.\nTemporal Event Memory\nThe above events are independently detected by focusing on individual local video segments. The lack of contextual information often results in inaccurate or incomplete event captions. While the hierarchical captioning strategy helps alleviate the issue, it cannot model the long-term temporal event dependency. For example, in the video shown in Fig-ure 1, we may have captured the event of \"a man enters the field\" at the beginning and \"a biker is performing\" at the middle of the video. However, we cannot answer questions such as why the man enter the field and who (man or woman)\nEvent-Grounded QA\nIntuitively, we can read the events $E'$ from the event memory and feed it to LLMs (e.g., GPT-40) along with the QAs to ac-complish answer prediction and moment localization. This can be achieved by prompting like \"... select a correct an-swer from {C} to the question {q} based on the events {E} and also output the time span $[t_s, t_e]$ of the event that car-ries the correct answer ...\". This method is straightforward but we find that the performance is not as good as expected. There is a large discrepancy where the LLM often gives the correct answer but with wrong time span or vice-versa. For improvement, we establish a mechanism to check for con-sistency between a predicted answer and the corresponding time span.\nWe evaluate consistency based on the cosine similarity $R$ between the answer $a$ and the video content within time span $[t_s, t_e]$:\n$R_{va} = cos(f_v, f_a) = \\frac{f_v \\cdot f_a}{\\|f_v\\||\\|f_a\\||},$ (2)\nwhere $f_a$ and $f_v$ are encodings of the answer text and video segment using CLIP (Radford et al. 2021). Predictions with low consistency (i.e., small $R_{va}$) will be feedbacked to LLM for adjusting its predictions. This processes will iterate mul-tiple times before getting the predictions with consistency that is higher than a threshold $\u03c3$ or reaching the predefined maximal iteration number $\u03b4$. More details are presented in the Supplementary."}, {"title": "Experiments", "content": "Configuration and Evaluation\nOur experiments are conducted on the test set of DeVE-QA. Additionally, we extend our experiments to NEXT-GQA (Xiao et al. 2024b). NExT-GQA supports research for grounded QA about multiple actions though not for event grounding. It contains 990 videos and 5,553 questions for testing. For hierarchical event captioning, the number of hi-erarchies $H$ is set to 3, and the segment lengths $L_s, L_m$ and $L_h$ are set to {10s, 35s, 65s} for DeVE-QA and {5s, 15s, 45s} for NEXT-GQA, respectively. For self-consistency checking, the similarity threshold $\u03c3$ is set to 0.6 by imple-mentation analysis in Table 5(d), and the maximal iteration number $\u03b4$ is set to 2 for efficiency. The thresholds are empir-ically determined according to the QA accuracy. For evalua-tion, we follow NEXT-GQA (Xiao et al. 2024b) to report QA accuracy Acc@QA, grounding quality Intersection over Pre-diction (IoP) and Intersection over Union (IoU), as well as grounded QA accuracy Acc@GQA, all in percentages (%).\nPerformance Analysis\nWe first adapt the prominent MLLMs (e.g., Video-LLaMA (Zhang, Li, and Bing 2023), InternVideo (Wang et al. 2023), VFC (Momeni et al. 2023), etc) that perform well on \"single event\" QA to DeVE-QA and compare them with DeVi. The models (except for LLaMA-Adapter (Zhang et al. 2023b)) are directly prompted for zero-shot VideoQA. We specify the adaptation in Supplementary. Most of these methods do not perform grounding, so we compare Acc@QA.\nTable 3 shows that DeVi, with an accuracy of 70.8%, out-performs the second-best model IG-VLM (Kim et al. 2024) significantly by 6.6%. Moreover, DeVi surpasses a native use of GPT-40 (feed multiple video frames and prompt GPT-40 for question answering) remarkably by 8.2% and a naive"}, {"title": "Ablation Study", "content": "We first conduct an ablation to the 3 major designs in DeVi on DeVE-QA. Table 6 shows that all three components significantly contribute to DeVi's success. Specifically, by substituting the hierarchical event captioning with a normal dense video captioning used in LLoVi (Zhang et al. 2023a), the results in Table 6 show that both QA and GQA accu-racy decline remarkable by 3.9% and 3.6%. Moreover, the ablation comparison in Table 7 demonstrate that without hi-erarchical event captioning strategy, DeVi 's performance on dense events drops apparently (e.g., -4.4% on QA) com-pared to single and double events (e.g., -1.6% and -3.4% on QA). We speculate that this demonstrate its ability of cap-turing specific information from different scales in multi-ple and complicated events. Then, we remove the temporal event contextualization module. The results again degrade by 2.0% on QA and 1.6% on GQA. This is understandable as contextualized captions are rectified with potential mis-"}, {"title": "Implementation Investigation", "content": "Dense Video Event Captioner Table 9 shows that a substi-tution of Video-LLaVA with VideoBLIP deteriorates the ac-curacy by near 4% and 7% for QA with and without ground-ing respectively. We speculate that apart from the larger size"}, {"title": "Conclusion", "content": "In this paper, we proposed to study question answering on dense video events to challenge the MLLMs from three aspects of dense-event captioning, long-form video under-standing, and faithful multimodal reasoning by grounding. We constructed the DeVE-QA dataset with manual efforts and proposed DeVi model. DeVi is a training-free MLLM approach that solves the aforementioned challenges by a set of tailored practices, including hierarchical dense event cap-"}, {"title": "Appendix A: DeVE-QA Dataset Construction", "content": "ActivityNet-Captions dataset (Krishna et al. 2017) is the data source of DeVE-QA. It contains 20k videos amounting to 849 hours with 100k descriptions, each with it's unique start and end timestamps. On average, the captions for each video describe 94.6% of the entire video content (Johnson, Karpathy, and Fei-Fei 2016), demonstrating that each caption annotation could cover the corresponding major events within the video. Furthermore, 10% of the temporal descrip-tions overlap with each other, showing that the events cover simultaneous events. By selecting ActivityNet-Captions as our data source, we first conduct raw data filtering with fil-ter criteria that 1) the descriptions should be more than 10 words, and 2) captions for each video cover at least 95% of the video. Then we perform random sampling over all the ActivityNet-Captions to get the final subset of 10,643 videos and 26,111 captions.\nAutomatic QA Generation\nDuring the generation process, we first perform automated QA generation with dense event captions by prompting GPT-4.0. Specifically, we feed 26,111 event captions of ActivityNet-Captions into GPT-4.0, and prompt it to gener-ate multiple (maximal 3 to limit the cost) different question-answer pairs pertaining to different aspects of a particular event caption.\nDuring the QA generation process, we also perform anal-ysis on one-shot vs. n-shot prompting strategy. To be spe-cific, one-shot strategy prompts once for all N captions It is cost-efficient by sending less tokens to GPT-4. However, the generated questions appear to be of low quality and are often similar to each other. Alternatively, n-shot strategy separately prompts for each caption. It is relatively cost-inefficient compared to one-shot because of the attached prompt, but it significantly improves the generated QA qual-ity. We speculate that N-shot prompting is able to utilize more tailored and content-specific information from each caption for generating questions. Moreover, it is likely that the one-shot prompting generate questions by using the in-formation from all N captions simultaneously, despite these captions being originally intended to be separate entities. quality because it allows for more tailored and context-specific questions for each caption, reducing redundancy and enhancing the diversity and relevance of the generated questions. Considering the quality, we eventually opt for the n-shot prompting strategy.\nDistractor Answers Retrieval\nAfter the QA generation process, question and correspond-ing correct answers are obtained. To curate the distractor answers and form multiple choices, we incorporate the fol-lowing steps: For each question, we first retrieve its Top-10 similar questions and use their correct answers as candidate wrong answers. In particular, the Top-10 similar questions is obtained by the similarity of first 3 words which indi-cate both the question types and the subject of activities. To ensure hard negatives, we additionally filter for video-irrelevant candidate answers. Specifically, for each question"}, {"title": "Appendix B:DeVi Design and Analysis", "content": "Hierarchical Dense Event Captioning\nThe dense events within videos often intertwine and vary in duration, posing a challenge for machines to accurately segment them for captioning. We propose the hierarchical"}]}