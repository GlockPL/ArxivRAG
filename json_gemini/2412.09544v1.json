{"title": "Sail into the Headwind: Alignment via Robust Rewards and Dynamic Labels against Reward Hacking", "authors": ["Paria Rashidinejad", "Yuandong Tian"], "abstract": "Aligning AI systems with human preferences typically suffers from the infamous reward hacking problem, where optimization of an imperfect reward model leads to undesired behaviors. In this paper, we investigate reward hacking in offline preference optimization, which aims to improve an initial model using a preference dataset. We identify two types of reward hacking stemming from statistical fluctuations in the dataset: Type I Reward Hacking due to subpar choices appearing more favorable, and Type II Reward Hacking due to decent choices appearing less favorable. We prove that many (mainstream or theoretical) preference optimization methods suffer from both types of reward hacking. To mitigate Type I Reward Hacking, we propose POWER, a new preference optimization method that combines Guia\u015fu's weighted entropy with a robust reward maximization objective. POWER enjoys finite-sample guarantees under general function approximation, competing with the best covered policy in the data. To mitigate Type II Reward Hacking, we analyze the learning dynamics of preference optimization and develop a novel technique that dynamically updates preference labels toward certain \"stationary labels\", resulting in diminishing gradients for untrustworthy samples. Empirically, POWER with dynamic labels (POWER-DL) consistently outperforms state-of-the-art methods on alignment benchmarks, achieving improvements of up to 13.0 points on AlpacaEval 2.0 and 11.5 points on Arena-Hard over DPO, while also improving or maintaining performance on downstream tasks such as mathematical reasoning. Strong theoretical guarantees and empirical results demonstrate the promise of POWER-DL in mitigating reward hacking.", "sections": [{"title": "Introduction", "content": "Aligning AI systems with human values is a core problem in artificial intelligence (Russell, 2022). After training on vast datasets through self-supervised learning, large language models (LLMs) typically undergo an alignment phase to elicit desired behaviors aligned with human values (Ouyang et al., 2022). A main alignment paradigm involves leveraging datasets of human preferences, with techniques like reinforcement learning from human feedback (Christiano et al., 2017) or preference optimization (Rafailov et al., 2024b). These methods learn an (implicit or explicit) reward model from human preferences, which guides the decision-making process of the AI system. This paradigm has been instrumental in today's powerful chat models (Achiam et al., 2023; Dubey et al., 2024).\nHowever, these alignment techniques are observed to suffer from the notorious reward hacking problem (Amodei et al., 2016; Tien et al., 2022; Gao et al., 2023; Casper et al., 2023), where optimizing imperfect learned reward leads to poor performance under the true reward assuming an underlying true reward exists (Skalse et al., 2022). One primary cause of the discrepancy between the learned and true rewards arises because preference data do not encompass all conceivable choices, making the learned reward model vulnerable to significant statistical fluctuations in areas with sparse data. Consequently, the AI system might be swayed toward choices that only appear favorable under the learned reward but are, in reality, subpar, or the system might be deterred from truly desirable choices that do not seem favorable according to the learned rewards.\nIn this paper, we investigate reward hacking in offline preference optimization, in which we are provided with an initial AI system (initial model) and a preference dataset. We do not assume that the preference dataset is necessarily constructed through sampling from the initial model, allowing to leverage existing datasets"}, {"title": "Background and Problem Formulation", "content": "2.1 Learning from Human Preference\nContextual bandit formulation. We adopt the contextual bandits formulation described by a tuple $(X, Y, r)$, where $X$ is the space of contexts (e.g., prompts), $Y$ is the space of actions (e.g., responses), and $r: X \\times Y \\rightarrow \\mathbb{R}$ is a scalar reward function. A stochastic policy (e.g., model or language model) $\\pi: X \\rightarrow \\Delta(V)$ takes in a context $x \\in X$ and outputs an action according to $y \\sim \\pi(\\cdot|x)$. We denote the set of all stochastic policies by $\\Pi := {\\pi: X \\rightarrow \\Delta(V)}$.\nPerformance metric. We assume that there exists an underlying (unknown) true reward function $r^*: X \\times Y \\rightarrow \\mathbb{R}$. Given the true reward function $r^*$ and a target distribution over contexts $x \\sim p(\\cdot)$, performance of a policy $\\pi$ is the expected true reward over contexts and actions\n$J(\\pi) := \\mathbb{E}_{x\\sim p, y\\sim \\pi(\\cdot|x)} [r^*(x, y)] .$$\\newline$ (1)\n\nThe Bradley-Terry model of human preferences. Consider a prompt $x \\in X$ and a pair of responses $y^0, y^1 \\in Y$. For any reward function $r$, the Bradley-Terry (BT) model characterizes the probability of preferring $y^1$ over $y^0$, denoted by $l = 1$, according to:\n$\\text{Pr}(l = 1 | x, y^1, y^0) = \\sigma \\big(r(x, y^1) - r(x, y^0)\\big),$$\\newline$(2)$\nwhere $\\sigma(z) := 1/(1 + \\text{exp}(-z))$ is the sigmoid function.\nOffline preference optimization. We consider an offline learning setup, where we start from an initial reference policy (model), denoted by $\\pi_{\\theta_0} = \\pi_{\\text{ref}}$, and an offline pairwise preference dataset $D = {(x, y^0, y^1, l)}$, comprising of $N$ iid samples. Prompt and response pairs are sampled according to a data distribution: $x, y^0, y^1 \\sim \\mu$, and preferences label is sampled according to the BT model corresponding to true rewards: $l \\sim Pr^*({\\cdot}|x, y^1, y^0)$. Importantly, we do not assume that the preference dataset is necessarily constructed through sampling from the initial model. To simplify notation, we define $y^+ = ly^1 + (1 - l)y^0$ and $y^- = (1 - l)y^1 + ly^0$ to denote the chosen and rejected responses in the dataset, respectively. Appendix A presents additional notation."}, {"title": "Direct Preference Optimization", "content": "A classical approach to learning from human preferences involves learning a reward model from dataset, followed by finding a policy through maximizing the learned reward typically regularized with a (reverse) KL-divergence to keep the learned policy closed to initial policy:\n$\\begin{aligned} \\hat{r} &\\in \\text{argmin}_r L_{\\text{BT}}(r) := -\\mathbb{E}_D [\\text{log} \\sigma \\big(r(x, y^+) - r(x, y^-)\\big)] \\\\  \\pi &\\in \\text{argmax}  \\mathbb{E}_{x\\sim p, y\\sim \\pi} [r(x, y)] - \\beta D_{\\text{KL}} [\\pi||\\pi_{\\text{ref}}], \\end{aligned}$$\\newline$(3)$\n\nHere, $D_{\\text{KL}} [\\pi||\\pi_{\\text{ref}}] := \\mathbb{E}_{x\\sim \\rho} [D_{\\text{KL}}[\\pi(\\cdot|x)||\\pi_{\\text{ref}}(\\cdot|x)]]$ and $L_{\\text{BT}}(r)$ is the negative log-likelihood according to the BT model. Rafailov et al. (2024b) observed that the policy maximization step in (3) can be computed in closed form and thus simplified the two-step process into a single minimization objective. This method is called direct preference optimization (DPO) and has inspired a series of works; see Tables 1 and 3 for several examples.\nSome representative variants of DPO that we theoretically analyze are IPO (Azar et al., 2024), which applies a nonlinear transformation to preferences to reduce overfitting, and SimPO (Meng et al., 2024), which removes the reference policy from the DPO objective. We also analyze two recent theoretical methods that come with finite-sample guarantees and aim at mitigating overoptimization: XPO (Huang et al., 2024), which replaces the KL divergence in DPO with a stronger $X^2$+KL divergence, and DPO+SFT (Liu et al., 2024; Cen et al., 2024), which adds a supervised finetuning term that increases log-likelihood of chosen responses in the preference dataset."}, {"title": "Reward Hacking in Preference Optimization", "content": "In this section, we investigate reward hacking in preference optimization. One driver of reward hacking is statistical errors present in the dataset. Typically, preference datasets suffer from partial coverage, lacking extensive samples across all possible options. As a result, preferences for poorly covered choices are subject to high levels of statistical fluctuations, given the fact that preference labels are Bernoulli random variables with probabilities described by the Bradley-Terry model (2). Subsequently, we describe two types of reward hacking, both originating from the presence of poorly covered choices (actions) in the dataset.\n3.1 Type I Reward Hacking\nType I Reward Hacking occurs when poorly covered, subpar choices in the dataset appear more favorable due to statistical errors, and that leads to a learned policy with a low expected true reward J(). In the following proposition, we prove that even in the favorable scenario that the high-reward actions are well-covered in the dataset, the existence of a single sample on a low-reward action can overwhelm many preference optimization algorithms, causing them to learning highly suboptimal policies."}, {"title": "Type I Reward Hacking in PO", "content": "Proposition 1 (Type I Reward Hacking in PO). Consider multi-armed bandits with bounded rewards $r^*(a) \\in [0, 1]$ and the softmax policy class, defined as\n$\\Pi_\\theta := {\\pi_\\theta(y) = \\text{exp}(\\theta(y))/Z_\\theta \\text{    } Z_\\theta = \\sum_y \\text{exp}(\\theta(y)), \\text{    } \\theta(y) \\in [0, 1]}$.$\\newline$ (4)\n\nDefine the best-in-class policy $\\pi_{\\theta^*} = \\text{argmax}_{\\pi \\in \\Pi_\\theta} J(\\pi)$. There exist three-armed bandit instances with $\\Pi_\\theta$ parameterization, high coverage of the optimal arms $\\mu(a \\in \\text{argmax}_a r^*(a)) > 1/2$, and bounded KL-divergence $D_{\\text{KL}}(\\pi_{\\theta^*} | \\Pi_{\\text{ref}})$, such that for any $N \\geq 2$, $\\beta > 0$, $\\gamma, \\tau > 0$, policy $\\hat{\\pi} \\in {\\hat{\\pi}_{\\text{ADPO}}, \\hat{\\pi}_{\\text{IPO}}, \\hat{\\pi}_{\\text{SimPO}}}$ or $\\pi = \\hat{\\pi}_{\\text{XPO}}$ for $0 < \\beta < 1/3$, suffers from a constant suboptimality $J(\\pi_{\\theta^*}) - J(\\hat{\\pi}) > 0.15$ with a constant probability of at least $(e(1+e))^{-1}$.\nWe defer the proof of the above proposition to Appendix C.1 and offer some intuition here. Figure 1(a) illustrates a failure instance, where the preference data has high coverage over the high-reward choice A but poor coverage on the low-reward choice C. Due to the Bradley-Terry model and the stochastic nature of human preferences, regions with poor coverage are prone to high statistical errors. Consequently, the low-reward choice C might be marked as preferred purely by chance.\u00b9 Proposition 1 demonstrates that algorithms such as DPO and SimPO overfit the untrustworthy preferences, as their objectives aim at increasing the parameter gap $\\theta(C) - \\theta(A)$, despite the preference being untrustworthy due to inadequate coverage. This can ultimately lead to a final policy that places significant weight on poor choices.\nType I Reward Hacking and the failure result in Proposition 1 are closely connected to the challenge of partial data coverage in offline RL (Levine et al., 2020), which can be robustly addressed through the principle of pessimism in the face of uncertainty. Pessimism can be applied in various ways such as reducing the rewards (values) of poorly covered actions (Kumar et al., 2020; Cheng et al., 2022) or keeping the learned policy close to data collection policy (Nachum et al., 2019). Although divergence-based methods DPO, IPO, and XPO aim at keeping the learned policy close to the initial policy, Proposition 1 shows that maintaining a small divergence from initial model does not induce a sufficient amount of pessimism to prevent Type I Reward Hacking.\u00b2\nRemark 1 (Comparison with previous theoretical results on failure of DPO). Failure result in Proposition 1 is rigorous and constructed under a realistic setting close to practice: the policy class is a softmax with bounded rewards and the KL divergence between initial and best-in-class policy is bounded. This makes Proposition 1 stronger than prior arguments on overoptimization in DPO, which rely on unbounded rewards (Azar et al., 2024), updates to model parameters despite receiving no samples (hence, conclusion breaking in gradient-based optimization) (Huang et al., 2024), or events with probabilities approaching zero (Song et al., 2024)."}, {"title": "Type II Reward Hacking", "content": "Type II Reward Hacking can occur when poorly covered, good choices in the dataset appear to be less favorable than their true value due to statistical errors, leading to the deterioration of the initial model after preference optimization. In the following proposition, we prove that many preference optimization methods are susceptible to Type II Reward Hacking.\nProposition 2 (Type II Reward Hacking in +PO). Consider the multi-armed bandits setting with the softmax policy class $\\Pi_\\theta$, as defined in (4). Let $\\pi_{\\theta^*} = \\text{max}_{\\pi \\in \\Pi_\\theta} J(\\pi)$ represent the best-in-class policy. There exists a three-armed bandit problem with $\\Pi_\\theta$ parameterization and $\\pi_{\\theta_0} = \\pi_{\\theta^*}$, such that for any $N > 3$, $\\beta > 0$, $\\eta \\geq 0$, $\\gamma$ and policy $\\hat{\\pi} \\in {\\hat{\\pi}_{\\text{DPO}}, \\hat{\\pi}_{\\text{DPO+SFT}}, \\hat{\\pi}_{\\text{SimPO}}}$ or $\\pi \\in {\\hat{\\pi}_{\\text{XPO}}, \\hat{\\pi}_{\\text{IPO}}}$ for $0 < \\beta, \\tau < 1$, the following holds with a constant probability of at least $(e(1+e))^{-1}$:\n$J(\\pi_{\\theta^*}) - J(\\hat{\\pi}) > 0.1.$\nProof of the above proposition can be found in Appendix C.2. An example of this type of reward hacking is illustrated in Figure 1(b), where the initial policy has a high probability on the high-reward choice C."}, {"title": "Against Type I Reward Hacking: Weighted Entropy Robust Rewards", "content": "We demonstrated that approaches involving divergence minimization remain vulnerable to reward hacking. Moreover, maintaining a small divergence can inadvertently lead to underoptimizing the preference dataset, as it may risk overlooking policies that, although well-covered, deviate significantly from the initial policy. Simply reducing $\\beta$ to alleviate underoptimization may not always be viable. For example, reducing $\\beta$ may reduce underopti-mization in one state while inadvertently amplify overoptimization in another state.\nThese reasons motivate us to explore an alternative route and consider regularizing the reward maximization objective with the concept of weighted entropy.\nDefinition 1 (Weighted Entropy; Guia\u015fu (1971)). The weighted entropy of a (discrete) distribution $p(\\cdot)$ with non-negative weights $w(y)$ is defined as $H_w(p) := - \\sum_y w(y)p(y) \\text{log} p(y)$.\nWeighted entropy extends Shannon's entropy by incorporating weights associated with each outcome, reflecting attributes such as favorableness or utility toward a specific goal (Guia\u015fu, 1971). Building on this, we consider a weighted-entropy reward (WER) maximization objective:\n$\\text{max}_{\\pi \\in \\Pi}  \\mathbb{E}_{x \\sim p, y \\sim \\pi} [r(x, y)] + \\beta H_\\omega(\\pi),$$\\newline$ (5)\n\nwhere $H_\\omega(\\pi) := \\mathbb{E}_{x\\sim \\rho}[H_w(\\pi(\\cdot|x))]$. This objective expresses the principle of maximum (weighted) entropy (Jaynes, 1957; Guiasu and Shenitzer, 1985), promoting the selection of policies with maximum entropy-thus favoring the most uniform or unbiased policies among those compatible with the constraints, such as achieving high rewards. Objective (5) extends the well-established maximum entropy framework in RL, used in various settings such as exploration (Haarnoja et al., 2018), inverse RL (Ziebart et al., 2008), and robust RL (Eysenbach and Levine, 2022).\n4.2 POWER: Preference Optimization with Weighted Entropy Robust Rewards\nTo mitigate reward hacking, we integrate the WER objective (5) with a robust (adversarial) reward framework, inspired by favorable theoretical guarantees (Liu et al., 2024; Cen et al., 2024; Fisch et al., 2024). Specifically, we find a policy that maximizes WER (5) against an adversarial reward, which seeks to minimize WER while fitting the preference dataset:\n$\\text{max}_{\\pi \\in \\Pi}  \\text{min}_{r \\in \\mathcal{R}}  L_{\\text{BT}}(r) + \\eta \\bigg( \\mathbb{E}_{x\\sim p, y\\sim \\pi} [r(x, y)] -  [\\mathbb{E}_{x\\sim p, y'\\sim \\pi'} [r(x, y')] + \\beta H_w(\\pi) ] \\bigg),$$\\newline$(6)$\n\nwhere $\\eta \\geq 0$, $\\mathcal{R}$ is a reward function class, and $L_{\\text{BT}}(r)$ is the negative log-likelihood of the BT model. We subtracted a baseline $\\mathbb{E}_{x\\sim \\rho, y'\\sim \\pi'} [r(x, y')]$ that computes the average reward with respect to some policy $\\pi'$, as the preference data under the BT model only reveal information on reward differences (Zhan et al., 2023a). This baseline plays a crucial role in simplifying the objective and establishing finite-sample guarantees, and we subsequently discuss reasonable choices for $\\pi'$."}, {"title": "POWER Objective", "content": "Under some regularity conditions (detailed in Appendix D.2), the objective (6) can be equivalently expressed as a minimax problem, which leads to a single-step preference objective presented in the proposition below; see Appendix D.3 for the derivation and proof.\nProposition 3 (POWER Objective). Let $w(y) > 0$ denote the weights in the weighted entropy $H_w(\\pi)$ and $\\pi_r$ denote the policy that maximizes the objective (5). Under certain regularity conditions on $\\mathcal{R}$ (Assumption 1) and for any $\\beta > 0$, solving the maximin objective (6) is equivalent to solving the following optimization problem:\n$\\text{min}  L_{\\text{BT}} \\big( \\beta (w(y) \\text{log} \\pi_r (y|x) + w(y)) \\big) - \\eta [\\mathbb{E}_{x\\sim p, y'\\sim \\pi'} [\\beta w(y') \\text{log} \\pi_r (y'|x)]  ] .$\\newline$(7)$\n\nWe call the above objective preference optimization via weighted entropy robust rewards, or POWER. The first term in the above objective is the Bradley-Terry loss with rewards set to $w(y) \\text{log} \\pi_r(y|x) + w(y)$, resulting in a reward gap expressed as a weighted difference of log probabilities of chosen and rejected responses. The second expectation is a weighted negative log-likelihood (a.k.a. supervised fine-tuning, or SFT) regularizer over the baseline policy $\\pi'$.\nRemark 2. Liu et al. (2024) propose an adversarial objective similar to (6) that uses KL divergence instead of weighted entropy. From a theoretical perspective, our approach with weighted entropy improves over this work, such as through mitigating underoptimization; see Section 4.3 for details. Moreover, our final algorithm presented in Algorithm 1 is considerably different from the DPO+SFT objective (Liu et al., 2024; Pal et al., 2024) and significantly outperforms empirically as shown in Section 6."}, {"title": "Finite-Sample Guarantees and Theoretical Benefits of POWER", "content": "The following theorem shows that POWER enjoys finite-sample guarantees on performance.\nTheorem 1 (Finite-Sample Performance Guarantees of POWER). Given a competing policy $\\pi \\in \\Pi$, assume a bounded concentrability coefficient $C'(\\mathcal{R}, \\pi') < \\infty$ as defined in Definition 2. Furthermore, assume realizability of the true reward function $r^* \\in \\mathcal{R}$, boundedness of rewards $\\forall r \\in \\mathcal{R}: r(x, y) \\in [0, R]$, and that the reward function class has a finite $\\epsilon$-covering number $N_\\epsilon$ under the infinity norm. Define $\\hat{R} := 1 + \\text{exp}(R)$, $\\epsilon = (\\hat{R} N)^{-1}$, $\\iota = \\sqrt{\\text{log}(N_\\epsilon)}/\\delta$. Set $\\eta = 1/(\\hat{R}^2 \\sqrt{N})$ and $\\beta = 1/\\sqrt{N}$ in the objective (6). Then with a probability of at least $1 - \\delta$, policy $\\hat{\\pi}_{\\text{POWER}}$ that solves (6) satisfies the following\n$J(\\hat{\\pi}_{\\text{POWER}}) \\geq J(\\pi) -  \\frac{1}{\\sqrt{N}} \\bigg( (\\iota C(\\mathcal{R}, \\pi'))^2 + 1\\bigg) \\hat{R}^2 \\iota + H_\\omega(\\pi) \\bigg).$\\newline\nFurthermore, let L denote the maximum response length. Selecting $w(y) = 1/|y|$ to be the inverse response length, one has\n$J(\\hat{\\pi}_{\\text{POWER}}) \\geq J(\\pi) -  \\frac{1}{\\sqrt{N}} \\bigg( (\\iota C(\\mathcal{R}, \\pi'))^2 + 1\\bigg) \\hat{R}^2 \\iota + \\text{log}(L|V|) \\bigg).$\\newline\nProof of the above theorem can be found in Appendix E.2. Below, we discuss the implications of the above theorem and the guidelines it offers for practical choices.\nGuarantees against Type I Reward Hacking. Theorem 1 shows that the policy learned by POWER competes with the best policy covered in the dataset, where the notion of coverage is characterized by single-policy concentrability, considered the gold standard in offline RL (Rashidinejad et al., 2021; Xie et al., 2021; Zhan et al., 2023a). This implies that as long as a favorable policy is covered in the preference data, POWER is robust to existence of poorly covered, subpar policies and thus mitigates Type I Reward Hacking. Moreover, as Theorem 1 does not impose a parametric form on the class $\\mathcal{R}$, guarantees hold for general function classes.\nBenefits of weighted entropy and choice of weights. A non-zero weighted entropy term ($\\beta > 0$) is essential in obtaining the one-step optimization problem in (7) and establishing its equivalence to the maximin problem, as this term induces strict concavity in the objective (5). Moreover, using KL-regularization leads to rates that grow with the divergence of competing policy and initial policy (Liu et al., 2024), which can be large or even unbounded. However, weighted entropy ensures bounded rates and thus mitigates underoptimization."}, {"title": "POWER Faced with Hard Instances and the Role of Partition Function", "content": "In the following proposition, we analyze the POWER objective (8) in the hard reward hacking instances of Proposition 1 and Proposition 2. Proof is presented in Appendix C.3.\nProposition 4. (I) Consider the three-armed bandit instance in Proposition 1. Then, for any $\\beta > 0$ and $\\eta > \\frac{(2+\\epsilon)}{N}$, POWER policy $\\hat{\\pi}_{\\text{POWER}}$ that solves the objective (8) is the best-in-class policy: $\\hat{\\pi}_{\\text{POWER}} = \\pi_{\\theta^*}$. (II) Consider the three-armed bandit instance in Proposition 2. Then, for any $\\beta > 0$, $\\eta \\geq 0$, policy $\\hat{\\pi}_{\\text{POWER}}$ that solves the objective (8) suffers from a constant suboptimality $J(\\pi_{\\theta^*}) - J(\\hat{\\pi}_{\\text{POWER}}) > 0.2$.\nProposition 4 confirms that POWER robustly (for any $\\beta > 0$ and $\\eta \\geq 1/N$) prevents Type I Reward Hacking in the hard instance of Proposition 1, where other preference optimization algorithms DPO, SimPO, IPO, and XPO fail. Yet, the above proposition shows that POWER suffers from Type II Reward Hacking, which the design dynamic labels in the following section."}, {"title": "Against Type II Reward Hacking: Dynamic Labels", "content": "We now turn our focus to mitigating Type II Reward Hacking, based on the following intuition: keeping the model's internal preferences close to initialization in the low-coverage regions (trust the preference labels less) while learning from the data in the high-coverage regions (trust the preference labels more).\nFor this purpose, we analyze the learning dynamics of preference optimization in the bandits setting with softmax parameterization $\\pi_{\\theta}(y) \\propto \\text{exp}(\\theta(y))$. We denote the dataset by $D = {(y^0, y^1, l)}$ with labels $l \\sim Pr^*({\\cdot}|y^0, y^1)$. We use $\\mu_{\\theta,1>0}$ to indicate the empirical probability of comparing $y^0$ with $y^1$, and $1-\\mu_{\\theta,1>0}$ the empirical probability of preferring $y^1$ over $y^0$. To simplify presentation, we consider POWER with $w(y) = 1$, $\\eta = 0$, $\\beta = 1$; a similar analysis can be extended to other objectives.\nReverse engineering label updates based on learning dynamics. Rather than using static preference labels, we allow the labels $l_t$ to evolve across gradient updates. Denote the parameter gap corresponding to two actions $y^1, y^0$ by $d_{\\theta_t}(y^1, y^0) := \\theta_t(y^1) - \\theta_t(y^0)$. We show in Appendix F.1 that isolated (batch) gradient updates on"}, {"title": "POWER with Dynamic Labels", "content": "POWER with Dynamic Labels. Our final algorithm POWER-DL (Algorithm 1) integrates the POWER objective with dynamic labels against reward hacking. In untrustworthy regions, POWER-DL interpolates between the initial model and robust rewards, allowing to trade off the two types of reward hacking through adjusting conservatism parameters $\\eta$ and $\\gamma$, reflecting relative quality of the initial model compared to preference data and up to removing conservatism completely by setting $\\eta = \\gamma = 0$. We highlight the fact that divergence-based methods aim at keeping the learned model close to the initial model wherever the learned model has a decent"}, {"title": "Experiments", "content": "We conduct experiments to assess different preference optimization methods on aligning LLMs across four settings, varying in dataset size and level of distribution shift between the initial model and data. We follow two pipelines: Helpsteer2 (Wang et al., 2024e), which employs smaller datasets, and Zephyr (Tunstall et al., 2023) with significantly larger datasets. We implement two distinct setups similar to Meng et al. (2024): the base setup that uses an existing preference dataset and the instruct setup that constructs a preference dataset by sampling from the initial model. These two setups allow evaluating across different levels of distribution shift between the initial model and preference data.\nHelpsteer2 setups. In the base setup, we train Llama-3-8B on the OpenAssistant2 dataset (K\u00f6pf et al., 2024) to create the initial model. We conduct preference optimization using the Helpsteer2 dataset (Wang et al., 2024e), selecting responses based on helpfulness scores and discarding ties, yielding about 7K samples. In the instruct setup, we use Llama-3-8B-Instruct as the initial model and generate a preference dataset from Helpsteer2 prompts. Following Wang et al. (2024e), we generate 10 responses per prompt with temperature 0.7. We then score them with Armo reward model (Wang et al., 2024c) and select the highest and lowest score responses as $y^+$ and $y^-$, respectively.\nZephyr setups. In the base setup, we obtain the initial model by training Llama-3-8B base model on the UltraChat-200K dataset (Ding et al., 2023). We then perform preference optimization on the UltraFeedback dataset (Cui et al., 2024), comprising approximately 61K samples. In the instruct setup and following Meng et al. (2024), we start from Llama-3-8B-Instruct and generate 5 responses with temperature 0.8 per prompt in the UltraFeedback dataset. As before, the highest and lowest score responses are selected as preference response pairs.\nEvaluation benchmarks. We primarily assess preference methods by evaluating the trained models on standard instruction-following benchmarks: AlpacaEval 2.0 (Li et al., 2023a; Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which evaluate the quality of the model responses. Following standard guidelines, for Arena-Hard, we report the win rate (WR) of the model's responses against responses from GPT-4-Turbo. For AlpacaEval 2.0, in addition to the WR against GPT-4-Turbo, we report the length-controlled (LC) win rate, designed to mitigate bias toward verbosity. We further evaluate the performance of models on MT-Bench (Zheng et al., 2023) and downstream tasks such as mathematics, reasoning, truthfulness, and instruction-following (Beeching et al., 2023).\nPreference optimization methods. We compare POWER-DL against various baselines; see Appendix H.1 for details. These include divergence-base methods DPO (Rafailov et al., 2024b), IPO (Azar et al., 2024), offline SPPO (Wu et al., 2024b), and XPO (Huang et al., 2024), along with robust variants such as conservative DPO (CDPO) (Mitchell, 2024), robust preference optimization (ROPO) (Liang et al., 2024), R-DPO (Park et al., 2024), and DPO+SFT (Pal et al., 2024; Liu et al., 2024). We also evaluate against reference-free methods CPO (Xu et al., 2024a), SLiC-HF (Zhao et al., 2023), RRHF (Yuan et al., 2024a), ORPO (Hong et al., 2024), and SimPO (Meng et al., 2024)."}, {"title": "Benchmark Results", "content": "POWER-DL outperforms SoTA methods on alignment benchmarks. Table 2 presents the results on alignment benchmarks. POWER-DL consistently outperforms other methods in both Helpsteer2 and Zephyr pipelines and across base and instruct settings. These improvements can largely be attributed to the integration of weighted entropy, which effectively counters underoptimization, and mitigation of reward hacking. Notably, POWER-DL surpasses other robust methods such as cDPO and ROPO demonstrating its efficacy in handling poorly covered samples. Additionally, POWER-DL improvements are more pronounced in the base setting,"}, {"title": "Discussion", "content": "We studied reward hacking in offline preference optimization. We identified two types of reward hacking stemming from statistical fluctuations in preference data. We demonstrated that many existing methods are vulnerable to both types of reward hacking, despite maintaining a small divergence from the initial model. To mitigate reward hacking, we introduced POWER-DL, a practical algorithm based on a weighted entropy robust reward framework augmented with dynamic preference labels. POWER-DL enjoys theoretical guarantees and achieves strong empirical performance. Future research directions include applications of dynamic labels to out-of-distribution robustness and investigating the interplay between statistical errors and reward misspecification in reward hacking."}, {"title": "Proof of Theorem 2", "content": "The proof is organized as follows. We start by establishing a lower bound on the dynamic labels. We subsequently use this lower bound to prove bounds on the parameter gap in low-coverage and high-coverage cases separately.\nLower bound on dynamic labels. The dynamics of labels are described by the following equation:\n$\\dot{l_t"}, "gamma \\left(\\frac{\\sigma(d_t) - \\mu_{1>0}}{\\mu_{1>0}} - l_t \\right).$\\newline$ (44)\n\nWithout loss of generality, we assumed that $\\mu_{1>0} > 1/2$. This condition is easily met by appropriately ordering the responses. Define:\n$\\kappa := \\frac{1 - \\mu_{1>0}}{\\mu_{1>0} - (1 - \\mu_{1>0})}.$\\newline$ (45)$\n\nGiven that $0 < \\sigma(d_t) < 1$ and the assumption $\\mu_{1>0} - (1 - \\mu_{1>0}) = 2\\mu_{1>0} - 1 > 0$, we find the following lower bound on $\\dot{l_t}$:\n$\\dot{l_t} = \\gamma \\left(  \\frac{\\sigma(d_t) - (1-\\mu_{1>0})}{\\mu_{1>0}} - l_t \\right) \\geq \\left( \\frac{-\\mu_{1>0} - (1-\\mu_{1>0})}{\\mu_{1>0}} -l_t \\right) = \\gamma (-\\kappa -l_t)$.\n\nThe subsequent lemma establishes a lower bound on $l_t$ using Gr\u00f6nwall's inequality, with its proof provided at the end of this section.\nLemma 4. Suppose that $l_t$ satisfies"]}