{"title": "MMGDreamer: Mixed-Modality Graph for Geometry-Controllable 3D Indoor Scene Generation", "authors": ["Zhifei Yang", "Keyang Lu", "Chao Zhang", "Jiaxing Qi", "Hanqi Jiang", "Ruifei Ma", "Shenglin Yin", "Yifan Xu", "Mingzhe Xing", "Zhen Xiao", "Jieyi Long", "Xiangde Liu", "Guangyao Zhai"], "abstract": "Controllable 3D scene generation has extensive applications in virtual reality and interior design, where the generated scenes should exhibit high levels of realism and controllability in terms of geometry. Scene graphs provide a suitable data representation that facilitates these applications. However, current graph-based methods for scene generation are constrained to text-based inputs and exhibit insufficient adaptability to flexible user inputs, hindering the ability to precisely control object geometry. To address this issue, we propose MMGDreamer, a dual-branch diffusion model for scene generation that incorporates a novel Mixed-Modality Graph, visual enhancement module, and relation predictor. The mixed-modality graph allows object nodes to integrate textual and visual modalities, with optional relationships between nodes. It enhances adaptability to flexible user inputs and enables meticulous control over the geometry of objects in the generated scenes. The visual enhancement module enriches the visual fidelity of text-only nodes by constructing visual representations using text embeddings. Furthermore, our relation predictor leverages node representations to infer absent relationships between nodes, resulting in more coherent scene layouts. Extensive experimental results demonstrate that MMGDreamer exhibits superior control of object geometry, achieving state-of-the-art scene generation performance.", "sections": [{"title": "Introduction", "content": "Deep generative models have initiated a new era of artificial intelligence-generated content, driving developments in natural language generation (Zheng et al. 2024), video synthesis (Liao et al. 2024a), and 3D generation (Poole et al. 2022).\nControllable Scene Generation refers to generating realistic 3D scenes based on input prompts, allowing for precise control and adjustment of specific objects within those scenes. It is widely applied in Virtual Reality (Bautista et al. 2022), Interior Design (\u00c7elen et al. 2024), and Embodied Intelligence(Yang et al. 2024; Zhai et al. 2024a), providing immersive experiences and enhancing decision-making processes. Within these applications, scene graphs serve as a powerful tool by succinctly abstracting the scene context and interrelations between objects, enabling intuitive scene manipulation and generation (Dhamo et al. 2021).\nDespite the significant progress made by retrieval-based (Lin and Mu 2024), semi-generative (Ren et al. 2024), and fully-generative (Zhai et al. 2024c) methods in graph-based controllable scene generation, these approaches predominantly rely on textual descriptions to construct input scene graphs. While text serves as a high-level representation encapsulating rich semantic information, it falls short in accurately describing the geometry of objects in the generated scenes, resulting in inadequate geometric control over the generated objects (Rombach et al. 2022). Moreover, each node in the scene graph contains only textual information about object category, which limits its adaptability to flexible user input. To address these limitations, we introduce MMGDreamer, a dual-branch diffusion model designed for processing multimodal information, incorporating a novel Mixed-Modality Graph (MMG) as a key component. As depicted in Fig. 1, the node of MMG can be represented in three ways: text, image, or a combination of both. Additionally, edges between nodes can be selectively provided or omitted based on user input. This flexible graph structure supports five types of user input, as illustrated in Fig. 1.A, significantly enhancing adaptability to diverse user demands and enabling precise control over object geometry in generated scenes.\nTo fully leverage the capabilities of MMG, MMG-Dreamer features two pivotal modules: the visual enhancement module and the relation predictor. When nodes of the input scene graph contain solely textual information, the visual enhancement module employs text embeddings to construct visual representations of these nodes. By incorporating visual priors associated with the text, this approach enriches the visual content of nodes, enhancing geometric control over the generated objects. The relation predictor, a relationship classifier based on the GCN, leverages prior knowledge and node representations within the scene to infer relationships between nodes in the absence of explicit relational information. By capturing global and local scene-object relationships, this module ensures the generation of more coherent and contextually appropriate scene layouts.\nWe briefly summarize our primary contributions as follows:\n\u2022 We introduce a novel Mixed-Modality Graph, where nodes can selectively incorporate textual and visual modalities, allowing for precise control over the object geometry of the generated scenes and more effectively accommodating flexible user inputs.\n\u2022 We present MMGDreamer, a dual-branch diffusion model for scene generation based on Mixed-Modality Graph, which incorporates two key modules: a visual enhancement module and a relation predictor, dedicated to construct node visual features and predict relations between nodes, respectively.\n\u2022 Extensive experiments on the SG-FRONT dataset demonstrate that MMGDreamer attains higher fidelity and geometric controllability, and achieves state-of-the-art performance in scene synthesis, outperforming existing methods by a large margin."}, {"title": "Related Work", "content": "Scene Graph. Scene graph provides a structured and hierarchical representation of complex scenes by using nodes (objects) and edges (relationships) (Zhou, While, and Kalogerakis 2019). Following their introduction, subsequent works have refined hierarchical scene graph (Rosinol et al. 2020) and focused on predicting local inter-object relationships (Koch et al. 2024; Liao et al. 2024b). Such advancements have driven the widespread application of scene graphs across both 2D and 3D domains, enabling sophisticated tasks such as image synthesis (Johnson, Gupta, and Fei-Fei 2018; Wu, Wei, and Lin 2023) and caption generation (Basioti et al. 2024) in 2D, as well as video synthesis (Cong et al. 2023), 3D scene understanding (Wald et al. 2020) and scene synthesis (Para et al. 2021) in 3D. However, in the current 3D indoor scene synthesis tasks, scene graphs are predominantly derived from text input by users (Strader et al. 2024). We propose the MMG, which more effectively accommodates flexible user inputs.\n3D Scene Generation. 3D scene generation is an area of ongoing research that focuses on developing plausible layouts (Engelmann et al. 2021) and generating accurate object shapes (Xu et al. 2023). A substantial body of contemporary research synthesizes scenes from text (Fang et al. 2023), panorama (Wang et al. 2023; Hara and Harada 2024),"}, {"title": "Preliminary", "content": "Scene Graph Representation. Scene graph can be formally defined as a directed graph G = {V, R}, where V = {vi | i \u2208 {1, ..., N}} represents the set of nodes (objects in the scene) and R = {ri\u2192j|i, j\u2208 {1,...,N},i \u2260 j} represents the set of edges (relationships between objects). The node vi represents an object in the scene and contains information about the object's category. The edges ri\u2192j define the connections between objects, which can include spatial relationships (e.g., front/behind) or attribute relationships (e.g., same style).\nScene Graph Encoder. Graph Convolutional Network (GCN) (Johnson, Gupta, and Fei-Fei 2018) facilitates the processing of graph-structured data by learning node representations through the layer-by-layer aggregation of features from neighboring nodes. In our work, we utilize the Triplet Graph Convolutional Network (Triplet-GCN) as scene graph encoder to process the scene graph, assuming that the initial node and edge attribute features are given by (dvi (80), \u03b4(0), \u03b4(0)). Specifically, each layer of GCN updates the node and edge representations according to the following formula:\n$(V, 8_{U_j}^{l+1}) = MLP1(\u03b4\u03c5, \u03b4\u03b4)$, (1)\n$8_{Vi}^{l+1} = Y + MLP2 (Avg( | vj \u2208 NG(vi)))$, (2)\nwhere l\u2208 {1,..., L - 1} denotes an independent layer within the Triplet-GCN, NG(vi) represents the set of all neighboring nodes of vi, Avg indicates the use of average pooling operation, and MLP\u2081 and MLP2 refer to the Multi-Layer Perceptron (MLP) layers.\nLatent Diffusion Model. Latent Diffusion Model (LDM) (Rombach et al. 2022) generally involves two Markov processes: a forward process that incrementally corrupts the data and a reverse process that progressively denoises it. Given a sample xo, the LDM first employs a pre-trained VQ-VAE (Van Den Oord, Vinyals et al. 2017) to encode xo into a reduced-dimensional latent representation 20. The forward process is defined by:\nq(zt | zt\u22121) = N(zt; \u221a1 \u2013 \u1e9etzt\u22121, \u03b2tI), (3)"}, {"title": "Method", "content": "We propose MMGDreamer, a framework adept at handling MMG as input for indoor scene synthesis tasks, as illustrated in Fig. 2. The MMG is a novel graph structure where nodes can optionally carry textual or visual information, thereby more effectively accommodating flexible user inputs. MMGDreamer first utilizes CLIP and an embedding layer to encode the MMG, producing the Latent Mixed-Modality Graph (LMMG). We then apply the visual enhancement module to construct visual information in the nodes of the LMMG, yielding a Visual-Enhanced Graph. Subsequently, a Relation Predictor is utilized to predict the missing edges between nodes, forming the Mix-Enhanced Graph. Finally, we model the relations within the scene using the Graph Encoder and employ a dual-branch diffusion model to generate the corresponding layout and shape, synthesizing the 3D indoor scene.\nMixed-Modality Graph. Generating fine-grained scenes using only text information is insufficient, as it cannot precisely control the geometry of generated objects. At the same time, users' flexible input should be multimodal, allowing for the selective input of text or images based on specific needs, as shown in Fig. 1.\u0391. However, existing methods (Hu et al. 2024) do not support this input format. Graphs, as a compact and flexible structural representation, enable the effective encoding of diverse attributes within nodes, facilitating the seamless integration of multimodal information. Furthermore, users' textual descriptions often lack information about the relationships between all objects. While methods such as EchoScene(Zhai et al. 2024b) and CommoScenes (Zhai et al. 2024c) utilize graphs to generate scenes, they impose strict relation constraints, making them less user-friendly. A graph structure that mimics natural language should feature sparse edge relations. To address these issues, we propose the Mixed-Modality Graph, a novel graph where nodes can contain both textual and visual modalities, and edges are selectable.\nA Mixed-Modality Graph Gm = {Vm, Rm} contains nodes and their relations:\nVm = {vn | i \u2208 1, ..., \u039d}, (5)\nRm = {rj|i, j \u2208 {1, . . ., N}, i \u2260 j}. (6)\nEach node vn = {[o, im] | [on] | [im]} represents an object with text category information [o], image information [im] or both text category and image information [om, im],"}, {"title": "Visual Enhancement Module", "content": "Incorporating visual features within graph nodes enhances the generation of object geometry. However, in the LMMG, some nodes only contain textual information. We introduce a visual enhancement module to bolster the ability to generate object shapes. This module employs an architecture similar to VQ-VAE, comprising an encoder E, a decoder D, and a codebook C, to effectively construct visual features from the textual features of nodes within the LMMG. The encoder E processes the textual features to into latent vectors h = E(tm). These latent vectors are then quantized using the codebook C, which contains a set of embedding vectors {ek}_1. The quantization process selects the n nearest embedding vectors from the codebook:\nhm = {ek; | kje arg minh \u2013 ek ||2}. (9)\nThe quantized latent vectors hm are subsequently processed by the decoder D to generate visual features \u00fbm = D(hm). The training objective for the visual enhancement module is to maximize the evidence lower bound (ELBO) for the likelihood of the data:\nLr = Eh~qe(h\\t) [log PD (uh) \u2013 BDKL(qE(h|t)||p(h))], (10)\nwhere qE(ht) denotes the latent vector distribution given the textual features, PD(uh) is the likelihood of the visual features given the latent vectors, and DKL denotes the Kullback-Leibler divergence. The prior p(h) is typically a Gaussian distribution, and \u1e9e is a weighting factor. To address the non-differentiable nature of the quantization process, the Gumbel-Softmax relaxation (Jang, Gu, and Poole 2016) technique is applied to optimize the ELBO. Utilizing this VQ-VAE-based framework, the visual enhancement module produces a Visual-Enhanced Graph Gr, enhancing the capability of the LMMG to generate accurate and detailed object geometry for scene generation tasks."}, {"title": "Relation Predictor", "content": "Relations are crucial in indoor scene generation, as they impact layout configuration. To address the challenge of missing relationships among nodes in the LMMG, we develop a Relation Predictor that infers these connections, enabling the generation of more reasonable layouts. The Relation Predictor takes triples of latent representations (fi, fij, f) as input. In cases where relationships are missing, fij is filled with zeros to ensure consistency in the feature space. The module comprises a GCN layer followed by a series of MLP layers. The GCN layer processes the input triples to capture the relational context between nodes, while the MLP layers further refine the edge predictions. The Relation Predictor is trained using a cross-entropy loss, defined as:\nLe = 1/NC \u03a3\u03a3 Yic log(Yic) (11)\nwhere N is the number of node pairs, C is the number of edge classes, Yic is the one-hot encoded true label, and \u0177ic is the predicted probability. The Relation Predictor refines the graph Gr into a Mixed-Enhanced Graph Gm, by predicting and integrating missing node relationships to improve overall layout coherence."}, {"title": "Shape and Layout Branch", "content": "We employ a dual-branch diffusion model to generate object shapes and scene layouts. To facilitate effective information exchange and relationship modeling between nodes during each denoising process, as depicted in Fig. 2.C.1, we employ a triplet-GCN structured module that integrates the echo mechanism (Zhai et al. 2024b) as a Graph Encoder Eg.\nShape Branch. For the shape branch, as shown in Fig. 2.C.2, we use Truncated Signed Distance Field (Curless and Levoy 1996) as shape representations and employ a pretrained and frozen VQ-VAE to encode them into latent representations zo and decode them back. At each denoising step t \u2208 {1, 2, . . ., T}, Eg is applied to process the latent codes z and the latent graph G (which originates from G), yielding updated representations and G. The updated nodes of G\u012f, denoted as V\u2081 = {v}, are used as conditions for denoiser \u20ac\u03b8 (3D-UNet). The training objective is to minimize the deviation between the true noise e and the predicted noise eo (\u017e, t, v). The loss function is defined as:\nLs = Ez, e~N(0,1),t ||\u20ac - Eo (\u017e, t, v) ||\u00b2. (12)\nLayout Branch. We utilize object bounding boxes to represent the layout of the scene. Each bounding box 10 is characterized by its location t\u2208 R\u00b3, size s \u2208 R\u00b3, and rotation angle. Specifically, the rotation angle is parameterized by [cos(4), sin(6\u00ba)]T. To ensure proper scale and numerical stability during training, to and s are normalized. As illustrated in Fig. 2.C, the layout branch utilizes Eg for relationship modeling. This results in updated latent layout representations it and refined graph node embeddings N\u2081 = {n}. Conditioned on the updated node embeddings, a 1D-UNet is utilized as the denoiser Ve for the denoising process. The corresponding loss function is formulated as:\nL\u2081 = E\u012bt,v~N(0,1),t || V \u2013 Vo(\u00ce,,)||. (13)\nThe overall training objective for the layout and shape branches is expressed as:\nLo = a1 L\u03b9 + a2Ls, (14)\nwhere 21 and 2 are weighting factors."}, {"title": "Training and Inference Strategy", "content": "The training process is divided into two stages. In the first stage, the visual enhancement module is trained with the loss function Lr, which utilizes textual information from nodes to construct the corresponding visual features. The Relation Predictor is trained with Le using triplet representations of the graph. In the second stage, the LMMG serves as the input, and the loss function Lo is employed to jointly optimize the graph encoder with the layout and shape branches, as depicted in Fig. 2.A and C. During inference, as shown in Fig. 2, the LMMG is processed through modules B and C to generate the indoor scene. Please see the Supplementary Material for further details."}, {"title": "Experiments", "content": "Experimental Settings\nEvaluation Dataset. We validate our approach using the SG-FRONT dataset (Zhai et al. 2024c), which provides comprehensive scene-graph annotations for indoor scenes. This dataset includes 45K object instances and 15 types of relationships within bedrooms, dining rooms, and living rooms. Nodes in the scene graphs represent object categories, while edges indicate relationships between the nodes. In our experiments, we extracted corresponding images from the 3D-FUTURE dataset (Fu et al. 2021) based on node IDs to construct a Full-Modality Graph (node contains text and image). We then applied a random mask to mask the text, images, and relationships between nodes in the Full-Modality Graph, producing the Mixed-Modality Graph.\nEvaluation Metrics. We evaluate the scene-level and object-level fidelity of the synthesized 3D scenes. Scene-level fidelity is quantified using Fr\u00e9chet Inception Distance (FID) (Heusel et al. 2017) and Kernel Inception Distance (KID) (Bi\u0144kowski et al. 2018), which measure the similarity between generated top-down renderings and real scenes renderings. For object-level fidelity, we assess the quality of generated object geometry using Minimum Matching Distance (MMD), Coverage (COV), and 1-Nearest Neighbor Accuracy (1-NNA) (Yang et al. 2019), all derived from Chamfer Distance (CD) (Fan, Su, and Guibas 2017)."}, {"title": "Conclusion", "content": "We present MMGDreamer, a dual-branch diffusion model for geometry-controllable 3D indoor scene generation, leveraging a novel Mixed-Modality Graph that integrates both textual and visual modalities. Our approach, enhanced by a Visual Enhancement Module and a Relation Predictor, provides precise control over object geometry and ensures coherent scene layouts. Extensive experiments demonstrate that MMGDreamer significantly outperforms existing methods, achieving state-of-the-art results in scene fidelity and object geometry controllability."}]}