{"title": "Improving the Generalization of Unseen Crowd Behaviors for Reinforcement Learning based Local Motion Planners", "authors": ["Wen Zheng Terence Ng", "Jianda Chen", "Sinno Jialin Pan", "Tianwei Zhang"], "abstract": "Deploying a safe mobile robot policy in scenarios with human pedestrians is challenging due to their unpredictable movements. Current Reinforcement Learning-based motion planners rely on a single policy to simulate pedestrian movements and could suffer from the over-fitting issue. Alternatively, framing the collision avoidance problem as a multi-agent framework, where agents generate dynamic movements while learning to reach their goals, can lead to conflicts with human pedestrians due to their homogeneity. To tackle this problem, we introduce an efficient method that enhances agent diversity within a single policy by maximizing an information-theoretic objective. This diversity enriches each agent's experiences, improving its adaptability to unseen crowd behaviors. In assessing an agent's robustness against unseen crowds, we propose diverse scenarios inspired by pedestrian crowd behaviors. Our behavior-conditioned policies outperform existing works in these challenging scenes, reducing potential collisions without additional time or travel.", "sections": [{"title": "I. INTRODUCTION", "content": "Mobile robots are increasingly used in various applications ranging from industrial automation, service delivery, to agriculture applications [1]. The ability of these robots to maneuver and navigate in complex and dynamic environments is crucial for their successful deployment. One key aspect of mobile robot navigation is local motion planning, which aims to find a feasible and safe path for the robot to follow in its immediate vicinity. This task is particularly challenging, as it needs to ensure safe, efficient, and smooth robot movements in the presence of dynamic obstacles (i.e., pedestrians) in the environment. To address this issue, Reinforcement Learning (RL) has been introduced to achieve local motion planning, which exhibits the high ability to handle more complex scenarios and increased levels of uncertainty [2]-[6].\nFor RL-based methods, the environment is crucial as it shapes the agent's understanding of the environment to train an optimal policy. Particularly, the scenes in the environment should fully reflect the inherent diversity and unpredictability of pedestrians' movements. For example, on the footpaths, human pedestrians may walk at different speeds or behave differently depending on their social norms. If their behaviors are not modeled comprehensively, it is challenging for the robot agent to learn a robust policy which works well against diverse and unseen crowd behaviors.\nVarious approaches have been proposed to generate pedestrian movements for training RL-based local motion planning policies, which can be classified into two categories."}, {"title": "II. PRELIMINARIES", "content": ""}, {"title": "A. RL-based Local Motion Planning", "content": "Local motion planning is a sequential decision-making task that can be formulated as a Markov Decision Process (MDP), defined by a tuple M = (S,A,P,R, \u03b3). Here, S is the state space, A is the action space, P is the state-transition model, R is the reward function, and y is a discount factor.\nA general form of the states is s = [Senv, Srobot, Sgoal], where Senv, Srobot and Sgoal contain information about the observed environment, robot and goal respectively. Similar to [5], we choose a realistic representation that uses distance readings from a 2D laser range finder to sense the environment Senv. We consider the sensor noise and obstacle occlusions, and make no assumptions about the shape, size, and number of obstacles, which are more closely aligned with the real world. Srobot reveals the state of the robot, usually the velocities and optionally the position. Sgoal is represented by either the relative or absolute goal position. The action space A is the set of permissible velocities in either the continuous or discrete space. The reward R can be normally represented as follows:\n$r_t = \n\\begin{cases}\nr_{goal} & \\text{if } ||p_t-g|| < d_{col}, \\\\\nr_{col} & \\text{else if collision}, \\\\\nr_{step} \\cdot (||p_{t-1}-g|| - || p_t - g||) & \\text{otherwise}, \n\\end{cases}$\nwhere rgoal is the reward for reaching the desired goal, rcol is the penalty for collision, rstep is the dense reward for getting closer to the goal, dcol is the distance threshold for reaching the goal, p and g are positions of the robot and goal.\nIn this MDP, we aim to use RL to find a policy \u03c0\u03c1 parametrized by \u03b8, which maps states to actions and maximizes the expected sum of discounted rewards, $J(\\theta) = \\mathbb{E} [\\sum_{t=0}^{T} \\gamma^t r_t]$, where T is the length of an episode."}, {"title": "B. Pedestrian Modelling", "content": "In local motion planning, the movement of dynamic obstacles, often represented by human pedestrians, is a crucial environmental factor. Each pedestrian's behavior significantly influences the environment and how the robot agent learns within the RL framework. Existing approaches to modeling pedestrians can be classified into two categories."}, {"title": "C. Behavior Diversity in RL", "content": "To address the above homogeneity concern, various approaches were proposed to increase agent behavior diversity. For single-agent scenarios, one popular solution [21] is to maximize the entropy of the policy in addition to the reward, to learn different behaviors to achieve the goal. Eysenbac et al. [22] introduced DIYAN to increase the diversity of agent behaviors by maximizing the mutual information between skills and states, resulting in better state exploration.\nIn the multi-agent scenario, some work increases the behavior diversity of multiple agents in the Centralized Training with Decentralized Execution (CTDE) framework [23]-[25]. When agents are assigned different tasks, each agent gets a distinct policy respectively, which shares a common critic network. With multiple policies, several ideas have been proposed to generate diverse behaviors among agents [26]-[28]. However, this comes at the expense of sample efficiency since each agent only updates its own policy instead of a unified policy."}, {"title": "III. APPROACH", "content": "We present our approach to learning robust agents through behavior diversity. Instead of using multiple policies to create diversity as in CTDE, we opt for a more sample-efficient method by using only a single policy. We first formulate the agent behaviors, and how they can be used to generate diversity among agents (Section III-A). Then we explain how the behaviors and diversity can be integrated together within a single policy (Section III-B). Finally, we describe how to train a behavior-conditioned policy in an end-to-end manner with all the integrated components (Section III-C)."}, {"title": "A. Agent Behavior", "content": "In Figure 1, people's approaches to walking towards a goal can vary: some prioritize speed with a longer path, while others choose a shorter route at a slower pace. When avoiding moving obstacles, some turn left, while others turn right. Although individuals may have unique behaviors, there can be similar patterns. We formalize this with discrete behavior tokens z\u2208 [0,1,...,M \u2013 1], where M is the total number of distinct behaviors. Each token represents a distinct pedestrian behavior, and different agents may share the same token.\nTo foster diversity amongst different agents, our goal is to assign different behavior tokens to different agents to exhibit distinct behaviors. In other words, for every state s, agents should perform different actions a depending on the assigned z. More formally, this idea can be formalised using information theory by maximizing the mutual information I((S,A);Z), where (S,A) is the joint distribution of S and A. Z ~ p(z), S, and A represent the random variables for behavior, state, and action respectively. Additionally, the diverse actions performed for different z should arise for every state instead of exploiting only certain states. For this, we minimize I(S;Z) as a regularizer. In sum, we maximize\n$F(\\theta) = I((S,A);Z) \u2013 I(S;Z) = (H[Z] \u2013 H[Z | S,A]) \u2013 (H[Z] \u2013 H[Z | S]) = -H[Z | S,A] + H[Z | S],$ (1)\nwhere H is the Shannon entropy. The first term implies it is easy to infer the behavior z given any (s, a). This makes sense intuitively as it means the agents are distinguishable due to their diverse behaviors and not behaving in a homogeneous way. The second term implies that the agents' behavior should not be distinguishable exclusively given s. It is intractable to compute p(z|s) and p(z|(s,a)) by integrating all states, actions, and skills. So we approximate the posteriors with learned discriminators qo (z|s) and qy(z|(s,a)). We instead optimize the variational lower bound derived using Jensen's Inequality [29]:\n$\\mathcal{F}(\\theta) = \u2212H[Z | S,A] + H[Z | S] = \\mathbb{E}_{z\u223cp(z),s\u223c\u03c0(z)}[logp(z | s)] - \\mathbb{E}_{z\u223cp(z),s\u223c\u03c0(z),a\u223c\u03c0(s,z)}[log p(z | s,a)] \\geq \\mathbb{E}_{z\u223cp(z),s\u223c\u03c0(z)}[log q_{\\phi} (z | s)] \u2013 \\mathbb{E}_{z\u223cp(z),s\u223c\u03c0(z),a\u223c\u03c0(s,z)}[logq_{\\psi}(z | s,a)] = \\mathcal{G}(\\theta),$ \nwhere s ~ \u03c0(z) means to first sample the action a from \u03c0 followed by sampling the environment to get the state s. It is non-trivial to directly optimize \u03b8 via maximizing the lower bound G(\u03b8) since s ~ \u03c0(z) has to be sampled through a non-differentiable simulator. Below we introduce how to optimize \u03b8 using an intrinsic reward alongside the RL objective."}, {"title": "B. Behavior-Conditioned Policy", "content": "First, we incorporate the idea of behaviors into our policy where we condition our policy on the agent's behaviors. Each agent, i, sample their actions from a shared behavior-conditioned policy as a ~ \u03c0\u04e9(\u00b7,s|z\u00b9), for behavior token ID z\u00b9 at timestep t. Each behavior token maps to an embedding in the policy network, enabling the policy to generate distinct behaviors for agents. To maximize such diversity, we introduce an intrinsic pseudo-reward rint motivated from maximizing G(0) derived previously:\n$r_{int} = log[q_{\\omega}(z_t | s_t, a_t)] \u2013 log[q_{\\phi}(z | s_t)].$  (2)"}, {"title": "C. Training Procedure", "content": "We adapt the training procedure from [5], alternating between sampling trajectories and updating the policy via the PPO algorithm [31]. Each agent uses an identical policy to collect data until a batch is gathered. Algorithm 1 outlines the training details. Key differences from [5] are highlighted in blue: (1) At each episode start, agent i samples a new behavior token z ~ pm(z), with pm(z) being a discrete uniform distribution with M behaviors (Line 6). This token is mapped to a 32-dimensional continuous embedding. (2) Agents sample from a policy conditioned on z\u00b9 (Line 7), allowing for varied actions based on behavior. (3) Intrinsic rewards for each agent are computed using discriminators qysa and qys, parameterized by sa and ys respectively, based on Eqn. (2). These rewards are added to the task reward in the replay buffer (Line 8). (4) We optimize discriminators qysa and qys with cross-entropy loss (Line 25) using the Adam optimizer [32]. Adding one standard deviation of Gaussian noise to discriminator inputs helps prevent overfitting. The loss is computed between predicted behavior tokens from on-policy samples and ground truth behavior."}, {"title": "IV. EXPERIMENTS", "content": "We conduct comprehensive experiments to demonstrate the effectiveness of our method over previous solutions. We simulate these experiments with new crowd behaviors not encountered during agent training."}, {"title": "A. Experimental Setup", "content": "Implementation. We simulate a large-scale group of robots using Stage [33], a popular robot simulator widely used in multi-agent research. Each agent is initialized as a non-holonomic differential drive robot (0.5m \u00d7 0.5m) equipped with a 2D-laser scanner to sense its surroundings. The 2D laser is set to 360 degrees FOV with a max range of 10m.\nFor agent states, rewards and neural network architecture, we follow the same setup as [5]. We make one change to the NN backbone by adding a behavior embedding derived from the behavior token, z, for the neural network input. Each discriminator is modeled with a two-layer feed-forward network with 128 hidden units and ReLU activations [34]."}, {"title": "B. Training Results", "content": "Figure 3 shows the stability of the discriminators and the intrinsic reward during training. The discriminator loss and intrinsic reward steadily improves until ~ 700 updates, which then flattens until ~ 4000 updates, possibly due to novel state-action exploration. Subsequently, both curves continue to improve again until the end of training where the main task has converged. The best advantage weight \u03b1, which combines the task and intrinsic reward is 0.1.\nNext, we investigate if behavior-conditioned policies exhibit different behaviors to reach the desired goal. We record the behaviors of agents starting from the origin and reaching a fixed goal behind a static obstacle. The trajectories of the 5 agents can be seen in Figure 4. These agents exhibit different behaviors when conditioned on different tokens z. While passing the obstacle, some agents are left-inclined whereas some are right-inclined which is consistent with the motivation illustrated in Figure 1. Additionally, we also observe the velocity diversity which is represented by the length of the arrow. For example, the blue agent (z = 2) travels slightly faster than the purple agent (z = 4) when taking a slightly longer route. While the diversity may appear slight, deploying it with multiple agents generates vastly different training trajectories, enhancing agent robustness."}, {"title": "C. Impact of the Number of Behaviors", "content": "We assess diversity's impact on agent robustness and its scalability with behavior count M. When M = 1, all agents share the same behavior, equivalent to the baseline policy in [5]. Our results are presented in Table II. We have the following three observations. First, having non-reactive pedestrians (NH,IN,VA,SO) is generally a more difficult task with fewer successful runs. Second, adding diversity (M\u2260 1) outperforms the default policy (M = 1) for all pedestrian types, including the challenging non-reactive pedestrians. This validates the effectiveness of our proposed method. Third, the optimal number of behaviors is M = 5 and the effect of diversity starts to diminish as we scale to higher values of M = 10 and 20. We hypothesize the diminishing effect is resulted from less frequent sampling when M increases. To investigate this, we increase num_updates/M for M = 10 and 20 to match M = 5 and find that the performance of M = 10 and M = 20 could match that of M = 5, validating our hypothesis. However, this comes at an expense of more updates. Overall, M = 5 provides a good balance between creating good diversity and sample efficiency."}, {"title": "D. Scalability", "content": "Here, we investigate the effect of increased numbers of agents N and report the results in Table III. As the number of agents, N, increases to 10 and 20, the scenes become more crowded, making it harder for them to reach their goals. This negatively affects the convergence speed, as agents get restarted more frequently due to collisions or other obstacles. Despite this, by adding diversity to the policy, we achieve consistent performance improvements across all pedestrian cases."}, {"title": "E. Intrinsic Rewards", "content": "In Section III-A, we formulate a cost function to promote diversity among agents. In the formulation, we require that diverse actions performed for different z should arise for every state instead of exploiting only certain states. This is achieved using a regularizer as part of the intrinsic reward proposed in Eqn. (2). From ablation experiments reported in Table IV, we observe that the policy trained with the intrinsic reward containing the regularization term, -log(q | s), outperforms the policy without this term. The performance improvement is consistent in all pedestrian setups including the challenging non-reactive pedestrians. Despite this, the policy trained without regularization still outperforms the base policy without the intrinsic reward.\nNext, we compare our method with state-space exploration based intrinsic rewards, DIYAN, from [22] which may implicitly encourage action diversity through novel state exploration. However, it still lacks action diversity compared to our proposed intrinsic reward in Eqn.(2), where the diversity of the action is explicitly encouraged. To measure the action diversity, we also introduce a new metric, 9, using the KL divergence of action distributions between pairwise agents:\n$ \\mathcal{D} =  \\frac{1}{|\\mathcal{T}| N_{ij}} \\sum_{t \\in \\mathcal{T}} \\sum_{i j} KL (\\pi(a|s, z=i)||\\pi(a|s, z=j))$ \nwhere t denotes a trajectory. Specifically, we collect a trajectory of 1000 steps using the trained policy with no intrinsic reward. From Table IV, our proposed method achieves higher action diversity 9 than the state-space exploration based intrinsic rewards. Also, we observe that higher values of D get translated into higher robustness in unseen crowd behaviors, achieving a greater success rate."}, {"title": "F. Comparisons with Prior Work", "content": "We quantitatively compare our proposed behavior-conditioned policy with existing solutions to demonstrate its robustness. In particular, we set up the baseline method as described in [5], equivalent to our proposed method with M = 1. Additionally, we added a safe policy proposed in [10], which uses safety zone rewards to encourage safe behaviors, which could crash less in unseen crowd movements. For our proposed method, we utilize the model trained with M = 5 and N = 5. Table V shows the comparison results against different metrics across 1000 episodes. Each metrics (success rate, extra time, extra distance, average speed) are similarly defined like in [5].\nOur proposed method consistently outperforms others across various pedestrian types, suggesting robust strategies for handling diverse crowd behaviors effectively. While the safe policy achieves a higher success rate than the base policy, it slightly falls short of our proposed policy. Collisions primarily contribute to non-successful episodes, surpassing timeouts. The safe policy with a safety buffer performs well in reactive setups (NH, VA), closely matching our policy's results. However, it struggles in non-reactive setups (IN, SO, VO, SF). The conservative behavior of the safe policy reduces collisions but increases time and distance compared to our proposed policy, sacrificing other metrics. Specifically, both time and distance taken by the safe policy are more than double those of our proposed policy."}, {"title": "G. Realistic Deployment", "content": "To validate our method in a more realistic setup, we deploy our best performing policy (N = 5, M = 5) on a Jackal robot in Gazebo simulator [37]. We utilize two maps (warehouse and hospital) from Arena-ROSNAV-3D [38] as seen in Figure. 5. For each episode, we randomly assign the start and goal position with 3 to 8 pedestrians within an open area of each map (~ 10m \u00d7 10m). The pedestrians movements are simulated using the social force model [36]."}, {"title": "V. CONCLUSION", "content": "This paper introduces a framework to increase an agent's ability to generalize to unseen crowd behaviors by utilizing diverse behaviors in a sample-efficient manner. Adding diversity in a multi-agent framework implicitly provides each agent with a more varied range of experiences, hence increasing its generalizability of unseen crowd behaviors. We demonstrate the robustness of the proposed method in an extensive set of evaluation scenes containing challenging pedestrians' behaviors. We also validate the scalability of our solution and practicality in realistic scenes. Our experiments also demonstrate that our method improves the success rates without negatively affecting other important metrics."}]}