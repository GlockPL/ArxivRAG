{"title": "Fast and Interpretable Mixed-Integer Linear Program Solving by Learning Model Reduction", "authors": ["Yixuan Li", "Can Chen", "Jiajun Li", "Jiahui Duan", "Xiongwei Han", "Tao Zhong", "Vincent Chau", "Weiwei Wu", "Wanyuan Wang"], "abstract": "By exploiting the correlation between the structure and the solution of Mixed-Integer Linear Programming (MILP), Machine Learning (ML) has become a promising method for solving large-scale MILP problems. Existing ML-based MILP solvers mainly focus on end-to-end solution learning, which suffers from the scalability issue due to the high di-mensionality of the solution space. Instead of directly learning the optimal solution, this paper aims to learn a reduced and equivalent model of the original MILP as an intermediate step. The reduced model often corresponds to interpretable operations and is much simpler, enabling us to solve large-scale MILP problems much faster than existing commercial solvers. However, current approaches rely only on the optimal reduced model, overlooking the significant preference information of all reduced models. To address this issue, this paper proposes a preference-based model reduction learning method, which considers the relative performance (i.e., objective cost and constraint feasibility) of all reduced models on each MILP instance as preferences. We also introduce an attention mechanism to capture and represent preference information, which helps improve the performance of model reduction learning tasks. Moreover, we propose a SETCOVER based pruning method to control the number of reduced models (i.e., labels), thereby simplifying the learning process. Evaluation on real-world MILP problems shows that 1) compared to the state-of-the-art model reduction ML methods, our method obtains nearly 20% improvement on solution accuracy, and 2) compared to the commercial solver Gurobi, two to four orders of magnitude speedups are achieved.", "sections": [{"title": "Introduction", "content": "Due to its strong expressiveness, Mixed-Integer Linear Programming (MILP) has been widely used in various critical domains, including supply chain and logistics (Chao, Jasin, and Miao 2024), service scheduling (Rosemarin, Rosenfeld, and Kraus 2019), energy management (Morales-Espa\u00f1a, Latorre, and Ramos 2013), transportation planning (Lowalekar, Varakantham, and Jaillet 2021; Li et al. 2024), chip design (Wang et al. 2024c,d), and chemistry research (Geng et al. 2023b). Commercial solvers, such as Gurobi, Cplex and Matlab, are mainly used to solve MILP problems. In real-world industrial applications, MILP instances often involve hundreds of thousands of decision variables and constraints (Morales-Espa\u00f1a, Latorre, and Ramos 2013; Li et al. 2021). Existing commercial solvers are based on exact solutions, which are computationally expensive and cannot meet the real-time demands of industrial applications.\nIn many scenarios, a large number of homogeneous MILPs with similar combinatorial structures need to be solved simultaneously. For example, online stochastic pro-gramming often involves solving similar MILP instances at each stage, with slightly modified input parameters while the structure remains unchanged (Lowalekar, Varakantham, and Jaillet 2018; Bertsimas and Stellato 2022). Machine Learn-ing (ML), with its powerful pattern recognition capability, can exploit the correlation between the structure and the solution of MILP, and has recently become a very promis-ing research topic for solving large-scale MILP (Bengio, Lodi, and Prouvost 2021; Zhang et al. 2023; Hentenryck and Dalmeijer 2024). Existing ML-based MILP solvers can be classified into two categories: 1) end-to-end solution pre-diction, i.e., directly learning the mapping between MILP instances and solutions (Donti, Rolnick, and Kolter 2021; Ding et al. 2020; Chen et al. 2023); and 2) learning to op-timize, i.e., learning to improve the process of traditional solvers (He, Daum\u00e9, and Eisner 2014; Khalil et al. 2017; Song et al. 2020a; Chi et al. 2022; Ling, Wang, and Wang 2024; Han et al. 2023; Balcan et al. 2024). Due to the high dimensionality of the solution space, existing ML methods that learn the optimal solution as a function of the input pa-rameters, suffer from the scalability issue. Furthermore, it is currently not possible to interpret the predicted solution or to understand it intuitively (Park and Hentenryck 2023).\nInstead of directly learning the optimal solution, this pa-per takes a different method to learn a reduced and equiv-alent model of the original MILP as an intermediate step. In Operations Research (Boyd and Vandenberghe 2014), an equivalently reduced model of the MILP constitutes the minimal information, including the set of active constraints and the value of integer variables at the optimal solution, required to recover the optimal solution. Model reduction learning has the following three advantages (Misra, Roald, and Ng 2022): 1) from the optimization perspective, the re-duced model is much easier than the original MILP model, which can be solved fast, 2) from the ML perspective, us-ing the reduced models as labels can reduce the dimension"}, {"title": "Related Work", "content": "Existing ML-based MILP solving methods can be catego-rized into three groups: 1) end-to-end solution prediction, i.e., directly learning the mapping between MILP instances and solutions; 2) learning to optimize, i.e., learning to accel-erate the solving process of traditional exact/heuristic meth-ods; 3) learning to simplify the MILP, i.e., learning to pre-solve or reduce the size of the MILP formulation.\nEnd-to-end Solution Prediction. Using ML to learn the mapping from MILP instances to a high-dimensional solu-tion space is straightforward, however, it often results in low prediction accuracy (Donti, Rolnick, and Kolter 2021; Park and Hentenryck 2023; Chen et al. 2023). Therefore, (Nair et al. 2020; Ye et al. 2023) only predicts values for partial variables and computes the values of the remaining vari-ables using the off-the-shelf solver. Directly predicting vari-able values cannot maintain the hard constraints (Ding et al. 2020). Instead, (Han et al. 2023) predicts an initial solution and searches for feasible solutions in its neighborhood.\nLearning to Optimize. For exact solving, there are al-ways hyperparameters and selection rules that need to be fine-tuned to accelerate the solving process. For example, the selection of branching variables and their values in Branch-and-Bound, the selection of the cutting rules in Cut-ting Plane, and the column generated in the Column Gen-eration algorithm. Using experienced data of these exact solvers, Imitation Learning (IL) and Reinforcement Learn-ing (RL) have been used to learn effective hyperparameters and selection rules (Wang et al. 2024a,b; Huang et al. 2022; Lin et al. 2022; Wang et al. 2023). On the other hand, for heuristic algorithms such as Local Search Heuristics (Cai and Su 2013), and Large Neighborhood Search Heuristics (Song et al. 2020b; Wu et al. 2021), ML can also be used to improve their effectiveness. For example, (Qi, Wang, and Shen 2021) use RL to iteratively explore better solutions in Feasible Pump, and (Nair, Alizadeh et al. 2020) use RL to search for better solutions within a neighborhood.\nAlthough these two directions introduce MILP to the ben-efits of ML and show promising results, they do not scale well to real-world applications. Directly predicting a high-dimensional solution is intractable. The efficiency of IL and RL-based optimization methods is limited by the decision horizon (Ye et al. 2024; Geng et al. 2024; Wang et al. 2022) (i.e., the number of integer variables). Another drawback is their inability to enforce the constraints accurately, making them unsuitable for real-world high stakes applications (Liu et al. 2023, 2024b; Geng et al. 2023a). In contrast, this paper utilizes model reduction theory and focuses on learning the mapping between an MILP instance and its optimal reduced model, providing a fast and interpretable MILP solution.\nLearning to Simplify the MILP. Large-scale MILP for-mulations usually contain much redundancy, which can be simplified by the pre-solve techniques (Achterberg and Wunderling 2013). To design high-quality pre-solve rou-tines, (Liu et al. 2024a) and (Kuang et al. 2023) recently use RL to determine which pre-solve operators to select and in what order. (Ye, Xu, and Wang 2024) instead use graph partition for problem division to reduce computational cost. These pre-solve-based simplification methods can only identify limited and explicit redundancy. To identify the minimal tight model, (Misra, Roald, and Ng 2022; Bertsi-mas and Kim 2023) first propose a classification method to predict the optimal reduced model. However, existing meth-ods only consider several equally desirable reduced models, overlooking the various performances of the reduced mod-els. This paper considers the importance of preference in-formation and designs an efficient method to exploit it to improve the learning accuracy of model reduction."}, {"title": "Parameterized MILP and Model Reduction", "content": "Parameterized MILP Problem. The MILP can be formal-ized as follows:\n$\\begin{aligned}\n\\min_{X} f(c, x)  \\\\\ns.t. g(A, x) &= \\begin{pmatrix}\ng_1(A_1,x_I,x_{-I}) \\le b_1  \\\\\ng_2(A_2,x_I,x_{-I}) \\le b_2 \\\\\n... \\\\\ng_m(A_m, x_I,x_{-I}) \\le b_m\n\\end{pmatrix}  \\\\\nx_I \\in Z^d, &x_{-I}\\in R^{n-d}\n\\end{aligned}$"}, {"title": "Framework Overview", "content": "The proposed framework comprises two phases: 1) Strategy Generation and Pruning for exploring a set of useful strate-gies $S$ as labels, and 2) Preference-based Strategy Learning for predicting the correct strategy $s \\in S$ for any instance $\\theta$ (see Figure 1 for details).\nStrategy Generation and Pruning aims to generate a set of useful strategies that can be applied to all MILP instances. Intuitively, we can explore as many MILP instances $\\theta$ as pos-sible and denote their optimal strategy $s^*(\\theta)$ as candidate strategy labels. However, the number of candidate strategies can grow quickly with respect to the number of instances, thereby making the strategy learning task very difficult. This paper proposes a SETCOVER technique for strategy pruning while ensuring that all generated instances can be covered, in which the cover means that there is at least one feasible strategy for an instance.\nPreference-based Strategy Learning aims to propose a machine learning approach to predict the optimal strategy $s^*(\\theta)$ for any MILP instance $\\theta$. Given the training data {$\\theta_i, s^*(\\theta)$}, existing strategy learning approaches typically only focus on the exact optimal strategy (Misra, Roald, and"}, {"title": "Strategy Generation and Pruning", "content": "In this section, our objective is to identify a set of useful strategies that will be used as labels for model reduction training and learning.\nStrategy Generation. It is difficult to determine the amount of instances required for strategy learning. Fol-lowing the approach in (Bertsimas and Stellato 2022), we first randomly generate instances $\\theta$ as well as their opti-mal strategy $s^*(\\theta)$ until the Good-Turning estimator $\\frac{N_1}{N}$ falls below a tiny value, where $N$ is the total number of instances and $N_1$ is the number of different strategies ap-peared exactly once. Specially, given $N$ independent in-stances $\\Theta_N = {\\theta_1,..., \\theta_N}$, we can generate $M$ different strategies $S(\\Theta_N) = {s_1,...,s_M}$. However, in large-scale MILP problems, the number of strategies (i.e., labels) $M$ can grow quickly, making the learning task very difficult. Motivated by this issue, we next propose a strategy pruning method based on SETCOVER technique.\nStrategy Pruning. In practice, each optimal strategy $s^*(\\theta)$ not only applies to the corresponding instance $\\theta_i$, but also may apply to other instances $\\theta_j(\\neq \\theta_i)$. Therefore, many candidate strategies are redundant and we can select only the most useful strategies to apply. We first model the rela-tionship between strategies $S(\\Theta_N)$ and instances $\\Theta_N$ by an Instance-Strategy bipartite graph $G(V_\\theta, V_s, E)$:\n*   The node set consists of the instance nodes $V_\\theta = {v_{\\theta_1},..., v_{\\theta_N}}$ and the strategy nodes $V_s = {v_{s_1},..., v_{s_M}}$. Each $v_{\\theta_i}$ represents an instance $\\theta_i \\in \\Theta_N$, and each $v_{s_i}$ represents a strategy $s_j \\in S(\\Theta_N)$.\n*   An edge $e_{i,j} \\in E$ exists between $v_{\\theta_i}$ and $v_{s_j}$ if apply-ing the strategy $s_j$ to the instance $\\theta_i$, and the infeasibility $p(\\theta_i, s_j)$ and suboptimality $d(\\theta_i, s_j)$ of the reduced prob-lem (5)-(7) are both below a tiny threshold $[\\epsilon_p, \\epsilon_d]$. The infeasibility is defined as:\n$p(\\theta_i, s_j) = ||(g(A, x^{s_j}) - b)^+||_{\\infty}/||b||,$\nwhere $x^{s_j}$ is the solution of the reduced problem\u00b9. $||b||$ normalizes the degree of constraint violation based on the magnitude of the constraint $g(A,x) < b$. The Subopti-mality measures the relative distance between the recov-ered solution $x^{s_j}$ of the reduced problem and the optimal solution $x^{\\theta_i}$ of the instance $\\theta$:\n$d(\\theta_i, s_j) = |f(c, x^{s_j}) - f (c, x^{\\theta_i})|/|f (c, x^{\\theta_i}).$\nThe objective of strategy pruning is to find a minimal sub-set of strategy nodes $V^s \\subseteq V_s$, such that for any $v^\\theta_i \\in V_\\theta$,"}, {"title": "Preference-based Strategy Learning", "content": "Given the pruned strategies $S^P$, this section proposes to learn the mapping from a MILP instance $\\theta$ to a suitable strategy $s(\\theta) \\in S^P$. Previous strategy learning approaches (Misra, Roald, and Ng 2022; Bertsimas and Stellato 2022) focus on predicting the correct strategy and considering all other strategies equally undesirable, failing to integrate the significant instance-strategy preference information deeply.\nPreference Computation. Given an instance $\\theta_i$, we would like to supply a reward $r(\\theta_i,s_j)$ to each strategy $s_j \\in S^P$. The reward $r(\\theta_i, s_j) \\in r$ is used to measure the outcome of applying the strategy $s_j$ to the instance $\\theta_i$. We follow the same criteria for calculating the relative feasibil-ity and suboptimality as in Eqs. (8) and (9):\n$r(\\theta_i, s_j) = -log(p(\\theta_i, s_j) + d(\\theta_i, s_j)).$\nDirectly learning the real reward function $r(\\theta_i, s_j)$ between $\\theta_i$ and $s_j$ is extremely challenging because the complex re-lationships among instances, strategies and rewards. Instead, to enhance simplicity and training stability, we propose to learn a proxy reward model $R_\\phi$ (where $\\phi$ denotes the pa-rameters of the machine learning approach) that can express preferences between strategies. Given an instance $\\theta_i$, for two instance-strategy pairs ($\\theta_i,s_j$) and ($\\theta_i, s_k$), we define the preference $\\succ$ generated by the rewards $r$:\n$(\\theta_i, s_j) \\succ (\\theta_i, s_k) \\Leftrightarrow r(\\theta_i,s_j) > r(\\theta_i, s_k).$\nInformally, $(\\theta_i, s_j) \\succ (\\theta_i, s_k)$ indicates that the instance $\\theta_i$ prefers the strategy $s_j$ to the strategy $s_k$.\nPreference-based Sampling. To train the proxy reward model $R_\\phi$, previous preference-based learning (e.g., RLHF (Christiano et al. 2017)) requires selecting all possible pair-wise comparisons as samples. For example, let $M^P = |S^P|$ denote the number of pruned strategies, there will be $\\binom{M^P}{2}$ preference samples for each instance at the training stage. The number of samples grow quadratically with the number of strategies, thereby increasing the cost of training. Fortu-nately, in our strategy learning problem, the instance pref-erences on strategies have a transitivity structure. For exam-ple, given the instance $\\theta_i$, if the strategy $s_j$ is preferred to $s_k$, and $s_k$ is preferred to $s_q$, we still have that $s_j$ is pre-ferred to $s_q$. This transitivity property can rank all candi-date strategies as a complete order based on preferences (i.e., Eq. (11)). Therefore, for each instance $\\theta_i$, when the candi-date strategies are ranked in decreasing order of their pref-erences (i.e., $S_{\\theta(j)} \\in S^P = {s_{\\theta(1)}, ..., s_{\\theta(M^P)}}$ is ranked"}, {"title": "Online Strategy Inference", "content": "To overcome the potential prediction errors, reliability can be increased by taking the Top-k output strategies as can-didates. Given the parameterized preference model $R_\\phi$, and the instance $\\theta_i$, let $S_k$ be the set of the $k$ strategies corre-sponding to the $k$ largest outputs\n$S_k = {s_j | f_{i,j} \\in Top-k({f_{i,1}, f_{i,2},..., f_{i,M^P}})},$\nwhere $r_{i,j} \\in R_i$ is the output of preference model $R_\\phi$. We select $S_k$ as the strategy candidates for the instance $\\theta_i$, and evaluate the strategies $s_j \\in S_k$ by solving the reduc-tion model $s_j(\\theta_i)$. And the $s_j$ with the lowest infeasibility $p(\\theta_i, s_j)$ is selected as the target strategy.\n$\\hat{s} = argmin_{s_j \\in S_k} p(\\theta_i, S_j).$\nTo solve the reduction model $s_j(\\theta_i)$, for special types of problems such as MIQP (Mixed-Integer Quadratic Program-ming) and MILP, the linear system can be simplified based on the KKT optimality conditions (Boyd and Vandenberghe 2014), further speeding up the solution time. The workflow in online stage is detailed in Figure 2"}, {"title": "Experiments", "content": "In this section, we compare our proposed method with the learning-based methods and the commercial solvers on real-world datasets to validate the performance and efficiency.\nEvaluation Metrics. We follow the same criteria for solv-ing quality as in (Bertsimas and Stellato 2022), using the ac-curacy metric. We consider solutions to be accurate if their"}, {"title": "Appendix", "content": "Dataset and Experimental Setup Supplement. We pro-vide a detailed introduction of the datasets. Datasets 1 and 2 are publicly available, and our settings are largely consis-tent with previous works. Dataset 3 represents five problem models currently employed by a company, and we briefly describe its scenario due to commercial privacy concerns. However, we have anonymized the data and uploaded a de-identified sample of the model's LP relaxation as a demo in the supplementary material.\nDataset 1 is from the MIPLIB library (Gleixner et al. 2021), and given in the (Aeq, beq, Aineq, bineq, c, lb, ub, I) format. The objective is to minimize $cx$ over the feasible region {$x$: Aineqx < bineq, Aeqx = beq, lb \u2264 x \u2264 ub},\nwhere $I$ is the set of indices for the integer decision vari-ables. We select the key parameters $\\theta$ for instance gener-ation, treating the original data as the center of the ball and generate parameters {$\\theta$}\n uniformly from the ball $B(\\theta,r)$. The ranges of $\\theta$ and $r$ in our experiments are broader than those in (Bertsimas and Kim 2023) for more challenging applications. The varying parameters $\\theta$ mainly include beq, bineq, or c and we ensure that none of their en-tries contain zeros to avoid unreasonable disturbances.\nDataset 2 represents the Fuel Cell Energy Management scenario. Switching fuel cells on and off reduces battery lifespan and increases energy loss; therefore, they are of-ten paired with energy storage devices (such as supercapac-itors) to reduce switching frequency during rapid transients. In this scenario, the objective is to control the energy bal-ance between the storage device and the fuel cell to match the required power demand (Frick, Domahidi, and Morari 2015). The goal is to minimize energy loss while maintain-ing the switching frequency of the fuel cell within an accept-able range to prevent lifespan degradation. The model is as follows:\n$\\begin{aligned}\n\\min_{P,z} f(P_t, z) &= \\sum_{t=0}^{T-1}(\\alpha P_t^2 + \\beta P_t + \\gamma z_t)\\\\\ns.t.\\\\\n&E_{t+1} = E_t + \\tau(P_t - P_t^{load}), t = 0, ...,T\\\\n&E_{min} < E_t < E_{max}, t = 0,...,T - 1\\\\n&0 < P_t < z_t P^{max}, t=0,...,T\\\\n&z_{t+1} = z_t + w_t, t = 0,...,T\\\\n&s_{t+1} = s_t + d_t - d_{t-\\tau}, t = 0, ..., T - 1\\\\n&s_t < n, t = 0,...,T\\\\n&G(w_t, z_t, d_t) < h, t = 0, ...,T\\\\n&E_0 = E_{init}, z_0 = Z_{init}, s_0 = S_{init}\\\\\n&z_t \\in {0,1}, d_t \\in {0,1}, w_t \\in {-1,0,1}.\n\\end{aligned}$\nIn the objective function, $z_t \\in {0,1}$ represents the ON-OFF state of the cell, and fuel consumption is given by $\\alpha P_t^2 + \\beta P_t + \\gamma z_t(\\alpha,\\beta,\\gamma > 0)$ when the battery is on ($z_t = 1$), where $P_t \\in [0, P^{max}]$ is the power provided by the fuel cell. This problem is a type of Mixed Integer Quadratic Program (MIQP). $E_t \\in [E^{min}, E^{max}]$ denotes the energy"}]}