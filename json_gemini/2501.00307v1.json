[{"title": "Fast and Interpretable Mixed-Integer Linear Program Solving by Learning Model Reduction", "authors": ["Yixuan Li", "Can Chen", "Jiajun Li", "Jiahui Duan", "Xiongwei Han", "Tao Zhong", "Vincent Chau", "Weiwei Wu", "Wanyuan Wang"], "abstract": "By exploiting the correlation between the structure and the solution of Mixed-Integer Linear Programming (MILP), Machine Learning (ML) has become a promising method for solving large-scale MILP problems. Existing ML-based MILP solvers mainly focus on end-to-end solution learning, which suffers from the scalability issue due to the high dimensionality of the solution space. Instead of directly learning the optimal solution, this paper aims to learn a reduced and equivalent model of the original MILP as an intermediate step. The reduced model often corresponds to interpretable operations and is much simpler, enabling us to solve large-scale MILP problems much faster than existing commercial solvers. However, current approaches rely only on the optimal reduced model, overlooking the significant preference information of all reduced models. To address this issue, this paper proposes a preference-based model reduction learning method, which considers the relative performance (i.e., objective cost and constraint feasibility) of all reduced models on each MILP instance as preferences. We also introduce an attention mechanism to capture and represent preference information, which helps improve the performance of model reduction learning tasks. Moreover, we propose a SETCOVER based pruning method to control the number of reduced models (i.e., labels), thereby simplifying the learning process. Evaluation on real-world MILP problems shows that 1) compared to the state-of-the-art model reduction ML methods, our method obtains nearly 20% improvement on solution accuracy, and 2) compared to the commercial solver Gurobi, two to four orders of magnitude speedups are achieved.", "sections": [{"title": "Introduction", "content": "Due to its strong expressiveness, Mixed-Integer Linear Programming (MILP) has been widely used in various critical domains, including supply chain and logistics (Chao, Jasin, and Miao 2024), service scheduling (Rosemarin, Rosenfeld, and Kraus 2019), energy management (Morales-Espa\u00f1a, Latorre, and Ramos 2013), transportation planning (Lowalekar, Varakantham, and Jaillet 2021; Li et al. 2024), chip design (Wang et al. 2024c,d), and chemistry research (Geng et al. 2023b). Commercial solvers, such as Gurobi, Cplex and Matlab, are mainly used to solve MILP problems. In real-world industrial applications, MILP instances often involve hundreds of thousands of decision variables and constraints (Morales-Espa\u00f1a, Latorre, and Ramos 2013; Li et al. 2021). Existing commercial solvers are based on exact solutions, which are computationally expensive and cannot meet the real-time demands of industrial applications.\nIn many scenarios, a large number of homogeneous MILPs with similar combinatorial structures need to be solved simultaneously. For example, online stochastic programming often involves solving similar MILP instances at each stage, with slightly modified input parameters while the structure remains unchanged (Lowalekar, Varakantham, and Jaillet 2018; Bertsimas and Stellato 2022). Machine Learning (ML), with its powerful pattern recognition capability, can exploit the correlation between the structure and the solution of MILP, and has recently become a very promising research topic for solving large-scale MILP (Bengio, Lodi, and Prouvost 2021; Zhang et al. 2023; Hentenryck and Dalmeijer 2024). Existing ML-based MILP solvers can be classified into two categories: 1) end-to-end solution prediction, i.e., directly learning the mapping between MILP instances and solutions (Donti, Rolnick, and Kolter 2021; Ding et al. 2020; Chen et al. 2023); and 2) learning to optimize, i.e., learning to improve the process of traditional solvers (He, Daum\u00e9, and Eisner 2014; Khalil et al. 2017; Song et al. 2020a; Chi et al. 2022; Ling, Wang, and Wang 2024; Han et al. 2023; Balcan et al. 2024). Due to the high dimensionality of the solution space, existing ML methods that learn the optimal solution as a function of the input parameters, suffer from the scalability issue. Furthermore, it is currently not possible to interpret the predicted solution or to understand it intuitively (Park and Hentenryck 2023).\nInstead of directly learning the optimal solution, this paper takes a different method to learn a reduced and equivalent model of the original MILP as an intermediate step. In Operations Research (Boyd and Vandenberghe 2014), an equivalently reduced model of the MILP constitutes the minimal information, including the set of active constraints and the value of integer variables at the optimal solution, required to recover the optimal solution. Model reduction learning has the following three advantages (Misra, Roald, and Ng 2022): 1) from the optimization perspective, the reduced model is much easier than the original MILP model, which can be solved fast, 2) from the ML perspective, using the reduced models as labels can reduce the dimension"}, {"title": "Related Work", "content": "Existing ML-based MILP solving methods can be categorized into three groups: 1) end-to-end solution prediction, i.e., directly learning the mapping between MILP instances and solutions; 2) learning to optimize, i.e., learning to accelerate the solving process of traditional exact/heuristic methods; 3) learning to simplify the MILP, i.e., learning to pre-solve or reduce the size of the MILP formulation.\nEnd-to-end Solution Prediction. Using ML to learn the mapping from MILP instances to a high-dimensional solution space is straightforward, however, it often results in low prediction accuracy (Donti, Rolnick, and Kolter 2021; Park and Hentenryck 2023; Chen et al. 2023). Therefore, (Nair et al. 2020; Ye et al. 2023) only predicts values for partial variables and computes the values of the remaining variables using the off-the-shelf solver. Directly predicting variable values cannot maintain the hard constraints (Ding et al. 2020). Instead, (Han et al. 2023) predicts an initial solution and searches for feasible solutions in its neighborhood.\nLearning to Optimize. For exact solving, there are always hyperparameters and selection rules that need to be fine-tuned to accelerate the solving process. For example, the selection of branching variables and their values in Branch-and-Bound, the selection of the cutting rules in Cutting Plane, and the column generated in the Column Generation algorithm. Using experienced data of these exact solvers, Imitation Learning (IL) and Reinforcement Learning (RL) have been used to learn effective hyperparameters and selection rules (Wang et al. 2024a,b; Huang et al. 2022; Lin et al. 2022; Wang et al. 2023). On the other hand, for heuristic algorithms such as Local Search Heuristics (Cai and Su 2013), and Large Neighborhood Search Heuristics (Song et al. 2020b; Wu et al. 2021), ML can also be used to improve their effectiveness. For example, (Qi, Wang, and Shen 2021) use RL to iteratively explore better solutions in Feasible Pump, and (Nair, Alizadeh et al. 2020) use RL to search for better solutions within a neighborhood.\nAlthough these two directions introduce MILP to the benefits of ML and show promising results, they do not scale well to real-world applications. Directly predicting a high-dimensional solution is intractable. The efficiency of IL and RL-based optimization methods is limited by the decision horizon (Ye et al. 2024; Geng et al. 2024; Wang et al. 2022) (i.e., the number of integer variables). Another drawback is their inability to enforce the constraints accurately, making them unsuitable for real-world high stakes applications (Liu et al. 2023, 2024b; Geng et al. 2023a). In contrast, this paper utilizes model reduction theory and focuses on learning the mapping between an MILP instance and its optimal reduced model, providing a fast and interpretable MILP solution.\nLearning to Simplify the MILP. Large-scale MILP formulations usually contain much redundancy, which can be simplified by the pre-solve techniques (Achterberg and Wunderling 2013). To design high-quality pre-solve routines, (Liu et al. 2024a) and (Kuang et al. 2023) recently use RL to determine which pre-solve operators to select and in what order. (Ye, Xu, and Wang 2024) instead use graph partition for problem division to reduce computational cost. These pre-solve-based simplification methods can only identify limited and explicit redundancy. To identify the minimal tight model, (Misra, Roald, and Ng 2022; Bertsimas and Kim 2023) first propose a classification method to predict the optimal reduced model. However, existing methods only consider several equally desirable reduced models, overlooking the various performances of the reduced models. This paper considers the importance of preference information and designs an efficient method to exploit it to improve the learning accuracy of model reduction."}, {"title": "Parameterized MILP and Model Reduction", "content": "Parameterized MILP Problem. The MILP can be formalized as follows:\n$\\begin{aligned}\n\\min_{X} & f(c, x) \\\\\ns.t. \\ g(A, x) &= \\begin{pmatrix}\ng_1(A_1,x_I,x_{-I}) \\le b_1 \\\\\ng_2(A_2,x_I,x_{-I}) \\le b_2 \\\\\n&...\\\\\ng_m(A_m, x_I,x_{-I}) \\le b_m\n\\end{pmatrix} \\\\\nx_I \\in \\mathbb{Z}^d, & \\ x_{-I}\\in \\mathbb{R}^{n-d}\n\\end{aligned}$\n(1)\n(2)\n(3)"}, {"title": "Strategy Generation and Pruning", "content": "In this section, our objective is to identify a set of useful strategies that will be used as labels for model reduction training and learning.\nStrategy Generation. It is difficult to determine the amount of instances required for strategy learning. Following the approach in (Bertsimas and Stellato 2022), we first randomly generate instances $\\Theta$ as well as their optimal strategy $s^*(\\theta)$ until the Good-Turning estimator $\\frac{N_1}{N}$ falls below a tiny value, where N is the total number of instances and $N_1$ is the number of different strategies appeared exactly once. Specially, given N independent instances $\\Theta_N = {\\theta_1,\\dots, \\theta_N}$, we can generate M different strategies $S(\\Theta_N) = {s_1,\\dots,s_M}$. However, in large-scale MILP problems, the number of strategies (i.e., labels) M can grow quickly, making the learning task very difficult. Motivated by this issue, we next propose a strategy pruning method based on SETCOVER technique.\nStrategy Pruning. In practice, each optimal strategy $s^*(\\theta)$ not only applies to the corresponding instance $\\theta_i$, but also may apply to other instances $\\theta_j(\\neq \\theta_i)$. Therefore, many candidate strategies are redundant and we can select only the most useful strategies to apply. We first model the relationship between strategies $S(\\Theta_N)$ and instances $\\Theta_N$ by an Instance-Strategy bipartite graph $G(V_\\theta, V_s, E)$:\n*   The node set consists of the instance nodes $V_\\theta = {v_\\theta^1, ..., v_\\theta^N}$ and the strategy nodes $V_s = {v_s^1, ..., v_s^M}$. Each $v_\\theta^i$ represents an instance $\\theta_i \\in \\Theta_N$, and each $v_s^j$ represents a strategy $s_j \\in S(\\Theta_N)$.\n*   An edge $e_{i,j} \\in E$ exists between $v_\\theta^i$ and $v_s^j$ if applying the strategy $s_j$ to the instance $\\theta_i$, and the infeasibility $p(\\theta_i, s_j)$ and suboptimality $d(\\theta_i, s_j)$ of the reduced problem (5)-(7) are both below a tiny threshold $[\\epsilon_p, \\epsilon_d]$. The infeasibility is defined as:\n$p(\\theta_i, s_j) = \\frac{\\|(g(A,\\hat{x}_{i,j}) - b)^+\\|_{\\infty}}{\\|b\\|},$\nwhere $\\hat{x}_{i,j}$ is the solution of the reduced problem\u00b9. $\\|b\\|$ normalizes the degree of constraint violation based on the magnitude of the constraint $g(A,x) < b$. The Suboptimality measures the relative distance between the recovered solution $\\hat{x}_{i,j}$ of the reduced problem and the optimal solution $x^*_i$ of the instance $\\theta$:\n$d(\\theta_i, s_j) = \\frac{|f(c, \\hat{x}_{i,j}) - f(c, x^*_i)|}{|f(c, x^*_i)|}.$\nThe objective of strategy pruning is to find a minimal subset of strategy nodes $V_s^* \\subseteq V_s$, such that for any $v_\\theta^i \\in V_\\theta,$\nthere exists a $v_s^j \\in V_s^*$ and $e_{i,j} \\in E$. This problem of strategy pruning can be reduced to the well known SETCOVER problem of finding the minimum sets to cover all elements. Thus, an efficient greedy algorithm can be employed to find the useful strategies (Khuller, Moss, and Naor 1999). The main idea of the greedy algorithm is to iteratively select the strategy node $v_s^j$ that is connected to the maximal uncovered instance nodes. This node $v_s^j$ is then added to the candidate set of strategies $V_s^*$ and this strategy selection process continues until all instance nodes are connected to at least one candidate strategy node. Finally, the set of pruned strategies $S_P$ can be obtained from $V_s^*$."}, {"title": "Preference-based Strategy Learning", "content": "Given the pruned strategies $S_P$, this section proposes to learn the mapping from a MILP instance $\\theta$ to a suitable strategy $s(\\theta) \\in S_P$. Previous strategy learning approaches (Misra, Roald, and Ng 2022; Bertsimas and Stellato 2022) focus on predicting the correct strategy and considering all other strategies equally undesirable, failing to integrate the significant instance-strategy preference information deeply.\nPreference Computation. Given an instance $\\theta_i$, we would like to supply a reward $r(\\theta_i,s_j)$ to each strategy $s_j \\in S_P$. The reward $r(\\theta_i, s_j) \\in \\mathbb{r}$ is used to measure the outcome of applying the strategy $s_j$ to the instance $\\theta_i$. We follow the same criteria for calculating the relative feasibility and suboptimality as in Eqs. (8) and (9):\n$r(\\theta_i, s_j) = -log(p(\\theta_i, s_j) + d(\\theta_i, s_j)).$\nDirectly learning the real reward function $r(\\theta_i, s_j)$ between $\\theta$ and $s_j$ is extremely challenging because the complex relationships among instances, strategies and rewards. Instead, to enhance simplicity and training stability, we propose to learn a proxy reward model $\\mathcal{R}_\\phi$ (where $\\phi$ denotes the parameters of the machine learning approach) that can express preferences between strategies. Given an instance $\\theta_i$, for two instance-strategy pairs $(\\theta_i,s_j)$ and $(\\theta_i, s_k)$, we define the preference $\\succ$ generated by the rewards $r$:\n$(\\theta_i, s_j) \\succ (\\theta_i, s_k) \\Leftrightarrow r(\\theta_i,s_j) > r(\\theta_i, s_k).$\nInformally, $(\\theta_i, s_j) \\succ (\\theta_i, s_k)$ indicates that the instance $\\theta_i$ prefers the strategy $s_j$ to the strategy $s_k$.\nPreference-based Sampling. To train the proxy reward model $\\mathcal{R}_\\phi$, previous preference-based learning (e.g., RLHF (Christiano et al. 2017)) requires selecting all possible pairwise comparisons as samples. For example, let $M_P = |S_P|$ denote the number of pruned strategies, there will be $\\binom{M_P}{2}$ preference samples for each instance at the training stage. The number of samples grow quadratically with the number of strategies, thereby increasing the cost of training. Fortunately, in our strategy learning problem, the instance preferences on strategies have a transitivity structure. For example, given the instance $\\theta_i$, if the strategy $s_j$ is preferred to $s_k$, and $s_k$ is preferred to $s_q$, we still have that $s_j$ is preferred to $s_q$. This transitivity property can rank all candidate strategies as a complete order based on preferences (i.e., Eq. (11)). Therefore, for each instance $\\theta_i$, when the candidate strategies are ranked in decreasing order of their preferences (i.e., $S_{\\theta(i)} \\in S = {s_{\\theta(1)}, ..., s_{\\theta(M_P)}}$ is ranked"}, {"title": "Algorithm 1: Preference-based Strategy Learning", "content": "In Eq. (13), based on the row-wise shared weights $W^q, W^k$, and $W^v$, a linear projection operation is acted on the input $[\\{{\\theta_i, S_P\\}}]$ to compute the queries $Q = [\\{\\theta_i, S_P\\}]W^q$, keys $K = [\\{\\theta_i, S_P\\}]W^k$, and values $V = [\\{\\theta_i, S_P\\}]W^v$. The main architecture consists of L layers:\n$A_L(A_{L-1} (...A_1([\\{\\theta_i, S_P\\}]))).$\nThe final output layer performs an affine transformation:\n$\\mathcal{R}_i = \\mathcal{R}_\\phi([\\{\\theta_i, S_P\\}]) = \\psi_L(A_L) = \\psi_L(W_LA_L + b),$\nwhere $\\psi_L$ is the activation function, $W_L$ and $b_L$ are weights of the output layer L. $\\mathcal{R}_i = \\{r_{i,j}\\}$ represents the predicted rewards of all strategies $s_j \\in S_P$ when applied on the instance $\\theta_i$. For this architecture, $\\phi$ is the whole set of parameters that needs to be learned.\nPreference-based Loss Function. In the training phase, the strategies $S_P$ can be ranked to $S_{\\sigma}$ by the rewards $r$, so we can get the reward output $\\mathcal{R}_i$ ordered by $S_{\\sigma}$ and form the ranked predicted rewards $\\hat{\\mathcal{R}}_{i,\\sigma} = {\\hat{r}_{i,\\sigma(1)}, ..., \\hat{r}_{i,\\sigma(M_P)}}$, where $\\sigma(j)$ represents the new position of $\\hat{r}_{i,\\sigma(j)}$ in the sequence. We can define the preference probability $p_{i,j}$ between each pair of adjacent strategies $s_{\\sigma(j)}$ and $s_{\\sigma(j+1)}$:\n$p_{i,j} = \\frac{exp(\\hat{r}_{i,\\sigma(j)})}{exp(\\hat{r}_{i,\\sigma(j)}) + exp(\\hat{r}_{i,\\sigma(j+1)})}.$\nIn order for the model output $\\mathcal{R}_i$ to yield correct relative preferences, we need to train $\\phi$ to maximize the probability $p_{i,j}$ for every ordered adjacent sample pair $(\\theta_i, s_{\\sigma(j)}) \\succ (\\theta_i, s_{\\sigma(j+1)})$. Therefore, we define the preference loss based on the preference:\n$L_p(\\phi) = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{M_P-1} [\\mu_{i,j} log(p_{i,j})+\n(1 - \\mu_{i,j}) log(1 - p_{i,j})],$\nwhere the preference labels $\\mu_{i,j} = \\begin{cases}\n1 \\ \\text{if } (\\theta_i, s_{\\sigma(j)}) \\succ (\\theta_i, s_{\\sigma(j+1)}),\\\\\n0.5 \\ \\text{otherwise}.\n\\end{cases}$\nIf the model outputs an incorrect order, such as the higher $\\hat{r}_{i,\\sigma(2)}$ leading $\\hat{r}_{i,\\sigma(1)} < \\hat{r}_{i,\\sigma(2)}$ and $\\hat{r}_{i,\\sigma(2)} > \\hat{r}_{i,\\sigma(3)}$, although $(\\hat{r}_{i,\\sigma(1)}, \\hat{r}_{i,\\sigma(2)})$ would be penalized by $L_p(\\phi)$, the error in $(\\hat{r}_{i,\\sigma(2)}, \\hat{r}_{i,\\sigma(3)})$ might even reduce the value of $L_p(\\phi)$. To address this issue, we introduce a reward difference-based loss to penalize the incorrect output of $\\hat{r}_{i,\\sigma(2)}$ and to reinforce the correct order within the sequence:\n$L_d(\\phi) = \\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{M_P-1} (\\hat{r}_{i,\\sigma(j)} - \\hat{r}_{i,\\sigma(j+1)} - \\delta_{i,j})^2,$\nwhere $\\delta_{i,j} = r(\\theta_i, s_{\\sigma(j)})-r(\\theta_i, s_{\\sigma(j+1)})$ is the target reward differences between adjacent strategies in the sequence. The loss can effectively deepen the relative preferences and improves training stability. To enhance the coordination between the loss functions, finally, our total loss is:\n$L_{total}(\\phi) = \\lambda_1 L_p(\\phi) + \\lambda_2 L_d(\\phi).$"}, {"title": "Online Strategy Inference", "content": "To overcome the potential prediction errors, reliability can be increased by taking the Top-k output strategies as candidates. Given the parameterized preference model $\\mathcal{R}_\\phi$, and the instance $\\theta_i$, let $\\mathbb{s}_k$ be the set of the $k$ strategies corresponding to the $k$ largest outputs\n$\\mathbb{S}_k = \\{s_j \\mid \\hat{r}_{i,j} \\in Top-k(\\{\\hat{r}_{i,1}, \\hat{r}_{i,2},..., \\hat{r}_{i,M_P}\\}).\\}$\nWe select $\\mathbb{S}_k$ as the strategy candidates for the instance $\\theta_i$, and evaluate the strategies $s_j \\in \\mathbb{S}_k$ by solving the reduction model $s_j(\\theta_i)$. And the $s_j$ with the lowest infeasibility $p(\\theta_i, s_j)$ is selected as the target strategy.\n$\\hat{s} = arg\\underset{s_j \\in \\mathbb{S}_k}{min}\\ p(\\theta_i, s_j).$\nTo solve the reduction model $s_j(\\theta_i)$, for special types of problems such as MIQP (Mixed-Integer Quadratic Programming) and MILP, the linear system can be simplified based on the KKT optimality conditions (Boyd and Vandenberghe 2014), further speeding up the solution time. The workflow in online stage is detailed in Figure 2"}, {"title": "Experiments", "content": "In this section, we compare our proposed method with the learning-based methods and the commercial solvers on real-world datasets to validate the performance and efficiency.\nEvaluation Metrics. We follow the same criteria for solving quality as in (Bertsimas and Stellato 2022), using the accuracy metric. We consider solutions to be accurate if their infeasibility and suboptimality are within a small tolerance. Given N test samples, the testing accuracy on this dataset is:\n$accuracy = \\frac{1}{N} |\\{\\theta_i \\mid p(\\theta_i, \\hat{s}_j) \\leq \\epsilon_1 \\land d(\\theta_i, \\hat{s}_j) < \\epsilon_2\\}|,$\nwhere the tolerances $\\epsilon_1$ for infeasibility and $\\epsilon_2$ for suboptimality are both set to 1 \u00d7 10\u22124.\nDatasets. We evaluate the performance through:\n1.  MIPLIB (Gleixner et al. 2021), six scenarios selected as in (Bertsimas and Kim 2023), the real-world MILP problems with varying scales and solving difficulties.\n2.  Fuel Cell Energy Management Problem (Frick, Domahidi, and Morari 2015), treated as the primary evaluation scenario by (Bertsimas and Stellato 2022). Its scale can be increased by increasing T for deeper analysis.\n3.  Inventory Management Problem, five large-scale (average number of 100,000 constraints) real-world industrial problems from a company's real supply chain scenarios.\nBaselines. We compare our method with:\n4.  Gurobi (Gurobi Optimization 2021), the advanced commercial solver. To make the comparison as fair as possible, we run Gurobi with \"warm-start\" enabled, that reuses the solution obtained from previous parameters.\n5.  Gurobi Heuristic, Gurobi's \u201cheuristic\u201d mode, a very fast heuristic algorithm with time limit of one second.\n6.  MLOPT (Bertsimas and Kim 2023), a model reduction based method that is the most applicable learning-based method in our scenario.\n7.  Predict and Search (Han et al. 2023), a solution-prediction based method with initial variable prediction"}, {"title": "Conclusion", "content": "This paper proposes a preference-based model reduction (i.e., strategy) learning for fast and interpretable MILP solving. There are two challenges for strategy learning: 1) how to generate sufficient strategies that are useful for strategy learning, and 2) how to integrate the performance information of all candidate strategies to improve strategy learning. This paper first introduces the SETCOVER technique to find a minimal set of strategies that can be applied to all MILP instances. Furthermore, the preference information of these available strategies on instances and the attention mechanism are integrated to improve the learning capacity. Simulations on real-world MILP problems show that the proposed method has a significant improvement in solving time and the accuracy of the output solutions."}, {"title": "Appendix", "content": "Dataset and Experimental Setup Supplement. We provide a detailed introduction of the datasets. Datasets 1 and 2 are publicly available, and our settings are largely consistent with previous works. Dataset 3 represents five problem models currently employed by a company, and we briefly describe its scenario due to commercial privacy concerns. However, we have anonymized the data and uploaded a de-identified sample of the model's LP relaxation as a demo in the supplementary material.\nDataset 1 is from the MIPLIB library (Gleixner et al. 2021), and given in the ($A_{eq}$, $b_{eq}$, $A_{ineq}$, $b_{ineq}$, c, lb, ub, I) format. The objective is to minimize $c^Tx$ over the feasible region $\\{x: A_{ineq}x \\leq b_{ineq}, A_{eq}x = b_{eq}, lb \\leq x \\leq ub\\}$, where I is the set of indices for the integer decision variables. We select the key parameters $\\Theta$ for instance generation, treating the original data as the center of the ball and_generate parameters $\\{{\\theta\\}}_i$ uniformly from the ball $\\mathcal{B}(\\theta,r)$. The ranges of $\\epsilon$ and r in our experiments are broader than those in (Bertsimas and Kim 2023) for more challenging applications. The varying parameters $\\Theta$ mainly include $b_{eq}$, $b_{ineq}$, or c and we ensure that none of their entries contain zeros to avoid unreasonable disturbances.\nDataset 2 represents the Fuel Cell Energy Management scenario. Switching fuel cells on and off reduces battery lifespan and increases energy loss; therefore, they are often paired with energy storage devices (such as supercapacitors) to reduce switching frequency during rapid transients. In this scenario, the objective is to control the energy balance between the storage device and the fuel cell to match the required power demand (Frick, Domahidi, and Morari 2015). The goal is to minimize energy loss while maintaining the switching frequency of the fuel cell within an acceptable range to prevent lifespan degradation. The model is as follows:\n$\\begin{aligned}\n\\min_{P,z} f(P_t, z) &= \\sum_{t=0}^{T-1}(\\alpha P^2_t + \\beta P_t + \\gamma z_t)\\\\\ns.t. \\\\\nE_{t+1} &= E_t + \\tau(P_t - P^{load}_t), t = 0, ...,T\\\\ E^{min} &< E_t < E^{max}, t = 0,...,T - 1\\\\ 0 &< P_t < z_t P^{max}, t=0,...,T\\\\ z_{t+1} &= z_t + w_t, t = 0,...,T\\\\ s_{t+1} &= s_t + d_t - d_{t-\\tau}, t = 0, ..., T - 1\\\\ s_t&\\leq \\eta, t = 0,...,T\\\\\nG(w_t, z_t, d_t)&\\leq h, t = 0, ...,T\\\\ E_0 &= E_{init}, z_0 = Z_{init}, s_0 = S_{init}\\\\\nz_t &\\in \\{0,1\\}, d_t \\in \\{0,1\\}, w_{\\tau} \\in \\{-1,0,1\\}.\n\\end{aligned}$"}, {"content": "In the objective function, $z_t \\in \\{0,1\\}$ represents the ONOFF state of the cell, and fuel consumption is given by $\\alpha P^2_t + \\beta P_t + \\gamma z_t$ ($ \\alpha, \\beta, \\gamma > 0$) when the battery is on ($z_t = 1$), where $P_t \\in [0, P^{max}]$ is the power provided by the fuel cell. This problem is a type of Mixed Integer Quadratic Program (MIQP). $E_t \\in [E^{min}, E^{max}]$ denotes the energy stored, and $\\tau > 0$ is the sampling time. $d_t \\in \\{0,1\\}$ determines whether the cell switches at time t, and $s_t$ is the number of switchings that have occurred up to time t. The complexity of the problem model is related to the time step T. For a fair comparison, we maintain consistency with the problem parameters $(\\theta)$ and sampling methods ($\\mathcal{B}(\\theta, r)$) used by (Bertsimas and Stellato 2022) in this scenario. The problem parameters are $\\Theta = (E_{init}, z_{init}, s_{init}, d_{past}, P^{load})$, where $d_{past} = (d_{-\\tau},...,d_{-1})$ and $P^{load} = (P^{load}_0..., P^{load}_T)$. Further information can be found in (Frick, Domahidi, and Morari 2015).\nDataset 3 is a type of Inventory Management Problem. The primary constraints in this scenario include the daily balance of goods entering and leaving the warehouse, limitations on the number of products and replacement parts, and demand fulfillment constraints. The objective function aims to minimize storage and transport costs while meeting demand. In practical applications, the key varying parameters $\\Theta$ include the initial storage quantities and the costs in the objective function c and constraints A, b. This dataset represents a large-scale, real-world supply chain application featuring hundreds of thousands of constraints and variables. For experimental settings, the AdamW was used as the optimizer, with an initial learning rate ranging between 0.0001 and 0.001, depending on the problem size. This learning rate was linearly decayed by a factor of 0.9 every 10 epochs, continuing until 100 epochs were completed. The model was trained with a batch size of 128. The parameter $\\lambda_1$ in (19), which coordinates $L_p$ and $L_d$ was set between 0.8 and 0.9, depending on the problem scenario. Further parameters and detailed settings can be found in the submitted code.\nSupplementary Results on the Fuel Cell Energy Management Problem. Due to space limitations in the main text, we only present the performance of our method on a subset of scenarios from Dataset 2. The complete results are shown in Figure 7. As can be seen, compared to MLOPT, our method consistently demonstrates superiority in infeasibility and suboptimality metrics across all scenarios at the same k values. Moreover, our method exhibits lower standard errors, indicating greater consistency across these metrics.\nCoverage Range of Strategies. (Bertsimas and Stellato 2022) proposed a heuristic pruning method that removes low-frequency strategies based on their occurrence"}, {"title": "Algorithm 2: Strategy Pruning on Bipartite Graph", "content": "In Algorithm 2: Strategy Pruning on Bipartite Graph", "2": "Strategy Pruning on Bipartite Graph Input: $V_\\theta$: Set of instance nodes", "V_s$": "Set of strategy nodes", "E": "Set of edges Output: $V_s^*$: The set of strategy nodes covering all instance nodes 1: $\\mathcal{U"}], "2": "V_s^* \\leftarrow \\O$", "3": "while $\\mathcal{U"}, "neq \\O$ do 4: $v_s^* \\leftarrow arg \\underset{v_s \\in V_s}{max}|\\{v_\\theta"]