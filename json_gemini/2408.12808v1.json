{"title": "VALE: A Multimodal Visual and Language Explanation Framework for Image Classifiers using eXplainable AI and Language Models", "authors": ["Purushothaman Natarajan", "Athira Nambiar"], "abstract": "Deep Neural Networks (DNNs) have revolutionised various fields, enabling the automation of tasks and minimizing human error. However, the internal workings of DNN and the rationale behind its decision-making processes remain unknown due to their 'black-box' nature. As a result, the results' lack of interpretability has limited the use of these models in high-risk scenarios. In an effort to explain and interpret the internal workings of DNNs, a new field of research i.e. eXplainable Artificial Intelligence (XAI), is emerged. Nevertheless, in real-world scenarios, \u03a7\u0391\u0399 encounters certain challenges, such as semantic gap in machine-human understanding, trade-off between interpretability & per-formance and context-specific explanation. To overcome such limitations, we propose a novel multimodal Visual and Language Explanation framework named as (\"VALE\") using explainable AI and language models. Upon the visual explanations provided by the XAI tool, an advanced zero-shot image segmentation model and a visual language model are incorportaed to extract the corresponding textual explanation. This multimodal visual & textual explanation bridges the semantic gap between human and machine interpretation of the results, by providing human-compliant results. In this paper, we conduct a pilot study of the VALE framework on image classification tasks. In particular, Shapley Additive Explanations (SHAP) are applied to the classified images to identify the most influential regions. Further, the object of interest is obtained using the Segment Anything Model (SAM), and the corresponding explana-tion are achieved via the state-of-the-art pre-trained Vision Language Models (VLM). Extensive experimental studies are conducted on two datasets: the ImageNet dataset and a tailor-made underwater SONAR image dataset, demonstrating real-world application in underwater image classification. Results show the promising performance of VALE multi-modal explanation framework.", "sections": [{"title": "1 Introduction", "content": "Image classification is a task that involves predicting class labels to images, based on their visual content [20]. It is extensively used in various applications,"}, {"title": "2 Background and Related Work", "content": "The rest of the paper are organized as follows: the related works on XAI for image classification and image-to-text explainer is presented in Section 2. The overall pipeline of the proposed VALE architecture is explained in Section 3. Further, the experimental setup and experimental results are summarized in Section 4 and Section 5, respectively. Finally, the conclusion and future works are summarized in Section 6."}, {"title": "2.1 Explainable AI(XAI) for Image Classification", "content": "The XAI techniques are classified into two categories: model-specific and model-agnostic. Model-specific techniques are designed to analyze and explain the be-haviour of a specific ML model, taking into account its unique architecture and complexities. These techniques are particularly useful for models such as decision trees and logistic regression [23]. On the other hand, model-agnostic techniques aim to provide explanations that are independent of the particular model being used. Model-agnostic techniques such as LIME, SHAP, CAM, etc. are valuable for explaining DNN models. LIME, developed by Ribeiro et al.n [21], explains image classification model predictions by locally perturbing the input sample and fitting it in a linear model to find the pertinent features for the prediction. Similarly, Bach et al. [3] plot the pixel-wise contribution in each layer of the neu-ral network to explain the model decision on a heat map. In 2017, Lundberg et al [17]. introduced an open-source library to use Shapley scores from cooperative game theory to explain black-box model predictions on structured and unstruc-tured data. SHAP explains classification model predictions in various domains, including medical [23], agriculture [1], aerial imagery [2], etc. Then, the highly popular XAI technique for image and video-based DL models by Selvaraju et al. [24] utilizes Gradient-weighted Class Activation Mapping (Grad-CAM) to vi-sualize DNN predictions as heat maps. In a recent study, Sun et al. [25] used a combination SHAP and LIME explainer to segment the object of interest in the image using the pre-trained model SAM to provide a better visual explanation over using a heat map provided by SHAP explainer."}, {"title": "2.2 Image-to-text Explainer", "content": "Image captioning, or image-to-text explanation, is the process of creating tex-tual descriptions for images [27]. These captions serve as a textual representation of the visual content contained within an image. Wang et al. [27] employed an image encoder and text decoder combination to produce captions for images. Most captioning models have a similar architecture, and visual-textual expla-nations are rarely studied together. However, there are models that explain the process of converting images to text. In a study by Dewi et al. [8], SHAP was used to analyze the performance of Azure Cognitive Service's image caption-ing model and other publicly available models in generating captions. Sahay et"}, {"title": "3 Methodology: Visual and Language Explanation", "content": "In this section, the multimodal Visual and Language Explanation (VALE) frame-work for image classification task is explained. Referring to Fig. 1, VALE con-sists of four separate components: Image classifier, Explainer (SHAP), Image segmenter (Segment Anything model) and Image-to-Text explainer (VLM). All of these modules are detailed in the forthcoming sections."}, {"title": "3.1 Image Classifier", "content": "Image classification is a technique for classifying images by utilizing historical data (training data). Convolutional Neural Networks (CNNs) are a type of ANN specifically used in the field of pattern recognition within images and are more efficient for image classification compared to traditional machine learning mod-els [18]. The prediction using CNNs is represented by the following expression.\n$f = softmax (W^{[L]} . flatten (pooling (g (W^{[1]} * X_{image} + b)))+b^{[2]})$.(1)"}, {"title": "3.2 Explainer: SHapley Additive exPlanations (SHAP)", "content": "The SHAP explainer offers a global approach to explain the predictions made by a black-box model. This explainer is based on cooperative gaming theory and the concept of Shapley values [28], [17]. The model's prediction is determined by computing the Shapley scores for each feature. The following expression com-putes the score of feature i on the overall prediction.\n$\\Phi_{i} = \\sum_{S\\subseteq F\\setminus \\{i\\}} \\frac{|S|!(|F| - |S| - 1)!}{|F|!}[f_{S\\cup \\{i\\}}(x_{S\\cup \\{i\\}}) - f_{S}(x_{S})].$(2)\nwhere, i is the contribution (SHAP value) of feature i, F is the set of all features, S is a subset of F excluding the feature i, |S|! is the factorial of the size of the set S, representing the number of ways to arrange S, |F|- |S| - 1 is the number of features not in S excluding i, and its factorial represents the number of ways to arrange the remaining players, F! is the factorial of the total number of features, representing the total number of ways to arrange all features, $f_{S\\cup \\{i\\}}(x_{S\\cup \\{i\\}})$ is the value function for the coalition S including feature i, $f_{S}(x_{S})$ is the value function for the coalition S without feature i. The above equation can also be written with respect to the prediction from model f to the specific input x,\n$\\Phi_{\\varepsilon}(f, X_{image}) = \\sum_{z' \\subseteq x'} \\frac{|z'|!(M - \\|z'\\| - 1)!}{M!}[f_{x}(z') - f_{x}(z' \\setminus \\{i\\})].$(3)\nwhere, z' is the number of non-zero entries in z', and $z' \\subseteq x'$ represents all z' vectors where the non-zero entries are a subset of the non-zero entries in x'. Based on these scores & for each feature, a heatmap is overlaid on the image to indicate the most important and least important features for the predicted class. Further,\n$P_{co-ordinates} = arg \\max_{1<i<n} (\\phi_{i}(f, X_{image})).$(4)\nwhere, n is the total number of SHAP values computed for the input image $X, \\phi_{i}(f, X_{image})$ represents the calculated SHAP values and $P_{coordinates}$ refers to the index i that corresponds to the highest value of $\\phi_{i}(f, X_{image})$ among all $\\phi_{i}(f, X_{image})$ values for i = 1,2,..., n. In our case, the index is the coordinates from the input image with the highest SHAP value."}, {"title": "3.3 Image Segmenter: Segment Anything Model", "content": "Image segmentation is the technique of partitioning an image into distinct groups of pixels, known as image segments. This process aids in object identification and creates boundaries within the image based on areas of interest, resulting in a more meaningful and simplified analysis. We employ the instance segmentation model viz. Segment Anything Model (SAM) [15] as the de-facto model to segment the region of interest (target object) from the image. SAM is chosen due to its robust zero-shot performance and its ability to generate segmentation using prompts such as points, boxes, and text. Refering to Section 3.2, SHAP explainer identifies and highlights the specific regions in the image that have the highest and lowest contribution to the predicted class. The coordinates $P_{coordinates}$ with the highest SHAP score in the image are used as the prompt (point) to generate the zero-shot image segmentation. This segments the input image and extracts the target object $X_{target}$ from the entire image."}, {"title": "3.4 Image-to-text Explainer via Language Model & Prompt Engineering", "content": "Image-to-text explanation, or image captioning, refers to the process of gener-ating textual descriptions or textual depictions of the visual content present in an image. Most image captioning models typically follow an encoder-decoder ar-chitecture [16]. The encoder is usually a Convolutional Neural Network (CNN) that captures relevant information from the image. The decoder, on the other hand, is a Recurrent Neural Network (RNN) that decodes the captured visual information into a descriptive sequence of text [29]. The key component in such models is the attention mechanism, which allows them to focus on the relevant parts of the image while generating each word in the caption. This captioning can be extended to visual question answering, wherein the user can interact with the generated captions and the user can also provide hints about the image to the model with prompts to get a highly relevant response from the VLM.\nPrompt engineering refers to the systematic approach of designing and im-proving the input queries (prompts) to obtain the desired response from the Language Model (LM). It expands the functionalities of language models with-out altering the core parameters, and it improves and directs language models to produce the desired output. For a custom model, it is important to fine-tune and refine the prompt to achieve the desired output, especially in specialized do-mains where the input data (image) is collected using non-standard processes. In such cases, mentioning the specific technique used in the input prompt leads to better output compared to the standard output. In our case, we strengthen the instructions by incorporating the predicted label from the classification model into the language instructions $X_{instructions}$ to develop the prompt $X_{prompt}$.\nDeveloping such large Vision Language Models (VLMs) is time-consuming and computationally expensive. Therefore, our study leverages the advantage of existing pre-trained VLMs to generate captions for the segmented image from SAM, $X_{target}$. Referring to Fig. 1, SAM predicts three masks, each with different confidence scores; the image with the highest confidence score is processed using a trained language model, which converts the image into a sequence of visual tokens $H_{target}$. The $X_{prompt}$ is processed using the same language model, which"}, {"title": "4 Experimental Setup", "content": "In this section, the dataset used for training, the implementation details, and the evaluation metrics used to assess the model performance are described."}, {"title": "4.1 Dataset for Learning", "content": "ImageNet Dataset: ImageNet is an open-source dataset consisting of 15 mil-lion labeled images and 1000 distinct labels [7]. Each label has a minimum of 1000 images associated with it and is one of the most widely used datasets for training image classification models. One of the main reasons for choosing this dataset is the availability of a diverse range of pre-trained models, which can be used for both commercial and research purposes [7]. The images in this dataset were obtained from numerous sources and have varying dimensions.\nSONAR Dataset: The availability of datasets for critical domains such as defence and medicine is limited due to their sparse and confidential nature. Therefore, we showcase the efficacy of the proposed architecture in the field of underwater SONAR imagery, using a curated tailor-made dataset. This SONAR data is collected by several publicly available datasets that are published for academic research i.e. Seabed Objects KLSG [14] and Sonar Common Target Detection Dataset (SCTD) [30]. In this dataset, we have obtained 753 images of ships, 123 images of planes, and 578 images of the seafloor."}, {"title": "4.2 Evaluation Metrics", "content": "The performance of the classification models is assessed using standard evalua-tion metrics i.e. Accuracy, Precision, Recall and F1-score [26]. There are no es-tablished methodologies for quantifying the performance of the SHAP explainer. However, the performance of the explainer can be visually assessed by analyzing the distribution of scores in the image through a heatmap. The relevant fea-ture in the image should receive a high SHAP value, while non-relevant features"}, {"title": "4.3 Implementation Details", "content": "In this study, five prominent pre-trained models, namely VGG16, Xception, In-ceptionV, ResNet50, and DenseNet121 are used as the image classifier models on the ImageNet dataset. To maintain consistency, we adopt 224 * 224 image dimensions as utilized in the pre-trained models. For the SONAR counterpart, we utilize transfer learning to develop an image classification model by employ-ing DenseNet121, customized with two active layers consisting of 1024 and 512 neurons, respectively. Additionally, we incorporate a dropout layer with a rate of 0.25 and a batch normalization layer. We train the model using an Adam optimizer with a learning rate of 0.0001 and a batch size of 16. For the SHAP explainer, we select a batch size of 50 and specified the maximum evaluation parameter count to be 1000. We choose the zero-shot image segmentation model SAM (Segment Anything Model) for image segmentation. To segment the target object, we utilize the coordinates obtained from the SHAP explainer as the in-put prompt for SAM. We utilize pre-trained VLM's such as Large Language and Vision Assistant (LLaVA) [16], Instuctblip [6], Generative Image-to-text Trans-former (GIT) [27], MiniCPM [12] and InternLM [10], with default parameters such as a temperature value of 0.2, no specified top P value, and a maximum output token limit of 1024, to provide textual explanations. Additionally, the prompt is engineered to align with our specific situation. The implementation is conducted on Google Colab, utilizing an A100 GPU with an allocation of 15GB for training and employing pytorch framework."}, {"title": "5 Experimental Results", "content": "The efficiency of the proposed architecture is accessed through random samples obtained from the ImageNet dataset and the results are explained below:"}, {"title": "5.1 Experimental Analysis on the ImageNet dataset", "content": "Image Classifier: The image classifiers are pre-trained, hence they do not require any additional training. The accuracies of the models are summarized in Table 1. To accommodate computational constraints, the model with the smallest number of parameters and the smallest size i.e. DenseNet121, which achieves an accuracy of 92.3% with 8.1 million parameters, is selected as the de-facto backbone network for further study. The input images are pre-processed and then directly predicted using the pre-trained model. For instance, for the image sample shown in Fig. 2(a), the model predicts the image class as a \u2018bald eagle' with a probability of 100%. This prediction is further explained through the SHAP explainer."}, {"title": "5.2 VALE for SONAR Image Classifier", "content": "In this section, we assess the efficacy of the proposed VALE architecture with a custom dataset and custom-built classification model for an 'in-the-wild' deploy-ment scenario. As explained in Section 3.1, transfer learning with the backbone model DenseNet121 [13] is used to develop a tri-class image classification model on the SONAR dataset. Since the dataset is imbalanced, we use synthetic image generation techniques such as flipping, cropping, and rotation to balance it [20]. We also apply a stratified random sampling approach to split the dataset into train, validate, and test sets, which allows us to effectively assess the image classi-fier's performance. Following extensive training, the classifier produces a training accuracy of 99.32% with 14 epochs. Classifier performance on the validation set and test set are 96.33% and 96%, respectively. The per-class classification report for the test dataset is given in Table 5.\nThe trained sonar image classifier predicted the image in first row of Table 9 as \"Airplane\". Using the proposed architecture, this prediction is further ex-plained using SHAP explanations. Instead of one prompt for SAM, we utilize the top two ($P_{coordinates}$). Note that the number of prompts does not increase the computation but it helps in better segmentation, especially in low-quality or pixelated images as depicted in Table 6. The prompt 'Explain the object in the image: 'Airplane'?' along with the segmented image provides the textual description as depicted in Table 9. Although the description appears satisfactory, it can be further refined to offer an explanation even for images of extremely poor quality images by tuning the prompt."}, {"title": "5.3 State-of-the-art Comparison", "content": "To showcase the efficacy of the VALE architecture, both qualitative and quantita-tive analysis is conducted with state-of-the-art approaches. Our work represents the very first attempt of a multimodal explainer utilizing XAI. As a result, there are no comparative metrics available. However, we have provided a summary of similar XAI approaches in Table 10. Sahay et al. [22] employed LIME to generate textual explanations. Bennetot et al. [4] utilized an encoder-decoder architecture to offer textual explanations for the corresponding visual counterpart. Another recent work Sun et al. [25] used LIME + SHAP to provide visual explanations using SAM. To the best of our knowledge, there is no existing work that offers textual explanations from the visual counterpart obtained from an explainer. In this work, we employed SHAP and pre-trained models to provide comprehensive explanations using both visual and textual components."}, {"title": "6 Conclusion and Future Works", "content": "This work presents a novel multimodal Visual and Language Explana-tion framework (VALE) based on a explainer for the first time in the XAI paradigm to explain the predictions made by image classifiers. The efficacy of VALE on the general ImageNet dataset and the specific underwater SONAR datasets is demonstrated. In both the cases, VALE highlighted the superior per-formance by integrating the SAM and VLM models within the XAI framework that reduces the semantic gap and boosts interpretability and confidence. The use-case scenario for classifying underwater objects using SONAR imagery fur-ther highlighted the practicality of in the wild. Future research aims to improve explainer efficacy by integrating additional XAI techniques like LIME and LRP."}]}