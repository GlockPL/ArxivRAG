{"title": "Understanding Variational Autoencoders with Intrinsic Dimension and Information Imbalance", "authors": ["Charles Camboulin", "Diego Doimo", "Aldo Glielmot"], "abstract": "This work presents an analysis of the hidden representations of Variational Autoencoders (VAEs) using the Intrinsic Dimension (ID) and the Information Imbalance (II). We show that VAEs undergo a transition in behaviour once the bottleneck size is larger than the ID of the data, manifesting in a double hunchback ID profile and a qualitative shift in information processing as captured by the II. Our results also highlight two distinct training phases for architectures with sufficiently large bottleneck sizes, consisting of a rapid fit and a slower generalisation, as assessed by a differentiated behaviour of ID, II, and KL loss. These insights demonstrate that II and ID could be valuable tools for aiding architecture search, for diagnosing underfitting in VAEs, and, more broadly, they contribute to advancing a unified understanding of deep generative models through geometric analysis.", "sections": [{"title": "1 Introduction", "content": "Variational Autoencoders (VAEs) [1] are a central paradigm in the field of representation learning [2] due to the breadth of practical applications [3, 4, 5] and depth of technological developments [6, 7, 8, 9] they have inspired. Their ability to shape useful representations for a wide range of tasks without supervision has made it crucial to interpret their structure and geometry. A first line of work focused on interpreting the latent space and improving the performance of the VAEs by disentangling the latent space features, making each relevant factor of variation of the data dependent on a single latent unit [9]. Since nearest neighbour relations can be used to define meaningful similarities [10, 11], a second line of work [12, 13] described the right notion of distance between datapoints in the latent space and used it to regularise training [14], to improve the clustering [15] and interpolation [16] of the data, and to devise ways to probe and modify their semantics [17].\nIn this study, we take a different perspective: we describe how two geometrical properties of the hidden representations of the VAE, their Intrinsic Dimension (ID), and their Information Imbalance (II) [18] change through all the hidden representations. The analysis of these quantities has proven useful in understanding high-level stages of information processing in convolutional networks [19, 20] and transformers [21]. For instance, in transformers for image generation layers rich in abstract information about data lie between two intrinsic dimension peaks [22] and, in language models, the II can be used to identify blocks that encode semantic information [23]. This work shows that these findings also hold in VAEs despite the very different architecture and training objective. More precisely, we find that (1) in VAEs, the ID profiles have two peaks in the middle of the encoder and decoder and a local minimum in correspondence to the bottleneck, and that (2) the II identifies a first phase of information compression in the encoder and a second one of information expansion in the decoder, irrespective of phases of expansion and compression of the ID. We also show that (3) these geometric features arise only if the bottleneck is larger and the ID of the data and develop the final part of training."}, {"title": "2 Methods.", "content": "Architectures and training details. We build encoder networks consisting of four convolutional layers with 64, 128, 256, and 256 channels and analyse the geometry of their hidden representations as the size K of the bottleneck increases from 2 to 128. As we train the networks on 32x32 pixel coloured images, the encoders have extrinsic dimensions of 3072 (in the input), 16384, 8192, 4096, 1056, and K (in the bottleneck). The decoder reconstructs the original input by mirroring the encoder architecture and adding a final sigmoid activation to generate the output image. We train each architecture for 200 epochs on the ELBO loss using an Adam optimiser with a weight decay of 10-4. We train the VAEs on the CIFAR-10 [24] and MNIST [25] datasets. In Sec. 3, we report the ID and II curves on 5,000 images selected from the CIFAR test set and show the results on MNIST in Sec. C of the Appendix.\nIntrinsic Dimension. The ID of a dataset can be formally defined as the minimum number of variables needed to describe the data with no loss of information [26]. In this work we use the 2NN estimator [27], which computes ID as $N/ \\Sigma_{i}^{N} log \\frac{\\mu_{i}}{ \\mu_{i}^{\\'} }$, where $\\mu_{i}$ is the ratio of the Euclidean distances between a point i and its second and first nearest neighbours, and N is the number of points in the dataset.\nInformation Imbalance. The II is a recent statistical measure grounded in information theoretic concepts that quantifies the asymmetric predictive power that one feature space carries on another [18]. Specifically, the II from space A to space B is written as \u2206(A \u2192 B) and is a number between zero and one. If (A \u2192 B) = 1, then the II is at its maximum, meaning that A carries no information on B. On the contrary, if \u2206(A \u2192 B) = 0, then the II is at its minimum, and A carries full information on B. Importantly, the II is not a measure of mutual information between spaces. In fact, the II is not symmetric as one space can generally be more predictive of another (for instance, if the data are distributed according to the non-invertible function y = sin(x), x carries more information about y than vice-versa). \u0394(A \u2192 B) is related to exponential of the conditional entropy H(CB | CA) of the copula variables CA and CB [18, 28] and, it can be robustly estimated by checking how nearest neighbour relationships in space A are preserved in space B. In practice, if $r_{i}^{A}(r_{i}^{B})$ denotes the distance rank of point j with respect to i in space A (B) the information imbalance can be computed as\n$\\Delta(A \\rightarrow B) = \\frac{2}{N^{2}} \\sum_{i,j | r_{i}^{A} < r_{i}^{B}} T_{ij}$                                                                                                                                                                                                                                                                (1)\nTo compute IDs and IIs, we use the estimators available in the DADApy package [29]."}, {"title": "3 Results and discussion", "content": "We begin our discussion in Section 3.1 by presenting ID and II profiles of trained VAEs and continue in Section 3.2 by studying how they appear during training. We find that ID and II effectively identify a sharp transition in the VAEs' information processing when the bottleneck size exceeds the input's ID and two distinct phases in the learning process."}, {"title": "3.1 Trained Architectures", "content": "The ID identifies a transition through the emergence of a double hunchback profile. The left panel of Fig. 1 shows how the ID changes from input to output for different bottleneck sizes. We first note that, independently of the architecture, the ID in the encoding part of the network increases from around 20 at the input (layer 0) to approximately 80 before decreasing to roughly match the size of the bottleneck layer (layer 5), creating a distinctive 'hunchback' shape. A similar pattern of expansion and compression is observed in the decoding network, leading to the appearance of a double hunchback curve in the ID. However, the second hunchback curve appears exclusively for VAE architectures with sufficiently large bottleneck sizes, roughly from 32 onwards. Interestingly, the transition occurs when the bottleneck size matches the ID of the input data, which can be read from the figure (at layer 0) to be approximately 30.\nThe II identifies a transition through distinct information processing modes. A similar transition after a specific bottleneck size can be observed through the analysis of II profiles. The central panel"}, {"title": "3.2 Training dynamics", "content": "The KL loss identifies two phases of training. The sharp transition described in the previous section is also reflected in qualitatively different training dynamics before and after a specific bottleneck size. The left and centre panels of Fig 2 display the the FID loss [30] and the KL loss, respectively. While the FID loss decreases monotonically as training progresses for all architectures, the KL loss exhibits a differentiated behaviour. Specifically, only for architectures with sufficiently large bottleneck dimension the KL loss goes through two distinct phases. In the first phase, roughly until epoch 10, the KL loss increases, while it decreases in the second phase until the end of training.\nThe II identifies two phases of training. A similar dynamic can be observed in the right panel of Fig. 2, which shows \u2206(l10 \u2192 lo), i.e., the II from the output layer to the input, as a function of the training epoch. The figure illustrates that, after a minimally large bottleneck size, the II"}, {"title": "4 Conclusions", "content": "In this study, we explored the use of geometric tools \u2013the ID and the II\u2013 to analyse the internal representations and training dynamics of Variational Autoencoders. We identify a sharp transition in the behaviour of VAEs as the size of the bottleneck layer increases above the ID of the data, leading to the emergence a double hunchback curve in the ID profile and to a qualitatively different information processing mechanism as measured by the II. Furthermore, we find that architectures with sufficiently large bottleneck undergo two distinct phases in the training dynamic.\nWe envisage two practical implications of our findings. First, they can inform architecture search since a well-performing VAE network can be expected (and measured) to exhibit a double hunchback ID curve and an II information processing involving a compression and a later expansion. Second, monitoring ID and II during training can provide valuable insights into the learning process, potentially enabling diagnostic tools for avoiding underfitting and improving training.\nMore generally, the geometric analysis we propose can be useful for robustly and nonparametrically compare architectures, when dealing with high-dimensional spaces. For example, the double hunch-back shape we observe in VAEs closely mirrors the findings of [22] in their analysis of Transformers trained on ImageNet, suggesting this shape may be a common feature of a large class of deep generative networks. Furthermore, the two training phases we identify resemble the two phases proposed in the Information Bottleneck (IB) theory of deep learning [31, 32]. However, while previous studies on the IB theory and its application to neural networks [33] have faced challenges due to the difficulty of accurately estimating mutual information [34, 35], our approach circumvents these issues by employing distance-based geometric tools that allow reliable studies of the information processing dynamics even in high-dimensional hidden representations.\nFurther work will involve validating these findings in other architectures and establishing a more rigorous connection between the geometric measures and information-theoretic concepts."}]}