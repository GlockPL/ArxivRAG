{"title": "Bridging the Gap between Real-world and Synthetic Images for Testing Autonomous Driving Systems", "authors": ["Mohammad Hossein Amini", "Shiva Nejati"], "abstract": "Deep Neural Networks (DNNs) for Autonomous Driving Systems (ADS) are typically trained on real-world images and tested using synthetic images from simulators. This approach results in training and test datasets with dissimilar distributions, which can potentially lead to erroneously decreased test accuracy. To address this issue, the literature suggests applying domain-to-domain translators to test datasets to bring them closer to the training datasets. However, translating images used for testing may unpredictably affect the reliability, effectiveness and efficiency of the testing process. Hence, this paper investigates the following questions in the context of ADS: Could translators reduce the effectiveness of images used for ADS-DNN testing and their ability to reveal faults in ADS-DNNs? Can translators result in excessive time overhead during simulation-based testing? To address these questions, we consider three domain-to-domain translators: CycleGAN and neural style transfer, from the literature, and SAEVAE, our proposed translator. Our results for two critical ADS tasks - lane keeping and object detection - indicate that translators significantly narrow the gap in ADS test accuracy caused by distribution dissimilarities between training and test data, with SAEVAE outperforming the other two translators. We show that, based on the recent diversity, coverage, and fault-revealing ability metrics for testing deep-learning systems, translators do not compromise the diversity and the coverage of test data nor do they lead to revealing fewer faults in ADS-DNNs. Further, among the translators considered, SAEVAE incurs a negligible overhead in simulation time and can be efficiently integrated into simulation-based testing. Finally, we show that translators increase the correlation between offline and simulation-based testing results, which can help reduce the cost of simulation-based testing. Our replication package is available online [1].", "sections": [{"title": "1 INTRODUCTION", "content": "Deep Neural Networks (DNNs) for Autonomous Driving Systems (ADS) typically undergo two main types of rigorous testing [2]: (1) Offline testing [3], which evaluates DNNs as standalone units using static datasets. (2) Online or simulation-based testing [4-8], which involves embedding DNNs into simulators to test them in interaction with their environment. Both offline and online testing are essential for ensuring the safety and reliability of ADS-DNNs.\nTypically, we use real-world images to train ADS-DNNs. But, for testing, particularly since online testing is required, images from simulators are inevitably used. Although advanced simulators can produce highly realistic images, there are still noticeable differences in style and texture compared to real-world images. \nDue to these dissimilarities, we may test ADS-DNNs on images that have a different distribution than those used for training them. However, it is recommended to test a DNN using a dataset that is close to the distribution of its training dataset [9]. Otherwise, neglecting this could lead to erroneous inaccuracies in test results.\nSeveral deep-learning domain-to-domain translator models have been proposed to convert an image from one data distribution to a different, but related, data distribution. For example, Pix2Pix [10] and CycleGAN [11] use generative adversarial networks (GANs) to adapt an image from one distribution to a corresponding image in another distribution, e.g., changing an image from a winter scene to its summer counterpart. In contrast, neural style transfer methods [12-14] take an image and modify it to embody a desired style, e.g., texture, while preserving the image's primary content.\nRecently, CycleGANs have been used to address the distribution gap between the training and testing datasets for ADS-DNNs [15-17]. Specifically, instead of using synthetic images directly for offline or online ADS testing, these images are first converted using CycleGANs and then fed to ADS-DNNs. This approach reduces the likelihood of ADS-DNNs encountering images outside their training set distribution during testing, resulting in a more realistic evaluation of ADS-DNNs. While these recent studies [15-17] and the adoption of translators in the deep-learning community suggest that translators may be necessary for ADS testing, the following key questions related to the reliability, effectiveness, and efficiency of translators for ADS testing remain unanswered:\nCan translators compromise the quality of test data in terms of diversity, coverage, or fault-revealing ability? Can synthetic images, before applying translators, reveal more flaws in an ADS-DNN than the translated versions of the synthetic images? Are translators efficient enough for online testing? Can translators help reduce the overall effort needed for ADS testing?\nThis paper presents an extensive study to answer the above questions. We compare the accuracy of ADS-DNNs and the quality of the test data for ADS-DNN testing before and after applying translators for both offline and online testing. In addition to the offline and online accuracy metrics, we use test data quality metrics including white-box test coverage metrics, specifically neuron coverage [18] and surprise adequacy [19], and black-box test data diversity metrics, i.e., geometric diversity [20] and standard deviation [21]. Further, we evaluate the fault-revealing ability of test data for ADS-DNNs using a clustering-based fault estimation metric [21].\nAs for the ADS-DNNs under test, we use five public-domain DNNs automating two critical ADS tasks: lane keeping and object detection. We use two widely-used, public datasets, the Udacity Jungle [22] and KITTI [23] for ADS-DNN training. For KITTI, a dataset representing its synthetic counterpart, known as vKITTI [24] is provided for testing. For Udacity Jungle, we develop a synthetic counterpart using the ADS simulator, BeamNG [25, 26], and use the same BeamNG simulator for online testing. We use three image-to-image translators that represent alternative architectures within this category of deep learning models: CycleGAN [11], which is a generative translator model, neural style transfer [12], which is an image perturbation method, and SAEVAE, our proposed translator architecture which aims to address the limitations of CycleGAN and neural style transfer. Specifically, SAEVAE aims to mitigate the high cost of training for CycleGAN and high translation time of neural style transfer.\nOur study starts by confirming the presence of a distribution gap between the real-world and synthetic datasets used for training and testing ADS-DNNs, and demonstrating the effectiveness of translators in bringing the data distributions closer and improving ADS-DNNs testing results in online and offline tests. The effectiveness results for translators are established through the following three research questions addressed in our paper:"}, {"title": "Mohammad Hossein Amini and Shiva Nejati", "content": "RQ1. How well do translators mitigate the distribution gap between training and test datasets? Answer. Our assessment, which is independent from the ADS-DNNs under test, shows that translators can effectively align the training and test datasets.\nRQ2. How well do translators mitigate the erroneous accuracy decline caused by disparities between training and test datasets in offline testing results? Answer. Domain-to-domain translators can significantly reduce the accuracy decline in offline testing, achieving up to a 57% accuracy improvement for the lane-keeping task and up to a 19% accuracy improvement for the object-detection task.\nRQ3. How well do translators reduce the occurrences and severity of failures in online ADS testing? Answer. Translators can reduce the frequency and severity of failures by up to 80% in online testing.\nWe then answer the following questions to evaluate the reliability and efficiency of translators in ADS testing. Specifically, the questions assess the impact of translators on the quality of test data, the time overhead incurred by using translators for online testing, and the potential effort savings in ADS testing using translators.\nRQ4. Can translators preserve the quality of test data in terms of diversity, coverage and fault-revealing ability? Answer. Based on five test quality metrics for DNN testing, translators in most cases preserve the quality of test data, and only slightly reduce test data quality in a few cases.\nRQ5. Is the time overhead incurred by using translators during online testing justified? Answer: Among the three studied translators, SAEVAE has the lowest time overhead, incurring only an 8% increase in simulation time during online testing.\nRQ6. Can translators reduce the cost of ADS testing by increasing the correlation between offline and online testing results? Answer. Translators improve the correlation between offline and online test results for three of our four ADS-DNNs. Increasing this correlation may help reduce the need to online testing, which is expensive, and replace it with more offline testing, which is less expensive.\nNovelty. We present the first study to assess the effectiveness, reliability and efficiency of image-to-image translators for ADS testing. Our empirical study is extensive and encompasses three different image-to-image translation methods for online and offline testing of ADS lane-keeping and object-detection tasks, using five ADS-DNNs, two public-domain datasets and the BeamNG simulator. We assess the reliability of translators in preserving test data quality with respect to five widely-used metrics [18-21, 21, 27].\nRelevance. The use of synthetic and simulator-generated images for testing DNN-enabled systems is inevitable in many domains, including ADS. Image-to-image translators are key to mitigating the erroneous accuracy results for DNN-enabled systems caused by the misalignment of test and training datasets, which is a by-product of using synthetic images for testing. Our paper studies the effectiveness, reliability, and efficiency of image-to-image translators for ADS testing. Our results are important to software engineering since we cannot reliably integrate translators in our testing practices without measuring their impact on our testing practices."}, {"title": "2 IMAGE-TO-IMAGE TRANSLATION", "content": "In this section, we review the two widely-used image-to-image translator models, GAN-based and neural style, and outline their limitations for ADS testing [10-14]. We then introduce an alternative translator, SAEVAE, proposed in this paper. SAEVAE aims to mitigate the high cost of training for CycleGAN and high translation time of neural style transfer.\nGAN-based Translators. GAN-based translators require as input two datasets from two different domains. Some GAN-based methods, e.g., Pix2Pix [10], require paired datasets, which demand extra manual effort. Thus, we consider CycleGAN [11] from the family of GAN-based translators that does not require dataset pairing. CycleGAN utilizes two GANs, each composed of a generator and a discriminator. One GAN learns to translate images from domain A to domain B, and the other learns the reverse. Each generator's job is to produce images that look as close as possible to those in its target domain, while each discriminator's job is to distinguish between images from its source and target domains.\nLimitation of CycleGAN: Training CycleGAN models is complicated and time-consuming because the simultaneous training of discriminator and generator models makes it difficult to reach equilibrium during the training process [28, 29].\nNeural Style Translators. Neural style transfer methods, unlike GAN-based translators, only need two images: one for style and another for content. The style image provides patterns and textures, while the content image supplies the objects. This technique modifies the style of the content image to match the style image, while retaining the original content. A notable technique by Gatys et al. [12] uses a pretrained convolutional neural network (CNN) [30] to separate style and content from images. Early CNN layers capture the image's style, while deeper layers focus on content. The process involves fixing the style image and the pretrained CNN iteratively modifying the content image to match the style by minimizing a loss function.\nLimitation of neural style translators: Neural style translators iteratively adjust the content image using gradient descent, leading to a high execution time.\nSAEVAE Translators. We introduce SAEVAE, a novel translator that aims to address the high training cost for CycleGANs and the high translation time for neural style transfer models. SAEVAE consists of two sequential components: (1) a Sparse Auto Encoder (SAE) [31], and (2) a Variational Auto Encoder (VAE) [32], hence the acronym SAEVAE. Both SAE and VAE are auto-encoders (AE), i.e., models that reproduce their inputs as outputs.\nLet $D_{real}$ and $D_{sim}$ be the real-world and the synthetic datasets, respectively. SAEVAE converts any image in $D_{sim}$ into a corresponding image in the distribution of $D_{real}$. The SAE's role is to extract and reconstruct the most influential features of $D_{sim}$, while the VAE's role is to validate how closely the image generated by the SAE matches the distribution of $D_{real}$.\nFigure 2 shows the training process of SAEVAE translators. We denote the SAE model by $F_s(x)$, and the VAE model by $F_v(x)$. First, the VAE is trained on $D_{real}$ so that it distinguishes samples belonging to $D_{real}$'s distribution, as these samples exhibit a lower reconstruction error compared to those from other distributions. Once the training of the VAE is complete, we freeze its parameters, meaning that no further VAE training occurs in subsequent steps. We denote the frozen VAE by $F'_v(x)$. Next, we train the SAE on $D_{sim}$ using the loss function $L$ defined as follows:\n$L = L_{SAE} + \\alpha L_{VAE}$ (1)\n$L_{SAE} = ||F_s(x) - x||_1$ (2)\n$L_{VAE} = ||F'_v(F_s(x)) - F_s(x)||_1$ (3)\nwhere $x \\in D_{sim}$, $|| \\cdot ||_1$ indicates the l\u2081 distance, and $\\alpha$ is a positive hyperparameter that tunes the balance between adherence to the distribution of $D_{real}$ versus the preservation of the content of the input image from $D_{sim}$. The $L_{SAE}$ term aims to reconstruct the contents of input $x$ from $D_{sim}$, while the $L_{VAE}$ term tries to adjust the SAE output, $F_s(x)$, towards the distribution of $D_{real}$ that was previously used for training the VAE. This way, the SAE model learns to transform images from $D_{sim}$ into images similar to images from $D_{real}$. To translate images from $D_{sim}$'s distribution using SAEVAE, we only use the SAE model and do not need to use the VAE model.\nComparing translators: SAEVAE has significantly less execution time, i.e., translation time, compared to the style transfer method. This is because, to infer images, SAEVAE only performs a forward pass of its SAE model, hence a short execution time. Further, SAEVAE has a lower training cost compared to CycleGAN. This is because in SAEVAE training, VAE is frozen after the initial pretraining. That is, in Equation 3, only $F_s$ is trained and $F'_v$ does not change. Hence, in contrast to CycleGAN, we do not train two complex DLs with competing goals simultaneously. Specifically, in our experiments, we observed that the average execution time for SAEVAE, CycleGAN, and neural style transfer are, respectively, 0.05 sec, 0.30 sec, and 4.69 sec. That is, SAEVAE's execution time is noticeably better than that of both CycleGAN and neural style transfer. Further, as we show in Section 5 and in RQ2 and RQ3, SAEVAE is more effective than CycleGAN and neural style transfer in improving ADS-DNNs' offline and online testing results when the ADS-DNNs have test and training datasets from different domains."}, {"title": "3 ONLINE TESTING WITH TRANSLATORS", "content": "Figure 3(a) shows the augmentation of online ADS testing using translators. A translator is integrated within the online testing loop, which comprises both a simulator and an ADS-DNN. Every image produced by the simulator is fed to this translator, which then transforms it to align with the distribution of the training set of the ADS-DNN. The images generated by the translator are passed to the ADS-DNN to determine its output, e.g., a steering angle value.\nFor example, Figure 1(c) shows the transformation of the simulator-generated image in Figure 1(b) by SAEVAE. As the figures show, the transformed image remains similar to the original synthetic image. However, although it might not be obvious to our eyes, the transformed image aligns better with the real-world image than the original synthetic image. We demonstrate this alignment through a metric that measures the closeness of image distributions in RQ1."}, {"title": "4 EVALUATION SETUP", "content": "Our study is motivated by the increasing need to test ADS-DNNS, trained with real-world images, on synthetic images. When the training and test datasets have different distributions, translators are proposed to bring the datasets' distributions closer. In this section, we assess the effectiveness, reliability and efficiency of translators for ADS testing. Our experiments begin with RQ1, which assesses the effectiveness of translators in reducing the distribution gap between the real-world and synthetic datasets. RQ2 and RQ3, respectively, examine the effectiveness of translators in improving the offline and online test accuracy results. To ensure the reliability of translators for ADS testing, RQ4 evaluates whether translators can preserve test data quality in terms of coverage, diversity, and fault-revealing ability. To assess the efficiency of translators, RQ5 evaluates their time overhead in online testing. Finally, RQ6 examines whether translators can help reduce the cost of ADS testing.\nRQ1. (Training and test data distributions gap mitigation) How effectively do translators mitigate the distribution gap between training and test data for ADS-DNNs? To properly evaluate ADS-DNNs, it is crucial to use test data that shares a similar distribution with their training data. This research question investigates how well translators align the distribution of synthetic test data with the distribution of the real-world data used for training them.\nMetrics: To compare the distribution of a given real-world dataset with its corresponding synthetic dataset, both before and after applying translators, we train a VAE [32] on the real-world dataset. We then compute the VAE reconstruction error for each image in the real-world dataset and in the synthetic dataset, both before and after translation. We use the mean absolute error (MAE) to calculate the reconstruction errors. A VAE trained on a real-world dataset likely exhibits larger reconstruction errors for non-real-world images [32-34]. We leverage this characteristic to compare the similarity between real-world and synthetic datasets.\nRQ2. (Offline testing accuracy gap mitigation) How effectively do translators mitigate the accuracy gap in offline ADS testing when training and test datasets are from different domains? This research question explores whether there is a decline in accuracy of ADS-DNNs when they are tested on synthetic images instead of real-world images. We then examine how well translators reduce this accuracy decline.\nMetrics: We evaluate the accuracy of ADS-DNNs for real-world and synthetic datasets. For the lane-keeping task, we use MAE, and for the object-detection task, we use the mean Average Precision (mAP), a widely recognized metric for object-detection [35, 36].\nRQ3. (Online testing failure reduction) How effectively do translators reduce the occurrence and severity of failures in online ADS testing? ADS-DNNs may exhibit failures during online testing due to the mismatch between the distributions of the simulator-generated images and their real-world training-set images. We investigate if the translators can reduce the occurrence or the severity of such failures in online testing.\nMetrics: We use both the out-of-bound (OOB) distance and the number of out-of-bounds (OOBs) to assess online testing results [16, 26, 37-39]. The OOB distance measures the average difference between the lane's width and the distance from the car to the lane's center across all time steps of each scenario. The OOB distance is maximized when the ego vehicle is centered in the lane and decreases with any deviation. A higher OOB distance value indicates better performance of the ADS-DNN in maintaining the vehicle closer to the lane center on average. In addition to the OOB distance, we report the number of out-of-bounds. An out-of-bounds occurs whenever the percentage of the ego vehicle that is outside the lane exceeds a specific threshold. We use a threshold of 0.5, meaning an OOB occurs when at least half of the car moves outside the lane. In the remainder of this paper, we refer to the OOB distance as OOB and to the number of OOBs as #OOB, respectively.\nRQ4. (Test data quality preservation) Can translators preserve the test data quality in terms of diversity, coverage and fault-revealing ability? We investigate whether using translated data for testing may result in the loss of any qualities of the original test data in exercising ADS-DNNs.\nMetrics: For test-data diversity, we use the geometric diversity (GD) [20] and standard deviation (SD) [21]. For test coverage, we use neuron coverage (NC) [18] and surprise adequacy (SA) [19]."}, {"title": "4.1 Offline Datasets and ADS-DNNS", "content": "Below, we first introduce the offline datasets for the lane-keeping and object-detection ADS tasks. For each task, we use a training dataset representing real-world features and a synthetic, test dataset generated by a simulator. We then describe different ADS-DNNs used to automate the tasks of lane keeping and object detection.\nLane-keeping datasets. For the lane-keeping training dataset, we use the Udacity Jungle dataset [22]. While this dataset is not taken from the real world, it is commonly used to train ADS-DNNs for the lane-keeping task because its images closely resemble real-world images. Furthermore, as we show in RQ1, the image distribution of the Udacity Jungle dataset is distinct from those generated by BeamNG, our ADS simulator, making this dataset a suitable match for our experiments. Each Udacity Jungle image is labelled with a ground-truth steering angle obtained from a human driver.\nWe generate the synthetic lane-keeping dataset corresponding to the Udacity Jungle dataset, using the BeamNG simulator [25]. We configure the BeamNG simulator to ensure that both the road and landscape match with those in the images of the Udacity Jungle dataset. To compute MAE results for RQ2 and RQ6, we need ground-truth steering angle labels for a test-set split of synthetic images. For RQ2, we automatically label the synthetic dataset generated by BeamNG with steering angles obtained from BeamNG's Al controller (BeamNG.AI). Since, for the RQ6 experiments, the ego vehicle is controlled by an ADS-DNN instead of BeamNG.AI, BeamNG does not provide ground-truth steering angles. Hence, we use an independent PID controller to label images for RQ6. For further details, see Section 5.7.1. The synthetic lane-keeping dataset is provided in our in our replication package [1].\nObject-detection datasets. We use KITTI [23] and virtual KITTI (vKITTI) [24], respectively, for the real-world and synthetic object-detection datasets. The KITTI benchmark is extensively used for object detection, while vKITTI, its synthetic counterpart, was developed using the Unity game engine [41]. The images in KITTI and vKITTI show a moving vehicle in various road environments."}, {"title": "4.2 Training ADS-DNNs and Translators", "content": "As per the process shown in Figure 3(b), we train the ADS-DNNs and the translators in our study using our real-world datasets for lane-keeping and object-detection tasks. Table 1 presents the properties of the datasets and how we split them for training ADS-DNNs and translators, and for answering research questions. Specifically, we split the real-world datasets into 70% for training and tuning ADS-DNNs, and 30% for testing, i.e., evaluating ADS-DNNs' accuracy on the real-world dataset. We train the ADS-DNNs with 50 epochs, batch size of 128 as per the standard practices [45].\nAs for the translators, we train SAEVAE, CycleGAN and neural style translators for both lane-keeping and object-detection tasks to bring the synthetic images closer to the distribution of real-world images. We train six translator models developed based on three different translator approaches discussed in Section 2 and for two different ADS tasks. Note that the datasets used for training translators do not need to be labelled.\nWe tune ADS-DNNs and translators based on a 30% random split from their training set as a validation set. For ADS-DNNs, we use a grid search for tuning and train several models and keep the best one yielding better loss on the validation set. For translators, we tune hyperparameters using grid search and initialize the weights using Xavier initialization [46] to reduce randomness and increase robustness. We then train five SAEVAEs and five CycleGANs. We stage their training by saving model weights at each epoch and record the model with the lowest validation loss. For SAEVAE, the loss for the trained models show negligible differences. This is due to the more stable training of SAEVAE, since unlike CycleGAN, the training of SAE and VAE is not intertwined. Training SAEVAE is similar to training any simple DNN, e.g., CNNs. For CycleGAN, we select the model with the lowest validation loss. For neural style translators, we choose several random images from the real-world datasets as style images. Among the resulting neural style translator models, we keep the one yielding the lowest MAE and the highest mAP values on the synthetic test set."}, {"title": "4.3 Setup for Online ADS Testing", "content": "We use the BeamNG simulator [25] for online testing of ADS-DNNs and to answer RQ3, RQ5, and RQ6. Specifically, we use the ADS testing framework provided by the Cyber-Physical Systems Testing Tool Competition track of the SBFT workshop [26]. This framework monitors the vehicle's ability to navigate the road while keeping within its lane. This framework provides alternative test input and road generators to challenge ADS-DNN's maneuvering [38, 39, 47]. Our goal, however, is to assess translators' performance on a diverse set of roads instead of exercising the maneuvering capabilities of ADS-DNNs. Hence, we use, among these road generators, the naive random road generator [47], which generates roads by interpolating four randomly generated waypoints. We discard all invalid roads and ensure that all the roads used in our study are valid. We set the ego vehicle's speed to 30km/h to be consistent with the average speed of the ego vehicle in the Udacity Jungle dataset. In addition, the generated roads are two-lane rural roads passing through a green landscape without any other vehicles, buildings, pedestrians, or objects. This ensures that the scenes produced by BeamNG are similar to the images in the Udacity Jungle dataset. The generated roads are available as part of our replication package [1]. We use the generated roads to test our four end-to-end ADS-DNNs, i.e., Autumn, Chauffeur, Dave2 and Epoch with and without a translator. Since YOLO is not an end-to-end ADS, it cannot drive a vehicle on its own. Hence, it cannot be used within this online test setup."}, {"title": "4.4 Test Data Quality Metrics", "content": "We consider test data quality metrics to measure coverage, diversity, and fault-revealing capability. These metrics can be white box or black box. White-box metrics use the architecture of the DNN under test, while black-box metrics are independent of the DNN's architecture. In our evaluation, we compute the black-box metrics for our synthetic datasets before and after translation, and the white-box metrics for each pair of an ADS-DNN and a synthetic dataset before and after translation. The metrics are described below.\nWhite-Box Metrics. We assess the coverage of each test set for each DNN under test using neuron coverage (NC) [18] and surprise adequacy (SA) [19]. Given a test set and a DNN, NC measures the percentage of neurons of the DNN whose activation levels exceed a specified threshold. We chose 0 for the threshold as suggested in the original paper introducing the NC metric [18]. To compute SA, given a test set and a DNN, we measure the percentage of the test inputs in the test set that are surprising for the DNN. To determine if a test input is surprising for a DNN, we calculate its surprise amount, defined as the Mahalanobis distance between the test input's latent space vector and the distribution of latent space vectors from the DNN's training set. A test input is surprising if its corresponding Mahalanobis distance is larger than a specified threshold. We choose the threshold to be one standard deviation above the mean of the surprise distribution of the DNN's training set to ensure that the majority of the training set images are not considered surprising for the DNN. Note that our preliminary results show that different thresholds for the SA metric do not change the relative comparison of test datasets' coverage before and after translation. More precisely, the choice of the SA threshold does not affect the relative differences in SA values, provided that the majority of the training set is considered unsurprising and consistent thresholds are used for both sides of the comparison.\nTo assess the fault-revealing ability of a given test set for a DNN under test, we compute the number of clusters in the latent space vectors obtained by feeding the test inputs in the test set to the DNN. This measure for the fault-revealing ability of the DNN's test data is adopted from previous work [21, 27]. The intuition behind this measure is that misbehavior-inducing test inputs that form a cluster in the latent space of the DNN under test likely point to the same fault. Hence, the number of clusters obtained for each test set can serve as a proxy for the number of faults that the test set can reveal.\nBlack-Box Metrics. We use geometric diversity (GD) [20] and standard deviation (SD) [21] to measure test data diversity. Given a test dataset, GD measures the hypervolume spanned by the latent space vectors of the dataset. These latent space vectors are derived by feeding the test set to a pretrained DNN, different from the DNN under test. We use VGG19 [30], a well-known vision model, for this purpose. The main idea behind GD is that the latent space vectors are compact representations of the high-level features of the test data, and can be used to assess how diversely the test set spans the feature space and exercises the DNN under test. For SD, we construct a vector in which each component represents the standard deviation of that component across all latent space vectors generated during GD calculations. The SD metric is the Euclidean norm of this vector. The goal of SD is to quantify the variability within the feature space of the test data."}, {"title": "5 RESULTS", "content": "In this section, $D_{real}$ denotes our real-world datasets, and $D_{sim}$ denotes our synthetic datasets. As shown in Table 1, $D_{sim}$ is exclusively used for testing, whereas $D_{real}$ is partitioned for both training and testing. For our results, we specify whether the training or testing partitions of $D_{real}$ are used, denoting them by $D_{real, train}$ and $D_{real, test}$, respectively. Further, the CycleGAN and neural style translators are referred to as cycleG and styleT, respectively.\n5.1 RQ1: Test data distribution gap mitigation\nWe investigate if translators can effectively mitigate the distribution gap between the $D_{real, test}$ data and the $D_{sim, test}$ data. For each task, we train a VAE on the $D_{real, training}$ data. We tune the VAE according to the tuning process described in Section 4.2 for our ADS-DNNs and translators. We then use the trained VAE to reconstruct $D_{real}$ and $D_{sim}$ test sets as well as the translations of $D_{sim}$ obtained by the SAEVAE, cycleG and styleT translators.\nFigure 4 compares the reconstruction-error distributions for the six different datasets for both lane-keeping and object-detection tasks. As expected, $D_{real}$ training and test sets have very close reconstruction-error distributions. Figure 4(a) shows a significant difference between the distributions of the $D_{sim}$ and $D_{real}$ test sets for the lane-keeping task. Further, it shows that SAEVAE can reduce the gap between the $D_{sim}$ and $D_{real}$ distributions more effectively compared to cycleG and styleT. As for the object-detection task, since the differences between the box plots in Figure 4(b) are not visually distinct, we report the statistical-test results in Table 2."}, {"title": "5.2 RQ2: Offline testing accuracy gap mitigation", "content": "To evaluate the effectiveness of translators, i.e., SAEVAE, cycleG and styleT, in reducing the offline testing accuracy gap, we first transform each $D_{sim}$ set for each ADS task using each translator. This process yields six different translated test sets, i.e., three for each ADS task. We then apply our five ADS-DNNs - four for lane keeping and one for object detection \u2013 to their respective $D_{real,test}$, $D_{sim}$, and the six translated test sets.\nFigure 5 shows the MAE results assessing the accuracy of the lane-keeping task of our four ADS-DNNs applied to $D_{real,test}$, $D_{sim}$, and the three translations of $D_{sim}$ by SAEVAE, cycleG and styleT, respectively. A lower MAE indicates better lane-keeping accuracy. As expected, the accuracy of $D_{sim}$ is significantly degraded compared to that of $D_{real,test}$, indicating a substantial decline in accuracy for synthetic images compared to real-world images. Among the three translations of $D_{sim}$, the one obtained by SAEVAE has the best accuracy, i.e., lowest MAE. Table 3 compares the MAE results in Figure 5 using the Wilcoxon statistical test and the Vargha-Delaney effect size. The table shows that the MAE results obtained by applying SAEVAE to $D_{sim}$ are significantly better than those for $D_{sim}$ before translation or with other translations.\nTable 4 shows the mAP results evaluating the accuracy of the object-detection task for $D_{real,test}$, $D_{sim}$, and the three datasets obtained by SAEVAE, cycleG and styleT, respectively. The mAP results are obtained by assessing if our object-detection ADS-DNN can identify approximately 24K car instances in the datasets. Note that we only assess the object-detection task for the car class, since other classes common to both KITTI and vKITTI have significantly fewer instances (i.e., less than 2,000), which is insufficient for training and assessing a model. Table 4 reports both mAP50 and mAP50-95: mAP50 calculates the mAP metric at a 50% IOU threshold, while mAP50-95 averages the mAP metric across IOU thresholds from 50% to 95%. A higher mAP indicates better object-detection accuracy. Similar to the results for lane keeping, the object-detection results confirms that (1) there is a significant accuracy gap between the real-world and synthetic images, and (2) SAEVAE outperforms the other translators in reducing the accuracy gap between real-world and synthetic images."}, {"title": "5.3 RQ3: Online testing failure reduction", "content": "In this research question", "times": "initially without a translator, and subsequently with each of the three translators separately.\nTable 5 shows the average OOB and #OOB over 100 scenarios for each lane-keeping ADS-DNN both without any translator and with each of the translators applied individually. Values highlighted in blue are the best, or within 0.01 of the best, OOB and #OOB values for each ADS-DNN. For OOB, higher values are better, and for #OOB, lower values are better. For both OOB and #OOB, SAEVAE yields the best or near-best averages for all ADS-DNNs. Further, in three cases, cycleG yields OOB and #OOB averages that are on par with those obtained by SAEVAE. However, the OOB and #OOB averages obtained by styleT and no translator are never on par with those obtained by SAEVAE or cycleG.\nTable 6 shows the statistical test and effect-size results comparing the OOB and #OOB distributions. For all ADS-DNNs except Dave2, the SAEVAE's OOB results are significantly better than those obtained without translators or with cycleG and styleT, with small, medium, or large effect sizes. Further, for all ADS-DNNs except Epoch, the SAEVAE's #OOB results are significantly better than those obtained"}]}