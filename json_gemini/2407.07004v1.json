{"title": "Empirical analysis of Biding Precedent efficiency in the Brazilian Supreme Court via Similar Case Retrieval", "authors": ["Rapha\u00ebl Tinarrage", "Henrique Ennes", "Lucas E. Resck", "Lucas T. Gomes", "Jean R. Ponciano", "Jorge Poco"], "abstract": "Binding precedents (S\u00famulas Vinculantes) constitute a juridical instrument unique to the Brazilian legal system and whose objectives include the protection of the Federal Supreme Court against repetitive demands. Studies of the effectiveness of these instruments in decreasing the Court's exposure to similar cases, however, indicate that they tend to fail in such a direction, with some of the binding precedents seemingly creating new demands. We empirically assess the legal impact of five binding precedents, 11, 14, 17, 26 and 37, at the highest court level through their effects on the legal subjects they address. This analysis is only possible through the comparison of the Court's ruling about the precedents' themes before they are created, which means that these decisions should be detected through techniques of Similar Case Retrieval. The contributions of this article are therefore twofold: on the mathematical side, we compare the uses of different methods of Natural Language Processing - TF-IDF, LSTM, BERT, and regex for Similar Case Retrieval, whereas on the legal side, we contrast the inefficiency of these binding precedents with a set of hypotheses that may justify their repeated usage. We observe that the deep learning models performed significantly worse in the specific Similar Case Retrieval task and that the reasons for binding precedents to fail in responding to repetitive demand are heterogeneous and case-dependent, making it impossible to single out a specific cause.", "sections": [{"title": "1 Introduction", "content": "In recent years, the progress in Natural Language Processing (NLP) sparked significant interest in its application within the legal domain. Brazil, renowned for having the world's highest volume of legal cases, is no exception and has already witnessed several implementations of Machine Learning methods in its judiciary system [51]. Notably, these algorithms have the potential to assist legal professionals in finding cases similar to a given one, or a given legal topic a task commonly referred to as Similar Case Retrieval. While these methods are supported by numerous mathematical studies and proofs of concept, one could argue that some of them lack empirical validation. This article seeks to bridge this gap by comparing various document retrieval methods and evaluating their results from a legal perspective.\nIn particular, this article explores the legal instrument of binding precedent (S\u00famula Vinculante, abbreviated BP), that emerged in Brazil in the 2004 judicial reform. Its purpose was to address the issue of an overwhelming number of cases with repetitive demands inundating the Brazilian Federal Supreme Court (Supremo Tribunal Federal, STF), which ideally should handle only a limited number of cases. This situation was partly due to the Brazilian legal system's civil law approach, where Supreme Court decisions don't serve as authority for lower courts, leading to continued case congestion. To mitigate this, Constitutional Amendment No. 45 introduced instruments inspired by the common law system, including binding precedents. They aimed at standardizing jurisprudence, providing normative force over lower instances and the broader public administration.\nAfter its publication, one expects the binding precedent to be cited frequently, until the legal understanding of the courts on the subject calms down, resulting in a significant decrease in the number of citations. However, among the most cited binding precedents, this trend is by no means observed. On the contrary, they show steady growth, as illustrated in Fig. 1 for the five precedents of interest in this paper: 11, 14, 17, 26 and 37. They were chosen for their high number of citations, as well as the variety of legal topics they cover, from administrative to criminal law.\nThe aim of this article is to shed light on the reasons why these binding precedents have not led, as expected, to a reduction in repeated demands. To this end, we employ Similar Case Retrieval methods to trace the history of these precedents and quantify certain trends. This information is subsequently used to provide a legal analysis. In summary, this article includes two main mathematical and two juridical contributions:\n1. The application and comparative analysis of classical algorithms for Similar Case Retrieval on a database of Brazilian legal documents (including TF-IDF-based models, LSTM, BERT and regex);\n2. The outline of a methodology for assessing the impact of a law on jurisprudence, through time series of similar cases and features' correlations;\n3. Application of the mentioned methodology to five binding precedents emitted by the Brazilian Federal Supreme Court, enabling an empirical study of the juridical mechanisms behind their inefficiency;\n4. The identification of five main hypotheses explaining the large number of cases reaching the Supreme Court."}, {"title": "Dataset and reproducibility", "content": "For our analysis, we will use a set of decisions of the Supreme Court produced between 1989 and 2018, collected and annotated by the project Supremo em N\u00fameros [23]. In fact, these documents were scraped from the official STF website, where they can be found one by one. The collection of documents gathered by the project is, however, not publicly available, and has been kindly provided to us. More precisely, we will consider two subsets of this collection, detailed further in Section 3.1: Dataset #1, consisting of all decisions citing a binding precedent (29,743 documents), and Dataset #2, gathering all decisions belonging to the topics \"administrative law\", \"criminal law\" or \"criminal procedure law\" (615,262 documents). We point that the data gathered by Supremo em N\u00fameros have already been used in several works [14, 13, 47]. In addition, other similar databases have been reported, such as the one maintained by the project Victor [20]."}, {"title": "Overview", "content": "The remainder of this article is organized as follows. In Section 2, we set out the legal context behind the Brazilian's Supreme Court binding precedents, with a particular focus on the five precedents specifically studied in this article, and we give an overview of the mathematical literature surrounding Similar Case Retrieval. Datasets #1 and #2 are introduced in Section 3, as well as the models used throughout this article (TF-IDF, LSTM and BERT), that we train on the former dataset. The application of these models"}, {"title": "2 Related works", "content": ""}, {"title": "2.1 Legal context", "content": ""}, {"title": "The BPs as a juridical tool", "content": "The type of precedent known in Brazil as S\u00famula Vinculante (Binding Precedent) emerged in the judiciary reform of 2004, through Constitutional Amendment No. 45 (EC 45/2004, Article 103-A2), to standardize decisions, based on the importation of the jurisdiction logic usual in common law [39, p. 827]. The introduction of these instruments can be understood as an attempt to unify decision-making, aiming to achieve equality and legal certainty, i.e., to avoid that identical cases are decided in different ways, violating the constitutional guarantee of equality before the law [8, p. 20]. In fact, the new Federal Constitution Article requires, for the creation of a BP, the fulfillment of three conditions: (i) dealing with a matter under current controversy among judicial bodies, or between them and the public administration; (ii) representing a risk of serious legal uncertainty; and (iii) being the subject of significant multiplication of processes on an identical legal issue. Thus, as a legal instrument, the BP seeks not only to guarantee equality in judicial decisions, but also to increase the efficiency of the Judiciary, by avoiding delays in jurisdictional intervention.\nTo date, 58 BPs have been published. It should be noted that, in the literature, there is a considerable criticism of the BPs, whether in terms of their institution, their formulation, or their consequences. These issues will be explored in detail throughout the article. We refer the reader to the work of [3] and [43] for a general analysis of BPs."}, {"title": "The precedents 11, 14, 17, 26 and 37", "content": "Once a BP is published, is it expected that the subject ceases to be a matter of controversial interpretation. Consequently, the issue should also cease to generate processes with identical demands and, primarily, prevent these from continuing to be brought before the Higher Courts. It is therefore important to establish, empirically, the degree of efficiency of this type of instrument, here understood as the degree of reduction in similar cases reaching the final stage of appeal. In this article, we analyze the impact of the creation of five binding precedents, chosen as those generating many cases, but also for the diversity of legal topics they cover. Three of them belong to criminal law (11, 14 and 26), and the others to administrative law (17 and 37). We give in the list below a brief summary of their content and refer the reader to the corresponding section for a deeper explanation, as well as the results of our analysis.\nBP 11, the 2008-11-12 (see Section 4.2): Determines the use of handcuffs acceptable only when some risk is anticipated, predicting disciplinary punishments to the responsible public agents and/or nullity of the penal process in the case of unjustified use.\nBP 14, the 2009-02-02 (see Section 4.3): Grants to the investigated individual and their attorneys full access to all documented evidence in ongoing criminal investigations.\nBP 17, the 2009-11-10 (see Section 4.4): In Brazil, public administration at all levels, when sentenced to pay debts resulting from juridical decisions, incorporate these"}, {"title": "2.2 NLP for legal documents", "content": ""}, {"title": "Text embeddings", "content": "The literature surrounding NLP for legal document analysis is rich, reflecting the growing interest in leveraging computational techniques to navigate the complexities of legal texts. Well-studied topics in this area include the analysis of the precedent network [36, 14, 41], Named Entity Recognition [13], summarization of legal texts [28, 26] and prediction of judicial outcome [2, 4, 5]. In addition, of particular interest to us is the detection of similar documents, reviewed further in the next paragraph.\nTo tackle these problems, a standard technique consists of embedding (i.e., vectorizing) the documents, the most classical methods being bag-of-words, TF-IDF, and n-grams. Among recent techniques of words and documents embeddings, we can cite GLOVE [42], contextualized word representation [44], word2vec [12], doc2vec [33], Latent Dirichlet Allocation (LDA) [7], entities and relations-based embedding [72], Universal Sentence Encoder (USE) [10] and TextCNN [11]. Another alternative are the pre-trained language models, such as BERT [21] and its variants [75, 45, 29, 67].\nIn particular, word2vec is used by [24] to study appellate court modifications in Brazilian legal documents, that is, modifcations by the Supreme Court of the lower Court judge's decision. On the other hand, TF-IDF has been already used for the analysis of binding precedents, and has been reported to outperform other embeddings (such as doc2vec and USE) [47]. We will consider, in this article, TF-IDF embeddings coupled with several classifiers, as well as a recurrent neural network (LSTM) and a Large Language Model (BERT)."}, {"title": "Automatic Similar Case Matching and Retrieval", "content": "At their most elementary level, although having a specific binding property, BPs are precedent and, as such, the task of searching for documents similar to the object of a BP, in content and language, may be modeled through textual proximity and/or classification techniques. The problem of identifying juridical decisions similar in a corpus of documents, known as automatic Similar Case Matching, has received significant attention in the literature. For example, a multitude of NLP embedding models have been compared in the CAIL2019-SCM dataset, a corpus of thousands of decisions published by the Supreme People's Court of China, in which the task is to determine, for a triple"}, {"title": "3 Datasets and methods", "content": ""}, {"title": "3.1 Datasets", "content": ""}, {"title": "Documents", "content": "Cases decided at the STF are consolidated in a document, containing the decision's text, as well as the publication date, the Justice that conducted the process (Justice rapporteur), and the document type (e.g., Complaint or Extraordinary Appeal). The research project Supremo em N\u00fameros gathered, in text format, more than 2,500,000 of these documents [23]. In this collection, we shall consider the subset of documents that cite a binding precedent, among the 58 precedents edited by STF until 2018. This amounts to 29,743 documents, which we refer to as Dataset #1 (see Table 1). In addition, we shall consider the so-called Dataset #2, consisting of those documents, in the whole collection, labeled as \u201cadministrative law\", \"criminal law\" or \"criminal procedure law\u201d, gathering 615,262 documents. These are the categories of the five particular BPs we will study (already presented in Section 2.1). We draw the reader's attention to the fact that Dataset #1 is not a subset of Dataset #2, though the latter contains approximately 36% of the former.\""}, {"title": "Labels", "content": "In the context of Similar Case Retrieval, Dataset #1 is particularly convenient. Indeed, since the 58 binding precedents cover different themes, one decides whether a document deals with a specific theme by reading the corresponding BP it cites. In opposition, no information is available in Dataset #2 regarding the juridical content of the cases. For this reason, we will employ the former to train our models. In the rest of this section, only Dataset #1 will be considered, while the application of our models to Dataset #2 is the content of Section 4."}, {"title": "3.2 Models for Similar Case Retrieval", "content": "In this section, we delve into the methodologies employed for analyzing our dataset, focusing on four distinct approaches: TF-IDF, LSTM, BERT, and regex. Together, they faithfully represent some of the most common NLP methods."}, {"title": "TF-IDF", "content": "Term Frequency-Inverse Document Frequency (TF-IDF) is a vector embedding technique divided into two parts. In the \"term-frequency\" part, each document d is embedded in the space $R^n$, where n is the number of different words on the corpora, and where the i-th component of the vector is the frequency that the i-th word of the corpora appears in the document. We denote it by $TF(i, d)$. The \"inverse document frequency\" part is another $R^n$ vector, with the i-th coordinate given by the logarithm\n$IDF(i, d) = log \\frac{\\text{#documents in the corpora}}{\\text{#documents in the corpora that i appears}}$\nFinally, the TF-IDF vector has the i-th coordinate given by the product\n$TF\\text{-}IDF(i, d) = TF(i, d) \\times IDF(i, d)$.\nBy itself, TF-IDF is only an embedding, so it must be coupled with some classification algorithm for us to perform Similar Case Retrieval. In this article, we will use three kinds of classification models for the embedded documents: linear Support Vector Machine (SVM), logistic regression, and random forests. We will not explicitly describe these classifiers here, but refer the interested reader to [27]."}, {"title": "LSTM", "content": "A recurrent network architecture was also implemented for this task, given the well-known capacity of these models to work with textual data, especially for longer texts [27, 50]. The implemented neural networks were composed of three layers: one embedding layer, followed by two layers consisting of 128 LSTM (Long Short-Term Memory) neurons each, all using relu activation functions. For the embedding layer, tensorflow was used, where the vocabulary (i.e., set of unique words in the corpora) was restricted to the 10,000 most common terms, using 1,000 out-of-bucket embedding for the rest of them. Moreover, because the architecture expects a constant input size of documents, only the first 512 tokens of each document were considered, using additional masking for shorter documents. Although naive, this approach is common when dealing with long text analysis in neural networks. Finally, the output consisted of a single neuron followed by a sigmoid non-linearity. We stress that no pre-training of the network's weights was used."}, {"title": "BERT", "content": "Bidirectional Encoder Representations from Transformer (BERT) [21] is a common Transformer-based model [66] used for several NLP tasks that can benefit from a deep embedding of a word or a sequence, e.g., a text classification. In this model, each token (word or part of a word) is vectorized and their vectors are sequentially updated to incorporate their neighbors' information. BERT models are released with their weights already optimized in a large amount of text in a self-supervised manner. The output of the model is a vector for each token and one for the entire sentence (corresponding to the first CLS token). These vectors can be used as features for any classification model, e.g., a logistic regression. One can also extend this neural network by adding a downstream neural network layer, like a linear classification layer, and then both the new layer and the rest of the model are trained together in the downstream task. Just like the LSTM implementations, BERT models have a text input size limited to 512 tokens. Because of that, texts were truncated to their first 510 tokens (to include other BERT special tokens).\nIn our work, we use a Portuguese BERT (BERTimbau BASE4) model fine-tuned in approximately 16 GB of Brazilian court decision raw data for eight epochs. This data gather decisions from the five most important Brazilian courts: Supremo Tribunal Federal, Superior Tribunal de Justi\u00e7a, Tribunal de Justi\u00e7a de S\u00e3o Paulo, Tribunal de Justi\u00e7a do Rio de Janeiro, and Tribunal Superior Eleitoral."}, {"title": "regex", "content": "Regular expression (regex) stands out as a commonly used tool among professionals for retrieving similar cases. In particular, it constitutes the search engine of the official STF website. We designed a regex search for each of the five BPs by selecting, with the help of legal expertise, the most important words in their statements (please refer to Sections 4.2 to 4.6 for the statement and juridical context of each BPs). We draw the reader's attention to the fact that, as our models are intended to detect, in a subsequent section, the documents before creation of the BPs, we do not use the words S\u00famula Vinculante or any specific mention of a law. More precisely, we consider:\nBP 11: the words algemas or algemado (handcuffs, handcuffed),\nBP 14: the expression acesso aos elementos/autos/documentos (access to elements, records or documents),\nBP 17: the word precat\u00f3rio and the expression juros de mora (court orders, late payment interest),\nBP 26: the expressions exame criminol\u00f3gico and progress\u00e3o de regime (criminological examination, regime progression),\nBP 37: the expressions isonomia, vencimentos and servidores p\u00fablicos (isonomy, salaries, public servants)."}, {"title": "3.3 Validation", "content": "Our first analysis focuses on Dataset #1, a collection of documents annotated by the binding precedent they cite. We are, therefore, dealing with a binary classification task. As we will see in this section, and as it is expected from the NLP literature, all the models perform well on this task. However, we remind the reader that the main objective of this article lies in the analysis of Dataset #2 (in Section 4), which is unannotated, and where the \"scores\u201d will be quite different."}, {"title": "Preparation of the data", "content": "Some preprocessing was applied to the documents before training. For both the TF-IDF and LSTM models, this encompassed the removal of Portuguese-specific accents, and of usual stop words (e.g., a, o, um, mas). Because BERT does not expect this sort of preprocessing, raw texts were given.\nAdditionally, we want to avoid the models from learning textual cues that mark direct citations to BPs, such as the very term S\u00famula Vinculante or similar. Therefore, regular expressions were used to substitute these terms by empty strings. Additionally, as our interest consists in detecting possible uses of BPs before their publications, dates were also removed. Once more, BERT models receive a slightly different masking process than the others: each BP citation is identified using regular expressions and is replaced by [MASK] tokens, keeping the same length as the original text.\nOur models are trained on Dataset #1 (described in Section 3.1). We randomly split the data set into training and test data, while preserving the distribution of the BPs among the documents. For such, 10% was dedicated for testing, and from the training data, a further 10% was used for validation. Although the pre-processing of input texts was not the same for each class of models, we ensured that all shared the same documents for training, testing, and validation. Lastly, given a BP, we attributed to the documents the labels \"1\" or \"0\", depending on whether the document cites or not the BP, and trained the models for this binary classification task."}, {"title": "Scores", "content": "To assess the performance of our binary classifiers, we use a handful of score metrics. By denoting P (number of class 1 examples), N (class 0), TP (class 1 predicted class 1), FP (class 0 predicted class 1), TN (class 0 predicted class 0), and FN (class 1 predicted class 0), we consider the following metrics:\nAccuracy = (TP + TN)/(P + N),\nPrecision = TP/(TP + FP),\nRecall = TP/P\n$F_1 = 2 \\text{Precision} \\cdot \\text{Recall}/(\\text{Precision} + \\text{Recall})$.\nIntuitively, accuracy measures the ratio of correct predictions, while precision measures the accuracy of pointing to class 1, and recall measures how much of class 1 is being correctly predicted. The $F_1$ score is the harmonic mean of recall and precision.\nMost models return a continuous output, e.g., a probability. In this case, defining when the model is predicting classes 0 or 1 depends on the classification threshold. The Area Under the Precision Recall Curve (AUPRC) swaps a classification threshold"}, {"title": "Discussion", "content": "One sees from Table 2 that the TF-IDF models, equipped with the classifiers SVM and logistic regression, consistently achieve the best performance across all BPs, measured by the F\u2081 score. On the other hand, the deep learning models LSTM and BERT show slightly lower scores. This phenomenon has already been reported in [47], using, as we do Brazilian legal documents (more precisely, data from Supremo em N\u00fameros). In addition, it has been observed that the TF-IDF-based models outperform certain more modern embeddings (such as Doc2vec, Universal Sentence Encoder, and Longformer). In this context, TF-IDF's superior performance is attributed to its ability to leverage"}, {"title": "3.4 Explainability", "content": "To understand further what the models have learned, we move on to study their most important features. For the TF-IDF models, common measures of importance can be employed and directly computed from the models. For BERT, however, there is no direct way of identifying the most important features, thus we will use the explainability algorithm LIME [48]. Note that, although not presented here, a similar analysis could be made with LSTM."}, {"title": "Explainability with TF-IDF", "content": "For the classifiers SVM, logistic regression and random forest, based on the TF-IDF vectorization, common measures of importance of features are respectively the weights of the linear kernel, the coefficients in the decision function, and the standard deviation of impurity decrease in the trees. We compute these quantities via the native functions of scipy. We inspect, for each model, the top features, and gather those common to all models."}, {"title": "Explainability with LIME", "content": "In the context of explainability, instead of directly assessing the weights of an interpretable model (e.g., a logistic regression), we can treat the model as a black box and verify its behavior when we disturb its input. For instance, one could be interested in"}, {"title": "4 Results and legal analysis", "content": "This section is concerned with the application of our models, trained on the first dataset (documents citing a BP), to the analysis of the second dataset (whole collection of documents emitted by the High Court). From a legal point of view, we are interested in uncovering the juridical mechanisms behind the observed increase in cases citing a binding precedent, visualized in Fig. 1. Besides, from a Machine Learning perspective, we aim to verify the ability of the models to generalize their results, when applied to a larger dataset. We shall start with a general description of our methodology for empirical evaluation of binding precedents (Section 4.1), then give a detailed analysis of each BP (Sections 4.2 to 4.6), and finally draw juridical conclusions (Section 4.7)."}, {"title": "4.1 Description of our methodology", "content": ""}, {"title": "Time series of similar cases", "content": "To evaluate the effect of a binding precedent on jurisprudence, we argue that one should not perform a mere reading of the cases citing that precedent, but also compare them to similar cases published before the precedent. This information can be visualized through the curve which associates, to each timestamp, the number of similar documents. In our context, we expect such a curve to behave as follows: the existence of a peak of documents just before the publication of the BP, followed by a steady decrease, until reaching a stable state, hopefully consisting of few documents. This behavior would indicate that the BP served its main purpose, which is to \"settle\" an increasingly contentious legal issue, clarifying for interested parties and the lower courts what the final position of STF is, thus reducing the likelihood of future conflicts involving the same issue.\nIf this behavior is not observed, as it is the case for the five precedents studied in this article, then one can add metadata information to the curve, to reveal the reasons for this deviant phenomenon. Namely, we will consider the type of process (e.g., habeas corpus or appeal), the state of provenance, the decision of the court (accepted, rejected or deferred), and the presence of certain words (specific to the legal subject considered).\nWe point out that to obtain a thorough analysis of the impact of a BP, it would have been appropriate to detect similar documents issued not only by the STF but by lower courts as well. This analysis, however, is beyond the scope of this article, given the current unavailability of such a dataset."}, {"title": "Generalization of the models", "content": "As discussed in Section 3.1, this article deals with Datasets #1 and #2, the former consisting of documents citing a binding precedent (29,743 documents), and the latter of all the documents in the categories \"administrative law\", \"criminal law\" and \"criminal procedure law\" (615,262 documents). In Section 3.3, we trained the models on Dataset #1 and presented the results (see Table 2). We will, in this section, apply these models on Dataset #2. In doing so, we face a significant problem: the models have been tuned to"}, {"title": "4.2 BP 11", "content": ""}, {"title": "Juridical context", "content": "During the trial of HC (Habeas Corpus) 91.9525 (08/07/2008), the defense requested the removal of handcuffs from the defendant, due to concerns about the negative perception that the sight of handcuffs could convey to the jury. This case triggered a highly publicized debate, which then shifted to discussing the use of handcuffs for public exposure, and more generally the sensationalization of criminal prosecution in the media. Based on the presumption of innocence, individual freedom, and the dignity of the human person, the High Court voted a few days later on the following:\nBinding Precedent 11. \"The use of handcuffs is only permitted in cases of resistance and a well-founded fear of escape or danger to the physical integrity of the prisoner or others, justified in writing, under penalty of disciplinary, civil, and criminal liability of the agent or authority and nullity of the arrest or the procedural act to which it refers, without prejudice to the civil liability of the State.\" (STF, 08/2008)\nS\u00famula Vinculante 11. \"S\u00f3 \u00e9 l\u00edcito o uso de algemas em casos de resist\u00eancia e de fundado receio de fuga ou de perigo \u00e0 integridade f\u00edsica pr\u00f3pria ou alheia, por parte do preso ou de terceiros, justificada a excepcionalidade por escrito, sob pena de responsabilidade disciplinar, civil e penal do agente ou da autoridade e de nulidade da pris\u00e3o ou do ato processual a que se refere, sem preju\u00edzo da responsabilidade civil do Estado.\u201d (STF, 08/2008)\nThis precedent asserts that the use of handcuffs is allowed only when explicitly justified. In particular, their use is prohibited in the context of trials or media exposure, which was deemed as humiliating by part of the public discourse. It has been considered a notable accomplishment in advancing the principles of the Democratic Rule of Law over those of a Police State [16, 53].\nIt is worth mentioning certain controversies surrounding the precedent. First, the nature of the nullity (absolute or relative) is not explicitly specified. Currently, the Supreme Court understands that the nature of the nullity is relative, as it depends on a demonstration of concrete harm to the defendant. However, [53] note that lower courts across the country have been deciding similar cases under the assumption of absolute nullity. Besides, as pointed out by [54] and [17], a question remains regarding the exact modalities of justification of the fear of escape or danger evoked in the text.\nAs already visualized in Fig. 1, the number of documents citing BP 11 has been rising steadily since 2008, with a notable jump in 2016. We aim, through our analysis, to uncover some of the reasons for this increase."}, {"title": "Mathematical results", "content": "We show in Fig. 3 the predictions of the five models on our whole collection of documents (Dataset #2), as well as a regex search for documents containing the words algemas or algemado (in English, \"handcuffs\" and \"handcuffed\"). As explained in Section 4.1, the thresholds of the models, trained on Dataset #1, are not adapted for Dataset #2: way too many documents are predicted. Therefore, we choose new thresholds with the following rule: the highest value such that at least 90% of the documents citing BP 11"}, {"title": "Juridical discussion", "content": "As a consequence of the analysis above, we take TF-IDF models to be the most reliable for the retrieval of documents similar to those citing BP 11. We shall see in the next sections that this conclusion holds for all the precedents considered. The case of BP 11, however, is unfortunate: no document is predicted by the models before the creation of the precedent, hence no interesting behavior can be observed in Fig. 3. This restricts the application of our methodology described in Section 4.1. Nevertheless, two juridical insights can be drawn. First, the regex search shows that the term \u201chandcuffs\" has come to be used in STF's cases recently, thanks to the edition of BP 11. It should be noted that a regex search for uso de for\u00e7a (use of force), a term commonly used as a synonym of handcuffs, only finds 153 documents, 74.8% of which also cite the precedent. On top of that, the fact that the TF-IDF models do not detect any similar documents before 2008 suggests that BP 11 not only changed the way cases are formulated, but really introduced this matter in the high court. This observation would fall under the New theme hypothesis, evoked in Section 4.1 as a way of explaining the inefficiency of the precedent in reducing the number of cases.\nWe see in this precedent that a BP does not solely serve to pacify jurisprudence, but it might also be used as a tool of judicial activism by STF, to resolve a situation deemed contrary to the Constitution. Among these situations is the widespread use of handcuffs, addressed in the precedent, which, despite being widely accepted by the public, the Justices deemed to be incorrect and to violate the basic rights of the defendants. In\""}, {"title": "Mathematical results", "content": "A quick analysis of the predicted documents in the bottom part of Fig. 6 indicates that, among the five models considered, only TF-IDF+random forest achieved satisfactory results, by faithfully following the groundtruth curve. This is also the case for the regex search of the expression acesso aos elementos/autos/documentos (access to elements, records, or documents).\nTo investigate why the other models estimate many documents, we will not consider the correlations, as we did for BP 11 (in Fig. 4), but simply the frequencies of the most relevant words. Almost all groundtruth documents cite acesso aos autos (access to records). Similarly, 96.3% of TF-IDF+randomforest's predictions mention autos, of which 81.8% also mention acesso. Besides, among TF-IDF+SVM and TF-IDF+logistic's predictions, respectively, 93.5% and 93.6% mention autos, while only 44.6% and 41.8% of them conjointly contain acesso. This suggests an underfitting of both models.\nOn the other hand, LSTM and BERT predict a quantity of documents not even mentioning the words acesso and autos. As it turns out, the decisions detected before the creation of BP 14 seemed of little relevance regarding its topic. Most of them"}, {"title": "Juridical discussion", "content": "First"}, {"title": "Empirical analysis of Biding Precedent efficiency in the Brazilian Supreme Court via Similar Case Retrieval", "authors": ["Rapha\u00ebl Tinarrage", "Henrique Ennes", "Lucas E. Resck", "Lucas T. Gomes", "Jean R. Ponciano", "Jorge Poco"], "abstract": "Binding precedents (S\u00famulas Vinculantes) constitute a juridical instrument unique to the Brazilian legal system and whose objectives include the protection of the Federal Supreme Court against repetitive demands. Studies of the effectiveness of these instruments in decreasing the Court's exposure to similar cases, however, indicate that they tend to fail in such a direction, with some of the binding precedents seemingly creating new demands. We empirically assess the legal impact of five binding precedents, 11, 14, 17, 26 and 37, at the highest court level through their effects on the legal subjects they address. This analysis is only possible through the comparison of the Court's ruling about the precedents' themes before they are created, which means that these decisions should be detected through techniques of Similar Case Retrieval. The contributions of this article are therefore twofold: on the mathematical side, we compare the uses of different methods of Natural Language Processing - TF-IDF, LSTM, BERT, and regex for Similar Case Retrieval, whereas on the legal side, we contrast the inefficiency of these binding precedents with a set of hypotheses that may justify their repeated usage. We observe that the deep learning models performed significantly worse in the specific Similar Case Retrieval task and that the reasons for binding precedents to fail in responding to repetitive demand are heterogeneous and case-dependent, making it impossible to single out a specific cause.", "sections": [{"title": "1 Introduction", "content": "In recent years, the progress in Natural Language Processing (NLP) sparked significant interest in its application within the legal domain. Brazil, renowned for having the world's highest volume of legal cases, is no exception and has already witnessed several implementations of Machine Learning methods in its judiciary system [51]. Notably, these algorithms have the potential to assist legal professionals in finding cases similar to a given one, or a given legal topic a task commonly referred to as Similar Case Retrieval. While these methods are supported by numerous mathematical studies and proofs of concept, one could argue that some of them lack empirical validation. This article seeks to bridge this gap by comparing various document retrieval methods and evaluating their results from a legal perspective.\nIn particular, this article explores the legal instrument of binding precedent (S\u00famula Vinculante, abbreviated BP), that emerged in Brazil in the 2004 judicial reform. Its purpose was to address the issue of an overwhelming number of cases with repetitive demands inundating the Brazilian Federal Supreme Court (Supremo Tribunal Federal, STF), which ideally should handle only a limited number of cases. This situation was partly due to the Brazilian legal system's civil law approach, where Supreme Court decisions don't serve as authority for lower courts, leading to continued case congestion. To mitigate this, Constitutional Amendment No. 45 introduced instruments inspired by the common law system, including binding precedents. They aimed at standardizing jurisprudence, providing normative force over lower instances and the broader public administration.\nAfter its publication, one expects the binding precedent to be cited frequently, until the legal understanding of the courts on the subject calms down, resulting in a significant decrease in the number of citations. However, among the most cited binding precedents, this trend is by no means observed. On the contrary, they show steady growth, as illustrated in Fig. 1 for the five precedents of interest in this paper: 11, 14, 17, 26 and 37. They were chosen for their high number of citations, as well as the variety of legal topics they cover, from administrative to criminal law.\nThe aim of this article is to shed light on the reasons why these binding precedents have not led, as expected, to a reduction in repeated demands. To this end, we employ Similar Case Retrieval methods to trace the history of these precedents and quantify certain trends. This information is subsequently used to provide a legal analysis. In summary, this article includes two main mathematical and two juridical contributions:\n1. The application and comparative analysis of classical algorithms for Similar Case Retrieval on a database of Brazilian legal documents (including TF-IDF-based models, LSTM, BERT and regex);\n2. The outline of a methodology for assessing the impact of a law on jurisprudence, through time series of similar cases and features' correlations;\n3. Application of the mentioned methodology to five binding precedents emitted by the Brazilian Federal Supreme Court, enabling an empirical study of the juridical mechanisms behind their inefficiency;\n4. The identification of five main hypotheses explaining the large number of cases reaching the Supreme Court."}, {"title": "Dataset and reproducibility", "content": "For our analysis, we will use a set of decisions of the Supreme Court produced between 1989 and 2018, collected and annotated by the project Supremo em N\u00fameros [23]. In fact, these documents were scraped from the official STF website, where they can be found one by one. The collection of documents gathered by the project is, however, not publicly available, and has been kindly provided to us. More precisely, we will consider two subsets of this collection, detailed further in Section 3.1: Dataset #1, consisting of all decisions citing a binding precedent (29,743 documents), and Dataset #2, gathering all decisions belonging to the topics \"administrative law\", \"criminal law\" or \"criminal procedure law\" (615,262 documents). We point that the data gathered by Supremo em N\u00fameros have already been used in several works [14, 13, 47]. In addition, other similar databases have been reported, such as the one maintained by the project Victor [20]."}, {"title": "Overview", "content": "The remainder of this article is organized as follows. In Section 2, we set out the legal context behind the Brazilian's Supreme Court binding precedents, with a particular focus on the five precedents specifically studied in this article, and we give an overview of the mathematical literature surrounding Similar Case Retrieval. Datasets #1 and #2 are introduced in Section 3, as well as the models used throughout this article (TF-IDF, LSTM and BERT), that we train on the former dataset. The application of these models"}, {"title": "2 Related works", "content": ""}, {"title": "2.1 Legal context", "content": ""}, {"title": "The BPs as a juridical tool", "content": "The type of precedent known in Brazil as S\u00famula Vinculante (Binding Precedent) emerged in the judiciary reform of 2004, through Constitutional Amendment No. 45 (EC 45/2004, Article 103-A2), to standardize decisions, based on the importation of the jurisdiction logic usual in common law [39, p. 827]. The introduction of these instruments can be understood as an attempt to unify decision-making, aiming to achieve equality and legal certainty, i.e., to avoid that identical cases are decided in different ways, violating the constitutional guarantee of equality before the law [8, p. 20]. In fact, the new Federal Constitution Article requires, for the creation of a BP, the fulfillment of three conditions: (i) dealing with a matter under current controversy among judicial bodies, or between them and the public administration; (ii) representing a risk of serious legal uncertainty; and (iii) being the subject of significant multiplication of processes on an identical legal issue. Thus, as a legal instrument, the BP seeks not only to guarantee equality in judicial decisions, but also to increase the efficiency of the Judiciary, by avoiding delays in jurisdictional intervention.\nTo date, 58 BPs have been published. It should be noted that, in the literature, there is a considerable criticism of the BPs, whether in terms of their institution, their formulation, or their consequences. These issues will be explored in detail throughout the article. We refer the reader to the work of [3] and [43] for a general analysis of BPs."}, {"title": "The precedents 11, 14, 17, 26 and 37", "content": "Once a BP is published, is it expected that the subject ceases to be a matter of controversial interpretation. Consequently, the issue should also cease to generate processes with identical demands and, primarily, prevent these from continuing to be brought before the Higher Courts. It is therefore important to establish, empirically, the degree of efficiency of this type of instrument, here understood as the degree of reduction in similar cases reaching the final stage of appeal. In this article, we analyze the impact of the creation of five binding precedents, chosen as those generating many cases, but also for the diversity of legal topics they cover. Three of them belong to criminal law (11, 14 and 26), and the others to administrative law (17 and 37). We give in the list below a brief summary of their content and refer the reader to the corresponding section for a deeper explanation, as well as the results of our analysis.\nBP 11, the 2008-11-12 (see Section 4.2): Determines the use of handcuffs acceptable only when some risk is anticipated, predicting disciplinary punishments to the responsible public agents and/or nullity of the penal process in the case of unjustified use.\nBP 14, the 2009-02-02 (see Section 4.3): Grants to the investigated individual and their attorneys full access to all documented evidence in ongoing criminal investigations.\nBP 17, the 2009-11-10 (see Section 4.4): In Brazil, public administration at all levels, when sentenced to pay debts resulting from juridical decisions, incorporate these"}, {"title": "2.2 NLP for legal documents", "content": ""}, {"title": "Text embeddings", "content": "The literature surrounding NLP for legal document analysis is rich, reflecting the growing interest in leveraging computational techniques to navigate the complexities of legal texts. Well-studied topics in this area include the analysis of the precedent network [36, 14, 41], Named Entity Recognition [13], summarization of legal texts [28, 26] and prediction of judicial outcome [2, 4, 5]. In addition, of particular interest to us is the detection of similar documents, reviewed further in the next paragraph.\nTo tackle these problems, a standard technique consists of embedding (i.e., vectorizing) the documents, the most classical methods being bag-of-words, TF-IDF, and n-grams. Among recent techniques of words and documents embeddings, we can cite GLOVE [42], contextualized word representation [44], word2vec [12], doc2vec [33], Latent Dirichlet Allocation (LDA) [7], entities and relations-based embedding [72], Universal Sentence Encoder (USE) [10] and TextCNN [11]. Another alternative are the pre-trained language models, such as BERT [21] and its variants [75, 45, 29, 67].\nIn particular, word2vec is used by [24] to study appellate court modifications in Brazilian legal documents, that is, modifcations by the Supreme Court of the lower Court judge's decision. On the other hand, TF-IDF has been already used for the analysis of binding precedents, and has been reported to outperform other embeddings (such as doc2vec and USE) [47]. We will consider, in this article, TF-IDF embeddings coupled with several classifiers, as well as a recurrent neural network (LSTM) and a Large Language Model (BERT)."}, {"title": "Automatic Similar Case Matching and Retrieval", "content": "At their most elementary level, although having a specific binding property, BPs are precedent and, as such, the task of searching for documents similar to the object of a BP, in content and language, may be modeled through textual proximity and/or classification techniques. The problem of identifying juridical decisions similar in a corpus of documents, known as automatic Similar Case Matching, has received significant attention in the literature. For example, a multitude of NLP embedding models have been compared in the CAIL2019-SCM dataset, a corpus of thousands of decisions published by the Supreme People's Court of China, in which the task is to determine, for a triple"}, {"title": "3 Datasets and methods", "content": ""}, {"title": "3.1 Datasets", "content": ""}, {"title": "Documents", "content": "Cases decided at the STF are consolidated in a document, containing the decision's text, as well as the publication date, the Justice that conducted the process (Justice rapporteur), and the document type (e.g., Complaint or Extraordinary Appeal). The research project Supremo em N\u00fameros gathered, in text format, more than 2,500,000 of these documents [23]. In this collection, we shall consider the subset of documents that cite a binding precedent, among the 58 precedents edited by STF until 2018. This amounts to 29,743 documents, which we refer to as Dataset #1 (see Table 1). In addition, we shall consider the so-called Dataset #2, consisting of those documents, in the whole collection, labeled as \u201cadministrative law\", \"criminal law\" or \"criminal procedure law\u201d, gathering 615,262 documents. These are the categories of the five particular BPs we will study (already presented in Section 2.1). We draw the reader's attention to the fact that Dataset #1 is not a subset of Dataset #2, though the latter contains approximately 36% of the former.\""}, {"title": "Labels", "content": "In the context of Similar Case Retrieval, Dataset #1 is particularly convenient. Indeed, since the 58 binding precedents cover different themes, one decides whether a document deals with a specific theme by reading the corresponding BP it cites. In opposition, no information is available in Dataset #2 regarding the juridical content of the cases. For this reason, we will employ the former to train our models. In the rest of this section, only Dataset #1 will be considered, while the application of our models to Dataset #2 is the content of Section 4."}, {"title": "3.2 Models for Similar Case Retrieval", "content": "In this section, we delve into the methodologies employed for analyzing our dataset, focusing on four distinct approaches: TF-IDF, LSTM, BERT, and regex. Together, they faithfully represent some of the most common NLP methods."}, {"title": "TF-IDF", "content": "Term Frequency-Inverse Document Frequency (TF-IDF) is a vector embedding technique divided into two parts. In the \"term-frequency\" part, each document d is embedded in the space $R^n$, where n is the number of different words on the corpora, and where the i-th component of the vector is the frequency that the i-th word of the corpora appears in the document. We denote it by $TF(i, d)$. The \"inverse document frequency\" part is another $R^n$ vector, with the i-th coordinate given by the logarithm\n$IDF(i, d) = log \\frac{\\text{#documents in the corpora}}{\\text{#documents in the corpora that i appears}}$\nFinally, the TF-IDF vector has the i-th coordinate given by the product\n$TF\\text{-}IDF(i, d) = TF(i, d) \\times IDF(i, d)$.\nBy itself, TF-IDF is only an embedding, so it must be coupled with some classification algorithm for us to perform Similar Case Retrieval. In this article, we will use three kinds of classification models for the embedded documents: linear Support Vector Machine (SVM), logistic regression, and random forests. We will not explicitly describe these classifiers here, but refer the interested reader to [27]."}, {"title": "LSTM", "content": "A recurrent network architecture was also implemented for this task, given the well-known capacity of these models to work with textual data, especially for longer texts [27, 50]. The implemented neural networks were composed of three layers: one embedding layer, followed by two layers consisting of 128 LSTM (Long Short-Term Memory) neurons each, all using relu activation functions. For the embedding layer, tensorflow was used, where the vocabulary (i.e., set of unique words in the corpora) was restricted to the 10,000 most common terms, using 1,000 out-of-bucket embedding for the rest of them. Moreover, because the architecture expects a constant input size of documents, only the first 512 tokens of each document were considered, using additional masking for shorter documents. Although naive, this approach is common when dealing with long text analysis in neural networks. Finally, the output consisted of a single neuron followed by a sigmoid non-linearity. We stress that no pre-training of the network's weights was used."}, {"title": "BERT", "content": "Bidirectional Encoder Representations from Transformer (BERT) [21] is a common Transformer-based model [66] used for several NLP tasks that can benefit from a deep embedding of a word or a sequence, e.g., a text classification. In this model, each token (word or part of a word) is vectorized and their vectors are sequentially updated to incorporate their neighbors' information. BERT models are released with their weights already optimized in a large amount of text in a self-supervised manner. The output of the model is a vector for each token and one for the entire sentence (corresponding to the first CLS token). These vectors can be used as features for any classification model, e.g., a logistic regression. One can also extend this neural network by adding a downstream neural network layer, like a linear classification layer, and then both the new layer and the rest of the model are trained together in the downstream task. Just like the LSTM implementations, BERT models have a text input size limited to 512 tokens. Because of that, texts were truncated to their first 510 tokens (to include other BERT special tokens).\nIn our work, we use a Portuguese BERT (BERTimbau BASE4) model fine-tuned in approximately 16 GB of Brazilian court decision raw data for eight epochs. This data gather decisions from the five most important Brazilian courts: Supremo Tribunal Federal, Superior Tribunal de Justi\u00e7a, Tribunal de Justi\u00e7a de S\u00e3o Paulo, Tribunal de Justi\u00e7a do Rio de Janeiro, and Tribunal Superior Eleitoral."}, {"title": "regex", "content": "Regular expression (regex) stands out as a commonly used tool among professionals for retrieving similar cases. In particular, it constitutes the search engine of the official STF website. We designed a regex search for each of the five BPs by selecting, with the help of legal expertise, the most important words in their statements (please refer to Sections 4.2 to 4.6 for the statement and juridical context of each BPs). We draw the reader's attention to the fact that, as our models are intended to detect, in a subsequent section, the documents before creation of the BPs, we do not use the words S\u00famula Vinculante or any specific mention of a law. More precisely, we consider:\nBP 11: the words algemas or algemado (handcuffs, handcuffed),\nBP 14: the expression acesso aos elementos/autos/documentos (access to elements, records or documents),\nBP 17: the word precat\u00f3rio and the expression juros de mora (court orders, late payment interest),\nBP 26: the expressions exame criminol\u00f3gico and progress\u00e3o de regime (criminological examination, regime progression),\nBP 37: the expressions isonomia, vencimentos and servidores p\u00fablicos (isonomy, salaries, public servants)."}, {"title": "3.3 Validation", "content": "Our first analysis focuses on Dataset #1, a collection of documents annotated by the binding precedent they cite. We are, therefore, dealing with a binary classification task. As we will see in this section, and as it is expected from the NLP literature, all the models perform well on this task. However, we remind the reader that the main objective of this article lies in the analysis of Dataset #2 (in Section 4), which is unannotated, and where the \"scores\u201d will be quite different."}, {"title": "Preparation of the data", "content": "Some preprocessing was applied to the documents before training. For both the TF-IDF and LSTM models, this encompassed the removal of Portuguese-specific accents, and of usual stop words (e.g., a, o, um, mas). Because BERT does not expect this sort of preprocessing, raw texts were given.\nAdditionally, we want to avoid the models from learning textual cues that mark direct citations to BPs, such as the very term S\u00famula Vinculante or similar. Therefore, regular expressions were used to substitute these terms by empty strings. Additionally, as our interest consists in detecting possible uses of BPs before their publications, dates were also removed. Once more, BERT models receive a slightly different masking process than the others: each BP citation is identified using regular expressions and is replaced by [MASK] tokens, keeping the same length as the original text.\nOur models are trained on Dataset #1 (described in Section 3.1). We randomly split the data set into training and test data, while preserving the distribution of the BPs among the documents. For such, 10% was dedicated for testing, and from the training data, a further 10% was used for validation. Although the pre-processing of input texts was not the same for each class of models, we ensured that all shared the same documents for training, testing, and validation. Lastly, given a BP, we attributed to the documents the labels \"1\" or \"0\", depending on whether the document cites or not the BP, and trained the models for this binary classification task."}, {"title": "Scores", "content": "To assess the performance of our binary classifiers, we use a handful of score metrics. By denoting P (number of class 1 examples), N (class 0), TP (class 1 predicted class 1), FP (class 0 predicted class 1), TN (class 0 predicted class 0), and FN (class 1 predicted class 0), we consider the following metrics:\nAccuracy = (TP + TN)/(P + N),\nPrecision = TP/(TP + FP),\nRecall = TP/P\n$F_1 = 2 \\text{Precision} \\cdot \\text{Recall}/(\\text{Precision} + \\text{Recall})$.\nIntuitively, accuracy measures the ratio of correct predictions, while precision measures the accuracy of pointing to class 1, and recall measures how much of class 1 is being correctly predicted. The $F_1$ score is the harmonic mean of recall and precision.\nMost models return a continuous output, e.g., a probability. In this case, defining when the model is predicting classes 0 or 1 depends on the classification threshold. The Area Under the Precision Recall Curve (AUPRC) swaps a classification threshold"}, {"title": "Discussion", "content": "One sees from Table 2 that the TF-IDF models, equipped with the classifiers SVM and logistic regression, consistently achieve the best performance across all BPs, measured by the F\u2081 score. On the other hand, the deep learning models LSTM and BERT show slightly lower scores. This phenomenon has already been reported in [47], using, as we do Brazilian legal documents (more precisely, data from Supremo em N\u00fameros). In addition, it has been observed that the TF-IDF-based models outperform certain more modern embeddings (such as Doc2vec, Universal Sentence Encoder, and Longformer). In this context, TF-IDF's superior performance is attributed to its ability to leverage"}, {"title": "3.4 Explainability", "content": "To understand further what the models have learned, we move on to study their most important features. For the TF-IDF models, common measures of importance can be employed and directly computed from the models. For BERT, however, there is no direct way of identifying the most important features, thus we will use the explainability algorithm LIME [48]. Note that, although not presented here, a similar analysis could be made with LSTM."}, {"title": "Explainability with TF-IDF", "content": "For the classifiers SVM, logistic regression and random forest, based on the TF-IDF vectorization, common measures of importance of features are respectively the weights of the linear kernel, the coefficients in the decision function, and the standard deviation of impurity decrease in the trees. We compute these quantities via the native functions of scipy. We inspect, for each model, the top features, and gather those common to all models."}, {"title": "Explainability with LIME", "content": "In the context of explainability, instead of directly assessing the weights of an interpretable model (e.g., a logistic regression), we can treat the model as a black box and verify its behavior when we disturb its input. For instance, one could be interested in"}, {"title": "4 Results and legal analysis", "content": "This section is concerned with the application of our models, trained on the first dataset (documents citing a BP), to the analysis of the second dataset (whole collection of documents emitted by the High Court). From a legal point of view, we are interested in uncovering the juridical mechanisms behind the observed increase in cases citing a binding precedent, visualized in Fig. 1. Besides, from a Machine Learning perspective, we aim to verify the ability of the models to generalize their results, when applied to a larger dataset. We shall start with a general description of our methodology for empirical evaluation of binding precedents (Section 4.1), then give a detailed analysis of each BP (Sections 4.2 to 4.6), and finally draw juridical conclusions (Section 4.7)."}, {"title": "4.1 Description of our methodology", "content": ""}, {"title": "Time series of similar cases", "content": "To evaluate the effect of a binding precedent on jurisprudence, we argue that one should not perform a mere reading of the cases citing that precedent, but also compare them to similar cases published before the precedent. This information can be visualized through the curve which associates, to each timestamp, the number of similar documents. In our context, we expect such a curve to behave as follows: the existence of a peak of documents just before the publication of the BP, followed by a steady decrease, until reaching a stable state, hopefully consisting of few documents. This behavior would indicate that the BP served its main purpose, which is to \"settle\" an increasingly contentious legal issue, clarifying for interested parties and the lower courts what the final position of STF is, thus reducing the likelihood of future conflicts involving the same issue.\nIf this behavior is not observed, as it is the case for the five precedents studied in this article, then one can add metadata information to the curve, to reveal the reasons for this deviant phenomenon. Namely, we will consider the type of process (e.g., habeas corpus or appeal), the state of provenance, the decision of the court (accepted, rejected or deferred), and the presence of certain words (specific to the legal subject considered).\nWe point out that to obtain a thorough analysis of the impact of a BP, it would have been appropriate to detect similar documents issued not only by the STF but by lower courts as well. This analysis, however, is beyond the scope of this article, given the current unavailability of such a dataset."}, {"title": "Generalization of the models", "content": "As discussed in Section 3.1, this article deals with Datasets #1 and #2, the former consisting of documents citing a binding precedent (29,743 documents), and the latter of all the documents in the categories \"administrative law\", \"criminal law\" and \"criminal procedure law\" (615,262 documents). In Section 3.3, we trained the models on Dataset #1 and presented the results (see Table 2). We will, in this section, apply these models on Dataset #2. In doing so, we face a significant problem: the models have been tuned to"}, {"title": "4.2 BP 11", "content": ""}, {"title": "Juridical context", "content": "During the trial of HC (Habeas Corpus) 91.9525 (08/07/2008), the defense requested the removal of handcuffs from the defendant, due to concerns about the negative perception that the sight of handcuffs could convey to the jury. This case triggered a highly publicized debate, which then shifted to discussing the use of handcuffs for public exposure, and more generally the sensationalization of criminal prosecution in the media. Based on the presumption of innocence, individual freedom, and the dignity of the human person, the High Court voted a few days later on the following:\nBinding Precedent 11. \"The use of handcuffs is only permitted in cases of resistance and a well-founded fear of escape or danger to the physical integrity of the prisoner or others, justified in writing, under penalty of disciplinary, civil, and criminal liability of the agent or authority and nullity of the arrest or the procedural act to which it refers, without prejudice to the civil liability of the State.\" (STF, 08/2008)\nS\u00famula Vinculante 11. \"S\u00f3 \u00e9 l\u00edcito o uso de algemas em casos de resist\u00eancia e de fundado receio de fuga ou de perigo \u00e0 integridade f\u00edsica pr\u00f3pria ou alheia, por parte do preso ou de terceiros, justificada a excepcionalidade por escrito, sob pena de responsabilidade disciplinar, civil e penal do agente ou da autoridade e de nulidade da pris\u00e3o ou do ato processual a que se refere, sem preju\u00edzo da responsabilidade civil do Estado.\u201d (STF, 08/2008)\nThis precedent asserts that the use of handcuffs is allowed only when explicitly justified. In particular, their use is prohibited in the context of trials or media exposure, which was deemed as humiliating by part of the public discourse. It has been considered a notable accomplishment in advancing the principles of the Democratic Rule of Law over those of a Police State [16, 53].\nIt is worth mentioning certain controversies surrounding the precedent. First, the nature of the nullity (absolute or relative) is not explicitly specified. Currently, the Supreme Court understands that the nature of the nullity is relative, as it depends on a demonstration of concrete harm to the defendant. However, [53] note that lower courts across the country have been deciding similar cases under the assumption of absolute nullity. Besides, as pointed out by [54] and [17], a question remains regarding the exact modalities of justification of the fear of escape or danger evoked in the text.\nAs already visualized in Fig. 1, the number of documents citing BP 11 has been rising steadily since 2008, with a notable jump in 2016. We aim, through our analysis, to uncover some of the reasons for this increase."}, {"title": "Mathematical results", "content": "We show in Fig. 3 the predictions of the five models on our whole collection of documents (Dataset #2), as well as a regex search for documents containing the words algemas or algemado (in English, \"handcuffs\" and \"handcuffed\"). As explained in Section 4.1, the thresholds of the models, trained on Dataset #1, are not adapted for Dataset #2: way too many documents are predicted. Therefore, we choose new thresholds with the following rule: the highest value such that at least 90% of the documents citing BP 11"}]}]}