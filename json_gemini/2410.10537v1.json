{"title": "Reproducible Machine Learning-based Voice Pathology Detection: Introducing the Pitch Difference Feature", "authors": ["Jan Vrba", "Jakub Steinbach", "Tom\u00e1\u0161 Jirsa", "Laura Verde", "Roberta De Fazio", "Noriyasu Homma", "Yuwen Zeng", "Key Ichiji", "Luk\u00e1\u0161 H\u00e1jek", "Zuzana Sedl\u00e1kov\u00e1", "Jan Mare\u0161"], "abstract": "Voice pathology is a recurrent issue affecting a substantial portion of the population. Machine learning (ML) models and their training on various databases, can enhance and support the diagnosis. In this study, we propose a robust set of features derived from a thorough research of contemporary practices in voice pathology detection. The feature set is based on the combination of acoustic handcrafted features. Additionally, we introduce pitch difference as a novel feature. We combine this feature set, containing data from the publicly available Saarbr\u00fccken Voice Database (SVD), with preprocessing using the K-Means Synthetic Minority Over-Sampling Technique algorithm to address class imbalance. Moreover, we applied multiple ML models as binary classifiers. We utilized support vector machine, k-nearest neighbors, naive Bayes, decision tree, random forest and AdaBoost classifiers. To determine the best classification approach, we performed grid search on feasible hyperparameters of respective classifiers and subsections of features. Our approach has achieved the state-of-the-art performance, measured by unweighted average recall in voice pathology detection on SVD database. We intentionally omit accuracy as it is highly biased metric in case of unbalanced data compared to aforementioned metrics. The results are further enhanced by eliminating the potential overestimation of the results with repeated stratified cross-validation. This advancement demonstrates significant potential for the clinical deployment of ML methods, offering a valuable tool for an objective examination of voice pathologies. To support our claims, we provide a publicly available GitHub repository with DOI 10.5281/zenodo.13771573. Finally, we provide REFORMS checklist.", "sections": [{"title": "1 Introduction", "content": "In the era of Big Data, artificial intelligence (AI) and digital Twin, the support of technologies for monitoring of people's health, detection of specific diseases, optimizing the effectiveness of therapy, and defining personalized and precision medicine is becoming increasingly widespread. Pathologies such as voice disorders can benefit from effective tools such as these technologies for their early detection. Voice disorders are characterized by the alteration of voice quality due to functional or morphological changes. Although these disorders are widespread, they are often underestimated, which can delay proper healing. Voice disorder diagnosis consists of several medical examinations, including acoustic analysis. This involves the processing and analysis of various acoustic features of the voice signal, which can reveal changes or alterations in voice quality caused by specific diseases [1]. In this context, AI can be a valid and powerful tool. However, utilization of these solutions imposes many additional tasks, ranging from data collection and preprocessing to ground truth labeling and correct algorithm selection, as well as correct experimental setup. Moreover, the choice of appropriate acoustic features is fundamental. The use of acoustic features"}, {"title": "2 Related Works", "content": "We used the Scopus database to conduct a research of current works in the field of voice pathology detection and classification from voice recordings. Specifically, we used two search strings:\n\u2022 (\"machine learning\" OR \"deep learning\" ) AND TITLE ( \"voice pathology\" ),\n\u2022 (\"machine learning\" OR \"deep learning\" ) AND TITLE ( \"voice disorder\" ).\nThe search resulted in a wide variety of articles. There was an observable trend in the data used, The majority of works utilized data from the Saarbr\u00fccken Voice Database (SVD) [4], Massachusetts Eye and Ear Infirmary (MEEI) database [5], Arabic Voice Pathology Database (AVPD) [6], VOice ICar fEDerico II database (VOICED) [7, 8], and Far Eastern Memorial Hospital (FEMH) data collected for the 2018 FEMH Voice Data Challenge [9]. A few works used their own datasets to train the classifier. We decided to concentrate primarily on studies using the SVD (see Section 3 for more information about the data) due to two main reasons. First, to the authors' best knowledge, SVD and VOICED are the only publicly available databases from the mentioned, disqualifying the remaining databases from being used as they do not allow to reproduce any experiments conducted on this data. Additionally, the preference for the SVD comes from its size, being larger than VOICED, and from the range of pathologies it contains. As we aim to develop a model for voice pathology detection, the data should also represent rare diseases along with the common ones. Note, that the SVD is unique in that it includes multiple recordings from the same subjects, which could result in data leakage if not properly managed.\nIn [10], authors of the study test various acoustic features in combination with XGBoost, IsolationForest, and DenseNet models to determine the pathologic samples, reaching an F1 score of 73.3% and unweighted average recall (UAR) we computed as 73.3%. While, in [11], features derived from self-supervised learning models, Data2Vec and Wav2Vec, alongside mel-frequency cepstral coefficients (MFCC) are explored. The reliability of these features to evaluate voice quality is tested using support vector machine (SVM) and deep neural networks (DNN), achieving an accuracy of 77.83% and UAR we computed as 77.86%."}, {"title": "2.1 Remarks on Deviations from Consensus-Based Reporting Standards in Machine Learning Research", "content": "During reviewing the works that we introduced in this section, we noticed that none of the researched work handled the potential data leakage introduced by the existence of multiple recordings produced by the same patient. Moreover, some works might have introduced data leakage by scaling data and oversampling before splitting into training and testing subsets. Then, we observed that most works did not report proper performance metrics that reflect the imbalanced dataset. Also, none of the works shared working code to reproduce the results and most works did not include enough information about the data selection, preprocessing methods, models and their parameters, hyperparameters, and architectures, or the validation process. As mentioned in the previous text, some works took a subset of the available data which, according to us, severely limits the applicability in clinical practice. Due to these facts, unfortunately, we suspect that most of the reported performances might be overoptimistic."}, {"title": "3 Data", "content": "Voice pathology detection studies often utilize the MEEI [5], VOICED [7, 8], FEMH Voice Data Challenge 2018 [9], and AVPD [6] datasets. However, the MEEI dataset is no longer available, and the FEMH dataset is not publicly available. We do not consider the AVPD dataset feasible for our study, as it includes only five types of pathologies: vocal fold cysts, nodules, paralysis, polyps, and sulcus. Thus, it does not reflect the number of various pathologies presented in the general population. The VOICED database comprises 20 different pathologies, but it contains a limited sample size of 208 recordings. We used the SVD [4] for our work, as was already mentioned in Section 2. The SVD was developed by the Phonetics group at the Department of Language Science and Technology, Saarland University and is obtainable at https://stimmdb.coli.uni-saarland.de/help_en.php4. The dataset contains data from 1853 patients and includes different types of voice recordings, such as pronunciations of various vowels in low, neutral, and high pitches, as well as tones alternating between high and low pitches. Additionally, the dataset features recordings of patients saying \"Guten Morgen, wie geht es Ihnen?\" (Good morning, how are you?). Alongside the voice recordings, the database also offers EGG signals for download. The database includes not only recordings but also an overview of patient information specific to each recording. To be precise, the database contains the recording's unique identifier, patient's unique identifier, sex, age at the time of recording, recording date, type of recording, list of patholo-"}, {"title": "4 Feature Extraction", "content": "Due to the nature of the data, our first step after trimming the quiet parts of the recordings was extracting information in the form of various acoustic features. Acoustic features can be categorized into several groups based on the type of transformation of the sound signal: time-domain, spectral, and cepstral features. We depended on various Python libraries for feature extraction, namely parselmouth for features related to fo and formants, spkit for Shannon entropy, torchaudio for linear-frequency cepstral coefficients (LFCC), librosa for the remaining acoustic features, and scikit-learn and numpy for calculating statistical values from the extracted features or from the sig-"}, {"title": "4.1 Time-Domain Features", "content": "The majority of the time-domain features stem from the measurement of regularity of the sound signal. The idea behind the measurement of (ir)regularities in a voice recording is that human voice is composed of periodic and random components and there is a difference between the amount of irregularity between the healthy and pathological voices."}, {"title": "4.1.1 Fundamental frequency", "content": "The periodic part is characterized by fo. Voice pathologies usually increase the ratio between the random and periodic components, also altering fo. Thus, pathologies might be observable from the change of these parameters. The main disadvantage of several time-domain measures is the necessity to estimate fo. In heavily altered voices, caused by some pathologies, it might be impossible to estimate fo. There are also multiple algorithms for fo estimation and generally, each may yield different results. We used the parselmouth library, a Python implementation of the Praat program [39], to determine fo, therefore, we utilized the algorithm described in [40] to extract fo for N segments of the samples, denoted as $f_o$. Given that the estimation of fo is calculated for individual segments of each recording, we took the mean (Equation 1) and sample standard deviation (Equation 2) of fo as features for each sample."}, {"title": "4.1.2 Pitch Difference", "content": "We determine the pitch difference as a difference between the maximum and minimum value of the extracted fo (Equation 3). Our reasoning is that a healthy voice should maintain a stable frequency for the full duration of the recording compared to the pathological voice which may fluctuate."}, {"title": "4.1.3 Jitter", "content": "Jitter, also called fundamental frequency perturbation, is a measure of irregularities in fo. Jitter is a well-known metric, often used in acoustic analysis for different types of signals. In voice pathology, higher values of jitter usually indicate pathology. There are several variations of the measure, in our work, we used the local jitter, also known as jitta (Equation 4) [39], calculated as the mean difference between periods of consecutive glottal cycles $T_i$ across N glottal cycles divided by the mean glottal cycle period of the signal T (Equation 5)."}, {"title": "4.1.4 Shimmer", "content": "Shimmer, also known as amplitude perturbation, measures irregularities in the amplitude of the sound wave and high values of shimmer also indicate potential voice pathology. There are several variations of shimmer similar to the jitter and we opted for the local shimmer, also called shim (Equation 6) [39]. Shim is calculated as the mean difference between amplitudes of consecutive glottal cycles $A_i$ across N glottal cycles divided by the mean glottal cycle amplitude of the signal A (Equation 7)."}, {"title": "4.1.5 Harmonics-to-Noise Ratio", "content": "HNR measures the ratio between the energies of the periodic and aperiodic signal components (Equation 8) [40, 39]. We use the recommended cross-correlation implementation of the harmonicity algorithm where the energy of the harmonic component in a windowed signal $X_{i, k}$ is calculated as the value of a cross-correlation of the signal amplitude s with the same signal shifted by the length $k_{max}$ where the cross-correlation yields the highest result (Equation 9). Note that the energy is normalized by the cross-correlation of the signal with itself without any shift. In Equation 9, i represents the window index, k represents the index of a value in the i-th window, m represents the index of an element with respect to the current window, and n represents the number of samples in a window. The length of"}, {"title": "4.1.6 Zero-Crossing Rate", "content": "Zero-crossing rate (ZCR) measures the rate at which a sound signal s changes signs in a signal split to a predetermined number of windows. This feature is not dependent on the estimation of fo. The calculation for the i-th window is described in Equation 10, where k represents the index of a value in the i-th window, m represents the index of an element with respect to the current window, and n represents the number of samples in a window, set to default value of 2048. Note that is is described in Equation 11, and sgn(x) is described in Equation 12."}, {"title": "4.1.7 Shannon Entropy", "content": "Shannon entropy measures the average level of uncertainty inferent to the possible outcomes of a random variable. Generally, it is defined as in Equation 13 for an independent variable X defined by a set of values X with a probability of occurrence p. In our case, the probabilities p are taken from the histogram of the signal amplitude. The number of bins nbins is determined by the Freedman-Diaconis rule (Equation 14), where N is the length of the signal and $Q_3$, $Q_1$ are amplitude values representing the third and first quartiles, respectively. This feature is also independent of fo."}, {"title": "4.1.8 Skewness", "content": "Skewness measures the concentration of values on either side of a data distribution. Based on the results of the study [41], we expected that the skewness value might be influenced by a pathology, and thus, we included this feature in our experiments. We calculate the skewness according to the definition in Equation 15. The individual cumulants $m_i$ can be determined by Equation 16."}, {"title": "4.2 Spectral Features", "content": "We use spectral features to measure characteristics and differences in the spectra of the sound signal. The most frequent transformation for obtaining spectral information is the Fourier transform. To gain more information about the spectral variations of the signal in time, the signal is split into short windows containing tens to hundreds of milliseconds of the original signal with varying overlaps between these windows. Then, the short-time Fourier transform (STFT) algorithm is applied to each window to gain the spectrum of each window. Finally, the spectra are concatenated and plotted in the form of a spectrogram - a heatmap showing time on the x-axis, frequency on the y-axis, and power of the spectrum as a color. In regard to the voice pathology detection, the expectations are that due to the difference in the ratio of (dis) harmonic components, the spectral features should be able to distinguish between pathological and healthy voices."}, {"title": "4.2.1 Spectral Flatness", "content": "Spectral flatness is used to quantify how much tone-like a signal is, as opposed to being noise-like [42]. It is calculated as a ratio of the geometric mean of the power spectrum S to its arithmetic mean measured across all bands [43] (see Equation 17), where N denotes the total number of bands of the power spectrum."}, {"title": "5 Data Augmentation", "content": "In order to address the issue of class imbalance in utilized dataset, that leads to classifiers with high recall and low specificity, we employed the k-means SMOTE algorithm [45]. This technique was specifically applied to the training dataset to enhance the model's ability to learn from minority class instances and thus improve its predictive performance.\nK-means SMOTE is an advanced oversampling method that combines the clustering capabilities of k-means [46] with the synthetic data generation process of SMOTE [47]. The algorithm operates in the following manner:\n1. Apply k-means to identify clusters that are"}, {"title": "6 Machine Learning Models", "content": "In our research, we selected SVM, KNN, DT, random forest (RF), AdaBoost and NB algorithms for binary classification of healthy versus pathological patients. Our choice is based on prior research, where these algorithms were successfully used for binary classification tasks. Additionally, different studies extracted various features from recordings, motivating us to evaluate different feature combinations and test the performance of the classifiers on such combinations. Given that we are dealing with relatively high-dimensional datasets with a limited number of samples, deep learning models would be challenging to apply effectively. Therefore, we believe that using traditional ML models is a suitable approach and aligns with findings from existing research.\nAll evaluated ML models were implemented via the scikit-learn v1.5.1 library [38]. The naming of hyperparameters in the tables in the following text corresponds to the naming of function parameters in scikit-learn."}, {"title": "6.1 Support Vector Machine", "content": "For SVM, we performed a grid search across the kernel functions, the regularization parameter (C), and the gamma parameter. Specifically, we tested the radial basis function (Equation 28) and polynomial kernels (Equation 29) of different degrees due to their occurrences in previous works. In both kernel functions, $\u03b3$ serves as a tunable hyperparameter. Note, that the expression $\u03b3 = $ \"auto\" stands for a where n represents the number of features. The combination of the hyperparameters is described in Table 3."}, {"title": "6.2 k-Nearest Neighbors", "content": "The KNN model has three tunable hyperparameters, namely the number of neighbors, the order of the Minkowski metric, and the weights of the neighbors. We tested only the first and second order p of this metric, which also represent the Manhattan and Euclidean distances. Additionally, we tested: 1) uniform weights, where all neighbors are given equal weight, and 2) distance-based weights, where the weight of each neighbor is inversely proportional to its distance from the classified sample. The combination of the hyperparameters is described in Table 4."}, {"title": "6.3 Gaussian Naive Bayes", "content": "The way the NB model is implemented, there is only one hyperparameter representing a portion"}, {"title": "6.4 Decision Tree", "content": "For the DT classifier [48], which is relatively fast in terms of computational time, we performed a grid search across various hyperparameters. To evaluate the quality of splits, we experimented with different criteria, including Gini impurity, log loss, and entropy. Regarding the strategy to choose the node for a split, we tested the \"best\" strategy, which chooses the best split, and the \"random\" strategy. The minimum number of samples required to split a node was evaluated across values ranging from 2 to 10. For the number of features considered when looking for the best split, we evaluated both the square root and the binary logarithm of the total number of features. The summarized specification of hyperparameters that were used for a grid search is in Table 5."}, {"title": "6.5 Random Forest", "content": "During preliminary testing of RF on several datasets, we concluded that the combination of the Gini criterion and the number of features to consider when looking for the best split equal to the square root of the total number of features would lead to better performance and allow us to decrease the dimensionality of the grid search. We utilized the bootstrapping technique for building the trees, keeping its default settings in scikit-learn. For the grid search, we considered two hyperparameters, namely the minimum number of samples in a node before split and number of trees in the forest. The values of hyperparameters tested in all possible combinations are in Table 6."}, {"title": "6.6 AdaBoost", "content": "We used DTs as weak learners for training the AdaBoost classifier [49]. Hyperparameters utilized in the grid search procedure were the learning rate and the number of estimators. See Table 7 for specific values of those hyperparameters."}, {"title": "7 Validation of Results", "content": "It is crucial to establish robust performance metrics for our model to accurately assess its capability in distinguishing between healthy and pathological patients. Equally important is to ensure that the reported validation results are robust, minimizing the influence of randomness to guarantee the reliability and consistency of our conclusions, as well as the possibility to independently reproduce our findings."}, {"title": "7.1 Used Metrics", "content": "The imbalance between healthy and pathological samples in the dataset (408 healthy females, 545 females with pathologies, 252 healthy males, and 443 males with pathologies) introduces a bias into commonly used metrics such as accuracy, F1 score, precision, and negative predictive value [50]. Especially accuracy can dangerously show overoptimistic results and provide misleading information [51]. To address this issue and provide metrics that reflect class imbalance, we evaluate the performance of the ML algorithms using sensitivity (Equation 32), specificity (Equation 33), unweighted average recall (Equation 35), geometric mean (Equation 34), and bookmaker informedness (Equation 36). As all these metrics provide information about the successful classification, we also evaluate the MCC (Equation 37), which, although not entirely unbiased, has a smaller bias compared to accuracy and also takes into account the errors [50].\nIn our research, true positive (TP) predictions are correctly predicted positive (pathological) samples, and true negative (TN) predictions are correctly predicted negative (healthy) samples. False positive (FP) predictions mark positive predictions of negative samples, and false negative (FN) predictions mark negative predictions of positive samples. An optimal classification algorithm should reach zero FP and FN predictions.\nSensitivity, also known as recall or true positive rate, is calculated as a ratio between the TP and all positive samples. It shows the ability of a classifier to classify positive samples.\nSpecificity, also known as selectivity or true negative rate, is calculated as a ratio between the TN and all negative samples. It demonstrates the ability of a classifier to classify negative samples. In the case of the dataset we utilized in this work, specificity is an important metric as it clearly shows the ability of our model to learn to predict the minority class.\nThe geometric mean (GM) is defined as the geometric mean of sensitivity and specificity (Equation 34)\nUAR is defined as an arithmetic average of sensitivity and specificity (Equation 35). The bookmakers informedness (BM) is defined by Equation 36. Note that BM, UAR, and GM give the sensitivity and specificity the same weight, regardless of the distribution of positive and negative samples in the dataset. Unlike to UAR, BM and GM penalize the performance on the minority class more, resulting in 0 values for 0 sensitivity (or specificity). MCC (Equation 37) summarizes the results using all available information about classification results in the form of TP, TN, FP, and FN."}, {"title": "7.2 Validation Approach", "content": "Due to the limited size of the dataset and its imbalance, we employed a stratified 10-fold crossvalidation method during the grid search, which should lead to less biased results of classifier performances [52]. The dataset was split into ten folds, each preserving the distribution of healthy and pathological samples. For each iteration, one fold was used as the validation set, while the remaining nine folds were used for training. The training data was augmented using the k-means SMOTE-based algorithm (see Section 5). Then, each feature in the training dataset was scaled to an interval [0, 1] using the min-max scaler. Parameters obtained for training dataset scaling were used to scale the validation dataset, which effectively prevents the data leakage.\nThis process yielded ten values for each performance metric. We then computed the mean value of each metric across the folds, following standard cross-validation practice. The workflow of the data pipeline, from the augmentation to the calculation of the results from the cross-validation, is illustrated in Figure 8.\nAfter completing the grid search, we identified the 1000 best performing classifier configurations for each classifier type based on MCC. To account"}, {"title": "7.3 Ensuring Reproducibility", "content": "To maintain reproducibility of our findings, we implemented multiple mechanisms to ensure that all computations provide the same results and that everybody can verify that our results were produced by the code we supply. The main problem we had to mitigate was that the data we used were not released under an explicit license. Therefore we could not share the original SVD data and we had to implement methods, based on using SHA256 checksum, to validate input data and intermediate results. This also caused more complicated setup of the working environment. Some algorithms used in the experiment were based on randomness, usually derived from the initial random values of trainable parameters. To mitigate these problems, we explicitly initialized pseudo-random number generators (PRNGs) with a seed (value 42 is used). Some differences between operating systems and versions of utilized software also hindered our efforts to provide a fully reproducible code. An example of such differences is the order of files listed in a directory. We tried"}, {"title": "8 Results & Discussion", "content": "All calculations were implemented in Python 3.12 [53]. The code is available at https://github.com/aailab-uct/Automated-Robust-and-Reproducible-Voice-Pathology-Detection. We used classifiers implemented in the scikit-learn v1.5.1 [38]. The ML pipeline and k-means SMOTE-based algorithm are implemented via the imbalance-learn v0.12.3 library [54]. The computations were done on a desktop computer with an AMD Ryzen 9 5900X 12 cores CPU running at 3701 MHz with 128 GB RAM, Samsung SSD 870 QVO 2TB hard drive, and GNU/Linux OS Ubuntu 22.04 LTS. The libraries used for feature extraction, data augmentation, and model training are listed in the requirements.txt file included in the aforementioned repository. As we mentioned and explained in Section 7, we decided against presenting the accuracy score as the data used for training and evaluation of the dataset was moderately imbalanced. Instead, we provide an alternative in the form of MCC, UAR, GM, and BM. However, if needed, accuracy can still be found in raw results reported in the repository. In Table 8, you can see the results for the best performing classifiers for each sex and classifier type, the configurations of the datasets that we used to reach the best results for the classifier are in Table 9 for females and Table 10 for males. Note that we did not optimize any parameters of the NB classifier, therefore, the standard deviation is kept"}, {"title": "9 Conclusion", "content": "In this work, we introduced several important topics related to voice pathology detection. We summarized the current state-of-the-art achievements of voice pathology detection and classification using the SVD in combination with various feature extraction techniques and ML classifiers. We explained the problematics of potential overoptimistic performance reports from voice pathology classification and the problems with reproducibility. We also addressed the potential problems with data leakage introduced by the SVD if multiple recordings from a single patient are not addressed properly. Moreover, we proposed a combination of data augmentation based on k-means SMOTE and feature extraction with the addition of a pitch difference as a new feature. We also established a way of validating a classifier performance in the field of voice pathology detection which would possibly prevent the reporting of overestimated results. While our sex-based models reached a MCC of 0.7099 for females (AdaBoost) and 0.6668 for males (SVM), we also examined and experimentally evaluated the contribution of various feature subsets, that are commonly used in voice pathology detection studies using ML classifiers. During our research, we identified, as many researchers before us, the lack of data for voice pathology detection as a significant limitation in current research and a major obstacle to the use of ML-based models in clinical settings. Without more comprehensive databases that include a broad range of pathologies and are recorded with at least the same sampling rate as the SVD, it is difficult to establish the external validity of these models. All calculations were implemented in Python 3.12 [53]. The code is available at https://github.com/aailab-uct/Automated-Robust-and-Reproducible-Voice-Pathology-Detection. We used classifiers implemented in the scikit-learn v1.5.1 [38]. The ML pipeline and k-means SMOTE-based algorithm are implemented via the imbalance-learn v0.12.3 library [54]. The computations were done on a desktop computer with an AMD Ryzen 9 5900X 12 cores CPU running at 3701 MHz with 128 GB RAM, Samsung SSD 870 QVO 2TB hard drive, and GNU/Linux OS Ubuntu 22.04 LTS. The libraries used for feature extraction, data augmentation, and model training are listed in the requirements.txt file included in the aforementioned repository. As we mentioned and explained in section 7, we decided against presenting the accuracy score as the data used for training and evaluation of the"}, {"title": "CRediT authorship contribution statement", "content": "Jan Vrba: Conceptualization, Formal analysis, Investigation, Methodology, Project administration, Software, Resources, Supervision, Visualization, Writing - original draft, Writing \u2013 review & editing\nJakub Steinbach: Conceptualization, Data curation, Formal analysis, Funding acquisition, Investigation, Methodology, Software, Resources, Writing - original draft, Writing \u2013 review & editing\nTom\u00e1\u0161 Jirsa: Formal analysis, Data curation, Funding acquisition, Software, Resources, Validation, Writing \u2013 original draft, Writing \u2013 review & editing\nLaura Verde: Conceptualization, Formal analysis, Methodology, Software, Writing - original draft, Writing - review & editing\nRoberta De Fazio: Formal analysis, Software, Writing \u2013 review & editing\nNoriyasu Homma: Methodology, Project administration, Supervision, Writing - review & editing\nYuwen Zeng: Validation, Writing \u2013 review & editing\nKei Ichiji: Validation, Writing - review & editing\nLuk\u00e1\u0161 H\u00e1jek: Resources, Validation, Writing - review & editing\nZuzana Sedl\u00e1kov\u00e1: Validation, Writing \u2013 review & editing\nJan Mare\u0161: Funding acquisition, Resources, Supervision, Writing \u2013 review & editing"}, {"title": "Declaration of competing interest", "content": "The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper."}, {"title": "A Reforms checklist", "content": "Module 1: Study design\n1a. State the population or distribution about which the scientific claim is made\nThe population for our scientific claim consists of German females aged 18 to 94 and German males aged 18 to 89. These individuals recorded their voices pronouncing the /a:/ vowel at a normal pitch at the Institute f\u00fcr Phonetik, Univer-"}]}