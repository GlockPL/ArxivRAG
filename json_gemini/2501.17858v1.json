{"title": "Improving Your Model Ranking on Chatbot Arena by Vote Rigging", "authors": ["Rui Min", "Tianyu Pang", "Chao Du", "Qian Liu", "Minhao Cheng", "Min Lin"], "abstract": "Chatbot Arena is a popular platform for evaluating LLMs by pairwise battles, where users vote for their preferred response from two randomly sampled anonymous models. While Chatbot Arena is widely regarded as a reliable LLM ranking leaderboard, we show that crowdsourced voting can be rigged to improve (or decrease) the ranking of a target model mt. We first introduce a straightforward target-only rigging strategy that focuses on new battles involving mt, identifying it via watermarking or a binary classifier, and exclusively voting for mt wins. However, this strategy is practically inefficient because there are over 190 models on Chatbot Arena and on average only about 1% of new battles will involve mt. To overcome this, we propose omnipresent rigging strategies, exploiting the Elo rating mechanism of Chatbot Arena that any new vote on a battle can influence the ranking of the target model mt, even if mt is not directly involved in the battle. We conduct experiments on around 1.7 million historical votes from the Chatbot Arena Notebook, showing that omnipresent rigging strategies can improve model rankings by rigging only hundreds of new votes. While we have evaluated several defense mechanisms, our findings highlight the importance of continued efforts to prevent vote rigging. Code is publicly available to reproduce all experiments.", "sections": [{"title": "1. Introduction", "content": "A variety of large language models (LLMs), both closed-source and open-source (OpenAI, 2024; Dubey et al., 2024), are now available to the community. Evaluating their alignment with human preferences is crucial for selecting suitable models in downstream applications (Ouyang et al., 2022). To meet this need, Chatbot Arena (Zheng et al., 2023a; Chiang et al., 2024) provides an open platform for conducting pairwise battles between LLMs, where users vote for their preferred response from two randomly selected anonymous models. These votes are used to compute Elo ratings for LLMs, with higher rankings on Chatbot Arena's leaderboard offering substantial promotional benefits.\nChatbot Arena is widely popular, but it relies on millions of user votes collected in the wild, which can be noisy and biased. Several strategies have been implemented to enhance the leaderboard's reliability and reduce potential gameability, including controlling for output length and style (Dubois et al., 2024; Li et al., 2024a), detecting anomalous voting patterns and bot activity (Chiang et al., 2024), categorizing prompts for data curation (Li et al., 2024b;c), and invalidating votes if anonymous model identities are revealed in the responses (Chiang et al., 2024).\nAlthough these strategies have significantly reduced (mostly unintentional) voting biases and noise, this paper demonstrates that crowdsourced votes in Chatbot Arena can still be maliciously rigged to manipulate the ranking of a target model mt, either improving or decreasing it. We first introduce a straightforward target-only rigging strategy that focuses solely on new battles involving mt, identifying it via watermarking (Zhao et al., 2024) or a binary classifier (Huang et al., 2025), and exclusively voting for mt wins."}, {"title": "2. Preliminaries", "content": "We first formalize the basic operations of Chatbot Arena in Section 2.1, including the mechanism for collecting pairwise human-annotated votes and calculating rating scores. Next, in Section 2.2, we introduce various threat models of vote rigging based on the adversary's accessibility."}, {"title": "2.1. Chatbot Arena", "content": "The Chatbot Arena leaderboard comprises K models, denoted as {m1,..., mk}, with their rating scores calculated on a collection of user votes V. To collect a new vote, a pair of model indices a and b is sampled from the joint distribution Py, where the subscript V indicates that the distribution depends on previously collected votes. The user can query both sampled models ma and m\u044c with any prompt string s\u2208 S, where S denotes the natural language space, and cast a vote for their preferred response between ma(s) and m\u044c(s). Then the vote set V will be updated according to the selected voting option:\ni. a wins: Va>b = VU {ea - eb}, V \u2190 Va>b;\nii. b wins: Va<b = VU {eb \u2013 ea}, V \u2190 Va<b;\niii. Tie: Va=b = VU {ea-eb}\u222a{eb - ea}, V \u2190 Va=b;\niv. Abstain: V is unchanged,\nwhere ek \u2208 RK is the k-th basis unit vector and we slightly abuse the notation of U to denote the appending operation.\nCalculation of rating scores. Chatbot Arena applies the Elo rating system to benchmark models. According to Chiang et al. (2024), Chatbot Arena initially used online Elo scores to calculate model ratings, but later switched to Bradley-Terry (BT) scores (Bradley & Terry, 1952) for better statistical estimation. Given a collected vote set V, we can calculate BT scores for the K models on the leaderboard, denoted in a vectorized form as rBT \u2208 RK, where rBT[k] is the BT score of the k-th model. The BT scores are derived from fitting the logistic relationships on V, formulated as\n$r^{BT} = \\arg \\min_{r} \\mathbb{E}_{v \\in V}[L_{BCE}(v, r)]$, (1)\nwhere LBCE (v, r) = \u2212 log(\u03c3(vr)) is the binary cross-entropy (BCE) loss, and \u03c3(\u00b7) is the Sigmoid function."}, {"title": "2.2. Threat Model", "content": "Throughout this paper, our adversarial rigging goal is to promote the ranking of a target model mt on Chatbot Arena through vote rigging. This is achieved by submitting new votes, where each voting option is strategically selected to promote the target model's ranking.\nBased on the adversary's accessibility, we pinpoint the key elements in our threat model as described below:\n\u2022 Historical votes (VH or rBT): whether the adversary has access to the historical voting data VH or can only access to the BT scores $r_{BT}$ from the public leaderboard;\n\u2022 Model identities (Real-name or Anonymous): whether the adversary can directly access the identities of the sampled models ma and my in each new battle;\n\u2022 Sampling distribution (Py or Unknown): whether the adversary can know the sampling distribution Py or not;\n\u2022 Other users' votes (\u00d8 or Vo): when the adversary submits (malicious) new votes, other users may also submit new votes simultaneously, denoted as Vo.\nFor example, when the adversary aims to manipulate the real-world Chatbot Arena platform, the threat model can be written as {rBT, Anonymous, Unknown, Vo}."}, {"title": "3. Vote Rigging Strategies", "content": "In this section, we discuss various vote-rigging strategies aimed at promoting the ranking of the target model mt. Generally, under a given threat model (where model identities are Anonymous), a rigging strategy manipulates new votes and consists of two key components:\n\u2022 The de-anonymizing function A(s, mk(s)) = k takes the user prompt s and the model response mk(s) as inputs, aiming to de-anonymize the true identity of mk (or its index k) through the predicted identity m\u1ef9 (or the index k). This function is typically trained or designed to maximize the probability P(k = k);\n\u2022 For each new vote between the sampled models ma and m\u044c, the vote manipulation function M(a, b) takes the identities a and b predicted by A as inputs and returns one of four voting options: a/a wins, b/b wins, Tie, or Abstain. Note that M may also depend on additional information, such as historical votes or ranks, as described in our omnipresent rigging strategy.\nIn the following, we elaborate on a vanilla target-only rigging strategy and our proposed omnipresent rigging strategy."}, {"title": "3.1. Target-Only Rigging", "content": "To promote the ranking of the target model mt, a straightforward approach is to rig votes only for new battles predicted to involve mt (specifically, when t \u2208 {a,b}). In this case, the de-anonymizing function focuses exclusively on identifying mt, formulated as At-only (s,mk(s)) \u2208 {t, \u00abt}, where \u2192t represents all other model indices.\nTwo concurrent works (Zhao et al., 2024; Huang et al., 2025) have explored similar target-only rigging strategies. These works implement the de-anonymizing function At-only using either watermarking/attribution techniques or a binary classifier. Based on the implemented At-only, they further define the vote manipulation function Mt-only as\n$M_{t-only}(a, b) = \\begin{cases} \\text{a wins} & \\text{if } a = t, \\\\ \\text{b wins} & \\text{if } b = t, \\\\ \\text{Passive} & \\text{otherwise}. \\end{cases}$ (2)\nwhere the Passive option can be set to Tie (T-Tie), Abstain (T-Abstain), a random selection (T-Random), or aligned with the normal user voting distribution (T-Normal). In our following experiments, we treat these target-only rigging strategies as our baselines."}, {"title": "3.2. Omnipresent Rigging", "content": "While target-only rigging strategies are straightforward, they are inefficient in practice, as they manipulate only the new votes predicted to involve mt. For example, with over 190 models on the Chatbot Arena platform and a uniform model sampling distribution, the probability of a specific target model being involved in a battle is only about 1%. Consequently, target-only rigging strategies may passively select the voting options for approximately 99% of new battles. As reported in Huang et al. (2025), improving a single ranking position for a target model (e.g., from rank 129 to 128 or rank 5 to 4) requires over 10,000 votes for low-ranked models and more than 20,000 votes for high-ranked models.\nTo enhance rigging efficiency, we draw inspiration from the following observation on Chatbot Arena's rating mechanism (an informal proof is provided in Appendix A.1):\nObservation (omni-property): when the BT scores rBT are calculated on a sufficient number of votes in V (by Eq. (1)), any new vote on a battle between ma and m\u044c can influence the ranking of the target model mt, even if mt is not directly involved in the battle (i.e., t \u2209 {a,b}).\nBased on this, we propose omnipresent rigging strategies, which actively manipulate every new vote, regardless of whether mt is involved in the battle. We implement the de-anonymizing function Aomni (s, mk(s)) \u2208 {1,...,K} as a multi-class classifier (detailed in Appendix B). For the vote on each new battle, Aomni predicts the identities of the sampled models as a and b. The design of the vote manipulation function Momni then depends on the adversary's accessibility to historical votes, as described below.\nBT-based omni rigging (Omni-BT). When the adversary has direct access to the historical voting data VH, it can combine its manipulated votes VM to form V = VHU VM. For a new battle between ma and m\u044c, the Omni-BT manipulation function can be expressed compactly as:\n$M_{omni}^{BT} = \\arg \\max_{V'}\\mathbb{R}^{BT} (r^{BT})$, (3)\nwhere V' \u2208 {Va<b, Va>b, Va=b, V} represents the four voting options: a/a wins, b/b wins, Tie, and Abstain, as introduced in Section 2.1. Here, RBT(\u00b7) denotes the rigging objective of Omni-BT."}, {"title": "4. Sanity Check with Idealized Rigging", "content": "We start by rigging against the idealized scenario using the threat model {VH,Real-name, Py, ()}. Results from this sanity check indicate our optimal rigging performance and serve as an upper bound for the capability analysis in Section 5. Without specific assumptions on the sampling distribution Py, we use uniform sampling with the marginal probability of a sampling mk being Pk = 1/K. Additionally, to understand how effective vote rigging performs, we include the w/o rigging case in which votes are sampled using the normal user voting distribution as comparisons. We report our initial results by rigging 20,000 new votes and defer results with larger numbers of votes to Appendix C.1. We demonstrate the ranking changes of diverse target models mt including Llama-2-13B-Chat (Touvron et al., 2023), Mistral-7B-Instruct-v0.2 (Jiang et al., 2023), Qwen1.5-14B-Chat (Bai et al., 2023), Vicuna-7B (Chiang et al., 2023b), Gemma-2-9B-it (Gemma et al., 2024b), and Phi-3-small-8k-Instruct (Abdin et al., 2024) and defer rigging results with 22 extra models (used in Huang et al. (2025)) to Appendix C.2.\nAs shown in Figure 2, all rigging strategies effectively improve mt's ranking compared to the scenarios without rigging, achieving an average of 6-rank improvement. Besides, our omnipresent strategies demonstrate significantly higher rigging efficiency against target-only strategies. For example, when rigging 20,000 new votes, the target-only rigging achieves only an average increase of 4, whereas both omnipresent rigging strategies notably outperform it, resulting in an approximately 10-rank promotion."}, {"title": "5. On Exploring the Rigging Capability", "content": "However, practical vote rigging is typically conducted with limited adversary's accessibility, potentially reducing the manipulation effectiveness. Here, we conduct a series of stress tests to explore whether our strategies remain effective against these more demanding rigging scenarios. Specifically, in Section 5.1 we conduct rigging under the threat model {VH, Anonymous, Py, 0} to explore the impact of inaccurate de-anonymization with predicted probability P(k = k) < 1, then in Section 5.2 we conduct rigging under the threat model {VH, Real-name, Unknown, ()} to simulate the influence of Unknown sampling distribution, and finally in Section 5.3, we conduct rigging under the threat model {VH, Real-name, Py, Vo} to incorporate the influence of concurrent user voting."}, {"title": "5.1. Rigging with Inaccurate De-Anonymization", "content": "Since our rigging strategies rely on m\u1ef9 predicted by the de-anonymizing function A(\u00b7) to select voting options, its predicted probability P(k = k) thus directly impacts the rigging effectiveness. To examine whether vote rigging remains effective against inaccurate de-anonymization,"}, {"title": "5.2. Rigging with Unknown Sampling Distribution", "content": "Practical sampling distributions could be Unknown to users, for example, newly released models might acquire a higher sampling probability to collect enough votes (Zhao et al., 2024). As a result, these non-uniform sampling strategies might potentially reduce Pt, i.e., the marginal probability of sampling mt, thereby decreasing the number of sampled battles containing mt. In this section, we sample new battles using Pt = \u03b2 \u00b7 1/K, where \u03b2 \u2208 [0, 1] controls the degree of probability reduction. When \u03b2 = 0, it indicates that no mt will be sampled for new battles. As shown in Table 2, decreasing Pt significantly reduces the effectiveness of target-only rigging, with most strategies failing completely at \u03b2 = 0.3. In contrast, omnipresent strategies show effective manipulation performance with over 5-rank improvement even when mt is not directly involved in battles."}, {"title": "5.3. Rigging with Concurrent User Voting", "content": "In addition to manipulated votes VM, concurrent votes Vo from other users remain unknown to the adversary, which could affect the rigging effectiveness. For instance, they would lead to an inaccurate calculation of omnipresent rigging objectives RBT(\u00b7) and Ron(\u00b7), thereby impacting the subsequent vote selection of their respective manipulation functions MBT(\u00b7) and Mon(\u00b7). To incorporate the influence of Vo, we use the combined votes V = VHUVMUVO to calculate the final rating. Our results in Table 3 demonstrate that the influence of Vo remains minor, which only introduces an average 1-rank decrease even with a Vo containing 100,000 votes. These findings suggest the resilience of vote rigging against concurrent user voting."}, {"title": "6. Case Study: Rigging Chatbot Arena", "content": "To demonstrate how to improve target model mt's ranking in the realistic leaderboard, we simulate vote rigging against the practical scenario with the threat model being {rBT, Anonymous, Unknown, Vo}. Through this case study, our preliminary findings would serve as a proof-of-concept that exposes the real-world rigging risks within the Chatbot Arena. Specifically, we extract 25 models with around 23,000 English-specific votes from the complete historical records to set up the simulation environment. We present ranking improvements of the target model mt, which is set to be one of the four models including Llama-2-13B-Chat, Mistral-7B-Instruct-v0.2, Qwen1.5-14B-Chat, and Vicuna-7B. Details on the overall model selection can be found in Appendix B.2."}, {"title": "6.1. Towards Simulating Real-world Vote Rigging", "content": "To demonstrate how to improve target model mt's ranking in the realistic leaderboard, we simulate vote rigging against the practical scenario with the threat model being {rBT, Anonymous, Unknown, Vo}. Through this case study, our preliminary findings would serve as a proof-of-concept that exposes the real-world rigging risks within the Chatbot Arena. Specifically, we extract 25 models with around 23,000 English-specific votes from the complete historical records to set up the simulation environment. We present ranking improvements of the target model mt, which is set to be one of the four models including Llama-2-13B-Chat, Mistral-7B-Instruct-v0.2, Qwen1.5-14B-Chat, and Vicuna-7B. Details on the overall model selection can be found in Appendix B.2."}, {"title": "6.2. Ablation Studies on Omnipresent Rigging", "content": "What if unseen prompts are employed for vote rigging? While our initial results use training prompts for rigging, reusing these limited prompts could be easily detected by the quality control in Chatbot Arena such as the simple prompt-deduplication strategy (Chiang et al., 2024). As a result, we aim to investigate whether unseen prompts are effective for rigging, especially without classifier retraining. Our results in Figure 3 (a) show that both omnipresent strategies still effective ranking improvement even rigging with unseen prompts. These preliminary findings indicate the potential scalability of using the multi-class classifier for de-anonymizing rather than directly relying upon the memorization of trained responses.\nExplore the effectiveness of omni rigging with unrecognized models. Since the Chatbot Arena may constantly introduce new models to the leaderboard, which become unrecognized by our trained classifier. To investigate how these models will affect the rigging performance, we include 5 additional models (described in Appendix B.2) that are outside the classification range and conduct rigging within these 30 models. Figure 3 (b) shows that both omnipresent strategies outperform target-only rigging, demonstrating a degree of resilience against unrecognized models.\nRigging the length-control leaderboard. In addition to the original leaderboard, the Chatbot Arean offers the length-control version, which explicitly disentangles the effect of response length in rating calculation. Here we aim to investigate whether our vote rigging is still effective when conducted on the length-control leaderboard. As illustrated in Table 5, our omnipresent rigging still maintains an effective rigging effect achieving an average of 4-rank improvement. Furthermore, we observe an intriguing phenomenon in which Vicuna-7B's ranking is significantly boosted compared to rigging the original leaderboard. While our initial strategies are not specifically designed to achieve this, our findings highlight another vulnerability within the"}, {"title": "7. Defense against Vote Rigging", "content": "To mitigate the risks of ranking manipulation, we discuss several methods to defend against vote rigging, including detecting malicious users and filtering anomalous votes."}, {"title": "8. Related Work", "content": "LLM evaluation. Developing LLMs benchmarking is a crucial task for measuring their intrinsic capabilities. Conventional benchmarks like GLUE (Wang et al., 2018), HumanEval (Chen et al., 2021), MMLU (Hendrycks et al., 2020), and GSM-8K (Cobbe et al., 2021) assess LLMs in a static manner, where they typically rely on predefined test cases. Although convenient, these benchmarks are difficult to comprehensively capture the open-ended generation capabilities (Liang et al., 2023; Peng et al., 2022) of emerging advanced models, and are typically associated with concerns such as dataset contamination (Yang et al., 2023; Sainz et al., 2023) and Out-Of-Distribution robustness (Yuan et al., 2023). To address these challenges, recent progresses (Zheng et al., 2023a; Li et al., 2023; Dubois et al., 2024) employ LLM-as-a-Judge where a strong language model such as GPT-4 (Achiam et al., 2023) serves as a referee for model assessment. While reducing the need for human annotation, these automatic evaluators might suffer from spurious features, such as verbosity and position bias (Dubois et al., 2024; Chen et al., 2024). Unlike traditional benchmarks, Chatbot Arena (Chiang et al., 2024) devises an online platform that allows site users to vote between a pair of anonymous models based on preferred responses. By leveraging crowdsourced voting, the leaderboard aggregates high-diversity human-annotated votes, which features Chatbot Arena the most popular and widely recognized LLM benchmark.\nVulnerability of LLM evaluation. Previous studies (Raina et al., 2024; Shi et al., 2024; Zheng et al., 2024) have exposed the vulnerability of the LLM-as-a-Judge by adversarially cheating the LLM evaluator. While these studies primarily concentrate on identifying vulnerabilities in automatic evaluation paradigms, our paper distinguishes them with a focus on rigging the human-voted Chatbot Arena. In concurrent with our work, Zhao et al. (2024); Huang et al. (2025) leverage strategies such as watermarking and binary-classifier to identify and exclusively vote for the target model mt, which can be absorbed within our general target-only rigging strategy. Additionally, we provide a more unified rigging framework along with an in-depth analysis of rigging capability. Our proposed omnipresent rigging strategy significantly improves the rigging efficiency and is effective even if mt is not directly involved in battles."}, {"title": "9. Conclusion", "content": "In this paper, we expose the vulnerability within Chatbot Arena where rankings of target model mt can be improved through a simple target-only rigging strategy. However, given the large number of models on Chatbot Arena, this strategy could be practically inefficient. To tackle this, we further propose the omnipresent rigging strategy by redesigning rigging objectives with omni-property, which significantly improves the rigging efficiency and is effective even without directly rigging mt. While our study primarily presents proof-of-concept experiments, practical adversaries could simply use the multi-class classifier or more advanced de-anonymizing functions Aomni(\u00b7) to predict model identities and cast malicious new votes to boost mt's ranking with substantial promotional benefits. In conclusion, our findings highlight the challenges of providing a faithful LLM evaluation with human-annotated votes. Furthermore, devising effective anti-rigging defenses would be critical in future research to preserve the integrity of not only the Chatbot Arena but also emerging voting-based evaluation systems."}, {"title": "Impact Statements", "content": "Due to Chatbot Arena's widespread popularity in LLM evaluation, it is possible that practical adversaries could exploit our rigging strategies to improve their own target models' ranking for substantial promotional benefits. These malicious behaviors would put other normal model developers' interests at risk, and even worse, undermine the reliability and trustworthiness of Chatbot Arena. On the other hand, while we have discussed several defense methods against vote rigging, our initial attempts at rigging defense highlight the difficulties in completely eliminating the manipulation effect. As a result, we encourage the community to focus on developing more robust defense mechanisms to mitigate the rigging vulnerabilities of Chatbot Arena as well as strengthening the integrity of emerging voting-based evaluation systems such as Copilot Areana and WebDev Arena."}, {"title": "A. Ablation Studies of Different Rigging Objectives for Omni Rigging", "content": "In this section, we illustrate why we choose $R_{rel}^{BT} = r_t^{BT} - r_{\\hat{t}}^{BT}$ that measures the relative rating increase between $m_t$ and $m_{\\hat{t}}$ as our rigging objective. For comparison, we implement a straightforward objective $R_{abs}^{BT} = r_t^{BT}$ that directly maximizes the absolute rating increase. We reconduct experiments under the setting in Section 4 and present comparison results of their average ranking increase across all manipulated battles in Figure 5. It is observed that by maximizing the relative rating increase, we achieve a more stable and efficient ranking promotion. In practice, the adversary may explore more effective rigging objectives, which is worth discussing in future studies."}, {"title": "B. Omnipresent Rigging based on Multi-Class Classifier", "content": "We train a classifier to acquire LLM identities based on their individual responses. Formally, let $f_{\\theta}(\\cdot): S \\to R^N$ denote the classifier parameterized by $\\theta$, where $N < K$ indicates the number of models being classified. We construct the training dataset D by prompting each considered model $m_n$ with a set of prompts and labeling their responses with corresponding indexes $n \\in \\{1, ..., N\\}$. These labeled corpus $d := (m_n(s), n)$ can then be utilized to optimize $\\theta$ by minimizing the Cross-Entropy (CE) Loss:\n$\\theta^* = \\arg \\min_{\\theta} E_{d \\in D} \\left[-\\log\\left(\\frac{\\exp(f_{\\theta}(m_n(s))[n])}{\\sum_{j=1}^N \\exp(f_{\\theta}(m_j(s))[j])}\\right)\\right],$ (8)"}, {"title": "A.1. An Informal Proof of Omni-Property", "content": "Given a collected voting set V and the target model mt, we assume, without loss of generality, that a (malicious or normal) user votes for a wins in a new battle between $m_a$ and $m_b$, where $t \\neq \\{a,b\\}$. After this new vote, the voting set is updated to $V_{a>b}$ as described in Section 2.1.\nIt is directly evident by Eq. (1) that the BT scores on $m_a$ and $m_b$ will change, i.e., $r^{BT}[a] \\neq r^{BT}[a]$ and $r^{BT}[b] \\neq r^{BT}[b]$. Then since the sampling distribution $P_V$ is always non-zero on all battle pairs and the collected voting set $V$ is assumed to be sufficiently large, it is reasonable to conclude that at least one vote on the battle between $m_t$ and $m_a$ or $m_t$ and $m_b$ is included in $V$. Consequently, the value of $r^{BT}[t]$ depends on $r_a^{BT}$ and/or $r_b^{BT}$, and  depends on $r_a^{BT}$ and/or $r_b^{BT}$. Thus, we can conclude that $r^{BT}[t] \\neq r^{BT}[t]$, indicating that a new vote on the battle between $m_a$ and $m_b$ will influence the BT score of the target model $m_t$."}, {"title": "A.2. Does Improving Relative Rating Increase Better than Improving Absolute Rating Increase for Omni-BT", "content": "In this section, we illustrate why we choose $R^{BT}(r^{BT}) = r^{BT}[t] - r^{BT}[\\hat{t}]$ that measures the relative rating increase between $m_t$ and $m_{\\hat{t}}$ as our rigging objective. For comparison, we implement a straightforward objective $R^{BT}(r^{BT}) = r^{BT}[t]$ that directly maximizes the absolute rating increase. We reconduct experiments under the setting in Section 4 and present comparison results of their average ranking increase across all manipulated battles in Figure 5. It is observed that by maximizing the relative rating increase, we achieve a more stable and efficient ranking promotion. In practice, the adversary may explore more effective rigging objectives, which is worth discussing in future studies."}, {"title": "A.3. What if Maximizing the Win Rate of One Model for Omni-On", "content": "Our original Omni-On strategy in Eq. (5) aims to maximize the average pairwise win rates over ma and m\u044c. Here, we investigate an intriguing question: what if we only consider maximizing the win rate of one model (either ma or m\u044c)? We formulate our problem into two straightforward rigging objectives: the first involves maximizing the win rate over the model with a higher ranking, with the objective as\n$R_{Min}^{On}(r^{BT}[t], r_{a}^{on}(\\gamma, \\mu), r_{b}^{on}(\\gamma, \\mu)) = min(W (r^{BT}[t], r_{a}^{on}(\\gamma, \\mu)), W (r^{BT}[t], r_{b}^{on}(\\gamma, \\mu))),$ (6)\nand the other one focuses on maximizing the win rate over the lower-ranking models, with the following objective\n$R_{Max}^{On}(r^{BT}[t], r_{a}^{on}(\\gamma, \\mu), r_{b}^{on}(\\gamma, \\mu)) = max(W (r^{BT}[t], r_{a}^{on}(\\gamma, \\mu)), W (r^{BT}[t], r_{b}^{on}(\\gamma, \\mu))).$"}, {}, {"title": "A.4. Explanation of Why We Do not Update  when Using the Omni-On Strategy", "content": "In Figure 6, we compare the results of updating (Update) and not updating (w/o Update) the  when using the Omni-On strategy. It is observed that updating  results in significantly inferior rigging performance, even when compared to the T-Abstain. This is due to the instability of the online Elo updating which is also discussed in Chiang et al. (2023a), leading to an inaccurate calculation of pair-wise win rates and thus affecting the vote selection of Omni-On."}, {"title": "B.1. Mechanisms of De-Anonymizing Functions", "content": "We train a classifier to acquire LLM identities based on their individual responses. Formally, let  : S \u2192 , denote the classifier parameterized by , where N < K indicates the number of models being classified. We construct the training dataset D by prompting each considered model with a set of prompts and labeling their responses with corresponding indexes n \u2208 {1, ..., N}. These labeled corpus d := (, n) can then be utilized to optimize  by minimizing the Cross-Entropy (CE) Loss:"}]}