{"title": "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning", "authors": ["Aleksander Ficek", "Jiaqi Zeng", "Oleksii Kuchaiev"], "abstract": "Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG) have become popular methods for adapting large language models while minimizing compute requirements. In this paper, we apply PEFT methods (P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer (RETRO) and a baseline GPT model across several sizes, ranging from 823 million to 48 billion parameters. We show that RETRO models outperform GPT models in zero-shot settings due to their unique pre-training process but GPT models have higher performance potential with PEFT. Additionally, our study indicates that 8B parameter models strike an optimal balance between cost and performance and P-tuning lags behind other PEFT techniques. We further provide a comparative analysis of between applying PEFT to an Instruction-tuned RETRO model and base RETRO model. This work presents the first comprehensive comparison of various PEFT methods integrated with RAG, applied to both GPT and RETRO models, highlighting their relative performance.", "sections": [{"title": "Introduction", "content": "Pre-trained large language models have made a demonstrable impact across applications in academia and industry. Many use cases, however, require LLMs adapted to specific tasks and unique information but lack the resources for extensive retraining. To address this, Parameter-Efficient Fine-Tuning (PEFT) (Han et al., 2024) and Retrieval-Augmented Generation (RAG) (Gao et al., 2023) have become popular methods due to their effectiveness and efficiency, inspiring new lines of research. PEFT has been proven to be a comparable substitute to Supervised Fine-Tuning (SFT) by achieving competitive performance at a fraction of the number of updated parameters (Han et al., 2024). In this paper we select P-tuning (Liu et al., 2023), Adapter modules (Houlsby et al., 2019) and Low-Rank Adaptation (LoRA) (Hu et al., 2021) as representative PEFT methods. P-tuning involves training continuous prompt embeddings to guide output for specific tasks without modifying base model parameters. Adapters operate by training fully connected layers inserted throughout the base model while keeping the remaining parameters frozen. LORA employs a similar strategy but further decomposes the inserted layers into low-rank matrices, enhancing efficiency.\nRetrieval-augmented generation (RAG) improves model quality by incorporating external knowledge through mechanisms like BM-25 or TF-IDF (Robertson et al., 2009), online web search (Page et al., 1999), or trained dense retriever models (Karpukhin et al., 2020). Any LLM can be transformed into a retrieval-augmented model by concatenating retrieved sources with the input query, provided it fits within the model's context window. Xu et al. (2023) found that retrieval significantly improves GPT model quality on long context tasks, reducing the \"lost in the middle\" effect (Liu et al., 2024) and offering inherent efficiency benefits.\nAlternatively, there exist multiple works (Borgeaud et al., 2022; Guu et al., 2020; Izacard et al., 2023; Nakano et al., 2021) that have integrated retrieval as part of model pretraining or finetuning to notable success when compared to typical GPT models despite being a much lesser explored domain. RETRO (Borgeaud et al., 2022) is of particular interest due to its unique approach of incorporating a retrieval module directly into the transformer architecture via a chunked-cross attention mechanism and ability to scale to trillions of tokens resulting in reduced perplexity. Subsequently, Wang et al. (2023b) showed that RETRO at sizes up to 9.5 billion parameters largely outperforms GPT on specific knowledge-intensive tasks. Furthermore, Wang et al. (2023a) illustrated that when scaled up to 48 billion parameters and instruction-tuned, RETRO performed better than equivalent GPT models on several question answering, reading comprehension and summarization tasks.\nIn this paper we continue the exploration of RETRO versus GPT through the lens of parameter efficient finetuning. We apply P-tuning, Adapter modules and LoRA to multiple tasks with retrieval for both RETRO and GPT models. To our knowledge, this paper provides the first in-depth comparison of various Parameter Efficient Fine-Tuning integrated with Retrieval-Augmented Generation, uniquely applied to both GPT and RETRO models."}, {"title": "Related Work", "content": "In this section we focus on recent work that combine finetuning with retrieval. A comprehensive survey (Gao et al., 2023) synthetized multiple comparative studies on PEFT and RAG, underscoring the potential benefits of combining these approaches as a promising direction for future investigation. There are multiple works that provide methods to combine RAG with fine-tuning to improve accuracy (Zhang et al., 2024a,b; Rangan and Yin, 2024). Multiple studies have explored the comparison between fine-tuning and retrieval. Lakatos et al. (2024) and Ovadia et al. (2023) reported improved accuracy using RAG over fine-tuning GPT models, while also noting suboptimal results when combining the two methods. Gupta et al. (2024) demonstrated improved outcomes by integrating both approaches for specific agriculture and geography tasks. Additionally, Soudani et al. (2024) compared the efficacy of these methods, including full and QLORA fine-tuning (Dettmers et al., 2024), in low-frequency entity question-answering tasks. These studies collectively suggest the need for comprehensive investigation into multiple PEFT techniques combined with RAG and maintain retrieval pretrained LLMs with PEFT to be unexplored, thereby motivating our research."}, {"title": "Experimental Setup", "content": "To cover several task categories, we use six datasets suited to benefit from retrieval and finetuning. We select Natural Questions (NQ) (Kwiatkowski et al., 2019), TriviaQA (TQA) (Joshi et al., 2017), NarrativeQA (NQA) (Ko\u010disk\u1ef3 et al., 2018) and Qasper (Dasigi et al., 2021) for document question answering, QuALITY (Pang et al., 2021) for multiple-choice question answering, and QMSum (Zhong et al., 2021) for query-based summarization (see detail statistics in Appendix B). Each of these datasets contain necessary external knowledge that must be filtered via retrieval and response behaviour that encourages finetuning. Following the official metrics, we use F1 score for evaluating document QA, exact match for mutliple-choice QA and the geometric mean of ROUGE-1/2/L (Lin, 2004) for summarization.\nIn order to undertand the effect of model scales, we use base GPT models of sizes 823M (Extra Small), 2.25B (Small), 8.5B (Medium), 22B (Large), and 43B (Extra Large), as introduced in Wang et al. (2023a), which were pretrained on a massive dataset of 1.2 trillion tokens. We employ the corresponding RETRO models from the same work as the foundation for our retrieval pretrained LLM experiments. Notably, the RETRO architecture features an encoder that extracts neighbors from an external database, which increases the total model size to 877M, 2.47B, 9.5B, 24B, and 48B, respectively. Wang et al. (2023a) found ablating the encoder after pretraining led to comparable results. In our paper we include it so that adapter modules and LoRA layers are added throughout decoder and encoder components. For more on the base models, we refer readers to the original work.\nWe follow Wang et al. (2023a); Xu et al. (2023) to use Dragon+ (Lin et al., 2023) as a retriever. Dragon+ is a dual encoder model that consists of a query encoder and a context encoder. We first chunk each context document with 100 words, and then encode both the questions and all chunks independently with corresponding encoders. The most relevant 5 chunks, ranked by the dot product of the question embedding and chunk embedding, are retrieved as neighbors. For GPT models, they are concatenated together (following the left to right order from the most relevant to least relevant) as the context of the prompt for generation. For RETRO models, they interact with the question during generation through chunked cross-attention.\nWe implement P-tuning in RETRO akin to GPT. Virtual tokens are added to the beginning of the decoder. Based on the design of chunked cross-attention (Wang et al., 2023a), left padding is added to ensure the length of input (virtual tokens + context + question) is a multiple of chunk size. Adapter and LoRA layers are inserted within the transformer blocks in both RETRO and GPT at the feedforward and attention layers. In RETRO they are also inserted in the transformer encoder ingesting the retrieved neighbors. We provide additional hyperparameter tuning, resource utilization and prompt template details in Appendix A."}, {"title": "Results", "content": "Table 1 shows the comprehensive comparison between GPT and RETRO models across five sizes and six datasets. From this table we observe:\n(1) RETRO is generally better than GPT at zero-shot settings. This superiority stems from its unique pre-training approach. By learning to extract salient information from retrieved text and integrate it into its generation process, RETRO develops the capability to harness relevant contextual knowledge, ultimately leading to its good zero-shot performance. In contrast, GPT relies on an auto-regressive loss during pre-training, focusing on accurately predicting next tokens without the benefit of external retrievals. As a result, GPT's ability to learn context-aware question-answering is limited to the presence of relevant data within the pre-training corpus, resulting in significantly less targeted training compared to RETRO.\n(2) Both RETRO and GPT models exhibit saturation points on these datasets around the 8B mark, with a similar pattern emerging between the two models, albeit with RETRO performing less well. This can be seen in Figure 1 and suggests that, for a specific task, a medium-sized PEFT model strikes the optimal balance between cost and performance, making it a sweet spot for many applications.\n(3) For GPT models, P-tuning performs worse at smaller models (<=8B), but it performs as good or better than LoRA and Adapter at the large model sizes. This difference is visualized in Figure 2 and Figure 3 (Appendix C). However, for RETRO models, P-tuning generally under performs the other PEFT methods across all model sizes.\n(4) The performance ceiling for PEFT-tuned models is significantly higher for GPT compared to RETRO as seen in Figure 4 (Appendix C). For example, using medium-sized models, the average score of LoRA with GPT is 40.85, while with RETRO it is 34.24. This disparity suggests that GPT has more room for improvement with PEFT tuning. This phenomenon can also be possibly explained by the two different pre-training strategies. Since GPT has been less targeted on retrieval-augmented generation during pre-training, it opens a larger room for improvement during fine-tuning.\n(5) We also conduct full fine-tuning on medium-sized GPT and RETRO models. For GPT model, we find that full fine-tuning achieves slightly better performance than PEFT on average, with a notable exception on the QASPER dataset, where the difference is more pronounced and full fine-tuning yields significantly better results. In contrast, for RETRO model, full fine-tuning generally performs worse than PEFT, demonstrating the effectiveness of PEFT. These findings are consistent with the observations of Hu et al. (2021)."}, {"title": "Comparing to Instruction-tuned RETRO", "content": "Instruction tuning post retrieval-augmented pre-training (Wang et al., 2023a) has been demonstrated to improve zero-shot performance on RETRO models. A natural thought is that whether Instruction-tuned RETRO (I-RETRO) serve as a better foundation for applying PEFT compared to the base RETRO. To investigate this, we additionally apply PEFT to a medium-sized I-RETRO model and show overall results in Table 2 and more granular results in Table 4 (Appendix C). Our findings reveal that while I-RETRO exhibits improved performance in the zero-shot setting, it has limited scope for further improvement using PEFT. Even with substantial hyperparameter tuning, the average scores across six datasets, using each of the three PEFT methods, demonstrate an approximately 10% gap between I-RETRO and base RETRO. We hypothesize that conceptually both models should be tunable to similar performance but will leave that exploration to future work."}, {"title": "Conclusion", "content": "This study explores Parameter-Efficient Fine-Tuning (PEFT) methods applied to Retrieval-Augmented Generation (RAG) models, comparing GPT and RETRO architectures. RETRO generally outperforms GPT in zero-shot settings due to their pre-training process that integrates external retrieval, enhancing contextual understanding. However, GPT models show a higher performance potential with PEFT, indicating more room for improvement during fine-tuning. Both RETRO and GPT models perform optimally around the 8B parameter mark, balancing cost and performance. While P-tuning is effective in larger models, it lags behind other methods in smaller models, particularly for RETRO. Applying PEFT to Instruction-tuned RETRO yields limited improvement compared to base RETRO, suggesting a saturation point in leveraging pre-training and fine-tuning benefits. Our comprehensive analysis offers valuable insights for optimizing large language models with PEFT and RAG to the community."}, {"title": "Limitations", "content": "Due to the breadth of experiments covered in this work we had to prioritze certain experiments over others. This resulted in us using only the medium sized GPT and RETRO models for additional fine-tuning and Instruction tuning experiments. We believe these results generalize to the other model sizes but leave that to be validated in future work."}, {"title": "Potential Risks", "content": "The environmental impact associated with training and fine-tuning large models is not negligable as it involves substantial computational resources and energy consumption. While PEFT aims to alleviate this by reducing the number of tunable parameters, works like ours still require significant compute to distinguish which methods are more promising."}, {"title": "Details on Experimental Setup", "content": "We conducted hyperparemter search by selectively modifying number of virtual tokens among the values of 40, 50, 90 and 100, Adapter/LoRA dimensions 16, 32 or 64 and learning rates 1e-4 to le-6.\nIn our experiments, we used up to 16 compute nodes, each with 8 A100-80GB SXM GPUs. When model is smaller, we increased the data parallelism size, using tools in NeMo framework.\nThe template we used to present context to GPT models is as follows.\ntitle: {title}\nsource: {source}\ntitle: {title}\nsource: {source}\ntitle: {title}\nsource: {source}\ntitle: {title}\nsource: {source}\ntitle: {title}\nsource: {source}\nQuestion: {question} Answer: The answer is"}, {"title": "Dataset Statistics", "content": ""}, {"title": "Supplementary Figures and Tables", "content": ""}]}