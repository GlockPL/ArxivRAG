{"title": "GPT vs RETRO: Exploring the Intersection of Retrieval and\nParameter-Efficient Fine-Tuning", "authors": ["Aleksander Ficek", "Jiaqi Zeng", "Oleksii Kuchaiev"], "abstract": "Parameter-Efficient Fine-Tuning (PEFT) and\nRetrieval-Augmented Generation (RAG) have\nbecome popular methods for adapting large lan-\nguage models while minimizing compute re-\nquirements. In this paper, we apply PEFT meth-\nods (P-tuning, Adapters, and LoRA) to a modi-\nfied Retrieval-Enhanced Transformer (RETRO)\nand a baseline GPT model across several sizes,\nranging from 823 million to 48 billion parame-\nters. We show that RETRO models outperform\nGPT models in zero-shot settings due to their\nunique pre-training process but GPT models\nhave higher performance potential with PEFT.\nAdditionally, our study indicates that 8B param-\neter models strike an optimal balance between\ncost and performance and P-tuning lags behind\nother PEFT techniques. We further provide\na comparative analysis of between applying\nPEFT to an Instruction-tuned RETRO model\nand base RETRO model. This work presents\nthe first comprehensive comparison of various\nPEFT methods integrated with RAG, applied\nto both GPT and RETRO models, highlighting\ntheir relative performance.", "sections": [{"title": "1 Introduction", "content": "Pre-trained large language models have made\na demonstrable impact across applications in\nacademia and industry. Many use cases, however,\nrequire LLMs adapted to specific tasks and unique\ninformation but lack the resources for extensive re-\ntraining. To address this, Parameter-Efficient Fine-\nTuning (PEFT) (Han et al., 2024) and Retrieval-\nAugmented Generation (RAG) (Gao et al., 2023)\nhave become popular methods due to their effective-\nness and efficiency, inspiring new lines of research.\nPEFT has been proven to be a comparable substi-\ntute to Supervised Fine-Tuning (SFT) by achieving\ncompetitive performance at a fraction of the num-\nber of updated parameters (Han et al., 2024). In"}, {"title": "2 Related Work", "content": "In this section we focus on recent work that com-\nbine finetuning with retrieval. A comprehensive\nsurvey (Gao et al., 2023) synthetized multiple com-\nparative studies on PEFT and RAG, underscor-ing the potential benefits of combining these ap-\nproaches as a promising direction for future inves-\ntigation. There are multiple works that provide\nmethods to combine RAG with fine-tuning to im-\nprove accuracy (Zhang et al., 2024a,b; Rangan and\nYin, 2024). Multiple studies have explored the com-\nparison between fine-tuning and retrieval. Lakatos\net al. (2024) and Ovadia et al. (2023) reported im-\nproved accuracy using RAG over fine-tuning GPT\nmodels, while also noting suboptimal results when\ncombining the two methods. Gupta et al. (2024)\ndemonstrated improved outcomes by integrating\nboth approaches for specific agriculture and geog-\nraphy tasks. Additionally, Soudani et al. (2024)\ncompared the efficacy of these methods, including\nfull and QLORA fine-tuning (Dettmers et al., 2024),\nin low-frequency entity question-answering tasks.\nThese studies collectively suggest the need for\ncomprehensive investigation into multiple PEFT\ntechniques combined with RAG and maintain re-\ntrieval pretrained LLMs with PEFT to be unex-\nplored, thereby motivating our research."}, {"title": "3 Experimental Setup", "content": "To cover several task categories, we use six datasets\nsuited to benefit from retrieval and finetuning.\nWe select Natural Questions (NQ) (Kwiatkowski\net al., 2019), TriviaQA (TQA) (Joshi et al., 2017),\nNarrativeQA (NQA) (Ko\u010disk\u1ef3 et al., 2018) and\nQasper (Dasigi et al., 2021) for document ques-\ntion answering, QuALITY (Pang et al., 2021) for\nmultiple-choice question answering, and QMSum\n(Zhong et al., 2021) for query-based summariza-\ntion (see detail statistics in Appendix B). Each of\nthese datasets contain necessary external knowl-\nedge that must be filtered via retrieval and response\nbehaviour that encourages finetuning. Following\nthe official metrics, we use F1 score for evaluat-\ning document QA, exact match for mutliple-choice\nQA and the geometric mean of ROUGE-1/2/L (Lin,\n2004) for summarization."}, {"title": "3.2 Models", "content": "In order to undertand the effect of model scales,\nwe use base GPT models of sizes 823M (Ex-\ntra Small), 2.25B (Small), 8.5B (Medium), 22B\n(Large), and 43B (Extra Large), as introduced in\nWang et al. (2023a), which were pretrained on a\nmassive dataset of 1.2 trillion tokens. We employ\nthe corresponding RETRO models from the same\nwork as the foundation for our retrieval pretrained\nLLM experiments. Notably, the RETRO architec-\nture features an encoder that extracts neighbors\nfrom an external database, which increases the to-\ntal model size to 877M, 2.47B, 9.5B, 24B, and 48B,\nrespectively. Wang et al. (2023a) found ablating\nthe encoder after pretraining led to comparable re-\nsults. In our paper we include it so that adapter\nmodules and LoRA layers are added throughout"}, {"title": "3.3 Retrieval", "content": "We follow Wang et al. (2023a); Xu et al. (2023)\nto use Dragon+ (Lin et al., 2023) as a retriever.\nDragon+ is a dual encoder model that consists of\na query encoder and a context encoder. We first\nchunk each context document with 100 words, and\nthen encode both the questions and all chunks inde-\npendently with corresponding encoders. The most\nrelevant 5 chunks, ranked by the dot product of\nthe question embedding and chunk embedding, are\nretrieved as neighbors. For GPT models, they are\nconcatenated together (following the left to right or-\nder from the most relevant to least relevant) as the\ncontext of the prompt for generation. For RETRO\nmodels, they interact with the question during gen-\neration through chunked cross-attention."}, {"title": "3.4 Parameter Efficient Fine-Tuning", "content": "We implement P-tuning in RETRO akin to GPT.\nVirtual tokens are added to the beginning of the\ndecoder. Based on the design of chunked cross-\nattention (Wang et al., 2023a), left padding is\nadded to ensure the length of input (virtual tokens\n+ context + question) is a multiple of chunk size.\nAdapter and LoRA layers are inserted within the\ntransformer blocks in both RETRO and GPT at\nthe feedforward and attention layers. In RETRO\nthey are also inserted in the transformer encoder\ningesting the retrieved neighbors. We provide addi-\ntional hyperparameter tuning, resource utilization\nand prompt template details in Appendix A."}, {"title": "4 Results", "content": "Table 1 shows the comprehensive comparison be-\ntween GPT and RETRO models across five sizes\nand six datasets. From this table we observe:\n(1) RETRO is generally better than GPT at\nzero-shot settings. This superiority stems from\nits unique pre-training approach. By learning to\nextract salient information from retrieved text and\nintegrate it into its generation process, RETRO de-\nvelops the capability to harness relevant contextual\nknowledge, ultimately leading to its good zero-\nshot performance. In contrast, GPT relies on an\nauto-regressive loss during pre-training, focusing\non accurately predicting next tokens without the\nbenefit of external retrievals. As a result, GPT's\nability to learn context-aware question-answering\nis limited to the presence of relevant data within\nthe pre-training corpus, resulting in significantly\nless targeted training compared to RETRO.\n(2) Both RETRO and GPT models exhibit satu-\nration points on these datasets around the 8B mark,\nwith a similar pattern emerging between the two\nmodels, albeit with RETRO performing less well.\nThis can be seen in Figure 1 and suggests that, for a\nspecific task, a medium-sized PEFT model strikes\nthe optimal balance between cost and performance,\nmaking it a sweet spot for many applications.\n(3) For GPT models, P-tuning performs worse at\nsmaller models (<=8B), but it performs as good or\nbetter than LoRA and Adapter at the large model\nsizes. This difference is visualized in Figure 2 and\nFigure 3 (Appendix C). However, for RETRO mod-\nels, P-tuning generally under performs the other\nPEFT methods across all model sizes.\n(4) The performance ceiling for PEFT-tuned\nmodels is significantly higher for GPT compared\nto RETRO as seen in Figure 4 (Appendix C). For\nexample, using medium-sized models, the aver-\nage score of LoRA with GPT is 40.85, while with\nRETRO it is 34.24. This disparity suggests that\nGPT has more room for improvement with PEFT\ntuning. This phenomenon can also be possibly\nexplained by the two different pre-training strate-\ngies. Since GPT has been less targeted on retrieval-\naugmented generation during pre-training, it opens\na larger room for improvement during fine-tuning.\n(5) We also conduct full fine-tuning on medium-\nsized GPT and RETRO models. For GPT model,\nwe find that full fine-tuning achieves slightly better\nperformance than PEFT on average, with a notable\nexception on the QASPER dataset, where the dif-\nference is more pronounced and full fine-tuning\nyields significantly better results. In contrast, for\nRETRO model, full fine-tuning generally performs\nworse than PEFT, demonstrating the effectiveness\nof PEFT. These findings are consistent with the\nobservations of Hu et al. (2021)."}, {"title": "4.2 Comparing to Instruction-tuned RETRO", "content": "Instruction tuning post retrieval-augmented pre-\ntraining (Wang et al., 2023a) has been demon-\nstrated to improve zero-shot performance on\nRETRO models. A natural thought is that whether\nInstruction-tuned RETRO (I-RETRO) serve as a\nbetter foundation for applying PEFT compared\nto the base RETRO. To investigate this, we addi-\ntionally apply PEFT to a medium-sized I-RETRO\nmodel and show overall results in Table 2 and more\ngranular results in Table 4 (Appendix C). Our find-\nings reveal that while I-RETRO exhibits improved\nperformance in the zero-shot setting, it has limited\nscope for further improvement using PEFT. Even\nwith substantial hyperparameter tuning, the average\nscores across six datasets, using each of the three\nPEFT methods, demonstrate an approximately 10%\ngap between I-RETRO and base RETRO. We hy-\npothesize that conceptually both models should be\ntunable to similar performance but will leave that\nexploration to future work."}, {"title": "5 Conclusion", "content": "This study explores Parameter-Efficient Fine-\nTuning (PEFT) methods applied to Retrieval-\nAugmented Generation (RAG) models, comparing\nGPT and RETRO architectures. RETRO gener-\nally outperforms GPT in zero-shot settings due\nto their pre-training process that integrates exter-\nnal retrieval, enhancing contextual understanding.\nHowever, GPT models show a higher performance\npotential with PEFT, indicating more room for\nimprovement during fine-tuning. Both RETRO\nand GPT models perform optimally around the 8B\nparameter mark, balancing cost and performance.\nWhile P-tuning is effective in larger models, it lags\nbehind other methods in smaller models, particu-\nlarly for RETRO. Applying PEFT to Instruction-\ntuned RETRO yields limited improvement com-\npared to base RETRO, suggesting a saturation point\nin leveraging pre-training and fine-tuning bene-\nfits. Our comprehensive analysis offers valuable\ninsights for optimizing large language models with\nPEFT and RAG to the community."}, {"title": "Limitations", "content": "Due to the breadth of experiments covered in this\nwork we had to prioritze certain experiments over\nothers. This resulted in us using only the medium\nsized GPT and RETRO models for additional fine-\ntuning and Instruction tuning experiments. We\nbelieve these results generalize to the other model\nsizes but leave that to be validated in future work."}, {"title": "Potential Risks", "content": "The environmental impact associated with training\nand fine-tuning large models is not negligable as it\ninvolves substantial computational resources and\nenergy consumption. While PEFT aims to alleviate\nthis by reducing the number of tunable parameters,\nworks like ours still require significant compute to\ndistinguish which methods are more promising."}, {"title": "A Details on Experimental Setup", "content": ""}, {"title": "A.1 Hyperparameter Tuning", "content": "We conducted hyperparemter search by selectively\nmodifying number of virtual tokens among the val-\nues of 40, 50, 90 and 100, Adapter/LoRA dimen-\nsions 16, 32 or 64 and learning rates 1e-4 to le-6."}, {"title": "A.2 Resource Utilization", "content": "In our experiments, we used up to 16 compute\nnodes, each with 8 A100-80GB SXM GPUs. When\nmodel is smaller, we increased the data parallelism\nsize, using tools in NeMo framework."}, {"title": "A.3 Prompt Template", "content": "The template we used to present context to GPT\nmodels is as follows.\ntitle: {title}\nsource: {source}\ntitle: {title}\nsource: {source}\ntitle: {title}\nsource: {source}\ntitle: {title}\nsource: {source}\ntitle: {title}\nsource: {source}\nQuestion: {question} Answer: The answer is"}, {"title": "B Dataset Statistics", "content": ""}, {"title": "C Supplementary Figures and Tables", "content": ""}]}