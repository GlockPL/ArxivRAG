{"title": "Just a Hint: Point-Supervised Camouflaged Object Detection", "authors": ["Huafeng Chen", "Dian Shao", "Guangqian Guo", "Shan Gao"], "abstract": "Camouflaged Object Detection (COD) demands models to expeditiously and accurately distinguish objects which conceal themselves seamlessly in the environment. Owing to the subtle differences and ambiguous boundaries, COD is not only a remarkably challenging task for models but also for human annotators, requiring huge efforts to provide pixel-wise annotations. To alleviate the heavy annotation burden, we propose to fulfill this task with the help of only one point supervision. Specifically, by swiftly clicking on each object, we first adaptively expand the original point-based annotation to a reasonable hint area. Then, to avoid partial localization around discriminative parts, we propose an attention regulator to scatter model attention to the whole object through partially masking labeled regions. Moreover, to solve the unstable feature representation of camouflaged objects under only point-based annotation, we perform unsupervised contrastive learning based on differently augmented image pairs (e.g. changing color or doing translation). On three mainstream COD benchmarks, experimental results show that our model outperforms several weakly-supervised methods by a large margin across various metrics.", "sections": [{"title": "1 Introduction", "content": "Camouflaged Object Detection (COD) aims to distinguish the disguised objects hiding carefully in the environment [8-10, 29]. Recently, it has attracted ever-growing research interest from the computer vision community and facilitates valuable real-life applications, such as search and rescue [8], species discovery [14], and medical image analysis [11,12]. Specifically, we tentatively figure out two key-points for this task: 1) Quickly probing the existence of camouflaged objects, and 2) Accurately detecting the corresponding locations pixel-wise within the given image. Towards this end, accurate pixel-level annotations are required, based on which several fully-supervised approaches achieve good performance [9,25]. However, since spotting camouflaged objects is also difficult for humans, such an annotation process is not only laborious but also time-consuming, e.g., 60 minutes for each image [9].\nTo address this issue, weakly-supervised COD has been explored recently. In [19], the authors try to use scribbles to replace pixel-wise annotations and propose a novel CRNet to solve this problem. But such a setting falls into a dilemma: 1) efficiency, the quality of scribble could not be ensured, arbitrary patterns could harm the learning process, 2) effectiveness, scribbles are expected to indicate the outline of boundaries, which also increases the annotation difficulty, as shown in Fig. 1.\nIn this work, we propose to fulfill the COD task with only point-based supervision. Such an idea is motivated by the cognitive process of the human vision system: we tend to recognize the most \"discriminative part\" of one camouflaged object [25], and then scatter our attention to figure out the whole object mask. Even sometimes the camouflaged object is not perceived at first glance, we could quickly notice the target with just a little hint when a friend points to the image and says \"carefully scrutinize there!\". Importantly, the whole point-supervised annotation process only takes within 2 seconds.\nHowever, supervision provided by a single point is rather limited, making the model easily collapse during training. In a similar but different task, Salient Object Detection (SOD), widely adopted solutions include detecting the edge map [16] first, or generating the prediction map [37] as the proposal region at the first stage. Unfortunately, all the previously mentioned solutions for generating proposal regions in SOD are not applicable in point-supervised COD, owing to the low contrast and ambiguous edges between the camouflaged objects and backgrounds (shown in S.M.). To address it, we explore a new point-to-region supervision generation strategy, hint area generator. Specifically, starting from the single point annotation, we adaptively expand it to a certain region, which obviously enriches the limited supervision, as shown in Fig. 2.\nMoreover, weakly-supervised COD methods suffer from partial detection, i.e., only discriminative parts of the object could gain adequate attention, while the whole shape is not accurately perceived, especially with the supervision of a single point. To make the model not only focus on local distinct ones, but nondiscriminatory parts, we propose an attention regulator module. By masking"}, {"title": "3 Methodology", "content": "The overall architecture of the proposed framework is shown in Fig. 4, which includes a) a hint area generator to extend supervision from a single point to the small hint region, b) an attention regulator to guide the model to focus\nCompared with other forms of mask supervision, supervision provided by a point is rather limited, which easily leads to model collapse, as shown in Tab. 5. To explain a little, an annotated point just functions like a \"hint\", only indicating the existence of the object, with no knowledge of the area, shape, or boundary. To solve this, we design a hint area generator to adaptively expand a single point to a relatively larger region. One of the most crucial things is to control the degree of point expansion, avoiding the resultant hint area being too small (limited effect) or too large (out of boundary). Especially avoiding excessive expansion to introduce noise, place quality over quantity, because incorrect supervision is fatal for WSCOD, as shown in Tab. 5.\nSpecifically, as shown in Fig. 4(a), we initialize the desired annotation region with a small square area around the annotated point. It just prevents the model training from collapsing if only using one pixel as a point. The side length of the square area is d. Then, after training the encoder w epochs as a warm-\nwhere \\( \\tau \\) is a threshold, \\( \\mathbb{I}(\\cdot) \\) is an indicator function, and N is the number of objects. Note that N could be achieved by counting the annotated point and thus is a deterministic parameter.\nThen, to prevent the calculated circular from being out of the object bound-ary, we use a hyperparameter, a, to alter the radius R using: \\( R = \\alpha r \\). Specifically, the annotation for an object n is enlarged from a single point \\( (x_n, y_n) \\), to a small circle region around that point with the estimated radius R, denoted as \\( C_{(x_n, y_n)}^R \\). It is noteworthy that the background is also annotated by such a small circular region, unfolding around the point \\( (x_b, y_b) \\), which is randomly picked from the background region.\nAs a result, with the help of the hint area generator, the original point su-pervision for the image I is enriched into several small regions, expanding su-pervision while minimizing the introduction of incorrect supervision as much as possible, represented as:\n\\begin{equation}\\mathcal{S}(I) = C_{(x_b, y_b)}^R \\cup \\sum_{n=1}^N C_{(x_n, y_n)}^R,  \\quad n = 1,...,N\\end{equation}\nwhere \\( \\mathcal{S}(I) \\) is the obtained supervision, as shown in Fig. 4.\nWeakly-supervised methods often suffer from partial detection problems, owing to the fact that model tends to focus too much on those discriminative regions. Inspired by the HaS method [21], the random mask is able to inhibit the strong response, we further design an attention regulator module for PCOD. Because in camouflaged scenes, people often annotate point labels on the most visually dis-criminative regions for the camouflaged objects, we leverage this prior to guide mask generation to scatter the attention from the directly supervised discrimi-native part to the whole object area, as shown in Fig. 4(b). Firstly, based on the given supervision region, \\( \\mathcal{C}^R \\), we could obtain a mask M:\n\\begin{equation}M = \\begin{cases}M_i = h(Z), & i \\in \\mathcal{C}^R \\\\M_i = 1, & i \\notin \\mathcal{C}^R \\end{cases}\\end{equation}\nwhere Z is a logical matrix consisting of zeros and ones with the same shape as I, h() denotes a shuffle operation, i is the pixel index.\nThen, we apply this mask to the original image I: \\( I^* = I * M \\), where * indicates element-wise multiplication. Through such formulation, the annotated discriminative area will be randomly masked. During the training process, the model needs to explore the recognition of the surrounding foreground areas of the mask to restore recognition of the masked foreground area, helping the model scatter attention to other regions within the foreground objects.\nIn COD, the foreground and the background share high similarity. The subtle difference sometimes comes from the texture abruption or the color inconsistency. Based on such observations, we hope at least from one aspect the discrepancy could be exposed.\nToward this goal, we propose to use unsupervised contrastive learning strat-egy to optimize the feature space, aiming to make the foreground object distin-guished from the background through the learned invariant patterns and stable representations.\nSpecifically, as shown in Fig. 4(c), for image I, we adopt two different visual transformations T\u2081 and T2, selecting from colorjitter, Gaussianblur, flip, etc. These visual transformations are able to change the appearance of images. For example, Gaussian blur will smooth the texture, and changing color is relatively harmless to the texture patterns. This process results in two transformed images \\( I^1 \\) and \\( I^2 \\): \\( I^1 = T_1(I) \\), and \\( I^2 = T_2(I) \\).\nWe encode the transformed images differently into two prediction maps \\( P^{(1)} \\) and \\( P^{(2)} \\), denoting as \\( P^1 = g(f(I^1)) \\), and \\( P^2 = f(I^2) \\), where f represents the encoder to be learned, and g is a small network cascaded over f to increase en-coding diversity. When a position-related or size-related transformation T (e.g., scale, crop) is applied to the image \\( I^1 \\), this T should be applied to \\( P^2 \\) to be aligned with \\( P^1 \\), and vice versa. Our objective is to minimize the distance be-tween these two prediction maps:\n\\begin{equation}\\min \\mathcal{D}(P^1, P^2) = \\sum_i P_i^1 - P_i^2|,\\end{equation}\nwhere i is the pixel index. Here we follow the design of UCL, i.e., stopping the gradient (stopgrad) update at one end, so the contrastive loss function is defined as:\n\\begin{equation}\\mathcal{L}_c = \\mathcal{D}(P^1, stopgrad(P^2)).\\end{equation}\nThe prediction gap using different transformations will be narrowed by min-imizing the above loss, making the learned feature representation robust.\nThe structure of our feature encoder is simple compared to previ-"}, {"title": "4 Experiments", "content": "Our experiments are conducted on three COD benchmarks, CAMO [22], COD10K [9], and NC4K [25]. In order to evaluate our point-supervised COD method, we relabel 4040 images (3040 from COD10K, 1000 from CAMO) and propose the Point-supervised Dataset (P-COD) for training and the remaining is for testing. In Fig. 1, we show an example of point annota-tion and compare it with other annotation methods. For each image, we simulate the hunting process to label only one point for each camouflaged object, with-out needing to worry about ambiguous boundaries, making the process of the annotation easy and natural.\nWe adopt four evaluation metrics: mean absolute error (MAE), S-measure (Sm) [6], E-measure (Em) [7], weighted F-measure (F) [26].\nWe implement our method with PyTorch and con-duct experiments on one GeForce RTX4090 GPU. We use the stochastic gradient descent optimizer with a momentum of 0.9, a weight decay of 5e-4, and triangle learning rate schedule with maximum learning rate 1e-3. The batch size is 8, and the training epoch is 60. It takes around 7 hours to train. During training and inference, input images are resized to 512 \u00d7 512.\nBeing the first point supervised COD method, our proposed approach is primarily benchmarked against scribble-supervised and fully-supervised methods. As demonstrated in Tab. 1, our method achieves sub-stantial improvements, with an average enhancement of 17.6% for MAE, 7.2%\nQuantitative Comparison. for Sm, 4.7% for Em, and 11.7% for \\( F_\\uparrow \\) compared to the state-of-the-art weakly-supervised COD method CRNet [19]. This highlights our capability to achieve better performance with fewer annotations. Our approach even outperforms fully supervised methods in several metrics across multiple datasets. Specifically, it excels against the majority of fully-supervised methods on CAMO and outpaces nearly half of them on other datasets.\nOur method produces prediction maps characterized by clearer and more complete object regions, along with sharper contours, sig-nificantly outperforming state-of-the-art weakly supervised COD method CR-Net [19], as shown in Fig. 5. Our method performs well in various challenging scenarios, including scenarios with tiny objects (row 4), high intrinsic similarities (row 3, 5), indefinable boundaries (row 1), and complex backgrounds (row 2, 6).\nWith lower parameter complexity and minimal com-putational cost, our model (25.44M parameters and 10.22G MACs) consistently outperforms the fully supervised SINet (48.95M parameters and 19.42G MACs) by 4.3% for MAE, 2.1% for Sm, and 4.0% for Em on the challenging \\(CAMO\\) dataset. It also outperforms the scribble supervised CRNet (32.65M parameters\nQualitative Evaluation. and 14.27G MACs) by 7.4% for MAE, 4.2% for Sm, 4.0% for Em, and 6.8%"}, {"title": "4.3 Ablation Study", "content": "As CAMO dataset is the most challenging one, indicated by the lowest scores in Tab. 1, all following ablation experiments are performed on it.\nTo reduce annotation costs, we in-tuitively annotated a point for each camouflaged object by humans. We conduct experiments on point positions, as shown in Tab. 2. For \\(Random\\), we randomly choose a point from real labels, and repeat three times, taking the average of the results. which shows our method far exceeds the center annotation and ran-dom annotation under conditions of low annotation cost. We attribute it to the model's utilization of the priors implicit in point annotations to simulate human cognitive processes. In addition, we conduct experiments for point num-bers (\\(Random\\)), as shown in Tab. 3. One point yields satisfactory performance.\nAnnotations, we proposed"}]}