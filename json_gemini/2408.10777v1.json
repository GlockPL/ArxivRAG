{"title": "Just a Hint: Point-Supervised Camouflaged Object Detection", "authors": ["Huafeng Chen", "Dian Shao", "Guangqian Guo", "Shan Gao"], "abstract": "Camouflaged Object Detection (COD) demands models to expeditiously and accurately distinguish objects which conceal themselves seamlessly in the environment. Owing to the subtle differences and ambiguous boundaries, COD is not only a remarkably challenging task for models but also for human annotators, requiring huge efforts to provide pixel-wise annotations. To alleviate the heavy annotation burden, we propose to fulfill this task with the help of only one point supervision. Specifically, by swiftly clicking on each object, we first adaptively expand the original point-based annotation to a reasonable hint area. Then, to avoid partial localization around discriminative parts, we propose an attention regulator to scatter model attention to the whole object through partially masking labeled regions. Moreover, to solve the unstable feature representation of camouflaged objects under only point-based annotation, we perform unsupervised contrastive learning based on differently augmented image pairs (e.g. changing color or doing translation). On three mainstream COD benchmarks, experimental results show that our model outperforms several weakly-supervised methods by a large margin across various metrics.", "sections": [{"title": "1 Introduction", "content": "Camouflaged Object Detection (COD) aims to distinguish the disguised objects hiding carefully in the environment [8-10, 29]. Recently, it has attracted ever-growing research interest from the computer vision community and facilitates valuable real-life applications, such as search and rescue [8], species discovery [14], and medical image analysis [11,12]. Specifically, we tentatively figure out two key-points for this task: 1) Quickly probing the existence of camouflaged objects, and 2) Accurately detecting the corresponding locations pixel-wise within the given image. Towards this end, accurate pixel-level annotations are required, based on which several fully-supervised approaches achieve good performance [9,25]. However, since spotting camouflaged objects is also difficult for humans, such an annotation process is not only laborious but also time-consuming, e.g., 60 minutes for each image [9].\nTo address this issue, weakly-supervised COD has been explored recently. In [19], the authors try to use scribbles to replace pixel-wise annotations and propose a novel CRNet to solve this problem. But such a setting falls into a dilemma: 1) efficiency, the quality of scribble could not be ensured, arbitrary patterns could harm the learning process, 2) effectiveness, scribbles are expected to indicate the outline of boundaries, which also increases the annotation difficulty, as shown in Fig. 1.\nIn this work, we propose to fulfill the COD task with only point-based supervision. Such an idea is motivated by the cognitive process of the human vision system: we tend to recognize the most \"discriminative part\" of one camouflaged object [25], and then scatter our attention to figure out the whole object mask. Even sometimes the camouflaged object is not perceived at first glance, we could quickly notice the target with just a little hint when a friend points to the image and says \"carefully scrutinize there!\". Importantly, the whole point-supervised annotation process only takes within 2 seconds.\nHowever, supervision provided by a single point is rather limited, making the model easily collapse during training. In a similar but different task, Salient Object Detection (SOD), widely adopted solutions include detecting the edge map [16] first, or generating the prediction map [37] as the proposal region at the first stage. Unfortunately, all the previously mentioned solutions for generating proposal regions in SOD are not applicable in point-supervised COD, owing to the low contrast and ambiguous edges between the camouflaged objects and backgrounds (shown in S.M.). To address it, we explore a new point-to-region supervision generation strategy, hint area generator. Specifically, starting from the single point annotation, we adaptively expand it to a certain region, which obviously enriches the limited supervision, as shown in Fig. 2.\nMoreover, weakly-supervised COD methods suffer from partial detection, i.e., only discriminative parts of the object could gain adequate attention, while the whole shape is not accurately perceived, especially with the supervision of a single point. To make the model not only focus on local distinct ones, but nondiscriminatory parts, we propose an attention regulator module. By masking"}, {"title": "2 Related Work", "content": "Weakly Supervised Camouflaged Detection. Pixel-level annotation in COD is very time-consuming, in order to reduce the cost of labeling, recent work has begun to explore weakly supervised camouflaged object detection. CRNet [19] firstly tries to use scribble labels to train the model. However, scribble labels have the problems of label diversity and poor control of label quality. We try to use simpler and more natural point labels.\nPoint Annotation. There have been several works on point annotations in weakly-supervised segmentation [1,4,32,34] and instance segmentation [2,24]. Semantic segmentation focuses on class-specific categories, which is different from COD task. PSOD [16] proposes a novel weakly-supervised Salient Object Detection (SOD) method using point supervision. But the drawbacks of it are also obvious: 1) extra edge detectors and edge information 2) flood fill algorithm introducing additional computation, and not appropriate for COD since camouflaged objects are not salient.\nContrastive Learning. The core of contrastive learning is to attract the positive sample pairs and repulse the negative sample pairs. Moco [18] maintained a queue of negative samples and turned one branch of siamese network into a momentum encoder to improve the consistency of the queue. BYOL [17] proposed a self-supervised image representation learning approach, achieving state-of-the-art results without negative samples. The mainstream paradigm of contrastive learning later shifted to only using positive samples. SimSiam [3] built an efficient and simple contrastive learning framework only using positive samples, which is free from the limitations of large batch size, momentum encoder, and negative samples. The above work [3,17,18] has proved that UCL can completely approach or even exceed the effect of supervised manner, and learn representations with strong generalization. Therefore, we try to introduce unsupervised contrastive learning to point-supervised COD to strengthen the representation of the model."}, {"title": "3 Methodology", "content": "The overall architecture of the proposed framework is shown in Fig. 4, which includes a) a hint area generator to extend supervision from a single point to the small hint region, b) an attention regulator to guide the model to focus"}, {"title": "3.1 Hint Area Generator", "content": "Compared with other forms of mask supervision, supervision provided by a point is rather limited, which easily leads to model collapse, as shown in Tab. 5. To explain a little, an annotated point just functions like a \"hint\", only indicating the existence of the object, with no knowledge of the area, shape, or boundary. To solve this, we design a hint area generator to adaptively expand a single point to a relatively larger region. One of the most crucial things is to control the degree of point expansion, avoiding the resultant hint area being too small (limited effect) or too large (out of boundary). Especially avoiding excessive expansion to introduce noise, place quality over quantity, because incorrect supervision is fatal for WSCOD, as shown in Tab. 5.\nSpecifically, as shown in Fig. 4(a), we initialize the desired annotation region with a small square area around the annotated point. It just prevents the model training from collapsing if only using one pixel as a point. The side length of the square area is d. Then, after training the encoder w epochs as a warm-"}, {"title": "3.2 Attention Regulator", "content": "Weakly-supervised methods often suffer from partial detection problems, owing to the fact that model tends to focus too much on those discriminative regions. Inspired by the HaS method [21], the random mask is able to inhibit the strong response, we further design an attention regulator module for PCOD. Because in camouflaged scenes, people often annotate point labels on the most visually discriminative regions for the camouflaged objects, we leverage this prior to guide mask generation to scatter the attention from the directly supervised discriminative part to the whole object area, as shown in Fig. 4(b). Firstly, based on the given supervision region, $C^R$, we could obtain a mask M:\n$M =\\begin{cases}M_i = h(Z), & i \\in C^R\\\\ M_i = 1, & i \\notin C^R\\end{cases}$\n(3)"}, {"title": "3.3 Representation Optimizer", "content": "In COD, the foreground and the background share high similarity. The subtle difference sometimes comes from the texture abruption or the color inconsistency. Based on such observations, we hope at least from one aspect the discrepancy could be exposed.\nToward this goal, we propose to use unsupervised contrastive learning strategy to optimize the feature space, aiming to make the foreground object distinguished from the background through the learned invariant patterns and stable representations.\nSpecifically, as shown in Fig. 4(c), for image I, we adopt two different visual transformations $T_1$ and $T_2$, selecting from colorjitter, Gaussianblur, flip, etc. These visual transformations are able to change the appearance of images. For example, Gaussian blur will smooth the texture, and changing color is relatively harmless to the texture patterns. This process results in two transformed images $I^1$ and $I^2$: $I^1 = T_1(I)$, and $I^2 = T_2(I)$.\nWe encode the transformed images differently into two prediction maps $P^{(1)}$ and $P^{(2)}$, denoting as $P^1 = g(f(I^1))$, and $P^2 = f(I^2)$, where f represents the encoder to be learned, and g is a small network cascaded over f to increase encoding diversity. When a position-related or size-related transformation T (e.g., scale, crop) is applied to the image $I^1$, this T should be applied to $P^2$ to be aligned with $P^1$, and vice versa. Our objective is to minimize the distance between these two prediction maps:\n$\\min D(P^1, P^2) = \\sum_i P_i^1 - P_i^2,\\quad$(4)\nwhere i is the pixel index. Here we follow the design of UCL, i.e., stopping the gradient (stopgrad) update at one end, so the contrastive loss function is defined as:\n$L_c = D(P^1, stopgrad(P^2)).\\quad$(5)\nThe prediction gap using different transformations will be narrowed by minimizing the above loss, making the learned feature representation robust."}, {"title": "3.4 Network", "content": "Encoder. The structure of our feature encoder is simple compared to previous work. Specifically, in order to capture long-distance feature dependency and"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Point Dataset. Our experiments are conducted on three COD benchmarks, CAMO [22], COD10K [9], and NC4K [25]. In order to evaluate our point-supervised COD method, we relabel 4040 images (3040 from COD10K, 1000 from CAMO) and propose the Point-supervised Dataset (P-COD) for training and the remaining is for testing. In Fig. 1, we show an example of point annotation and compare it with other annotation methods. For each image, we simulate the hunting process to label only one point for each camouflaged object, without needing to worry about ambiguous boundaries, making the process of the annotation easy and natural.\nEvaluation Metrics. We adopt four evaluation metrics: mean absolute error (MAE), S-measure ($S_m$) [6], E-measure ($E_m$) [7], weighted F-measure ($F_\\omega$) [26].\nImplementation Details. We implement our method with PyTorch and conduct experiments on one GeForce RTX4090 GPU. We use the stochastic gradient descent optimizer with a momentum of 0.9, a weight decay of 5e-4, and triangle learning rate schedule with maximum learning rate 1e-3. The batch size is 8, and the training epoch is 60. It takes around 7 hours to train. During training and inference, input images are resized to 512 \u00d7 512."}, {"title": "4.2 Compare with State-of-the-arts", "content": "Quantitative Comparison. Being the first point supervised COD method, our proposed approach is primarily benchmarked against scribble-supervised and fully-supervised methods. As demonstrated in Tab. 1, our method achieves substantial improvements, with an average enhancement of 17.6% for MAE, 7.2% for $S_m$, 4.7% for $E_m$, and 11.7% for $F_\\omega$ compared to the state-of-the-art weakly-supervised COD method CRNet [19]. This highlights our capability to achieve better performance with fewer annotations. Our approach even outperforms fully supervised methods in several metrics across multiple datasets. Specifically, it excels against the majority of fully-supervised methods on CAMO and outpaces nearly half of them on other datasets.\nQualitative Evaluation. Our method produces prediction maps characterized by clearer and more complete object regions, along with sharper contours, significantly outperforming state-of-the-art weakly supervised COD method CR-Net [19], as shown in Fig. 5. Our method performs well in various challenging scenarios, including scenarios with tiny objects (row 4), high intrinsic similarities (row 3, 5), indefinable boundaries (row 1), and complex backgrounds (row 2, 6).\nParameter Complexity. With lower parameter complexity and minimal computational cost, our model (25.44M parameters and 10.22G MACs) consistently outperforms the fully supervised SINet (48.95M parameters and 19.42G MACs) by 4.3% for MAE, 2.1% for $S_m$, and 4.0% for $E_m$ on the challenging CAMO dataset. It also outperforms the scribble supervised CRNet (32.65M parameters and 14.27G MACs) by 7.4% for MAE, 4.2% for $S_m$, 4.0% for $E_m$, and 6.8%"}, {"title": "4.3 Ablation Study", "content": "As CAMO dataset is the most challenging one, indicated by the lowest scores in Tab. 1, all following ablation experiments are performed on it.\nAblation Study on Point Supervision. To reduce annotation costs, we intuitively annotated a point for each camouflaged object by humans. We conduct experiments on point positions, as shown in Tab. 2. For Random, we randomly choose a point from real labels, and repeat three times, taking the average of the results. which shows our method far exceeds the center annotation and random annotation under conditions of low annotation cost. We attribute it to the model's utilization of the priors implicit in point annotations to simulate human cognitive processes. In addition, we conduct experiments for point numbers (Random), as shown in Tab. 3. One point yields satisfactory performance. Increasing the points shows diminishing returns.\nEffectiveness of Hint Area Generator. As shown in Tab. 5, our proposed hint area generator obtains a significant improvement over the direct use of point"}, {"title": "Effectiveness of Attention Regulator", "content": "We propose attention regulator and apply it with UCL. As shown in Tab. 7, the results (Bold) are significantly improved after using attention regulator. In addition, to further demonstrate the superiority of regulator in weakly-supervised COD, we compare it with the previous mask approaches HaS [21] and Cutout [5], results are shown in Tab. 8.\nEffectiveness of Representation Optimizer. We conduct ablation experiments for representation optimizer. Firstly, we test different types of contrastive loss and find that L1 loss performs best, with a significant improvement over the baseline, as shown in Tab. 6. In addition, we conduct exhaustive experiments for data augmentation, which is important for UCL in representation optimizer. Specifically, the augmentation is divided into three aspects, color-texture related (ColorJitter, GuassBlur), position related (Flip, Translate), and size related (Crop, Scale). It shows that color-texture and size augmentation can improve performance, but position augmentation yields unsatisfactory results, as shown in Tab. 7. We employ UCL exclusively with positive samples and test the key components in ablation study, as shown in Tab. 9. Fig. 6 intuitively illustrates that the representation optimizer enhances the precision of prediction maps. Fig. 7 and 8 show that our model is able to continuously optimize representation through representation optimizer and separate entangled objects from background, making the model eventually learn robust representations."}, {"title": "Impact of the Hyperparameter $\\tau$, $\\alpha$, w, d", "content": "Four hyperparameters exist in hint area generator: $\\tau$, $\\alpha$, w, d. We test diverse values for these parameters, as shown in Tab. 11. So we set $\\tau$ = 150, $\\alpha$ = 4, w = 15 and d = 10. It is worth noting that the values of $\\tau$, w, and d do not have much impact on the results with different settings. In addition, the $\\tau$ can be replaced with unsupervised k-means clustering method (in S.M.). Our model is robust to parameters. It is because the hint area generator is designed to approximate the size of the camouflaged object through the prediction map. Whether the contour is fine and details are perfect, in the prediction map, has little influence on the estimation of the object size. The accurate estimation and localization of the camouflaged object are explored by the attention regulator and the representation optimizer modules."}, {"title": "4.4 Transferability Studies", "content": "Extension to Scribble. We apply our method to the scribble dataset, and the results are shown in Tab. 10 (More results in supplementary materials) reveal that our model, utilizing scribble labeling, achieves competitive results compared to the state-of-the-art fully supervised method ZoomNet [29]. It showcases the generalization ability of our method.\nExtension to SOD task. Our method excels not only in COD but also demonstrates competitive performance in SOD, as shown in Tab. 12. Considering the difference between these two tasks, we may wonder why our method consistently performs well on such two seemingly different tasks. We attribute this to the generality and rationality of the designed structure. In fact, point-supervised SOD and COD have a clear commonality, i.e., the demands of sufficient and accurate"}, {"title": "5 Conclusion", "content": "In this paper, we proposed an effective solution for COD by utilizing point-based supervision. The introduction of the P-COD dataset, based on human cognitive processes, streamlines the annotation process. To overcome the challenge of single-point supervision, we designed a hint area generator bolstering supervision. Representation optimizer optimized model representation by UCL. The attention regulator promoted a holistic object perception, ensuring a comprehensive understanding of camouflaged objects without fixating on distinct parts. Experimental results show that our method outperforms existing weakly-supervised methods largely and even surpasses many fully-supervised ones."}]}