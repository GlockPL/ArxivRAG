{"title": "EXPLORING APPLICATIONS OF STATE SPACE MODELS\nAND ADVANCED TRAINING TECHNIQUES IN SEQUEN-\nTIAL RECOMMENDATIONS: A COMPARATIve Study\nON EFFICIENCY AND PERFORMANCE", "authors": ["Baderko Makar", "Kulibaba Stepan", "Obozov Mark", "Nikolay Kutuzov", "Alexander Gasnikov"], "abstract": "Recommender systems aim to estimate the dynamically changing user preferences\nand sequential dependencies between historical user behaviour and metadata. Al-\nthough transformer-based models have proven to be effective in sequential recom-\nmendations, their state growth is proportional to the length of the sequence that is\nbeing processed, which makes them expensive in terms of memory and inference\ncosts. Our research focused on three promising directions in sequential recom-\nmendations: enhancing speed through the use of State Space Models (SSM), as\nthey can achieve SOTA results in the sequential recommendations domain with\nlower latency, memory, and inference costs, as proposed by Liu et al. (2024); im-\nproving the quality of recommendations with Large Language Models (LLMs) via\nMonolithic Preference Optimization without Reference Model (ORPO); and im-\nplementing adaptive batch- and step-size algorithms to reduce costs and accelerate\ntraining processes.", "sections": [{"title": "INTRODUCTION", "content": ""}, {"title": "RELATED STUDY", "content": ""}, {"title": "SEQUENTIAL RECOMMENDATION TASK DEFINITION", "content": "Consider the user set $U = {U_1, U_2, ..., U_N}$, item set $V = {U_1, U_2, ..., \u03c5_K}$ and $S_u = {U_1, U_2, ..., U_{n_u}}$ as the chronologically ordered interaction sequence for user $u \u2208 U$, where $n_u$ is the the length of the sequence. Given $S_u$ the task is to predict the next interacted item, $U_{n_u+1}$."}, {"title": "TRANSFORMERS", "content": "Recently, transformer models have been shown to be effective in sequential recommendation tasks\nas the backbone of larger models Kang & McAuley (2018) and as individual LLMs Li et al. (2023),\nYue et al. (2023). Despite their success, attention-based methods face inference inefficiencies due to\nthe quadratic computational complexity inherent in attention operators and their rapid state growth,\nwhich is proportional to the sequence length. Also, without special mechanisms Liu et al. (2023),\ntransformers can't handle long contexts and, consequently, long user histories."}, {"title": "STATE SPACE MODELS", "content": "State Space Model (SSM) is a recent framework for sequence modelling defined by linear ordinary\ndifferential equations:\n$h'(t) = Ah(t) + Bx(t)$ $y(t) = Ch(t)$\nWhere A, B, C are learnable matrices, h(t) is the latent space, x(t) is the input sequence and y(t)\nis the output sequence. To compute sequence-to-sequence transformations efficiently, the matrix A\nmust be structured, so structured SSMs have been introduced Gu et al. (2022). A general form of a\nstructured SSM is defined by the equations:\n$h_t= Ah_{t-1}+ Bx_t$ $Y_t = Ch_t$\nWhere $A \u2208 R^{(N,N)}$, $B \u2208 R^{(N,1)}$, $C \u2208 R^{(N,1)}$. They map a 1-dimensional sequence $x \u2208 R^T \u2192 y \u2208\nRT$ through an implicit latent state $h \u2208 R^{(T,N)}$. To operate directly on sequences, the discretisation\nrule $(f_A, f_B)$ is applied to continuous parameters $(\u2206, \u00c5, B)$, by $A = f_A(\u2206, \u00c5)$ and $B = f_B(A, B)$,\nwhere $A$ is the parameterised step size."}, {"title": "MAMBA BLOCK", "content": "In order to adaptively focus on relevant information while filtering out noise, the Mamba block Gu\n& Dao (2024) introduces an extension to structured SSMs by adding a data-dependent selection\nmechanism. An important feature of Selective SSMs is their ability to be computed efficiently on\nthe GPU using kernel fusion, parallel scanning and recomputation mechanisms.\nIn contrast to transformers' $O(n^2 \u00b7 d)$, SSMs provide $O(n \u00b7 d^2)$ complexity, which makes them a\nmore efficient alternative, especially when operating on long sequences (Dao & Gu (2024)) ."}, {"title": "MAMBA APPLICATIONS TO SEQUENTIAL RECOMMENDATIONS", "content": "Several applications of Mambas selective SSMs to recommender systems have been introduced. Liu\net al. (2024), Wang et al. (2024). As Mamba has already shown efficient results in different areas\nZhu et al. (2024) Erol et al. (2024) Chen et al. (2024), we propose a more complex exploration of\nMamba applications to recommender systems."}, {"title": "UNIVERSAL GRADIENT METHOD FOR STOCHASTIC OPTIMIZATION", "content": "The release of the Universal Stochastic Gradient Method (USGM) by Rodomanov et al. (2024)\nrepresents a noteworthy development in the field of stochastic optimization, that is the backbone of\nmachine learning.\nThe classical Stochastic Gradient Descent (SGD) by Robbins (1951) is one of the earliest techniques\nin the field of stochastic optimization. Its novelty at the time of publication helped it gain popularity.\nHowever, because hyperparameter values are only defined prior to the start of the training phase,\nSGD achieves poor convergence rates in real-world applications.\nA number of SGD-based adaptive strategies, including Adam by Kingma & Ba (2017) and Adagrad\nby Duchi et al. (2011), have been put forth to address the issue. Although they are more efficient\nthan SGD, because they modify the step size according to the aggregate of the previous gradients,\nwhen applied to non-convex optimization problems, they might still experience difficulty achieving\nconvergence.\nThe USGM develops the idea of adaptiveness further, as the algorithm can adapt to the curvature\nof the loss function, thus providing more accurate gradient estimates even in highly noisy envi-\nronments. Classic optimization algorithms are typically designed to handle either smooth or non-\nsmooth problems. USGM, however, is an algorithm capable of addressing both classes of problems"}, {"title": null, "content": "and adapting to oracle noise with unknown variance. The algorithm can adjust to H\u00f6lder-Lipschitz\ncoefficients without requiring precise calculation of those. The only hyperparameters used by the\nalgorithm are the upper bound on the diameter of the search hypersphere, the initial learning rate\nand batch size, what makes it a more practical choice for real-world applications.\nNotation\nThe same mathematical notation as in the original USGM paper will be used.\nThe loss function is defined as f and the parameters of our model are defined as x.\nTo characterize the smoothness of f the H\u00f6lder constant is introduced for each $v\u2208 [0,1]$:\n$L_v := sup_{x,y\u2208Dom f,x\u2260y, g(x)\u2208\u2202f(x),g(y)\u2208\u2202f(y)} \\frac{||g(x) - g(y)||* } {||x - y||^v}$ (1.1)\nwhere g(x) and g(y) are the stochastic approximations of subgradients of our loss function f at\npoints x and y respectively.\nThe dual norm is defined in the standard way:\n$||s||_* := max_{||x||=1} (s, x) = (s, B^{-1}s)^{1/2}, s\u2208R^n.$ (1.2)\nOf course, for certain values of the exponent $v \u2208 [0, 1]$, it may happen that $L_\u2081 = +\u221e$. However,\nwe assume that there exists (at least one) exponent for which the corresponding H\u00f6lder constant is\nfinite.\nFor any t \u2208 R, by $[t]_+ := max{t, 0}$, its positive part is denoted. For random variables X and \u00a7,\nby E[X] and E[X], the expectation of X w.r.t. \u00a7, and the full expectation of X, respectively are\ndenoted."}, {"title": "Algorithm 1 Universal Stochastic Gradient Method", "content": "1: Initialize: $x_0 \u2208 dom f$, $D > 0$, $H_0 := 0$, $g_0 \u223c \u011d(x_0)$.\n2: for k = 0,1,... do\n3: $x_{k+1} = arg min_{z\u2208dom f} {\\langle g_k, x\\rangle + \\frac{H_k}{2} ||x - x_k||^2}$.\n4: $g_{k+1} \u223c \u011d(x_{k+1})$.\n5: $H_{k+1} := H_k + [\\frac{\u03b2_{k+1}-H_{k+1}^2}{D^2}+2\u03c3^2}+\\]_+$\n6: where $r_{k+1} = ||x_{k+1} - x_k||$, $\u03b2_{k+1} = \\langle g_{k+1} - g_k, x_{k+1} - x_k\\rangle$.\n7: end for\nThe authors of the paper also define the upper bound for the stochastic approximation of the sym-\nmetrized Bregman distance for points $x_k$ and $x_{k+1}$ ($\u03b2_{k+1}$) as follows:\n$\u03b2_{k+1} = (f'(x_{k+1}) \u2212 f'(x_k) + \u2206_{k+1}, x_{k+1} \u2212 x_k) \u2264 L_vr_{k+1}^{v} + \u03c3_{k+1}r_{k+1},$ (1.3)\nwhere $f'(x_k) := E_{\u03be_k}[g_k] \u2208 \u2202f(x_k)$, $\u2206_{k+1} := \u03b4_{k+1} \u2014 \u03b4_k$ with $\u03b4_k := g_k - f'(x_k)$ being the error of\nthe stochastic gradient (such that $E||\u03b4_k||^2 < \u03c3^2$), and $\u03c3_{k+1} := ||\u2206_{k+1}||$."}, {"title": "ODDS RATIO PREFERENCE OPTIMIZATION", "content": "Odds Ratio Preference Optimization (ORPO), Hong et al. (2024) incorporates an odds ratio-based\npenalty to the conventional negative log-likelihood (NLL) loss for differentiating the generation\nstyles between favored and disfavored responses.\nF an input sequence x, the average log-likelihood of generating the output sequence y, of length m\ntokens, is computed as in the Equation 1. The odds of generating the output sequence y given an"}, {"title": null, "content": "input sequence x are defined in the Equation 2:\n$log P_\u03b8(y|x) = \\frac{1}{m} \u2211_{t=1}^{m}log P_\u03b8 (y_t|x, y_{<t})$ (1)\n$odds_\u03b8(y|x) = \\frac{P_\u03b8(y|x)}{1 - P_\u03b8(y|x)}$ (2)\nIntuitively, $odds_\u03b8(y|x) = k$ implies that it is k times more likely for the model \u03b8 to generate the\noutput sequence y than not generating it. Thus, the odds ratio of the chosen response $Y_w$ over the\nrejected response $Y_\u03b9$, $OR_\u0398(Y_w, Y_\u03b9)$, indicates how much more likely it is for the model \u03b8 to generate\n$Y_w$ than $Y_\u03b9$ given input x, defined in Equation 3.\n$ORO_\u0398(Y_w, Y_\u03b9) = \\frac{odds_\u03b8(Y_w|x)}{odds_\u03b8(Y_\u03b9|x)}$ (3)"}, {"title": "OBJECTIVE FUNCTION OF ORPO", "content": "The objective function of ORPO in Equation 4 consists of two components: 1) supervised fine-tuning\n(SFT) loss ($L_{SFT}$); 2) relative ratio loss ($L_{OR}$).\n$L_{ORPO} = E_{(x,y_w,Y_\u03b9)}[L_{SFT} + \u03bb \u00b7 L_{OR}]$ (4)\n$L_{SFT}$ follows the conventional causal language modeling negative log-likelihood (NLL) loss func-\ntion to maximize the likelihood of generating the reference tokens. $L_{OR}$ in Equation 5 maximizes\nthe odds ratio between the likelihood of generating the disfavored response $Y_{yw}$ and the disfavored\nresponse $Y_\u03b9$. The log odds ratio are wrapped with the log sigmoid function so that $L_{OR}$ could be\nminimized by increasing the log odds ratio between $y_w$ and $y_\u03b9$.\n$L_{OR} = - logo(log( \\frac{odds_\u03b8 (Y_w|x)}{odds_\u03b8 (y_\u03b9|x)} ))$ (5)\nTogether, $L_{SFT}$ and $L_{OR}$ weighted with \u03bb tailor the pre-trained language model to adapt to the\nspecific subset of the desired domain and disfavor generations in the rejected response sets."}, {"title": "GRADIENT OF ORPO", "content": "The gradient of $L_{Ratio}$ further justifies using the odds ratio loss. It comprises two terms: one\nthat penalizes the wrong predictions and one that contrasts between chosen and rejected responses,\ndenoted in Equation 6for $d = (x, y_\u03b9, Y_w) ~ D$.\n$\u2207L_{OLOR} = \u03b4(d) \u00b7 h(d)$ (6)\n$\u03b4(d) = 1 + [ \\frac{odds_{\u0398^P}(Y_w|x)}{odds_{\u0398^P}(y_\u03b9|x)} ]^{-1}$ (7)\n$h(d) = \\frac{\u2207_\u03b8log P_\u03b8(Y_w|x)}{1 - P_\u03b8(Y_w|x)} - \\frac{\u2207_\u03b8log P_\u03b8(y_\u03b9|x)}{1 - P_\u03b8(y_\u03b9|x)}$ (8)\nWhen the odds of the favored responses are relatively higher than the disfavored responses, \u03b4(d)\nin Equation 7 will converge to 0. This indicates that the \u03b4(d) will play the role of a penalty term,\naccelerating the parameter updates if the model is more likely to generate the rejected responses.\nMeanwhile, h(d) in Equation 8 implies a weighted contrast of the two gradients from the chosen\nand rejected responses. Specifically, $1 \u2013 P(y|x)$ in the denominators amplifies the gradients when\nthe corresponding side of the likelihood $P(y|x)$ is low. For the chosen responses, this accelerates\nthe model's adaptation toward the distribution of chosen responses as the likelihood increases."}, {"title": "OUR RESULTS", "content": ""}, {"title": "2MAMBA4REC", "content": "To compare the Mamba2 model more fairly with its predecessor (Mambal) we used the model\narchitecture from the Mamba4Rec article by Liu et al. (2024), replacing the Mamba block with the\nMamba2 block introduced by Dao & Gu (2024)."}, {"title": "MAMREC", "content": "We implemented the standard GPT4Rec architecture from Li et al. (2023) while replacing the GPT-\n2 model Radford et al. (2019) with a Mamba2 Dao & Gu (2024) model in order to achieve better\nperformance."}, {"title": "HYDRA LAYER", "content": "To apply the sequence-to-sequence potential of the Hydra block, we provide a custom Hydra layer\nthat combines the Hydra block with a standard feed-forward network. The main part of our standard\narchitecture consists of Hydra layers. We have found that Hydra layers work more effectively than\nMamba layers. Then we use the same PFFN that was introduced in Mamba4Rec Liu et al. (2024)\nPFFN(H) = GELU(HW(1) + h(1))W(2) + (2)\nWhere $W(1) \u2208 R^{D\u00d74D}$, $W(2) \u2208 R^{D\u00d74D}$, $b(1) \u2208 R^D$ are parameters of two dense layers and we use\nGELU activation."}, {"title": "PREDICTION LAYER", "content": "Prediction layer is adopted from SASRec and Mamba4Rec, last item embedding is used to generate\nthe final prediction scores:\n$\u0177 = Softmax(hE^T) \u2208 R^{|V|}$\nwhere $h \u2208 R^D$ is the last item embedding from the Hydra layer and $\u0177 \u2208 R^{|V|}$ represents the\nprobability distribution over the next item in the item set V."}, {"title": "BENCHMARKS", "content": "We tested our models on 3 benchmarks: Amazon Reviews '23 Beauty and Personal care, Amazon\nReviews '23 Video Games and MovieLens-1M.\nThe number listed in parentheses after the number of parameters of each models is the number of\nparameters after deducting the size of all embedding layers if present."}, {"title": "LATENCY", "content": "For the LLMs, the measure tokens-per-second was selected, whilst the average time to generate\nsuggestions for a single user was chosen to compare the others.\nSince all of the latency values that we are dealing with are relatively small, it is important to ascer-\ntain the measurement's margin of error in order to comprehend the experimental results that were\nproduced.\nConsequently, the bootstrap sampling method was applied. 30 samples comprising 1500 users ran-\ndomly chosen from the original dataset were used to individually measure the latency values. Fol-\nlowing the removal of outliers, the distribution of the latencies was examined, and the confidence\nintervals were computed.\nThe latency measurements were performed on one Nvidia A100-80GB graphics card."}, {"title": "INTERPRETATION OF RESULTS", "content": "It is evident that Hydra shows metrics that are similar to those of Mamba, but its latency is four to\nfive times lower. Simultaneously, models such as LlamaRec exhibit superior performance; however,\ntheir substantial parameter count may constrain their applicability in real-world situations. Metrics-\nwise and latency-wise, SSM-based models outperform the baseline SASRec model on average.\nAccording to the study, SSMs can outperform LLMs while still operating at a greater speed, since\nthey require less number of parameters."}, {"title": "APPLICATION OF ORPO TO RECOMMENDER SYSTEMS", "content": "To improve quality and incorporate human preference into the LLM, we used the ORPO technique\ndescribed above."}, {"title": "BASELINE", "content": "As a baseline, we considered the standard LlamaRec pipeline with Llama 7B LLM. Both retrieval\nand LLM were fine-tuned as described in the LlamaRec paper prior to the ORPO procedure."}, {"title": "DATA", "content": "We used the standard LlamaRec format. To construct the text input, we prepend an instruction to\ndescribe the task, followed by both history and candidate items, represented by their titles. Our\nprompt template is: \" Instruction: Given the user's history in chronological order, recommend an\nitem from the candidate pool with its index letter; Input: User history: history; Candidate pool:\ncandidates; Answer: label\" where history, candidates and label are replaced by history item titles,\ncandidate item titles and the labeld of each data item. For inference, the label position is left empty\nfor the model to provide predictions."}, {"title": null, "content": "We then created a pair data set, where each pair consists of a winner and a loser. It consists of\n2 parts. The first part was created from the last 2 items from the user history, where the winner\nis the item with the higher rating and the loser the opposite. To create the second part, we used\nnegative sampling training LightFM model and took the last user's movie as the winner and the\nworst recommendation of LightFM as the loser."}, {"title": "EXPERIMENTAL SETUP", "content": "For the ORPO procedure we used lr = 5e-6, alpha = 0.05, and an inverse sqrt scheduler with 100\nwarmup steps, gradient accumulation step equal to 1 and 5 training epochs. We considered 2 datasets\n- Movielens-1M and Amazon VideoGames."}, {"title": "ORPO RESULTS", "content": "ORPO procedure provide slight impact on LLM perfomace not affecting original latency. See Figure\n2 and 3."}, {"title": "ADAPTIVE BATCHING", "content": ""}, {"title": "ALGORITHM", "content": "We were able to apply the concept of adaptive batching to the aforementioned algorithm (USGM)\nand enhance its performance in training SSMs in the sequential recommendations domain."}, {"title": null, "content": "In stochastic optimization, the variance of the gradient estimates reduces as the batch size increases.\nSpecifically, the variance decreases proportionally to $\\frac{1}{\\sqrt{B}}$. Concurrently, the standard deviation (\u03c3)\nof the gradient estimates decreases at the rate of $\\frac{\\sigma}{\\sqrt{B}}$. From here 1.3, we can derive the equation\n2.1 Thus, the variance and standard deviation of the gradient estimations decrease as the batch size\nincreases. But as the batch size increases, the impact of this reduction decreases. Additional bigger\nincreases in batch size result in insignificant decreases in variance, as accuracy gains have landed on\na plateau. Therefore, increasing the batch size further at this stage is not effective, since it will not\nprovide any improvements in accuracy, but will require additional computational resources.\n$\u03b2_{k+1} \u2264 L_vr_{k+1}^{v} + \\frac{\u03c3_{k+1}r_{k+1}}{\\sqrt{B}}$, (2.1)\nFurther into the paper we will consider this upper estimate an equality to indirectly calculate the\nvalues of $[L_vr_{k+1}^{v}]_+$ and $\u03c3_{k+1}r_{k+1}$, since we know the exact value of $\u03b2_{k+1}$ on each iteration of the\nalgorithm.\nAs the coefficient $\u03b2_{k+1}$ is computed on every iteration of the algorithm and the batch size is known,\nwe can use the Weighted Least Squares method (with loss defined by the equation 2.2) in linearized\naxes ($\\frac{1}{\\sqrt{B}}$ as the X axis and the $\u03b2_{k+1}$ as the Y axis) to compute the values of $L_vr_{k+1}^{v}$ and $\u03c3_{k+1}r_{k+1}$,\nthe intercept and the slope of the linearized function respectively.\n$L = \\sum_{k=0}^{K}(F(c_1, c_2, B_k) \u2013 \u03b2_{k+1})^2 \u00b7 (1 \u2013 \u03b1)^{K-k}$, (2.2)\nwhere F is the sought function $F = c_1 + \\frac{c_2}{\\sqrt{B}} \u2248 \u03b2_{k+1}$, $c_1$ and $c_2$ are the values of $L_vr_{k+1}^{v}$ and\n$\u03c3_{k+1}r_{k+1}$ respectively, predicted by the WLS algorithm. The value of alpha was empirically set to\n\u03b1 = 0.01, s.t. the first points almost diminish.\nIn the proposed algorithm, we will increase the batch size on each iteration ($B_k := B_{k-1} + B_0$) if\n$c_1 \u2260 \\frac{c_2}{\\sqrt{B}}$ and recalculate the values of $c_1$ and $c_2$ using WLS ($\\underset{c_1,c_2}{L} \u2192 min$).\nWhen we reach the plateau ($c_1 = \\frac{c_2}{\\sqrt{B}}$), we fix the current batch size until the end of the epoch. We\nconsider this batch size perfect w.r.t the current conditions and label it as $B^{**}$."}, {"title": null, "content": "When the epoch ends, we lower the batch size ($B_i := B_{i}^{k}$), since the perfect batch size ($B^*$)\ncould have changed since the last epoch. However, if it's bigger than $B_{i}^{k}$, the algorithm will\nincrease it during the next iterations. By doing so, we give the algorithm the ability to not continue\ntraining with excessive resource if the calculated batch size turns out to be too big in the current\ncircumstances or to increase it even more if needed.\nSince $B_k := B_{k-1} + B_0$, $B_{k} = m_k B_0 : \u2200k (m_k \u2208 Z)$. That means we can use the original\nDataloader class from PyTorch (Paszke et al. (2019)) wihout any modifications and take $m_k$ batches\nof initial size ($B_0$) on each iteration.\nExperiments have shown that the use of values of beta and the batch size only from the current\nepoch in WLS almost do not change the desired Batch size ($B_i$) in comparison with using all of\nthe previous values recorded during the training procedure. This proves that the values of $c_1$ and $c_2$\nare, indeed, true for all parts of the loss landscape as we can see during the training procedure. This\nfact goes along well with the proposed theoretical assumptions, as these coefficients in part consist\nof values, that are constant for the whole function (the H\u00f6lder-Lipschitz constant $L_v$) and therefore\nshould not change when we adjust the values of model's parameters x during optimization.\nSince the number of iterations of the algorithm with adaptive batching and of those, with which it\nDataset size\nBatch size\nwas compared, differs, since the equality $\\frac{N_{iterations}}{Dataset size} = \\frac{Batch size}{const}$ is not true for the proposed\nalgorithm, a scaled measure, which is proportional to the epoch number is used as the X axis in the\ncomparison plot, displayed in the Figure 3.\nThe experiments conducted using the 2Mamba4Rec pipeline, introduced in this paper, on the\nMovieLens-100k dataset have shown promising results regarding the innovative adaptive batch-\ning algorithm, applied to the USGM optimization algorithm by Rodomanov et al. (2024), when\ncompared with the well-known Adam optimizer by Kingma & Ba (2017)."}, {"title": "DISCUSSION", "content": "Limitations. ORPO and LLM related procedures require a lot of computational resources, espe-\ncially for larger models. LLM inference itself also requires optimization. SSM based models have\npoor CPU performance due to their structure and implementation. Furthermore, optimizer that we\nproposed has poor robustness among its hyperparameters.\nPotential impact. We believe that ideas and obtained results from our work can inspire the commu-\nnity to continue the research in the field of recommender systems.\nBroader impact. The goal of this paper is to advance the field of Machine Learning. There are many\npotential societal consequences of our work, none of which we feel must be specifically highlighted\nhere."}]}