{"title": "EXPLORING APPLICATIONS OF STATE SPACE MODELS AND ADVANCED TRAINING TECHNIQUES IN SEQUENTIAL RECOMMENDATIONS: A COMPARATIve Study ON EFFICIENCY AND PERFORMANCE", "authors": ["Baderko Makar", "Kulibaba Stepan", "Obozov Mark", "Nikolay Kutuzov", "Alexander Gasnikov"], "abstract": "Recommender systems aim to estimate the dynamically changing user preferences and sequential dependencies between historical user behaviour and metadata. Although transformer-based models have proven to be effective in sequential recommendations, their state growth is proportional to the length of the sequence that is being processed, which makes them expensive in terms of memory and inference costs. Our research focused on three promising directions in sequential recommendations: enhancing speed through the use of State Space Models (SSM), as they can achieve SOTA results in the sequential recommendations domain with lower latency, memory, and inference costs, as proposed by Liu et al. (2024); improving the quality of recommendations with Large Language Models (LLMs) via Monolithic Preference Optimization without Reference Model (ORPO); and implementing adaptive batch- and step-size algorithms to reduce costs and accelerate training processes.", "sections": [{"title": "INTRODUCTION", "content": null}, {"title": "RELATED STUDY", "content": null}, {"title": "SEQUENTIAL RECOMMENDATION TASK DEFINITION", "content": "Consider the user set $U = {U_1, U_2, ..., U_N}$, item set $V = {U_1, U_2, ..., \u03c5_\u03ba}$ and $S_u = {U_1, U_2, ..., U_{n_u}}$ as the chronologically ordered interaction sequence for user $u \u2208 U$, where $n_u$ is the the length of the sequence. Given $S_u$ the task is to predict the next interacted item, $U_{n_u+1}$."}, {"title": "TRANSFORMERS", "content": "Recently, transformer models have been shown to be effective in sequential recommendation tasks as the backbone of larger models Kang & McAuley (2018) and as individual LLMs Li et al. (2023), Yue et al. (2023). Despite their success, attention-based methods face inference inefficiencies due to the quadratic computational complexity inherent in attention operators and their rapid state growth, which is proportional to the sequence length. Also, without special mechanisms Liu et al. (2023), transformers can't handle long contexts and, consequently, long user histories."}, {"title": "STATE SPACE MODELS", "content": "State Space Model (SSM) is a recent framework for sequence modelling defined by linear ordinary differential equations:\n\n$h'(t) = Ah(t) + Bx(t)$ \n\n$y(t) = Ch(t)$\n\nWhere A, B, C are learnable matrices, h(t) is the latent space, x(t) is the input sequence and y(t) is the output sequence. To compute sequence-to-sequence transformations efficiently, the matrix A must be structured, so structured SSMs have been introduced Gu et al. (2022). A general form of a structured SSM is defined by the equations:\n\n$h_t= Ah_{t-1}+ Bx_t$\n\n$Y_t = Ch_t$\n\nWhere $A \u2208 R^{(N,N)}, B\u2208 R^{(N,1)}, C\u2208 R^{(N,1)}$. They map a 1-dimensional sequence $x \u2208 R^T \u2192 y \u2208 R^T$ through an implicit latent state $h \u2208 R^{(T,N)}$. To operate directly on sequences, the discretisation rule $(f_A, f_B)$ is applied to continuous parameters $(\\Delta, \u00c5, B)$, by $A = f_A(\\Delta, \u00c5)$ and $B = f_B(A, B)$, where A is the parameterised step size."}, {"title": "MAMBA BLOCK", "content": "In order to adaptively focus on relevant information while filtering out noise, the Mamba block Gu & Dao (2024) introduces an extension to structured SSMs by adding a data-dependent selection mechanism. An important feature of Selective SSMs is their ability to be computed efficiently on the GPU using kernel fusion, parallel scanning and recomputation mechanisms.\nIn contrast to transformers' $O(n^2 \u00b7 d)$, SSMs provide $O(n \u00b7 d^2)$ complexity, which makes them a more efficient alternative, especially when operating on long sequences (Dao & Gu (2024)) ."}, {"title": "MAMBA APPLICATIONS TO SEQUENTIAL RECOMMENDATIONS", "content": "Several applications of Mambas selective SSMs to recommender systems have been introduced. Liu et al. (2024), Wang et al. (2024). As Mamba has already shown efficient results in different areas Zhu et al. (2024) Erol et al. (2024) Chen et al. (2024), we propose a more complex exploration of Mamba applications to recommender systems."}, {"title": "UNIVERSAL GRADIENT METHOD FOR STOCHASTIC OPTIMIZATION", "content": "The release of the Universal Stochastic Gradient Method (USGM) by Rodomanov et al. (2024) represents a noteworthy development in the field of stochastic optimization, that is the backbone of machine learning.\nThe classical Stochastic Gradient Descent (SGD) by Robbins (1951) is one of the earliest techniques in the field of stochastic optimization. Its novelty at the time of publication helped it gain popularity. However, because hyperparameter values are only defined prior to the start of the training phase, SGD achieves poor convergence rates in real-world applications.\nA number of SGD-based adaptive strategies, including Adam by Kingma & Ba (2017) and Adagrad by Duchi et al. (2011), have been put forth to address the issue. Although they are more efficient than SGD, because they modify the step size according to the aggregate of the previous gradients, when applied to non-convex optimization problems, they might still experience difficulty achieving convergence.\nThe USGM develops the idea of adaptiveness further, as the algorithm can adapt to the curvature of the loss function, thus providing more accurate gradient estimates even in highly noisy envi-ronments. Classic optimization algorithms are typically designed to handle either smooth or non-smooth problems. USGM, however, is an algorithm capable of addressing both classes of problems"}, {"title": "Notation", "content": "The same mathematical notation as in the original USGM paper will be used.\nThe loss function is defined as f and the parameters of our model are defined as x.\nTo characterize the smoothness of f the H\u00f6lder constant is introduced for each $v\u2208 [0,1]$:\n\n$L_v := \\sup_{\\substack{x,y\u2208Domf,x\u2260y,\\ g(x)\u2208\u2202f(x),g(y)\u2208\u2202f(y)}} \\frac{||g(x) - g(y)||* }{||x - y||^v}$                                                                                     (1.1)\n\nwhere g(x) and g(y) are the stochastic approximations of subgradients of our loss function f at points x and y respectively.\nThe dual norm is defined in the standard way:\n\n$||s||_* := \\max_{||x||=1} (s, x) = (s, B^{-1}s)^{1/2}, s\u2208R^n$.                                                                      (1.2)\n\nOf course, for certain values of the exponent v \u2208 [0, 1], it may happen that $L_v = +\u221e$. However, we assume that there exists (at least one) exponent for which the corresponding H\u00f6lder constant is finite.\nFor any t \u2208 R, by $[t]_+ := \\max{t, 0}$, its positive part is denoted. For random variables X and \u00a7, by E[X] and E[X], the expectation of X w.r.t. \u00a7, and the full expectation of X, respectively are denoted."}, {"title": "Odds Ratio Preference Optimization", "content": "Odds Ratio Preference Optimization (ORPO), Hong et al. (2024) incorporates an odds ratio-based penalty to the conventional negative log-likelihood (NLL) loss for differentiating the generation styles between favored and disfavored responses.\nF an input sequence x, the average log-likelihood of generating the output sequence y, of length m tokens, is computed as in the Equation 1. The odds of generating the output sequence y given an"}, {"title": "OBJECTIVE FUNCTION OF ORPO", "content": "The objective function of ORPO in Equation 4 consists of two components: 1) supervised fine-tuning (SFT) loss (LSFT); 2) relative ratio loss (LOR).\n\n$LORPO = E_{(x,y_w,y_l)} [LSFT + \u03bb \u00b7 LOR]$                                                                                                   (4)\n\nLSFT follows the conventional causal language modeling negative log-likelihood (NLL) loss function to maximize the likelihood of generating the reference tokens. LOR in Equation 5 maximizes the odds ratio between the likelihood of generating the disfavored response Yyw and the disfavored response yr. The log odds ratio are wrapped with the log sigmoid function so that LOR could be minimized by increasing the log odds ratio between yw and y\u0131.\n\n$LOR = -log\u03c3(log(\\frac{odds_\u03b8(y_w|x)}{odds_\u03b8(y_l|x)}))$                                                                                                                              (5)\n\nTogether, LSFT and LOR weighted with \u03bb tailor the pre-trained language model to adapt to the specific subset of the desired domain and disfavor generations in the rejected response sets."}, {"title": "GRADIENT OF ORPO", "content": "The gradient of LRatio further justifies using the odds ratio loss. It comprises two terms: one that penalizes the wrong predictions and one that contrasts between chosen and rejected responses, denoted in Equation 6for d = (x, y\u0131, Yw) ~ D.\n\n$\u2207_{OLOR} = \u03b4(d) \u00b7 h(d)$                                                                                                                                          (6)\n\n$\u03b4(d) = [1 + \\frac{odds_{\u03b8^{P}}(y_w|x)}{odds_{\u03b8^{P}}(y_l|x)}]^{-1}$                                                                                                                                   (7)\n\n$h(d) = \\frac{\u2207_{\u03b8}logP_\u03b8(y_w|x)}{1 - P_\u03b8(y_w|x)} - \\frac{\u2207_{\u03b8}logP_\u03b8(y_l|x)}{1 - P_\u03b8(y_l|x)}$                                                                                                                               (8)\n\nWhen the odds of the favored responses are relatively higher than the disfavored responses, d(d) in Equation 7 will converge to 0. This indicates that the d(d) will play the role of a penalty term, accelerating the parameter updates if the model is more likely to generate the rejected responses.\nMeanwhile, h(d) in Equation 8 implies a weighted contrast of the two gradients from the chosen and rejected responses. Specifically, $1 \u2013 P(y|x)$ in the denominators amplifies the gradients when the corresponding side of the likelihood P(yx) is low. For the chosen responses, this accelerates the model's adaptation toward the distribution of chosen responses as the likelihood increases."}, {"title": "OUR RESULTS", "content": null}, {"title": "2MAMBA4REC", "content": "To compare the Mamba2 model more fairly with its predecessor (Mambal) we used the model architecture from the Mamba4Rec article by Liu et al. (2024), replacing the Mamba block with the Mamba2 block introduced by Dao & Gu (2024)."}, {"title": "MAMREC", "content": "We implemented the standard GPT4Rec architecture from Li et al. (2023) while replacing the GPT-2 model Radford et al. (2019) with a Mamba2 Dao & Gu (2024) model in order to achieve better performance."}, {"title": "HYDRA LAYER", "content": "To apply the sequence-to-sequence potential of the Hydra block, we provide a custom Hydra layer that combines the Hydra block with a standard feed-forward network. The main part of our standard architecture consists of Hydra layers. We have found that Hydra layers work more effectively than Mamba layers. Then we use the same PFFN that was introduced in Mamba4Rec Liu et al. (2024)\n\nPFFN(H) = GELU(HW^(1) + h^(1))W^(2) + b^(2))\n\nWhere $W^{(1)} \u2208 R^{D\u00d74D}, W^{(2)} \u2208 R^{D\u00d74D}, b^{(1)} \u2208 R^D$ are parameters of two dense layers and we use GELU activation."}, {"title": "PREDICTION LAYER", "content": "Prediction layer is adopted from SASRec and Mamba4Rec, last item embedding is used to generate the final prediction scores:\n\n$\\hat{y} = Softmax(hE^T) \u2208 R^{|V|}$\n\nwhere $h \u2208 R^D$ is the last item embedding from the Hydra layer and $\\hat{y} \u2208 R^{|V|}$ represents the probability distribution over the next item in the item set V."}, {"title": "BENCHMARKS", "content": "We tested our models on 3 benchmarks: Amazon Reviews '23 Beauty and Personal care, Amazon Reviews '23 Video Games and MovieLens-1M.\nThe number listed in parentheses after the number of parameters of each models is the number of parameters after deducting the size of all embedding layers if present."}, {"title": "LATENCY", "content": "For the LLMs, the measure tokens-per-second was selected, whilst the average time to generate suggestions for a single user was chosen to compare the others.\nSince all of the latency values that we are dealing with are relatively small, it is important to ascertain the measurement's margin of error in order to comprehend the experimental results that were produced.\nConsequently, the bootstrap sampling method was applied. 30 samples comprising 1500 users ran-domly chosen from the original dataset were used to individually measure the latency values. Following the removal of outliers, the distribution of the latencies was examined, and the confidence intervals were computed.\nThe latency measurements were performed on one Nvidia A100-80GB graphics card."}, {"title": "INTERPRETATION OF RESULTS", "content": "It is evident that Hydra shows metrics that are similar to those of Mamba, but its latency is four to five times lower. Simultaneously, models such as LlamaRec exhibit superior performance; however, their substantial parameter count may constrain their applicability in real-world situations. Metrics-wise and latency-wise, SSM-based models outperform the baseline SASRec model on average.\nAccording to the study, SSMs can outperform LLMs while still operating at a greater speed, since they require less number of parameters."}, {"title": "APPLICATION OF ORPO TO RECOMMENDER SYSTEMS", "content": "To improve quality and incorporate human preference into the LLM, we used the ORPO technique described above."}, {"title": "BASELINE", "content": "As a baseline, we considered the standard LlamaRec pipeline with Llama 7B LLM. Both retrieval and LLM were fine-tuned as described in the LlamaRec paper prior to the ORPO procedure."}, {"title": "DATA", "content": "We used the standard LlamaRec format. To construct the text input, we prepend an instruction to describe the task, followed by both history and candidate items, represented by their titles. Our prompt template is: \" Instruction: Given the user's history in chronological order, recommend an item from the candidate pool with its index letter; Input: User history: history; Candidate pool: candidates; Answer: label\" where history, candidates and label are replaced by history item titles, candidate item titles and the labeld of each data item. For inference, the label position is left empty for the model to provide predictions.\nWe then created a pair data set, where each pair consists of a winner and a loser. It consists of 2 parts. The first part was created from the last 2 items from the user history, where the winner is the item with the higher rating and the loser the opposite. To create the second part, we used negative sampling training LightFM model and took the last user's movie as the winner and the worst recommendation of LightFM as the loser."}, {"title": "EXPERIMENTAL SETUP", "content": "For the ORPO procedure we used lr = 5e-6, alpha = 0.05, and an inverse sqrt scheduler with 100 warmup steps, gradient accumulation step equal to 1 and 5 training epochs. We considered 2 datasets - Movielens-1M and Amazon VideoGames."}, {"title": "ORPO RESULTS", "content": "ORPO procedure provide slight impact on LLM perfomace not affecting original latency. See Figure 2 and 3."}, {"title": "ADAPTIVE BATCHING", "content": null}, {"title": "ALGORITHM", "content": "We were able to apply the concept of adaptive batching to the aforementioned algorithm (USGM) and enhance its performance in training SSMs in the sequential recommendations domain."}, {"title": "In stochastic optimization", "content": "In stochastic optimization, the variance of the gradient estimates reduces as the batch size increases. Specifically, the variance decreases proportionally to $\\frac{1}{\\sqrt{B}}$. Concurrently, the standard deviation (\u03c3) of the gradient estimates decreases at the rate of $\\frac{\\sigma}{\\sqrt{B}}$. From here 1.3, we can derive the equation 2.1 Thus, the variance and standard deviation of the gradient estimations decrease as the batch size increases. But as the batch size increases, the impact of this reduction decreases. Additional bigger increases in batch size result in insignificant decreases in variance, as accuracy gains have landed on a plateau. Therefore, increasing the batch size further at this stage is not effective, since it will not provide any improvements in accuracy, but will require additional computational resources.\n\n$\\beta_{k+1} \\leq Lu r_{k+1} + \\frac{\\sigma_{k+1} r_{k+1}}{\\sqrt{B}}$                                                                                                  (2.1)\n\nFurther into the paper we will consider this upper estimate an equality to indirectly calculate the values of $[Lu r_{k+1} v r]_{+}$ and $\\sigma_{k+1} r_{k+1}$, since we know the exact value of $\\beta_{k+1}$ on each iteration of the algorithm.\nAs the coefficient $\\beta_{k+1}$ is computed on every iteration of the algorithm and the batch size is known, we can use the Weighted Least Squares method (with loss defined by the equation 2.2) in linearized axes ($\\frac{1}{\\sqrt{B}}$ as the X axis and the $\\beta_{k+1}$ as the Y axis) to compute the values of $Lu r_{k+1}$ and $\\sigma_{k+1} r_{k+1}$, the intercept and the slope of the linearized function respectively.\n\n$L = \\sum_{k=0}^K (F(c_1, c_2, B_k) - \\beta_{k+1})^2 \u00b7 (1 - a)^{K-k}$,                                                                                                                                                   (2.2)\n\nwhere F is the sought function $F = c_1 + \\frac{c_2}{\\sqrt{B}} \\approx \\beta_{k+1}$, $c_1$ and $c_2$ are the values of $Lu r_{k+1}$ and $\\sigma_{k+1} r_{k+1}$ respectively, predicted by the WLS algorithm. The value of alpha was empirically set to \u03b1 \u2248 0.01, s.t. the first points almost diminish.\nIn the proposed algorithm, we will increase the batch size on each iteration ($B_k := B_{k-1} + B_0$) if $c_1 \\neq \\frac{c_2}{\\sqrt{B}}$ and recalculate the values of $c_1$ and $c_2$ using WLS ($L \\to \\min$).\n\nWhen we reach the plateau ($c_1 = \\frac{c_2}{\\sqrt{B}}$), we fix the current batch size until the end of the epoch. We consider this batch size perfect w.r.t the current conditions and label it as $B_{*}$."}, {"title": "When the epoch ends", "content": "When the epoch ends, we lower the batch size ($B_i := \\frac{B_{ik}}{\\sqrt{k}}$, since the perfect batch size ($B_{*}$) could have changed since the last epoch. However, if it's bigger than $\\frac{B_{ik}}{\\sqrt{k}}$, the algorithm will increase it during the next iterations. By doing so, we give the algorithm the ability to not continue training with excessive resource if the calculated batch size turns out to be too big in the current circumstances or to increase it even more if needed.\nSince $B_k := B_{k-1} + B_0$, $B_k = m_k B_0 : \u2200k (m_k \u2208 Z)$. That means we can use the original Dataloader class from PyTorch (Paszke et al. (2019)) wihout any modifications and take $m_k$ batches of initial size ($B_0$) on each iteration.\nExperiments have shown that the use of values of beta and the batch size only from the current epoch in WLS almost do not change the desired Batch size ($B_i$) in comparison with using all of the previous values recorded during the training procedure. This proves that the values of c1 and c2 are, indeed, true for all parts of the loss landscape as we can see during the training procedure. This fact goes along well with the proposed theoretical assumptions, as these coefficients in part consist of values, that are constant for the whole function (the H\u00f6lder-Lipschitz constant $L_v$) and therefore should not change when we adjust the values of model's parameters x during optimization.\nSince the number of iterations of the algorithm with adaptive batching and of those, with which it was compared, differs, since the equality $\\frac{Dataset sise}{N iterations} = \\frac{Batch sise}{const}$ is not true for the proposed algorithm, a scaled measure, which is proportional to the epoch number is used as the X axis in the comparison plot, displayed in the Figure 3."}, {"title": "DISCUSSION", "content": "Limitations. ORPO and LLM related procedures require a lot of computational resources, espe-cially for larger models. LLM inference itself also requires optimization. SSM based models have poor CPU performance due to their structure and implementation. Furthermore, optimizer that we proposed has poor robustness among its hyperparameters.\nPotential impact. We believe that ideas and obtained results from our work can inspire the commu-nity to continue the research in the field of recommender systems.\nBroader impact. The goal of this paper is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}]}