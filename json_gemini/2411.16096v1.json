{"title": "ENCLIP: Ensembling and Clustering-Based Contrastive Language-Image Pretraining for Fashion Multimodal Search with Limited Data and Low-Quality Images", "authors": ["Prithviraj Purushottam Naik", "Rohit Agarwal"], "abstract": "Multimodal search has revolutionized the fashion industry, providing a seamless and intuitive way for users to discover and explore fashion items. Based on their preferences, style, or specific attributes, users can search for products by combining text and image information. Text-to-image searches enable users to find visually similar items or describe products using natural language. This paper presents an innovative approach called ENCLIP, for enhancing the performance of the Contrastive Language-Image Pretraining (CLIP) model, specifically in Multimodal Search targeted towards the domain of fashion intelligence. This method focuses on addressing the challenges posed by limited data availability and low-quality images. This paper proposes an algorithm that involves training and ensembling multiple instances of the CLIP model, and leveraging clustering techniques to group similar images together. The experimental findings presented in this study provide evidence of the effectiveness of the methodology. This approach unlocks the potential of CLIP in the domain of fashion intelligence, where data scarcity and image quality issues are prevalent. Overall, the ENCLIP method represents a valuable contribution to the field of fashion intelligence and provides a practical solution for optimizing the CLIP model in scenarios with limited data and low-quality images.", "sections": [{"title": "1 Introduction", "content": "The fashion industry is highly visual and dynamic, driven by constantly evolving trends and consumer preferences. Over 300 million people have jobs in the fashion industry, which generates global GDP of $1.3 trillion [12]. The rising prominence of e-commerce and online shopping platforms highlighted the need to develop fast and effective fashion search strategies. Traditional text-based search systems often fall short in capturing the rich visual aspects of fashion"}, {"title": "2 Related work", "content": "This section reviews previous related works on Multimodal Search in fashion domain.\nIn 2023 Min Wang et al.[16] proposed a study that focuses on training a visual question-answering (VQA) [2] system for apparel in fashion photoshoot images using a large-scale multimodal dataset. By employing diverse templates and emphasizing challenging concepts, they achieved a VQA model surpassing human expert-level accuracy, demonstrating the effectiveness of visual language models"}, {"title": "3 Background", "content": "This section discusses Multimodal Search and CLIP as used in the study."}, {"title": "3.1 Multimodal Search", "content": "Multimodal search refers to a search approach that employs several methodologies in order to obtain pertinent outcomes [8]. It is intended to mimic the flexibility and agility with which the human mind generates, processes, and rejects irrelevant ideas. It is a type of search that allows users to search for information"}, {"title": "3.2 CLIP", "content": "The neural network model known as CLIP (Contrastive Language-Image Pretraining) [15] was created by OpenAI. The model has been trained using a vast dataset consisting of pairings of images and corresponding texts.\nCLIP is a powerful model with the potential to revolutionize the way images and text are interacted with. It is still under development, but it has already been used for a variety of interesting applications.\nThe basic architecture of CLIP is as follows:\n\u2013 The model is first pre-trained on a massive dataset of image-text pairs.\n\u2013 The model then consists of two main components: a vision transformer [10] and a language model [9].\n\u2013 The vision transformer is responsible for encoding the images into a vector representation.\n\u2013 The language model is responsible for encoding the text into a vector representation.\n\u2013 The two vector representations are then compared to each other to determine the similarity between the image and the text.\nThe comparison of the two vector representations is done using a contrastive loss function [13]. The contrastive loss function is designed to maximize the similarity between the image and text representations when they are actually related, and to minimize the similarity between the image and text representations when they are not related. The basic architecture of CLIP is relatively simple, but it is very effective at learning the semantic similarities between images and text."}, {"title": "4 Methodology", "content": "This dataset, fashion-product-images-small [1], provided by HuggingFace, is used in this study. It consists of around 44072 60x80 pixel resolution images with details and descriptions about the image in respective columns. It has 7 master categories (Apparel, Accessories, Footwear, Personal Care, Free items, Home, Sporting Goods), 45 subcategories."}, {"title": "4.2\nPreprocessing", "content": "The dataset consists of around 44072 (image, text) pairs. It is divided into three sets of 80% for training, 10% for validation, and 10% for testing. There are 35258 (image,text) pair in the training set, 4407 (image,text) pair in the validation set and 4407 (image,text) pair in the test set. The training model requires the images to be scaled from their original dimensions of 60x80 pixels to 224x224 pixels. All column values except for id and year are considered as captions for the corresponding image. Balanced Batch Sampling is performed as a sampling technique to ensure that each batch of data contains an equal representation of different classes. It is designed to address the issue of class imbalance, where some classes have significantly fewer samples than others, which can lead to biased model training."}, {"title": "4.3 Model Architecture and Training", "content": "Adamax is used as an optimization technique. Categorical cross-entropy was employed as a loss function for both image and text. Tokenization of the caption for an image was performed before giving it to the model for training. The Cosine Annealing learning rate scheduler, which gradually reduces the learning rate in a cosine annealing pattern, was used.\nThe cosine annealing pattern is designed to help the model converge to a better solution by allowing it to explore different areas of the loss landscape during training. It helps to avoid getting stuck in local minima and potentially find a more optimal solution.\nAccording to Equation 1, the total loss is calculated as the average of the image loss and text loss.\n total loss = $\\frac{image loss + text loss}{2}$                                                                                      (1)\nThe model was trained across 10, 30, 50, 80, and 100 epochs, using a batch size of 128. It was executed on a Google Colab Pro platform equipped with an Nvidia Tesla T4 graphics card. During training, the model took an average of 280 seconds per epoch."}, {"title": "4.4 Ensemble and Clustering Strategy", "content": "Rationale for Model Selection: Five CLIP models are trained, each initialized with the same architecture but trained for different epochs: 10, 30, 50, 80, and 100 (To demonstrate the working of the Algorithm 1, five fine-tuned models with incremental training are considered in this study). This approach captures various learning stages of the model, from initial learning to fine-tuning. It was observed that results in initial epoch training and final epoch training complemented each other to give the best results collectively. Combining these models in an ensemble allows us to leverage the strengths of each.\nLatent space of the Algorithm: Latent space is a lower-dimensional space that captures the essential features of the input data. In ENCLIP approach, each model encodes an image into a fixed-size vector (e.g., 512 dimensions), capturing the essential features and patterns in the data while reducing its dimensionality using t-SNE (t-distributed Stochastic Neighbor Embedding) to 2D for visualization purposes. This helps in understanding the distribution and structure of the data in a more interpretable form.\nWeighted Sum Approach: To combine the predictions of each model, a weighted sum approach is employed. The weights are determined based on the epoch number, with later epochs receiving higher weights:\nweighted_score = $\\sum_{n=0}^{z-1}0.1 \u00d7 (2^n) \u00d7 occurrence$                                                          (2)\nThis weighting strategy ensures that more emphasis is placed on models that have undergone extensive training while still incorporating the insights from earlier training stages.\nEncoding Images: Each model is used to encode the images into a latent space, resulting in z sets of encoded images(z - Number of fine-tuned models; Five fine-tuned models (z=5) were considered in this study for demonstration of the Algorithm 1). These encoded representations capture the high-dimensional features learned by each model.\nDimensionality Reduction: To visualize and analyze the encoded features, t-SNE (t-distributed Stochastic Neighbor Embedding) is applied, reducing the dimensionality to 2D while preserving the local structure of the data.\nCluster Analysis and Ranking: K-Means clustering is performed on the t-SNE-transformed features to group similar images. The number of clusters (k) is set to values between 4 and 6, reflecting the diversity in the dataset categories."}, {"title": "Cluster Label Assignment:", "content": "Cluster labels are assigned to each image based on the K-Means results, and the frequency of each image appearing in different clusters is analyzed."}, {"title": "Frequency and Weighted Sum Calculation:", "content": "For each image, the frequency across different epochs is calculated, and a weighted sum based on predefined weights is computed. This helps in ranking the images according to their importance and relevance within each cluster."}, {"title": "Final Selection:", "content": "Images are ranked and selected based on their weighted sum and frequency, ensuring that the most representative and informative images are prioritized."}, {"title": "Algorithm 1 Ensemble-Cluster based Image Selection and Ranking Algorithm (ENCLIP Algorithm)", "content": "Require: Image outputs from Fine-tuned models M M1, M2,..., mz number of images to select N.\nEnsure: Selected and ranked N images.\n, \n1: Perform ensembling by considering all fine-tuned models M image outputs in the latent space.\n2: Assign frequency values to the images based on its occurrence in each fine-tuned model's output.\n3: Assign weighted scores for the images based on its occurrence in each fine-tuned model's output as following:\nweighted_score = $\\sum_{n=0}^{z-1}0.1 \u00d7 (2^n) X occurrence$                                                             (3)\nz = number of fine-tuned models used.\n4: Apply K-means Clustering on all fine-tuned model's output in the latent space.\n5: Initialize an empty list selected_heads to store head images.\n6: Store all the fine-tuned models image outputs in selected_heads and sort them in descending order based on frequency\n7: Initialize an empty set S to store the final selected images.\n8:10\n9: while |S < N do\n10:        Select the head image (selected_heads[i]) and consider clusters with the head image as the head cluster.\n11:        Select all the images in those head clusters and rank the images based on sort(frequency, weighted_score) and store them in S.\n12:        ii+1\n13: end while\n14: Return the top N images from S."}, {"title": "4.5 Evaluation Metrics", "content": "In this section, the evaluation metrics used to measure the performance of the ENCLIP approach is discussed. [4,7].\n\u2013 Precision(PREC): It can be defined as the percentage of relevant items out of those items selected by the query.\nPREC = $\\frac{Nrs}{Ns}$                                                                                                (4)\nwhere $N_{rs}$ is the number of the recommended items that user prefer and $N_s$ is the number of the recommended items.\n\u2013 Precision@k(PREC@k): It can be defined as the proportion of relevant recommended items in a recommendation list of size k for a query.\nPREC@k = $\\frac{Nrs@k}{k}$                                                                                                   (5)\nwhere $N_{rs@k}$ is the number of the recommended items that user prefer in a recommendation list of size k.\n\u2013 Mean Average Precision(mAP): The mean average precision (MAP) is the average of multiple recommendation precision. The more precision of the recommended items are, the higher the mAP is, and the formula of mAP is as follows [4]:\nAVG_PREC@k(q) = $\\frac{\\sum_{i=1}^{k}PREC@k \u00d7 rel(k)}{Number of relevant items}$                                                                   (6)\nmAP = $\\frac{1}{Q} \\sum_{q=1}^{Q}AVG_PREC@k(q)$                                                                                (7)\nwhere Q is the number of recommendation, k is the rank, rel(k) represents the relativity function given rank k, PREC@k represents the precision given rank k."}, {"title": "5 Evaluation Results and Discussion", "content": "Table. 1 and Table. 2 compare the retrieved results for the query \"Give me polo neck t-shirt for men\", with Table. 1 focusing on different fine-tuned CLIP models and Table. 2 comparing Pre-trained CLIP, FashionCLIP, and ENCLIP approaches. In Table. 2, the following points are observed:\n\u2013 In Pre-trained CLIP Model, the search output predominantly features full sleeved round neck t-shirts.\n\u2013 In FashionCLIP Model, the search output appears satisfactory but includes some full sleeved round neck t-shirts."}, {"title": "6 Conclusion", "content": "The introduction of multimodal search has brought about a revolutionary change in the fashion industry by providing users with a seamless and intuitive way to explore and discover fashion items. By integrating text and image information, users can now search for products based on their preferences, style, or specific attributes. This advancement has greatly enhanced the overall user experience and opened up new possibilities for fashion discovery. This paper presents an innovative approach called ENCLIP, specifically tailored to enhance the fine-tuning performance of the Contrastive Language-Image Pretraining (CLIP) model in the domain of fashion intelligence. The ENCLIP approach addresses the challenges posed by limited data availability and low-quality images, which are prevalent in the fashion industry. In ENCLIP approach, the process involves training and ensembling multiple instances of the CLIP model. Additionally, a clustering technique is employed to group similar images together to get the top N images based on the user's query for searching the top relevant N images.\nThe efficacy of the ENCLIP approach is demonstrated through experimental results, comparing it with recent studies. By leveraging the clustering techniques and training ensembles of CLIP models, the method successfully overcomes the limitations of data scarcity and low-quality images. However, it is important to note some limitations of the current approach. The ENCLIP approach may not be well-suited for fine-grained querying in the case of text-to-image search, where users require highly specific search results. The model's generalization capabilities may be limited in such cases, and further research is needed to improve its performance in fine-grained fashion queries. Performance in the case of fine-grained queries could be improved by adding more textual descriptions to enhance textual understanding in terms of data quality or by exploring more advanced multimodal fusion techniques[3]. The advancements made in this paper pave the way for further advancements in fashion intelligence and contribute to the ongoing evolution of multimodal search technology."}]}