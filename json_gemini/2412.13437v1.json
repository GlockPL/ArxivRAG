{"title": "Deploying Foundation Model Powered Agent Services: A Survey", "authors": ["Wenchao Xu", "Jinyu Chen", "Peirong Zheng", "Xiaoquan Yi", "Tianyi Tian", "Wenhui Zhu", "Quan Wan", "Haozhao Wang", "Yunfeng Fan", "Qinliang Su", "Xuemin Shen"], "abstract": "Foundation model (FM) powered agent services are regarded as a promising solution to develop intelligent and personalized applications for advancing toward Artificial General Intelligence (AGI). To achieve high reliability and scalability in deploying these agent services, it is essential to collaboratively optimize computational and communication resources, thereby ensuring effective resource allocation and seamless service de-livery. In pursuit of this vision, this paper proposes a unified framework aimed at providing a comprehensive survey on deploying FM-based agent services across heterogeneous devices, with the emphasis on the integration of model and resource optimization to establish a robust infrastructure for these services. Particularly, this paper begins with exploring various low-level optimization strategies during inference and studies approaches that enhance system scalability, such as parallelism techniques and resource scaling methods. The paper then discusses several prominent FMs and investigates research efforts focused on inference acceleration, including techniques such as model compression and token reduction. Moreover, the paper also investigates critical components for constructing agent services and highlights notable intelligent applications. Finally, the paper presents potential research directions for developing real-time agent services with high Quality of Service (QoS).", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid advancement of artificial intelligence (AI) has positioned foundation models (FMs) as a cornerstone of innovation, driving progress in various fields such as natural language processing, computer vision, and autonomous systems. These models, characterized by their vast parameter spaces and extensive training on broad datasets, incubate numerous applications from automated text generation to advanced multi-modal question answering and autonomous robot services [1]. Some popular FMs, such as GPT, Llama, ViT, and CLIP, are pivotal in pushing the boundaries of AI capabilities, offering sophisticated solutions for processing and analyzing large volumes of data across different formats and modalities.\nThe continuous advancement of FMs significantly enhances Al's ability to comprehend and interact with the world in a manner akin to human cognition.\nHowever, traditional FMs are typically confined to providing question-and-answer services and generating responses based on pre-existing knowledge, often lacking the ability to in-corporate the latest information or employ advanced tools. FM-powered agents are designed to enhance the capability of FM. These agents are incorporated with dynamic memory management, long-term task planning, advanced computa-tional tools, and interactions with the external environment [2]. For example, FM-powered agents can call different external APIs to access real-time data, perform complex calculations, and generate updated responses based on the most current information available. This approach improves the reliability and accuracy of the responses and enables more personalized interactions with users.\nDeveloping a serving system with low latency, high re-liability, high elasticity, and minimal resource consumption is crucial for delivering high-quality agent services to users. Such a system can efficiently manage varying query loads while maintaining swift response and reducing resource costs. Moreover, constructing a serving system on heterogeneous edge-cloud devices is a promising solution to leverage the idle computational resources at the edge and the abundant computational clusters available in the cloud. The collaborative inference of edge-cloud devices can enhance overall system efficiency by dynamically allocating tasks to various edge-cloud machines based on computational load and real-time network conditions.\nAlthough many research works investigate edge-cloud col-laborative inference for small models, deploying FMs under this paradigm for diverse agent services still faces several severe challenges. First, the fluctuating query load severely challenges the model serving. The rapidly growing number of users want to experience intelligent agent services with FMs. For example, as of April 2024, ChatGPT has approx-imately 180.5 million users, with around 100 million of users being active weekly [3]. These users access the service at different times, resulting in varying request rates. An elastic serving system should dynamically scale the system capacity according to the current system characteristics. Secondly, the parameter space of an FM is particularly large, reaching the scale of several hundred billion, which is a significant challenge to the storage system. However, the storage capacity of edge devices and the consumer GPU is limited, making it unable to accommodate an entire model. The large number of parameters results in significant inference overhead and long execution latency. Therefore, it is necessary to design model compression methods and employ different parallelism approaches in diverse execution environments. In addition, users have different service requirements and inputs in differ-ent applications. For example, some applications prioritize low latency, while others prioritize high accuracy. This necessitates dynamic resource allocation and adjustment of the inference process. Moreover, AI agents need to deal with lots of hard tasks under complex environments, which require effective management of large-scale memory, real-time processing of updated rules, and specific domain knowledge. Additionally, agents possess distinct personalities and roles, necessitating the design of an efficient multi-agent collaboration framework.\nTo address the aforementioned challenges and promote the development of the real-time FM-powered agent service, this survey proposes a unified framework and investigates various research works from different optimization aspects. This framework is shown in Figure 1. The bottom layer is the execution layer, where edge or cloud devices execute an inference with FMs. Joint computation optimization, I/O optimization, and communication optimization are applied to accelerate inference and promote the building of a powerful infrastructure for FMs. The resource layer, comprised of two components, facilitates the deployment of the model on various devices. Parallelism methods design different model splitting and placement strategies to utilize the available resources and improve throughput collaboratively. Resource scaling dynamically adjusts the hardware resources during runtime based on query load and resource utilization, thereby improving overall scalability. The model layer focuses on the optimization of FMs. Two lightweight methods, including model compression and token reduction, are specifically de-signed to promote the widespread adoption of FMs. Based on these FMs, many AI agents are constructed to accomplish var-ious tasks. Numerous methods have been proposed to enhance the four key components of agents, which encompass the multi-agent framework, planning capabilities, memory storage, and tool utilization. Ultimately, leveraging the aforementioned techniques, all kinds of applications can be developed to deliver intelligent and low-latency agent services to users."}, {"title": "A. Previous Works", "content": "Many research works focus on system optimization to deploy machine learning models in edge-cloud environments. KACHRIS reviews some hardware accelerators for Large Language Models (LLMs) to address the computational chal-lenges [4]. Tang et al. summarize scheduling methods designed for optimizing both network and computing resources [5]. Miao et al. present some acceleration methods to improve the efficiency of LLMs [6]. This survey includes system optimiza-tions, such as memory management and kernel optimization, as well as algorithm optimizations, such as architectural design and compression algorithms, to accelerate model inference. Xu et al. focus on the deployment of Artificial Intelligence-Generated Content (AIGC), and they provide an overview of mobile network optimization for AIGC, covering the processes of dataset collection, AIGC pre-training, AIGC fine-tuning, and AIGC inference [7]. Djigal et al. investigate the appli-cation of machine learning and deep learning techniques in resource allocation for Multi-access Edge Computing (MEC) systems [8]. The survey includes resource offloading, resource scheduling, and collaborative allocation. Many research works propose different algorithms to optimize the design of FMs and agents. [1], [9] and [10] present popular FMs, especially LLMs. [11], [12] and [13] summarizes model compression and inference acceleration methods for LLM. [2], [14], and [15] review the challenges and progress for the development of agents.\nIn summary, the above studies either optimize edge-cloud resource allocation and scheduling for small models or design acceleration or efficiency methods for large FMs. To the best of our knowledge, this paper is the first comprehensive survey to review and discuss the deployment of real-time FM-powered agent services in heterogeneous devices, a research direction that has gained significant importance in recent years. We design a unified framework to fill this research gap and review current research works from different perspectives. This framework not only delineates essential techniques for the deployment of FMs but also identifies key components of FM-based agents and corresponding system optimizations specifically tailored for agent services."}, {"title": "B. Contribution", "content": "This paper presents a comprehensive survey on the deploy-ment of FM-powered agent services in edge-cloud environ-ments, covering optimization approaches spanning from hard-ware to software layers. For the convenience of readers, we provide an outline of the survey in Figure 2. The contributions of this survey are summarized in the following aspects:\n\u2022 This survey proposes the first comprehensive framework to provide a deep understanding of the deployment of FM-powered agent services within the edge-cloud envi-ronment. Such a framework holds the potential to foster the advancement of AGI greatly."}, {"title": "II. EXECUTION OPTIMIZATION", "content": "Edge deployment of FMs poses severe challenges due to the heterogeneity of edge devices in terms of computing ability, hardware architecture, and communication bandwidth [16]. It is important to design computation optimization methods for different devices to accelerate model inference. Figure 3 provides an overview of this chapter. This diagram provides a global perspective for algorithm design (e.g., computation optimization). It also depicts the components of an edge computing system, including heterogeneous backends and the network. Table I shows some commonly used hardware devices at the edge. Traditionally, devices with CPUs have been deployed at the edge environment. However, due to their limited parallel computing capabilities and memory constraints, various specialized accelerators are designed for Deep Learning (DL) tasks. FPGAs have gained widespread attention for their programmable and parallel computing ca-pabilities. ASICs, while non-programmable after manufac-ture, offer high speed and low power consumption. Many edge devices, such as personal computers and smartphones, have different computational resources, such as GPU and CPU. Consequently, many approaches focus on optimizing computational efficiency by jointly utilizing these resources for model inference. In-memory computing, with non-Von-Neumann architecture, has emerged as a prominent research area because its intrinsic parallelism can significantly reduce the I/O latency and improve computational efficiency. To enhance the understanding of different devices, we provide some detailed information about different hardware devices as below.\n1) FPGAs: FPGAs are widely deployed in edge computing applications and are re-configurable hardware devices that are efficient in power consumption. FMs are built upon Transform-ers, which contain non-linear computation components, such as layer normalization, SoftMax, and non-ReLU activation functions. Serving these models requires specific accelerator designs. On the other hand, matrix multiplications are con-ducted during the inference of FMs. Their substantial compu-tational complexity poses challenges for optimizing FPGA-based accelerators. To solve these problems, a specialized hardware accelerator is designed for Multi-Head Attention (MHA) and feed-forward networks (FFN) in Transformers [17]. It incorporates a matrix partitioning strategy to optimize resource sharing between Transformer blocks, a computation flow that maximizes systolic array utilization, and optimiza-tions of nonlinear functions to diminish complexity and la-tency. MnnFast provides a scalable architecture specifically for Memory-augmented Neural Networks [18]. It incorporates a column-based streaming algorithm, zero-skipping optimiza-tion, and a dedicated embedding cache to tackle the challenges posed by large-scale memory networks. NPE offers software-like programmability to developers [19]. Unlike previous FPGA designs that implement specialized accelerators for each nonlinear function, NPE can be easily upgraded for new models without requiring extensive reconfiguration, making it a more cost-effective solution. DFX is a multi-FPGA system that optimizes the latency and throughput for text generation. It leverages model parallelism and an optimized, model-and-hardware-aware dataflow to handle the sequential nature of text generation. It demonstrates the superior speed and cost-effectiveness of FPGA compared to GPU implementations [20].\nAdditionally, the development of Transformer-OPU (Over-lay Processor) [21] provides a flexible and efficient FPGA-based processor that expedites the computation of Transformer networks. Moreover, implementing a tiny Transformer model through a Neural-ODE (Neural Ordinary Differential Equa-tion) approach [22] leads to a substantial reduction in model size and power usage, making it ideal for edge computing de-vices in Internet-of-Things (IoT) applications. FlightLLM [23] addresses the computational efficiency, memory bandwidth utilization, and compilation overheads on FPGAs for LLM inference. It includes a configurable Digital Signal Processor (DSP) chain optimized for varying sparsity patterns in LLMs and always-on-chip activations during decoding and a length adaptive compilation method for dynamic sparsity patterns and input lengths.\nIn summary, the above designs for FPGA-based FM in-ference focus on several key aspects: enhancing computa-tional efficiency through specialized architectures such as systolic arrays and DSP chains, optimizing memory usage via techniques like matrix partitioning and dedicated caches, reducing latency through efficient dataflows and model-and-hardware-aware optimizations, and improving adaptability and programmability to accommodate diverse and evolving model structures and non-linear functions.\n2) ASIC: An ASIC is an integrated circuit chip customized for a specific application. For example, router ASICs can handle packet processing and signal modulation. It often includes microprocessors, memory, and other components as a System-on-Chip (SoC). Recent advancements in ASIC design significantly enhance the performance and efficiency of attention mechanisms, which is crucial for applications across NLP and CV. The A\u00b3 [24] accelerator employs algo-rithmic approximations and a prototype chip to significantly enhance energy efficiency and processing speed. Essentially, the attention mechanism is a content-based search to evaluate the correlation between the currently processed token and previous tokens. A\u00b3 addresses the inefficiency of matrix-vector multiplication in the self-attention mechanism, which is sub-optimal for the content-based search, by implement-ing an efficient greedy candidate search method. Similarly, ELSA (Efficient, Lightweight Self-Attention) [25] tackles the quadratic complexity of self-attention by selectively filtering out less important relations in self-attention and implements an ASIC to achieve high energy efficiency.\nSpAtten [26] prunes Transformer models at both the token and head levels and reduces the model size with quantiza-tion to minimize the computational and memory demands of Transformers. Sanger [27] framework enables sparse attention mechanisms through a reconfigurable architecture that sup-ports dynamic software pruning and efficient sparse opera-tions. Additionally, the Energon [28] co-processor, working together with other FM accelerators, introduces a dynamic sparse attention mechanism that uses a mix-precision multi-round filtering algorithm to optimize query-key pair eval-uations. The above ASIC solutions demonstrate significant advancements in speed and energy efficiency compared to traditional devices while still preserving high accuracy. They pave the way for real-time, resource-efficient implementations for complex FMs.\n3) In-memory Compute: In-memory computing (IMC) is an emerging computational paradigm performing computational tasks directly within memory, eliminating frequent I/O require-ments between processors and memory units. IMC is a scal-able and energy-efficient solution to handle long sequence data in Transformers. [29] introduces ATT, a fault-tolerant Resistive Random-access Memory (ReRAM) accelerator specifically designed for attention-based neural networks. This acceler-ator capitalizes on the high-density storage capabilities and low leakage power of ReRAM to address the compatibility issues between traditional neural network architectures and the complex data flow of attention mechanisms. ReTransformer [30] is a ReRAM-based IMC architecture for Transformers. It accelerates the scaled dot-product attention mechanism, utilizes a matrix decomposition technique to avoid storing intermediate results, and designs the sub-matrix pipelines of MHA.\nThe iMCAT utilizes a combination of crossbar arrays to store the matrix in the memory array and uses Content Addressable Memories (CAM) to overcome the significant memory and computational bottlenecks in processing long sequences with MHA [30]. Recent works design different op-timization methods to deploy computationally intensive Trans-former models on edge AI accelerators [31]. The Google TPU, categorized as ASIC and IMC, is widely used in edge and cloud computing. Large Transformer models can be executed efficiently on the Coral Edge TPU by optimizing the computa-tional graph and employing quantization techniques, ensuring real-time inference with minimal energy consumption. The techniques employed in IMC, including matrix decomposition and quantization, reduce the computational resources required to serve FMs, thereby facilitating the deployment of FMs at the edge.\n4) CPU & GPU: Current optimization algorithms, such as CPU-GPU hybrid computation, are hardware-independent and can be applied to various backends. Different methods are designed to accelerate Transformer inference by optimizing the MHA, FFN, skip connections, and normalization modules, which are identified as critical bottlenecks in Transformer architectures [32]. [33] aims to optimize the execution time of the SoftMax layer by decomposing it into multiple sub-layers and then fusing them with adjacent layers. LLMA [34] leverages the overlap between the text generated by LLMs and existing reference texts to enhance computational parallelism and speed up the inference process without compromising output quality. These algorithms are hardware-agnostic and can be applied to various backends besides CPU and GPU. UltraFastBERT optimized inference by activating only a frac-tion of the available neurons, thereby maintaining competitive performance levels [35]. Intel CPU clusters can serve LLMs by adopting the algorithms [36], such as quantization and optimized kernels, specifically designed to accelerate compu-tations on CPUs.\nMany optimizations have been proposed by coordinately utilizing GPUs and CPUs to improve the efficiency and speed of Transformer models. Similarly, [37] introduces PowerInfer, a high-speed LLM inference engine that leverages the high locality and power-law distribution in neuron activation to reduce GPU memory demands and CPU-GPU data transfers. [38] aims to reduce latency by coordinating CPU and GPU computing while mitigating the I/O bottlenecks by overlapping the data processing and I/O time. [39] utilizes a single GPU to serve LLMs, prioritizing throughput at the expense of latency. It designs a scheduling algorithm to schedule model parameters, KV caches, and activations between CPU, GPU, and disk. These advancements focus on hardware-independent algorithms and CPU-GPU collaborative inference strategies, signifying a robust movement towards more efficient FMs."}, {"title": "B. Memory Optimization", "content": "1) Vanilla FMs: Besides computational costs, memory overhead is another major challenge in deploying LLMs at resource-constrained edge devices. The inference process of the LLM involves two key stages: the prefill and decode phases. The prefill phase is responsible for creating the KV cache based on the prompts, while the decode phase focuses on generating the subsequent token autoregressively. The first stage is compute-intensive, and the primary challenge of the decoding stage lies in the memory wall. This barrier prevents the loading of the vast parameters associated with LLMs and the maintenance of the KV cache throughout the inference pro-cess. Consequently, considerable research effort has focused on memory scheduling, utilizing multiple storage levels to execute a single model. For example, jointly optimizing CPU and GPU memory usage makes it possible to deploy LLMs on personal computers, smartphones, and other weak devices. Contextual sparsity, which refers to the input-dependent activation of a small subset of neurons in LLMs, has been exploited to reduce computational and memory costs. [40] proposes a method to predict contextual sparsity on the fly and an asynchronous, hardware-aware implementation to acceler-ate LLM inference. [41] explores storing model parameters in flash memory and loading only the required subset during inference, considering bandwidth, energy constraints, and read throughput.\nSeveral studies have focused on optimizing the attention mechanism in LLMs to reduce I/O costs. [42] introduces the Multi-Query Attention to accelerate decoding by reducing memory bandwidth requirements during incremental infer-ence. [43] proposes Grouped-Query Attention (GQA), an in-termediate between MHA and Multi-Query Attention (MQA), achieving a balance between quality and speed. [44] intro-duces PagedAttention, which is inspired by virtual memory and paging techniques, to efficiently manage key-value cache memory and integrate it into vLLM, a high-throughput LLM serving system. [45] proposes FlashAttention, an I/O-aware attention algorithm that reduces memory accesses between High Bandwidth Memory (HBM) and on-chip Static Random-Access Memory (SRAM) in GPU, which avoids fully loading the large attention matrix, reducing I/O overhead significantly. [46] further improves FlashAttention to enhance efficiency and scale Transformers to longer sequence lengths by strategically reducing the number of non-matrix-multiplication operations and paralleling the sequence dimension. This adjustment en-ables improved resource utilization across the GPU thread block. FlashDecoding++ [47] introduces asynchronized partial parallel SoftMax. SoftMax requires subtracting a value from all input numbers before exponential computing to avoid over-flow of the denominator. The subtracted value is usually the maximum input value, so synchronization is required to obtain this maximum value when computing softmax in parallel. However, FlashDecoding++ can avoid this synchronization by utilizing a unified max value pre-decided according to the statistical LLM-specified input distribution to the SoftMax.\nBMInf [48] introduces the Big Model Inference and Tuning toolkit, which employs model quantization, parameter-efficient tuning, and CPU-GPU scheduling optimization to reduce com-putational and memory costs. [49] proposes Splitwise, a tech-nique that schedules the prompt computation and token gen-eration phases to different machines, optimizing hardware uti-lization and overall system efficiency. FastServe is a distributed serving system that optimizes job completion time for LLMs in interactive AI applications [50]. [51] introduces SpecInfer, a system that accelerates generative LLM serving through tree-based speculative inference and verification. LLMCad is an innovative on-device inference engine specifically designed for efficient generative NLP tasks [52]. It executes LLM on weak devices with limited memory capacity by utilizing a compact LLM that resides in memory to generate tokens and a high-precision LLM for validation.\n2) MoE FMs: Mixture-of-Experts (MoE), replacing an FFN with a router and multiple FFNs, is a promising approach to enhance the efficiency and scalability of LLMs. How-ever, deploying MoE-LLMs presents challenges in memory scheduling because of their huge number of parameters and the uncertain choices of experts, particularly in resource-constrained environments. Recent research has addressed these challenges through novel architecture designs, model compres-sion techniques, and efficient inference engines. [53] proposes DeepSpeed-MoE, an end-to-end solution for training and deploying large-scale MoE models. [54] introduces EdgeMoE, an on-device inference engine designed for MoE-LLMs that loads popular experts to GPU to prioritize memory and com-putational efficiency. [55] proposes a novel expert offloading strategy utilizing the intrinsic properties of MoE-LLMs. It uses Least Recently Used (LRU) caching to store experts since certain experts are reused across consecutive tokens. It also accelerates the loading process by predicting the selection of experts for future layers based on the hidden states of earlier layers.\nSeveral studies have focused on developing efficient serving systems and inference engines for MoE-LLMs. [56] introduces MOE-INFINITY, an efficient MoE-LLMs serving system that implements cost-efficient expert offloading through activation-aware techniques, significantly reducing latency overhead and deployment costs. [57] proposes Fiddler, a resource-efficient inference engine that orchestrates CPU and GPU collabora-tion to accelerate the inference of MoE-LLMs in resource-constrained settings. Furthermore, [58] introduces Pre-gated MoE, an algorithm-system co-design approach that employs a novel pre-gating function to enhance the inference ca\u0440\u0430-bilities of MoE-LLMs by enabling more efficient activation of sparse experts and reducing the memory footprint. These advancements in MoE-LLMs deployment and inference en-gines demonstrate the ongoing efforts to make these powerful models more accessible and efficient across various computing environments."}, {"title": "C. Communication Optimization", "content": "In addition to computation and memory optimization, com-munication overhead is another major challenge in deploying large models within the edge-cloud environment. The hetero-geneity of edge devices and communication channels necessi-tates a scheduler for computation and network resources when executing FM tasks across different devices.\n[59] introduces an \u201cIntelligence-Endogenous Management Platform\" for Computing and Network Convergence (CNC), which efficiently matches the supply and demand within a highly heterogeneous CNC environment. The CNC brain is prototyped using a deep reinforcement learning model. It theo-retically comprises four key components: perception, schedul-ing, adaptation, and governance, collectively supporting the entire CNC lifecycle. Specifically, Perception perceives the incoming service request and real-time computing resources. Scheduling assigns the workload to heterogeneous computing nodes in a heterogeneous network. Adaptation is adapting to dynamic resources by ensuring the continuity of services through backup measures, and Governance is the self-governed decentralized computing nodes.\n[60] discusses the integration of LLMs into 6G vehicular networks, focusing on the challenges and solutions related to computational demands and energy consumption. It proposes a framework where vehicles handle initial LLM computations locally and offloads more intensive tasks to Roadside Units (RSUs), leveraging edge computing and 6G networks' ca-pabilities. The authors formulate a multi-objective optimiza-tion framework that aims to minimize the cumulative cost incurred by vehicles and RSUs in processing computational tasks, including the cost associated with communication. Lin-guaLinked [61] is a system to deploy LLMs on distributed mobile devices. The high memory and computational demands of these models typically exceed the capabilities of a single mobile device. It utilized load balancing and ring topology to optimize the delay of computation and communication\""}, {"title": "D. Integrated Frameworks", "content": "With the emergence of computational and memory op-timization methods, numerous integrated frameworks have integrated these techniques, supporting various LLMs and lowering the barriers to LLM deployment. These frameworks leverage CPU and GPU backends, enabling the widespread local execution of LLMs.\nThe most popular framework to deploy LLMs at the edge is the llama.cpp [66], which pioneered the open-source imple-mentation of LLM execution using only C++. We provide an overview of various open-source frameworks and their respec-tive features in Table II. The frameworks can be categorized into heterogeneous and GPU-only backends, ranging from mobile and embedded devices to high-end computing systems. Different suppliers offer specialized development platforms and APIs for their hardware products (i.e., CPU and GPU). CUDA (Compute Unified Device Architecture) is a parallel computing platform developed for NVIDIA GPU. ROCm (Radeon Open Compute platform) is a software platform for high-performance computing using AMD GPU. Metal is a low-overhead hardware-accelerated 3D graphic and compute shader API developed by Apple for its own GPUs. Some frameworks like Vulkan and OpenCL (Open Computing Lan-guage) facilitate development across different hardware and operating systems. Vulkan is an API for cross-platform access to GPUs for graphics and computing. OpenCL is a frame-work for writing programs that execute across heterogeneous backends, including CPU, GPU, FPGA, and other hardware.\nRecently, several novel frameworks have been specifi-cally designed for the deployment of LLM-based applica-tions/agents. These frameworks provide abstract interfaces that facilitate the design of complex LLM-based applications, such as chain-of-thought reasoning and retrieval-augmented LLMs. LangChain is a powerful framework aimed at simplifying the development of applications that interact with LLMs [77]. It offers a set of tools and abstractions that enable developers to construct complex, dynamic workflows by chaining various operations, such as querying a language model, retrieving documents, or processing data. Parrot is a serving system for LLM applications that optimizes performance across multiple requests [78]. It introduces the concept of a semantic variable to define the input or output information of LLM requests. To enhance system performance, Parrot schedules requests based on a predefined execution graph and latency sensitivity, and it shares the key-value cache among requests to accelerate the prefilling process. SGLang is another LLM agent framework designed to efficiently execute complex language model pro-grams [79]. It simplifies the programming of LLM applications by providing primitives for generation and parallelism control. Additionally, it accelerates the execution of these applications by reusing key-value caches, enabling faster constrained de-coding and designing API speculative execution."}, {"title": "III. RESOURCE ALLOCATION AND PARALLELISM", "content": "Edge-cloud computing leverages strong cloud servers and distributed edge devices to handle tasks near the data source. As shown in Figure 4, adaptive resource allocation is crucial for achieving an optimal balance between system performance and cost in edge-cloud environments. Resource management facilitates the optimal utilization of system resources, includ-ing processing power, storage space, and network bandwidth. Adaptive algorithms are designed to automatically adjust resource configurations based on real-time workloads and environmental changes, ensuring optimal performance under varying conditions."}, {"title": "A. Resource Allocation and Scaling", "content": "Despite their numerous advantages, several challenges also arise in edge-cloud environments. 1) In edge-cloud environ-ments, resources need to be allocated and managed between the central cloud and multiple edge nodes. Distributed re-source management may increase the system's complexity and require more fine-grained scheduling and coordination mechanisms. 2) The load in edge computing scenarios is highly dynamic. Accurately predicting this query load and dynamically adjusting resource allocation demands is difficult. 3) To meet the real-time processing requirements, the system must quickly adapt to changes in execution environments and adjust processing strategies and resource allocations in time. 4) Modern computing environments provide a variety of computing resources (CPU, GPU, TPU, etc.). Optimizing the allocation of these heterogeneous resources for various models based on their computational requirements and current workloads is a complex task.\nWe have summarized previous research works on resource allocation and adaptive optimization in Table III. The collected research works are divided into two categories: optimization for model services in cloud environments and optimization for IoT applications in edge computing environments. 1) Cloud Environments. This task is to efficiently deploy, manage, and scale machine learning models in the cloud. The objective is optimizing resource usage, reducing computing costs, and achieving performance goals in the face of dynamic and unpre-dictable query loads. This includes resource provisioning al-gorithms and strategies for utilizing heterogeneous computing resources. 2) Edge Environments. Because of the unique archi-tectures and application scenarios in edge environments, data needs to be processed closer to the user or the geographic loca-tion of the data source. This requires well-designed scheduling algorithms and fault tolerance and resilience mechanisms to minimize latency and mitigate network overhead.\n1) Cloud Environments. The following articles present various advanced solutions to optimize resource allocation in the cloud. Clipper uses model containers to encapsulate the model inference process in a Docker container [80]. Clipper supports replicating these model containers across the cluster to increase the system throughput and utilize additional hardware accelerators for serving. MArk is a generic inference system built on Amazon Web Services. It utilizes serverless functions to address service delays with horizontal and vertical scaling. Horizontal scaling expands the system by adding more hardware instances, while vertical scaling expands the system by increasing the resources of a single instance [81]. MArk uses a Long Short-Term Memory (LSTM) network for multi-step workload prediction. Leveraging the workload prediction results, MArk determines the instance type and quantity required to meet the SLOs using a heuristic approach. Nexus adopts the squishy bin packing method to batch different types of tasks on the same GPU, enhancing resource efficiency by considering the latency requirements and execution costs of each task [82]. It also merges multiple tasks into the same GPU execution cycle as long as the latency constraints are not violated. InferLine utilizes a low-frequency planner and a high-frequency tuner to manage the machine learning prediction pipeline effectively [83]. The low-frequency combinatorial planner finds the cost-optimal pipeline configuration under a given latency SLO. The high-frequency auto-scaling tuner monitors the dynamic request arrival pattern and adjusts the number of replicas for each model. Clockwork achieves an adaptive resource manage-ment framework through a fine-grained central controller over worker scheduling and resource management [84], [102]. DNN workers pre-allocate all GPU memory and divide it into three categories: workspace, I/O cache, and page cache. This can avoid repeated memory allocation calls and improve pre-dictability. To cope with changes in query load, INFaaS adopts two automatic scaling mechanisms: vertical auto-scaling at the model level and horizontal auto-scaling at the Virtual Machine (VM) level [85]. The model auto-scaling is handled by the Model-Autoscaler, which decides each model variant's scaling operations (replication, upgrade, or downgrade) by solving an integer linear programming (ILP) problem. The vertical auto-scaling adds a new VM if the utilization of any hardware resource exceeds a configurable threshold. Morphling utilizes model-agnostic meta-learning techniques to effectively navi-gate the high-dimensional configuration space, such as CPU cores, GPU memory, GPU timeshare, and GPU type [86]. It can significantly reduce the cost of configuration search and quickly find near-optimal configurations. Cocktail designs a resource controller to manage CPU and GPU instances in a cost-optimized manner and a load balancer to allocate queries to appropriate instances [87]. It also proposes an autoscaler that leverages predictive models to predict future request loads and dynamically adjusts the number of instances in each model pool based on the importance weight of the models. Kairos designs a query distribution mechanism to intelligently allocate queries of different batch sizes to different instances in order to maximize throughput [88]. Kairos transforms the query distribution problem into a minimum-cost bipartite matching problem and uses a heterogeneity coefficient to represent the relative importance of different types of instances. This allows for better balancing of the resource usage of different instance types. SHEPHERD comprises a planner (HERD), a request router, and a scheduler (FLEX) for each serving group [89]. HERD is responsible for partitioning the entire GPU cluster into multiple service groups. Besides, it performs periodic planning and informs each GPU worker of their designated service group and the models it must serve. SpotServe is a serverless LLM system that adjusts the GPU instances and updates the parallelism strategy flexibly [90]. It uses a bipar-tite graph matching algorithm (Kuhn-Munkres algorithm) to orchestrate model layers to hardware devices, thus maximizing the reusable model parameters and key-value caches."}, {"title": "2) Edge Environments", "content": "Given the limited and heterogeneous nature of edge computing resources, efficient management and scaling of these resources are crucial. The following papers present efficient solutions to improve the performance and efficiency of edge systems. To optimize resource allocation and interference management, [91] proposes a resource allocation scheme with Lagrangian and Karush-Kuhn-Tucker conditions. Based on the number of associated IoT end devices (IDs) and the queue length of the edge gateways (EGs), this scheme calculates the optimal resource allocation parameters and allocates resource blocks to each EG. A decentralized resource management technique is proposed in [92] to deploy latency-sensitive IoT applications on edge devices. The key design is the deployment policy module, which is responsible for finding a task allocation scheme that meets the application's require-ments based on the bids from the bidder nodes and deploying the application to the corresponding edge nodes. The authors in [93] introduce a novel method, called MDSA, to address the challenge of joint model partitioning and resource allocation for latency-sensitive applications in mobile edge clouds. They employ the task-oriented online scheduling method, COS, to collaboratively balance the workload across computing and network resources, thereby preventing excessive wait times. [94] designs a DRL algorithm to determine whether a task needs to be offloaded and to allocate computing resources efficiently. Tasks generated by mobile user equipment (UE) will be submitted to the task queue wait for execution. The algorithm models the task queue as a Poisson distribution. Then it uses the DRL method to select the suitable computing node for each task, learning the optimal strategy during the algorithm training process. Authors of [95] formulate the resource allocation problem using a Markov decision process model. They enhance the DQN algorithm by incorporating multiple replay memories to refine the training process. This modification involves using different replay memories to store experiences under different circumstances. The CE-IoT is an online cloud-edge resource provisioning framework based on delay-aware Lyapunov optimization to minimize operational costs while meeting latency requirements of requests [96]. The framework allows for online resource allocation decisions without prior knowledge of system statistics. The authors of [97] propose a dynamic optimization scheme that coordinates and allocates resources for multiple mobile devices in fog computing systems. They introduce a dynamic subcarrier allo-cation, power allocation, and computation offloading scheme to minimize the system execution cost through Lyapunov optimization. LaSS is a platform designed to run latency-sensitive serverless computations on edge resources [98]. LaSS employs model-driven strategies for resource allocation, auto-scaling, fair-share allocation, and resource reclamation to efficiently manage serverless functions at the edge. In [99], the authors discuss resource provisioning and allocation in FaaS edge-cloud environments, considering decentralized approaches to optimize CPU resource utilization and meet re-quest deadlines. This paper design resource allocation and configuration algorithms with varying degrees of centralization and decentralization. KneeScale optimizes resource utilization for serverless functions by dynamically adjusting the number of function instances until reaching the knee point, where increasing resource allocation no longer provides significant performance benefits [100]. KneeScale utilizes the Kneedle algorithm to detect the knee points of functions through online monitoring and dynamic adjustment. CEC is a containerized edge computing framework that integrates workload prediction and resource pre-provisioning [101]. It adjusts the resource allocation of the container through the PRCT algorithm to achieve zero steady-state error between the actual response time and the ideal response time."}, {}]}