{"title": "Efficient Synaptic Delay Implementation in Digital Event-Driven AI Accelerators", "authors": ["Roy Meijer", "Paul Detterer", "Amirreza Yousefzadeh", "Alberto Pati\u00f1o-Saucedo", "Guanghzi Tang", "Kanishkan Vadivel", "Yinfu Xu", "Manil-Dev Gomonyb", "Federico Corradi", "Bernab\u00e9 Linares-Barranco", "Manolis Sifalakis"], "abstract": "Synaptic delay parameterization of neural network models have remained largely unexplored but recent literature has been showing promising results, suggesting the delay parameterized models are simpler, smaller, sparser, and thus more energy efficient than similar performing (e.g. task accuracy) non-delay parameterized ones. We introduce Shared Circular Delay Queue (SCDQ), a novel hardware structure for supporting synaptic delays on digital neuromorphic accelerators. Our analysis and hardware results show that it scales better in terms of memory, than current commonly used approaches [1]-[3], and is more amortizable to algorithm-hardware co-optimizations, where in fact, memory scaling is modulated by model sparsity and not merely network size. Next to memory we also report performance on latency area and energy per inference.", "sections": [{"title": "I. INTRODUCTION", "content": "In biological neural networks synaptic delays are known to be subject to learning and play an important role in the propagation and processing of information [4]. Hence, configurable delays are a basic feature offered in many neuromorphic neural-network accelerators [1]\u2013[3], [5]. However, up until recently synaptic delays have rarely been used in practice, because (i) it has been unclear how to parameterize and optimize them in models and (ii) because their use has been assumed expensive and in terms of memory and complicated in digital accelerators.\nInterestingly, in the last couple of years, progress has been made with effectively training models parameterized with synaptic delays [6]\u2013[10], which shows that these models are more compact, and have simpler structure [6], [10], while achieving comparable or higher performance [6], [8], than their counterparts with memoryless synapses. They have also been observed to exhibit sparser activation patterns [6]. This raises the question whether the compactness and sparseness of the model outweigh the speculated memory, energy, and latency costs of the synaptic delay implementation in hardware. The study in [6] confirmed that this appears to be indeed the case when comparing the memory and energy estimates of the models with and without synaptic delays. These estimates were obtained by considering synaptic activity in recurrent and feed-forward models trained with and without synaptic delays, and when using common hardware structures for synaptic delays that are used in Loihi [2] and TrueNorth [1].\nThe questions that emerged and motivated this work, concern: (i) the scalability of delay structures as model size and complexity grow; and (ii) the efficiency of incorporating synaptic delay structures in hardware, considering factors such as memory, IC area and inference latency, which are relevant in edge AI applications.\nIn this paper we introduce a new synaptic delay structure and its implementation in a digital circuit design for multicore neuromorphic accelerators. It is based on a circular shared-queue and its operation is leveraged by a simple extension of the Address Event Representation (AER) packet format [11], commonly used in neuromorphic systems for communicating events. Compared to existing approaches [1]\u2013[3], our approach is more memory and area efficient because its overhead does not increase with the size of the network models, but rather with the neural network's activation density. This activation density can be minimized during training, thus, allowing for a hardware-algorithm co-optimization. Importantly, the introduction of synaptic delays has minimal impact on power, area, and energy requirements of the accelerator.\nWe have evaluated our hardware implementation on the Seneca neuromorphic platform [12]. We report on inference fidelity, hardware metrics (energy per inference, latency, area), and some comparisons with other reference digital neuromorphic systems that implement delays [1], [2]."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "In Spiking Neural Networks (SNNs), information is conveyed through pulses that travel between the presynaptic and postsynaptic neurons. These pulses encode a specific weight associated with the synaptic connection, and if there are synaptic delays incorporated into the model, these pulses require some time to propagate from the presynaptic neuron to the postynaptic one.\nIn [6], Patino et al. introduced a simple (i.e. hardware-friendly) framework for parameterizing and training models with synaptic delays, whereby for each discrete delay level D, there is an additional synapse between each presynaptic neuron and postynaptic neuron pair, each with a unique trainable weight. Spikes from presynaptic neurons are received by postynaptic neurons over D discrete time steps, with varying weights. The number of delay levels determines how many times a postynaptic neuron receives the same repeated spike from a presynaptic neuron. In this case, delay parameterization involves extending the weight matrix structure from 2D to 3D (2D weight matrix for each delay value). It has dimensions $D \\times I \\times J$, where D is the delay resolution (that is, the number"}, {"title": "III. SYNAPTIC DELAY ARCHITECTURE", "content": "A. Shared Circular Delay Queue Structure\nThe proposed delay structure draws inspiration from the shared queue model in [1] and incorporates the concept of \"double-buffering\". Instead of a linear chain of FIFOs (one per delay level), it simplifies the design to only two FIFOs: the pre-processing queue (PRQ) and the post-processing queue (POQ). These two FIFOs are interconnected in a circular arrangement (see Figure 2), which is why we refer to this configuration as the \"Shared Circular Delay Queue\" (SCDQ) in this paper.\nThere is an SCDQ module at every compute core of a neural network accelerator. Conceptually it is positioned between a presynaptic and a postsynaptic layer, with the input of the SCDQ queue facing the presynaptic layer and the output of the SCDQ facing the postsynaptic layer, and is shared across all synapses between neuron pairs. If multiple layers can be mapped in the same compute core then an SCDQ can also be shared across layers. As a result the memory complexity is not a function of the number of layers in a model, but rather a function of the number of compute cores in use (and the aggregate activation traffic).\nWhen an event packet (AER) is received at the SCDQ input, a delay counter will be added to it, which counts the maximum number of algorithm timesteps by which the event should be delayed. The event then traverses the PRQ and arrives at the output if delivery to some postsynaptic neuron is expected at the current algorithm timestep. It will (also) be pushed to the POQ if it is expected to be delayed for delivery at a future timestep. At the POQ the delay counter of events is decremented for one timestep. At some point, a special event packet signaling the end of the algorithmic timestep intercepted at the output, will trigger a buffer-swap between POQ and PRQ. When this happens, the events previously held in POQ will be propagated through PRQ towards the output (if their delay counter has reached a delay value due for delivery in this new timestep) and/or will be pushed back again to the POQ for further delay and delivery in a future timestep. When the delay counter of an event expires, this event will no longer be pushed to the POQ and will only be forwarded to the output.\nIt is important to note that by contrast to its predecessor [1], SCDQ makes it possible for an event to \u201corbit\" and exit the queue at several timepoints. This way it can support event delays not only per axon but also per axon-dendrite pair (i.e. per individual synapse), which we will discuss further in sectionIV.\nSCDC consists of six functional submodules shown in the block diagram of Figure ??: the write controller and read controller which get triggered by external transactions, the PRQ and POQ, the pruning filter, and memory. The pruning filter and memory will be discussed in more detail in sections III-B and IV-C respectively. The write controller is responsible for reading the events from the input and writing them to the PRQ and adding to them an initialized delay counter value. Additionally, it is responsible for swapping the PRQ and POQ buffers when a special end-of-timestep (EOT) event"}, {"title": "B. Zero-Skipping Delay-Forwarding", "content": "One observes in Figure 1 that the network structure can become quite intricate due to the presence of numerous synapses. The number of synapses increases linearly with the maximum number of delay steps, D, which we aim to accommodate. In practical scenarios, weight training, quantization, and the potential use of suitable regularization techniques, can efficiently remove a substantial number of synapses [6]. This results in a much sparser or pruned model that is well suited for effective inference in applications.\nIn a zero-skipping capable accelerator (typically most neuromorphic, but also several conventional accelerators) a zero weight or activation will skip loading weights from memory and carrying out the partial MAC, thereby saving energy and likely improving latency. When accommodating synaptic delays this decision is to be taken after an event exits the shared delay queue and is about to be received by the postsynaptic neurons. This can unnecessarily occupy FIFO memory, consume power for repeated lookups in weight-memory, and affect latency in SCDQ; if some or all of the postsynaptic neuron connection have zero weights.\nTo reduce this overhead a zero-skiping delay-forwarding capability is implemented in the pruning filter of SCDQ. A binary matrix called WVU (which stands for \"weight value useful\") is maintained locally, where every element represents a delayed axon. WVU has two dimensions: dimension I representing every presynaptic neuron, and dimension D representing every delay level. If $WVU_{i,d}$ = 1, this means that for the current delay value d and the presynaptic neuron address i, at least one of the weights is a non-zero value. If $WVU_{i,d}$ = 0, this means that for all i and d, the weights are 0 (which implies that event delivery can be skipped in this timestep). Using this method, the SCDQ can determine for every event in the PRQ if it is useful for the postsynaptic layer (or if it should be ignored).\nTo determine when an event should be removed from the delay queue (i.e. every next timestep of the event, $WVU_{i,d}$ will be 0), a row of WVU, which corresponds to some presynaptic neuron i, can be treated as a binary number. If we count the number of leading zeros (clz(x)) of this binary number, we know for which delay counter value d the event does not have to move from the PRQ to the POQ anymore."}, {"title": "C. Hardware IP in Seneca Neuromorphic processor", "content": "To evaluate the SCDQ structure, we implemented a hardware IP block (Delay IP) and integrated it in the Seneca [12] neuromorphic processor. We also implemented a software version of it that could be run on the RISC-V controller of a Seneca core (NCC).\nSeneca is a fully digital time-multiplexed multicore accelerator similar to Loihi [2], SpiNNaker [3] and TrueNorth [1].\nWhile SpiNNaker and Loihi use ring buffers to support synaptic delays, TrueNorth and Seneca (through the Delay IP of SCDQ) use variations of the general shared queue structure explained in section II.\nA simplified version of a Seneca core (NCC) [12] is shown in Figure 4. It includes a RISC-V controller of pre- and post-processing events (multiplexing/demultiplexing them from/to neurons), the Axon Message Interface (AMI) for input/output buffering of events, a vector of 8 Neuro-Processing Elements (NPEs) that execute SIMD operations for neurons and other functions in parallel, local SRAM memory for storing neuron and other state variables. Typically, each layer of a neural network model is mapped to one or more NCCs, although if a layer is sufficiently small, then multiple layers can also be mapped onto one NCC. NCCs communicate through a Network-on-Chip (NoC). The RISC-V controller is programmable in C, while NPE are programmable in Assembly-style microcode.\nIn this architecture and for the testing purposes of this work, the Delay IP has been integrated at every core between the AMI and the NoC (Figure 4). A full and more compact integration could make it part of the AMI or part of the NoC since there are queues already in both components, thereby leading to more memory/latency improvements (than what is shown in the results)."}, {"title": "D. Experimental Setup", "content": "The models used for the tests in this section undergo initial training and testing in PyTorch. Next, they are mapped to Seneca (bfloat16 quantization) and Loihi (int8 quantization) for measuring latency, energy consumption, area utilization, and memory usage.\nSince Seneca is not taped out yet, we have estimated its power consumption by using power estimations based on a netlist provided by Imec, in a 22 nm process from Global Foundry. Energy and area measurements are acquired using the Cadence Joules and Genus tools, respectively. The Joules tool provides accuracy within 15% of the sign-off power. The Genus tool is used to estimate the area. Latency, memory, and fidelity measurements are obtained from hardware test bench simulations. The same models are run in PyTorch to assert fidelity between hardware and software.\nWe trained three different SNN models featuring LIF neu-rons, on the Spiking Heidelberg Digits benchmark [14] (input size 700, and 20 output classes). To optimize simulation time, we carefully designed a test set comprising 100 unique data points, allowing us to individually assess each class with the networks we examined.\nEach model consists of four layers of neurons, with synaptic delays between each pair of layers, except for the first layer (input-hidden1). The structure of the neuron/layer configurations for these three models is as follows: 700-48-48-20, 700-32-32-20, and 700-24-24-20. We set a maximum delay of 60 discrete time steps with a stride of 2, i.e. pruning every second delay axon. Furthermore, we applied per-synapse delay pruning during training, based on the approach outlined in [6], effectively reducing the number of synapses to only 15 per pair of neurons.\nFor the 700-48-48-20 model, we also explored a variation involving axonal-only delay pruning. This means that instead of removing individual synapses, we removed entire groups of synapses per axon until only 15 delay axons remained (700-48-48-20 Ax). The goal here was to understand the tradeoff in hardware resource savings between these two pruning strategies.\nFor each model on Seneca, we occupied 3 NCCs, one projection per core (as shown in Figure 4), while on Loihi we could fit each model in 1 core. Additionally, in Seneca, tests were done with the Delay IP and without the Delay IP (where the SCDQ was implemented in C on the RISC-V controller)."}, {"title": "IV. RESULTS", "content": "A. Software-Hardware Model Fidelity\nTable I reports on the fidelity between the software run model and the hardware model deployed on Seneca. The results confirm the consistency between PyTorch, and Seneca.\nThis consistency is further visualized in Figures 5 and 6, which show for one data point the evolution of the neuron activations and the neuron membrane state. The accuracy of 48-48-20 Ax (axonal delay pruning) is decreased because of the axon grouping constraint that does include in an axon group the 15 smallest weights. However, it will show improvements in energy and latency.\nB. Energy, Latency & Memory\nTable II reports (average) hardware measurements per inference in Loihi, in Seneca with Delay IP, and without Delay IP. Energy is calculated using dynamic power consumption, to exclude the high static power consumption of the Latermont of Loihi of 74.4 mW. The static power consumption of Seneca is 462 \u03bcW. Comparing the middle and right columns justifies the circuit-based implementation of SCDQ as it improves energy efficiency and latency by a significant amount. The energy efficiency increases between 3.5\u00d7 (48-48-20) and 3.1\u00d7 (24-24-20). Similarly, latency decreases between 4.3\u00d7 (48-48-20) and 3.5\u00d7 (24-24-20). In other words, the benefits increase with the total activations volume (seen in larger networks). For the axonal pruned network (48-48-20 Ax), both energy efficiency and latency improve by approximately 1.4\u00d7 compared to the synaptic pruned network (48-48-20). This is also expected since the activation sparsity is higher for 48-48-20 Ax than for 48-48-20.\nIn comparison to Loihi (Ring Buffer), the energy efficiency of Seneca (SCDQ) appears approximately 1.2 \u2013 1.9\u00d7 lower, but note that this involves 1 core in Loihi versus 3 cores on Seneca. However, the gap is closing as the network size increases and that is justified by the costly contribution of the RISCV controller (the larger the network, the more compute work can be allocated to the NPEs).\nIn Seneca Delay IPs contribute only 2% - 3% to this energy consumption, with the remaining 97% - 98% coming from the NCCs (of which 67% - 82% of this energy consumption is related to pre- and post-processing events in RISC-V cores for networks with Delay IPs, and 86% - 88% for the networks without Delay IP).\nWhat is more interesting for the role of SCDQ is the inference latency. On Seneca, both with the circuit implementation and the RISC-V implementation of SCDQ, the latency decreases with the network size (due to less activations), while it, surprisingly, remains constant on Loihi's Ring Buffers. And for the circuit implementation (Delay IP), it is consistently less than 0.5\u00d7 that of Loihi, despite the larger number of cores used and routing between them.\nIn Section II, we touched upon the better memory scalability of a Shared Delay Queue compared to the Ring Buffer implementation of synaptic delays. This is also attested in [6]. To illustrate how SCDQ improves upon the vanilla Shared Delay Queue structure [1], in Figure 7, we provide a worst-case estimate (assuming dense activations and no delay pruning) between two fully connected layers. The figure illustrates the buffer capacity required for both a Shared Delay Queue and SCDQ, as the number of delay levels increases and as the number of postsynaptic neurons increases. It is evident that SCDQ exhibits superior scalability (next to being also more flexible).\nC. Area Optimizations\nTaking $EDAP = E \\cdot L \\cdot A$ as a performance metric that combines energy per inference E, latency per inference L, and area A, where lower means better, we find that for the 48-48-20 model *, Seneca achieves 43.6\u00b5J\u00b7 4.25ms \u00b7 1.58mm\u00b2 \u2248 2.93\u00b710-13 Jsm\u00b2.\nIn the initial Delay IP implementation of SCDQ, the memory block in Figure 4 was implemented with flip-flops. To reduce the total area, flip-flop memory was replaced with a 2048-word 16-bit SRAM memory, with enough capacity for the observed maximum of 1596 events in the queues. This resulted in an 81% reduction in total area, with a total of 15463 \u00b5m\u00b2 for the Delay IP (see Table III). In this case EDAP for Seneca 43.6\u00b5J 4.25ms \u00b7 1.45mm\u00b2 \u2248 2.68 \u00b7 10-13 Jsm\u00b2 for the 48-48-20 network. With this area optimization the Delay IP reduces from $\\frac{81624}{472400}$ = 17% of the area one Seneca core down to a mere $\\frac{15463}{472400}$ = 3% only of the size of one core."}, {"title": "V. DISCUSSION AND FUTURE WORK", "content": "Seneca [12] as an event-driven processor uses source-routing of event packets (AER) from a presynaptic neuron to postsynaptic neurons. A presynaptic event is thus broadcast to all postsynaptic neurons after exiting the SCDQ. As a result, and due to the way we model synaptic delay levels using separate axons [6], pruning a single connection to a postsynaptic neuron (synapse) at the model will reduce neither incoming traffic nor outgoing traffic (and memory use) in SCDQ. That is, an event from a presynaptic neuron will remain in the queue for delivery to the remaining synapses at other delay steps. However, if all synapses of an axon are pruned, there is no recipient neuron for an event at a certain delay step. In this case, the event can be dropped earlier from the SCDQ, reducing memory use. For this reason, axonal pruning may also reduce latency (and energy). This is confirmed in TableII and also in Figure 8, where dark dots represent the times that events are fired from presynaptic neurons and lighter colored dots represent the lifespan of these events in the SCDQ thereafter (fading based on the counter values).\nIn Seneca, it is possible to share a core across network layers, which would mean that more than one layer can in fact share the same Delay IP. If there is no barrier synchronization between layers (EOT events), this is likely to further improve inference latency in an event-driven accelerator and also improve the hardware efficiency of the Delay IP. But to maintain task performance of the model in this case requires a more sophisticated training procedure and algorithm-hardware co-optimizations, which is left for future work.\nSCDQ has memory overhead of $a \\cdot I \\cdot (2 \\cdot D - 1)$, scaling with O(\u03b1\u00b7 \u0399\u00b7 D). By reference to TrueNorth's [1] constraints of 16 timesteps of delay, 256 neurons, and for sparsity a = 1 (no spasity), TrueNorth's Shared Delay Queue needs queue capacity for 34816 events, while SCDQ only requires storage for 7936 events. Each event in the Delay IP instantiation of SCDQ has a bit width of 16 bits. With reference to Loihi's [2] constraints of 64 delay time steps, 48 neurons, and 8 bits per weight, the additional memory overhead of Loihi is only 24576 bits, which is less than the 97536 bits of the Delay IP. However, this comparison assumes no activation sparsity (a = 1), and the memory overhead of Loihi does not scale with activation sparsity. If $a \\le 0.25$, the SCDQ has a lower memory overhead than Loihi. SpiNNaker [3] has a constraint of 16 time steps of delay, 256 neurons, and 16-bit weights per neuron, resulting in a memory overhead of 65536 bits for the Ring Buffers. Assuming a = 1, the SCDQ has a memory overhead of 126976 bits in this case and when $a \\le 0.5$, SCDQ is more memory efficient than SpiNNaker.\nTo further improve the memory efficiency of the SCDQ, the implementation can be modified to use only one FIFO instead of two and to store pointers to shared delay counters in events instead of the delay counter itself. Only the D delay counters need to be maintained in this case. Utilizing a single FIFO can be achieved by writing incoming events to the FIFO and reading them multiple times until they are no longer used. At the end of a timestep, only the read and write pointers need to be modified according to the delay values. This improvement is currently tested in software and seems to improve memory efficiency by approximately 2x, reducing the total memory requirements to $a \\cdot I \\cdot D$ (previously $a \\cdot I \\cdot (2 \\cdot D \u2212 1)$)."}, {"title": "VI. CONCLUSION", "content": "This work introduces SCDQ, a novel hardware structure for enabling synaptic delays on neuromorphic accelerators. It builds upon and improves previous designs and approaches [1]\u2013[3]. It scales better in terms of memory and area, and is more amortizable to algorithm-hardware co-optimizations; in fact, memory scaling is modulated by model sparsity and not merely network size. It has negligible energy overhead and halves inference latency."}]}