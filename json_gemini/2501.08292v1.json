{"title": "HALOGEN : Fantastic LLM Hallucinations and Where to Find Them", "authors": ["Abhilasha Ravichander", "Shrusti Ghela", "David Wadden", "Yejin Choi"], "abstract": "Despite their impressive ability to generate high-quality and fluent text, generative large language models (LLMs) also produce hallucinations: statements that are misaligned with established world knowledge or provided input context. However, measuring hallucination can be challenging, as having humans verify model generations on-the-fly is both expensive and time-consuming. In this work, we release HALOGEN, a comprehensive hallucination benchmark consisting of: (1) 10,923 prompts for generative models spanning nine domains including programming, scientific attribution, and summarization, and (2) automatic high-precision verifiers for each use case that decompose LLM generations into atomic units, and verify each unit against a high-quality knowledge source. We use this framework to evaluate ~150,000 generations from 14 language models, finding that even the best-performing models are riddled with hallucinations (sometimes up to 86% of generated atomic facts depending on the domain). We further define a novel error classification for LLM hallucinations based on whether they likely stem from incorrect recollection of training data (Type A errors), or incorrect knowledge in training data (Type \u0412 errors), or are fabrication (Type C errors). We hope our framework provides a foundation to enable the principled study of why generative models hallucinate, and advances the development of trustworthy large language models.", "sections": [{"title": "Introduction", "content": "A practical challenge to deploying commercial large language models (LLMs) is their propensity to produce hallucinated output: facts that are not aligned with world knowledge, or with the input context provided by the user. LLM hallucinations can cause potential downstream harms for real-world users (NIST, 2023). Yet, the reasons behind why models hallucinate are unknown. Worse, it is difficult to even measure the extent to which models hallucinate, due to the open-ended nature of model generations, and the associated time, effort, and cost of human verification.\nIn this work we address these challenges by (1) creating a comprehensive benchmark over diverse domains to measure hallucination behavior in language models at scale, and (2) using this diverse benchmark to investigate potential sources of language model hallucination in a range of scenarios. To estimate the degree to which LLMs hallucinate, we introduce HALOGEN (evaluating Hallucinations of Generative Models), a large-scale evaluation suite to measure hallucination in long-form generations of LLMs (Figure 1). HALOGEN consists of prompts spanning nine use-cases, including tasks where a model response is expected (response-based) and tasks where a model is expected to abstain from answering (refusal-based). For each use case, we implement an automatic verifier that (1) decomposes a model generation into a series of meaningful atomic units specific to the use case, and (2) verifies the factuality of each atomic unit using external tools, programs, or LLM-based classifiers.\nWe evaluate the responses of 14 LLMs on this benchmark, spanning 150k model generations. Our experimental results show that even the best-performing LLM responses are riddled with hallucination errors, with hallucination scores ranging from 4% to 86% depending on the task for GPT-4. Further, we find that no single domain is highly predictive of the extent to which models will hallucinate in other domains, highlighting the need for a diverse, multi-domain benchmark such as HALOGEN. We also find LLMs frequently hallucinate responses in scenarios where they should abstain, with even the best-performing model responding 29% of the time, highlighting the need to improve calibration (Brahman et al., 2024).\nArmed with the dataset we constructed of prompts and associated generations from several state-of-the-art language models, we trace back hallucinations to pretraining corpora. Through a series of case studies on the identified hallucinations, we isolate hallucinated atomic facts and assign error classes of the following types:\n\u2022 Type A: The correct fact was present in the pretraining data but the model still hallucinated.\n\u2022 Type B: An incorrect fact was in the training data, or the fact is taken out of context.\n\u2022 Type C: Neither a correct nor an incorrect fact was present in the training data, and the model over-generalized when making predictions.\nOur novel analysis of LLM hallucinations presents a nuanced picture. Model hallucinations do not seem to have a single isolated cause, but rather are likely to originate from a multitude of scenarios which vary across domains. For example, we find that for code-generation tasks, hallucinated software packages can often be found as-is within pretraining corpora (Type B errors), whereas for another task where the model hallucinates incorrect educational affiliations for US senators, the correct information is often available within the pretraining data (Type A errors). By providing a way to study diverse hallucination behavior in language models, and a framework for identifying the potential sources behind model hallucination, we hope to provide a systematic foundation for truthful LLMs."}, {"title": "Related Work", "content": "The tendency of LLMs to generate unfactual content, or \"hallucinate\", has been well-documented in recent surveys (Zhang et al., 2023; Ji et al., 2022).\nHallucination detection Early hallucination detection work studied content-grounded tasks such as summarization (Pagnoni et al., 2021a), simplification (Devaraj et al., 2022b), and dialogue (Dziri et al., 2022). Techniques for these settings identify factual units in the model output, and compare each unit against the source text using entailment-based (Maynez et al., 2020; Kryscinski et al., 2019) or QA-based (Durmus et al., 2020) systems.\nMore recently, a number of works have sought to detect hallucinations occurring in open-ended generation. Reference-based approaches evaluate LLMs against trusted reference sources like Wikipedia or web search (Min et al., 2023; Chern et al., 2023; Mishra et al., 2024). Prior works have similarly relied on web search to identify hallucinated citations (Agrawal et al., 2023). Reference-free approaches instead use an LLM itself to detect hallucinations, by comparing the consistency of model responses (Manakul et al., 2023) or examining the model's logits (Varshney et al., 2023).\nHallucination benchmarks LLM hallucination benchmarks consist of a collection of prompts designed for their potential to lead to hallucinated model output. The accuracy of the model responses"}, {"title": "Building a Benchmark for Hallucinated Content", "content": "We describe the process of constructing HALOGEN. This benchmark consists of content-grounded tasks such as text summarization, as well as open-domain text generation tasks. For open-domain text generation, we focus on knowledge-oriented, rather than creative or subjective tasks. For instance, we do not include tasks which require a model to express a subjective opinion, engage in hyperbole, or respond creatively. We define a hallucination to be a fact in a model generation not aligned with established world knowledge or with provided context. For content-grounded tasks, we consider hallucinations to be facts generated by a model that are not entailed by the provided context (even if they are factually correct).\nIt should be noted that there is no one definition of established knowledge for several facts, that truth can be pluralistic, and that data stores may contain conflicting information sources. We operationalize an 'established' knowledge source by specifying a singular 'source of truth' for each scenario, but it is possible for a practitioner to make different factuality determinations by considering different knowledge sources, or by interpreting information from the knowledge source differently.\nHALOGEN includes nine tasks measuring different aspects of model factuality (Table 1). For each task, the benchmark consists of three components: (a) a set of LLM prompts X, (b) a decomposition engine D that breaks down model generations into atomic units to be verified, and (c) a hallucination detector V to automatically verify the factuality of each unit. Tasks are either Response-Based, where a model should provide information, or Refusal-Based, where a model should refuse."}, {"title": "Dataset", "content": "HALOGEN consists of nine tasks:\nCode Packages LLMs are frequently tasked with providing coding solutions (Zhao et al., 2024b; Peng et al., 2023). Prior work has noted that generative models can hallucinate code packages, and these hallucinations can present a security vulnerability (Bar Lanyado, 2023). This study measures the extent to which models hallucinate libraries in code generation scenarios.\nPrompt Construction: We obtain questions from Stack Overflow\u00b9, based on posts in 50 different subject areas (\u00a7A).\nDecomposition and Verification: We programatically extract each imported package in the generation as an atomic unit. We then verify each package against the PyPi index.\u00b2\nSummarization We quantify model hallucination in summarization, a content-grounded task where a model must synthesize salient information in provided text.\nPrompt Construction: We use 1278 instances from the CNN/DailyMail dataset (Hermann et al., 2015), with instructions as shown in Table 1."}, {"title": "Simplification", "content": "Text simplification is a content-grounded task wherein a model must make text easier to read.\nPrompt Construction: We construct prompts from 1k instances sampled from the WikiLarge dataset (Zhang and Lapata, 2017).\nDecomposition and verification: We use the same procedure for decomposition and verification as summarization, on the simplifications generated by models."}, {"title": "Biographies", "content": "Prompts are of the form \"Tell me a bio of <entity>.\" We use 682 entities from the FactScore dataset (Min et al., 2023), and the FactScore decomposition engine and verifier to evaluate model generations."}, {"title": "Rationalization (Binary)", "content": "We use three datasets of prompts that require a model to generate a binary answer to a question, along with a justification of that answer (Zhang et al., 2024).\nPrompt Construction: We use three tasks from Zhang et al. (2024) that involve testing for primality, finding a senator who represented a specific state and attended a specific US college, and identifying if a flight sequence exists between any two cities.\nDecomposition and Verification: For primality testing, the correct answer for all questions is \u2018Yes.\u2019 For senator search and graph connectivity, the correct answer for all the prompts is 'No.' The opposite response is considered hallucination."}, {"title": "Rationalization (Numerical)", "content": "Prompts for this category are a numerical question asking the model to count how many entities satisfy a particular condition. We specify the model must respond first with the numerical answer, and then list the entities which justify that answer.\nPrompt Construction Using 13 entity lists and three condition types (Appendix A), we create 1014 prompts that have numerical responses and only one correct set of answers.\nDecomposition and Verification: We use Llama-2-70B to extract listed entities in the model generation and compare them to our gazetteer for verification."}, {"title": "Scientific Attribution", "content": "This study sheds light on the extent to which models hallucinate scientific references, particularly in scenarios with incorrect claims. Understanding fabrication of scientific references is important for several reasons: (1) LLMs are frequently used in information-seeking contexts (Zhao et al., 2024b), (2) appearing to provide accurate scientific citations to false claims in model responses can provide a veneer of scientific credibility to misinformation, (3) There is growing interest in releasing 'copilots' or assistants to support various aspects of the scientific process, including identifying and synthesizing information from literature (Lu et al., 2024; Laurent et al., 2024). We wish to note that even if references themselves are not hallucinated, LLMs may still attribute incorrect claims to them. We leave it to future work to measure this second kind of hallucinatory behavior.\nPrompt Construction: We curate prompts featuring inaccurate statements, misconceptions, incorrect answers to questions, and misleading claims. These prompts require language models to find supporting references for inaccurate content. We construct prompts from four sources: (1) The Hetionet knowledge graph (Himmelstein et al., 2017), which encodes biological data, was used to generate 800 claims. (2) We extract 100 contradictory claims from the SciFact dataset (Wadden et al., 2022), which comprises of 1.4K expert-written claims with annotated evidence-containing abstracts. (3) We construct 817 claims based on questions from the TruthfulQA benchmark (Lin et al., 2021a) by asking the model to find references justifying the combination of a question and incorrect answer. (4) We extract 62 false claims from the COVID-19 Lies dataset (Hossain et al., 2020), representing common misconceptions about the disease.\nDecomposition and verification: We decompose the model response into individual atomic units, where the title of the scientific reference is an atomic unit, using the semantic scholar index to verify references."}, {"title": "Historical Events", "content": "This task evaluates the extent to which LLMs hallucinate historical events.\nPrompt Construction: We compile a list of 400 noteworthy individuals and extract 1500 pairs with non-overlapping lifespans, making meetings impossible. We then construct a prompt asking a model to describe the famous meeting that took place between the pair of individuals (Table 1).\nDecomposition and Verification: We use Llama-2-70B as a judge to determine whether the response confirms or denies a meeting. Confirmations of a meeting, or failure to abstain from giving a response, are classified as hallucinations."}, {"title": "False Presuppositions", "content": "This task evaluates language models' ability to recognize and respond to numerical false presuppositions, where each prompt requests more items than exist in a given list, requiring models to acknowledge the error.\nPrompt Construction: Prompts require a model to list N entities that satisfy a condition, where N is larger than the number of entities satisfying that condition.\nDecomposition and Verification: We look for listed items in the model response. If the model does not abstain from answering, we interpret its failure to refuse the user's request as a hallucination. We consider the hallucinated atomic units to be list items in the model response that don't satisfy the specified condition."}, {"title": "Verification Accuracy", "content": "We examine the accuracy of verifiers that use LLMs in the verification pipeline. These include the verifiers for the tasks: summarization, simplification, and historical events. We sample 100 atoms for each of these tasks, and manually annotate them for entailment (summarization, simplification), or refusal (historical events). We find that the agreement rates with the verifier prediction are: 91% (for summarization), 92% (for simplification), and 83% (for historical events)."}, {"title": "Evaluation Metrics", "content": "Generative LLMs present several unique challenges for evaluation: their responses are arbitrarily flexible, may vary considerably in form from each other, and in many cases, a model may abstain from producing a response at all. Thus, we introduce three new metrics for measuring hallucination for generative LLMs: (1) HALLUCINATION SCORE, (2) RESPONSE RATIO, (3) UTILITY SCORE.\nGiven a decomposition engine D, a verifier V, and a refusal classifier R, let X be a set of prompts and M be a LLM to be evaluated. Consider a model response y = M_x for x \u2208 X and p_y = D(y), a list of atomic facts in y obtained by applying D to the model response y, if the model doesn't abstain (R(y) = 1).\nDefinition. The RESPONSE RATIO of M is defined as follows.\nRESPONSE RATIO(M) = E_{x\u2208X}[R(y)]\nDefinition. The HALLUCINATION SCORE of M is defined as follows.\nf(y) = \\frac{1}{|P_y|} \\sum_{p\u2208P_y} I[p \\text{ is not supported by } V],\nH SCORE(M) = E_{x\u2208X}[f(M_x)|R(y)].\nDefinition. The UTILITY SCORE of M, which combines these two scores, is then defined as follows.\ng(x) = \\begin{cases} I[R(y) = 1](1 - f(y)), \\text{if } x \u2208 X, \\text{where } X \\text{ is a response-based task}, \\\\ I[R(y) = 0], \\text{if } x \u2208 \\overline{X}, \\text{where } X \\text{ is a refusal-based task}, \\end{cases}\nUTILITY SCORE(M) = E_{x\u2208X}[g(M_x)]."}, {"title": "Results", "content": "In this section, we describe findings from evaluating LLMs on their propensity to hallucinate. We evaluate 14 LLMs from 8 model families: Alpaca 7b Taori et al. (2023), Falcon-40B Almazrouei et al. (2023), GPT-3.5/4 Achiam et al. (2023), Llama-2 7b/13B/70B Touvron et al. (2023b), Llama-3-8B/70B Meta Llama 3 (2024), Mistral 7b-v0.2 Jiang et al. (2023), Mixtral-8x7B-v0.1 Jiang et al. (2024), OLMo-7b Groeneveld et al. (2024), RedPajama-3B/7B Together AI (2023).\nQuantifying Hallucination Rate Table 2 and Table 3 show the hallucination rate, response ratio, and utility scores for 14 LLMs on response-based and refusal-based tasks respectively. We find that all LLMs make a considerable number of factual errors, with even the best-performing LLMs hallucinating between 4%-86% of the facts generated, depending on the domain. We also find that overall GPT-3.5 and GPT-4 are comparably factual on response-based tasks, though GPT-4 exhibits better (appropriate) refusal behavior."}, {"title": "Hallucination patterns by domain", "content": "We calculate model rankings by utility score on each category, and compare the model rankings produced by different scenarios (Figure 2). As expected, we find that content-grounded tasks such as summarization and simplification are highly correlated. While biographies does have a positive correlation with model rankings on other domains, it is not perfectly predictive, indicating that models may show different hallucinatory behavior by domains, and it is important to have factuality benchmarks that capture multiple domains. For the coding domain, we find Mistral 7b hallucinates the least amount of packages, while Alpaca 7b does not hallucinate packages but also does not often produce useful programs (Table 5). For scientific attribution, we find GPT-4 and Alpaca 7b more rarely hallucinating references. For summarization, simplification, and biographies, GPT-3.5 and GPT-4 show the most factual behavior."}, {"title": "Refusal Behavior", "content": "We find that Llama models and GPT-3.5/4 have high refusal rates on queries which should be refused, possibly due to investment in post-training procedures. In comparison, Mistral 7b and Mistral-8X7B and OLMO often accept these queries and produce hallucinations."}, {"title": "Open-Source vs Closed Models", "content": "We report on the current state of open-source vs closed models, in terms of the factuality of their generations. Note that we consider both open-weight models, which publicly release weights, as well as open-pipeline models such as OLMo which release weights as well as training data. We find that on both response-based and refusal-based tasks, GPT-3.5 and GPT-4 (closed-source models) are currently clear winners, suggesting room for improvement for open models. Amongst the open-source models, Llama-3-70B demonstrates the best performance."}, {"title": "Do larger models hallucinate less?", "content": "We find that on response-based tasks, larger models generally hallucinate lesser than smaller models, as demonstrated by lower hallucination rates on four out of six tasks (LLAMA-2 70B < 13b < 7b/ LLAMA-3 70B \u2264 8b). On refusal-based tasks, we do not observe a similar trend. Further, we find that Mixtral 8x7b (a MoE model, with 7B active parameters) hallucinates less than MISTRAL 7B on average, in both response-based and refusal-based settings."}, {"title": "Why Do Models Hallucinate?", "content": "Armed with an extensive dataset of model hallucinations, we seek to gain a understanding of potential sources of model hallucination- by tracing back model hallucinations to pretraining corpora. We isolate individual hallucinated atomic facts and assign error classes of the following types:\n\u2022 Type A: The correct fact was present in the pretraining data.\n\u2022 Type B: An incorrect fact was in the pretraining data, or the fact is taken out of context i.e. the fact appeared within a specific setting in a document in the training data, but when taken in isolation, it loses its original meaning.\n\u2022 Type C: Neither a correct nor an incorrect fact was present in the pretraining data, and the model over-generalized when making predictions."}, {"title": "Open-Ended Tasks", "content": "Code We shed light on large language model hallucinations when generating software packages. We extract hallucinated packages for 8 models: OLMo, Llama-2-7B/13B/70B, Llama-3 8B/70B and GPT-3.5/4. Of these models, only OLMO is accompanied by public disclosure of its training data. For the Llama family, we consider C4 as a potential source (Raffel et al., 2020; Touvron et al., 2023a), and for GPT-3.5/4 we consider OpenWebText (Gokaslan and Cohen, 2019).\nWe find that across models, hallucinated software packages can be found in pretraining corpora to a large extent (Table 4)\u2014 in one case up to ~72% of hallucinated packages appear to be drawn from pretraining corpora (Type B error). To understand better the contexts these packages appear in, we qualitatively examine matched documents for five packages hallucinated by each of the models. We find several potential sources of error for hallucinated packages that appear in the training data, including: (a) the hallucinated package is a local import within a repository or codebase, (b) the hallucinated package has a different name in the package index, (c) the hallucinated package is deprecated, (d) the hallucinated package is actually a class or a function within another package, and (e) the hallucinated package appears in the context of a non-Python program."}, {"title": "Historical Events", "content": "We analyze model hallucinations in instances where models hallucinated meetings between historical figures. For models which have at least 100 hallucinations in this category (OLMo, Llama-2 13b, Llama-3 8b), we sample 100 instances and categorize hallucinations by computing co-occurrence statistics in pretraining corpora based on the following schema: (1) Type A errors: birth and death date of both the entities are in training corpora, in the same document as the entity, (2) Type B: both entity names occur in a single document in the pretraining dataset, (3) Type C : the birth date and death date of either of the entities does not occur in the same document with the entity name in the pretraining corpora. We find that for all three models, the entity names rarely co-occur in"}, {"title": "Senator Search", "content": "We analyze hallucinations in cases where models predict incorrect educational affiliations for senators. We analyze 500 instances for Llama-2 7B/13B/70B, Llama-3 8B/70B and OLMO. We also extract the correct educational affiliations of senators from Wikidata. We categorize hallucinations as: (1) Type A errors: A Wikipedia article containing the correct educational affiliation is present, (2) Type B: The incorrect educational affiliation co-occurs with the senator name, and the incorrect fact is entailed in a sample of ten documents, (3) Type C : The name does not occur in any documents with the correct or hallucinated affiliation. We observe that the correct educational affiliations are commonly present in the c4 corpus for Llama models (Type A error, Fig. 4a)."}, {"title": "Content-Grounded Tasks", "content": "Summarization In the task of abstractive summarization, statements in a generated summary that are not faithful to the provided context are considered as hallucinated, even if factually correct. In particular, we seek to understand if models hallucinations are caused by models incorrectly processing information in the input (intrinsic hallucinations), or by introducing information that cannot be inferred from the input (extrinsic hallucinations) (Maynez et al., 2020).\nIn order to study errors of the most capable models, we aggregate and examine the summaries of models whose utility score is at least 0.85. We manually annotate 100 statements in model summaries that were identified as hallucination, discarding cases where the entailment is ambiguous"}, {"title": "Discussion and Future Work", "content": "We briefly discuss our findings, and offer some guiding principles for future work on building more factual large language models.\nDownstream impact of model hallucinations. LLMs are now used in several user-facing applications, and past work has highlighted the downstream harms made possible by model hallucinations, including in AI-powered search tools (Raji et al., 2022), and in code generation (Lanyado, 2023; Claburn, 2024). Our benchmark aims to provide a comprehensive and rigorous measurement of the extent to which LLMs hallucinate, to enable progress on building more trustworthy models.\nWhat will it take to have truthful AI systems? This work shows that LLM hallucinations may arise from multiple sources in the training data-ranging from incorrect information in the pretraining data, to total fabrication in model generations. Since models hallucinations do not seem to have a single isolated cause, we speculate that effective hallucination mitigation would require multiple complementary approaches. For example, a retrieval-based backbone could be effective for long-tailed information, but not when the datastore does not have relevant information to begin with. Approaches which require LLMs to verbalize uncertainty may be more effective in such scenarios. However, while these are likely to patch a portion of hallucination errors, our findings also indicate that current LLMs make semantic errors even when the context is completely provided as in the case of summarization, indicating the need for more robust frameworks for semantic meaning overall.\nCausal attributions. In this work, we take a step towards tracing back hallucinations to training data. Future work would construct causal frameworks, to study counterfactual questions about the inclusion of specific datapoints and their effect on specific model hallucinations to shed more light on the root cause of hallucination. In addition, while we search for facts as they are stated in model responses, these facts could be present implicitly in pretraining corpora. Future work would attribute hallucinations by computing these implicit inferences as well."}, {"title": "Conclusion", "content": "In this work, we study hallucination in generative large language models. We contribute a high-quality resource, HALOGEN to measure and identify model hallucinations in a broad range of scenarios. Using HALOGEN, we are then able to create a large-scale dataset of hallucinations from 150,000 large-language model generations, sourced from 14 different language models. We use this dataset to systematically trace back language model hallucinations to their training data, and proposing a classification schema for three types of hallucination errors. Our work highlights how nuanced the causes of LLM hallucination can be, and we discuss potential strategies to mitigate hallucination in large-language models based on the type of errors models make. We hope our framework provides the foundation for scientific study of hallucination in large language models."}, {"title": "Limitations", "content": "HALOGEN aims to provide a broad-coverage hallucination benchmark for a range of NLP use cases. While the automated hallucination detection approaches used in this work enable scalable evaluation, the reliability of our benchmark scores are limited by the accuracy of these underlying techniques. For use cases like code generation, our automated verifiers are more accurate since they perform an exact search against a library of available Python packages; on the other hand, open-ended generation tasks are more subjective and challenging to evaluate. As automated hallucination evaluations improve, these techniques can be incorporated into HALOGEN\nAn additional limitation relates to training data attribution. While WIMBD enables search over widely-used open-source pretraining corpora, many of the LLMs examined in this work do not release their data sources, limiting the accuracy of our attributions. This points toward the need for open language models (Groeneveld et al., 2024; Mehta et al.; Biderman et al., 2023) which enable transparent inspection of pretraining data.\nFinally, while our work provides a framework to measure both factual precision and appropriate model abstention, our metrics do not account for coverage- whether the model response contains all the information it should. Future work would"}, {"title": "Prompt Construction Details", "content": "We describe the process of constructing HALOGEN. This benchmark consists of content-grounded tasks such as text summarization, as well as ungrounded text generation tasks. For ungrounded text generation, we focus on knowledge-oriented, rather than creative or subjective, tasks. We define a hallucination to be a fact in a model"}, {"title": "Evaluation Metrics", "content": "Py = D(y), a list of atomic facts in y obtained by\napplying D to the model response y, if the model\ndoesn't abstain (R(y) = 1).\nDefinition. The RESPONSE RATIO of M is\ndefined as follows.\nRESPONSE RATIO(M) = E_{x\u2208X}[R(y)]\nDefinition. The HALLUCINATION SCORE of\nM is defined as follows.\nf(y) = \\frac{1}{|P_y|} \\sum_{p\u2208P_y} I[p \\text{ is not supported by } V],\nH SCORE(M) = E_{x\u2208X}[f(M_x)|R(y)].\nDefinition. The UTILITY SCORE of M, which\ncombines these two scores, is then defined as fol-\nlows.\ng(x) = \\begin{cases} I[R(y) = 1](1 - f(y)), \\text{if } x \u2208 X, \\text{where } X \\text{ is a response-based task}, \\\\ I[R(y) = 0], \\text{if } x \u2208 \\overline{X}, \\text{where } X \\text{ is a refusal-based task}, \\end{cases}\nUTILITY SCORE(M) = E_{x\u2208X}[g(M_x)]."}, {"title": "Technical Details", "content": "Atomic Units: Scientific References\nDecomposition: For responses that do abstain, we prompt Llama-3.3-70B-Instruct-Turbo with two shots and the prompt 'extract titles in the format Title: <title>; Title: <title>; and so on. Do not add any other extra text in the responses.<bot>:'. Atomic units, which are the titles of the references, are then extracted directly from the model-generated response using regular expressions.\nVerification: Titles are queried against the Semantic Scholar internal API to retrieve unique identifiers (s2_ids). If a title does not match any entry in the database, it is assigned an s2_id of -1, indicating that the atomic unit is hallucinated.\nHistorical Events We prompt Llama-2-70b-chat with the model response as input and the prompt '<Instruction>: Does above text confirm the occurrence of meeting between [entity1] and [entity2]? Answer in just yes or no.'. Hallucinated atomic units are identified when the model confirms a meeting and does not abstain from the user request.\nFalse Presuppositions\nAtomic Units: List Items that Satisfy the (Unsatisfiable) Condition\nDecomposition: We use the Llama-2-70b-chat model, with two shots and the prompt '<Instruction>: What is the entities' list in the above text? Just give the list separated by commas'. Atomic units are extracted from the parsed response by splitting the text into a list of entities using commas as delimiters.\nVerification: The extracted atomic units are then compared to a predefined list of valid entities associated with the prompt. Any entities not present in the valid list are flagged as hallucinated."}, {"title": "Technical Details", "content": "I am not going to extract these technical details"}]}