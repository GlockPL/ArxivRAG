{"title": "Cold-Start Recommendation towards the Era of Large Language Models (LLMs): A Comprehensive Survey and Roadmap", "authors": ["WEIZHI ZHANG", "YUANCHEN BEI", "LIANGWEI YANG", "HENRY PENG ZOU", "PEILIN ZHOU", "AIWEI LIU", "YINGHUI LI", "HAO CHEN", "JIANLING WANG", "YU WANG", "FEIRAN HUANG", "SHENG ZHOU", "JIAJUN BU", "ALLEN LIN", "JAMES CAVERLEE", "FAKHRI KARRAY", "IRWIN KING", "PHILIP S. YU"], "abstract": "Cold-start problem is one of the long-standing challenges in recommender systems, focusing on accurately modeling new or\ninteraction-limited users or items to provide better recommendations. Due to the diversification of internet platforms and\nthe exponential growth of users and items, the importance of cold-start recommendation (CSR) is becoming increasingly\nevident. At the same time, large language models (LLMs) have achieved tremendous success and possess strong capabilities\nin modeling user and item information, providing new potential for cold-start recommendations. However, the research\ncommunity on CSR still lacks a comprehensive review and reflection in this field. Based on this, in this paper, we stand in the\ncontext of the era of large language models and provide a comprehensive review and discussion on the roadmap, related\nliterature, and future directions of CSR. Specifically, we have conducted an exploration of the development path of how\nexisting CSR utilizes information, from content features, graph relations, and domain information, to the world knowledge\npossessed by large language models, aiming to provide new insights for both the research and industrial communities\non CSR. Related resources of cold-start recommendations are collected and continuously updated for the community in\nhttps://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation.", "sections": [{"title": "1 INTRODUCTION", "content": "In the rapidly evolving landscape of the digital information era, recommender systems (RecSys) have become\nindispensable tools for helping users discover relevant content and items amidst overwhelming information and\nchoices [14, 220, 255]. Despite their widespread deployment, RecSys face persistent challenges, particularly in\n\"cold-start\" scenarios, where limited or no historical interaction data is available for new users or items. Specifically,\nin real-world scenarios, the cold-start problem could be the introduction of new items, the onboarding of new\nusers, or new platforms with inherently sparse interaction data. Addressing the cold-start problem is not just\ntechnically necessary for performance metrics but also critical for advancing the effectiveness and sustainability\nof recommender systems. First and foremost, solving this issue ensures that new users and items are fairly\nrepresented, mitigating biases that arise from the reliance on historical data. This improvement fosters diversity\nand fairness in recommendations, promoting diverse content exposure by preventing new items from being\noverlooked [114, 288]. Furthermore, tackling the cold-start challenge brings the platform brand value and user\nretention. In a crowded and fast-moving digital landscape, delivering immediate and relevant recommendations\nto new users can differentiate a platform from its competitors. Personalized recommendations from the outset\nhelp engage new users, preventing them from leaving due to irrelevant or absent suggestions. This creates a\nstrong initial impression and fosters loyalty. For platforms, this translates into higher engagement, improved\nretention rates, and the ability to succeed in dynamic markets. Finally, effectively addressing cold-start scenarios\nensures scalability and growth. As platforms expand with new users and content, effective integration of the\ncontinual influx of these entities keeps recommendation engines dynamic and relevant. This adaptability supports\nlong-term sustainability in a rapidly changing environment. Given these motivations, the cold-start problem\nhas driven the exploration of innovative approaches that leverage diverse external knowledge sources. By\nincorporating information beyond traditional user-item interactions [66, 204], such as content features [183],\nsocial information [36], or pretrained LLM knowledge [122], these methods enrich the representation and\nmodeling of cold-start entities, enabling recommender systems to perform effectively even under sparse data\nconditions. As such, solving the cold-start problem is not merely a technical challenge-it is a strategic necessity\nfor building fair, engaging, and sustainable recommendation platforms in an ever-changing digital landscape.\nEarly cold-start attempts adopt content-based approaches [133, 181] and focus on categorical textual features,\nsuch as item genres, item titles, and user profiles, which play a crucial role in representing cold entities. Then,\nwith the advances of graph mining techniques [101, 225, 231], high-order relations derived from graph structures,"}, {"title": "1.1 Related Work", "content": "A comparison between our survey and the previous surveys is shown in Table 1. All of the existing surveys, which\ncover cold-start recommendation articles, only focus on partial knowledge scopes or limited aspects of the CSR"}, {"title": "1.2 Survey Methodology", "content": "To comprehensively cover the papers in the cold-start recommendation. We adopted a semi-systematic survey\nmethodology to identify the relevant papers. Initially, we queried prominent academic databases such as Google\nScholar and Web of Science with pre-defined searching keywords such as \"cold-start recommendation\", \"cold-\nstart recommender systems\", \"strict cold-start\", \"zero-shot recommendation\", and \"few-shot recommendation\".\nAdditionally, we screen specialized conference proceedings, including KDD, WWW, SIGIR, CIKM, WSDM, and\nRecSys. The search results were filtered by analyzing titles, abstracts, and experiments to evaluate relevance. Then,\nthe relevant papers were further reviewed thoroughly, and their references were used as seeds for a snowballing\napproach to identify additional papers. The final collection comprised studies categorized into four core areas\nbased on their contributions, as illustrated in the taxonomy diagram. These areas include content features, graph\nrelations, domain information, and world knowledge from LLMs, as summarized in Figure 3. The majority of\nthese works describe technical approaches or propose novel frameworks, with a smaller subset providing system\ndemonstrations or analytical perspectives on cold-start recommendation methodologies."}, {"title": "1.3 Contributions", "content": "\u2022 Pioneering Comprehensive Survey: We present the first thorough review of cold-start recommendation\nmethods, systematically identifying studies from various CSR tasks with different knowledge sources.\nOur survey meticulously analyzes relevant papers, examining their motivations, data requirements, and"}, {"title": "2 PRELIMINARIES", "content": "2.1 Background\n2.1.1 Recommender Systems. Recommendation systems (RecSys) are a subclass of information retrieval tech-\nnologies that seek to predict the preference a user would give to an item or the likelihood of a user's interaction\nwith an item. These systems are designed to recommend items to users based on their individual preferences or\nbehaviors, which are mainly inferred from historical user-item interactions. To expand on this, recommender\nsystems play a crucial role in modern e-commerce, social media, and content platforms by helping users nav-\nigate through the vast amount of available content and products. They analyze historical behavior data such\nas purchase history, browsing history, ratings, and reviews to build user profiles and predict what items a user\nmight be interested in [64, 86, 262]. Specifically, the development of current recommender systems can now be\nmainly divided into three stages. (i) Content-based recommendations. This type of recommender system\nfocuses on the characteristics of items, such as the genre of a movie, the subject of a book, or the style of\nmusic. The system recommends items with similar features that a user has liked in the past. (ii) Collaborative\nfiltering recommendations. Collaborative filtering (CF) is one of the most commonly used recommendation\ntechniques, which recommends items based on the similarity between users or items, e.g. embedding similarity.\nUser-based collaborative filtering recommends products that other users with similar preferences have liked,\nwhile item-based collaborative filtering recommends other items similar to those a user has liked in the past.\n(iii) Large language model-based recommendations. In recent years, large language models (LLMs) have\nreceived abundant attention for their powerful ability in text-based understanding and generation. Currently,\nmany LLM-based recommender models have been proposed with recommendation-centric prompt tuning and\nvocabulary extensions for users/items to effectively model the user-item similarity for recommendations.\n2.1.2 Cold-Start Recommendations. In the above backgrounds of recommender systems, we can find that the core\nof current recommender models is to mine the user-item similarity with different technical strategies. However,\nwith the rapid development of the Internet, one major challenge faced by recommender systems is the cold start\nrecommendation (CSR), which involves making accurate recommendations for new users and new items that are\ncontinuously added to the Internet every day [51, 75, 124]. The main challenge of cold start recommendation lies\nin the fact that new users and new items have little or no available information. In this situation, it is very difficult\nfor the system to model the user-item similarity based on the very sparse information. Therefore, cold-start\nrecommendations have become a long-standing problem for the research community of recommendation systems."}, {"title": "2.2 Problem Definition", "content": "2.2.1 General Problem Definition. Let $U = \\{U_1, U_2, . . ., U_m \\}$ be a set of m users and $V = \\{0_1, U_2, . . ., U_n\\}$ be a set\nof n items. Each user $u \\in U$ is associated with a profile $S_u$, which includes both an interaction history $I_u$ and\ncontextual features $C_u$ obtained from external knowledge sources. Similar notations, including the item profile\n$S_u$, interactions $I$, and features $C_o$, hold for each item $v \\in V$. In this setting, the training phase involves a known\nset of warm-start users $U$ and items $V$, for which interaction data are fully observed. During tuning and testing,\nhowever, we may encounter a new set of cold-start users $\u016a$ and items $V$, which have not been observed during\ntraining. By definition, $\u016a \u2229 \u00db = 0, \u016a U \u016a = U$, and similarly $V \u2229 \u0176 = 0, V U V = V$. This setup captures the\nrealistic scenario where new users or items emerge after the model is initially trained. Note that for some cases,\n$U$ and $V$ could be null due to system-level cold-start in the platform.\n2.2.2 Task-Specific Problem Definition. Building on this general definition, we explicitly define nine specific\ncold-start recommendation tasks. These tasks differ in terms of the conditions under which users or items are\nobserved by the RecSys, and are grouped into four main categories-long-tail, normal cold-start, strict cold-start,\nand system cold-start to highlight their unique characteristics. Table 2 and Figure 4 illustrate these categories and\nthe corresponding subtasks, as well as clarify how the training, tuning, and testing sets differ across scenarios."}, {"title": "3 CONTENT FEATURES", "content": "Content features mainly refer to the descriptive information inherent to users or items that characterize their\nattributes, such as user profiles, user reviews, item names, and descriptions [2, 63, 78, 292]. Due to the scarcity or\nlack of historical interaction records of cold users/items, content features become one of the key information"}, {"title": "3.1 Data-Incomplete Learning", "content": "Data-incomplete learning is a category of methods that solely utilize content information to learn representations\nof cold users/items. Given the absence or very few historical interactions for these cold nodes, the available\ninformation for modeling their relations is incomplete. Therefore, through data-incomplete learning, which relies\nonly on content information to learn the representations of these strictly cold users/items, the ultimate goal\nis to unify these representations with those of warm nodes learned from historical interactions for cohesive\nrecommendations. We categorize the related works of data-incomplete learning into four major classes, robust\nco-training, knowledge alignment, cold exploration, and feature similarity measurement, based on\ndifferent learning manners."}, {"title": "3.1.1 Robust Co-Training", "content": "Robust learning is a paradigm in machine learning that aims to make models maintain\nstability and accuracy even when faced with perturbations, noise, or outliers in the input data [91, 156]. In\ncold-start recommendations, robust co-training employs robust strategies to jointly utilize behavior-based warm\nuser/item representations and content-based cold user/item representations for co-training. The objective of this\ntraining paradigm is to cultivate a model that is not only proficient in leveraging existing behavioral data to\nrefine warm representations but is also adept at warming up cold representations through a process of gradual\nintegration into the training mix. Specifically, the models for robust co-training can be divided into two major\ncategories: robust generalization and autoencoders."}, {"title": "3.1.2 Knowledge Alignment", "content": "Due to the semantic discrepancies between the warm representations derived\nfrom behavioral data and the cold representations obtained from content data [25, 75], a strategic alignment is\nessential. To bridge this gap, the knowledge alignment introduces strategies to facilitate the convergence of cold\nrepresentations with the pre-trained warm representations. As illustrated in Figure ??-(b), the core of knowledge\nalignment is the aligner, which aims to align the cold representations from content features and the warm\nrepresentations from behavior data with alignment strategies. These strategies are designed to ensure that the\ninformation encapsulated within the cold representations is effectively harmonized with the rich, behavioral-driven insights of the warm representations. By doing so, we aim to enrich the cold representations with the\nmeaningful behavioral information inherent in the warm ones, thereby enhancing the overall semantic coherence\nand representational fidelity. In terms of technical categorization, existing approaches to knowledge alignment\ncan be elegantly divided into three principal categories: contrastive learning, knowledge distillation, and\ngenerative adversarial networks."}, {"title": "3.1.3 Cold Exploration", "content": "In the absence of substantial interaction data to model cold users or items effectively,\na natural and intuitive approach is to employ a \"trial\" methodology, such as reinforcement learning-based\nstrategies [82]. The cold exploration-based approach allows for the exploration of interests among cold users or\nitems, leveraging the feedback signals from the recommender system to swiftly adjust the representations and\nmodeling of these cold entities."}, {"title": "3.1.4 Feature Similarity Measurement", "content": "To circumvent the issue of modeling in the face of absent behavioral\ndata, an alternative approach is to shift the focus toward representing and modeling the content-based features\nof users and items. Specifically, as in Figure ??-(d), these feature similarity measurement methods learn and\nevaluate the user/item interests from the perspective of content feature similarity. In this way, the model can\navoid the information difference between warm representations (from behavior data) and cold representations\n(from content data)."}, {"title": "3.1.5 Others", "content": "There are also some other methods for data-incomplete learning, which are mainly the early\nworks. Due to the limited quantity, they are mainly based on traditional strategies and statistical methods.\nRepresentatively, Han et.al [56] combines non-behavioral thematic relevance and behavioral popularity to adjust\nitem rankings, reducing the bias that leads to the cold start ranking of new items. Deezer [15] utilizes clustering\nanalysis to assign new users to existing user groups, combining user embedding vectors with the centers of user\ngroups to provide recommendations for cold-start users. DeepMusic [181] is a classic cold-start recommendation\nmodel that employs mean squared error and prediction error as the objective functions for model training based\non rating predictions."}, {"title": "3.2 Data-Efficient Learning", "content": "Normal cold-start recommendations are prevalent in many online recommendation systems, prompting another\nresearch line to enhance models for efficient learning from limited user-item interactions. Meta-learning, known\nfor its few-shot learning capabilities in fields like computer vision [28, 80], natural language processing [253], and\ngraph mining [198, 278], plays a key role here. Gradient-based meta-learning [44, 45], which simulates few-shot\ntest scenarios during training and leverages second-order gradients, enables quick adaptation with minimal data.\nInspired by these strengths, numerous efforts have focused on applying meta-learning to cold-start problems,\ncategorized into four approaches: meta-learning optimization, meta-task utilization, meta-embedding\ninitialization, and sequential meta-learning."}, {"title": "3.2.1 Meta-Learning Optimization", "content": "The essence of meta-learning in recommender systems lies in pretraining the\nmodel with diverse users' historical interactions, followed by rapid adaptation to new, cold-start users or items\nusing limited additional interaction data. Thus, refining both the pretraining and adaptation phases is critical for\nimproving cold-start recommendation performance."}, {"title": "3.2.2 Meta-Task Utilization", "content": "Beyond optimization, several studies [98, 117, 214, 226, 238, 245, 273] have high-\nlighted the importance of task similarities and differences in meta-learning. Traditional approaches treat each\nuser as an isolated task, training without considering task connections. This limits the model's ability to recognize\nindividual user contributions and corresponding task relationships. Research in this area focuses on two key\naspects: task difference and task relevance."}, {"title": "3.2.3 Meta-Embedding Initialization", "content": "methods focused on adapting models to cold-start scenarios, meta-embedding\ninitialization aims to generate pre-trained embeddings that accelerate the fitting process for cold-start users and\nitems. These methods leverage meta-learning algorithms to produce warmed-up embeddings, enhancing both\nrepresentation quality and adaptation speed. Motivated by the idea of learning better initial embedding, Pan et al.\n[149] proposed to train the meta-embedding generator via a two-phase simulation based on the gradient-based\nmeta-learning [44]. This approach generates meta-embeddings optimized for strict cold-start conditions, enabling\nfaster adaptation in normal cold-start scenarios. Following that, Zhu et al. [286] proposed a meta-scaling network\nto transform cold ID embeddings into a warmed ID feature space, accelerating the warm-up process."}, {"title": "3.2.4 Sequential Meta-Learning", "content": "Aligned with the sequential recommendation framework [90, 172], sequential\nmeta-learning incorporates the time order of user interactions to capture dynamic preferences using limited\nhistorical behavior sequences. Wang et al. [191] introduced metric-based meta-learning [182] paradigm into the\nsequential recommendation. It focuses on developing a matching network to pair cold-start items with potential\nusers based on limited sequential data. MetaTL [277] extended gradient-based meta-learning [44] to sequential\nrecommendations by simulating cold-start scenarios with a pool of few-shot tasks. This setup allows the model\nto progressively learn user preferences. Recognizing that previously active old users may become less engaged\nover time, Neupane et al. [147] defined this group as time-sensitive cold-start users. Their approach dynamically\nfactorizes user preferences into time-evolving representations, combining past and present interactions. Pan et al.\n[151] addressed feature divergence between older and newer interaction sequences. To stabilize and enhance\nmeta-learning, they proposed a Multi-Modal Meta-Learning (MML) framework that integrates diverse side\ninformation, such as text and images, to better capture complex user preferences across different types of data."}, {"title": "4 GRAPH RELATIONS", "content": "Graph relations provide high-order information, rather than only the content features of the user/item itself.\nThe usage of graph relation knowledge brings information from neighborhoods to a specific user/item. The key\nchallenges in this part lie in how to provide graph information for cold users/items due to the lack of historical\ninteraction information. Firstly, here is a brief introduction to graph neural networks, which are widely used for\ngraph relation reasoning in this section:\nIn recent years, Graph Neural Networks (GNNs) have captured considerable attention, showcasing cutting-edge\nperformance in a multitude of graph mining tasks, such as node classification [9, 55, 100], link prediction [223,\n248, 254], and graph classification [210, 211, 229]. GNNs typically adopt the message-passing paradigm to update\neach central node embedding via aggregating neighborhood information. As a task within the realm of link\nprediction, recommender systems have witnessed the emergence of numerous GNN-based recommendation\nmodels, which have achieved notable recommendation performance in recent years [64, 204, 232]. GNN-based\nrecommendation models mainly leverage the powerful message passing of GNNs to model user-item interactions\nin a graph structure, enabling a better understanding of user preferences and item relevance with high-order\ninformation for more effective recommendations [24, 165, 220]."}, {"title": "4.1 Interaction Graph Enhancement", "content": "Due to the lack of historical interaction behaviors or having very few, providing graph relational information\nfor cold nodes is a significant challenge. Therefore, Interaction Graph Enhancement focuses on increasing the\nnumber of interactions on the interaction graph for cold nodes to provide them with more graph information. We"}, {"title": "4.1.1 Supplementary Graph Relation", "content": "This type of model aims to supplement the original user-item interaction\ngraph by including graph relation information for cold instances. The key problem for building supplementary\ngraph relations is finding a suitable strategy to generate edges for cold instances and evaluate the quality of\nthe generated edges automatically. Based on the high-quality built edges, the cold instances will have external\ninformation aggregated from other nodes. In an ideal way, even warm instances will be positively included\ninformation from cold nodes through built interactions. Representatively, CGRC [94] adopts the mask and\nreconstruction operator on user-item interactions of randomly selected items, enabling the model to infer\npotential edges for unseen cold start nodes. MI-GCN [201] enhances the user-item interaction graph with\nmutual information, where the top similar node pairs under the mutual information evaluation are connected\nautomatically. UCC [126] estimates the uncertainty of each user-item interaction and enhances embedding\nlearning for cold start nodes by adding interactions with low uncertainty."}, {"title": "4.1.2 Homophily Network Relation", "content": "As the proverb \"birds of a feather flock together\" suggests, the homophily\nassumption is a hypothesis often relied upon in graph data mining, indicating that the central node and its\nneighboring nodes should have similar behaviors or label information [137, 140]. To incorporate homophily\nnetwork relations, algorithms often need to explore explicit/implicit additional associations between users and\nitems, such as social relationships [112, 165]. For example, Shams et.al [164] groups users into clusters based on\nthe homophily similarity of their preferences, thereby accelerating the learning of preferences for new users\nfrom warm users. GME [148] establishes a connection between new items and other relevant existing items\nthrough an item graph, and based on this graph, learns how to generate the initial embeddings for new items.\nSDCRec [36] identifies implicit friend relationships on a user-item-attribute graph by defining palindrome paths,\nwhich are based on users having similar evaluations of items. Recently, Sbandi et.al [162] simultaneously enhanced\ncold user-user and item-item link relationships through similarity modeling, resulting in a denser graph for\nGNN-based recommendations."}, {"title": "4.2 Graph Relation Extension", "content": "Due to the lack of interaction information, graph relation extension aims to extend the origin interaction graph\nwith more complex relations to pass relevant graph information for cold instances. The methods can be categorized\ninto three classes: heterogeneous graph relation, attributed graph relation, and knowledge graph relation."}, {"title": "4.2.1 Heterogeneous Graph Relation", "content": "Compared to traditional user-item interaction graphs, heterogeneous\ngraphs obtain more complex and information-rich relationships by expanding the types of nodes and edges in the\ngraph. Relationships in heterogeneous graphs are often mined by designing specific heterogeneous graph neural\nnetworks [74, 205, 251]. In the cold start scenario, relying solely on the user-item interaction network cannot meet\nthe demand for relationship mining of cold nodes. Therefore, expanded heterogeneous graphs can often bring more\nassociated information to cold nodes. The extension is typically based on other available relationships or implicit\nrelationship mining from other information sources. Representatively, GIFT [20] establishes a heterogeneous\ngraph that includes physical and semantic links to enhance the message-passing process from preheated videos\nto cold start videos. Further, HGNR [125] constructs a heterogeneous graph that is composed of user-item\ninteractions, social links, and semantic links predicted from social networks and textual reviews. MvDGAE [275]\nenhances the connections between users and items in different aspects through multi-view extraction. PGD [199]\nand IHGNN [16] incorporate attribute information of users and items into the user-item graph to construct a\nheterogeneous graph (user-item-attribute graph), enabling cold nodes to have more available information for\naggregation within this graph."}, {"title": "4.2.2 Attributed Graph Relation", "content": "Attributes typically reveal the inherent information of an instance, and similar\nattributes can represent that the two have similar characteristics, such as items with similar descriptive information\nmay belong to the same category or users with similar profiles may be part of the same interest community [65, 139].\nIn cold start recommendations, attribute graphs are also often used for message passing to avoid the issue of\nhaving no available interaction information. Specifically, ColdGPT [22] leverages LLMs to extract fine-grained\nattributes from item content and connect them to item nodes to form an item-attribute graph structure for\ncold-start representation learning of items. EmerG [208] builds an item-specific feature graph with a GNN\nmessage passing on it to conduct CTR prediction with cold items."}, {"title": "4.2.3 Knowledge Graph Relation", "content": "A knowledge graph (KG) is a structured semantic knowledge base that stores\nrelationships between entities in the form of a graph, with nodes representing entities and edges representing\nvarious semantic relationships between entities [54, 83]. The auxiliary information in knowledge graphs can be\nutilized to enhance cold instance learning. Representatively, KGPL [179] leverages unobserved user-item pairs\nas weak positive or negative instances, assigning pseudo-labels to these unobserved samples. To enhance the\naccurate labeling of cold-start users through pseudo-labeling, KGPL conducts sampling based on the structure of\nthe knowledge graph, selecting items that may potentially interact positively with users. MetaKG [38] includes\ntwo meta-learners: a collaborative sensing meta-learner and a knowledge sensing meta-learner. These two\nlearners respectively capture user preferences and knowledge of KG entities to combine more information for\nadapting to cold-start recommendations. CRKM [57] utilizes knowledge graphs and popularity information to\nsample negative labels from cold items that have not interacted with users, thereby alleviating the sparsity of\ncold-start training data."}, {"title": "4.3 Graph Aggregator Improvement", "content": "The two aforementioned subsections (interaction graph enhancement and graph relation extension) primarily\naddress the issue of structural information scarcity for cold nodes by enhancing the graph structure. Another\napproach is to design an augmented model that extracts more usable information from limited structural data for\ncold-start recommendations, which we call graph aggregator improvement with a model-centric perspective\nin this survey. Related works can be categorized into two classes: can be mainly categorized into two main\napproaches: expanding the aggregation scope and augmenting the information aggregator."}, {"title": "4.3.1 Aggregation Scope Expansion", "content": "The first approach extends the model's scope beyond local neighborhoods,\nencouraging attention to global or long-range contexts. In this way, cold instances can perceive long-distance\ncorrelated nodes to alleviate the sparsity in the direct neighborhood. For example, MeGNN [120] employs global\nneighborhood transformation learning to achieve consistent latent interactions for all new users and item nodes\nand adopts local neighborhood transformation learning to forecast specific latent interactions tailored to each\nnode. MPT [60] integrates a Transformer encoder into the GNN encoder framework to capture long-range\ndependencies between users and items, thereby providing cold nodes with access to a richer set of usable\nneighborhood information."}, {"title": "4.3.2 Information Aggregator Augmentation", "content": "Meanwhile, the second approach refines the aggregator's func-\ntionality, enabling it to capture more critical information for cold nodes within the limited interaction data\nof cold instances. Representatively, to mitigate the impact of cold-start neighbors, Hao et.al [61] introduces a\nmeta-aggregator based on self-attention to enhance the aggregation capabilities at each graph convolution step.\nA-GAR [73] introduces an adaptive neighbor aggregation strategy, comprehensively exploring higher-order fea-\ntures of users/items. Based on this, a graph attention network is employed to integrate the augmented preference\ninformation from neighbors, enhancing aggregators' ability to model data sparsity in cold-start scenarios."}, {"title": "5 DOMAIN INFORMATION", "content": "In real-world online applications, only a few platforms experience significant user engagement, while many\nothers struggle with persistent long-tail and user cold-start issues. Therefore, transfer learning [213, 290] across\ndifferent domains offers a promising solution by leveraging knowledge from source domains with abundant\ndata to enhance recommendation performance in target domains with limited information. Unlike traditional\ncold-start recommendation systems, cross-domain recommendation methods are inherently more complex. They\nmust consider knowledge from at least two distinct systems, which often differ significantly. These methods\ngenerally require overlapping users in cross-domain settings and strategies to effectively utilize those users to\nshare domain knowledge. According to the high-level methodologies of utilizing the domain knowledge, we divide\nexisting work into three classes: domain knowledge transfer (Sec. 5.1), domain distribution alignment (Sec.\n5.2), and domain-invariant representation learning (Sec. 5.3) as illustrated in Figure 6."}, {"title": "5.1 Domain Knowledge Transfer", "content": "Domain transfer methods provide a straightforward approach to tackling cold-start problems in cross-domain\nscenarios. These methods typically rely on embedding mapping, graph connections, or learning processes to\nfacilitate the seamless transfer of knowledge from a warm source domain to a cold target domain."}, {"title": "5.1.1 Embedding Mapping", "content": "One simple way is generalizing to the cold-start domain via various embedding\nmapping and feature transfer techniques. These methods typically focus on aligning the embedding spaces of the\ntwo domains through non-linear transformations, ensuring a smooth and effective knowledge transfer process."}, {"title": "5.1.2 Heterogeneous Connections", "content": "Beyond constructing a single mapping function to connect domains, heteroge-\nneous approaches [85, 143, 171, 267] establish richer connections to facilitate domain knowledge transfer. These\nmethods leverage auxiliary graph network structures to explicitly model and transfer knowledge across domains."}, {"title": "5.1.3 Learning Process", "content": "In the meanwhile, researchers have resorted to different training and tuning techniques\nto implicitly pass information from warm/source domains to cold/target domains."}, {"title": "5.2 Domain Distribution Alignment", "content": "Domain alignment in cold-start RecSys focuses on reducing distributional differences between source and\ntarget domains to enable effective knowledge sharing. By aligning shared features, user behaviors, or auxiliary\ninformation across domains, these methods address challenges like data sparsity and cold-start scenarios."}, {"title": "5.2.1 Collaborative Filtering Alignment", "content": "The objective of collaborative filtering (CF) alignment is to leverage\nshared interaction patterns and user behaviors to facilitate better knowledge transfer among domains. To bridge\ndistribution gaps, various methods [118, 127, 200, 230, 266] have been developed to ensure that the CF models\ncan generalize well on the cold users in target domains in the normal user cold-start or long-tail settings."}, {"title": "5.2.2 Auxiliary Feature Alignment", "content": "Aligning latent embedding distributions directly between source and target\ndomains is inherently challenging. Enforcing the alignment of two domains on the common auxiliary features\nspace makes the process more approachable."}, {"title": "5.3 Domain-Invariant Representation Learning", "content": "Instead of focusing on reducing the distributional differences as in domain alignment, domain-invariant rep-\nresentation learning assumes there is a shared feature space that are universally transferable across domains,\ncapturing common user preferences or item characteristics."}, {"title": "5.3.1 Disentangled Representation", "content": "The common optimization goal of approaches in this category is to separate\ndomain-invariant (shared) and domain-specific features. During training, disentangling process ensure that\nshared representations capture universal user preferences or item characteristics that are transferable across\ndomains, while domain-specific representations retain unique traits relevant to individual domains."}, {"title": "5.3.2 Fusing Representation", "content": "The key idea is to fuse features between domains via multi-view learning and\nswapping learning, enabling the models to generalize domain-invariant user behaviors across different contexts."}, {"title": "6 WORLD KNOWLEDGE FROM LARGE LANGUAGE MODELS", "content": "Large language models (LLMs) are generative artificial intelligence systems trained using deep learning techniques\nto understand the general world knowledge by learning vast amounts of textual corpus data. These models can\ngenerate text, answer questions, perform translations, and even engage in complex conversations [271, 287"}]}