{"title": "X-Recon: Learning-based Patient-specific High-Resolution CT Reconstruction from Orthogonal X-Ray Images", "authors": ["Yunpeng Wang", "Kang Wang", "Yaoyao Zhuo", "Weiya Shi", "Fei Shan", "Lei Liu"], "abstract": "Rapid and accurate diagnosis of pneumothorax, utilizing chest X-ray and computed tomography (CT), is crucial for assisted diagnosis. Chest X-ray is commonly used for initial localization of pneumothorax, while CT ensures accurate quantification. However, CT scans involve high radiation doses and can be costly. To achieve precise quantitative diagnosis while minimizing radiation exposure, we proposed X-Recon, a CT ultra-sparse reconstruction network based on ortho-lateral chest X-ray images. X-Recon integrates generative adversarial networks (GANs), including a generator with a multi-scale fusion rendering module and a discriminator enhanced by 3D coordinate convolutional layers, designed to facilitate CT reconstruction. To improve precision, a projective spatial transformer is utilized to incorporate multi-angle projection loss. Additionally, we proposed PTX-Seg, a zero-shot pneumothorax segmentation algorithm, combining image processing techniques with deep-learning models for the segmentation of air-accumulated regions and lung structures. Experiments on a large-scale dataset demonstrate its superiority over existing approaches. X-Recon achieved a significantly higher reconstruction resolution with a higher average spatial resolution and a lower average slice thickness. The reconstruction metrics achieved state-of-the-art performance in terms of serveral metrics including peak signal-to-noise ratio. The zero-shot segmentation algorithm, PTX-Seg, also demonstrated high segmentation precision for the air-accumulated region, the left lung, and the right lung. Moreover, the consistency analysis for the pneumothorax chest occupancy ratio between reconstructed CT and original CT obtained a high correlation coefficient. Code will be available at: https://github.com/wangyunpengbio/X-Recon", "sections": [{"title": "I. INTRODUCTION", "content": "Clinically, an abnormal accumulation of air in the pleural cavity between the lungs and the chest wall is referred to as pneumothorax [1]. Pneumothorax is a serious disease and a relatively common clinical problem, with an annual incidence measured at 22.7 cases per 100,000 people [2]. Patients with pneumothorax may experience symptoms such as chest pain, lung atrophy, dyspnea, and, in severe cases, shock and asphyxia [2]. In clinical practice, X-rays and computed tomography (CT) scans are commonly used to diagnose pneumothorax [3]. Specifically, the two-dimensional (2D) radiography (X-ray) is less radiation-intensive. Nearly all hospitals, including those in remote and technologically underserved rural areas, are equipped with X-ray machines. These machines provide X-ray images for patients, facilitating primary care. In contrast, three-dimensional (3D) CT provides more detailed images of the body's interior, which can be used to diagnose the presence of a pneumothorax and estimate its size and location, ascertain if it's a tension pneumothorax, and evaluate for mediastinal displacement. Nevertheless, CT equipment is expensive, and CT scans involve relatively higher radiation exposure for the long scanning time. Consequently, it's of clinical significance to devise techniques for producing high-quality CT images with reduced radiation exposure.\nDriven by the above requirements and inspired by X-ray and CT acquisition principles, current research has attempted to generate high-precision 3D CT images using low-dose biplane X-ray images. This way of modeling offers two significant advantages. Firstly, it allows for a substantial reduction in the radiation dose absorbed by patients, decreasing the radiation dose range from 1 to 10 mSv for chest CT images to as low as 0.02 mSv when using biplane X-ray images [4], [5]. Additionally, these methods only necessitate basic biplane X-ray images, and highly efficient approaches, such as our proposed method, have the potential to be implemented on existing X-ray machines without incurring extra equipment expenses. This could have a positive impact on a larger number of patients, particularly in developing countries.\nGenerally, the process of projecting complex 3D geometries into flat 2D images introduces information loss. Therefore, reconstructing a 3D image from 2D images is a formidable challenge and has been a long-standing problem in the field of medical imaging as well as computer vision. These methods reconstruct the shape of 3D objects based on indirect information such as light source, focus, texture, or motion [6]. However, traditional methods require multiple views of identical objects using a calibrated camera, which is challenging in realistic scenarios. Recent learning-based view synthesis techniques have shown more promising results in the computer vision and graphics community. A series of neural networks, represented by Nerf [7], learn robust reconstruction functions in a data-driven manner that can be adapted to perform image reconstruction in complex scenes. This has also stimulated enthusiasm in both the academic and industrial communities.\nSimilarly, CT reconstruction was originally realized through mathematical inversion of the projected images from multi-view images. However, traditional reconstruction algorithms require a large number of projected images to avoid artifacts, which increases exposure time and radiation dose. Although efforts have been made to reduce the number of samples used for CT reconstruction, these sparse-sampling CT reconstruction methods still require dozens of images from different shot angles and are ineffective in reducing radiation exposure. The problem of ultra-sparse reconstruction using only biplane X-ray to reconstruct CT scans has not been solved properly. In recent years, a few successful attempts at inverse mapping have emerged. Henzler et al. [8] first applied a deep Convolutional Neural Network (CNN) to single-radiograph tomography and reconstructed 3D cranial volumes from 2D X-rays. Kasten et al. [9] used an end-to-end CNN for 3D reconstruction of knee bones from bi-planar X-ray images. Shen et al. [10] developed a deep network system with representation, transformation, and generation modules to generate volumetric tomography images from single or multiple 2D X-rays.\nIn this study, we proposed a learning-based CT reconstruction network called X-Recon for ultra-sparse 3D tomographic image reconstruction using biplane X-rays acquired from orthogonal views as shown in Fig. 1. X-Recon utilizes a generative adversarial framework, which includes a generator equipped with a multi-scale fusion rendering module (MFusionRen) and a 3D coordinate convolution layer that replaces traditional convolution layers in the discriminator. To supervise the model, we introduce a multi-angle projection loss based on the Projective Spatial Transformer (ProST). This approach allows the incorporation of anatomical prior knowledge and the learning of the mapping relationship from X-rays to CT using a large training dataset. Extensive experiments conducted on a large-scale dataset demonstrate that X-Recon achieves a significantly higher CT reconstruction resolution, with dimensions of 224 pixels \u00d7 224 pixels \u00d7 224 pixels and an average spatial resolution of 1.6 mm. This performance sets a new state-of-the-art (SOTA) standard in ultra-sparse 3D tomographic image reconstruction, offering unprecedented resolution. For reliability evaluation, we employed a zero-shot segmentation framework called PTX-Seg, which combines traditional image processing and deep learning models to delineate the air-accumulated region and lungs for the calculation of the percentage of pleural cavity occupied by pneumothorax. The Pearson's correlation coefficient calculated between the X-Recon reconstructed CT and the real CT for pneumothorax pleural cavity occupancy ratio reached 0.77, which demonstrates its potential for clinical applicability. In summary, this study makes the following contributions:"}, {"title": "II. RELATED WORKS", "content": "A series of shape-from-X methods [6] have been proposed to recover 3D information from 2D images in the field of multi-view stereo vision. Starting from a geometrical viewpoint by providing a mathematically formulated representation of the projection process from 3D to 2D, image reconstruction is subsequently performed by dedicated algorithms to solve the inverse problem of projection. For example, binocular vision-based approaches [11] require feature matching of different viewpoints, followed by recovering the 3D coordinates of the image pixels based on the principle of optical triangulation, whereas contour-based 3D reconstruction methods [11] require precisely segmented 2D contours.\nRecently, researchers [10] have been motivated by the human perception pattern, which suggests that one can infer the approximate size and geometry of an object using only one eye based on prior knowledge. Deep learning view synthesis techniques, such as Neural Radiance Fields (NeRF) [7], have the ability to directly recover the 3D geometry of an object from one or more RGB images without requiring a complex camera calibration process. These techniques transform the 3D reconstruction problem into a generative problem and incorporate prior knowledge. NeRF [7] along with its variants [7], [12]-[14], have shown even more promising results in the computer vision and graphics community. There are significant differences in the imaging principles between the reflection-based visible light imaging of such methods and the transmission-based X-ray imaging required for this study, and it is not possible to directly apply them to this study. However, the idea of introducing prior knowledge through deep learning has profoundly influenced the present work and is reflected in the study of biplane X-ray image reconstruction for CT."}, {"title": "III. METHODOLOGY", "content": "In this paper, we have made the following contributions in terms of methodology: 1) a generalized CT reconstruction network is proposed for X-ray images, called X-Recon, along with several dedicated loss functions to enhance the algorithm's performance. 2) a zero-shot pneumothorax segmentation algorithm, referred to as PTX-Seg, which combines traditional image processing techniques with deep learning models, is introduced for the subsequent segmentation task. The detailed introductions and training pipelines are described in the following sections."}, {"title": "A. X-Recon: Dual View CT Reconstruction Network", "content": "The overall structure of X-Recon is depicted in Fig. 2, and the X-Recon follows the adversarial network paradigm with an additional digitally reconstructed radiograph (DRR) branch. In this study, the ortho-lateral chest X-ray images are simultaneously fed into the generator of X-Recon as a prior condition to obtain the reconstructed CT, which is then used to calculate the reconstruction loss between the reconstructed CTs and the CTs acquired from real patients. Additionally, the projective spatial transformer (ProST) is employed to output the reconstructed CT and employ a multi-angle projection loss between the reconstructed CT and the real CT for supervision, further enhancing the reconstructed image. Furthermore, the ortho-lateral chest X-ray image is also supplied to the discriminator as a prior condition, enhancing the information available to the discriminator and improving its performance. This, in turn, facilitates the adversarial training procedure of the generator, leading to improved image quality in the reconstructed CT.\n1) Generator: The detailed architecture of the generator in X-Recon is illustrated in Fig. 3, comprising three independent modules. The first two encoder-decoder components utilize the same network structure to process images from different viewpoints. The last component is the multi-scale fusion rendering (MFusionRend) module, which serves as the final prediction of the reconstructed image. Specifically, the twin encoder-decoders process posteroanterior and lateral chest X-rays in separate pathways. Because a single posteroanterior chest X-ray image cannot capture the lateral information of an object, and a lateral-only chest X-ray loses the frontal information, X-Recon addresses this limitation by taking dual orthogonal view images, including both frontal and lateral chest X-ray images, as input. The twin encoder-decoder networks extract features from these two views and forward them to the subsequent fusion and decoding module. The MFusionRend module includes a decoder main branch for integrating information from dual orthogonal viewpoints and an output branch for generating features at multiple spatial levels.\nX-Recon employs dense connection modules as the fundamental unit in the encoder. Each of these densely connected modules is composed of a down-sampling module with a step size of 2, a densely connected convolution block, and a compression block that reduces the output channel by half. The ultimate goal of these designs is to create compressed and extracted feature representations while preserving information from the input image at various spatial levels. This is achieved by cascading densely connected modules and transmitting them to the corresponding decoders through skip connections.\n2) Discriminator:: Unlike traditional generative adversarial networks (GANs), X-Recon constructs the discriminator with a fully convolutional form. In the traditional GAN, the discriminator outputs a single number indicating the likelihood that the input sample is real. However, in our fully convolutional discriminator network, the input image is transformed into a matrix. Each value in this matrix represents the probability that the corresponding region in the original image is real. This scheme has proved effective and previous work [24] has demonstrated that this approach exhibits strong generalization in the domain of high-resolution and high-definition image generation. Besides, the translation invariance offered by traditional convolution is conducive to the improved learning of robust features, particularly in tasks like image classification. Nevertheless, in cases where the object within the image remains relatively stationary, the translation invariance of traditional convolution can potentially limit its capacity to capture positional information. To mitigate this problem, the Coordinate Convolution (CoordConv) [25] was introduced. CoordConv integrates coordinate information as part of the feature map, allowing the network to maintain a degree of translational dependency during the task's learning process. In the context of CT reconstruction, the position of each organ in the chest image remains relatively fixed. Allowing the network to retain positional information for each organ can enhance the model's performance.\nWith the same expectations, X-Recon extended the coordinate convolution layer to a 3D format and replaced the conventional convolution layer in the original discriminator network. Furthermore, the discriminator of X-Recon is constructed in a 3D format. Firstly, the feature extraction as well as the fusion of positional information is performed through a 3D coordinate convolutional layer. Then, the feature map is passed through three cascaded convolutional down-sampling modules. Each module consists of a 3D convolutional layer with a step of two, a normalization layer, and a corrected linear unit with leakage. Finally, the feature map is compressed by an output convolutional layer to obtain the final output matrix."}, {"title": "B. PTX-Seg: Zero-shot Pneumothorax Segmentation Framework", "content": "For pneumothorax segmentation, we introduced a zero-shot segmentation framework called PTX-Seg, which combines learning-based methods with traditional image processing techniques based on the anatomical priors. PTX-Seg is a two-branch segmentation framework with detailed procedures outlined in the Algorithm 1. In the learning-based branch, we initially perform lung parenchyma segmentation using the widely recognized deep learning method, specifically, U-Net, with pre-trained weights obtained from the winning solution of the LOLA11 Challenge [26]. The input CT image I will undergo multiple frozen U-Net $F_i(\\cdot)$ to obtain lung parenchyma regions $M_l$. For robust segmentation, the union model ensemble is utilized to ensure that the lung parenchyma is contained in $M_l$ and followed by a closed operation with kernel $K_l$.\nSince pneumothorax is not included in the training set of U-Nets, we require the auxiliary branch to obtain air-accumulated pneumothorax regions that incorporate anatomical priors. This branch begins by binarizing the entire CT sequence to provide an initial separation of the patient's body and regions containing air. After the threshold binarization Threshold(\u00b7) with the intensity threshold $t_b$, the coarse body mask $M_{bc}$ is obtained. However, it's worth noting that the air is also present in the trachea and lungs, which could be erroneously categorized as part of the body mask. To address this, a 3D closure operation using kernel $K_1$ is applied to fill small holes within the lungs and close off the tracheal region. Subsequently, the mask of the non-body regions $M_b'$ could be obtained using seed-point-based region-growing operation Grow(\u00b7) with growing pattern T using two corner points, i.e. $X_0$ = (0,0,0) and $X_{max}$ = (D,W, H). Thus, inverting $M_b'$ yields the fine body mask $M_b$. The coarse segmentation of pneumothorax region $M_{pc}$ could be obtained by using the intersection of lung parenchyma regions $M_l$ from the learning branch and the body region from $M_b$, followed by the binarization with a threshold value of $t_p$. Finally, the segmentation mask of pneumothorax $M_p$ is achieved by refining $M_{pc}$ by identifying the portion with CT values less than -950 (as the CT value of air is -1000) and subsequently using a 3D closure operation is applied to the extracted area, and regions with air volume threshold $v_t$ exceeding 10 ml are selected as the final pneumothorax air accumulation regions."}, {"title": "C. Loss functions of X-Recon", "content": "In the context of CT reconstruction, X-Recon aims to ensure a high level of structural consistency between the reconstructed CT and the real CT acquired from patients. As a result, constraints are necessary to enhance the consistency between the reconstructed CT and the real CT at the voxel level."}, {"title": "1) Reconstruction loss:", "content": "As CT is a 3D single-channel image, which demands a high level of structural accuracy within objects, this study employs the L2 loss as a constraint for reconstruction in X-Recon. This loss is used to attain reconstruction results that closely resemble the real CT structure of the patient. The reconstruction loss is defined as follows:\n$\\mathcal{L}_{Re} = \\mathbb{E}(||y-G(x)||_2)$\nwhere $y$ represents the ground truth CT image, and $G(x)$ represents the CT image reconstructed by the generator G(.)."}, {"title": "2) Digitally Reconstructed Radiograph loss (DRR loss):", "content": "In addition to the utilization of L2 loss as a constraint in 3D space, X-Recon also seeks to impose constraints on the overall structure in 2D projection space. Inspired by image registration techniques [27] that align X-ray and CT images, X-Recon generalizes the spatial transformer to projection geometry, introducing what is referred to as the Projective-Spatial-Transformer (ProST), which enables differentiable projection rendering. The ProST facilitates end-to-end image processing and gradient-based optimization in X-Recon. Another objective of X-Recon is to ensure the consistency of the 2D projection of the reconstructed CT with the projection of the real CT from various image views. Therefore, X-Recon follows conventions in medical imaging, opts to constrain the projections in three orthogonal views: Posteroanterior, Axial, and Lateral. To enhance robustness, X-Recon introduces stochastic viewpoint rotation and translation during training. Similar to the classical Pix2pix model [24], X-Recon employs L1 loss to ensure the output results align with the input conditions. In this context, L1 loss is applied to multi-angle projected images to enhance image edge sharpness. This defines a multi-angle projection loss, expressed as follows:\n$\\frac{1}{3}(L_{DRRPa} + L_{DRRLa} + L_{DRRAx})$\n= $[E(||P_{Pa}(y) - P_{Pa}(G(x))||_1) + E(||P_{Ax}(y) - P_{Ax}(G(x))||_1) + E(||P_{La}(y) - P_{La}(G(x))||_1)]$\nwhere PA represents the posteroanterior view, Ax stands for the axial view, and La refers to the lateral view. This formula computes the sum of the L1 losses of the projected views between the real CT and the reconstructed CT for all views."}, {"title": "3) Generative adversarial loss (GAN loss):", "content": "The generative adversarial network (GAN) [28] is a classical generative model renowned for its capability to formulate real data distributions. The conditional generative adversarial network (cGAN) [29] is one of the extensions of the original GAN, specifically designed to supervise and enhance the data generation process. The supervised generation is achieved by using additional information as conditional inputs to the model which can be in the form of category labels, segmentation masks, and even data for different modalities. Inspired by cGAN, X-Recon introduces prior information, namely 2D X-ray images, into its training process. Given that the reconstruction task involves a transformation from X-ray to CT, necessitating the preservation of the same semantic information as the original X-ray, X-Recon incorporates the 2D X-ray image as prior knowledge within the generator to guide the CT reconstruction process. Furthermore, this prior condition is also integrated into the discriminator to elevate its learning capabilities, thereby motivating the generator to enhance the quality of the reconstructed CT images. The GAN model is trained using the least squares generative adversarial network (LSGAN) loss, and the loss is defined as follows:\n$\\mathbb{E}_{y \\sim P_{CT}, x \\sim P_{XRay}}(||D(y|x) - 1||_2) + \\mathbb{E}_{x \\sim P_{XRay}}(||D(G(x)|x) - 0||_2)]$\n$\\mathbb{E}_{x \\sim P_{XRay}}(||D(G(x)|x) - 1||_2)]$\nwhere $D(y|x)$ represents the discriminator's assessment of the real CT providing prior conditions of input 2D chest X-ray images, and $D(G(x)|x)$ stands for the discriminator's evaluation of the CT reconstructed by the generator under the same prior condition."}, {"title": "IV. EXPERIMENTS", "content": "This study collected data from a total of 255 patients with pneumothorax between January 2015 and October 2021. Additionally, a control group consisting of 279 healthy subjects with normal CT images from July 2019 to October 2021 was also included. In total, this study included CT sequences from 534 subjects. The data collection, usage, and research protocol are approved by the Ethics Committee of the Shanghai Public Health Clinical Centre. The Picture Archiving and Communication System (PACS) of the Public Health Clinical Centre was utilized to identify patients with spontaneous or traumatic pneumothorax. To ensure stable performance analysis and fair performance comparison, collected images undergo visual review by clinical experts, and low-quality images are excluded from this study. After data standardization, each patient corresponds to only one CT examination. All available chest CT images were collected, and the dataset was anonymized by removing patient privacy-related records. For the performance validation and analysis of PTX-Seg, a senior radiologist was invited to annotate and delineate both lungs and regions of pneumothorax using the ITK-SNAP tool as the gold standard in the subsequent segmentation evaluation.\nConsidering the practical challenge of acquiring paired X-rays and CTs due to the high cost and ethical concerns associated with exposing patients to additional radiation, this study employed a Digitally Reconstructed Radiograph (DRR) [30] technique for data simulation. The DRR was used to train the X-Recon network using patients' CT data, generating X-ray images by simulating an X-ray source and detector. More specifically, when provided with a CT sequence, the DRR technique was utilized to generate simulated chest X-ray images in both the posteroanterior view (PA view) and lateral view, effectively producing ortho-lateral chest X-ray images."}, {"title": "B. Implementation Details", "content": "Training Pipeline: The training and test sets were divided according to a four-to-one ratio. The Network was trained using the Adam optimizer with an initial learning rate of 5e-5, betas = (0.5, 0.99), and weight decay of 5e \u2013 4 with a linear decay of the learning rate to 0 in the subsequent 100 rounds of training. The input X-ray image size was set to 224 \u00d7 224, and the output CT image size was set to 224 \u00d7 224 \u00d7 224. To generate high-resolution and high-accuracy CTs in an end-to-end manner within the constraints of limited graphics processing unit (GPU) memory, the batch size is set to 1 in our implementation. Under conditions with small batch size, X-Recon utilized instance normalization, and the process is accelerated by an Nvidia A100 GPU with 40 GB video random access memory (VRAM)."}, {"title": "C. Segmentation Performance of PTX-Seg", "content": "Firstly, the segmentation performance of PTX-Seg was quantitatively evaluated (e.g., Table. I), using the annotations labeled by radiologists as the ground truth to assess the segmentation accuracy for both lungs as well as regions. A total of four metrics were used to assess the segmentation performance, i.e. Dice coefficient, Jaccard coefficient, 95th percentile of Hausdorff Distance (HD95), and Average Surface Distance (ASD). As can be seen from Table. I, the segmentation accuracy of PTX-Seg for the right lung, left lung, and air-accumulated regions is 96.93%, 97.54%, and 96.32%, respectively, demonstrating excellent segmentation precision for these objects."}, {"title": "D. Reconstruction results of X-Recon", "content": "The reconstruction results of X-Recon are visualized in Fig. 5, using examples of a healthy individual and a patient with pneumothorax. Real CT images and X-Recon reconstructed images are displayed from three anatomical perspectives: transverse, sagittal, and coronal views. In the case of the healthy individual, both lung parenchyma are clearly visible in the reconstructed CT, and the position and morphology of each organ match those in the real CT. In patients with pneumothorax, CT images show rims of gas around the edges of the lung which may track up the fissures, and compressed lung edges are visible at the inner margins. In the reconstructed CT images of pneumothorax patients, the tracheal direction closely resembles that of the real CT, the pneumothorax area and lung boundaries are distinctly discernible, and the volume and location of the pneumothorax tend to match those in the real CT."}, {"title": "E. Ablation Experiments", "content": "Subsequently, this study conducted ablation experiments on X-Recon, focusing on network structure and loss functions."}, {"title": "V. DISCUSSION", "content": "This study introduced a novel learning-based method for reconstructing CT images using biplane X-rays acquired from orthogonal views, referred to as X-Recon. The performance of X-Recon is rigorously evaluated through both qualitative and quantitative experiments. Additionally, a zero-shot pneumothorax segmentation algorithm with high segmentation precision, namely PTX-Seg, is proposed and used for further quantification evaluation for image reconstruction. The PTX-Seg could reach an excellent segmentation accuracy in terms of Dice, which obtained 96.93%, 97.54%, and 96.32% for the right lung, left lung, and air-accumulated region respectively. As shown in Fig. 5, the dual-view reconstruction method X-Recon realized a high-resolution ultra-sparse reconstruction performance in reconstructing the structure of large organs such as the heart, chest wall and maintaining detailed information of the lung parenchyma while simulating the diseased areas. The high-quality reconstructed CT demonstrated the feasibility of using ortho-lateral X-ray reconstruction for chest disease diagnosis. In the subsequent quantitative experiments, we evaluated the effectiveness of different network modules and the influence of different loss functions on X-Recon.\nFurthermore, X-Recon was employed in the diagnostic task of pneumothorax and used for quantitative evaluation of clinical metrics. As the most clinically interesting and practically valuable metric is the percentage of the pleural cavity occupied by the pneumothorax air-accumulated region, this study used the proposed pneumothorax segmentation algorithm PTX-Seg to segment and statistically measure the relevant regions in both the patient's real CT and the reconstructed CT. Besides, this study obtained the percentage of pleural cavity occupied by pneumothorax in both the real CT and the reconstructed CT. The results show that Pearson's correlation coefficient for this indicator reached 0.77, and Pearson's correlation coefficients for indicators such as lung parenchyma and air-accumulated volume exceeded a high correlation of 0.8. This indicates that the CT reconstructed by X-Recon holds promise for clinical applications.\nAlthough X-Recon performs well in multiple experiments, it does have some limitations to be improved in future studies. Considering the practical challenges of obtaining paired X-rays and CTs due to cost and ethical concerns related to additional patient radiation exposure, this study used X-rays simulated by the DRR technique instead of real patient scans. Although the DRR-simulated X-rays exhibit good realism, there is still room for improvement in capturing fine tissue structures like the trachea and blood vessels. Additionally, all CT scans were acquired in the supine position, resulting in the chest X-rays synthesized by DRR also being in the supine position. This differs from the target clinical scenario of standing position, which may impact the performance of clinical applications. Additionally, there is still room for improving the resolution of the reconstructed CT. Although the current reconstructed CT of X-Recon has achieved a resolution of 224 x 224 \u00d7 224, which is already higher than the current SOTA methods with resolutions of 128 \u00d7 128 \u00d7 128 [8], [10], its spatial resolution, averaging 1 to 2 mm, still falls short of that offered by high-resolution thin-layer CT scans.\nTherefore, in future works, it's essential to address the problem of the discrepancy between DRR-simulated X-rays and real X-rays. In the realm of algorithms, future developments are expected to be supplemented by research in style migration to reduce such discrepancies. In terms of data, studies on the phantom may offer solutions to overcome data acquisition limitations and produce paired real chest X-ray and CT data. To achieve a higher resolution of reconstructed CT, novel methodologies and theories can be derived and introduced from the recent advances of the vision community to simplify the network architecture and further increase the resolution of the reconstructed CT."}, {"title": "VI. CONCLUSION", "content": "This study proposed a learning-based X-Recon network, designed to reconstruct high-resolution CT images based on ortho-lateral chest X-rays. Additionally, a zero-shot pneumothorax segmentation algorithm, PTX-Seg, is proposed for patient-specific correlation analysis. The expanded experiments demonstrate that X-Recon can produce high-precision CT sequences with reduced radiation exposure and cost. Furthermore, the study enables accurate three-dimensional quantitative assessment of pneumothorax-related indicators. X-Recon's capabilities can also be extended to diagnose other chest-related diseases, potentially inspiring more researchers to address this challenge and advance the field of precision medicine."}]}