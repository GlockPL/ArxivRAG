{"title": "On Computational Limits of FlowAR Models: Expressivity and Efficiency", "authors": ["Chengyue Gong", "Yekun Ke", "Xiaoyu Li", "Yingyu Liang", "Zhizhou Sha", "Zhenmei Shi", "Zhao Song"], "abstract": "The expressive power and computational complexity of deep visual generative models, such as flow-based and autoregressive (AR) models, have gained considerable interest for their wide-ranging applications in generative tasks. However, the theoretical characterization of their expressiveness through the lens of circuit complexity remains underexplored, particularly for the state-of-the-art architecture like FlowAR proposed by [Ren et al., 2024], which integrates flow-based and autoregressive mechanisms. This gap limits our understanding of their inherent computational limits and practical efficiency. In this study, we address this gap by analyzing the circuit complexity of the FlowAR architecture. We demonstrate that when the largest feature map produced by the FlowAR model has dimensions $n \\times n \\times c$, the FlowAR model is simulable by a family of threshold circuits TC\u00ba, which have constant depth O(1) and polynomial width poly(n). This is the first study to rigorously highlight the limitations in the expressive power of FlowAR models. Furthermore, we identify the conditions under which the FlowAR model computations can achieve almost quadratic time. To validate our theoretical findings, we present efficient model variant constructions based on low-rank approximations that align with the derived criteria. Our work provides a foundation for future comparisons with other generative paradigms and guides the development of more efficient and expressive implementations.", "sections": [{"title": "1 Introduction", "content": "Visual generation has become a transformative force in artificial intelligence, reshaping capabilities in creative design, media synthesis, and digital content creation. Advances in deep generative models, such as Generative Adversarial Networks (GANs) [GPAM+20], Variational Autoencoders (VAEs) [Doe16], diffusion models [HJA20, SSDK+20] and flow-based model [KD18], have enabled the synthesis of high-fidelity images, videos, and 3D assets with unprecedented diversity and realism. The introduction of the visual autoregressive model (VAR) [TJY+24] represents a significant shift in the paradigm in the visual generation field. The VAR model adopts a coarse-to-fine Scale-wise prediction to replace the traditional autoregressive image generation techniques. This innovative technique enables the VAR model to effectively capture visual distributions while outperforming diffusion transformers in image generation benchmarks.\nRecently, the introduction of FlowAR [RYH+24] has further advanced the field of autoregressive visual generation. Specifically, FlowAR streamlines the scale design of VAR, improving general-ization for predictions at the next scale and enabling seamless integration with the Flow Matching model [LGL23] for high-quality image generation. It is worth noting that FlowAR has achieved cutting-edge results in multiple empirical studies of visual generation.\nAs the visual generation model architectures grow increasingly sophisticated to meet the de-mands of high-resolution and photorealistic generation, critical questions arise regarding their com-putational efficiency and intrinsic expressive power. While empirical improvements in generation quality dominate the current discourse, comprehending the theoretical foundations of these models continues to be a pivotal challenge. To tackle the challenge mentioned above, some prior researchers have made significant contributions. For example, [MS24] show that DLOGTIME-uniform TC\u00ba circuits can simulate softmax-attention transformers; later, [CLL+24a] show that the introduction of RoPE will not enhance the express power of transformer; [KLL+25b] present the circuit complexity for the VAR model. Up to now, the expressiveness from a circuit complexity perspective of the FlowAR model remains unexplored. This gap raises an important question:\nDoes the Flow Matching architecture enhance the expressive power of the VAR Model?\nThis study seeks to explore this question through the lens of circuit complexity. First, we provide a model formulation for each module of FlowAR. Our insight is that using circuit complexity theory, we prove that each module of FlowAR, including the Attention Layer, Flow-Matching Layer, and others, can be simulated by a constant-depth, polynomial-size TC\u00ba circuit. Ultimately, the combined result shows that the entire FlowAR architecture can be simulated by a constant-depth, polynomial-size TC\u00ba circuit. Therefore, our conclusion offers a negative response to the question: despite the inclusion of the flow-matching mechanism, the expressive power of FlowAR, in terms of circuit complexity, is on par with that of the VAR model.\nIn addition, we explored the runtime of the FlowAR model inference process and potential efficient algorithms. Specifically, we analyzed the runtime of each module in the FlowAR model and found that the bottleneck affecting the overall runtime originates from the computation of the attention mechanism. As a result, we accelerated the original attention computation using low-rank approximation, which makes the overall runtime of the FlowAR model almost quadratic.\nThe primary contributions of our work are summarized below:\n\u2022 Circuit Complexity Bound: FlowAR model can be simulated by a DLOGTIME-uniform TC\u00ba family. (Theorem 5.9)\n\u2022 Provably Efficient Criteria: Suppose the largest feature map produced by the FlowAR model has dimensions $n\\times n\\times c$ and c = O(log n). We prove that the time complexity of"}, {"title": "2 Related Work", "content": "Flow-based and diffusion-based models. Another line of work focuses on flow-based and diffusion-based models for image and video generation [HJA20, HHS23, LTL+24]. The latent diffusion model (LDM) [RBL+22] transforms image generation from pixel space to latent space, reducing the computational cost of diffusion-based generative models. This transformation enables these models to scale to larger datasets and model parameters, contributing to the success of LDM. Subsequent works, such as U-ViT [BNX+23] and DiT [PX23], replace the U-Net architecture with Vision Transformers (ViT) [Dos20], leveraging the power of Transformer architectures for image generation. Later models like SiT [AAK21] incorporate flow-matching into the diffusion process, further enhancing image generation quality. Many later studies [EKB+24, JSL+24, WSD+24, WCZ+23, WXZ+24] have pursued the approach of integrating the strengths of both flow-matching and diffusion models to develop more effective image generation techniques. More related works on flow models and diffusion models can be found in [HST+22, SWYY25, WSD+24, LSSZ24b, LLSS24, HWL+24a, HWL+24b, LZW+24, CGL+25b, CCL+25, CSY25, CLL+25b, CGL+25a, SSZ+25a, SSZ+25b].\nCircuit complexity. Circuit complexity is a key field in theoretical computer science that ex-plores the computational power of Boolean circuit families. Different circuit complexity classes are used to study machine learning models, aiming to reveal their computational constraints. A significant result related to machine learning is the inclusion chain AC\u00ba C TC\u00ba C NC\u00b9, although it is still unresolved whether TCO = NC\u00b9 [Vol99a, AB09]. The analysis of circuit complexity limitations has served as a valuable methodology for evaluating the computational capabilities of diverse neural network structures. Recent investigations have particularly focused on Trans-formers and their two principal derivatives: Average-Head Attention Transformers (AHATs) and SoftMax-Attention Transformers (SMATs). Research has established that non-uniform thresh-old circuits operating at constant depth (within TC complexity class) can effectively simulate AHAT implementations [MSS22], with parallel studies demonstrating similar computational ef-ficiency achieved through L-uniform simulations for SMAT architectures [LAG+22]. Subsequent theoretical developments have extended these investigations, confirming that both architectural variants can be effectively approximated using DLOGTIME-uniform TC\u00ba circuit models [MS24]. In addition to standard Transformers, circuit complexity analysis has also been applied to vari-ous other frameworks [CLL+24b, KLL+25b]. Other works related to circuit complexity can be referenced in [CLL+24a, LLS+24a, LLL+24, CLL+25a, LLS+25b, LLS+25a]."}, {"title": "3 Preliminary", "content": "All notations employed throughout this paper are present in Section 3.1. Section 3.2 introduces circuit complexity axioms. In Section 3.3, we define floating-point numbers and establish the complexity bounds of their operations.\n3.1 Notations\nGiven a matrix $X \\in \\mathbb{R}^{h\\times w\\times d}$, we denote its tensorized form as $X \\in \\mathbb{R}^{h\\times w\\times d}$. Additionally, we define the set [n] to represent {1,2,\u2026,n} for any positive integer n. We define the set of natural numbers as $\\mathbb{N} := {0,1,2,... }$. Let $X \\in \\mathbb{R}^{m\\times n}$ be a matrix, where $X_{i,j}$ refers to the element at the i-th row and j-th column. When xi belongs to {0,1}*, it signifies a binary number with arbitrary length. In a general setting, xi represents a length p binary string, with each bit taking a value of either 1 or 0. Given a matrix $X \\in \\mathbb{R}^{n\\times d}$, we define $||X||_{\\infty}$ as the maximum norm of X. Specifically, $||X||_{\\infty} = \\max_{i,j} |X_{i,j}|$.\n3.2 Circuit Complexity Class\nFirstly, we present the definition of the boolean circuit.\nDefinition 3.1 (Boolean Circuit, [AB09]). A Boolean circuit $C_n: {0,1}^n \\rightarrow {0,1}$ is formally specified through a directed acyclic graph (DAG) where:\n\u2022 Nodes represent logic gates from the basis {AND, OR, NOT}.\n\u2022 Source nodes (in degree 0) corrspond to input Boolean variables $x_1,..., x_n$.\n\u2022 Each non-source gate computes its output by applying its designated Boolean operation to values received via incoming edges.\nThen, we proceed to show the definition of languages related to a specific Boolean circuit.\nDefinition 3.2 (Languages, page 103 of [AB09]). A language $L \\subseteq {0,1}^*$ is recognized by a Boolean circuit family $C = {C_n}_{n\\in\\mathbb{N}}$ if:\n\u2022 The family is parameterized by input length: Cn operates on n Boolean variables.\n\u2022 Membership equivalence: $\\forall x \\in {0,1}^*, C_{|x|}(x) = 1 \\Leftrightarrow x \\in L$.\n\u2022 Circuit existence: For every string length $n \\in \\mathbb{N}$, C contains an appropriate circuit Cn.\nThen, we present different language classes that can be recognized by different circuit families. Firstly, we introduce the NC class.\nDefinition 3.3 (NC\u00b9 Complexity Class, [AB09]). The complexity class NC\u00b9 comprises all languages recognized by Boolean circuit families ${C_n}$ satisfying:\n\u2022 Size(Cn) = O(poly(n)).\n\u2022 Depth(Cn) = $O((log n)^i)$.\n\u2022 Gate constraints: (1) AND, OR gates have bounded fan-in (2) NOT gates have unit fan-in.\nAC\u00b2 circuits relax the gate fan-in restriction of NC\u00b2 circuits. We present the definition of AC\u00b2 as the following:"}, {"title": "4 Model Formulation for FlowAR Architecture", "content": "In this section, we provide a mathematical definition for every module of FlowAR. Section 4.1 provides the definition of up-sample and down-sample functions. In Section 4.2, we mathematically define the VAE tokenizer. Section 4.3 presents a mathematical formulation for every module in the autoregressive transformer in FlowAR. Section 4.4 provides some important definitions of the flow-matching architecture. In Section 4.5, we also provide a rigorous mathematical definition for the overall architecture of the FlowAR Model during the inference process.\n4.1 Sample Function\nWe define the bicubic upsampling function.\nDefinition 4.1 (Bicubic Upsampling Function). Given the following:\n\u2022 Input tensor: $X \\in \\mathbb{R}^{h\\times w\\times c}$ where h, w, c represent height, width, and the number of channels, respectively.\n\u2022 Scaling factor: A positive integer $r \\geq 1$.\n\u2022 Bicubic kernel: $W : \\mathbb{R} \\rightarrow [0, 1]$\nThe bicubic upsampling function $\\phi_{up}(X,r)$ computes an output tensor $Y \\in \\mathbb{R}^{rh\\times rw\\times c}$. For every output position i \u2208 [rh], j \u2208 [ru], l \u2208 [c]:\n$Y_{ijl} = \\sum_{s=-1}^2\\sum_{t=-1}^2W(\\frac{i}{r} - (i\\%r) + s) \\cdot W(\\frac{j}{r} - (j\\%r) + t)\\cdot X_{\\lfloor \\frac{i}{r}\\rfloor+s,\\lfloor \\frac{i}{r}\\rfloor+t,l}$\nNext, we define the downsampling function.\nDefinition 4.2 (Linear Downsampling Function). Given the following:"}, {"title": "5 Complexity of FlowAR Architecture", "content": "This section presents key results on the circuit complexity of fundamental modules in the FlowAR architecture. Section 5.1 analyzes matrix multiplication, while Section 5.2 examines the up-sampling and down-sampling functions. In Sections 5.3 and 5.4, we compute the circuit complexity of the MLP and FFN layers, respectively. Sections 5.5 and 5.6 focus on the single attention layer and layer normalization. Section 4.4 addresses the flow-matching layer. Finally, Section 5 presents our main result, establishing the circuit complexity bound for the complete FlowAR architecture.\n5.1 Computing Matrix Products in TC\u00ba\nwe demonstrate that matrix multiplication is computable in TC\u00ba, which will be used later.\nLemma 5.1 (Matrix multiplication in TC\u00ba, [CLL+24a]). Let the precision $p < poly(n)$. Let $X \\in \\mathbb{F}_p^{1\\times d}, Y \\in \\mathbb{F}_p^{d\\times n_2}$ be matrices. Assume $n_1 \\leq poly(n), n_2 \\leq poly(n)$. The matrix product XY can be computed by a uniform TC\u00ba circuit with:\n\u2022 Size: poly(n).\n\u2022 Depth: $d_{std} + d_{\\oplus}$.\nwhere $d_{std}$ and $d_{\\oplus}$ are defined in Definition 3.9.\n5.2 Computing Down-Sampling and Up-Sampling in in TC\u00ba\nIn this section, we show that Up-Sampling can be efficiently computable by a uniform TC\u00ba circuit.\nDefinition 4.1.\nAssume n = h = w. Assume r < n. Assume c < n. Assume p < poly(n).\nThe function $\\phi_{down}$ can be computed by a uniform TC\u00ba circuit with\n\u2022 Size: poly(n).\n\u2022 Depth: $d_{std} + d_{\\oplus}$.\nwhere $d_{std}$ and $d_{\\oplus}$ are defined in Definition 3.9.\nProof. By Definition 4.2, we know that down-sampling computation is essentially matrix multipli-cation. Then, by Lemma 5.1, we can easily get the proof.\n5.3 Computing Multiple-layer Perceptron in TC\u00ba\nWe prove that MLP computation can be efficiently simulated by a uniform TC\u00ba circuit.\nLemma 5.4 (MLP computation in TC\u00ba, informal version of Lemma B.1). Given an input tensor $X \\in \\mathbb{R}^{h\\times w\\times c}$. Let MLP(X, c, d) be the MLP layer defined in Definition 4.6. Under the following constraints:"}, {"title": "6 Provably Efficient Criteria", "content": "6.1 Approximate Attention Computation\nIn this section, we introduce approximate attention computation, which can accelerate the compu-tation of the attention layer.\nDefinition 6.1 (Approximate Attention Computation AAttC(n, d, R, \u03b4), Definition 1.2 in [AS23]). Given an input sequence $X \\in \\mathbb{R}^{n\\times d}$ and an approximation tolerance $\u03b4 > 0$. Let Q, K, V \u2208 \\mathbb{R}^{n\\times d} be weigh matrices bounded such that\n$\\max{\\{||Q||_{\\infty}, ||K||_{\\infty}, ||V||_{\\infty}\\} } < R$\nThe Approximate Attention Computation AAttC(n, d, R, \u03b4) outputs a matrix N \u2208 \\mathbb{R}^{n\\times d} sat-isfying:\n$||N - Attn(X)||_{\\infty} \\leq \u03b4$"}, {"title": "7 Conclusion", "content": "In this work, we have addressed several fundamental questions about the FlowAR architecture, making significant contributions to both theoretical understanding and complexity efficiency. By rigorously analyzing the architecture of FlowAR, we demonstrated that despite its sophisticated in-tegration of flow-based and autoregressive mechanisms, it resides within the complexity class TC\u00ba. Specifically, we proved that each module of FlowAR, including the attention and flow-matching layers, can be simulated by a constant-depth, polynomial-size circuit. As a result, the entire FlowAR model can be simulated within the same bounds, revealing that its expressive power in terms of circuit complexity is comparable to that of the VAR model. Beyond the theoretical anal-ysis, we identified the computational bottleneck in FlowAR's attention mechanism and developed an efficient variant using low-rank approximation techniques. This optimization achieves nearly quadratic runtime O($n^{2+o(1)}$), a substantial improvement over the original O($n^{4+o(1)}$) complexity, while maintaining an error bound of 1/ poly(n). Our findings provide both a theoretical foundation for understanding the computational limits of FlowAR and practical guidelines for implementing more efficient variants, offering valuable insights for future development of generative architectures and establishing a framework for comparisons with other generative paradigms."}, {"title": "A Notations", "content": "Roadmap. Section A presents all the notations of this paper. In Section B, we present some missing proofs in Section 5. Section C presents provably efficient criteria of the fast FlowAR model.\nA Notations\nGiven a matrix $X \\in \\mathbb{R}^{hwd}$, we denote its tensorized form as $X \\in \\mathbb{R}^{h\\times w\\times d}$. Additionally, we define the set [n] to represent {1,2,\u2026,n} for any positive integer n. We define the set of natural numbers as $\\mathbb{N} := {0,1,2,... }$. Let $X \\in \\mathbb{R}^{m\\times n}$ be a matrix, where $X_{i,j}$ refers to the element at the i-th row and j-th column. When xi belongs to {0,1}*, it signifies a binary number with arbitrary length. In a general setting, xi represents a length p binary string, with each bit taking a value of either 1 or 0. Given a matrix $X \\in \\mathbb{R}^{n\\times d}$, we define $||X||_{\\infty}$ as the maximum norm of X. Specifically, $||X||_{\\infty} = \\max_{i,j} |X_{i,j}|$"}, {"title": "B Supplementary Proof for Section 5", "content": "In this section, we present some missing proofs in Section 5.\nB.1 Computing Multiple-layer Perceptron in TC\u00ba\nThis section presents the detailed proof for Lemma 5.4.\nLemma B.1 (MLP computation in TC\u00ba, formal version of Lemma 5.4). Given an input tensor $X \\in \\mathbb{R}^{h\\times w\\times c}$. Let MLP(X, c, d) be the MLP layer defined in Definition 4.6. Under the following constraints:\n\u2022 Satisfy h = w = n,\n\u2022 Channel bounds: c, d \u2264 n,\n\u2022 Precision: p \u2264 poly(n),\nThe MLP(X, c, d) function can be computed by a uniform TC\u00bacircuit with:\n\u2022 Size: poly(n).\n\u2022 Depth: $2d_{std} + d_{\\oplus}$.\nwith $d_{std}$ and $d_{\\oplus}$ defined in Definition 3.9."}, {"title": "C Provably Efficient Criteria", "content": "C.1 Running Time Analysis for Inference Pipeline of Origin FlowAR Architec-ture\nWe proceed to compute the total running time for the inference pipeline of the origin FlowAR architecture."}]}