{"title": "Position: Emergent Machina Sapiens Urge Rethinking Multi-Agent Paradigms", "authors": ["Hepeng Li", "Yuhong Liu", "Jun Yan"], "abstract": "Artificially intelligent (AI) agents that are capable of autonomous learning and independent decision-making hold great promise for addressing complex challenges across domains like transportation, energy systems, and manufacturing. However, the surge in AI systems' design and deployment driven by various stakeholders with distinct and unaligned objectives introduces a crucial challenge: how can uncoordinated AI systems coexist and evolve harmoniously in shared environments without creating chaos? To address this, we advocate for a fundamental rethinking of existing multi-agent frameworks, such as multi-agent systems and game theory, which are largely limited to predefined rules and static objective structures. We posit that AI agents should be empowered to dynamically adjust their objectives, make compromises, form coalitions, and safely compete or cooperate through evolving relationships and social feedback. Through this paper, we call for a shift toward the emergent, self-organizing, and context-aware nature of these systems.", "sections": [{"title": "1. Introduction", "content": "The advancement in autonomous AI systems, such as deep reinforcement learning (Mnih et al., 2015; Silver et al., 2016), agentic artificial intelligence (Shavit et al., 2023; Chan et al., 2023), self-supervised learning (LeCun & Bengio, 2020; Chen et al., 2020), meta-learning (Finn et al., 2017; Rajeswaran et al., 2019), etc., has ushered in a new era of machina sapiens, where machines transcend their traditional roles to become independent learners, problem-solvers, and human assistants and collaborators. The emergence of these autonomous systems promises transformative capabilities in addressing complex tasks across domains, such as smart transportation (Ault & Sharon, 2021), smart grid and energy systems (Wan et al., 2019; Chen et al., 2022), robotics (Kalashnikov et al., 2018; Tang et al., 2024), economies (Hartwich et al., 2023), and more.\nWith the enormous potential of AI adoptions, we envision an ecosystem where different types of autonomous systems, designed and deployed independently by various stakeholders with distinct goals, will interact, evolve, and coexist as agents (Sutton & Barto, 2018). Autonomous cars from multiple manufacturers will share the same roads, intelligent energy management systems deployed by utilities and prosumers will co-manage the power grid, and robotic assistants customized for diverse user needs will collaborate in shared spaces like hospitals, warehouses, and homes. However, in such environments, agents may find it challenging to achieve their goals amonst other AI agents if they only adhere to the rules and objectives as designed. The underlying conflicts, competition, and misalignment in the goals and behaviors could lead to failures at the societal level, which in turn undermine the success of individual agents, causing chaotic disruptions that compromise the safety, reliability, and ethical integrity of real-world AI applications.\nThe interconnected agentic ecosystems raises critical questions: How can Al agents with different goals and architectures coevolve and collaborate in shared environments? How can they adapt to unforeseen situations without jeopardizing system-wide safety and performance? These challenges extend beyond technical coordination. Autonomous systems must learn to cooperate, negotiate trade-offs, and adhere to societal rules that balance individual objectives with collective well-being. For instance, should a delivery robot prioritize speed over safety when sharing sidewalks with humans? How should an autonomous car handle conflicting interests between its passengers and other road users? As the diversity and ubiquity of AI systems grow, these questions become increasingly urgent.\nTraditional frameworks such as multi-agent reinforcement learning (Tan, 1997; Claus & Boutilier, 1998; Kar et al., 2013; Lowe et al., 2017b; Zhang et al., 2019) and game theoretical methods (Littman, 1994; Slumbers et al., 2023; Mao et al., 2024; Yang et al., 2024) have been widely used to model interactions among agents, but they often fall short when applied to such open environments (Gal & Grosz, 2022) for several reasons:"}, {"title": "Our Position", "content": "Our Position: We posit that the growing trend of agentic AI development demands a fundamental rethinking of how we conceptualize interoperability among an influx of AI agents. Instead of relying on pre-engineered rules and static reward structures, agents should possess the flexibility to adapt to the diverse and evolving world. This flexibility involves the capacity to align their individual goals with broader systems to balance their objectives with collective considerations, such as safety, fairness, and ethical standards. By fostering such autonomy, agents can not only coexist but also co-thrive in open-ended environments, which would otherwise challenge existing paradigms.\nRealizing this vision entails empowering agents to adapt their learning mechanisms, negotiate trade-offs, form coalitions, and engage safely in cooperation or competition. Al agents must be capable of proactively collecting feedback and critics by building trust in dynamic relationships and interactions with peers. This paradigm prioritizes adaptability, harmony, and resilience, shifting from pre-engineered frameworks to self-organizing and context-aware systems. To this end, we advocate for the development of novel methodologies and design principles for multi-agentic ecosystems that foster cooperation among AIs and between AI and humans, ensuring that the benefits of AI align with societal values.\nContributions: Our position aims to contribute by:\n\u2022 Inspiring Future Research Directions: We highlight the opportunities and challenges posed by the increasing prevalence of autonomous, diverse, and independently developed AI agents in shared environments. We encourage the research community to explore innovative methodologies by reexamining existing paradigms while addressing the complexities of emergent AI ecosystems.\n\u2022 Proposing a Framework for Dynamic Interaction: We advocate for a new framework that integrates dynamic norms and adaptive protocols as fundamental components for governing multi-agent interactions. This framework equips agents to continuously evolve their behaviors based on feedback, fostering alignment, coordination, and resilience in diverse, open-ended environments.\n\u2022 Grounding Challenges in Real-World Contexts: We situate the theoretical challenges of emergent agentic ecosystems within practical scenarios. This contextualization underscores the necessity of adaptive and dynamic approaches while demonstrating how the proposed framework can accommodate real-world complexities."}, {"title": "2. The Ecology of An Agentic AI Ecosystem", "content": "2.1. The Nature and Nurture of Autonomous Agents\nAnalyzing how autonomous agents develop and perform in multi-agent settings is helpful in distinguishing between nature and nurture. Although these terms may evoke biological analogies, we use them here to articulate two complementary facets of agent behavior: nature is relatively fixed at inception, and nurture evolves as circumstances change.\nFrom a nature perspective, each agent begins with an intrinsic capacity to:\n\u2022 Evolve greedily in a goal-driven manner\n\u2022 Learn autonomously from interactive feedback\n\u2022 Optimize quantitatively with precise metrics\nThese built-in qualities establish a stable foundation: agents know what they are striving for, have independent means of adapting their strategies, and can precisely measure outcomes relevant to their predefined goals.\nYet, as no environment remains entirely static, agents must also embody a nurture component, where\n\u2022 Goal may shift by contextual and dynamic factors"}, {"title": "2.2. Alternative Positions and Opposing Views", "content": "Several existing approaches challenge or diverge from our position, which offer meaningful contributions but may fall short of addressing the full complexity and dynamism of real-world, uncoordinated Al ecosystems.\nAgentic AI: A Focus on Individual Autonomy. Agentic AI (Wooldridge & Jennings, 1995; Durante et al., 2024; Shavit et al., 2023), including agentic AI workflows (Zhang et al., 2024), emphasizes the development of highly autonomous agents capable of independently learning to achieve their objectives with minimal external guidance. This approach prioritizes self-sufficiency, advanced decision-making capabilities, and optimization of individual goals, often relying on techniques like reinforcement learning, meta-learning (Yun et al., 2023), or hierarchical policies. Sufficiently advanced agentic AI could overcome many coordination challenges by developing internal mechanisms to handle conflicts, adapt to changes, and negotiate with peers.\nWhile agentic AI excels at individual autonomy, its emphasis on independent optimization introduces challenges in multi-agent settings. When each agent optimizes for its own objectives without shared guiding principles, misalignment can emerge at the system level, particularly in environments where collective welfare is critical. Agents may compete for resources, disrupt one another's plans, or fail to recognize mutually beneficial opportunities due to their isolated learning processes. Relying solely on individual agents to resolve coordination challenges risks fragmented and unpredictable behaviors, which may be difficult to regulate or steer toward desirable outcomes in open-ended environments.\nContext-Aware Multi-Agent Systems. Context-aware multi-agent systems have long been explored as a means of enhancing coordination and adaptability in multi-agent environments (WANG & WANG, 2007; Du et al., 2024). These approaches use context modeling to enhance agents' ability to perceive, interpret, reason, learn, and adapt to environmental and situational factors for optimized decision-making. By integrating information such as task dependencies, resource availability, and agent roles, they enable structured adaptation, aligning agent behaviors with system-wide objectives while maintaining flexibility in uncertain settings. Many approaches incorporate explicit rules (Jelen et al., 2022), consensus mechanisms (Amirkhani & Barshooi, 2022), shared societal expectations (McKee et al., 2021), or structured/learnable communication protocols (Sukhbaatar et al., 2016; Lowe et al., 2017a) help agents make informed decisions while minimizing conflicts.\nContext-aware multi-agent systems often rely on engineered coordination mechanisms, where context parameters are pre-encoded. However, emerging AI societies involve agents that may have divergent goals and must learn to interact without relying on pre-engineered contextual models. In such cases, rigid context-aware mechanisms may limit agents' ability to dynamically shape their own interactions. Moreover, these systems typically require centralized or semi-centralized coordination, such as explicit goal alignment or externally provided context. While these mechanisms enhance predictability, they may restrict the emergence of organic relationships and self-organized norms that allow agents to develop truly adaptive strategies in open environments. This limitation becomes particularly evident when agents must not only adapt to a static context but also shape and redefine the context itself through their interactions."}, {"title": "3. A Multi-Agentic AI Framework", "content": "3.1. Components of the Framework\nThe proposed framework is built around the idea of a dynamic ecosystem of AI agents. These agents act, react, and evolve in a world of diverse environments to pursue their objectives while being subject to environmental and societal boundaries. The framework comprises three key components: agents, world, and norms.\nAgents: Agents are social, intelligent entities that act autonomously, learn from experience, and interact with others. An agent is defined as $A := (J, \u03c0, O, B, G)$, where J de-"}, {"title": "3.2. Connecting Norms, Agents, and the World", "content": "Evolution of Norms. Norms are the backbone of the proposed framework, shaping the interactions, behaviors, and learning mechanisms of agents. Norms can be classified into two types: fixed norms, which establish a static structure with predetermined rules, e.g., MARL and games, and dynamic norms, which evolve through agent interactions and feedback to enable adaptive and self-organized relationship building. We primarily focus on dynamic norms.\nThe evolution of norms begins with initial protocols, $P = {P_1,..., P_n}$, that establish rules for agents to build cooperation or manage conflicts. For instance, a basic protocol, $P_i = a \\cdot H_i$, may dictate resource-sharing mechanisms by discouraging hoarding as follows:\n$J'_i = J_i \u2013 P_i, \\forall i$  (1)\nwhere $J_i$ is agent $i$'s original goal (e.g., selfishly hoarding), $J'_i$ is the goal subject to the protocol $P_i$, and $a$ is the penalty coefficient for the hoarding behavior metric $H_i$.\nProtocols can emerge as penalties on agents' objectives, as shown in (1), incentivizing agents to adjust their goals to align with collective priorities. Alternatively, protocols may act as constraints that limit the actions of agents, ensuring that their individual objectives do not violate safety, fairness, or ethical boundaries. This is particularly useful in applications like autonomous vehicles.\nProtocols are learnable and evolve over time, helping agents achieve individual goals by managing conflicts, developing cooperation, or delegating subtasks to other agents. The evolution of protocols could be influenced by the agents' relationship network N, their societal impacts I, and the expectations $E$ held by peers. When agents consistently meet or exceed the expectations of their peers, they reinforce existing protocols, fostering trust and enhancing their influence. Conversely, deviations from expected behaviors-whether due to conflicts, inefficiencies, or environmental shifts-can trigger the refinement or replacement of protocols.\nThe relationship network N, the impacts I, and the expectations $E$ are not static. They evolve as agents interact with each other. As agents learn more about their environment and the behaviors of others, their actions and the resulting impacts shift, changing how they are perceived and how they perceive others. These dynamic relationships guide agents toward more effective adaptation. Through this continuous evolution of N, I, and E, agents gradually refine their strategies, norms, and interactions, aiming to achieve their goals in a dynamic ecosystem.\nThe stability of protocols can be analyzed using a dynamical system representation:\n$\\frac{dP}{dt} = -\\gamma \\cdot (P \u2013 P^*),$   (2)\nwhere $\\gamma$ is a stability coefficient and $P^*$ is the equilibrium protocol. A special case is that the equilibrium optimizes collective outcomes: $P^* = arg \\max_P \\sum_i J_i(\\pi_i, P)$, where $P^*$ can be seen as protocols used for traditional cooperative MARL problems. Note that protocols may not necessarily converge, indicating an ever-evolving ecosystem.\nEvolution of Relationships. The relationships among all agents collectively form the network N, serving as the social fabric of the framework. Agent i's relationships can be"}, {"title": "3.3. Evaluating the Framework: Metrics for Success", "content": "In the proposed framework, every agent strives to achieve its own goals while operating under the constraints of societal norms and collective dynamics. However, the inherent competition for limited resources, conflicting objectives, and interconnected nature make it challenging for all agents to fully realize their goals simultaneously. Evaluating such a framework requires metrics that capture not only the success of individual agents but also the health, fairness, stability, and adaptability of the entire system.\nBalancing Individual and Collective Success. A central challenge in the proposed framework is balancing individual agents' goals with collective harmony. Each agent acts to optimize its own objectives, but in shared environments, these actions inevitably influence - and are influenced by - others. This interconnectedness creates scenarios where adhering to norms may initially seem like a penalty to an agent's goal. However, such adherence is often necessary to ensure long-term success in an ecosystem where agents coexist and interact dynamically. A key question is how to evaluate whether a norm is appropriate for an agent to achieve its goal without being overly penalized. Effective norms align collective objectives with individual incentives, enabling agents to cooperate while pursuing their own goals. Metrics for this evaluation must capture the trade-offs involved. For example, measuring the individual utility gain can reveal how much a norm contributes to an agent's success in a multi-agent ecosystem. Similarly, assessing norm efficiency highlights how well a norm improves collective outcomes without imposing undue burdens on specific agents.\nSacrifices and Fairness. Forming norms often necessitates sacrifices, requiring agents to prioritize collective benefits over individual goals. An agent may delay its own objective to comply with a norm that enhances system-wide stability, such as slowing down to prevent traffic congestion. A fundamental challenge is ensuring these sacrifices yield tangible benefits. To evaluate sacrifices, metrics such as fairness (Grupen, 2023) assess whether the burden of sacrifice is shared proportionally across agents, considering factors like resource availability, task importance, or agent capacity. However, fairness does not imply equality. For example, in hierarchical settings, lower-level \u201cworker\u201d agents may accept greater sacrifices, such as reduced rewards, to support a \"boss\" agent tasked with achieving higher-level objectives. The critical metric here is whether these sacrifices are justifiable and evaluated fairly based on their contribution to the"}, {"title": "Stability and Adaptability", "content": "Stability and Adaptability. Norms are crucial in guiding agents toward predictable and harmonious behaviors, ensuring the system functions cohesively over time. Metrics such as norm stability evaluate whether agents consistently adhere to a defined set of norms, fostering reliable interactions and reducing uncertainties. Similarly, behavioral predictability measuring how agents' actions align with collective objectives can offer insights into how effectively norms shape agent behavior to benefit the system. However, stability alone is insufficient in dynamic environments where static norms risk becoming obsolete. Norm adaptability must also be ensured to accommodate changes in agent goals, environmental conditions, and emerging challenges. Gradual and predictable norm evolution ensures that the system remains flexible and responsive without causing disruptions. Striking a balance between stability and adaptability is a defining feature of this framework's approach to norm management. By embedding mechanisms for controlled norm evolution, the framework ensures that agents can collectively adapt to shifting conditions while preserving a foundation of harmonious interactions. This balance is key to maintaining long-term system stability."}, {"title": "4. Case Studies", "content": "4.1. Autonomous Driving\nImagine a city where autonomous vehicles (AVs) from different makers share the streets. Each AV is trained to minimize travel time, reduce fuel consumption, and optimize route selection while avoiding collisions, but their approaches vary. For example, one might brake aggressively, while another calculates its stopping distance more cautiously. Without a shared understanding, these differences can lead to expectation misalignment, causing AVs to react unpredictably to one another (Shalev-Shwartz et al., 2016; Palanisamy, 2020; Wachi, 2019; Antonio & Maria-Dolores, 2022; Kiran et al., 2022; Gulino et al., 2023). This misalignment can lead to traffic congestion, as illustrated in Figure 2(a). Additionally, AVs may clash unexpectedly: one stops abruptly to avoid another, only to confuse a third, triggering a chaotic ripple effect that can escalate into a chain-reaction accident, as seen in Figure 2 (b). Over time, the uncertainty forces AVs to adopt overcautious behaviors-driving slowly, leaving large gaps, and hesitating at intersections. While these strategies reduce accidents, they disrupt traffic flow, leading to longer commutes, wasted fuel, and overall inefficiencies.\nNow, consider that these AVs are developed with dynamic norms. They learn flexible protocols that enable them to deviate from their original goals to explore different behaviors. For instance, protocols may be rewards that initially encourage AVs to drive more aggressively. Then, they experiment-one car tests how closely it can follow another without causing a collision. Mistakes happen, and a minor fender-bender might teach a valuable lesson about boundaries. Over time, the cars observe patterns, exchange signals, and learn to anticipate one another's reactions. This iterative process enables protocols to evolve over time.\nIn this process, two crucial forces-impacts and expectations-drive the evolution of protocols. Each AV begins by evaluating the impacts of its actions. For example, an AV merging aggressively into another lane may observe how this forces neighboring cars to brake suddenly, increasing the risk of collision and creating congestion. Recognizing this negative impact enables it to revise its merging protocol to balance assertiveness with consideration for others. Furthermore, these AVs learn to anticipate the expectations of others. A car approaching a busy roundabout knows the standard rules: yield or proceed predictably. When it hesitates too long, it disrupts the rhythm, confusing others and slowing the entire system. Through trial and error, it begins to act with confidence, aligning its actions with the expectations of other vehicles.\nThe relationship network influences how AVs anticipate expectations. Repeated interactions between the same vehicles strengthen trust, enabling them to predict one another's actions better. Over time, this shared understanding fosters coordination. For instance, an AV that consistently signals its intentions clearly encourages others in its network to rely on its predictability, reducing uncertainty and hesitation. Conversely, a vehicle frequently violating expectations by breaking unpredictably or merging erratically may erode trust, forcing others to adopt more defensive strategies."}, {"title": "4.2. Case Study 2: Energy Autonomy", "content": "Energy autonomy refers to a new paradigm of distributed, agentic ecosystem where different stakeholders may adopt their chosen AI algorithms into energy management systems (EMS), designed to act and optimize toward corresponding objectives (Juntunen & Martiskainen, 2021). This shift occurs as distributed energy resources (DERs) like solar panels, battery storage, and electric vehicles are increasingly run by standalone EMS. While the grid has been traditionally controlled by electric utilities, DER owners can adopt their own Al solutions for their own use cases; these AI-aided EMS become self-serving agents acting on behalf of their stakeholders to learn and optimize toward individual objectives (Daneke, 2020).\nThe result is a complex network of inter-operating agentic EMS driven by different algorithms and goals. They can no longer treat other agents as part of the environment; instead, each agent observes, learns from, influences, and confronts other interacting agents. These agentic EMS can thus become self-serving players in the electricity market with oscillatory learning objectives, processes, and decisions.\nAn illustrative example of the resulting challenges can be found in conflicts along the sequential decision chain by agentic EMS, shown on the left of Figure 3. An intelligent vehicle battery management system can learn to optimize its state of charge for longer life via restrictive charging and battery balancing; such decisions may contradict a smart wall charger's algorithm trying to maximize charging power for a favorable price or a planned trip. Building automation systems will learn from occupant profiles and preferences to optimize the energy consumption of connected appliances; the resulting frequent set-point changes can lead to unpredictable patterns unlike load models used by utilities, who are adopting AI for better load forecasting and demand-side management. The conflicts above may be a result of not only insufficient coordination across developers but also self-serving policies learned by the agents from inherently incompatible objectives across different stakeholders. Collaborative norms and protocols are thereby needed atop individual agentic EMS designs to describe, analyze, and resolve such conflicts.\nAnother illustrative example is implicit collusion in a decentralized market among agentic EMS, as shown on the right of Figure 3. When various DER owners adopt autonomous AI to optimize pricing on the electricity market, tacit collusion may occur when the agents learn to conspire toward an unfair disadvantage spontaneously: A shared data analytics platform collecting information from subscribing DER providers may inform the best pricing to all subscribers who will thus (un)knowingly price gauge simultaneously; collusion can also be triggered when multiple agents learn to form a coalition and started price gauging without explicit communication or formal agreement. In either case, common interests and information exchanges among agents a common presumption in multi-agent system theories can lead to tacit collusion and unfair advantages over some other agents. New tools and metrics are needed to monitor and detect such behaviors closely, ensuring collaborations do not cross the line of collusion."}, {"title": "5. Challenges, Risks, and Opportunities", "content": "5.1. Managing Emergent and Unpredictable Behaviors\nIn the envisioned multi-agent ecosystems, emergent behaviors arise from interactions between agents and their environments. While such behaviors can lead to innovative and efficient solutions, they can also result in unintended consequences, such as resource monopolization, inefficiencies, or conflicts. The unpredictability of emergent dynamics complicates system design and oversight, particularly in open environments where agents continuously adapt and learn.\nTo address these challenges, future research must explore what kinds of norms can encourage beneficial emergent behaviors while minimizing harmful outcomes. One promis-"}, {"title": "5.2. Designing Algorithms for Dynamic Objectives", "content": "Agents often face shifting priorities and evolving goals as they interact with dynamic environments and diverse peers. Enabling agents to adjust their objectives while maintaining stability and coherence presents a significant technical challenge. Developing algorithms for dynamic goal-setting and adaptation is a crucial research direction. Such algorithms must enable agents to align their evolving goals with the broader needs of the system while preserving individual autonomy. Techniques such as reinforcement learning and meta-learning show promise in handling dynamically changing objectives, but further innovation is needed to address the trade-offs between short-term responsiveness and long-term optimization. Research should also explore constraints and safeguards to ensure that agents' adaptations remain consistent with the overall system's goals and values."}, {"title": "5.3. The Dilemma of Self-Reference", "content": "The ability of autonomous agents to revise their own objectives introduces a fundamental recursive challenge: how can an agent establish its initial goal if the process of goal revision itself requires a predefined objective? This creates a chicken-and-egg dilemma-an agent needs a goal to guide its behavior, yet once granted autonomy to modify that goal, the basis for the initial objective becomes unclear. This raises a key question: if agents determine their own goals, what serves as the initial guiding principle, and how does the agent decide whether a revision is an improvement or a deviation? Without an initial reference point, goal revision risks becoming arbitrary or contradictory. This poses a risk in multi-agent systems, where alignment among agents is critical-if each agent independently shifts priorities, collective coordination may break down.\nTo address this dilemma, agents need an anchoring framework for goal revision, ensuring modifications occur within a coherent evaluative structure rather than being entirely self-referential. This framework should enable adaptive yet bounded evolution to balance between flexibility and constraint, allowing agents to explore alternative objectives while preventing divergence that could lead to instability."}, {"title": "5.4. Ethical Issues of Free Will in Agent", "content": "The capacity of agents to autonomously revise their objectives poses a profound ethical question: to what extent can such agents be said to possess a form of free will? In a sense, agents become products of their own design, which challenges traditional notions of AI control and autonomy. This ambiguity raises critical concerns about accountability (Dai, 2024). If a self-modified agent's actions result in harm or conflict with societal values, where does responsibility lie-with the designer, the user, or the agent itself? Such dilemmas are especially urgent in high-stakes domains like healthcare or autonomous transportation, where goal realignment could have significant consequences.\nTo navigate this terrain, agents must operate within ethical constraints that ensure their goal modifications remain aligned with human benefits. Transparent mechanisms are essential to make these modifications interpretable, allowing human stakeholders to monitor and guide agents' evolving priorities. Interdisciplinary efforts involving ethics, philosophy, and law are crucial to addressing questions of moral agency, responsibility, and oversight. As agents grow increasingly autonomous, clear frameworks will be needed to balance their capacity for self-determination with the broader priorities of society."}, {"title": "5.5. Human-Agent Collaboration", "content": "As AI agents increasingly interact with humans in real-world settings, ensuring effective collaboration will be a critical challenge. Agents must not only understand and respond to human preferences but also adapt to diverse cultural, ethical, and contextual factors, ensuring that their actions align with the expectations of diverse stakeholders. Designing systems that facilitate transparent and effective communication between humans and agents is a key priority. This includes developing interfaces and protocols that help humans to better understand, predict, and trust agent behaviors intuitively. In contrast, human oversight can guide and shape agent behaviors, together with agents' knowledge, co-develop norms, facilitating collaborative frameworks that combine the strengths of human judgment and machine autonomy. By addressing these factors, future research can pave the way for robust and equitable human-agent partnerships."}, {"title": "6. Conclusion", "content": "We posit that the rise of autonomous AI systems necessitates a fundamental rethinking of interoperability among AI agents in open environments. We advocate for a shift toward adaptive and self-organizing AI ecosystems, propose a framework that integrates adaptive norms, evolving protocols, and dynamic relationships, and highlight key challenges and opportunities."}]}