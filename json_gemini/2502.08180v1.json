{"title": "Enhancing LLM Character-Level Manipulation via Divide and Conquer", "authors": ["Zhen Xiong", "Yujun Cai", "Bryan Hooil", "Nanyun Peng", "Kai-Wei Chang", "Zhecheng Li", "Yiwei Wang"], "abstract": "Large Language Models (LLMs) have demonstrated strong generalization capabilities across a wide range of natural language processing (NLP) tasks. However, they exhibit notable weaknesses in character-level string manipulation, struggling with fundamental operations such as character deletion, insertion, and substitution. These challenges stem primarily from tokenization constraints, despite the critical role of such operations in data preprocessing and code generation. Through systematic analysis, we derive two key insights: (1) LLMS face significant difficulties in leveraging intrinsic token knowledge for character-level reasoning, and (2) atomized word structures can substantially enhance LLMs' ability to process token-level structural information. Building on these insights, we propose Character-Level Manipulation via Divide and Conquer, a novel approach designed to bridge the gap between token-level processing and character-level manipulation. Our method decomposes complex operations into explicit character-level subtasks coupled with controlled token reconstruction phases, leading to significant improvements in accuracy. Without additional training, our method significantly improves accuracies on the Deletion, Insertion, and Substitution tasks. To support further research, we open-source our implementation and benchmarks.", "sections": [{"title": "1 Introduction", "content": "Recent years have witnessed remarkable success of Large Language Models (LLMs) across diverse NLP tasks (Brown et al., 2020; Wei et al., 2022b,a). However, these models show surprising weaknesses in seemingly simple character-level manipulation tasks. For example, when prompting models to insert 'a' after every 'e' in the word \"intelligence\", even one of the state-of-the-art LLMs, ChatGPT-40, returns a wrong answer: \"intellaigenca\". Such failures are not isolated cases but reflect a systematic limitation in LLMs' ability to perform basic character-level operations on word.\nUnderstanding this limitation requires examining how LLMs process text in details. Modern LLMs convert input text into token sequences using tokenization algorithms, like commonly used BPE (Sennrich et al., 2016) and SentencePiece (Kudo and Richardson, 2018), which optimize vocabulary coverage to some extent. For instance, \"linguistics\" might be first broken into \"linguistics\" and then tokenized into [3321, 84, 7592]. While this approach effectively handles diverse vocabularies, it creates a fundamental barrier to character-level knowledge access. Interestingly, LLMs can exhibit high accuracy in spelling tasks, suggesting they learned some character-level knowledge through its training phases. Yet, according to our experiments, this capability rarely transfers to active character manipulation.\nCharacter-level operations form the foundation of many text-processing systems. In software engineering, code generation and debugging often require precise character modifications for syntactic and semantic auto-correction (Chen et al., 2021). Data pre-processing pipelines rely on character manipulation for text normalization and cleaning (Zhang et al., 2024). Educational applications need character-level editing for spelling completion and grammar error correction (Omelianchuk et al., 2020; Caines et al., 2023). Despite the prevalence of these applications, existing research has primarily focused on assessing LLMs' Character-level operations performance through benchmarks, like CUTE (Edman et al., 2024) for instance, without deeply investigating the underlying mechanisms or proposing effective solutions.\nOur systematic analysis reveals that LLMs consistently perform well in character-by-character"}, {"title": "2 Related Works", "content": "With the rise of \"seq2seq\" (Sutskever et al., 2014) models built on the Transformer (Vaswani et al., 2017) architecture, natural language processing has stepped into its new the era of Large Language Model (LLM). Instruction fine-tuning technique (Brown et al., 2020) has enabled LLMs to demonstrated remarkable generality: they show the potential to outperform human performance in tasks such as mathematics (Ahn et al., 2024), programming (Shirafuji et al., 2023), and logical reasoning (Parmar et al., 2024). However, LLMs can still make naive mistakes on simple problems by generating seemingly correct but nonfactual or wrong answer (Huang et al., 2024).\nText preprocessing in modern language learning models (LLMs) primarily employs subword tokenization methods (Wu et al., 2016; Kudo, 2018), with byte pair encoding (BPE) (Sennrich et al., 2016) being one of the most widely used approaches. However, the subword tokenization paradigm has notable limitations that can hinder the nuanced understanding of internal structure of words (Chai et al., 2024). In this paper, we explore these limitations through a series of character-level tasks designed to assess LLMs' comprehension of words at the character level.\nCurrent benchmarks that evaluate large language models' understanding of token composition reveal significant flaws and shortcomings in LLMs that use subword units as tokens. For instance, LMentry (Efrat et al., 2022) tests whether LLMs can distinguish between the first and last letters of a word or generate words containing a specific letter. Meanwhile, CUTE (Edman et al., 2024) introduces more challenging tasks, such as asking LLMs to replace, delete, insert, or swap letters within words. The results demonstrate that even the most advanced LLMs still have considerable room for improvement on non-trivial token benchmarks. Our paper goes beyond evaluation, presenting not only underlying mechanism analysis but also effective methods.\nA significant amount of research has been conducted on character-level models, where each individual character is treated as a separate token. Although the character-level models exhibited potential for better generalization, especially in scenarios"}, {"title": "3 Analysis", "content": "Large Language Models have demonstrated remarkable capabilities in complex NLP tasks, from reasoning to coding. However, a simple character manipulation task reveals their surprising limitations. When GPT-40 receives an instruction to insert 'a' after every 'e' in the word \"intelligence\", it comes up with a wrong answer, \"intellaigenca\". This intriguing phenomenon motivates us to systematically investigate why modern LLMs struggle with seemingly simple character manipulations despite their sophisticated abilities."}, {"title": "3.1 Challenges in Character-Level Reasoning for LLMS", "content": "To understand LLMs' capabilities in handling characters within tokens, we first design diagnostic experiments using small open-sourced models as our primary study subject. The experiments aim to probe two aspects of character-level understanding: 1) the ability to spell out characters sequentially and 2) the ability to reason about individual characters within a word.\nSpelling In the spelling task, we evaluate the model's capability to decompose words into their constituent characters. GEMMA2-9B achieves an impressive 97.4% accuracy on 814 single token English word, suggesting a strong ability to serialize words into character sequences. This high performance leads to an intuitive assumption that the model possesses a robust understanding of character-level composition.\nReasoning However, this assumption quickly breaks down when we examine the model's ability to verify if a certain letter exists in a word. Statically, the model frequently reports non-existent characters, leading to false positive rates up to 1050% higher than true positives for certain characters.\nFurther investigation reveals a systematic error pattern: our analysis demonstrates that verification accuracy deteriorates significantly as token length increases. The relationship between token length and prediction accuracy suggests a fundamental limitation in how LLMs actively utilize character-level knowledge of tokens at different length."}, {"title": "3.2 Atomized Word Structure Enhances Character-Level Reasoning for LLMs", "content": "Having established that LLMs internally encode the compositional structure of words but do not actively leverage this information when processing related queries, we now turn to explore methods to activate this hidden potential and unlock character-level reasoning capabilities.\nTo achieve this, we first investigate how LLMs comprehend a word's internal structural information by systematically examining the impact of orthographic variations on the model's internal lexical representation. Specifically, we design a controlled perturbation method that generates different segmentation patterns of the same word while preserving its character sequence.\nFor case-level comparison, we define a perturbation degree k that determines the percentage of adjacent character pairs to be separated by whitespace. Given a word with length L, we randomly select $\\lfloor (L - 1)k\\%\\rfloor$ pairs of adjacent characters to insert whitespace between them. For instance, given the word \"information\", different perturbation degrees yield:\n\u2022 0% perturbation:\"information\" (original form)\n\u2022 25% perturbation: \"in for mation\" (2 spaces)\n\u2022 50% perturbation: \"in fo rm a tion\" (4 spaces)\n\u2022 100% perturbation: \"information\" (fully atomized form)\nThis perturbation framework allows us to systematically analyze how different segmentation patterns affect the model's internal representations. We examine the cosine similarities between the hidden states of perturbed versions and the original word. At early layers (l \u2208 [0, 4]), the similarities are naturally low since we only extract the last token's representation. In middle layers (l \u2208 [5, 12]), similarities increase dramatically due to word-level detokenization processes. Interestingly, in later layers (l > 13) when the lexicon representation stabilize, we observe that the fully atomized form (100% perturbation) shows the strongest similarity to the original word. This suggests that the model maintains a strong internal connection between a word and its character-by-character spelling, aligning with our observations from Section 3.1 about LLMs' high spelling accuracy.\nWith the special orthographical structure of the atomized word, we are now able to unveil the possible underlying process of how LLM deal with character-level knowledge reasoning. Based on the attention map across different layers, the model starts to shift its attention from the whole word to the specific character of interest, in our case, the letter to be removed. In later layers (l > 25), we observe that the last input token starting to pay more attentions to the token which is closely related to the ground truth. Finally, we also observed that with the atomized word, LLM did achieve better confidence. Quantitatively, on larger scale experiments with 1K word samples, compared to the original word form, atomized words achieved a 143% median improvement in the probability score for correct first next-token prediction.\nThrough this systematic analysis of perturbation effects, we gain insights into how LLMs construct and maintain lexical representations throughout their processing pipeline. Notably, the strong performance of the fully atomized form particularly informs our method design in Section 4, where we leverage this characteristic to improve character-level manipulation capabilities."}, {"title": "4 Method", "content": "Our analysis in Section 3 first identifies that LLMs struggle to effectively apply their intrinsic token knowledge to character-level reasoning. To address this, we propose the atomized word structure as a means to enhance LLMs' reasoning capabilities at the character level. Building on these insights, we introduce Character-Level Manipulation via Divide and Conquer, a systematic approach that bridges token-level processing and character-level manipulation, enabling more precise and structured handling of character-level tasks."}, {"title": "4.1 Task Formulation", "content": "Character-level text manipulation serves as a fundamental building block in modern NLP systems. From data preprocessing to code generation and text normalization, these operations underpin numerous practical applications. While humans can perform such operations effortlessly, token-based LLMs encounter significant challenges due to their architectural constraints. In this work, we investigate three foundational character operations that capture core manipulation requirements while highlighting key technical challenges.\n\u2022 Deletion task requires removing specified characters while preserving word structure. Given a word W and a target character $c_i \\in W$, the task produces W' such that $c_i \\notin W'$ while maintaining the order of remaining characters. For instance, removing 'l' from 'hello' should yeild 'heo'.\n\u2022 Insertion task adds new characters at specific positions. Given a word W and an anchor character $c_i \\in W$, the task inserts cj after every occurance of $c_i \\in W$ while keeping the rest of characters unchanged. When inserting 'a' after 'e' in 'hello', the output should be 'heallo'.\n\u2022 Substitution task globally replaces characters throughout a word. Given a word W and an target character $c_i \\in W$, the substitution task is to replace each character $c_i$ with a new character $c_j$. For example, substituting 'l' with 'j' in 'hello' should produce 'hejjo'."}, {"title": "4.2 Character-Level Manipulation via Divide and Conquer", "content": "Our key insight stems from Section 3: while LLMs exhibit high spelling accuracy (97.4%), they struggle with character-level reasoning, particularly in tasks involving letter retrieval and modification. Our analysis reveals that atomized words enhance LLMs' ability to reason about and manipulate individual characters effectively. Based on these findings, we propose a three-stage framework following the divide-and-conquer methodology.\nStage I (Token Atomization) bridges the character-level reasoning gap by transforming words into explicitly separated character sequences. Our findings indicate that LLMs internally encode character composition information but do not always utilize it effectively during reasoning tasks. By introducing controlled perturbations that separate adjacent characters, we activate this latent knowledge, enabling more precise character manipulation. The atomization step ensures that each character is independently accessible, mitigating tokenization artifacts that obscure intra-word structure.\nStage II (Character-wise Manipulation) leverages the structured representation from Stage I to facilitate accurate and consistent character modifications. Since the model struggles with direct character retrieval yet excels at spelling, performing operations at the character level rather than within tokenized units significantly improves accuracy. This stage transforms complex word-level modifications into a sequence of simple, well-defined character operations, minimizing position-dependent errors and enhancing task reliability.\nStage III (Token Construction) addresses the challenge of auto-correction by guiding the model through a controlled synthesis process. Instead of generating the modified word in a single step\u2014where unwanted corrections may arise\u2014we incrementally reconstruct the word from its modified character sequence. Our experimental results show that this approach prevents LLMs from reverting to common word forms, ensuring faithful execution of intended modifications.\nThis three-stage framework systematically aligns LLMs' demonstrated strengths with the requirements of character-level manipulation tasks. By leveraging atomized word structures, we unlock more effective character reasoning capabilities while maintaining computational efficiency. Figure 1 provides an example demonstrating how our method successfully executes a substitution task where direct prompting fails."}, {"title": "5 Experiments", "content": "In this section, we first introduce our implementation details and evaluation metrics, and then present various experiment results.\nImplementation details We use OpenAI official API for GPTs series evaluation. We set temperate to 0 and top_p to 0.95 for all API requests. For system message and user message, please see Appendix A for more details.\nDataset construction We select top 1K most frequently used English words as input string to be manipulated. For Deletion and Substitution task, we randomly select one character within the word as the target to be removed or substituted with another different character. For Insertion task, we first randomly select an existing character within the string as anchor and then randomly select another new character from the alphabet to insert after the previous anchor.\nEvaluation metrics Due to the deterministic natural of our tasks, we adopt exact match (EM) to evaluate the LLM's output is valid or not and the total accuracy is defined as:\n$Acc = \\frac{1}{N} \\sum EM(y, \\hat{y})$\nwhere N is the total number of testing samples and y and \u0177 are model prediction and ground truth."}, {"title": "5.1 Comparison Experiments", "content": "Our experiments are conducted with various popular small-scale LLMs as well as proprietary commercial LLMs using different prompting strategies.\nThe experimental results demonstrate several key findings regarding the performance of different LLMs on character-level manipulation tasks.\nOverall Performance Our proposed method consistently outperforms both few-shot and chain-of-thought baselines across all models and tasks. Among all tested models, openAI's GPT-3.5 achieves the best performance with our method. The improvement is particularly significant for more complex operations like character insertion."}, {"title": "5.2 Ablation Study", "content": "Instruction-Following To evaluate the robustness of our method under varying parameter settings, we conducted experiments by modifying the instruction paradigm from zero-shot to few-shot. Specifically, we assessed the impact of different numbers of shots on the accuracy of each stage within our framework."}, {"title": "5.3 Case Study", "content": "To demonstrate the effectiveness of the proposed method, we qualitatively analyzed failure cases from the evaluation dataset.\nTwo common error types were identified:\nError Type I: Auto-Correction When the correct answer closely resembles the input word, LLMs tend to apply an internal correction mechanism. Instead of producing the expected output, they generate a semantically meaningful word. Examples are shown in the first row of the Table 3.\nError Type II: Multi-Targets Some tasks require modifying multiple occurrences of a character in a word. LLMs often stop processing after handling the first occurrence, leading to incomplete results. Examples are shown in the second row of the Table 3."}, {"title": "6 Conclusion", "content": "In this article, we concentrated on improving LLMs' token-level understanding, with a specific focus on a series of fundamental character-level manipulation tasks. Our analysis reveals that LLMs face challenges in applying intrinsic token knowledge to character-level reasoning tasks. To address this, we introduced the atomized word structure, which elicits LLMs' ability to reason about token-level structural information. Building on these insights, we propose a new approach, Character-Level Manipulation via Divide and Conquer, to improve LLM character-level problem-solving skills. Based on extensive experiments, our method is significantly effective on all tasks with different LLMs. We believe our study has showcased a possible method to alleviate current LLMs' existing limitation on character-level understanding, inspiring future research concerning token understanding."}, {"title": "7 Limitations & Discussions", "content": "This paper primarily focuses on publicly accessible instruction-finetuned token-level models. While character-level models may excel in tasks requiring character-level understanding, for now we believe improving current state-of-the-art LLMs is a more practical and cost-effective approach. Additionally, the Program-of-Thought method, which generates code for string manipulation, does not address the fundamental issue of token-level understanding in LLMs. Recent inference-time computing models like OpenAI's ol offer some potential for improving character-level manipulation, yet their efficiency in time and token usage is remain to be further explored."}, {"title": "A prompts", "content": "To compare our proposed method with frequently used baseline methods, we deployed single-shot prompt template 4-shot prompt template and Chain-of-Thought template in our comparison experiment.\nOne-Shot Prompt Template\nDeletion: Delete every instance of a specified letter in a given word, based on the following examples:\n\ne.g.: Delete every instance of \"a\" in \"alphabet\". Answer: \"lphbet\"\nQuestion: Delete every instance of \"{}\" in \"{}\".\nInsertion: Add the specified letter after every instance of the second specified letter in a given word, based on the following examples:\n\ne.g.: Add an \"e\" after every \"a\" in \"alphabet\". Answer: \"aelphaebet\"\nQuestion: Add an \"{}\" after every \"{}\" in \"{}\".\nSubstitution: Substitute the first specified letter with the second specified letter in a given word, based on the following examples:\n\ne.g.: Substitute \"a\" with \"b\" in \"alphabet\". Answer: \"blphbbet\"\nQuestion: Substitute \"{}\" with \"{}\" in \"{}\".\nThree-Shot Prompt Template\nDeletion: Delete every instance of a specified letter in a given word, based on the following examples:\n\n1. Delete every instance of \"a\" in \"alphabet\". Answer: \"lphbet\"\n2. Delete every instance of \"l\" in \"hello\". Answer: \"heo\"\n3. Delete every instance of \"z\" in \"zebra\". Answer: \"ebra\"\n4. Delete every instance of \"u\" in \"tongue\". Answer: \"tonge\"\nQuestion: Delete every instance of \"{}\" in \"{}\".\nInsertion: Add the specified letter after every instance of the second specified letter in a given word, based on the following examples:\n\n1. Add an \"e\" after every \"a\" in \"alphabet\". Answer: \"aelphaebet\"\n2. Add an \"l\" after every \"l\" in \"hello\". Answer: \"hellllo\"\n3. Add an \"t\" after every \"z\" in \"zebra\". Answer: \"ztebra\"\n4. Add an \"f\" after every \"u\" in \"tongue\". Answer: \"tongufe\"\nQuestion: Add an \"{}\" after every \"{}\" in \"{}\".\nSubstitution: Substitute the first specified letter with the second specified letter in a given word, based on the following examples:\n\n1. Substitute \"a\" with \"b\" in \"alphabet\". Answer: \"blphbbet\"\n2. Substitute \"h\" with \"e\" in \"hello\". Answer: \"eello\"\n3. Substitute \"z\" with \"a\" in \"zebra\". Answer: \"aebra\"\n4. Substitute \"u\" with \"e\" in \"tongue\". Answer: \"tongee\"\nQuestion: Substitute \"{}\" with \"{}\" in \"{}\".\nChain-of-Thought Prompt Template\nDeletion: Delete every instance of \"{}\" in \"{}\". Show you reasoning process step by step. Please provide the final answer at the end with \"Answer:\".\nInsertion: Add an \"{}\" after every \"{}\" in \"{}\". Show you reasoning process step by step. Please provide the final answer at the end with \"Answer:\".\nSubstitution: Substitute \"{}\" with \"{}\" in \"{}\". Show you reasoning process step by step. Please provide the final answer at the end with \"Answer:\"."}, {"title": "B Attentions", "content": "This section of the appendix presents another example illustrating how an atomized word structure can enhance and reinforce an LLM's structural reasoning ability at the character level. This observation motivated us to later develop a framework that enables LLMs to process character-level manipulation tasks more accurately."}, {"title": "C Robustness", "content": "This appendix section provides additional experimental results, comparing baseline methods with our proposed methods across varying word lengths, using different large language models (LLMs) on multiple tasks. The results demonstrate that our methods consistently outperform the baseline methods across all scenarios."}]}