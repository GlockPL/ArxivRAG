{"title": "R2GenCSR: Retrieving Context Samples for Large Language Model based X-ray Medical Report Generation", "authors": ["Xiao Wang", "Yuehang Li", "Fuling Wang", "Shiao Wang", "Chuanfu Li", "Bo Jiang"], "abstract": "Inspired by the tremendous success of Large Language Models (LLMs), existing X-ray medical report generation methods attempt to leverage large models to achieve better performance. They usually adopt a Transformer to extract the visual features of a given X-ray image, and then, feed them into the LLM for text generation. How to extract more effective information for the LLMs to help them improve final results is an urgent problem that needs to be solved. Additionally, the use of visual Transformer models also brings high computational complexity. To address these issues, this paper proposes a novel context-guided efficient X-ray medical report generation framework. Specifically, we introduce the Mamba as the vision backbone with linear complexity, and the performance obtained is comparable to that of the strong Transformer model. More importantly, we perform context retrieval from the training set for samples within each mini-batch during the training phase, utilizing both positively and negatively related samples to enhance feature representation and discriminative learning. Subsequently, we feed the vision tokens, context information, and prompt statements to invoke the LLM for generating high-quality medical reports. Extensive experiments on three X-ray report generation datasets (i.e., IU-Xray, MIMIC-CXR, CheXpert Plus) fully validated the effectiveness of our proposed model. The source code of this work will be released on https://github.com/Event-AHU/Medical_Image_Analysis.", "sections": [{"title": "1. Introduction", "content": "X-ray image based medical report generation is one of the typical applications of Artificial Intelligence (AI) in healthcare. It aims to utilize powerful AI models to directly generate high-quality medical reports from given X-ray images, thereby alleviating the workload of doctors and reducing patient waiting times. Although the performance of this task has made significant strides with the development of AI, it still falls short of matching the expertise of professional physicians due to various challenges. Due to privacy concerns surrounding X-ray data, there is a lack of quality and diversity in the training datasets, and the rarity of certain diseases and abnormal conditions also results in poor generalization performance of the models. Therefore, the research on X-ray image based report generation is still a very important but unsolved problem.\nDue to the good generalization of deep learning, the performance of X-ray medical report generation has seen steady improvements. For example, Cao et al. propose the Multi-modal Memory Transformer Network (MMTN) [5] for image-report consistent medical report generation. Jin et al. introduce the PromptMRG [19] which attempts to enhance the diagnostic accuracy in the report by feeding diagnosis-aware prompts. Li et al. proposed a new radiological reports DCL [24] framework that uses dynamic graphs to integrate specific and general knowledge to improve visual representation. This framework also enhances visual and textual representation by leveraging contrastive learning objectives. KiUT [18] utilizes the U connection between the encoder and decoder to effectively integrate different levels of visual information. It introduces a knowledge graph and uses a distillation technique to produce reports that are more aligned with real-world conditions. ME-Transformer [46] simulates multi-expert joint diagnostics by introducing multiple learnable \u201cexpert\u201d tokens. These tokens focus on different areas while interacting with each other to capture reliable and complementary visual information, facilitating the parallel generation of diagnostic reports. Although these models have achieved good results, their overall performance is still far from reaching the level of human experts. How to narrow the gap further with professional doctors remains a question worth pondering and researching.\nInspired by the great success of LLMs in natural language processing (NLP), some researchers also introduced LLMs for medical report generation. Specifically, Liu et al. [26] propose to augment the LLMs-based radiology report generation using in-domain instance induction and coarse-to-fine decoding. Although better performance can be obtained, these models still achieve their efficiency and performance bottlenecks due to the following issues: Firstly, the performance of current LLMs heavily relies on the tokens humans feed into, for example, the prompt sentence, visual tokens, etc. More comprehensive inputs can guide the LLMs to generate high-quality medical reports. However, seldom of current works consider the context samples (e.g., the samples with/without disease) which may be very important cues for the text generation of the current sample. Secondly, they adopt the Transformer network as the vision backbone which is computationally expensive (O(N^2)). When handling long-range visual tokens (e.g., the raw X-ray images are usually high-definition), the self-attention in the Transformer often performs poorly on the speed, memory usage, etc.\nWith these issues in mind, in this work, we propose a novel X-ray medical report generation framework that adopts the Mamba [14] as the backbone and mines the context samples to guide the large language models for high-quality report generation. A comparison between existing models and ours is illustrated in Fig. 1 (a-d). We partition the input X-ray image into patches and project them into visual tokens. Then, the Mamba backbone network is adopted for efficient and effective visual token processing. More importantly, we retrieve some context samples from the training subset for each image in the mini-batch to help our report generator understand which samples are likely to have diseases and which do not. These context samples are also processed using the Mamba backbone and subtracting the global tokens of the current image to get the residual tokens. We provide the context prompts to assist the LLM in distinguishing whether certain tokens are related to diseases. As shown in Fig. 1 (e), it is easy to find that these retrieved positive and negative samples are easily distinguishable from the perspective of the t-SNE [40] feature distribution. Finally, we feed these visual tokens, context tokens, and prompts into the LLM for high-quality X-ray medical report generation. Extensive experiments on two widely used benchmark datasets fully validated the effectiveness of our proposed framework.\nTo sum up, the main contributions of this work can be listed as follows:\n1). We propose a novel large language model based X-ray medical report generation framework that is augmented by context samples in the training phase, termed R2GenCSR.\n2). We propose an efficient and effective half-precision vision Mamba that achieves comparable performance to the widely used Transformer backbone network for the X-ray medical report generation task.\n3). Extensive experiments on the widely used IU-Xray, MIMIC-CXR, and CheXpert Plus datasets fully validated the effectiveness of our proposed X-ray report generation framework."}, {"title": "2. Related Work", "content": "In this section, we will review the related works on the X-ray Medical Report Generation, Large Language Models, State Space Models, and Context Sample Retrieval. More related works can be found in the following surveys [44, 53].\n2.1. X-ray Medical Report Generation\nExisting X-ray medical report generation models can be divided into CNN (Convolutional Neural Networks)-based, RNN (Recurrent Neural Networks)-based, and Transformer-based frameworks. To be specific, Li et al. [23] propose a model that combines CNNs and RNNs to generate medical reports from chest X-ray images. Jing et al. [20] develop a medical report generation model based on the LSTM [16] (Long Short-Term Memory) framework. They first predict the possible diseases using LSTM and then generate medical reports based on those predictions. Chen et al. [7] demonstrate the effectiveness of generating detailed and accurate radiological reports from X-ray images using the Transformer model, leveraging visual and text data to improve performance. Wang et al. [43] pre-train a ViT model on the high-resolution X-ray images using masked auto-encoder for medical report generation.\n2.2. Large Language Models\nThe combination of medical report generation and Large Language Models (LLMs) has become the most popular research direction in report generation. LLMs are the focus of current research which can be divided into two categories: base LLMs and professional LLMs. The most famous early LLM is Google's BERT [11], which understands text via a bidirectional encoder, significantly improving the performance of natural language processing tasks by pre-training on large amounts of unlabeled text and then fine-tuning on specific tasks. Meta develops a foundational large language model, LLAMA [37], with several billion to several hundred billion parameters. LLaMA significantly reduces computing resources and energy requirements while maintaining high performance, demonstrating broad potential in practical applications. GPT [30] (Generative Pre-trained Transformer) series consists of large-scale language models developed by OpenAI for natural language generation and understanding. GPT-3 [4] is renowned for its 175 billion parameters and its powerful capabilities in both generating and understanding language.\nFor the LLMs developed for medical domains, R2Gen-GPT [45] proposes a medical report generation method based on LLM which combines the image and text and fed into the decoder Llama2-7B [38] for report generation. RGRG [36] applies a practice similar to object detection tasks to medical report generation by using GPT-2 [32] to generate separate sentences for the detected areas and then reconnect the sentences. MedicalGPT [48] is a healthcare language modeling project based on the ChatGPT training process. Developed by Google, Med-PaLM2 [33] is a medical language model that combines an improved foundational language model (PaLM 2 [1]), domain-specific fine-tuning for the medical field. Med-PaLM2 improves performance by over 19% compared to Med-PaLM [39] and has reached a new state-of-the-art level. MedVersa [54] is capable of handling multimodal medical inputs and outputs, supporting real-time task specification. Inspired by these work, we propose to further improve the quality of X-ray medical reports via large language models in this paper.\n2.3. State Space Model\nDue to the high computational cost in the widely used Transformer networks, the State Space Model (SSM) [44] is proposed to achieve linear complexity. To be specific, the S4 [15] model introduces a new structured state space approach through a layered and modular design. This improves the modeling ability of long sequence dependencies and significantly enhances the efficiency and accuracy of sequence modeling. S5 [35] is an in-depth improvement and simplification of the S4 model, aiming to enhance computational efficiency and ease of use while retaining powerful sequence modeling capabilities. Mamba [14] proposes a time-varying state-space model based on a selection mechanism to efficiently model long sequences. With the success of Mamba, researchers have applied it to a variety of research fields. In the field of vision related to medical report generation, the SSM-only approach Vim [55] (Vision Mamba) has achieved good results in terms of performance and efficiency, especially for processing high-resolution images. This is accomplished by adding positional embeddings to image sequences and utilizing bi-directional SSMs to compress visual representations. VMamba [28] proposes a visual Mamba model with a global receptive field and linear complexity. Its success comes from the Cross-Scan Module (CSM), which scans simultaneously from all four corners of the feature map, ensuring that each element in the feature map integrates information from all other locations in different directions. Mamba-2 [9] is an improved version based on the Mamba architecture. By incorporating SSD theory and structural attention mechanisms, enhances performance and efficiency while maintaining the advantages of Mamba. Inspired by the linear computational cost, in this work, we propose to encode the X-ray image using a half-precision vision Mamba network and achieve similar performance on three X-ray medical report generation benchmark datasets.\n2.4. Context Sample Retrieving\nRetrieval-Augmented Generation (RAG) is a hybrid approach that combines retrieval and generation to enhance the performance of NLP tasks, particularly those requiring extensive knowledge and contextual information. BG-Former [42] (Batch-Graph Transformer) introduced a new Transformer architecture called SSA (Structure-constrained Self-attention), which deeply mines the relationships between samples to provide a new method for robust and differentiated data representation and learning. CricaVPR [29] generates more robust image features by correlating multiple images in a batch through an attention mechanism, using cross-image differences like perspective and illumination as cues for feature learning. CRAG [50] introduces a search evaluator to assess the quality of retrieved documents and enhance search results through large-scale web searches. At the core of EgoInstructor [47] is the retrieval-augmented module, which utilizes existing third-person video resources to help the model better understand and describe first-person perspective video content. Retrieval augmentation can significantly improve generation quality and reduce dependence on the size of the training dataset. RALF [17] (Retrieval-Augmented Layout Transformer), proposed by Horita et al., enhances the generation process by retrieving layout examples most similar to the input image, thereby overcoming the challenge existing methods face in capturing high-dimensional layout structures when training data is limited. EVCAP [22] is a retrieval-augmented image captioning method based on external visual-name memory. It constructs external memory using object images and names, and generates image captions through a retrieval-augmented model. In the field of medical report generation, RAG can also serve as a clinical decision support tool by combining medical databases and research papers, helping physicians quickly access the latest research on disease diagnosis, treatment options, and drug information. Our extensive experiments on three benchmark datasets support the effectiveness of context samples for the medical report generation task."}, {"title": "3. Methodology", "content": "In this section, we will first give a review of the Mamba network and an overview of our proposed R2GenCSR framework. Then, we will dive into the details of the R2GenCSR framework, with a focus on Input Representation, Context Sample Retrieval, LLM for Report Generation, and Loss Function. More details will be introduced in the subsequent subsections.\n3.1. Preliminary: Mamba\nCurrent widely used Mamba networks are developed based on the continuous State Space Model (SSM). It maps a one-dimensional function or sequence \\(x(t) \\in \\mathbb{R}^{p}\\) to \\(y(t) \\in \\mathbb{R}^{q}\\) through a hidden state \\(h(t) \\in \\mathbb{R}^{n}\\). The computing procedure can be summarized as follows:\n\\(h'(t) = Ah(t) + Bx(t),\\) (1)\n\\(y(t) = Ch(t),\\) (2)\nwhere \\(A \\in \\mathbb{R}^{n \times n}\\), \\(B \\in \\mathbb{R}^{n \times p}\\), \\(C \\in \\mathbb{R}^{q \times n}\\) denotes the state matrix, input matrix, and output matrix.\nAs the image and text we processed are discrete data, the aforementioned continuous SSMs needed to be transformed into discrete ones. For example, the S4 [15] and Mamba model adopts the Zero-Order Hold (ZOH) to realize this goal, i.e.,\n\\(A = \\exp(\\Delta A),\\) (3)\n\\(B = (\\Delta A)^{-1}(\\exp(\\Delta A) \u2013 I) \\cdot \\Delta B.\\) (4)\nwhere the \\(\\Delta\\) is a timescale parameter (also called step size). Thus, we can reformulate the discrete version of SSM as:\n\\(h_t = Ah_{t-1} + Bx_t,\\) (5)\n\\(y_t = Ch_t.\\) (6)\nTo further strengthen the SSM, Gu et al. propose the Mamba [14] which makes the model varying from time-invariant to dependent. And also speed the training and inference using a couple of hardware-aware algorithms. Inspired by the success of Mamba in natural language processing, researchers also adapt it to the computer vision community, e.g., the VMamaba [28] used in this paper, and vision Mamba [55]. We prefer the readers to check the reference [44] for more details.\n3.2. Overview\nIn this paper, we propose a novel contextual sample retrieval guided large language model framework for efficient X-ray medical report generation. As shown in Fig. 2, it can be divided into three main parts, i.e., the Mamba vision backbone, context retrieval module, and large language model (LLM) for report generation. Given the X-ray image, we first extract its visual tokens using the Mamba backbone. Meanwhile, we retrieve context samples (X-ray samples with and without disease) from the training subset based on the input image and embed them into visual and text tokens. Then, the residual tokens which measure the difference between the input and context samples can be obtained via the subtract operator. Finally, we feed the vision tokens, context residual tokens, and prompt statements into the LLM to generate a high-quality medical report. One can note that the proposed framework stands out from existing methods by incorporating context retrieval and using a linear complexity vision backbone, which enhances feature representation and discriminative learning while maintaining computational efficiency.\n3.3. R2GenCSR Framework\nIn this subsection, we will introduce the R2GenCSR framework from the perspective of Input Representation, Context Sample Retrieval, LLM for Report Generation, and Loss Function.\n3.3.1 Input Representation\nAssume the dataset contains N X-ray images, \\(X = \\{X_1, X_2, ..., X_N\\}\\), where each \\(x_i \\in \\mathbb{R}^{C \times H \times W}\\) represents a X-ray image with C channels, height H, and width W. For each X-ray image x, the corresponding feature map \\(v \\in \\mathbb{R}^{H \times W \times \\hat{C}} = VMamba(x)\\) can be obtained after feeding it into the VMamba backbone, where H and W are the spatial dimensions of the feature map, and \\(\\hat{C}\\) is the number of feature channels. The reason our framework adopts VMamba instead of conventional visual Transformer models, such as ViT and Swin-Transformer, is because the computational complexity of this model is linear (O(N)), requiring lower computational resources. As shown in Fig. 2, the basic VMamba block consists of Layer Normalization (LN), Linear Layer, DW-Conv layer, SiLU activation layer, SS2D module, and also the skip connections.\nThen, two distinct types of representations are generated based on feature map v, i.e., global features \\(v_G\\) and sequential tokens \\(v_S\\). Specifically, a 2D global average pooling is applied to v over the spatial dimensions to get the global features \\(v_G \\in \\mathbb{R}^{\\hat{C}}\\). The sequential tokens \\(v_S \\in \\mathbb{R}^{\\hat{H}\\hat{W} \times E}\\) are obtained by flattening the v along the spatial dimension, then, processed by projection layer (Proj) and layer norm (LN) operations. Mathematically speaking,\n\\(V_G = avgPool2D(v)\\) (7)\n\\(\u03c5_S = LN(Proj(Flatten(v)))\\) (8)\nHere, global features \\(V_G \\in \\mathbb{R}^{\\hat{C}}\\) capture the global information of the X-ray image, meanwhile, the sequential tokens \\(v_S \\in \\mathbb{R}^{\\hat{H}\\hat{W} \times E}\\) learns the channel representations.\n3.3.2 Context Sample Retrieval\nGiven the visual tokens of the input X-ray images, one can directly feed them into the large language model to generate the medical reports, clearly, this approach will still reach its performance bottleneck. The context learning methods [12, 42] suggest that other samples in the training set may still play a positive role in each round of model optimization. Therefore, we consider designing a method to mine samples that are positively and negatively correlated with the current sample to enhance discriminative feature learning, thereby better guiding the LLM to generate accurate medical reports.\n\u2022 Positive and Negative Sample Selection. For each sample in a mini-batch, we can retrieve its context samples based on keywords in the medical reports. In our implementation, we exploit two different approaches: 1). We adopt the CheXbert [34] to find the possible 14 kinds of diseases from the annotated medical report. If the sample is annotated as No Finding, we treat it as a negative context sample (without disease), otherwise positive (with disease). 2). We find that the medical report with and without Note symbol can be roughly divided based on visual features, as illustrated in Fig. 1 (e). Similarly, we can treat the context samples with/without Note as the positive/negative samples, respectively. In addition, we also randomly select context samples to augment the training of our proposed R2GenCSR model.\nAfter the context samples are retrieved, we extract the global visual features of X-ray images as \\(\\{Ug,1, Ug,2, ..., Ug,n\\}, Ug,i \\in \\mathbb{R}^{\\hat{C}}\\) using the Mamba backbone. These features are then projected into the language space of the LLM using a learnable projection layer, resulting in \\(\\{Cg,1, Cg,2,..., Cg,n\\}, Cg,i \\in \\mathbb{R}^E\\). This projection aligns the visual features with the text embeddings used by the LLM, facilitating seamless integration of visual and textual information.\n\u2022 Residual Calculation. To guide the large language model to generate more accurate medical reports, in this paper, we measure the difference between the current input X-ray sample and context samples, and term the difference as residual tokens. For each context sample, we assign a disease prompt \"With disease\u201d or \u201cNormal\" and also take the visual-prompt difference into consideration, i.e.,\n\\(R^+_i = [Ug - C_{g,1}, Ug \u2013 C_{g,2},..., Vg \u2013 C_{g,n}]\\) (9)\n\\(R^-_i = [Vg - C_{g,1}, Vg \u2013 C_{g,2}, ..., Vg - C_{g,n}]\\) (10)\n\\(Rt = [Vg - t1, Vg - t2, ..., Vg - tp]\\) (11)\nwhere \\(c^+_i\\) and \\(c_{g,i}\\) denote the projected and tokenized features of the i-th positive and negative image, respectively. \\([,]\\) denotes the concatenate operation. \\(R^+_i\\) and \\(R^-_i\\) represent the residual tokens for the positive and negative examples, while \\(R_t\\) are the residual tokens for the disease prompt. \\(t_p\\) is the p-th tokenized and projected text token.\n\u2022 Prompt Construction. All operations are conducted on high-dimensional language space. Each token, including those in the residuals and the original text, is converted into an embedding vector by the previous step, which ensures all elements of the prompt are represented in a manner that the LLM can effectively understand and process. Therefore, the final prompt for the LLM is constructed by concatenating the residuals and tokenized features as follows:\n\\(Prompt = [R_t, R^-_i, Rt, R^+_i, R_t,T,Us,T]\\) (12)\nHere, T and \\(v_s\\) are the tokenized text and vision token representations of the current image, respectively. This structured input is fed into the LLM to generate the final medical report, incorporating both visual and textual context in a coherent and informative manner.\n3.3.3 LLM Head & Loss Function\nThe Large Language Model plays a central role in generating detailed and accurate medical reports from X-ray images. As described in the previous sections, the input to the LLM consists of a composite prompt that includes both visual and textual residuals derived from positive and negative context samples, along with the tokenized text corresponding to the input X-ray image. Once the contextual information has been integrated, the LLM generates the medical report. The generation process involves decoding the embedded and contextually enriched prompt into a coherent and comprehensive text. In our experiments, various LLMs are evaluated to achieve higher performance, including lightweight LLM Qwen1.5 (0.5B, 1.8B, 4B, 7B) [2], the medium-sized Llama2 (7B) [38], Llama3 (8B) [13], and large-scale MedicalGPT (13B) [48]. For even larger LLMs, considering the computational cost, this paper will not consider them for the time being.\nTo optimize our R2GenCSR framework, we adopt the cross-entropy loss function to measure the difference between the generated medical reports and the ground truth annotations. Specifically, we apply instruction-tuning to the LLM to generate medical reports, maintaining its original auto-regressive training objective. During this process, the LLM is fine-tuned specifically on the tokens of the medical report, guided by the instruction prompt that encapsulates the visual and textual residuals. Our loss function is defined as the negative log-likelihood of the sequence of report tokens. This can be formulated as:\n\\(L = - \\sum_{i=1}^T log p_\u03b8 (y_i|Prompt, Y_{r,<i})\\) (13)\nwhere \\(\u03b8\\) denotes the trainable parameters, and T is the length of the whole medical report. \\(y_i\\) is the token being predicted at the current step i, Prompt is the instruction prompt that includes the residuals and tokenized features, and \\(Y_{r,<i}\\) is the sequence of report tokens before the current prediction token \\(y_i\\). The instruction-tuning ensures that the LLM generates a report that aligns with the provided instructions and context, thus producing a coherent and informative medical report.\""}, {"title": "4. Experiment", "content": "4.1. Datasets and Evaluation Metrics\nWe evaluate the performance of our model on three datasets", "10": "MIMIC-CXR [21", "6": "dataset. A brief introduction to these datasets is given below.\n\u2022 IU-XRay [10", "parts": "indication", "7": 1, "21": "is a large publicly available dataset of chest radiographs with free-text radiology reports. These records"}, {"6": "is a large", "7": 1, "41": "BLEU [31", "25": "and METEOR [3"}, {"7": "R2GenCMN [8", "45": ".", "": "Chexbert [34", "No Finding": "erving as negative samples", "resolutions": 512}]}