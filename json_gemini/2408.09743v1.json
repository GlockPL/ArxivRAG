{"title": "R2GenCSR: Retrieving Context Samples for Large Language Model based\nX-ray Medical Report Generation", "authors": ["Xiao Wang", "Yuehang Li", "Fuling Wang", "Shiao Wang", "Chuanfu Li", "Bo Jiang"], "abstract": "Inspired by the tremendous success of Large Language\nModels (LLMs), existing X-ray medical report generation\nmethods attempt to leverage large models to achieve bet-\nter performance. They usually adopt a Transformer to ex-\ntract the visual features of a given X-ray image, and then,\nfeed them into the LLM for text generation. How to extract\nmore effective information for the LLMs to help them im-\nprove final results is an urgent problem that needs to be\nsolved. Additionally, the use of visual Transformer mod-\nels also brings high computational complexity. To address\nthese issues, this paper proposes a novel context-guided ef-\nficient X-ray medical report generation framework. Specifi-\ncally, we introduce the Mamba as the vision backbone with\nlinear complexity, and the performance obtained is compa-\nrable to that of the strong Transformer model. More impor-\ntantly, we perform context retrieval from the training set for\nsamples within each mini-batch during the training phase,\nutilizing both positively and negatively related samples to\nenhance feature representation and discriminative learning.\nSubsequently, we feed the vision tokens, context informa-\ntion, and prompt statements to invoke the LLM for gener-\nating high-quality medical reports. Extensive experiments\non three X-ray report generation datasets (i.e., IU-Xray,\nMIMIC-CXR, CheXpert Plus) fully validated the effective-\nness of our proposed model.", "sections": [{"title": "1. Introduction", "content": "X-ray image based medical report generation is one of\nthe typical applications of Artificial Intelligence (AI) in\nhealthcare. It aims to utilize powerful AI models to directly\ngenerate high-quality medical reports from given X-ray im-\nages, thereby alleviating the workload of doctors and reduc-\ning patient waiting times. Although the performance of this\ntask has made significant strides with the development of\nAI, it still falls short of matching the expertise of profes-\nsional physicians due to various challenges. Due to privacy\nconcerns surrounding X-ray data, there is a lack of quality\nand diversity in the training datasets, and the rarity of cer-\ntain diseases and abnormal conditions also results in poor\ngeneralization performance of the models. Therefore, the\nresearch on X-ray image based report generation is still a\nvery important but unsolved problem.\nDue to the good generalization of deep learning, the\nperformance of X-ray medical report generation has seen\nsteady improvements. For example, Cao et al. propose the\nMulti-modal Memory Transformer Network (MMTN) [5]\nfor image-report consistent medical report generation. Jin\net al. introduce the PromptMRG [19] which attempts to\nenhance the diagnostic accuracy in the report by feeding\ndiagnosis-aware prompts. Li et al. proposed a new radi-\nological reports DCL [24] framework that uses dynamic"}, {"title": "2. Related Work", "content": "In this section, we will review the related works on the\nX-ray Medical Report Generation, Large Language Models,\nState Space Models, and Context Sample Retrieval. More\nrelated works can be found in the following surveys [44,\n53].\n2.1. X-ray Medical Report Generation\nExisting X-ray medical report generation models can\nbe divided into CNN (Convolutional Neural Networks)-\nbased, RNN (Recurrent Neural Networks)-based, and\nTransformer-based frameworks. To be specific, Li et al. [23]\npropose a model that combines CNNs and RNNs to gener-\nate medical reports from chest X-ray images. Jing et al. [20]\ndevelop a medical report generation model based on the\nLSTM [16] (Long Short-Term Memory) framework. They\nfirst predict the possible diseases using LSTM and then gen-\nerate medical reports based on those predictions. Chen et\nal. [7] demonstrate the effectiveness of generating detailed\nand accurate radiological reports from X-ray images using\nthe Transformer model, leveraging visual and text data to\nimprove performance. Wang et al. [43] pre-train a ViT\nmodel on the high-resolution X-ray images using masked\nauto-encoder for medical report generation."}, {"title": "2.2. Large Language Models", "content": "The combination of medical report generation and Large\nLanguage Models (LLMs) has become the most popular re-\nsearch direction in report generation. LLMs are the focus of\ncurrent research which can be divided into two categories:\nbase LLMs and professional LLMs. The most famous early\nLLM is Google's BERT [11], which understands text via\na bidirectional encoder, significantly improving the perfor-\nmance of natural language processing tasks by pre-training\non large amounts of unlabeled text and then fine-tuning on\nspecific tasks. Meta develops a foundational large language\nmodel, LLAMA [37], with several billion to several hundred\nbillion parameters. LLaMA significantly reduces comput-\ning resources and energy requirements while maintaining\nhigh performance, demonstrating broad potential in practi-\ncal applications. GPT [30] (Generative Pre-trained Trans-\nformer) series consists of large-scale language models de-\nveloped by OpenAI for natural language generation and un-\nderstanding. GPT-3 [4] is renowned for its 175 billion pa-\nrameters and its powerful capabilities in both generating and\nunderstanding language.\nFor the LLMs developed for medical domains, R2Gen-\nGPT [45] proposes a medical report generation method\nbased on LLM which combines the image and text and\nfed into the decoder Llama2-7B [38] for report generation.\nRGRG [36] applies a practice similar to object detection\ntasks to medical report generation by using GPT-2 [32] to\ngenerate separate sentences for the detected areas and then\nreconnect the sentences. MedicalGPT [48] is a healthcare\nlanguage modeling project based on the ChatGPT training\nprocess. Developed by Google, Med-PaLM2 [33] is a med-\nical language model that combines an improved founda-\ntional language model (PaLM 2 [1]), domain-specific fine-\ntuning for the medical field. Med-PaLM2 improves perfor-\nmance by over 19% compared to Med-PaLM [39] and has\nreached a new state-of-the-art level. MedVersa [54] is ca-\npable of handling multimodal medical inputs and outputs,\nsupporting real-time task specification. Inspired by these\nwork, we propose to further improve the quality of X-ray\nmedical reports via large language models in this paper."}, {"title": "2.3. State Space Model", "content": "Due to the high computational cost in the widely used\nTransformer networks, the State Space Model (SSM) [44]\nis proposed to achieve linear complexity. To be specific,\nthe S4 [15] model introduces a new structured state space\napproach through a layered and modular design. This im-\nproves the modeling ability of long sequence dependencies\nand significantly enhances the efficiency and accuracy of se-\nquence modeling. S5 [35] is an in-depth improvement and\nsimplification of the S4 model, aiming to enhance compu-\ntational efficiency and ease of use while retaining powerful\nsequence modeling capabilities. Mamba [14] proposes a"}, {"title": "2.4. Context Sample Retrieving", "content": "Retrieval-Augmented Generation (RAG) is a hybrid ap-\nproach that combines retrieval and generation to enhance\nthe performance of NLP tasks, particularly those requir-\ning extensive knowledge and contextual information. BG-\nFormer [42] (Batch-Graph Transformer) introduced a new\nTransformer architecture called SSA (Structure-constrained\nSelf-attention), which deeply mines the relationships be-\ntween samples to provide a new method for robust and dif-\nferentiated data representation and learning. CricaVPR [29]\ngenerates more robust image features by correlating mul-\ntiple images in a batch through an attention mechanism,\nusing cross-image differences like perspective and illumi-\nnation as cues for feature learning. CRAG [50] intro-\nduces a search evaluator to assess the quality of retrieved\ndocuments and enhance search results through large-scale\nweb searches. At the core of EgoInstructor [47] is the\nretrieval-augmented module, which utilizes existing third-\nperson video resources to help the model better understand\nand describe first-person perspective video content. Re-\ntrieval augmentation can significantly improve generation\nquality and reduce dependence on the size of the training\ndataset. RALF [17] (Retrieval-Augmented Layout Trans-\nformer), proposed by Horita et al., enhances the gener-\nation process by retrieving layout examples most similar\nto the input image, thereby overcoming the challenge ex-\nisting methods face in capturing high-dimensional layout\nstructures when training data is limited. EVCAP [22] is a"}, {"title": "3. Methodology", "content": "In this section, we will first give a review of the Mamba\nnetwork and an overview of our proposed R2GenCSR\nframework. Then, we will dive into the details of the\nR2GenCSR framework, with a focus on Input Represen-\ntation, Context Sample Retrieval, LLM for Report Genera-\ntion, and Loss Function. More details will be introduced in\nthe subsequent subsections."}, {"title": "3.1. Preliminary: Mamba", "content": "Current widely used Mamba networks are developed\nbased on the continuous State Space Model (SSM). It maps\na one-dimensional function or sequence $x(t) \\in \\mathbb{R}^p$ to\n$y(t) \\in \\mathbb{R}^q$ through a hidden state $h(t) \\in \\mathbb{R}^n$. The com-\nputing procedure can be summarized as follows:\n$h'(t) = Ah(t) + Bx(t)$,\n(1)\n$y(t) = Ch(t)$,\n(2)\nwhere $A \\in \\mathbb{R}^{n \\times n}, B \\in \\mathbb{R}^{n \\times p}, C\\in \\mathbb{R}^{q \\times n}$ denotes the state\nmatrix, input matrix, and output matrix.\nAs the image and text we processed are discrete data,\nthe aforementioned continuous SSMs needed to be trans-\nformed into discrete ones. For example, the S4 [15] and\nMamba model adopts the Zero-Order Hold (ZOH) to real-\nize this goal, i.e.,\n$A = exp(\\Delta A)$,\n(3)\n$B = (\\Delta A)^{-1}(exp(\\Delta A) \u2013 I)\\cdot \\Delta B$.\n(4)\nwhere the $ \\Delta $ is a timescale parameter (also called step size).\nThus, we can reformulate the discrete version of SSM as:\n$h_t = Ah_{t-1} + Bx_t$,\n(5)\n$y_t = Ch_t$.\n(6)\nTo further strengthen the SSM, Gu et al. propose the\nMamba [14] which makes the model varying from time-\ninvariant to dependent. And also speed the training and in-\nference using a couple of hardware-aware algorithms. In-\nspired by the success of Mamba in natural language pro-\ncessing, researchers also adapt it to the computer vision"}, {"title": "3.2. Overview", "content": "In this paper, we propose a novel contextual sample re-\ntrieval guided large language model framework for efficient\nX-ray medical report generation. As shown in Fig. 2, it\ncan be divided into three main parts, i.e., the Mamba vi-\nsion backbone, context retrieval module, and large language\nmodel (LLM) for report generation. Given the X-ray image,\nwe first extract its visual tokens using the Mamba backbone.\nMeanwhile, we retrieve context samples (X-ray samples\nwith and without disease) from the training subset based on\nthe input image and embed them into visual and text tokens.\nThen, the residual tokens which measure the difference be-\ntween the input and context samples can be obtained via the\nsubtract operator. Finally, we feed the vision tokens, con-\ntext residual tokens, and prompt statements into the LLM to\ngenerate a high-quality medical report. One can note that\nthe proposed framework stands out from existing methods\nby incorporating context retrieval and using a linear com-\nplexity vision backbone, which enhances feature representa-\ntion and discriminative learning while maintaining compu-\ntational efficiency."}, {"title": "3.3. R2GenCSR Framework", "content": "In this subsection, we will introduce the R2GenCSR\nframework from the perspective of Input Representation,\nContext Sample Retrieval, LLM for Report Generation, and\nLoss Function.\n3.3.1 Input Representation\nAssume the dataset contains N X-ray images, $X$ =\n{$X_1,X_2,...,X_N$}, where each $x_i \\in \\mathbb{R}^{C \\times H \\times W}$ represents\na X-ray image with C channels, height H, and width W.\nFor each X-ray image x, the corresponding feature map\n$v \\in \\mathbb{R}^{H\\times W \\times \\hat{C}} = VMamba(x)$ can be obtained after feed-\ning it into the VMamba backbone, where H and W are the\nspatial dimensions of the feature map, and $ \\hat{C} $ is the num-\nber of feature channels. The reason our framework adopts\nVMamba instead of conventional visual Transformer mod-\nels, such as ViT and Swin-Transformer, is because the com-\nputational complexity of this model is linear (O(N)), re-\nquiring lower computational resources. As shown in Fig. 2,\nthe basic VMamba block consists of Layer Normalization\n(LN), Linear Layer, DW-Conv layer, SiLU activation layer,\nSS2D module, and also the skip connections.\nThen, two distinct types of representations are generated\nbased on feature map v, i.e., global features $v_G$ and sequen-\ntial tokens $v_S$. Specifically, a 2D global average pooling is\napplied to v over the spatial dimensions to get the global"}, {"title": "3.3.2 Context Sample Retrieval", "content": "Given the visual tokens of the input X-ray images, one\ncan directly feed them into the large language model to\ngenerate the medical reports, clearly, this approach will\nstill reach its performance bottleneck. The context learning\nmethods [12, 42] suggest that other samples in the training\nset may still play a positive role in each round of model op-\ntimization. Therefore, we consider designing a method to\nmine samples that are positively and negatively correlated\nwith the current sample to enhance discriminative feature\nlearning, thereby better guiding the LLM to generate accu-\nrate medical reports.\n\u2022 Positive and Negative Sample Selection. For each sam-\nple in a mini-batch, we can retrieve its context samples"}, {"title": "\u2022 Prompt Construction.", "content": "All operations are conducted on\nhigh-dimensional language space. Each token, including\nthose in the residuals and the original text, is converted into\nan embedding vector by the previous step, which ensures all\nelements of the prompt are represented in a manner that the\nLLM can effectively understand and process. Therefore, the\nfinal prompt for the LLM is constructed by concatenating\nthe residuals and tokenized features as follows:\n$Prompt = [R_t, R_t^-, R_t^+, R, R^+, T, U_s, T]$\n(12)\nHere, T and $v_S$ are the tokenized text and vision token rep-\nresentations of the current image, respectively. This struc-\ntured input is fed into the LLM to generate the final medical\nreport, incorporating both visual and textual context in a co-\nherent and informative manner."}, {"title": "3.3.3 LLM Head & Loss Function", "content": "The Large Language Model plays a central role in gen-\nerating detailed and accurate medical reports from X-ray\nimages. As described in the previous sections, the input to\nthe LLM consists of a composite prompt that includes both\nvisual and textual residuals derived from positive and neg-\native context samples, along with the tokenized text corre-\nsponding to the input X-ray image. Once the contextual in-\nformation has been integrated, the LLM generates the med-\nical report. The generation process involves decoding the\nembedded and contextually enriched prompt into a coher-\nent and comprehensive text. In our experiments, various\nLLMs are evaluated to achieve higher performance, includ-\ning lightweight LLM Qwen1.5 (0.5B, 1.8B, 4B, 7B) [2],\nthe medium-sized Llama2 (7B) [38], Llama3 (8B) [13], and\nlarge-scale MedicalGPT (13B) [48]. For even larger LLMs,\nconsidering the computational cost, this paper will not con-\nsider them for the time being.\nTo optimize our R2GenCSR framework, we adopt the\ncross-entropy loss function to measure the difference be-\ntween the generated medical reports and the ground truth\nannotations. Specifically, we apply instruction-tuning to the\nLLM to generate medical reports, maintaining its original\nauto-regressive training objective. During this process, the\nLLM is fine-tuned specifically on the tokens of the medical\nreport, guided by the instruction prompt that encapsulates\nthe visual and textual residuals. Our loss function is defined\nas the negative log-likelihood of the sequence of report to-\nkens. This can be formulated as:\n$L = -\\sum_{i=1}^T log \\,p_\\theta (y_i|Prompt, Y_{r,<i})$\n(13)\nwhere $ \\theta $ denotes the trainable parameters, and T is the\nlength of the whole medical report. $y_i$ is the token being\npredicted at the current step i, Prompt is the instruction\nprompt that includes the residuals and tokenized features,\nand $ Y_{r,<i}$ is the sequence of report tokens before the cur-\nrent prediction token $y_i$. The instruction-tuning ensures that\nthe LLM generates a report that aligns with the provided in-\nstructions and context, thus producing a coherent and infor-\nmative medical report."}, {"title": "4. Experiment", "content": "4.1. Datasets and Evaluation Metrics\nWe evaluate the performance of our model on three\ndatasets, including IU-XRay [10], MIMIC-CXR [21], and\nCheXpert Plus [6] dataset. A brief introduction to these\ndatasets is given below.\n\u2022 IU-XRay [10] is one of the most widely used publicly\naccessible medical image datasets in the field of medical\nreport generation, which was released on year 2016. It con-\ntains 7,470 images and 3,955 radiology reports, each con-\nsisting of four parts: indication, comparison, Findings, and\nImpression. For a fair comparison, we have used the same\ndataset partition protocols as R2GenGPT and set the train-\ning/test/val for the dataset to 7:1:2.\n\u2022 MIMIC-CXR [21] is a large publicly available dataset\nof chest radiographs with free-text radiology reports. These\nrecords, comprising 377, 110 radiographic images and 227,\n835 radiology reports collected from 65, 379 individuals,\nspan the years 2011-2016 and originate from the Beth Is-\nrael Deaconess Medical Center Emergency Department in\nBoston, MA. For a fair comparison, we used the same\ndataset partition protocols as R2GenGPT, where 270,790\nsamples were used to train the model, and another 2,130\nand 3,858 samples were used as validation and test sets, re-\nspectively.\n\u2022 CheXpert Plus [6] is a large, newly released, organized\nmed dataset with 223K radiology report-X-ray pairs from\n64.7K patients, with each report detailed into 11 sections\nand X-rays in DICOM format with 47 metadata elements.\nIt's annotated for 14 chest conditions and patient metadata.\nWe utilize the Findings as our ground truth and randomly\npartition the dataset into a ratio of 7:1:2, which consists of"}, {"title": "4.2. Implementation Details", "content": "In our experiments, the input X-ray image is default re-\nsized as 224 \u00d7 224, and the beam search is adopted for the\nreport generation. The beam width is set as 5 and 3 for\nthe IU-Xray and MIMIC-CXR datasets, respectively. The\ntraining procedure is conducted on a server with NVIDIA\nA800 80GB GPUs using a mixed precision. We train the\nproposed R2GenCSR for 20 and 25 epochs on the MIMIC-\nCXR and IU-Xray dataset. Mini-batch sizes are 36 and\n32 for the MIMIC-CXR and IU-Xray datasets, respectively,\nboth trained at a learning rate of 1e-4. The CheXpert Plus\ndataset adopts the same training protocol as MIMIC-CXR.\nMore details can be found in our source code."}, {"title": "4.3. Comparison on Public Benchmarks", "content": "\u2022 Results on IU-Xray dataset.\nAs shown in Table 1,\nwe compare our results to state-of-the-art (SOTA) meth-\nods on the IU X-Ray datasets. It is important to note that\nthe R2GenGPT method's reported results were based on\na concatenation of Impression and Findings as the test-\ning ground truth, which we believe is not representative\nof general scenarios so we re-trained their method using\nonly with Findings. Our method demonstrates competitive\nperformance, achieving a BLEU-4 score of 0.206, which\nsurpasses existing methods, highlighting the effectiveness\nof our context-guided efficient X-ray medical report gen-\neration framework. Additionally, our approach attains in"}, {"title": "4.4. Component Analysis", "content": "In the general scenario, we conduct a detailed compo-\nnent analysis of our proposed framework on the IU-Xray\ndataset to investigate the impact of each key module, in-\ncluding VMamba, the utilization of context information,\nfixed pair strategy, and the performance of different Lan-\nguage Models (Qwen1.5 and Llama2), as shown in the Ta-\nble 3. The results #1 and #4 indicate that with all the com-\nponent existent can significantly improve the performance\nacross various evaluation metrics (Bleu-1, Bleu-4, ROUGE-L, and METEOR). Specifically, we use Swin Transformer\ninstead while VMamba absent, by comparing #2 with #8\nand #6 with #7 that the VMamba module outperforms the\nSwin Transformer in Bleu-1, Bleu-4, ROUGE-L, it confirm\nthat Vmamba extracting efficient visual features from X-\nray images. When comparing #1 with #2 and #4 with #6,\nthe utilization of context is shown to facilitate the model\nin producing high-quality reports. Furthermore, compared\n#1 with #3 and #4 with #5, the fixed pair strategy resulting\nin improved Bleu-4 and METEOR metrics for both Llama2\nand Qwen1.5 backends, is contribute to effectively utiliz-\ning both positively and negatively samples. The #7 with #8"}, {"title": "4.5. Ablation Study", "content": "In this section, we conduct a series of ablation studies to\nevaluate the impact of various components within our pro-\nposed context-guided efficient X-ray medical report gener-\nation framework.\n\u2022 Analysis of different VMamba backbones. As shown\nin Table 4, we evaluate the Tiny, Small, and Base ver-\nsions of VMamba on the IU-Xray dataset. As the scale\nof the VMamba backbone increases, the performance met-\nrics show a consistent improvement. The Base version of"}, {"title": "4.6. Visualization", "content": "\u2022 Feature Maps. The X-ray image and its correspond-\ning feature map are displayed side by side in Figure 4. It is\nevident from the feature map that the VMamba vision back-\nbone effectively extracts discriminative visual features from\nthe X-ray image, emphasizing regions of interest such as le-\nsions, organs, and other anomalies. These extracted features"}, {"title": "4.7. Limitation Analysis", "content": "Although our proposed R2GenCSR achieves better per-\nformance on three large-scale report generation benchmark\ndatasets, however, our model may still limited by the fol-\nlowing issues: 1). We adopt a simple retrieval strategy for\ncontext sample mining, more advanced retrieval techniques\ncan be exploited to achieve better performance; 2). The\nknowledge about the disease is ignored in our R2GenCSR\nframework, this information may be useful to guide the X-\nray image report generation task. In our future works, we\nwill consider to improve the proposed framework from the\naforementioned two aspects."}, {"title": "5. Conclusion", "content": "In this paper, we have developed a novel context-guided\nefficient X-ray medical report generation framework with\nthe aim of enhancing the performance of Large Language"}]}