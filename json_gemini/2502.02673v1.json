{"title": "MedRAX: Medical Reasoning Agent for Chest X-ray", "authors": ["Adibvafa Fallahpour", "Jun Ma", "Alif Munim", "Hongwei Lyu", "Bo Wang"], "abstract": "Chest X-rays (CXRs) play an integral role in driv-ing critical decisions in disease management andpatient care. While recent innovations have ledto specialized models for various CXR interpre-tation tasks, these solutions often operate in iso-lation, limiting their practical utility in clinicalpractice. We present MedRAX, the first versatileAI agent that seamlessly integrates state-of-the-art CXR analysis tools and multimodal large lan-guage models into a unified framework. MedRAXdynamically leverages these models to addresscomplex medical queries without requiring addi-tional training. To rigorously evaluate its capa-bilities, we introduce ChestAgentBench, a com-prehensive benchmark containing 2,500 complexmedical queries across 7 diverse categories. Ourexperiments demonstrate that MedRAX achievesstate-of-the-art performance compared to bothopen-source and proprietary models, representinga significant step toward the practical deploymentof automated CXR interpretation systems. Dataand code have been publicly available at https://github.com/bowang-lab/MedRAX.", "sections": [{"title": "1. Introduction", "content": "Chest X-rays (CXRs) have been widely used to makecritical decisions in disease detection, diagnosis, andmonitoring, comprising the largest proportion of over4.2 billion diagnostic radiology procedures performedannually worldwide (United Nations Scientific Committeeon the Effects of Atomic Radiation, 2022). However, thesystematic evaluation of key anatomical structures placesa significant time burden on radiologists, often requiringhours of careful analysis (Bahl et al., 2020).\nThe gradual introduction of AI into clinical practice hasdemonstrated promising potential to alleviate this burden.Task-specific AI models have shown success in automatingvarious aspects of CXR interpretation, from classificationand segmentation to automated report generation (Yanget al., 2017; Huang et al., 2023; Tanno et al., 2024; Ouis &Akhloufi, 2024). When integrated into clinical workflows,these tools have improved report turnaround times and in-terobserver agreement (Baltruschat et al., 2021; Ahn et al.,2022; Pham et al., 2022; Shin et al., 2023). However, thefragmented nature of these solutions\u2014each operating in iso-lation-has hindered their widespread adoption in practicalclinical settings (Erdal et al., 2023; Fallahpour et al., 2024).\nFoundation models (FMs), including large language mod-els (LLMs) and large multimodal models (LMMs), haveemerged as a promising solution to this fragmentation, en-abling unified, scalable AI-driven image-text reasoning formedical tasks. OpenAI's GPT-4 played a pivotal role in es-tablishing the dominance of this approach with its unprece-dented scale. Trained on an enormous volume of multimodaldata, it has shown exceptional performance in medical un-derstanding and reasoning without explicit training (Noriet al., 2023; Yan et al., 2023; Javan et al., 2024; Eriksenet al., 2024). LLaVA-Med (Li et al., 2024b), trained on15 million biomedical figure-caption pairs, established newbenchmarks in medical visual question answering (VQA),showcasing strong generalization in zero-shot image inter-pretation. CheXagent (Chen et al., 2024a) focused specifi-cally on CXR analysis, achieving performance comparableto GPT-4 despite using significantly fewer parameters.\nWhile FMs have significantly advanced the field, they facecritical limitations that hinder their direct clinical appli-cation. LMMs frequently experience hallucinations andinconsistencies in their reasoning, particularly concerningfor medical applications where accuracy is paramount. Theyalso struggle with the complex multi-step reasoning requiredfor diagnostic tasks, often failing to systematically evaluateall relevant anatomical structures or integrate findings acrossdifferent regions of the image. Their end-to-end architec-ture, while elegant, lacks the transparency and specializationthat comes from purpose-built medical AI tools. These lim-itations suggest the need for a more structured, tool-basedapproach that can combine the flexibility of foundation mod-els with the reliability of clinical AI systems."}, {"title": "2. Related Work", "content": "To bridge this gap, we present MedRAX, the first special-ized AI agent framework for CXR interpretation. Our keycontributions include:\n\u2022 MedRAX, a specialized AI agent framework that seam-lessly integrates multiple CXR analysis tools withoutadditional training, dynamically orchestrating special-ized components for complex medical queries.\n\u2022 ChestAgentBench, a comprehensive evaluation frame-work with 2,500 complex medical queries across 7categories, built from 675 expert-curated clinical casesto assess multi-step reasoning in CXR interpretation.\n\u2022 Experiments show that MedRAX outperforms bothgeneral-purpose and biomedical specialist models,demonstrating substantial improvements in complexreasoning tasks while maintaining transparent work-flows.\n\u2022 Development of a user-friendly interface, enabling flex-ible deployment options from local to cloud-based so-lutions that address healthcare privacy requirements.\n2.1. LLM-based Agent Architectures\nThe emergence of AI agents built upon LLMs has funda-mentally changed how we approach autonomous reason-ing, planning, and tool use. Recent surveys on LLM-basedagents (Xi et al., 2025; Zhao et al., 2023; Masterman et al.,2024) have outlined a generalizable agent framework com-prising three core components: (1) a reasoning engine drivenby LLMs, (2) perceptual modules that process multimodalinputs, and (3) action mechanisms that execute API calls,retrieve information, or interact with external tools.\nThis paradigm shift has enabled AI agents to surpass tradi-tional task-specific models by dynamically adapting to di-verse applications without additional training. However, de-spite these advances, there are very few LLM-based agentsthat have been evaluated for domain-specific robustness,particularly in high-stakes medical applications where hal-lucinations, lack of systematic reasoning, and specializedtool integration remain significant challenges.\n2.2. Medical Agents\nBy enabling LMMs to operate in a collaborative, agentic set-ting, frameworks such as MDAgents (Kim et al., 2024) havedemonstrated enhanced clinical reasoning through multi-agent interaction. Similarly, MMedAgent (Li et al., 2024a)explores tool integration across multiple medical imagingmodalities, allowing LMMs to leverage external machinelearning models for more robust decision-making.\nHowever, MDAgents introduces significant computationaloverhead due to multi-agent coordination, while MMedA-gent's broad focus across imaging modalities may diluteits domain-specific expertise. Additionally, MMedAgentrequires retraining to integrate new tools, reducing its flexi-bility for adapting to evolving clinical workflows.\nMore recently, o1-powered AI agents (Jaech et al., 2024)have been proposed as an alternative to traditional model-based approaches, demonstrating strong multi-step reason-ing and improved diagnostic consistency. However, thesesystems also face critical challenges: (1) high computationaldemands, making them impractical for real-time applica-tions, (2) closed-source and proprietary nature, limitingcustomization and adaptation to specific medical require-ments, and (3) redundant reasoning in simpler tasks, leadingto inefficiencies in tool selection and execution.\n2.3. Evaluation Frameworks\nTo systematically evaluate LLM-based agents, severalbenchmarks have been introduced. AgentBench (Liu et al.,2023) assesses multi-step reasoning, memory retention, tooluse, task decomposition, and interactive problem-solving,revealing that even top-performing models like GPT-40 andClaude-3.5-Sonnet struggle with long-term context reten-tion and autonomous decision-making. Expanding on theselimitations, MMAU (Yin et al., 2024) evaluates agent capa-bilities across five domains-tool use, graph-based reason-ing, data science, programming, and mathematics. Resultshighlight persistent weaknesses in structured reasoning anditerative refinement.\nIn software engineering, SWE-bench (Jimenez et al., 2023)presents 2,294 real-world GitHub issues to evaluate LLMs'ability to modify large codebases. By January 2025, thebest-performing agent has solved less than 65% of issues,underscoring the challenges of multi-file reasoning and iter-ative debugging. These benchmarks collectively highlightLLM agents' deficiencies in contextual understanding, struc-tured planning, and domain-specific tool use, reinforcingthe need for specialized, clinically validated AI frameworksin high-stakes applications such as medical imaging.\nBeyond general-purpose benchmarks, MedAgentBench(Jiang et al., 2025) assesses LLMs' ability to retrieve pa-tient data, interact with clinical tools, and execute structureddecision-making in interactive healthcare environments. Re-sults indicate that even the best-performing model, GPT-40,achieves only 72% accuracy, with substantial performancevariability across different medical tasks. These findingsreinforce the need for domain-specific benchmarks that eval-uate AI agents not just on general reasoning but on theirability to integrate into real-world clinical workflows."}, {"title": "3. MedRAX", "content": "We present MedRAX, an open-source agent-based frame-work that can dynamically reason, plan, and execute multi-step CXR workflows. Compared to previous approaches(Chen et al., 2024b; Bansal et al., 2024), MedRAX integratesmultimodal reasoning abilities with structured tool-baseddecision-making, allowing real-time CXR interpretationwithout unnecessary computational overhead. By balanc-ing computational efficiency with domain specializationand eliminating the need for retraining when incorporatingnew tools, MedRAX offers greater adaptability to evolvingclinical needs. Our framework integrates heterogeneousmachine learning models from lightweight classifiers tolarge LMMs-specialized for diverse downstream tasks, al-lowing it to decompose and solve complex medical queriesby reasoning across multiple analytical skills.\n3.1. LLM Driven Agent\nMedRAX employs a LLM as the core to drive a ReAct(Reasoning and Acting) loop, which breaks down complexmedical queries into sequential analytical steps (Yao et al.,2023). The system processes a user query through iterativecycles of (1) observation - analyzing the current state andquery, (2) thought - determining required actions, and (3) ac-tion - executing relevant tools and integrating findings from\nprevious steps to inform subsequent reasoning. Throughout this process, the system maintains a short-term memoryof user interactions, tool outputs, and images to supportmulti-turn interactions. The reasoning loop continues untilthe system either generates a response or asks the user foradditional input (Algorithm 1).\n3.2. Flexible Tool Integration\nMedRAX integrates state-of-the-art models for variousdownstream CXR interpretation tasks:\n\u2022 Visual Question Answering (VQA).\nAnswering free-form questions about CXR images bycombining visual understanding with medical knowledge.\nModels: CheXagent, a vision-language foundation modeltrained on CheXinstruct, with over 8.5M samples across35 tasks, capable of fine-grained visual reasoning andCXR interpretation (Chen et al., 2024b).\nLlaVA-Med, a biomedical 7B VLM, trained on 600Kbiomedical image-caption pairs from PMC-15M and 60Kinstruction-tuning data (Li et al., 2024b).\n\u2022 Segmentation.\nPartitioning CXR images into semantically meaningfulregions by assigning each region to anatomical structures."}, {"title": "3.3. Modularity", "content": "MedRAX is built on the LangChain and LangGraph frame-works. The reasoning engine can be any LLM, accommo-dating both text-only and multimodal models, from open-source to proprietary. This flexibility enables deploymentsranging from local installations to cloud-based solutions,addressing diverse healthcare privacy requirements. Ourreference implementation uses GPT-40 with vision, whilesupporting integration of alternative models.\nEach tool operates as an independent module with definedloading and inference. Tools can be modified, replaced,or repurposed for multiple tasks without affecting othercomponents. Integration of new tools requires only a classdefinition specifying the tool's input/output formats andcapabilities, with the LLM learning its usage without anytraining. The framework decouples tool creation from agentinstantiation, enabling multiple agents to share tools andallowing each to access its own customized set of tools."}, {"title": "3.4. User-friendly Interface", "content": "MedRAX includes a production-ready interface built withGradio that facilitates seamless deployment in clinical set-tings. The interface supports uploading of radiological im-ages in all standard formats, including DICOM, and main-tains an interactive chat session for natural multi-turn inter-actions. The interface further provides transparency into toolexecution by tracking and displaying intermediate outputs.This end-to-end implementation enables quick integrationof MedRAX into existing clinical workflows."}, {"title": "4. ChestAgentBench", "content": "Existing medical VQA benchmarks typically focus on sim-ple, single-step reasoning tasks. In contrast, ChestAgent-Bench offers several distinctive advantages:\n\u2022 It represents one of the largest medical VQA benchmarks,with 2,500 questions derived from expert-validated clini-cal cases, each with comprehensive radiological findings,detailed discussions, and multi-modal imaging data.\n\u2022 The benchmark combines complex multi-step reasoningassessment with a structured six-choice format, enablingboth rigorous evaluation of advanced reasoning capabili-ties and straightforward, reproducible evaluation.\n\u2022 The benchmark features diverse questions across sevencore competencies in CXR interpretation, requiring inte-gration of multiple visual findings and reasoning to mirrorthe complexity of real-world clinical decision-making.\n4.1. Dataset\nWe utilize Eurorad, the largest peer-reviewed radiologicalcase report database maintained by the European Society ofRadiology (ESR). This database contains detailed clinicalcases consisting of patient histories, clinical presentations,and multi-modal imaging findings. Each case includes de-tailed radiological interpretations across different modalities,complemented by in-depth discussions that connect findingswith clinical context, and concludes with reasoned interpre-tations, differential diagnosis list and a final diagnoses.\nFrom its chest imaging section, we curated 675 patient caseswith associated chest X-rays and complete clinical docu-mentation. These cases covered 53 unique areas of interestincluding lung, thorax, and mediastinum.\n4.2. Benchmark Creation\nChestAgentBench comprises six-choice questions, each de-signed to evaluate complex CXR interpretation capabilities.\nWe first established seven core competencies alongside rea-soning that are essential for CXR interpretation:\n\u2022 Detection: Identifying specific findings. (e.g., \"Is there anodule present in the right upper lobe?\")\n\u2022 Classification: Classifying specific findings. (e.g., \"Isthis mass benign or malignant in appearance?\")\n\u2022 Localization: Precise positioning of findings. (e.g., \"Inwhich bronchopulmonary segment is the mass located?\")\n\u2022 Comparison: Analyzing relative sizes and positions.(e.g., \"How has the pleural effusion volume changed com-pared to prior imaging?\")\n\u2022 Relationship: Understanding relationship of findings.(e.g., \"Does the mediastinal lymphadenopathy correlatewith the lung mass?\")\n\u2022 Diagnosis: Interpreting findings for clinical decisions.(e.g., \"Given the CXR, what is the likely diagnosis?\")\n\u2022 Characterization: Describing specific finding attributes.(e.g., \"What are the margins of the nodule - smooth, spic-ulated, or irregular?\")\n\u2022 Reasoning: Explaining medical rationale and thought.(e.g., \"Why do these findings suggest infectious ratherthan malignant etiology?\")\nThese competencies are combined into five question types,each designed to evaluate specific combinations of corecompetencies while requiring medical reasoning:\n\u2022 Detailed Finding Analysis: detection, localization, andcharacterization\n\u2022 Pattern Recognition & Relations: detection, classifica-tion, and relationships\n\u2022 Spatial Understanding: localization, comparison, andrelationships\n\u2022 Clinical Decision Making: classification, comparison,and diagnosis\n\u2022 Diagnostic Characterization: classification, characteri-zation, and diagnosis\nFor each clinical case and question type, we first promptedGPT-40 to analyze the case and generate a six-choice ques-tion that would best assess the target analytical skills ofthat question type. We then instructed it to ensure the ques-tion has the necessary context from the clinical case and itscorrect answer could be explicitly verified from the case'sradiological findings and discussion.\nThe benchmark uses a straightforward accuracy metric (per-centage of correct answers) to enable easy evaluation acrossdifferent agent architectures. All questions underwent qual-ity check, during which we removed questions that exhibitedissues such as ungrounded answers or missing information."}, {"title": "5. Experiments", "content": "5.1. Implementations\nMedRAX uses GPT-40 as its backbone LLM and is de-ployed on a single NVIDIA RTX 6000 GPU, using thesame configuration as described in Section 3. It integratesCheXagent (Chen et al., 2024b) and LLaVA-Med (Li et al.,2024b) for visual QA, Maira-2 for grounding (Bannur et al.,2024), a model trained on ChestX-Det for segmentation(Lian et al., 2021), TorchXRayVision for classification (Co-hen et al., 2022), and a model trained on CheXpert Plus forreport generation (Chambon et al., 2024).\nMedRAX implements tool execution with structured JSONAPI calls, where the agent formulates precise requests withrequired arguments (e.g., image paths, text prompts) to calltarget tools. All baseline models are evaluated using theirofficial implementations and recommended configurations.\nModel responses are processed using regex to extract letterchoices. For unclear responses, errors, or timeouts, we retryup to three times. Responses that remain invalid or do notchoose a single choice are marked incorrect.\n5.2. Experimental Setup\nWe evaluate MedRAX against four models: LLaVA-Med, afinetuned LLaVA-13B model for biomedical visual questionanswering ((Li et al., 2024b), CheXagent, a Vicuna-13BVLM trained for CXR interpretation (Chen et al., 2024b),along with GPT-40 and Llama-3.2-90B Vision as popularclosed and open-source multimodal LLMs respectively.\nWe evaluate models on two complementary benchmarks:(1) ChestAgentBench, our proposed benchmark describedin Section 4, which assesses comprehensive CXR reasoningthrough 2,500 six-choice questions across seven categories:detection, classification, localization, comparison, relation-ship, characterization, and diagnosis. Model performance ismeasured by accuracy across all questions.\n(2) CheXbench, a popular benchmark that evaluates sevenclinically-relevant CXR interpretation tasks. We specificallyfocus on the visual question answering (238 questions fromRad-Restruct (Pellegrini et al., 2023) and SLAKE (Liu et al.,2021) datasets) and fine-grained image-text reasoning (380questions from OpenI dataset) subsets, as they most closelymirror complex clinical workflows that require precise dif-ferentiation between similar findings."}, {"title": "5.3. Quantitative Analysis", "content": "ChestAgentBench. Shown in Table 1, MedRAX achievesconsistently state-of-the-art performance (63%) across allseven categories, a significant improvement over the base-line models. There is a clear performance hierarchy amongmodels, with GPT-40 (56.4%) and Llama-3.2-90B (57.9%)performing notably better than specialized medical modelslike CheXagent (39.5%) (Chen et al., 2024a) and LLaVA-Med (28.7%) (Li et al., 2024b). Interestingly, general-purpose VLMs outperform domain-specific ones across allcategories, with particularly large gaps in characterizationand diagnosis tasks.\nCheXbench. Shown in Table 2, we observe distinct per-formance patterns across different task types. On visualQA tasks, MedRAX demonstrates strong performance onRad-Restruct (68.7%) and SLAKE (82.9%). This notablysurpasses both domain-specific CheXagent (57.1%, 78.1%)(Chen et al., 2024a) and larger general-purpose models likeGPT-40 (53.9%, 85.4%), suggesting that our tool-based ap-proach particularly excels at fine-grained visual understand-ing. However, on image-text reasoning tasks, we observe asignificant performance drop across all models, with eventhe best-performing CheXagent achieving only 59.0% accu-racy, almost equal to random performance (50% baseline)."}, {"title": "5.4. Case Studies", "content": "We present two representative cases that compare MedRAXto GPT-40 (Figure 5.3).\nMedical Device Identification (Eurorad Case 17576).\nThis question asks the model to determine the type of tubepresent in the CXR. GPT-4o incorrectly suggests an endotra-cheal tube based on the central positiong of the tube alone.MedRAX, integrated findings from multiple tools like re-port generation and visual QA, and correctly identifies achest tube despite one tool (LLaVA-Med (Li et al., 2024b))suggesting otherwise. This demonstrates MedRAX's abil-ity to resolve conflicting tool outputs through systematicreasoning.\nMulti-step Disease Diagnosis (Eurorad Case 16703).\nThis questions asks about diagnosing the predominant dis-ease and comparing its severity across lungs. GPT-40 mis-interprets the CXR as showing pneumonia with right lungpredominance. MedRAX, through sequential tool applica-tion of report generation for disease identification and seg-mentation for lung opacity analysis, correctly determinesleft pneumothorax as the main finding. This demonstratesMedRAX's ability to break down complex queries into tar-geted analytical steps."}, {"title": "6. Discussion", "content": "Our experiments demonstrate that MedRAX achieves state-of-the-art performance in complex CXR interpretation tasks,outperforming both general-purpose and specialized medi-cal models. We discover valuable insights about structuredtool orchestration in medical AI, suggesting that a hybrid ap-proach-leveraging both large-scale reasoning capabilitiesand domain-specific expertise-offers superior performanceover purely end-to-end models.\nTask Decomposition. MedRAX demonstrates that a ReAct-based architecture dynamically composes complex reason-ing chains while maintaining computational efficiency. Theperformance gap between MedRAX and end-to-end modelssuggests that explicit decomposition of reasoning steps pro-vides advantages that scale alone cannot achieve. The pro-cess produces clear decision traces, enhancing transparencyand interpretability, with implications beyond medical imag-ing for structured model-tool integration.\nGeneralists Versus Specialists. A key insight is the su-perior performance of general-purpose models (GPT-40,Llama-3.2-90B) over specialized medical models (LLaVA-Med (Li et al., 2024b), CheXagent (Chen et al., 2024a)).This suggests that medical model specialization may sacri-fice broader reasoning capabilities provided by large-scalepretraining. MedRAX bridges this gap by integratingdomain-specific tools while maintaining generalist reason-ing, demonstrating the benefits of hybrid architectures.\nLimitations. While MedRAX excels in structured reason-ing, it sometimes struggles with resolving contradictorytool outputs, particularly in fine-grained visual tasks whenclassification and segmentation tools provide conflicting in-terpretations of subtle patterns. Additionally, the system'scomputational overhead from running multiple specializedtools can impact response times compared to end-to-endmodels. The framework also lacks robust uncertainty quan-tification mechanisms.\nFuture Work. Our initial observations suggest the impor-tance of balanced tool utilization, where neither completereliance on tools nor their complete absence produced op-timal results. While formal analysis is needed, our find-ings indicate that prompting strategies encouraging criticalevaluation of tool outputs may play a key role in systemperformance. This presents an interesting direction for un-derstanding the interaction between LLM reasoning andtool utilization. Future work would consider applying re-inforcement learning to boost reasoning ability, as verifiedin DeepSeek-R1 (Guo et al., 2025). Additionally, compre-hensive clinical validation will be crucial for establishingMedRAX's practical utility in real-world settings."}, {"title": "7. Conclusion", "content": "MedRAX establishes a new benchmark in AI-driven CXRinterpretation by integrating structured tool orchestrationwith large-scale reasoning. Our evaluation on ChestA-gentBench demonstrates its superiority over both general-purpose and domain-specific models, reinforcing the advan-tages of explicit stepwise reasoning in medical AI. Thesefindings highlight the potential of combining foundationmodels with specialized tools, a principle that could be ap-plied to broader domains in healthcare and beyond. Futurework should focus on optimizing tool selection, uncertainty-aware reasoning, and expanding MedRAX's capabilities tomultimodal medical imaging for greater clinical impact."}]}