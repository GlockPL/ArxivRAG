{"title": "Mitigation of Camouflaged Adversarial Attacks in Autonomous Vehicles\u2014A Case Study Using CARLA Simulator", "authors": ["Yago Romano Martinez", "Brady Carter", "Abhijeet Solanki", "Wesam Al Amiri", "Syed Rafay Hasan", "Terry N. Guo"], "abstract": "Autonomous vehicles (AVs) rely heavily on cameras and artificial intelligence (AI) to make safe and accurate driving decisions. However, since AI is the core enabling technology, this raises serious cyber threats that hinder the large-scale adoption of AVs. Therefore, it becomes crucial to analyze the resilience of AV security systems against sophisticated attacks that manipulate camera inputs, deceiving AI models. In this paper, we develop camera-camouflaged adversarial attacks targeting traffic sign recognition (TSR) in AVs. Specifically, if the attack is initiated by modifying the texture of a stop sign to fool the AV's object detection system, thereby affecting the AV actuators. The attack's effectiveness is tested using the CARLA AV simulator and the results show that such an attack can delay the auto-braking response to the stop sign, resulting in potential safety issues. We conduct extensive experiments under various conditions, confirming that our new attack is effective and robust. Additionally, we address the attack by presenting mitigation strategies. The proposed attack and defense methods are applicable to other end-to-end trained autonomous cyber-physical systems.", "sections": [{"title": "I. INTRODUCTION", "content": "The transportation landscape is undergoing a significant transformation with the rapid development of AVs, which are becoming an integral part of connected intelligent trans-portation systems [1]-[3]. AVs hold the potential to enhance road safety, improve mobility access, and reduce transportation costs, revolutionizing the way we travel. This transformation is largely driven by the advanced perception system, which enables AVs to comprehend and navigate the dynamic driving environment. A key element of AV development is traffic sign recognition (TSR), which is a crucial component that ensures the safe and effective operation of these AVs by identifying and interpreting road signs in real time [4]. TSR involves several stages: the acquisition of traffic sign images, preprocessing, detection, and ultimately the classification of traffic signs [5]. This process allows AVs to accurately perceive and respond to traffic signals, contributing to safer navigation and adherence to road regulations. Unlike other perception tasks that can rely on a fusion of sensor data, such as obstacle detection or lane tracking, TSR predominantly hinges on data from camera sensors [6]. The AV relies on a camera to capture images of traffic signs, which are then transmitted to an AI model, specifically Deep neural networks (DNNs), for interpretation and decision-making. DNNs have shown remarkable effectiveness in recognizing traffic signs under standard conditions, achieving accuracy rates that exceed human capabilities [7].\nHowever, the use of DNN in TSR has exposed a vulnerabil-ity to adversarial attacks targeting AV sensors. These attacks alter traffic sign images to generate \u201cadversarial examples\u201d, which deceive the DNNs, potentially leading to incorrect interpretation of traffic signs and dangerous decisions by AVs. Therefore, ensuring the robustness of AV systems against adversarial sensor attacks, particularly those targeting camera perception, is critical for their secure operation.\nRecent research studies [8], [9] have demonstrated that adversarial attacks, such as modifying signs using stickers or light projections, can cause autonomy stacks to misclassify traffic signs. For instance, these attacks might cause a stop sign being mistakenly identified as speed limit sign, potentially causing dangerous situations or accidents. However, these studies have some limitations. First, human drivers can often detect these attacks. Drivers may notice stickers or lighting inconsistencies that do not match the surrounding environment on a sign, limiting the effectiveness of these attacks in semi-autonomous AVs (e.g., Tesla's), where the driver can intervene and regain control. Additionally, these studies do not address scenarios where attacks are undetectable by humans, leaving uncertainty about the risks posed to fully autonomous systems [10]. Therefore, there is an urgent need to analyze more sophisticated cyberattacks, those that do not require physical access and can deceive systems by altering objects remotely, to gain critical insights for ensuring AVs safety in the face of evolving cyber threats. Vision attacks, which manipulate camera inputs, can significantly disrupt object detection sys-tems, leading to potential navigation errors and safety risks. The existing studies have explored the affect of adversarial attacks on the machine learning model capability. However, to the best of authors' knowledge, joint investigation of attack and defense of such attacks in a controlled and realistic AV traffic simulator is not well studied.\nIn this paper, our research focuses on simulating and ana-lyzing the impacts of adversarial attacks on camera perception, which directly affects object detection and, consequently, the navigation tasks of AVs. Specifically, we aim to:\n1) Evaluate the resilience of AV security systems against camouflage adversarial attacks that manipulate camera inputs."}, {"title": "II. SIMULATION ENVIRONMENT", "content": "Our research utilizes the CARLA AV simulator [11], which offers a highly customized and controlled environment for conducting experiments and collecting data that would be im-practical or hazardous in real-world scenarios. This allows for in-depth testing of adversarial attack strategies and defensive measures without the costs and risks of real-world testing.\nIntegrating the Robotic Operating System (ROS) [12] with CARLA creates a robust and modular framework for studying the effects of adversarial attacks. The CARLA-ROS-Bridge architecture ensures synchronized data exchange among all sensors and processing units via standardized messaging formats in communication protocols. This enables real-time operational testing of AV navigation systems' responses to adversarial attacks and the effectiveness of their countermea-sures. The integration is essential for simulating complex attack patterns and thoroughly evaluating AV systems in real-world scenarios. By leveraging CARLA and ROS, our research aims to identify critical vulnerabilities in camera perception and devise robust strategies to bolster the cybersecurity of AVs. The insights gained from this research are imperative for ensuring AVs' safe and secure operation against evolving cyber threats. As we move towards a future where AVs are ubiquitous on our roads, our findings will contribute to developing more resilient AV systems, enhancing public trust and safety."}, {"title": "III. THREAT MODEL AND OVERVIEW OF THE PROPOSED ATTACK AND DEFENSE APPROACH", "content": "Fig. 1 shows the architecture of the proposed adversarial attack along with defense methods. In our threat model, an attacker aims to deceive the traffic sign detection model on AVs to disrupt their actuators, potentially causing safety issues. Specifically, the attacker generates an adversarial patch via existing adversarial image generation techniques [13]\u2013[16], applying it to a traffic sign to mislead the object detection model used by the AV camera, thereby affecting the AV's ac-tuating systems. To defend against such adversarial attacks, we have implemented two defense strategies: Defense 1 (adjusted braking), which adaptively adjusts braking system based on the AV's speed and distance to a detected stop sign (which is shown on the top right hand side of Fig. 1). Defense-2 which uses sensor fusion from a side camera. Overall, this model focuses on these attack vectors and evaluates the effectiveness of the defensive measures in maintaining the AV's safety."}, {"title": "IV. METHODOLOGY OF ADVERSARIAL ATTACKS", "content": "To mimic adversarial attacks in CARLA, the images of a stop sign asset are altered as described in Fig. 2. First, CARLA and Unreal Engine (UE) 4 [17] have to be built from source in order to have direct access to the assets that need to be edited. The material instance of the object must be located in the content drawer of the Unreal Editor. Next, the referenced texture must be located and exported as a bitmap file. After exporting the texture, it is opened in GIMP (GNU Image Manipulation Program) [18] and edited to the desired adversarial attack. The unique patterns generated were inspired by research papers [13]-[16]. Once the adversarial images are created, the material instance of the stop sign must be updated to reference the adversarial image instead. A separate CARLA build is then made for each adversarial stop sign, facilitating easy and consistent testing for each attack. Additionally, instead of directly changing the texture of an asset, it is more efficient to add a sticker to the asset blueprint in the content drawer, allowing for any necessary texture to be applied to create an adversarial attack."}, {"title": "B. Attack Vectors and Effects", "content": "Different adversarial attacks are applied to modify the texture of a stop sign image, and the impact of these attacks on object detection performance is analyzed. Initially, the object detection performance is tested on a non-attacked stop sign image, achieving a detection rate of 75%, as shown in Fig. 3a. Then, the stop sign image is attacked using Chen, Eykholt, Lu version-2, Lu version-3, and Yang adversarial attacks [13]-[16]. As illustrated in Fig. 3b-3f, all of these attacks result in a noticeable drop in object detection performance compared to the non-attacked image, with Lu version-2 exhibiting the lowest detection score among all tested attacks.\nFollowing this, the impact of the attacks is tested on the AV's auto-braking system in CARLA. The AV was positioned at a distance that allowed it to reach the desired speed while keeping the stop sign outside the camera's object detection range. Once the AV reached the desired speed and the stop sign entered the detection range, the AV applied auto-braking to stop at the stop sign. It is observed that, without any attack on the stop sign image, the AV successfully performed a complete stop at the desired location. However, with the adversarial attacks on the stop sign image, the object detection perfor-mance degraded, causing a delayed auto-braking response. As a result, the AV failed to stop in time and passed the stop sign, leading to a potential safety issue."}, {"title": "C. Mitigating Attack Vectors", "content": "To address the vulnerabilities exposed by adversarial stop signs, we propose two defense methods: 1) Defense-1 (ad-justed braking) and 2) Defense-2 (sensor fusion using a side camera).\n1) Adjusted Braking: This strategy involves adjusting the AV's braking based on the distance from the stop sign and its velocity. It calculates the distance using object detection and takes the AV's velocity into account to ensure that the AV stops at the desired location. To do so, we calculate the focal length in pixels $f$ using the formula [19]:\n$f = \\frac{H_{image}}{2 \\cdot tan (\\frac{FOV}{2})}$\nwhere $H_{image}$ is the height of the camera sensor in pixels and $FOV$ is the field of view of the camera in degrees. Using the focal length, the distance $d$ to the stop sign is calculated as follows [19]:\n$d = \\frac{H_{real} f}{h_{bbox}}$\nwhere $H_{real}$ is the real height of the stop sign and $h_{bbox}$ is the height of the bounding box in pixels. After that, we adjust the braking system to account for both the current distance from the stop sign and the AV's velocity, enabling a more adaptive response and ensuring realistic testing conditions. Two additional parameters can be calculated in the following: the deceleration $a$ required to stop the AV, and the brake control message $b$ that indicates the required braking force. As described in [20], the deceleration $a$ is given by:\n$a = \\frac{v^2}{2 \\cdot d_{stop}}$\nwhere $v$ is the AV speed and $d_{stop}$ is the distance to the stop sign. The brake control message $b$ can be calculated as follows:\n$b = min (\\frac{a}{A_{max}} \\cdot M, 1)$\nwhere $a_{max}$ is the reference maximum deceleration that a typical vehicle can safely apply under ideal conditions, usually set to 9.81m/s\u00b2, M is the brake multiplier, which adjusts the applied braking force and is typically set between 0.6 and 1. Note that $b$ ranges form 0 to 1, where 0 indicates no braking and 1 indicates full braking.\n2) Sensor Fusing Using a Side Camera: This strategy leverages a side camera on the AV to apply full braking once the stop sign is detected, adding an extra layer of safety. This approach ensures that if the frontal camera is compromised, the side camera can take over and stop the AV."}, {"title": "V. ASSESSMENT RESULTS", "content": "In this section, we evaluate the effectiveness of the proposed adversarial attack and the corresponding mitigation strategies. Notably, we focus on the Lu adversarial attack, as it results in the most significant decline in object detection performance."}, {"title": "A. Adversarial Stop Sign Attacks", "content": "The testing procedure involves evaluating an AV's auto-braking to both non-attacked and attacked stop sign image in the CARLA simulator. During the test, the AV accelerates to a specified speed (85 km/h), maintaining that speed until a stop sign is detected. Once the stop sign is detected with a confidence above a certain threshold, constant braking is applied. Fig. 4a shows that the AV successfully stopped at the intended position when the non-attacked stop sign was in place. However, when an attack was applied to the stop sign, the AV passed the stop sign without stopping. Fig. 4b illustrates the final location of the AV when faced with the adversarially attacked stop sign. This finding highlights the significant impact of adversarial attacks on AV behavior, underscoring the necessity for effective countermeasures."}, {"title": "B. Adversarial Stop Sign Mitigation Strategies", "content": "To validate the effectiveness of the implemented defense strategies, we conducted a series of tests to compare the AV's stopping locations during an adversarial attack and after applying our defense strategies.\nAs given in Table I, during the attack, the AV's final position was approximately at (-5.4, -60.8), which is 10 m beyond the stop sign located at (-9.3, -50.5). With the adjusted braking defense strategy, we successfully reduced the detection distance required for effective braking, ensuring the AV stop a few meters before the stop sign. The final stopping position is illustrated in Fig. 5a. Using the side camera, the final stopping position was (-5.4, -47.5), a few meters before the stop sign, as given in Table I and depicted in Fig. 5b. These results confirm that both defense methods are effective in mitigating adversarial stop sign attacks, ensuring that the AV stops at a safe and accurate location.\nFurthermore, the effectiveness of these methods has been tested by placing the CARLA AV in different towns (maps provided by CARLA simulator), with normal and adversarial stop signs positioned at varying distances from the road, categorized as standard, far, and near. Various parameters, such as the time to complete stop and the distance to stop sign when brake is applied, are measured. Fig. 6 shows that the AV comes to a complete stop when encountering a non-attacked stop sign, regardless of its location-standard, near, or far from the road-in different towns after applying our combined defense strategies."}, {"title": "VI. CONCLUSIONS", "content": "We introduced a sophisticated framework for attacking AV camera sensors and deceiving AI models, demonstrating its effectiveness on the CARLA simulator. The attack focuses on targeting traffic sign recognition (TSR) to manipulate AV actuators. In particular, the attack modifies the texture of a stop sign to fool the AV's object detection model, delaying the brak-ing system and potentially causing safety issues. Following this, we presented and tested effective mitigation strategies that can be applied as countermeasures against adversarial attacks on TSR, ensuring AV safety. The comprehensive analysis of the proposed attack and defense methods offers valuable insights into securing and ensuring the safe operation of AVs, especially in a zero-trust environment."}]}