{"title": "Enhancing Depressive Post Detection in Bangla: A Comparative Study of TF-IDF, BERT and FastText Embeddings", "authors": ["Saad Ahmed Sazan", "Mahdi H. Miraz", "A B M Muntasir Rahman"], "abstract": "Due to massive adoption of social media, detection of users' depression through social media analytics bears significant importance, particularly for underrepresented languages, such as Bangla. This study introduces a well-grounded approach to identify depressive social media posts in Bangla, by employing advanced natural language processing techniques. The dataset used in this work, annotated by domain experts, includes both depressive and non-depressive posts, ensuring high-quality data for model training and evaluation. To address the prevalent issue of class imbalance, we utilised random oversampling for the minority class, thereby enhancing the model's ability to accurately detect depressive posts. We explored various numerical representation techniques, including Term Frequency \u2013 Inverse Document Frequency (TF-IDF), Bidirectional Encoder Representations from Transformers (BERT) embedding and FastText embedding, by integrating them with a deep learning-based Convolutional Neural Network-Bidirectional Long Short-Term Memory (CNN-BiLSTM) model. The results obtained through extensive experimentation, indicate that the BERT approach performed better the others, achieving a F1-score of 84%. This indicates that BERT, in combination with the CNN-BiLSTM architecture, effectively recognises the nuances of Bangla texts relevant to depressive contents. Comparative analysis with the existing state-of-the-art methods demonstrates that our approach with BERT embedding performs better than others in terms of evaluation metrics and the reliability of dataset annotations. Our research significantly contribution to the development of reliable tools for detecting depressive posts in the Bangla language. By highlighting the efficacy of different embedding techniques and deep learning models, this study paves the way for improved mental health monitoring through social media platforms.", "sections": [{"title": "1. Introduction", "content": "Depression is a serious condition characterised by worsening of negative emotions [1]. Prolonged sadness or enduring a difficult situation that causes continuous, unbearable sufferings can lead to depression. If left untreated, depression can even lead to the suicide [2]. According to the World Health Organization (WHO), an estimated 280 million people, i.e. 3.8% of the global population, experience\nThis research aims to enhance depressive post detection in Bangla by exploring and comparing three different embedding techniques: Term Frequency-Inverse Document Frequency (TF-IDF) [6], Bidirectional Encoder Representations from Transformers (BERT) [7] and FastText embeddings [8]. Each technique offers distinct advantages compared to others. TF-IDF is a well-established method that assigns importance to the words based on their frequency in a particular document relative to a corpus; BERT captures deep contextualised embeddings by understanding the nuances of language through a transformer architecture;"}, {"title": "2. Related Works", "content": "While there have been numerous studies conducted in widely spoken languages such as English, there is a notable lack of research in Bangla regarding depressive text analysis. This section provides a comprehensive literature review on this topic.\nThe study by Uddin et al. [7] delved into the understanding of depression through Bangla social media data. The researchers utilised advanced models such as Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM) Recurrent Neural Networks. The development of a small yet meticulously organised dataset consisting of Bangla tweets is the novelty of their work. The findings of their research demonstrated that adjusting the settings of these models, also known as hyper-parameter tuning, significantly impacted their accuracy. It has also been reported that the GRU models outperformed the LSTM models, particularly with the smaller dataset used. However, this justification was solely based on the achieved accuracy scores, which can sometimes be misleading. In fact, the reduced size of the dataset can result in a less generalisable model. This implies that the model may perform well on the reduced dataset but poorly on new, unseen data due to the lack of diverse training examples. This constitutes a significant research gap in their work.\nAnother study by Uddin et al. [8] deployed a Long Short-Term Memory (LSTM) Deep Recurrent Network for depression analysis on Bangla social media data. The study involved the creation of a small dataset of Bangla tweets, which was then stratified. The paper demonstrated the impact of hyper-parameter tuning on the efficacy of depression analysis on a small Bangla social media dataset. The data was sourced from Twitter, with 5,000 Bangla tweets collected through repeated sampling, allowing for random repetitions of tweets. The 5,000 Bangla tweets were categorised into four groups: depressive (984 tweets), non-depressive (2930 tweets), ambiguous (699 tweets) and incomplete sentences (387 tweets). The initial dataset exhibited an imbalance, with 2,930 non-depressive tweets and only 984 depressive tweets, which could lead to accuracy and overfitting issues. Consequently, 984 non-depressive tweets were chosen to balance the dataset with 984 depressive tweets, excluding ambiguous and incomplete tweets. The study applied the LSTM Deep Recurrent model to analyse Bangla tweets for predicting human depression. The findings revealed that the LSTM model with a size of 128, batch size of 25 with 10 epochs and 5 layers with 20 epochs achieved high depression detection accuracies. This suggests that high accuracy can be achieved for small datasets in complex psychological tasks such as depression analysis by tuning the Deep Recurrent model. However, it is important to note that this approach has limitations similar to those of the previous\nwork [7]. The justifications provided were solely based on the accuracy metric and down-sampling the dataset may lead to a loss of model generalisation. Moreover, smaller datasets can also impact the reliability of evaluation metrics.\nChowdhury et al. [9] aimed to automatically extract sentiment or polarity expressed by users in Bangla Twitter posts or \"tweets\". Since no labelled training corpus of Bangla tweets was available, they utilised tweets obtained from the Twitter API, which were designated for the training set, to construct the dataset for training the classifier. For this purpose, they utilised a semi-supervised method called self-training bootstrapping. Their practical findings were promising for a resource-scarce language such as Bangla, achieving an accuracy of 93% for SVM, using unigrams with emoticons as features. The underlying assumption was that users share tweets to convey opinions and subjective content, effectively narrowing down the classification problem to identifying the overall polarity of tweets as either negative or positive. To construct the training corpus, they employed a semi-supervised bootstrapping approach, eliminating the need for labour-intensive manual annotation. Drawing from previous research in English, Support Vector Machine (SVM) and Maximum Entropy (MaxEnt) were found to outperform other classifiers in this domain. Consequently, for classification, they utilised SVM and MaxEnt, conducting a comparative analysis of the performance of these two machine learning algorithms by experimenting with various sets of features. However, it is worth noting that their dataset was labelled by the people who are not expert in identifying the polarity which is one of the key limitations of their work. Non-experts could introduce biases based on their subjective understanding of the data, potentially skewing the model's performance and creating a bias toward non-expert perspectives.\nTasnim et al. [10] employed various machine learning algorithms to detect depressive Bangla text from social media posts. Feature extraction methods such as count vectorisation, TF-IDF and word embedding were applied to a dataset containing 6,178 texts gathered from social media. The dataset was self-generated and perfectly balanced. The experiment utilised Multinomial NB, Aggressive Classifier, Decision tree classifier, Neural Network and Linear Support Vector Machine. Additionally, two deep learning models, namely Bidirectional LSTM (BiLSTM) and Gated Recurrent Units (GRU), were employed. Each model was subjected to 10-fold cross-validation, with each fold serving as a test dataset for the corresponding training iteration. Their research achieved a classification accuracy of 97% using the decision tree algorithm and 94% with the bidirectional LSTM deep learning model for predicting depressive text in the Bangla language. However, similar to the work by Chowdhury et al. [9], this study also faced limitations related to the labelling of the dataset by non-experts.\nAkhter et al. [11] proposed the use of machine learning algorithms and user information to detect cyberbullying in Bangla text. They gathered a dataset from social media, labelling it as bullied or not bullied, to train various classification models. Cross-validation results showed that a support vector machine (SVM) algorithm achieved a 97% detection accuracy. The study aimed to develop a novel method for analysing Bangla content on social media by combining text analytics and machine learning algorithms and to compare its performance with other techniques. They extensively explored suitable algorithms for Bangla text categorisation, including Naive Bayes, SVM, Decision Tree and K-Nearest Neighbours, using the WEKA software platform. The experiments involved 2,400 Bangla texts from social media posts, with 10% labelled as bullying, and a 10-fold cross-validation model was used to evaluate the models' performance. It is worth noting that the model was trained using an imbalanced dataset, which can lead to bias towards the majority class. Additionally, similar to [9] and [10], the dataset was not labelled by an expert, potentially leading to biased classification.\nIn the work by Hassan et al. [12], a substantial textual dataset of both Bangla and Romanised Bangla texts was provided, marking the first of its kind. This dataset underwent pre-processing and multiple validations and was made ready for sentiment analysis (SA) implementation and experiments. Furthermore, the dataset was tested in a Deep Recurrent model, specifically Long Short-Term Memory (LSTM), using two types of loss functions: binary cross-entropy and categorical cross-entropy. Experimental pre-training was also conducted by utilising data from one validation set to pre-train the other and vice versa. Their key contributions included providing a dataset comprising 10,000 Bangla and Romanised Bangla text samples, each annotated by two adult Bangla speakers, pre-processing the data for easy usability by researchers, applying deep recurrent models to the Bangla and Romanised Bangla text corpus and experimenting with pre-training the dataset of one label for another (and vice versa) to assess potential\nimprovements in results. It is important to consider that the achieved level of accuracy may not be deemed fair. Moreover, solely prioritising accuracy without considering other factors may raise concerns about the validity of the justification.\nThese studies exhibit several common limitations, including the absence of datasets labelled by experts, the training of models on imbalanced datasets, and the reliance solely on accuracy scores, which can be misleading. Our research primarily addresses these limitations. We utilised a dataset labelled by domain experts, addressed the issue of class imbalance and provided a comprehensive set of evaluation metrics to offer a holistic view of our model's performance."}, {"title": "3. Methodology and System Architecture", "content": "Our research introduces a comprehensive method to predict depressive Bangla posts, aiming to accurately determine whether a social media post exhibits any depressive characteristics or not. The approach begins with dividing the dataset into training and testing sets to facilitate model validation. To address the imbalance in the minority class, which often poses a challenge in such datasets, random oversampling techniques have been employed. This step ensures that the training set has a more balanced representation of depressive and non-depressive posts. Next, the text undergoes preprocessing to remove noise, such as irrelevant characters and symbols, enhancing the quality of the input data. Word embedding for text vectorisation has then been utilised, transforming the textual data into numerical vectors which can be effectively processed by machine learning algorithms. With the pre-processed and vectorised data, a deep learning model tailored to predict depressive Bangla posts has then been constructed and trained. This model leverages the capabilities of neural networks to capture complex patterns and nuances in the text, improving prediction accuracy. An overview of our proposed approach, including all these steps, is illustrated in Figure 2, providing a visual representation of the entire process from data preparation to model training and evaluation. Algorithm 1 illustrates the pseudocode of the proposed methodology."}, {"title": "Algorithm 1. Proposed methodology", "content": "1. Data Preparation\n   1.1 Import necessary libraries\n   1.2 Load Data\n      1.2.1 Load the dataset containing Bangla depressive and non-depressive post\n      1.2.2 Split the data into text (X) and label (y)\n   1.3 Preprocess Data\n      1.3.1 Tokenize and clean the text data\n      1.3.2 Apply TF-IDF/BERT/FastText vectorization to convert text data into numerical vectors.\n   1.4 Handle class imbalance using random over sampler method\n2. Model Training\n   2.1 Build the CNN-BiLSTM Model\n      2.1.1 Initialize a sequential model\n      2.1.2 Add Convolutional layers for feature extraction\n      2.1.3 Add MaxPooling layer to down-sample the extracted features\n      2.1.4 Add Bidirectional LSTM layers to capture contextual information\n      2.1.5 Add Dense layers for the final classification\n   2.2 Compile the model\n   2.3 Train the model\n      2.3.1 Fit the model to training data with a specified number of epochs and batch size\n      2.3.2 Use validation set to monitor the training progress\n3. Evaluate the Model"}, {"title": "3.1. Dataset", "content": "This segment provides a description of the dataset. The dataset was created by Uddin et al. [7] and available on GitHub5. Initially, they collected the data from various tweets. As a secondary source, they also collected Bangla depressive data using user through distributing online questionnaire. The dataset was manually labelled by a sociology student, as a domain expert in human behaviours. There are a total of 2930 non-depressive posts and 984 depressive posts in the dataset. This dataset is the largest publicly available collection of Bangla depressive and non-depressive posts. Additionally, this dataset has been meticulously labelled by experts, ensuring high-quality and reliable annotations, which is a key factor in our selection. While there are other datasets, they are not publicly accessible and most of them lack expert annotation. These factors make the selected dataset the optimal choice for our study."}, {"title": "3.2. Data Split and Random Oversampling", "content": "As the dataset is imbalanced, training a model with it will be biased, and the evaluation metrics will not reflect the appropriate results. We have incorporated random oversampling to deal with this issue. Random oversampling is a technique used to address class imbalance in datasets where one class is significantly underrepresented compared to the others. Random oversampling increases the number of instances of the minority class by randomly duplicating them.\nIt is important to note that there is a necessity to oversample the training set instead of the whole dataset, because oversampling the entire dataset could lead to data leakage when performing cross- validation or evaluating the model on a separate test set. Data leakage occurs when information from the test set leaks into the training process, leading to overly optimistic performance estimates. By oversampling only the training set, it has been ensured that the test set remains untouched and representative of real- world data. The data split was done in a ratio of 70-30, i.e. 70% of the data was kept for training and the remaining 30% for testing. Therefore, amongst the 3914 posts, 2739 were initially kept for training and the remaining 1175\u2013were kept for testing. Then, 20% of the training data was kept for validation purposes during the training process."}, {"title": "3.3. Text Pre-processing", "content": "Text preprocessing is an essential step in natural language processing (NLP) that involves cleaning and transforming raw text into a format that is suitable for analysis and modelling. Proper preprocessing can significantly improve the performance of NLP models by reducing noise and ensuring consistency in the data. In our dataset, there are some noises that should be pre-processed before being passed to the model for analysis. For an optimal result, the following preprocessing steps have been conducted:\n1. Removing Emojis: Emojis can introduce noise into the dataset. Removing emojis ensures that the text is standardised and focuses on the language content, making it easier to process and analyse.\n2. Removing English Words: Our primary goal is to analyse Bangla text; English words might not be relevant and can distort the analysis. Removing them helps in focusing the analysis strictly on Bangla language patterns.\n3. Removing Punctuations: Punctuation marks can sometimes interfere with the text processing algorithms, especially those which are not punctuation-aware. Removing them can simplify the text, making it easier for the models to learn patterns in the data.\n4. Removing Extra White Spaces: Extra white spaces can create inconsistencies in the dataset. They can affect tokenisation and other preprocessing steps, leading to irregularities in text representation. Removing unnecessary white spaces ensures that the text data is clean and compact, making it more efficient for storage and faster for processing.\nFor instance, the sentence \u2018\u09b6\u09c1\u09ad \u09b8\u0995\u09be\u09b2 \u09aa\u09ac\u09bf\u09a4\u09cd\u09b0 \u099c\u09c1\u09ae\u09cd\u09ae\u09be\u09b0 \u09a6\u09bf\u09a8 \u099c\u09c1\u09ae\u09cd\u09ae\u09be \u09ae\u09cb\u09ac\u09be\u09b0\u0995! \u09b8\u09c1\u09a8\u09cd\u09a6\u09b0 \u09b9\u09cb\u0995 \u09b8\u09ac\u09be\u09b0 \u099c\u09c0\u09ac\u09a8 #Jumma' will be transformed to \u2018\u09b6\u09c1\u09ad \u09b8\u0995\u09be\u09b2 \u09aa\u09ac\u09bf\u09a4\u09cd\u09b0 \u099c\u09c1\u09ae\u09cd\u09ae\u09be\u09b0 \u09a6\u09bf\u09a8 \u099c\u09c1\u09ae\u09cd\u09ae\u09be \u09ae\u09cb\u09ac\u09be\u09b0\u0995 \u09b8\u09c1\u09a8\u09cd\u09a6\u09b0 \u09b9\u09cb\u0995 \u09b8\u09ac\u09be\u09b0 \u099c\u09c0\u09ac\u09a8' after the preprocessing steps."}, {"title": "3.4. Numerical Representation of Text", "content": "Numerical representation of text refers to the process of transforming text data into numerical vectors or matrices that can be processed by machine learning algorithms or other mathematical models. This conversion is necessary because most machine learning algorithms require numerical input data to perform computations and effectively learn patterns. In our work, different representations of textual data, including TF-IDF, BERT embedding and FastText embedding, have been employed."}, {"title": "3.4.1. TF-IDF", "content": "Term Frequency-Inverse Document Frequency (TF-IDF) is a statistical measure used in information retrieval and text mining to evaluate the importance of a word in a document relative to a collection of documents (corpus). It is a numerical statistic intended to reflect how important a word is to a document in a corpus. The importance proportionally increases to the number of times a word appears in the document but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. TF measures how frequently a term appears in a document, while IDF measures how important a term is in the entire corpus. The formulas for finding TF and IDF are as follows:\n$TF (t,d) = \\frac{Number \\space of \\space times \\space term \\space t \\space appears \\space in \\space document \\space d}{Total \\space number \\space of \\space terms \\space in \\space document \\space d}$\n$IDF (t) = log(\\frac{Total \\space number \\space of \\space documents}{Number \\space of \\space documents \\space containing \\space term \\space t})$\n$TF \u2013 IDF (t, d) = TF(t,d) \u00d7 IDF(t)$"}, {"title": "3.4.2. Word Embedding", "content": "Word embeddings are a type of word representation that allows words to be mapped to the vectors of real numbers. Each word is represented as a dense vector with fixed dimensions. These vectors capture the semantic meanings of the words in a continuous vector space, where words with similar meanings are located close to each other. BERT and FastText are both powerful word-embedding models used in natural language processing tasks. While BERT provides deep contextualised embeddings that capture bidirectional context, FastText leverages Sub-word information to effectively handle morphological variations.\n\u2022 BERT Embedding: BERT is a state-of-the-art pre-trained language representation model developed by Google [7]. It uses a transformer-based architecture, which enables it to capture bidirectional contexts in the texts. BERT is pre-trained on large text corpora using masked language modelling (MLM) and next-sentence prediction (NSP) tasks. BERT generates embeddings by tokenising input text, mapping tokens to initial word embeddings and passing them through multiple transformer encoder layers to capture contextualised representations. These embeddings provide a deep contextual understanding of words within sentences, allowing for accurate processing of natural language data across various tasks.\n\u2022 FastText Embedding: FastText is a word embedding model developed by Facebook's AI Research (FAIR) lab [8]. FastText embedding works by leveraging sub-word information, breaking down"}, {"title": "3.5. Developing Deep Learning-based (CNN-BiLSTM) Model", "content": "The CNN-BiLSTM model is a sophisticated neural network architecture that combines Convolutional Neural Networks (CNNs) [13] and Bidirectional Long Short-Term Memory (BiLSTM) [14] networks which can efficiently predict binary classes in sequential data. The model begins with a Conv1D layer that applies 100 filters of size 3 to the input data, effectively capturing local patterns through the use of the ReLU [15] activation function, which introduces non-linearity. This is followed by a BatchNormalisation [16] layer that stabilises the training process by maintaining the mean and variance of the activations close to 0 and 1, respectively. A MaxPooling1D [17] layer then reduces the dimensionality of the feature maps, retaining the most significant features while decreasing computational load.\nThe core of the model's ability to handle sequential data lies in the Bidirectional LSTM layer, which processes the input sequence in both forward and backward directions. This bidirectional approach captures dependencies from both the past and the future states, providing a comprehensive understanding of the context. Another BatchNormalisation layer follows, further estabilising and normalising the outputs from the BiLSTM layer.\nThe subsequent flatten layer converts the 2D matrix output into a 1D vector, preparing it for the fully connected dense layers. The first dense layer, with 256 neurons, employs the ReLU activation function and 12 regularisation to learn complex representations of the input features, while preventing overfitting. A Dropout layer [18] with a 30% dropout rate adds further regularisation by randomly dropping neurons during training. This process is repeated with a second dense layer containing 128 neurons and a higher 12 regularisation value, followed by another Dropout layer.\nThe final output layer consists of two neurons and uses the softmax activation function to output probabilities for each class, facilitating binary classification. The use of softmax allows the model to clearly distinguish between the two classes based on the computed probabilities. The architecture of our proposed CNN-BiLSTM model is shown in Figure 6. The total number of parameters required for the CNN-BiLSTM architecture is 10,034,594, with 10,033,882 trainable parameters\nOverall, the combination of CNN for feature extraction, BiLSTM for contextual understanding and various normalisation and regularisation techniques ensures that the model effectively trains, performs effectively on new, unobserved data and efficiently predicts binary classes in sequential datasets."}, {"title": "3.5.1. Model Compilation", "content": "The model was compiled using the Adam optimiser [19], which has a learning rate of 0.001 (the default) and categorical_crossentropy as its loss function. Adam optimiser has been chosen because of its adaptive"}, {"title": "3.5.2. Model Training", "content": "The model was trained using 60 epochs and a batch size of 16. For validation, the training set was split into an 80-20 ratio, with 20% of the data used for validation during the training process. To prevent overfitting, early stopping and an adjustable learning rate were incorporated as callbacks. Early stopping monitored the validation loss with a patience level set to 10. The ReduceLROnPlateau callback continuously monitored the validation loss and reduced the learning rate by a factor of 0.2 if the validation loss did not improve for 2 epochs. This reduction helped fine-tune the model by allowing it to take smaller steps in the parameter space, particularly when the model was close to converging but needed finer adjustments for optimal performance. The entire experiment was conducted on a Kaggle notebook using a CPU accelerator."}, {"title": "4. Experimental Results and Performance Analysis", "content": "As part of our research, several experiments were conducted to evaluate the performance of our proposed CNN-BiLSTM model using different textual representation techniques. Our objective was to determine and compare the model's performance using these techniques.\nIn Table 3, the performance of three different word representation approaches across four evaluation metrics is presented. The BERT representation achieved the highest F1 score of 84%. In comparison, TF-IDF and FastText embeddings achieved slightly lower scores of 82% and 83%, respectively. Moving on to accuracy, TF-IDF, BERT and FastText achieved scores of 81%, 83%, and 82%, respectively. In terms of precision and recall, similar patterns emerged. While the precision scores are 82%, 85% and 84%, respectively, the recall scores are similar to the accuracy scores. These results indicate that the BERT approach achieves the highest F1 score, while FastText and TF-IDF displayed comparable performance in accuracy, precision and recall. Despite TF-IDF achieving the highest AUC (84%), its F1 score (82%), accuracy (81%), and precision (82%) are slightly lower compared to BERT and FastText. This discrepancy implies that while TF-IDF is overall good at distinguishing between classes, it might not be as effective as BERT in handling the balance between precision and recall.\nBERT embeddings emerge as the most effective technique for detecting depressive posts, as evidenced by the highest F1 score. This makes it a preferable choice when the goal is to achieve a balanced model with both high precision and recall. While TF-IDF provides a strong AUC score, suggesting good overall discrimination ability, its lower F1 score highlights potential issues in balancing false positives and negatives. In fact, FastText offers a middle ground with consistent performance, making it a reliable alternative."}, {"title": "4.1. Practical Consequences of Findings", "content": "The practical consequences of our findings are significant for several stakeholders, including mental health professionals, researchers and technology developers working with Bangla textual data. Firstly, the ability of our model to accurately identify depressive posts with a balanced approach ensures that the mental health professionals can rely on automated systems to flag potentially at-risk individuals. This early detection is crucial for timely intervention and support, including saving lives. Secondly, the use of BERT embeddings in our CNN-BiLSTM architecture demonstrates the effectiveness of advanced natural language processing techniques in capturing the nuanced semantics of Bangla text. This encourages further research into applying these techniques to other low-resource languages, globally broadening the scope of mental health monitoring tools. Moreover, our comprehensive evaluation metrics, including precision, recall and F1 scores, provide a more transparent and robust assessment of model performance, moving beyond mere accuracy. This transparency is critical for building trust amongst the users and the other stakeholders, ensuring that the model's predictions are reliable and actionable. Lastly, by addressing class imbalance in our dataset, precedent is set for future studies to consider data representativeness, thereby improving the generalisability and fairness of AI systems in mental health applications. Overall, our findings highlight the potential of combining advanced NLP techniques with thoughtful data handling practices to create effective and trustworthy tools for mental health monitoring in Bangla and other underrepresented languages."}, {"title": "5. Limitations and Future Works", "content": "This section demonstrates the limitations faced throughout this work, along with our future research directions."}, {"title": "5.1. Limitations", "content": "\u2022 Small Dataset Size: One key limitation is the small size of our dataset, which restricts the model's ability to generalise to a broader range of data. This constraint limits the scope and applicability of our findings.\n\u2022 Test Set Size: The test set size was not sufficient to conclusively determine the model's performance across a diverse range of inputs. A larger test set would have provided a more robust evaluation of the model's capabilities.\n\u2022 Lack of External Validation: We were unable to test the robustness of our model with other datasets, as they were not publicly available. This limits our ability to confirm the model's performance on different data sources."}, {"title": "5.2. Future Works", "content": "In the future, we aim to address the above-mentioned limitations by developing a larger public dataset to enhance the robustness and generalisability of our model. Expanding the dataset will allow for more comprehensive testing and validation, ultimately leading to a more reliable tool for detecting depressive posts in Bangla. This larger dataset will facilitate improved model training, ensuring better performance across diverse data samples. Additionally, our plan also includes exploring more advanced techniques and architectures to further improve the detection accuracy and efficiency of our model."}, {"title": "6. Conclusion", "content": "Our work aimed to contribute to the identification of individuals experiencing ongoing depression, as reflected in their social media posts. Given the scarcity of research in the Bangla language, it is essential to develop a reliable tool for detecting depressive posts written in Bangla. To this end, our research utilised a dataset containing both depressive and non-depressive posts that were carefully annotated by domain experts. To address the issue of class imbalance, random oversampling was employed for the minority class, ensuring that our model could effectively learn from a more balanced dataset. This step was crucial in preventing the model from being biased towards the majority class and enhancing its ability to accurately detect depressive posts. Several experiments were conducted with various numerical representation techniques to convert textual data into meaningful vectors for machine learning. These techniques included Term Frequency-Inverse Document Frequency (TF-IDF), BERT embeddings and FastText embeddings. These representations were then applied to a deep learning-based CNN-BiLSTM model, which combined the strengths of Convolutional Neural Networks (CNN) for feature extraction and Bidirectional Long Short- Term Memory (BiLSTM) networks for understanding contextual information from both directions in a sequence. Our results indicated that the TF-IDF representation achieved the best performance in classifying the classes, with an AUC of 84%, which is better than both BERT and FastText embeddings. This finding highlights the effectiveness of TF-IDF in capturing the essential features of Bangla text in our dataset. Additionally, while BERT did not outperform TF-IDF in terms of AUC, it achieved a good F1 score, demonstrating its capability to effectively balance precision and recall. When comparing our work with other published research, we found that our results were superior in terms of various evaluation metrics, including accuracy, precision, recall and F1-score. Additionally, the reliability of our dataset annotations, performed by domain experts, contributed to the acceptance of our model."}]}