{"title": "From Loops to Oops: Fallback Behaviors of Language Models Under Uncertainty", "authors": ["Maor Ivgi", "Ori Yoran", "Jonathan Berant", "Mor Geva"], "abstract": "Large language models (LLMs) often exhibit undesirable behaviors, such as hallucinations and sequence repetitions. We propose to view these behaviors as fallbacks that models exhibit under uncertainty, and investigate the connection between them. We categorize fallback behaviors sequence repetitions, degenerate text, and hallucinations and extensively analyze them in models from the same family that differ by the amount of pretraining tokens, parameter count, or the inclusion of instruction-following training. Our experiments reveal a clear and consistent ordering of fallback behaviors, across all these axes: the more advanced an LLM is (i.e., trained on more tokens, has more parameters, or instruction-tuned), its fallback behavior shifts from sequence repetitions, to degenerate text, and then to hallucinations. Moreover, the same ordering is observed throughout a single generation, even for the best-performing models; as uncertainty increases, models shift from generating hallucinations to producing degenerate text and then sequence repetitions. Lastly, we demonstrate that while common decoding techniques, such as random sampling, might alleviate some unwanted behaviors like sequence repetitions, they increase harder-to-detect hallucinations.", "sections": [{"title": "1 Introduction", "content": "While large language models (LLMs) have been known to generate human-like language remarkably well (Radford et al., 2019; Brown et al., 2020; Touvron et al., 2023; OpenAI et al., 2024), there are growing concerns about their propensity for undesirable behaviors, such as degenerate\u00b9 or repetitive text (Holtzman et al., 2020), hallucinations (Ji et al., 2022; Zhang et al., 2023), and verbatim recollection of training samples when processing out-of-distribution inputs (Nasr et al., 2023a). Previous work (Kim et al., 2024; Snyder et al., 2023)\n\u00b9Degenerate text includes repetitive textual patterns and/or rephrasing of previously generated text.\nhas studied these phenomena, and suggested solutions, but has done so for each in isolation, without considering the interactions between them.\nIn this work, we propose that the behaviors illustrated in Figure 1 can be viewed collectively as fallback behaviors, which emerge when the model faces uncertainty. We aim to categorize and analyze the relationship between these behaviors across a range of LLMs. To this end, we create controlled yet natural settings to manage the level of uncertainty, force the generation of factual information, and evaluate the correctness of each prediction. We then test three model families-Pythia (Biderman et al., 2023), Llama 2 (Touvron et al., 2023), Llama 3 (AI, 2024), and OLMO (Groeneveld et al., 2024)-considering various factors that could influence the emergence of different fallbacks: (a) number of parameters, (b) number of pretraining tokens, (c) instruction-following training, and (d) decoding algorithms. We observe a clear ordering in the appearances of different fallbacks, as demonstrated in Figure 2, which persists across all the"}, {"title": "2 Fallback Behaviors", "content": "Since their emergence as powerful generative tools (Radford et al., 2019; Brown et al., 2020), there have been numerous reports of unwanted behaviors of LLMs. Holtzman et al. (2020) showed that greedy decoding methods can cause degenerate generations, such as incoherent text or repetitions. They propose the use of nucleus sampling to mitigate this problem. In another study, Nasr et al. (2023b) demonstrate that when given out-of-distribution inputs, models might output their training data verbatim. More recently, growing evidence (Zhu et al., 2024; Zhang et al., 2023; Aichberger et al., 2024, inter alia) suggests that even the best models often generate convincing yet factually incorrect text, typically referred to as \u201challucinations\" or \"confabulations\" (Ji et al., 2022; Bang et al., 2023). Xiao and Wang (2021) investigate model uncertainty, defining it as the probabilistic uncertainty in predicting the next token, which is influenced by the model's training and knowledge, and identify it as a cause of hallucinations.\u00b2\n\u00b2While the model may experience internal uncertainty when predicting the next token, this might not manifest in the final logits. Once a fallback behavior is selected, it may promote the token, resulting in very high logit value.\nIn this paper, we propose to view these seemingly independent phenomena as fallback behaviors that models adopt when uncertain, and we investigate the relationships between them. Furthermore, we hypothesize that the strength of an LLM influences its fallback behavior, and aim to understand what factors determine how a model would behave in cases of uncertainty.\nSpecifically, we focus on the following phenomena: 1. Sequence repetition When models face an inability to produce plausible continuations, they tend to repeat previously generated sequences, which are known to be plausible within the current context. 2. Degenerate text As shown by Holtz-"}, {"title": "3 Experimental Setting", "content": "To directly investigate the relation between fallback behaviors and their contributing factors, our setup exposes models to naturally-occurring controlled cases of uncertainty while recalling facts.\nWe use the following datasets:\n1. TRIVIAFACTS (TRIV): We manually curate 95 high-quality domain-diverse questions with a list of answers (list questions) that (a) contains multiple elements, but no more than 15, (b) requires only knowledge that appears very frequently in the web, and (c) includes short elements without multiple correct phrasings. Due to its high-quality and highly controllable setup, we base the bulk of our experiments on this dataset, with minor modifications described in-line as relevant to each experiment.\n2. BIOGENERATION (BIO): To investigate fallback behaviors in unstructured natural text, we follow Min et al. (2023a) and prompt the model to create biographies of entities from five popularity levels, analyzing the resulting facts. We sample 25 entities from each popularity level.\n3. QAMPARI (QAMP): We sample 100 questions from the dataset introduced by Amouyal et al. (2023), which contains naturally occurring list questions with answers from Wikipedia.\u00b3\n\u00b3While similar to our TRIVIAFACTS dataset, this dataset is known to have non-exhaustive answer sets and different possible phrasings for the same answers, which can result in correct answers being flagged as hallucinations.\n4. FAKEQAMPARI (FQAMP): We replace the subjects from QAMPARI with made-up names, verifying they do not exist, forcing uncertainty.\nTable 1 provides example questions. For additional details on the datasets, see Appendix A.\nGenerating predictions To remove behaviors caused by different decoding schemes, we perform our analyses with greedy decoding unless stated otherwise (Section 5.1 investigates the effects of temperature sampling). When requiring models to provide answers to the TRIV data, we prompt the model to produce a list of up to 25 answers (see Table 1 for example), thus forcefully pushing the models to recall facts, even when there are none. We ablate this behavior by removing the pre-defined length of the answer list and including specific demonstrations to complete the list while abstaining instead of generating incorrect facts (Section 5.2).\nGiven a generated output, we first parse it into a set of facts, and evaluate each one independently as correct, repeated or hallucinated. For the list questions datasets (TRIV, QAMP and FQAMP), this parsing is mostly straightforward as the generations are structured as lists and the ground truth is given as a set of answers. For open-ended generation, we use FactScore (Min et al., 2023a) to extract atomic facts and verify them against the entities' Wikipedia entries. As models frequently continue generating tokens after the completion of the instruction prompt, we further detect what the model did at that point. Namely, we consider the following options: 1) generating EOS token, 2) changing the topic, for example, by creating a new list/biography which we refer to as topic change or 3) continuing to predict tokens indefinitely (until the token budget is exhausted) within the same sentence/paragraph of the answer which we note by bad format. For additional information on the parsing process and example generations and endings, see Appendix B.\nWe perform our experiments on a variety of model families, sizes, pretraining corpora sizes and finetuning stages. We evaluate both the base and chat-specific checkpoints of Llama 2 which were finetuned on instruction and dialogue data, as well as on the recently introduced Llama 3 models (Touvron et al., 2023; AI, 2024). Secondly, we use OLMO (Groeneveld et al., 2024), which comes in multiple model sizes, includes intermediate check-"}, {"title": "4 Fallback Behaviors of Different LLMs", "content": "Increasing model strength through additional pretraining, more parameters, or instruction-tuning shifts from simple fallback behaviors to more complex ones, i.e., from repetitions to hallucinations.\nScaling up models and training data improves performance and reduces undesired artifacts (Kaplan et al., 2020; Brown et al., 2020). Incorporating instruction-following phases aligns LLMs' outputs with human preferences (Ouyang et al., 2022). In this section, we investigate how these improvements influence model behavior under uncertainty. We first focus on the trade-off between sequence repetitions and hallucinations, measured as discrete shifts in fallback behaviors. Due to the various forms and broad definition of degenerate text, measuring its appearance is not straightforward. We revisit this in depth in Section 6, demonstrating that the fallback shift is a continuous rather than discrete process."}, {"title": "4.1 Model scale dictates the fallback behavior", "content": "To minimize confounding factors and understand the direct effect of model scale on fallback behaviors, we generate predictions with greedy decoding over our TRIVIAFACTS data, and analyze the results by model family and scale. Since Pythia models were all trained on the same data in the"}, {"title": "4.2 Models shift fallbacks during pretraining", "content": "Most LLMs are trained with autoregressive language modeling objective, maximizing the log probability of the next token given some context. Increasing the number of tokens seen during training lowers perplexity and improves language understanding (Kaplan et al., 2020). Using intermediate checkpoints from the OLMo and Pythia model families, we study fallback behavior during pretraining. Figure 4 depicts the fallbacks breakdown of Pythia-6.9B across training, showing that initially, after seeing only 2-4 billion tokens, it mainly repeats facts (blue). As training continues, it produces more correct answers (green) and more incorrect unique facts, i.e., hallucinations (orange), while repeating facts less. Similar trends were observed for Pythia-12B checkpoints, as well as for OLMO models (see Figures 19 and 20)."}, {"title": "4.3 Instruction finetuning shifts behaviors", "content": "Recently, instruction finetuning (Ouyang et al., 2022) has been adopted as a valuable method to improve model performance and align its generation with human preferences. Although one might assume such training reduces hallucinations, OpenAI et al. (2024) suggest it increases model miscalibration, resulting in hallucinations associated with high logit values.\nRepeating the experiments from Section 4.1 and comparing models to their instruction-tuned counterparts, we see a similar shift in fallback behavior; instruction-tuned models generate fewer repeated sequences and more hallucinations on average (Figures 21 and 22). For the OLMo and Llama 2 family which had undergone more finetuning than Dolly checkpoints, the results are much more pronounced, with the hallucinations portion almost doubling be-"}, {"title": "4.4 Similar trends in open-ended generation", "content": "To mimic real-world user requests, we use the BIOGENERATION dataset, sampling completions from a subset of the Pythia model scales with a temperature of \\( \\tau = 0.5 \\). We use FactScore (Min et al., 2023b) to parse each generated biography into atomic facts and let ChatGPT 3.5 (Ouyang et al., 2022) verify them against Wikipedia entries.\u2074\n\u2074While FactScore, which relies on ChatGPT 3.5, is not fully robust and can miss some atomic facts or incorrectly label them, we assume such errors occur at similar rates across generations, resulting in reliable trend analysis.\nWithout the limitation on number of facts to produce, we observe that the larger models also generate more facts. Interestingly, when averaging over the very-rare entities (Figure 5) we see that even in this natural scenario, when models are required to elaborate on topics they know little about, they fall back to the same behaviors, with shifts occurring as predictably as in the controlled settings. Similar trends emerge over the rest of the popularity levels, though the more frequent an entity is, the more likely the models are to be able to recall facts for them and less uncertainty they face, thus making the results less useful for our analysis (Figures 27 to 31). This aligns only partially with Min et al. (2023b), who found stronger models generate more atomic facts and struggle more with rare entities."}, {"title": "5 Factors Influencing the Fallback Behaviors of an LLM", "content": "While LLMs have some internal capability to avoid hallucinations, this fallback behavior is inherent to their generation scheme and is likely unavoidable with current decoding methods. Mitigating degenerate text through random sampling can come at the cost of more hallucinations.\nIn this section, we shift our focus from comparing different models to understanding the factors influencing the fallback behaviors of a frozen pretrained model used for inference."}, {"title": "5.1 Effect of sampling methods", "content": "So far in our experiments, we mostly used greedy decoding to obtain responses from the models. However, in real-world applications, decoding often includes sampling from the model's output distribution and including repetition/frequency penalties (e.g., (Holtzman et al., 2020; Keskar et al., 2019; Kumar et al., 2022)). We analyze the effect of temperature sampling on fallback behaviors by repeating our experiment on TRIVIAFACTS, while generating five different sequences per model for each input, with increasing levels of randomness (using larger values of temperature). Figure 6 shows the results for Pythia-12B, revealing a surprising finding \u2013 while higher levels of randomness mitigate some repetitiveness and help models to break out of endless loops in the generation, it\n\u2075This is expected as even when copying previous tokens with high probability such as 0.99, continuing the generation long enough will inevitably lead to generating a token that is not part of the loop, thus steering the text to a new direction.\nalso causes a shift towards hallucinations. Moreover, we observe that introducing randomness reduces the number of correct facts, which could be attributed to the random skipping of correct facts associated with low confidence in the model.\nTo assess whether our findings in Section 4.1 hold in realistic setups that involve random sampling, we repeat them with \\( \\tau = 0.5 \\). This choice of temperature sets a good balance between the model's performance on the task and the amount of degenerate text it outputs (Figure 6). Figure 23 confirms that even with additional randomness, all fallback behaviors emerge, with shifts occurring as predictably as in the greedy-decoding case."}, {"title": "5.2 Models \"volunteer\" hallucinations", "content": "In many previous experiments, we deliberately placed the model in high-uncertainty scenarios, prompting it to produce additional facts when none existed. To lift this restriction, we modify the base prompt in the TRIVIAFACTS data by removing the requirement for a specific number of answers (25). For example, the modification to the example in the first row of Table 1 would result in the prompt \"The following colors are in the Olympic rings:\", allowing the model to stop generating additional answers once it exhausted its knowledge. However, the same trends continue to emerge (Figure 24), showcasing that even in natural situations without synthetically induced uncertainty, LLMs still fall back on the same behaviors and fail to use more appropriate \u201cexit strategies\", such as topic changes or explicit abstaining."}, {"title": "5.3 Can fallbacks be prevented by prompting?", "content": "Kadavath et al. (2022) find encouraging evidence that LLMs may be calibrated (i.e. their confidence approximates the true probability of the output) and able to assess what they don't know, especially when they are larger. However, Kapoor et al. (2024a) recently showed that this observation does not hold for some popular open-source models, and Yona et al. (2024) showed that LLMs struggle to express their uncertainty in words. We aim to understand whether LLMs are internally aware of their uncertainty, and if this awareness can reduce unwanted behaviors.\nWe modify the prompts as follows: For instruction-tuned models, we add the following prefix to each prompt \u201cComplete the following list with facts you are sure of, and stop when you cannot recall additional facts\"."}, {"title": "6 Fallback Behaviors in One Generation", "content": "As models generate longer texts, they shift in their fallback behavior, first generating hallucinations and eventually producing degenerate text.\nWhile we established that both model strength and the decoding method impact the fallback behaviors of a model, these parameters are decided ahead of time. In this section, we focus on a single model at a time and investigate the effect of generation length on emergence of fallback behaviors."}, {"title": "6.1 Emergence of fallbacks during generation", "content": "We look at the facts generated by the model in response to a given query as an ordered list of labels (i.e. correct, hallucination, repetition). For example, Figure 7 depicts in each row the 25 labels of the facts produced by Pythia-12B for each of the 95 samples in TRIVIAFACTS. Surprisingly, we observe that almost without exception, the model first generates all the correct facts it can (green),"}, {"title": "6.2 Fallback shifts is a continuous process", "content": "When analyzing generations as a discrete list of facts, we also find discrete shifts in fallback behaviors as the model's uncertainty grows. While examining strict repetition of facts is of particular interest, one may also consider a softer measure of degenerate text generation. For example, producing a list of URLs such as https://page.domain/xyz, https://page.domain/xyq, https://page.domain/wyz is not strictly a repetition nor necessarily a hallucination, but it is clear that some form of conditional"}, {"title": "7 Related Work", "content": "Repetitions and degenerate text Holtzman et al. (2020) first attributed the tendency of LMs to generate highly repetitive and degenerate text to greedy sampling and suggested nucleus sampling as a possible mitigation. Follow-up work demonstrated the role of uncertainty in generating degenerate text and proposes various solutions using random, controlled, or constrained generation techniques Keskar et al. (2019); Kumar et al. (2022); Zhang et al. (2022); Finlayson et al. (2023); Su et al. (2022). In a parallel approach, Olsson et al. (2022) focused on understanding the intrinsic mechanisms that cause models to copy previous inputs.\nAs random sampling techniques became ubiquitous and LLMs grew in size and capability, the main focus shifted toward understanding the causes and proposing solutions to the generation of non-factual text, commonly referred to as hallucinations. Recent work has focused on understanding how and why hallucinations emerge during generation (Rashkin et al., 2023; Zhang et al., 2023; Adlakha et al., 2023; Kim et al., 2024), reducing hallucinations (e.g., by retrieval-augmentation or prompting) (Roller et al., 2021; Shuster et al., 2021; Dhuliawala et al., 2023), and detecting hallucinations (Zhou et al., 2021; Liu et al., 2022; Honovich et al., 2022; Min et al., 2023b; Mishra et al., 2024; Gottesman and Geva, 2024). For a survey on hallucinations in natural language generation, see Ji et al. (2022). In concurrent work, Denison et al. (2024, inter alia) demonstrated how models generalize during training and learn to shift from simple dishonest strategies such as sycophancy to generating false facts with premeditation. Similarly, Band et al. (2024) suggest that the problem of hallucinating rather than abstaining is an issue with the calibration of the model, as was also hypothesized by OpenAI et al. (2024)."}, {"title": "8 Conclusion", "content": "This work links the notorious unwanted behaviors of LLMs, such as degenerate and repetitive text and hallucinations, showing that they are all fallback behaviors for models when faced with uncertainty. We provide abundant evidence that these behaviors emerge with a clear ordering between their appearances, either when comparing similar models of different strength, different decoding strategies and even in a single generation. Our work suggests that these fallback behaviors are inherent to current LLMs and that the ubiquitous methods to alleviate such issues may simply replace one fallback by another. Moreover, increasing model strength by training longer or adding more parameters enhances overall performance and shifts its fallbacks towards more complex range. However, as generation length grows, even the strongest models will resort to hallucinations and may eventually produce degenerate text."}, {"title": "Limitations", "content": "In this work we study different fallback behaviors of language models when faced with uncertainty. While we conduct multiple experiments trying to mimic real-world usage of such models as much as possible, there are several confounders that may still differentiate our controlled experiments than behavior in the wild. First, many of the commodity products are wrapped in additional levels of verification layers to reduce the effect of such behaviors, and it is possible that when observed as a whole, the behaviors of these products could differ significantly than their underlying language models.\nSecond, while we study the effect of random sampling, it becomes more common to apply even stricter modifiers to the language model next-word"}, {"title": "A Datasets", "content": "As discussed in Section 3, we make use of multiple datasets for our experiments. In this section, we provide further details on the construction of each of them. We release all the datasets used in our experiments at https://github.com/Mivg/fallbacks."}, {"title": "A.1 TRIVIAFACTS", "content": "When creating this dataset, we set the following desiderata:\n1. Exhaustiveness: In order to label all correct answers as such, we verify that our ground-truth answer set is exhaustive.\n2. Non-ambiguity: To avoid incorrectly labeling a correct answer as a hallucination, we avoid questions where the answers may be phrased in many ways and focus on short answers with a single common way to refer to them.\n3. Easiness: To be sure models are able to recall correct facts, we choose only questions where the answer list contains at least some answers that any graduate student can produce, as a proxy to the knowledge contained in language models that are trained on web data and Wikipedia.\n4. Diversity: To avoid biases in evaluations, we set out to create a diverse set of questions that relates to as many domains as possible, spanning science, sports, culture, politics, geography, and more.\n5. Uncertainty: To ensure questions induce uncertainty, we restrict the size of the ground-truth answer set to 10. As we ask models to produce 25 answers, we thus ensure they will become uncertain even when recalling all the correct answers."}, {"title": "A.2 BIOGENERATION", "content": "Min et al. (2023b) introduce FactScore, which uses an LLM (e.g., ChatGPT) to parse an unstructured passage into atomic facts and verify them independently against a knowledge base. Min et al. (2023b) use topics from Wikipedia and divide them into five popularity levels based on their frequency in the knowledge base, from very rare to very frequent. They then require a model to generate an open-ended biography for each topic (entity) and use the respective Wikipedia page as the ground-truth knowledge source to verify facts. We randomly sample 25 topics from each of the five popularity levels and use them throughout our experiments."}, {"title": "A.3 QAMPARI", "content": "We use the dataset as introduced by Amouyal et al. (2023). We filter the questions to those where the answer is a set of size \u2265 3 and sample 100 random questions. Then, we rephrase each question to be in the format depicted in Table 1. Where appropriate, some of the questions follow a slightly different template. For example, one of the questions is \u201cLarry Cohen wrote and directed the following 25 works of art:\u201d."}, {"title": "A.4 FAKEQAMPARI", "content": "We take the questions from QAMPARI (see Appendix A.3) and employ ChatGPT to replace the subject entity in each question with an alternative entity name which sounds plausible but does not exist (for example, choosing a Japanese name for an anime creator). We manually verified the generated questions, while checking that all new subjects feel plausible in the context and that they refer to entities that do not exist."}, {"title": "B Parsing Answers", "content": "In this section, we provide further implementation details on the process used to parse the answers from each generation. We make our code and the model generations available in https://github.com/Mivg/fallbacks."}, {"title": "B.1 List Questions", "content": "In Table 1, we show examples of inputs given to the model for completion. To steer models to complete the answers in a specified format, we append to each question the suffix \"\\n 1.\". In almost all cases, the models indeed generate completions in the format of \u201c\\n1. <answer 1>\\n 2. <answer 2> . \", which allows us to easily extract the answers (Figure 10). In some cases, the generations include no \\n or extra newline characters, but those cases are easily dealt with by a simple regex. The next most common case is a given answer as a comma-separated list. We identify those cases when none of the above options were detected and at least five commas appeared in the first sentence of the generation (no newlines or periods). Finally, if none of the above was detected but newline characters are identified (one or more) between short lines, we treat each new non-empty line as an answer. If none of the cases were identified, or a detected structured format is violated without new line characters, we collect the answers until that point and mark the difference from the expected 25 answers as missing due to Bad Format. All answers are normalized by removing extra white spaces to evaluate for repetitions.\nSince pretrained language models are often trained with sequences of a predetermined length, they do not produce end-of-sequence (EOS) tokens and continue to generate tokens until their budget is exhausted. As such, to avoid false-positive hallucination detection, we have to be able to identify when the model stopped"}, {"title": "B.2 Biographies Generations", "content": "As we rely on FactScore (Min et al., 2023b) to extract atomic facts and evaluate them, our task here is mainly identifying topic changes to avoid false detection of hallucinations. To do so, we look for a prefix indicating a new biography is being generated, for special keywords such as References, Discography, etc., if multiple newline characters appear, or if a title-like line is encountered. We also consider any new line that starts with the subject name or a pronoun as a continuation of the biography.\nAfter extracting only the part of the generation we consider as the biography and indicating the strategy to change the topic, we split the biography"}, {"title": "C How Models Change Topics", "content": "Throughout our experiments, we analyze the classification of atomic facts in models' generations, up until a point where they change the topic. Nasr et al. (2023a); Geiping et al. (2024) demonstrate how extremely out-of-distribution inputs can cause models to fall back into recalling training samples verbatim, and Haviv et al. (2023); Stoehr et al. (2024) showed that generation of memorized sequences has a distinct internal \u201cprofile\u201d from generation of non-memorized sequences. In contrast, Prashanth et al. (2024, inter alia) investigate memorization and find that it can be caused by different factors such as duplicated training data, repeating common and predictable patterns, and the recollection of sequences that are neither."}, {"title": "D.1 Exclusion of Llama 2 13b Checkpoint", "content": "We find that Llama 2 13b as released in meta-llama/Llama-2-13b-hf almost always produces extremely poor results. Upon manual inspection of its outputs, we conclude that there is likely a bug in the uploaded weights of this specific model, and thus we exclude it. We provide a few examples in Figure 15. We note that for the other checkpoints, as well as the chat variants of all three sizes, the generation are considerably of higher quality, further supporting this decision."}, {"title": "D.2 Order of Facts in a Single Generation", "content": "Section 6.1 introduces the ShiftScore, which measures how predictable the order of facts is with respect to the hierarchy between fallback behaviors as introduced in this work. To perform the Mann-Whitney U-test, we consider only answer sets with"}]}