{"title": "SynCo: Synthetic Hard Negatives in Contrastive Learning for Better Unsupervised Visual Representations", "authors": ["Nikolaos Giakoumoglou", "Tania Stathaki"], "abstract": "Contrastive learning has become a dominant approach in self-supervised visual representation learning, with hard negatives-samples that closely resemble the anchor-being key to enhancing the discriminative power of learned representations. However, efficiently leveraging hard negatives remains a challenge due to the difficulty in identifying and incorporating them without significantly increasing computational costs. To address this, we introduce SynCo (Synthetic negatives in Contrastive learning), a novel contrastive learning approach that improves model performance by generating synthetic hard negatives. Built on the MoCo framework, SynCo introduces six novel strategies for creating diverse synthetic hard negatives that can be generated on-the-fly with minimal computational overhead. SynCo achieves faster training and better representation learning, achieving a top-1 accuracy of 68.1% in ImageNet linear evaluation after only 200 epochs on pretraining, surpassing MoCo's 67.5% with the same ResNet-50 encoder. Additionally, it transfers more effectively to detection tasks: on the PASCAL VOC, it outperforms both the supervised baseline and MoCo, achieving an AP of 82.5%; on the COCO dataset, it sets a new benchmark with 40.4% AP for bounding box detection and 35.4% AP for instance segmentation. Our synthetic hard negative generation procedure significantly enhances the quality of visual representations learned through self-supervised contrastive learning.", "sections": [{"title": "1. Introduction", "content": "Contrastive learning has emerged as a prominent approach in self-supervised learning, significantly advancing representation learning from unlabeled data. This technique, which discriminates between similar and dissimilar data pairs, has shown premise in visual representation tasks. Seminal works such as SimCLR [6] and MoCo [17] established instance discrimination as a pretext task. These methods generate multiple views of the same data point through augmentation, training the model to minimize the distance between positive pairs (augmented views of the same instance) while maximizing it for negative pairs (views of different instances).\nDespite its efficacy, instance discrimination faces significant challenges. A key limitation is its dependence on numerous negative samples for optimal performance, often leading to increased computational costs. For example, SimCLR requires large batch sizes to provide sufficient negatives [6]. While approaches like MoCo mitigate some issues through a dynamic queue and momentum encoder [8, 17], they still confront the challenge of selecting and maintaining high-quality hard negatives-samples particularly similar to positives and thus more difficult to distinguish.\nRecent studies [3, 6, 8, 10, 20, 29, 33, 39] highlight the importance of carefully crafted data augmentations in learning robust representations. These transformations likely provide more diverse, challenging copies of images, increasing the difficulty of the self-supervised task. This self-supervised task is a pretext problem (e.g., predicting image rotations [12] or solving jigsaw puzzles [27]) designed to induce learning of generalizable features without explicit labels. At the same time, data mixing techniques operating at either the pixel [43, 46] or the feature level [37] help models learn more robust features that improve both supervised and semi-supervised learning on subsequent (target) tasks.\nThe concept of hard negatives has been explored extensively as a means to enhance the robustness and generalization of contrastive learning models. Hard negatives are those that lie close to the decision boundary, making them crucial for fine-tuning the model's discriminative capabilities. [20] introduced MOCHI, which integrates hard negative mixing into the MoCo framework, demonstrating significant improvements in performance. The task of efficiently creating and choosing hard negatives is important in contrastive self-supervised learning.\nIn this paper, we present SynCo (Synthetic negatives in Contrastive learning), a novel approach to contrastive learning that leverages synthetic hard negatives to enhance the learning process. Building on the foundations of MoCo, SynCo introduces six distinct strategies for generating synthetic hard negatives, each designed to provide diverse and challenging contrasts to the model. These strategies include: (1) Interpolated negatives; (2) Extrapolated negatives; (3) Mixup negatives; (4) Noise-injected negatives; (5) Perturbed negatives; and (6) Adversarial negatives. By incorporating these synthetic samples, SynCo aims to push the boundaries of contrastive learning, improving both the efficiency and effectiveness of the training process.\nThe main contributions of our work are as follows:\n1.  We introduce SynCo, a novel contrastive learning approach that improves representation learning by leveraging synthetic hard negatives. SynCo enhances the discriminative capabilities of models by generating challenging negatives on-the-fly from a memory queue, using six distinct strategies that target different aspects of the feature space to provide a comprehensive approach to hard negative generation. This process improves model performance without significant increases in computational demands, achieving faster training and better representation learning.\n2.  We empirically show improved downstream performance"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Contrastive Learning", "content": "Present contrastive learning methods learn representations by instance discrimination as a pretext task, treating each image as its own class. The core idea is to pull together an anchor and a \"positive\u201d sample in the embedding space while pushing apart the anchor from \"negative\u201d samples [6, 21]. Positive pairs are typically generated by creating multiple views of each data point [33], such as through luminance and chrominance decomposition [32], random augmentation [6, 17], image patches [36], or student-teacher model representations [5, 13, 28]. This approach achieves state-of-the-art performance on downstream tasks. The common training objective, provided by InfoNCE [36] or its variants [6, 10, 35], maximizes mutual information [2, 18], requiring numerous negative pairs. While large batch sizes [6] address this, they're resource-intensive. Alternatively, MoCo [8, 17], PIRL [25], and InstDis [41] use memory structures. Recent advancements explore strategies like invariance regularizers [26], dataset-derived positives [10], and unified contrastive formulas [31]. Some approaches eliminate negative samples via asymmetric Siamese structures or normalization [5, 7, 13], while others prevent model collapse through redundancy reduction [44] or regularization [4]. Despite these innovations, instance discrimination's fine-grained nature can lead to false-negative pairs [47]."}, {"title": "2.2. Hard Negatives", "content": "In contrastive learning, using hard negatives is crucial for improving the learning process and model performance. Hard negatives are challenging yet relevant samples that help in defining the embedding space more effectively, as they allow the model to better distinguish between similar features. The use of hard negatives involves selecting negatives that are similar to positive samples but are different enough to aid the model in learning distinctive features. Dynamically sampling these hard negatives during training prevents the model from easily minimizing the loss, which enhances its learning capabilities [6, 17]. MoCo [17] uses a dynamic queue and a"}, {"title": "3. Hard Negatives in Contrastive Learning", "content": ""}, {"title": "3.1. Contrastive Learning with Memory", "content": "Contrastive learning seeks to differentiate between similar and dissimilar data pairs, often treated as a dictionary lookup where representations are optimized to align positively paired data through contrastive loss in the embedding space [17]. Given an image x, and a distribution of image augmentation T, we create two augmented views of the same image using the transformation $t_q, t_k \\sim T$, i.e., $x_q = t_q(x)$ and $x_k = t_k(x)$. Two encoders, $f_q$ and $f_k$, namely the query and key encoders, generate the vectors $q = f_q(x_q)$ and $k = f_k(x_k)$ respectively. The learning objective minimizes a contrastive loss using the InfoNCE criterion [36]:\n$\\mathcal{L}(q, k, Q) = - \\log \\frac{\\exp(q \\cdot k / \\tau)}{\\exp(q \\cdot k / \\tau) + \\sum_{n \\in Q} \\exp(q \\cdot n / \\tau)}$\nwhere k is $f_k$'s output from the same augmented image as q, and $Q = \\{n_1, n_2, ..., n_k \\}$ includes outputs from different images, representing negative samples of size K. The temperature parameter $\\tau$ adjusts scaling for the $l_2$-normalized vectors q and k. The memory bank Q can be defined as an external memory containing every other image in the dataset [25, 32, 41], a queue of the last batches [17], or every other image in the current minibatch [6].\nThe log-likelihood function of the contrastive loss in Equation (1) is defined over the probability distribution created by applying a softmax function for each input/query q. The gradient of the loss with respect to the query q is given by:"}, {"title": "3.2. Understanding Hard Negatives", "content": "Hard negatives are critical for contrastive learning [1, 14, 19, 20, 24, 41]. Sampling negatives from the same batch leads to a need for larger batches [6] while sampling negatives from a memory bank that contains every other image in the dataset requires the time consuming task of keeping a large memory up-to-date [25, 41]. In the latter case, a trade-off exists between the \"freshness\u201d of the memory bank representations and the computational overhead for re-computing them as the encoder keeps changing. MoCo [17] offers a compromise between the two negative sampling extremes: it keeps a queue of the latest K features from the last batches, encoded with a second key encoder that trails the (main/-query) encoder with a much higher momentum. For MoCo, the key feature k and all features in Q are encoded with the key encoder.\nHardness of negatives. Figure 2 illustrates the evolving difficulty of negatives during MoCo-v2 training on ImageNet-100. We plot the top 1024 matching probabilities $p_z$. Initially, at epoch 0, the logits exhibit a relatively uniform distribution. However, as training progresses, an increasing number of negatives cease to contribute significantly to the loss. This suggests that the majority of memory negatives become less effective for the proxy task over time. Previous works [6, 17] have shown that larger memory or batch sizes are necessary for obtaining more challenging negatives. Our analysis confirms that many memory negatives provide diminishing benefits to the learning process as training advances.\nDifficulty of the proxy task. MoCo and SimCLR learn augmentation-invariant representations. We follow the hypothesis of [20] that proxy task difficulty correlates with transformation set complexity. Figure 3 shows the proxy task performance (percentage of queries ranking the key over all negatives) for MoCo and MoCo-v2 on ImageNet-100. MoCo-v2, despite lower proxy task performance due to harder augmentations, achieves better linear classification"}, {"title": "4. Synthetic Hard Negatives in Contrastive Learning", "content": "Based on these observations, we present an approach for generating synthetic hard negatives in the embedding space using six distinct strategies. We refer to the proposed approach as SynCo (\"Synthetic negatives in Contrastive learning\")."}, {"title": "4.1. Hard Negatives Generation", "content": "Let q be the query image, k its key, and $n \\in Q$ represent the negative features from a memory structure of size K. The loss for the query is calculated using logits $l(z_i) = q \\cdot Z_i/\\tau$ fed into a softmax function. Define $Q = \\{n_1, n_2, ..., n_k\\}$ as the ordered set of all negative features, such that $l(n_i) > l(n_j)$ for all $i < j$, meaning the set of negative features is sorted by decreasing similarity to the query feature. The hardest negatives are identified by truncating the ordered set Q, keeping only the first $N < K$ items, denoted as $Q_N$.\nInterpolated hard negatives (type 1). For each query q, we propose to generate $N_1$ hard negative features by mixing the query q with a randomly chosen feature from the N hardest negatives in $Q_N$. Let $H^1 = \\{h_1^1, h_2^1, ..., h_{N_1}^1 \\}$ be the set of synthetic negatives to be generated. Then a synthetic point $h_i \\in H^1$ would be given by:\n$h_i^1 = \\alpha_k q + (1 - \\alpha_k) \\cdot n_i, \\qquad \\alpha_k \\in (0, \\alpha_{max})$\nwhere $n_i \\in Q_N$ and $\\alpha_k$ is randomly sampled from a uniform distribution in the range $(0, \\alpha_{max})$. The resulting synthetic hard negatives are then normalized and added to the set of negative logits for the query. Interpolation creates a synthetic embedding that lies between the query and the negative in the embedding space. We set $\\alpha_{max} = 0.5$ to ensure that the query's contribution is always smaller than the one of the negative. This is similar to the hardest negatives (type 2) of MOCHI.\nExtrapolated hard negatives (type 2). For each query q, we propose to generate $N_2$ hard negative features by extrapolating beyond the query embedding in the direction of the hardest negative features. Similar to the interpolated method, we use a randomly chosen feature from the N hardest negatives in $Q_N$. Let $H^2 = \\{h_1^2, h_2^2, ..., h_{N_2}^2 \\}$ be the set of synthetic negatives to be generated. Then a synthetic point $h_i \\in H^2$ would be given by:\n$h_i^2 = n_i + \\beta_k (n_i - q), \\qquad \\beta_k \\in (1, \\beta_{max})$\nwhere $n_i \\in Q_N$ and $\\beta_k$ is randomly sampled from a uniform distribution in the range $(1, \\beta_{max})$. These synthetic features are also normalized and used to enhance the negative logits. Extrapolation generates a synthetic embedding that lies beyond the query embedding in the direction of the hardest negative. We choose $\\beta_{max} = 1.5$.\nMixup hard negatives (type 3). For each query q, we propose to generate $N_3$ hard negative features by combining pairs of the N hardest existing negative features in $Q_N$. Let $H^3 = \\{h_1^3, h_2^3, ..., h_{N_3}^3 \\}$ be the set of synthetic negatives to be generated. Then a synthetic point $h_i \\in H^3$ would be given by:\n$h_i^3 = \\gamma_k \\cdot n_i + (1 - \\gamma_k) \\cdot n_j, \\qquad \\gamma_k \\in (0, 1)$\nwhere $n_i, n_j \\in Q_N$ and $\\gamma_k$ is randomly sampled from a uniform distribution in the range (0, 1). The resulting synthetic hard negatives are then normalized and added to the set of negative logits for the query. Mixup combines pairs of the hardest existing negative features to create a synthetic"}, {"title": "4.2. Discussion on Synthetic Hard Negatives", "content": "Intuition of synthetic hard negatives. Each SynCo strategy enhances model generalization through challenging contrasts. Type 1 interpolates between query and hard negatives, increasing sample diversity throughout training. Type 2 extrapolates beyond the query, pushing embedding space boundaries and improving robustness to difficult contrasts. Type 3 combines pairs of hard negatives, encouraging more generalized and robust feature learning. Type 4 injects Gaussian noise, promoting invariance to minor feature fluctuations and enhancing generalization. Type 5 modifies embeddings based on similarity gradients, refining discriminatory power by directing the model towards harder negatives. Type 6 applies adversarial perturbations, creating the most challenging contrasts to distinguish deceptively similar samples.\nA toy example of the proposed synthetic hard negative generation is presented in Figure 4. The latter figure visualizes the different types of synthetic hard negatives generated from a query point and three original negative points in a 2D embedding space. Dotted lines connect the query to extrapolated negatives and between pairs of mixup negatives, illustrating the relationships used to generate these synthetic points.\nIs the proxy task more difficult? Figure 3 illustrates the proxy task performance for various configurations of SynCo. We observe that incorporating synthetic negatives leads to faster learning and improved performance. Each type of synthetic negative accelerates learning compared to the MoCo-v2 baseline, with the full SynCo configuration showing the most significant improvement and the lowest final proxy task performance. This indicates that SynCo presents the most challenging proxy task. The lower performance when including synthetic negatives suggests that $max \\ l(h_i) > max \\ l(n_i)$, $h_i \\in H^i$, $n_i \\in \\mathbb{Q}_N$."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Implementation Details", "content": "We learn representations on the common ImageNet ILSVRC-2012 [9] and its smaller ImageNet-100 subset [32] using a ResNet-50 [15] encoder. All runs of SynCo are based on MoCo-v2 [8]. We developed our approach on top of the official public implementation of MoCo and reproduced it on our setup. We run all experiments on 4 NVIDIA A40 GPUs. For linear classification on ImageNet, we follow the common protocol and report results on the validation set.\nWe report performance after learning linear classifiers for 100 epochs, with an initial learning rate of 30.0 (10.0 for ImageNet-100), a batch size of 256 and a cosine learning rate schedule. For training, unless stated otherwise, we use $K = 65k$ ($K = 16k$ for ImageNet-100). For SynCo, we also have a warm-up of 10 epochs, i.e. for the first epochs we do not synthesize hard negatives. We set SynCo's hyperparameters $\\sigma$, $\\delta$, and $\\eta$ to 0.01. For hard negative generation, we use the top $N = 1024$ hardest negatives, with $N_1 = N_2 = N_3 = 256$ and $N_4 = N_5 = N_6 = 64$. For ImageNet, we report accuracy for a single-crop testing. For object detection on PASCAL VOC [11] we follow [17] and fine-tune a Faster R-CNN [30], R50-C4 on trainval07+12 and test on test2007. For COCO object detection, we follow [17] and fine-tune a Mask R-CNN [16], R50-C4 on train2017 and test on val2017. We use the open-source Detectron2 [40] code and report the common AP, AP50 and AP75 metrics. Similar to [17], we do not perform hyperparameter tuning for the object detection task. Detailed implementation details are provided in the supplementary material."}, {"title": "5.2. Linear Evaluation on ImageNet", "content": "We evaluate the SynCo representation by training a linear classifier on top of the frozen features that were pre-trained on ImageNet, following the procedure described in [6, 13, 22, 23, 36]. Full details of the linear evaluation protocol are described in the supplementary material. With a standard ResNet-50 and for 200 epoch pretraining, SynCo obtains 67.9% $\\pm$0.16% top-1 accuracy and 88.0% $\\pm$ 0.05% top-5 accuracy (Table 1). Specifically, SynCo achieves a +0.4% top-1 accuracy improvement over MoCo-v2 and a +1.0% improvement over MoCHI with 200 epochs of pre-training, leveraging synthetic hard negatives. While MOCHI, which employs hard negatives, achieves lower performance than MoCo-v2, our method, which generates synthetic hard negatives, not only avoids this drop in performance but actually improves it. SynCo also surpasses the state-of-the-art methods SimCLR and SimSiam. When training for 800 epochs, SynCo obtains 70.6% top-1 accuracy (89.8% top-5 accuracy) (Table 2). This represents a +1.9% improvement in top-1 accuracy over MOCHI and a -0.5% decrease compared to MoCo-v2 under the same 800 epochs pretraining settings. These results show that SynCo achieves higher accuracy than its competitors with only a minor computational overhead for generating synthetic hard negatives, while also enabling faster training convergence."}, {"title": "5.3. Semi-supervised Training on ImageNet", "content": "We evaluate the performance obtained when fine-tuning SynCo representation, pretrained for 800 epochs, on a classification task with a small subset of ImageNet's train set, this"}, {"title": "5.4. Transferring to Detection", "content": "We evaluate the SynCo representation, pretrained for 200 epochs, by applying it to the detection task. We follow the protocol of [17], and as used in [20] for a fair comparison. Detailed configurations for object detection experiments are provided in in the supplementary material."}, {"title": "5.5. Ablation Study", "content": "We perform ablations studies on ImageNet-100. The results of our ablations are presented in Figure 6. Our findings consistently demonstrate that various SynCo configurations outperform the MoCo-v2 baseline. Additional ablation studies and analyses are presented in the supplementary material.\nType of hard negative. We evaluate the impact of each synthetic hard negative type on pretraining. For this, we select the top $N = 1024$ hardest negatives and generate $N_i = 256$, $i = 1, 2, ..., 6$ negatives. We train SynCo without hard negatives (equivalent to MoCo-v2) for 100 epochs and measure top-1 and top-5 accuracy. Subsequently, we train SynCo using each type of hard negative individually, and then using all six types in combination. The results of these ablations are presented in Figure 6a. We see that every SynCo configuration outperform the MoCo-v2 baseline.\nParameter sensitivity analysis. We conducted ablations on the parameters $\\sigma$, $\\delta$, and $\\eta$ of SynCo's type 4, type 5, and type 6 negatives, respectively. The results, presented in Figure 6b, show that varying these parameters does not lead to significant differences in performance. This suggests that SynCo is robust across a wide range of values for $\\sigma$, $\\delta$, $\\eta$.\nQueue size. We investigate the effect of queue size Q on performance. We train SynCo and MoCo-v2 with reduced queue sizes. Our results, presented in Figure 6c, reveal that SynCo performs comparably to MoCo-v2 across various"}, {"title": "6. Conclusion", "content": "In this paper, we built upon the observation that harder negatives are needed for more effective self-supervised contrastive learning. Based on that, we presented SynCo, a synthetic hard negative generation approach that improves the quality of representations learned in an unsupervised way, offering better transfer learning performance and utilization of the embedding space. We showed that our method learns generalizable representations, which is important considering the high compute cost of self-supervised learning. We found that multiple SynCo configurations provide considerable gains, and that using synthetic hard negatives consistently improves transfer learning performance.\nWhile our experiments primarily utilized the MoCo framework, the proposed hard negative generation strategies are general and can be applied to any contrastive learning method that benefits from hard negatives, including SimCLR[6], CPC [36], PIRL [25], and others [10, 21, 34, 39, 42]. These methods, which employ the InfoNCE loss function (or its variants [6, 10]) and instance discrimination as the pretext task, can benefit from the enhanced hard negative generation strategies proposed by SynCo. The introduction of synthetic hard negatives can provide these methods with more challenging and informative contrasts, potentially leading to better feature representations. Also, the applicability of SynCo extends beyond visual representation learning. The concept of hard negatives can be applied to various domains such as natural language processing, audio processing, and other areas where contrastive learning is relevant."}]}