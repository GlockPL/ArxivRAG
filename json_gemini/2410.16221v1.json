{"title": "On Creating an English-Thai Code-switched Machine Translation in Medical Domain", "authors": ["Parinthapat Pengpun", "Krittamate Tiankanon", "Amrest Chinkamol", "Jiramet Kinchagawat", "Pitchaya Chairuengjitjaras", "Pasit Supholkhan", "Pubordee Aussavavirojekul", "Chiraphat Boonnag", "Kanyakorn Veerakanjana", "Hirunkul Phimsiri", "Boonthicha Sae-jia", "Nattawach Sataudom", "Piyalitt Ittichaiwong", "Peerat Limkonchotiwat"], "abstract": "Machine translation (MT) in the medical domain plays a pivotal role in enhancing healthcare quality and disseminating medical knowledge. Despite advancements in English-Thai MT technology, common MT approaches often underperform in the medical field due to their inability to precisely translate medical terminologies. Our research prioritizes not merely improving translation accuracy but also maintaining medical terminology in English within the translated text through code-switched (CS) translation. We developed a method to produce CS medical translation data, fine-tuned a CS translation model with this data, and evaluated its performance against strong baselines, such as Google Neural Machine Translation (NMT) and GPT-3.5/GPT-4. Our model demonstrated competitive performance in automatic metrics and was highly favored in human preference evaluations. Our evaluation result also shows that medical professionals significantly prefer CS translations that maintain critical English terms accurately, even if it slightly compromises fluency. Our code and test set are publicly available https://github.com/ preceptorai-org/NLLB_CS_EM_NLP2024.", "sections": [{"title": "1 Introduction", "content": "Medical-domain machine translation (MT) serves as a critical component in enhancing healthcare quality and disseminating medical knowledge. By providing accurate translations of medical research publications, MT enables local medical professionals without English proficiency to overcome the linguistic barrier and have access to more medical academic resources, which are predominantly written in English (Pecina et al., 2014; McLean et al., 2013). This accessibility is crucial for facilitating continuing medical education, which has been shown to be an effective strategy for healthcare professionals to enhance care quality and patient outcomes (Bloom, 2005; Randhawa et al., 2013).\nDespite the research in the English-Thai MT field, most of the common MT techniques are not yet suitable for the medical domain due to the need for precise translation of medical terminology. Translating medical terminology accurately is challenging due to the lack of equivalent Thai terms for some English medical keywords. Thus, it is understandable why common techniques of machine translation in the medical domain MT such as Google NMT, No Language Left Behind (NLLB) (Team et al., 2022), GPT-4 (Achiam et al., 2023), or Gemini-Pro (Team et al., 2023) cannot translate medical keywords precisely, as shown in Figure 1. Previous studies have aimed to improve translation accuracy through terminology integration (Nieminen, 2023; Semenov et al., 2023), yet their application to the Thai language and specifically in the medical field remains limited.\nRather than focusing on enhancing the accuracy of terminology translation, our objective is to preserve medical terms in their original English form within the translated output, thus removing the need to handle terminology translation. This strategy is characterized as code-switching (CS), deviating from conventional monolingual translation practices. Apart from reducing the task's complexity, framing this problem as CS is also preferred by Medical Doctors (MDs). A previous study from the Thai COVID-19 administrative unit (Toomaneejinda et al., 2022) suggests that medical professionals prefer retaining medical and technical keywords in English, with the rest of the translation in Thai, to avoid potential translation inaccuracies. Other studies (Alqurashi, 2022; Nur'Aini and Fanani, 2019; Wood, 2018; Rodr\u00edguez Tembr\u00e1s, 2016) also suggest that this phenomenon exists in other languages as well, including Arabic, Javanese, and Spanish. As shown in Figure 1, keeping medical terms in English preserves the original meaning and is preferred by MDs.\nHowever, CS datasets are usually scarce, preventing researchers from developing a CS translator. Several initiatives have been made to address this issue. For example, the LinCE dataset (Aguilar et al., 2020) is one of the publicly available CS datasets focused on the general domain created to mitigate this problem. Additionally, various studies have focused on enhancing CS dataset efficiency through augmentation techniques (Gowda et al., 2022; Sugiyama and Yoshinaga, 2019; Menacer et al., 2019), pre-training techniques (Yang et al., 2020; Iyer et al., 2023), synthetic CS dataset generation techniques (Tarunesh et al., 2021; Appicharla et al., 2021; Xu and Yvon, 2021). However, these previous works did not focus on medical texts. Furthermore, the adaptation of such research to the Thai language context remains limited. This, in turn, leads to a significant scarcity of the English-Thai CS translation dataset, especially in the medical domain, as shown in Table 1.\nIn this work, we aim to achieve two objectives: (i) address the data scarcity in medical-domain English-Thai MT and (ii) validate our hypothesis that doctors prefer CS translations to monolingual ones in medical contexts. To achieve the first objective, we create a new English-Thai CS dataset for the medical domain. Our process begins by generating initial CS translations (pseudo-CS) of English medical texts using a widely available monolingual translator. During this translation process, we apply a keyword masking algorithm to preserve key medical terms. We then hire an annotator to post-process and clean a portion of the generated translations, as opposed to doing the whole translation process, to save both time and resources.\nTo achieve our second objective, we conduct comprehensive evaluations using both traditional MT metrics and MD evaluations to confirm our hypothesis. This involves evaluating 52 models, including an off-the-shelf translator, large language models (LLMs), and our fine-tuned CS models based on NLLB. Furthermore, we assess the translations for factual accuracy and MD preference by having MDs directly rate them and by distributing preference ranking questionnaires, respectively. Our findings reveal that our fine-tuned CS model based on NLLB is preferred by MDs due to its factual accuracy, even though it achieves a lower score in traditional metrics when compared to off-the-shelf translators like Google NMT. These results indicate that traditional MT metrics are inadequate for evaluating medical-domain translations and that MDs prefer CS translations over monolingual ones.\nTo summarize, our key contributions are:\n\u2022 We propose the first benchmark dataset specifically designed for medical English-Thai CS translation.\n\u2022 We develop the first open-source model tailored for medical English-Thai CS translation, which is preferred by MDs over Google NMT and GPT-3.5 systems.\n\u2022 We present a comprehensive evaluation of various models on our benchmark, including 52 models, 8 metrics, and 27 MD evaluators. These results reveal a misalignment between traditional MT metrics and the judgments of medical professionals, and underscore the preference of MDs for CS translations.\n\u2022 Our code, test set, and translation models are publicly available at https://github.com/ preceptorai-org/NLLB_CS_EM_NLP2024."}, {"title": "2 Related Works", "content": "2.1 Neural Machine Translation (NMT)\nNMT has gained prominence in both academic and commercial sectors, largely due to advancements in Transformer-based architectures (Vaswani et al., 2023). Various models designed for NMT, such as mT5 (Xue et al., 2021), mBART (Liu et al., 2020), OPUS (Tiedemann and Thottingal, 2020), and NLLB (Team et al., 2022), have been developed. However, as previously mentioned, most of these NMT models cannot perform precise terminology translations, which disqualifies them from the medical domain.\nThe emergence of Large Language Models (LLMs) has further changed the NMT landscape. LLMs, such as GPT-4, have demonstrated emergent abilities in machine translation, excelling in paragraph-level translations without the need for extensive fine-tuning on large parallel corpora (Wei et al., 2022; Kocmi et al., 2023). A few studies (Zhu et al., 2023; Robinson et al., 2023; Bawden and Yvon, 2023) have suggested that LLMs are not yet effective translators, especially in low-resource languages including Thai. Nevertheless, it has been shown that LLMs are proficient at generating CS data for many languages (Yong et al., 2023). To the best of our knowledge, no research has comprehensively investigated the performance of LLMs (both closed and open-source) in translating the Thai language, especially in the medical domain.\n2.2 Evaluation Metrics for NMT\nThe assessment of Machine Translation (MT) quality is a continually evolving field of research. Several automated metrics have been proposed to measure MT quality through lexical analysis, including BLEU (Papineni et al., 2001), chrF (Popovi\u0107, 2015), METEOR (Banerjee and Lavie, 2005), and Translation Edit Rate (Snover et al.). Furthermore, various neural-network-based metrics have been devised to enhance the measurement of MT quality using neural networks: COMET (Rei et al., 2020), Mask-Language-Modeling Score (Zheng et al., 2021), and BLEURT (Sellam et al., 2020). While these metrics provide effective means to assess translations, several studies have also shown their limitations, indicating that these metrics do not always correlate well with human evaluation (Mathur et al., 2020; Callison-Burch et al., 2006; Roy et al., 2021). It still remains unclear whether these metrics align well with the specific use cases of medical-domain MT, where the precise translation of terminology is more important than overall sentence fluency.\nHuman evaluation is also crucial, especially in a medical context where technically accurate and human-readable translations are necessary. Graham et al. (2013) attempted to better standardize crowd jurisdictions on with Likert-type continuous rating scales. After that, a band scale was proposed by (Menacer et al., 2019; Tarunesh et al., 2021) for more consistent qualitative evaluation among human judges. Bai et al. (2022); Askell et al. (2021) introduced the concept of the Elo Rating to benchmark multiple translation systems' performances. Elo Rating allows for a leaderboard-like relative comparison between these systems. All these works provided valuable perspectives on how to conduct human preference evaluations on NMT models. Using these studies as a basis of our human evaluation on translation models, we choose to use an improved Elo-based metric called the Glicko score, which was developed by Glickman to accounts for the uncertainty in Elo-based calculations (Glickman, 1995a,b, 1999)."}, {"title": "3 Benchmark Data Collection", "content": "3.1 English Text collection\nOur dataset of English medical texts was collected from an in-house LLM-based application designed to tackle intricate medical questions, with an emphasis on differential diagnosis and multiple-choice problems. The dataset consists of 10,000 medical excerpts, with an additional 250 excerpts reserved for testing purposes.\n3.2 Pseudo-translation Masking and Generation\nIn the absence of an existing CS translator, we adopt a masking approach to create our CS translation dataset. Inspired by the Language Identification (LID) translation pipeline (Ramadurgam and Mundada), this method involves augmenting a standard monolingual translator with a keyword masking strategy. By identifying the important medical keywords and selectively translating the rest of the sentence, this method allows for the retaining of domain-specific terminology after translation. Using this, we establish a pseudo-CS translator, which forms the basis of our benchmark dataset.\nThe overview of the procedure for the Keyword masking algorithm is as follows:\n1. Use GPT-4 to identify medical keywords in the original English sentence. We specifically chose GPT-4 for its capabilities in Named Entity Recognition (NER) of medical terms (see Appendix F for our evaluation) and its flexibility, which allows us to manually adjust the types of terms to include or exclude in order to mimic medical CS as closely as possible.\n2. Replace each medical keyword with a unique placeholder. This results in an English text where medical terms are masked.\n3. Process the masked English text through an MT model to obtain a masked Thai text. In this text, the non-medical parts are translated, while the placeholders remain untouched.\n4. Substitute the unique placeholder tokens with their original English medical keywords to produce the final Thai-English pseudo-CS translation.\nExpanding on Step 3, the masked sentences from the previous step are translated to generate pseudo-CS translations. All English excerpts and their corresponding masked versions are processed through the keyword-masked Google translation system, resulting in Thai pseudo-CS translations. To ensure proper alignment between the English and Thai+English (as in the target translations contain CS between Thai and English) content, both the original English excerpts and the CS translations are segmented into chunks of fewer than 256 tokens. We then re-validate that the English and Thai texts contain the same number of chunks.\nRegarding the size of our dataset, our dataset size is competitive when compared to existing code-switched datasets. In terms of the number of samples, our dataset has 64K records, while a single language pair within the LinCE has 7k to 67k records. For the total token counts, our dataset has 640K tokens, while a single language pair within LinCE has 33k to 808k tokens. We split our dataset into 63,982 English-to-Thai CS translation pairs for training and 1,100 translation pairs for the test benchmark.\n3.3 Test dataset Constitution\nTo ensure the quality of our test set, we employ human annotators to recheck its fluency with the instruction in Figure 4. After annotation, the dataset goes through an NLP pipeline to correct typos and adjust spacing. Subsequently, it then undergoes another round of validation by MDs to ensure its readability and factual accuracy. The MDs make further corrections to improve the accuracy of the translation chunks compared to their source text. This process ensures that every sentence and CS word is correct as verified by MDs; LLMs only serve to reduce the time spent here.\n3.4 Training Data Procedure\nAs mentioned in the previous step, we utilize both human annotators and MDs to assess the quality of our test set. However, applying the same process to the training data would be 64 times more expensive than the test set. To mitigate this issue, we employ data augmentation and filtering techniques to improve the quality of our training dataset.\n\u2022 Data Augmentation: Inspired by the backtranslation augmentation method (Sugiyama and Yoshinaga, 2019), we prompt Gemini-Pro to rephrase the existing CS translation while retaining a roughly similar CS boundary. The rephrased CS sentences are then back-translated to generate corresponding English sentences, thereby constructing new translation pairs.\n\u2022 Filtering: We filter the training CS translation dataset based on a rough measure of its quality. We use the COMET score metric (which assesses semantic similarity) to estimate the quality of the translation dataset and filter out samples that did not achieve a COMET score of at least 0.6."}, {"title": "4 Experimental Setup", "content": "4.1 Baseline Models\nOff-the-Shelf Translator (1 model). We leverage Google NMT as our off-the-shelf translator, utilizing the version released on January 17, 2024.\nLarge Language Models (18 models). This set includes OpenThaiGPT 7B, OpenThaiGPT 13B, Typhoon 7B (Pipatanakul et al., 2023), SeaLLM 7B (Nguyen et al., 2023), Llama2 7B, Llama2 13B (Touvron et al., 2023), Google's Gemini-Pro, GPT 3.5, and GPT 4. Each large language model has two prompt variants: one prompted to generate monolingual translations (denoted as the \"MN\" variant) and another prompted to generate CS translations (denoted as the \"CS\" variant). All local LLMs (OpenThaiGPT, Typhoon, SeaLLM, Llama2) are evaluated using bfloat16 precision. The rest are accessed via API calls with default settings and a temperature of 0.1. The GPT models used are based on the 1106-preview version. The Gemini-Pro model is utilized as presented through the API on January 17th, 2024.\nCS Baseline (6 models): We employ a state-of-the-art language translation model, NLLB. We utilized NLLB 3.3B as a base model and fine-tuned it on six variants of our training dataset (Section 3.4) as follows:\n\u2022 NLLB-1: Initial 64k dataset (64k)\n\u2022 NLLB-2: Augmentation of the 64k dataset (64k)\n\u2022 NLLB-3: Initial dataset plus augmentation of the 64k dataset (128k)\n\u2022 NLLB-4: Filtered 64k dataset (30k)\n\u2022 NLLB-5: Filtered augmentation dataset (40k)\n\u2022 NLLB-6: Filtered 64k dataset plus filtered augmentation dataset (70k)\nIt is important to note that our augmentation technique, which utilizes an LLM to rephrase translation pairs, likely results in an overall improvement in the COMET score of the augmented dataset. Setting a fixed COMET score threshold for dataset filtration results in the augmented filtered dataset containing more records than the initially filtered dataset. The exact training configurations are listed in Appendix C. In addition, the inference is performed using bfloat16 quantization.\n4.2 Evaluation Metrics\nWe evaluate 52 translation systems\u201426 with the masking system and 26 without the masking system during the inference step (see Section 3.2)\u2014 using standard machine translation metrics and MD evaluators to further validate our results.\n4.2.1 Machine Metric Evaluation\nWe evaluate all our translation models using our MD-annotated test set. The following metrics are employed for evaluation:\n\u2022 Lexical score (BLEU (Papineni et al., 2001), chrF (Popovi\u0107, 2015), METEOR (Banerjee and Lavie, 2005)).\n\u2022 Translation Edit Rate, which includes Character Error Rate (CER) and Word Error Rate (WER).\n\u2022 Semantic score (COMET (Rei et al., 2020; Guerreiro et al., 2023)).\n\u2022 CS boundary F1 Score, inspired by (Sterner and Teufel, 2023). The CS boundary F1 Score is calculated using the common formula, i.e., the harmonic mean of precision and recall. Precision is defined as the proportion of correctly identified English words in the generated translation compared to those in the reference translation. Recall is the proportion of English words in the reference translation that are correctly identified in the generated translation.\nDetails on the implementation of these metrics are provided in Appendix D.\n4.2.2 Human Evaluation\nAnticipating a lower number of human respondents, we only selected the MD-preferred models for human evaluation. To ensure that each model is compared against each other at least 30 times within a comprehensive evaluation of 52 models, it would require at least 39,000 data points, or approximately 390 respondents, to achieve a statistically significant result. By selecting only 8\u201311 models, we can reduce this number to 2,000 data points or 20 respondents. Our methodology is as follows.\nBefore human evaluation We assess the factual accuracy of translations produced by each model by soliciting evaluations from four medical professionals. These professionals assess each translation's factual correctness using our specific rubric. The evaluation process is outlined as follows:\n\u2022 10 English texts are randomly selected from our test set and translated using 52 different translation systems.\n\u2022 Medical professionals are instructed to individually rate each translation for factual correctness on a scale from 1 to 7, according to a detailed rubric provided in Table 4. Each medical professional is unaware of the translations' source models, and the sequence of translations they evaluate is randomized to prevent bias.\n\u2022 The score for each model, as rated by an evaluator, is determined by calculating the median of the scores assigned to its translations.\n\u2022 An arithmetic mean of the median scores across all evaluators is then calculated to assign each model its preliminary final score.\nSubsequent human evaluations are conducted only on model categories (differentiated by base translation model and usage of keyword masking) that achieved ratings higher than the Google NMT.\nHuman Preference Evaluation We perform a human preference evaluation to determine which models are preferred by crowd-sourced medical practitioners, assessing their preference for translations as well as their performance on our human dataset. Note that, in this step, we only ask medical professionals on our chatbot platform to assist in evaluating translations.\n\u2022 We evaluate 10 translation models and the human label based on the previous step. This involves selecting one model from each category identified in the last step.\n\u2022 We design a self-administered, web-based survey using a ranking format to enhance participants' experiences (Revilla and H\u00f6hne, 2020). Given that ranking five items requires approximately 40 seconds (Sauro et al., 2023) and our items consisted of a few sentences, we include ten ranking questions, each estimated to take approximately 1.5 minutes to complete.\n\u2022 For each participant, we randomly sample 10 English texts from our benchmark test set. For each text, we present five versions of the translations, each randomly selected from the list of \"comprehensible\" models along with the human-annotated translation.\n\u2022 Participants are asked to rank each translation sample based on the factual accuracy of the sentence and their preference (as shown in Figure 2). We specifically instruct them to disregard the proportion of English text retained in the translation (as shown in Figure 8).\n\u2022 Subsequently, we use the preference data from the human evaluation to calculate the Glicko Rating, measuring the comparative preference of each model against the others and the human annotator. The initial Glicko rating is set according to the standard, with $r = 1500$ and $RD = 350$.\nMoreover, we implement a simple filter to monitor each participant's response time to the survey. Participants who completed the survey in less than 5 minutes were flagged as potentially invalid, and their choice ordering was re-examined to confirm the validity of their responses. A row is flagged as an invalid record if the choice order remains nearly identical across questions despite variations in the translation model."}, {"title": "5 Main Experimental Results", "content": "The full evaluation results for all 52 models on our dataset are presented in Table 2. We categorize the results into two groups: (i) traditional machine translation (MT) metrics and (ii) human preference.\nTraditional MT metrics. We present our two best models: NLLB-1 (initial 64k dataset) and NLLB-4 (filtered 64k dataset) with Mask. These models demonstrate remarkable results among open-source models and achieve competitive results against closed-source models. NLLB-1 (without masking) achieved the top CS F1 score in its category, showing remarkable performance compared to off-the-shelf models and LLMs. NLLB-4 with Mask also obtained a competitive CS F1 score compared to those models equipped with masks, rivaling GPT-4 + Mask. In conclusion, these results underscore the importance of training models on source data rather than relying on off-the-shelf models. The NLLB results show that we achieve comparable outcomes to those of Google-NMT and Gemini-Pro on machine translation metrics, namely BLEU and chrF. On the other hand, the code-switch metric (CS F1) indicates that NLLB models retain medical keywords more effectively than off-the-shelf MT models.\nHuman preference. As shown in Table 3, human preference evaluation received responses from 23 medical doctors (MDs). The Glicko rating calculation results show that both NLLB models are preferred over Google NMT and LLMs like GPT-3.5. Both models are also almost equally preferred when compared to translations from Gemini-Pro models. Thus, we can summarize that machine translation metrics might not fully satisfy medical doctors' preferences. The results from the MT metrics contradict human preferences, which we will discuss further in Section 6.1. Confirming our hypothesis, we also found that MDs preferred CS translation over translating all words into the target language, as indicated by the CS F1 metric. MD preferences are discussed in more detail in the following section, Section 6.2."}, {"title": "6 Discussion", "content": "6.1 Automated Metrics Versus Factual Accuracy\nThe evaluation results reveal an unexpected outcome: Google NMT consistently achieves top scores across nearly all machine metrics among the 52 models, despite the lack of medical terminology preservation. Similarly, Google NMT with Mask dominates in almost every automated metric among the masked models (a better rank breakdown can be seen in Table 7). Nevertheless, a closer examination of individual samples still reveals that Google NMT frequently translates medical terminology imprecisely (as shown in Figure 3). We hypothesize that Google NMT's superior performance in automated metrics is due to its fluency in translating non-essential parts of the medical text, which constitutes the bulk of our dataset. Conversely, the accuracy of medical-domain translations rather depends on the precise translation of critical medical terms, an area where Google NMT falls short. This is further supported by the minimal correlation between most automated metric scores and factual accuracy, especially among models that are rated higher than 3 in factual accuracy (see Figure 5).\nIn fact, the CS F1 metric addresses this issue by focusing on the preservation of key medical terms, demonstrating a stronger positive correlation with factual accuracy ratings. However, it is still not a comprehensive metric, as it only assesses the retention of English keywords without considering the quality of the Thai translation. A trade-off consideration between the retention of precise medical terms and the fluency of the overall translation may be necessary to develop a more suitable automated metric for medical translation tasks.\n6.2 MD Evaluation\nOur human evaluation within the MD population further supports our hypothesis that traditional automated metrics are not well-suited for medical-domain MT. This is shown by the significant correlation observed between Glicko ratings and both factual accuracy scores ($r = 0.698$) and CS F1 scores ($r = 0.516$), as opposed to the weak correlation (less than 0.3) between traditional automated metrics and Glicko ratings (seen in Figure 7)."}, {"title": "7 Conclusion", "content": "This paper presents an approach for performing MT in the medical domain using a CS translation to generate translations preferred by medical professionals. We developed a method for generating CS translation data, trained a CS translation model leveraging this data, and evaluated its performance against multiple strong baselines. The experimental results demonstrate that although most automated metrics might be suitable for measuring translation fluency, they are inadequate for assessing factual accuracy or medical doctors' preference in the translations. While current MT technologies may offer monolingual translations with high fluency, medical professionals exhibit a clear preference for CS translations that accurately preserve crucial terms in English, even at the expense of fluency."}, {"title": "Limitations", "content": "There are inherent risks associated with machine translation (MT), particularly the potential for misinterpreting medical terminology and technical terms. While our models have shown promising results, there is still a possibility of inaccuracies in translation that could affect daily practice.\nMoreover, there is still some potential for further improvement. We have not conducted extensive human preference evaluations on all 52 models because doing so would require more than 390 MD respondents, whom we cannot find or hire. Also, we have not optimized prompts for LLMs to produce the best CS translations yet. Our MDs deemed the translations generated by these prompts acceptable internally, so we selected them. Lastly, we have not conducted an extensive hyperparameter search for NLLB training. To limit the cost of the fine-tuning process, we selected the standard learning rate and learning rate scheduler that is used throughout the field and fixed it for the entire fine-tuning process of NLLB."}, {"title": "Ethical Considerations", "content": "Our human annotators were undergraduate students majoring in linguistics at a university in Thailand. We ensured that they received monthly monetary compensation at an industry-competitive salary. We compensated our annotators by first measuring their annotation speed in terms of the number of words processed per hour. After that, we established a monthly target for the annotators to achieve, and we paid our annotators a fixed salary. Other human evaluators who respond to our questionnaire participate voluntarily. The participants were promised free usage of our upcoming product as compensation by randomly selecting five participants. In this regard, we have to collect their names and emails to prevent spamming attempts. For the remaining participants, we informed them that we would compensate for their work by releasing a questionnaire dataset without the respondents' information to the public domain, which we will release under a CC-BY-NC 4.0 license.\nRegarding the licensing of models, we strictly adhere to the intended uses outlined by their respective licenses. The NLLB weight checkpoints we use as our pretrained weights are licensed under CC-BY-NC 4.0, which allows us to distribute our newly fine-tuned NLLB weights to the public for non-commercial use. We have also adhered to the Llama2 and SeaLLM Licenses by not using their outputs to enhance any language model and by restricting their use to research benchmarking purposes only. Additionally, we followed OpenAI and Google Gemini's Terms and Conditions strictly: we did not compete with OpenAI and Gemini's models but rather used them fairly for research purposes.\nAll local LLM inferences, NLLB fine-tuning, and NLLB inferences for translation were performed on a single A100 GPU, also with the maximum amount of batching possible. We used a total of 60 GPU hours for fine-tuning NLLB, 24 GPU hours for performing local LLM inferences, and 3.5 GPU hours for performing 24 variants of NLLB inferences.\nRegarding a potential leak of personal information, our source English texts inherently contain no personal information, as they are outputs from our own LLM product with no personal information in the prompt. We conduct an initial screening of the test benchmark dataset regardless, which confirms the absence of personal information in any of the English texts. We also instructed our internal annotator to remove any identifying information in case any is found within the annotation process. Another potential concern arises when we collect names and email addresses from MD evaluators. However, we use this information solely for spam tracking purposes and do not disclose or utilize this personal data for any other reason, except to contact individuals later regarding compensations for free usage."}, {"title": "B Prompts for Large Language Model Translation", "content": "CS Translation Prompt\nYou are a linguist with expertise in medicine and had your training in Thailand.\nYou are well acquainted to how's Thai MD usually code switched between Thai Language and English when they're communicating medical-related information among each other.\nFor instance, you never translate the following English medical terms and jargons, symptoms, technical terms, and pharmaceutical terms into Thai.\nHence, task is to examine the medical-related information text input and translated them into Thai with the previously given constraint and information.\nMonolingual Translation Prompt\nTranslate the following text input into Thai in Medical Context\nGPT4 Medical NER Prompt\nAnnotate the medical report with HTML-like tags. The output should start with <annotated> and end with </annotated>.\nUse the following tags to annotate the respective terms:\n<patho> for pathological and medical symptoms terms\n<pharm> for pharmaceutical terms and drugs' names\n<taxo> for scientific names and taxonomical-like names\n<anato> for anatomical terms\n<chem> for chemical names\n<med> for medical practices and jargons\nFYI:\nDrug names sometimes start with a single charactor followed by full stop then full name.\nFor example: A. Parafivir, B. Paracetamol.\nAnatomical terms must include limbs, organs, cells, and organelle.\nCNLLB Training Configuration\nLoraConfig:\nr = 16,\nlora_alpha = 16,\ntarget_modules = [\"q_proj\", \"v_proj\"],\nlora_dropout = 0.1,\nbias = \"none\",\nTrainingArguments:\nnum_train_epochs = 10,\nevaluation_strategy = \"steps\",\nlogging_strategy =\"steps\",\nsave_strategy =\"steps\",\neval_steps=5000,\nlogging_steps=500,\nsave_steps=5000,\nbf16=True,\nseed=42,\ndata_seed=42,"}, {"title": "D Evaluation Metric Implementation details", "content": "The evaluation environment was established using Python 3.11, incorporating the following key libraries and their respective versions:\n\u2022 PyTorch 2.2.0: Used for neural network-based computations and model loading, supporting the latest deep learning model features and optimizations.\n\u2022 NLTK 3.8.1: Provided tools for text processing and evaluation metrics, including BLEU, METEOR, and CHRF scores.\n\u2022 PyThaiNLP 4.0.2: Essential for processing the Thai language, used specifically for tokenizing Thai text and for the implementation of the NewMM tokenizer (Phatthiyaphaibun et al., 2023).\n\u2022 JiWER 3.0.2: Employed for calculating Word Error Rate (WER) and Character Error Rate (CER), key in assessing model performance in speech recognition tasks.\n\u2022 Unbabel Comet 2.2.1: Employed for calculating the COMET score using the XCOMET-XL (Guerreiro et al., 2023) model.\nWe implemented a Python script on our own to calculate the Glicko rating based on (Glickman, 1999). The RD/Glicko evaluation was established using an initial rating of 1500 and an RD of 350. All the ratings are calculated at once, eliminating the need for nondeterminism. The 95% confidence interval is reported using 2 times the RD."}, {"title": "E Disclaimer for Participants", "content": "Notice to Participants\n\u2022 This study focuses exclusively on medical questions.\n\u2022 Our system leverages LLM technology currently under development. Do not use the output as medical facts.\n\u2022 Participant's inputs, system outputs and feedback will be reviewed and used to improve the system capability.\n\u2022 To comply with Thai PDPA law, do not disclose real patient information or any patient identifiable information in general. Use hypothetical clinical cases only."}, {"title": "F GPT-4's Medical NER Performance", "content": "Although our confidence in GPT-4's capabilities in medical keyword extraction was already substantial, based on its performance in various analyses (Nori et al., 2023), we have conducted an experiment to determine the medical NER performance of multiple systems."}, {"title": "G Medical Doctor Annotator Instruction", "content": "Instruction\n\u2022 Please review the annotations from human annotators in the Google Spreadsheet provided.\n\u2022 Look for any serious errors in the labels.\n\u2022 Pay attention to whether any technical words have been lost during the cleaning process.\n\u2022 Leave a comment in the spreadsheet for any errors that you may find."}]}