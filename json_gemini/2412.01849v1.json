{"title": "Towards Data-centric Machine Learning on Directed Graphs: a Survey", "authors": ["Henan Sun", "Xunkai Li", "Daohan Su", "Junyi Han", "Rong-Hua Li", "Guoren Wang"], "abstract": "In recent years, Graph Neural Networks (GNNs) have made significant advances in processing structured data. However, most of them primarily adopted a model-centric approach, which simplifies graphs by converting it into undirected formats and emphasizes model designs. This approach is inherently constrained in real-world applications due to inevitable information loss in simple undirected graphs and data-driven model optimization dilemmas associated with exceeding the upper bounds of representational capacity. As a result, there has been a shift toward data-centric methods that prioritize improving graph quality and representation. Specifically, various types of graphs can be derived from naturally structured data, including heterogeneous graphs, hypergraphs, and directed graphs. Among these, directed graphs offer distinct advantages in topological systems by modeling causal relationships, and directed GNNS have been extensively studied in recent years. However, a comprehensive survey of this emerging topic is still lacking. Therefore, we aim to provide a comprehensive review of directed graph learning, with a particular focus on a data-centric perspective. Specifically, we first introduce a novel taxonomy for existing studies. Subsequently, we re-examine these methods from the data-centric perspective, with an emphasis on understanding and improving data representation. It demonstrates that a deep understanding of directed graphs and its quality plays a crucial role in model performance. Additionally, we explore the diverse applications of directed GNNs across 10+ domains, highlighting their broad applicability. Finally, we identify key opportunities and challenges within the field, offering insights that can guide future research and development in directed graph learning.", "sections": [{"title": "I. INTRODUCTION", "content": "Graph Neural Networks (GNNs), as an emerging paradigm in machine learning for structured data, have attracted significant attention within the computing community [1]\u2013[4]. Specifically, GNNs overcome the limitations of traditional methods by employing recursive message-passing mechanisms that facilitate information flow across the graph structure. Through this iterative process, GNNs generate rich, embedded representations of nodes, edges, and entire graphs for various graph-based downstream tasks, effectively capturing the underlying structural and relational information. Their wide applicability across industrial domains, such as recommendation systems [5]\u2013[9], anomaly detection [10]\u2013[12], traffic flows [13]\u2013[17] and drug-drug interaction networks [18]\u2013[22].\nIn the development of GNNs, significant attention has been devoted to model-centric studies, which focus on designing complex GNNs tailored to specific datasets and diverse learning tasks. From the data perspective, researchers often simplify graph data by converting it into undirected forms, enabling a focus on optimizing model architectures. These approaches, commonly referred to as undirected GNNs, aim to enhance GNN performance by prioritizing model innovation over complex data representations, driving progress in various application domains [23]\u2013[26]. However, this simplification assumes that the input data has been adequately preprocessed or refined into a suitable format\u2014an assumption that often does not hold in most real-world scenarios [27]. Consequently, the limitations of undirected GNNs hinder their practical effectiveness in real-world applications.\nLimitation 1: Hindering Advancement in Academia. From a theoretical neural representation perspective, simplifying natural graphs into undirected forms overlooks crucial dimensions of information, such as node/edge heterogeneity (heterogeneous graphs), group relationships between nodes and edges (hypergraphs), and edge directionality (directed graphs). The loss of such critical information cannot be recovered during the training phase of any well-designed model. Once essential data characteristics, like directed relationships or node heterogeneity, are omitted in the graph construction stage, the model's learning process is inherently limited. This constraint prevents the model from fully capturing the complexity of the original graph structure, thereby hindering its ability to make accurate predictions or derive meaningful insights from incomplete data representations. In other words, sub-optimal data representations create a model optimization dilemma, where complex neural architectures may fall into local minima, overlooking the crucial role of data representation in dictating the upper bound of the representational capacity. For instance, ignoring edge directionality may lead to hindering models' capability of addressing the entanglement of homophily and heterophily (whether connected nodes share similar features or labels) that has long plagued the graph learning community in the context of undirected graphs [28], [29].\nLimitation 2: Hindering Deployment in Industry. From an industrial deployment perspective, simplified undirected graph representations often fall short of capturing the complex, nuanced relationships that exist in real-world applications."}, {"title": "II. BACKGROUNDS", "content": "This section outlines the foundational definitions for directed graph neural networks, abbreviated as directed GNNs. First, we introduce the notations utilized throughout this paper, summarized in Table II. Following that, we describe three kinds of common downstream tasks that are commonly adopted in the directed GNNs researches."}, {"title": "A. Notations", "content": "Through out this paper, we use bold uppercase characters, such as A, to denote matrices, bold lowercase characters (e.g. x) to denote vectors, and calligraphic characters, for example V, to denote sets. We use Pytorch-style indexing conventions for matices and vectors. For instance, A[i, j] denotes the value of matrix A at the i-th row and the j-th column. Also, A[i, :] and A[:, j] denotes the i-th row and j-th column of matrix A, respectively, and x[i] denotes the i-th value of vector x. The notations employed in this survey are summarized in Table II.\nWe consider a general graph representation method G = (V,E) with |V| = n nodes and |E| = m edges, in which Vi, vj \u2208 V denotes i-th and j-th nodes of graph G, and eij = (Vi, vj) \u2208 V denotes the directed edge from node vi to node vj. We can also represent the graph structure by the adjacency matrix A \u2208 {0,1}n\u00d7n, where A[i, j] = 1 represent eij E E and A[i, j] = 0 represent eij \u2209 E. A graph may contain node attributes, effectively organized within a node feature matrix X \u2208 Rn\u00d7f, where xi \u2208 Rf or X[i, :] \u2208 Rf represent the feature of node vi. A graph may also have node-level labels, represented by a label vector y, where the i-th value y[i] denotes the label of node vi."}, {"title": "B. Downstream Tasks", "content": "Based on the scale of prediction targets, we classify the downstream tasks of directed GNNs into three categories: node-level tasks, edge-level tasks, and graph-level tasks."}, {"title": "Node-level Tasks", "content": "Node-level directed GNNs tasks are designed to predict specific properties for each node within one or more graphs. Common node-level tasks include node classification [23], node regression [98], and node clustering [99]. Node classification, the most widely studied of these tasks, involves predicting a discrete label for each node. In training models for node classification, a cross-entropy loss function is often applied to minimize the discrepancy between predicted logits and the true labels of training nodes. In contrast, node regression seeks to predict continuous properties of each node, while node clustering aims to group nodes into distinct clusters without requiring access to label information."}, {"title": "Edge-level Tasks", "content": "Edge-level directed GNNs tasks aim to predict specific properties of each edge within one or more graphs. Common edge-level tasks include edge classification [100] and link prediction [101]\u2013[103]. For example, link prediction seeks to determine the likelihood and its directionality of a connection existing between two nodes. Typically, this task is framed as a ternary classification problem, with a cross-entropy loss function applied to optimize link prediction models. Unlike link prediction, which focuses on the existence and the directionality of edges, edge classification predicts a discrete label for each edge, identifying its particular characteristics rather than solely its presence."}, {"title": "Graph-level Tasks", "content": "In contrast to node- and edge-level tasks, graph-level tasks focus on modeling and making predictions at the level of the entire graph, rather than on nodes or edges. These tasks commonly include graph classification [104]\u2013[107], graph regression [108], graph matching [109], and graph generation [110]. Among these, graph classification serves as a foundational graph-level task, involving the assignment of a label or class to an entire graph. To optimize models for graph classification, the cross-entropy between graph-level predictions and true labels is minimized. Other graph-level tasks address distinct objectives: graph regression aims to predict continuous properties of various graphs, graph matching seeks to evaluate the degree of similarity between pairs of graphs, and graph generation focuses on creating new graph samples."}, {"title": "III. DIRECTED GRAPH NEURAL NETWORKS METHODS", "content": "Directed GNNs are designed to learn representations that embed directed graphs into a low-dimensional space, while aiming to retain the intrinsic properties of the original directed graphs. Directed GNN methods can be categorized into three primary frameworks: the message-passing framework, eigenpolynomial-based framework, and self-attention-based framework, as shown in Table III. The primary distinction"}, {"title": "A. Message-passing Framework", "content": "Motivations. Message passing mechanisms are central to GNNs, designed to enable information exchange among nodes to learn features within the graph structure. This mechanism typically consists of two main steps: Propagation and Aggregation. In the propagation step, each node receives and transmits feature information from neighboring nodes, reflecting inter-node connectivity. In the aggregation step, each node combines the received information to update its own representation. This layered approach allows the model to progressively capture both the local neighborhood structure and each node's characteristics, resulting in node representations that incorporate information from both the node itself and its surroundings.\nIn the case of undirected graphs where the adjacency matrix A is symmetric and D is the degree matrix of A, many GNNs adhere to strictly spatial symmetric message-passing mechanisms, leveraging the balance of information from both directions of each edge within a neighborhood. For node u, the l-th message-passing scheme can be expressed as follows:\n$Z_u^l = Agg\\left(W^{(l)}, H_u^{(l)}\\right)$,\n$H_u^{(l)} = Prop\\left(\\Pi, \\left\\{Z_v^{(l-1)}\\right\\}_{v \\in N(u)}, \\left\\{e_{vu}\\right\\}_{v \\in \\varepsilon} \\right\\)$,\nwhere \u03a0 is the propagation kernel (e.g. \u010e\u00af\u00bd\u00c3\u010e\u00af\u00bd), defining the rule for information flow between nodes. $Z_u^l$ represents the feature representation of node u at the l-th layer of the model and $Z_u^{(0)} = X_u$. The Propagation function Prop (\u00b7) gathers and distributes feature information from neighbors based on the propagation rule \u03a0. The Aggregation function Agg (.) is responsible for combining the feature information propagated from neighboring nodes, which depends on the learnable parameter matrix $W^{(l)}$.\nTo capture the asymmetric topology of directed graphs, some spatial-based directed GNNs continue to follow the strict symmetric message-passing paradigm [125]\u2013[128]. However, the asymmetry of edges in directed graphs introduces directed dependencies between nodes, necessitating a message-passing process that considers edge directionality to accurately capture the topology of directed graphs. Consequently, recent advances in directed GNNs have extended the traditional framework by designing customized directionality-aware Propagation and Aggregation mechanisms, thereby enhancing the model's capacity to capture directed relationships within the graph.\nMethods. EDGNN [129] extends traditional GNN propagation by separately handling incoming and outgoing edges and including edge labels. From the aggregation perspective, it applies different weight matrices to self-features and neighbor features, allowing the model to assign varying levels of importance to self-information and neighborhood information. D-VAE [112] employs an asynchronous message passing scheme to encode and decode DAGs, respecting the computation dependencies. The encoder and decoder of D-VAE both utilize GNNs to update the current node's state by aggregating the states of its predecessor nodes. DGCN [92] proposes a novel propagation mechanism that incorporates both first-order and second-order proximity of neighbors. This innovative approach leads to the development of aggregator that includes two distinct sets of independent learnable parameters. DAGNN [113] processes nodes based on the partial order relation of DAGs. In each layer, the node representation is updated by the attention-based propagator and the GRU-based aggregator. NSTE [48] generates two vectors for each node, capturing its dual roles as both a source and a target in directed edges. The model introduces tunable parameters for weighting the degrees in its asymmetric propagation kernel. The use of separate aggregators for the source and target representations allows the model to distinguish the roles of nodes effectively. GRANDE [80] leverages an attention mechanism to aggregate information from both in-neighbors and out-neighbors, effectively integrating neighborhood information from both incoming and outgoing edges of nodes. It also outputs both node and edge representations by performing message passing over the original graph and its augmented edge adjacency graph. D-HYPR [114] propagates information through the graph by considering multi-ordered four canonical types of partitioned neighborhoods. The model employs hyperbolic space for"}, {"title": "B. Eigenpolynomial-based Framework", "content": "Motivations. The eigenpolynomial-based framework utilizes the Graph Fourier Transform [130] to facilitate the exchange of information between the spatial representation of graph topology and node features, and the spectral representation of eigenvectors and eigenvalues, thereby designing an appropriate spectral filter for graph representation learning. Initially, the framework decomposes the graph filter basis, which encapsulates information pertaining to graph topology and node features, into n orthogonal eigenvectors along with their corresponding eigenvalues. This process effectively transforms information from the spatial domain into the spectral domain. Subsequently, graph filtering is applied to the eigenvalues to isolate the desired frequency components. Finally, an inverse Graph Fourier Transform is employed to revert the information from the spectral domain back to the spatial domain. According to the principles of Graph Fourier Transform theory, the filtering operation in the graph spectrum domain is equivalent to employing eigenpolynomials derived from the graph filter bases in the spatial domain. Consequently, methods utilizing the eigenpolynomial-based framework favor the implementation of graph representation learning through eigenpolynomials, thereby circumventing the computationally intensive process of decomposing eigenvectors. Formally, a graph spectrum filter can be expressed as follows:\n$x* g_{\\theta} = U g_{\\theta}(A) U^T x = \\sum_i \\theta_i T_i(L) x$,"}, {"title": "C. Sequence-based Framework", "content": "Motivations. The sequence-based framework in GNNs draws inspiration from the Transformer architecture, reinterpreting structured graphs as sequences of tokens (e.g. nodes and subgraphs). Initially, tokenization is applied to represent the graph as a series of elements. For instance, a node tokenizer, denoted as Y(G) = {P_{v_1}, P_{v_2},\u00b7\u00b7\u00b7, P_{v_N} }, tokenizes each node $v_i$ as a numerical token $P_{v_i}$. Alternatively, a subgraph tokenizer, represented by \u03a6(G) = {P_{s_1}, P_{s_2},\u00b7\u00b7\u00b7,P_{s_T} }, tokenizer each subgraph $s_i$ as a numerical token $P_{s_i}$. Following the tokenization step, the encoding step would be applied to capture the long-range dependencies among item pairs in the sequence: H(i) = \u03a8(AGG_i(Y(G)/\u03a6(G))), where i is the i-th encoder and $AGG_i$ is the i-th aggregation function. Most methods enhance performance within this framework by focusing on refining the tokenizer and encoder mechanisms, thereby strengthening the model's capability to interpret and learn from sequence-based representations of graph structures.\nMethods. PACE [119] enhances the node tokenizer within the sequence-based framework by introducing the dag2seq framework, which efficiently transforms DAGs into unambiguous sequences of node tokens. This approach facilitates expressive directed graph representation learning by effectively capturing the unique, long-range dependencies inherent in DAGs. Unlike traditional methods that utilize topological order as the sequence ordering, PACE adopts a canonical order in conjunction with a Transformer encoder and a one-layer injective GNN. This setup ensures injectivity between the original DAGs and the generated node sequences, preserving structural fidelity and supporting robust downstream learning tasks. TMDG [124] introduces two encoding techniques within the sequence-based framework using the node tokenizer and subgraph tokenizer to enable models to effectively capture long-range relationships among node and subgraph tokens while maintaining sensitivity to directed structural topology. The first encoding technique incorporates the Magnetic Laplacian, following the Magnet method [51], by selecting an appropriate hyper-parameter q for positional encoding, thereby enabling the model to reflect graph directionality. The second technique extends the concept of traditional random walks on undirected graphs by integrating bidirected random walks (i.e., both forward and backward directions) into the positional encoding of the sequence-based framework. These encoding mechanisms aim to enrich the model's ability to recognize directed dependencies and structural nuances within graph representations. DAGRA [120] also enhances the encoding"}, {"title": "IV. REVISITING DIRECTED GNNS FROM THE VIEW OF DATA-CENTRIC MACHINE LEARNING", "content": "As outlined in Section I, extensive former researches have primarily focused on model-centric approaches in graph learning, emphasizing model architecture development while often overlooking the critical role of the graph data itself. However, substantial evidence suggests that both the nuanced understanding of graph structure and the quality of graph data can significantly impact model performance. Consequently, this section adopts a data-centric perspective to re-evaluate existing directed GNNs, investigating how these models interpret and refine graph data to enhance their effectiveness and predictive accuracy, as shown in Table IV. First, we begin by introducing three primary views for examining graph data, which form the conceptual foundation for designing and refining directed GNNs. Following the discussion of these graph views, we explore various techniques for graph data improvement, focusing on topological enhancement and node feature enhancement. These enhancement techniques, tailored to different graph views, provide critical strategies for increasing the expressiveness and performance of directed GNN models."}, {"title": "A. Graph Data Understanding & Exploitation", "content": "The initial step in designing a GNN is to develop a robust understanding of the underlying graph data. Graph data can be examined from several perspectives, such as topological, spectral, and sequential views, each providing a distinct interpretation and offering unique insights into the data's structure. These varied perspectives influence the architecture of directed GNNs in different ways. Traditional model-centric approaches often treat these views interchangeably, which may overlook the distinct informational contributions each perspective provides. For example, models might be primarily informed by topological insights yet reinterpreted through a spectral lens, potentially diluting the strengths of each view. In contrast, a data-centric approach to graph machine learning recognizes that these different perspectives are integral to informing innovative model design. Each view\u2014whether topological, spectral, or sequential\u2014reveals unique structural or relational aspects of the graph data, which can guide the development of more expressive and effective directed GNN models. This section will elaborate on the intrinsic characteristics of these perspectives, aiming to advance the graph learning field by fostering models that better capture the nuanced information within graph data.\nGraph Topological View. Viewing a graph as a composition of nodes and edges represents the most intuitive and direct approach to understanding its structure. Unlike Euclidean data, which possesses a regular topology and fixed dimensionality, graph data is irregular and cannot directly leverage traditional neural network architectures, such as Convolutional Neural Networks (CNNs) commonly used in the domain of computer vision. This structural distinction has led to the development of the message-passing framework, specifically designed to facilitate machine learning on irregular, non-Euclidean data. Numerous directed GNNs [28], [29], [48], [49], [53], [80], [81], [92], [112]\u2013[115], [129] utilize the message-passing framework to learn directed graph representations, as outlined in Section III-A. These models conceptualize the graph as a set of nodes connected by directed edges and introduce various types of propagation and aggregation mechanisms within the message-passing framework to enable efficient information exchange across the graph topology.\nGraph Spectral View. Viewing a graph through the lens of a discretized graph spectrum provides a relatively abstract"}, {"title": "B. Graph Data Improvement", "content": "Unlike image or text data, where data points are embedded within regular grid structures, graph-structured data represent"}, {"title": "1) Topological Enhancement", "content": "To enhance the topological structure of graph data, substantial efforts have been directed towards this area of research. As outlined in the data-centric graph learning pipeline illustrated in Fig. 1, the stages of graph construction and pre-processing present valuable opportunities for improvement. For instance, the original graph data can be transformed into alternative graph structures, such as multigraphs, directed acyclic graphs, or random graphs, to better capture node relationships specific to an application domain. Additionally, various topological modification techniques can be applied to refine directed graphs, including structural augmentation methods and diffusion-based approaches, to optimize the graph representation and improve model performance."}, {"title": "Methods for Directed Multigraphs", "content": "In numerous real-world applications, such as financial transactions, social networks, and biological systems, multiple interactions between the same pair of nodes are common. This prevalence has underscored the growing significance of directed multigraphs, which are an extension of traditional directed graphs that permit multiplicity in edges. For instance, GRANDE [80] and GNNDM [81] adopt directed multigraph modeling to represent financial transaction networks more accurately, as this approach captures the multiplicity of relationships (i.e., multiple transactions) between specific nodes. These models also design a tailored message-passing framework that enables the transfer of information across multiple edges between two nodes, thus enhancing their ability to reflect complex relational structures within the data."}, {"title": "Methods for Directed Acyclic Graphs", "content": "Directed acyclic graphs are a fundamental data structure prevalent in various domains, including source code, logical formulas, and architecture of a neural network. They represent computations and processes that have a clear starting point and a well-defined sequence of operations without cycles. With the rise of GNNs, there is a growing interest in extending their capabilities to handle DAGs effectively. The motivation behind developing specialized GNNs for DAGs stems from their unique partial orders defined by their edges, which represents the sequence of operations or dependencies. Traditional model-centric directed GNNs do not inherently respect this order and may not capture the full computational semantics of DAGs. This has led to the development of D-VAE [112], DAGNN [113], PACE [119] and DAGRA [120], which are designed to leverage the structural and computational properties of DAGs for improved learning and reasoning. The former two directed GNNs develop corresponding message-passing scheme that is tailored to the scenarios of DAGs, while the latter two introduce sequence-"}, {"title": "Methods for Directed Random Graphs", "content": "To enhance the performance and robustness of directed GNNs during training, particularly given the scarcity and limited quality of graph data, the application of random graphs as a topological enhancement technique has become essential. Several directed GNNs leverage directed random graphs to strengthen model expressiveness and stability in directed graph learning. For instance, FDGCN [118] utilizes the Mixed Membership Stochastic Block Model (MMSBM) to generate random graphs, which are incorporated during additional training epochs to enrich the training data. Similarly, MagNet [51], DIMPA [49], and DiGT [50] apply the Directed Stochastic Block Model (DSBM). This model takes edge existence and directionality into account, governed by two hyperparameters, a and \u1e9e, to generate directed graphs that align with the model's training requirements. Additionally, Dir-GNN [28] generates random graphs with varied levels of homophily by employing a modified preferential attachment process, supporting diverse structural properties in the directed graph data."}, {"title": "Methods for Directed Augmented Graphs", "content": "After the graph is constructed from the original data, various kinds of graph augmentation can be further applied in the pre-training process to obtain the more appropriate graph topology for the following training process. MotifNet [90] alters the original graph topology by introducing the induced motif matrix to facilitate the graph machine learning on the directed graphs. Inspired by the station distribution of random walk, DGCN [116] reweights the original graph adjacency matrix by P = DotA to ease the over-smoothing problem and achieve directed graph machine learning. Differing from the reweighting technique mentioned above, DGCN [92], DIMPA [49] and D-HYPR [114] propose and encode the k-order proximity into the adjacency matrix, to incorporate features from neighbors of multiple hops and extend the reception field of the model. Drawing the inspiration from personalized PageRank, DiGCN [93] and DiGCL [117] augment the original graph topology by employing the approximate directed Laplacian operator to facilitate more powerful and expressive directed graph learning while lower the computation cost. MagNet [51], Framelet-Magnet [95], HoloNet [96], LightDiC [53] and MMLPE [122] enhance the original graph adjacency matrix through replacing the real-value in the matrix by complex-value and encoding the edge directionality into the phase of the complex element. A2DUG [115] and ADPA [29] adopt a similar graph augmentation technique by introducing multi-hop propagator to alter the original graph adjacency matrix. While A2DUG employ shallow propagators which ignore the directionality of edges, ADPA proposes a more advanced propagators which encode edge directionality into the various Directed Patterns (DPs)."}, {"title": "Methods for Directed Diffused Graphs", "content": "Diffusion-based structural enhancement techniques take inspiration from the graph sequence view illustrated in Section IV-A, which conceptualizes a graph as a sequence of node or subgraph to-"}, {"title": "2) Node Feature Enhancement", "content": "In graph-structured data, node features serve as critical elements that convey the semantic information of each component. For example, in citation networks, node features are often represented as word-bag vectors that capture the content of each paper, providing valuable semantic insights for downstream tasks. However, the quality of these features in real-world datasets is often suboptimal. Also in citation networks, for instance, extracted features may lack significant terms or include extraneous ones, leading to issues of missing or noisy features that can adversely impact the performance of graph-based learning models. To enhance the quality of node features in directed graph data, two kinds of primary data-centric approaches have been actively pursued in recent years: node feature augmentation and node feature denoising. The focus of node feature augmentation lies in supplementing or reconstructing appropriate node features that align well with the training objectives of directed GNNs. In contrast, node feature denoising is primarily concerned with removing extraneous or misleading features that could adversely affect the training performance of directed GNNs. Together, these methods aim to improve feature reliability, facilitating more accurate and robust model performance."}, {"title": "Methods for Directed Augmented Node features", "content": "Node feature augmentation primarily aims to enhance or reconstruct node features to better align with the specific training objectives of directed GNNs. This process seeks to enrich node representations by either supplementing missing feature data or modifying the existing feature vectors that capture relevant information, ultimately supporting improved model performance in directed graph tasks. Recent research has increasingly emphasized the development of more expressive and robust directed GNNs through advanced node feature augmentation techniques. For instance, DIMPA [49], NSTE [48] and DiGT [50] enhance node features by introducing dual encoding, which splits the original node features into distinct components (e.g., source and target) to generate enriched features tailored to directed graph contexts. D-HYPR [114] further advances node feature augmentation by mapping original features from Euclidean space into hyperbolic space, leveraging the complex, non-Euclidean latent anatomy provided by hyperbolic projection to support more effective directed graph"}, {"title": "Methods for Directed Denoised Node features", "content": "Node feature denoising aims to remove erroneous or potentially detrimental features that could impair the training and performance of directed GNNs. This approach seeks to identify and filter out noisy data elements that may obscure essential patterns within the graph, thereby enhancing the model's capacity to learn meaningful representations and bolstering robustness in directed graph learning tasks. Although research in this area remains limited, investigating node feature enhancement through denoising holds significant promise for improving model reliability and interpretability in directed GNNs. Drawing inspiration from the graph spectral view illustrated in Section IV-A, MGC [94] denoises the node features by minimizing the graph signal denoising formula as $\\min_x {\\mu||x - x||_3 + S_p(x)}$, where x = x + n is the noisy graph signal composed of the pure part x and the noisy part n, \u03bc > 0 is the trade-off coefficient and Sp(x) is the Dirichlet energy [133]."}, {"title": "V. INDUSTRIAL APPLICATIONS OF DIRECTED GNNS", "content": "Directed graph representation learning is crucial in the scenarios of widespread industrial applications. Drawing on a thorough review of existing literatures in graph representation learning, we classify these real-world applications by identifying the specific tasks associated with directed graph embedding, with a particular emphasis on nodes, directed edges, and the overall graph structure. It is important to note that each study is associated solely with the tasks explicitly described in the respective articles."}, {"title": "A. Node-level Applications", "content": "In this section, we elucidate the role of directed graph representation learning in the context of industrial applications defined on the node-level of a directed graph, such as classification, regression, and recommendation. For each task, we additionally expound on the specific application domains in which it has been employed."}, {"title": "Node Classification", "content": "The primary objective of the node classification task is to accurately assign labels to the unlabeled nodes within a graph by leveraging the label information provided for a subset of labeled nodes. This paper presents two representative applications: phishing detection within the Ethereum network (PEAE-GNN [30], GAE_PDNA [31], GrabPhisher [32], TSGN [33], ATGraph [34]) and conversational emotion recognition (FER-DGNN [54], DialogueGCN [55], DAG-ERC [56], GraphCFC [57], COGMEN [58]). In the context of phishing account detection within the Ethereum network, Ethereum is represented as a directed graph, where nodes correspond to individual accounts, edges signify transactions, and the direction of the edges reflects the flow of transactions. Graph neural networks classify the nodes as either normal accounts or phishing accounts by leveraging the embeddings derived from the nodes. To facilitate the recognition of emotions within individual utterances of a conversation, the entire conversational exchange is configured as a directed graph. In this graph-based model, each"}, {"title": "Node Regression", "content": "The task of node regression is to predict a continuous numerical value associated with each node within a network. This process leverages node-specific features, structural information, or a combination of both to develop a predictive model. Our investigation reveals that node regression on directed graphs holds significant potential for applications in traffic prediction, particularly in traffic flows prediction (T-DGCN [13], Traffic-GCNN [14], STGNN [15], SeqGNN [16], DGNNs [17] ) and pedestrian trajectory forecasting (TFDGCNN [59], TDAGCN [60], DBSTGNN [61], MDST-DGCN [62], SGCN [63]). In traffic flow prediction, the problem is commonly formulated as a sequence of directed graphs, with each graph representing the traffic conditions at a specific time interval. Within these graphs, nodes correspond to individual road segments, while edges capture the connectivity between segments. The directionality of the edges signifies the flow of traffic across the network. Graph neural networks predict the probability of congestion for each segment by utilizing the node embedding representations associated with those segments. In contrast to traffic flow prediction, pedestrian trajectory prediction typically represents pedestrians as nodes and models the distances between pedestrians as edges. For each pedestrian, the graph neural network predicts the most probable position at a specified time by utilizing its embedding representation."}, {"title": "Node Recommendation", "content": "The goal of node recommendation is to ascertain and endorse the top-k nodes that demonstrate a significant likelihood of establishing a link with a designated query node within a directed graph. In recommendation system, directed graphs serve as a potent instrument to structurally encode the complex, high-order relationships that extend beyond the traditional user-item interactions. For example, directed graphs are capable of explicitly modeling additional dimensions of user-item interactions, such as product asymmetry (DAEMON [6], MERLIN [7], BLADE [8]) Furthermore, they can represent supplementary interaction data, such as the co-occurrence information between items (UGRec [5])."}, {"title": "B. Edge-level Applications", "content": "In this section, we will explore the role of directed graph representation learning in relation to edge-level tasks such as link prediction and edge classification, as well as its relevant application domains."}, {"title": "Link Prediction", "content": "Link prediction in directed graphs involves not only determining the presence of an interaction relationship between pairs of entities based on their attributes and currently observed links but also necessitates forecasting the directed aspect of these interactions. Our review indicates that link prediction techniques utilizing directed graphs have"}, {"title": "Edge Classification", "content": "The goal of the edge classification task is to accurately assign labels to the unlabeled edges by leveraging the information derived from a subset of labeled edges, in conjunction with the graph's topological structure. In the domain of financial network analysis, edge classification tasks utilizing directed graphs are extensively applied (GRANDE [80]) In the case of a transaction network with users as nodes and transactions as edges, the direction of an edge is typically understood as the direction of its corresponding cash flow. Graph neural networks categorize the edges within the transaction network as suspicious or normal transactions by analyzing the embedded information derived from the processed nodes and edges."}, {"title": "C. Graph-level Applications", "content": "In the ensuing discourse, we will delve into the application of directed graph representation learning in the graph-level tasks, including graph classification, graph regression, and graph matching. Furthermore, we will conduct a detailed examination of the industrial application scenarios pertinent to each task type."}, {"title": "Graph Classification", "content": "The objective of the graph classification task is to assign labels to entire graphs by analyzing both their structural and feature information. In our investigation, numerous studies focus on the application domains of malware detection (SPNGNN [69], GNN-MDF [70], NF-GNN [71], NT-GNN [72], SDGNet [73]), social network analysis (Bi-GCN [74], OID-GCN [76], EBGCN [77], RoSGAS [78]), and brain network analysis (DBNA [35], CGCNN [36], Hodge-GNN [37], TBDS [38], DSL-GNN [39]). In the domain of malware detection, directed graphs are commonly constructed from control flow graphs, where each node represents a basic block of instructions, and the edges signify interactions between these blocks. By employing graph neural networks for representation learning of the entire graph, malware detection can be conceptualized as a binary classification task applied to the graph. In the field of social network analysis, potential rumors are represented as graphs, wherein nodes correspond to individual posts associated with the rumor, and edges signify interaction relationships between these posts, such as responses or retweets. By leveraging graph neural networks, the entire rumor graph is analyzed to assess the veracity of the information. In brain network analysis, the nodes and edges of the directed graph are constructed based on the power spectrum density (PSD) and effective brain connectivity (EBC), which are extracted from electroencephalography (EEG) data. Subsequent processing by graph neural networks enables the tasks of emotion recognition and disease detection"}, {"title": "Graph Regression", "content": "Graph regression tasks involve the prediction of continuous numerical targets that are associated with the entirety of a graph. This is achieved by leveraging the node features, edge features, and the overarching topological structure of the graph. Our investigation reveals that the applications of graph regression techniques significantly enhance the performances of models in the area of chip design (GCPRMM [84"}]}