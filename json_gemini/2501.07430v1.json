{"title": "Diff-Ensembler: Learning to Ensemble 2D Diffusion Models for Volume-to-Volume Medical Image Translation", "authors": ["Xiyue Zhu", "Dou Hoon Kwark", "Ruike Zhu", "Kaiwen Hong", "Yiqi Tao", "Shirui Luo", "Yudu Li", "Zhi-Pei Liang", "Volodymyr Kindratenko"], "abstract": "Despite success in volume-to-volume translations in medical images, most existing models struggle to effectively capture the inherent volumetric distribution using 3D representations. The current state-of-the-art approach combines multiple 2D-based networks through weighted averaging, thereby neglecting the 3D spatial structures. Directly training 3D models in medical imaging presents significant challenges due to high computational demands and the need for large-scale datasets. To address these challenges, we introduce Diff-Ensembler, a novel hybrid 2D-3D model for efficient and effective volumetric translations by ensembling perpendicularly trained 2D diffusion models with a 3D network in each diffusion step. Moreover, our model can naturally be used to ensemble diffusion models conditioned on different modalities, allowing flexible and accurate fusion of input conditions. Extensive experiments demonstrate that Diff-Ensembler attains superior accuracy and volumetric realism in 3D medical image super-resolution and modality translation. We further demonstrate the strength of our model's volumetric realism using tumor segmentation as a downstream task.", "sections": [{"title": "1. Introduction", "content": "Volume-to-volume image translation is highly useful for volumetric medical imaging, such as magnetic resonance imaging (MRI) and X-ray computed tomography (CT). It solves inverse problems of image reconstruction [15, 31, 49], handling sparse [24, 36], limited [8, 35], and/or noisy [8, 52] imaging data. It also supports image synthesis, where volume translation can convert images from one imaging modality to another, such as multi-contrast MRI [9, 42, 51, 53], CT-ultrasound [48], and MR-histopathology [28]. These models enable numerous downstream image processing tasks, ranging from image reconstruction [37] to analysis [1, 12]). Moreover, as suggested in previous work[11], generating 3D realistic volume in such tasks is important since most models in downstream analysis tasks, such as tumor segmentation[16], are trained on 3D volumes.\nRecent advances in volume-to-volume translation have introduced methods that combine perpendicular 2D diffusion models [4, 27], achieving improved accuracy and volumetric consistency. However, these methods cannot model the distribution of the entire volume since the generated images are produced by an averaging of the 2D networks without 3D representations, resulting in limited realism in 3D. Although 3D networks are more adept at capturing volumetric structures, previous works [11, 27] highlight substantial challenges in their increased demands for computational resources and large datasets that are costly to acquire in medical imaging. To the best of our knowledge, within the domain of 3D medical image inverse problems, no fully 3D models have demonstrated superior accuracy over 2D-based models due to these practical limitations. Designing a 3D network of similar size with 2D models is suggested by [34], which is a promising approach given the rich 3D context and strong pre-trained 2D models. However, this is generally infeasible with existing 2D diffusion models[41], which require around 300 GB of GPU memory for training with extremely long training time. As a result, current 3D diffusion models [11] are designed to be much smaller with insufficient capacity to demonstrate competitive performance in inverse problems.\nTo effectively introduce 3D representations into volumetric translation, we present Diff-Ensembler, a pioneering model for volumetric translation that directly and effectively captures the distribution of 3D volumes. Diff-Ensembler adopts a two-stage training strategy: (1) It first trains multiple 2D diffusion models [40] on perpendicular planes. (2) It then utilizes a 3D fusion network to produce the final translation in each diffusion step. Meanwhile, using the alignment layers, the 3D ensemble network effectively uses the hierarchical feature maps from the 2D model. This mixture-of-experts (MoE) approach capitalizes on the strengths of both 2D and 3D networks, achieving highly accurate and spatially realistic 3D outputs. Moreover, Diff-Ensembler is able to decrease the capacity requirement for an accurate 3D diffusion model by introducing pre-trained 2D diffusion models in multiple slice directions, addressing the computational challenge of 3D diffusion models. Additionally, by ensembling diffusion models conditioned on various input modalities, Diff-Ensembler seamlessly supports multi-modality fusion.\nThe mathematical intuition of Diff-ensembler lies in the properties of diffusion models and their associated score functions[44]. As the score function models the gradient of the probability distribution, it is inherently suitable for an iterative ensemble. Previous works [4, 6] also have demonstrated this by showing strong performance with a straightforward weighted averaging of score functions. Consequently, Diff-Ensembler replaces the weighted averaging process with a 3D network, enhancing robustness while addressing the computational demands of 3D models. To the best of our knowledge, Diff-ensembler is the first work that uses a mixture-of-experts(MoE) model in the score function space, which provides new insights for diffusion models ensembling. Diff-ensembler can also function as a plug-and-play mechanism compatible with various combinations of 2D models from previous studies [4, 6, 30], consistently delivering performance improvements across various 2D backbones.\nThe efficiency and effectiveness of Diff-ensembler have been evaluated in various MRI image processing tasks on the BraTS [2] and HCP [10] dataset, including image super-resolution and modality translation. Our experimental results demonstrate that Diff-Ensembler performs superior volume translation over current state-of-the-art (SoTA) models. By learning to ensemble perpendicular 2D models conditioned on different input modalities, Diff-Ensembler shows strong performance without retraining new 2D models."}, {"title": "2. Related Work", "content": "3D medical image generation and translation. Attempts have been made to generate dense 3D volumes for medical imaging. Direct 3D-based diffusion models [11, 34] face difficulties due to high computational and dataset demands, limiting them to smaller models that result in moderate accuracy in tasks like super-resolution. Patch-wise, slicewise, or cascaded generation strategies have been utilized to accommodate high-dimensional data within the constraints of limited GPU memory [47]. This cascade approach introduces several drawbacks. The multi-stage nature of the process can lead to compounded errors, where initial inaccuracies in the low-resolution base are propagated and amplified during the refinement stages. Additionally, the patch-based refinement often struggles with maintaining global consistency across the image, resulting in visible seams or inconsistencies in the final output. Latent 3D models [11, 25, 56] have been exploited to reduce the degrees of freedom of 3D volume representation by compressing the entire 3D data into a low-dimensional latent space. Subsequently, a generative model is constructed within this compressed latent space. Also, the process of reducing dimensionality can lead to substantial reconstruction errors, compromising the accuracy and fidelity of the generated images. Sequential slice generation from Auto-regressive models [38, 57] or simultaneous multiple-slice generation may mitigate this issue of error accumulation over slices. Yet, these approaches face difficulties in sustaining coherence for long-range structures.\nMore related to our approach, integrating multiple 2D models trained along perpendicular directions is a promising approach. TPDM [27] first proposes to combine two perpendicular 2D diffusion models to improve 3D imaging, where the weighted average of scores from pre-trained 2D models estimates the score function of a 3D model. Building on this concept, following up works, such as TOSM [30] and MADM [4], further improve the model performance by including 2D models in all three directions and using multiple consecutive 2D slices in 2D models. These models generate highly accurate results by effectively leveraging the high-resolution information in each 2D plane. Directly inspired by these works, we use a 3D model to ensemble multiple perpendicularly trained 2D models instead of the weighted average. This results in a more accurate and realistic 3D generation since the 3D ensembler network can introduce 3D features and directly model the 3D distribution.\nModel ensembling. Ensemble techniques, which include key methodologies such as bagging, boosting, and stacking, have been developed further through specialized algorithms like Random Forest, AdaBoost, XGBoost, and Mixture of"}, {"title": "3. Diffusion Ensembler in 3D", "content": "Problem formulation. We formulate the volume-to-volume translation task as conditional sampling, aiming to generate the target medical image volumes $y \\in \\mathbb{R}^{b_1,b_2,b_3}$ from given volumes $x \\in \\mathbb{R}^{c,b_1,b_2,b_3}$ where $b_1$, $b_2$ and $b_3$ denote the size along 3 spatial dimensions, and $c$ is the number of given volumes. In addition, we aim to effectively model the 3D volumetric distribution directly with 3D representations. The input, \u00e6, could contain a low-resolution volume and/or a volume of another modality."}, {"title": "3.1. Overall Framework of Diff-Ensembler", "content": "We designed the Diff-Ensembler as a conditional diffusion model. Following DDPM and Palette [18, 41], our model gradually adds Gaussian noise to the target image in the training dataset during the forward or diffusion process as follows:\n$q (Y_t|Y_{t-1}) = \\mathcal{N} \\left(Y_{t}; \\sqrt{1 - \\beta_t} Y_{t-1}, \\beta_t \\epsilon I\\right),$\n$q(Y_T|Y_0) = q(Y_0) \\prod_{t=1}^T q(Y_t|Y_{t-1})$\nwhere $y_0 \\sim q(y)$ is the target image and $\\beta_t$ is the variance of noise added at timestep $t$. The forward process produces a sequence of increasingly noisy variables $Y_1, ..., Y_T$, after sufficient noising steps, the process reaches a pure Gaussian noise, i.e., $y_T \\sim \\mathcal{N}(0, I)$.\nDuring training, our denoising diffusion model, $\\epsilon_\\theta(y_t, x,t)$, is trained to predict the noise added into $y$, given $y_t$. Demonstrated effectively in works [40], the sampling process can be guided by concatenating the noisy image $y_t$ with condition $x$. The conditioning denoising process is then to optimize $\\epsilon_\\theta(y_t, x, t)$ by:\n$L_t = ||\\epsilon(\\sqrt{\\alpha_t}y_0 + \\sqrt{1 - \\alpha_t}\\epsilon, x, t) - \\epsilon||^2$\nwhere $\\alpha_t := \\prod_{t=1}^T (1 - \\beta_t)$, and we sample $y_0, x \\sim p(y_0, x), \\epsilon \\sim \\mathcal{N}(0, I)$.\nDuring sampling in the reverse or generative process, we also follow Palette [41] to generate images by iteratively removing the added noise in the sequence $Y_{T-1}, \u2026, Y_1, Y_0$, from a standard Gaussian prior $y_T \\sim \\mathcal{N}(0,I)$. In addition, inspired by [7, 43, 45], we explore self-consistency for solving inverse problems. More specifically, in each diffusion sampling step, we estimate noise with our denoising"}, {"title": "3.2. 2D Diffusion Models", "content": "The two 2D diffusion models, $\\epsilon_{\\theta_a}^{2D(a)}$ and $\\epsilon_{\\theta_b}^{2D(b)}$, are trained on two perpendicular slices of the volumes using a standard conditional diffusion framework [40]. We take gradient descent steps on the following objectives for both 2D diffusion models during training:\n$\\nabla_{\\theta_a} ||\\epsilon_{\\theta_a}^{2D(a)} \\left(Y_t[:, i, :], x[:, i, :], t\\right) - \\epsilon||^2$\n$\\nabla_{\\theta_b} ||\\epsilon_{\\theta_b}^{2D(b)} \\left(y_t[:, :, j], x[:, :, j], t\\right) - \\epsilon||^2$\nHere, $i$ and $j$ are the indices for the slices along two perpendicular planes, which are sampled uniformly: $i \\sim Uniform\\{0, ..., b_2\\}$, $j \\sim Uniform\\{0, ..., b_3\\}$. After proper training, the high-capacity 2D model can provide a decently accurate estimation of $\\epsilon$ for every volume slice."}, {"title": "3.3. 3D Ensembling Model", "content": "The 3D ensembling model, $\\epsilon_{\\theta_{3D}}^{3D}$, is trained to fuse the pre-trained 2D diffusion models to capture the desired volumetric image distributions. In this stage, we first obtain the inference results, $\\hat{Y}^{2D(a)}$ and $\\hat{Y}^{2D(b)}$, from the 2D diffusion models, $\\epsilon_{\\theta_a}^{2D(a)}$ and $\\epsilon_{\\theta_b}^{2D(b)}$, by iterating through the sliced directions:\n$\\hat{Y}^{2D(a)} [:, i, :] = \\epsilon_{\\theta_a}^{2D(a)} \\left(y_t[:, i, :], x[:, i, :], t\\right) \\text{for } i \\in [0, b_2)$\n$\\hat{Y}^{2D(b)} [:, :, j] = \\epsilon_{\\theta_b}^{2D(b)} \\left(y_t[:, :, j], x[:, :, j], t\\right) \\text{for } j \\in [0, b_3)$\nDuring training of the 3D diffusion model, both 2D models return $Y$'s, which contains the predicted noise and a hierarchical feature map of the model: $\\hat{Y}^{2D(a)} = (\\epsilon_{\\theta_a}^{2D(a)}, F_{\\theta_a}^{2D(a)})$, $\\hat{Y}^{2D(b)} = (\\epsilon_{\\theta_b}^{2D(b)}, F_{\\theta_b}^{2D(b)})$.\nThe 3D model is designed to effectively ensemble the outputs of multiple 2D models through a Mixture-of-Experts framework. Specifically, at each diffusion step, the 3D model takes as input the original image $x$, the noisy intermediate state $y_t$, and the noise estimation $\\hat{\\epsilon}$ obtained from the 2D diffusion models. This approach leverages the complementary strengths of 2D and 3D networks, resulting in highly accurate and spatially consistent 3D reconstructions. Furthermore, feature maps $F$ from the 2D models are incorporated as supplementary information to enhance the ensembling process. (see Fig. 2). These feature maps capture rich, multi-scale representations from the 2D diffusion processes, serving as auxiliary evidence that mitigates the risk of information bottlenecks between the 2D and 3D stages. Such bottlenecks could otherwise limit the effectiveness of the ensemble and lead to sub-optimal performance. Thus, the 3D network is trained to perform the ensembling process using the following formulation using an L2 loss:\n$||\\epsilon_{\\theta_{3D}}^{3D} (Y_t, x, \\hat{Y}^{2D(a)}, \\hat{Y}^{2D(b)}, t) - \\epsilon||^2$\nAs in Eq. 2, we sample $y_0, x \\sim p(y_0, x)$, and $\\epsilon \\sim \\mathcal{N}(0, I)$. Although, the inference results from 2D models, $\\hat{Y}^{2D(a)}$ and $\\hat{Y}^{2D(b)}$, already help the training of the 3D model, 3D ensemble model still needs to be trained from scratch. To improve training speed, we initially pre-train the model on 3D patches, $(y_0, x) = crop(y_0, x)$, and then fine-tune it on the full volumes. Due to the translation invariance of our convolution-based networks, we empirically find that a naively pre-train on the patches results in a decently good network initialization, thereby effectively improving the training convergence. While existing works, such as [50], could potentially enhance this patch-wise diffusion training process, we leave such optimizations for future work.\nIn this work, the network architecture of the 3D model, $\\epsilon_{\\theta_{3D}}^{3D}$, is designed following the 3D palette [11]. Specifically, we adopt a Unet-like denoising model with time-step embeddings and concatenate the 3D input $x$, the noisy target $y_t$, and the noise estimated by the 2D models $\\epsilon_{\\theta_a}^{2D(a)}$, $\\epsilon_{\\theta_b}^{2D(b)}$ as the input of the 3D model. In the encoder, each down-sampling block is enriched with corresponding feature maps from the features of both 2D models, $F_{\\theta_a}^{2D(a)}$, and $F_{\\theta_b}^{2D(b)}$. Inspired by [33], we feed these two features into two 1-layer convolution networks separately. These convolution layers map the two feature maps into the correct shapes required in the 3D model, while also aligning the 2D feature maps with the 3D feature maps through projection. The aligned feature maps are then added to the features in the 3D model. Additionally, rather than directly outputting the predicted noise, our 3D U-Net-like model produces two components:"}, {"title": "3.4. Multi-modality Fusion", "content": "In volumetric translation for medical imaging, the conditions for translating a new image can be multifaceted. For instance, DDMM-Synth [29] suggested using both MRI and low-resolution CT scans to produce high-resolution CT images. Training a separate model for each possible combination of input conditions would result in exponential time complexity, making it generally impractical. Therefore, a model that can integrate pre-trained models across diverse conditions provides significant advantages. Diff-Ensembler addresses this challenge by naturally integrating multiple diffusion models, each conditioned on individual modalities, through a 3D network architecture that functions similarly to fusing two 2D models described in 3.3. This approach leverages the efficiency of patch-wise pre-training for accelerated 3D training. To further enhance the speed of multi-modality fusion, we employ a smaller variant of our model, adjusting the number of channels in each layer. In this training, we do not leverage the feature maps $F$ from the 2D models, further optimizing computational efficiency."}, {"title": "4. Experiments", "content": "Datasets. We conducted experiments using the BraTS 2021 training dataset [2], which includes 1,251 volumetric brain scans with tumors across 4 modalities: FLAIR, T1, T1ce, and T2. Since ground-truth annotations are not publicly available for the validation and test datasets, we randomly divided the training dataset into a 0.8:0.2 split for training and evaluation purposes, allowing its use for downstream tasks as well. Each scan was center-cropped to a dimension of 192x192x152 to remove the blank background."}, {"title": "4.5. Ablation Studies", "content": "We provide an ablation study in Tab. 4 on the key design elements for Diff-Ensembler, which includes: (1) Feature merging: The 2D models not only contribute their outputs but also pass their feature maps to the 3D model. (2) Finetune: We initially pre-train the model on 3D patches and then fine-tune it on the full volume to speed up training. (3) Consistency: Inspired by DPS [7] and score-SDE [45], we implement self-consistency projections at each denoising step. All these designs show performance gain in the super-resolution task. In addition, we benchmark the smaller variant in Sec. 4.4 for comparison, which shows a moderate performance drop compared to our best model."}, {"title": "5. Conclusion", "content": "In this work, we have introduced Diff-Ensembler, an innovative hybrid model that integrates the strengths of both 2D and 3D diffusion models. By training multiple 2D diffusion models on orthogonal planes and fusing their outputs through a 3D ensemble model, Diff-Ensembler successfully overcomes the inherent limitations of each approach. Diff-Ensembler provides strong insights for diffusion model ensembling as the first work to adopt an MoE model in the score function space. Empirical evaluations on various 3D MRI image translation tasks, including super-resolution and modality translation, have shown that Diff-Ensembler achieves unmatched accuracy, realism, and volumetric consistency. In addition to computational and memory efficiency, the approach offers considerable flexibility in merging models conditioned on different domains.\nLimitations Unlike some other multi-stage models [19, 58], Diff-Ensembler struggles with joint end-to-end training due to the substantial computational demands of simultaneously managing high-capacity 2D models and the volumetric complexities of 3D tasks. In addition, the model's dependency on patchwise pre-training for efficient 3D model learning presents limitations for tasks requiring the integration of long-range spatial information, such as large-area inpainting and compressed sensing MRI. Therefore, Diff-Ensmebler may require longer training for such tasks."}, {"title": "7. Overview", "content": "In this supplementary material, we first discuss the uncertainty awareness results performed by our model and baselines by running the inference multiple times in Sec. 8. We provide more randomly selected results (We do exclude samples with low-quality GT) for more baselines and our variants in Sec. 9. Then, we provide our super-resolution result in an additional dataset, HCP dataset [10] in Sec. 10. We also provide more details on training and inference, including a detailed model architecture in Sec. 11 and training/inference speed in Sec. 12. We finally introduce a more detailed method for self-consistency projection in Sec. 13 and downstream task results in Sec. 14."}, {"title": "8. Uncertainty Awareness", "content": "As with most diffusion-based models, our models and some of our baselines can have uncertainty estimations. To study this uncertainty, we perform inference five times for each sample in our validation set. This gives us 5 PSNR and SSIM values for each data sample. We then calculate the standard deviation (std) of the PSNR and SSIM for each sample and include the mean std across the entire validation set in Tab. 5. This further validates that our performance boost in PSNR and SSIM is significant. For the main variant, TPDM and Ours-TPDM, in the super-resolution task, we have a 1.01 boost in PSNR, which is much larger than the std of PSNR for both models (0.0066 and 0.0298). Even for MADM and Ours-MADM, where we have the most marginal PSNR boost, the boost is still 0.3, around 10 times larger than the std for both models (0.0308 and 0.0276). In contrast, in modality translation, the std is significantly larger since the uncertainty in this task is much larger than in others, indicating the PSNR drop is not as significant. In fact, previous work [40] argues that PSNR prefers blurry results, and highly diverse and realistic results typically have low PSNR in tasks with high uncertainty.\nIn addition, this inference also provides a mean \u03bc\u03b5 and std estimation \u03c3\u03b5 for each voxel. We use Mean Absolute Calibration Error (MACE) [26] to measure the uncertainty awareness of our model and baseline. MACE measures the absolute difference between the predicted uncertainty and the actual error, as shown in Eq. 7.\n$\\text{MACE} = \\frac{1}{N} \\sum_{i=1}^{N} |\\sigma_i - |y_i - \\mu_i||$ As demonstrated in Table 5, all variants of our model exhibit lower MACE values compared to their respective baselines. This indicates that the standard deviation (std) predicted by our model, derived from multiple inferences, provides a more accurate estimation of the true error relative to the ground truth. Consequently, our model exhibits improved uncertainty awareness. For qualitative results in uncertainty awareness, we demonstrate our model's results with the uncertainty map and error map across various tasks and variants in Fig. 14 15 16 17 18 19 20 21. As shown in the figures, the uncertainty map aligns well with the actual error map, demonstrating decent uncertainty awareness for all models. Notably, our model usually has a higher uncertainty in modality translation tasks in Fig. 16 and 17. In the modality translation task, the p(y|x) should have a high variance in overall contrast. Our model outputs samples that are highly diverse in overall contrast, indicating that Diff-Ensembler is able to model the target 3D conditional distribution p(yx) better. In contrast, our baselines tend to output the mean estimation for overall contrast, demonstrating higher PSNR but limited capability of generating diverse and realistic results."}, {"title": "9. Additional qualitative result", "content": "We show results for the variants that show the best metrics. Namely, we show results for MADM and Ours-MADM in the super-resolution task in Fig. 14 and 15, and show results for TOSM and Ours-TOSM for the other two tasks in Fig. 18 19 16 and 17. We include more samples in all 3 views in Fig. 20 and Fig. 21 in super-resolution tasks in addition to Fig. 3. Each figure contains 2 sample volumes, each of which contains visualizations in all three views in three rows. We show all results with uncertainty and error maps.\nSimilarly to Fig. 3, we find that both MADM and TOSM demonstrate similar artifacts as TPDM in high-frequency details due to a direct averaging in the score function. In contrast, Diff-Ensmebler consistently demonstrates better 3D consistency and realism across all views by introducing pixel-space 3D representation and networks to replace the weighted averaging in the score function space. For example, in the 6-th row of Fig. 14 16, and 18, the results from baselines are blurry at the top left part of the brain, whereas Diff-Ensembler shows more smooth and consistent results."}, {"title": "10. Result on HCP dataset", "content": "We present our super-resolution results on the FLAIR modality in the HCP [10] dataset to show our model is generalizable across datasets. The HCP dataset consists of 1251 MRI volumes with a resolution of 192x152x152. In contrast to the BraTs dataset, HCP comprises healthy brains with no brain tumors. Experiments results in Tab. 6 show that our model shows around 1.5 performance boost in PSNR. We also present the qualitative results in Fig. 4, including all 3 views. Again, Diff-Ensembler shows better 3D consistency and realism. For example, in the top-right part in the third view, our baseline demonstrates jittering and artifacts, while our model produces more realistic detail and smoother edges."}, {"title": "11. Detailed model architecture", "content": "In this section, we show detailed model architecture for 2D, 3D, and the smaller variant of the 3D model in Tab. 9, Tab. 10, and Tab. 11, respectively. In addition, we show other related hyper-parameters in Tab. 7. We modified the architecture of the 2D diffusion model from Palette [41] and the 3D models from med-ddpm [11].\nGiven the differences in problem setting and dataset between our work and that of Palette, we conduct a comprehensive hyper-parameter search based on the super-resolution tasks. This search explores various configurations, including the number of channels, transformer layers, and learning rate, among others. The hyper-parameter search is conducted to optimize the performance of our baseline models, Palette2D, Palette3D, and Palette2.5D, in Tab. 1. While such a search could potentially enhance the performance of our proposed model, we do not perform a hyper-parameter search to optimize the performance of Diff-Ensembler, TPDM, TOSM, and MADM. This practice ensures a fair comparison between our model and their corresponding baselines, TPDM, TOSM, and MADM. Moreover, this shows that our model can be a plug-in-and-play mechanism for existing pre-trained 2D and 3D model architecture."}, {"title": "12. Training and inference speed", "content": "We present training inference speed in Tab. 8. All experiments are done with RTX A100-40GB GPU. Since Diff-Ensembler needs to train an additional model on top of the baselines, our training time is inevitably higher. We need 16 GPU days to train our 3D models, which results in a 16-day increase in training time for most model variants compared to their corresponding baselines. Our models are also relatively slower in inference since we need to perform inference for an additional 3D model. However, as mentioned in Sec. 1, the 3D model is naturally limited in size due to computational challenges in training. Therefore, 3D inference is more efficient than slice-wise 2D inference. As a result, the increase in inference time is significantly smaller than in training. As shown in Tab. 8, our 3D model is around 30% faster than one 2D model and, therefore, leads to a 36% increase in inference time for Ours-TPDM and 26% for Ours-MADM and Ours-MADM.\nMoreover, we find that the TPDM-based models are significantly faster than other variants of the models. Given the advantage of computational efficiency, we use TPDM and Ours-TPDM as our main variables for the model and the baseline. Furthermore, to perform a more complete ablation study, the smaller 3D model decreases the inference and training time of the 3D model by 75% while showing a consistent performance boost over TPDM and a moderate performance drop compared to Ours-TPDM as shown in Tab. 1."}, {"title": "13. Details for consistency projection", "content": "In this section, we provide the exact definition and detail for self-consistency projection mentioned in Sec. 3.1. In this work, we address the inverse problem using a diffusion model with consistency projections. The goal is to recover a high-resolution image, y, from its low-resolution observation x, which is obtained through a linear degradation process. Specifically, the degradation process is modeled as: x = Ay.\nIn the 3D case, the degradation operator A represents a downsampling operation that reduces the resolution of a volume y by a factor of 4 along each spatial dimension (x, y and z) and resizes it back to the original resolution. This means that each voxel in the low-resolution volume x corresponds to the average of a [4\u00d74\u00d74] region in the high-resolution volume y. Specifically, let y, x \u2208 Rb1\u00d7b2\u00d7b3. The operator matrix A \u2208 Rb1\u00d7b2\u00d7b3,b1\u00d7b2\u00d7b3 downscales the high-resolution volume y into the low-resolution volume x by averaging over [4x4x4] blocks of voxel of y. Therefore, A is a sparse matrix where each non-zero entry corresponds to the average of a block of [4x4x4] voxels in y being averaged to form a block of voxel in \u00e6. Therefore, A is for the places where x and y belong to the same block, and A would be 0 elsewhere:\n$A[(i, j, k), (p, q, r)] = \\begin{cases}\\frac{1}{64} & \\text{if } x(i, j, k), y(p,q,r) \\in \\text{block}\\\\0 & \\text{Otherwise}\\end{cases}$ Since we are doing average over [4x4x4], x(i,j,k) and y(p, q, r) are in the same block if and only if i//4 == p//4, j//4 == q//4, and k//4 ==r//4.\nIn our diffusion process, we use \u0177o(t) \u2190 \u0177o(t) \u2013 AT(AAT)\u00af\u00b9(Ayo(t) \u2013 x) to make every of our mean prediction of yo a plausible estimation with x = Ayo(t)\nTo compute matrix multiplication more efficiently in a super-resolution setting, we actually use \u0177o(t) \u2190 \u0177o(t) \u2013 (Ayo(t) \u2013 x) in our code. This works in the average pooling downsample because AAy = Ay since A represents the degradation process composed of average pooling followed by resizing the image back to its original resolution."}, {"title": "14. Details in downstream task", "content": "In Section 4.3", "inputs": "the ground truth FLAIR modality", "16": "which takes four modalities (T1", "metrics": 1, "as": "nRecovery Rate = (Prediction - Downsample) / (Ground Truth - Downsample) where Prediction refers to the segmentation performance using predicted FLAIR, Downsample is the performance with downsampled FLAIR, and Ground Truth is the performance with ground truth FLAIR.\nFig. 5, 6, 7 illustrate the Dice score and Recovery rate comparisons across tumor categories. Dashed lines represent the lower and upper bounds. They show that segmentation performance with the predicted FLAIR modality from Diff-Ensembler-based models outperforms other methods, as Diff-Ensembler-based models are constantly positioned higher than others.\nWe also show qualitative results in the tumor segmentation task, on TPDM, TOSM, and Diff-Ensembler built based on these two models. Fig 8 and Fig 9 show the results in super-resolution, Fig 10 and Fig 11 show the results in modality translation, and Fig 12 and Fig 13 show the results given both conditions. In Fig 8"}]}