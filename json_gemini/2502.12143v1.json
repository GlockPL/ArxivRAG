{"title": "Small Models Struggle to Learn from Strong Reasoners", "authors": ["Yuetai Li", "Xiang Yue", "Zhangchen Xu", "Fengqing Jiang", "Luyao Niu", "Bill Yuchen Lin", "Bhaskar Ramasubramanian", "Radha Poovendran"], "abstract": "Large language models (LLMs) excel in complex reasoning tasks, and distilling their reasoning capabilities into smaller models has shown promise. However, we uncover an interesting phenomenon, which we term the Small Model Learnability Gap: small models (\u22643B parameters) do not consistently benefit from long chain-of-thought (CoT) reasoning or distillation from larger models. Instead, they perform better when fine-tuned on shorter, simpler reasoning chains that better align with their intrinsic learning capacity. To address this, we propose Mix Distillation, a simple yet effective strategy that balances reasoning complexity by combining long and short CoT examples or reasoning from both larger and smaller models. Our experiments demonstrate that Mix Distillation significantly improves small model reasoning performance compared to training on either data alone. These findings highlight the limitations of direct strong model distillation and underscore the importance of adapting reasoning complexity for effective reasoning capability transfer.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) (Anthropic, 2023; Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023a) have demonstrated remarkable performance in complex reasoning tasks, enabling advancements in mathematical problem-solving, logical inference, and structured decision-making (Cobbe et al., 2021; Shao et al., 2024; Yang et al., 2024). A key advancement in improving LLM complex reasoning capability is the chain-of-thought (CoT) prompting. This technique decomposes complex problems into intermediate reasoning steps, enhancing both performance and interpretability. (Wei et al., 2023). However, the high computational cost of LLMS hinders their deployment on resource-constrained devices, motivating the development of smaller models that offer similar capabilities at reduced cost."}, {"title": "Preliminaries", "content": null}, {"title": "2.1 Notation", "content": "Let $x = (X_1,X_2,...,X_n)$ represent an input sequence (e.g., a prompt), and $y = (Y_1, Y_2, \u2026\u2026\u2026, Y_m)$ be the corresponding output sequence. We consider a LLM parameterized by $\\theta$, which predicts the next token following a conditional distribution $\\pi_{\\theta}(Y_t | X, Y_{1:t-1})$. We denote by $CoT(y) \\subseteq y$ the subset of tokens in the generated output that encodes a chain-of-thought, often serving as a reasoning trace or explanatory sequence.\nThroughout this work, we use the term short CoT, to describe concise reasoning paths to arrive at solutions (Min et al., 2024; Yeo et al., 2025) and long CoT to describe an extended reasoning sequence that is not only longer but also demonstrates more complex reflective thoughts (Qwen, 2024b; Yeo et al., 2025). Additionally, we use the term large teacher CoT to refer to the reasoning trace generated by a larger teacher model, and the term small teacher CoT for the reasoning steps produced by a smaller teacher model. Please see Appendix D for more examples."}, {"title": "2.2 Supervised Fine-Tuning (SFT)", "content": "Supervised fine-tuning (SFT) is widely adopted to enhance reasoning capabilities of LLMs on a dataset $D = \\{(x^i, y^i)\\}_{i=1}^N$, where $y^i$ can be short CoT, long CoT, strong model CoT or weak model CoT sequences. The SFT process updates the parameters $\\theta$ of a language model by minimization the negative log-likelihood loss over the instruction dataset $D$."}, {"title": "Small Model Learnability Gap", "content": "In this section, we fine-tune student models using different CoT data. We then reveal the small model learnability gap given the performance of fine-tuned models."}, {"title": "3.1 Experiment Setup", "content": "Datasets. We use the 7,500 prompt set of MATH (Hendrycks et al., 2021). This dataset encompasses seven math topics such as advanced calculus, geometry, and linear algebra.\nStudent models. Our study considers ten student models from the Qwen (Qwen, 2024a) and Llama (Meta, 2024a,b) model families of varying sizes. These models include the Instruct version of Qwen2.5-0.5B, Qwen2.5-1.5B, Qwen2.5-3B, Qwen2.5-7B, Qwen2.5-14B, and Qwen2.5-32B, and the Instruct version of Llama3.2-1B, Llama3.2-3B, Llama3.1-8B, and Llama3.3-70B. A comprehensive overview of the student models is presented in Table 4 of Appendix A.\nTeacher models. To compare long CoT with short CoT, we use QwQ-32B-Preview (Qwen, 2024b) to generate long CoT sequences and Qwen2.5-32B-Instruct as the response generator for short CoT. Within each model family, we designate the larger scale model as the large teacher and the smaller scale model as the small teacher. This includes Qwen2.5-72B-Instruct vs Qwen2.5-3B-Instruct, Llama3.1-70B-Instruct vs Llama3.1-8B-Instruct, and Gemma2-27B-it vs Gemma2-9B-it.\nEvaluation Benchmarks. We evaluate the reasoning capability of fine-tuned student models on a set of commonly used benchmarks, including MATH (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), AMC 2023, AIME 2024, and the English math subset of OlympiadBench (He et al., 2024). These benchmarks span a wide range of challenge levels, from elementary mathematics to advanced competition problems. We define the student model performance as the average score on five benchmarks. Unless otherwise specified, all fine-tuned models are evaluated in a zero-shot setting using greedy decoding. We set the maximum generation tokens as 16k. Please see Appendix A for detailed experimental setup.\nWe define the following performance scores:\n\u2022 $P_{Long}$: Performance score of a student model fine-tuned on long CoT data.\n\u2022 $P_{short}$: Performance score of a student model fine-tuned on short CoT data.\n\u2022 $P_{Large}$: Performance score of a student model fine-tuned on CoT from a larger teacher.\n\u2022 $P_{Small}$: Performance score of a student model fine-tuned on CoT from a smaller teacher.\nTraining Setup. Teacher models generate responses by rejection sampling (Dong et al., 2023; Gulcehre et al., 2023; Tong et al., 2024; Yuan et al., 2023; Yue et al., 2023; Zelikman et al., 2022) By default, teacher models employ greedy decoding. By combining the math problem instructions with corresponding solutions generated by teacher models, we construct problem-solution pairs to fine-tune student models."}, {"title": "3.2 Long CoT Gap", "content": "This section evaluates the reasoning capabilities of student models fine-tuned over long CoT data and short CoT data. We quantify the performance difference between long and short CoT data using long CoT gap $\\Delta_{Long}$, defined as:\n$\\Delta_{Long} = P_{Long} - P_{Short}$.\nFigure 2 provides a comprehensive overview of the long CoT gap $\\Delta_{Long}$ across different student models. The detailed benchmark scores on MATH, GSM8K, AIME, AMC, and OlympiadBench are deferred to Table 7 in Appendix B. We report the following key takeaways."}, {"title": "Takeaway 1: Long CoT Gap", "content": "Small student models tend to benefit more from short CoT, while large student models gain greater advantages from long CoT.\nWe observe that long CoT is more effective for larger models, consistently leading to improved performance across most math benchmarks. For example, the student model Qwen2.5-32B-Instruct improves about 15 points across all math metrics on average.\nHowever, long CoT data is not effective for smaller models, yielding significantly less improvement compared to short CoT. On the MATH and AMC benchmarks, student model Qwen2.5-1.5B-Instruct performs over 10 points lower when fine-tuned with long CoT data. This shows that smaller models may not be able to effectively learn and utilize the long CoT paradigm. Please see more attribution analysis in Section 3.4."}, {"title": "3.3 Large Teacher CoT Gap", "content": "We investigate how effective small models may learn from large teacher and small teachers. We define a large teacher CoT gap as:\n$\\Delta_{Large} = P_{Large} \u2013 P_{Small}$\u00b7"}, {"title": "3.4 Analysis of Small Model Learnability Gap", "content": "Domain knowledge affects learnability gap. We observe that math expert models, in spite of small model size, exhibit a smaller learnability gap for both long CoT and large teacher CoT data compared to general models in Figure 4. Specifically, we compare the learnability gaps between the student models Qwen2.5-Math-1.5B-Instruct and Qwen2.5-1.5B-Instruct. Our findings show that the long CoT gap of the small math expert model is significantly smaller than that of general small models. Furthermore, the performance improvement of Qwen2.5-Math-1.5B when fined-tuned with large teacher CoT exceeds that of Qwen2.5-1.5B, suggesting that math expert models benefit more substantially from large teacher CoT. We conjecture that a key factor leading to the small model learnability gap is the limited in-domain knowledge of small student models. We summarize this observation in the following takeaway."}, {"title": "Takeaway 3: Effect of Domain Knowledge", "content": "Limited domain knowledge of small models may hinder their learning from strong reasoning teachers.\nBase models exhibit a more significant learnability gap. We observe that base models generally exhibit a more significant learnability gap than Instruct models in Figure 5. This suggests that it is more challenging for small base models to effectively learn from long CoT data or large teacher COT."}, {"title": "Takeaway 4: Base vs Instruct", "content": "Small base models experience more significant learnability gap than Instruct models.\nSpeaking styles shift. We adopt the method from (Lin et al., 2023) to evaluate the rank shift of each token before and after fine-tuning on long CoT and Large teacher CoT data. This allows us to compare the token distribution shifts induced by the fine-tuning process. We then annotate the tokens that exhibit the largest rank shifts as the most shifted tokens. Our analysis reveals that these tokens are predominantly associated with expressive and stylistic elements, such as \u201cwait\u201d, \u201cBut\u201d, and \"Let\u201d. Please see Appendix C for more details."}, {"title": "Takeaway 5: Speaking Styles Shift", "content": "Long CoT and large teacher CoT primarily shift the student model's distribution of tokens associated with speaking styles."}, {"title": "4 Mix Distillation: Bridge Small Model Learnability Gap", "content": "This section presents our Mix Distillation approach to bridge the small model learnability gap."}, {"title": "4.1 Mix Distillation", "content": "We propose Mix Distillation to address the learnability gap observed in small models. This approach blends easier-to-learn data with more challenging data for small models, thereby leveraging the strengths of both.\nOur insight is that small models tend to perform better on data that closely matches their inherent distribution (such as short CoT or small teacher CoT), while they struggle with data that exhibits greater distribution shifts. The token distribution of the mixed long CoT and large teacher CoT data may become closer to that of small models' inherent distribution, thereby enabling them to learn more effectively from challenging datasets.\nWe propose Mix-Long, which combines long CoT and short CoT data with a weight of long CoT $\\alpha$ and short CoT $1-\\alpha$. Similarly, we proposed Mix-Large, which combines large teacher CoT with a weight of $\\alpha$ and small teacher CoT with a weight of $1 - \\alpha$."}, {"title": "4.2 Experiment Results", "content": "We use Qwen2.5-3B-Instruct as the student model and MATH (7.5k) as the training set. We distill different teacher models to generate responses as the baseline. They include QwQ-32B (long CoT), Qwen2.5-32B (short CoT), Qwen2.5-72B (large teacher CoT), Qwen2.5-3B (small teacher CoT). We add Deepseek-R1-32B (DeepSeek-AI, 2025) as the teacher model to generate another set of long CoT data as baseline. We set $\\alpha$ = 0.2 in both configurations of Mix-Long and Mix-Large.\nExperimental results demonstrate that both Mix-Long and Mix-Large surpass baselines in most evaluation metrics. We show that the small student model could achieve improved performance by Mix Distillation compared to training on a single dataset. For instance, Qwen2.5-3B-Instruct improves by more than 8 points on MATH and AMC using Mix-Long, compared to direct training on long CoT data. It also shows a more than 7-point gain on MATH, AIME and AMC for Qwen2.5-3B-Instruct by Mix-Large compared with training on large teacher CoT data. This implies that it is easier for small student models to learn from datasets generated by Mix Distillation."}, {"title": "Takeaway 6: Mix Distillation Bridges Gap", "content": "By mixing long CoT data (resp. large teacher CoTs) and short CoT data (resp. small teacher CoT), the small student model could achieve better performance compared to training on either data alone."}, {"title": "5 Related Work", "content": null}, {"title": "5.1 Chain-of-Thought", "content": "Early research on CoT primarily focused on short CoT, where models produce succinct reasoning paths to reach a solution (Lambert et al., 2025; Longpre et al., 2023; Wei et al., 2023; Yu et al., 2024). Recently, researchers have turned to long CoT prompting, which encourages the generation of extended and detailed reasoning chains (DeepSeek-AI, 2025; Hou et al., 2025; Kimi Team, 2025; NovaSky, 2025; OpenAI, 2024; Pan et al., 2025; Zeng et al., 2025). The model systematically explores multiple paths (branching) and reverts to earlier points if a particular path proves wrong (backtracking). Although several studies have investigated methods such as distillation and reinforcement learning to integrate long CoT capabilities into LLMs, these efforts have predominantly concentrated on large models. In contrast, our work specifically targets the challenges associated with training smaller models."}, {"title": "5.2 Synthetic Reasoning Data", "content": "Although human-crafted reasoning datasets have been used to enhance LLM reasoning capabilities (Hendrycks et al., 2021; LI et al., 2024), their development is both time-consuming and labor-intensive. Recent advancements have streamlined this process by generating instructions or responses directly from LLMs (Hui et al., 2024; Toshniwal et al., 2024; Xu et al., 2024; Yue et al., 2023; Zhang et al., 2025) or extracting data directly from web (Paster et al., 2023; Yue et al., 2024), yielding more detailed and diverse chain-of-thought reasoning pathways. Recent study has investigated the impact of various response generators (Kim et al., 2024b), suggesting that in the domains of instruction following and reasoning, responses from stronger teacher models do not necessarily produce the most effective learning effects for student models. However, these investigations have not recognized student model size as a critical factor influencing this phenomenon, nor have they performed the more attribution and mitigation analyses as in this paper."}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we show that long CoT data and large model responses were not uniformly beneficial for small student models. We found that small models may perform better when fine-tuned with short CoT and small model CoT. We termed this challenge as the Small Model Learnability Gap. The reason behind it may be that small student models excel on data that closely match their inherent distribution but struggle with significant distribution shifts. To bridge the gap, we introduced Mix Distillation, including Mix-Long, which combined long CoT and short CoT data in a ratio, and Mix-Large, which integrated large and small teacher CoT. Experimental results showed that both Mix-Long and Mix-Large outperform baselines across most evaluation metrics, which implied mix distillation outperforms training on a single data distribution. This paper provided practical insights for optimizing post-training strategies to enhance small language model reasoning capability.\nWe will explore several promising directions as future work. First, we will refine mix distillation by optimally combining diverse data sources and proposing more fine-grained mixing algorithms to boost reasoning capabilities. Second, we propose to study how strong reasoning teachers can generate data that is better suited for tuning small student models, thereby facilitating more effective knowledge transfer. Third, we will conduct further theoretical and model interpolability studies on the small model learnability gap. Lastly, we will investigate which SFT methods yield the best initial policies for subsequent RL procedure, ultimately enhancing overall model performance."}, {"title": "Ethical Statement", "content": "This paper focuses on the evaluation and enhancement of reasoning capabilities in small language models through distillation techniques. The dataset and benchmarks used in our experiments are publicly available. We do not introduce or endorse any applications that could cause harm or be misused. This paper does not present any ethical concerns."}]}