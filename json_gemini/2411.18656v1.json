{"title": "The Return of Pseudosciences in Artificial Intelligence: Have Machine Learning and Deep Learning Forgotten Lessons from Statistics and History?", "authors": ["J\u00e9r\u00e9mie Sublime"], "abstract": "In today's world, AI programs powered by Machine Learning are ubiquitous, and have achieved seemingly exceptional performance across a broad range of tasks, from medical diagnosis and credit rating in banking, to theft detection via video analysis, and even predicting political or sexual orientation from facial images. These predominantly deep learning methods excel due to their extraordinary capacity to process vast amounts of complex data to extract complex correlations and relationship from different levels of features.\nIn this paper, we contend that the designers and final users of these ML methods have forgotten a fundamental lesson from statistics: correlation does not imply causation. Not only do most state-of-the-art methods neglect this crucial princi- ple, but by doing so they often produce nonsensical or flawed causal models, akin to social astrology or physiognomy. Consequently, we argue that current efforts to make AI models more ethical by merely reducing biases in the training data are insufficient. Through examples, we will demonstrate that the potential for harm posed by these methods can only be mitigated by a complete rethinking of their core models, improved quality assessment metrics and policies, and by maintaining humans oversight throughout the process.", "sections": [{"title": "Introduction", "content": "Since the advent of Deep Learning technologies in recent years, artificial intelligence (AI) systems have become pervasive across a wide range of real-wold applications. Indeed, Machine Learning (ML) specialists have been busy generating a continuous flow of AI solutions targeted at all sorts of applications which are often far beyond their own areas of expertise: theft detection via video surveillance in local shopping centres, credit risk analysis software in banking, marketing algorithms that determine which advertisements to display based on personal data, AI assisted diagnosis, etc. There are more controversial examples, such as seemingly innocuous facial analysis software designed to infer political or orientation, and even AI-driven judicial decision to decide to decide bail eligibility based on an individual's background data. The list of applications, some trivial and others deeply consequential, is ever-expanding with each passing day.\nGiven the seriousness and sensitivity of some of these tasks delegated to AI systems, there is a growing interest within both the ML and ethics communities to advocate for fairer and socially responsible AI algorithms [1, 2] that minimize harm's risks and are as unbiased as possible. This movement has taken various forms including the development of explainable AI [3], and a stronger focus on training Machine Learning algorithms using data that have been curated to reduce biases of all kinds [4].\nHowever, it is this paper's goal to demonstrate that possible biases in data repre- sent only a small part of the issue, and that the primary ethical concern with current machine learning and deep learning methods lies in the undue attribution of causal- ity by their designers and users. Indeed, while it is undeniable that Deep Learning methods -and in particular convolutional networks [5] for images and videos, as well as autoencoders [6] for complex data- are highly effective at identifying complex and intricate relationships as well as correlations from large amount of training data, it remains a fundamental error to assume that such systems, which are inherently statis- tical, can be trusted for sensitive tasks that should require explainability. We contend that bestowing these deep learning-based systems with what amounts to \"oracle-like\" powers is not only selling snake oil, but also akin to endorsing pseudosciences such as Lombrosianism, physiognomy, and social astrology. Moreover, we argue that, in addition to their lack of knowledge and disregard for historical context in various application domains, too many Machine Learning researchers seem have forgotten the fact that the field of machine learning originated as a branch of statistics, where a key tenet is that correlation does not imply causation.\nFinally, we assert that efforts at developing AI algorithms that \"do no harm\" are destined to fail as long as AI projects formerly reliant on human intervention are driven exclusively by ML experts who are too often removed from the application domain, and who rely on metrics that prioritise outcomes over fairness.\nWithin this context, this paper will develop the following points:\n\u2022 Through an exploration of various recent machine learning applications, either cur- rently in use or nearing deployment, we will demonstrate how pseudosciences such as physiognomy and racial theories have been revived, rebranded with a modern veneer, and even legitimised through the use of AI."}, {"title": "State of the Art on potentially misguided and harmful AI applications", "content": "In this section, we provide a comprehensive overview of recent AI applications that have a strong potential to be harmful, or could at the very least be misguided. To clarify our use of the terms potentially harmful and misguided, we will focus on those AI applications that could lead to critical, life-altering decisions or have a significant impact on people's safety and privacy. These applications can broadly be divided into two categories, which may at time overlap: those related to justice, security and law enforcement; and applications with sociological or societal implications, including the health sector.\nIn the field of justice and law enforcement, several AI systems have been tested already, and attempts to forecast criminal behaviour through AI and statistical meth- ods are not particularly new [7, 8]. However, the deployment of AI systems from machine learning research labs into actual courtrooms is a more recent development. An example of this is the OASys (Offender Assessment System) AI in the United Kingdom [9, 10] which assists probation officers in assessing the risk posed by indi- viduals within the justice system and determining the best course of action. Similar systems for criminal risk assessment have been explored for suitability in Thailand [11], the USA [12, 13], Finland [14], the Netherlands [8, 15], and there is a substantial body of literature discussing such AI programs at various stages of development and implementation [16]. Although few of these systems have been fully integrated thus far, the reduced costs compared with human operators, along with bold claims of high accuracy, suggest that this trend will continue to grow.\nIn fact, AI has already become prevalent in one area of security: video surveillance. In this area, numerous ssystems have been developed, not only for facial recognition [17], but also to automatically detect thefts in banks [18], tracking thieves via CCTV"}, {"title": "The reanimation of pseudosciences by ML methods and its ethical implications", "content": "To begin explaining how the Machine Learning and Deep Learning applications we have just discussed have revived pseudoscientific ideas, we must first explore the basic functionalities of these methods and the assumptions they rely on. In other words, we will first clarify how these methods learn and operate.\nWithin the context of this papers, we focus on two specific supervised Machine Learning tasks:\n\u2022 Classification: It consists in accurately predicting a category, a class or a label based on the attributes of observed data.\n\u2022 Regression: A similar task, but where the goal is to predict the values (numerical value) of one or more variables based on other attributes.\nTo accomplish these tasks, Machine Learning methods use evidence, referred to as training data, to perform predictions or classifications. The aim is therefore to gener- alise from the training data (also called the training set) to new, and previously unseen data (the test set). It can therefore be asserted that machine learning is fundamentally about inductive inference.\nMachine Learning methods are built on a variety of principles, primarily derived from statistical and probabilistic models. However, it is the advances in Deep Learning and neural networks technologies that have propelled the field forward, enabling the analysis of data types that were previously too challenging for traditional ML methods, such as images, videos, time series, and even conventional data with a very large number of features.\nA neural network is a complex algorithm that takes a large number of features as inputs: a large number of variables, pixels or patches of pixels from an image, etc. The network then computes intermediate representations within multiple layers [39], and produces an output which can be a class, a label, or a value, depending on the task. Training such a network involves presenting it with a large set of labelled examples to tune its weight parameters. This process is known as parametrisation and relies on gradient backpropagation with respect to some objective function, which typically seeks to minimise the error between the network's output and the expected result from the training set label. Through this training process, the network's weight will converge, transforming said network into a highly complex mathematical function that maps inputs to outputs. From there, assuming the relevant hypotheses of similarity between the training and the test sets hold, the network should be able to infer outcomes for previously unseen instances, and to generalise from particular cases to broader classes [40, 41].\nWhile we do not dispute the reality of correlations identified by Deep Learning algorithms, nor their high efficiency, or ability to generalize -though we do not exclude"}, {"title": "The silent return of physiognomy, Lombrosianism, phrenology, distorted sociobiology, social astrology and other quackeries with a new AI polish", "content": "Unless you are a native english speaker with a background in ethics, philosophy or modern history, you may not be familiar with terms such as physiognomy, phrenol- ogy and Lombrosianism. However, you may have heard about sociobiology and social astrology. Before we move forward with our discussion on the susceptibility of machine learning and deep learning to pseudoscience, let us define some of these terms:\nPhysiognomy is \u201cthe facility to identify, from the form and constitution\nof external parts of the human body, chiefly the face, exclusive of all\ntemporary signs of emotions, the constitution of the mind and the heart.\"\nGeorg Christoph Lichtenberg, 1778\nHowever, physiognomy is much older than Lichtenberg, and already 2 millenia before dawn, the Babylonians spoke of \u201cphysiognomic omens\u201d regarding facial features that could be predictive of life trajectory [44].\nPhrenology -or craniology- involves the measurement of bumps on the\nskull to predict mental traits.\nOriginally proposed by German physician Franz Joseph Gall in 1796 when he was trying to figure out the function of different brain areas. The discipline became popular in the 1830s and 1840s -in particular in the USA- where it was used as an argument by physicians such as Charles Caldwell in an attempt to prove the superiority of white people over African people (thus justifying slavery and segregation), and by Samuel Morton to justify the persecution of native Americans.\nLombrosianism is a theory in criminology developed in the late 19th\ncentury by Italian physician Cesare Lombroso. This theory suggest that\ncriminal behavior is innate and can be identified through physical traits.\nMister Lombroso believed that criminals were biologically different from\nnon-criminals, often marked by \u201catavistic\" features that resembled\nearlier stages of human evolution (such as certain facial structures or\nbody types). This theory supports the idea that criminals are \u201cborn,\u201d not\nmade, and could be distinguished by these primitive traits.\n[45].\nAll three pseudosciences were extensively used for racist purposes, including jus- tification of slavery and imperialism during the 18th and 19th century, and later for eugenics and other imperialist policies in the 20th century under the German Third Reich.\nDistorted sociobiology is nothing less than the modern version of Lombrosianism, phrenology and and physiognomy. It seeks to apply and distort advances in biology and genetics to predict similar outcomes (such as life trajectories, IQ, predispositions to criminal behavior, propensity to lie, mental traits, and sexual orientation) based on factors like ethnic background -which often means \"race\"-, social group (religious or cultural), genetics, neurochemistery, or behavioral traits (such as heart beat, eye move- ments and voice patterns). As for social astrology, while seemingly more innocuous, it is akin to these disciplines in its attempt to infer characteristics like political orien- tation, sexual orientation, or religion based on similar features, along with additional data about individuals' social backgrounds.\nNow that you are more familiar with the definitions of these pseudosciences, it should be clearer that many of the AI algorithms discussed so far in this work are part of these pseudo-disciplines."}, {"title": "Impact of ML quality metrics on the social harm potential of AI algorithms", "content": "Since we have just discussed the legitimization of AI methods in part due to their high scores on quality metrics, in this section we will examine some of these metrics and how they are used to assess the value of Machine Learning and Deep Learning algorithms. We will particularly focus on the quality metrics used for classification tasks, as this type of Machine leanring methods are most susceptible to pseudoscientific distortion. In essence, the goal of any Machine Learning classification method is to achieve high accuracy on the training and test set to ensure robust generalization features. Accuracy is simply defined as the number of correctly classified instance divided by the total number of instance. While classifier can be multi-class (predicting the correct class or label among more than two categories) or binary (determining whether an observed data belongs to a given single class or not), multi-class and binary classifiers are typically evaluated in the same way, with each multi-class label being assessed individually as if it were binary. Binary quality measures are then computed using the following elements:\n\u2022 Number of true positive cases (TP) : Data assigned to a class and that really belong to this class.\n\u2022 Number of false positive cases (FP): Data assigned to a class but that do not belong to it.\n\u2022 Number of true negative cases (TN): Data not assigned to a class and that really do not belong to this class.\n\u2022 Number of false negative cases (FN): Data not assigned to a class but that should belong to it.\nUsing these notations, the classifier's accuracy for a given class is computed as follows:\nAccuracy = $\\frac{TP+TN}{TP+TN+FP+FN}$ (1)\nAnother very common binary measure is the recall, which in some fields is also called the \"hit rate\" or \"sensitivity\". It is the percentage of correctly detected positive cases:\nRecall = $\\frac{TP}{TP + FN}$ (2)\nSince accuracy is the most well known quality index inside and outside the Machine Learning community, it comes to no surprise that this metric is frequently used in applied machine learning papers and in marketing AI products that are intended for public release. Following accuracy, recall is another common secondary quality metric, particularly in security applications where detecting as many positive cases as possible (e.g., identifying all criminals) is often a key performance indicator. This is similarly important in health-related applications where missing a deadly disease is more critical than a false positive diagnosis.\nFocusing primarily on these two metrics can pose significant issues regarding the potential harm of AI algorithms. For example, what about the consequences of false positives in criminal conviction ? Or the impact of being unfairly rejected for a job due to a face recognition system making an erroneous assessment ? Notice how we are not even talking about biases here. Rather, we are addressing the inadequacy of using the wrong metrics to evaluate the safety of AI systems. Yet, there are metrics that could be used to mitiate such risks. The precision for instance is the probability that a predicted positive case is really positive:\nPrecision = $\\frac{TP}{TP+FP}$ (3)\nThere is also its opposite, the specificity (or true negative rate) which measures how well a binary classifier identifies negative cases. Specificity is important when the cost of false positives is high, as it indicates the model's ability to correctly reject negative cases:\nSpecificity = $\\frac{TN}{TN + FP}$ (4)\nDespite the existence of these important quality metrics, accuracy and, to a lesser extent, recall are typically highlighted first and prominently featured in abstracts or when advertising for an AI system. Precision and specificity, on the other hand, are"}, {"title": "Assessing the real impact of error made by AI systems", "content": "To evaluate the potential impact of these systems if implemented at a large scale, as well as the real impact of quality metrics, we propose using available data and esti- mates about population and CCTV cameras in four large metropolitan areas: London (UK), Beijing (China), Hyderabad (India) and New York (USA). Indeed, given current surveillance trends, CCTV cameras, which are already extensively used for security purposes in many countries, are likely to be the primary medium for deploying A\u0399 systems based on physiognomy or Lombrosianism for crime detection and analysis. For our simulations, we will make the following assumptions:\n\u2022 The accuracy, precision and recall are known and reliable for our AI models. We will test in a setting where accuracy, recall and precision have the same values, indicating a balanced models. And we will test values 90%, 95% and 99% for these metrics.\n\u2022 To avoid speculative computation, we will consider that each person living in these metropolitan cities will be tested once, and that the AI algorithms remain consistent in their predictions (i.e.: even if a person were to be analyzed several times, the classification would remain the same).\nSince it is challenging to determine the exact percentage of criminals within the general population, we have used estimates available online for the four cities, all derived from crime rates. We acknowledge that this approach is not ideal and may not be entirely accurate. A more uniform criminal percentage across all cities might have provided a more consistent comparison. However, we believe that exploring a broader range of settings will actually offer valuable insights into the behavior of AI models across different scenarios and provides us with a better opportunity to study these models.\nAssuming a population of size N and an AI model for which the accuracy, recall and precision are known. Then, from an estimated percentage of criminals r in our population N, computing the number of false positive is done as follows. First, we compute the number of true positive cases:\nTP = recall \u00d7 r \u00d7 N (6)\nThen, we can re-arrange Equation (3) which gives us:"}, {"title": "The Myth of theory-free inference and unbiased training data", "content": "In an ideal world, one might contend that scientific practices should be value-free in order to minimize harm potential and reduce bias. However, numerous philosophers and scientists have compellingly argued that Science and the knowledge it produces are shaped by human normative values [50, 51]. Thus, the notion \u201cvalue-free\" Science may well be a myth. Indeed, scientific research is always conducted within a broader context, and its value depends on the specific applications it serves and its direct (or indirect) impacts on human lives [52]. In essence, any scientific research that serves a purpose can never truly be \"value-free\".\nIn the field of Machine Learning and Artificial Intelligence, a parallel concept to \"value-free\" science has emerged in the form of the so-called \"theory-free\" models [49, 53]. Proponents of theory-free models argue that because these models do not rely on specific mechanisms from application fields and are \"data-driven\", they would be free from human biases, preconceived judgments, and ontological categories. We contend that the argument of \u201ctheory-free\u201d AI models is a fallacy, scientific quackery, and far too often serves as a smoke screen to legitimize bigotry through a \"data-driven\" pseudo-truth:\nFirst, to suggest that explanations can be developed without a model or theory undermines the scientific method. The notion that that a large amount of data would remove the need for theory and mechanisms -or worst imply that said theory and mechanism do not exist or can be denied because of empirical evidences [54]- is fun- damentally flawed. Disregarding theory in applied fields further implies that experts knowledge the historicity of a given field are irrelevant. This is certainly not the case, and field knowledge remains in our opinion the best way to avoid repeating the errors of the past.\nSecond, the belief that \"data-driven\" models are inherently fair and that the pat- terns and conclusions drawn from large datasets represent undeniable truths is a clear fallacy. Historical proponents of physiognomy and eugenics also subscribed to the idea that good research practices should consists in \"gathering as many facts as possible without any theory or general principle that might prejudice a neutral and objective view of these facts\" [55]. This perspective mirrors the \u201cdata-driven\" AI methods that have resurrected physiognomy and other pseudosciences under the guise of modern AI. Regardless of the validity of patterns identified by these methods, constructing a model or ideology based solely on correlation, but without understanding causation, is a profound misunderstanding.\nThen, the assertion that Machine Learning and Artificial Intelligence are \"theory- free\" and \"model-free\" and, therefore \"value-free\" is incorrect. Although many AI and DL models do not rely on any specific model, we have seen how they work in section 3.1: They have complicated models, sometimes called \"black-box models\", but they have models nonetheless. And while these models indeed appear \"theory-free\""}, {"title": "Conclusion", "content": "In this paper, we have explored the troubling resurgence of pseudoscientific method- ologies within the realm of Artificial Intelligence. In particular, we have discussed how the Deep Learning technology made it easier to hide the pseudoscientific nature of some applied tasks due to their inherent complexity, black-box type model, but also thanks to their seemingly high accuracy. Our analysis further highlights a critical issue: despite their advanced capabilities, these AI systems have often neglected fun- damental lessons from statistics, and in particular the principle that correlation does not imply causation.\nWe have shown how the high performances of these models and reliance on the \"theory-free\" ideology made it possible to inadvertently replicate and even exacerbate the errors of past pseudosciences. This includes approaches reminiscent of Lombrosian- ism and physiognomy, which once justified discriminatory practices through dubious correlations. We have further demonstrated that many state-of-the-art AI models promoting such pseudosciences, by focusing excessively on performance metrics that"}]}