{"title": "DINO-WM: WORLD MODELS ON PRE-TRAINED VISUAL FEATURES ENABLE ZERO-SHOT PLANNING", "authors": ["Gaoyue Zhou", "Hengkai Pan", "Yann LeCun", "Lerrel Pinto"], "abstract": "The ability to predict future outcomes given control actions is fundamental for physical reasoning. However, such predictive models, often called world models, have proven challenging to learn and are typically developed for task-specific solu-tions with online policy learning. We argue that the true potential of world models lies in their ability to reason and plan across diverse problems using only passive data. Concretely, we require world models to have the following three properties: 1) be trainable on offline, pre-collected trajectories, 2) support test-time behavior optimization, and 3) facilitate task-agnostic reasoning. To realize this, we present DINO World Model (DINO-WM), a new method to model visual dynamics with-out reconstructing the visual world. DINO-WM leverages spatial patch features pre-trained with DINOv2, enabling it to learn from offline behavioral trajectories by predicting future patch features. This design allows DINO-WM to achieve ob-servational goals through action sequence optimization, facilitating task-agnostic behavior planning by treating desired goal patch features as prediction targets. We evaluate DINO-WM across various domains, including maze navigation, tabletop pushing, and particle manipulation. Our experiments demonstrate that DINO-WM can generate zero-shot behavioral solutions at test time without relying on expert demonstrations, reward modeling, or pre-learned inverse models. Notably, DINO-WM exhibits strong generalization capabilities compared to prior state-of-the-art work, adapting to diverse task families such as arbitrarily configured mazes, push manipulation with varied object shapes, and multi-particle scenarios.", "sections": [{"title": "1 INTRODUCTION", "content": "Robotics and embodied AI has seen tremendous progress in recent years. Advances in imitation learning and reinforcement learning has enabled agents to learn complex behaviors across diverse tasks Lee et al. (2024); Zhao et al. (2023); Ma et al. (2024); Hafner et al. (2024); Hansen et al. (2024); Agarwal et al. (2022); Haldar et al. (2024). Despite this progress, generalization remains a major challenge Zhou et al. (2023). Existing approaches predominantly rely on policies that, once trained, operate in a feed-forward manner during deployment-mapping observations to actions without any further optimization or reasoning. Under this framework, successful generalization inherently requires agents to possess solutions to all possible tasks and scenarios once training is complete, which is only possible if the agent has seen similar scenarios during training Brohan et al. (2023b;a); Reed et al. (2022); Etukuru et al. (2024). However, it is neither feasible nor efficient to learn solutions for all potential tasks and environments in advance.\nInstead of learning the solutions to all possible tasks during training, an alternate is to fit a dynamics model on training data and optimize task-specific behavior during runtime. These dynamics models, also called world models Ha & Schmidhuber (2018), have a long history in robotics and control Sut-ton (1991); Todorov & Li (2005); Williams et al. (2017). More recently, several works have shown that world models can be trained on raw observational data Hafner et al. (2019; 2024); Micheli et al. (2023); Robine et al. (2023); Hansen et al. (2024). This enables flexible use of model-based op-timization to obtain policies as it circumvents the need for explicit state-estimation. Despite this, significant challenges still remain in it use for solving general-purpose tasks."}, {"title": "2 RELATED WORK", "content": "We build on top of several works in building world models, optimizing them, and using compact visual representations. For conciseness, we only discuss the ones most relevant to DINO-WM.\nModel-based Learning: Learning from models of dynamics has a rich literature spanning the fields of control, planning, and robotics Sutton (1991); Todorov & Li (2005); Astolfi et al. (2008); Holkar & Waghmare (2010); Williams et al. (2017). Recent works have shown that modeling dynamics and predicting future states can significantly enhance vision-based learning for embodied agents across various applications, including online reinforcement learning Hafner et al. (2024); Micheli et al. (2023); Hansen et al. (2024); Robine et al. (2023), exploration Mendonca et al. (2021; 2023a); Sekar et al. (2020), planning Finn & Levine (2017); Ebert et al. (2018); Hafner et al. (2019), and imitation learning Pathak et al. (2018). Several of these approaches initially focused on state-space dynamics Deisenroth & Rasmussen (2011); Chua et al. (2018); Lenz et al. (2015); Nagabandi et al. (2019), and has since been extended to handle image-based inputs, which we address in this work. These world models can predict future states in either pixel space Finn & Levine (2017); Ebert et al. (2018); Ko et al. (2023); Du et al. (2023) or latent representation space Yan et al. (2021). Predicting in pixel space, however, is computationally expensive due to the need for image reconstruction and the overhead of using diffusion models. On the other hand, latent-space prediction is typically tied to objectives of reconstructing images Hafner et al. (2019; 2024); Micheli et al. (2023), which raises concerns about whether the learned features contain sufficient information about the task. Moreover, many of these models incorporate reward prediction Hafner et al. (2024); Micheli et al. (2023); Robine et al. (2023), or use reward prediction as auxiliary objective to learn the latent representation Hansen et al. (2024; 2022), inherently making the world model task-specific. In this work, we aim to decouple task-dependent information from latent-space prediction, striving to develop a versatile and task-agnostic world model capable of generalizing across different scenarios.\nGenerative Models as World Models: With the recent excitement of large scale foundation mod-els, there have been initiatives on building large-scale video generation world models conditioned on agent's actions in the domain of self-driving Hu et al. (2023), control Yang et al. (2023); Bruce et al. (2024), and general-purpose video generation Liu et al. (2024). These models aim to generate video predictions conditioned on text or high-level action sequences. While these models have demon-strated utility in downstream tasks like data augmentations, their reliance on language conditioning limits their application when precise visually indicative goals need to be reached. Additionally, the use of diffusion models for video generation makes them computationally expensive, further re-stricting their applicability for test-time optimization techniques such as MPC. In this work, we aim to build a world model in latent space rather than in the raw pixel space, which enables more precise planning and control.\nPretrained Visual Representations: Significant advancements have been made in the field of visual representation learning, where compact features that capture spatial and semantic information can be readily used for downstream tasks. Pre-trained models like ImageNet pre-trained ResNet He et al. (2016), I-JEPA Assran et al. (2023), and DINO Caron et al. (2021); Oquab et al. (2024) for images, as well as V-JEPA Bardes et al. (2024) for videos, and R3M Nair et al. (2022), MVP Xiao et al. (2022) for robotics have allowed fast adaptation to downstream tasks as they contain rich spatial and semantic information. While many of these models represent images using a single global feature, the introduction of Vision Transformers (ViTs) Dosovitskiy et al. (2021) has enabled the use of pre-trained patch features, as demonstrated by DINO Caron et al. (2021); Oquab et al. (2024). DINO employs a self-distillation loss that allows the model to learn representations effectively, capturing semantic layouts and improving spatial understanding within images. In this work, we leverage DINOv2's patch embeddings to train our world model, and demonstrate that it serves as a versatile encoder capable of handling multiple precise tasks."}, {"title": "3 DINO WORLD MODELS", "content": "Overview and Problem formulation: Our work follows the vision-based control task framework, which models the environment as a partially observable Markov decision process (POMDP). The POMDP is defined by the tuple (O, A, p), where O represents the observation space, and A denotes the action space. The environment's dynamics are modeled by the transition distribution p(Ot+1 |\no<t, a<t), which predicts future observations based on past actions and observations.\nIn this work, we aim to learn task-agonstic world models from pre-collected offline datasets, and use these world models to perform visual reasoning and control at test time. At test time, our system starts from an arbitrary environment state and is provided with a goal observation in the form of an RGB image, in line with prior works Wu et al. (2020); Ebert et al. (2018); Mendonca et al. (2023b), and is asked to perform a sequence of actions ao, ..., ar such that the goal state can be achieved. This approach differs from world models used in online reinforcement learning (RL) where the objective is to optimize rewards for a fixed set of tasks at hand Hafner et al. (2024); Hansen et al. (2024), or from text-conditioned world models, where goals are specified through text prompts Du et al. (2023); Ko et al. (2023)."}, {"title": "3.1 DINO-BASED WORLD MODELS (DINO-WM)", "content": "We model the dynamics of the environment in the latent space. More specifically, at each time step t, our world model consists of the following components:\nObservation model: $z_tence(z_tOt)$\nTransition model: $Zt+1 ~ Po(Zt+1 | Zt-H:t, at-H:t)$\nDecoder model (optional for visualization): $\\hat{O_t} ~ qo(Otzt)$\nwhere the observation model encodes image observations to latent states zt, and the transition model takes in a history of past latent states of length H. The decoder model takes in a latent zt, and reconstruct the image observation ot. We use 0 to denote the parameters of these models. Note that our decoder is entirely optional, as the training objectives for the decoder is independent for training the rest part of the world model. This eliminates the need to reconstructing images both during training and testing, which reduces computational costs compared to otherwise coupling together the training of the observational model and the decoder, as in Hafner et al. (2024); Micheli et al. (2023).\nDINO-WM models only the information available from offline trajectory data in an environment, in contrast to recent online RL world models that also require task-relevant information, such as rewards Hansen et al. (2022; 2024); Hafner et al. (2020), discount factors Hafner et al. (2022); Robine et al. (2023), and termination conditions Hafner et al. (2024); Micheli et al. (2023)."}, {"title": "3.1.1 OBSERVATION MODEL", "content": "With the goal of learning a generic world model across many environments and the real world, we argue that the observation model should 1) be task and environment independent, and 2) contain"}, {"title": "3.1.2 TRANSITION MODEL", "content": "We adopt the ViT Dosovitskiy et al. (2021) architecture for the transition model as it is a natural choice for processing patch features. However, a few modifications are required to the architecture to allow for additional conditioning on proprioception and controller actions.\nOur transition model takes in a history of past latent states Zt-H:t\u22121 and actions at-H:t-1, where H is a hyperparameter denoting the context length of the model, and predicts the latent state at next time step zt. To properly capture the temporal dependencies, where the world state at time t should only depend on previous observations and actions, we implement a causal attention mechanism in the ViT model, enabling the model to predict latents autoregressively at a frame level. Specifically, each patch vector z\u012f for the latent state zt attends to ${z_H:t-1}=1$. This is different from past work IRIS Micheli et al. (2023) which similarly represent each observation as a sequence of vectors, but autoregressively predict z\u012f at a token level, attending to {z-H:t-1}=1 as well as {z}\nil<k We argue that predicting at a frame level and treating patch vectors of one observation as a whole better captures global structure and temporal dynamics, modeling dependencies across the entire observation rather than isolated tokens, leading to improved temporal generalization.\nTo model the effect of the agent's action to the environment, we condition the world model's pre-dictions on these actions. Specifically, we concatenate the K-dimensional action vector, mapped from the original action representation using a multi-layer perceptron (MLP), to each patch vector z\u012f for i = 1,..., N. When proprioceptive information is available, we incorporate it similarly by concatenating it to the observation latents, thereby integrating it into the latent states.\nWe train the world model with teacher forcing. During training, we slice the trajectories in to segments of length H + 1, and compute a latent consistency loss on each of the H predicted frames. For each frame, we compute\n$\\mathcal{L}_{pred} = ||pe (ence(Ot\u2212H:t), (at\u2212H:t)) \u2013 enco (Ot+1)||^2$ (1)\nwhere & is the action encoder model that can map actions to higher dimensions. Note that our world model training is entirely performed in latent space, without the need to reconstruct the original pixel images."}, {"title": "3.1.3 DECODER FOR INTERPRETABILITY", "content": "To aid in visualization and interpretability, we use a stack of transposed convolution layers to decode the patch representations back to image pixels, similar as in Razavi et al. (2019). Given a pre-collected dataset, we optimize the parameters @ of the decoder qe with a simple reconstruction loss defined as:\n$\\mathcal{L}_{rec} = ||q\u03b8(zt) - ot||^2$, where $zt = enco(ot)$ (2)\nThe training of the decoder is entirely independent of the transition model training, offering several advantages: 1) The quality of the decoder does not affect the world model's reasoning and planning capabilities for solving downstream tasks, and 2) During planning, there is no need to reconstruct raw pixel images, thereby reducing computational costs. Nevertheless, the decoder remains valuable as it enhances the interpretability of the world model's predictions."}, {"title": "3.2 VISUAL PLANNING WITH DINO-WM", "content": "Arguably, to evaluate the quality of the world model, it needs to be able to allow for downstream rea-soning and planning. A standard evaluation metric is to perform trajectory optimization at test time"}, {"title": "4 EXPERIMENTS", "content": "Our experiments are designed to address the following key questions: 1) Can we effectively train DINO-WM using pre-collected offline datasets? 2) Once trained, can DINO-WM be used for visual planning? 3) To what extent does the quality of the world model depend on pre-trained visual representations? 4) Does DINO-WM generalize to new configurations, such as variations in spatial layouts and object arrangements? To answer these questions, we train and evaluate DINO-WM across five environment suites (full description in Appendix A.1) and compare it to a variety of state-of-the-art world models that model the world both in latent space and in raw pixel space."}, {"title": "4.1 ENVIRONMENTS AND TASKS", "content": "We consider five environment suites in our evaluations spanning simple navigation environments and manipulation environments with varying dynamics complexity. For all environments, the obser-vation space is RGB images of size (224, 224).\na) Point Maze: A simple 2D point maze navigation environment in the D4RL suite Fu et al. (2021). A point agent with 2-dimensional action space moves in a U-shape maze. The agent's dynamics incorporate physical properties such as velocity, acceleration, and inertia, making the movement realistic. The objective of the task is to navigate the maze and reach arbitrary goal locations from arbitrary starting location.\nb) Push-T: This manipulation environment was introduced in Chi et al. (2024) to study precise manipulation. The environment features a pusher agent interacting with a T-shaped block. The goal is to guide both the agent and the T-block from a randomly initialized state to a known feasible target configuration within 25 steps. The task requires both the agent and the T to match the target locations. Unlike previous setups, the fixed green T no longer represents the"}, {"title": "4.2 BASELINES", "content": "We compare DINO-WM with the following state-of-the-art models commonly used for control:\na) IRIS Micheli et al. (2023): IRIS employs a discrete autoencoder to translate visual inputs into tokens, and a GPT Transformer that predicts tokens of future observations. It combines these components to learn policies and value functions through imaginative procedures.\nb) DreamerV3 Hafner et al. (2024): DreamerV3 learns a world model to interpret visual inputs into categorical representation. It predicts future representations and rewards based on given action and trains an actor-critic policy from its imagined trajectories.\nc) TD-MPC2 Hansen et al. (2024): TD-MPC2 learns a decoder-free world model in latent space and uses reward signals to optimize the latents. It serves as a strong baseline for reconstruction-free world modeling.\nd) AVDC Ko et al. (2023): AVDC leverages a diffusion model to generate an imagined video of task execution based on initial observation and a textual goal description. It then estimates optical flow between frames to capture object movements and generates robot arm commands."}, {"title": "4.3 \u039f\u03a1\u03a4\u0399MIZING BEHAVIORS WITH DINO-WM", "content": "With a trained world model, we study if DINO-WM be used for zero-shot planning directly in the latent space.\nFor the PointMaze, Push-T, and Wall environments, we sample 50 initial and goal states to measure the success rate across all instances. Due to the environment stepping time for the Rope and Granular environments, we evaluate the Chamfer Distance (CD) on 10 instances for them. In the Granular environment, we sample a random configuration from the validation set, with the goal of pushing the materials into a square shape at a randomly selected location and scale."}, {"title": "4.4 DOES PRE-TRAINED VISUAL REPRESENTATIONS MATTER?", "content": "We use different pre-trained general-purpose encoders as the observation model of the world model, and evaluate their downstream planning performance. Specifically, we use the following encoders commonly used in robotics control and general perception: R3M Nair et al. (2022), ImageNet pre-trained ResNet-18 Russakovsky et al. (2015); He et al. (2016) and DINO CLS Caron et al. (2021)."}, {"title": "4.5 GENERALIZING TO NOVEL ENVIRONMENT CONFIGURATIONS", "content": "We would like to measure the generalization capability of our world models not just across different goals in an environment, but across different environments themselves. For this we construct three families of environments, where the world model will be deployed in an unseen environment for unseen goals. Our families of environments consist of WallRandom, PushObj, and GranularRandom"}, {"title": "4.6 QUALITATIVE COMPARISONS WITH GENERATIVE VIDEO MODELS", "content": "Given the prominence of generative video models, it is reasonable to presume that they could read-ily serve as world models. To investigate the usefulness of DINO-WM over such video generative models, we compare it with imagined rollouts from AVDC Ko et al. (2023), a diffusion-based gen-erative model. As seen in Figure 7, we find that the diffusion model trained on benchmarks produce future images that are mostly visually realistic, however they are not physically plausible as we can see that large changes can occur in a single timestep of prediction, and may have difficulties in reaching to the exact goal state. Potentially stronger generative models in the future could alleviate this issue.\nWe also compare DINO-WM with a variant of AVDC, where the diffusion model is trained to generate the next observation Ot+1 conditioned on the current observation of and action at, rather than generating an entire sequence of observations at once conditioned on a text goal. As detailed in Appendix A.5, it can be seen that the action-conditioned diffusion model diverges from the ground truth observations over long-term predictions, making it insufficient for accurate task planning."}, {"title": "4.7 DECODING AND INTERPRETING THE LATENTS", "content": "Although DINO-WM operates in latent space and the observation model is not trained with pixel reconstruction objectives, training a decoder is still valuable for interpreting the model's predictions. We evaluate the image quality of predicted futures across all models and find that our approach out-performs others, even those whose encoders are trained with environment-specific reconstruction objectives."}, {"title": "5 CONCLUSION, LIMITATIONS & FUTURE WORK", "content": "In this work, we introduce DINO-WM, a simple yet effective technique for modeling visual dy-namics in latent space without the need for pixel-space reconstruction. We have demonstrated that DINO-WM captures environmental dynamics and generalizes to unseen configurations, indepen-dent of task specifications, enabling visual reasoning at test time and generating zero-shot solutions for downstream tasks through planning. DINO-WM takes a step toward bridging the gap between task-agnostic world modeling and reasoning and control, offering promising prospects for generic world models in real-world applications. For limitations, DINO-WM still relies on the availabil-ity of ground truth actions from agents, which may not always be feasible when training with vast video data from the internet. Additionally, while we currently plan in action space for downstream task solving, an extension of this work could involve developing a hierarchical structure that inte-grates high-level planning with low-level control policies to enable solving more fine-grained control tasks."}, {"title": "A APPENDIX", "content": "A.1 ENVIRONMENTS AND DATASET GENERATION\na) Point Maze: In this environment entroduced by Fu et al. (2021), the task is for a force-actuated 2-DoF ball in the catesian directions x and y to reach a target goal. The agent's dynamics incorporate physical properties such as velocity, acceleration, and inertia, making the movement realistic. We customize the environment by altering the maze configuration to test the model's generalization ability in unseen situations. We generate 2000 fully random trajectories to train our world models.\nb) Push-T: This environment introduced by Chi et al. (2024) features a pusher agent inter-acting with a T-shaped block. The goal is to guide both the agent and the T-block from a randomly initialized state to a known feasible target configuration within 25 steps. The task requires both the agent and the T to match the target locations. Unlike previous setups, the fixed green T no longer represents the target position for the T-block but serves purely as a visual anchor for reference. Success requires precise understanding of the contact-rich dynamics between the agent and the object, making it a challenging test for visuomotor con-trol and object manipulation. Additionally, we introduce variations by altering the shape and color of the object to assess the model's capability to adapt to novel tasks. We generate a dataset of 18500 samples replayed the original released expert trajectories with various level of noise and evaluate the model's performance across all different shapes to assess its adaptability.\nc) Wall: This custom 2D navigation environment features two rooms separated by a wall with a door. The agent's task is to navigate from a randomized starting location in one room to a goal in the other, passing through the door. We present a variant where wall and door positions are randomized, testing the model's generalization to novel configurations. For the fixed wall setting, we train on a fully random dataset of 2000 trajectories each with 50 time steps. For the variant with multiple training environment configurations, we generate 10240 random trajectories.\nd) Rope Manipulation: Introduced in Zhang et al. (2024), this task is simulated with Nvidia Flex Zhang et al. (2024) and consists of an XArm interacting with a soft rope placed on a tabletop. The objective is to move the rope from an arbitrary starting configuration to a goal configuration specified at inference time. For training, we generate a random dataset of 1000 trajectories of 20 time steps of random actions from random starting positions, while testing involves goal configurations set from varied initial positions, incorporating random variations in orientation and spatial displacement.\ne) Granular Manipulation: This environment uses the same simulation setup as Rope Ma-nipulation and involves manipulating about a hundred particles to form desired shapes. The training data consists of 1000 trajectories of 20 time steps of random actions starting from the same initial configuration, while testing is performed on specific goal shapes from di-verse starting positions, along with random variations in particle distribution, spacing, and orientation."}, {"title": "A.2 ENVIRONMENT FAMILIES FOR TESTING GENERALIZATION", "content": "1. WallRandom: Based on the Wall environment, but with randomized wall and door positions. At test time, the task requires navigating from a random starting position on one side of the wall to a random position on the other side, with non-overlapping wall and door positions seen during training.\n2. PushObj: Derived from the Push-T environment, where we introduce novel block shapes, including Tetris-like blocks and a \"+\" shape. We train the model with four shapes and evaluate on two unseen shapes. The task involves both the agent and object reaching target locations.\n3. GranularRandom: Derived from the Granular environment, where we initialize the scene with a different amount of particles. The task requires the robot to gather all particles to a square shape at a randomly sampled location. For this task, we directly take the models that are trained with a fixed amount of materials used in Section 4.3."}, {"title": "A.3 PRETRAINING FEATURES", "content": "a) R3M: A ResNet-18 model pre-trained on a wide range of real-world human manipulation videos Nair et al. (2022).\nb) ImageNet: A ResNet-18 model pre-trained on the ImageNet-1K dataset Russakovsky et al. (2015).\nc) DINO CLS: The pre-trained DINOv2 model provides two types of embeddings: Patch and CLS. The CLS embedding is a 1-dimensional vector that encapsulates the global information of an image."}, {"title": "A.4 PLANNING OPTIMIZATION", "content": "In this section, we detail the optimization procedures for planning in our experiments."}, {"title": "A.4.1 MODEL PREDICTIVE CONTROL WITH CROSS-ENTROPY METHOD", "content": "a) Given the current observation 00 and the goal observation og, both represented as RGB images, the observations are first encoded into latent states:\n$z0 = enc(00), zg = enc(og)$. (3)\nb) The planning objective is defined as the mean squared error (MSE) between the predicted latent state at the final timestep T and the goal latent state:\n$C = ||zT - zg||^2$, where $2t = p(2t\u22121, at\u22121), z0 = enc(00)$. (4)\nc) At each planning iteration, CEM samples a population of N action sequences, each of length T, from a distribution. The initial distribution is set to be Gaussian.\nd) For each sampled action sequence {a0, a1,..., aT\u22121}, the world model is used to predict the resulting trajectory in the latent space:\n$2t = P(2t-1, at\u22121)$, t = 1,...,T. (5)\nAnd the cost C is calculated for each trajectory.\ne) The top K action sequences with the lowest cost are selected, and the mean and covariance of the distribution are updated accordingly.\nf) A new set of N action sequences is sampled from the updated distribution, and the pro-cess repeats until success is achieved or after a fixed number of iterations that we set as hyperparameter.\ng) After the optimization process is done, the first k actions ao, ...ak is executed in the envi-ronment. The process then repeats at the next time step with the new observation."}, {"title": "A.4.2 GRADIENT DESCENT:", "content": "Since our world model is differentiable, we also consider an optimization approach using Gradient Descent (GD) which directly minimizes the cost by optimizing the actions through backpropagation.\na) We first encode the current observation 00 and goal observation og into latent spaces:\n$z0 = enc(00), zg = enc(og)$. (6)\nb) The objective remains the same as for CEM:\n$C = ||\u00c2T - zg||^2$, where $2t = p(2t\u22121, at-1), z0 = enc(00)$. (7)\nc) Using the gradients of the cost with respect to the action sequence {a0, a1,..., \u0430\u0442\u22121}, the actions are updated iteratively:\n$At \\frac{\\partial C}{\\partial a_t}$ = at - \u03b7 $\\frac{\\partial C}{\\partial a_t}$ t=0,..., T \u2013 1, (8)\nwhere n is the learning rate\nd) The process repeats until a fixed number of iteractions is reached, and we execute the first k actions ao, ..., ak in the enviornment, where k is a pre-determined hyperparameter."}, {"title": "A.4.3 PLANNING RESULTS", "content": "Here we present the full planning performance using various planning optimization methods. CEM denotes the setting where we use CEM to optimize a sequence of actions, and execute those actions in the environment without any correction or replan. Similarly, GD denotes optimizing with gradient decent and execute all planned actions at once in an open-loop way. MPC denotes allowing replan and receding horizon with CEM for optimization."}, {"title": "A.5 COMPARISON WITH ACTION-CONDITIONED GENERATIVE MODELS", "content": "We compare DINO-WM with a variant of AVDC, where the diffusion model is trained to gener-ate the next observation Ot+1 conditioned on the current observation of and action at, rather than generating an entire sequence of observations at once conditioned on a text goal. We then present openloop rollout results on validation trajectories using this action-conditioned diffusion model,"}, {"title": "A.6 HYPERPARAMETERS AND IMPLEMENTATION", "content": "We present the DINO-WM hyperparameters and relevant implementation repos below. We train the world models for all environments with the same hyperparameters."}, {"title": "A.7 ADDITIONAL PLANNING VISUALIZATIONS", "content": "We present additional visualizations for planning with DINO-WM. In this setting, all planning instances share the same initial observations but have different goal observations to demonstrate"}]}