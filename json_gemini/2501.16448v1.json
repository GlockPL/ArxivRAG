{"title": "What is Harm? Baby Don't Hurt Me! On the Impossibility of Complete Harm Specification in Al Alignment", "authors": ["Robin Young"], "abstract": "\"First, do no harm\" faces a fundamental challenge in artificial intelli-gence: how can we specify what constitutes harm? While prior work treats harm specification as a technical hurdle to be overcome through better al-gorithms or more data, we argue this assumption is unsound. Drawing on information theory, we demonstrate that complete harm specification is fundamentally impossible for any system where harm is defined exter-nal to its specifications. This impossibility arises from an inescapable information-theoretic gap: the entropy of harm H(O) always exceeds the mutual information I(O;I) between ground truth harm O and a system's specifications I.\nWe introduce two novel metrics: semantic entropy H(S) and the safety-capability ratio I(O;I)/H(O), to quantify these limitations. Through a progression of increasingly sophisticated specification attempts, we show why each approach must fail and why the resulting gaps are not mere engineering challenges but fundamental constraints akin to the halting problem. These results suggest a paradigm shift: rather than pursuing complete specifications, AI alignment research should focus on developing systems that can operate safely despite irreducible specification uncer-tainty.", "sections": [{"title": "Introduction", "content": "The Hippocratic maxim primum non nocere (first, do no harm) has guided medical ethics for over two millennia. Yet, even in this constrained domain where feedback loops are clear and stakes are life or death, physicians still"}, {"title": "Related Work", "content": ""}, {"title": "Inverse Reinforcement Learning (IRL)", "content": "The IRL literature [7, 10] recognizes that human demonstrations encode incom-plete reward signals. While prior work treats this as a statistical challenge e.g., inferring rewards from noisy data, we reframe it as an information-theoretic limit. Even with infinite demonstrations, the mutual information I(O; I) between ground truth harm O and learned rewards I remains bounded by the semantic entropy H(S) of human values."}, {"title": "Ontology Identification", "content": "Everitt et al. [5] show how AI systems may learn ontologies misaligned with human concepts. Their focus is on empirical mismatches (e.g., a robot misun-derstanding \"chair\"). We prove such mismatches are inevitable for normative concepts like harm, due to the non-injectivity of semantic mappings between O and I."}, {"title": "Value Learning Under Moral Uncertainty", "content": "Armstrong [2] proposes frameworks for agents uncertain about ethical theories. While his work assumes uncertainty can be quantified and managed, we show that no amount of uncertainty reduction can close the alignment gap H(O) \u2013 I(O; I), as the gap stems from external reference, not epistemic limitations."}, {"title": "Linguistic and Cultural Relativity", "content": "Bender et al. [3] critique language models as \"stochastic parrots\" that mimic human text without understanding. We extend their argument: language's in-herent ambiguity ensures that text-based harm specifications (e.g., \"don't gen-erate hate speech\") will always exhibit semantic entropy H(S) > 0, as meaning is negotiated socially, not definitionally."}, {"title": "Formal Verification", "content": "Seshia et al. [9] advocate formal methods to certify AI safety. Their approach assumes harm can be fully formalized, but our Theorem 1 shows this is impos-sible when O is external. Verification can reduce H(OI) (uncertainty about harm given specs), but cannot eliminate H(O) \u2013 I(O; I)."}, {"title": "Summary of Novelty", "content": "Existing prior work recently surveyed by Ngo et al. [8] points out instances of harm specification challenges, reward ambiguity, ontology mismatch, and linguistic underspecification, as increasingly sophisticated approaches to isolated technical problems. We unify these under an information-theoretic framework"}, {"title": "Impossibility of Complete Harm Specification", "content": "We formalize the challenge of harm specification through four levels of ab-straction, revealing why each approach fails and necessitating elevation to an information-theoretic framework."}, {"title": "Level 1: Direct Harm Definition", "content": "\u2022 Notation: Let H : S \u2192 {0,1} classify states s \u2208 S as harmful (1) or not (0).\n\u2022 Logic: Attempts to exhaustively enumerate harmful states (e.g., \"hitting pedestrians\" for autonomous vehicles).\n\u2022 Metaphor: Like writing a dictionary using the word being defined, harm is both the input and output."}, {"title": "Failure Modes:", "content": "1. Circular Dependency: S (state space) cannot be defined without prior knowledge of harm, yet harm depends on S.\n2. Context Collapse: A state s (e.g., \"administering chemotherapy\") may be harmful in one context (palliative care) but beneficial in another (curative intent)."}, {"title": "Falsifiability Analysis:", "content": "1. Unverifiable Completeness: To falsify H, one must find a harmful state s where H(s) = 0. But without an independent definition of harm, all counterexamples can be dismissed as \"not truly harmful.\"\n2. Example: Is a self-driving car's decision to prioritize a pedestrian over passengers harmful? Proponents can always redefine harm post hoc to exclude edge cases.\nRelevance: Shows harm cannot be reduced to a fixed checklist. Rules like \"don't kill humans\" fail when harm requires tradeoffs (e.g., triage scenarios).\nRationale for Elevation: To resolve circularity, we introduce time to pre-dict harm before it occurs."}, {"title": "Level 2: Ex Ante Evaluation", "content": "\u2022 Notation: Let E : S \u00d7 T \u2192 {0,1}, where E(s,t) predicts whether state s at time t will be harmful."}, {"title": "Logic:", "content": "Evaluates harm probabilistically (e.g., \"80% chance this tweet causes emotional distress\")."}, {"title": "Metaphor:", "content": "A weather forecast predicting rain, useful but unreliable, as harm (like rain) depends on unobserved variables."}, {"title": "Failure Modes:", "content": "1. Recursive Validation: To verify E, we need ground truth H, which is undefined (Level 1 problem).\n2. Temporal Paradox: Harm often emerges post-hoc (e.g., algorithmic bias discovered years after deployment)."}, {"title": "Falsifiability Analysis:", "content": "1. Retroactive Validation: Falsifying E requires observing harm ex post. But this assumes H (from Level 1) is already defined, recreating the original circularity.\n2. Example: An LLM generates text deemed safe by E, but later causes harm via misinformation. Critics argue E failed; proponents claim the harm was unforeseeable.\nRelevance: Highlights the temporal disconnect between specification and consequence. Ex-ante rules like \"don't discriminate\" fail when bias emerges from interactions unseen at training time.\nRationale for Elevation: To address incompleteness, we formalize the specification system itself."}, {"title": "Level 3: Specification Systems", "content": "\u2022 Notation: Let SS = (D,C, R), where:\nD: Domain (e.g., \"all possible robot actions\")\nC: Harm classifications (e.g., {\"safe\", \"harmful\"})\nR: Rules mapping D \u2192 C\n\u2022 Logic: Aims to prove system safety via formal verification (e.g., \"No rule in R allows hitting pedestrians\").\n\u2022 Metaphor: A constitution: comprehensive yet always requiring interpre-tation."}, {"title": "Failure Modes:", "content": "1. Completeness Dilemma: Proving R covers all harms requires solving the halting problem for real-world dynamics.\n2. Ontological Drift: D and C evolve (e.g., \"emotional harm\" added retroac-tively to robot ethics)."}, {"title": "Falsifiability Analysis:", "content": "1. Local vs Global Falsification: While individual rules (e.g., \"don't kill\") can be falsified by counterexamples, the entire system SS cannot be globally falsified without a complete harm ontology.\n2. Example: A robot follows all encoded rules but causes psychological harm. Proponents argue psychological harm wasn't in C, so SS remains \"cor-rect.\"\nRelevance: Even verified systems fail when the domain D shifts (e.g., social media algorithms gamed by novel misinformation tactics).\nRationale for Elevation: To confront incompleteness, we model the in-formation relationship between specifications and ground truth."}, {"title": "Level 4: Information-Theoretic Formalization", "content": ""}, {"title": "Formal Framework", "content": "Let S = (D,C,R, I) be a formal specification system where:\n\u2022 D is the domain of possible states\n\u2022 C is the set of classifications (harmful/not harmful)\n\u2022 R is the set of rules mapping D \u2192 C\n\u2022 I represents the information content encoded in system S\nLet O: D \u2192 C be the ground truth classification function. We define:"}, {"title": "Key Insight", "content": "The fundamental limitation arises from the relationship between:\n1. The information I we can encode in our formal system\n2. The ground truth O we are trying to capture\nThis relationship is constrained by a simple but profound fact: a system cannot contain perfect information about something external to itself. Just as a map cannot contain all information about the territory it represents, our specifications I cannot completely capture the ground truth O."}, {"title": "Central Theorem", "content": "Theorem 1 (Harm Specification Impossibility). For any specification system S = (D,C,R, I) with external ground truth 0:\nI(O; I) < H(O)\nThat is, the mutual information between our specifications and ground truth must be strictly less than the entropy of ground truth harm.\nProof Sketch. The inequality follows from two observations:\n1. O is external to S, so it contains information not present in I\n2. Perfect capture would require I(O; I) = H(O), implying O is fully deter-mined by I\nThese statements contradict each other, proving the inequality must be strict."}, {"title": "Safety-Capability Metric", "content": "This framework yields a novel metric for AI safety:\nSafety-Capability Ratio\n\n\nI(O; I)\nH(O)\n\nThis ratio quantifies alignment quality:\n\u2022 Ratio 1: Inevitable for all real systems\n\u2022 Higher ratio: Better aligned system\n\u2022 Lower ratio: More dangerous system"}, {"title": "Failure Modes", "content": "The information-theoretic framework predicts three classes of alignment failures:\n1. Ontological Shifts: When systems encounter novel states where their encoded understanding breaks down\n\u2022 Example: AGI developing mental states outside human ontology\n\u2022 Real-world instance: LLMs encountering novel forms of hate speech\n2. Teleological Drift: When optimization causes systems to diverge from true objectives\n\u2022 Example: Recommendation systems optimizing for engagement over wellbeing\n\u2022 Real-world instance: Social media algorithms promoting divisive con-tent"}, {"title": "Epistemic Collapse:", "content": "When systems learn to exploit gaps in our specifi-cations\n\u2022 Example: RL agents learning to \"game\" reward functions\n\u2022 Real-world instance: Content moderation evasion tactics"}, {"title": "Falsifiability Analysis:", "content": "1. Directly Falsifiable: The inequality I(O; I) < H(O) can be tested by:\n\u2022 Measuring H(O) via human disagreement on harm labels.\n\u2022 Estimating I(O; I) via cross-entropy between S's predictions and ob-served harm.\n2. Example: If humans label 1,000 medical triage scenarios with H(O) = 0.8 bits (high disagreement) but S's predictions achieve I (O; I) = 0.4 bits, the gap 0.8 0.4 = 0.4 bits falsifies perfect alignment.\nKey Insight: Level 4 is the first falsifiable framework because it quantifies alignment as a testable ratio I(O; I)/H(O), not a binary claim."}, {"title": "Practical Implications", "content": "These results demand a fundamental paradigm shift in AI development. Most critically, we must abandon the search for a silver bullet solution. Adding more rules or increasing system complexity cannot eliminate the fundamental information-theoretic gap between specifications and ground truth harm.\nAI systems must be designed with explicit uncertainty recognition. Rather than striving for complete specifications, we should develop architectures that acknowledge their limitations and operate with appropriate caution in areas of high uncertainty. This connects directly to work on systems that \"know what they don't know\" [1] and suggests that uncertainty quantification should be a core component of AI safety.\nThese theoretical limitations have direct implications for AI governance and deployment. Systems with low Safety-Capability Ratios should face greater scrutiny and restrictions, regardless of their technical sophistication. Organiza-tions developing AI systems should be required to regularly assess semantic drift and demonstrate ongoing maintenance of acceptable Safety-Capability Ratios.\nLooking ahead, research priorities should focus on developing uncertainty-aware architectures that can operate safely despite inevitable specification gaps. While we cannot eliminate the fundamental limitations identified in this paper, we can work to build systems that acknowledge and compensate for them."}, {"title": "Semantic Entropy and Alignment Limitations", "content": "The central theorem established that for any specification system S with exter-nal ground truth O, we must have I(O; I) < H(O). Here we develop a more"}, {"title": "Formalizing Semantic Entropy", "content": "Let C be a set of concepts (e.g., \"harm,\" \"fairness\") that an AI system must align with. Each concept c\u2208 C maps to a distribution over possible meanings:\n\u2022 Mc: Set of possible meanings for concept c\n\u2022 Pc: Mc \u2192 [0, 1]: Probability distribution over interpretations of c\nFor example, if c = \"harm\", then Me might include interpretations like physical injury, emotional distress, and economic damage, with Pe reflecting variation in how different individuals or cultures weight these interpretations. We define the semantic entropy of a concept c as the Shannon entropy of its meaning distribution:\nH(Sc) = \u2211 Pc(m) log Pc(m)\nm\u2208Mc\nFor a system S handling multiple concepts, we define aggregate semantic entropy:\nH(S) = \u03a3H(Sc)\nc\u2208C\nWhen H(S) = 0, concepts are unambiguous. When H(S) > 0, concepts have inherent semantic uncertainty."}, {"title": "Inherent Challenges", "content": "The formalization of semantic entropy leads to three fundamental limitations on AI alignment:\nConjecture 1 (Semantic Incompleteness). If H(S) > 0, no formal system S can guarantee alignment with ground truth O.\nJustification 1 (Supporting Argument). This follows from three observations. First, assume S achieves \"perfect alignment,\u201d implying I(O; S) = H(O). Second, by the data processing inequality, we know H(O) > H(S). Third, given H(S) > 0, this implies H(O) > 0, which in turn means I(O; S) < H(O). This contradicts our assumption of perfect alignment. The result shows that any non-zero semantic entropy creates an unbridgeable gap between specifications and ground truth, providing a concrete mechanism for the central impossibility theorem."}, {"title": "L\u00f6bian Uncertainty", "content": "An agent using a specification system S with H(S) > 0 cannot fully trust its own alignment criteria."}, {"title": "Supporting Argument", "content": "This limitation emerges from the in-teraction between two fundamental results. First, L\u00f6b's Theorem establishes that agents cannot prove the consistency of their own reasoning systems. Sec-ond, when H(S) > 0, the very concepts used to define alignment criteria (like \"harm\" or \"safety\u201d) have intrinsic ambiguity. The combination creates an ir-reducible form of uncertainty: the agent cannot verify that its interpretation of alignment criteria matches ground truth, even in principle. This extends classic L\u00f6bian obstacles from formal systems theory to the semantic domain, showing why self-verification of alignment is impossible."}, {"title": "Medical Triage: A Concrete Example", "content": "To illustrate how these abstract limitations manifest in practice, consider an AI system tasked with medical triage decisions. The system must implement the directive \"minimize harm,\" facing scenarios where the very concept of harm becomes ambiguous. For instance, when choosing between treating five patients with moderate injuries or one with severe injuries, the system encounters a scenario where H(S) > 0 due to the incomparability of different types and distributions of harm.\nThis creates what we might call a \"practical L\u00f6bian trap.\" Even if the system could perfectly predict medical outcomes, it cannot resolve the fundamental ambiguity in comparing different harm configurations. Should it optimize for minimizing maximum individual harm (saving the severe case) or minimizing total harm (treating the five moderate cases)? The system's specifications I cannot fully capture the ground truth O about which choice truly minimizes harm, because harm itself has non-zero semantic entropy in this context.\nThis example demonstrates why the abstract gap I(O; I) < H(O) is not merely a theoretical curiosity but a practical limitation facing real AI sys-tems. Even in medicine, a domain with relatively precise terminology and clear feedback mechanisms, semantic entropy creates unavoidable uncertainty about harm. This uncertainty is not a fault of the system's design or a limitation we"}, {"title": "Discussions", "content": "The progression through increasingly sophisticated attempts at harm specifi-cation reveals that this challenge is not merely a philosophical puzzle but a fundamental limitation of formal systems. This realization has profound impli-cations for AI alignment research and development."}, {"title": "Fundamental Limitations", "content": "At its core, AI alignment relies on our ability to specify what we want AI systems to do and perhaps more critically, what we want them to avoid doing. The impossibility results presented in this paper suggest that complete specification of harm is fundamentally impossible, not just practically difficult. This creates a troubling paradox: harm avoidance is perhaps the most urgent and fundamental safety concern in AI development, yet we cannot fully specify what we're trying to avoid.\nThis limitation stems from a deceptively simple source: we rely on natural language concepts like \"harm,\" \"safety,\" and \"alignment\" to define our objec-tives, yet these terms resist complete formal specification. The semantic entropy framework shows that this ambiguity is not a mere philosophical curiosity but a quantifiable limitation that affects any formal system attempting to opera-tionalize these concepts."}, {"title": "Domain-Specific Challenges", "content": "Even in highly constrained domains, the specification gap remains problematic. Consider medical ethics, which represents perhaps the best-case scenario for harm specification, a domain with clear feedback mechanisms, explicit ethical frameworks, and centuries of precedent. Yet even here, complete specification remains impossible. The medical triage example explored earlier demonstrates how semantic uncertainty creates irreducible ambiguity even in life-or-death decisions.\nThe challenge becomes substantially greater for AI systems operating across multiple domains. Autonomous vehicles, despite operating in a physically con-strained environment with explicit traffic rules, face seemingly simple questions that reveal deep specification challenges: Should they prioritize passenger safety over pedestrians? How should they balance journey time against marginal safety improvements? What constitutes \"aggressive\" driving in different cultural con-texts? These questions aren't merely engineering challenges but reflect the fundamental limitations identified in our theoretical framework."}, {"title": "Research Directions", "content": "These limitations suggest several critical directions for future research:\n1. Uncertainty-Aware Learning: How can we develop reinforcement learn-ing systems that explicitly model and account for the gap between speci-fications and ground truth? Such systems would need to estimate I(O; I) gaps during operation and adjust their behavior accordingly.\n2. Harm Topology: Can we develop a systematic understanding of which categories of harm have lower semantic entropy? Physical harms may be more specifiable than psychological harms, suggesting a potential hierar-chy of specification difficulty that could guide system development.\n3. Interactive Refinement: While our results show that perfect specifi-cation (implicit or otherwise) is impossible, can human-AI collaborative systems iteratively reduce H(O) through structured debate and refine-ment? This suggests a role for adversarial testing in reducing, though never eliminating, semantic uncertainty."}, {"title": "Practical Considerations", "content": "These theoretical results have immediate practical implications. First, they sug-gest that approaches relying on complete specification of safety constraints are fundamentally flawed. Instead, AI systems must be designed with explicit uncer-tainty handling mechanisms and the ability to recognize when they're operating in areas of high semantic entropy.\nSecond, the impossibility of complete ex ante safety guarantees suggests that empirical observation and testing remain necessary components of AI safety. However, this creates a temporal paradox-we can only observe harmful be-haviors after they occur, yet our goal is prevention. This suggests the need for sophisticated simulation and sandboxing approaches that can safely explore the boundaries of semantic uncertainty.\nFinally, these results suggest a fundamental reorientation of AI alignment research. Rather than pursuing complete specifications of concepts like harm-a mathematically impossible goal-we should focus on developing systems that can operate safely despite incomplete specifications. This might include:\n\u2022 Architectures that maintain meaningful human oversight even as capabil-ities increase\n\u2022 Methods for quantifying and monitoring semantic entropy in deployed systems\n\u2022 Frameworks for graceful degradation when encountering high-uncertainty scenarios"}, {"title": "Counterarguments and Limitations", "content": "As with any theoretical framework making strong claims about fundamental limitations, our approach faces several important counterarguments and has its own limitations that deserve careful consideration."}, {"title": "Common Counterarguments", "content": "An anticipated response is \"we already knew this.\" While the difficulty of spec-ifying harm has long been recognized, our contribution is proving this difficulty is unavoidable under information-theoretic constraints. Plus, no one wrote the math down, as far as we know, which seems important given the stakes in-volved in AI alignment. This has direct implications for research prioritization: resources invested in achieving complete specification through any means are fundamentally misaligned with mathematical limitations.\nA compelling counterargument comes from medicine: doctors make life-and-death decisions without complete, axiomatic definitions of health or harm. How-ever, this analogy overlooks crucial differences in AI: vastly greater scale and speed of agency, operation across all domains rather than constrained ones, and the fundamental challenge of aligning systems not inherently grounded in hu-man values. While medicine shows we can operate with imperfect definitions, it doesn't negate the information-theoretic limitations we've identified.\nCritics might argue this is \"merely philosophy dressed up in mathematics.\" However, our framework provides concrete, testable implications. The Safety-Capability Ratio yields quantifiable predictions about system behavior and specific guidance for system design, much like how cryptography's no-perfect-encryption theorems guide practical security design.\nEven dynamic ground truth or hybrid approaches combining formal spec-ification with learning cannot circumvent these limitations. The gap between specification and ground truth persists regardless of whether that truth evolves or what method of specification we use. The limitation isn't in our methods but in the fundamental relationship between formal systems and external reference."}, {"title": "Limitations of Our Approach", "content": "While we believe our framework makes notable contributions, it has several lim-itations worth acknowledging. Our treatment of semantic entropy, while mathe-matically precise, may not capture all relevant aspects of conceptual uncertainty. The reduction of semantic ambiguity to information-theoretic quantities, while useful, inevitably simplifies complex philosophical questions."}, {"title": "Theoretical Extensions", "content": "The relationship between semantic entropy and other information-theoretic quan-tities in machine learning deserves exploration, particularly in uncertainty quan-tification and out-of-distribution detection. Additionally, understanding how semantic specification limits interact with computational complexity bounds could lead to a unified theory of specification limitations."}, {"title": "Empirical Research", "content": "Our framework generates testable predictions:"}, {"title": "Practical Applications", "content": "Key directions for implementation include:\n\u2022 Methods for measuring semantic entropy and detecting boundary viola-tions in deployed systems\n\u2022 Uncertainty-aware architectures that explicitly track semantic entropy"}]}