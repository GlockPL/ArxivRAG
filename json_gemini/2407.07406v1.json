{"title": "Weakly-supervised Medical Image Segmentation with Gaze Annotations", "authors": ["Yuan Zhong", "Chenhui Tang", "Yumeng Yang", "Ruoxi Qi", "Kang Zhou", "Yuqi Gong", "Pheng Ann Heng", "Janet H. Hsiao", "Qi Dou"], "abstract": "Eye gaze that reveals human observational patterns has increasingly been incorporated into solutions for vision tasks. Despite recent explorations on leveraging gaze to aid deep networks, few studies exploit gaze as an efficient annotation approach for medical image segmentation which typically entails heavy annotating costs. In this paper, we propose to collect dense weak supervision for medical image segmentation with a gaze annotation scheme. To train with gaze, we propose a multi-level framework that trains multiple networks from discriminative human attention, simulated with a set of pseudo-masks derived by applying hierarchical thresholds on gaze heatmaps. Furthermore, to mitigate gaze noise, a cross-level consistency is exploited to regularize overfitting noisy labels, steering models toward clean patterns learned by peer networks. The proposed method is validated on two public medical datasets of polyp and prostate segmentation tasks. We contribute a high-quality gaze dataset entitled GazeMedSeg as an extension to the popular medical segmentation datasets. To the best of our knowledge, this is the first gaze dataset for medical image segmentation. Our experiments demonstrate that gaze annotation outperforms previous label-efficient annotation schemes in terms of both performance and annotation time. Our collected gaze data and code are available at:\nhttps://github.com/med-air/GazeMedSeg.", "sections": [{"title": "1 Introduction", "content": "Recent studies have witnessed increasing interest in incorporating human factors into deep learning applications [13,24]. Eye tracking data, serving as a popular tool reflecting the underlying cognitive processes [27], has stood out as a promising and accessible media for human-AI interaction. Previous works commonly utilize gaze as auxiliary information to guide deep networks [9,15,22,23], with recent explorations of employing gaze as the sole supervision signal for label-efficient classification [19]. However, leveraging gaze for supervising image segmentation models remains under-explored yet valuable, since it alleviates"}, {"title": "2 Gaze Annotation Collection", "content": "Gaze annotation scheme. We develop the eye-tracking program utilizing SR Research Experiment Builder platform. At the beginning of gaze annotation,\neach annotator goes through a 9-point gaze calibration process. Our gaze annotation collection consists of two stages. When presented with an image, the\nannotator (with eye-tracker) first roughly scans the image and locates the target\nobjects. Following that, the annotator is requested to scan the objects thoroughly. Typically, participants start from central areas and then move on to the\nboundaries, ensuring that all parts of the target are covered. This step avoids\npartial activation by encouraging annotators to pay more attention to the target\nobjects. Therefore, the noise will be relatively weakened when normalizing the\nheatmap. After finishing the annotating, a key is pressed to switch to the next\nimage. More details on the eye-tracking settings can be found in the Appendix.\nGazeMedSeg dataset. Our collected GazeMedSeg dataset includes gaze annotations for two public medical segmentation datasets. We use the Kvasir-SEG [10] dataset for polyp segmentation from gastrointestinal images, and the\nNCI-ISBI [2] dataset for prostate segmentation from T2-weighted MR images.\nThe Kvasir-SEG dataset includes 900 training and 100 testing images and the\nNCI-ISBI dataset includes 60 training and 10 testing volumes, where we retain\nslices containing prostate and obtain 789 training and 117 testing images. One\nannotator finishes the annotation of all images in the datasets, and we use it in\nour major experiments. We also invite two additional annotators to annotate a\nsubset for sensitivity studies (Sec. 4.2). All annotators are experienced in medical\nimaging and are well-trained for eye-tracking trials."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Multi-level Learning from Discriminative Attention", "content": "The original gaze data is a series of gaze positions collected at a certain frequency.\nGiven the gaze positions, the attention heatmaps are obtained by convolving an\nisotropic Gaussian over them. We further apply dense conditional random fields\n(CRF) [11] to enhance the initial heatmaps and generate pseudo-masks. However,\nthe quality of pseudo-masks varies significantly with distinct thresholds (Fig. 1).\nIt is hard to balance the over-activation and under-activation of foregrounds via a\nfixed global threshold, because human subjectively pay discriminative attention\nto target objects. For instance, annotators may focus on the most discriminative\npart while only roughly scanning ambiguous parts of an object, which may be\nneglected with a fixed global threshold.\nOur idea is to train m deep networks simultaneously supervised by pseudo-\nmasks generated from m different thresholds (Fig. 2 (a)), simulating multi-level"}, {"title": "3.2 Cross-level Consistency for Noise Compensation", "content": "Another essential aspect of gaze-supervised segmentation is the inevitable noise\nin the pseudo-masks. The noises can be introduced in the process of human\ninterpretation, gaze estimation by eye-trackers, heatmap generation, etc. We\nobserve a memorization effect [1] of deep networks when training with gaze data\non the medical image segmentation task. As shown in Fig. 2 (b), the model\ncaptures clean patterns on incorrectly annotated pixels of pseudo-masks at the\nbeginning of training, but eventually overfits on noisy labels.\nIn the multi-level framework, though the networks are diverged with distinct\nsupervisions, they share the same input and various pseudo-masks supervise\ndifferent representations of the shared gaze data. Based on the assumption that\nnetworks learn clean patterns at the beginning of the memorization, for each\nnetwork, we propose to exploit the knowledge learned by peer networks of other\nlevels to compensate for the noisy label via a consistency term.\nTo ensure noise-robust consistency, we first use a non-parametric local pixel\npropagation (LPP) module to filter the feature of each pixel by propagating\nthe features of surrounding pixels in local regions inspired by recent works [5,7]\nproving noise-robust feature correspondence distillation. Given the feature map\n\u0444, for each pixel feature \u03c6p, the LPP module computes the transform \u03c6\u0302p as:\n\u03c6\u0302p := \u2211q\u2208PP softmax(max{cos(\u03c6p, \u03c6q), 0})\u22c5\u03c6q, (1)\nwhere cos denotes the cosine similarity, Pp denotes the set of neighboring pixels\n(e.g., a 3 x 3 region with dilation 1) of pixel p. This refinement has a feature\ndenoising/smoothing effect that reduces the outliers and enhances the features\nwith local context by introducing spatial smoothness which encourages spatially\nclose pixels to be similar. Given an arbitrary pair of levels i,j\u2208 Z+, where\ni, j \u2264 m and i \u2260 j, the consistency loss applied on the i-th level maximizes dot-\nproduct between the propagated prediction p\u0302i of i-th level and non-propagated\nprediction pj of j-th level, i.e., Lcons := \u2212p\u0302i \u22c5pj. Notably, we have pi = gi(\u03c6\u0302i) for\nthe propagated prediciton and pj = gj(\u03c6j) for the non-propagated one, where g\ndenotes the shallow classifier."}, {"title": "3.3 Overall Optimization of Gaze Supervision", "content": "The overall loss for the i-th level is the combination of the supervision term and\nthe consistency term over all other peer-level j:\nLi = LCE + \u03bbm\u22121m \u2211j=1,j\u2260iLcons (2)\nwhere LCE is the cross-entropy loss and can be replaced by any other segmenta-\ntion loss such as dice loss, and \u03bb is empirically set to 3. Note that when optimizing\nthe network of the i-th level, the parameters of all other networks are frozen.\nThe key to understanding the overall optimization process lies in the trade-off\nof cross-entropy and consistency terms. Intuitively, LCE trains a set of divergent\nnetworks utilizing multi-level pseudo-masks. However, this term tends to vanish\nafter the early learning stage and each network starts to overfit on the respective\nnoisy label. The consistency term Lcons compensates for it, implicitly forcing\nnetworks to continue learning from clean patterns learned by networks of other\nlevels. The mechanism can be viewed as pushing networks to struggle to find\na balance between divergence and consistency, in which the hyper-parameter \u03bb\ncontrols this balance. It is worth noting that both divergence and consistency\nare equally essential for optimization. The consistency term ensures robustness\nto noise and the cross-entropy term expands and diversifies clean knowledge\nlearned and prevents collapsing into a single network."}, {"title": "4 Experiments", "content": "We validate the proposed gaze annotation scheme on the aforementioned Kvasir-SEG [10] dataset for polyp segmentation and NCI-ISBI [2] dataset for prostate\nsegmentation. For all datasets, we only utilize weak annotations for training and\nreport performance on the testing set. We train a 2D UNet [18] from scratch for\n15k iterations with a NVIDIA A40 GPU. We use Adam optimizer with batch\nsize 8 and learning rate 4e-4. More details on the training recipe can be found\nin the Appendix."}, {"title": "4.1 Comparison Among Label-efficient Annotation Schemes", "content": "Comparison with state-of-the-art weakly-supervised methods. We com-\npare the new gaze-based annotation scheme with full mask supervision and other\nstate-of-the-art weakly-supervised methods using different sparse annotations in-\ncluding box, point, and scribble on two datasets. To compare annotation time,\nwe also invite the same annotator to annotate a randomly sampled subset of\nthe Kvasir-SEG dataset containing 100 images using other annotation schemes\nand report the annotation time in Table 1. Note that we annotate the bounding\nbox using extreme points clicking [16], and annotate points using the scheme\nsuggested by [4]. The estimated time is close to that reported in the litera-\nture [4,12,16,21], where the narrow gaps may come from the difference in the"}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we propose to train deep networks with gaze annotations effi-\nciently collected using a new gaze annotation scheme for medical image segmen-\ntation. The proposed method can be seamlessly integrated into standard training\npipelines. The results show that gaze annotation achieves a sweet performance\nand annotation time trade-off compared to other annotation forms.\nOur explorations give rise to several potential directions for future work: (1)\nWhile we expect gaze supervision to be broadly applicable to medical applica-\ntions, the multiplied complexities in the training and especially the inference"}, {"title": "A. Gaze Data Collection", "content": ""}, {"title": "B. Implementation Details", "content": ""}]}