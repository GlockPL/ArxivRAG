{"title": "Weakly-supervised Medical Image Segmentation with Gaze Annotations", "authors": ["Yuan Zhong", "Chenhui Tang", "Yumeng Yang", "Ruoxi Qi", "Kang Zhou", "Yuqi Gong", "Pheng Ann Heng", "Janet H. Hsiao", "Qi Dou"], "abstract": "Eye gaze that reveals human observational patterns has increasingly been incorporated into solutions for vision tasks. Despite recent explorations on leveraging gaze to aid deep networks, few studies exploit gaze as an efficient annotation approach for medical image segmentation which typically entails heavy annotating costs. In this paper, we propose to collect dense weak supervision for medical image segmentation with a gaze annotation scheme. To train with gaze, we propose a multi-level framework that trains multiple networks from discriminative human attention, simulated with a set of pseudo-masks derived by applying hierarchical thresholds on gaze heatmaps. Furthermore, to mitigate gaze noise, a cross-level consistency is exploited to regularize overfitting noisy labels, steering models toward clean patterns learned by peer networks. The proposed method is validated on two public medical datasets of polyp and prostate segmentation tasks. We contribute a high-quality gaze dataset entitled GazeMedSeg as an extension to the popular medical segmentation datasets. To the best of our knowledge, this is the first gaze dataset for medical image segmentation. Our experiments demonstrate that gaze annotation outperforms previous label-efficient annotation schemes in terms of both performance and annotation time.", "sections": [{"title": "1 Introduction", "content": "Recent studies have witnessed increasing interest in incorporating human factors into deep learning applications [13,24]. Eye tracking data, serving as a popular tool reflecting the underlying cognitive processes [27], has stood out as a promising and accessible media for human-AI interaction. Previous works commonly utilize gaze as auxiliary information to guide deep networks [9,15,22,23], with recent explorations of employing gaze as the sole supervision signal for label-efficient classification [19]. However, leveraging gaze for supervising image segmentation models remains under-explored yet valuable, since it alleviates annotators' workload by alleviating the need for labor-intensive pixel-wise annotation. Unlike existing label-efficient annotation schemes that provide sparse supervision with bounding boxes [6,20], points [4] or scribbles [25], gaze data yield dense pixel-wise supervision signals, which is crucial for medical images featuring ambiguous boundaries and low contrast. These motivate us to investigate the potential of gaze-supervised medical image segmentation.\nA straightforward way to gaze supervision is training with pseudo-masks generated by binarizing gaze heatmaps with a fixed threshold, where the dense gaze heatmaps contain continuous values indicating the degree of observational attention. A similar approach is widely employed in image-level semantic segmentation [3,26] for binarizing class activation maps (CAMs) [29]. In practice, this approach yields suboptimal performance for gaze supervision due to the distinct characteristics of gaze data as discriminative and noisy. Firstly, the quality of pseudo-masks is sensitive to the selection of threshold (Fig. 1). And it is unreasonable to rive precise object boundaries with a global threshold over all images since human annotators usually pay discriminative attention even on different parts of a single object. Secondly, the error in eye tracking and human subjectivity makes gaze data a noisy supervision signal for segmentation. For example, the annotator may check every suspicious area when annotating the targets, thus some noisy gaze will be left. Current noise-robust approaches are based on the symmetric or asymmetric assumptions of simulated noise and design robust loss functions [8,28] or regularization [14]. For correlated real-world gaze noise, however, the assumptions on simulated noise do not necessarily hold.\nThe key to robust gaze supervision lies in the unity and consideration of the aforementioned two characteristics of gaze data. Inspired by multi-expert models [17] benefiting from comprehensively integrating knowledge from multiple experts, we propose to fuse multiple diverged networks learning from multi-level human attention, simulated by applying a set of hierarchical thresholds on gaze heatmaps. These networks are designed to learn heterogeneous knowledge from discriminative human attention. Moreover, to mitigate gaze noise, we exploit the clean knowledge learned by peer networks of different levels to compensate for overfitting gaze noise with analysis on the memorization effect of deep networks.\nIn this paper, we propose a new gaze annotation scheme that collects dense annotation in an annotator-friendly and efficient manner for segmentation tasks. Utilizing the scheme, we introduce the gaze dataset GazeMedSeg, which extends the Kvasir-SEG [10] and NCI-ISBI [2] datasets with gaze data of multiple annotators. To train with gaze, we propose a multi-level approach that trains multiple divergent deep networks to ensemble information from different levels of human discriminative attention. In addition, a cross-level consistency regularization term over predictions smoothed by a local pixel propagation module is exploited to compensate for overfitting on noisy gaze labels. The advantage of the proposed neat approach is in its ability to seamlessly fit into standard training pipelines with no changes to model architectures. In experiments, we validate gaze annotation on polyp and prostate segmentation tasks using our GazeMed-Seg dataset. Compared to the existing label-efficient annotation schemes, gaze supervision narrows the gaps with full supervision and consistently boosts performance by over 2.0% in Dice while being 15.4% faster to annotate, striking a sweet trade-off between performance and annotation time."}, {"title": "2 Gaze Annotation Collection", "content": "Gaze annotation scheme. We develop the eye-tracking program utilizing SR Research Experiment Builder platform. At the beginning of gaze annotation, each annotator goes through a 9-point gaze calibration process. Our gaze annotation collection consists of two stages. When presented with an image, the annotator (with eye-tracker) first roughly scans the image and locates the target objects. Following that, the annotator is requested to scan the objects thoroughly. Typically, participants start from central areas and then move on to the boundaries, ensuring that all parts of the target are covered. This step avoids partial activation by encouraging annotators to pay more attention to the target objects. Therefore, the noise will be relatively weakened when normalizing the heatmap. After finishing the annotating, a key is pressed to switch to the next image. More details on the eye-tracking settings can be found in the Appendix.\nGazeMedSeg dataset. Our collected GazeMedSeg dataset includes gaze annotations for two public medical segmentation datasets. We use the Kvasir-SEG [10] dataset for polyp segmentation from gastrointestinal images, and the NCI-ISBI [2] dataset for prostate segmentation from T2-weighted MR images. The Kvasir-SEG dataset includes 900 training and 100 testing images and the NCI-ISBI dataset includes 60 training and 10 testing volumes, where we retain slices containing prostate and obtain 789 training and 117 testing images. One annotator finishes the annotation of all images in the datasets, and we use it in our major experiments. We also invite two additional annotators to annotate a subset for sensitivity studies (Sec. 4.2). All annotators are experienced in medical imaging and are well-trained for eye-tracking trials."}, {"title": "3 Methodology", "content": "The original gaze data is a series of gaze positions collected at a certain frequency. Given the gaze positions, the attention heatmaps are obtained by convolving an isotropic Gaussian over them. We further apply dense conditional random fields (CRF) [11] to enhance the initial heatmaps and generate pseudo-masks. However, the quality of pseudo-masks varies significantly with distinct thresholds (Fig. 1). It is hard to balance the over-activation and under-activation of foregrounds via a fixed global threshold, because human subjectively pay discriminative attention to target objects. For instance, annotators may focus on the most discriminative part while only roughly scanning ambiguous parts of an object, which may be neglected with a fixed global threshold.\nOur idea is to train m deep networks simultaneously supervised by pseudo-masks generated from m different thresholds (Fig. 2 (a)), simulating multi-level human attention. Each network is independently initialized, resulting in varying learning capabilities. Being supervised by pseudo-masks with different activation degrees, the networks evolve to learn various representations of human attention.\nIn practice, we select a pair of thresholds based on annotators' feedback to generate diverse heatmaps that closely resemble ground truth and complement each other, with one tending to erode (under-activate) and the other dilate (over-activate) targets. Additional thresholds are linearly interpolated from the pair. Empirical results indicate that two levels are sufficient for decent performance (Sec. 4.2). The final segmentation prediction is obtained by ensembling the predictions of these networks. We maintain the multi-level structure throughout both the training and inference stages."}, {"title": "3.2 Cross-level Consistency for Noise Compensation", "content": "Another essential aspect of gaze-supervised segmentation is the inevitable noise in the pseudo-masks. The noises can be introduced in the process of human interpretation, gaze estimation by eye-trackers, heatmap generation, etc. We observe a memorization effect [1] of deep networks when training with gaze data on the medical image segmentation task. As shown in Fig. 2 (b), the model captures clean patterns on incorrectly annotated pixels of pseudo-masks at the beginning of training, but eventually overfits on noisy labels.\nIn the multi-level framework, though the networks are diverged with distinct supervisions, they share the same input and various pseudo-masks supervise different representations of the shared gaze data. Based on the assumption that networks learn clean patterns at the beginning of the memorization, for each network, we propose to exploit the knowledge learned by peer networks of other levels to compensate for the noisy label via a consistency term.\nTo ensure noise-robust consistency, we first use a non-parametric local pixel propagation (LPP) module to filter the feature of each pixel by propagating the features of surrounding pixels in local regions inspired by recent works [5,7] proving noise-robust feature correspondence distillation. Given the feature map \u03a6, for each pixel feature \u03c6p, the LPP module computes the transform \u03c6\u0302p as:\n\u03c6\u0302p := \u2211q\u2208Pp softmax (max{cos(\u03c6p, \u03c6q), 0}) \u00b7 \u03c6q,   (1)\nwhere cos denotes the cosine similarity, Pp denotes the set of neighboring pixels (e.g., a 3 x 3 region with dilation 1) of pixel p. This refinement has a feature denoising/smoothing effect that reduces the outliers and enhances the features with local context by introducing spatial smoothness which encourages spatially close pixels to be similar. Given an arbitrary pair of levels i,j\u2208 Z+, where i, j \u2264 m and i \u2260 j, the consistency loss applied on the i-th level maximizes dot-product between the propagated prediction p\u0302i of i-th level and non-propagated prediction pj of j-th level, i.e., Lconsij := \u2212p\u0302i \u00b7 pj. Notably, we have p\u0302i = gi(\u03c6\u0302i) for the propagated prediciton and pj = gj(\u03c6j) for the non-propagated one, where g denotes the shallow classifier."}, {"title": "3.3 Overall Optimization of Gaze Supervision", "content": "The overall loss for the i-th level is the combination of the supervision term and the consistency term over all other peer-level j:\nLi = LCEi + \u03bbm\u22121\u2211j=1,j\u2260iLconsij,  (2)\nwhere LCE is the cross-entropy loss and can be replaced by any other segmentation loss such as dice loss, and \u03bb is empirically set to 3. Note that when optimizing the network of the i-th level, the parameters of all other networks are frozen.\nThe key to understanding the overall optimization process lies in the trade-off of cross-entropy and consistency terms. Intuitively, LCE trains a set of divergent networks utilizing multi-level pseudo-masks. However, this term tends to vanish after the early learning stage and each network starts to overfit on the respective noisy label. The consistency term Lcons compensates for it, implicitly forcing networks to continue learning from clean patterns learned by networks of other levels. The mechanism can be viewed as pushing networks to struggle to find a balance between divergence and consistency, in which the hyper-parameter \u03bb controls this balance. It is worth noting that both divergence and consistency are equally essential for optimization. The consistency term ensures robustness to noise and the cross-entropy term expands and diversifies clean knowledge learned and prevents collapsing into a single network."}, {"title": "4 Experiments", "content": "We validate the proposed gaze annotation scheme on the aforementioned Kvasir-SEG [10] dataset for polyp segmentation and NCI-ISBI [2] dataset for prostate segmentation. For all datasets, we only utilize weak annotations for training and report performance on the testing set. We train a 2D UNet [18] from scratch for 15k iterations with a NVIDIA A40 GPU. We use Adam optimizer with batch size 8 and learning rate 4e-4. More details on the training recipe can be found in the Appendix."}, {"title": "4.1 Comparison Among Label-efficient Annotation Schemes", "content": "Comparison with state-of-the-art weakly-supervised methods. We compare the new gaze-based annotation scheme with full mask supervision and other state-of-the-art weakly-supervised methods using different sparse annotations including box, point, and scribble on two datasets. To compare annotation time, we also invite the same annotator to annotate a randomly sampled subset of the Kvasir-SEG dataset containing 100 images using other annotation schemes and report the annotation time in Table 1. Note that we annotate the bounding box using extreme points clicking [16], and annotate points using the scheme suggested by [4]. The estimated time is close to that reported in the literature [4,12,16,21], where the narrow gaps may come from the difference in the complexity of different target objects to be annotated. For all datasets, we simulate weak annotations based on the ground truth. Note that we randomly sampled 10 pixels and 10 background pixels inside and outside the bounding box respectively as suggested by [4] for point annotations, and we follow [21] to simulate scribble annotations. In Table 1, our results show that gaze supervision outperforms previous weakly-supervised methods trained with other sparse annotation schemes and achieves over 95% of the fully-supervised performance.\nTrade-off between performance and annotation time. We compare the proposed gaze annotation scheme with other label-efficient sparse annotation schemes for image segmentation under the same annotation budget, i.e., the time required to annotate training data. Fig. 3 presents our results on Kvasir-SEG [10], proving that gaze annotation boosts weakly-supervised segmentation by striking a sweet performance/annotation time trade-off by maximizing performance with the least annotation time among existing weak annotation schemes."}, {"title": "4.2 Ablation Studies", "content": "Modules and hyper-parameters. To evaluate the impact of critical components of the proposed method, we study the effectiveness of the proposed components for gaze supervision in Fig. 5 (a). We further present the result of different choices of hyper-parameter m (number of levels) and \u03bb in Fig. 5 (b). Our experiments show that m = 2 is optimal for gaze training while increasing m gives limited benefits but leads to greater training and inference complexity. The results on \u03bb echo the intuition of it in Sec. 3.3. We also observe that having \u03bb greater than 7 results in a degenerated model that collapses to consistency with performance worse than simply ensembling without consistency regularization. We further visualize gaze-supervised predictions in Fig. 4, where the model with consistency regularization demonstrates resistence to gaze noise.\nSensitivity to annotator. We invite two additional annotators to annotate a subset of the Kvasir-SEG training set containing 500 images and train a UNet on this subset using different annotation schemes. Note that all annotators receive the same training for gaze annotating. The results presented in Fig. 5 (c) show that though eye-tracking is subjective, different annotators demonstrate comparable annotation time and supervision quality, consistently outperforming other annotation schemes."}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we propose to train deep networks with gaze annotations efficiently collected using a new gaze annotation scheme for medical image segmentation. The proposed method can be seamlessly integrated into standard training pipelines. The results show that gaze annotation achieves a sweet performance and annotation time trade-off compared to other annotation forms.\nOur explorations give rise to several potential directions for future work: (1) While we expect gaze supervision to be broadly applicable to medical applications, the multiplied complexities in the training and especially the inference stage hamper real-time scenarios since multiple networks are maintained even though we have shown that m = 2 is sufficient. One potential direction is to aggregate networks at a certain frequency in the training and only keep an aggregated model for inference. (2) Though specialized hardware is required to collect gaze data, we foresee that eye-tracking will not pose a bottleneck for clinical practicality even at present with the advance of commercial VR/XR headsets with precise and affordable eye-tracking capabilities. (3) We focus on binary segmentation in this paper, and the extension to multiple cases is straightforward via annotating each class separately and deciding the label for each pixel as the class with the highest value in gaze heatmaps of different classes."}]}