{"title": "PEA: Enhancing LLM Performance on Computational-Reasoning Tasks", "authors": ["Zi Wang", "Shiwei Weng", "Mohannad Alhanahnah", "Somesh Jha", "Tom Reps"], "abstract": "Large Language Models (LLMs) have exhibited remarkable capabilities across diverse domains, prompting investigations into their potential as generic reasoning engines. While recent studies have explored inference-time computation to enhance model performance on complex problems, current research lacks a formal framework to characterize the complexity of reasoning tasks. This study introduces the Predicate-Enumeration-Aggregation (PEA) framework, a formal approach to describe and solve a class of important reasoning tasks termed computational reasoning problems. The PEA framework decomposes these problems into predicate and enumeration components, using LLMs to synthesize programs based on specified predicates, enumeration, and aggregation rules. These synthesized programs are then executed to obtain solutions to the computational tasks. We demonstrate the framework's efficacy on benchmark tasks including Boolean satisfiability problems, game of 24, and planning problems. Empirical evaluation reveals that PEA substantially enhances the performance of underlying models on benchmark computational problems, yielding an average accuracy improvement of approximately 50%, coupled with increased efficiency.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have exhibited significant generalization capabilities across diverse domains, prompting investigations into their potential as generic reasoning engines. Recent studies have explored inference-time computation techniques [Welleck et al., 2024, Snell et al., 2024], particularly prompt engineering methods such as Chain-of-Thought (CoT), to enhance LLM performance on complex reasoning tasks [Wei et al., 2022]. These approaches have successfully improved model performance and expanded LLMs' practical applications. However, despite the growing focus on enhancing model capabilities through inference-time computation for complex reasoning tasks, the current literature lacks a formal framework to precisely describe and characterize the complexity of reasoning problems.\nThis study identifies a class of reasoning problems, termed computational reasoning problems, which are particularly challenging for LLMs [Yao et al., 2023, Hao et al., 2024, Valmeekam et al., 2023], such as planning problems and arithmetic games. Informally, these problems can be accurately described using succinct programmatic representations. We propose a formal framework to describe and algorithmically solve these problems.\nThe framework employs first-order logic, equipped with efficiently computable predicates and finite domains. A first-order logic formula [Kleene, 1971], is represented syntactically as:\n$Q_1X_1Q_2X_2... Q_nX_n\\cdot P(X_1, ..., X_n)$,\nwhere $Q_i$'s are quantifiers and $P(\\cdot)$ is a predicate.\nTo illustrate, the statement \u201cThe sum of an odd number and an even number is odd\" can be expressed as:\n$(\\forall x, y, z)(\\exists l,m,n).[x = 2l \\land y = 2m + 1 \\land z = x + y]$\n$\\Rightarrow z = 2n + 1$\nwhere all $x, y, z, l, m, n \\in Z$, the integer set.\nAs used in this paper, additional restrictions are enforced, limiting variables to finite domains and ensuring predicates are efficiently evaluable. While these limitations preclude the representation of certain mathematical propositions, such as those involving infinite sets like Z, they facilitate algorithmic resolution of problem correctness through exhaustive enumeration and aggregation of solution candidates. Despite reduced expressiveness, this approach remains sufficiently powerful for describing numerous real-world problems (see Section 3.2). To illustrate, we consider the Boolean Satisfiability (SAT) problem [Karp, 1972], formally expressed as:\n$\\exists x \\in {0,1}^n. B(x) = True$,\nwhere $B(x)$ represents a Boolean logic formula. The SAT problem determines whether a satisfying assignment exists for $B$. To ascertain the validity of $\\exists x \\in {0,1}^n. B(x) = True$, a brute-force approach enumerates all possible candidates $c$ from ${0,1}^n$ and evaluates $B(c)$.\nGiven this formal framework for describing computational problems, we propose a method to solve real-world problems using LLMs (see Section 3.3). Our approach employs an algorithm that performs exhaustive enumeration of valid candidate values for variables bounded by quantifiers, and comprehensive predicate evaluation. This strategy inherently aligns with programmatic implementation, leveraging programs' intrinsic capacity for precise evaluation and thorough enumeration when correctly formulated. We introduce the Predicate-Enumeration-Aggregation (PEA) framework, which operates as follows: (i) the predicate, enumeration, and aggregation rules are described in natural language; (ii) LLMs are instructed to generate programs that fulfill these natural-language tasks; and (iii) the synthesized programs are executed to determine the validity of the quantified predicate (see Sections 3.4 and 3.5). Figure 1 illustrates the pipeline of the PEA framework.\nThis algorithmic framework can be conceptualized as employing a brute-force, trial-and-error approach to problem-solving. Due to the expressiveness of PEA, the algorithm may be optimal asymptotically given the hypothesized essential lower-bounds in many computational problems [Impagliazzo and Paturi, 2001, Vassilevska Williams, 2015], potentially necessitating extensive enumerations. The PEA framework"}, {"title": "2 Related Work", "content": "First-order logic, initially championed by Hilbert as the formal language for mathematical description, formed the basis of his ambitious formalism project in mathematics [Hilbert, 1926]. This endeavor was subsequently challenged by G\u00f6del and Turing through their seminal work on first-order arithmetic and computability theory [G\u00f6del, 1931, Turing et al., 1936]. Kleene [1943] further extended Turing's halting problem construction to develop the arithmetical hierarchy using alternating quantified computable predicates. Computational complexity theorists later adapted ideas from Turing, G\u00f6del, and Kleene to establish foundational results in time and space complexity, catalyzing systematic studies in complexity theory and algorithm design [Arora and Barak, 2009, Kleinberg and Tardos, 2005]. Our proposed formalism can be viewed as an adaptation of these computational constructions to model real-world reasoning problems. This approach enables formal categorization of reasoning problems and potentially introduces additional theoretical tools for their analysis.\nLLMs have demonstrated significant potential across various domains, particularly through the application of test-time computation to enhance their performance. Prompt engineering techniques, such as CoT and Tree of Thoughts (ToT) [Wei et al., 2022, Yao et al., 2023], have been shown to augment the reasoning capabilities of less robust LLMs on complex reasoning tasks. However, a formal framework for classifying the complexity of reasoning problems has been lacking. This study proposes a formal description of reasoning, which aims to facilitate researchers' understanding of the underlying complexity of reasoning problems.\nWe employ LLMs to synthesize programs that implement our PEA framework, addressing real-world computational problems. While recent works have utilized LLMs for program synthesis in reasoning tasks, our approach distinguishes itself by targeting more complex reasoning problems [Gao et al., 2023, Weir et al., 2024]. From a first-order-logic perspective, we go beyond quantifier-free formulations used in previous studies to quantified problems (see appendix for a detailed discussion).\nA parallel line of research uses established solvers as underlying reasoning engines for real-world problems, with LLMs facilitating the translation process [Liu et al., 2023, Hao et al., 2024]. However, a programming language constrained by solver binding significantly reduces its programmability and often necessitates substantial human effort to ensure translation correctness, particularly in API usage and adherence"}, {"title": "3 Method", "content": "This section provides a detailed exposition of the PEA framework, encompassing four key aspects: syntax (formal structure), semantics (real-world problem modeling), computation (algorithmic problem-solving approaches), and implementation (practical realization)."}, {"title": "3.1 Notation and definitions", "content": "Let X be a finite set. Predominantly, we consider X = {0,1}, the binary set. We use x to denote variables, and c to denote concrete values. Central to our analysis is the concept of an n-ary unquantified predicate, denoted as P: Xn \u2192 {0,1}. This predicate function serves to determine whether n variables satisfy the relation specified by P. For instance, $P(x, y) : x + y = 5$ denotes the set of (x, y) pairs such that the sum of the values of x and y equals 5\u2014e.g., P(1, 4) = 1 and P(1,2) = 0. We introduce the existential ($\\exists$) and universal ($\\forall$) quantifiers, denoted by the set Q = {$\\exists$,$\\forall$}. The logical connectives $\\land$, $\\lor$ and $\\neg$ represent \u201cand\u201d, \"or\" and \"not\" operations, respectively. Quantifiers bind variables within predicates, forming quantified predicates of the form:\n$Q_1x_1Q_2x_2... Q_nx_n\\cdot P(x_1,...,x_n)$,\nwhere $Q_i \\in Q$. Variables bound by quantifiers are termed bounded; while those not bound are free. A predicate with all variables substituted by concrete values is called a concrete predicate.\nThe validity of quantified predicates serves as an abstraction for validating real-world problems. In the case of predicates where all variables are substituted with specific values, the validity can be unambiguously determined through evaluation of the concrete predicate. However, predicates containing free variables have indeterminate validity. For example, x + 2 = 5 is neither true nor false due to the unknown value of x. The validity of fully bounded quantified predicates can be ascertained within a predefined domain. For instance, given $X = {2,3}$,$\\exists x. x + 2 = 5$ is true while $\\forall x. x + 2 = 5$ is false. Conversely, if $X = {3}$, then $\\forall x.x + 2 = 5$ is true."}, {"title": "3.2 Computational problems", "content": "In this section, we introduce an abstraction of computational problems through the framework of quantified predicates. We focus our attention on Equation (1), where $P(x_1, . . ., x_n)$ is an efficiently decidable predicate. The notion of efficient decidability in this context implies that for any substituted values $c_1, . . ., c_n$, the evaluation of $P(c_1, . . ., c_n)$ can be executed within polynomial time relative to the encoding size of $c_1, . . ., c_n$. As is well known, the formulation presented in Equation (1) possesses sufficient expressive power to abstract several well-known classical computational problems Immerman [1999].\nNP class. The NP complexity class represents a fundamental category in computational-complexity theory. Problems within NP can be formulated as decision problems concerning the validity of existentially quantified predicates. A paradigmatic example of this class is SAT, which holds a central position as an NP-complete problem. The SAT problem can be expressed in the following form:\n$\\exists x. P(x)$,"}, {"title": "3.3 Algorithmic solution", "content": "Having established Equation (1) as a general formulation for computational problems, we introduce an algorithmic approach to determine their validity. Without loss of generality, Let $X = {c_1, c_2}$ and $P : X^2 \\rightarrow {0, 1}$ and assume that the evaluation of each concrete predicate $P(c_i, c_j)$ is computationally efficient. Under these assumptions, it becomes theoretically feasible to exhaustively enumerate all legitimate substitutions to evaluate the validity of the problem. The algorithmic procedure for this approach is outlined as follows:\n1. For an existentially quantified predicate: $\\exists x \\in X^2. P(x) = P(c_1, c_1) \\lor P(c_1, c_2) \\lor P(c_2, c_1) \\lor P(c_2, c_2)$.\n2. For a universally quantified predicate: $\\forall x. P(x) = P(c_1, c_1) \\land P(c_1, c_2) \\land P(c_2, c_1) \\land P(c_2, c_2)$.\nFor more general finite domains and quantifier domains, we can unravel the quantifiers sequentially according to the leading quantifier and ultimately resolve the validity of the quantified predicate based on all concrete predicate evaluations. In complexity theory, this result is known as PSPACEC EXPTIME [Arora and Barak, 2009].\nThis algorithmic solution represents a worst-case scenario, exhibiting exponential-time complexity. Given the expressive power of predicates in general computational problems, this approach may be no worse asymptotically than the theoretical optimum achievable. The approach thus aligns with the Exponential Time Hypothesis [Impagliazzo and Paturi, 2001], which posits that even for the SAT problem\u2014a seemingly simple instance of Equation (1)\u2014algorithms cannot surpass the performance of brute-force methods, inevitably resulting in exponential time complexity. However, it is noteworthy that in many practical applications, the solution enumeration space is typically constrained to a magnitude of a few hundred thousand, rendering the approach computationally feasible within these bounds. Moreover, in numerous applications, the enumeration space can be optimized through the pruning of ineffective candidates, thereby enhancing the efficiency of the algorithmic process."}, {"title": "3.4 Program synthesis", "content": "Our objective is to accurately solve real-world computational problems. In accordance with the algorithmic resolution presented in Section 3.3, we aim to enumerate all concrete predicates. This approach necessitates the fulfillment of two critical conditions:\n1. The evaluation of the predicate with specific value assignments must consistently yield accurate results."}, {"title": "3.5 Implementation", "content": "The PEA framework implementation employs a structured-prompt methodology, combining natural-language problem descriptions with specific Python code-generation instructions. These instructions detail function names and input/output descriptions. For problem instances that cannot be represented by primitive Python types, the LLM is directed to synthesize compound types, encapsulated as Python classes, to describe the problem structure.\nUsers have the option to prompt the model for autonomous generation of pruning instructions during enumeration synthesis, based on the natural language problem description. This approach enhances system efficiency and autonomy in problem-solving tasks, reducing human intervention.\nCode integrity is checked through syntactic and semantic validation. Syntactic checks employ Python's abstract syntax tree (AST) module to confirm function presence, input/output types, non-empty definitions, and correct problem-instance conversion. Semantic checking executes the generated code with a fixed example (either provided by or sampled from the dataset) and compares the output to the expected result. Successful code synthesis requires passing all these validation checks, as outlined in Algorithm 1. It is noteworthy that the synthesis of optimized enumeration functions presents increased complexity due to the intricate nature of pruning instructions. Consequently, in instances where the model fails to successfully synthesize code incorporating these pruning instructions, it defaults to the generation of naive enumeration functions."}, {"title": "4 Evaluation", "content": "This section presents empirical evaluations comparing our methodology with existing prompt-engineering techniques, addressing three primary research questions:\nRQ1: What is the capacity of LLMs to generate exhaustive enumerations?\nRQ2: Could PEA enhance LLMs' reasoning capabilities through program synthesis and local executions?\nRQ3: Compared to direct reasoning, does PEA achieve more efficient reasoning by combining reasoning with program execution?\nRQ1 examines LLM reasoning limitations for problems with succinct programmatic but extensive end-to-end representations. This inquiry is foundational, because many computational-reasoning problems are hypothesized to have essential computational lower-bounds [Impagliazzo and Paturi, 2001, Vassilevska Williams, 2015], potentially necessitating lengthy enumerations. RQ2 investigates whether the PEA program-synthesis framework enhances an LLM's reasoning capability on computational tasks by leveraging their coding proficiency, aiming to determine if programmatic problem translation improves an LLM's performance on complex reasoning tasks. RQ3 evaluates the efficiency of the PEA framework, including PEA representation translation and local server execution, to assess whether it offers a more efficient alternative to direct reasoning by LLMs.\nModels and server. We use recent language models with advanced coding and reasoning capabilities, specifically GPT-4o, OpenAI 01-mini, and o1 models [OpenAI et al., 2024]. The majority of computational cost stems from LLM queries. Local Python execution is performed on a server with thirty-two AMD EPYC 7313P 16-core processors, 528 GB of memory. Supplementary small-scale experiments employing recent DeepSeek models were conducted, with results presented in the appendix [DeepSeek-AI et al., 2024, 2025]."}, {"title": "Method and baselines.", "content": "We implemented PEA as described in Section 3.5, using a fixed example for code integrity checks, which is removed from the evaluation dataset if present. Comparative analysis employed Direct Query (DQ) and CoT as principal baselines, with ToT as an additional baseline for the Game of 24. Our core PEA prompts are provided in the appendix.\nExperimental design. To address RQ1, LLMs are evaluated on their ability to generate enumerations for combinatorial problems, specifically Cartesian-product combinations and permutations (see Section 4.1). The performance metric is the percentage of total possible enumerations successfully outputted by the models. For RQ2, we selected a range of computational-reasoning tasks, including SAT, Game of 24, and two planning tasks derived from recent LLM literature [Marino, 2024, Yao et al., 2023, Valmeekam et al., 2023]. PEA and various baselines were applied to find correct answers (see Sections 4.2 to 4.4). Among all computational tasks, optimization of enumeration in PEA code synthesis is only activated for the logistics planning problem. For RQ3, the time used by PEA and other tools to solve the entire dataset was measured (see Section 4.5). Notably, for each task, PEA's program synthesis is performed only once, with the resulting program being reused across the entire dataset, as each data point represents a concrete problem instance serving as input to the synthesized program.\nThe PEA-generated code undergoes integrity checks with a maximum of $m = 10$ iterations, as per Algorithm 1. A 30-second execution timeout is imposed on synthesized code. Solutions are considered unfound if their execution exceeds the timeout threshold.\nFindings. Our experiments yielded the following results pertaining to the research questions:\nRQ1: LLMs exhibit accurate enumeration for small search spaces (typically less than a few hundred elements) but default to generating Python code for larger enumerations rather than providing exhaustive results. This preference for programmatic approaches over end-to-end representations aligns with our premise of leveraging LLMs' coding capabilities for complex reasoning tasks, where code serves as a reasoning generator and its execution produces concrete reasoning outcomes.\nRQ2: Our evaluation of computational problems demonstrates that reasoning via code generation and execution consistently outperforms direct reasoning when using identical underlying models. Programs passing integrity checks successfully resolve a substantial portion of the dataset, underscoring the efficacy of programmatic approaches in computational problem-solving. The 01 and 01-mini models significantly surpass GPT-4o in direct reasoning. Moreover, these models exhibit superior proficiency in synthesizing complex code, particularly in generating optimized solutions that adhere to enumeration pruning instructions. This enhanced capability further augments PEA's performance on more intricate tasks.\nRQ3: Code synthesis and program execution provide efficient reasoning, capitalizing on decades of optimization in modern computer architectures. This approach offers a significant advantage over recent powerful reasoning models, which, despite their accuracy, require substantially more time for problem-solving. Notably, successful code generation typically necessitates fewer than three LLM queries to pass integrity checks. This minimal query requirement results in negligible computational cost when amortized across the entire dataset, further enhancing the method's overall efficiency.\nThe following subsections offer a comprehensive analysis of each experimental task."}, {"title": "4.1 Enumerations with LLM", "content": "In our evaluation, we employed random 3-character strings as variable-name candidates. For Cartesian product-combination tasks, LLMs were instructed to output n-variable product expressions, producing all possible combinations of n variables. Each variable was selected from a pool of m 3-character strings, resulting in $m^n$ product combinations. Permutation tasks required LLMs to generate all possible permutations of"}, {"title": "4.2 SAT problem", "content": "The SAT problem involves determining a truth-value assignment that satisfies a given Boolean formula.\nFormal Description. The SAT problem can be formally defined within the PEA framework as follows: (i) The predicate is a Boolean formula (for which a truth assignment evaluates to either True or False.) (ii) The enumeration comprises all possible truth assignments of the variables. (iii) The aggregation determines whether any truth assignment in the enumeration satisfies the predicate.\nDataset. We use a Boolean-satisfiability dataset from Marino [2024], comprising formulas in Conjunctive Normal Form (CNF). The dataset comprises 100 formulas each of 2-CNF, 3-CNF, and 4-CNF types, where clauses contain 2, 3, or 4 literals, respectively. While 2-CNF formula satisfiability is in class P, 3-CNF and 4-CNF satisfiability problems are NP-complete. One 3-SAT formula is used for code semantic validation and excluded from the testing set, resulting in a total of 299 formulas for evaluation.\nResults. Given the computational-complexity differences, results are presented separately: 2-CNF instances are aggregated in the appendix, while 3-CNF and 4-CNF instances are reported together in Table 2. This distinction reflects the P-class nature of 2-CNF SAT problems and the NP-hardness of 3-CNF and 4-CNF SAT problems. PEA achieves perfect correctness in all cases, even when using the relatively weak reasoning model GPT-4o. Notably, all models perform exceptionally well on 2-CNF SAT problems, as detailed in the appendix."}, {"title": "4.3 Game of 24", "content": "The Game of 24 (G24) is a mathematical puzzle where participants are presented with four numbers and tasked with constructing an expression that evaluates to 24. This expression can be formulated using the arithmetic operations of addition, subtraction, multiplication, and division, with the optional use of parentheses.\nFormal description. The Game of 24 can be formulated as follows: (i) The predicate evaluates whether an arithmetic expression composed of four given numbers equals 24. (ii) The enumeration generates all valid expressions using +, -, \u00d7, \u00f7, and permissible parenthetical patterns. (iii) The aggregation determines if at least one expression evaluates to n or if no such expression exists.\nDataset. We extract the most difficult 100 instances of Game of 24 numbers from the dataset of ToT [Yao et al., 2023]. The difficulty of these instances is quantified by ToT's success rate. One example from the dataset, excluded from the 100 selected instances, is used for code validation.\nResults. The results of the Game of 24 are presented in Table 3. PEA demonstrates perfect performance across all models, while the most advanced ol model, even when equipped with CoT, resolves only 60 instances."}, {"title": "4.4 Planning problems", "content": "We use a recent planning benchmark dataset from Valmeekam et al. [2023], designed to assess LLMs' reasoning capabilities in solving planning problems. The benchmark comprises several variants of two primary tasks: Blocksworld (BW) and Logistics (LOGI).\nThe Blocksworld task is a planning problem where participants are presented with several colored blocks on a table. The task requires executing valid actions to move these blocks. Each problem instance provides an initial block configuration and goal conditions representing desired block-stacking conditions. Participants must devise a feasible plan, comprising a sequence of actions, to achieve the goal state. Our evaluation uses a more challenging variant, the optimal-cost Blocksworld problem, which necessitates finding a minimum-cost action sequence."}, {"title": "4.5 Efficiency", "content": "To evaluate the efficiency of each prompt method, we measure the per-instance solving time. For PEA, the initial program-synthesis cost is amortized across all instances, because it occurs only at the problem's outset. Table 6 presents the per-instance computation costs. Notably, the PEA method not only enhances problem-solving accuracy but also demonstrates reduced computation time in most cases across all models"}, {"title": "5 Conclusion", "content": "This paper introduces the PEA framework, which formally characterizes computational reasoning tasks and enhances LLMs' reasoning capabilities through their coding proficiency. Our central premise posits that many computational problems have inherent lower bounds, and synthesizing programs as reasoning generators is often more efficient and simpler than performing concrete reasoning."}, {"title": "A PEA vs PAL", "content": "PAL [Gao et al., 2023] and our PEA framework framework both utilize code generation to enhance LLM performance on reasoning tasks. From a first-order logic perspective, PAL addresses less complex tasks involving direct concrete predicate evaluation without quantifiers, while PEA is designed to handle more complex logical structures. To illustrate, consider the following example of a PAL task:"}, {"title": "B Additional empirical results", "content": ""}, {"title": "B.1 Cartesian product combination", "content": "The results of the Cartesian product-combination evaluation are presented in Table 7. These findings demonstrate a similar pattern to those observed in the permutation evaluation."}, {"title": "B.2 2-SAT", "content": "The results for the 2-SAT problem are presented in Table 8. Given that 2-SAT is a P-class problem with efficient algorithmic solutions, it is noteworthy that all models demonstrated commendable performance in this evaluation."}, {"title": "B.3 Deepseek preliminary results", "content": "Preliminary small-scale experiments were conducted utilizing DeepSeek models, specifically DeepSeek-V3 (deepseek-chat) and DeepSeek-R1 (deepseek-reasoner), for Blocksworlds and Logistics planning tasks via the DeepSeek API [DeepSeek-AI et al., 2024, 2025]. PEA when applied to these models, necessitated multiple iterations to synthesize programs meeting sanity check criteria. However, the generated programs failed to achieve sufficient accuracy. While the web interface demonstrated rapid response times for direct queries and CoT approaches, API requests exhibited slow performance and poor connection stability. The obtained results did not match the quality of those from OpenAI 01. Consequently, despite intentions to conduct a comprehensive evaluation, the decision was made to defer full-scale testing pending improved service reliability."}, {"title": "C PEA Prompts", "content": "This section presents the core PEA prompts utilized in our evaluation, highlighting in pink the PEA components. For prompts incorporating enumeration optimization strategies, the strategies are highlighted in yellow and noted to be LLM-generated rather than user-crafted. The majority of the prompt content for planning problems is directly from the problem descriptions provided in the dataset."}]}