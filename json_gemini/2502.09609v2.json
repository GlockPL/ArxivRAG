{"title": "Score-of-Mixture Training: One-Step Generative Model Training Made Simple via Score Estimation of Mixture Distributions", "authors": ["Tejas Jayashankar", "J. Jon Ryu", "Gregory Wornell"], "abstract": "We propose Score-of-Mixture Training (SMT), a novel framework for training one-step generative models by minimizing a class of divergences called the \u03b1-skew Jensen-Shannon divergence. At its core, SMT estimates the score of mixture distributions between real and fake samples across multiple noise levels. Similar to consistency models, our approach supports both training from scratch (SMT) and distillation using a pretrained diffusion model, which we call Score-of-Mixture Distillation (SMD). It is simple to implement, requires minimal hyperparameter tuning, and ensures stable training. Experiments on CIFAR-10 and ImageNet 64\u00d764 show that SMT/SMD are competitive with and can even outperform existing methods.", "sections": [{"title": "1. Introduction", "content": "Fast and efficient sampling is a key characteristic sought after in modern generative samplers. For many years, generative adversarial networks (GANs) (Goodfellow et al., 2014) set the benchmark for high-quality one-step generative sampling. However, due to the inherent training instabilities associated with discriminator training, attention has recently shifted toward diffusion-based generative models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Karras et al., 2022b). These models trade-off sampling efficiency for more stable training and significantly improved downstream sample quality through iterative sampling.\nMore recently, the diffusion distillation approach has been studied as an appealing option to significantly reduce the number of sampling steps. Early work (Luhman & Luhman, 2021; Salimans & Ho, 2022; Meng et al., 2023; Berthelot et al., 2023) focused on training a student model with a lower sampling budget by condensing multiple teacher denoising steps into one. The most recent works on distillation improve performance further by leveraging a pretrained model for distribution matching via minimization of the reverse KL divergence (Luo et al., 2024a; Yin et al., 2024b;a; Salimans et al., 2024; Xie et al., 2024). While attractive, distillation approaches necessitate a pretrained diffusion model which adds a significant overhead on the required compute.\nAs yet another alternative, consistency models (Song et al., 2023; Song & Dhariwal, 2024b) and their variants (Kim et al., 2024) have been proposed for training few-step generative models from scratch by simulating the trajectories of the induced probability flow ODE (Song et al., 2020) of a diffusion process. While consistency models have demonstrated promising results in both distillation and training from scratch, training is sensitive to the choice of noise schedule and distance measure (Geng et al., 2024).\nIn this paper, we tackle the problem of training high-quality one-step generative models more directly, i.e., without simulating an iterative reverse diffusion process for sampling or leveraging a pretrained diffusion model during training. Starting from first principles of statistical divergence minimization, we show that a high-quality one-step generative model can be trained from scratch in a stable manner, via the multi-noise-level denoising score matching (DSM) technique (Vincent, 2011) used in diffusion models. We emphasize that we do not require a simulation of the reverse diffusion process in our framework.\nThe proposed framework achieves the best of several worlds: (1) a new, simple statistical divergence minimization framework without probability paths of ODE (like GAN), (2) stable training using denoising score matching (like diffusion models), (3) training from scratch without a pretrained diffusion model (like consistency models), and (4) near state-of-the-art one-step image generative performance (like GAN and consistency models). We also demonstrate that the proposed method can be extended to distill from a pretrained diffusion model, and can achieve performance similar to state-of-the-art methods for the same. \nThe rest of the paper is organized as follows: In Sec. 2 we introduce the necessary background and related works central to our proposed method. In Sec. 3 we introduce our"}, {"title": "2. Preliminaries and Related Work", "content": "In one-step generative modeling, we wish to align the generated sample distribution $q_\\theta(x) := \\int \\delta(x - g_\\theta(z))q(z) dz$ with the true data distribution $p(x)$. Here, $g_\\theta : Z \\rightarrow X$ is a parametric neural sampler which is also often called an implicit generative model that transforms samples from a base measure $q(z)$. In this section, we review some popular methods for training generative models, which will serve as preliminaries for our framework. More detailed discussion on the literature is deferred to Appendix B.\nGenerative Adversarial Networks. The most prominent approach in training implicit generative models is the generative adversarial network (GAN) (Goodfellow et al., 2014). In its most standard and widely used form, it alternates between the gradient steps of discriminator and generator training, which are\n$\\min_\\psi E_{p(x)} [\\textit{sp}(-\\ell_\\psi(x))] + E_{q_\\theta(x)} [\\textit{sp}(\\ell_\\psi(x))],$ (1)\n$\\min_\\theta E_{q_\\theta(x)} [\\textit{sp}(-\\ell_\\psi(x))],$ (2)\nrespectively, where $\\textit{sp}(y) := \\log(1 + e^y)$ denotes the soft-plus function.\\footnote{Here, we will call $\\ell_\\psi(x)$ the discriminator, which is supposed to capture the log density ratio $\\log \\frac{p(x)}{q_\\theta(x)}$}\nThis so-called adversarial training can be understood as minimizing the Jensen-Shannon divergence (JSD) with the help of discriminator, via the variational characterization of JSD.\nThe generator loss $E_{q_\\theta(x)}[\\textit{sp}(-\\ell_\\psi(x))]$ in the second line is the so-called non-saturating version, while the original GAN generator loss $E_{q_\\theta(x)}[-\\textit{sp}(\\ell_\\psi(x))]$ is referred to as saturating.\nDespite the popularity of GANs, training them is notoriously difficult. Although various techniques have been proposed to regularize the GAN objective\u2014through alternatives to JSD (Nowozin et al., 2016; Arjovsky et al., 2017; Mao et al., 2017), novel regularizers (Miyato et al., 2018), and specialized network architectures (Karras et al., 2021; Brock et al., 2019; Sauer et al., 2022)\u2014the discriminator training remains unstable. This has sparked increasing interest in developing new objectives for training generative models which we briefly discuss below.\nDiffusion Models. Diffusion models or score-based generative models (Sohl-Dickstein et al., 2015; Ho et al., 2020) are state-of-the-art generative models that are based on the principles of thermodynamic diffusion. Given a forward stochastic differential equation (SDE) process\n$dx_t = f(x_t, t) dt + g(t) dw_t,$\nwhere $f(x_t, t)$ is the drift function, $g(t)$ is the diffusion function, and $w_t$ represents a Brownian noise process, diffusion models simulate the reverse (generative) process, which is also an SDE\n$dx_t = [f(x_t, t) - g(t)^2 \\nabla_{x_t} \\log p(x_t)] dt + g(t) dw_t.$\nAn equivalent deterministic probability flow ODE with the same marginals as the SDE can also be used in practice:\n$dx_t = (f(x, t) - \\frac{1}{2} g(t)^2 \\nabla_{x_t} \\log p(x_t)) dt.$\nThus, to generate samples, diffusion models are trained to learn the score of the data distribution at multiple noise levels $\\sigma_t$ via denoising score matching (DSM) (Vincent, 2011), i.e., by minimizing\n$\\mathcal{L}_{DSM}(\\theta) = E_{p(x)q(\\epsilon)p(t)} [W(t)||s_\\theta(x_t; t) - s(x_t|x)||^2],$\nwhere $p(t)$ denotes a distribution over different noise levels, $x_t := x + \\sigma_t \\epsilon, \\epsilon \\sim \\mathcal{N}(0, I)$, and $s(x_t|x) := \\nabla_{x_t} \\log p(x_t|x)$. It is easy to show that $s_\\theta(x_t; t) = \\nabla_{x_t} \\log p(x_t)$ using Tweedie's formula (Robbins, 1956). Sampling can then be achieved by Langevin dynamics (Song"}, {"title": "3. Training from Scratch", "content": "In this section, we introduce our new framework, Score-of-Mixture Training (SMT). We describe how to efficiently train one-step generative models from scratch, i.e.,, without a pretrained diffusion model. In Sec. 4, we explain how the framework can be adapted to leverage a pretrained diffusion model when available, referring to this variant as Score-of-Mixture Distillation (SMD).\nThe key ingredient of this framework is distribution matching using a new family of statistical divergences (Sec. 3.1), whose gradient can be approximated by estimating the score of mixture distributions of real and fake distributions\n$\\mathcal{D}_{JS}^{(\\alpha)}(q_\\theta, p) := \\frac{1}{\\alpha} D_{KL}(q_\\theta || \\alpha p + (1 - \\alpha)q_\\theta)$\n$\\hspace{0.6in} + \\frac{1}{1 - \\alpha} D_{KL}(p || \\alpha p + (1 - \\alpha)q_\\theta)$\nfor some $\\alpha \\in (0, 1)$, which we call the $\\alpha$-skew Jensen-Shannon divergence ($\\alpha$-JSD) (Nielsen, 2010). This divergence belongs to $f$-divergences (Csisz\u00e1r et al., 2004).\nInterestingly, $\\alpha$-skew JSD naturally interpolates between the forward Kullback\u2013Leibler divergence (KLD) $D_{KL}(p || q_\\theta)$ (when $\\alpha \\rightarrow 0$), the standard definition of JSD (when $\\alpha = \\frac{1}{2}$), and the reverse KLD $D_{KL}(q_\\theta || p)$ (when $\\alpha \\rightarrow 1$). In contrast to the forward KLD and reverse KLD, the $\\alpha$-skew JSD with $\\alpha \\in (0, 1)$ is well-defined even when there is a support mismatch in $p$ and $q_\\theta$, which may be the case especially in the beginning of training.\nHence, we propose to minimize a weighted sum of the $\\alpha$-JSD's for different $\\alpha$'s, as divergences with different $\\alpha$'s exploit different geometries between two distributions. For example, it is known that minimizing the forward and reverse KLD leads to mode-covering and mode-seeking behaviors, respectively, and we can enforce better support matching behavior by considering the entire range of $\\alpha$.\nTo minimize this family of divergences in practice, we consider its gradient expression:\nSuppose that $E_{q_\\theta(x)} [\\nabla_\\theta \\log q_\\theta(x)] = 0$.\\footnote{It is a standard assumption in the literature (Hyv\u00e4rinen, 2005), which holds under a mild regularity assumption on the parametric model $q_\\theta(x)$ so that $\\int \\nabla_\\theta q_\\theta(x) dx = \\nabla_\\theta \\int q_\\theta(x) dx$.}\nThen, we have\n$\\nabla_\\theta \\mathcal{D}_{JS}^{(\\alpha)}(q_\\theta, p)$\n$= \\frac{1}{\\alpha} E_{q(z)} [\\nabla_\\theta g_\\theta(z) (s_{\\theta,0}(x) - s_{\\theta;\\alpha}(x)|_{x=g_\\theta(z)}],$ (4)\nwhere we define the score of the mixture distribution\n$s_{\\theta;\\alpha}(x) := \\nabla_x \\log(\\alpha p(x) + (1 - \\alpha)q(x)).$\nThis proposition suggests that we can update the generator $g_\\theta(z)$ using this gradient expression, provided that we can estimate the score of the mixture distribution $s_{\\theta;\\alpha}(x).$"}, {"title": "3.2. Learning with Multiple Noise Levels", "content": "To achieve stable training, we opt to minimize the divergence at different noise levels by considering the convolved distributions, $p_t := p * \\mathcal{N}(0, \\sigma_t^2 I_D)$ and $q_{\\theta,t} := q_\\theta * \\mathcal{N}(0, \\sigma_t^2 I_D)$. This idea is widely used in the existing distillation methods. We borrow the variance-exploding Gaussian noising process notation from Karras et al. (2022b) where $\\sigma_t \\in [\\sigma_{min}, \\sigma_{max}]$. As we also integrate over different $\\alpha$'s, the final objective becomes\n$\\mathcal{L}_{gen}(\\theta) := E_{p(\\alpha)p(t)} [\\mathcal{D}_{JS}^{(\\alpha)}(q_{\\theta,t}, p_t)],$ (5)\nwhere we will prescribe the choice of $p(\\alpha)$ in Sec. 3.5. Similar to Eq. (4), the gradient of the divergence at noise level $t$ can be approximated via the amortized score as\n$\\nabla_\\theta \\mathcal{D}_{JS}^{(\\alpha)}(q_{\\theta,t}, p_t) \\approx \\frac{1}{\\alpha} \\gamma_\\psi(\\theta; \\alpha, t)$\n$:= \\frac{1}{\\alpha} E_{q(z)} [\\nabla_\\theta g_\\theta(z) (s_\\psi(x_t; \\theta, t) - s_{\\theta;\\alpha}(x_t)|_{x=g_\\theta(z)}],$ (6)\nwhere the amortized score model $s_\\psi(x_t; \\alpha, t)$, which is conditioned on the noise level $t$, is an estimate of $s_{\\theta;\\alpha,t}(x_t) := \\nabla_x \\log(\\alpha p(x_t) + (1 - \\alpha)q_\\theta(x_t))$. We provide a practical implementation of the amortized score model as a small modification of a diffusion model architecture in Sec. 3.4. We remark in passing that this expression can be understood as a generalization of the gradient update of Eq. (3) used in the existing reverse-KLD-based distillation schemes.\nFinally, we can then approximate the generator gradient as\n$\\nabla_\\theta \\mathcal{L}_{gen}(\\theta) \\approx E_{p(\\alpha)p(t)} [\\gamma_\\psi(\\theta; \\alpha, t)].$\nImportantly, similar to existing distillation methods, the gradient only involves the output of the score model, but not its gradient. This is beneficial since such extra gradient information requires expensive backpropagation through the score model to the generator (Zhou et al., 2024)."}, {"title": "3.3. Estimating Score of Mixture Distributions", "content": "Estimating the score of the mixture distribution turns out to be as simple as minimizing a mixture of the score matching losses, as stated in the following proposition:\nFor any $\\alpha \\in [0, 1]$, the minimizer of the objective function\n$\\mathcal{L}(\\psi; \\alpha) = \\alpha E_{p(x)} [||s_\\psi(x; \\alpha) - s_p(x)||^2]$\n$+ (1 - \\alpha) E_{q_\\theta(x)} [||s_\\psi(x; \\alpha) - s_{q_\\theta}(x)||^2] (7)$\nsatisfies $s_\\psi^*(x; \\alpha) = s_{\\theta;\\alpha}(x)$.\nSince we train with multiple noise levels, we are interested in the marginal score of $x_t = x + \\sigma_t \\epsilon, \\epsilon \\sim \\mathcal{N}(0, I)$ at some noise level $\\sigma_t$. We can use denoising score matching (Vincent, 2011) to define an equivalent sample-only objective to learn the score using Tweedie's formula. Namely, to approximate $s_{\\theta;\\alpha,t}(x_t)$ using the amortized score model $s_\\psi(x; \\alpha, t)$, we can minimize\n$\\mathcal{L}_{score}(\\psi) := E_{p(\\alpha)p(t)} [\\mathcal{L}_{score}(\\psi; \\alpha, t)],$\nwhere\n$\\mathcal{L}_{score}(\\psi; \\alpha, t) := \\alpha E_{p(x)q(\\epsilon)} [||s_\\psi(x; \\alpha,t) + \\epsilon/\\sigma_t||^2]$\n$+ (1 - \\alpha) E_{q_\\theta(x)q(\\epsilon)} [||s_\\psi(x; \\alpha, t) + \\epsilon/\\sigma_t||^2]. (8)$\nFor a formal statement. In practice, we parametrize the score model in the form of a denoiser and reconstruct the score from the denoiser output via Tweedie's formula; see Appendix C.1."}, {"title": "3.4. Practical Design of Amortized Score Network", "content": "With an additional conditioning scheme to embed auxiliary information about $\\alpha$ in addition to the noise level $\\sigma_t$, any existing diffusion model backbone can be used to parametrize the amortized score network $s_\\psi(x; \\alpha, t)$. Here, we describe how we can modify the popular UNet-based score architectures (Song et al., 2020; Nichol & Dhariwal, 2021; Karras et al., 2022b) with minimal modifications.\nFirst, drawing from the noise embedding sensitivity analysis by Song & Dhariwal (2024b), we opt for a Fourier embedding $c_\\alpha$ with a default scale of 16. This choice ensures that the embedding is sufficiently sensitive to fluctuations in $\\alpha$, particularly during the early stage of training.\nThen, we concatenate the $\\alpha$-embedding with the embedding of other auxiliary information (e.g., $t$ and labels) and apply a single SiLU (Elfwing et al., 2018) activated linear layer:\n$c_{out} = \\text{silu} (W_{aux} c_{aux} + W_{\\alpha} c_{\\alpha}).$"}, {"title": "3.5. Training", "content": "Our training scheme alternates between the score estimation with the score matching objective in Eq. (8), and the generator training with Eq. (6), where we plug-in $s_\\psi(x_t; \\alpha, t)$ in place of $s_{\\theta;\\alpha,t}(x_t)$. This is similar in spirit to GAN training, but the DSM technique in our framework in place of the discriminator training naturally stabilizes training. The overall training framework is\nTo train both the generator and score model, we sample $\\alpha$ from a uniform distribution over 1000 equally spaced points in $[0, 1]$, ensuring a dense enough grid to generalize to any $\\alpha$. For score training, we further ensure that 25% of the sampled $\\alpha$'s are zero, since this is always used in our gradient update; see Eq. (6).\nIn practice we compute the gradient with an adaptive weight $w(x_t, x, \\alpha, t)$ to ensure that the scale of the gradient for each minibatch sample is roughly uniform for different values of $\\alpha$ and $t$. Hence, we modify the generator gradient in Eq. (6) as\n$\\gamma_\\psi(\\theta; \\alpha, t) := E_{q(z)} [\\nabla_\\theta g_\\theta(z) \\times \\{\\frac{s_\\psi(x_t; \\theta, t) - s_{\\psi}(x_t; \\alpha, t)}{|| s_\\psi(x_t; \\theta, t) - s_{\\psi}(x_t; \\alpha, t) ||_2 } w(x_t, x, \\alpha, t)}|_{x=g_\\theta(z)}],$ (9)\nwhere the weighting is defined as\n$w(x_t, x, \\alpha, t) := W_{\\alpha}(x_t,t) W_{DMD}(x_t, x, t).$ (10)\nHere $W_{DMD}$ is the adaptive noise weighting introduced by (Yin et al., 2024b) and $W_{\\alpha}(x_t, t)$ is a new weighting inspired by the pseudo-Huber norm (Song & Dhariwal, 2024a; Geng et al., 2024)\n$W_{\\alpha}(x_t, t) := \\frac{\\alpha}{||s_\\psi(x_t; 0, t) - s_{\\psi}(x_t; 1, t)||^2} \\frac{||s_\\psi(x_t; 0, t) - s_{\\psi}(x_t; 1, t)||^2}{||s_\\psi(x_t; 0, t) - s_{\\psi}(x_t; \\alpha, t)||^2}.$\nThis weighting still preserves the limiting forward KLD behavior of the objective as $\\alpha \\rightarrow 0$ and simplifies to DMD\nGiven a discriminator $\\ell_\\psi(x_t; t)$, we minimize a non-saturating version of the $\\alpha$-JSD loss:\n$\\mathcal{L}_{GAN}^{(t)}(\\theta) = E_{q_\\theta(x)} [\\textit{sp}(-\\ell_\\psi(x_t;t) - \\log \\frac{\\alpha}{1-\\alpha})].$ (11)"}, {"title": "4. Distilling from Pretrained Diffusion Model", "content": "In our development so far, we do not assume access to a pretrained diffusion model. In this section, we show how a practitioner can train an one-step generative model leveraging a pretrained diffusion model, if available, within our framework. The proposed distillation scheme is comparable or even outperforms the state-of-the-art distillation schemes."}, {"title": "4.1. How To Leverage Pretrained Diffusion Model", "content": "In the distillation setup, we treat the pretrained diffusion model as the data score $s_p(x_t; t)$, and thus training the score of mixture $s_{\\theta;\\alpha}(x_t; t)$ using a single, amortized model may not be the most efficient parameterization. Hence, instead, we consider the following expression\n$s_{\\theta;\\alpha}(x) = D_{\\theta;\\alpha}(x) s_p(x) + (1 - D_{\\theta;\\alpha}(x)) s_{q_\\theta}(x),$\nwhere\n$D_{\\theta;\\alpha}(x) := \\frac{\\alpha p(x)}{\\alpha p(x) + (1 - \\alpha)q(x)} = \\sigma(\\log \\frac{p(x)}{q_\\theta(x)} + \\log \\frac{\\alpha}{1-\\alpha}).$\n we can express the score of mixture $s_{\\theta;\\alpha}(x)$ as a mixture of scores $s_p$ and $s_{q_\\theta}$, where the weight is $(D_{\\theta;\\alpha}(x), 1 - D_{\\theta;\\alpha}(x))$. This suggests that instead of an amortized modeling of the score of mixture, we can use an alternative parameterization,\n$s_{exp}(x; \\alpha) := D(x; \\alpha) s_p(x) + (1 - D(x; \\alpha)) s_{fake}(x),$"}, {"title": "4.2. Implementation and Training", "content": "We can leverage any existing diffusion model architectures directly for the fake score $s_{fake}(x_t; t)$. We parametrize the discriminator $\\ell_\\psi(x_t; t)$ similar to the noise-conditional discriminator in our training from scratch setting (see Sec. 3.5). The difference is that we can train the discriminator by minimizing the DSM loss in Eq. (14) naturally, without an additional GAN loss. When training the generator, we plug in this approximate log density ratio into Eq. (11) to regularize the generator updates.\nWe also train in an alternating fashion. Since we have access to a pretrained score model, we use this to initialize the weights of both the generator and the fake score model. We utilize the same sampling distribution for $\\alpha$ as in our training from scratch setup (see Sec. 3.5). The"}, {"title": "6. Concluding Remarks", "content": "In this paper, we show that high-quality one-step generative models can be trained from scratch and in a stable manner, without simulating the reverse diffusion process or probability flow ODE as in diffusion models and consistency models. The key distinctive idea in our framework is a new multi-divergence minimization paradigm implemented by estimating the score of mixture distributions. For stable training, we borrow multi-level noise learning and denoising score matching techniques from the diffusion literature. Our empirical results show that accurate score estimation facilitates stable minimization of statistical divergences. We hope this work offers a fresh perspective on generative modeling and inspires further research in the field."}, {"title": "Limitations and Future Work", "content": "While SMT/SMD achieve strong empirical performance, there is still room for improvement in both architecture and training strategies. Additionally, although SMT/SMD attain highly competitive FID for one-step generation from scratch, models with a few-step generation capability-such as consistency models-can further improve FID with additional iterations. Finally, given the generality of our framework, we believe these ideas could extend to other complex modalities, including speech and audio synthesis. We leave such directions for future work."}, {"title": "Impact Statement", "content": "We introduce Score-of-Mixture Training, a simple yet effective one-step generative modeling framework that requires minimal design effort and hyperparameter tuning. We hope its ease of implementation will drive further research into efficient, state-of-the-art neural sampling. However, we acknowledge the potential risks of misuse, including the generation of fake, biased, or misleading content. Our work focuses on fundamental research using standard machine learning datasets, but we recognize the importance of ensuring generative models are secure and privacy-preserving to democratize this technology responsibly."}, {"title": "Deferred Statements and Proofs", "content": ""}, {"title": "A.1. Proof of Proposition 3.1", "content": "Proof of Proposition 3.1. We can simplify the gradient of each term separately as follows:\n$\\nabla_\\theta D_{KL}(q_\\theta||\\alpha p + (1 - \\alpha)q_\\theta) = E_{q_\\theta(x)} [\\nabla_\\theta \\log \\frac{q_\\theta(x)}{\\alpha p(x) + (1 - \\alpha)q_\\theta(x)}]$ + $E_{q(z)} [\\nabla_\\theta g_\\theta(z) (s_{\\theta;0}(x) - s_{\\theta;\\alpha}(x))|_{x=g_\\theta(z)}]$\n$\\nabla_\\theta D_{KL}(p||\\alpha p + (1 - \\alpha)q_\\theta) = - E_{p(x)} [\\nabla_\\theta \\log(\\alpha p(x) + (1 - \\alpha)q_\\theta(x))]$.\nHere, note that in the first expression, we invoke the chain rule: for some function $f_0 : X \\rightarrow R$, we have\n$\\nabla_\\theta f_0(g_\\theta(z)) = (\\nabla_x f_0(x))|_{x=g_\\theta(z)} + \\nabla_\\theta g_\\theta(z)(\\nabla_x f_0(x))|_{x=g_\\theta(z)}.$\nCombining these two terms with the weights, we get the gradient of the $\\alpha$-skew JSD:\n$\\nabla_\\theta \\mathcal{D}_{JS}^{(\\alpha)}(q_\\theta, p) = \\frac{1}{\\alpha} \\nabla_\\theta D_{KL}(q_\\theta||\\alpha p + (1 - \\alpha)q_\\theta) + \\frac{1}{1-\\alpha} \\nabla_\\theta D_{KL}(p||\\alpha p + (1 - \\alpha)q_\\theta)$\n$= \\frac{1}{\\alpha} E_{q(z)} [\\nabla_\\theta g_\\theta(z) (s_{\\theta;0}(x) - s_{\\theta;\\alpha}(x))|_{x=g_\\theta(z)}]$\n$- \\frac{1}{\\alpha (1-\\alpha)} E_{\\alpha p(x)+(1-\\alpha)q_\\theta(x)} [\\nabla_\\theta \\log(\\alpha p(x) + (1 - \\alpha)q_\\theta(x))]$\n$+ \\frac{1}{\\alpha} E_{q_\\theta(x)} [\\nabla_\\theta \\log q_\\theta(x)]$\n$= \\frac{1}{\\alpha} E_{q(z)} [\\nabla_\\theta g_\\theta(z) (s_{\\theta;0}(x) - s_{\\theta;\\alpha}(x))|_{x=g_\\theta(z)}].$\nHere, we use the assumption that $E_{q_\\theta(x)} [\\nabla_\\theta \\log q_\\theta(x)] = 0.$"}, {"title": "A.2. Proof of Proposition 3.2", "content": "We can write the objective $\\mathcal{L}(\\psi; \\alpha)$ as\n$\\mathcal{L}(\\psi; \\alpha) = \\int \\{( \\alpha p(x) + (1 - \\alpha)q(x)) ||s_\\psi(x; \\alpha)||^2 - 2( \\alpha p(x)s_p(x) + (1 - \\alpha)q(x)s_{q_\\theta}(x)) \\}^2 dx + C'$\n$- \\int \\{ \\frac{(\\alpha p(x)s_p(x) + (1 - \\alpha)q(x)s_{q_\\theta}(x))}{\\alpha p(x) + (1 - \\alpha)q(x)} - s_\\psi(x; \\alpha)\\}^2(\\alpha p(x) + (1 - \\alpha)q(x)) dx + C'$\nHence, it is clear that the global minimizer should be\n$s_\\psi^*(x; \\alpha) = \\frac{\\alpha p(x)s_p(x) + (1 - \\alpha)q(x)s_{q_\\theta}(x)}{\\alpha p(x) + (1 - \\alpha)q(x)}$\n$= \\frac{\\alpha p(x)s_p(x) + (1 - \\alpha)q(x)s_{q_\\theta}(x)}{\\alpha p(x) + (1 - \\alpha)q(x)}$\n$= \\frac{\\alpha \\nabla_x p(x) + (1 - \\alpha) \\nabla_x q(x)}{\\alpha p(x) + (1 - \\alpha)q(x)}$\n$= \\nabla_x \\log(\\alpha p(x) + (1 - \\alpha)q_\\theta(x)).$"}, {"title": "A.3. Deferred Statements", "content": "Let $\\alpha \\in [0, 1]$ be fixed and $\\sigma_t$ be some fixed noise level. Then, the minimizer of the objective function\n$\\mathcal{L}_{score}(\\psi; \\alpha, t) := \\alpha E_{p(x)q(\\epsilon)} [||s_\\psi(x; \\alpha,t) + \\epsilon/\\sigma_t||^2] + (1 - \\alpha) E_{q_\\theta(x)q(\\epsilon)} [||s_\\psi(x; \\alpha,t) + \\epsilon/\\sigma_t||^2]$ (16)\nsatisfies\n$s_\\psi^*(x; \\alpha, t) = s_{\\theta;\\alpha,t}(x).$\nwe can write the objective $\\mathcal{L}(\\psi; \\alpha, t)$ as\n$\\mathcal{L}_{score}(\\psi; \\alpha, t) = \\int \\int (\\alpha p(x_t) + (1 - \\alpha)q(x_t)) ||(s_\\psi(x_t; \\alpha,t) + \\frac{\\epsilon}{\\sigma_t})||^2 dx d\\epsilon.$\nThis is a standard minimum mean square estimation (MMSE) problem for which the global minimizer is the conditional mean,\n$\\frac{1}{\\sigma_t^2} E_{\\alpha p_t + (1 - \\alpha)q_{\\theta,t}} [x - x_t]$\n$\\frac{1}{\\sigma_t^2} E_{\\alpha p_t + (1 - \\alpha)q_{\\theta,t}} [xx_t]$\n$= \\nabla_{x_t} \\log(\\alpha p(x_t) + (1 - \\alpha)q_\\theta(x_t)).$\nHere we use that $x_t = x + \\sigma_t \\epsilon$ and make the connection to the marginal score in the last line using Tweedie's formula (Robbins, 1956).\nLet $\\alpha \\in [0, 1]$, $s_p(x)$ be the data score, $s_{q_\\theta}(x)$ be the score of the generated samples. Then, the score of the mixture distribution can be expressed as\n$s_{\\theta;\\alpha}(x) = D_{\\theta;\\alpha}(x) s_p(x) + (1 - D_{\\theta;\\alpha}(x)) s_{q_\\theta}(x),$\n$D_{\\theta;\\alpha}(x) := \\sigma(\\log \\frac{p(x)}{q_\\theta(x)} + \\log \\frac{\\alpha}{1-\\alpha}).$ (18)"}]}