{"title": "Fine-tuning LLMs for Autonomous Spacecraft Control: A Case Study Using Kerbal Space Program", "authors": ["Alejandro Carrasco", "Victor Rodriguez-Fernandez", "Richard Linares"], "abstract": "Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous agents that take actions based on the content of the user text prompt. This study explores the use of fine-tuned Large Language Models (LLMs) for autonomous spacecraft control, using the Kerbal Space Program Differential Games suite (KSPDG) as a testing environment. Traditional Reinforcement Learning (RL) approaches face limitations in this domain due to insufficient simulation capabilities and data. By leveraging LLMs, specifically fine-tuning models like GPT-3.5 and LLaMA, we demonstrate how these models can effectively control spacecraft using language-based inputs and outputs. Our approach integrates real-time mission telemetry into textual prompts processed by the LLM, which then generate control actions via an agent. The results open a discussion about the potential of LLMs for space operations beyond their nominal use for text-related tasks. Future work aims to expand this methodology to other space control tasks and evaluate the performance of different LLM families. The code is available at this URL: https://github.com/ARCLab-MIT/kspdg.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are, without a doubt, the last major breakthrough in the evolution of artificial intelligence systems. Since the release of ChatGPT [1] at the end of 2022, we have seen a plethora of applications and use cases emerge across various industries. From generating human-like text to aiding in code completion, LLMs have significantly impacted the way we interact with technology and the possibilities of what AI can achieve.\nIn recent months, the use of LLMs is expanding beyond text-based applications to become language agents capable of taking actions based on the context of the system in which they are integrated. By leveraging the contextual information available to them, LLMs can make informed decisions and perform tasks autonomously. This new way of creating autonomous agents intersects with the usage of Reinforcement Learning (RL) algorithms, and provides a way to overcome some of its well-known limitations, such as the sample inefficiency, and the need for a well-defined reward function. Some recent studies have demonstrated how some powerful LLMs, such as GPT-4, can surpass state-of-the-art RL algorithms in complex games just through studying academics texts and reasoning [2], executing sophisticated trajectories and achieving good zero-shot performance.\nThis work is focused on the domain of space applications and the development of autonomous agents for guidance and control of spacecrafts. In this context, the creation of AI-based agents has mainly been tackled through RL during recent years, and in fact, we can find RL-based agents for different tasks such as sensor-tasking [3] and planetary landing [4]. However, unlike other AI research areas, the space domain lacks of publicly available simulation environments, which are crucial for training AI agents in complex space operations and providing a standard benchmark for evaluating different AI and autonomous control methods. To address this issue, Allen et al. introduced SpaceGym [5], a set non-cooperative game environments that are intended to spur development and act as proving grounds for autonomous and AI decision-makers in the orbital domain. Among the available environments in SpaceGym, in this work we focus on the Kerbal Space Program Differential Games suite (KSPDG). KSPDG is a suite of differential games, such as pursuit-evasion scenarios, encoded within the Kerbal Space Program (KSP) game engine \u00b9 and standardized with OpenAI Gym [6] and PettingZoo [7] interfaces, facilitating the use of diverse AI techniques, including multi-agent reinforcement learning.\nWhile KSPDG presents an innovative framework for testing AI and autonomous control methods in space applications, it is unsuitable for RL training, due to technical and non-technical reasons. On the one hand, the KSP engine, which underpins KSPDG, lacks the capacity for the parallel, accelerated, and headless operations essential for extensive faster-than-real-time RL training. On the other hand, the principled stance of KSPDG's creators to focus on evaluation rather than training emphasizes the need for a \"true test set\" environment where overfitting is minimized, and the genuine and unbiased capabilities of AI agents are tested. This approach diverges from the typical RL methodology that relies on iterative training and fine-tuning of agents within a specific simulation environment.\nTo overcome the limitations of RL in creating autonomous agents for environments such as KSPDG, as well as for other space operations where numerous simulated data cannot be provided, we propose to adapt the current trend of LLM-based agents to develop an \u201cintelligent\" operator that controls a spacecraft based on the real-time telemetry of the environment, using language exclusively as the input and output of the system. As depicted in Figure 1, we design the classic RL loop by interfacing the simulation environment (KSPDG) with a LLM, transforming the real-time observations (or state) of the mission as textual user prompts that are fed to the model. The LLM then processes the prompt and replies with an action that will be plugged in KSPDG to control the spacecraft. Our agent was ranked 2nd in the KSPDG challenge 2, and was presented via a live demonstration during a special session at AIAA SciTech in January 2024.\nIn our previous study [8], prompt engineering was the primary focus, and LLM models demonstrated outstanding performance with zero-shot and few-shot prompts. That work utilized prompt engineering to effectively control a spacecraft in the Kerbal Space Program (KSP) simulation, achieving exceptional but non-generalizable results. Additionally, some fine-tuning experiments were conducted using the OpenAI fine-tuning API. At the time, the latest and most powerful model available for fine-tuning was gpt-3.5-turbo-0125. The customization of this fine-tuning process relied mainly on the data and three hyperparameters: number of epochs, batch size, and learning rate multiplier [9]. These experiments achieved moderate results due to the API's limitations and its economical cost of fine-tuning.\nThe objective of the current study is to expand upon previous research by incorporating a set of open-source tools to overcome the limitations of the earlier framework. In contrast to the previous paper, this study focuses solely on the PE1_I3_E3 scenario. PE refers to the pursuer-evader problem (rendezvous), I3 denotes the initial position (2.7 km of separation distance), and E3 represents a heuristic maneuvering technique. [10]. In the past two years, the landscape of LLMs has undergone significant transformations, characterized by frequent and substantial updates. Given these advancements, the \"mighty\" ChatGPT model is now rivaled by capable models such as Claude[11], Gemini[12], and open-source models like Mistral [13] and LLaMA[14]. This research focuses on these latter models due to their enhanced flexibility and open source nature, which is crucial for pioneering research in this relatively unexplored area.\nThe integration of a code agent with KSP is facilitated through a Remote Procedure Call (RPC) program that connects to the selected environment within the game. After each state update, the agent is able to execute an action from a defined set of continuous throttle commands. For a more verbose interaction with the LLM, the actions are verbal\u2014forward, backward, right, left, up, and down\u2014which are then converted into full throttle, full reverse throttle, or no action for each of the three thrusts. This discretization also allows the model to be more like a 'human pilot' instead of a control algorithm."}, {"title": "2 Results", "content": "In Table 1, we present the results of fine-tuned GPT models using human gameplays and LLaMA models using navball agent gameplays. GPT models show gradual improvement, surpassing the baseline after two training gameplays, indicating the potential of a fine-tuned model with more gameplays. Note that the simple fine-tuning in the GPT experiments used only one file, basic prompting, and the default hyperparameters selected by the OpenAI API. The LLaMA dataset was divided into subsets of 10, 25, and 50 gameplay files. One subset\u2014the 10 files utilized a sliding window technique, where previous actions were included to provide the model with additional context. LLaMA models consistently exceed their baseline performance, where the closest distance is ~120 meters better than GPT's baseline5. The best LLaMA models perform exceptionally well, demonstrating the potential benefits of larger datasets, indicating the potential of a fine-tuned model with more gameplays. Finally, the model utilizing the sliding window technique demonstrates great performance results leveraging the context of the LLM.\nThe fine-tuning trajectories in Figure 3 indicate that the data ingested by the model aids in understanding the problem and determining appropriate actions. However, these trajectories also show that the model's prior knowledge and reasoning still influence its performance. For instance, an incorrect hint, as depicted by the GPT trajectory, deteriorates the model's performance and makes the agent recede from the evader once it overshoots (meaning when it goes past the evader). In contrast, an \"agnostic\" prompt that complements rather than dictates the model's reasoning can even surpass the dataset results."}, {"title": "3 Discussion", "content": "The preliminary results of this study demonstrate that LLMs possess the capability to perform technical tasks beyond merely generating verbose text. Fine-tuning these models enhances their reasoning for autonomous space control missions without solely depending on the hints and reasoning provided in the prompt. This process yields a generalized model that can interact as an agent in KSP rendezvous missions.\nThe fine-tuned LLaMA model clearly surpasses the navball agent results, even in average distance, which poses a singular use case where the model outperforms the agent responsible of creating its own training data. The tests were run on validation sets, thus remarking the generalization capacity LLMs offer.\nHowever, the complexity of a trajectory is not solely determined by distance. This highlights the necessity for more complex and dynamic scenarios that require additional metrics. This issue will be addressed in the next stages of this research.\nMoreover, the close distances achieved by the LLaMA models pave the way for exploring docking operations, where the increased complexity will test the robustness of the model's chosen trajectories.\nWe also plan on leveraging the advantages offered by multi-modal LLMs, such as the recently released GPT-4o [22] (where 'o' stands for 'omni') and the Phi-3 family [23], an open-source model incorporating multi-modal capabilities. Specifically, we intend to utilize vision capabilities in conjunction with language, as both modalities show the potential for creating an agent with human-like decisions."}]}