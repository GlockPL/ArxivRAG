{"title": "Fine-tuning LLMs for Autonomous Spacecraft Control: A Case Study Using Kerbal Space Program", "authors": ["Alejandro Carrasco", "Victor Rodriguez-Fernandez", "Richard Linares"], "abstract": "Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous agents that take actions based on the content of the user text prompt. This study explores the use of fine-tuned Large Language Models (LLMs) for autonomous spacecraft control, using the Kerbal Space Program Differential Games suite (KSPDG) as a testing environment. Traditional Reinforcement Learning (RL) approaches face limitations in this domain due to insufficient simulation capabilities and data. By leveraging LLMs, specifically fine-tuning models like GPT-3.5 and LLaMA, we demonstrate how these models can effectively control spacecraft using language-based inputs and outputs. Our approach integrates real-time mission telemetry into textual prompts processed by the LLM, which then generate control actions via an agent. The results open a discussion about the potential of LLMs for space operations beyond their nominal use for text-related tasks. Future work aims to expand this methodology to other space control tasks and evaluate the performance of different LLM families. The code is available at this URL: https://github.com/ARCLab-MIT/kspdg.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are, without a doubt, the last major breakthrough in the evolution of artificial intelligence systems. Since the release of ChatGPT [1] at the end of 2022, we have seen a plethora of applications and use cases emerge across various industries. From generating human-like text to aiding in code completion, LLMs have significantly impacted the way we interact with technology and the possibilities of what AI can achieve.\nIn recent months, the use of LLMs is expanding beyond text-based applications to become language agents capable of taking actions based on the context of the system in which they are integrated. By leveraging the contextual information available to them, LLMs can make informed decisions and perform tasks autonomously. This new way of creating autonomous agents intersects with the usage of Reinforcement Learning (RL) algorithms, and provides a way to overcome some of its well-known limitations, such as the sample inefficiency, and the need for a well-defined reward function. Some recent studies have demonstrated how some powerful LLMs, such as GPT-4, can surpass state-of-the-art RL algorithms in complex games just through studying academics texts and reasoning [2], executing sophisticated trajectories and achieving good zero-shot performance.\nThis work is focused on the domain of space applications and the development of autonomous agents for guidance and control of spacecrafts. In this context, the creation of AI-based agents has mainly been tackled through RL during recent years, and in fact, we can find RL-based agents for different tasks such as sensor-tasking [3] and planetary landing [4]. However, unlike other Al research areas, the space domain lacks of publicly available simulation environments, which are crucial for training AI agents in complex space operations and providing a standard benchmark for evaluating different AI and autonomous control methods. To address this issue, Allen et al. introduced SpaceGym [5], a set non-cooperative game environments that are intended to spur development and act as proving grounds for autonomous and AI decision-makers in the orbital domain. Among the available environments in SpaceGym, in this work we focus on the Kerbal Space Program Differential Games suite (KSPDG). KSPDG is a suite of differential games, such as pursuit-evasion scenarios, encoded within the Kerbal Space Program (KSP) game engine \u00b9 and standardized with OpenAI Gym [6] and PettingZoo [7] interfaces, facilitating the use of diverse AI techniques, including multi-agent reinforcement learning.\nWhile KSPDG presents an innovative framework for testing Al and autonomous control methods in"}, {"title": "2 Results", "content": "In Table 1, we present the results of fine-tuned GPT models using human gameplays and LLaMA models using navball agent gameplays. GPT models show gradual improvement, surpassing the baseline after two training gameplays, indicating the potential of a fine-tuned model with more gameplays. Note that the simple fine-tuning in the GPT experiments used only one file, basic prompting, and the default hyperparameters selected by the OpenAI API. The LLaMA dataset was divided into subsets of 10, 25, and 50 gameplay files. One subset\u2014the 10 files utilized a sliding window technique, where previous actions were included to provide the model with additional context. LLaMA models consistently exceed their baseline performance, where the closest distance is ~120 meters better than GPT's baseline5. The best LLaMA models perform exceptionally well, demonstrating the potential benefits of larger datasets, indicating the potential of a fine-tuned model with more gameplays. Finally, the model utilizing the sliding window technique demonstrates great performance results leveraging the context of the LLM.\nThe fine-tuning trajectories in Figure 3 indicate that the data ingested by the model aids in understanding the problem and determining appropriate actions. However, these trajectories also show that the model's prior knowledge and reasoning still influence its performance. For instance, an incorrect hint, as depicted by the GPT trajectory, deteriorates the model's performance and makes the agent recede from the evader once it overshoots (meaning when it goes past the evader). In contrast, an \"agnostic\" prompt that complements rather than dictates the model's reasoning can even surpass the dataset results."}, {"title": "3 Discussion", "content": "The preliminary results of this study demonstrate that LLMs possess the capability to perform technical tasks beyond merely generating verbose text. Fine-tuning these models enhances their reasoning for autonomous space control missions without solely depending on the hints and reasoning provided in the prompt. This process yields a generalized model that can interact as an agent in KSP rendezvous missions.\nThe fine-tuned LLaMA model clearly surpasses the navball agent results, even in average distance, which poses a singular use case where the model outperforms the agent responsible of creating its own training data. The tests were run on validation sets, thus remarking the generalization capacity LLMs offer.\nHowever, the complexity of a trajectory is not solely determined by distance. This highlights the necessity for more complex and dynamic scenarios that require additional metrics. This issue will be addressed in the next stages of this research.\nMoreover, the close distances achieved by the LLaMA models pave the way for exploring docking operations, where the increased complexity will test the robustness of the model's chosen trajectories.\nWe also plan on leveraging the advantages offered by multi-modal LLMs, such as the recently released GPT-4o [22] (where 'o' stands for 'omni') and the Phi-3 family [23], an open-source model incorporating multi-modal capabilities. Specifically, we intend to utilize vision capabilities in conjunction with language, as both modalities show the potential for creating an agent with human-like decisions."}]}