{"title": "OmniPrism: Learning Disentangled Visual Concept for Image Generation", "authors": ["Yangyang Li", "Daqing Liu", "Wu Liu", "Allen He", "Xinchen Liu", "Yongdong Zhang", "Guoqing Jin"], "abstract": "Creative visual concept generation often draws inspiration from specific concepts in a reference image to produce rel-evant outcomes. However, existing methods are typically constrained to single-aspect concept generation or are eas-ily disrupted by irrelevant concepts in multi-aspect concept scenarios, leading to concept confusion and hindering cre-ative generation. To address this, we propose OmniPrism\u00b9, a visual concept disentangling approach for creative image generation. Our method learns disentangled concept rep-resentations guided by natural language and trains a dif-fusion model to incorporate these concepts. We utilize the rich semantic space of a multimodal extractor to achieve concept disentanglement from given images and concept guidance. To disentangle concepts with different seman-tics, we construct a paired concept disentangled dataset (PCD-200K), where each pair shares the same concept such as content, style, and composition. We learn disentangled concept representations through our contrastive orthogo-nal disentangled (COD) training pipeline, which are then injected into additional diffusion cross-attention layers for generation. A set of block embeddings is designed to adapt", "sections": [{"title": "1. Introduction", "content": "With the advancement of text-to-image (T2I) generation based on diffusion models [24, 28, 29, 31], creative image generation has gained increasing attention, particularly in visual concept generation, which enables the generation of specific concepts from reference images, offering stronger flexibility and control. Visual concepts in an image can be divided into three orthogonal components: content (seman-tic subjects, background, etc.), style (art style, color, etc.), and composition (relation, camera views, layout, pose, etc.). These concepts can be utilized individually or in combina-tion to produce creative outcomes.\nSome recent studies focus on extracting single concepts from reference images for downstream generation tasks, such as subject customization [5, 9, 18, 30, 31, 33, 43], stylization [10, 15, 26, 34, 42], relationship customization [13, 14], and spatial condition generation [48]. How-ever, these methods are limited to specific aspects of con-cept and lack flexibility for generating diverse concepts (see Fig. 2 (a)). Other works [2, 3] use subject masks to generate a single subject concept from images with multiple subjects, achieving relatively diverse subject disentanglement. How-ever, they do not address abstract concepts that cannot be selected with a mask, such as style or relationships. Addi-tionally, these methods often require fine-tuning during in-ference or complex additional conditions for each sample, which limits their applicability.\nOther approaches aim to learn multi-aspect concept gen-eration within a unified model. Xu et al. [44] and Yael et al. [39] bind the concepts and attributes of an image to a set of tokens, but their similarity of the generated concepts is limited. Some works utilize multimodal encoders to ex-tract language-driven visual representations. For example, BLIP-Diffusion [19] fuses image and text features through attention, while DeaDiff [26] injects visual representations into specific layers of U-Net to generate disentangled style concepts. SSR-Encoder [50] aligns visual representations to the CLIP [27] language space. However, these methods cannot disentangle different concepts in the representation space, leading to concept confusion, i.e., irrelevant concepts blend in images, as shown in red circle of Fig. 2 (b).\nIn this work, we explore learning to disentangle different concepts in reference images using natural language guid-ance and generate corresponding results. Previous research face challenges with concept confusion due to: 1) the lack of data samples that highlight target concepts, leading mod-els to reconstruct the reference image during training, thus failing to accurately perceive the target concepts; 2) inad-equate differentiation between different concepts, resulting in concept coupling in the representation space and mixed results influenced by irrelevant concepts. To address these issues, we design a data-driven method that explicitly con-structs positive and negative samples to highlight target con-cepts and employs a contrastive learning mechanism to en-courage the model to disentangle different concepts from the reference image.\nSpecifically, we propose a visual concept generation ap-proach based on contrastive orthogonal disentangled learn-ing, termed OmniPrism. We introduce a learnable Q-Former [20] as a multimodal representation extractor for its rich semantic space. To extract corresponding concept rep-resentations based on natural language, we design a Con-trastive Orthogonal Disentangle (COD) learning mecha-nism, which constrains the concept representations (guided by language) to be close to the same concept and orthogonal to irrelevant concepts, thus achieving concept disentangle-ment. We also design a novel block embedding, which adds a learnable embedding to each diffusion block to adaptively align different concept domains for generating more match-ing concept results. These disentangled concept represen-tations are fed into a set of additional cross-attention layers in the U-net for generation. To facilitate learning concept disentanglement and generation through our method, we construct a Paired Concept Disentanglement Dataset (PCD-200K) with 200K pairs, where each pair contains a shared concept and other distinct concepts.\nIn addition, by the advantages of constraining different types of concept representations to be orthogonal during training, these concepts can be combined without conflict,"}, {"title": "2. Related Work", "content": "Visual concept generation based on diffusion models is typ-ically conditioned on both image and text, aiming to gen-erate results that incorporate visual concepts from image (subject, style, relation, etc.) while faithful to text prompts. Text Inversion [8] optimizes an extra embedding for a refer-ence image in the text domain to generate similar subjects. DreamBooth [30] binds a unique identifier in the token do-main to the reference image by optimizing the model and generate customized results. ControlNet [48] adds an addi-tional control branch to learn the structural representations (e.g. edges, depth) from the reference images and gener-ate corresponding results. Some encoder-based methods [4, 43, 45] introduce image encoders (VIT [7], DINO [47], etc.) to extract representations of reference images and train a model to fit them for training-free generation. IP-Adapter [46] designs a set of plug-and-play adapters to help the models learn image representations without compromis-ing its original capabilities. Other works injects different conditions into different U-Net blocks [1, 26, 40, 41] or dif-ferent time steps [1, 49] during inference to separate con-cepts. However, these methods are limited to single con-cept and cannot to be flexibly applied to multiple concepts, leading to concept conflict. In contrast, our method uses language as guidance to disentangle desired concepts from images, thereby achieving accurate multi-aspect visual con-cept generation without concept conflict."}, {"title": "2.2. Multi-Aspect Visual Concept Generation", "content": "Compared with single-aspect visual concept generation, multi-aspect visual concept generation incorporates addi-tional conditions as guidance to flexibly generate the de-sired concepts in the reference images. Some inference-tuning works [2, 16, 51] using additional spatial control (mask, layout, etc.) or multiple reference images, binding the desired concept to a language mnemonic to achieve con-cept disentanglement. However, these methods are time-consuming for learning each sample and require complex control conditions, which limits their practical application. Xu et al. [44] and Yael et al. [39] bind the concepts and attributes of an image to a set of tokens, but the similar-ity of the generated concepts was limited. Other works use a multimodal encoder to extract concept representa-tions based on text. Blip-Diffusion [19] employs Q-Former [20] to jointly learn image features and text features of sub-ject name. However, they only use this multi-modal in-teraction to enhance visual representations and cannot dis-entangle visual concepts described by text. DEADiff [26] uses \"content\" and \u201cstyle\u201d as guidance to extract represen-tations and inject them into certain U-Net layers to disen-tangle style concepts. However, they cannot disentangle concepts except style since their limited language domain. SSR-Encoder [50] aligns image representations with lan-guage guidance in the CLIP [27] representations space and generate corresponding concept as language guidance. But they suffer from the trade-off of concept fidelity and concept independence, since the CLIP space lacks the property of concept disentanglement. Different from these works, our approach disentangles different concept representations by contrastive orthogonal disentangled learning, thereby gen-erating desired concepts without confusion and achieving disentangled multi-aspect visual concept generation."}, {"title": "3. Method", "content": "Given a text prompt $T_{tar}$, a reference image $I_{ref}$, and a concept guidance $T_{cg}$, our goal is to generate an image $I_{tar}$ that is faithful to $T_{tar}$ and incorporates the desired concept without irrelevant concepts in $I_{ref}$. To achieve this, we propose a disentangled visual concept generation approach termed OmniPrism, which employs a concept extractor (Sec. 3.2) to disentangle specified concept rep-resentations $f_{cpt}$ from $I_{ref}$ guided by $T_{cg}$ and feed $f_{cpt}$ into additional cross-attention layers of diffusion models. A set of block embeddings $e_1$ to $e_N$ (Sec. 3.2) are de-signed to align the concept domain of corresponding diffu-sion model blocks, n is the number of diffusion blocks. We design a Contrastive Orthogonal Disentangled (COD) learn-ing mechanism (Sec. 3.3) and combine it with our PCD-200K (Sec. 3.4) dataset to achieve the concept disentangle-ment."}, {"title": "3.1. Preliminary", "content": "Diffusion models [29] are generative models that use a network $\\epsilon_{\\theta}$ to gradually denoise random Gaussian noise $z_T \\sim \\mathcal{N}(0, 1)$ to learn the data distribution. This is the reverse process of adding noise to the image $z_0$ by $T$ steps Markov chain. Diffusion models use text prompts $c$ as con-ditions for text-to-image generation, and the training objec-tive is to predict the noise $\\epsilon$ added to the noised latent $z_t$ at time step $t$, which can be simplified as a variant of the variational bound:\n$\\mathcal{L}_{idm} = \\mathbb{E}_{z, c, \\epsilon \\sim \\mathcal{N}(0, 1), t}[||\\epsilon_{\\theta}(z_t, c, t) - \\epsilon||^2]$.\nDuring inference, $\\epsilon$ gradually denoises $z_T \\sim \\mathcal{N}(0, 1)$ using various samplers [21, 23, 36].\nFor conditional generation, recently diffusion mod-els employ classifier-free guidance [12] to jointly train conditional models $\\epsilon_{\\theta}(z_t, c, t)$ and unconditional models $\\epsilon_{\\theta}(z_t, \\emptyset, t)$ by dropping out $c$, thereby the noise predicted during inference is:\n$\\hat{\\epsilon}_{\\theta} (z_t, c, t) = \\omega \\cdot \\epsilon_{\\theta}(z_t, c, t) + (1 - \\omega) \\cdot \\epsilon_{\\theta}(z_t, \\emptyset, t)$,\nhere $\\omega$ is the guidance scale which controls the strength of condition $c$ in generated results."}, {"title": "3.2. Disentangling Visual Concept For Generation", "content": "Images contain rich visual concept representations, and vi-sual encoders like ViT [6] cannot distinguish these con-cepts. To efficiently obtain disentangled visual concept representations for generation, we use natural language as guidance to disentangle corresponding concepts from ref-erence image. Previously, Blip-Diffusion [19], DEADiff [26], and SSR-Encoder [50] use multi-modal encoders as concept extractor. However, they are all struggled in disen-tangling general visual concepts from images. Inspired by them, we introduce a pre-trained Q-Former [20] with strong multi-modal alignment capabilities as concept extractor to learn from paired data how to disentangle different concepts guided by language, as illustrated in Fig. 3.\nSpecifically, we construct a PCD-200K dataset (Sec. 3.4), each sample includes reference and target image/prompt, and a natural language phrase as concept guidance $T_{cg}$ which describes the shared concept in both images. The CLIP [27] representations of the reference image $f_1$ and concept guidance $f_{cg}$ are fed into the concept extractor $E$ and interact with a learnable query $q$ through several cross-attention layers, the output $H$ of the first attention layer is:\n$H_c = Attn(cat(q + e_i, f_{cg})W_q, f_1W_k, f_1W_v)$,\nwhere $Attn(Q, K, V) = softmax(\\frac{QW^T}{\\sqrt{d}})V$,\nwhere $e_i$ is the block embedding corresponds to the i-th dif-fusion block, $W$ are attention weights of $E$. Subsequently, the disentangled concept representation $f_{cpt}$ output from $E$ are fed into the i-th diffusion block through a set of addi-tional cross-attention adapters:\n$H_{i+1} = \\mu \\cdot Attn(H_iW_q, f_{cpt} W_k, f_{cpt} W_v) + Attn(H_iW_q, c_tW_k, c_tW_v)$,\nwhere $W'$ are the newly added attention weights, $W$ are the original attention weights, $\\mu$ is the scale of attention out-put of $f_{cpt}$. The classifier-free guidance becomes:\n$\\hat{\\epsilon}_{\\theta} (z_t, c_t, f_{cpt}, t) = \\omega \\cdot \\epsilon_{\\theta}(z_t, c_t, f_{cpt}, t) + (1 - \\omega) \\cdot \\epsilon_{\\theta} (z_t, \\emptyset, \\emptyset, t)$,\nBlock Embeddings. Recent works [26, 40] have found that different blocks in diffusion model have varying impacts on generated results. Coarse layers tend to learn low-level con-cepts such as style and color, while fine layers capture high-level semantic concepts. Therefore, some works [26, 41] inject reference style image features into the coarse layers"}, {"title": "3.3. Contrastive Orthogonal Disentangled Learning", "content": "Previous works [19, 26, 50] use multimodal encoders to ex-tract visual representations guided by language, but they lack a effective concept disentangling mechanism and get conflict results. To extracted concept-disentangled repre-sentations, we design a Contrastive Orthogonal Disentan-gled (COD) learning mechanisms in training stage to disen-tangle each types of visual concepts into mutually orthogo-nal dimensions in the representation space. Since these rep-resentations are orthogonal, they can be easily combined for multi-concept generation without conflict.\nSpecifically, we design a learnable anti-query $q_a$ of the same size as $q$ to capture irrelevant concepts $f_{cpt}^a$ in the reference image as Eq. (3). To learn $q$ and $q_a$, we input $I_{tar}, q$ and the concept guidance $T_{cg}$ into our concept ex-tractor to obtain the concept representation $f_{cpt}^{tar}$ in $I_{tar}$. Subsequently, $f_{cpt}^a$ and $f_{cpt}$ are concatenated and fed to the additional cross-attention layers of U-Net to generate $I_{ref}$. This additional training branch facilitates the combine generation of different concepts. Furthermore, we design a contrastive orthogonal disentangled Loss $\\mathcal{L}_{COD}$, which constrains $f_{cpt}$ and $f_{cpt}^{tar}$ to be similar in the representation space, and $f_{cpt}^a$ and $f_{cpt}^{tar}$ to be orthogonal:\n$\\mathcal{L}_{COD} = |cos(f_{cpt}, f_{cpt}^{tar})| - |cos(f_{cpt}^a, f_{cpt}^{tar})|$\nwhere $cos(\\cdot)$ is cosine similarity, and the total loss is:\n$\\mathcal{L}_{total} = \\mathcal{L}_{idm} + \\lambda \\cdot \\mathcal{L}_{COD}$,\nwhere $\\lambda$ is the weight to control the influence of $\\mathcal{L}_{COD}$."}, {"title": "3.4. Paired Concept-Disentangled Dataset", "content": "Previous visual concept generation methods [4, 8, 30, 43, 45] typically use L2 loss to reconstruct training sam-ples, which makes it difficult to extract different concepts from a image. Therefore, we design a data construction pipeline to generate paired data specific to certain con-cepts, named paired concept-disentangled dataset (PCD-200K) with 200K paired data. We divide the visual con-cepts in the image into three components: content, style, and composition, which are the fundamental visual con-cepts of an image. Our goal is to describe the desired visual concepts in natural language. Therefore, each pair of our dataset include reference image $I_{ref}$ and target image $I_{tar}$, reference prompt $T_{ref}$ and target prompt $T_{tar}$, and concept guide $T_{cg}$ in simple natural language, which indicates the shared visual concept between the two images. Specifically, we design three different pipelines to build different visual concepts. We first apply GPT-4 2 to generate refer-ence and target prompts and concept guidance. For con-tent concept, we use FLUX 3 and Kolors-inpainting [37] to generate two images that are different except for the shared subject. For style concept, we apply Instant-Style [41] to generate two images with the same style. For composition, we use ControlNet-Depth [48] to generate two images with the same composition. Concept guidance for content is the name of the subject (e.g. cat, man), while for the other two types of concept is \"style\" and \u201ccomposition\u201d, as it is chal-lenging to accurately describe these concepts in natural lan-guage. We give a more detailed description of the data con-struction pipeline in the supplementary material."}, {"title": "4. Experiments", "content": "Implementation Details. Our experiments are based on Stable Diffusion XL 1.0 [25], and using CLIP-L/14 [27] as text and image encoder. We initialize our concept ex-tractor with the pre-trained BLIP-2's Q-Former [20], and the token number of learnable query, anti-query and block embeddings are set to 32. The additional cross-attention adapters to U-Net are initialized from IP-Adapter-XL-Plus [46]. We adopt AdamW optimizer [22] with a learning rate of le-5 and $\\lambda$ is set to 1e-4. Our model is trained on a single machine with 8*80GB A100 GPUs for 80,000 steps, with a batch size of 4 per GPU. During inference, we adopt DDIM sampler [36] with 20 sample steps and the guidance scale of classifier-free guidance $\\omega$ is set to 5.\nEvaluation Metrics. To fairly evaluate our performance against other methods, we collate 120 sets for different con-cepts, each set containing 4 results. We use the follow-ing metrics for evaluation. Mask CLIP-I [27] to measure the CLIP image similarity with masked subject in refer-ence. CLIP-T [11] to measure the CLIP similarity with text prompts. Style Similarity [35] to measure the style simi-larity with style reference. Aesthetic Score [32] to evaluate the quality of generated images using LAION-Aesthetics Predictor V24. All CLIP versions are ViT-B-32. We do not evaluate composition due to the lack of corresponding met-rics and comparison methods."}, {"title": "4.2. Main Results", "content": "We demonstrate the capabilities of our method from mul-tiple aspects, as shown in Fig. 4. Our method supports the disentangled generation of multiple concepts guided by nat-ural language, including single subject, individual subjects within multiple subjects, style, and composition (e.g. rela-tion, pose). These results proves that our method possesses a robust concept disentangled representation space and a strong visual concept disentangled generation capability. In addition, by constraining different types of concept repre-sentations to be orthogonal, these features can be naturally integrated into the same generated result in any combination without interfering with each other.\nQualitative Comparison. To fully evaluate the superior-ity of our method, we compare our method with the state-of-the-art visual concept generation methods, including IP-Adapter [46], BLIP-Diffusion [19], DEADiff [26] and SSR-Encoder [50]. The results are shown in Fig. 5. Both IP-Adapter and our OmniPrism are based on SDXL [25], and others are base on SD-v1.5 [29] as their latest version. It can be seen that existing visual concept generation methods"}, {"title": "4.3. Ablation Study", "content": "To evaluate the effectiveness of each component in our method, We conduct ablation experiments by removing each component from the original method. For block em-beddings (BE), we simply remove them. For contrastive orthogonal disentangled learning, we firstly remove the or-thogonality by modifying the Eq. (6) to:\n$\\mathcal{L}_{LCD} = |cos(f_{cpt}, f_{cpt}^{tar})| - cos(f_{cpt}^a, f_{cpt}^{tar})$,\nthen we remove all the contrastive orthogonal disentangled learning by delete the anti-query $q_a$ and $\\mathcal{L}_{COD}$. The results are shown in Fig. 8.\nEffect of Block Embedding. Our block embedding is de-signed to adaptively match the extracted concept represen-tations to the concept domains of different diffusion blocks. As shown in the second column of Fig. 8, after removing the block embedding, the extracted concepts are not well aligned to the concept domain of the diffusion model, the generated results exhibit discrepancies with the concepts in the reference image."}, {"title": "Effect of Contrastive Orthogonal Disentangled learning.", "content": "Our COD learning mechanism is designed to help the con-cept disentanglement. As shown in the third column of Fig. 8, without it, the generated results contain concepts ir-relevant to concept guidance (e.g., the boy in the first row), and the fidelity to the desired concept is compromised by these irrelevant concepts.\nEffect of Orthogonality. Removing only the orthogonality in COD Learning can still produce concept disentangled re-sults. However, when combining multiple concepts, these non-orthogonal concept representations interfere with each other and become mixed, as shown in the 4th column of Fig. 8. By adding orthogonality, the interference between concepts is greatly reduced when combining multiple dif-ferent types of concepts, resulting in the ideal concept com-bination results, as shown in the last column of Fig. 8."}, {"title": "5. Conclusion", "content": "In this paper, we propose OmniPrism, an innovative ap-proach for disentangled visual concept generation that dis-entangles individual concepts or combines multiple con-cepts into outputs. By integrating a multimodal Q-Former as a concept extractor and a Contrastive Orthogonal Dis-entangle (COD) learning mechanism, our method effec-tively disentangles different visual concepts into distinct clusters within the representation space guided by natural language. A novel block embedding further enhances the alignment of the concept representations with the diffusion block's concept domains, allowing for high-fidelity gener-ation that closely matches prompts and desired concepts. Moreover, the construction of the Paired Concept Disentan-glement Dataset (PCD-200K) offers a valuable resource for advancing research in this field. Our OmniPrism not only addresses concept confusion but also allows for the flexi-ble combination of multiple concepts from different images, paving the way for more innovative applications in visual concept generation. Future work could explore extending this framework to more complex scenarios and further re-fining the disentanglement process to enhance the diversity and applicability of generated concepts."}]}