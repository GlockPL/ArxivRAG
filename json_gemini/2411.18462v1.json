{"title": "Draft Model Knows When to Stop:\nA Self-Verification Length Policy for Speculative Decoding", "authors": ["Ziyin Zhang", "Jiahao Xu", "Tian Liang", "Xingyu Chen", "Zhiwei He", "Rui Wang", "Zhaopeng Tu"], "abstract": "Speculative Decoding (SD) has become an important technique in accelerating the inference speed of large language models. Conventional SD methods employ a fixed draft length, which ignores the token generation difficulty across tasks. Consequently, in this paper, we address such an issue and introduce SVIP - a difficulty-aware dynamic draft length policy for speculative decoding systems. Based on a theoretical lower bound of draft token acceptance rate and its inference-time approximation, SVIP adaptively determines the lengths of draft sequences based on the entropy of each draft token distribution. Experimental results on mainstream SD benchmarks and frameworks demonstrate the superior performance of SVIP, achieving up to 20% walltime speedup on SpecBench over baseline SD methods and 60% speedup on MT-Bench for long-form generation of up to 8K tokens. Moreover, SVIP is totally training-free and compatible with any existing SD methods that generate draft tokens autoregressively. Experimental results also show that SVIP yields consistent walltime improvement on top of GliDe & CaPE and EAGLE-2.", "sections": [{"title": "Introduction", "content": "Speculative decoding (Leviathan et al., 2023; Chen et al., 2023) is a novel technique that markedly enhances the generation wall-time of large language models (LLMs). This approach employs a small and efficient amateur model to draft sequences, while concurrently utilizing a larger and more powerful expert model to verify the drafts. By avoiding the autoregressive generation of each token through the target LLM, speculative decoding achieves improved efficiency while preserving the quality of the output.\nMany variants of speculative decoding have been proposed. A line of work focuses on developing"}, {"title": "Method", "content": "The overall objective of SVIP is to dynamically adapt draft length on-the-fly, stopping early if the current draft token's acceptance probability is low and otherwise continuing drafting. To introduce our method, we first provide the background on speculative decoding in Section 2.1. Then, based on the key observation that the acceptance probability of a draft token depends on the target model confidence - which is unavailable in the drafting phase, we derive a theoretical lower bound for the acceptance rate based on the draft model's entropy"}, {"title": "Preliminaries on Speculative Decoding", "content": "Suppose we have two LLMs p and q, where p is the larger (target) model, and q is the smaller (draft) model. Given an input sequence $x_{<t}$ of length t, and a draft length y, the draft model first samples y tokens $x_{t+1},\u2026\u2026, x_{t+y}$ autoregressive, which are verified by the target models in parallel to acquire the confidences $p(x_{t+1}),\u00b7\u00b7\u00b7, p(x_{t+y})$.\nThen, each draft token $x_{n+j}$ is accepted with probability $\\frac{p(x_{n+j})}{q(x_{n+j})}$, and otherwise rejected. In the latter case, a corrected token is sampled from the residual distribution $\\frac{\\max(q(x_{n+j})-p(x_{n+j}),0)}{\\Sigma_{i} \\max(q(x^i_{n+j})-p(x^i_{n+j}),0)}$, which guarantees that the overall output distribution is exactly the same as $p(x_{n+j})$ (Leviathan et al., 2023; Chen et al., 2023). This process is repeated until a maximum sequence length T is reached.\nThe complete algorithms for speculative decoding are given in Appendix A."}, {"title": "Theoretical Lower Bound of Acceptance Rate", "content": "From Section 2.1, it's easy to derive that given an input sequence $x_{<t}$ and a draft token $x_{t}$, its acceptance probability is $\\min \\Big(1, \\frac{p(x_{t})}{q(x_{t})}\\Big)$. Let $\\beta=\\mathbb{E}_{q(x_{t})}\\Big[\\min \\Big(1, \\frac{p(x_{t})}{q(x_{t})}\\Big)\\Big]$ denote the expected acceptance probability over $x_t$.\nThe short hand $p(x_{n})$ is used to denote the conditional probability $p(x_{n}|x_{<n})$ when there is no ambiguity. Throughout the work we use subscripts to indicate token indices in a sequence (e.g. $x_n$ for n-th token), and superscripts to indicate element indices in a vector (e.g. $x^i_n$ for i-th element in $x_n$)."}, {"title": "Empirical Estimation of Acceptance Rate", "content": "So far in this section, we have been assuming access to the target distribution p(xt) of the next token, which is unavailable in the drafting phase at inference time. Thus, to apply Equation (5) for actual acceptance rate estimation, we must approximate the cross entropy $H_{q,p}$ with information from only the draft model.\nTo tackle this issue, we first plot the relationship between $H_{q,p}$ and $H_q$ in Figure 3. For all three model families, the entropy ratio $H_{q,p}/H_q$ are concentrated in a narrow range between 1.0 and 1.3. Thus, we choose to approximate $H_{q,p}$ with a constant multiplication of $H_q$. Plugging it into Equation (5), we now have\n$\\beta \\geq 1 - \\sqrt{cHq}$,\nwhere c is a constant controlling the approximation ratio between $H_{q,p}$ and $H_q$. In Figure 4, we visualize the values derived from Equation (2), (5), and (6).\nWith Equation (6) providing a way to estimate acceptance probability using only the draft model's entropy, we can now adapt the draft length on-the-fly. After generating each draft token, we compute the estimated acceptance probability lower bound, and stop the draft process if it's lower than a certain threshold h. We note that since both c and h are"}, {"title": "Experiments", "content": "We validate the effectiveness of SVIP on Spec-Bench (Xia et al., 2024) using three distinct target models: Pythia 6.9B (Biderman et al., 2023), Qwen2.5 14B (Yang et al., 2024), and LLaMA-3 70B (Dubey et al., 2024), with Pythia-160M, Qwen2.5 0.5B, and LLaMA-3 8B as the draft models respectively.\nAs baselines, we consider two simple policies for draft length: 1) a constant draft length of 5, which is commonly used in the literature, and 2) the heuristics implemented in Hugging Face Transformers library (Wolf et al., 2019), where the draft length for the next draft iteration is increased by 2 if all draft tokens in the current iteration are accepted, and otherwise decreased by 1.\nWe set the sampling temperature to 0 on SpecBench (the alternatives are discussed in Appendix C). For each model, the entropy threshold t in SVIP is chosen from {0.2, 0.3, 0.4, 0.5} based on performance on 8 samples held out from MT-Bench (Zheng et al., 2023), which are 0.4 for Pythia, and 0.3 for Qwen2.5 and LLaMA-3. All experiments with Pythia and Qwen are conducted on a single 40GB A100, while experiments with LLaMA are conducted on 5 40GB A100s. To mitigate the impact of system performance varitations, we repeat all experiments with Pythia and Qwen for three times (using different random seeds when they are used) and report the average speedup over target-model-only autoregressive decoding. Also, since the memory consumption of verifying n draft tokens is quadratic in n, we limit the maximum draft length to 40 in both heuristics and SVIP scenarios, beyond which we start to encounter out-of-memory issues."}, {"title": "Long-form Generation", "content": "Most existing works on speculative decoding (Chen et al., 2023; Du et al., 2024) limit their experiments to generating short sequences of 128 tokens. To verify the wide applicability of SVIP, we also conduct experiments on long-form generation with up to 8K context. For this purpose, we use MT-Bench (Zheng et al., 2023) as the dataset and set the sampling temperature to 1, as we found that when using greedy decoding in long-form generation, both the draft and the target models are prone to repeat themselves, resulting in very low information entropy (see Appendix D for details). Other settings follow Section 3.1."}, {"title": "Applying SVIP to Other Draft Methods", "content": "In Section 3.1 and 3.2, we evaluated SVIP on vanilla speculative decoding, where a standard pre-trained Transformer decoder model from the target model's family is used as the draft model. However, in the past years many works on speculative decoding have proposed other stronger or more efficient draft models (Cai et al., 2024; Du et al., 2024; Li et al., 2024b). Since most of these works assume a constant draft length, SVIP is orthogonal to them and can be applied on top of them without any additional training.\nSpecifically, we consider GliDe with a CaPE (Du et al., 2024), where the draft model - named GliDe - is a small transformer decoder with cross-attention to the target model's hidden representations, while CaPE is a complementary tree expansion method to increase the acceptance rate of the draft token at each position. Following the settings of Du et al. (2024), we use Vicuna 7B, 13B, and 33B (Chiang et al., 2023) as the base models (for which the draft"}, {"title": "Related Work", "content": "Since Leviathan et al. (2023) and Chen et al. (2023) introduced speculative decoding into large language models, numerous works have followed their tracks in pursuit of more efficient LLM inference. We broadly categorize these works into three types: better draft models, draft tree expansion, and draft length control, which are orthogonal to each other. A more comprehensive review of speculative de-"}, {"title": "Conclusion", "content": "We propose SVIP, a flexible, training-free, and plug-and-play dynamic draft length policy for speculative decoding systems. Based on a theoretical lower bound of acceptance probability and its empirical approximation, SVIP determines whether to continue draft generation or to quit drafting based on the draft model's entropy after the generation of each draft token. With extensive experiments spanning various base models, draft methods, test domains, and generation length, we validated the effectiveness of SVIP, sparking new insights on speculative decoding and more efficient large language models."}, {"title": "The Complete Speculative Decoding Algorithms", "content": "In Algorithm 2 to 6, we present the complete algorithms of the vanilla speculative decoding in both the greedy decoding and the sampling scenarios. For the sampling scenario, the Verify and Correct methods in Algorithm 6 resolve to Algorithm 2 and 4. For greedy decoding, they resolve to Algorithm 3 and 5."}, {"title": "Alternatives for Acceptance Rate Lover Bound Computation", "content": "In Section 2, we used Pinsker's inequality to compute a lower bound for the expected acceptance probability:\n$\\beta = \\sum_x \\min (p(x), q(x))$\n$\\geq 1 - \\sqrt{2 \\cdot KL(q||p)}$.\nAnother way to compute the lower bound of acceptance probability can be derived from Bretagnolle-Huber inequality (Bretagnolle and Huber, 1978):\n$\\beta \\geq 1 - \\sqrt{1-e^{-KL(q||p)}}$.\nCompared with the Pinsker's bound, it's trivial to see that this bound is guaranteed to be always larger than 0. However, in practice we find that the Pinsker's bound is 11% tighter for Qwen2.5, 20% tighter for Pythia, and 43% tighter for LLaMA-3."}]}