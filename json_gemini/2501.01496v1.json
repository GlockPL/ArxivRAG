{"title": "ORACLE: A Real-Time, Hierarchical, Deep-Learning Photometric Classifier for the LSST", "authors": ["Ved G. Shahd", "Alex Gagliano", "Konstantin Malanchev", "Gautham Narayan"], "abstract": "We present ORACLE, the first hierarchical deep-learning model for real-time, context-aware classification of transient and variable astrophysical phenomena. ORACLE is a recurrent neural network with Gated Recurrent Units (GRUs), and has been trained using a custom hierarchical cross-entropy loss function to provide high-confidence classifications along an observationally-driven taxonomy with as little as a single photometric observation. Contextual information for each object, including host galaxy photometric redshift, offset, ellipticity and brightness, is concatenated to the light curve embedding and used to make a final prediction. Training on ~0.5M events from the Extended LSST Astronomical Time-Series Classification Challenge, we achieve a top-level (Transient vs Variable) macro-averaged precision of 0.96 using only 1 day of photometric observations after the first detection in addition to contextual information, for each event; this increases to >0.99 once 64 days of the light curve has been obtained, and 0.83 at 1024 days after first detection for 19-way classification (including supernova sub-types, active galactic nuclei, variable stars, microlensing events, and kilonovae). We also compare ORACLE with other state-of-the-art classifiers and report comparable performance for the 19-way classification task, in addition to delivering accurate top-level classifications much earlier. The code and model weights used in this work are publicly available at our associated GitHub repository.", "sections": [{"title": "1. INTRODUCTION", "content": "The chemical composition, density structure, and kinematics of a transient astrophysical phenomenon are all resolved through spectroscopy. For example, spectroscopy reveals the presence of elements in supernova ejecta, changes in density in collapsing stars through the broadening of emission lines, and motion of transients by measuring redshifts.\nThe fraction of discovered transient events that can be characterized spectroscopically, given current spectroscopic resources, will drop to < 1% with the advent of the Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST; Ivezi\u0107 et al. 2019). LSST's high \u00e9tendue, the product of the collecting area and solid angle seen by the detector, results in a survey speed an order of magnitude higher than any existing telescope, and it will discover ~ 102 more transients, particularly at high (z 1) redshift.\nOur paucity of spectroscopic follow-up resources, balanced against the rapid evolution of some of the rarest phenomena, has driven a need for near-instantaneous triaging to select the objects that will maximize an instrument's scientific return. While spectroscopic analysis reveals the underlying physics of a transient, the most immediate use of spectroscopy is to place an object within a taxonomic context - the realm of classification.\nLacking spectra, many groups have focused on developing photometric classifiers for diverse transient and variable phenomena (Lochner et al. 2016; Muthukrishna et al. 2019; M\u00f6ller & de Boissi\u00e8re 2020; Carrasco-Davis"}, {"title": "2. MOTIVATION", "content": "Taxonomies cluster diverse astrophysical phenomena into a comprehensible framework as a first attempt at understanding their fundamental nature. Transient and Variable phenomena can be classified in different ways: by the type of temporal variability they exhibit, such as periodic or stochastic, or by the physical processes driving them. For example, supernovae (SNe) are categorized based on their presumed explosion mechanism, such as thermonuclear or core collapse. Because each event is unique, transients can be further sub-divided into smaller and smaller bins based on a panoply of observed and intrinsic properties, including their progenitor systems, formation channels, and photometric or spectroscopic characteristics (for example, Calcium Rich Transients can be separated from SNe-Ib/c by the presence of strong Ca emission lines in their spectra). We present a time-domain taxonomy in Figure 1, which we adopt for this work. This taxonomy is loosely inspired by the structure of our training data set (See Section 3), in order to maintain consistency with the training labels. Taxonomies are rarely complete, and typically do not consider factors extrinsic to the event, even if they modify the event's characteristics - e.g., dust\u00b9. Nevertheless, this taxonomic clustering can be useful to characterize a population of astrophysical transients with limited information, including sub-types.\nAlthough manual classifications of time-domain phenomena have historically fallen along every level of a proposed taxonomy, automated classifiers have generally considered a fixed level of granularity. This leads to significant preprocessing to construct a labeled dataset for training, whereby events not labeled at the leaves are removed and events labeled with even higher granularity are consolidated (e.g., SNe Ib and SNe Ic, typically considered together as SNe Ib/c). This framing of the problem results in an 'all-or-nothing' classification task that does not naturally generalize to diverse science goals or observation phases.\nORACLE was designed to leverage every level of a time-domain taxonomy, giving us the ability to derive useful class information even when our model cannot produce confident classifications at the leaves. Adopt-"}, {"title": "2.1. Engineering Considerations", "content": "Broadly speaking, there are two approaches to achieve hierarchical classification:\n\u2022 Build a single classifier that can output the entire classification tree global hierarchical classification (GHC, Bertinetto et al. 2019; Villar et al. 2023; Schuurmans & Frasincar 2023).\n\u2022 Build a family of classifiers, one for each node in the tree, except for the leaf nodes, which classify between their children using the output of the classifier(s) higher up in the taxonomy - local hierarchical classification (LHC, S\u00e1nchez-S\u00e1ez et al. 2021).\nLHCs have multiple drawbacks, including increased training time as a result of using many classifiers and a training process that is sensitive to small changes in the classifier outputs at higher levels in the taxonomy, because of the inherent need to fine tune several classifiers. A critical engineering concern while developing a production-grade classification pipeline that can operate at LSST scale is the constraint of running several classifiers in series, as the next classifier in the hierarchy can only run inference after the output from the"}, {"title": "3. DATASET AND T\u0391\u03a7\u039f\u039d\u039f\u039c\u03a5", "content": "We use the simulated \"Extended LSST Astronomical Time-series Classification Challenge\" (ELASTICC2 v2.0, Narayan & ELASTICC Team 2023) dataset for training, validating, and testing ORACLE. Using a simulated dataset, such as ELASTICC v2.0, avoids the inconsistencies in class labels associated with observed events. ELASTICC contains 32 different models, which were mapped to 19 astrophysical classes (documented in Appendix A). This mapping introduces additional diversity to the labeled classes, and prevents a classifier from overfitting to the specific details of a transient model.\nTo classify the ELASTICC dataset, the natural choice is for the leaf nodes to reflect the true label of the object since any machine learning model developed with it only needs to predict from a known set of classes. The structure of layers higher in the taxonomy is free, and can be driven by specific science goals.\nThere is no universally-adopted taxonomy for time-domain astrophysics. Indeed, there is no universal notion of what the \"true class\" is for many of the most scientifically-valuable objects, e.g. objects such as SN2021foa (Farias et al. 2024), which exhibits \"flip-flop\" behavior, transitioning from IIn-to-Ibn-to-IIn, and such objects might be best identified by anomaly detection techniques (e.g. LAISS and AstroMCAD; Aleo et al. 2024; Gupta et al. 2024). Additionally, different ob-"}, {"title": "4. DATA AUGMENTATION", "content": "First, the dataset was divided into training and testing sets using a 70:30% split. Then, we keep at most 40,000 samples of a class in the training set and at most 20,000 samples of a class in the testing set where a sample is defined as a tuple of a light curve array and a static features array. We introduce the per-class sample limit to achieve a balance between model performance and training/inference time. From the modified training set, an additional 5% of the data was reserved for validation. The number of samples from each class for the training, validation, and testing sets are documented in Table 3.\nSince the objective of this work is to build a real-time system capable of classifying partial-phase light curves, we explicitly train ORACLE on augmented data. For each light curve in the training set, we sample a fraction, f, from U(0.1,1). Then, we truncate the light curve to this fraction of the original number of observations, where the number of observations includes both detection and non-detections. This data augmentation step is repeated at the start of each training epoch to ensure that the model is trained on light curves at several stages of an event's evolution, and to make the model robust to incomplete photometry.\nFor the validation set, we augmented each light curve to 10%, 40%, 60%, and 100% of its original number of observations. This results in a new augmented validation set with 4 times as many samples and is representative of events at different phases of their evolution.\nFor the testing data set, each light curve was augmented to observations within d days since first detection (or trigger), where d = 2\", and n \u2208 Z \u2229 [0, 10].\nThe difference in data augmentation method between the training and validation set is to enable the evaluation of the model's performance at earlier phases than considered in training, a meaningful strategy to demonstrate performance on out-of-distribution events. Finally, we padded all the time series to a length of 500, in order to\""}, {"title": "5. MODEL ARCHITECTURE", "content": "There are several approaches to classifying irregularly-sampled light curves. Some popular approaches involve:\n\u2022 Extracting statistical features from the light curve and using them for classification with machine-learning models such as Random Forests (S\u00e1nchez-S\u00e1ez et al. 2021) or multi layer perceptrons (Villar et al. 2023).\n\u2022 Directly use the time series photometry for classification, using some type of recurrent neural network (RNN) (Muthukrishna et al. 2019).\n\u2022 Computing an encoded representation of the light curves (e.g., with a VAE, like VRAENN; Villar et al. 2021), which can then be used with a downstream classifier. In modern transformer-based models, a positional encoding is used to preserve phase information (Cabrera-Vives et al. 2024).\n\u2022 Interpolating light curves and then using the interpolated data for classification (Boone 2019).\nIn this work, the time-series data was processed with a RNN, after minor transformations (See Table 1). While discussing the merits of each approach in detail is outside the scope of this paper, it is worth noting that there is much discussion about the interpretability of features extracted by neural networks during the training process. In particular, neural networks are known to extract features that are often nebulous and difficult to explain,"}, {"title": "6. LOSS FUNCTION", "content": "The model must learn to make predictions along a specified classification hierarchy, and the selected loss function must facilitate this goal. For this work, we use a modified version of the Hierarchical Cross Entropy Loss function (HXE, Bertinetto et al. 2019), known as the Weighted Hierarchical Cross Entropy (WXHE, Villar et al. 2023), which introduces an additional weighting factor with the goal of improving the model's performance on unbalanced datasets which are typically found in astrophysics. We summarize the HXE and the WHXE below.\nFirst, the probability of class C in the hierarchy is described as\np(C) = \\prod_{l=0}^{h(C)-1} p(C^{(l)} | C^{(l+1)}), (2)\nwhere h(C) is the height of node C in the hierarchy. This means that C(0) is a leaf node, while C(H) is the root node, assuming H is the height of the tree. Since all classifications begin with an Alert, we use p(C(H)) = p(Alert) = 1. We can also express the conditional probabilities as\np(C^{(l)} | C^{(l+1)}) = \\frac{\\sum_{A \\in Leaves(C^{(l)}))} p(A)}{\\sum_{B \\in Leaves(C^{(l+1)}))} p(B)}, (3)\nwhere Leaves(C) represents the set of leaves for the sub tree starting at Node C. Next, an additional term was introduced to weigh the losses at different nodes, based on where they appear in the hierarchy.\n\\lambda(C) = \\exp(-\\alpha d(C)) (4)\nHere, d(C) represents the depth of the node C in the hierarchy and \u03b1 is a free parameter that can be tweaked to affect the depth whose performance gets prioritized during training. For example, \u03b1 = 0 weighs all nodes equally while \u03b1 > 0 preferentially weighs nodes higher in the taxonomy. Incorporating these elements, Bertinetto et al. (2019) defined the HXE loss as\nL_{HXE}(P, C) = - \\sum_{l=0}^{h-1} \\lambda(C^{(l)}) \\log p(C^{(l)} | C^{(l+1)}). (5)\nSince astrophysical data sets are often class imbalanced, Villar et al. (2023) modified the HXE loss by adding a factor to weight the losses based on the number of occurrences of each class in the data set. The new weight term is defined as\nW(C^{(l)}) = \\frac{N}{N_{nodes} \\cdot N_C}, (6)\nwhere N is the total number of events in the dataset, Nnodes is the number of unique classes, and Nc is the number of events of class C. Combining these elements, Villar et al. (2023) formulated the WHXE loss function as follows:\nL_{WHXE}(P, C) = - \\sum_{l=0}^{h-1} W(C^{(l)}) \\lambda(C^{(l)}) \\log p(C^{(l)} | C^{(l+1)}) (7)\nWe have included a generic implementation for both the HXE and WHXE loss functions for Tensor-Flow/Keras in the repository for this project\u00b3."}, {"title": "7. TRAINING", "content": "While we conducted some experiments to find a set of hyper-parameter values which worked well for training ORACLE, no formal hyper-parameter tuning was used to achieve the results in Section 8. The hyper-parameter values used for training the model are documented in Table 4.\nWe recorded the mean loss on the augmented validation set for each epoch of training. Then, the model with the lowest mean loss was used (Figure 4).\nWe use a learning rate with an exponential decay schedule with a decay rate of 0.9 for every 10000 steps, which mimics annealing, and allows the network to avoid settling in local minima early in the training process."}, {"title": "8. RESULTS", "content": "Given the hierarchical nature of the problem, we can separately consider the performance of the network at every layer in our defined taxonomy. We use the label level_1 to refer to the classification performance at a"}, {"title": "8.1. Inference run-time performance", "content": "Much like the classification performance metrics comparisons shown in Table 8, it is difficult to do an apples-to-apples comparison for the inference run-time performance without running comprehensive tests on multiple classifiers, on the same hardware. Here, we adhere to the statistics reported by Cabrera-Vives et al. (2024) for ATAT, albeit with different hardware, in the hopes of standardizing the metrics reported. We run inference on 20,000 samples using a batch size of 1 and 2000, and record the average time"}, {"title": "8.2. Model Comparisons", "content": "In this section, we qualitatively and quantitatively compare ORACLE with other photometric classifiers from the literature.\nIn Table 8, we report the classification performance of ORACLE along with other popular real-time classifiers: ATAT (Cabrera-Vives et al. 2024), RAPID (Muthukrishna et al. 2019), SCONE (Qu et al. 2021), and SUPERNNOVA (M\u00f6ller & de Boissi\u00e8re 2020). ORACLE achieves performance competitive with these other general-purpose photometric classifiers at all post-trigger phases. When compared with ATAT, we observe marginally worse performance at early times, likely because the WHXE loss function causes the model to preferentially learn the top of the hierarchy (as a result of the \u03b1 parameter described in Section 6). At late times, ORACLE improves and achieves classification performance indistinguishable from ATAT. Notably, both classifiers struggle to classify SN subtypes and CaRTs"}, {"title": "8.3. The scientific impact of early hierarchical classification", "content": "In this section, we concretely demonstrate how the hierarchical classifications made possible by ORACLE can help with the earlier identification, and possibly follow up, of rare transients such as Tidal Disruption Events (TDE) and kilonovae (KNe).\nFigure 10 shows the distribution of days required for ORACLE to produce a confident classification (class score > 0.9) for TDEs in the test set. Among the TDEs that we do classify with high confidence, we can clearly see that level-1 (median of 1 day) and level_2 (median of 11 days) classifications pass the confidence threshold much sooner than the leaf (median of 21 days) classification. This allows observers to request for spectroscopic follow-up with significantly more time for event characterization, while still knowing more about the source"}, {"title": "9. DISCUSSION", "content": "While Neural Networks are often called \"black box\" models, we can study some of ORACLE's predictions and explore possible explanations for them. However, we must exercise caution when interpreting these results. This is especially true for classifiers trained on synthetic data, since misclassification may be reflective"}, {"title": "9.1. Misclassification of Long Transients", "content": "From the confusion matrix in Figure 9, it is evident that SN subtypes act as contaminants for long transients at the leaf level. Interestingly, this corresponds to contamination between the SN and the Long classes at level_2 of the taxonomy (Figure 8) - a contamination that can also be seen in the SUPERNNOVA classifier (M\u00f6ller & de Boissi\u00e8re 2020; Fraga et al. 2024).\nAs explained in the discussion for the loss function (Equation 2), the model outputs the conditional probabilities for each node in the taxonomy. We then multiply the conditional probabilities along the path from the root node to the node of the class, in order to compute the true class probabilities. In doing so, we propagate probabilities downstream.\nThis method leads to desired behavior for correctly classified sources, since more granular predictions are strengthened by correct classifications higher in the taxonomy.\nWhen the classifier does mislabel a source higher up in the taxonomy, and that misclassification leads to leaf-level classifications, we argue that this is still the preferred behavior. We have already demonstrated that predictions higher in the taxonomy are more reliable and thus the presence of low classification probabilities (between long transients and SNe) at level_2, should affect more granular predictions. Indeed, if the data is not sufficient to break basic degeneracies, our classifiers should not be producing confident classifications at lower levels in the taxonomy.\nThese choices manifest as \"blocky\" structures in the leaf confusion matrix at early phases (Figure 9). Specifically, when ORACLE mis-labels a long variable as a SN at level 2, it may then proceed to pick the highest likelihood SN as the true class of the source, resulting in this contamination for long transients.\nAs more data are made available and it becomes possible to break basic degeneracies (level_2), downstream (leaf) classification performance also improves."}, {"title": "9.2. Misclassification of CaRTs", "content": "Another takeaway from Figure 6 and Figure 9 is the poor classification performance for Calcium Rich Transients (CaRTs) compared to other classes. CaRTs are often mis-classified as SNe-Ib/c, SNe-Iax, or SNe-II, even at late times. Part of this contamination may be explained by the inherent similarities in the light curves for these classes (Figure 11), especially when they are poorly sampled.\nFor example, the contamination with Type-Iax SNe may be explained by virtue of both being fainter and redder than other SN subtypes, as well as having faster rise times. Additionally, it is not obvious how the late-time spectroscopic differences between CaRTs and SN sub-types (such as Ib, Ic, and II) are imprinted on the photometry. CaRTs and other core collapse SNe are thought to have similar progenitor systems, which may contribute to similarities in their light curves.\nWe note that ORACLE is not unique in these mis-classifications. Other classifiers trained on ELASTICC and ZTF data similarly struggle to classify CaRTs (Muthukrishna et al. 2019; Cabrera-Vives et al. 2024). Muthukrishna et al. (2019) postulate that the contamination likely stems from the fast rise times of CaRTs, which is similar to core collapse SNe. Cabrera-Vives et al. (2024) report better classification performance for CaRTs using a baseline Random Forest model and find some evidence that it is due to the relatively small number of samples in the training set and the ability of a model to overfit to the training set.\nThus, while it is clear that part of the difficulty in classifying CaRTs comes from similarities in their light curves with other SN types, the relatively better performance with a Random forest model hints at the need for improved minority-class data augmentations for neural network-based methods like ORACLE (this work), ATAT, and RAPID. A more detailed analysis is required to understand if the root problem is the limited training data or something else entirely."}, {"title": "9.3. SNII and TDE Confusion", "content": "At early times, ORACLE often mislabels TDEs as Type-II SNe (Figure 9). Intuitively, these classes should be easily separable based on their HOSTGAL_SNSEP parameter, which describes the angular separation between the transient and the center of their matched host galaxy in arcseconds (assuming the majority of galaxies are correctly identified). Figure 12 shows the distribution for the Transient-Host separation for both Type II SNe and TDEs. Here, we observe significant overlap between the two classes, which makes this separation difficult at early times. This problem is made worse by the fact that about 1.4% of Type-II SNe and about 10% of TDEs"}, {"title": "10. CONCLUSION", "content": "In this work, we demonstrate that hierarchical models are incredibly effective at classifying astrophysical phenomena at late times, while also providing useful class information at earlier times compared to flat multi-label classifiers. This makes hierarchical classification methods particularly desirable for real-time applications in astrophysics where an observational taxonomy exists and where full-phase light curves will not always be available.\nFurthermore, the comparable performance of ORACLE relative to state-of-the-art, non-hierarchical models from the literature, at every level of the taxonomy, highlights that there may be few drawbacks to adopting this approach for future surveys. By increasing the fidelity of time-domain simulations used to train these photometric classifiers (e.g., with the injection of bogus alerts and with larger samples of rare classes), additional differences between techniques may emerge.\nORACLE is both open-source and open-weight. All the codes for the training and analysis of the model are publicly available on GitHub (https://github.com/uiucsn/ELASTiCC-Classification). Additionally, we include a general-purpose Keras/Tensorflow implementation for the HXE and WHXE losses within the repository. This implementation can be easily adopted for other hierarchical classification tasks, including those outside of astronomy."}, {"title": "11. FUTURE WORK", "content": "Given the potential value of ORACLE for early follow-up of scientifically valuable targets (that can be flagged ~10 days earlier than is possible with a leaf-level classification, as shown in Fig. 10), we aim to integrate ORACLE end-to-end into automated time-domain follow-up systems for Rubin. One such exam-"}]}