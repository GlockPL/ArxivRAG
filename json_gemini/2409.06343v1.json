{"title": "Compute-Update Federated Learning: A Lattice Coding Approach", "authors": ["Seyed Mohammad Azimi-Abarghouyi", "Lav R. Varshney"], "abstract": "This paper introduces a federated learning framework that enables over-the-air computation via digital communications, using a new joint source-channel coding scheme. Without relying on channel state information at devices, this scheme employs lattice codes to both quantize model parameters and exploit interference from the devices. We propose a novel receiver structure at the server, designed to reliably decode an integer combination of the quantized model parameters as a lattice point for the purpose of aggregation. We present a mathematical approach to derive a convergence bound for the proposed scheme and offer design remarks. In this context, we suggest an aggregation metric and a corresponding algorithm to determine effective integer coefficients for the aggregation in each communication round. Our results illustrate that, regardless of channel dynamics and data heterogeneity, our scheme consistently delivers superior learning accuracy across various parameters and markedly surpasses other over-the-air methodologies.", "sections": [{"title": "I. INTRODUCTION", "content": "In our current era, wireless edge devices like smartphones, autonomous vehicles, and sensors are becoming more advanced and ubiquitous. This evolution presents an opportunity to leverage machine learning techniques to build global models from the dispersed data these devices generate. Yet, streaming large datasets from these endpoints to centralized servers comes with numerous challenges, including data privacy, latency, power, and bandwidth limitations. Federated learning (FL) offers an innovative workaround to these obstacles [2]. It allows devices to process machine learning locally, ensuring data stays on the device itself. In the FL paradigm, devices iteratively train models and send updates to a server, which aggregates them until convergence. This method proves invaluable in wireless settings, especially in IoT-rich ecosystems where network reliability and resource availability are often compromised. Given that these devices interact with the edge server over a shared wireless medium, FL also presents an intricate communication puzzle to solve. While FL can even be leveraged to enhance wireless communications [3], traditional wireless communication techniques used in FL rely on orthogonal multiple access techniques. While these techniques prevent interference by requiring individual transmissions from each device to the server, they introduce significant communication latency and demand substantial resources [4].\nHarnessing interference from concurrent multi-access transmissions of edge devices, over-the-air computation [5]-[7] emerges as a promising strategy. Building on this foundation, a method termed over-the-air FL has been developed to handle the aggregation phase of FL, in the presence of interference, all within a single resource block [4]. This integration of communication and computation allows over-the-air FL to function more efficiently, consuming fewer resources and achieving lower latency than FL using orthogonal transmissions. However, previous research on over-the-air FL has assumed analog modulation, where the transmitter can freely shape the carrier waveform by opting for any real number I/Q coefficients [8]-[17]. This assumption may not be possible for the majority of existing wireless devices, as they come with digital modulation chips that may not support arbitrary modulation schemes.\nMoreover, with a power control approach for channel compensation, past work required perfect channel state information at the transmitter (CSIT) for all devices to determine their transmission powers and phases to counteract wireless channel effects during the aggregation process. This approach requires devices to have extra hardware for accurate channel adjustments. Moreover, under poor channel conditions, a device might be unable to participate in the learning process or it could need high transmission power. This is problematic due to the physical constraints on both the instantaneous and average power capabilities of the device [8]\u2013[18]. One strategy for power control is truncated power control, which is suitable for scenarios with a single-antenna server [9]-[13], [18]. In this case, each device only needs to know its own channel, known as local CSIT. Another strategy is joint device selection and power control schemes as suggested in [14]\u2013[17]. These studies require global channel knowledge of all devices before each transmission, referred to as global CSIT, in order to centralize their power optimization process. In essence, the device selection strategy seeks to maximize the number of devices included in each communication round. This issue generally corresponds to an integer program, which is inherently NP-hard. Past work also requires perfect synchronization among the transmitters [9]\u2013[13], [18]. These requirements lead to a substantial overhead for channel estimation training and complex feedback mechanisms before any transmission, causing increased delays and reduced spectral efficiency.\nThese objectives contradict the aim of FL to enable low-cost distributed learning for a wide range of digital devices constrained by power and hardware limitations [19]. The special case of over-the-air FL using BPSK modulation is studied in [18]. Nonetheless, while significantly reducing implementation cost and resource requirements, one-bit quantization can result"}, {"title": "A. Prior Work", "content": "In [28], [29], a source coding scheme based on lattice codes has been used to quantize model parameters in FL. In [29], privacy enhancement is also considered. In these works, orthogonal transmission is used by the individual transmitters to prevent interference. In fact, after quantization, each transmitter maps quantized model parameters to bits and use conventional orthogonal digital communications. This use of lattice-based quantization is based on the principles of subtractive dithered lattice quantization [30], which is grounded in information-theoretic arguments. This method is recognized for its capability to achieve the highest possible finite-bit representation accuracy as per rate-distortion theory, with a margin of error that can be controlled [28], [31]. The proposed approach in [28] achieves a more accurate quantized representation for FL applications compared to traditional scalar quantization techniques, including both probabilistic and deterministic approaches, e.g., [32]. In a sense, it uses functional quantization [33].\nPrevious studies [34]\u2013[38] have separately shown that lattice codes have benefits when used in compute-and-forward relaying as a channel coding scheme [34]. The lattice structure, where any integer combination of codewords is a codeword itself, allows for interference to be exploited during transmission and decoding, resulting in high bit-rate throughput. In the compute-and-forward framework, the assumption is that bits are already present at the transmitters, and nested lattice coding defined over a finite field is employed for modulation to provide end-to-end bit transmission. The main reason to use such integer combinations in compute-and-forward is because they can be decoded at much higher rates than individual messages at the relays, despite these combinations not having inherent explicit meaning. Based on the computation rate metric from information theory, multiple independent combinations with highest rates are chosen from different relays and finally transmitted to the receiver for decoding individual messages [34], [36]."}, {"title": "B. Key Contributions", "content": "We propose a joint source-channel coding scheme that incorporates novel transmission and aggregation strategies, aiming to boost the resistance of over-the-air FL to interference and noise and achieve the desired learning outcomes. This scheme is accompanied by innovative digital transmitter and receiver architectures that leverage the lattice structure. Notably, it does not rely on any prior knowledge or CSIT, leading to a blind approach. Our scheme effectively mitigates errors resulting from learning and communication impairments, providing robustness in the presence of channel uncertainties and under a limited number of antennas at the server.\nCompute-Update Scheme and its Transmitter-Receiver Architecture: We provide an end-to-end real-valued model parameter transmission framework for FL, named compute-update FL- FedCPU. On the transmitter side, we use lattice codes over the infinite field of real numbers for quantization of the model parameters. Our quantization approach is based on normalization and dithering. On the server side, we decode a single integer combination of transmitted quantized model parameters as a lattice point, which after processing yields a new adjustable form of aggregation. Here, our scheme assigns a clear and explicit significance to integer combinations. This is due to the additive structure of model parameter aggregation in FL, where integer coefficients can be directly and purposefully linked to aggregation weights, giving them a distinct meaning and function. Additionally, lattice Voronoi region over the decoded integer combination protects against the decoding error arising from interference and noise. This level of protection is absent in analog modulation schemes, given their continuous transmitted values [8]\u2013[17], [21]-[27]. Also, the aggregation method we propose marks a significant shift from the traditional practice in FL of employing fixed, predefined weights, which are either equal or based on the size of local datasets. This conventional approach, recommended for ideal, error-free communication conditions as outlined in the seminal FL paper [2], has been consistently used in over-the-air FL research [8]-[18], [21]-[27]. However, this persists despite the fact that over-the-air FL deals with imperfect communication scenarios, plagued by interference and noise. In response to this, our method of aggregation is designed to strategically tailor the aggregation weights. This is crucial because in our model, each integer combination results in a unique decoding error, leading to substantial variability. In or-"}, {"title": "II. SYSTEM MODEL", "content": "There are K devices and a single server as the basic setup for FL systems, see Fig. 1. All the devices are single-antenna units, but the server has M antennas. There is no prior knowledge of local data statistics of the devices, consistent with [8]-[18], [21]-[27]. The downlink channels from the server to the devices are considered error-free. The validity of this assumption comes from the server's high transmission power, its efficient antenna capability, and the exclusive transmission to the devices in the downlink. The uplink channel from each device k to the m-th antenna of the server at communication round t is represented by $h_{mk,t} = |h_{mk,t}|e^{\\angle h_{mk,t}} \\in \\mathbb{C}$, where $|h_{mk,t}|$ is the channel gain and $\\angle h_{mk,t}$ is the channel phase\u00b9. The $\\angle h_{mk,t}$ spans the entire range from 0 to 2\u03c0. All channels are assumed invariant during one time slot required for an uplink transmission, while they change independently from one time slot to another. Furthermore, all devices have frame-level synchronization with no carrier frequency offset, and are considered to have the same transmission power constraint P. However, by appropriately adjusting the channel coefficients, we can integrate asymmetric power constraints. Let the entire channel matrix $H\\in \\mathbb{C}^{M\\times K}$ be\n$H =\\begin{bmatrix}h_{11,t} & \\dots & h_{1K,t} \\\\ \\vdots & & \\vdots \\\\ h_{M1,t} & \\dots & h_{MK,t}\\end{bmatrix}$ (1)\nThis channel model is extensively adopted in several other over-the-air FL schemes [8]\u2013[18], [21]\u2013[27]. The server is the only node that knows H as CSIR after the transmission, whereas the devices have no any knowledge of channels. Accurate CSIR is acquired through the individual uplink transmission of training sequences from each device to the server.\u00b2 This is the main assumption in the blind over-the-air FL strategy [22]-[25] in line with many wireless systems that use transmissions with constant power, avoiding channel compensation at the transmitter side [34]\u2013[41]. Such a strategy maintains a steady average signal energy regardless of channel variations, limits the signal's dynamic range, and reduces device hardware complexity. It emerges as a particularly promising solution for IoT applications equipped with low-capability devices. Nevertheless, the majority of over-the-air FL methods with power control strategy, e.g., [14]\u2013[17], need both CSIR and CSIT before each transmission.\nPartial asynchrony among the devices can be modeled as part of the channel phase.\nConsistent with previous works [8]-[17], [24], [25], we do not account for channel estimation error in this study.\nB. Learning Algorithm\nDevice $k \\in \\{1, ..., K\\}$ has its own local dataset $\\mathcal{D}_k$. The learning model is characterized by the parameter vector $w \\in \\mathbb{R}^{s\\times 1}$, where s indicates the model's size. Subsequently, the local loss function for the model vector w when applied to $\\mathcal{D}_k$ is\n$F_k(w) = \\frac{1}{|D_k|} \\sum_{\\xi_i \\in D_k} f(w, \\xi_i)$, (2)\nwhere $f(w, \\xi_i)$ represents the per-sample loss function, capturing the prediction error of w for a given sample $\\xi_i$. As a"}, {"title": "A. Transmission Scheme", "content": "Based on a lattice $\\Lambda$,\u00b3 the transmission preparation procedure carried out by each device k is as follows.\n1) The model update parameters undergo normalization to achieve zero mean and unit variance. This normalization serves two purposes. Firstly, zero-mean entries ensure the subsequent estimate of aggregation remain unbiased. Secondly,\nIn our framework, any lattice can be used. Optimizing for a specific lattice structure is beyond the scope of our work."}, {"title": "B. Aggregation Scheme", "content": "After simultaneous transmission by all the devices in a single resource block, the baseband received signal at antenna m of the server, $y_m \\in \\mathbb{C}^{s\\times 1}$, is\n$y_m = \\sum_{k=1}^{K} h_{mk}x_k + z_m$, (16)\nwhere $z_m \\in \\mathbb{C}^{s\\times 1}$ is complex Gaussian noise, with each entry having a variance of $\\sigma_n^2$. It is the standard multiple access communication model [8]\u2013[18], [21]\u2013[27], [34]\u2013[40]. Thus, the real-valued representation of (16) is as follows\n$Y = HX + Z$, (17)\nwhere\n$Y = \\begin{bmatrix}Re \\{y\\}\\\\ \\vdots \\\\ Re \\{y_M\\}\\\\ Im \\{y_1\\}\\\\ \\vdots \\\\ Im \\{y_M\\}\\end{bmatrix} \\in \\mathbb{R}^{2M\\times s}, Z = \\begin{bmatrix}Re \\{z_1\\}\\\\ \\vdots \\\\ Re \\{z_M\\}\\\\ Im \\{z_1\\}\\\\ \\vdots \\\\ Im \\{z_M\\}\\end{bmatrix} \\in \\mathbb{R}^{2M\\times s}$\n(18)\n$\\[Re \\{H\\}\\\\ Im \\{H\\}\\begin{bmatrix} \\in \\mathbb{R}^{2M\\times K} , X = \nRe \\{x_1\\}\\\\ \\vdots \\\\ Re \\{x_K\\}\\end{bmatrix} \\in \\mathbb{R}^{K\\times s},$ (19)\nThus, the resource efficiency, in terms of the number of communication resources used per any $1 < K < \\infty$ devices, is $\\frac{1}{K}$. Based on (17) and the transmission scheme in Subsection III.A, the receiver architecture for the aggregation at the server includes two layers as follows.\n1) First Layer: An equalization vector $b \\in \\mathbb{R}^{2M\\times 1}$ is used to obtain\n$b^TY = b^THX + b^TZ$. (20)\nThen, the decoder aims to recover a lattice point or equally a linear integer combination of quantized model updates $\\sum_{k=1}^{K} a_k \\Delta w_k = a^T \\Delta W$ directly from $b^T Y$, and\n$\\Delta W = \\begin{bmatrix} \\Delta w_1,\\\\ \\vdots \\\\ \\Delta w_K\\end{bmatrix} \\in \\mathbb{R}^{K\\times s}$,\n(21)\nFor this decoding, the result $b^TY$ is scaled and then quantized to its nearest lattice point as\n$Q_{\\Lambda} (\\sqrt{\\frac{1 + 2\\sigma_q^2}{P}} b^T Y) = a^T \\Delta W$. (22)\nWe can rewrite $\\sqrt{\\frac{1 + 2\\sigma_q^2}{P}}b^T Y$ as\n$a^T \\Delta W + \\sqrt{\\frac{1 + 2\\sigma_q^2}{P}} (b^TH - a)X + \\sqrt{\\frac{1 + 2\\sigma_q^2}{P}}b^TZ$. (23)"}, {"title": "III. FEDCPU: COMPUTE-UPDATE SCHEME", "content": "The FedCPU consists of two primary components: the transmission scheme employed by the devices and the aggregation scheme utilized by the server. Through simultaneous transmission, FedCPU introduces a novel adjustable version of the aggregation vector, where model updates from devices are weighted by integer coefficients that are not predefined. This is because each set of integer coefficients results in a unique learning performance. This version exploits the lattice structure and the additive nature of wireless multiple-access channels, given as\n$\\Delta W_{G,t+1} = \\frac{1}{1^T a_t} \\sum_{k=1}^{K} a_{k,t} \\Delta w_{k,t}$ (11)"}, {"title": "DMSE(a)", "content": "From the SNR and H as seen in (29), the DMSE arises due to interference and noise impacting the decoding process. It is also worth noting that the matrix $(I + SNR H^T H)^{-1}$ is positive definite.\nThe value of DMSE(a) can be interpreted as the variance of the effective noise in decoding $a^T\\Delta W$. Therefore, the decoding error probability, denoted as $P_D$, which represents the probability of decoding any other lattice point besides $a^T\\Delta W$, is equated to the event where the decoding noise lies outside $a^T\\Delta W + V$.\nRemark 1: For a decoding noise within $a^T\\Delta W + V$, the system is fully protected from interference and noise.\nRemark 2: Unlike conventional digital communications, where a decoding error results in an entirely incorrect message, in FedCPU a decoding error merely introduces an additive estimation error in (32). This estimation error depends on the distance between the decoded lattice point and the target lattice point. In essence, while our goal is to minimize the decoding error to enhance system performance, it does not constitute a critical constraint or a bottleneck for the system.\n2) Second Layer: After decoding and removing the dithers, the proposed aggregation vector in (11) at the output of the receiver can be estimated as\n$\\[ \\Delta w = Q_{\\Lambda} \\Bigg( \\sqrt{\\frac{1+2\\sigma_q^2}{P}} b^T Y \\Bigg) - a^T D \\\\\n= \\eta \\frac{1}{1^T a_t} a^T  \\sum_{k=1}^{K} a_{k} \\theta_{k} 1_\\tau, (30)\nwhere \u03b7 is a normalizing factor, and\n$D= \\begin{bmatrix}d_{1,}^T\\\\ \\vdots \\\\ d_{K,}\\end{bmatrix} \\in \\mathbb{R}^{K\\times s}$ (31)\nOwing to the proposed transmit normalization, the mean of the initial term in (30) is zero, and the deliberate inclusion of the second term in (30) guarantees the precise mean recovery of (11). Consequently, the estimation in (30) is unbiased. It differes from [2], [8]-[29], [32] which use predefined weights in FL, such as equal or proportional to local dataset sizes. In Section IV, we quantify the effects of the proposed estimation given by (30) with an arbitrary vector a on learning convergence.\nWe now prove that the aggregation expressed in (11) can be derived from the estimation outlined in (30). To illustrate this, we reformulate (30) as follows."}, {"title": "IV. CONVERGENCE ANALYSIS", "content": "The following theorem presents the convergence analysis of FedCPU in terms of the optimality gap. For perspective on the innovation of our analysis, it is worth highlighting that existing convergence studies in the field have primarily been confined to scenarios with equal aggregation weights, perfect communication conditions, or strongly convex loss functions, operating under an array of assumptions on learning parameters. In contrast, our analysis is versatile to cater to an expansive set of loss functions. It not only allows for straightforward identification of the impact of parameters and any imperfections in FedCPU but also facilitates suggesting universal aggregation metrics for choosing integer coefficients, as detailed in Section V. The analysis assumes minimal common assumptions from the literature.\nAssumption 1 (Lipschitz-Continuous Gradient): The gradient of the loss function F(w) as given in (3) adheres to Lipschitz continuity characterized by a positive constant L. Consequently, for any model vectors w\u2081 and w\u2082, the conditions below are satisfied.\n$F(w_2) \\le F(w_1) + \\nabla F(w_1)^T (w_2 - w_1) + \\frac{L}{2} ||w_2 - w_1||^2$, (38)\n$\\lVert \\nabla F(w_2) - \\nabla F(w_1) \\rVert \\le L \\lVert w_2 - w_1 \\rVert$. (39)\nAssumption 2 (Gradient Variance Bound): At a device, the local gradient estimate g serves as an unbiased estimate of the ground-truth gradient \u2207F(w), and its variance remains bounded, as\n$\\mathbb{E}\\{\\lVert g - \\nabla F(w) \\rVert^2\\} \\le \\frac{\\sigma_g^2}{B}$ (40)\nAssumption 3 (Polyak-Lojasiewicz Inequality): Let $F^* = F(w^*)$ be from problem (4). There exists a constant \u03b4 \u2265 0 for which the subsequent condition holds.\n$\\lVert \\nabla F(w) \\rVert^2 \\ge 2 \\delta (F(w) - F^* )$. (41)"}, {"title": "QMSE(a)", "content": "In (32), the first term is the aggregation (11) and the remaining terms are mainly due to the quantization errors. Thus, the overall error against this estimation in terms of the MSE, referred to as qunatization MSE, is\n$\\begin{aligned}QMSE(a) & = \\mathbb{E} \\Bigg\\{\\frac{1}{K} \\sum_{k=1}^K a_k \\Delta w_k - \\eta \\frac{1}{1^T a} \\sum_{k=1}^K a_k E_k \\Bigg\\}^2 \\\\& = \\mathbb{E} \\Bigg\\{\\frac{1}{K} \\sum_{k=1}^K a_k \\Delta w_k\\Bigg\\}^2 + \\frac{\\eta^2}{(1^T a)^2} \\mathbb{E} \\Bigg\\{ \\sum_{k=1}^K a_k E_k \\Bigg\\}^2 \\\\& = \\frac{1}{K^2} \\Bigg(\\mathbb{E} \\Bigg{\\frac{1}{\\eta^2}} \\sum_{k=1}^K a_k \\sigma_k \\Delta w_k\\Bigg\\}^2 + \\frac{\\eta^2}{(1^T a)^2}  \\Bigg(\\frac{1}{K} \\sum_{k=1}^K a_k E_k\\Bigg\\}^2\\\\ & = \\frac{1}{(1^T a)^2} \\Bigg\\{\\frac{1}{\\eta^2}a^T (I-\\diag(\\sigma))^{-1} a + |a|^2 \\frac{\\eta^2}{\\sigma_1^2}a^T \\diag(2)a\\Bigg\\},\\end{aligned}$ (33)\nwhere $\\sigma = [\\sigma_1,...,\\sigma_K]^T$. The factor \u03b7 that minimizes the quantization MSE is presented in the next theorem.\nTheorem 2: The optimal \u03b7 for a given coefficient vector a is\n$\\eta_{opt} = \\frac{(1 + \\sigma_q^2) ||a||^2}{a^T\\diag(\\sigma) a}$ (34)\nProof: We can expand $\\frac{1}{(1^T a)^2} QMSE(a)$ as\n$\\frac{1}{(1^T a)^2} \\Bigg\\{\\frac{1}{\\eta^2}a^T (I-\\diag(\\sigma))^{-1} a + |a|^2 \\frac{\\eta^2}{\\sigma_1^2}a^T \\diag(2)a\\Bigg\\}$ (35)"}, {"title": "Lt", "content": "By taking derivative with respect to \u03b7 and equating the resulting expression to zero, we obtain\n$\\frac{\\partial}{\\partial \\eta} (\\frac{-aT(I-\\diag(\\sigma))a}{\\eta^3} + a^T \\diag(2) a \\eta) = \\frac{2 aT(I-\\diag(\\sigma))a}{\\eta^3} + a^T \\diag(2) a = 0$ (36)\nWhich leads to the final result\nSubstituting the optimal \u03b7, the QMSE is\n$QMSE(a) = \\frac{S}{(1 + \\sigma_q)^2 } \\big(\\frac{\\eta^2}{a^T\\diag(2) a}\\big)$ (37)"}, {"title": "V. INTEGER COEFFICIENT SELECTION", "content": "For the selection of integer coefficient vectors $a_t$ for all $t \\in \\{0,..., T - 1\\}$, inspired by Remark 7, we aim to minimize the optimality gap as described in Theorem 3. Equally, this minimization can be broken down into individual subproblems, each corresponding to one round t.\n$\\begin{array}{l}a_t = arg \\underset{a\\in Z_K \\backslash \\{0\\}\\,min  L_t(a) \\ \\ \\ \\ = arg \\underset{a \\in Z^K}{\\min} \\frac{\\sigma g^2}{B}  \\frac{||a||^2}{(1^Ta)^2} + QMSE (a) \\  \\ \\ = arg \\underset{a \\in Z^K}{\\min} \\frac{S}{(1 + \\sigma_q)^2 } \\big(\\frac{\\eta^2}{a^T\\diag(2) a}\\big)\\\\Q.e.d\\end{array}$ (38)"}, {"title": "VI. EXPERIMENTAL RESULTS", "content": "The task involves classifying images from two datasets of standard MNIST and Fashion-MNIST with the parameter values given in Table 1, unless otherwise stated. We have constructed the classifier using a Convolutional Neural Network (CNN). This CNN features two convolutional layers, both of 3 \u00d7 3 size with ReLU activation\u2014 the first containing 32 channels and the second 64. Each of these layers is succeeded by a 2 \u00d7 2 max pooling. Subsequent layers include a fully connected layer with 128 units with ReLU activation, culminating in a softmax output layer. The lattice generator matrix is G = diag {G8,..., G8} where G8 represents the generator matrix of the E8 lattice [45]. Both i.i.d. and non-i.i.d. distributions of dataset samples among devices are considered. For the non-i.i.d. scenario, each device contains samples exclusively from two classes, and the sample count differs from one device to another. Performance is gauged by the learning accuracy in relation to the test dataset throughout the global iteration count, denoted by t. The outcome for performance is determined by averaging 20 realization samples. This ensures Gaussian channel distribution is considered- specifically, channel gain with Rayleigh fading $\\sim exp(\\frac{1}{2})$ and the channel phase following the uniform distribution $\\sim U(0, 2\\pi)$.\nIn Fig. 5, the accuracy is shown for different local iterations $\\tau$ in the MNIST and i.i.d. scenario. It is evident that by augmenting $\\tau$ or t, there is a positive impact on learning performance. Additionally, the performance sees a significant boost when integrating multiple local iterations instead of"}, {"title": "VII. CONCLUSIONS", "content": "We introduced a federated learning scheme incorporating lattice codes, pioneering a new over-the-air computation method. The proposed scheme offers adjustable quantization, enabling distributed learning through digital modulation. Additionally, it ensures resilience against interference and noise through coding. In this scheme, with no need for channel compensation at the transmitter end, an integer combination of lattice quantized model parameters is reliably decoded and processed for aggregation. We derived the optimality gap for the learning process within the scheme, contingent on the integer coefficients and encompassing both communication and learning factors. To determine these integer coefficients, we have suggested a tailored aggregation metric rooted in the gap. For the optimization of the metric, which falls under NP-hard integer programming, we proposed a method for convexification and presented an efficient algorithm. In spite of channel conditions and data heterogeneity, experimental findings showcased the superior learning accuracy of the proposed scheme, outperforming existing over-the-air alternatives. Furthermore, even with a limited number of antennas at the server, the proposed scheme can nearly achieve the performance level anticipated in a scenario devoid of transmission errors."}, {"title": "APPENDIX A", "content": "The update for the learning model at round t + 1 is represented as\n$\\[ \\begin{aligned}\\Delta W_{G,t+1} &= W_{G,t+1} - W_{G,t} \\\\&= \\frac{\\mu_t}{K} \\sum_{k=1}^K a_{k,t} w_t  + eq(a_t) \\\\&= \\frac{\\mu_t}{K} \\sum_{k=1}^K a_{k,t} \\nabla F_k (W_{k,t,i}, \\xi) + eq(a_t),\\end{aligned}\\] (54)"}]}