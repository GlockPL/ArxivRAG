{"title": "Enhanced Extractor-Selector Framework and Symmetrization Weighted Binary Cross-Entropy for Edge Detections", "authors": ["Hao Shu"], "abstract": "Recent advancements have demonstrated the effectiveness of the extractor-selector (E-S) framework in edge detection (ED) tasks, which achieves state-of-the-art (SOTA) performance in both quantitative metrics and perceptual quality. However, this method still falls short of fully exploiting the potential of feature extractors, as selectors only operate on highly compressed feature maps that lack diversity and suffer from substantial information loss. Additionally, while union training can improve perceptual quality, the highest evaluation scores are typically obtained without it, creating a trade-off between quantitative accuracy and perceptual fidelity. To address these limitations, we propose an enhanced E-S architecture, which utilizes richer, less-loss feature representations and incorporates auxiliary features during the selection process, thereby improving the effectiveness of the feature selection mechanism. Additionally, we introduce a novel loss function, the Symmetrization Weight Binary Cross-Entropy (SWBCE), which simultaneously emphasizes both the recall of edge pixels and the suppression of erroneous edge predictions, thereby enhancing the predictions both in the perceptual quality and the prediction accuracy. The effectiveness and superiority of our approaches over baseline models, the standard E-S framework, and the standard Weight Binary Cross-Entropy (WBCE) loss function are demonstrated by extensive experiments. For example, our enhanced E-S architecture trained with SWBCE loss function achieves average improvements of 8.25%, 8.01%, and 33.25% in ODS, OIS, and AP, measured on BIPED2 compared with the baseline models, significantly outperforming the standard E-S method. The results set new benchmarks for ED tasks, and highlight the potential of the methods in beyond.", "sections": [{"title": "1 Introduction", "content": "Edge detection (ED) is a foundational problem in computer vision, with widespread applications in higher-level tasks such as image inpainting [Nazeri et al., 2019], object detection [Zhan et al., 2007], and image segmentation [Muthukrishnan and Radha, 2011]. Over the past decade, deep learning methods such as convolutional neural networks (CNNs) have substantially advanced ED by enabling the extraction of hierarchical, discriminative features directly from data. These data-driven techniques have significantly improved performance on benchmark datasets. Notable works such as Holistically-Nested Edge Detection (HED) [Xie and Tu, 2015], Bi-Directional Cascade Network (BDCN) [He et al., 2022], and Dense Extreme Inception Network (Dexi) [Soria et al., 2023b] have set new performance standards by excelling in feature extraction and demonstrating the power in handling complex edge structures.\nDespite the impressive advances in feature extraction, traditional deep-learning ED methods often overlook the critical aspect of effective feature selection. To address this gap, the extractor-selector (E-S) framework was recently introduced [Shu, 2025] as a novel solution to enhance edge detection performance. This paradigm integrates a feature selector with a feature extractor to generate more accurate edge maps by pixel-wise feature selection, which has achieved state-of-the-art (SOTA) results in both quantitative evaluations and perceptual quality.\nHowever, the current E-S framework still faces limitations, primarily due to the compression of feature maps during the feature extraction process. As selectors operate on highly compressed feature maps, significant information is lost, which may reduce the overall quality of edge detection. Additionally, while union training has been shown to improve perceptual edge predictions in such a framework, it could lead to suboptimal quantitative results, creating a trade-off between perceptual accuracy and quantitative performance. These challenges highlight the need for improving feature selection and training strategies in the E-S method.\nTo address these limitations, we explore two key aspects in this paper: (1) an enhancement of the E-S architecture to better leverage the feature extraction capabilities of pre-existed extractors, and (2) the introduction of a novel loss function that facilitates perceptual edge predictions, while maintaining SOTA quantitative results, for the E-S method. In detail, on one hand, we propose an enhanced E-S architecture that improves the effectiveness of feature selection by utilizing richer, less-compressed intermediate features as well as extra"}, {"title": "2 Previous Works", "content": "This section provides an overview of related work in the domains of datasets, ED models, the E-S paradigm, and loss functions, providing a review of their contributions and limitations."}, {"title": "2.1 Datasets", "content": "In the early stages of ED research, the task was often conflated with related problems such as contour detection, boundary detection, and segmentation. Consequently, early edge detection studies leveraged datasets that were not explicitly designed for ED but were adapted from other tasks. Notable among these is the Berkeley Segmentation Dataset (BSDS300) and its extended version BSDS500 [Martin et al., 2001], which include 300 and 500 RGB images, respectively, with contour annotations from multiple human annotators. Similarly, the Multi-cue Boundary Dataset (MDBD) [M\u00e9ly et al., 2016] provides 100 high-resolution images annotated with boundary information across a variety of scenes, while the NYU Depth Dataset (NYUD) and its extension NYUD2 [N. Silberman et al., 2012] contain 1,449 images with segmentation labels. Other widely utilized datasets include PASCAL VOC [Everingham et al., 2010], Microsoft COCO [Lin et al., 2014], SceneParse150 [Zhou et al., 2017], and Cityscapes [Cordts et al., 2016]. While these datasets have been instrumental in the advancement of ED, their primary focus on other vision tasks limits their specificity and applicability to modern ED researches.\nAs ED matured as an independent research area, more specialized datasets were created to address the unique requirements of the task. The BIPED dataset, later refined into BIPED2 [Soria et al., 2020], includes 250 high-resolution images with single-edge annotations. The BRIND dataset [Pu et al., 2021] re-annotates the BSDS500 dataset, categorizing edges into four distinct types, thus enhancing its utility for more nuanced ED tasks. The UDED dataset [Soria et al., 2023a], consisting of 29 carefully selected images with high-quality edge annotations, further contributes to the benchmark suite for ED.\nA persistent challenge in all ED datasets is the reliance on human annotations for groundtruth labeling. Such labels are inherently noisy, often containing missing edges, errors, or inconsistencies. Furthermore, annotation uncertainty arises when different annotators produce divergent results for the same image, or even when the same annotator annotates the same image twice. To mitigate these issues, recent studies have focused on improving annotation quality through advanced labeling techniques and developing models robust to noisy annotations [Fu and Guo, 2023][Wang et al., 2024][Shu, 2024]."}, {"title": "2.2 Models", "content": "The evolution of ED methodologies can be broadly categorized into three phases: heuristic-based methods, statistical approaches, and deep learning-based methods.\nHeuristic-Based Methods:\nEarly ED techniques relied on simple heuristic criteria such as intensity gradients to design efficient computational algorithms. Prominent methods include the Sobel operator [Kittler, 1983] and the Canny detector [Canny, 1986], which have remained influential due to their simplicity and effectiveness. Despite their historical significance, these methods face limitations in adaptability to diverse datasets, as their fixed coefficients are often insufficient to distinguish edges from textures in complex datasets."}, {"title": "Statistical and Early Learning-Based Methods:", "content": "In the second phase, statistical and learning-based methods were introduced to improve ED by incorporating hand-crafted features such as Chernoff information[Konishi et al., 2003], histograms[Arbel\u00e1ez et al., 2011], textures[Martin et al., 2004], and sketch tokens [Lim et al., 2013]. Techniques such as structured forests [Doll\u00e1r and Zitnick, 2015], nearest-neighbor search [Ganin and Lempitsky, 2014], and logistic regression [Ren, 2008] were employed to classify pixels or patches as edge or non-edge. These approaches demonstrated improved robustness and accuracy compared to heuristic methods, yet their reliance on manual feature engineering constrained scalability and limited their generalization to a wider range of datasets."}, {"title": "Deep Learning-Based Methods:", "content": "The third and current phase has been driven by the emergence of deep learning, particularly convolutional neural networks (CNNs). CNN-based models have revolutionized edge detection by facilitating hierarchical feature extraction and enabling end-to-end learning. One of the pioneering models in this phase, Holistically-Nested Edge Detection (HED) [Xie and Tu, 2015], introduced multi-scale feature learning, where features from various network layers are fused to improve edge prediction. The Richer Convolutional Features (RCF) model [Liu et al., 2017] extended this concept by incorporating features from multiple levels within the same layer to achieve finer ED. Subsequent models such as Bi-Directional Cascade Networks (BDCN) [He et al., 2022], Pixel Difference Networks (PiDiNet) [Su et al., 2021], and Dense Extreme Inception Networks (Dexi) [Soria et al., 2023b] have further advanced edge detection through innovations in architecture, supervision, and feature fusion strategies."}, {"title": "2.3 The Extractor-Selector Paradigm", "content": "Most existing edge detection models primarily focus on feature extraction while giving less attention to feature selection. Traditional deep learning models rely on computationally expensive feature extractors, followed by simple aggregation techniques, such as 1\u00d71 convolution layers, to fuse features. While effective, this approach often falls short in complex ED tasks, as it does not adequately select the most relevant features from the extracted ones.\nTo address this gap, the E-S paradigm was recently proposed [Shu, 2025]. This framework retains traditional feature extractors but introduces a dedicated selector that processes multi-scale features to produce final edge maps. The selector improves overall performance, both accuracy and perceptual quality, by selecting the most pertinent features for edge predictions.\nHowever, in the original implementation of the E-S paradigm, only the final-layer outputs of the feature extractor were selected, a design choice that provided simplicity but also limited the framework's potential for further performance gains. Drawing inspiration from models like RCF [Liu et al., 2017], which utilize richer features from multiple layers, a more effective strategy would involve providing the selector with richer, less-compressed features to further improve the performance."}, {"title": "2.4 Loss Functions", "content": "The (imbalance version of) Weighted Binary Cross-Entropy (WBCE) loss function is the most widely used one in ED. However, WBCE often requires post-processing techniques such as Non-Maximum Suppression (NMS) to refine the edge predictions and enhance perceptual quality. To address these limitations, alternative loss functions have been explored, including Dice loss [Deng et al., 2018], which balances edge preservation with the suppression of the total edge-pixels in the predictions, and tracing loss [Huan et al., 2022], which separates the loss calculation between edge pixels and background textures to strengthen the predicted edges.\nAnother challenge arises from the imbalance between positive and negative samples in ED data, which can undermine model performance. To mitigate this, besides the imbalance design of WBCE, loss functions like AP-loss [Chen et al., 2019][Cetinkaya et al., 2024] have been introduced improve prediction accuracy.\nDespite these advancements, the adoption of perceptual-improving loss functions often involves trade-offs, such as reduced precision or the need for cumbersome hyperparameter tuning across different datasets and models. These issues restrict the general applicability of these loss functions. Thus, the design of loss functions that can simultaneously optimize both quantitative evaluation and perceptual quality, without requiring dataset-specific and model-specific hyperparameter adjustments, remains a challenging yet important area of research."}, {"title": "3 Methodology", "content": "This section provides the main schemes of the work, including the enhanced E-S architecture and the SWBCE loss function. For more technical details, please refer the codes."}, {"title": "3.1 The Enhanced E-S Architecture", "content": "The proposed architecture builds upon the E-S paradigm described in [Shu, 2025]. As illustrated in Figure 1, the paradigm consists of two key components: the feature extractor and the feature selector. The extractor generates feature maps, which are then selected by the selector to produce the final edge predictions."}, {"title": "3.2 The SWBCE Loss Function", "content": "While WBCE achieves high performance in terms of recall and precision, it struggles to suppress the most challenging pixels, which contribute significantly to blurred edges or polluted non-edge regions. Specifically, non-edge pixels near edges often exhibit similar behavior to edge pixels due to their proximity, resulting in blurred edges, while textural regions may contain prominent features that mimic edges, leading to polluted predictions. These pixels (called the challenging pixels) are impactful but particularly difficult to suppress. During training, WBCE tends to reduce the loss for other pixels instead of focusing on these more impactful but challenging ones, arising from the differential difficulty of suppression.\nTo mitigate these limitations, the Symmetrization Weighted Binary Cross-Entropy (SWBCE) loss function is designed by emphasizing both the suppression of the challenging pixels and the recall of edge pixels. Observing that the challenging pixels are non-edge ones which however are predicted to be edges, the key insight behind SWBCE then lies in assigning higher weights to predicted edge pixels. This aligns with the discriminative nature of ED that generally, a pixel is viewed as an edge one only if there is strong evidence supporting its classification, whereas a pixel is classified as a non-edge one by default unless evidence suggests otherwise.\nFinally, to leverage existing methods effectively, the SWBCE loss is formulated as the weighted average of two terms: the standard WBCE loss, weighted by ground-truth labels to recall edges, and a loss function symmetrical to the standard WBCE, weighted by predictions to suppress non-edges. Formally, SWBCE is defined as:\n$LSWBCE(\\hat{Y},Y) = \\frac{LLabel(\\hat{Y}, Y) + b \\times Lpred(\\hat{Y}, Y)}{1+b}$\nwhere, $LLabel$ is the standard WBCE loss function, while $LPred$ is novelly designed by symmetrizing the WBCE loss function, weight by predictions, and b is a hyperparameter balancing $LLabel$ and $LPred$, typically set to 1:\n$LLabel(\\hat{Y}, Y) = \u2013 \\sum_{Yi \\in Y} a_i[y_ilog(\\hat{y}_i) + (1 \u2212 y_i)log(1 \u2013 \\hat{y}_i)] = -\\alpha \\sum_{Yi\\in Y^{+}} log(\\hat{y}_i) \u2013 \\lambda(1 \u2013 \\alpha) \\sum_{Yi\\in Y^{-}} log(1 \u2013 \\hat{y}_i)$\nwhere, $\\hat{y}_i$ is the pixel in the prediction $\\hat{Y}$ corresponding to the pixel $y_i$ in the groundtruth Y, $Y^+$ is the set of edge pixels in Y (positive samples), $Y^\u2212$ is the set of non-edge pixels in Y (negative samples). For $y_i \\in Y^+$, $a_i = \\alpha = \\frac{Y_{and}}{Y}$ and for $y_i \\in Y^\u2212$, $a_i = \\lambda(1 \u2212 \\alpha)$ are their weights, respectively. $\\lambda = 1.1$ as suggested in previous works. And:\n$LPred(\\hat{Y},Y) = \u2013 \\sum_{Y\\in Y} \\hat{a}_i[y_ilog(\\hat{y}_i) + (1 - y_i)log(1 \u2013 \\hat{y}_i)]$\nwhere,\n\u2022 $ \\hat{a}_i = \\frac{\\hat{Y_{and}}}{\\hat{Y}} + (1 - \\frac{\\hat{Y_{and}}}{\\hat{Y}})\\lambda $ is the weight of the pixel $\\hat{y}_i$.\n\u2022 $\\hat{I}_p = \\sum_{\\hat{y} \\in \\hat{Y}} \\hat{y}_i$ is the influence of predicted edges\n\u2022 $\\hat{I}_N = |\\hat{Y}| \u2013 \\hat{I}_p$ is the influence of predicted non-edges.\n\u2022 $\\lambda$ = 1.1 is a balancing hyperparameter as in $LLabel$.\nThe SWBCE loss function emphasizes both the recall of edge pixels by $LLabel$ and the error suppression of the challenging pixels by $LPred$. The latter is a symmetry of the former, weighting by predictions to focus on suppressing errors on non-edge pixels, while it has also considered the imbalance of edges and non-edges in the prediction, as the standard WBCE did for groundtruthes."}, {"title": "3.3 Training Methods", "content": "To assess of the enhanced E-S architecture and SWBCE loss function, various training schemes are utilized.\nTo compare the performance of the enhanced E-S architecture with baseline models and the standard E-S architecture, a three-stage training process is adopted, following [Shu, 2025], including (1) Pre-train the feature extractor using its default loss function, (2) Train the feature selector using WBCE loss function with frozen coefficients of the extractor, and (3) Union train the extractor and selector, also using the WBCE loss function.\nAnd to evaluate the proposed SWBCE loss function, the enhanced E-S architecture is also trained with the SWBCE loss function without stage (3), since the simplified process has enabled the models to achieve both high quantitative performance and satisfactory perceptual quality."}, {"title": "4 Experiments", "content": "This section presents the evaluations of the proposed methods on dataset BIPED2, UDED, and BRIND, where three previous models, HED, BDCN, and Dexi, are employed as baseline models and extractors."}, {"title": "4.1 Benchmarks", "content": "The quantitative evaluation follows the algorithm presented in [Martin et al., 2004], measuring the Optimal Dataset Scale (ODS), Optimal Image Scale (OIS), and Average Precision (AP), under set error toleration distances. However, the error toleration distance is set to be the stringiest 1-pixel as suggested in [Shu, 2024], in contrast to the traditional 4 to 11 pixels, for a high standard. Moreover, all evaluations were conducted without post-processing such as NMS, and data is processed similarly as in [Shu, 2025].\nThe proposed E-S models were evaluated against baseline models without selectors, as well as those based on the standard E-S architecture, using the WBCE loss function. Additionally, the SWBCE loss function was assessed in comparison to WBCE loss function in the enhanced E-S architectures."}, {"title": "4.2 Experiment Results", "content": "The quantitative results are summarized in Tables 1 with representative visual predictions shown in Figure 4. They consistently demonstrate the effectiveness of the proposed methods. Across all datasets, the proposed methods yield the most significant performance improvements, underscoring their superior performance. In details:"}, {"title": "Quantitative Analysis", "content": "For the BIPED2 with the WBCE loss function and without the union training, the proposed architecture achieves average improvements of 7.28%, 7.05%, and 22.00% in ODS, OIS, and AP, respectively, when compared to models without selectors. In contrast, models employing the standard E-S architecture show relatively modest improvements of 3.72%, 3.53%, and 8.00%, respectively. Moreover, by employing the SWBCE loss function, the proposed architecture achieves further improvements of 8.25%, 8.01%, and 33.25%, respectively, outperforming the former ones.\nFor the UDED dataset with the WBCE loss function and without the union training, the proposed architecture achieved notable gains of 5.43%, 4.01%, and 20.19% in ODS, OIS, and AP, respectively. In contrast, the standard E-S architecture produced smaller improvements of 1.95%, 1.60%, and 4.07%. Moreover, by employing the SWBCE loss function, the proposed E-S architecture achieves further improvements of 6.69%, 5.21%, and 26.30%, respectively, also outperforming the former ones.\nFor the BRIND dataset with the WBCE loss function and without the union training, the proposed architecture exhibits improvements of 3.50%, 3.15%, and 10.35% for ODS, OIS, and AP, respectively, compared to models without selectors. By comparison, models employing the standard E-S architecture yield gains of 1.83%, 1.20%, and 5.51%, respectively. Moreover, by employing the SWBCE loss function, the proposed E-S architecture achieves improvements of 3.35%, 3.15%, and 26.65%, respectively, competitive on ODS and OIS while better on AP compared to the former ones."}, {"title": "Qualitative Analysis", "content": "Figure 4 presents visual comparisons of the predicted outputs. It showcases full images alongside cropped regions (the blue and yellow boxes). The rows represent the original images, groundtruthes, and model predictions, respectively. Notably, the proposed enhanced E-S architecture generates more perceptual edge predictions compared to both the standard E-S architecture and the baseline model, using WBCE loss function, while the usage of SWBCE loss function further and significantly improves the performance."}, {"title": "5 Conclusion and Further Works", "content": "In this paper, we have presented an enhanced framework of the E-S paradigm originally presented in [Shu, 2025] for ED, designed to overcome the limitations of the standard one that insufficiently utilizes feature representations during the selection process. Additionally, we have presented the SWBCE loss function, aiming to simultaneously achieve SOTA quantitative scores and perceptual quality, for the same tasks. By leveraging richer and less-loss feature representations and incorporating extra features, the proposed enhanced E-S architecture provides a more effective solution for ED tasks, achieving significant improvements, while by emphasizing both recall of the edges and suppressing erroneously predicted edges, the SWBCE loss function offers an advanced method for generating high-quality edge predictions both quantitative and perceptual. The effectiveness and superiority of both schemes are confirmed by extensive experiments over various datasets and baseline models. Notably, the enhanced E-S architecture combined with SWBCE achieved an average improvement of about 8.25%, 8.01%, and 33.25% on ODS, OIS, and AP, respectively on BIPED2 dataset compared to the baseline models. These findings establish new benchmarks for ED tasks, and indicate the potential of the methods for border application in beyond.\nDespite these notable achievements, several avenues for further research remain. First, designing specific extractors based on the E-S architecture rather than directly accessing previous ones might provide a more effective solution for ED. Second, the computational efficiency of the standard and enhanced E-S architectures could be improved to better support resource-constrained scenarios such as real-time edge detection for high-resolution images, which would make the framework more practical for applications like autonomous systems. Additionally, the SWBCE loss function could be further optimized to achieve even more precise edge predictions or adapted for related tasks such as semantic segmentation. Nevertheless, the presented work provides exciting opportunities for advancing ED and further tasks."}]}