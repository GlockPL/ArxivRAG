{"title": "Dynamic neural network with memristive CIM and CAM for 2D and 3D vision", "authors": ["Yue Zhang", "Woyu Zhang", "Shaocong Wang", "Ning Lin", "Yifei Yu", "Yangu He", "Bo Wang", "Hao Jiang", "Peng Lin", "Xiaoxin Xu", "Xiaojuan Qi", "Zhongrui Wang", "Xumeng Zhang", "Dashan Shang", "Qi Liu", "Kwang-Ting Cheng", "Ming Liu"], "abstract": "The brain is dynamic, associative and efficient. It reconfigures by associating the inputs with past experiences, with fused memory and processing. In contrast, Al models are static, unable to associate inputs with past experiences, and run on digital computers with physically separated memory and processing. We propose a hardware-software co-design, a semantic memory-based dynamic neural network (DNN) using memristor. The network associates incoming data with the past experience stored as semantic vectors. The network and the semantic memory are physically implemented on noise-robust ternary memristor-based Computing-In-Memory (CIM) and Content-Addressable Memory (CAM) circuits, respectively. We validate our co-designs, using a 40nm memristor macro, on ResNet and PointNet++ for classifying images and 3D points from the MNIST and ModelNet datasets, which not only achieves accuracy on par with software but also a 48.1% and 15.9% reduction in computational budget. Moreover, it delivers a 77.6% and 93.3% reduction in energy consumption.", "sections": [{"title": "Introduction", "content": "The human brain's efficiency in executing complex tasks on a remarkably low energy budget relies on the synergy of dynamic reconfigurability, associative memory with past experience, and collocation of memory and information processing.\nThe brain's processing of information is dynamic and adaptive. Rather than maintaining a static computational model, the brain's neural networks exhibit complex and dynamic activation and connectivity patterns in response to various stimuli and tasks. The dynamic connectivity of the brain primarily hinges on structural plasticity1\u20133. Moreover, the brain displays another layer of dynamic behavior; when individuals encounter objects or tasks of varying complexity, different brain regions are activated4. Neural activity patterns of the brain can vary across different brain regions and change over time, enabling the brain to integrate sensory information, make decisions, and adapt to new situations. The dynamic nature of the brain allows it to process tasks efficiently while consuming minimal power. In contrast, the majority of neural networks are static with their fixed topology, lacking such adaptability, leading to inefficient resource allocation and limitations in handling evolving information or tasks (Fig.1(a)).\nIn addition, the brain is capable of associating new information with its past experience, such as in the realm of semantic and high-level visual processing. Biological experiments have demonstrated the brain's aptitude for accelerating the recognition of familiar objects via memory encoding and retrieval processes within activated neurons, functioning as a semantic cache that allows for rapid retrieval of past experience. Traditional computers, however, rely on address-based information storage and search, rather than associate observations in terms of similarity (Fig.1(b)).\nMoreover, the brain employs chemical synapses not only to store information but also to process information right where they are stored, featuring low power consumption and high parallelism. This is contrasted with the high energy consumption and latency challenges posed by the current von Neumann architecture digital computers, where the separation between computing and storage units necessitates frequent data transfers 7 (Fig.1(c)).\nHere we propose a hardware-software co-design, semantic memory-based dynamic neural network (DNN) using memristor, to mirror the 3 characteristics of the brain computing paradigm.\nSoftware-wise, semantic memory-based DNN aims to equip artificial neural networks with the dynamic reconfigurability of the brain(Fig.1(a)). The network associates the new information with its past experience and allocates computations based on demand, more efficient and effective than static networks in many applications9\u201312. Also, the network's adaptability allows it to balance accuracy and efficiency by adjusting to varying computational budgets in real-time13\u201318, for scenarios where computational budgets may change or vary, such as in resource-constrained devices or distributed computing environments 19.\nHardware-wise, we employ memristor, an emerging device that physically resembles the synapses of the brain, to physically implement the ternary weights of noise-robust dynamic neural network and the associative semantic memory, using computing- in-memory (CIM) and content-addressable memory (CAM) circuits, respectively.\nMemristor-based CIM circuits use simple physical laws for matrix-vector multiplications. As matrix weights are physically stored in a memristor array and multiplication is performed right at where the weight is stored, they amalgamate computing and memory elements, which not only overcomes the von Neumann bottleneck but also leads to high parallelism 20\u201340 (Fig.1(c)).\nMemristor-based CAM circuit is an associative memory inspired by the brain (Fig.1(b)), functioning as semantic cache memories in the dynamic neural networks41. Unlike traditional electronic memory that relies on addresses for information retrieval, the brain and CAM feature for parallel content-based searches36,42\u201347. Specifically, CAM measures the distances between input vector and stored vectors within the memory, eliminating the need for data movement like the CIM47\u201352.\nIn this article, we validate our co-design using a 40nm memristor macro on representative image and 3D points classification tasks. Using ResNet53 and PointNet++54, we showcase notable reductions in computational budget, achieving a 48.1% reduction for classifying the MNIST55 dataset and a 15.9% reduction for the ModelNet56 dataset by semantic memory-based DNN. Moreover, we observe substantial improvements in energy efficiency, resulting in a 77.6% and 93.3% reduction in energy consumption for the classification of respective datasets when compared to a state-of-the-art graphic processing unit (GPU). These findings open up avenues for future research on emulating the brain's adaptive allocation of computational resources, associative recall of past experience, and collocation of memory and processing."}, {"title": "Results", "content": "Hardware-software co-design: Semantic memory-based DNN using memristor-based CIM and CAM\nFig. 2 illustrates the semantic memory-based DNN using memristor-based CIM and CAM. The DNN optimizes resource allocation and speeds up the forward inference with an early-exiting mechanism according to the semantic memory57. The early-exiting dynamically adjusts the number of layers per forward inference, contingent upon the complexity of the input sample, thereby creating a more adaptable network.\nAs illustrated in Fig.2(a-b), we use ternary weights (-1, 0, 1) in the network instead of full-precision floating-point numbers (as to be discussed later, ternary quantization outperforms full-precision weights subject to analogue computing noise). Our work uses pre-trained backbone network without training the exits, and can exit from any of the layers before the final layer during inference57 (see Supplementary Note 1 and Supplementary Table 1 for comparison of early exit work). To compute semantic memory, we infer samples from the training set and apply global average pooling (GAP) to the intermediate layers, converting the feature maps into one-dimensional semantic vectors (see Supplementary Note 2 for global average pooling). These semantic vectors represent the semantic centers for each respective category. Semantic centers undergo ternary quantization and are stored in the CAM. Upon the query of a new test-set sample, the network calculates associated search vectors according to each layer's output feature map and compares them to the cached semantic centers in the CAM (see Supplementary Note 3 for operation of CAM). As such, each semantic memory is able to make a classification based on the cosine similarity between a search vector and the stored semantic centers, at different confidence.\nFor relatively simple input samples, the brain can react quickly without the need for deep-level thinking. Similarly, as shown in Fig.2(a), the forward propagation terminates at a shallow layer (e.g. layer l) with a clearly recognizable cat image. This is because the semantic vector of layer l, when compared to the semantic center vectors in the CAM, yields a sufficiently large confidence in its prediction. The network thus classifies the input accordingly, bypassing the remaining layers and thus enhancing efficiency. For more complex input samples, as shown in Fig.2(b), the forward pass goes to deeper layers to capture high-level features for accurate classification. As depicted in the Fig.2(a-b), the computational budget increases with the number of layers in the forward inference, and contributes to the overall computational cost. Therefore, this early exit approach improves efficiency by avoiding unnecessary computations and streamlining the inference process.\nWe physically implement the DNN and semantic memory using a memristor macro. Fig.2(d) shows the marco integrated onto a printed circuit board along with a Xilinx ZYNQ system-on-chip, forming a hybrid analogue-digital computing platform (see Supplementary Figure 1 for the system design and photo). The analogue CIM and CAM cores consist of CMOS-compatible nanoscale TaN/TaOx/Ta/TiN memristors (Fig.2(e-f)) fabricated using the backend-of-line process on a 40 nm technology node tape-out (Fig.2d). As shown in Fig.2(a-c), the ternary weights of the neural network are encoded into the memristive conductances in CIM, and CAM stores the semantic centers. The input samples are then quantized and mapped to voltages applied to CIM, with the output currents representing output feature maps. The feature maps are then digitized, activated, and pooled in the digital core before being fed to the next layer. The feature maps also undergo GAP, yielding the search vector. The cosine distances between the search vector (as voltage input to the CAM) and the semantic centers stored in the CAM is revealed by the match line signals(e.g. currents here). The currents are then digitized to determine the confidence level associated with each semantic center.\nDynamic ResNet for 2D vision\nFirst, we applied our co-design to classify the MNIST dataset5using the ResNet model53. Fig.3(a) shows an experimental example of the forward inference on the memristor-based hybrid analogue-digital system. The model and the semantic memory were first ex situ trained, before being quantized and physically mapped to memristor-based CIM and CAM (see Method). During inference, a handwritten image (e.g. digit eight) is mapped to voltages and the CIM physically computes the feature maps of each residual block. These feature maps produce associated search vector of each residual block after GAP, for being compared with semantic centers of the same block in CAM by measuring cosine similarities. Upon the forward pass reaching the fourth block, the calculated similarity for the eighth class surpasses the threshold, indicating a sufficient confidence level to classify the input handwritten digit as eight. The forward pass thus stops here and all remaining residual blocks are skipped.\nTo benchmark the classification performance of semantic centers of different residual blocks, intra-class distance and inter-class distance are measured, as shown in Fig.3(b-d). T-distributed stochastic neighbour embedding (t-SNE) dimension reduction59 visualizes the distribution of search vectors (small labels, voltages applied to CAM) of 100 randomly selected test samples and semantic centers (large labels, stored in CAM) from the second, fifth, and nineth residual blocks. It is observed that samples possess significantly different sample-center distances, qualifying the demand of a dynamic and adaptive network. In addition, different residual block develops different classification capability, necessitating unique threshold adjustments for each layer to guarantee optimal performance. Here we optimize the thresholds for semantic memory of each residual block with Tree-structured Parzen Estimator (TPE)60,61 as to be discussed later.\nFig.3(e) shows the ablation and comparison studies. Software static full-precision (SFP) and ternary quantized (Qun) models are of accuracy 98.0% and 96.5%, respectively, which slightly reduces to 97.5% (EE) and 96.0% (EE. Qun) with the dynamic early-exit. Taking into account analogue memristor noise, the simulated ternary quantized dynamic model (EE. Qun+Noise) shows an accuracy 96.1%, consistent with our experimental observation (Mem) 96.0%, accompanied with a 48.1% reduction of computational budget (see Supplementary Note 4 for performance comparison with LeNet). Fig.3(f) shows the experimentally acquired confusion matrix, in which the prevalence of diagonal elements corroborates the high classification accuracy. Fig.3(g) shows the computational budget of inference samples at each residual block and the probabilities of samples passing through each block (see Supplementary Note 5 for computational budget breakdown). It is observed that most samples only need to go through the first four residual blocks while difficult samples propagate to deep residual blocks, which significantly reduces the overall computational budget without compromising the performance.\nFig.3(h) shows the energy consumption breakdown of the randomly selected 100 inference samples (see Supplementary Note 6 for operating power and speed estimation). The light (dark) grey bar is the estimated energy consumption of a state-of-the-art GPU running the static (dynamic) ResNet model, consuming 1.83 \u00d7 107 pJ (9.19 \u00d7 10\u00ba pJ) (see Supplementary Note 7 for energy consumption modelling). The dynamic model saves about 49.8% energy due to the decreased network connectivity. For the hybrid analogue-digital system, the blue, green and red bars show energy consumption of memristor (1.21 \u00d7 104 pJ for CIM and 77.1 pJ for CAM, see Supplementary Table 2 and Table 3 for energy efficiency of CAM and CIM, respectively), analogue to digital conversion (1.57 \u00d7 10\u00ba pJ for CIM and 4.55 \u00d7 104 pJ for CAM), and digital peripherals (3.73 \u00d7 105 pJ for activation and pooling, while 6.63 \u00d7 104 pJ for sorting). The overall energy consumption of DNN on the projected memristor-based hybrid analogue-digital system is approximately 2.06 \u00d7 10\u00ba pJ, a 77.6% reduction of the energy consumption compared to software dynamic ResNet due to in-memory computing. Furthermore, compared to static ResNet running on GPU, our co-design achieves a 8.9-fold enhancement in inference energy efficiency (see Supplementary Note 8 for energy breakdown in ResNet).\nTernary quantization for analogue noise suppression\nNoise suppression is critical for analogue computing with memristor. There are two major noises in our co-design, the write and read noise. The write noise originates from inevitable programming stochasticity with memristor, while the read noise roots on temporal fluctuations of conductance due to the combined effect of programming instability and other electronic noises 62\u201364.\nFig.4(a) illustrates the two noises. The conductance of five randomly selected memristors programmed under the same condition are continuously sampled 10,000 times. The conductance of each memristor over time follows a quasi-normal distribution, with unique mean and standard deviation. This is also shown in Fig.4(b), the mean conductance map of a memristor array programmed under the same condition, and Fig.4(c), the map of conductance standard deviation over 10,000 reading cycles. Fig.4(d) shows the correlation between mean conductance and the standard deviation. The mean conductance of memristor reflects the write noise, while the standard deviation reflects the read noise. Fig.4(e) shows the histogram of Fig. 4(b), where the mean conductance of different memristors follows a quasi-normal distribution and 15% write noise. The impact of noise on CIM is shown in Fig.4(f), where final result of the computation is plotted against the exact result. Such noise also impacts CAM in a similar manner as examplified by Fig.4(g).\nTo mitigate the write and read noise influence over the memristor-based CIM and CAM, we adopt ternary quantization of weights and semantic centers (see Methods), which outperforms mapping full-precision models subject to the write and read noise. We simulate write noise impact on the classification accuracy of dynamic ResNet with and without ternary weight quantization. As shown in Fig.4(h), the ternary quantized network exhibits considerable resilience against increasing write noise, while the full-precision mapped network performance quickly degrades with increasing programming noise. Furthermore, we simulate read noise impact in conjunction with a fixed 15% write noise. As shown in Fig.4(i), the ternary quantized network shows a 10% accuracy improvement over direct mapping full-precision weights to memristors.\nDynamic PointNet++ for 3D vision\nDespite 2D images, the advent of 3D sensing technologies has underscored the significance of 3D point clouds in capturing and interpreting the spatial attributes of 3D objects. The classification of 3D point clouds is instrumental in autonomous driving, robotics, and augmented reality. Here, we simulate our co-design using PointNet++54 model to classify ModelNet56 dataset. The PointNet++ contains multiple set abstraction layers. These layers work in a hierarchical manner, each layer selects representative points, transforms features of all points neighboring to the representative points, and aggregates the neighboring point features to update features of representative points (see Methods for the details of the PointNet++). In this way, the PointNet++ captures both local and global features in a point cloud at different depth of the model, widely used for 3D points classification and segmentation. For better illustration, We randomly select ten categories from the ModelNet dataset. Fig.5(a) uses a chair as an example, the point features are transformed to voltages and then applied to CIM for feature transformation. The search vectors are calculated for each set abstraction layer and matched with the semantic centers stored in CAM. The system gains enough confidence about the matching results upon forward passing to the fifth set abstraction layer, triggering an early exit and bypassing the remaining layers.\nFig.5(b-d) uses t-SNE59 to visualize the distribution of the search vectors of 100 randomly selected test samples and semantic centers. The second (Fig.5(b)), the forth (Fig.5(c)) and the sixth (Fig.5(d)) set abstraction layer again shows inference samples are of diverse difficulty and the model develops increasing discrimination capability over the depth."}, {"title": "Solving optimization problem on trading off between budget and accuracy using \u03a4\u03a1\u0395", "content": "The threshold of each layer plays a critical role in determining whether an input sample can exit early for computational budget reduction, or keep on forward passing for better accuracy, which balance between accuracy and efficiency. The objective here is to find a Pareto-optimal solution that strikes a trade-off between computational budget and accuracy for the DNN. We first use grid search to explore the correlation between computational budget and accuracy. Then we design a target function that incorporates both computational budget and accuracy, and use the method of TPE60,61 to optimize the threshold of each layer. The optimal solution for the target function yields a DNN that balances computational budget and accuracy.\nFig.6(a) shows the grid search results for the thresholds for the co-design using ResNet, which illustrates the trade-off between computational budget and accuracy. Based on that, we devise a target function depends on both computational budget and accuracy.\n$$max_{dm} Acc (dm) \u00d7 \\left(\\frac{DCB}{B}\\right)^\u03c9$$\nHere, the target budget drop B = 0.50, Acc(dm) is the classification accuracy of the DNN, DCB is the drop of computational budget. The weight factor, w, is empirically selected to ensure that Pareto-optimal solutions yield similar rewards under different accuracy-budget trade-offs. We observe in grid search that an additional 1% increase in accuracy corresponds to an approximate 4.35% increase in computation budget when accuracy is beyond 94%. Consequently, w is set to 0.127. For convenience, we solve the dual problem. Fig.6(b) shows the negative objective function which decreases with the budget drop or increase of accuracy (see Method), which is also visualized in Fig.6(c) as a curved surface.\nWe minimize the negative objective function by identifying the best thresholds of each layer. As TPE method does not model the interaction between thresholds, we use that to solve the Pareto-optimal problem. The first step of TPE is to initialize sampling through random search. We denote the accuracy and budget drop as x and the calculated score as y, the observed results{(x(1),y(1)), ..., (x(k),y(k))} are then divided into two groups: the group with good (poor) performance, marked as green dot (yellow triangle) in Fig.6(c)), and the splitting value for dividing the two groups is score*. TPE defines the probability distribution p(x|y) using the following two probability density functions:\n$$p(x|y) = \\begin{cases}l(x) & \\text{if } y \\geq y^*\\\\g(x) & \\text{if } y^* > y\\end{cases}$$\nFig.6(f) shows the density function of good samples (purple 3D surface) and the density function of bad samples (blue 3D surface) which are modeled by Parzen estimators (see Methods for a detailed description)60,61. TPE utilizes the following expected improvement (EI) function as an acquisition function:\n$$E I_{y*}(x) = \\int_{-\\infty}^{y*} (y^* - y)p(y|x) dy$$"}, {"title": "Discussion", "content": "We demonstrate a semantic memory-based DNN using memristor-based CIM and CAM. CIM processes data directly within memory, resulting in reduced energy consumption and latency compared to conventional digital computers of von Neumann architecture. Additionally, CAM functions as the semantic memory and associates new inputs with past experience in a brain-like manner. Leveraging both CIM and CAM, we physically implement noise-proof ternary quantized DNN and optimize the thresholds using TPE by solving the Pareto-optimal problem, which results in dynamic connectivities not only reducing computational budget but also retaining network performance. We validate our co-designs using a 40nm memristor macro on ResNet and PointNet++ for classifying images and 3D points from the MNIST and ModelNet datasets. Our approach achieves accuracy comparable to software while reducing the computation budget by 48.1% and 15.9% compared to static neural networks. Furthermore, it offers a significant reduction in energy consumption, with a 77.6% and 93.3% decrease compared to a state-of-the-art GPU. Our results lay the foundation for future machine intelligence that can potentially parallel the adaptability and efficiency of the human brain."}, {"title": "Materials and Methods", "content": "Fabrication of memristor array\nMemristors are integrated on the 40nm standard logic platform, forming a 512\u00d7512 crossbar array. The memristors are constructed between the metal 4 and metal 5 layers of the backend-of-line process, including bottom electrodes, top electrodes, and a transition metal oxide dielectric layer. The bottom electrodes have a patterned via with a diameter of 60nm, formed with the method of photolithography and etching. Physical vapor deposition and chemical mechanical polishing are employed to deposit material within the via. A 10 nm TaN buffer layer is deposited on the bottom electrode via using physical vapor deposition. Subsequently, 5 nm Ta is deposited and then oxidized in an oxygen environment to form an 8 nm Taox dielectric layer. On the top electrode, we sequentially deposited 3 nm Ta and 40 nm TiN using physical vapor deposition. Next, the logic backend-of-line metal is deposited using standard logic process. Memristors in the same column share top electrode connections, and memristors in the same row share bottom electrode connections. After a post-annealing process in vacuum at 400\u00b0C for 30 minutes, the fabrication of the memristor chip is completed.\nHybrid analogue-digital computing platform\nThe hybrid analogue-digital computing platform consists of a 40 nm memristor chip and the a Xilinx ZYNQ system-on-chip. There is an 8-channel digital-to-analogue converter (DAC80508, TEXAS INSTRUMENTS) in the system to generate parallel 64-channel analogue voltages which ranges from 0V to 5V, it is used to transform input digital signals into corresponding analogue signals. The convergence currents are converted to voltages using a transimpedance amplifier (OPA4322-Q1, TEXAS INSTRUMENTS) and read out as digital signals by an analogue-to-digital converter (ADS8324, TEXAS INSTRUMENTS) with a 14-bit resolution for signal collections. To perform vector-matrix multiplication, a 4-channel analogue multiplexer (CD4051B, TEXAS INSTRUMENTS) with an 8-bit shift register (SN74HC595, TEXAS INSTRUMENTS) applies DC voltages to the word lines of the memristor chip. The multiplication results carried by the current from the source line are converted into voltages and passed to the Xilinx ZYNQ system-on-chip for further processing.\nDNN-based ResNet\nTaking advantage of the high and low resistance states exhibited by memristors, we leverage the properties of two such memristors to effectively represent a ternary value, employing the principles of Ohm's law and Kirchhoff's laws. The memristors in crossbar array are partitioned into two matrices for programming the weights of ResNet and values stored in CAM. If both corresponding memristors are adjusted to the high-resistance state, they represent a weight of 0. If the memristor in the first part is in the low resistance state while the second memristor is in the high resistance state, they represent a weight of 1. Conversely, if the memristor in the first part is in the high resistance state while the second memristor is in the low resistance state, they represent a weight of -1. In the experiment, the ResNet model consists of 11 residual blocks, with a total of approximately 88k weight parameters, and there are approximately 2k values stored in CAM.\nDNN-based PointNet++\nPointNet++ is built upon the foundation of PointNet, which consists of a series of Multi-Layer Perceptrons (MLPs) and a global feature aggregation step using a symmetric function. PointNet++ uses Set Abstraction (SA) layers to capture local features in the point cloud using Farthest Point Sampling (FPS) strategy to select representative points and apply local PointNet operations on their neighboring points within a specific radius. In the experiment, the DNN-based PointNet++ consists of eight SA layers with varying radius and numbers of representative points. Additionally, PointNet++ consists of Feature Propagation (FP) Layers, which use nearest neighbor interpolation and feature concatenation from different layers to help the network reconstruct global point cloud features from the hierarchical local features.\nTernary Quantization\nWhen training a ternary network, ternary quantization is performed during the forward propagation, while weight adjustment using full-precision values is carried out during the backward propagation. The method of ternary quantization is as follows:\n$$w_{min} = \\text{argmin }W^l$$\n$$w_{max} = \\text{argmax }W^l$$\n$$l_{in} = w_{min} + (w_{max} - w_{min})/3$$\n$$h_{in} = w_{max} - (w_{max} - w_{min})/3$$\nwhere W\u00b9 represent the whole weight of block 1, lin and hin are two intervals. The output ternary quantized weight is as follows:\n$$W_q \\begin{cases} -1, & \\text{if } w_i \\leq l_{in}\\\\0, & \\text{if } h_{in} \\geq w_i \\geq l_{in}\\\\1, & \\text{if } w_i > h_{in}\\end{cases}$$\nTree Parzen Estimators\nBayes' rule is a fundamental concept in probability theory and statistics. It describes the probability of an event based on prior knowledge or information. The formula for Bayes' theorem is expressed as:\n$$P(A/B) = \\frac{P(B|A)P(A)}{P(B)}$$\nWhere P(A/B) represents the conditional probability of event A given event B. P(B|A) represents the conditional probability of event B given event A. P(A) is the probability of event A occurring. P(B) is the probability of event B occurring. The advantage of using Bayesian inference is that it leverages prior experience to make inferences about the next sample. By incorporating prior information into the analysis, Bayesian methods can accelerate the process of finding the optimal solution. TPE uses EI as the acquisition function. However, since p(ylx) cannot be obtained directly, we employ Bayes' rule of 6 to perform the transformation in 3. The details for deriving the final expression in 3 are as the following:\n$$p(x) = \\int p(x|y)p(y)dy = \u03b3l(x) + (1 - \u03b3)g(x)$$\n$$E I_{y*}(x) = \\int_{-\\infty}^{\\infty} (y^* - y)p(x|y)p(y)dy = \\int \u03b3l(x) \\int_{-\\infty}^{y*} (y^* - y)p(y)dy - \\int (1 - \u03b3)g(x) \\int_{-\\infty}^{y*} p(y)dy$$\nWhere y represents the quantile of TPE, used to divide l(x) and g(x), ranging between 0 and 1. For example, if y is set to 0.2 and there are 10 observed samples, the top 2 best performed samples will be used to construct the distribution of good samples, while the remaining 8 samples will be used to construct the distribution of bad samples.\nThe the Parzen window68 is a technique for kernel density estimation. It estimates the probability density of an estimated value based on the current observed data and the type of prior distribution. The probability density estimation function for Parzen window is as follows:\n$$p(x) = \\frac{k}{n.V} = \\frac{k}{n(\\sqrt{nh^2})^n} \\sum_{i=1}^n \u03c6(\\frac{x_i - x}{h})$$\nThe window function determines weight assigned to each observed data point based on its distance from the point xi, which means that the weights are higher for data points closer to xi and decrease as the distance increases. When using a Gaussian function as the window function, the density function is as:\n$$p(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{\\sqrt{2\u03c0}\u03c3} exp(-( \\frac{x_i - x}{2\u03c3})^2)$$\nwhere $$\\int \u03c6(x) dx = 1$$."}, {"title": "Acknowledgements", "content": "Funding\nThis research is supported by the National Key R&D Program of China (Grant No. 2020AAA0109005), the National Natural Science Foundation of China (Grant Nos. 62122004, 62374181, 62488201), the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDB44000000), Beijing Natural Science Foundation (Grant No. Z210006), Hong Kong Research Grant Council (Grant Nos. 27206321, 17205922, 17212923). This research is also partially supported by ACCESS \u2013 AI Chip Center for Emerging Smart Systems, sponsored by Innovation and Technology Fund (ITF), Hong Kong SAR.\nAuthor contributions\nConceptualization: YZ, ZW, DS. Methodology: YZ, ZW, DS, WZ. Investigation: YZ, DS, ZW. Visualization: YZ, DS, ZW. Funding acquisition: HJ, PL, XX, DS, ZW, XZ, QL, KTC, ML. Project administration: DS, ZW. Supervision: ZW, DS, XQ. Writing - original draft: YZ. Writing \u2013 review and editing: ZW, DS, YZ, SW, NL, YY, YH, BW, HJ, PL, XX, XQ, WZ, XZ, QL, KTC, ML.\nCompeting Interests\nThe authors declare no competing interests.\nData and materials availability\nAll data needed to evaluate the conclusions in the paper are present in the paper and/or the Supplementary Materials."}]}