{"title": "Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation", "authors": ["Giorgio Franceschelli", "Mirco Musolesi"], "abstract": "Despite the increasing use of large language models for creative tasks, their outputs often lack diversity. Common solutions, such as sampling at higher temperatures, can compromise the quality of the results. Drawing on information theory, we propose a context-based score to quantitatively evaluate value and originality. This score incentivizes accuracy and adherence to the request while fostering divergence from the learned distribution. We propose using our score as a reward in a reinforcement learning framework to fine-tune large language models for maximum performance. We validate our strategy through experiments in poetry generation and math problem solving, demonstrating that it enhances the value and originality of the generated solutions.", "sections": [{"title": "Introduction", "content": "Foundation models (Bommasani et al., 2021), particularly large language models (LLMs) (Bubeck et al., 2023; Gemini Team et al., 2023; Touvron et al., 2023), are significantly transforming creative activities. They can serve as a foundation for co-creation systems involving human and artificial authors (Lin et al., 2023); they can be utilized to generate software code (Rozi\u00e8re et al., 2023); they can even be employed to foster scientific research (Boiko et al., 2023). However, the nature of the self-supervised learning algorithms used for the training of these models tend to make them to generate samples as close as possible to the training data distribution (Franceschelli and Musolesi, 2024a). In addition, fine-tuning, such as that based on reinforcement learning from human feedback (RLHF) (Christiano et al., 2017), is often necessary to generate appropriate and accurate responses. However, this process tends to further reduce output diversity (Kirk et al., 2024), and linguistic creativity tends to be lower than that of humans (Lu et al., 2025). On the contrary, LLMs for creative tasks should produce more novel and surprising texts that maintain a high level of correctness and adherence to the request. One typical solution is to sample at higher temperature to increase diversity. However, this might lead to generating incoherent text (Peeperkorn et al., 2024).\nIn order to address the issues described above, we propose a new training approach for creative tasks based on CoVO, a Context-based score for Value and Originality, with the goal of taking into consideration both value and originality of the neurally-generated text in the optimization of LLMs. The definition of CoVO is grounded in the analysis of mutual information (MacKay, 2003) between the model's outputs and inputs, and vice versa. More specifically, we formulate a new optimization problem where, given a specific input, the desired output is derived by simultaneously maximizing the conditional probability of the input given the output and minimizing the conditional probability of the output given the input under the generative model. In this way, we optimize for solutions that are appropriate for the input request but also different from the outputs we would normally obtain from the model. In particular, we show that our information-theoretic score can be used as a reward in RL-based fine-tuning algorithms, guiding pre-trained models toward more diverse yet valuable solutions.\nWe also present the theoretical foundations of our approach and we discuss several strategies for the practical implementation of the proposed approach and its adaptation to current LLMs. Experiments on math problem solving and poetry generation show that our score correlates with domain-specific measures for value and originality, and that our approach can increase the quality and diversity of outputs, presenting itself as a candidate for creativity applications of current foundation models.\nThe remainder of the paper is structured as follows. First, we provide an overview of the state"}, {"title": "Related Work", "content": "The quest to provide a mathematical and computational definition of creativity has been a significant focus in recent decades. Numerous methods have been developed to define various dimensions or attributes for evaluating the creativity of AI-generated products (see, for example, Franceschelli and Musolesi, 2024a). However, these methods are often domain-specific and typically require substantial human effort to implement and assess. In contrast, solutions based on information theory (Shannon, 1948; Cover, 1999) offer a more universally applicable approach.\nInformation-theoretic methods can quantify creativity by measuring the novelty and complexity of generated outputs, without the need for extensive human intervention, making them suitable for a wide range of domains. Bayesian surprise (Baldi and Itti, 2010), i.e., the divergence between a prior and a posterior belief, has been extensively used to measure either novelty (Fran\u00e7a et al., 2016; Varshney et al., 2019) or surprise (Mazzaglia et al., 2022; Schmidhuber, 2010). Nevertheless, Varshney (2019) demonstrated that there is a mathematical limit for Bayesian surprise when combined with quality measures. Surprisal (Tribus, 1961), i.e., Shannon's self-information, has also been used (Bunescu and Uduehi, 2019; Fernandez Monsalve et al., 2012); Barto et al. (2013) extensively discuss surprisal and Bayesian surprise, and how novelty differ from them. Crucially, in the context of RL, surprisal has been used as a form of intrinsic motivation to encourage the agent to explore more (Achiam and Sastry, 2017). Sun et al. (2025) apply this idea to improve exploration in RLHF (Christiano et al., 2017). Burns (2006) proposes to use entropy for expectation and violation, plus posterior probability for explanation in the context of aesthetic experience. Additionally, mutual information has been applied to neural conversation models to improve both diversity and appropriateness (Li et al., 2016). However, all these existing approaches are not able to capture and simultaneously optimize value and originality at the same time."}, {"title": "Information Theory and Creativity", "content": "The quest to provide a mathematical and computational definition of creativity has been a significant focus in recent decades. Numerous methods have been developed to define various dimensions or attributes for evaluating the creativity of AI-generated products (see, for example, Franceschelli and Musolesi, 2024a). However, these methods are often domain-specific and typically require substantial human effort to implement and assess. In contrast, solutions based on information theory (Shannon, 1948; Cover, 1999) offer a more universally applicable approach.\nInformation-theoretic methods can quantify creativity by measuring the novelty and complexity of generated outputs, without the need for extensive human intervention, making them suitable for a wide range of domains. Bayesian surprise (Baldi and Itti, 2010), i.e., the divergence between a prior and a posterior belief, has been extensively used to measure either novelty (Fran\u00e7a et al., 2016; Varshney et al., 2019) or surprise (Mazzaglia et al., 2022; Schmidhuber, 2010). Nevertheless, Varshney (2019) demonstrated that there is a mathematical limit for Bayesian surprise when combined with quality measures. Surprisal (Tribus, 1961), i.e., Shannon's self-information, has also been used (Bunescu and Uduehi, 2019; Fernandez Monsalve et al., 2012); Barto et al. (2013) extensively discuss surprisal and Bayesian surprise, and how novelty differ from them. Crucially, in the context of RL, surprisal has been used as a form of intrinsic motivation to encourage the agent to explore more (Achiam and Sastry, 2017). Sun et al. (2025) apply this idea to improve exploration in RLHF (Christiano et al., 2017). Burns (2006) proposes to use entropy for expectation and violation, plus posterior probability for explanation in the context of aesthetic experience. Additionally, mutual information has been applied to neural conversation"}, {"title": "LLMs and Creativity", "content": "Since the introduction of GPT models (Brown et al., 2020; OpenAI, 2023) and their competitors (e.g., Touvron et al., 2023), researchers have been keenly exploring the potential for LLMs to exhibit creativity and the methods to achieve this (Franceschelli and Musolesi, 2024b). For example, human creativity tests like the Alternate Uses Test have been employed to evaluate the creativity of LLMs (Stevenson et al., 2022) and to investigate methods for enhancing their performance (Goes et al., 2023; Summers-Stay et al., 2023). Porter and Machery (2024) report that non-expert poetry readers already favor AI-generated poems over human-authored ones. In contrast, Davis (2024) argues that ChatGPT's poetry is incompetent and banal. Either way, instead of being used off-the-shelf, LLMs can be fine-tuned to produce more rhyming poems (Popescu-Belis et al., 2023) or utilized in zero-shot settings to emulate the writing styles of famous authors (Sawicki et al., 2023). It has also been shown that these models can be fine-tuned via RLHF (Christiano et al., 2017) to write short poems that human evaluators find more creative (Pardinas et al., 2023). Finally, it is possible to leverage quality-diversity algorithms to generate more creative products; these methods can be based on human (Ding et al., 2023) or AI (Bradley et al., 2024) feedback to measure the quality of the generated outputs."}, {"title": "Preliminaries", "content": "A \\(\\theta\\)-parameterized autoregressive language model is a probability distribution \\(p_{\\theta}(x)\\) over a variable-length text sequence \\(x = (x_1 ... x_T)\\), where T is the sequence length and each token \\(x_t\\) is in a finite vocabulary V of size N. The probability distribution is factorized as \\(p_{\\theta}(x) = \\prod_{t=1}^{T} p_{\\theta}(x_t | x_{<t})\\), where \\(x_{<t} = x_{1}...x_{t-1}\\). The language model is usually trained to maximize the likelihood of the true distribution \\(p^{*}(x)\\) for any x from a reference dataset (the training set). In other words, given an input \\(x_{<t}\\), the model learns to approximate the probability of each token from V be-"}, {"title": "Language Modeling", "content": "A \\(\\theta\\)-parameterized autoregressive language model is a probability distribution \\(p_{\\theta}(x)\\) over a variable-length text sequence \\(x = (x_1 ... x_T)\\), where T is the sequence length and each token \\(x_t\\) is in a finite vocabulary V of size N. The probability distribution is factorized as \\(p_{\\theta}(x) = \\prod_{t=1}^{T} p_{\\theta}(x_t | x_{<t})\\), where \\(x_{<t} = x_{1}...x_{t-1}\\). The language model is usually trained to maximize the likelihood of the true distribution \\(p^{*}(x)\\) for any x from a reference dataset (the training set). In other words, given an input \\(x_{<t}\\), the model learns to approximate the probability of each token from V be-"}, {"title": "Reinforcement Learning for Language Models", "content": "Due to its adherence to the formal framework of Markov decision processes (Sutton and Barto, 2018), RL can be used as a solution to the generative modeling problem in the case of autoregressive tasks such as text generation (Bachman and Precup, 2015). The LLM plays the role of the agent, and each generated token represents an action \\(a_t\\). The current version of the generated output \\(x_t\\) is part of the state \\(s_t\\) (potentially with additional information such as initial prompts). Finally, the reward \\(r_{t+1}\\) measures the \"quality\" of the current output. A common strategy is to assign a zero reward for each \\(x_t, t \\neq T\\) and a sentence-based reward when the final output is generated. Within this framework, any policy-based method, such as Proximal Policy Optimization (PPO) (Schulman et al., 2017), can be employed to train or fine-tune the LLM to optimize a given objective. Indeed, RL facilitates the use of non-differentiable reward functions, enabling the optimization of test-time metrics, domain-specific targets, and human preferences (Franceschelli and Musolesi, 2024c). To avoid deviating too much from the pre-trained model while also encouraging exploration, the full reward function usually considers a Kullback-Leibler (KL) divergence term as follows (Stiennon et al., 2020):\n\\(R(z, x) = r(z, x) - \\beta log \\frac{p_{\\theta}(x | z)}{p_{OPT}(x | z)}\\),\nwhere \\(\\theta_{OPT}\\) is the set of parameters of the pre-trained model, \\(\\beta\\) weighs the KL penalty, and \\(r(z, x)\\) can be any non-differentiable function to score the model's output.\nIn particular, RLHF (Christiano et al., 2017) has been widely adopted as the final training stage in various popular LLMs to align them with human preferences, i.e., by using a learned reward function that approximates human feedback. While highly effective, RLHF suffers from several open problems (Casper et al., 2023). These include obtaining accurate, unbiased, and representative human feedback, as well as making the process resource-"}, {"title": "A Context-Based Score for Valuable and Original Generation", "content": "Our goal is to derive a score that is able to quantify both value and originality at the same time. As discussed in depth by Csikszentmihalyi (2014), creativity depends on the context in which the product is created, as the context provides the task identification and the domain information necessary to generate and validate the outcome. In turn, the output aims to solve the given task and provide a meaningful, original contribution to the current domain. Thus, our proposed score has its roots in mutual information, which represents a quantitative way to study the relationship between contextual, prior information and a produced posterior outcome. More specifically, we start from the (point-wise) mutual information between two variables x and y:\n\\(I(x, y) = h(x) - h(x|y) = h(y) - h(y|x)\\)\nwhere the entropy is \\(h(a) = - log p(a)\\), therefore:\n\\(I(x, y) = log p(x | y) - log p(x) = log p(y | x) - log p(y).\\)\nLet us now assume x to be our input vector x and y our output vector y, obtaining:\n\\(I(x, y) = log p(y | x) - log p(y).\\)\nWe can generalize I(x, y) with two scaling factors:\n\\(I(x, y, \\lambda_1, \\lambda_2) = \\lambda_1 log p(y|x) - \\lambda_2 log p(y),\\)"}, {"title": "A Context-Based Score for Valuable and Original Generation", "content": "Our goal is to derive a score that is able to quantify both value and originality at the same time. As discussed in depth by Csikszentmihalyi (2014), creativity depends on the context in which the product is created, as the context provides the task identification and the domain information necessary to generate and validate the outcome. In turn, the output aims to solve the given task and provide a meaningful, original contribution to the current domain. Thus, our proposed score has its roots in mutual information, which represents a quantitative way to study the relationship between contextual, prior information and a produced posterior outcome. More specifically, we start from the (point-wise) mutual information between two variables x and y:\n\\(I(x, y) = h(x) - h(x|y) = h(y) - h(y|x)\\)\nwhere the entropy is \\(h(a) = - log p(a)\\), therefore:\n\\(I(x, y) = log p(x | y) - log p(x) = log p(y | x) - log p(y).\\)\nLet us now assume x to be our input vector x and y our output vector y, obtaining:\n\\(I(x, y) = log p(y | x) - log p(y).\\)\nWe can generalize I(x, y) with two scaling factors:\n\\(I(x, y, \\lambda_1, \\lambda_2) = \\lambda_1 log p(y|x) - \\lambda_2 log p(y),\\)"}, {"title": "Implementation and Optimization with Autoregressive Models", "content": "We now discuss the implementation of the CoVO score with autoregressive models. Using the notation introduced in Section 3.1, in the context of a \\(\\theta\\)-parameterized LLM, \\(p(y|x)\\) can be expressed as \\(\\prod_{t=1}^{T} p_{\\theta}(y_t | y_{<t}, x)\\). However, considering just the product of all the conditioned probabilities for an optimization problem would lead to preferring shorter sequences. To avoid this, we propose to use the T-th root: \\(\\sqrt[T]{\\prod_{j=1}^{T} p_{\\theta}(y_t | y_{<t}, x)}\\). By leveraging the properties of the logarithm, we obtain:\n\\(SARO = \\frac{\\lambda_v}{|x|} \\sum_{i=1}^{|x|} log p_{\\theta} (x_i | x_{<i}, y) - \\frac{\\lambda_o}{|y|} \\sum_{j=1}^{|y|} log p_{\\theta} (y_j | y_{<j}, x)\\)\nIt is worth noting that the vocabulary of an LLM can be extremely large, which can cause \\(p_{\\theta}(a|b)\\) to be small even when a is the most probable event given b. In particular, when an LLM generates y given x and then evaluates both \\(p_{\\theta}(y|x)\\) and \\(p_{\\theta}(x|y)\\), this can lead to a significant discrepancy between the magnitude of value and diversity. Since y has been sampled from \\(p_{\\theta}\\), its probability would be high by definition. However, there may be various ways (possibly through synonyms) to define y, leading to a smaller probability of x.\nInspired by Macedo et al. (2004), we propose to normalize \\(p_{\\theta}(a|b)\\) via \\(n' = \\frac{n - n_{min}}{n_{max} - n_{min}}\\). For probabilities, \\(n_{min} = 0\\), while \\(n_{max} = max p_{\\theta} (a | b)\\), thus obtaining the overall mapping for \\(p_{\\theta}: \\frac{p_{\\theta}(y_t | y_{<t}, x)}{max p_{\\theta} (y_{<t}, x)}\\). Once again, by applying the properties of logarithms, we obtain:\n\\(SCOVOM_{serom} (x, y,p_{\\theta}) = \\lambda_v S_v(x, y,p_{\\theta}) + \\lambda_o S_o(x, y,p_{\\theta}) = \\frac{\\lambda_v}{|x|} \\sum_{i=1}^{|x|} (log p_{\\theta} (x_i | x_{<i}, y) - max log p_{\\theta} (x_{<i}, y)) - \\frac{\\lambda_o}{|y|} \\sum_{j=1}^{|y|} (log p_{\\theta} (y_j | y_{<j}, x) - max log p_{\\theta} (y_{<j}, x))\\)"}, {"title": "Implementation and Optimization with Autoregressive Models", "content": "We now discuss the implementation of the CoVO score with autoregressive models. Using the notation introduced in Section 3.1, in the context of a \\(\\theta\\)-parameterized LLM, \\(p(y|x)\\) can be expressed as \\(\\prod_{t=1}^{T} p_{\\theta}(y_t | y_{<t}, x)\\). However, considering just the product of all the conditioned probabilities for an optimization problem would lead to preferring shorter sequences. To avoid this, we propose to use the T-th root: \\(\\sqrt[T]{\\prod_{j=1}^{T} p_{\\theta}(y_t | y_{<t}, x)}\\). By leveraging the properties of the logarithm, we obtain:\n\\(SARO = \\frac{\\lambda_v}{|x|} \\sum_{i=1}^{|x|} log p_{\\theta} (x_i | x_{<i}, y) - \\frac{\\lambda_o}{|y|} \\sum_{j=1}^{|y|} log p_{\\theta} (y_j | y_{<j}, x)\\)\nIt is worth noting that the vocabulary of an LLM can be extremely large, which can cause \\(p_{\\theta}(a|b)\\) to be small even when a is the most probable event given b. In particular, when an LLM generates y given x and then evaluates both \\(p_{\\theta}(y|x)\\) and \\(p_{\\theta}(x|y)\\), this can lead to a significant discrepancy between the magnitude of value and diversity. Since y has been sampled from \\(p_{\\theta}\\), its probability would be high by definition. However, there may be various ways (possibly through synonyms) to define y, leading to a smaller probability of x.\nInspired by Macedo et al. (2004), we propose to normalize \\(p_{\\theta}(a|b)\\) via \\(n' = \\frac{n - n_{min}}{n_{max} - n_{min}}\\). For probabilities, \\(n_{min} = 0\\), while \\(n_{max} = max p_{\\theta} (a | b)\\), thus obtaining the overall mapping for \\(p_{\\theta}: \\frac{p_{\\theta}(y_t | y_{<t}, x)}{max p_{\\theta} (y_{<t}, x)}\\). Once again, by applying the properties of logarithms, we obtain:\n\\(SCOVOM_{serom} (x, y,p_{\\theta}) = \\lambda_v S_v(x, y,p_{\\theta}) + \\lambda_o S_o(x, y,p_{\\theta}) = \\frac{\\lambda_v}{|x|} \\sum_{i=1}^{|x|} (log p_{\\theta} (x_i | x_{<i}, y) - max log p_{\\theta} (x_{<i}, y)) - \\frac{\\lambda_o}{|y|} \\sum_{j=1}^{|y|} (log p_{\\theta} (y_j | y_{<j}, x) - max log p_{\\theta} (y_{<j}, x))\\)"}, {"title": "Experiments", "content": "We evaluate the effectiveness of our RL strategy through two case studies: poetry generation (Section 6.1) and mathematical problem resolution (Section 6.2)1."}, {"title": "Poetry Generation", "content": "The first set of experiments aims to teach the LLM to generate poems that are both more original and valuable. More specifically, we follow the approach outlined by Bradley et al. (2024) and instruct the model to write a poem in a particular style and tone. We consider the Llama3-8B model (Grattafiori et al., 2024) as our pre-trained agent. Since we do not use the instruction-tuned"}, {"title": "Experimental Setup", "content": "The first set of experiments aims to teach the LLM to generate poems that are both more original and valuable. More specifically, we follow the approach outlined by Bradley et al. (2024) and instruct the model to write a poem in a particular style and tone. We consider the Llama3-8B model (Grattafiori et al., 2024) as our pre-trained agent. Since we do not use the instruction-tuned"}, {"title": "Mathematical Problem Resolution", "content": "The second set of experiments aims to teach the LLM to solve mathematical problems through more diverse procedures. In particular, we focus on the Mistral-based (Jiang et al., 2023) MetaMath"}, {"title": "Experimental Setup", "content": "The second set of experiments aims to teach the LLM to solve mathematical problems through more diverse procedures. In particular, we focus on the Mistral-based (Jiang et al., 2023) MetaMath"}, {"title": "Limitations", "content": "In the following, we will discuss the limitations of the proposed CoVO score. First of all, our score is only a quantifiable approximation of a particular theoretical perspective of creativity based on the dimensions of value and originality. It reflects a specific view of the evaluation of creativity based on the generated outputs and does not consider potential alternative theories (for example, arising from different cultures (Lubart, 1999)) and perspectives (Rhodes, 1961).\nOur experiments focused exclusively on transformer-based LLMs and were evaluated using two case studies. While their generalizability is supported by the theoretical framework discussed in the paper, the resulting performance was experimentally evaluated for a finite number of scenarios. Additionally, optimizing for our score necessitates further training, which incurs a significant computational cost and requires additional data or specific environments for effective learning. Moreover, this optimization may be susceptible to reward hacking (Skalse et al., 2022), where the model adopts undesirable strategies to boost the reward without genuinely enhancing value and originality."}, {"title": "Ethical Considerations", "content": "The authors are aware of the potential impact that generative technologies might have on the production of artistic outputs and, as a consequence, on human artists. This work may contribute to enhancing the quality of generated outputs. However, the authors argue that typical traits of human creativity, such as the active participation of artists in the creative process, cannot be directly replicated by machines. The authors refer interested readers to a previous work of theirs (Franceschelli and Musolesi, 2024b), in which these themes are discussed in detail."}, {"title": "Implementation Details", "content": "The experiments were carried out using a local server with an NVIDIA A100 GPU. Table 4 reports the full training parameters for the experiments on poetry generation (Section 6.1). The prompt for generation leverages Nothing gold can stay by Robert Frost, Fame is a bee by Emily Dickinson, and Epitaph by William Carlos Williams for few-shot learning:\nWrite a fatalistic epigram poem of high,award winning quality.\nNature's first green is gold,\nHer hardest hue to hold.\nHer early leaf's a flower;\nBut only so an hour.\nThen leaf subsides to leaf.\nSo Eden sank to grief,\nSo dawn goes down to day.\nNothing gold can stay.\nWrite an ironic quatrain poem of high,award winning quality.\nFame is a bee.\nIt has a song-\nIt has a sting-\nAh, too, it has a wing.\nWrite a naturalistic epitaph poem of high, award winning quality.\nAn old willow with hollow branches\nSlowly swayed his few high fright tendrils\nAnd sang:\nLove is a young green willow\nShimmering at the bare wood's edge.\nWrite a {tone} {style} of high, award winning quality.\nThe training phase includes requests with tone-style pairs sampled among \u2018dark', 'happy', 'mysterious', \u2018reflective' or 'romantic' for the tone, and \u2018ballad', 'haiku', \u2018hymn\u2019, \u2018limerick' or 'sonnet' for the style.\nAt inference time we also consider 'cinquain',"}]}