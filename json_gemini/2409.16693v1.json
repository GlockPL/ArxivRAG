{"title": "CaBRNet, An Open-Source Library For Developing And Evaluating Case-Based Reasoning Models", "authors": ["Romain Xu-Darme", "Aymeric Varasse", "Alban Grastien", "Julien Girard-Satabin", "Zakaria Chihani"], "abstract": "In the field of explainable AI, a vibrant effort is dedicated to the design of self-explainable models, as a more principled alternative to post-hoc methods that attempt to explain the decisions after a model opaquely makes them. However, this productive line of research suffers from common downsides: lack of reproducibility, unfeasible comparison, diverging standards. In this paper, we propose CaBRNet, an open-source, modular, backward-compatible framework for Case-Based Reasoning Networks:\nhttps://github.com/aiser-team/cabrnet.", "sections": [{"title": "Introduction", "content": "As a reflection of the social and ethical concerns related to the increasing use of AI-based systems in modern society, the field of explainable AI (XAI) has gained tremendous momentum in recent years. XAI mainly consists of two complementary avenues of research that aim at shedding some light into the inner-workings of complex ML models. On the one hand, post-hoc explanation methods apply to existing models that have often been trained with the sole purpose of accomplishing a given task as efficiently as possible (e.g., accuracy in a classification task). On the other hand, self-explainable models are designed and trained to produce their own explanations along with their decision. The appeal of self-explainable models resides in the fact that rather than using an approximation (i.e., a post-hoc explanation method) to understand a complex model, it is better to directly enforce a simpler (and more understandable) decision-making process during the design and training of the ML model, provided that such a model would exhibit an acceptable level of performance."}, {"title": "CaBRNet: One framework to rule them all...", "content": "In a field of research as wide as XAI, where the proliferation of tools and methods is increasing, seeking a common framework is a natural temptation that often leads to yet another tool that evolves besides the rest. To minimize this risk, CaBRNet is designed with three main objectives, essential for a lasting acceptance: supporting past state-of-the-art methods through backward compatibility, facilitating present developments by striving for modularity, and ensuring their reusability in future works through reproducibility."}, {"title": "Modularity", "content": "While each model has its own specific properties (e.g., decision tree in ProtoTree, scoring sheet in PIP-Net, linear layer in ProtoPNet), CBR image classifiers from the state of the art [2, 8, 9, 10, 15, 7] share a common architecture, displayed on Figure 1. In CaBRNet, this common architecture can be parametrized at will using a set of YAML files that are stored in each training directory for reproducibility (see Sec. 2.3).\nMore precisely, as shown in Figure 1 and Table 1, the architecture of a CBR image classifier starts with a backbone usually the feature extractor of an"}, {"title": "Backward compatibility", "content": "Significant work has already been carried out on CBR classifiers. Therefore, ensuring that any result obtained with previously existing codebases (e.g., model training) can be reused in the CaBRNet framework is paramount and has been one of our main and earliest priorities, in an effort to ensure backward compatibility.\nFirst, any previously trained model can be loaded and imported within the CaBRNet framework, i.e., models trained using the original code proposed by the authors of ProtoPNet and ProtoTree can be used by our framework for other purposes (e.g., benchmarks) without having to retrain the model from scratch.\nSecond, our implementation of existing architectures (currently, ProtoPNet and ProtoTree) supports two modes: i) default mode, where all operations have been reimplemented so that they are as up-to-date as possible with the latest PyTorch versions; ii) compatibility mode, as a sanity check (i.e., to make sure that our implementation does not deviate from previous implementations), whose purpose is to be accurate with previous works at the operation level\u00b9."}, {"title": "Reproducibility and transparency", "content": "Reproducibility is key for the transparency of research and good software engineering alike. However, replicating machine learning (ML) results can be particularly tricky2. One reason for this is the inherent randomness at the core of ML programs (and the lack of documentation of settings related to that randomness). The rapid update pace of common ML frameworks leads to the regular deprecation of APIs and features, which may make a code impossible to run without directly changing its source, or going through the rabbit hole of dependency hell. As an example, the mixed-precision training setting, available from PyTorch v1.10, uses different types for CPU and GPU computations, which may lead to different results. One last part is the under-specification of the dataset constitution (curation process, classes imbalance, etc.) and preprocessing pipelines (test/train splits, data transformations).\nReproducibility can cover many notions. In [5], it is defined as the following: \"A ML study is reproducible if readers can fully replicate the exact results reported in the paper.\" We strive to provide a similar definition: \"the same set of parameters will always yield the same results\", with the following limitations:\nreproducibility can be ensured only for a given hardware/software configuration. The software configuration is specified through the requirements"}, {"title": "... and in the light evaluate them", "content": "In this section, we describe our design choices for the CaBRNet benchmark, with the purpose of proposing a framework for the systematic evaluation of CBR models."}, {"title": "Going beyond accuracy", "content": "Current CBR models rely on two assumptions: i) proximity in the latent space (w.r.t. a given distance metric) is equivalent to similarity in the visual space; ii) there exists a simple mapping between a latent vector and a localized region in the original image", "integrates": "n"}]}