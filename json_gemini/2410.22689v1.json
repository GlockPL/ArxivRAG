{"title": "Multi-Task Interactive Robot Fleet Learning with Visual World Models", "authors": ["Huihan Liu", "Yu Zhang", "Vaarij Betala", "Evan Zhang", "James Liu", "Crystal Ding", "Yuke Zhu"], "abstract": "Recent advancements in large-scale multi-task robot learning offer the potential for deploying robot fleets in household and industrial settings, enabling them to perform diverse tasks across various environments. However, AI-enabled robots often face challenges with generalization and robustness when exposed to real-world variability and uncertainty. We introduce SIRIUS-FLEET, a multi-task interactive robot fleet learning framework to address these challenges. SIRIUS-FLEET monitors robot performance during deployment and involves humans to correct the robot's actions when necessary. We employ a visual world model to predict the outcomes of future actions and build anomaly predictors to predict whether they will likely result in anomalies. As the robot autonomy improves, the anomaly predictors automatically adapt their prediction criteria, leading to fewer requests for human intervention and gradually reducing human workload over time. Evaluations on large-scale benchmarks demonstrate SIRIUS-FLEET's effectiveness in improving multi-task policy performance and monitoring accuracy. We demonstrate SIRIUS-FLEET's performance in both RoboCasa in simulation and Mutex in the real world, two diverse, large-scale multi-task benchmarks. More information is available on the project website:\nhttps://ut-austin-rpl.github.io/sirius-fleet", "sections": [{"title": "1 Introduction", "content": "In recent years, there have been significant advancements in developing robots capable of performing various tasks [1, 2, 3]. The rapid progress in generalist robots holds great potential for deploying robot fleets [4, 5, 6] in households and industrial environments where the robots operate under a generalist multi-task policy. Despite these research advances, robots face challenges with generalization and robustness when deployed in real-world environments, which are often diverse and unstructured. These challenges undermine the safety and reliability of robot systems and limit their applicability in practical scenarios.\nTo address these challenges, a series of works have been developed on interactive imitation learning (IIL) [7, 8, 9, 10, 11, 12, 13] and interactive fleet learning (IFL) [14, 5, 6]. Prior work on human-in-the-loop learning [7, 13, 15, 10] has proposed involving humans in real-time monitoring and correction to ensure trustworthy deployment. However, these methods often require continuous human supervision. To reduce the high human workload, runtime monitoring approaches [16, 17, 18, 19, 12, 20, 21] have been proposed. These approaches automatically monitor robot performance, identify anomalies, and request human control when needed. Methods like out-of-distribution (OOD) detection [20, 21, 18] and failure detection [12] have been introduced to identify anomaly cases of robot execution. However, these methods have primarily been used in single-task settings, limiting their effectiveness for large-scale, multi-task fleet deployment.\nWe introduce SIRIUS-FLEET, a multi-task interactive robot fleet learning framework. SIRIUS-FLEET consists of a multi-task policy and a runtime monitoring mechanism. SIRIUS-FLEET enables"}, {"title": "2 Related Work", "content": "Multi-Task Robot Learning. Recent advancements in multi-task robot learning have seen the development of agents capable of performing diverse tasks across various domains [2, 26, 27, 1, 28, 29, 30, 31] These advances have been driven by innovations in policy architectures [1, 32, 33, 34], the availability of large-scale datasets [35, 36, 37, 38], and new robotics benchmarks [32, 39]. Despite this progress, many of these models are deployed as static, one-off implementations, which limits their robustness and generalization in real-world, unstructured environments. In contrast, SIRIUS-FLEET is the first framework for robot manipulation that enables iterative improvement of multi-task policies through human-in-the-loop interaction during deployment.\nRobot Fleet Learning. The recent large-scale robot deployment and data collection efforts [40, 41, 42] have increased interest in robot fleet learning [43, 4, 5, 44, 45, 6]. This area of research addresses key challenges such as resource allocation [43, 14], decentralized and federated learning [5, 44, 45], and system management [6]. However, two significant challenges remain: supervising large robot fleets with minimal human oversight and enabling multi-task policy improvement over time using deployment data. SIRIUS-FLEET addresses these challenges by combining runtime monitoring with continuous policy updates in a multi-task, interactive fleet learning framework. This ensures that policies improve iteratively based on deployment data and human feedback.\nInteractive Imitation Learning. Human-in-the-loop methods have been introduced to ensure safe and trustworthy deployment by allowing robots to learn from human interventions during task execution [10, 9, 7, 13, 15, 8, 11]. To reduce the burden of constant human oversight, runtime monitoring techniques have been developed to identify anomalies during task execution [12, 17, 46, 16, 47, 48]. Two main areas of focus are unsupervised out-of-distribution (OOD) detection [18, 49, 19, 50, 20, 21], and failure detection, which can be done either by binary classification [51, 52, 53] or by learning risk functions from trajectory data [12, 14]. Recent advancements in foundation models have also led to the use of Large Language Models (LLMs) and Vision Language Models (VLMs) to identify anomalies for policies based on these models [54, 55, 56]. While prior work has primarily focused on task-specific dynamics models for single-task settings, SIRIUS-FLEET introduces a visual world model trained on diverse datasets before deployment. This model can predict anomalies across various tasks, making SIRIUS-FLEET scalable without additional training during deployment."}, {"title": "3 SIRIUS-FLEET: Multi-Task Interactive Robot Fleet Learning", "content": ""}, {"title": "3.1 Background", "content": ""}, {"title": "3.1.1 Problem Formulation", "content": "We formulate multi-task interactive robot fleet learning as a finite-horizon Markov Decision Process (MDP), where N robots operate in N independent MDPs. The i-th robot operates in its respective i-th MDP, defined as M\u1d62 = (S, A, T, H\u1d62, \u03bc, R\u1d62), where S is the state space, A is the action space, T: S\u00d7A \u2192 S is the transition dynamics, H\u1d62 is the horizon length, \u03bc is the initial state distribution, and R\u1d62: S \u00d7 A \u2192 \u211d is the reward function. In sparse-reward settings, R\u1d62 is replaced with a goal predicate g\u1d62 : S \u2192 {0,1}. The collection of MDPs, {M\u1d62}\u1d62=1\u1d3a, can be reformulated as a single unified MDP with shared state space S, action space A, and transition function T. Data from all robots are aggregated to learn a unified multi-task policy \u03c0(a | s, g\u1d62), which maximizes the expected return: max J(\u03c0) = E\u209b\u209c,\u2090\u209c~\u03c0,\u00b5\u2080 [\u03a3\u209c=1\u1d34 g\u1d62(s\u209c)]."}, {"title": "3.1.2 Multi-Task Interactive Fleet Deployment", "content": "We consider an interactive learning framework [7, 13, 17] for a fleet of robots [4], where learning and deployment happens iteratively with a human policy \u03c0\u1d34 in the loop. Each robot bootstraps its initial policy \u03c0\u2080 via behavioral cloning from a set of human demonstrations, D\u2070. Starting from deployment round i = 1, each robot executes tasks using policy \u03c0\u1d62 with runtime monitoring, supervised by anomaly predictors E\u1d62. During policy execution, E\u1d62 determines whether the current state may lead to anomalies. Upon predicting a potential anomaly, the system signals the human to monitor the process. While monitoring, the human can choose to actively intervene [57] and take control if necessary, allowing the human policy \u03c0\u1d34(s, g) to override the robot policy \u03c0\u1d62(s, g). The resulting trajectories \ud835\udcaf = (s\u209c, a\u209c, r\u209c, c\u209c) are added to the data buffer D' of the current deployment round, where c\u209c indicates whether timestep t is controlled by human action. The data buffer is then updated as D\u2071\u207a\u00b9 = D\u2071 \u222a D', which is used to train the next-round policy \u03c0\u1d62\u208a\u2081 and anomaly predictors E\u1d62\u208a\u2081."}, {"title": "3.2 Runtime Monitoring for Multi-Task Fleet Deployment", "content": "We present SIRIUS-FLEET's runtime monitoring mechanism, which supervises multiple tasks simultaneously across diverse environments during deployment. The system is designed to meet three key goals: 1) generalization\u2014the anomaly predictors are built on shared embeddings from the visual world model, allowing them to be used across different tasks; 2) task adaptability-it adjusts dynamically to the evolving progress of each task during deployment; and 3) failure preemption-it predicts anomalies before they occur, enabling timely prediction and intervention. To achieve this, we train a visual world model that simulates task progress and supports anomaly prediction across various tasks, as illustrated in Figure 2."}, {"title": "3.2.1 Training the Visual World Model", "content": "Inspired by recent advances in world models [22, 23, 24, 25], we train a visual world model on diverse robot trajectory frames to predict future task outcomes and prevent potential failures. The visual world model, trained by reconstructing image observations, predicts future latent embeddings. This world modeling approach is effective for several reasons: 1) pixel reconstruction is a readily available form of supervision, allowing the model to learn without manual annotations; 2) reconstructing image frames helps the model capture the fine-grained visual details necessary for precise manipulation tasks, and 3) it helps to develop the model's ability to predict changes in the robot's visual environment over time.\nSIRIUS-FLEET trains an autoregressive visual world model W as the backbone for downstream anomaly prediction. The world model W = (E\u1d67, D\u2093, T\u1d67) consists of an encoder E\u1d67, a decoder"}, {"title": "3.2.2 Building the Downstream Anomaly Predictors", "content": "The visual world model captures changes in task outcomes over time. Its learned representation can be used for the anomaly predictors to predict future anomalies. Since individual tasks vary, we train task-specific anomaly predictors on the frozen representation to predict failures and OOD anomalies.\nFailure Prediction. We train a failure classifier F for each task using the frozen image embeddings from the visual world model. Failure labels are from trajectories \ud835\udcaf = (s\u209c, a\u209c, r\u209c, c\u209c) in the last round D' (see Section 3.1.2), where c\u209c marks human interventions (human). The trajectory segment before each intervention are labeled as failures (failure) [13, 17]. The classifier is trained using a cross-entropy loss L\ua730 = -\u03a3\u209c=1\u1d3a y\u1d62 log(\u0177\u1d62) with balanced sampling, where y\u1d62 \u2208 {rollout,failure,human}.\nThe failure classifier is a small, computationally efficient model trained on frozen world model embeddings (training time 1.5 hour).\nOut-of-Distribution (OOD) Prediction. We identify OOD states using k-means clustering. The frozen visual world model W generates embeddings from sampled trajectories in the data buffer. We use Principal Component Analysis (PCA) to reduce the embedding dimensions to l and calculate c k-means centroids for each task. To predict OOD for an embedding z, we reduce its dimensions, find the nearest centroid, and calculate its L2 distance d. A state is identified as OOD if d exceeds the task threshold \u03b8g. \u03b8g is determined by d\u2080, which is the distance of the top \u03b8g percentile of the distances to the nearest centroids from the validation latent embeddings. For efficiency, we perform task-specific k-means clustering on image embeddings for the specific task without training additional models.\nAdaptive Decision Boundaries. Anomaly predictors should adjust their anomaly prediction threshold as robot performance improves during deployment. In a multi-task setting, the varying task performances across different tasks pose additional challenges. We adjust anomaly predictors by loosening the decision boundaries for high-performing tasks and tightening them for low-performing tasks. For failure prediction, we finetune it using the most recent round of deployment data D', with human intervention labels reflecting the updated human perceived risk."}, {"title": "3.2.3 SIRIUS-FLEET in Operation", "content": "Continual Model Improvement. While the visual world model W is trained once and frozen, the policy and the anomaly predictors are continually finetuned over deployment: the policy is updated with the data from previous rounds using weighted sampling, the failure classifier for failure prediction is finetuned with the most recent intervention labels, and the K-means clustering in OOD prediction expands its latent space coverage after each round.\nAnomaly Predictors at Runtime. During deployment, the visual world model predicts each task's future embeddings over L steps. We generate N possible future scenarios by sampling from the cVAE latent space and making L-step predictions N times. Each predicted future embedding is then assessed individually by the anomaly predictors. For failure prediction, we compute the average failure score across future embeddings. For OOD prediction, we calculate the average distance of each future state to its nearest cluster centroid and compare the final average distance against the OOD threshold.\nMulti-Task Policy Training. We train a Transformer-based [61] multi-task policy, as shown in Figure 4. The policy inputs image observations, robot proprioceptive data, language task goals, and output robot actions. The design follows that in RoboMimic [32] and RoboCasa [39]."}, {"title": "4 Experiments", "content": "In our experiments, we aim to address the following questions: 1) How well does SIRIUS-FLEET continue to improve its policy during deployment with effective runtime monitoring? 2) How does SIRIUS-FLEET's runtime monitoring performance compare with baseline methods? 3) Are the anomaly prediction made by SIRIUS-FLEET at the right moments, qualitatively?"}, {"title": "4.1 Evaluation Setup", "content": "To measure the effectiveness of SIRIUS-FLEET's multi-task fleet learning, we evaluate how both the policy and the runtime monitoring perform and evolve over time. Following prior works [62, 13, 17], we evaluate the system in a human-in-the-loop setting through rounds of iterative deployment.\nEvaluation Setting. Our evaluation spans three rounds of policy updates and runtime monitoring. Full human supervision is required in the first round since no anomaly predictors have been trained"}, {"title": "4.2 Evaluation Results", "content": "System Performance Over Time. Figure 6 presents the performance of SIRIUS-FLEET over several deployment rounds. The autonomous policy improves consistently, showing a 13% increase in simulation and 45% in real-world settings. SIRIUS-FLEET maintains high combined policy performance, exceeding 95% in both environments, demonstrating the effectiveness of runtime monitoring and timely human intervention. Additionally, SIRIUS-FLEET shows improved ROHE performance"}, {"title": "5 Conclusion", "content": "We introduce SIRIUS-FLEET, a framework for multi-task interactive robot fleet learning that combines a multi-task policy with runtime monitoring. Driven by a visual world model and task-adaptive anomaly predictors, SIRIUS-FLEET improves policy performance and reduces human intervention through timely anomaly prediction.\nLimitations. SIRIUS-FLEET is best suited for quasi-static manipulation tasks, where anomalies can be easily corrected with teleoperation. However, applying SIRIUS-FLEET to dynamic tasks could be challenging. Also, our experiments involve a small group of five human operators; future work could expand this by conducting large-scale human studies to understand the effects of diverse human interventions on runtime monitoring and policy training. Lastly, our evaluations used a single embodiment (Franka Emika Panda arm). Future research can extend SIRIUS-FLEET to cross-embodiment fleet learning, making it more generalizable across different robot platforms [36]."}, {"title": "6 Appendix", "content": ""}, {"title": "6.1 Table of Contents", "content": "\u2022 Ablations (Section 6.2)\n\u2022 Implementation Details (Section 6.3)\n\u2022 Qualitative Analysis and Discussion (Section 6.4)\n\u2022 Additional Details on Tasks (Section 6.5)"}, {"title": "6.2 Ablations", "content": "Multi-Task World Model vs. Single-Task World Model. We empirically show how the multi-task world model is more generalizable than the single-task version in terms of future latent state prediction accuracy. Accurate future latent state prediction is essential for effective error prediction, as the downstream error predictors directly use the predicted future states. We compared the performance of SIRIUS-FLEET's multi-task world model against six single-task world models in terms of mean squared error (MSE) for future latent state prediction. As shown in Table 1, the multi-task world model consistently outperforms the single-task models across all tasks. This indicates that the multi-task world model is crucial for providing more accurate state predictions.\nAblation of Failure and OOD Prediction. We examine the impact of OOD and failure prediction components on human intervention overlap accuracy: We have a human operator fully supervise the robot execution of a policy and can intervene whenever an unsafe state is observed. We then apply the learned error predictors to the collected trajectories and compare failure states identified by the error predictors with those intervened by the human operator. This shows how the error predicted aligns with humans' risk assessment.\nAs shown in Table 2, the combined use of OOD prediction and failure prediction by SIRIUS-FLEET outperforms using either component alone across all tasks. This indicates that both accurate OOD and failure prediction are essential to the overall policy performance, as they work together to identify potential errors during deployment. Videos of trajectories predicted as OOD or failures are available on the project's website.\nMulti-task Policy vs. Single-task Policy. We show that training on multi-task settings improves the overall performance of robot policy than training on individual single tasks. We show the policy success rate (%) for 7 different tasks in Figure 8, where we compare the multi-task policy trained on all tasks with single-task policies trained on individual tasks. For most tasks, SIRIUS-FLEET'S multi-task policy performs better than the single-task policy trained on that specific task. This aligns with the observation in prior works [31, 36, 2] that multi-task training helps the policy generalize better."}, {"title": "6.3 Implementation Details", "content": "We present the details of hyperparameters for training the world model, policy, and error predictors in the following table:\n\u2022 World Model Architecture: Table 3;\n\u2022 Policy Architecture: Table 4;\n\u2022 Training: Table 5;\n\u2022 Failure Prediction: Table 6;\n\u2022 OOD Prediction: Table 7."}, {"title": "6.4 Qualitative Analysis and Discussion", "content": "Can SIRIUS-FLEET catch important errors and give meaningful intervention timings? We conduct a qualitative analysis of where the error predictors predict error during robot execution using active human monitoring. Specifically, we have a human operator who fully supervises the robot's policy execution and can intervene whenever an unsafe state is observed. We then apply the learned error predictors to the collected trajectories and compare failure states identified by the error predictors with those intervened by the human operator. This shows how the error predicted aligns with humans' risk assessment.\nWe show examples of the human intervention region and errors predicted by SIRIUS-FLEET and the baselines in Figure 9 to 14. We use the same trajectory for all methods. The three instances humans gave interventions are: 1) the robot is not aiming at the object it is grasping; 2) the robot pauses at grasping; 3) the arm is too stiff and does not bend down. We visualize if the errors given by SIRIUS-FLEET and the baselines are aligned with the intervention humans gave.\nThe green area indicates times of actual human intervention; the blue plot indicates the error value predicted by the different methods. We also show the video frame at the corresponding timings to illustrate the failure modes captured (or missed) by the different methods.\nNote that SIRIUS-FLEET, PATO, and ThriftyDAgger each have two separate components, so we visualize each individually. PATO and ThriftyDAgger share the same ensemble uncertainty component, so we visualize one of them.\nAs shown in Figure 9 and 10, our method can capture a similar error pattern to that the human judges. MoMaRT (Figure 11) is prone to predicting false positives due to pixel changes. PATO (VAE part, Figure 12) often predicts false negatives; PATO (ensemble part, Figure 13) often has high oscillations and extreme values. ThriftyDAgger's (Figure 14) risk Q function is data-intensive to train and often has generalization errors; it cannot accurately reflect the task progress and local failure modes."}, {"title": "6.5 Additional Details on Tasks", "content": ""}, {"title": "6.5.1 Real robot experiments", "content": "Dataset. We use the Mutex [27] dataset for real robot experiments. The Mutex dataset is a diverse multi-task dataset containing 50 different tasks from 8 task suites (where one task suite is a distinct set of objects and receptacles), specified with natural language. More details on the Mutex dataset can be found in the original Mutex [27] paper. We use the original 50 tasks for training the visual world model; for human workload consideration, we sample 10 of the tasks (among 5 task suites) for multi-task policy learning and runtime monitoring at deployment. The list of 10 tasks for policy learning and runtime monitoring experiments is shown in Figure 15.\nTraining. To train the visual world model, we use 30 demonstrations for each of the 50 tasks. To train the multi-task policy, 30 demonstrations per task are used to bootstrap the initial BC policy; then, for each of the human-in-the-loop deployment round for runtime monitoring (see Evaluation Setting in Section 4.1), 60 robot rollouts per task are collected for each round.\nFor the initial BC policy, we train the BC transformer policy for 2000 epochs. For each subsequent round, we finetune the initial BC policy on the newly aggregated data for another 800 epochs. We use this finetuned BC policy checkpoint to perform runtime monitoring experiments.\nEvaluation. We use 1 seed and the checkpoint at a fixed epoch (epoch 2000 for the initial policy, epoch 800 for the finetuned per-round policy) to evaluate the real robot policy. We evaluate all 10 tasks, conduct 20 trials per task for 200 trials, and report the average success rate across tasks. We report the per-task autonomous policy performance in Figure 17 and per-task combined policy performance in Figure 18 for reference."}, {"title": "6.5.2 Simulation", "content": "Dataset. We use RoboCasa [39] as our simulation environments, which contains a diverse range of tasks We note one important difference in the environment definition in RoboCasa: the environments are defined by a group of tasks that has similar task semantics, rather than by one single task of a"}]}