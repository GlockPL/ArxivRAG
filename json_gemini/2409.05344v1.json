{"title": "GOPT: Generalizable Online 3D Bin Packing via Transformer-based Deep Reinforcement Learning", "authors": ["Heng Xiong", "Changrong Guo", "Jian Peng", "Kai Ding", "Xuchong Qiu", "Long Bai", "Jianfeng Xu"], "abstract": "Robotic object packing has broad practical applications in the logistics and automation industry, often formulated by researchers as the online 3D Bin Packing Problem (3D-BPP). However, existing DRL-based methods primarily focus on enhancing performance in limited packing environments while neglecting the ability to generalize across multiple environments characterized by different bin dimensions. To this end, we propose GOPT, a generalizable online 3D Bin Packing approach via Transformer-based deep reinforcement learning (DRL). First, we design a Placement Generator module to yield finite subspaces as placement candidates and the representation of the bin. Second, we propose a Packing Transformer, which fuses the features of the items and bin, to identify the spatial correlation between the item to be packed and available sub-spaces within the bin. Coupling these two components enables GOPT's ability to perform inference on bins of varying dimensions. We conduct extensive experiments and demonstrate that GOPT not only achieves superior performance against the baselines, but also exhibits excellent generalization capabilities. Furthermore, the deployment with a robot showcases the practical applicability of our method in the real world. The source code will be publicly available at https://github.com/Xiong5Heng/GOPT.", "sections": [{"title": "I. INTRODUCTION", "content": "With the prosperity of the global trade and e-commerce market, warehouse automation has developed rapidly in recent years. Efficient object placement in the warehouse through optimal packing strategies can bring numerous benefits, such as reduced labor requirements and cost savings [1].\nIn this paper, it is assumed that the robot picking is well implemented. Researchers have commonly addressed the placement challenge in robot packing by formulating it as an online 3D Bin Packing Problem (3D-BPP) [2], [3]. As one of the classic combinatorial optimization problems, 3D-BPP strives to place a set of known cuboid items in an axis-aligned fashion into a bin to maximize space utilization. However, observing all items and obtaining full knowledge about them is challenging in many real-world scenarios. The online 3D-BPP is a more practical variant of 3D-BPP that refers to packing items one by one under the observation of only the incoming item.\nDue to the limited knowledge, the online 3D-BPP cannot be solved by exact algorithms [4]. Researchers have previously focused on developing heuristics with the greedy objectives for the problem, which are designed by abstracting the experience of human packers [5]. However, while intuitive, these heuristics typically yield sub-optimal solutions."}, {"title": "II. RELATED WORK", "content": "The 3D-BPP is a classical optimization problem and is known to be strongly NP-hard [11]. We herein briefly review related heuristic and DRL-based methods."}, {"title": "A. Heuristic Methods", "content": "Early works primarily focus on designing efficient heuristics for their simplicity. Researchers attempt to define some packing rules distilled from human workers' experience, such as First Fit [12], Best Fit [13], and Deepest-Bottom-Left-Fill [14]. Corner points (CP) [15], extreme points (EP) [16], empty maximal spaces (EMS) [17], and internal corners point (ICP) [18] endeavor to represent potential free spaces where items can be packed for enhancing heuristic methods. For instance, Ha et al. [5] propose OnlineBPH, which selects one EMS to minimize the margin between the faces of the item to be packed and the faces of the EMS. Yarimcam et al. [19] provide heuristics expressed in terms of policy matrices by employing the Irace parameter tuning algorithm [20]. Wang et al. [21] propose Heightmap-Minimization (HM) which favors the placement that minimizes occupied volume. To mitigate the uncertainties originating from the real world, Shuai et al. keep deformed boxes stacked close to enhance stability [22]. Hu et al. develop a Maximize-Accessible-Convex-Space (MACS) strategy to optimize the available empty space for packing potential large future items [23]. These methods are intuitive and effective; however, they rely on hand-crafted rules and lack the capacity to demonstrate superior performance consistently across diverse problem settings. Our work draws on the representation of empty spaces in heuristics, but uses DRL to learn packing patterns without being limited by domain expert knowledge."}, {"title": "B. DRL-based Methods", "content": "DRL has shown promise in solving certain combinatorial optimization problems [24], [25]. Therefore, there is a trend to use DRL to solve the 3D-BPP recently. Que et al. [26] tackle the offline 3D-BPP with variable height by using DRL with Transformer structure to sequentially address subtasks of position, item selection, and orientation. Instead, we focus on the online 3D-BPP and determine the position and orientation simultaneously. To the best of our knowledge, Deep-Pack [27] is the first to use a DRL-based model to solve a 2D online packing problem, with potential extensions to the online 3D-BPP. It takes an image showing the current state of the bin as input and outputs the pixel location for packing the incoming item. Verma et al. [6] combine a search heuristic with DRL and propose a two-step strategy for solving the problem with any number and size of bins. Zhao et al. [2], [10] formulate the problem as a constrained MDP and adopt ACKTR method [28] to train a CNN-based DRL agent. In [2], the DRL agent comprises an actor, a critic, and a predictor to estimate action probabilities, value, and feasibility mask, respectively. It is then improved by decomposing the packing action into the length and width dimensions and orientation to reduce action space [10]. They subsequently introduce the Packing Configuration Tree (PCT) based on heuristic search rules and incorporate it into a DRL agent [8]. The agent employs Graph Attention Networks [29] as the policy and is also trained with ACKTR. To investigate the synergies of heuristics and DRL, Yang et al. [7] propose PackerBot, which utilizes heuristic reward to assist the DRL agent to perform better. Xiong et al. [3] introduce a candidate map mechanism to reduce the complexity of exploration and improve performance for the CNN-based DRL agent trained with A2C [30]. These methods usually concatenate features of the item and the bin directly to learn policies. In contrast, GOPT first proposes free sub-spaces within a bin and utilizes a modified Transformer to discern the relations among these spaces and the relations between them and the current item. Our method ensures generalizability across diverse packing environments."}, {"title": "III. METHODOLOGY", "content": "As shown in Fig. 1, a robot randomly picks an object from an unstructured pile with a set of box-shaped items of various dimensions. The complete knowledge about all items is unavailable in advance. One camera measures the dimensions of the picked item, which is then placed into the packing bin. This specific scenario can be characterized as an online 3D-BPP. The objective is to place as many items into the bin as possible and maximize the bin's space utilization.\nWe define the front-left-bottom (FLB) vertex of the bin with dimensions (L,W,H) as the origin (0,0,0), and the directions along the length, width, and height as X, Y, and Z directions, respectively, as shown in Fig. 2a. For items,\n$(x_t, y_t, z_t)$ denotes the FLB coordinate of the t-th item with dimensions $(l_t, w_t, h_t)$.\nIn the robot packing task, the following physical constraints must be taken into consideration.\nOrthogonal placement: Items are placed orthogonally into the bin, and their sides are aligned with the bin's sides."}, {"title": "B. Placement Generator", "content": "For the selected item to be packed, we predict the horizontal position $(x_t, y_t)$ and the corresponding orientation of its placement in the bin. The vertical position $z_t$ is analytically determined by the lowest placement position due to gravity. As aforementioned, there are two possible orientations for one item. Therefore, when placing an item into a bin with dimensions (L, W, H), it results in a total number of $L \u00d7 W \u00d7 2$ possible placements [2]. On the one hand, this quantity is unbearable for the packing problem with the sequential-decision nature because it will grow exponentially with larger bin dimensions. On the other hand, some are inevitably unproductive for the item to be packed within this placement set.\nWith the aim of constraining the potentially large placement search space, we design a Placement Generator (PG) module to produce a finite and efficient placement subset based on the incoming item and current bin configuration. We first explicitly represent the real-time status of the bin by utilizing the heightmap. Other methods that leverage planned placements for previous items as the representation [8] lack feedback and closed-loop control. In contrast, the heightmap can be derived from the visual observation captured by a camera conveniently when deploying PG in a real-world robot packing task. Drawing from the empty maximal space"}, {"title": "C. Reinforcement Learning Formulation", "content": "DRL problems are commonly modeled as a Markov Decision Process (MDP). An MDP with parameters $(S, A, P, R, \u03b3)$ is utilized to characterize the packing environment in this paper, where $S$ denotes the state space, $A$ denotes the action space, $P:S\u00d7A\u00d7S \u2192 [0,+\u221e)$ stands for the transition probabilities, $R : S \u00d7 A \u2192 R$ is the scalar reward function, and $\u03b3\u2208 (0,1]$ is the discount factor for balancing the near-term and long-term rewards in DRL. Reinforcement learning algorithms aim to learn a policy $\u03c0 : S \u00d7 A \u2192 R$, which determines the probability of selecting an action a given a state s. The objective of the policy is to maximize the cumulative discounted reward over an episode, expressed as $\\sum_t \u03b3^t r_t$, where t denotes the time step, and $r_t$, $a_t$, and $s_t$ represent the reward, action, and state at time step t, respectively. In the following, we formulate the online 3D-BPP as an MDP for DRL training.\nState: At each time step t, the policy receives a state $S_t$, comprising the incoming item to be packed $S_{t,item}$ and the current bin configuration $S_{t,bin}$. For the first part, the dimension of the item $(l_t, w_t, h_t)$ is essential. Some studies [3], [7] employ this three-dimensional vector explicitly as the item representation, while others prefer a three-channel map for the convenience of neural network design [2], [9]. In the map representation, each channel is assigned $l_t$, $w_t$, and $h_t$, respectively. To account for both the geometry and optional orientations, we propose an item representation which is a\n2\u00d73 matrix, $S_{t,item} = \\begin{bmatrix} l_t & w_t & h_t \\ w_t & l_t & h_t \\end{bmatrix}$, where $(l_t, w_t, h_t)$ and $(w_t, l_t, h_t)$ represent the dimensions of the item after rotating it by 0\u00b0 and 90\u00b0. For the second part, the existing methods include the heightmap [3], the list of packed items [8], and the weighted 3D voxel grid [9]. We choose to leverage the proposed PG (Section III-B) to produce a sequence of EMSs satisfying placement constraints as the bin's configuration. The sequence is padded or clipped to a fixed length N with dummy EMSs, i.e. $S_{t,bin} = \\{E_i\\}_{i=1}^N$.\nAction: Given the packing state $S_t = (S_{t,item}, S_{t,bin})$, the action $a_t$ involves selecting both an orientation and an EMS for the current item from the sequence of available EMSs. The size of the action space $A$ depends solely on the length of the sequence and the number of optional orientations, i.e., $||A|| = 2N$, irrespective of the bin dimensions. During training, we select the action $a_t$ according to the probability distribution over actions $\u03c0(\u00b7 | s_t)$, where \u00b7 represents the set of all possible placements in $s_t$. During testing, we select the action in a deterministic manner by choosing the placement with maximum probability in $\u03c0(\u00b7 | s_t)$. Note that the probability distribution applying the pairwise action mask between EMSs and orientations ensures that the policy samples valid actions unless no EMS satisfies the constraints.\nState-Transition: In our setting, the transition model is assumed to be deterministic, implying that a specific pair $(s_t, a_t)$ consistently leads to the same subsequent state $s_{t+1}$.\nReward: The target of the packing problem is to maximize the space ratio of the bin. Hence, we formulate the reward as the step-wise enhancement in space utilization, represented as $r_t = \\frac{l_t \u00b7 w_t \u00b7 h_t}{L \u00b7 W \u00b7 H}$. This dense reward encourages the DRL agent to perform more steps in an episode, thereby leading to more packed items and greater space utilization."}, {"title": "D. Network Architecture", "content": "The design of a neural network architecture for the DRL agent is important because the chosen architecture affects the agent's learning and generalization capabilities across varied environments. A simplistic network would be to concatenate the bin and item representations [2] or embeddings [7]. However, this method results in a model whose convolutional and linear layer sizes are contingent upon the dimensions of the bin, rendering the trained model impractical for application across different bins.\nTo overcome the challenge of generalization, we propose an attention-based network architecture that focuses on the correlation between the item and the bin's partial spaces. As illustrated in Fig. 2a, this architecture comprises three primary components: the Packing Transformer, the actor network, and the critic network. Our network takes the bin representation $S_{t,bin} \u2208 R^{N\u00d76}$ (i.e., a sequence of EMSs from PG) and the item representation $s_{t,item} \u2208 R^{2\u00d73}$ (i.e., item's dimensions) as inputs. These inputs are then individually processed by Multi-Layer Perceptrons (MLP), which are two-layer linear networks with LeakyReLU activation function. The embedding dimensions of both EMS and the item are set to 128. Subsequently, we then extract features from the embeddings using the designed Packing Transformer, inspired by cross-modality learning across language and vision [32]. The EMS and item features are then fed into the actor network to generate a probability distribution of potential actions, and fed into the critic network to estimate the expected cumulative reward based on the current state.\nPacking Transformer is depicted in detail in Fig. 2b. It is constructed by stacking multiple (three in practice) identical encoder blocks, each containing two self-attention layers, one bi-directional cross-attention layer, and four MLP blocks of two layers comprising {128, 128} neurons. The bi-directional cross-attention layer consists of two unidirectional cross-attention layers, one from EMS to item and the other from item to EMS. Residual connections and layer normalization (Norm) are applied after each layer. The self-attention layers play an important role in establishing the intrinsic connections between EMSs or item dimensions, while the bi-directional cross-attention layer facilitates the discovery of inner-relationships from one to another.\nActor and critic networks are both implemented with the MLP layers shown in Fig. 2a. In the actor network, both the EMS and item features are processed through an MLP, and the results are multiplied to compute a score map of actions. This is followed by an element-wise multiplication with the action mask to eliminate infeasible actions."}, {"title": "E. Training Method", "content": "We employ the Proximal Policy Optimization (PPO) algorithm [33] to train the proposed GOPT. PPO is a popular on-policy reinforcement learning algorithm that alternates between collecting data via interactions with the environment and optimizing the following objective, which is approxi-"}, {"title": "IV. EXPERIMENTS", "content": "Our method is implemented utilizing PyTorch [35] and adopts the PPO algorithm within the Tianshou framework [36] for policy training. The maximum number of EMS is set to 80 during each packing step. We train the policy for 1000 epochs and collect a total of 40,000 environment steps over 128 parallel environments in every epoch. Policy updates occur after every 640 environment steps (calculated as 5 \u00d7 128 steps), with a batch size of 128. The Adam optimizer, coupled with a linearly descending learning rate scheduler starting from 7 \u00d7 10-5 is utilized for optimization. In terms of PPO loss calculation, the coefficients for value and entropy loss $C_1$, $C_2$ are 0.5 and 0.001, respectively, and the clipped ratio $\u03f5$ is 0.3. The discount factor \u03b3 is set to 1 to consider future and immediate rewards equally important. For policy updates, we use GAE with $\u03bb_{GAE}$ = 0.96. Our policy training is conducted on a computer equipped with an NVIDIA GeForce RTX 3090 and an Intel Core i7-14700K CPU, reaching convergence from scratch in about six hours.\nFor experimental validation, we utilize the RS dataset [2] for training and evaluating our DRL agent. The bin dimensions L \u00d7 W \u00d7 H are set to 10 \u00d7 10 \u00d7 10, and the dimensions of items follow $\\frac{min(L,W,H)}{2} < l_t, w_t, h_t < min(L,W,H)$. The dataset comprises 125 types of heterogeneous items, and sequences are dynamically generated by bootstrap sampling during training to reflect the variability in practical scenarios. An additional set of 1000 instances is generated for evaluation purposes, and the average performance is recorded."}, {"title": "B. Performance Evaluation", "content": "1) Baselines: To illustrate the superiority of our method for the online 3D-BPP, we select representative methods with publicly available implementations as baselines. We categorize these methods into two groups. The first group consists of four heuristic methods: OnlineBPH [5], Best Fit based on EP [16] that packs item in the lowest extreme point, MACS [23], and HM [21]. The second comprises three DRL-based methods: Zhao et al. [2], PCT [8], and Xiong et"}, {"title": "C. Generalization", "content": "The capacity of learning-based methods to generalize across diverse datasets and unseen scenarios has consistently been a subject of scrutiny and interest. This section evaluates the generalization performance of our method across various bins of different dimensions and unseen items.\nGeneralization on different bins. In addition to the initial bin dimensions for the aforementioned training, we introduce three other environments where the bin dimensions are set to 30 \u00d7 30 \u00d7 30, 50 \u00d7 50 \u00d7 50, and 100 \u00d7 100 \u00d7 100, respectively, and the item dimensions in the dataset are scaled up correspondingly. These environments are named Bin-10, Bin-30, Bin-50, and Bin-100. The search space for actions increases as the dimensions of bins grow, resulting in a higher complexity for finding a solution. To assess our method's generalization ability regarding the bin dimensions, we directly transfer our policy, trained solely in Bin-10, to the other three environments without fine-tuning. We additionally train and test our proposed GOPT, along with several DRL-based baseline methods [2], [3], [8], [10], separately in different environments for greater persuasiveness. The results in terms of Uti and Num are summarized in Table II. It is noted that Zhao et al.'s method [2] fails to converge in Bin-100. According to Table II, GOPT not only maintains consistent performance across different environments but also consistently outperforms other methods. Significantly, the policy GOPT (Bin-10) without retraining shows stable performance in environments divergent from the training one. Other DRL-based methods do not possess such ability as they need to be retrained when encountering varying bin dimensions. Intriguingly, some of them achieve relatively high performance in Bin-30. We surmise that this is due to a balance between the increased number of model parameters and the moderate problem complexity for this size, allowing for enhanced fitting capacity without the excessive difficulty observed at larger bins.\nGeneralization on unseen items. Additionally, we con-"}, {"title": "D. Ablation Studies", "content": "Additional ablation studies are conducted to thoroughly analyze the impact of various components in our method. These components encompass the Placement Generator (PG), item representation (IR), and Packing Transformer (PT). We exclude PG and provide the neural network with all the placements and the corresponding masks to elucidate its effect. We also present results obtained without transforming the item representation from a three-dimensional vector to the proposed mode. Additionally, we conduct experiments by removing PT (GOPT w/o PT) and replacing PT with MLP (GOPT-MLP) and GRU (GOPT-GRU) to gain insights into its significance. \nAs shown in Table I and Fig. 5, all three components introduced in this study exhibit favorable outcomes in line with our expectations. The comparative analyses indicate the"}, {"title": "E. Real World Experiment", "content": "We establish a physical robot packing testbed to verify the applicability of our method in the real world"}, {"title": "V. CONCLUSIONS", "content": "We contribute a novel framework called GOPT for online 3D bin packing. GOPT embraces the Placement Generator module to generate placement candidates and represent the state of a bin with these candidates. Meanwhile, the Packing Transformer identifies spatial correlations for packing, which employs a cross-attention mechanism to fuse information from items and the bin effectively. Extensive experiments prove GOPT's superiority over existing methods, demonstrating notable enhancements not only in packing performance but also in generalization capabilities. Specifically, trained GOPT policy can generalize both across varying bins and unseen items. Finally, we successfully apply the trained packing policy in a robotic system, demonstrating its practical applicability. In the future, we plan to extend our method's"}]}