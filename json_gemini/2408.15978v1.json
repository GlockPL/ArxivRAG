{"title": "WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task Execution with Strategic Exploration", "authors": ["Yao Zhang", "Zhen Han", "Zijian Ma", "Yu Wu", "Yunpu Ma", "Volker Tresp"], "abstract": "LLM-based autonomous agents often fail to execute complex web tasks that require dynamic interaction, largely due to the inherent uncertainty and complexity of these environments. Existing LLM-based web agents typically rely on rigid, expert-designed policies specific to certain states and actions, lacking the flexibility and generalizability needed to adapt to unseen tasks. In contrast, humans excel by exploring unknowns, continuously adapting strategies based on new observations, and resolving ambiguities through exploration. To emulate human-like adaptability, web agents need strategic exploration and complex decision-making. Monte Carlo Tree Search (MCTS) is well-suited for this, but classical MCTS struggles with vast action spaces, unpredictable state transitions, and incomplete information in web tasks. In light of this, we develop WebPilot, a multi-agent system with a dual optimization strategy that improves MCTS to better handle complex web environments. Specifically, the Global Optimization phase involves generating a high-level plan by breaking down tasks into manageable subtasks, continuously refining this plan through reflective analysis of new observations and previous subtask attempts, thereby focusing the search process and mitigating challenges posed by vast action spaces in classical MCTS. Subsequently, the Local Optimization phase executes each subtask using a tailored MCTS designed for complex environments, effectively addressing uncertainties and managing incomplete information by iteratively refining decisions based on new observations. Experimental results on WebArena and MiniWoB++ demonstrate the effectiveness of WebPilot. Notably, on WebArena, WebPilot achieves SOTA performance with GPT-4, achieving a 93% relative increase in success rate over the concurrent tree search-based method. WebPilot marks a significant advancement in general autonomous agent capabilities, paving the way for more advanced and reliable decision-making in practical environments. Our code is publicly released at github.com/WebPilot.", "sections": [{"title": "1 Introduction", "content": "The advanced reasoning capabilities of Large Language Models (LLMs) (Yang et al. 2023; Achiam et al. 2023; Team et al. 2023; Anthropic 2024) have significantly expanded the potential for developing autonomous web agents capable of navigating and interacting within complex, dynamic environments (Lai et al. 2024; Deng et al. 2024). To fully harness this potential, these agents must excel in tasks such as complex information retrieval, long-horizon task execution, and the integration of diverse information sources (Wang et al. 2024; Zhou et al. 2023b).\nHowever, despite the advanced reasoning capabilities of LLMs, current LLM-based web agents (Sodhi et al. 2024; Ma et al. 2023) often fall short in executing complex web tasks that require dynamic interaction. This limitation arises primarily from their heavy reliance on rigid, expert-designed policies tailored to specific states and actions. While these policies are meticulously crafted to address well-defined scenarios, they inherently lack the flexibility and generalizability needed to adapt to the uncertain and variable nature of real-world web environments, as well as to unseen tasks.\nIn contrast, humans excel at handling complex web tasks due to their cognitive flexibility (Daw, Niv, and Dayan 2005), which allows them to explore unknowns, adjust plans dynamically based on new observations, and resolve ambiguities through trial and error. This adaptability enables humans to navigate uncertain environments, make decisions with incomplete information, and modify strategies in real time. Monte Carlo Tree Search (MCTS) mirrors this cognitive process, making it particularly effective in emulating human web navigation strategies. MCTS facilitates the exploration of unknowns by expanding nodes during the tree search, helping web agents discover effective actions. It adjusts tactical-level strategies during the search process, refining action generation at each node based on the feedback from the expansion, much like how humans iteratively adjust their actions in response to new observations. When encountering dead ends or unclear paths-nodes with low potential or uncertain outcomes-MCTS reassesses and explores alternative branches, effectively addressing the limitations of LLMs in handling unfamiliar web environments.\nDespite its potential, classical MCTS (Browne et al. 2012) struggles in complex web environments due to vast action spaces, unpredictable state transitions, and incomplete information. While recent methods like LLM-MCTS (Zhao, Lee, and Hsu 2024) and LATS (Zhou et al. 2023a) integrate LLMs for heuristic guidance, they are limited to tasks with smaller action spaces and lower complexity, reducing their effectiveness in real-world scenarios. Similarly, RAP (Hao et al. 2023a) optimizes inference paths but lacks the flexibility for dynamic interaction in complex environments. Reward mechanism within MCTS also remains challenging for complex environments; current approaches either rely on direct environment reward (Zhou et al. 2023a), which is impractical for real-world tasks, or use overly simplistic scoring systems (e.g., binary or low-resolution scales (Koh et al. 2024)), failing to accurately capture the nuanced and evolving nature of web environments. These limitations highlight the need for more adaptable and robust MCTS-based methods capable of effectively navigating complex web tasks.\nIn response to these challenges, we introduce WebPilot, a versatile multi-agent system designed with a dual optimization strategy grounded in the principles of MCTS, specifically tailored for enhanced adaptability in complex environments. WebPilot first applies Global Optimization, decomposing tasks and refining high-level plans through reflective analysis. This enables the system to dynamically adapt to evolving objectives while effectively managing the complexities of vast action spaces. Following this, WebPilot employs Local Optimization to execute each subtask using a customized MCTS approach, addressing uncertainties and incomplete information by iteratively refining decisions based on new observations.\nSpecifically, the Global Optimization phase is driven by Planner, Controller, and Extractor. It begins with Hierarchical Task Decomposition (HTD), where Planner breaks down complex tasks into manageable subtasks, narrowing the focus and effectively addressing the vast action spaces that challenge classical MCTS. Reflective Task Adjustment (RTA) then refines the high-level plan based on new observations, allowing WebPilot to adapt dynamically. Controller monitors subtask progression, assessing subtask completeness and generating reflections if the subtask requires re-execution, ensuring accurate and adaptive task completion. Throughout this process, Extractor gathers essential information to support task execution. This coordinated approach ensures WebPilot remains adaptable and efficient in dynamic environments.\nFor each subtask, WebPilot employs Local Optimization strategies driven by the Explorer, Verifier, Appraiser, and Controller to enhance execution in dynamic environments. Goal-Oriented Selection (GOS) harnesses the intuitions of LLMs to efficiently steer WebPilot toward the most promising states for subtask completion. Reflection-Enhanced Node Expansion (RENE) uses real-time feedback to continuously refine tactical-level strategies as conditions evolve. Dynamic Evaluation and Simulation (DES) continuously assesses actions and anticipates potential outcomes by integrating real-time feedback with one-step forward simulations. Maximal Value Backpropagation (MVB) prioritizes promising paths by focusing on strategies with the highest potential, updating values based on maximum future rewards. By integrating Local Optimization with Global optimization strategies, WebPilot ensures adaptable task execution, harnessing the specialized abilities and responsibilities of multiple agents to outperform existing web agents in dynamic environments.\nExperiments on MiniWoB++ (Liu et al. 2018) and WebArena (Zhou et al. 2023b) are chosen to assess the performance of WebPilot in environments with varying complexity. The results highlight the superiority of WebPilot, particularly in the complex, realistic web environment. In WebArena, WebPilot achieves an impressive 37.2% success rate, surpassing the current SOTA method, SteP (Sodhi et al. 2024), which relies on rigid, expert-designed policies tailored to specific states and actions. Notably, WebPilot demonstrates a remarkable 93% relative increase in success rate over concurrent tree-based methods (Koh et al. 2024). Even when using GPT-3.5, WebPilot remains highly competitive with GPT-4-based SOTA methods, achieving a 29.1% success rate. These results underscore the exceptional ability of WebPilot to handle the uncertainty and complexity inherent in real-world web environments.\nThe primary contributions of this work are as follows:\n1. We introduce WebPilot, an autonomous multi-agent system designed for complex web environments, combining global and local MCTS-inspired optimization strategies to enable human-like flexibility in exploration, adaptation, and decision-making at both the subtask and action levels.\n2. We develop a Hierarchical Reflection Mechanism, incorporating Strategic Reflection in Global Optimization and Tactical Reflection in Local Optimization, which significantly enhances adaptive learning and decision-making in evolving environments.\n3. We introduce a novel Granular Bifaceted Self-Reward Mechanism that guides MCTS by integrating action effectiveness with goal-oriented potential, allowing for more precise assessments in dynamic and ambiguous environments.\n4. WebPilot achieves SOTA performance, particularly in challenging benchmarks like WebArena, demonstrating substantial advancements in general autonomous agent capabilities for complex real-world tasks."}, {"title": "2 Related Work", "content": "In this section, we provide a comparative analysis of LLM-based web agents and MCTS-based agents, with a detailed comparison presented in Tab. 1. For a more in-depth analysis of these agents, please refer to the Appendix."}, {"title": "2.1 LLM-Based Autonomous Web Agents", "content": "Recent advancements in LLMs have paved the way for the development of web agents that leverage the reasoning abilities of LLMs to interact with web environments. One line of LLM-based web agent (Kim, Baldi, and McAleer 2024; Sun et al. 2024; Prasad et al. 2023; Fu et al. 2024; Ma et al. 2023; Zheng et al. 2023; Tao et al. 2023) predominantly relies on environment-specific state-action pairs embedded within demonstrations to respond to specific observations. For instance, SteP (Sodhi et al. 2024), currently the SOTA on WebArena(Zhou et al. 2023b), utilizes rigid, expert-designed policies tailored to particular states and actions. However, these agents often struggle with exploring and adapting to realistic, unseen web tasks. Another line of web agents (Li et al. 2023; Zhou et al. 2023a; Pan et al. 2024; Koh et al. 2024) adopts a strategy of freely exploring and discovering unknown environments. Despite efforts like Auto Eval & Refine (Pan et al. 2024), which incorporates an evaluator into Reflexion (Shinn et al. 2024), and LM-Tree Search (Koh et al. 2024), which employs a search-based method in realistic environments, these agents still encounter challenges with complex tasks, leaving room for significant advancements. WebPilot, in contrast, is a multi-agent system employing a dual optimization strategy, excelling in exploring unseen tasks and dynamically adjusting strategies and actions based on new observations. This capability enables WebPilot to demonstrate superior adaptability in more complex environments."}, {"title": "2.2 LLM-MCTS Applications", "content": "MCTS, originally developed for the game of Go (Coulom 2006; Browne et al. 2012), is renowned for its effectiveness in handling exploration problems. Enhanced by the Upper Confidence bounds applied to Trees (UCT) method (Kocsis and Szepesv\u00e1ri 2006), MCTS has found extensive use in fields such as robotics(Zhao, Lee, and Hsu 2024), strategy games (Jang et al. 2021), and autonomous vehicles (Lenz, Kessler, and Knoll 2016). Recently, researchers have integrated LLMs with MCTS to tackle various NLP tasks, including QA (Hong et al. 2023; Xie et al. 2024; Chi, Yang, and Klein 2024), prompt refinement techniques (Wang et al. 2023), and complex mathematical reasoning problems (Tian et al. 2024; Zhang et al. 2024). Building on this integration, LLM-based agents have also incorporated MCTS to enhance their exploratory and decision-making capabilities. For instance, LATS (Zhou et al. 2023a) applies MCTS to simple web tasks. However, traditional MCTS-based methods often encounter difficulties in scenarios with vast action spaces, unpredictable state transitions, and incomplete information in web tasks. WebPilot addresses these challenges by utilizing a tailored MCTS designed specifically for complex environments, effectively navigating and optimizing decision-making processes even in highly uncertain situations."}, {"title": "3 Methodology", "content": "In \u00a73.1, we formally describe the web exploration task, highlighting the challenges posed by the uncertain nature and dynamic of these environments. To overcome these challenges, WebPilot employs a dual optimization strategy, i.e., Global Optimization and Local Optimization. During the Global Optimization phase, as detailed in \u00a73.2, WebPilot generates high-level plans and continuously refines these plans through reflective analysis. This is followed by the Local Optimization phase, described in \u00a73.3, where WebPilot engages in low-level strategic exploration. The entire process is outlined in Algo. 1, with additional details provided in the Appendix."}, {"title": "3.1 Promblem Formulation", "content": "Our objective is to enable the LLM-based web agent to effectively solve a task T in a web environment E by emulating human web navigation strategies. Web environments are inherently partially observable, which limits the information available to agents and complicates problem-solving. This partial observability occurs because web content can change dynamically, meaning the agent cannot fully anticipate or know the state of certain elements-like updated content or availability-until it interacts with them. Consequently, agents must often make decisions under conditions of uncertainty and incomplete information. Following WebArena (Zhou et al. 2023b), we use an accessibility tree, referred to as actree, to represent the observation, which captures the structure and interactive elements of the web page. However, due to the lack of specific web domain knowledge, LLMs often struggle to recognize or utilize the functionalities of various web elements. As a result, the agent must actively explore the environment to gather critical information about both the task and the functionality of the web elements, making informed decisions despite these uncertainties and incomplete information.\nSpecifically, this process can be modeled as a Partially Observable Markov Decision Process (POMDP). The environment E is defined by a state space S, an action space A, and an observation space O. The transition function F: S \u00d7 A \u2192 S dictates how states evolve based on actions taken, typically in a deterministic manner governed by the environment. Task execution requires the agent to make decisions based on partial observations of at each time step t. Each action \\(a_t\\) results in a new state \\(s_{t+1}\\) and an updated observation \\(O_{t+1}\\). The evaluation function eval(a, s), defined by the environment, assesses the success of task execution. Here, a = {\\(a_1\\),...,\\(a_n\\)} represents the sequence of executed actions, and s = {\\(s_1\\),..., \\(s_n\\)} denotes the corresponding sequence of intermediate states. This function evaluates whether the state transitions satisfy the criteria established by the task T."}, {"title": "3.2 Global Optimization: Adaptive Strategy Refinement through Reflective Adjustment", "content": "The Global Optimization phase emulates human cognition by leveraging prior knowledge to generate an initial plan for unfamiliar tasks. However, due to the lack of specific web domain knowledge in LLMs and the dynamic, uncertain nature of web environments, this initial plan misses critical details and struggles to remain effective as the environment evolves. To address this, WebPilot continuously refines the initial plan through reflective analysis of new observations and previous subtask outcomes. Global Optimization involves two key components: Hierarchical Task Decomposition (HTD) and Reflective Task Adjustment (RTA), which are facilitated by the Planner, Controller, and Extractor.\nHierarchical Task Decomposition (HTD) begins with Planner breaking down complex tasks into smaller, manageable subtasks \\(T_i\\), thereby creating a flexible high-level plan that can adapt to the uncertain and ever-changing conditions of web environments. In generating this plan, Planner utilizes only a few high-level demonstrations to ensure robust and adaptive task decomposition; an example of this is provided in the Appendix. This approach allows WebPilot to dynamically adjust its strategies for specific aspects of each task, making it more responsive to environmental changes. Unlike the concurrent search-based web agent (Koh et al. 2024), which struggles with complex tasks due to its expanding search space, HTD ensures that each subtask is more targeted and efficient. This decomposition enables WebPilot to refine each subtask in real time, adjusting dynamically to evolving conditions without requiring a complete overhaul of the entire task execution. By concentrating on these manageable subtasks, WebPilot employs MCTS-enhanced decision strategies, specifically through the Local Optimization phase as described in \u00a73.3, to minimize unnecessary search paths and optimize decision-making within a focused scope, effectively mitigating the challenges posed by vast action spaces that often hinder classical MCTS. The effectiveness of Planner is demonstrated through ablation studies, detailed in \u00a74.4.\nReflective Task Adjustment (RTA) Upon completing each subtask in the Local Optimization phase, as will be discussed in \u00a73.3, WebPilot reassesses and refines its high-level plan to ensure alignment with the overall task T. Guided by Controller and Planner, this process critically evaluates the execution of each subtask against expected outcomes, allowing WebPilot to recalibrate its strategy based on new observations. Controller plays a crucial role in this process by assessing whether the current observation \\(o_t\\) and the executed action sequences a align with the subtask \\(T_i\\). It then generates a subtask completeness \\(Comp_t\\). If the completeness assessment indicates that the subtask is not complete, Controller initiates a re-execution of the subtask. Before this re-execution, the \\(Comp_t\\), along with the associated observation and executed actions, is used to generate a Strategic Reflection, i.e., subtask reflection \\(R_{sub}\\). This reflection guides the repeated execution of the subtask, leveraging the experience from the previous attempt to avoid repeating the same errors. Meanwhile, Extractor continuously gathers critical information to support the successful completion of the task. An example of Controller handling task completeness and subtask reflection is provided in the Appendix."}, {"title": "3.3 Local Optimization: MCTS-Enhanced Decision Strategies", "content": "The Local Optimization phase in WebPilot is inspired by the human-like adaptability required to navigate and solve complex web tasks, effectively captured by MCTS. For each subtask \\(T_i\\) and its subtask-specific goals Objective, which define the expected outcomes or milestones to be achieved within that subtask, Explorer, Verifier, and Appraiser work together to complete the task. Explorer identifies optimal actions, Verifier ensures these actions are valid and non-redundant, and Appraiser evaluates both the immediate effectiveness of an action and its potential to achieve the intended goal, offering continuous feedback for a more nuanced and accurate assessment. Throughout this process, Controller assesses whether the subtask is completed and determines if further actions are needed, ensuring alignment with the overall task.\nThe Local Optimization phase of WebPilot, akin to classical MCTS, follows four key stages, as shown in Fig. 2: 1) Goal-Oriented Selection leverages the initial intuitions of LLMs to steer WebPilot toward the most promising paths for subtask completion, emulating how humans use prior knowledge to navigate tasks. 2) Reflection-Enhanced Node Expansion integrates feedback after each node expansion, enabling WebPilot to reassess and refine its strategy dynamically, much like human reflection informs decision-making. 3) Dynamic Evaluation and Simulation allows WebPilot to assess current states by analyzing executed actions and simulating potential outcomes, mirroring human foresight in evaluating consequences. 4) Maximal Value Backpropagation prioritizes long-term potential by continuously updating value estimates based on the maximum future rewards. Through this comprehensive Local Optimization phase, WebPilot effectively balances exploration and exploitation, leading to more efficient and informed decision-making in complex tasks.\nGoal-Oriented Selection (GOS) directs Explorer toward high-value nodes by leveraging the initial intuitions derived from the LLM. Although these intuitions are not specifically tailored to the environment, they provide valuable insights that effectively narrow the action space. These insights arise from the extensive pre-existing knowledge base of LLM, enabling it to make informed estimates about which actions are likely to be more promising, even without explicit training in the specific web domain. GOS employs a modified version of the PUCT selection method, inspired by AlphaGo (Silver et al. 2017):\n\\begin{equation}\nU(s, a) = W_{puct} \\sqrt{\\frac{\\sum_b N(s,b)}{1+N(s,a)}},\n\\end{equation}\nwhere \\(W_{puct}\\) represents the exploration bias factor balancing the exploration and exploitation, and N(s, a) represents the total count of conducting the action a in state s.\nUnlike classical MCTS, as deployed in RAP (Hao et al. 2023a), which prioritizes unexplored nodes due to the infinite potential assigned by traditional UCT (Kocsis and Szepesv\u00e1ri 2006), GOS refines this approach to better manage vast action spaces. The initial insights of LLM often assign high value to the first visited node based on its broad knowledge base. In contrast to UCT, which would mandate exploring unvisited nodes even when the current node is nearly optimal, GOS modifies the selection formula by adding a +1 term in the denominator. This adjustment enables GOS to direct the Explorer toward the most promising paths, minimizing unnecessary exploration and efficiently navigating complex environments, similar to how humans use prior knowledge in decision-making.\nReflection-Enhanced Node Expansion (RENE) navigates the vast action spaces of the web by integrating reflective feedback, i.e., Strategic Reflection and Tactical Reflection, at each step, enabling WebPilot to continuously refine its strategy, enhancing decision-making through focused exploration and exploitation.\nSpecifically, to explore the state space efficiently, WebPilot departs from traditional MCTS by generating and expanding only one action per step. Explorer generates actions \\(a_t\\) and corresponding intents \\(I_t\\) in real time, continuously adjusting to evolving conditions with guidance from reflective feedback. Verifier ensures that the action \\(a_t\\) is both valid and unique among sibling nodes. Formally,\n\\begin{equation}\na_t, I_t = Explorer(o_t, T_i, H_t, R_t, C_{t-1}),\n\\end{equation}\nwhere \\(T_0\\) the subtask, and \\(H_t = {a_1,...,a_{t-1}}\\) the action history. If available, the Tactical Reflections-comprising simulation reflection \\(R_{simt}\\), parent reflection \\(R_{pt}\\), and sibling reflection \\(R_{st}\\)-along with the Strategic Reflection, which is the subtask reflection \\(R_{sub}\\), are incorporated into \\(R_t = {R_{simt}, R_{pt}, R_{st}, R_{sub}}\\) and used together with the continuation reason \\(C_{t-1}\\) to leverage prior exploration. This combined use of available reflections helps narrow the action space and optimize the MCTS process.\nUpon executing \\(a_t\\), the environment returns the resulting state and observation \\((s_{t+1}, O_{t+1})\\). Explorer then compares the current observation \\(O_{t+1}\\) with the previous one \\(o_t\\) to determine whether the action intent \\(I_t\\) has been achieved:\n\\begin{equation}\nEffect(a_t) = Explorer(O_{t+1}, o_t, I_t)\n\\end{equation}\nFollowing this evaluation, Child Reflection \\(R_{ct}\\) and Sibling Reflection \\(R_{st}\\), are generated:\n\\begin{equation}\nR_{ct}, R_{st} = Explorer(Effect(a_t), T_i, Objective_i, o_t, H_t),\n\\end{equation}\nwhere Effect(\\(a_t\\)) captures the impact of \\(a_t\\) on the current state, and \\(Objective_i\\) denotes the subtask-specific goals, which define the expected outcomes or milestones to be achieved within that subtask \\(T_i\\). Detailed descriptions of how Effect(\\(a_t\\)) is determined are provided in the Appendix. Child Reflection guides the generation of the next action, with \\(R_c\\) becoming the parent reflection \\(R_{pt+1}\\) for the child node, ensuring continuity in the decision-making process. This continuity is crucial for complex tasks, where maintaining a coherent reasoning path is essential. Disruptions in this flow can significantly impair performance, as demonstrated in \u00a74.4. Sibling Reflection \\(R_{st}\\) enhances exploration by leveraging insights from previous sibling node explorations, enabling the agent to focus on promising areas and uncover new possibilities when encountering similar scenarios. Together, Child Reflection and Sibling Reflection help WebPilot narrow the vast action space to a more manageable subset when generating actions. By integrating feedback from reflections on already executed actions, WebPilot effectively analyzes transitions and assesses decisions made under incomplete information, thereby enhancing its overall efficiency and performance in dynamic environments. The roles and impacts of these reflection mechanisms are further detailed in \u00a74.4, with illustrative examples of these reflections provided in the Appendix. For a comprehensive view of the Tactical Reflection, see Fig. 3.\nDynamic Evaluation and Simulation (DES) dynamically assesses how generated actions align with evolving task states by leveraging real-time feedback rather than relying on predefined reward structures typical of classical MCTS. This adaptive evaluation ensures that each action remains responsive to the changing environment, guiding the agent toward the evolving goal.\nThe reward function is crucial in MCTS, but prior methods often rely on direct feedback from the environment (Zhou et al. 2023a), which is impractical for realistic web tasks, or use binary success/failure outcomes or simplistic intermediate states (Koh et al. 2024). Such approaches frequently misjudge the ambiguous and evolving nature of web environments, leading to inaccurate evaluations. Moreover, intermediate steps on the web are challenging to categorize as simply correct or incorrect because their effectiveness in contributing to the final task outcome may not be immediately apparent. Inspired by the A* algorithm (Hart, Nilsson, and Raphael 1968), Appraiser evaluates both the effectiveness of the executed action \\(a_t\\) and the potential of the resulting observation \\(O_{t+1}\\) to achieve the intended goal, providing a more nuanced and dynamic assessment. This approach is refined using a granular 0-10 scoring system, which allows for a more precise evaluation of action impact, capturing the subtleties of evolving and uncertain web environments.\nThis Granular Bifaceted Self-Reward Mechanism evaluates both the effectiveness of the action taken, \\(S_{eff}(a_t)\\), and the potential of the resulting observation, \\(S_{fut}(O_{t+1})\\), using a precise 0-10 scoring system. This approach allows for a more nuanced assessment, capturing subtle differences in action quality and future potential, which is crucial for determining whether to proceed with the current strategy. Illustrative examples of this novel mechanism are provided in the Appendix.\n\\begin{equation}\nS_{eff}(a_t), S_{fut} (O_{t+1}) = Appraiser(Effect(a_t), O_{t+1}, T_i),\n\\end{equation}\nwhere Apprasier assesses how well the state aligns with the subtask \\(T_i\\), and Effect(\\(a_t\\)) represents the change between the former and current state.\nThe overall reward \\(S_{total}\\) aggregates these scores to represent the value of the current state:\n\\begin{equation}\nS_{total}(a_t, O_{t+1}) = w_{eff} \\cdot S_{eff}(a_t) +w_{fut} \\cdot S_{fut}(O_{t+1}),\n\\end{equation}\nwhere \\(w_{eff}\\) and \\(w_{fut}\\) balance action effectiveness and future potential."}, {"title": "4 Experiment", "content": "To demonstrate the broad applicability of WebPilot, we evaluate our method on two benchmarks: WebArena (Zhou et al. 2023b) and MiniWoB++ (Liu et al. 2018). WebArena, comprising 812 human-annotated web tasks, is designed to assess the ability of agents to perform actions on complex, realistic websites. These tasks are diverse, long-horizon, and closely mirror the types of activities humans routinely engage in online. Notably, WebPilot operates as a text-only agent, leveraging the accessibility tree, i.e., actree, of the webpage without relying on visual observations\u2014a limitation we aim to address in future work. WebArena is chosen as the primary benchmark due to its close simulation of real-world web environments. Additionally, we validate WebPilot on MiniWoB++, an environment offering a range of simpler yet varied web tasks, from basic interactions like button-clicking to more complex activities such as form-filling, which require reasoning capabilities. For evaluation, we use the Success Rate (SR) metric as defined in (Zhou et al. 2023b) for WebArena, and follow (Li et al. 2023) for MiniWoB++, focusing on 43 tasks that can be completed solely through text representation.\nBaseline Models We compare WebPilot against several baseline models, including the current SOTA SteP (Sodhi et al. 2024) and the concurrent search-based model LM-Tree Search (LM-TS) (Koh et al. 2024). We utilize GPT-3.5 and GPT-40 \u00b9, each configured with a max_tokens limit of 4096 tokens, and a temperature setting of 0.3, while all other parameters are kept at their default values.\nImplementation Details We run WebPilot with parameters optimized for both efficiency and performance. Specifically, we limit the max node count per subtask to 10 and set the exploration bias at 5 to balance exploration and exploitation. Refer to the Appendix for details."}, {"title": "4.2 Results on WebArena", "content": "As shown in Tab. 2, WebPilot consistently outperforms existing methods. In WebArena, WebPilot with GPT-40 demonstrates a remarkable 93% relative increase in SR compared to concurrent tree search-based methods LM-TS, achieving a 37.2% SR. This performance surpasses the current SOTA method, SteP (Sodhi et al. 2024), which relies on rigid, expert-designed policies tailored to specific states and actions. This substantial improvement highlights the effectiveness of the adaptable and dynamic approach of WebPilot in navigating complex web environments."}, {"title": "Greater Flexibility and Adaptability in WebPilot", "content": "Compared with SteP, WebPilot demonstrates a significant 7.7% improvement in the Gitlab domain. This advantage arises from the strategic use of high-level demonstrations, which equip the agent with general web domain knowledge rather than confining it to rigid, expert-designed policies specific to certain states and actions, as SteP does. The Gitlab domain, characterized by its diverse and complex tasks, as well as dynamic, multi-step scenarios, highlights the ability of WebPilot to generalize knowledge and adapt strategies in real time. This broader understanding allows WebPilot to more effectively infer and address unseen tasks. Additionally, the exploratory approach of WebPilot, which mirrors human adaptability, facilitates dynamic navigation and problem-solving in unfamiliar scenarios, further enhancing its overall performance in uncertain web environments."}, {"title": "Superior Task Performance in WebPilot Through Strategic Decomposition and Reflective Feedback", "content": "WebPilot significantly outperforms the search-based method, LM-TS, particularly in the Reddit and GitLab domains, due to two key factors. First, the lack of strategic decomposition by the Planner in LM-TS makes navigating the vast state space considerably more challenging, as the agent lacks clear guidance, leading to less efficient exploration and poorer performance. The importance of this decomposition is highlighted by the superior results achieved by WebPilot, as demonstrated by the ablation studies in \u00a74.4. Second, WebPilot employs hierarchical reflections after each node expansion, enabling continuous reassessment and refinement of its strategy. Further details can be found in \u00a74.4."}, {"title": "Enhanced Reasoning and Planning Capabilities Remain Crucial for Improvement", "content": "Even with the less powerful GPT-3.5, WebPilot demonstrates a significant improvement over the WebArena baseline, highlighting its effectiveness in leveraging MCTS-inspired strategies to navigate complex environments. However, transitioning from GPT-3.5 to GPT-40 yields substantial gains, particularly in the Shopping, Reddit, and GitLab, with SR increases of 11.8%, 6.6%, and 9.4%, respectively. These improvements are largely driven by the enhanced reasoning and planning capabilities of GPT-40, which are crucial for tasks requiring precise inference and information retrieval in Shopping, as well as for navigating the more complex and diverse environments of GitLab and Reddit. The advanced planning abilities and the capacity to generalize domain knowledge from limited demonstrations to unseen tasks enable WebPilot to generate more effective plans, better understand the environment, and execute accurate actions. This underscores the importance of addressing the core challenges that current LLMs face in reasoning and planning, suggesting that further enhancements can be achieved with more powerful LLMs."}, {"title": "4.3 Results on MiniWoB++", "content": "As shown in Tab. 3, WebPilot achieves results competitive with the SOTA, SteP. The slight edge of SteP is due to its use of 10 action-level demonstrations, while WebPilot uses only 4 high-level demonstrations, leaving exploration to the agent. The simplicity of many MiniWoB++ tasks, which require minimal actions, also reduced the need for extensive exploration, limiting the advantage of WebPilot. Despite this, our method proved effective with far fewer demonstrations compared to other LLM-based agents. Detailed analysis of MiniWoB++ can be found in the Appendix."}, {"title": "4.4 Ablation Studies", "content": "To evaluate the impact of each component in WebPilot, we categorized tasks into information-seeking (IS) and website interaction (WI), including site navigation and content configuration. IS tasks focus on extracting information, while WI tasks require executing complex action sequences. For the ablation experiments, we selected 50 IS tasks and 50 WI tasks where WebPilot successfully completed the objectives. This selection allowed us to focus on cases where the components are functioning optimally, providing a clearer understanding of how each component contributes to overall performance. The results, shown in Tab. 4, highlight the critical role each component plays in the effectiveness of WebPilot. Notably, WI tasks are more adversely affected than IS tasks, underscoring the importance of our design in equipping the agent to handle complex web exploration tasks. Relevant examples illustrating the following findings are provided in the Appendix."}, {"title": "Importance of Child Reflection for Maintaining Coherent Thought Processes", "content": "The Child Reflection mechanism is crucial for maintaining a coherent thought process during exploration, aligning with the principles of Chain-of-Thought reasoning (Wei et al. 2022). This reflection ensures that when generating actions for child nodes, the model has access to the parent node behind previous actions and its intended next steps. This continuity enables the model to produce more accurate and contextually relevant actions, preserving the logical flow necessary for complex decision-making. Without Child Reflection, this coherence is disrupted, leading to a significant 30% decline in performance, particularly in WI tasks, where maintaining a consistent thought process is essential for success."}, {"title": "Critical Role of Sibling Reflection in Effective and Diverse Exploration", "content": "Sibling Reflection is key to optimizing exploration within MCTS and expanding into diverse, promising areas, particularly for complex tasks. By leveraging insights from previously explored sibling nodes, WebPilot reduces redundancy and focuses on high-potential paths, ensuring valuable solutions are not missed. This mechanism enhances exploration effectiveness, especially for complicated web interaction tasks, as evidenced by WI tasks being more affected, as shown in Tab. 4."}, {"title": "Controller is Critical for Subtask Accuracy and High-Level Decision-Making", "content": "Controller is crucial for Global Optimization, enabling the reassessment and refinement of plans. After each subtask, Controller evaluates its completeness and, in collaboration with Planner, refines the overall plan. Without Controller, the verification of subtask completion is significantly compromised, leading to a noticeable performance decline, particularly in WI tasks. The greater impact on these tasks, which"}]}