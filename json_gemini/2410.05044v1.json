{"title": "PhotoReg: Photometrically Registering 3D Gaussian Splatting Models", "authors": ["Ziwen Yuan", "Tianyi Zhang", "Matthew Johnson-Roberson", "Weiming Zhi"], "abstract": "Building accurate representations of the environment is critical for intelligent robots to make decisions during deployment. Advances in photorealistic environment models have enabled robots to develop hyper-realistic reconstructions, which can be used to generate images that are intuitive for human inspection. In particular, the recently introduced 3D Gaussian Splatting (3DGS), which describes the scene with up to millions of primitive ellipsoids, can be rendered in real time. 3DGS has rapidly gained prominence. However, a critical unsolved problem persists: how can we fuse multiple 3DGS into a single coherent model? Solving this problem will enable robot teams to jointly build 3DGS models of their surroundings. A key insight of this work is to leverage the duality between photorealistic reconstructions, which render realistic 2D images from 3D structure, and 3D foundation models, which predict 3D structure from image pairs. To this end, we develop PhotoReg, a framework to register multiple photorealistic 3DGS models with 3D foundation models. As 3DGS models are generally built from monocular camera images, they have arbitrary scale. To resolve this, PhotoReg actively enforces scale consistency among the different 3DGS models by considering depth estimates within these models. Then, the alignment is iteratively refined with fine-grained photometric losses to produce high-quality fused 3DGS models. We rigorously evaluate PhotoReg on both standard benchmark datasets and our custom-collected datasets, including with two quadruped robots. The code is released at https://ziweny11.github.io/photoreg.", "sections": [{"title": "I. INTRODUCTION", "content": "Constructing representations is a central requirement to enable autonomous robot operations. Robots often carry extroceptive sensors, such as cameras, to observe the en- vironment, and representations are needed to condense this collected information [1], [2], [3], [4]. In this paper, we focus our attention on photorealistic reconstruction models. These models uniquely convert robot sensor inputs into models with which photorealistic images can be rendered. These photorealistic images can better enable non-roboticists to visualize the robot's environment. In particular, recent developments in the area have led to the development of 3DGS, which has, for the first time, enabled the real-time rendering of photorealistic images from the reconstruction. This breakthrough has significant implications for robotics, where the ability to visualize complex environments in real- time enhances robot navigation, manipulation, and interac- tion.\nThis paper studies the problem of combining multiple 3DGS models, built separately, into a single unified model. Solving this problem will allow a team of robots to explore and map large unknown spaces in a decentralized manner.\nA unified model, created by merging the individual repre- sentations of each robot, can be distributed to all robots. Relative to sharing all the image data with each robot in the team, distributing the condensed representation only enables efficient usage of bandwidth and latency. To tackle the challenge of 3DGS fusion, we present our Photometrical 3D Gaussian Registration framework (PhotoReg). Classical registration methods, such as iterative closest point (ICP) and its variants [5], [6], [7], focus primarily on aligning point clouds by minimizing the distance between corresponding points [8]. However, the continuous and complex geometric representations inherent in 3DGS differ significantly from the discrete point sets managed by traditional registration methods, necessitating innovative approaches like PhotoReg for effective alignment.\nPhotoReg utilizes 3D foundation models, trained on Internet-scale datasets, to derive initial 3D structures from 2D image pairs. These models provide rough estimations that facilitate the initial alignment of 3DGS models, espe- cially in scenarios where the overlap between the models is minimal. As the individual 3DGS models may not be of the same scale, PhotoReg actively aligns their scales by considering confidence-aware depth estimates in each model. Subsequently, PhotoReg optimizes fine-grained photometric losses, which measure the quality of rendered images from the model, to ensure tight alignment between the 3DGS. We provide extensive empirical evaluations of PhotoReg, both on classic benchmark datasets and on custom-collected data. This includes a custom dataset collected by two quadrupeds operating in a common area.\nConcretely, PhotoReg makes the following methodological innovations:"}, {"title": "II. RELATED WORK", "content": "Robots operating in un- known environments require internal representations to un- derstand their surroundings, to effectively generate motions [9], [10], [11]. Traditionally, this has been maps of occupancy [12], [13] or dynamics [14], [15], [16], [17]. Developments in neural networks have led to Neural Radiance Field (NeRF) [18] which learn photorealistic 3D scenes, where images from novel views can be rendered. Subsequent extensions sped up training [19]. However, rendering was often slow. Recent work on 3DGS [20] proposes to model the scene as a mixture of 3D Gaussians which enables real-time photorealistic rendering. Extensions to 3DGS [4], [21] have improved the robustness of the method.\nIn robot perception, registration refers to finding the transformation between two 3D structures. Registering two point clouds has been widely studied over time. ICP [22] alternatively finds the pairs of correspondence points and estimate the rigid body transformation between them, based on the closest-point assumption. Variants such as color ICP [23], Point to Plane ICP [24], and Robust ICP [25] have improved the method in terms of accuracy and efficiency. Methods that register two NeRFs have been explored. NeRF2NeRF [26] proposes to align two NeRFs by manually selecting key points. DReg-NeRF [27] further ad- vances 3D registration by automating the alignment of NeRF models using deep learning. An attempt has been made to explore 3DGS registration: LoopSplat [28] introduces a novel loop closure technique by registering 3D Gaussian splats. However, LoopSplat relies on RGB-D images for depth sensor readings, which limits its applicability when depth sensors are unavailable or unreliable. PhotoReg enables the registration of 3DGS in the absence of depth sensors.\nRobotics benefits from trans- former models trained on internet-scale data [29]. Particu- larly for robot perception, such visual foundation models include [30], [31], [32]. These models act as plug-and- play modules to facilitate a range of downstream tasks. For example, DINOv2 [33] was trained on internet-scale unlabeled data, employing self-supervision techniques that allow it to develop a deep understanding of visual content without the need for explicit annotations. DUSt3R [34] is a 3D foundation model used in PhotoReg. It is designed to generate 3D pointmaps from RGB images, enabling pose estimation, and has been applied to downstream robot manipulator perception [35], [36]. PhotoReg leverages the emergent capabilities of foundation models to perform robust alignment."}, {"title": "III. PRELIMINARIES: FOUNDATION MODELS", "content": "This work makes use of foundation models, which are large deep-learning models trained on internet-scale datasets. These models are intended as plug-and-play modules, used to facilitate a range of downstream tasks without retrain- ing on specific datasets. In this section, we briefly outline two foundation models used in our PhotoReg framework: DUSt3R and DINOv2. More details are available in the original papers, [34] and [33].\nAt the core of DUSt3R is a large vision trans- former [37]. It takes as input 2 RGB images of width W and height H, $I_1, I_2 \\in \\mathbb{R}^{W\\times H\\times 3}$ and outputs 2 corresponding 3D pointmaps $X_{1,1}, X_{2,1} \\in \\mathbb{R}^{W\\times H\\times 3}$ with associated con- fidence maps $C_1,C_2 \\in \\mathbb{R}^{W\\times H}$ and depth maps $D_1, D_2 \\in \\mathbb{R}^{W\\times H}$, from which it further recovers a variety of geometric quantities, such as relative camera poses and fully-consistent 3D reconstruction. In our proposed PhotoReg framework, we will make use of the above outputs. This workflow is illustrated in fig. 2. As DUSt3R is entirely data-driven, it does not need to identify hand-crafted features within our images to find correspondence. As a result, it is capable of accurately finding relative camera poses, even when visual overlap between the inputted image pair is minimal. Our PhotoReg framework capitalizes on this feature, enabling the alignment of GS models with minimal overlap.\nDINOv2 [33]: DINOv2 is a visual foundation model that employs a transformer model, trained in a self-supervised manner over extensive image datasets. It takes as input a single image and outputs a corresponding vector embed- ding. These embeddings are generally invariant to spatial transformation, with semantically similar objects being close in this embedding space. We use DINOv2 to search for"}, {"title": "IV. METHODOLOGY", "content": "The primary challenge addressed in this work involves the fusion of 3DGS models. Specifically, given input 3DGS models, $G_1$ and $G_2$, our proposed method aims to discover a transformation function $T$ that cohesively aligns $G_2$ to $G_1$ within the coordinate frame of $G_1$. As 3DGS are of arbitrary scale, $T$ needs to handle $G_1$ and $G_2$ that may be of vastly differing scales. Before delving into the details of our proposed method, PhotoReg, we first provide a math- ematical definition of 3DGS models and the corresponding transformation functions.\nA 3DGS model $G$ consists of a set of 3D Gaussians, where each Gaussian is defined by its 3D position, $\\mu$; a covariance matrix, $\\Sigma$, describing the spread and orientation of the Gaussian distribution in 3D space; opacity, $\\alpha$; and Spherical Harmonics (SH) coefficients $c$, containing color information:\n$G = {(\\mu, \\Sigma, \\alpha, c)}.$ (1)\nTransformation: The transformation of a 3DGS model involves applying scaling, rotation, and translation to each attribute of each Gaussian. Let $T_R$ denote the transformation function that maps a 3DGS model from some general coor- dinate frame A to coordinate frame B. This function takes as input $[G]_A$, the 3DGS model in coordinate frame A, and outputs $[G]_B$, the corresponding 3DGS model in frame B after the transformation, denoted as$[G]_B = T_R([G]_A)$.\nThe transformation function $T_R$ can be composed by scaling factor $s \\in \\mathbb{R}$, rotation $R \\in SO(3)$, and translation $t \\in \\mathbb{R}^3$, respectively. Suppose $[G]_A = {(\\mu_a, \\Sigma_a, \\alpha_A, c_A)}$ and $[G]_B = {(\\mu_B, \\Sigma_B, \\alpha_B, c_B)}$. The transformation mapping for each attribute is defined as follows:\n*   3D Position: $\\mu_B = sR\\mu_A + t$.\n*   Covariance Matrix: $\\Sigma_B = R\\Sigma_A$."}, {"title": "B. Notation and Transforming Gaussian Splats", "content": "*   Opacity: $\\alpha_B = \\alpha_A$ (Spatially invariant).\n*   Spherical Harmonics (SH) Coefficients: The SH co- efficients are translation-invariant and transformed by:\n$c_B = D(R, order). c_A$.\nwhere $D(R, order)$ is the Wigner D-matrix [38] corre- sponding to the rotation for a known specific order."}, {"title": "C. PhotoReg Overview", "content": "PhotoReg consists of four sequential stages: Foundational Image Matching: Select rendered image pairs of adjacent regions in each input 3DGS model as input into the 3D foundation model; Initial Estimation: Obtain an initial estimation of rotation and translation for alignment through the 3D foundation model; Scale Estimation: Resolve scale discrepancies between the 3DGS models through confidence- weighted depth maps; Optimization: Optimize scale, rota- tion, and translation simultaneously through photometric loss minimization. The workflow is illustrated in fig. 4 and further discussed in the following subsections."}, {"title": "D. Foundational Image Matching", "content": "Here, we detail the Foundational Image Matching (FIM) procedure, which utilizes visual foundation models, notably DINOv2, to initiate our registration process. The input to the FIM process consists of two 3DGS Models, denoted $G_1$ and $G_2$, each associated with a set of camera poses, $C_1$ and $C_2$. The primary aim of FIM is to identify and extract two high-quality rendered images at given camera poses from $G_1$ and $G_2$ which are similar at a semantic level, and invariant to viewing poses. That is, two images of the same object but at vastly differing angles will be identified as similar.\nTo select appropriate images as input into the 3D foundation model, we first render image sets $I_1$ and $I_2$ from $G_1$ and $G_2$ at diverse poses. We aim to select a suitable image pair $img_1 \\in I_1$ and $img_2 \\in I_2$ for coarse registration. We seek $V1 \\in V_1$ and $v2 \\in V_2$ such that\n$argmax_{v_1 \\in V_1,v_2\\in V_2} Cos(v_1, v_2)$ (2)\nThis approach identifies pairs of images, with one generated from $G_1$ and the other from $G_2$, which are semantically and visually similar."}, {"title": "E. Coarse Registration via 3D Foundation Models", "content": "We proceed to use the image pair ($img_1, img_2$) as input into a 3D foundation model, DUSt3R [34], to obtain an initial coarse registration, to approximately align $G_2$ into the coordinate frame of $G_1$. We input $img_1$ and $img_2$ into DUSt3R, which estimates a rigid transformation $T_{img_1}$, with rotation and translation. However, the scaling factor between $G_1$ and $G_2$ remains unknown: DUSt3R takes 2D images as inputs, making it impossible to recover the scaling directly.\nThe next step involves applying the transformations ob- tained from the 3D foundation model back to the orig- inal 3DGS models. This process involves a sequence of transformation steps, as illustrated in fig. 5. We define the coordinate frames as follows: For a coordinate frame $P$, $[G_i]_P$ denotes the 3DGS model $G_i$ under the coordinate frame $P$. Specifically, $P_{o i}$ is the original frame of the 3DGS"}, {"title": "F. Scale Estimation", "content": "In this step, we estimate the scale ratio $s_0^0$ by comparing depth maps at corresponding camera poses across different coordinate frames. A depth map can be represented as a two-dimensional matrix, where each element indicates the distance from the viewpoint (e.g., a camera) to a point in the scene along the line of sight.\nFrom a given camera pose, depth maps can be extracted from standard 3DGS models [39], and are also outputted by DUSt3R, during the initial coarse alignment. Pixel-wise confidence maps for the depth maps are also outputted. Here, we denote the depth maps for $img_1$ and $img_2$ obtained from their respective 3DGS models as $D_1, D_2 \\in \\mathbb{R}^{W\\times H}$, and the depth maps from DUSt3R with the same images as $D_1, D_2 \\in \\mathbb{R}^{W\\times H}$, along with pixel-wise confidence maps $C_1, C_2 \\in \\mathbb{R}^{W\\times H}$. Here, $W$ and $H$ denote the image width and height.\nA key insight is that the depths $D_1$ and $D_2$ are in the same coordinate system and of the same scale. We can then estimate a confidence-weighted scale between $G_1$ and $G_2$,\n$s_0^0 = \\frac{\\Sigma (C_1 \\odot (D_1/D_1))}{\\Sigma (C_2 \\odot (D_2/D_2))},$ (5)\nwhere $\\odot$ denotes the element-wise product. With the estimate of scale $s_0$, we have an initial transformation $T_{cl}$ which roughly aligns $G_1$ and $G_2$. We now shift our focus to refining the alignment."}, {"title": "G. Precise Refinement via Photometric Optimization", "content": "After roughly aligning $G_1$ and $G_2$, we further refine the alignment by rendering images at a novel pose, $C$, from both $G_1$ and $G_2$. Then, we minimize the photometric loss between the rendered images, and optimize with respect to our transformation parameters. We note that the differentia- bility of 3DGS models and rendering enables gradient-based optimization to be propagated back to the transformation parameters. The loss used computes the $L1$ distance, masked by binary indicators whether there is anything rendered at a given pixel, between the rendered images, at the same pose, but from $G_1$ and $G_2$. Specifically,\n$\\mathcal{L} = l1_{masked}(F(G_1, \\check{C}), F(G_2, \\check{C}), M_1 M_2)$, (6)\nwhere $F$ is the differentiable rendering function that gener- ates an image given camera pose and the 3DGS model. The masked $L1$ distance, $l1_{masked}$, gives the $L1$ distance masked by an element-wise binary mask. The element-wise binary mask used is $M_1 M_2$, where $M_1$ and $M_2$ are binary masks indicating whether anything has been rendered at each pixel.\nWe differentiate $\\mathcal{L}$ with respect to the parameters of $T_{cl}$, given as $(s_0, R_c, t)$. We use gradient-based optimizers to minimize the loss and perform the detailed alignment."}, {"title": "V. EXPERIMENTS", "content": "In this section, we evaluate the performance of the pro- posed PhotoReg method on merging two or more Gaussian Splatting models with different levels of overlap. The foun- dation models we used within the framework are DUSt3R and DINOv2. DUSt3R produces coarse 3D reconstruction from rendered images; DINOv2 performs feature extraction based on robust visual features from images. We seek to empirically answer several major questions:\n1) Can PhotoReg, with foundation models, produce ac- curate registration between two Gaussian Splatting models with partial overlap?\n2) How does its performance compare to other classic registration methods?\n3) How is the performance of PhotoReg on Gaussian Splatting Models with minimal overlap?\n4) Can PhotoReg handle registration of multiple Gaussian Splatting models?\n5) Can PhotoReg be used on image sequences collected on multiple real quadruped robots?"}, {"title": "A. Gaussian Splatting Registration with PhotoReg", "content": "We use the Playroom, Truck, and Train dataset used in [20], in conjunction with those we have gathered independently. Each dataset from [20] is divided into two subsets. Additionally, we also collect multiple real-world datasets within a room, which we name Workroom 1 and Workroom 2. In Workroom 1, there is very low overlap between the image sequences, while Workroom 2 contains a moderate level of overlap. Each dataset contains indepen- dently recorded image sequences. We also have a dataset Sofa collected in a different space, using cameras mounted"}, {"title": "B. Comparisons with Baseline Methods", "content": "The core at Gaussian Splatting Registration is obtaining the transformation function between target 3DGS models. Here, we outline competitive baselines that are used to stress-test the performance of PhotoReg: Iterative Closest Point (ICP) Traditionally, ICP is applied for 3D point cloud alignment by iteratively minimizing the distance between corresponding points until convergence. In the case of 3DGS models, we treat each Gaussian's mean as a point in the cloud. The initial estimation for ICP is provided through DUSt3R, the foundation model. COLMAP [44] is the most widely used Structure-from-Motion method. We can obtain the transformation matrix from camera poses estimated by COLMAP to align each 3DGS model.\nWe compare our method with ICP with scaling enabled, as implemented in Open3D [8], COLMAP, and Ground Truth. Ground Truth is obtained by building a 3DGS model on all the image sequences used. This represents an upper- bound to the quality of the aligned and merged model. The resultant GS Model from each method is evaluated by metrics previously described."}, {"title": "C. Performance in Low Overlap Scenarios", "content": "Our method, PhotoReg, demonstrates robust performance in scenes with low overlap, where traditional methods often falter. As detailed in table I, within the Workroom 1 dataset, where there is a low overlap between the separate im- age sequences, PhotoReg successfully merges the individual 3DGS models with rendering quality approaching that of the ground truth. In contrast, COLMAP fails to produce any transformation, and ICP results in inaccurate alignments. We observe that COLMAP relies heavily on identifying common visual feature points for matching and alignment. When the overlap of the image sequences is low and common visual features cannot be identified, COLMAP struggles to find alignments between the scenes. ICP, in contrast, depends on point cloud registration. The overlapping regions are typically on the periphery of the input 3DGS scenes. In low overlap situations, ICP struggles to identify correspondences between the points. PhotoReg, our proposed method, circum- vents the drawbacks associated with both COLMAP and ICP. Unlike COLMAP, which relies on traditional visual features, PhotoReg leverages foundation models to uncover regions that are visually and semantically correspondent. These data- driven models are invariant to viewing angles, and adept at recovering alignments even when common visual features are sparse or obscured, as is often the case in low overlap or poor-quality image scenarios. This capability demonstrates PhotoReg's resilience and effectiveness, even in conditions of minimal overlap."}, {"title": "D. Multiple Gaussian Splatting Registration", "content": "In many scenarios, such as robotic mapping of unknown environments, teams of more than two robots may be de- ployed. Each robot contributes to the mapping process by"}, {"title": "E. PhotoReg on Robot-collected Data", "content": "We applied our PhotoReg framework to data collected by two quadruped robots equipped with USB cameras. Each robot was assigned to scan a separate side of the scene. To separate the two regions, a black canvas was placed in the middle, blocking the view between the left and right sides, while still allowing some overlap in shared areas, such as bookshelves. Using the images captured by each quadruped, we generated individual 3DGS models. These models were then processed through our PhotoReg framework to produce a fused GS. The resulting fused model, with rendered images and an illustration of the Gaussian clouds before and after the alignment are illustrated in fig. 10. The merged 3DGS generates high-quality images at regions of overlap."}, {"title": "VI. CONCLUSIONS AND FUTURE WORK", "content": "We present a novel framework, PhotoReg, to register photorealistic Gaussian Splatting models. To tackle this, we advocate for applying pre-trained foundation models to produce correspondences and an initial alignment. We de- velop methodologies to resolve scale disparities between the individual models. Then, PhotoReg fine-tunes the results by optimizing for a precise alignment using photometric losses. We demonstrate by doing the accuracy and robustness of our results on a variety of benchmark datasets and also collected real-world data. In particular, we empirically demonstrate that PhotoReg outshines competitive baselines when aligning individual models with little overlap. Future research avenues"}]}