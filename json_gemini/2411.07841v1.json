{"title": "Federated Learning for Discrete Optimal Transport with Large Population under Incomplete Information", "authors": ["Navpreet Kaur", "Juntao Chen", "Yingdong Lu"], "abstract": "Optimal transport is a powerful framework for the efficient allocation of resources between sources and targets. However, traditional models often struggle to scale effectively in the presence of large and heterogeneous populations. In this work, we introduce a discrete optimal transport framework designed to handle large-scale, heterogeneous target populations, characterized by type distributions. We address two scenarios: one where the type distribution of targets is known, and one where it is unknown. For the known distribution, we propose a fully distributed algorithm to achieve optimal resource allocation. In the case of unknown distribution, we develop a federated learning-based approach that enables efficient computation of the optimal transport scheme while preserving privacy. Case studies are provided to evaluate the performance of our learning algorithm.", "sections": [{"title": "I. INTRODUCTION", "content": "Optimal transport is a fundamental framework used to design efficient resource distribution schemes between two parties, typically referred to as sources and targets [1]. Its application spans various domains, including supply chains (e.g., distributing raw materials from factories to manufacturers), task allocation (e.g., assigning tasks to employees), and more recently, domain adaptation in machine learning [2].\nHowever, the traditional optimal transport paradigm assumes that complete information about all participants is available and that the transport network can be explicitly characterized [3]. This assumption is limiting in modern contexts, such as supply chains and machine learning, where the scale of the network can be vast and dynamic, involving many target nodes requesting resources. In such scenarios, it is often infeasible for resource providers to acquire comprehensive knowledge about the preferences of all target nodes. Therefore, the standard optimal transport framework must be extended to accommodate large-scale, heterogeneous populations of targets with incomplete information.\nIn this paper, we present a new optimal transport framework designed to address the challenges posed by large populations with varying preferences. The heterogeneity of target nodes is represented by a type distribution function, categorizing target nodes based on their preferences for resources. We focus on two key settings: one where the target type distribution is known and one where it is unknown. In the first case, we propose a fully distributed algorithm that optimally allocates resources among the target nodes. In the second case, where the target type distribution is unavailable to the source nodes (transport planners), we introduce a federated learning approach. This approach enables the source nodes to collaboratively and efficiently update transport schemes as new information about the target nodes is gradually collected.\nOur federated learning algorithm is particularly advantageous in scenarios where privacy is a concern, as it allows each target node to calculate local solutions without sharing private data directly with the central planner [4]. Instead, the local solutions are aggregated to form a global transport plan. This method has practical applications in privacy-preserving systems [5] and mobile computing environments [6].\nThe paper is organized as follows. Section II establishes the large-scale discrete optimal transport framework for resource allocation. Section III develops a distributed algorithm to compute the optimal transport plan when the target's type distribution is known, and Section IV proposes a federated learning algorithm to compute the solution when such information is unknown. Section V presents case studies to showcase the developed mechanism. Section VI concludes the paper."}, {"title": "A. Related Works", "content": "1) Federated Learning with Heterogeneous Data: Extensive research has explored the application of federated learning algorithms to heterogeneous data. One of the primary challenges in federated learning is managing the inherent heterogeneity of data, which arises due to differences in statistical distributions, model architectures, and data representations across local devices. This variability complicates the process of aggregating local solutions at the central planner, as the non-uniform nature of the data makes it difficult to apply a standardized method across all local updates. Prior studies [7]\u2013[9] have identified these challenges, categorizing them into different forms of heterogeneity, including statistical heterogeneity, model heterogeneity, and data space heterogeneity. Various strategies have been proposed to address these issues. For example, [8] and [10] offer solutions aimed at mitigating these complexities with [10] specifically introducing a clustering-based approach to enhance the robustness of the algorithm. In our work, we tackle these challenges by categorizing target nodes into distinct types based on probability distribution functions (PDFs). This categorization enables the federated learning algorithm to be efficiently applied within the optimal transport framework, even in scenarios involving large populations with incomplete information.\n2) Optimal Transport and Federated Learning: Several studies have explored the integration of federated learning with the optimal transport framework to address the inherent"}, {"title": "II. LARGE-SCALE DISCRETE OPTIMAL TRANSPORT", "content": "In this section, we present the framework for discrete optimal transport for resource allocation over a large-scale network."}, {"title": "A. Framework", "content": "1) Nodes: sources and targets: We consider a transport network with $N$ target nodes (resource receivers) and $M$ source nodes (resource providers). Since the number of target nodes $N$ is large, we categorize those nodes into different types. This type parameter is denoted by $x \\in \\mathcal{X}$, where the size of $\\mathcal{X}$, $|\\mathcal{X}|$, is finite. The number of type-$x$ target nodes is denoted as $n_x$ for $x \\in \\mathcal{X}$. Type distribution of target nodes is denoted by $P_1$ with $P_1(x)$ representing the proportion of target nodes of type $x$, i.e.,\n$P_1(x) = \\frac{n_x}{N}, x \\in \\mathcal{X} $.\nWe assume that $P(x) > 0$ for $\\forall x \\in \\mathcal{X}$ (otherwise we can just remove this type) and $\\sum_{x \\in \\mathcal{X}} P_1(x) = 1$. Further denote by $\\mathcal{I} := {1,2,...,M}$ the set of source nodes that distribute resources to the targets in the network. We assume that $N\\gg M$ due to the feature of large-population of target nodes of the network.\n2) Network Topology: For each source node $y \\in \\mathcal{Y}$, denote $\\mathcal{X}_y$ as set of target nodes connected to $y$. We assume, for each target node type $x \\in \\mathcal{X}$, each node of type-$x$ connects to the same set of source nodes and it is denoted as $\\mathcal{Y}_x$. We further assume that $\\mathcal{X}_y, \\forall y$ and $\\mathcal{Y}_x, \\forall x$ are nonempty. Otherwise, the corresponding nodes are isolated in the network and do not participate in the resource matching. For convenience, we denote by $\\mathcal{E}$ the set including all feasible transport paths in the network, i.e., $\\mathcal{E} := {\\{x,y\\}|x \\in \\mathcal{X}_y,y \\in \\mathcal{Y}_x\\}$. Here, $\\mathcal{E}$ also refers to the set of all edges in the established bipartite graph for resource transportation.\n3) Problem Formulation: Denote by $A_{xy} \\in \\mathbb{R}_+$ the amount of resources transported from the source node $y \\in \\mathcal{Y}$ to the destination of type $x \\in \\mathcal{X}$. Let $\\Pi := {A_{xy}}_{x \\in \\mathcal{X}_y,y \\in \\mathcal{Y}}$ be the designed transport plan. To this end, the centralized optimal transport problem for utility maximization can be formulated as follows:\n$\\max \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}_x} t_{xy}(A_{xy}) P_1(x)N + \\sum_{y \\in \\mathcal{Y}} \\sum_{x \\in \\mathcal{X}_y} S_{xy}(A_{xy}) P_1(x)N$\ns.t. $\\underline{p}_x \\le \\sum_{y \\in \\mathcal{Y}_x} \\pi_{xy} \\le \\overline{p}_x, \\forall x \\in \\mathcal{X}$,\n$\\underline{q}_y \\le \\sum_{x \\in \\mathcal{X}_y} \\pi_{xy} P_1(x) \\le \\overline{q}_y, \\forall y \\in \\mathcal{Y}$,\n$\\pi_{xy} \\ge 0, \\forall \\{x,y\\} \\in \\mathcal{E}$,\nwhere $t_{xy}: \\mathbb{R}_+ \\to \\mathbb{R}$ and $S_{xy}: \\mathbb{R}_+ \\to \\mathbb{R}$ are utility functions for target node $x$ and source node $y$, respectively. Furthermore, $\\overline{p}_x \\ge \\underline{p}_x \\ge 0, \\forall x \\in \\mathcal{X}$ and $\\overline{q}_y \\ge \\underline{q}_y \\ge 0, \\forall y \\in \\mathcal{Y}$. The constraints $\\underline{p}_x \\le \\sum_{y \\in \\mathcal{Y}_x} \\pi_{xy} \\le \\overline{p}_x$ and $\\underline{q}_y \\le \\sum_{x \\in \\mathcal{X}_y} \\pi_{xy} P_1(x) \\le \\overline{q}_y$ capture the limitations on the amount of requested and transferred resources at the type $x$ target node and source node $y$, respectively.\nWe have the following assumption on the utility functions.\nAssumption 1. The utility functions $t_{xy}$ and $S_{xy}$ are increasing concave functions of $A_{xy}, \\forall x \\in \\mathcal{X},\\forall y \\in \\mathcal{Y}$."}, {"title": "III. DISTRIBUTED OPTIMAL TRANSPORT WITH KNOWN TARGET'S TYPE DISTRIBUTION", "content": "Here, we establish the distributed algorithm for the formulation in (1). Our first step is to rewrite the optimization problem in the ADMM form by introducing ancillary variables $\\pi_{xy}^t$ and $\\pi_{xy}^s$. The superscripts t and s indicate that the corresponding parameters belong to the target node or the source node, respectively. We then set $\\pi_{xy} = \\pi_{xy}^t$ and $\\pi_{xy} = \\pi_{xy}^s$, indicating the solutions proposed by the targets and sources are consistent with the ones proposed by the central planner. This reformulation facilitates the design of a distributed algorithm which allows us to iterate through the process in obtaining the optimal transport scheme. To this end, the reformulated optimal transport problem is presented as follows:\n$\\min \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}_x} t_{xy}(A_{xy}^t) P_1(x) - \\sum_{y \\in \\mathcal{Y}} \\sum_{x \\in \\mathcal{X}_y} S_{xy}(A_{xy}^s) P_1(x)$ \ns.t. $A_{xy} = A_{xy}^t, \\forall \\{x,y\\} \\in \\mathcal{E}$,\n$\\pi_{xy} = A_{xy}^s, \\forall \\{x,y\\} \\in \\mathcal{E}$,\n$\\Pi_t \\in \\mathcal{F}_t, \\Pi_s \\in \\mathcal{F}_s$,\nwhere\n$\\mathcal{F}_t:= {\\{A_{xy}\\}_{x \\in \\mathcal{X}_y,y \\in \\mathcal{Y}}, A_{xy} \\ge 0, \\underline{p}_x \\le \\sum_{y \\in \\mathcal{Y}_x} \\pi_{xy}^t \\le \\overline{p}_x, \\{x,y\\} \\in \\mathcal{E} },$\n$\\mathcal{F}_s:= {\\{A_{xy}\\}_{x \\in \\mathcal{X},y \\in \\mathcal{Y}_x}, A_{xy} \\ge 0, \\underline{q}_y \\le \\sum_{x \\in \\mathcal{X}_y} \\pi_{xy}^s P_1(x) \\le \\overline{q}_y, \\{x,y\\} \\in \\mathcal{E} }$.\nTo solve (2), we leverage the alternating direction method of multipliers (ADMM). First, let $\\alpha_{xy}$ and $\\alpha_{xy}$ be the Lagrangian multipliers associated with the constraint $A_{xy} = A_{xy}^t$ and $A_{xy} = A_{xy}^s$, respectively. Then, the Lagrangian function"}, {"title": "IV. FEDERATED LEARNING UNDER UNKNOWN TYPE DISTRIBUTION", "content": "This section aims to design an efficient optimal transport mechanism when the type distribution of target nodes $P_r(x)$ is unknown. In this scenario, the transport planner does not have complete information on the resource receivers due to its large population feature or too expensive computationally to extract the exact information. Instead, statistical update on the type distribution will be provided by sequentially revealed data on the targets nodes. To this end, we need to develop a data-driven method to counteract the unknown target's type information, which is a federated learning algorithm with a convergence guarantee to compute the optimal transport plan.\nSpecifically, although the type distribution is fixed, but is unknown to the transport planer. At each step of the algorithm, we assume that the type for one of the nodes will be identified according to the unknown distribution. For $k = 1,2,..., N$, $x(k)$ denotes the node type revealed at k-th step of the algorithm. For the algorithm, at k-th step, the $x(k)$-th type target nodes will update their transport decision.\nDenote $\\Pi_0$ the transport plan before the update. Define the prox operator based on $\\mu > 0, \\Pi_0$ and $x \\in \\mathcal{X}$ as,\n$\\mathcal{Z}_{\\mu} (\\Pi_0;x) = \\arg \\min_{\\Pi} - \\sum_{y \\in \\mathcal{Y}_x} t_{xy}(A_{xy}) - \\sum_{y \\in \\mathcal{Y}_x} S_{x_iy}(A_{x_iy})$\n$+ \\frac{1}{2\\mu} ||\\Pi - \\Pi_0||^2$.\nEvidently, $- \\sum_{y \\in \\mathcal{Y}_x} t_{x_iy}(A_{x_iy}) - \\sum_{y \\in \\mathcal{Y}_x} S_{x_iy}(A_{x_iy})$ represents the cost related to $x_i$ and $\\frac{1}{2\\mu} ||\\Pi - \\Pi_0||^2$ the square penalty for change.\nAt step $k$, with existing transport plan $\\Pi(k)$ and pre-specified learning rate $\\mu_k$, local planner at target node of type $x(k)$ addresses the following problem locally:\n$\\mathcal{Z}_{\\mu_k} (\\Pi(k);x(k) = x) = \\arg \\min_{\\Pi} - \\sum_{y \\in \\mathcal{Y}_x} t_{xy}(A_{xy}) - \\sum_{y \\in \\mathcal{Y}_x} S_{xy}(A_{xy}) + \\frac{1}{2\\mu_k} ||\\Pi - \\Pi(k)||^2$.\nThen, the local solution $\\mathcal{Z}_{\\mu_k} (\\Pi(k);x(k) = x)$ that contains only the transport plans from the sources to $x(k)$, computed by the target node is sent to the central planner who will project the solution to the feasible space and broadcast it to the entire transport network. The projection step executed by the central planner is as follows:\n$\\Pi(k+1) = Proj_{\\mathcal{L}_k} (\\mathcal{Z}_{\\mu_k} (\\Pi(k);x(k) = x_i); \\Pi_{-x_i}),$\nwhere the feasible set $\\mathcal{L}_k$ is determined by\n$\\mathcal{L}_k = {\\Pi | \\underline{P} \\le \\sum_{y \\in \\mathcal{Y}_x} \\pi_{xy} \\le \\overline{p}_x, \\forall x \\in \\mathcal{X}, \\\\ \\underline{q}_y \\le \\sum_{x \\in \\mathcal{X}_y} \\pi_{xy} \\hat{P}(x) \\le \\overline{q}_y, \\forall y \\in \\mathcal{Y}, \\\\ A_{xy} \\ge 0,\\forall \\{x,y\\} \\in \\mathcal{E} };$\nwith\n$\\hat{P}(x) = \\sum_{j=1}^{k} \\frac{\\mathbb{I}_{x(j)=x}}{k}$\ndenoting the empirical type distribution of target nodes up to type $k$ and $\\Pi_{-x_i}$ denoting the solution of all types of targets except type $x_i$.\nThe developed federated learning algorithm is summarized in Algorithm 2.\nRemark 1. The total population parameter $N$ does not need to be accurately known at the beginning. The reason is that $N$ is a constant in the objective function in (1) and thus can be excluded. The other place $N$ plays a role is in the second constraint in (1) which affects the project step (15). During the learning, the central planner can first have an estimate of $N$, say $\\tilde{N}$, which might be an overestimate. The final learned transport plan could be easily scaled with a ratio $N/\\tilde{N}$ to reconstruct the optimal strategy when the learning process finishes."}, {"title": "A. Convergence Proof of Algorithm 2", "content": "We show that $\\mathcal{z}_{\\mu}(\\Pi_x;x_i)$ is a smooth convex function under proper assumptions. and denote:\n$f(\\Pi;x_i) = - \\sum_{y \\in \\mathcal{Y}_{x_i}} t_{xy}(A_{x;y}) - \\sum_{y \\in \\mathcal{Y}_{x_i}} S_{x_iy}(A_{x_iy}),$\n$f_{\\mu}(\\Pi;x_i) := f(\\mathcal{z}_{\\mu}(\\Pi;x_i);x_i) + \\frac{1}{2\\mu} ||\\mathcal{z}_{\\mu} (\\Pi;x_i) - \\Pi||^2$.\nFor the consequence analysis, we have two extra assumptions on the utility functions $t_{xy}$ and $s_{xy}$.\nAssumption 2. The utility functions $t_{xy}$ and $S_{xy}$ is proper and Lipschitz, continuous, and let $L_t$ and $L_s$ denote their Lipschitz constant, that is, for any $m, n \\in \\mathbb{R}^n$, we have, $||S_{xy}(m) - S_{xy}(n)|| \\le L_s||m-n||$ and $||t_{xy}(m) -t_{xy}(n)|| \\le L_t||m-n||$."}, {"title": "V. CASE STUDIES", "content": "In this section, we evaluate the performance of our federated learning algorithm through two distinct case studies. The first case study assesses the efficiency of the algorithm, and the second case study explores the algorithm's resilience to changes during the learning process. In both case studies, we consider a scenario with three types of target nodes and two source nodes. The transport network can be represented as a complete bipartite network graph. The utility functions are defined as: $t_{xy}(A_{xy}) = d_{xy}A_{xy}$ and $S_{xy}(A_{xy}) = s_{xy}A_{xy}$, where\n$[Y_{xy}]_{x\\in \\mathcal{X},y\\in \\mathcal{Y}} =  \\begin{bmatrix}2 & 2\\\\3 & 2\\\\1 & 4\\end{bmatrix}, [s_{xy}]_{x\\in \\mathcal{X},y\\in \\mathcal{Y}} =  \\begin{bmatrix}2 & 2\\\\4 & 2\\\\2 & 4\\end{bmatrix}$.\nThe upper bound of resources received by three types of targets is $\\overline{p} = [2,3,4]$ and $\\overline{q} = [4,4] *300$. The lower bounds, $\\underline{p}$ and $\\underline{q}$ are 0 for all nodes. The learning rate is defined as $\\mu = \\frac{5}{i}$, where $i$ represents the iteration step. Furthermore, we consider $N = 8000$ target nodes with a type distribution of $\\overline{P} = [0.5,0.3,0.2]$."}, {"title": "A. Distributed Resource Allocation Efficiency", "content": "We show the efficiency of the designed federated learning algorithm by running Algorithm 2 for 8000 iterations (N = 8000). The results are displayed in Figure 1. Figure 1(a) illustrates the PDF of the sampled types. The three target types-labeled as Type 1, Type 2, and Type 3-correspond to the type distribution previously described. The transport utility, which serves as the key metric for evaluating the algorithm's efficiency, is shown in 1(c). By the end of the 8000 iterations, the utility values indicate that the federated learning algorithm successfully converged to the centralized optimal solution. Figure 1(b) illustrates the corresponding resource allocation across the transport network, showing the distribution of resources from source nodes to the different target types. Finally, Figure 1(d) demonstrates that the total amount of resources received by each target type approaches the optimal allocation, confirming the algorithm's effectiveness in balancing resource demands across the network."}, {"title": "B. Resilience in the Federated Learning Algorithm", "content": "The second case study investigates the algorithm's ability to adapt to a dynamic environment. In real-world scenarios, changes in the network, such as fluctuations in the number of targets, shifts in type distribution, or varying preferences, are common. To simulate this, we introduce a shift in the target type distribution after 600 iterations, changing from $\\overline{P}_1 = [0.50,0.30,0.20]$ to $\\overline{P} = [0.12,0.65,0.23]$. Figure 2 presents the results, showcasing how the algorithm adjusts to this new distribution and re-optimizes resource allocation in response to the changes.\nFigure 2(a) illustrates the updated probability distribution of the target nodes after 8000 iterations. The distribution gets closer to the new distribution but is not completely exact, as it accounts for the samples that are part of the old distribution. Figure 2(b) highlights the adjustments made in the resource allocation scheme after the distribution shift at iteration 600. This adaptability is further emphasized in Figure"}, {"title": "VI. CONCLUSION", "content": "In this paper, we proposed and explored an optimal transport framework designed to handle large-scale, heterogeneous target populations, addressing both scenarios where the target type distribution is known and where it is unknown. Through the development of a distributed ADMM algorithm and a federated learning-based approach, we demonstrated the capability of efficiently computing optimal transport solutions in complex environments. Our federated learning algorithm, in particular, proves to be highly effective in scenarios involving incomplete information, adapting dynamically while preserving the privacy of individual nodes. Future work will develop distributed and privacy-preserving federated learning for large-scale optimal transport."}, {"title": "APPENDIX", "content": "A. Proof of Lemma 1\nProof. The optimality condition of problem $\\min_{z\\in \\mathbb{R}^n} f(z;x_i)+ \\frac{1}{2\\mu} ||\\Pi-z||^2$ is given by:\n$\\frac{1}{\\mu} (\\Pi-z_{\\mu}(\\Pi;x_i)) \\in \\partial f(z_{\\mu}(\\Pi;x_i);x_i)$.\nThe above inclusion implies $g_f(z_{\\mu}(\\Pi;x_i);x_i) \\in \\partial f(z_{\\mu}(\\Pi;x_i); x_i)$, such that:\n$\\frac{1}{\\mu} ||z_{\\mu} (\\Pi; x_i) - \\Pi||^2$\n$=(g_f(z_{\\mu}(\\Pi;x_i);x_i), \\Pi - z_{\\mu} (\\Pi;x_i))$\n$=(g_f(\\Pi,x_i), \\Pi - z_{\\mu} (\\Pi, x_i))$\n$+ (g_f(z_{\\mu} (\\Pi;x_i); x_i) - g_f(\\Pi;x_i), \\Pi - z_{\\mu} (\\Pi;x_i))$\n$\\le (g_f(\\Pi;x_i), \\Pi - z_{\\mu} (\\Pi;x_i)),$\nwhere the inequality follows from the convexity of $f$. Finally, the lemma follow by applying the Cauchy-Schwarz inequality on the right-hand.\nB. Proof of Theorem 1\nFor any $\\mu > 0$, the function $f(\\Pi;x_i) + \\frac{1}{2\\mu} ||\\Pi - \\Pi_0||^2$ is strongly convex, we have:\n$f(\\Pi;x_i) + \\frac{1}{2\\mu} ||\\Pi - \\Pi_0||^2$\n$\\ge f(z_{\\mu} (\\Pi_0;x_i); x_i) + \\frac{1}{2\\mu} ||z_{\\mu} (\\Pi_0;x_i) - \\Pi_0||^2 + \\frac{1}{2\\mu} ||z_{\\mu} (\\Pi_0;x_i) - \\Pi||^2$\n$=f_{\\mu} (\\Pi_0; x_i) + \\frac{1}{2\\mu} ||z_{\\mu} (\\Pi_0; x_i) - \\Pi||^2, \\forall \\Pi \\in \\mathbb{R}^n$"}]}