{"title": "Improving the Transferability of Adversarial Examples by Inverse Knowledge Distillation", "authors": ["Wenyuan Wu", "Zheng Liu", "Yong Chen", "Chao Su", "Dezhong Peng", "Xu Wang"], "abstract": "In recent years, the rapid development of deep neural networks has brought increased attention to the security and robustness of these models. While existing adversarial attack algorithms have demonstrated success in improving adversarial transferability, their performance remains suboptimal due to a lack of consideration for the discrepancies between target and source models. To address this limitation, we propose a novel method, Inverse Knowledge Distillation (IKD), designed to enhance adversarial transferability effectively. IKD introduces a distillation-inspired loss function that seamlessly integrates with gradient-based attack methods, promoting diversity in attack gradients and mitigating overfitting to specific model architectures. By diversifying gradients, IKD enables the generation of adversarial samples with superior generalization capabilities across different models, significantly enhancing their effectiveness in black-box attack scenarios. Extensive experiments on the ImageNet dataset validate the effectiveness of our approach, demonstrating substantial improvements in the transferability and attack success rates of adversarial samples across a wide range of models.", "sections": [{"title": "Introduction", "content": "The rapid advancements and significant achievements in deep learning over the past few years have led to an increased focus on its security aspects. A key security concern is the susceptibility of these systems to minimal, nearly undetectable adversarial noise [Szegedy et al., 2013]. This vulnerability suggests a high risk of deliberate attacks, particularly in technologies such as facial recognition and autonomous driving. Although it is crucial to research methods to bolster deep learning models against adversarial attacks, it is of equal importance to investigate strategies for launching attacks on these models.\nCurrent attack strategies create an adversarial sample by incorporating elaborate adversarial perturbations into the input. These perturbations are usually generated by generating networks [Zhao et al., 2018; Joshi et al., 2019; Qiu et al., 2020; Xiao et al., 2021] or gradient-based optimization techniques [Goodfellow et al., 2014; Madry et al., 2017; Dong et al., 2018; Xie et al., 2019; Dong et al., 2019; Lin et al., 2019]. The latter, the gradient-based approach, is the mainstream. The central idea behind these methods is to generate adversarial perturbations through gradients, which are calculated by maximizing the loss function associated with the target task.\nExisting attack methods demonstrate high efficiency in white-box scenarios but face significant challenges in black-box models, where internal model information is inaccessible. This limitation significantly increases the difficulty of attacks. To address this issue, research efforts are primarily categorized into two approaches: query-based attacks and transfer-based attacks. Query-based attacks generate adversarial samples through extensive queries but incur high computational costs and time overhead. In contrast, transfer-based attacks leverage the transferability of adversarial samples, generating them on surrogate models for application to black-box models, offering greater efficiency and practicality. Consequently, this paper focuses on transfer-based attacks, aiming to enhance the transferability of adversarial samples and improve their effectiveness in black-box attack scenarios.\nAt present, transfer-based attack methods have covered a variety of technologies, including input transformation-based attacks [Dong et al., 2019; Liang and Xiao, 2023; Wang et al., 2021a; Xie et al., 2019], advanced gradient attacks [Dong et al., 2018; Li et al., 2023; Lin et al., 2019; Wang and He, 2021], ensemble attacks [Qian et al., 2023; Tram\u00e8r et al., 2017; Xiong et al., 2022], feature-based attacks [Ganeshan et al., 2019; Wang et al., 2023; Zhang et al., 2022], and so on. Although these methods can improve the transferability of attacks to some extent, they usually come with high computational costs. Moreover, existing methods often fail to adequately account for the differences between surrogate models and target models. As a result, the generated adversarial perturbations tend to perform better on surrogate models but may not retain the same effectiveness when applied to target models. Specifically, these methods suffer from a lack of diversity in attack gradients, relying heavily on a single fixed direction to generate adversarial samples while neglecting other potential directions. This fixed direction is typically determined by the computations of the surrogate model, causing the generated adversarial samples to overfit the surrogate model and limiting their effectiveness in attacking the target model.\nTo address this challenge, we propose Inverse Knowledge Distillation (IKD), a novel and effective method aimed at mitigating overfitting by enhancing gradient diversity. IKD integrates a distillation-inspired mechanism into the loss computation of gradient-based attack methods, optimizing not only the alignment with the specified label but also the divergence in output feature distributions between adversarial and benign samples on the surrogate model. This approach introduces richer gradient information, breaking the constraints of fixed gradient directions and significantly enhancing gradient diversity. As illustrated in Figure 1, by maximizing the difference in feature distributions, IKD reduces dependence on the specific decision boundaries of surrogate models, compelling the optimization process to prioritize more generalized perturbations. This effectively prevents adversarial samples from overfitting to surrogate models and substantially improves their transferability.\nThe main contributions of our work can be summarized as follows:\n\u2022 We propose a simple and effective adversarial attack method: Inverse Knowledge Distillation (IKD) attack. IKD can effectively reduce the overfitting problem and enhance the adversarial sample's transferability. Our Inverse Knowledge distillation (IKD) method is compatible and easy to integrate with gradient-based attack methods to improve attack effectiveness.\n\u2022 We reveal that different distillation methods have significant differences on the transferability of adversarial perturbations. Compared with mean square error (MSE) and cross-entropy (CE) loss, the KL divergence based distillation method shows the best effect in improving the transferability of the adversarial sample.\n\u2022 We conducted a large number of experiments on the ImageNet dataset to verify the validity of the proposed IKD method. The experimental results show that the proposed method significantly improves the success rate of almost all attack methods, which proves the potential and advantage of IKD in practice."}, {"title": "Related Works", "content": "The primary attack techniques comprise of the generative-based [Zhao et al., 2018; Song et al., 2018; Joshi et al., 2019; Qiu et al., 2020; Xiao et al., 2021], and gradient-based strategies [Goodfellow et al., 2014; Madry et al., 2017; Dong et al., 2018; Xie et al., 2019; Dong et al., 2019; Lin et al., 2019]. FGSM (Fast Gradient Sign Method) [Goodfellow et al., 2014] is a simple and fast method for generating adversarial examples.\nSince then, various methods have been proposed to enhance the attacking capability of adversarial samples. In DIM [Xie et al., 2019], randomization operations of random resizing and padding of the original image were introduced. TIM [Dong et al., 2019] proposes a translation-invariant attack method by convolving the gradient with a Gaussian kernel, further augmenting the attacking capability of the samples. Inspired by Nesterov's accelerated gradient [Nesterov, 1983], SIM [Lin et al., 2019] modified the accumulation of gradients to effectively predict and enhance the adversariality of the samples. VT [Wang and He, 2021] considered the gradient variance of the previous iteration to adjust the current gradient, thereby stabilizing the update direction and avoiding poor local optima. EMI [Wang et al., 2021b] accumulated the gradients of data points sampled in the direction of the previous iteration's gradient to find a more stable gradient direction. MagicGAN [Chen et al., 2022] devises a multiagent discriminator capable of adapting to the decision boundaries of diverse target models. This provides a more varied gradient information spectrum, facilitating the creation of adversarial perturbations. MTAA [Chen et al., 2023] employs a representation that preserves relationships to study patterns that are adversarial. APAA [Yuan et al., 2024] directly utilizes the precise gradient direction with a scale factor to generate adversarial perturbations, thereby enhancing the attack success rate of adversarial samples, even with lesser perturbations.\nHowever, none of these methods take into account the effect of the lack of diversity of attack gradients on the adversarial sample's transferability. Therefore, the adversarial samples generated by these methods are heavily dependent on the specific decision boundaries of the surrogate model, thus reducing the attack effect on the target model."}, {"title": "Adversarial Defense", "content": "The goal of adversarial defense is to enhance the resilience of the target model when adversarial samples serve as inputs. Defense approaches can primarily be classified into three types: adversarial detection, adversarial purification, and adversarial training. Adversarial detection techniques [Wang et al., 2019; Meng and Chen, 2017; Liang et al., 2018; Zheng and Hong, 2018], in most instances, obviate the need for model retraining, thereby substantially reducing the complexity of the undertaking. The detection of adversarial instances hinges on the study of the characteristics of adversarial perturbations and their statistical deviations from normal instances. This approach enables the differentiation of adversarial instances during the operation of DNN models, thereby safeguarding them from potential adversarial attacks. Adversarial purification techniques [Liao et al., 2018; Liu et al., 2019; Jia et al., 2019], typically aim to eliminate noise from adversarial samples before they are input into the classifier. Adversarial training techniques [Madry et al., 2017; Tram\u00e8r et al., 2017; Pang et al., 2020], on the other hand, utilize adversarial samples as additional training data to boost the model's robustness."}, {"title": "Method", "content": "The task of adversarial attack involves making subtle modifications to the original image by introducing imperceptible noise, to cause the target model to misclassify the resulting adversarial samples. For instance, a gradient-based approach generates adversarial samples by maximizing the cross-entropy loss function, and its optimization objective can be expressed as:\narg max L(f\u03b8(xadv), y), s.t.||xadv - x||p \u2264 \u03f5, (1)\nxadv\nwhere xadv denote the adversarial sample, xadv = x + \u03b4, with x representing the benign sample and \u03b4 the adversarial perturbation. y represents the ground truth label. f\u03b8 is a well-trained classification model parameterized by \u03b8, and L(,) denotes the cross-entropy loss in the classification task. || ||p represents the lp-norm. In this study, we continue the previous work [Dong et al., 2019; Li et al., 2023; Xie et al., 2019] and use the l\u221e norm to measure the size of the adversarial perturbation. In this framework, the maximum allowable correction of the perturbation \u03b4 is controlled by \u03f5, and the perturbation satisfies a specific constraint, that is, it is in the l\u221e sphere with radius \u03f5 centered on x. Specifically, the disturbance \u03b4 must satisfy ||\u03b4||p \u2264 \u03f5, which ensures that the adversarial perturbation does not deviate too far from the original sample x, thus ensuring that the generated adversarial sample is reasonable in the input space, while maintaining the effectiveness of the attack.\nHowever, the problem in Equation (1) can only be solved if the classification model is directly accessible, i.e., in a white-box setting. In a black-box scenario, where the target model f\u03b8 is not directly accessible, this method cannot be directly applied for attacks. In such cases, directly solving the problem becomes infeasible. To address this limitation, a potential solution is to generate adversarial samples on a surrogate model f\u03b8 and exploit their transferability to attack the inaccessible target model f\u03b8. Given the inherent differences between the surrogate and target models, improving the transferability of adversarial samples generated by the surrogate model f\u03b8 is crucial. This is because the transferability of adversarial samples determines their effectiveness across different models. In black-box attacks, for instance, highly transferable adversarial samples can successfully bypass the defenses of the target model. Therefore, the primary focus of this paper is to enhance the transferability of adversarial samples generated by surrogate models."}, {"title": "Inverse Knowledge Distillation Attack", "content": "Knowledge distillation (KD) is a model compression technique that transfers the knowledge of a large, complex model (referred to as the \"teacher model\") to a smaller, simpler model (referred to as the \"student model\"), thereby reducing the computational complexity of the student model while maintaining performance that is close to that of the teacher model [Hinton, 2015]. This method guides the training of the student model by learning from either the output probability distribution or the intermediate representations of the teacher model.\nInspired by knowledge distillation techniques, we propose the Inverse Knowledge Distillation (IKD) method. Unlike knowledge distillation, our IKD attack method does not involve the concepts of teacher and student models, nor does it require the student model to learn from the teacher model. Instead, our approach aims to maximize the disparity between the predictive output f\u03b8(x) of the surrogate model f\u03b8 on benign input data x and the predictive output f\u03b8(xadv) of the surrogate model on adversarial input data xadv, without compromising the performance of the white-box adversarial attack. In this case, the optimization objective in Equation (1) becomes the following:\narg max(Lhard(f\u03b8(xadv), y) + \u03b3 \u00b7 Lsoft(f\u03b8(xadv), f\u03b8(x))), (2)\nxadv\nwhere \u03b3 is the distillation weight. The objective is to minimize the amount of knowledge contained in the adversarial sample xadv that is related to the surrogate model f\u03b8, in order to avoid overfitting of the adversarial sample xadv to the surrogate model f\u03b8 and to enhance the transferability of the adversarial sample.\nThe soft label loss Lsoft, defined in Equation (2), can take various common forms, such as Kullback-Leibler (KL) divergence, mean square error (MSE), and cross-entropy (CE). In IKD, we select KL divergence as the soft label loss, formulated as:\nLsoft(f\u03b8(xadv), f\u03b8(x)) = KL(f\u03b8(x)||f\u03b8(xadv))\n= \u2211 f\u03b8(x)(i) log f\u03b8(x)(i) , (3)\ni f\u03b8(xadv)(i)\nThe KL divergence measures how much information is lost when f\u03b8(xadv) is used to approximate f\u03b8(x). It provides unique advantages in quantifying the difference between the probability distributions of benign samples f\u03b8(x) and adversarial samples f\u03b8(xadv).\nThe primary reasons for selecting KL divergence over MSE or CE as the soft label loss are as follows. First, the gradient of KL divergence with respect to f\u03b8(xadv) is given by:\n\u2202KL(f\u03b8(x)||f\u03b8(xadv))\n= f\u03b8(x)(i) , (4)\n\u2202f\u03b8(xadv)(i)\nf\u03b8(xadv)(i)\nwhich exhibits two key characteristics: asymmetric sensitivity and probabilistic emphasis. Asymmetric sensitivity refers to the fact that KL divergence penalizes larger deviations of f\u03b8(xadv) from f\u03b8(x) more heavily. This asymmetry is crucial for capturing directional differences in the probability distributions, ensuring that adversarial samples move away from the benign sample distribution and, consequently, the decision boundary of the surrogate model. Probabilistic emphasis, on the other hand, means that the gradient is weighted by f\u03b8(x), prioritizing high-probability regions of the benign distribution. This alignment enhances the goal of modifying the adversarial sample's predictions.\nSecond, the gradient of MSE with respect to f\u03b8(xadv) is given by:\n\u2202MSE(f\u03b8(x)||f\u03b8(xadv))\n= 2 (f\u03b8(xadv) \u2212 f\u03b8(x)). (5)\n\u2202f\u03b8(xadv)(i)\nCompared to KL divergence, MSE has two main drawbacks: symmetric penalization and gradient uniformity. Symmetric penalization refers to the fact that MSE treats overestimation and underestimation of probabilities symmetrically, which is suboptimal for adversarial attacks. Gradient uniformity means that MSE does not account for the relative importance of the probability f\u03b8(xadv). As a result, MSE applies uniform penalties across all categories, diminishing its effectiveness in optimizing the high-probability regions of the target distribution. Consequently, MSE lacks the probabilistic emphasis and asymmetric sensitivity needed for generating effective adversarial attacks.\nFinally, the gradient of Cross-Entropy (CE) with respect to f\u03b8(xadv) is given by:\n\u2202CE(f\u03b8(x)||f\u03b8(xadv))\n= f\u03b8(x) , (6)\n\u2202f\u03b8(xadv)(i)\nf\u03b8(xadv)(i)\nAlthough both CE and KL divergence share the same gradient structure, they differ in their original forms. In CE, the rightmost logarithmic term is log f\u03b8(xadv) , whereas in KL divergence, it is log f\u03b8(adv). This difference introduces two key disadvantages for CE. First, CE cannot capture the relationship between f\u03b8(x) and f\u03b8(xadv) when f\u03b8(x) is close to zero. In such cases, CE fails to fully account for the deviation of f\u03b8(xadv), potentially leading to suboptimal gradient updates. Second, CE only focuses on the direction from f\u03b8(xadv) to f\u03b8(x). In contrast, the mutual penalization in KL divergence ensures that both f\u03b8(x) and f\u03b8(xadv) are pushed away from each other in a balanced manner, a property that CE lacks.\nAdditionally, a series of experiments are conducted to validate the effectiveness of selecting KL divergence as the soft label loss, as detailed in Section 4.3."}, {"title": "Enhanced Gradient Diversity", "content": "The introduction of the IKD enhances the diversity of gradients used during the adversarial example generation. Traditional gradient-based attack methods, such as FGSM and I-FGSM, often rely on the gradient of the loss with respect to the input, which can lead to similar perturbations along specific directions of the input space.\nTo address this issue, we introduce IKD into the loss calculation. This modification ensures that, in addition to the gradient of the classification loss with respect to the input, the optimization process now incorporates the gradient of the divergence between the distributions of benign and adversarial examples. This additional term encourages the attack to move not only along the steepest descent direction of the adversarial loss but also in directions that maximize the difference between the output distributions of the benign and adversarial samples. As a result, the adversarial example is pushed to explore a broader range of directions in the input space, leading to greater gradient diversity.\nThis increased gradient diversity prevents the optimization from getting stuck in local minima and allows the adversarial perturbations to better generalize across different models, which is essential for improving transferability."}, {"title": "Attack Algorithm", "content": "Similar to previous work [Xie et al., 2019; Long et al., 2022], the proposed method can be seamlessly integrated with any gradient-based attack technique. For instance, using MIFGSM as an example, we combine inverse knowledge distillation (IKD) with MIFGSM to introduce a new attack method, named MIFGSM-IKD. In this process, inverse distillation is incorporated into the gradient update rules of the MIFGSM attack to improve the transferability of adversarial samples. Specifically, by introducing IKD into the loss function, inverse distillation adjusts the gradient update direction, enabling the generated adversarial samples to not only effectively attack the surrogate model but also exhibit enhanced cross-model transferability. We propose a new adversarial sample update formula for the MIFGSM-IKD method, as follows:\nxadv = x, g0 = 0,\nLtotal = Lhard(f\u03b8(xadv), y) + \u03b3Lsoft(f\u03b8(xadv), f\u03b8(x)),\ngt+1 = \u03bc \u00b7 gt +\n\u2207x(Ltotal)\n, (7)\n||\u2207x(Ltotal)||1\nxadv = xadv + \u03b1 \u00b7 sign(gt+1),\nwhere gt and xadv represent the gradient and the adversarial sample generated at the t-th iteration, respectively. The parameter \u03b1 denotes the attack step size and controls the magnitude of the update to the adversarial sample in each iteration. \u03bc is an attenuation factor used to adjust the influence of historical information during gradient updating. || \u00b7 ||1 represents the L\u2081 norm. Thus, the IKD can be applied as an enhancement to the gradient-based attack method to improve its effectiveness in black-box settings."}, {"title": "Experiments", "content": "We use a subset\u00b9 of the ImageNet dataset [Russakovsky et al., 2015] for the experiment. This subset consists of 1,000 images, covering nearly all the major categories in ImageNet, and has been widely used in previous related studies. In the experimental setup, we choose a pixel value range of 0-255 and set the maximum perturbation budget to 16, using the L\u221e norm for the disturbance. Specifically, the perturbation size is constrained such that the adversarial perturbation must satisfy ||\u03b4|| \u2264 16, ensuring that the generated adversarial sample does not deviate significantly from the original sample. Additionally, to match the standardized input format, we adjust the image resolution to 3 \u00d7 224 \u00d7 224, which complies with the standard preprocessing requirements of the ImageNet dataset.\nWe evaluate all attack methods using conventional training models and defense models. A total of 9 standard models and 2 defense models, provided by the timm package [Wightman, 2019], are assessed. Specifically, the models used include: ResNet50 [He et al., 2016a], DenseNet121 [Huang et al., 2017], ResNeXt50 [Xie et al., 2017], VGG19BN [Simonyan, 2014], InceptionResNet-v2 [Szegedy et al., 2017], Inception-v3 [Szegedy et al., 2016], Inception-v4 [Szegedy et al., 2017], ResNet101 [He et al., 2016b], ResNet152 [He et al., 2016b], Inception-v3adv [Tram\u00e8r et al., 2017] and InceptionResNet-v2adv,ens [Tram\u00e8r et al., 2017].\nTo evaluate the effectiveness of different attack methods, we use the attack success rate (ASR) for both white-box and black-box models as measurement indicators.\nTo thoroughly evaluate our proposed approach, we selected several existing baseline attack methods for comparison, including MIFGSM [Dong et al., 2018], DIFGSM [Xie et al., 2019], TIFGSM [Dong et al., 2019], NIFGSM [Lin et al., 2019], SINIFGSM [Lin et al., 2019], VMIFGSM [Wang and He, 2021], and VNIFGSM [Wang and He, 2021]. These methods encompass various types of gradient-based attacks, providing a broad frame of reference to effectively demonstrate the performance of our approach against multiple attack strategies.\nTo ensure the comparability and consistency of the experiments, we set the maximum permissible perturbation \u03f5 = 16,\n2\nthe number of iterations T = 10, the attack step size \u03b1 = 2\n255,\nand the momentum term attenuation factor \u00b5 = 1.0, following previous research [Dong et al., 2019; Wang and He, 2021; Xie et al., 2019]. These parameter settings are consistent with those commonly used in the literature, enabling a fair comparison of different attack methods. To facilitate the implementation and comparison of attack strategies, we employ the attack toolkit Torchattacks [Kim, 2020] and retain its default parameters, with the exception of the custom settings for \u03f5 and T. This ensures that all methods are compared within the same attack framework, minimizing the influence of other factors on the experimental outcomes. All experiments are implemented in the PyTorch framework and executed on one NVIDIA GeForce RTX 3090 GPU."}, {"title": "Experiments Results", "content": "In this section, we integrate the proposed Inverse Knowledge Distillation (IKD) method with existing attack strategies to investigate potential improvements in attack performance. The resulting combined methods are denoted by the suffix \u201c-IKD\u201d, such as MIFGSM-IKD. The comparison methods include classical attacks (e.g., MIFGSM and NIFGSM), attacks based on input transformations (e.g., DIFGSM and TIFGSM), and advanced gradient-based attacks (e.g., SINIFGSM, VMIFGSM, and VNIFGSM).\nAs shown in Table 1, our method outperforms the comparison methods on the vast majority of black-box models. Specifically, we observe a significant improvement in average transferability across almost all eleven models tested. Notably, our method achieves a 6.8% increase in average transferability compared to DIFGSM on the VGG19BN model, with our DIFGSM-IKD method attaining an average transferability of 57.8%, while DIFGSM alone achieves 51.0%. Furthermore, our approach also outperforms most comparison methods under defense models in terms of average attack success rate, demonstrating the effectiveness of our method in attacking defensive mechanisms. Additionally, we find that DenseNet121 is the most vulnerable model, exhibiting the highest average transferability, which challenges the conventional belief that deeper models are inherently more robust than shallower ones. This observation suggests that model robustness is more closely linked to architectural design than to depth alone. Finally, we acknowledge that in some cases, our method may show slightly lower performance than comparison methods in either white-box or black-box settings. This may be due to the introduction of the IKD, which could slightly interfere with the generation of adversarial perturbations. Additional experimental results can be found in the supplementary material."}, {"title": "Ablation Study", "content": "To thoroughly investigate the potential factors that may influence the performance of our method, we conduct two ablation experiments in this section: (1) the effect of IKD's soft label loss selection in inverse knowledge distillation, and (2) the impact of the weight of the IKD.\nTo investigate the effect of different types of IKD's soft label loss on adversarial transferability, we employ Mean Squared Error (MSE), Cross-Entropy (CE), and Kullback-Leibler (KL) Divergence for inverse knowledge distillation. Specifically, we conduct experiments on ResNet50 (RN50) using MSE, CE, and KL divergence as the soft label loss in IKD. The average evaluation results are presented in Figure 2. From the results, it is evident that KL divergence demonstrates superior adversarial transferability on RN50 compared to the other two methods. For RN50, the mean transferability with MSE, CE, and KL divergence as the IKD's soft label loss were 63.2%, 63.4%, and 65.2%, respectively. The transferability across different soft label loss varies significantly. Based on these findings, we opt to use KL divergence as the soft label loss for IKD. For more detailed experimental results, please refer to the supplementary material.\nThe inverse knowledge distillation method exhibits varying influences on the gradient direction based on the weight parameter, which in turn affects the adversarial transferability. To investigate this relationship, we perform a grid search over the weight parameter \u03b3 of inverse knowledge distillation in the range {0.001, 0.01, 0.1, 1, 10, 100, 1000}, using ResNet50 (RN50) as the surrogate model. The average evaluation results are presented in Figure 3. As observed, with the increase in the IKD weight, the average transferability of RN50 initially remains stable within a certain range, and then gradually declines. This suggests that the inverse knowledge distillation method proposed in this paper is not confined to a specific weight value. Based on these findings, we select \u03b3 = 0.01 as the weight for the inverse knowledge distillation attack. For more detailed experimental results, please refer to the supplementary material."}, {"title": "Conclusion", "content": "In this paper, we propose an inverse knowledge distillation (IKD) method aimed at significantly improving the transferability of adversarial samples. Specifically, we enhance the diversity of attack gradients by incorporating the IKD's soft label loss into the model's loss function. IKD not only encourages the model to focus on the classification error of the target label during adversarial sample generation but also optimizes the gradient update direction through the inverse knowledge distillation process. As a result, our approach effectively mitigates overfitting and enhances the transferability of adversarial samples. The proposed IKD method integrates seamlessly with existing gradient-based attack techniques, thereby boosting attack performance. Experimental results demonstrate the efficacy of the proposed method, with extensive evaluations on the ImageNet dataset confirming significant improvements in attack success rates across several baseline attack strategies."}, {"title": "Algorithm", "content": "Taking MIFGSM as an example, the specific algorithmic workflow of MIFGSM-IKD is detailed in Algorithm 1. By incorporating the momentum term and an appropriate IKD strategy, adversarial samples are iteratively updated to achieve effective attacks on the target model.\nInput: A classifier f with loss function L; a benign sample x and ground-truth label y;\nInput: The size of perturbation e; iterations T and decay factor \u00b5\nOutput: An adversarial sample xadv with ||xadv \u2013 x|| < E.\n\u03b1 = 1;\n2: g0 = 0; xadv = x;\nfor t = 0 to T-1 do\nInput xady to f and obtain the hard label loss Lhard(f(xadv), y);\nInput xadv and x to f and obtain the soft label loss Lsoft(f(xadv), f(x));\nCalculate the total loss Ltotal = Lhard(f(xadv),y)+yLsoft(f(xadv), f(x));\nObtain the gradient \u2207x(Ltotal);\nUpdate gt+1 by accumulating the velocity vector in the gradient direction as\n\u25bcx(Ltotal)\n;\ngt+1 = \u03bc. 9t +\n(Ltotal) ||1\nUpdate xady by applying the sign gradient as\nmadv\nXt+1 = xadv + a sign(gt+1);\nend for\nreturn xadv = xadv."}, {"title": "Experiments", "content": "Table 3 demonstrates the effects of IKD on classical and input transformation-based attacks when using ResNeXt50, Inception-v3, Inception-v4, and ResNet152 as surrogate models.\nTable 4 illustrates the impact of IKD on advanced gradient attacks using ResNeXt50, Inception-v3, Inception-v4, and ResNet152 as surrogate models.\nTable 5 presents the impact of different IKD methods on the success rate of transfer-based attacks, with ResNet50 serving as the surrogate model.\nTables 6 and 7 presents the effect of IKD weight on the transfer-based attack success rate."}]}