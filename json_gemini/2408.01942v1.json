{"title": "Visual Grounding for Object-Level Generalization in Reinforcement Learning", "authors": ["Haobin Jiang", "Zongqing Lu"], "abstract": "Generalization is a pivotal challenge for agents following natural language instructions. To approach this goal, we leverage a vision-language model (VLM) for visual grounding and transfer its vision-language knowledge into reinforcement learning (RL) for object-centric tasks, which makes the agent capable of zero-shot generalization to unseen objects and instructions. By visual grounding, we obtain an object-grounded confidence map for the target object indicated in the instruction. Based on this map, we introduce two routes to transfer VLM knowledge into RL. Firstly, we propose an object-grounded intrinsic reward function derived from the confidence map to more effectively guide the agent towards the target object. Secondly, the confidence map offers a more unified, accessible task representation for the agent's policy, compared to language embeddings. This enables the agent to process unseen objects and instructions through comprehensible visual confidence maps, facilitating zero-shot object-level generalization. Single-task experiments prove that our intrinsic reward significantly improves performance on challenging skill learning. In multi-task experiments, through testing on tasks beyond the training set, we show that the agent, when provided with the confidence map as the task representation, possesses better generalization capabilities than language-based conditioning.", "sections": [{"title": "1 Introduction", "content": "In the field of artificial intelligence, the ability of agents to understand and follow natural language instructions in an open-ended manner is crucial. However, the scope of training content for an agent's policy learning is always finite. Zero-shot generalization task which involves being instructed to interact with diverse objects not encountered during training, from the vast realm of human vocabulary, represents a pivotal step towards creating general artificial intelligence systems capable of adapting to a wide range of real-world scenarios. As a popular open-ended 3D game, Minecraft serves as an ideal testbed for learning and evaluating generalization ability. At its core, Minecraft offers procedurally generated worlds with unlimited size and a large variety of tasks ranging from navigation and combat to building and survival. Compared with canonical game environments such as Go, Atari, and StarCraft, Minecraft mirrors the complexity of real-world challenges and offers a wide range of objects and tasks with natural language instructions.\nGiven the finite data scope for the agent's policy learning, it is infeasible for the policy to directly comprehend the vast array of object names beyond the training set, which human language instructions might contain. To equip an agent with zero-shot generalization ability over objects, integration of a vision-language model (VLM) is a promising way. A VLM aligns images and language vocabularies into the same feature space, bridging the gap between visual observations and natural language instructions. Therefore, it has the capability to ground the agent's unseen text, e.g., names of novel objects, in visual observations, enabling the agent to comprehend instructions not encountered during training. CLIP emerges as a significant model and has become widely used. Recent works have extensively adopted CLIP as a foundation model for open-vocabulary object detection and open-vocabulary segmentation, leveraging its rich vision-language knowledge. Moreover, CLIP even exhibits remarkable segmentation capabilities and explainability without fine-tuning.\nCLIP's visual grounding ability to perform segmentation inspires two routes for enhancing the agent's policy learning in Minecraft by transferring VLM knowledge into reinforcement learning (RL). The first approach is transfer via reward. The pixel area of the target object can be used as a surrogate for the distance between the agent and the target object. It can then serve as an intrinsic reward to guide the agent towards the target object, thereby facilitating interaction. The second one is transfer via representation. The segmentation result can replace the language instruction as a unified, more accessible task representation. Practical research in robotics proves that models with such location input show superior performance compared to mere text input. Most importantly, the segmentation is open-vocabulary , which means it remains effective for instructions containing novel objects not encountered during agent training.\nThanks to MineCLIP, a variant of CLIP fine-tuned on Internet-scale Minecraft videos from YouTube, it becomes accessible to develop a general-izable agent in Minecraft, following the aforementioned inspirations. Initially, MineCLIP is merely used as a tool to measure the similarity between a sequence of visual observations and the instruction. It serves as an intrinsic reward for RL and achieves notable performance in Minecraft. Based on this foundation, our goal is to further explore the potential capabilities of MineCLIP, enabling it to offer additional visual grounding information beyond the similarity between observations and instructions to aid the agent's policy learning and improve its generalization ability.\nIn this paper, we propose a CLIP-guided Object-grounded Policy Learning method, namely COPL, that transfers the vision-language knowledge contained in MineCLIP to RL at a minimal cost. By visual grounding, we generate a confidence map of the target object indicated in the language instruction via our modified MineCLIP. We extend MineCLIP with modifications inspired by MaskCLIP so that it can segment the specified object from the image. As illustrated in Figure 1 (left), our approach can convert instructions into unified two-dimensional confidence maps. To leverage this object-grounded result, we first design an intrinsic reward that takes into account the pixel area and location of the target object in the image observation. By doing so, we also address a deficiency of the original MineCLIP reward : it is insensitive to the distance to the target object. Furthermore, we integrate the resulting confidence map into the policy input as a task representation, as illustrated in Figure 1 (right). Based on this adjustment, our agent can possess zero-shot generaliza-tion capability over objects through multi-task RL trained on only a limited set of instructions. Due to the reuse of MineCLIP's encoding, such generalization capability does not incur significant additional computational overhead.\nWe evaluate COPL on basic skill learning and zero-shot object-level gener-alization in Minecraft. Firstly, we conduct a group of single-task experiments to show that our object-grounded intrinsic reward successfully enables the agent to acquire various challenging basic skills while the MineCLIP reward fails . Then we extend our evaluation to instruction-following scenarios, where we train the agent with a set of instructions. In our test, the agent exhibits the capacity to execute instructions involving previously unseen targets, effectively demonstrat-ing its generalization ability over objects. COPL's success rate on unseen targets surpasses that of language-conditioned methods by around 300% in the hunting domain and 100% in the harvest domain. Though we implement and evaluate COPL in Minecraft, we believe our method is extendable to other similar open-ended environments and draws insights into transferring VLM knowledge into RL for training generalizable agents."}, {"title": "2 Preliminary", "content": "Problem Statement. In this paper, we focus on object-centric tasks in Minecraft, where the agent is instructed to interact with diverse objects. By zero-shot object-level generalization, we mean that the agent is instructed to interact with objects beyond the training scope without fine-tuning during evaluation. To formalize, we denote the set of objects with which the agent learns to interact during the training phase as $C_t$, and the set of objects with which the agent is required to interact during the evaluation phase as $C_e$. To test the generalization ability of the agent, $C_e$ consists of objects that are not in $C_t$. For example, during training, the agent learns to accomplish language instructions \"hunt a cow\" and \"hunt a sheep\". However, during evaluation, it will encounter instructions like \"hunt a horse\" or \"hunt a chicken\", where neither horse nor chicken appears in the instructions during training. Note that we do not consider generalization ability concerning unseen actions, which could be left as future work. Therefore, instructions during evaluation should have the same behavior patterns as those learned in training. For instance, when training with \"hunt sth.\" and \"harvest sth.\", testing with \"explore the world\" is not considered.\nZero-Shot Generalization. Our zero-shot object-level generalization in Minecraft is a specific form of broader zero-shot generalization (ZSG) defined in the contextual Markov decision process (CMDP) framework in RL. While ZSG typically involves adapting to new environments or tasks, our focus is on enabling agents to follow instructions and interact with novel objects not encountered during training. This object-level adaptation aligns with ZSG's objective of performing effectively in unseen scenarios. In other words, the \u201ccontext\" in our case is specifically defined as the target objects indicated in instructions.\nMineCLIP for Minecraft RL. MineCLIP is a VLM pre-trained on Internet-scale Minecraft videos from YouTube, learning the alignment between video clips (16 frames) and natural language. Similar to CLIP, MineCLIP adopts a ViT as the image encoder and a GPT as the text encoder. The main difference between MineCLIP and CLIP is that MineCLIP takes as input a sequence of 16 images. Therefore, MineCLIP incorporates an additional module to aggregate the 16 embeddings generated by the image encoder. The proposed two mechanisms include a temporal transformer (MineCLIP[attn]) and direct average pooling (MineCLIP[avg]). In this paper, we choose the former as our base model due to its better performance in Programmatic tasks compared to the latter. For RL in Minecraft, MineCLIP provides an intrinsic reward function $R_i: G \\times S^{16} \\rightarrow R$, representing the similarity between the image observation sequence of the previous 16 steps $[s_{t-15},..., s_{t-1}, s_{t}]$ and the task prompt $g$."}, {"title": "3 Related Work", "content": "Minecraft Research. Broadly, challenges in Minecraft can be categorized into high-level task planning and low-level skill learning. For high-level planning, where agents must make decisions on which skills to employ sequentially based on the given instruction, the field has converged towards leveraging large language models (LLMs) . Regarding learning low-level skills, the difficulty lies in the absence of well-defined dense reward and a vast variety of objects to interact with in Minecraft. Unlike the convergence in high-level planning approaches, two distinct routes have emerged in low-level learning. The first route, represented by MineCLIP, utilizes the reward derived from the alignment between text and video clip or other manually designed rewards for RL. The second one follows the principles of VPT, where skills are acquired through imitation learning based on large-scale demonstration. Our work falls in the scope of low-level skill learning with RL.\nInstruction-Following RL. Language has been widely explored in goal-conditioned RL for its compositional structure . This feature allows goal-conditioned policies to better capture the latent structure of the task space and generalize to unseen instructions that combine seen words. With the development of LLM and VLM, language also becomes a means of providing intrinsic rewards in RL. The similarity or correlation between instructions and current states provides dense rewards to guide the agent's learning more effectively. Our work stands out by enabling the policy to generalize to instructions that contain previously unseen targets.\nCLIP for Embodied AI. CLIP provides diverse usage for AI research. We categorize these applications into three areas: encoding, retrieving and locating. Encoding, the most common use of CLIP, leverages CLIP encoders to represent images and/or texts. Our work also utilizes the MineCLIP image encoder to process raw image observations. Retrieving mostly involves navigation tasks, where CLIP assists in selecting the most matching image from a set based on the given instruction . The most relevant usage to our work is locating, which applies methods like MaskCLIP or GradCAM on CLIP to determine the position of the specific object in images . Based on the object location, agents can conduct planning with a depth detector or imitation learning . In contrast, our work focuses on training agents via RL with information solely extracted from image observations, without any extra spatial information or demonstration."}, {"title": "4 Method", "content": "In this section, we detail the implementation of our COPL method in Minecraft. We introduce how to exploit the visual grounding capability of MineCLIP through modifications, enabling the segmentation of the target object indicated in the language instruction (Section 4.1). This process yields an object-grounded con-fidence map, where each element represents the probability of the specified tar-get's presence. Based on this confidence map, we first implement VLM knowledge transfer via reward, presenting a simple but effective object-grounded intrinsic reward to guide the agent toward the target (Section 4.2). Then, we propose transfer via representation, where we integrate the confidence map into the pol-icy as a task representation (Section 4.3). This integration equips the agent with zero-shot generalization ability over objects by grounding the novel object in a comprehensible visual representation."}, {"title": "4.1 Visual Grounding", "content": "Prior to segmentation, we must extract the correct target that the agent needs to interact with from the provided language instruction. Consider an example instruction: \"hunt a cow in plains with a diamond sword\". In this case, it is cow that should be extracted from the instruction as the target object, rather than plains or diamond sword, for the following segmentation. This can be easily accomplished by LLMs with appropriate prompts. Details on prompt de-signing and conversations with GPT-4 can be found in Appendix A.1.\nIn the standard CLIP, the image encoder, a ResNet or ViT, aggregates the visual features from all spatial locations through attention pool-ing. Recent works reveal that these features on each spatial location contain rich local information so that they can be used to perform zero-shot pixel-level predictions. In brief, the cosine similarities between these features and the outputs of the CLIP text encoder are also valid and informative. Con-cretely, MaskCLIP makes use of the value-embedding of each spatial location in the last attention module, while CLIPSurgery studies the feature of each spatial location in the final output and introduces an additional path. Inspired by MaskCLIP, we make adaptations to MineCLIP architecture to generate a confidence map for a specified target without fine-tuning.\nTo begin, we introduce the modification to the vision pathway of MineCLIP. We make changes to extract dense features from the last block of ViT. As il-lustrated in the rightmost part of Figure 2, the scaled dot-product attention in multi-head attention module is removed, while the value-embedding trans-formation is retained. Then the transformed embeddings excluding that of CLS token are fed into the remaining modules within the ViT to obtain the final embedding of each patch. In this way, these patch embeddings share the same space as the original ViT output. As shown in Figure 2, the modified image encoder outputs patch embeddings instead of image embedding. However, these embeddings are not yet aligned with the embedding space of MineCLIP. In MineCLIP, the image encoder is followed by a temporal transformer that ag-gregates the embeddings of 16 images. Therefore, these patch embeddings also need to pass through the temporal transformer to guarantee alignment. Notably, these embeddings do not form a temporal sequence together as the input of the transformer. Instead, each patch embedding is individually processed by the temporal transformer, treated as a sequence of length 1. In this way, we obtain patch embeddings in the MineCLIP embedding space.\nIn the language pathway, no modification is made to the MineCLIP text encoder. The target name is encoded using the text encoder, along with a list of negative words . We construct a negative word list containing objects that frequently appear in Minecraft. For a detailed description of the word list, please refer to Appendix A.2. Given the patch embeddings encoded through the modified image encoder and the temporal transformer in the same embedding space of MineCLIP, we can calculate cosine similarities between patch embed-dings and text embeddings, following the same approach as CLIP. Subsequently, we use softmax with the same temperature used in MineCLIP to determine the probabilities of objects' presence on each patch. Finally, we extract and reshape the probabilities of the target object to form the confidence map. The result-ing confidence map consists of the same number of elements as the patches, with each element representing the probability of the target's presence on the corresponding patch. Examples of the confidence maps are shown in Figure 3.\nIn our preliminary experiment, we qualitatively attempt off-the-shelf open-vocabulary detection models but find their performance to be impaired by the domain gap between Minecraft and the real world, not as satisfactory as the results of our domain-specific model modified based on MineCLIP, as demon-strated in Appendix A.4. Another advantage of our method is that the genera-tion of the confidence map can be integrated with the calculation of MineCLIP reward or the encoding of images using MineCLIP encoder, thus avoiding signif-icant computational costs from incorporating additional segmentation models."}, {"title": "4.2 Transfer via Reward", "content": "The object-grounded confidence map of the target contains rich spatial infor-mation that can be utilized to facilitate RL through reward designing. The area occupied by the target in the image can serve as a proxy for estimating the distance to the target, based on the principle that the closer the target is to the agent, the larger its area in the image and vice versa. Therefore, a reward proportional to the area of the target would guide the agent towards the tar-get effectively. Additionally, we argue that the agent should be encouraged to aim at the target, i.e., adjust the perspective to center the target in the field of view. This would help the agent further stabilize its orientation and increase the chance of interacting with the target when it is close enough. In Minecraft, interaction can only occur when the crosshair in the center of the agent view aligns with the target. Moreover, when multiple target objects are present in the view, the agent should learn to focus on a single target rather than attempting to keep all of them in view. This could also be interpreted in a more general way, such as humans usually place the target at the center of the visual field for better perception and interaction.\nBased on these principles, we introduce an object-grounded intrinsic reward function named focal reward. At each time step $t$, it is computed as the mean of the Hadamard product between the current target confidence map $m_t$, and a Gaussian kernel denoted as $m^k$:\n$r^f = \\text{mean} \\left( m_t \\circ m^k \\right).$\n(1)\nHere, $m_t$ and $m^k$ share the same dimensions with height $H$ and width $W$. Each element of the Gaussian kernel is defined as:\n$m_j^k = \\exp \\left( -\\frac{(i - \\mu_1)^2}{2 \\sigma_1^2} - \\frac{(j - \\mu_2)^2}{2 \\sigma_2^2} \\right),$\n(2)\nwhere $\\mu_1 = (H + 1) / 2$, $\\sigma_1 = H / 3$, $\\mu_2 = (W + 1) / 2$, and $\\sigma_2 = W / 3$. This reward function is designed to be directly proportional to the area occupied by the target and inversely proportional to the distance between the target patches and the center of the view.\nAs noted in, the MineCLIP reward, which relies on the similarity between the agent's preceding image observations and the provided instruction, is uncor-related with the distance between the agent and the target. This phenomenon is demonstrated in Figure 4, where the MineCLIP reward does not consistently increase as the agent gets closer to the target. Consequently, in practice, the agent trained with the MineCLIP reward tends to \"stare at\" the target at a distance, rather than approaching it. This tendency obstructs the agent from learning some hard-exploration skills, particularly those that require multiple times of interactions with the targets, such as hunting. In contrast, our focal reward ad-dresses this deficiency, increasing consistently when the agent approaches the target cow, as illustrated in Figure 4.\nThe confidence map generated from the modified MineCLIP may sometimes contain noisy activation. Therefore, we process the raw confidence map to enhance its quality before using it to compute the intrinsic reward. Firstly, we set the value corresponding to the patch where a word from the negative word list has the highest probability instead of the target, to zero. This operation reduces the impact of noisy activation on non-target patches. Secondly, we set values in the confidence map lower than a threshold $\\tau = 0.2$ to zero, while those higher than this threshold are set to one, so as to amplify the distinction between patches corresponding to the target and those unrelated to it. We ablate the Gaussian kernel and denoising process in Section 5.1."}, {"title": "4.3 Transfer via Representation", "content": "To train an instruction-following agent, the conventional practice involves di-rectly taking the natural language instruction as the task representation into the policy network. These instructions are typically encoded using a recurrent network or a language model such as BERT and CLIP. In contrast, we extract the target object from the instruction using GPT-4 and subsequently convert it into a two-dimensional matrix, i.e., the confidence map. Our underlying assumption is that this two-dimensional object-grounded repre-sentation offers more intuitive and accessible information for the policy network compared to the intricate space of language embeddings. When facing an instruc-tion containing the name of a target object not encountered during training, our method grounds this novel text in the two-dimensional map, rendering it a comprehensible representation for the policy network. As a result, the agent can follow the guidance of the confidence map, navigate towards the novel target object, and finally interact with it.\nIn our implementation, we adopt the network architecture of MineAgent, which uses the MineCLIP image encoder to process image observations and MLPs to encode other information such as pose. We introduce an additional branch to encode the confidence map and fuse these features through concate-nation. The policy network takes this fused multi-modality feature as input and outputs action distribution. Details regarding the policy network's architecture are available in Appendix B.2. We use PPO as the base RL algorithm and train the agent with reward $r_t = r_{env} + \\lambda r_f$, where $r_{env}$ denotes the environmen-tal reward and $\\lambda$ is a hyperparameter controlling the weight of the focal reward. According to the empirical results in Appendix E.2, we simply set $\\lambda = 5$ for all experiments in the paper as we do not want to bother tuning this hyperparame-ter. We employ the multi-task RL paradigm, where the agent is trained to finish tasks in a predefined instruction set. Unlike typical multi-task reinforcement learning, our agent's learning objective is to not only master the training tasks but also to understand the mapping between the confidence map and the target object within the image observation, in order to perform zero-shot generalization to novel instructions involving unseen target objects."}, {"title": "5 Experiments", "content": "We conduct experiments in MineDojo , a Minecraft simulator that offers di-verse open-ended tasks. Firstly, we perform single-task experiments to evaluate the effectiveness of our proposed focal reward. Then we extend our evaluation to multi-task experiments and examine the performance of COPL on multiple instructions. Lastly, but most importantly, we investigate the zero-shot gener-alization ability of COPL when confronted with instructions containing unseen targets. Details about Minecraft environments and RL hyperparameters in our experiments are described in Appendix B.1 and Appendix E.1, respectively."}, {"title": "5.1 Single-Task Experiments", "content": "Our single-task evaluation consists of tasks learning four challenging basic skills : hunt a cow, hunt a sheep, hunt a pig, and hunt a chicken. In each task, the agent spawns in plains biome alongside several animals. The agent will receive a reward from the environment if it successfully kills the target animal. The difficulty of these basic skills lies in that animals, once attacked, will attempt to flee, requiring the agent to keep chasing and attacking the target animal. More details about the Minecraft task settings are available in Appendix C.1.\nEvaluation. We compare our focal reward with the following three baselines: (1) MineCLIP reward based on the similarity score between image ob-servations and the instruction \"hunt a {animal} on plains with a diamond sword\"; (2) NDCLIP reward , an intrinsic reward for exploration that mea-sures the novelty of observation's MineCLIP embedding; (3) Sparse reward, training the agent with the environmental reward only. Results are reported in Table 1, including mean and variance, calculated from evaluating four models that are each trained with a unique random seed and the same number of en-vironment steps (the same applies hereinafter). We can observe that only our focal reward leads to the mastery of all four skills by guiding the agent to con-sistently approach the target. In contrast, the MineCLIP reward fails because it cannot capture the distance between the agent and the target, offering limited benefit to these tasks. The failure of NDCLIP reward suggests that exploration provides minimal assistance in learning these challenging skills due to the huge observation space of Minecraft. These methods' learning curves on each task are available in Appendix C.2. We also report results on harvest skill learning and additional analysis in Appendix C.3."}, {"title": "5.2 Multi-Task and Generalization Experiments", "content": "We conduct multi-task experiments to verify the effectiveness and zero-shot gen-eralization capability of COPL. Given that tasks in Minecraft require different behavior patterns, we design two task domains, the hunting domain and the harvest domain. The hunting domain consists of four instructions: \"hunt a cow\", \"hunt a sheep\", \"hunt a pig\", and \"hunt a chicken\". These tasks share a common behavior pattern: repeatedly approach the target, aim at it, and at-tack. The harvest domain also contains four instructions: \"milk a cow\u201d, \u201cshear a sheep\", \"harvest a flower\", and \"harvest leaves\". Tasks in the harvest domain are individually easier than those in the hunting domain but demand disparate behavior patterns. For example, \"harvest a flower\" requires the at-tack action while the other tasks require the use action. More details about the task settings in multi-task experiments are available in Appendix D.1.\nEvaluation. We compare COPL with two language-conditioned reinforcement learning (LCRL) baselines, both taking language as input : (1) LCRL[t], which utilizes the target embedding encoded by the MineCLIP text encoder as input, sharing the same information as COPL albeit in language; (2) LCRL[i], which utilizes the instruction embedding encoded by the MineCLIP text encoder as input. We also evaluate (3) One-Hot, a naive multi-task base-line, using a one-hot vector as the task indicator. All these methods are trained with the focal reward and the only difference is their conditioning task represen-tations. In the hunting domain, as shown in Table 3 (upper), COPL significantly outperforms other baselines, indicating that the confidence map provides a more accessible and informative task representation compared to the language embed-ding and one-hot vector, respectively. In contrast, the harvest domain presents a different picture. As illustrated in Table 4 (upper), all methods achieve sim-ilar performance. These results suggest that when tasks become easy enough, the impact of the task representation's complexity diminishes. These methods' learning curves on each task are available in Appendix D.2. We also evaluate two recent Minecraft basic skill models trained via imitation learning, and STEVE-1 . Note that our intention is not to directly compare the perfor-mance of COPL with that of these two models, given the significant differences in their training paradigms and scopes of data. Rather, their evaluations only serve as a reference here due to the lack of alternative Minecraft foundation models trained via RL.\nGeneralization. Given that the two domains involve distinct behavior patterns, we conduct separate evaluations to assess the zero-shot generalization ability over target objects of these models trained in the hunting domain and the harvest domain. We test the hunting models with four novel instructions involving un-seen animals: \u201chunt a llama", "hunt a horse": "hunt a spider", "hunt a mushroom cow": "The results in Table 3 (lower) show that COPL effectively transfers the learned skill to unseen targets, achieving high success rates. As we set the episode to be terminated if any animal is killed, the high success rates also prove COPL's ability to distinguish the target from other animals, rather than indiscriminately attacking them. Detailed precision of each method is re-ported in Appendix D.3. STEVE-1 shows poor performance across all hunting tasks except", "spider": "We suppose that its base model, VPT , pos-sesses a strong prior on killing specific animals like spiders and heavily affects the behavior of STEVE-1 on following other hunting instructions. achieves relatively higher success rates on", "cow": "hunt a sheep", "hunt a pig": "ue to these tasks being in its training set. Its lower performance on other tasks indicates its limitations in generalization ability."}, {"title": "6 Discussion", "content": "Beyond Object-Centric Tasks. We focus on object-centric tasks in this paper, employing an intrinsic reward function to guide a learning agent towards the tar-get. However, not all tasks in Minecraft necessitate approaching the target, such as the two Creative tasks, dig a hole and lay the carpet, adopted in . We explore training separate single-task policies using the focal reward with targets, hole and carpet, respec-tively. Results in Figure 5 show effective guidance to-wards task completion. Nev-ertheless, for zero-shot gen-eralization, we observe that both our hunting and har-vest models show limited performance on dig a hole due to its different behav-ior pattern from the training tasks and the difficulty in selecting a consistent target dirt block. Details on results of the two tasks are further discussed in Appendix G.\nLimitations. One limitation of our method is the difficulty in defining a target for COPL to condition on, i.e., for the agent to aim at, when facing non-object-centric tasks, such as dig a hole and build a house. Solving such creative and building tasks may entail generative models . Additionally, our method compromises the theoretical generality of MineCLIP due to not consider-ing the action in instructions and only grounding target objects. Consequently, our method significantly improves its practicality in object-centric tasks, as ver-ified in our experiments. Future work could focus on grounding language that describes actions and learning tasks requiring more complicated manipulation."}, {"title": "7 Conclusion", "content": "We propose COPL, a novel approach designed to address object-centric tasks and perform zero-shot object-level generalization in Minecraft. Our method effec-tively transfers the wealth of vision-language knowledge encoded in MineCLIP into RL via reward designing and task representation. By comprehensive evaluations, we prove COPL's effectiveness in acquiring multiple basic skills and its generalization ability over target objects indicated in instructions, enabling it to apply the learned skills to follow unseen instructions that involve objects beyond the training tasks. Our work demonstrates the potential of integrating multimodal models, such as VLMs, into RL. Our method can be applied to other similar open-world environments by grounding natural language instructions in visual data and guiding the agent toward targets likewise. We hope COPL could contribute to the development of agents capable of understanding and respond-ing to natural language instructions."}, {"title": "A Segmentation Details", "content": ""}, {"title": "A.1 Extracting Targets via LLM", "content": "In order to extract the target name from a natural language instruction, we resort to GPT-4 . We input the following prompt:\nYou are a helper agent in Minecraft. I will describe a task, and your job is to identify the specific entity (name of blocks, anim-als, or items) that I need to directly interact with to complete this task. This means you should tell me what entity I need to aim at with the crosshair in the center of my screen, assuming I have all the necessary items and tools in my inventory. Please provide the name of the target entity.\nRequirements:\n1. For tasks with multiple possible solutions or those requiring additional context, provide the most straightforward or common solution.\n2. Only output the target entity after \"Target: \", DO NOT add other examples, descriptions, or explanations.\nFor example:\nTask: \"Obtain 8 obsidian blocks with a diamond pickaxe and a water bucket.\"\nThinking: \"To mine obsidian, the player must aim their crosshair at an obsidian block and use a diamond or netherite pickaxe on it. Ob-sidian is created when flowing water touches a lava source block. Therefore, the entity to aim at for this task is obsidian.\"\nTarget: \"Obsidian\"\nTask: \"Obtain milk with an empty bucket.\"\nThinking: \"To fill a bucket with milk, the player must aim their crosshair at a cow and use the empty bucket on it. Therefore, the entity to aim at for this task is a cow.\"\nTarget: \"Cow\"\nWe try some instructions without telling the name of the true target entity:\nTask: \"Harvest wool\"\nThinking:\nGPT-4:\nTo harvest wool in Minecraft, you need to aim your crosshair at a sheep and use shears on it. This will allow you to collect wool without harming the sheep.\nTarget: \"Sheep\"\nWe:\nTask: \"Collect beef\"\nThinking:\nGPT-4:\nTo collect beef in Minecraft, you need to aim your crosshair at a cow and use a weapon or your hand to kill it. Upon death, the cow will drop beef.\nTarget: \"Cow\"\nWe:\nTask: \"Get mutton as food\"\nThinking:\nGPT-4:\nTo get mutton as food in Minecraft, you need to aim your crosshair at a sheep and use a weapon or your hand to kill it. Upon death, the sheep will drop mutton.\nTarget: \"Sheep\"\nThese results show that GPT-4 understands the underlying relationship be-tween the target item (wool, beef, mutton) and the target animal (sheep, cow, sheep) based on its prior knowledge about Minecraft. Certainly, GPT-4 also works on instructions containing the target entity directly:\nWe:\nTask: \"Harvest a flower\"\nThinking:\nGPT-4:\nTo harvest a flower in Minecraft, you need to aim your crosshair at the specific flower you want to collect and either use your hand or any tool to break and collect it.\nTarget: \"Flower\"\nWe:\nTask: \"Hunt a cow\"\nThinking:"}, {"title": "A.2 Negative Words", "content": "For better segmentation, the negative word list should contain names of objects that frequently appear in Minecraft. To this end, we utilize the TF-IDF algorithm to select top-100 words from the subtitles of YouTube videos , excluding stop words like \"we\" and \"is\", as well as modal particles such as \"yeah\" and \"uh\". Additionally, we filter out verbs and some irrelevant nouns from the top-100 words to reduce noise. The final negative word list is shown below:\ndiamond, block, village, house, iron, farm, chest, dragon, redstone, water, tree, zombie, sword, stone, door, armor, lava, fish, portal, chicken, wood, wall, glass, cave, stair, bed, torch, fire, creeper, island, food, slab, book, head, button, apple, skeleton, potion, spider, egg, pickaxe, arrow, boat, horse, hopper, box, wool, table, seed, cow, brick, trap, dog, bow, dirt, roof, leaves, sand, window, bucket, coal, hole, pig, ice, bone, stick, flower, tower, sheep, grass, sky.\nFurthermore, in constructing text embeddings, we employ prompt engineer-ing to improve zero-shot ability on classification . Same as MaskCLIP , we utilize 85 prompt templates such as \"a photo of many {}.\". The mean of these embeddings is set to be the text embedding of the target. During segmen-tation, if the target object already exists in the list, it will be removed from the list in advance."}, {"title": "A.3 Segmentation Results", "content": "We provide more examples of confidence maps, as illustrated in Figure 6. Our modified MineCLIP effectively locates these target objects."}, {"title": "A.4 Off-the-shelf Object Detection Models", "content": "We choose one off-the-shelf object detection model, Grounded SAM , to evaluate its effectiveness in Minecraft. In order to conduct a fair side-by-side comparison between it and our method, we Google searched \"minecraft [object name"}]}