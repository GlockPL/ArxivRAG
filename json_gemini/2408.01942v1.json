{"title": "Visual Grounding for Object-Level Generalization in Reinforcement Learning", "authors": ["Haobin Jiang", "Zongqing Lu"], "abstract": "Generalization is a pivotal challenge for agents following natural language instructions. To approach this goal, we leverage a vision-language model (VLM) for visual grounding and transfer its vision-language knowledge into reinforcement learning (RL) for object-centric tasks, which makes the agent capable of zero-shot generalization to unseen objects and instructions. By visual grounding, we obtain an object-grounded confidence map for the target object indicated in the instruction. Based on this map, we introduce two routes to transfer VLM knowledge into RL. Firstly, we propose an object-grounded intrinsic reward function derived from the confidence map to more effectively guide the agent towards the target object. Secondly, the confidence map offers a more unified, accessible task representation for the agent's policy, compared to language embeddings. This enables the agent to process unseen objects and instructions through comprehensible visual confidence maps, facilitating zero-shot object-level generalization. Single-task experiments prove that our intrinsic reward significantly improves performance on challenging skill learning. In multi-task experiments, through testing on tasks beyond the training set, we show that the agent, when provided with the confidence map as the task representation, possesses better generalization capabilities than language-based conditioning. The code is available at https://github.com/PKU-RL/COPL.", "sections": [{"title": "1 Introduction", "content": "In the field of artificial intelligence, the ability of agents to understand and follow natural language instructions in an open-ended manner is crucial [4, 5, 10, 50]. However, the scope of training content for an agent's policy learning is always finite. Zero-shot generalization task which involves being instructed to interact with diverse objects not encountered during training, from the vast realm of human vocabulary, represents a pivotal step towards creating general artificial intelligence systems capable of adapting to a wide range of real-world scenarios [10,54]. As a popular open-ended 3D game, Minecraft serves as an ideal testbed for learning and evaluating generalization ability. At its core, Minecraft offers procedurally generated worlds with unlimited size and a large variety of tasks ranging from navigation and combat to building and survival [17, 58, 61, 63, 68]. Compared with canonical game environments such as Go [53], Atari [39], and StarCraft [16,57], Minecraft mirrors the complexity of real-world challenges and offers a wide range of objects and tasks with natural language instructions.\nGiven the finite data scope for the agent's policy learning, it is infeasible for the policy to directly comprehend the vast array of object names beyond the training set, which human language instructions might contain. To equip an agent with zero-shot generalization ability over objects, integration of a vision-language model (VLM) is a promising way [62]. A VLM aligns images and language vocabularies into the same feature space, bridging the gap between visual observations and natural language instructions. Therefore, it has the capability to ground the agent's unseen text, e.g., names of novel objects, in visual observations, enabling the agent to comprehend instructions not encountered during training. CLIP [44] emerges as a significant model and has become widely used [18, 37, 41, 46, 51, 52]. Recent works have extensively adopted CLIP as a foundation model for open-vocabulary object detection [19, 27,64] and open-vocabulary segmentation [12, 31, 47], leveraging its rich vision-language knowledge. Moreover, CLIP even exhibits remarkable segmentation capabilities and explainability without fine-tuning [30, 67].\nCLIP's visual grounding ability to perform segmentation inspires two routes for enhancing the agent's policy learning in Minecraft by transferring VLM knowledge into reinforcement learning (RL). The first approach is transfer via reward. The pixel area of the target object can be used as a surrogate for the distance between the agent and the target object. It can then serve as an intrinsic reward [2] to guide the agent towards the target object, thereby facilitating interaction. The second one is transfer via representation. The segmentation result"}, {"title": "2 Preliminary", "content": "Problem Statement. In this paper, we focus on object-centric tasks in Minecraft, where the agent is instructed to interact with diverse objects. By zero-shot object-level generalization, we mean that the agent is instructed to interact with objects beyond the training scope without fine-tuning during evaluation. To formalize, we denote the set of objects with which the agent learns to interact during the training phase as Ct, and the set of objects with which the agent is required to interact during the evaluation phase as Ce. To test the generalization ability of the agent, Ce consists of objects that are not in Ct. For example, during training, the agent learns to accomplish language instructions \"hunt a cow\" and \"hunt a sheep\". However, during evaluation, it will encounter instructions like \"hunt a horse\" or \"hunt a chicken\", where neither horse nor chicken appears in the instructions during training. Note that we do not consider generalization ability concerning unseen actions, which could be left as future work. Therefore, instructions during evaluation should have the same behavior patterns as those learned in training. For instance, when training with \"hunt sth.\" and \"harvest sth.\", testing with \"explore the world\" is not considered.\nZero-Shot Generalization. Our zero-shot object-level generalization in Minecraft is a specific form of broader zero-shot generalization (ZSG) defined in the contextual Markov decision process (CMDP) framework [26] in RL. While ZSG typically involves adapting to new environments or tasks, our focus is on enabling agents to follow instructions and interact with novel objects not encountered during training. This object-level adaptation aligns with ZSG's objective of performing effectively in unseen scenarios. In other words, the \u201ccontext\" in our case is specifically defined as the target objects indicated in instructions.\nMineCLIP for Minecraft RL. MineCLIP is a VLM pre-trained on Internet-scale Minecraft videos from YouTube [17], learning the alignment between video clips (16 frames) and natural language. Similar to CLIP [44], MineCLIP adopts a ViT [14] as the image encoder and a GPT [45] as the text encoder. The main difference between MineCLIP and CLIP is that MineCLIP takes as input a sequence of 16 images. Therefore, MineCLIP incorporates an additional module to aggregate the 16 embeddings generated by the image encoder. The proposed two mechanisms include a temporal transformer (MineCLIP[attn]) and direct average pooling (MineCLIP[avg]). In this paper, we choose the former as our base model due to its better performance in Programmatic tasks compared to the latter [17]. For RL in Minecraft, MineCLIP provides an intrinsic reward function Ri : G \u00d7 S16 \u2192 R, representing the similarity between the image observation sequence of the previous 16 steps [st\u221215,..., St-1, St] and the task prompt g."}, {"title": "3 Related Work", "content": "Minecraft Research. Broadly, challenges in Minecraft can be categorized into high-level task planning and low-level skill learning. For high-level planning, where agents must make decisions on which skills to employ sequentially based on the given instruction, the field has converged towards leveraging large language models (LLMs) [42, 58, 60, 61, 63, 68]. Regarding learning low-level skills, the difficulty lies in the absence of well-defined dense reward and a vast variety of objects to interact with in Minecraft. Unlike the convergence in high-level planning approaches, two distinct routes have emerged in low-level learning. The first route, represented by MineCLIP [17], utilizes the reward derived from the alignment between text and video clip or other manually designed rewards for RL [63]. The second one follows the principles of VPT [3], where skills are acquired through imitation learning based on large-scale demonstration [7,8,32]. Our work falls in the scope of low-level skill learning with RL.\nInstruction-Following RL. Language has been widely explored in goal-conditioned RL for its compositional structure [34]. This feature allows goal-conditioned policies to better capture the latent structure of the task space and generalize to unseen instructions that combine seen words [9,11,22,38,43]. With the development of LLM and VLM, language also becomes a means of providing intrinsic rewards in RL. The similarity or correlation between instructions and current states provides dense rewards to guide the agent's learning more effectively [15, 17, 28, 35]. Our work stands out by enabling the policy to generalize to instructions that contain previously unseen targets.\nCLIP for Embodied AI. CLIP [44] provides diverse usage for AI research. We categorize these applications into three areas: encoding, retrieving and locating. Encoding, the most common use of CLIP, leverages CLIP encoders to represent images and/or texts [24, 36, 52]. Our work also utilizes the MineCLIP image encoder to process raw image observations. Retrieving mostly involves navigation tasks, where CLIP assists in selecting the most matching image from a set based on the given instruction [6, 10, 13,50]. The most relevant usage to our work is locating, which applies methods like MaskCLIP [67] or GradCAM [49] on CLIP to determine the position of the specific object in images [18, 59, 66]. Based on the object location, agents can conduct planning with a depth detector [18] or imitation learning [59, 66]. In contrast, our work focuses on training agents via RL with information solely extracted from image observations, without any extra spatial information or demonstration."}, {"title": "4 Method", "content": "In this section, we detail the implementation of our COPL method in Minecraft. We introduce how to exploit the visual grounding capability of MineCLIP through modifications, enabling the segmentation of the target object indicated in the language instruction (Section 4.1). This process yields an object-grounded confidence map, where each element represents the probability of the specified target's presence. Based on this confidence map, we first implement VLM knowledge transfer via reward, presenting a simple but effective object-grounded intrinsic reward to guide the agent toward the target (Section 4.2). Then, we propose transfer via representation, where we integrate the confidence map into the policy as a task representation (Section 4.3). This integration equips the agent with zero-shot generalization ability over objects by grounding the novel object in a comprehensible visual representation."}, {"title": "4.1 Visual Grounding", "content": "Prior to segmentation, we must extract the correct target that the agent needs to interact with from the provided language instruction. Consider an example instruction: \"hunt a cow in plains with a diamond sword\". In this case, it is cow that should be extracted from the instruction as the target object, rather than plains or diamond sword, for the following segmentation. This can be easily accomplished by LLMs with appropriate prompts. Details on prompt designing and conversations with GPT-4 [1] can be found in Appendix A.1.\nIn the standard CLIP [44], the image encoder, a ResNet [21] or ViT [14], aggregates the visual features from all spatial locations through attention pooling. Recent works [30, 67] reveal that these features on each spatial location contain rich local information so that they can be used to perform zero-shot pixel-level predictions. In brief, the cosine similarities between these features and the outputs of the CLIP text encoder are also valid and informative. Con-cretely, MaskCLIP [67] makes use of the value-embedding of each spatial location in the last attention module, while CLIPSurgery [30] studies the feature of each spatial location in the final output and introduces an additional path. Inspired by MaskCLIP, we make adaptations to MineCLIP architecture to generate a confidence map for a specified target without fine-tuning.\nTo begin, we introduce the modification to the vision pathway of MineCLIP. We make changes to extract dense features from the last block of ViT. As illustrated in the rightmost part of Figure 2, the scaled dot-product attention in multi-head attention [56] module is removed, while the value-embedding transformation is retained. Then the transformed embeddings excluding that of CLS token are fed into the remaining modules within the ViT to obtain the final"}, {"title": "4.2 Transfer via Reward", "content": "The object-grounded confidence map of the target contains rich spatial infor-mation that can be utilized to facilitate RL through reward designing. The area occupied by the target in the image can serve as a proxy for estimating the distance to the target, based on the principle that the closer the target is to the agent, the larger its area in the image and vice versa. Therefore, a reward proportional to the area of the target would guide the agent towards the tar-get effectively. Additionally, we argue that the agent should be encouraged to aim at the target, i.e., adjust the perspective to center the target in the field of view. This would help the agent further stabilize its orientation and increase the chance of interacting with the target when it is close enough. In Minecraft, interaction can only occur when the crosshair in the center of the agent view aligns with the target. Moreover, when multiple target objects are present in the view, the agent should learn to focus on a single target rather than attempting to keep all of them in view. This could also be interpreted in a more general way, such as humans usually place the target at the center of the visual field for better perception and interaction.\nBased on these principles, we introduce an object-grounded intrinsic reward function named focal reward. At each time step t, it is computed as the mean of the Hadamard product between the current target confidence map mt, and a Gaussian kernel denoted as mk:\n$r_{f} = mean (m_{t} \\circ m_{k}).$\nHere, mt and mk share the same dimensions with height H and width W. Each element of the Gaussian kernel is defined as:\n$m_{j}^{k} = exp (-\\frac{(i - \\mu_{1})^2}{2\\sigma_{1}^{2}} - \\frac{(j - \\mu_{2})^2}{2\\sigma_{2}^{2}}),$\nwhere \u03bc1 = (H + 1)/2, \u03c31 = H/3, \u03bc2 = (W + 1)/2, and \u03c32 = W/3. This reward function is designed to be directly proportional to the area occupied by the target and inversely proportional to the distance between the target patches and the center of the view.\nAs noted in [7], the MineCLIP reward, which relies on the similarity between the agent's preceding image observations and the provided instruction, is uncor-related with the distance between the agent and the target. This phenomenon is"}, {"title": "4.3 Transfer via Representation", "content": "To train an instruction-following agent, the conventional practice involves di-rectly taking the natural language instruction as the task representation into the policy network [15, 22, 24, 40]. These instructions are typically encoded using a recurrent network or a language model such as BERT [23] and CLIP [44]. In contrast, we extract the target object from the instruction using GPT-4 [1] and subsequently convert it into a two-dimensional matrix, i.e., the confidence map. Our underlying assumption is that this two-dimensional object-grounded repre-sentation offers more intuitive and accessible information for the policy network compared to the intricate space of language embeddings. When facing an instruc-tion containing the name of a target object not encountered during training, our method grounds this novel text in the two-dimensional map, rendering it a comprehensible representation for the policy network. As a result, the agent can follow the guidance of the confidence map, navigate towards the novel target object, and finally interact with it.\nIn our implementation, we adopt the network architecture of MineAgent [17], which uses the MineCLIP image encoder to process image observations and MLPs to encode other information such as pose. We introduce an additional branch to encode the confidence map and fuse these features through concate-nation. The policy network takes this fused multi-modality feature as input and outputs action distribution. Details regarding the policy network's architecture are available in Appendix B.2. We use PPO [48] as the base RL algorithm and train the agent with reward $r_{t} = r_{env} + \\lambda r_{f}$, where $r_{env}$ denotes the environmen-tal reward and \u03bb is a hyperparameter controlling the weight of the focal reward. According to the empirical results in Appendix E.2, we simply set \u03bb = 5 for all"}, {"title": "5 Experiments", "content": "We conduct experiments in MineDojo [17], a Minecraft simulator that offers di-verse open-ended tasks. Firstly, we perform single-task experiments to evaluate the effectiveness of our proposed focal reward. Then we extend our evaluation to multi-task experiments and examine the performance of COPL on multiple instructions. Lastly, but most importantly, we investigate the zero-shot gener-alization ability of COPL when confronted with instructions containing unseen targets. Details about Minecraft environments and RL hyperparameters in our experiments are described in Appendix B.1 and Appendix E.1, respectively."}, {"title": "5.1 Single-Task Experiments", "content": "Our single-task evaluation consists of tasks learning four challenging basic skills [3,7,17]: hunt a cow, hunt a sheep, hunt a pig, and hunt a chicken. In each task, the agent spawns in plains biome alongside several animals. The agent will receive a reward from the environment if it successfully kills the target animal. The difficulty of these basic skills lies in that animals, once attacked, will attempt to flee, requiring the agent to keep chasing and attacking the target animal. More details about the Minecraft task settings are available in Appendix C.1.\nEvaluation. We compare our focal reward with the following three baselines: (1) MineCLIP reward [17] based on the similarity score between image ob-servations and the instruction \"hunt a {animal} on plains with a diamond sword\"; (2) NDCLIP reward [55], an intrinsic reward for exploration that mea-sures the novelty of observation's MineCLIP embedding; (3) Sparse reward, training the agent with the environmental reward only. Results are reported in, including mean and variance, calculated from evaluating four models that are each trained with a unique random seed and the same number of en-vironment steps (the same applies hereinafter). We can observe that only our focal reward leads to the mastery of all four skills by guiding the agent to con-sistently approach the target. In contrast, the MineCLIP reward fails because it cannot capture the distance between the agent and the target, offering limited benefit to these tasks. The failure of NDCLIP reward suggests that exploration provides minimal assistance in learning these challenging skills due to the huge observation space of Minecraft. These methods' learning curves on each task are available in Appendix C.2. We also report results on harvest skill learning and additional analysis in Appendix C.3."}, {"title": "5.2 Multi-Task and Generalization Experiments", "content": "We conduct multi-task experiments to verify the effectiveness and zero-shot gen-eralization capability of COPL. Given that tasks in Minecraft require different behavior patterns, we design two task domains, the hunting domain and the harvest domain. The hunting domain consists of four instructions: \"hunt a cow\", \"hunt a sheep\", \"hunt a pig\", and \"hunt a chicken\". These tasks share a common behavior pattern: repeatedly approach the target, aim at it, and at-tack. The harvest domain also contains four instructions: \"milk a cow\u201d, \u201cshear a sheep\", \"harvest a flower\", and \"harvest leaves\". Tasks in the harvest domain are individually easier than those in the hunting domain but demand disparate behavior patterns. For example, \"harvest a flower\" requires the at-tack action while the other tasks require the use action. More details about the task settings in multi-task experiments are available in Appendix D.1.\nEvaluation. We compare COPL with two language-conditioned reinforcement learning (LCRL) baselines, both taking language as input [22, 24, 34, 40]: (1) LCRL[t], which utilizes the target embedding encoded by the MineCLIP text encoder as input, sharing the same information as COPL albeit in language; (2) LCRL[i], which utilizes the instruction embedding encoded by the MineCLIP text encoder as input. We also evaluate (3) One-Hot, a naive multi-task base-line, using a one-hot vector as the task indicator. All these methods are trained with the focal reward and the only difference is their conditioning task represen-tations. In the hunting domain, as shown in (upper), COPL significantly outperforms other baselines, indicating that the confidence map provides a more accessible and informative task representation compared to the language embed-ding and one-hot vector, respectively. In contrast, the harvest domain presents a different picture. As illustrated in (upper), all methods achieve sim-ilar performance. These results suggest that when tasks become easy enough, the impact of the task representation's complexity diminishes. These methods' learning curves on each task are available in Appendix D.2. We also evaluate"}, {"title": "7 Conclusion", "content": "We propose COPL, a novel approach designed to address object-centric tasks and perform zero-shot object-level generalization in Minecraft. Our method effec-tively transfers the wealth of vision-language knowledge encoded in MineCLIP [17] into RL via reward designing and task representation. By comprehensive evaluations, we prove COPL's effectiveness in acquiring multiple basic skills and its generalization ability over target objects indicated in instructions, enabling it to apply the learned skills to follow unseen instructions that involve objects beyond the training tasks. Our work demonstrates the potential of integrating multimodal models, such as VLMs, into RL. Our method can be applied to other similar open-world environments by grounding natural language instructions in visual data and guiding the agent toward targets likewise. We hope COPL could contribute to the development of agents capable of understanding and respond-ing to natural language instructions."}, {"title": "A Segmentation Details", "content": ""}, {"title": "A.1 Extracting Targets via LLM", "content": "In order to extract the target name from a natural language instruction, we resort to GPT-4 [1]. We input the following prompt:\nYou are a helper agent in Minecraft. I will describe a task, and your job is to identify the specific entity (name of blocks, anim-als, or items) that I need to directly interact with to complete this task. This means you should tell me what entity I need to aim at with the crosshair in the center of my screen, assuming I have all the necessary items and tools in my inventory. Please provide the name of the target entity.\nRequirements:\n1. For tasks with multiple possible solutions or those requiring additional context, provide the most straightforward or common solution.\n2. Only output the target entity after \"Target: \", DO NOT add other examples, descriptions, or explanations.\nFor example:\nTask: \"Obtain 8 obsidian blocks with a diamond pickaxe and a water bucket.\"\nThinking: \"To mine obsidian, the player must aim their crosshair at an obsidian block and use a diamond or netherite pickaxe on it. Ob-sidian is created when flowing water touches a lava source block. Therefore, the entity to aim at for this task is obsidian.\"\nTarget: \"Obsidian\"\nTask: \"Obtain milk with an empty bucket.\"\nThinking: \"To fill a bucket with milk, the player must aim their crosshair at a cow and use the empty bucket on it. Therefore, the entity to aim at for this task is a cow.\"\nTarget: \"Cow\"\nWe:\nWe try some instructions without telling the name of the true target entity:"}, {"title": "A.2 Negative Words", "content": "For better segmentation, the negative word list should contain names of objects that frequently appear in Minecraft. To this end, we utilize the TF-IDF algorithm to select top-100 words from the subtitles of YouTube videos [17], excluding stop words like \"we\" and \"is\", as well as modal particles such as \"yeah\" and \"uh\". Additionally, we filter out verbs and some irrelevant nouns from the top-100 words to reduce noise. The final negative word list is shown below:\ndiamond, block, village, house, iron, farm, chest, dragon,\nredstone, water, tree, zombie, sword, stone, door, armor, lava,\nfish, portal, chicken, wood, wall, glass, cave, stair, bed,\ntorch, fire, creeper, island, food, slab, book, head, button,\napple, skeleton, potion, spider, egg, pickaxe, arrow, boat,\nhorse, hopper, box, wool, table, seed, cow, brick, trap, dog,\nbow, dirt, roof, leaves, sand, window, bucket, coal, hole, pig,\nice, bone, stick, flower, tower, sheep, grass, sky.\nFurthermore, in constructing text embeddings, we employ prompt engineer-ing to improve zero-shot ability on classification [44]. Same as MaskCLIP [67], we utilize 85 prompt templates such as \"a photo of many {}.\". The mean of these embeddings is set to be the text embedding of the target. During segmen-tation, if the target object already exists in the list, it will be removed from the list in advance."}, {"title": "A.3 Segmentation Results", "content": "We provide more examples of confidence maps, as illustrated in Figure 6. Our modified MineCLIP effectively locates these target objects."}, {"title": "A.4 Off-the-shelf Object Detection Models", "content": "We choose one off-the-shelf object detection model, Grounded SAM [25, 33], to evaluate its effectiveness in Minecraft. In order to conduct a fair side-by-side comparison between it and our method, we Google searched \"minecraft [object name] screenshot\" in the image tab, and selected the first two images that include objects and have them fully in the field of view. The evaluation objects includes pig, cow, sheep, mushroom cow, tree, flower, and horse. We follow the setting in the official demo to evaluate the effectiveness of Grounded SAM on detecting these objects in Minecraft. For both Grounded SAM and our modified MineCLIP in this evaluation, we use the same word list which consists of the seven evaluation objects and grass.\nThe detection results of the two methods are illustrated in Figure 7. For a more detailed evaluation, we quantify the number of objects present in each image, the number detected by Grounded SAM, and the number detected by our method. These quantitative results are summarized in. Across all images, there are 24 target objects. Grounded SAM can successfully identify 14 objects, which translates to a detection rate of 58.3%. In contrast, our method demonstrates a significantly higher efficacy, successfully detecting 22 of the 24 objects, achieving a detection rate of 91.7%. There are two failures in our method. One is the sunflower in the bottom-right corner of the first flower image, and the"}, {"title": "B Policy Learning Details", "content": ""}, {"title": "B.1 Observation Space and Action Space", "content": "The observation space adopted in our experiments consists of RGB, compass, GPS, voxels, and biome index. The shape and description of each modality are listed in Table 6. We simplify the original action space of MineDojo [17] into a 2-dimensional multi-discrete action space. The first dimension contains 12 discrete actions about movement: no_op, move forward, move backward, move left, move right, jump, sneak, sprint, camera pitch -30, camera pitch +30, camera yaw -30, and camera yaw +30. The second dimension includes 3 discrete actions about interacting with items: no_op, attack, and use."}, {"title": "B.2 Network Architecture", "content": "The input of COPL agent includes observations from the environment listed in Table 6, the agent's action taken at the last time step at-1, and the confidence map. As illustrated in all inputs except the RGB image are processed by MLPs with three hidden layers and ReLU activation. In this step, voxels, biome index, and previous action are first embedded into dense vectors. The RGB image is processed using the MineCLIP image encoder to generate an embedding. All these processed features are concatenated and processed by an MLP with one hidden layer and ReLU activation. Then a GRU layer is implemented to integrate the historical information. The policy head and the value head take as input the output of GRU and both process it using an MLP with three hidden layers and ReLU activation. The policy head generates the distribution of actions, and the value head outputs the estimated value of the current state. Some variants are as follows: (1) Single-task model: In single-task experiments, the agent does not take as input the confidence map; (2) LCRL: The branch of confidence map is replaced by the MineCLIP text encoder processing the target name or the instruction; (3) One-Hot: The branch of confidence map is replaced by an MLP processing the one-hot vector which indicates the index of the current task. The MLP has one hidden layer with size 32 and ReLU activation."}, {"title": "C Single-Task Experiments", "content": ""}, {"title": "C.1 Settings", "content": "Our single-task experiments include four tasks: hunt a cow, hunt a sheep, hunt a pig, and hunt a chicken. The parameters we used to make environ-ments in MineDojo are listed in Table 7. In all tasks, the agent spawns in plains"}, {"title": "C.2 Learning Curves", "content": "Learning curves of four methods on for Minecraft tasks are shown in Each curve shows the mean success rate of four runs with different seeds and shaded regions indicate standard error (the same applies hereinafter). We can observe that only our focal reward leads to the mastery of all four skills by guiding the agent to consistently approach the target."}, {"title": "C.3 Additional Experiments", "content": "We conduct additional single-task experiments on three harvest tasks includ-ing milk a cow, shear a sheep, and chop a tree, where MineCLIP reward achieves nonzero success rates [17]. The environment parameters for each task"}, {"title": "E Hyperparameters", "content": ""}, {"title": "E.1 PPO Hyperparameters", "content": "In our experiments, we use PPO [48] as our base RL algorithm. lists the hyperparameters for PPO across all tasks. Unlike MineAgent [17], our imple-mentation does not include self-imitation learning and action smoothing loss. We find that vanilla PPO is able to achieve high performance in our experiments. For single-task experiments, we train RL models for 1,000,000 environment steps. For multi-task experiments in the hunting domain, we train RL models for 2,000,000 environment steps. For multi-task experiments in the harvest domain, we train RL models for 1,500,000 environment steps."}, {"title": "E.2 Intrinsic Reward Coefficient", "content": "To determine the optimal scale of intrinsic reward that can effectively guide reinforcement learning while avoiding conflicts with the environmental reward, we conduct an experiment to evaluate the performance of our focal reward with different \u03bb values. Figures 13a and 13b illustrates the performance of our focal reward with different \u03bb, including 0.5, 5, and 50, on hunt a cow and hunt a sheep. Focal reward with \u03bb = 5 outperforms \u03bb = 50 and \u03bb = 0.5 on two tasks. Therefore, we consistently set \u03bb = 5 for all experiments in the main text.\nRegarding the MineCLIP reward, we set the coefficient to 1.0, following the original setting of MineAgent in [17]. The optimal coefficient of ND reward in [55] for find task is 0.003, and its sparse environmental reward is 1.0. Considering the environmental reward is set to 100 in our experiments, we decided to increase the coefficient for NDCLIP from 0.003 to 0.3 in our implementation."}, {"title": "E.3 Gaussian Kernel", "content": "The introduction of a Gaussian kernel is to guide the agent to center a target object within its field of view. The Gaussian kernel should create a high contrast between the center and the edge, as well as between the edge and areas outside the field of view. Therefore, the variance of the Gaussian kernel would influence the performance of the focal reward. To evaluate the impact of different vari-ances, we conduct an experiment with \u03c3 = (H/5,W/5), \u03c3 = (H/3, W/3), and \u03c3 = (H/2, W/2). As illustrated in Figures 13c and 13d, \u03c3 = (H/3, W/3) outper-forms the others. We suppose that a wider Gaussian kernel with \u03c3 = (H/2, W/2) fails to provide sufficient contrast between the center and the edge. Conversely, a narrower Gaussian kernel with \u03c3 = (H/5, W/5) cannot provide sufficient con-trast between the edge and areas outside the field of view."}, {"title": "F Baselines Implementation", "content": "MineCLIP. We adopt the provided prompt templates in MineDojo to de-sign task prompts for MineCLIP reward computation in single-task experi-ments. For hunting tasks, we use the prompt \"hunt a {animal} on plains with a diamond sword\". For additional harvest tasks in Appendix C.3, we use the prompts \u201cobtain milk from a cow in plains with an empty bucket\", \"shear a sheep in plains with shears\", and \"chop trees to obtain log with a golden axe\", respectively.\n[7]. We use the released plains model for evaluation. The goal is set to be the name of the target animal."}, {"title": "G Creative Tasks", "content": "For dig a hole, the agent spawns with a diamond shovel; for lay the carpet, the agent spawns with 64 carpets. For each task, we train an agent with MineCLIP reward and an agent with our focal reward. The prompts used to calculate MineCLIP reward are \"dig a hole\" and \"put carpets on the floor\", respec-tively. We run the trained models in the environment and record the agent's"}]}