{"title": "TOMCAT: THEORY-OF-MIND FOR COOPERATIVE AGENTS IN TEAMS VIA MULTIAGENT DIFFUSION POLICIES", "authors": ["Pedro Sequeira", "Vidyasagar Sadhu", "Melinda Gervasio"], "abstract": "In this paper we present ToMCAT (Theory-of-Mind for Cooperative Agents in Teams), a new framework for generating ToM-conditioned trajectories. It combines a meta-learning mechanism, that performs ToM reasoning over teammates' underlying goals and future behavior, with a multiagent denoising-diffusion model, that generates plans for an agent and its teammates conditioned on both the agent's goals and its teammates' characteristics, as computed via ToM. We implemented an online planning system that dynamically samples new trajectories (replans) from the diffusion model whenever it detects a divergence between a previously generated plan and the current state of the world. We conducted several experiments using ToMCAT in a simulated cooking domain. Our results highlight the importance of the dynamic replanning mechanism in reducing the usage of resources without sacrificing team performance. We also show that recent observations about the world and teammates' behavior collected by an agent over the course of an episode combined with ToM inferences are crucial to generate team-aware plans for dynamic adaptation to teammates, especially when no prior information is provided about them.", "sections": [{"title": "1 Introduction", "content": "Recent advances in Artificial Intelligence (AI) present an opportunity to greatly improve collaboration between humans and computers to address complex tasks. In particular, AI has the ability to easily generate long courses of action that"}, {"title": "2 Related Work", "content": "Multiagent Reinforcement Learning (MARL): To create agents that can adapt to a diverse set of teammates, one option is to use Reinforcement Learning (RL) to train policies that maximize a reward function while treating teammates as an element of the dynamic environment. E.g., in independent Q-learning (IQL) (Lee et al., 2022) an agent relies only\nThroughout the paper we use \u201cteams\u201d to refer to groups of agents in general. While in our experiments we focus on collaboration, TOMCAT's mechanisms are agnostic to the alignment of the agents' self interests, i.e., can be used in collaborative or competitive settings. Similarly, we use \"teammate\" to refer to any other agent in the group."}, {"title": "3 TOMCAT", "content": "As depicted in Fig. 1, our approach uses two models to generate ToM-conditioned behavior for online prediction and adaptation in the context of teams. The Theory-of-Mind network (ToMnet, left) is responsible for making predictions about teammates' behavior conditioned on their observed behavior, while a Multiagent Diffusion network (MADiff, right) generates agent trajectories for all team members conditioned on the ToM reasoning."}, {"title": "3.1 Preliminaries", "content": "We model the problem of an agent modeling and adapting to the behavior of others using the formalism of interactive partially observable Markov decision processes (I-POMDPs) (Han and Gmytrasiewicz, 2019). I-POMDP allows unifying the ToMnet (Rabinowitz et al., 2018) and MADiff (Zhu et al., 2024) approaches\u2014it supports modeling different types or families of agents in the same task as in the ToMnet approach, and characterizing the decision-making problem of individuals in multiagent settings as the MADiff approach. However, it departs from the latter in that it models self-interested agents and is thus applicable to general-sum tasks. Formally, an I-POMDP for an agent i is defined as the tuple \\(M = (\\check{S}_i, A, T_i, R_i, \\check{N}_i, O_i)\\), where \\(\\check{S}_i = S \\times N_{Ag-1} M_j\\) is the set of interactive states, where S is the physical environment state space and \\(M_j\\) is the set of possible models for agent j representing its internal state, A is the joint action space, \\(R_i : \\check{S}_i \\times A \\rightarrow \\mathbb{R}\\) is agent i's reward function, \\(T_i : S \\times A \\rightarrow S\\) the transition function, \\(\\Omega_i\\) the observation space, and \\(O_i : S \\times A_i \\rightarrow \\Omega_i\\) the conditional observation function.\nFor a given task, we assume a family of agents \\(A = \\bigcup_i A_i\\), akin to player types in Bayesian game theory (Harsanyi, 1967), where each agent type is characterized by some private attribute vector, i.e., unobservable by others, containing all information relevant to its decision making. Without loss of generality, here we consider individual reward functions of the form \\(R_i(s_t) = \\Phi(s_t)^T\\theta_i\\), consisting of linear combinations of (physical) state features, \\(\\Phi_k : S \\rightarrow \\mathbb{R}\\), \\(k = 1, ..., |\\Phi|\\), weighted by some weight vector, \\(\\theta_i\\). Therefore, we consider that agent types can be succinctly expressed via a profile function, \\(\\psi : A \\rightarrow \\mathbb{R}^n\\), and in our experiments set \\(\\psi_i(A_i) := \\theta_i\\), i.e., each agent type is motivated to optimize over different aspects of the same general task. For simplicity, we write \\(\\psi_i\\) to denote agent i's profile. As for the models \\(m_j \\in M_j\\) of others, here we allow any representation of teammate j's private information, including predictions over its profile, beliefs or future behavior.\nFor training the different modules, we consider observed trajectories of the form \\(t_i = \\{(o_t^i, a_t^i)\\}_{t=0}^{T-1}\\), where \\(o_t^i = O_i(a_t^i, s_t)\\) and \\(a_t^i \\sim \\pi_i(o_t^i)\\) are respectively agent i's observation over the global state, \\(s_t\\), and its action, taken at trajectory step t. We assume that an agent's observations collected through \\(O_i\\) include partial information about its teammates' overt behavior that will be used to make predictions about them during task performance."}, {"title": "3.2 Theory-of-Mind Reasoning", "content": "As mentioned earlier, our first goal is to create a mechanism capable of inferring the motivations and intent of teammates from their observed behavior. To achieve that, we follow the approach in (Rabinowitz et al., 2018) which aims to develop a machine ToM by training a neural network called a ToMnet to model the mental states and behaviors of other agents through meta-learning. The ToMnet learns to predict agents' future actions and infer their goals and beliefs, including false beliefs, by observing their behavior in an environment of interest.\nAs outlined in Fig. 1 (left), the ToMnet consists of three modules:\nCharacter Net: processes trajectories from past episodes of an observer agent, i, acting in the context of a team, with the goal of characterizing the general behavioral tendencies of teammates \\(j \\in \\mathcal{N}_{-i}\\). Given past trajectories, \\(\\mathcal{N}_{past}\\), and the observer's profile, \\(\\psi_i\\), it outputs a character embedding, as \\(\\text{e}_{char, i} := \\frac{1}{\\mathcal{N}_{past}} \\sum_{p=1}^{\\mathcal{N}_{past}}f_{\\theta}(\\psi_i, \\tau_{i,p})\\), where \\(f_{\\theta}\\) is a learned neural network. The length of past trajectories is denoted by \\(T_{past}\\).\nMental Net: focuses on the current episode's trajectory so far to provide the necessary context about teammates' behavior to infer their current mental state. It computes a mental embedding as \\(\\text{e}_{ment,i} := g_{\\phi}(\\psi_i, [\\tau_i]_{T^{cur}:t-1}, \\text{e}_{char,i})\\), where \\([\\tau_i]_{T^{cur}:t-1}\\) comprises agent i's last \\(T_{cur}\\) timesteps in the current trajectory and \\(g_{\\phi}\\) is another learned neural network.\nPrediction Net: uses the outputs of the Character and Mental Nets to update the observer's beliefs about the teammates' behavior and their underlying characteristics. In particular, the Prediction Net is capable of inferring each teammate's next-step action distribution, \\(\\hat{\\pi}_j := \\Delta(A_i)\\), and the distribution over the sign of its profile elements, \\(\\hat{\\psi}_j := \\Delta(\\{-1,0,1\\}^n)\\), given the output of \\(d_{\\psi}(\\psi_i, o_t^i, \\text{e}_{char, i}, \\text{e}_{ment,i})\\), where \\(d_{\\psi}\\) is another learned neural network serving as the shared torso for the different prediction heads. The Prediction Net can also estimate other statistics of teammates' future behavior, such as the likelihood of picking up objects in the environment or state occupancies.\nThe ToMnet is parametrized by \\(\\theta, \\phi\\) and \\(\\psi\\) and is trained end-to-end. It leverages meta-learning to construct a general ToM (the model's parameters encode a prior for modeling diverse agent types) while dynamically updating an agent-specific ToM (the model's predictions update a posterior over the characteristics of a particular teammate from recently\nSuch information does not include teammates' reward functions, policies or identifiers.\nIn contrast to (Rabinowitz et al., 2018), we average the embedding for each past trajectory when computing \\(\\text{e}_{char}\\) to allow for variable number of past trajectories."}, {"title": "3.3 Multiagent Diffusion Policies", "content": "The second component of our architecture depicted in Fig. 1 (right) allows for learning multiagent policies conditioned on the ToM information provided by the ToMnet. The idea is to learn policies that can be used by an agent to adapt to its teammates online, while interacting with them to perform some task. To achieve that, we adopted Multiagent Diffusion (MADiff) (Zhu et al., 2024), a framework that extends single-agent diffusion policies (Janner et al., 2022; Ajay et al., 2023) for flexible behavior planning in teams. MADiff trains a conditional generative behavior model from diverse data and treats multiagent planning as sampling from that model. Furthermore, we adopt a decentralized execution paradigm, where we assume that each agent makes its own decision without any communication with other agents, based only on local information from its own observations.\nFollowing diffusion probabilistic models (Sohl-Dickstein et al., 2015; Ho et al., 2020), data generation is modeled with a predefined iterative diffusion (forward) process, \\(q(\\tau_k | \\tau_{k-1}) := \\mathcal{N}(\\tau_k; \\sqrt{\\alpha_k} \\tau_{k-1}, (1-\\alpha_k)I)\\), and a trainable denoising (reverse) process, \\(p_{\\theta}(\\tau_{k-1} | \\tau_k) := \\mathcal{N}(\\tau_{k-1} | \\mu_{\\theta}(\\tau_k, k), \\Sigma_k)\\), where k is a diffusion/denoising step, \\(\\mathcal{N} (\\mu, \\Sigma)\\) denotes a Gaussian distribution with mean \\(\\mu\\) and variance \\(\\Sigma\\), and \\(\\alpha_k \\in \\mathbb{R}\\) determines the variance schedule.\nMADiff is defined as a multiagent offline learning problem, where we have access to a static dataset, \\(\\mathcal{D}\\), containing joint trajectories of the form \\(\\tau := [\\tau^0, ..., \\tau^{N_{Ag}-1}]\\). Since action sequences tend not to be smooth (Ajay et al., 2023), we only model observed state sequences as in (Zhu et al., 2024). Namely, given a joint trajectory \\(\\tau \\in \\mathcal{D}\\), we sample sequences of joint observations, \\(\\hat{\\tau} := [(\\text{o}_t^{0},...,\\text{o}_{t+H-1}^{0}), ..., (\\text{o}_t^{N_{Ag}-1},...,\\text{o}_{t+H-1}^{N_{Ag}-1})]\\), where \\(t \\sim U(1,T)\\) is the time at which an observation was made by agent i, sampled uniformly at random, and H is the diffusion horizon. To retrieve actions from generated plans we use \\(a^i_t := I_{\\xi}(\\text{o}^i_t, \\text{o}^i_{t+1})\\), where \\(I_{\\xi}\\) is an inverse dynamics model shared amongst agents that is independent from but simultaneously trained with \\(p_{\\theta}\\).\nTo condition trajectory generation on ToM reasoning, we sample from perturbed distributions of the form \\(p_{\\delta}(\\hat{\\tau}) \\propto p_{\\delta}(\\hat{\\tau}) \\gamma(\\tau)\\), where \\(\\gamma(\\tau)\\) is an additional input to the diffusion model encoding particular properties of joint trajectory \\(\\tau\\). Following the decentralized approach, in ToMCAT perturbations are made from the perspective of an observer agent i, which we denote using \\(\\gamma(\\tau_i)\\), and can include any of the following information:\nProfile: the observer agent's profile, \\(\\psi_i\\), for which the joint trajectories are being generated.\nCharacter: the embedding provided by the Character Net characterizing teammates \\(j \\in \\mathcal{N}_{-i}\\), i.e., \\(\\text{e}_{char,i} (\\{\\tau_{i,p}\\}_{p=1}^{\\mathcal{N}_{past}})\\), where \\(\\tau_{i,p} \\in \\mathcal{D}\\) are other trajectories sampled from the dataset in which agent i interacted with the same (types of) teammates.\nMental: the Mental embedding characterizing teammates' mental states, i.e., \\(\\text{e}_{ment,i}([\\tau_i]_{0:t-1})\\).\nReturns: any measure of behavior optimality that we wish to condition trajectory generation on, e.g., the agent's individual reward, \\(R_i(\\tau_i)\\), or the task reward common to all agents.\nIn situations where we want to condition trajectory generation on multiple factors, we concatenate them to form \\(\\gamma(\\tau_i)\\). In addition, we follow (Zhu et al., 2024; Ajay et al., 2023; Janner et al., 2022) and constrain trajectory generation on the agent's current observation, \\(\\text{o}_t^i\\), and on the C previous steps, in a process akin to pixel in-painting in image generation (Sohl-Dickstein et al., 2015). This leads to an augmented trajectory \\(\\hat{\\tau} := [(\\text{o}_{t-C}^{i},...,\\text{o}_{t-1}^{i}, \\text{o}_t^{i},...,\\text{o}_{t+H}^{i})]_{i=0}^{N_{Ag}}.\nThe MADiff model is parametrized by \\(\\delta\\) and \\(\\xi\\) which are simultaneously trained by sampling trajectories from dataset \\(\\mathcal{D}\\) and randomly selecting an agent to be the \u201cobserver\u201d, while other agents are the teammates. Since we follow a decentralized paradigm, the aforementioned in-painting information for the teammates is zeroed-out since the observer does not have access to their observations. Notwithstanding, MADiff is trained to predict the future observations of all agents given its local observations."}, {"title": "3.4 Dynamic Replanning", "content": "To generate plans (trajectories), we use classifier-free guidance (Ho and Salimans, 2022) with low temperature sampling, where we assume an online setting where an agent i is both observing and interacting with teammates in the context of some task. Generating plans using diffusion models can be computationally intensive (Janner et al., 2022; Ajay et al., 2023), so Janner et al. (2022) proposed a warm-start solution where a plan generated for previous timesteps is reused by first diffusing it for a small number of steps\u2014a fraction of the original diffusion steps\u2014and then denoising the resulting noise for the same number of steps, thereby generating a new plan. However, this approach assumes that plans taken at"}, {"title": "4 Experiments & Results", "content": "In this section, we detail the experiments carried out to assess the usefulness of TOMCAT in generating dynamic, ToM-conditioned strategies for online adaptation to diverse teammates in collaborative tasks. We detail our experimental scenario, how we trained various RL agents to generate ground-truth behavioral data, and how we trained ToMnet and MADiff models therefrom. We then detail the different experiments and analyze the main results."}, {"title": "4.1 Data Collection", "content": "To test our approach, we designed a scenario in the Overcooked-AI simulation platform (Carroll et al., 2019), which is a benchmark domain for research in human-AI collaboration. The custom scenario used in our experiments is illustrated in Fig. 2. Briefly, the task objective is for a team of \\(N_{Ag} = 2\\) agents to collaborate in order to maximize the number of soups served within a time limit. Agents need to collect onions, place them in pots, wait for soups to cook, then serve them in the corresponding station. Each agent has | \\(A_i\\) |= 6 available actions at each timestep: they can move in four cardinal directions, perform a \u201cno-op\u201d action, or \u201cinteract\u201d with an object in the environment (e.g., pickup or place items). Agents' initial locations in the environment are randomized. Our custom scenario, inspired by the environment in (Wu et al., 2021), adds a bottleneck (middle counter) to increase the usefulness of specialized roles in the task and decrease the likelihood that agents can perform the task on their own, requiring effective coordination for success.\nAs mentioned earlier, here we consider reward functions consisting of linear combinations of state features, and set the profile for an agent i as \\(\\psi_i := \\theta_i\\), where \\(\\theta_i\\) is the vector of weights associated with each feature. We designed a"}, {"title": "4.2 Model Training", "content": "We trained the ToMnet in a supervised manner, where we first sampled a profile, \\(\\psi_i \\sim \\Psi\\), where \\(\\Psi\\) is the set of possible profiles listed in Table A.1. We set agent i to be the observer, and sampled a joint trajectory, \\(\\tau \\sim \\mathcal{D}\\) where the team, \\(\\mathcal{N} := \\{i, j\\}\\), is composed of the observer and a teammate, j. We then selected a timestep, \\(t \\sim U(1, T)\\), select the \u201ccurrent\u201d observation, \\(o_t^i\\), and set the \u201ccurrent\u201d trajectory to \\([\\tau_i]_{t-T^{cur}:t-1}\\), where \\(T_{cur}\\) = 10. We further selected"}, {"title": "4.3 Model Analysis", "content": "We now provide some insights from training the ToMnet and MADiff models. Regarding the ToMnet, Fig. 3b plots the 2D t-SNE representations of the character embeddings, \\(\\text{e}_{char}\\), computed for the trajectories in D. We can see that the Character Net learns different internal representations for different profile \u201cfamilies\u201d denoting distinct behavior tendencies in the task. As in (Rabinowitz et al., 2018), this shows that the ToMnet learns useful representations about teammates' characteristics. However, because we condition ToMnet predictions on the observer's own profile, our results further show that it distinguishes between different types of interacting partners, who may require different types of collaboration. Fig. 3c plots the prediction accuracy of the teammate's next action when varying the length of past trajectories, \\(T_{past}\\) and the number of previous steps in the current trajectory \\(T_{cur}\\), provided to the ToMnet, while fixing \\(\\mathcal{N}_{past}\\) = 1. We observe that with a prior trajectory of only approximately \\(T_{past}\\) = 40 steps and information on the last \\(T_{cur}\\) = 10 steps, the ToMnet achieves maximal performance. The main insight is that the ToMnet does not require much past information about a teammate in order to make accurate predictions about its underlying behavior characteristics. This is an indication that, even in the absence of prior data about a teammate, an observer agent could potentially use observations collected during a trajectory and feed those as past trajectories to compute character embeddings."}, {"title": "4.4 Agent Experiments", "content": "We now address different research questions to assess the usefulness of online dynamic replanning and the extent to which TOMCAT agents, equipped with different ToM capabilities, can adapt to teammates, both in the presence and absence of priors about their behavioral characteristics. For all experiments reported below and for each test condition, we generated 500 episodes of 200 timesteps each, where we paired a ToMCAT agent with a RL agent, randomly assigning profiles to each agent at the start of an episode. In addition, in each episode, the policy of the RL agent was selected from the set trained via MARL, as detailed in Sec. 4.1, corresponding to the policy that was concurrently trained with another RL agent using the same profile as the ToMCAT agent. In practice, this means that the RL agent \"knows\" the profile of the ToMCAT agent and selects the corresponding optimal policy."}, {"title": "4.4.1 Dynamic Replanning", "content": "We want to address the following research question: \u201cCan the ToMCAT system perform well when deployed in a dynamic environment without requiring replanning at every step?\u201d To address that question, unlike prior works that measure CPU time on a specific platform, here we use the number of planning steps as a measure of computational \"resource usage\" and assess its impact on task and individual agent performance.\nIn this experiment, we used the MADiff model trained with history (i.e., C = 16) conditioned on all the ToM variables (i.e., \\(\\gamma(\\tau_i) := [\\psi_i, \\text{e}_{char}, \\text{e}_{ment}]\\)) as the base model for the ToMCAT agent. To test the usefulness of the online dynamic replanning capability detailed in Sec. 3.4, we compare it against replanning at fixed time intervals, namely: at every timestep (Always), every 10 timesteps (10 Steps), and every H = 64 timesteps (Horizon). In order to compute the character embedding, \\(\\text{e}_{char}\\), the TOMCAT agent is provided \\(\\mathcal{N}_{past}\\) = 4 past trajectories sampled from D, where an agent with the same profile interacted with an agent with the teammate's profile."}, {"title": "4.4.2 Known Teammates", "content": "In this experiment we want to address the question: \"Can ToM reasoning improve team performance in the presence of strong teammate priors?\" Here we want to understand whether ToM is important for TOMCAT agents when paired"}, {"title": "4.4.3 Unknown Teammates", "content": "Here we want to address the question: \"Can TOMCAT agents learn from and adapt to unknown teammates?\" We adopted a methodology similar to that of the previous experiment, but where the ToM agent was not provided any past trajectories about its teammate, i.e., \\(\\mathcal{N}_{past}\\) = 0. Instead, for the first 100 timesteps of an episode, the agent updated a buffer of observations, and used that to compute the character embedding, \\(\\text{e}_{char}\\). We also used a different baseline against which to compare the MADiff agents, namely a team of RL agents that did not \"know\" their teammate. Recall from Sec. 4.1 that we co-trained RL policies via MARL for all pairs of agent profiles. Since we designed 7 profiles, this results in 6 policies trained for each profile. As such, for the RL Unknown condition, we selected for each RL agent a policy from the corresponding set of profile policies uniformly at random."}, {"title": "5 Conclusions & Future Work", "content": "This paper presented TOMCAT, a new framework that combines ToM reasoning with multiagent planning for fast adaptation in complex multiagent tasks. It integrates a ToM network (ToMnet) that learns a strong prior of possible team behaviors and leverages meta-learning to make predictions about underlying characteristics of teammates from minimal data, with a multiagent denoising-diffusion policy (MADiff) approach that generates plans for the agent and its teammates conditioned by ToM embeddings of the team. An online dynamic replanning mechanism monitors plan execution and triggers the sampling of a new plan whenever there is a discrepancy between the plan and the current state of the world. We performed various experiments using different parameterizations of TOMCAT agents under different conditions in a simulated collaborative cooking domain. The results show the usefulness of the replanning mechanism in attaining good pairwise performance without the need to replan at short, fixed-time intervals. It also underscores the importance of reasoning about teammates' characteristics as informed by the ToM embeddings and the agent's own characteristics to allow for fast adaptation to different teammates, especially when no prior information is provided about them.\nFuture work: We are currently designing a joint model approach, where ToMnet and MADiff systems are combined into a probabilistic generative model that allows sampling multiagent plans conditioned on ToM information, while \"forcing\" the model to predict desired characteristics of teammate motivations and behaviors. We are also addressing the challenges of making ToMCAT agents more robust to unknown teammates and exploring how the framework can be applied in ad hoc teamwork settings. Finally, we plan to apply ToMCAT to learn from human data to infer profiles, behavior trends of people performing complex joint tasks, and explore its application in adversarial settings requiring nested To\u041c."}, {"title": "A Appendix", "content": "Here we provide additional implementation details and results."}, {"title": "A.1 Dynamic Replanning Algorithm", "content": "Our dynamic replanning algorithm (Alg. 1) unfolds as follows: for a given agent i, we first initialize the character embeddings for teammates from past trajectories, if available. We also initialize empty queues for the observation history, h, and the planned sequence of observations, \\(\\mathcal{T}_{plan}\\). At each step, after the agent makes a new observation in the environment, we check if replanning is needed (line 4). If a plan has been previously generated and is not depleted, we compute the mean squared error between the current observation and the observation predicted in the plan, where \\(\\eta: \\Omega \\rightarrow [0, 1]\\) is an observation normalizer, which is detailed below in Sec. A.5.2. If the observation difference is higher than a predefined threshold, \\(\\lambda\\), we generate a new plan (lines 5\u201311). First, the mental embedding is computed from the current trajectory and all conditioning variables\u2014any combination of returns (R_i (\\tau_i)), observer profile (\\psi_i), character embedding (\\text{e}_{char}), mental embedding (\\text{e}_{ment})\u2014are concatenated into \\gamma(\\tau_i) (line 5). We then generate a new plan through conditional sampling, starting with \\(\\uparrow_K \\sim \\mathcal{N}(0, \\alpha I)\\). At each diffusion step k, the portion of the observer agent in \\(\\uparrow_k\\) is \u201cin-painted\u201d with the corresponding current observation and C steps of observation history (line 8). We iteratively refine (denoise) trajectories \\(\\uparrow^*\\) using the perturbed noise \\( \\hat{\\epsilon} \\) following (Zhu et al., 2024; Ajay et al., 2023), where \\(\\epsilon_{\\delta} (\\tau_k, k)\\) is the unconditional noise, \\( \\omega \\) is a scalar for extracting the distinct portions of the trajectory that exhibit \\( \\gamma(\\tau_i) \\), and \\( denoise \\) is the noise scheduler predicting the previous sample, \\(\\tau_{k-1}\\), from \\(\\uparrow_k \\) by propagating the diffusion process from \\(\\epsilon \\). Once the final, denoised trajectory, \\( \\hat{\\tau}^0 \\), is generated, we discard the first C + 1 steps\u2014since they were only used to condition the generation process\u2014and extract agent i's portion, \\(\\hat{\\tau}_i \\), re-initializing the plan queue, \\(\\mathcal{T}_{plan}\\) (line 11). The agent then retrieves an action by using the inverse dynamics model \\( I_{\\xi} \\) on consecutive observations extracted from the plan (line 12)."}, {"title": "A.2 Agent Profiles", "content": ""}, {"title": "A.3 Multiagent RL Training", "content": "For training, we adopted the PPO algorithm (Schulman et al., 2017) implementation in Ray's RLLib toolkit. Each pair was trained for 1 000 iterations with 64 parallel workers. We used a learning rate of 8 \u00d7 10-4 and a linearly decreasing entropy loss coefficient. At each iteration, a batch of data was collected from workers, each spwaning the overcooked environment with the layout shown in Fig. 2, randomizing the initial locations of agents. Each episode ran for a period of 400 timesteps. We collected 25 600 timesteps per batch, with a mini batch size of 6 400 for gradient updates, training for 420 iterations of 8 epochs each."}, {"title": "A.4 ToMnet", "content": ""}, {"title": "A.4.1 Dataset/Feature Preparation", "content": "We started with the pairwise RL agent dataset, \\(\\mathcal{D}\\), generated as described above. We sampled 2000 datapoints for each agent/profile to train the ToMnet, where in each trial we selected a teammate uniformly at random. Each datapoint consisted of the following: we randomly sampled \\(\\mathcal{N}_{past}\\) = 4 trajectories of the agent pair of \\( T_{past} \\) = 100 steps from D to serve as \"past\u201d trajectories for the Character net; we sampled an additional trajectory and split it at a timestep, \\( t \\sim \\mathcal{U}(1,T - 30) \\), selected uniformly at random. The observation of the observer agent at t was used as the \"current\u201d observation to be fed to the Prediction net, while the past \\( T_{cur} \\) = 10 timesteps acted as the \"current\u201d trajectory for the Mental net, where the data was reversed and zero-padded whenever t < \\( T_{cur} \\). The teammate's future data in the current trajectory (a minimum of 30 timesteps) was used for calculating various ground-truth targets against which to train the Prediction net. Namely, we used the following prediction targets: (a) the teammate's next step action, modeled as a discrete distribution, \\( \\Delta(A) \\); (b) the sign of each of its profile reward weights, \\( \\theta_j \\), modeled as a distribution over \\( \\{-1,0,1\\} \\); and (c) the successor representation (SR), corresponding to statistics of the teammate's future behavior. In particular, SR consisted of three types of data: (i) binary features\u2014whether the closest pot is empty, cooking or ready and similarly for the next closest pot (total 6 features), encoded as Bernoulli distributions; (ii) categorical features\u2014relative frequency of the teammate carrying an onion, soup, dish, tomato or nothing (total 1 feature), encoded as a discrete distribution; (iii) numeric features\u2014mean path distance to the teammate and x and y locations in the environment (total 3 features), encoded as a multivariate Gaussian distribution. To obtain the SR means, we used discounted averaging over the teammate future data with a discount factor of 0.99."}, {"title": "A.4.2 Training and Losses", "content": "The ToMnet architecture consisted of three networks as detailed in Sec. 3.2-a Character Net, a Mental Net and a Prediction Net. The Character and Mental Nets consisted of an LSTM layer of hidden size 64 (dropout of 0.2) that produces an initial embedding (final hidden state of the LSTM). The Character Net takes \\( \\mathcal{N}_{past} \\) trajectory data each of length \\( T_{past} \\) = 100 as input, and to calculate the Character embedding, \\( \\text{e}_{char} \\), we first concatenated the LSTM's embedding with the observer's profile weights, sending the result through a dense/linear layer to obtain an embedding of size \\( \\text{e}_{char} \\) = 8. The Mental Net takes the current trajectory of length \\( T_{cur} \\) = 10 as input, where we masked out incomplete timesteps of a trajectory. To calculate the Mental embedding, \\( \\text{e}_{ment} \\), we concatenated the LSTM's embedding with the profile's weights and \\( \\text{e}_{char} \\) before passing the result through a dense layer, resulting in an embedding of size \\( \\text{e}_{ment} \\) = 8.\nFinally, to obtain the different predictions, we first concatenated both embeddings with the observer's profile weights and the current observation to obtain the prediction input. This is then sent through the different prediction heads to produce probabilistic predictions for the different targets, i.e.,, next action, profile, SR features detailed earlier. Each prediction head consists of a dense layer of size 64 (dropout of 0.2) with output dimension equaling the number of possible outputs for that prediction, namely, |A| = 6 for the next action, \\( \\theta_j \\) \u00d7 3 = 39 for the profile sign prediction, 6 for the SR binary features, 5 for the SR categorical features, and 3 for the SR numeric features. We used the negative log-likelihood (NLL) loss for the next action and profile predictions, binary cross-entropy loss for SR binary predictions, cross-entropy loss for SR categorical prediction, and a Gaussian NLL loss for SR numeric features' predictions.\nFor training the ToMnet, we used the 2000 datapoints generated as explained above for each agent type, resulting in 1 400 datapoints, using a train-validation split of 80%-20% respectively. We used an Adam optimizer with weight decay of 5 \u00d7 10-4. We trained the model for 2000 epochs via mini-batch gradient descent with batch size of 128 and learning rate of 5 \u00d7 10-4, using an early stopping criterion based on a validation loss (maximum number of steps without improvement set to 10 and a maximum tolerance of 0.01)."}, {"title": "A.5 MADiff", "content": ""}, {"title": "A.5.1 Trajectory Augmentation", "content": "To train the MADiff component, we sampled joint trajectories similarly to how we trained the ToMnet, but augmented trajectories by computing the ToM embeddings for each timestep using the previously-trained ToMnet as explained in Sec. 4.2. We used the same data parameters used to train the ToMnet, i.e.,, \\( \\mathcal{N}_{past} \\) = 4 past trajectories of \\( T_{past} \\) = 100"}]}