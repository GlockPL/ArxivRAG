{"title": "Addressing Image Hallucination in Text-to-Image Generation through Factual Image Retrieval", "authors": ["Youngsun Lim", "Hyunjung Shim"], "abstract": "Text-to-image generation has shown remarkable progress with the emergence of diffusion models. However, these models often generate factually inconsistent images, failing to accurately reflect the factual information and common sense conveyed by the input text prompts. We refer to this issue as Image hallucination. Drawing from studies on hallucinations in language models, we classify this problem into three types and propose a methodology that uses factual images retrieved from external sources to generate realistic images. Depending on the nature of the hallucination, we employ off-the-shelf image editing tools, either InstructPix2Pix or IP-Adapter, to leverage factual information from the retrieved image. This approach enables the generation of images that accurately reflect the facts and common sense.", "sections": [{"title": "1 Introduction", "content": "Recent text-to-image generation methods have made remarkable progress due to the emergence of diffusion models. However, many text-to-image generation models still fail to accurately reflect the underlying facts or changes over time and circumstances conveyed by input text prompts. As a result, they generate prototypical examples that differ from reality. For instance, the Statue of Liberty, completed in 1886, initially had a copper-brown color because its surface was covered with copper. Over the decades, the color gradually changed to its current blue-green hue due to oxidation. However, when the prompt \u201cThe Statue of Liberty in 1890\" is entered into a recent text-to-image model such as Dall-E 3 [Betker et al., 2023], only the turquoise Statue of Liberty is generated, as shown in Figure 1.\nGenerating inaccurate images can spread misinformation and misconceptions [Robertson, 2024]. It poses a serious issue in fields where factual accuracy is crucial, such as education and journalism. Recent practices of utilizing foundation models, such as Stable Diffusion [Rombach et al., 2022] or Imagen [Saharia et al., 2022], have become prevalent in text-to-image generation. However, inaccurate outputs from these models could propagate biases and distortions into subsequent models. Additionally, the reliability of AI models hinges on their ability to deliver accurate results. While large language models (LLMs) have become widely used in industry, text-to-image generation models still lack practical usability. This may be because generated images do not yet accurately reflect factual information. Therefore, it is essential for image generation models to produce factually accurate and trustworthy images when applied to the aforementioned applications.\nDespite its significance, the prevention of generating inaccurate images has been underestimated, and there is no appropriate term to refer to this issue in the context of text-to-image generation. In this study, we define this problem as \"image hallucination\". Image hallucination includes not only misalignment between text prompts and generated images but also the generation of factually defective images. This concept is more complex than alignment because it involves understanding meanings and facts not included in the text prompt itself. In this study, we focus on the issue of text-to-image generation failing to generate facts.\nBecause image hallucination is extensive, it is difficult to address all issues. Thus, we highlight the hallucinations that are judged to be representative based on [Huang et al., 2023] by categorizing them into three types: (1) Factual inconsistency caused by co-occurrence bias, (2) outdated knowledge hallucination, and (3) factual fabrication that produces counterfactual. We will discuss in detail the three types of hallucinations that will be addressed later.\nTo solve the previously mentioned problem, external knowledge can be utilized to guide the image generation model. Recently, retrieval-augmented language models have demonstrated significant potential in addressing hallucination [Borgeaud et al., 2022; Yu et al., 2023]. Given input text, such models retrieve relevant documents from external memory and generate fact-based answers. Recent study [Yasunaga et al., 2022] has expanded retrieval and generation to encompass both images and text, training multimodal generation models to effectively utilize retrieved information.\nWe develop this idea and introduce a tuning-free method to enhance the image generation model for producing fact-based images. Initially, an image is generated via an existing text-to-image model. Then, the input prompt is used as the search query to retrieve N images in order of relevance. Among the retrieved images, the user selects one factual image to be used as guidance to eliminate image hallucination. Depend-"}, {"title": "2 Related Work", "content": "Text-to-image diffusion models [Rombach et al., 2022; Saharia et al., 2022] have made significant progress but often struggle with complex prompts. The early approach utilized additional inputs, such as key points, to achieve better control [Yang et al., 2023]. Recent advancements leverage LLMs to manage layout directly [Wu et al., 2023], thereby improving prompt alignment. Diffusion models enable various image edits, ranging from global styles to precise object manipulation, but often lack precision for detailed spatial adjustments [Hertz et al., 2022]. We address this issue by utilizing images from external sources to create fact-based images.\nSome research focuses on generative models trained to retrieve information in multimodal settings. For example, Re-Imagen [Chen et al., 2022b] generates images from retrieved images with text prompts, and MuRAG [Chen et al., 2022a] generates language answers using retrieved images. Unlike these, our approach does not need any training.\nVarious strategies exist to mitigate hallucination in LLMs. Data-related issues can be addressed by enhancing data quality and employing better labeling techniques [Lin et al., 2021]. For training-related hallucinations, improved model architectures and advanced regularization techniques [Liu et al., 2024] are recommended. In this paper, we use fact-based images to eliminate hallucinations."}, {"title": "3 Methods", "content": "We focus on the problem of hallucination where the generated image does not reflect the common sense and facts conveyed by the text, rather than a simple misalignment between the text prompt and the generated image. Based on the paper analyzing hallucinations in language models [Huang et al., 2023], we categorize representative image hallucinations into three categories: factual inconsistency caused by co-occurrence bias, Outdated knowledge hallucination that fails to reflect up-to-date information, and factual fabrication where the generated image has little to no basis in reality.\nFactual inconsistency refers to situations where the output of a generation model contains facts based on real-world information but is contradictory or inaccurate. Specifically, factual inconsistency caused by co-occurrence bias occurs because generation models rely on patterns in the training data, which may be imbalanced. For instance, although the Statue of Liberty was originally copper brown, models generate it with its current bluish-green hue due to the predominance of such data, ignoring its historical appearance.\nOutdated knowledge hallucination occurs due to the inability to reflect temporal information. The internal knowledge of a generation model is not updated once trained. Therefore, external information must be used to generate new, updated knowledge over time. For example, current text-to-image generation models cannot accurately generate images of presidents from specific historical periods.\nFactual fabrication generates scenarios unlikely or impossible when compared to reality. For example, San Francisco rarely experiences snow in the winter, having only witnessed it three times since the 20th century. However, if the prompt is \"The Golden Gate Bridge in winter\u201d, an image with a significant amount of snow is generated."}, {"title": "3.1 Image Hallucination", "content": "We propose two pipelines, as shown in Figure 2. They utilize retrieved images to reflect real-world knowledge and common sense that generative models cannot capture based on text prompts alone."}, {"title": "3.2 Retrieval-augmented Factual Text-to-Image Generation", "content": "To search for factual information relevant to a given text prompt, we employ Google's Custom Search JSON API to retrieve images. Among the retrieved images, the user selects the one that best represents the factual information they wish to generate. The number of images retrieved per prompt is a hyperparameter that can be adjusted based on individual requirements. If the desired image is not retrieved for a given prompt, increasing this number facilitates a broader selection of candidate images."}, {"title": "Image Retrieval Interaction", "content": "We propose two methodologies that utilize the retrieved images, depending on the target of the hallucination. First, if a hallucination occurs in a specific property (e.g., the color of an object), we use the retrieved image to obtain instructions and apply InstructPix2Pix. InstructPix2Pix, which combines the knowledge of LLM (GPT-3) and text-to-image generation model (Stable Diffusion), edits images according to human instructions. To train the model, it needs instruction and paired images from paired captions(input and edited caption). The instruction and edited caption are obtained by inputting the initial caption into LLM (GPT-3). Then, the paired captions are entered into the diffusion model to get corresponding images. Based on this, we input the initially generated image and the retrieved image into GPT-4 to generate an instruction based on differences between the two images. For example, the most significant difference between the retrieved image of \"The Statue of Liberty in 1890\" and the generated image is the color. Thus, we input both images into the LLM and obtain the instruction, \"The statue needs to be colored copper brown.\" We input this instruction and the initial image into the pre-trained InstructPix2Pix to correct the hallucination.\nOn the other hand, if hallucination occurs in a broad area involving many complex subjects, such as a person (including components like the face, hair, clothing, etc.), text prompts or text instructions alone may not be sufficient as generation conditions. Therefore, to incorporate features that cannot be fully expressed in text, the retrieved image must be used as a prompt. For this purpose, we utilize the IP-Adapter, which"}, {"title": "Overall pipeline", "content": "We utilize DALL-E 3 as the model for initial image generation. GPT-4 is employed as the LLM that generates instructions and prompts. The InstructPix2Pix and IP-Adapter models are pre-trained models based on Stable Diffusion v1.5.\nFigure 3 illustrates hallucinations caused by factual inconsistencies due to co-occurrence bias, and presents the corresponding experimental results. In each example, the input prompt and the retrieved image (middle image) are compared with DALL-E 3's initial output (left image). The initial generation fails to accurately reflect the facts. By providing instructions derived from the discrepancies between the initial and retrieved images to InstructPix2Pix, factually accurate images are obtained (right image). For instance, the Statue of Liberty is correctly depicted in its copper brown color as it appeared in 1890, and Mt. Fuji in summer is realistically shown with most of the snow melted from its peak.\nFigure 4 demonstrates examples of outdated knowledge hallucinations and the experimental results. When the target of such hallucinations possesses complex and diverse factual information, such as a person, rectifying these inaccuracies through text alone is particularly challenging. Therefore, factual images from search results (middle image) are used directly as prompts. By inputting the retrieved image and a corresponding factual prompt into the IP-Adapter, images that accurately reflect factual information about individuals are generated (right image). Using our methodology, we successfully produce images that accurately depict Angela Merkel as the female Chancellor of Germany in 2015, and Marcelo Rebelo de Sousa as the President of Portugal in May 2019.\nFigure 5 depicts hallucinations and experimental results concerning factual fabrication. The initial image does not properly reflect the fact that San Francisco rarely experiences snow in winter. To address this, the same method used for factual inconsistency caused by co-occurrence bias is applied. Instructions obtained from GPT-4, directing the removal of all snow from the initial generation, along with the initial image, are input into InstructPix2Pix. This process generates an image reflecting the factual information of San Francisco with no snow."}, {"title": "4 Experiments and Results", "content": "We plan to expand our research to address a broader range of hallucinations. We aim to resolve image hallucinations more comprehensively and to develop quantitative evaluation metrics and benchmarks for this purpose."}, {"title": "5 Future works", "content": ""}]}