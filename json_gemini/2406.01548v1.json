{"title": "How to discretize continuous state-action spaces in Q-learning: A symbolic control approach", "authors": ["BELAMFEDEL ALAOUI Sadek", "SAOUD Adnane"], "abstract": "Q-learning is widely recognized as an effective approach for synthesizing controllers to achieve specific goals. However, handling challenges posed by continuous state-action spaces remains an ongoing research focus. This paper presents a systematic analysis that highlights a major drawback in space discretization methods. To address this challenge, the paper proposes a symbolic model that represents behavioral relations, such as alternating simulation from abstraction to the controlled system. This relation allows for seamless application of the synthesized controller based on abstraction to the original system. Introducing a novel Q-learning technique for symbolic models, the algorithm yields two Q-tables encoding optimal policies. Theoretical analysis demonstrates that these Q-tables serve as both upper and lower bounds on the Q-values of the original system with continuous spaces. Additionally, the paper explores the correlation between the parameters of the space abstraction and the loss in Q-values. The resulting algorithm facilitates achieving optimality within an arbitrary accuracy, providing control over the trade-off between accuracy and computational complexity. The obtained results provide valuable insights for selecting appropriate learning parameters and refining the controller. The engineering relevance of the proposed Q-learning based symbolic model is illustrated through two case studies.", "sections": [{"title": "I. INTRODUCTION", "content": "Q-learning, introduced by Watkins and Dayan [1], has become a prominent research area in reinforcement learning, garnering significant attention for its ability to autonomously learn optimal policies. Its successful applications span diverse domains, including games, robotics, and control systems, as evidenced by notable works such as [2]. The allure of Q-learning lies in its simplicity and effectiveness in tackling problems with discrete state and action spaces. However, real-world scenarios often present challenges that traditional Q-learning approaches struggle to address, particularly when dealing with continuous state-action spaces [3], [4], [5].\nContinuous state-action spaces are characterized by an infinite number of possible state-action pairs, rendering the explicit tabular representation of states impractical [6]. Consequently, there has been a growing interest in extending Q-learning techniques to handle continuous state-action spaces, as seen in works such as [3], [4], [5], [7]. Among these techniques, space discretization-based methods have been explored in [8], [9], [10], [11].\nSpace discretization methods partition continuous state and action spaces into discrete subsets called cells, viewing system trajectories as trajectories of these discrete cells. However, the straightforward approach of space discretization, as described in earlier works [8], [9], [10], [11], can lead to an undesirable mismatch in the reachable set of points within an action. The state space is divided into a uniform grid, with each point represented by the center of the corresponding cell. The successor state is computed based on the system's difference equation, but only for the center of the current cell. Consequently, the trajectory of a point within the starting cell may not end up in the same cell as its center.\nTo address the Q-learning problem for continuous state action spaces, this paper presents a novel approach using symbolic abstraction to mitigate underestimation of the reachable set by intersecting the it with multiple discrete cells. This over-approximation introduces conservatism, rendering the discrete system non-deterministic [12]. Building upon the Q-learning based approach for non-deterministic transition systems proposed in [13], this paper introduces a new Q-learning based discretized reward. As a result of this extension, two Q-tables are generated: the minimal and maximal Q-values, effectively bounding the Q-values of the continuous state-action spaces. The theoretical development establishes conditions under which the minimal and maximal Q-values converge to an optimal Q-values. Additionally, a direct relation between the tightness of these Q-values and the abstraction parameters is established. The analysis demonstrates that under specific learning configurations, the bounds become tighter, leading to a convergence of the extracted policy towards the optimal policy of the continuous state-action space. The effectiveness of the proposed algorithm is evaluated through its application to the mountain car control problem [14] and to the Van Der Pol oscillator. The evaluation reveals that either the maximally or minimally computed Q-values can effectively control the system. Moreover, the proposed algorithm can achieve any desired precision on the Q-values for continuous state-action spaces, resulting in the convergence of the extracted policy towards the optimal policy of the continuous spaces. Furthermore, the evaluation shows that reducing the distance between quantizer levels leads to tighter bounds and improves the accuracy of the approximation of the optimal Q-values in terms of similarity between the maximally and minimally computed policies."}, {"title": "II. RELATED WORKS", "content": "Space discretization in Q-learning is a thriving area of research, driven by the need to adapt Q-learning techniques to handle continuous state-action spaces. To discretize a Q-learning problem, it is necessary to analyze how to construct subgoals for the agent. In this context, [15] presents an algorithm that combines Q-learning with a locally weighted learning method to select behavioral primitives and generate subgoals for the agent. Similarly, [16] focuses on identifying subgoals by partitioning local state transition graphs. Quad-Q learning, as developed by [17], contributes to the theory of Q-learning by extending it to a quad-based framework, providing an innovative solution applicable to learning how to partition large intractable problem domains into smaller problems that can be solved independently. Hierarchical Reinforcement Learning (HRL) methods have also been explored to address space discretization challenges. MAXQ, introduced by [18], decomposes a learning problem into a hierarchy of subtasks, each of which is learned using Q-learning. Grid-based discretization methods have been extensively investigated. For example, [8] introduces an approximate dynamic programming approach, utilizing grid-based discretization to address high-dimensional continuous control problems. Additionally, [9] explore a multiresolution approach, where the grid resolution varies based on the attainable subgoals. Building on this approach, [10] extends the multiresolution technique with pseudo-randomized discretization to enhance its adaptability. The works [19] and [20] revolve around the concept of adaptive discretization, primarily achieved through non-uniform partitioning of the space. This approach involves continuously refining the partition based on the density of the observed samples, offering valuable insights into data-driven approaches for achieving more efficient and effective learning. Moreover, [11] propose an adaptive space partitioning technique that automatically adjusts the granularity of the discretization based on the system's exploration. The work in [21] establishes an algorithm to solve value iteration over continuous actions interval Markov Decision Processes that uses maximization problems instead of maximisation minimisation for efficiency. In the context of controlling non-deterministic finite transition systems, [13] introduced a Q-learning algorithm tailored to address the challenges posed by such systems. Non-deterministic finite transition systems are discrete systems whose transition probabilities are not explicitly defined. However, it is essential to note that the work presented here and the study conducted by [13] are distinct in their problem statements and objectives."}, {"title": "III. PRELIMINARIES AND PROBLEM STATEMENT", "content": "The symbols N, N>0, R, and R\u22650 represent the sets of natural numbers, strictly positive natural numbers, real numbers, and positive real numbers, respectively. When referring to a set X, the notation 2x denotes the set of all subsets of X. Given any a \u2208 R, |a| denotes the absolute value of a. Given any u = (u1,..., un) \u2208 Rn, the infinity norm of u is defined by ||u|| = max |ui|. Given sets X and Y, we denote by f : X \u2192 Y an ordinary map of X into Y. We denote the closed, open, and half-open intervals in R by [a, b], (a, b), [a, b), and (a, b], respectively. For any set S \u2286 Rn of the form S = Uj=1 Sj for some M\u2208 N, where Sj = \u03a0i=1[ai, bi], with |ai \u2013 bi| < d, and non-negative constant \u03b7 \u2264 \u1fc6, where \n\u0e17\u0e35 = min nsj and nsj = min {bi - ai/2,..., |ai - ai/2|}, j=1,...,M\nwe define [S]\u03b7 = {a \u2208 S | ai = ki\u03b7, ki \u2208 Z, i = 1, ..., n} if \u03b7 \u2260 0, and [S]\u03b7 = S if n = 0. The set [S]\u03b7, will be used as a finite approximation of the set S with precision \u03b7 \u2260 0. Note that [S]\u03b7 \u2260 0 for any \u03b7 \u2264 \u00f1. B = {x \u2208 Rn| ||x||\u221e\u2264 1} denotes the unit ball."}, {"title": "B. System dynamics", "content": "Consider a control problem in which the system evolves based on a discrete-time dynamical equation defined as follows:\n(\u03a3): \u03bek+1 = f(\u03bek, \u03c5k), (1)\nthe environment state is denoted as \u03bek and belongs to the continuous space S \u2282 Rn, while the control input is represented by \u03c5k: A(\u03bek) \u2192 S, with A(\u03bek) the set of admissible actions that belongs to the set of all actions A, i.e. A(\u03bek) \u2286 A \u2286 Rm. The actions are decisions made by a controller at each time step k \u2208 N. The map f : S \u00d7 A \u2192 S is assumed to be known and generally nonlinear. Let \u03c0 = (\u03c50, \u03c51,...) \u2208 \u03a0\u2286 A\u03c9, with \u03c5k \u2208 A(\u03bek), represents a potentially infinite sequence of actions. The notation (\u03bek, \u03be0, \u03c0) is used to denote the state reached at time k starting from the initial state \u03be0 under the sequence of actions \u03c0.\nIn the subsequent analysis, we make the assumption that the map f fulfils the following Lipschitz assumption.\nAssumption 1. There exist Lipschitz constants Lf\u03be, Lfv such that for any \u03be,\u03be \u2208 S and v \u2208 A(\u03be), v \u2208 A(\u03be), the following inequality holds:\n||f(\u03be, v) \u2013 f(\u03be, v)|| < Lf\u03be||\u03be \u2013 \u03be|| + Lfv||v \u2013 v|| (2)\nwhere A(\u03be) and A(\u03be) denote the sets of admissible control inputs at state \u03be, \u03be respectively.\nThe following assumption is used through the paper.\nAssumption 2. The state and action space S and A are compact.\nThe following auxiliary lemma will be used to prove the main results of the paper.\nLemma 1. Under Assumption 2, there exists a positive constant LA such that for every distinct \u03be,\u03be \u0395 S, \u03c5 \u2208 A(\u03be) and v \u2208 A(\u03be) we have:\n||v - v|| < LA||\u03be \u2013 \u03be||, (3)\nProof. Suppose that A is a compact set, then there exist a constant C such that ||v \u2013 v|| \u2264 C. Let's prove that (3) holds for any distinct \u03be,\u03be \u2208 S and v \u2208 A(\u03be), v \u2208 A(\u03be). Inequality (3) can be written as ||v \u2013 v|| < LA||\u03be \u2013 \u03be||. Let M =\nmax\n\u03b5\u03be\u03b5\u03c2  &&||\u03be-\u03be||>0\n||\u03be-\u03be||\nthe existence of M \u2265 0 is guaranteed since \u03be and are distinct and the state space S is compact. Thus,\n||v \u2013 v|| \u2264 CM||\u03be \u2013 \u03be|| implies that ||v \u2013 v|| \u2264 LA||\u03be \u2013 \u03be||,\nwhere LA = CM is a positive constant. This completes the proof."}, {"title": "C. Why earlier discretisation methods are not performing for Q-learning?", "content": "In the context of the control system \u03a3, the Q-learning control scheme [1] is employed to learn the Q-values through a sequence of observations, actions, and rewards. To guide the algorithm towards achieving the desired goal, a reward map g: S\u00d7 A \u2192 R is associated with each state-action pair. Indeed, at each time step k \u2208 N, the current state \u03bek is observed and a decision \u03c5k is selected from the admissible set of control inputs A(\u03bek). After \u03c5k is performed, the system goes to a next state \u03bek+1 = \u03be'. Associated with this state transition, an immediate reward g(\u03bek, \u03c5k) is gained. The object of the controller is to find an optimal policy that maximizes \u03a3i=0\u03b3ig(\u03bek+i, \u03c5k+i), where \u03b3 \u2208 (0, 1) is the discount factor. Taken arbitrary policy \u03c0, the Q-values are defined by:\n\u03b1\u03c0(\u03be, \u03b1) = g(\u03be, \u03b1) + \u03b3 max q\u03c0(\u03be', \u03c5'). (4)\n\u03c5' \u0395\u0391(\u03be')\nThe objective in Q-Learning is to estimate the Q-values for an optimal policy when the reward function is known a priori. The optimal Q-values, denoted q*(\u03be, v) are generally learned by Algorithm 1."}, {"title": "Algorithm 1 Q-Learning Algorithm", "content": "1: Initialize the Q-table, a learning rate \u03b1 and \u03b3 the discount factor.\n2: Define the state and action spaces\n3: for each episode do\n4: Initialize the current state\n5: while episode not finished do\n6: Choose an action based on the current state and exploration-exploitation strategy\n7: Apply the action to \u03a3 and observe the reward g(\u03be, v) and next state \u03be'\n8: Update the Q-value for the current state-action pair using the Q-learning update equation:\nQ(\u03be, \u03c5) \u2190 Q(\u03be, \u03c5) + \u03b1(g(\u03be, v) + \u03b3 max\n\u03c5'\u0395\u0391(\u03be')\nQ(\u03be', \u03c5') - Q(\u03be, \u03c5))\n9: Set the current state to the next state\n10: end while\n11: end for\n12: Extract the learned policy from the Q-table\nThe next result from [1] shows under which conditions Algorithm 1 converges."}, {"title": "Theorem 1. Given bounded |g| < R \u2208 R\u22650, learning rates 0 < \u03b1 < 1 and", "content": "\n\u03a3\u03b1k(\u03be, \u03b1) = \u221e, \u03a3[\u03b1k(\u03be, \u03b1)]\u00b2 < \u221e,\nk=1\nk=1\nthen qk (\u03be,\u03b1) \u2192 q* as k \u2192 \u221e \u2200\u03be, a with probability 1.\nThe problem of applying Algorithm 1 to the system \u03a3 relies on the fact that \u03be and v are both defined in a continuous space making updating the Q-values impossible over an infinite number of state action pairs [8], [6], [7]. To overcome this problem, discretization methods have been proposed to effectively handle infinite state and action spaces. One such method is uniform discretization [8], [9], [22], which divides the state and action spaces into evenly sized intervals. Another approach is Voronoi discretization [11], which partition the spaces based on the proximity of the states and actions to specific points. Algorithm 2 provides a detailed description of how Q-learning with uniform discretization operates.\nIt is worth noting that such discretisation methods (uniform and Voronoi) can result in a discretization-induced reachable mismatch, see Figure ??. In these approaches, the state space is divided into a uniform grid, and each point is represented by the centre of the cell to which it belongs. The successor state is computed based on the system's difference equation (1), but only for the centre of the current cell. As a consequence, the trajectory of a point within the starting cell may not end up in the same cell as its centre. This mismatch arises because the discretisation method does not take into account the trajectories that cross cell boundaries, causing the actual trajectory to deviate from the idealized trajectory represented by the cell centres. Relying solely on the points xi \u2208 G, see Algorithm 2 to compute successor states can lead to inaccuracies in the learned Q-values and the resulting policy."}, {"title": "D. Problem Statement", "content": "This paper addresses several key challenges in controlling system (1) with continuous state and action spaces using Q-learning. The first challenge is to identify a suitable discretization method that effectively captures the essential dynamics of the system. Once a suitable discretization method is identified, the next challenge is to learn the optimal Q-values of the discrete system while evaluating the conservatism introduced by the discretization process. By analysing the parameters of the discretization, the paper aims to gain insight into establishing an arbitrary precision \u03b5 between the resulting discrete Q-values and the Q-values of the original continuous system (1). To the best of our knowledge, these research questions represent novel and unexplored points for Q-learning."}, {"title": "E. Solution strategy", "content": "To address the research questions at hand, this paper proposes an abstraction-based Q-learning approach. This approach involves three steps, abstraction, Q-learning and refinement, see Figure 1. In the \"Abstraction\", the state-input spaces is partitioned into intervals called cells. By analysing"}, {"title": "Algorithm 2 Q-Learning with uniform discretisation", "content": "Input: State set: S, Action set: A, Number of partitions for state set: n, Number of partitions for action set: p\nOutput: Approximated Q-values for the discrete state-action pairs\nPartition the state set S into n disjoint subsets: S1, S2, ..., Sn. Ensure that each subset covers the entire state space, i.e., S = U=1Si.\nSelect a representative point xi from each subset Si to form a finite aggregated state set G = {x1, x2, ..., xn}. Generally, xi is the centres of the cell i.\nPartition the action set A into p disjoint subsets: A1, A2,..., Ap. Ensure that each subset covers the entire action space, i.e., A = U=1Aj.\nSelect a representative action aj from each subset Aj to form a finite aggregated decision set H = {a1, a2, ..., ap }.\nInitialize the Q-values for each discrete state-action pair in G \u00d7 H as Q(x, u) = 0, for all x \u2208 G and u \u2208 H.\nfor each episode do\nInitialize state x\nwhile episode not finished do\nChoose action a \u2208 H using an exploration or exploitation strategy (e.g., epsilon-greedy)\nExecute action a and observe next state \u03be' and reward g(x, a)\nAssociate \u03be' \u2208 S with the closest x' \u2208 G\nUpdate the Q-values using the Q-learning update rule:\nQ(x, a) \u2190 Q(x, a) + \u03b1(g + \u03b3 max Q(x', a'))\na' EH\nUpdate current state x \u2208 G by x' \u2208 G\nend while\nend for"}, {"title": "Definition 1. A transition system \u03a3", "content": "For the transition system \u03a3, the set of enabled input for a state \u03be \u2208 S is formally defined by,\n\u03c5\u2208 A(\u03be) \u2194 f(\u03be, v) \u2286 S, (5)\nand,\n\u2200\u03c5 \u2208 A(\u03be), F(\u03be, v) = f(\u03be, v). (6)"}, {"title": "Definition 2.", "content": "Definition 2. Let \u03a31 = (S1, A1, \u03941) and \u03a32 = (S2, A2, \u03942) be metric systems with Y1 = Y2. A relation R \u2286 \u03a31 \u00d7 \u03a32 is an alternating simulation relation from \u03a31 to \u03a32 if the following two conditions are satisfied:\n1) for every s1 \u2208 S1 there exists 82 \u2208 S2 with (s1, 82) \u2208 R;\n2) for every (s1, 82) \u2208 R and for every a1 \u2208 A1(81) there exists a2 \u2208 A2 (82) such that for every s2 \u2208 \u03942 (82, a2) there exists s'1 \u2208 \u03941(81, a1) satisfying (s'1, s'2) \u2208 R.\nWe say that \u03a31 is alternatingly simulated by \u03a32, denoted by \u03a31 As \u03a32, if there exists an alternating simulation relation from \u03a31 to \u03a32.\nAs a space discretisation method, this paper proposes to build the Q-learning upon an abstracted model of the system"}, {"title": "IV. SYMBOLIC MODELS", "content": "Define the notion of transition systems adopted from [12], which allows to represent the discrete time system (1) and its abstraction in a unified way."}, {"title": "\u03a3. An abstraction of system \u03a3 involves partitioning the contin-uous state and input spaces into a finite number of intervals,", "content": "\u03a3. An abstraction of system \u03a3 involves partitioning the continuous state and input spaces into a finite number of intervals, called cells, using a quantizer, denoted as q = (\u03b7, \u03bc). The parameter \u03b7 \u2208 R\u22650 (respectively, \u03bc \u2208 R\u22650) represents the interval spacing that is used to approximate the state space (respectively, the input space). The resulting abstract system is denoted as the transition system ED formally defined by,\n\u03a3D = (SD, AD, \u2206, \u1e21, g), (7)\nwhere,\n\u2022 The state space SD is constructed by discretizing the continuous state space S into ng \u2265 1 states. This discretization is achieved by dividing the state space into intervals or partitions\u00b9, where each element of the partition, denoted by s, can be represented as an interval s = [s1, s2]. The parameter \u03b7 \u2208 R+ is used to control the level of discretization, determining the size of the intervals of the state representation.\n\u2022 The input space AD is constructed by discretizing the continuous input-space A into n\u03c5 \u2265 1 inputs using a finite partition \u03bc\u2208 R+ as a state-space discretization parameter. Each element a of the partition can be described as an interval a = [v1, v2];\n\u2022 The transition relation is defined for s, s' \u2208 Sp and a \u2208 An as s' \u2208 \u2206(s, a) if and only if,\ns' \u2229 {f(sc, ac) + (Lf\u03be \u03b7 + Lfv\u03bc)B} \u2260 0,\nwhere sc = + and ac = ;\n\u2022 \u1e21(s,a): SD \u00d7 AD \u2192 R is defined by max max g(\u03be, v) and represents the maximal immediate reward achievable when applying action a from the discrete state s;\n\u2022 g(s,a) : Sp \u00d7 AD \u2192 R is defined by min min g(\u03be, v) and represents the minimal immediate reward achievable when applying action a from the discrete state s;\nFor the transition system ED, the set of enabled inputs for a state s \u2208 SD is formally defined by,\na \u2208 Ap(s) \u21d4 s' \u2229 {f(sc, ac) + (Lf\u03be \u03b7 + Lfv\u03bc)B} \u2286 SD, (8)\nwhere sc = + and ac = , and\n\u2200a \u2208 AD(s), A(s, a) = s' \u2229 {f(sc, ac) + (Lf\u03be\u03b7 + Lfv\u03bc)B}. (9)\nThe following proposition relates formally system ED to system \u03a3 by an alternating simulation relation."}, {"title": "Proposition 1. Consider the transition systems \u03a3 =", "content": "Proposition 1. Consider the transition systems \u03a3 = (S, A, F, g, g) and \u03a3D = (SD, AD, \u2206,\u1e21, g) then, the relation,\nR = {(\u03be, s) \u2208 S \u00d7 Sp\u00a6 \u00a7 \u2208 s},\nis an alternating simulation relation from ED to system \u03a3.\nProof. The first condition of alternating simulation follow directly from the form of R and from the fact that ED and \u03a3 have the same sets S = UM1Si, Si \u2208 SD."}, {"title": "V. Q-LEARNING FOR SYMBOLIC MODEL", "content": "Inspired by the construction in [13], this paper proposes to build the Q-learning on the reward structure in (7) and defines the maximal and minimal return by,\nGk=g(sk+1,ak+1)+\u00a5\u1e21(sk+2,ak+2)+y\u00b2\u04ef(sk+3, ak+3)+... =\nGk=g(sk+1, ak+1)+\u00a5g(sk+2,ak+2)+y\u00b2g(sk+3,ak+3)+...\nbecause Gk = \u03a3i=0\u03b3ig(sk+i+1, ak+i+1) resp. (Gk = \u03a3i=0\u03b3ig(sk+i+1, ak+i+1)) and Gk+1 = \u03a3i=0\u03b3iGk+i+2 resp. (Gk+1 = \u03a3i=0Gk+i+2), both satisfy the consistency relation,\nGk = g(sk+1, ak+1) + \u03b3Gk+1,\nGk = g(sk+1, ak+1) + \u03b3Gk+1\u00b7 (11)\nwhere \u03b3\u2208 (0,1] is the discount rate and the tail of a state run starting at time k \u2265 0, together with the corresponding sequence of rewards is given by,\nSk\nA(sk) g(sk+1,ak+1)\nSk+1\nA(sk+2) g(sk+2,ak+2)\nSk+2\nSk+3\nA(sk+3) g(sk+3,ak+3)\nWhile there may be some similarities between certain aspects of this section and the work by [13], it is important to highlight that the approach presented here is entirely novel. This paper introduces a distinct construction that leverages the concept of maximally and minimally return, which generalizes the one in [13]."}, {"title": "A. Maximal and Minimal State Value Functions under a Policy", "content": "The maximal and minimal state value functions of a state s \u2208 SD under a policy \u03c0, denoted \u016a\u03c0(s) and V\u03c0(s), respectively, are defined as the maximal and minimal return that can be obtained by starting in state s and following policy \u03c0.\n\u016a(s) = max {Gk | s = Sk, \u03c0(Sr) \u2208 AD,\u315c \u2265 k},\nU(s) = min {Gk | s = Sk, \u03c0(Sr) \u2208 AD,\u315c \u2265 k}.\nThe Bellman equations for the maximal and minimal state value functions can be derived directly from equation (11) and the definitions of V and u, as shown in the following proposition.\nProposition 2. For each (s,a) \u2208 SD \u00d7 AD, define \u2206\u03c0(s) = \u2206(s, \u03c0(s)), for all s \u2208 Sp. If the maximal and minimal value functions under policy \u03c0 exist, then they satisfy the following equations,\n\u03c5\u03c0(s) = \u1e21(s, \u03c0(s)) + \u03b3\nmax \u03c5\u03c0 (s'),\ns' \u0395\u0394\u03c0 (s)\n\u03c5\u03c0(s) = g(s, \u03c0(s)) + \u03b3\nmin \u03c5\u03c0 (s') . (12b)\ns' \u0395\u0394\u03c0 (s)"}, {"title": "Corollary 1. If y \u2208 (0,1), then the state value functions \u03c0 and v exist, are unique, and satisfy the Bellman equations (11).", "content": "Corollary 1. If y \u2208 (0,1), then the state value functions \u03c5\u03c0 and v\u03c0 exist, are unique, and satisfy the Bellman equations (11).\nProof. This proof has been omitted because it follows similar steps used to prove [13, Corollary 1] with the difference that each state value function is defined with its corresponding discrete reward function \u1e21 or g.\nIt is worth noting that in the context of non-deterministic transition system, two state value functions are need to be defined instead of a single one for two reasons. Firstly, for each state, there are two reward functions, \u1e21 and g, to capture all possible rewards when transiting to the successor state s' \u2208 \u2206\u03c0(s) C SD. Secondly, because the successor of a discrete state is not necessarily unique, these reward functions permits to identify the range of rewards of the successors."}, {"title": "B. Maximal and Minimal q-Functions under a Policy \u03c0", "content": "Now, define the maximal and minimal state-action value function for state s \u2208 SD and action a \u2208 AD under policy \u03c0 by,\n\u03b1\u03c0(s, a) = max {Gk | Sk = s, ak = a, \u03c0(s')},\nq(s, a) = min {Gk | Sk = s, ak = a, \u03c0(s')}.\nwhere \u1fe6\u03c0(s, a) and q (s, a), denote the maximal and minimal return that can be obtained starting in state s, taking action a, and following \u03c0 thereafter.\nThe following proposition is straightforward from the consistency relation (11) and the definitions of \u1fe6\u03c0, q\u03c0, \u03c5\u03c0, and \u03b1\u03c0."}, {"title": "Proposition 3. If y \u2208 (0,1), then \u0256\u201e and q\u201e exist, are unique, and satisfy the following consistency relations", "content": "\u012a\u201e(s,a) = \u1e21(s, \u03c0(s)) + \u03b3\nmax (s'),\ns' \u0395\u0394(s,a)\nq(s, a) = g(s, \u03c0(s)) + \u03b3\nmin q (s') . (14b)\ns' \u0395\u0394(s,a)\nIn view of (3) and (4), one has that \u03c5\u03c0(s) = \u012a\u03c0(\u03c2, \u03c0(s)) and \u03c5\u03c0(s) = q (\u03b4,\u03c0(s)). Therefore, combining these equations, one can obtain Bellman equations for \u0256 and q,\n\u012a\u201e(s, a) = \u1e21(s, \u03c0(s)) + \u03b3 max max \u03c5\u03c0 (s', a'), (15a)\ns' \u0395\u0394(s,a) a' EA(s')\nq(s, a) = g(s, \u03c0(s)) + \u03b3 min max q (s', a').\ns' \u0395\u0394(s,a) a' EA(s')"}, {"title": "C. The optimal state action value function", "content": "The maximally and minimally optimal value functions are defined as follows:\n\u2200s \u2208 SD, v*(s) = max\n\u03c0\n\u1fe6\u03c0(s), v*(s) = max v(s) (16)\n\u03c0\nwhere all maximally and minimally optimal policies \u03c0* (\u03c0*) have their own maximal optimal state-value function v* (v*). Using these functions, the maximally and minimally optimal state-action value functions are defined by,\nq*(s, a) = max\u012b\u201e(s, a),\nq*(s, a) = max q (s, a), (17b)\nwhere q*(s,a) is the maximal and minimal return that can be obtained starting in state s, taking action a, and acting optimally thereafter. These functions satisfy the following equations:\nq*(s, a) = \u1e21(s, a) + \u03b3\nmax\ns' \u0395\u0394(s,a) a'\u2208A(s')\nmax q* (s'),\nq*(s, a) = g(s, a) + \u03b3 min max q* (s'). (18b)\ns' \u0395\u0394(s,a) a' \u0395A(s')\nThe following theorem characterizes the existence and uniqueness of the maximally and minimally optimal state value functions of a symbolic model ED."}, {"title": "Theorem 2. If \u03b3 \u2208 (0,1), then there exists a unique maximally and minimally optimal state value function v* and a unique minimally optimal state value function v* that satisfy,", "content": "v*(s) = max {g(s, a) + \u03b3 max \u03c5*(s')}, (19a)\naEAD\ns'EA(s,a)\nv*(s) = max g(s, a) + \u03b3 min v*(s') (19b)\naEAD\ns'EA(s,a)"}, {"title": "Proof. By the Bellman optimality principle, optimal value functions involve taking the optimal action at the first step and acting optimally thereafter.", "content": "Proof. By the Bellman optimality principle, optimal value functions involve taking the optimal action at the first step and acting optimally thereafter. This leads to consistency relations:\nv*(s) = max {g(s, a) + \u03b3\nmax * (s')},\na\u0395\u0391\ns'\u0395\u0394(s,a)\nv*(s) = max {g(s, a) + \u03b3\nmin\na\u0395\u0391\ns'\u0395\u0394(s,a)\n*($')}.\nDefine TV(s) = maxa\u2208 A {r(s, a) + \u03b3 maxs'e\u25b3(s,a) V (s')} and T*V(s) = maxa\u2208 A {r(s, a) + \u03b3 mins'\u2208\u25b3(s,a) V (s')}, as operators. Then, for value functions V and U:\n||T*V \u2013 T*U||\n||T*V - T*U||\n\u2264 \u03b3||V \u2013 U||\u221e,\n\u2264 \u03b3||V - U||\u221e.\nThus, both operators T* and T* are \u03b3-contractions. Therefore, by the Banach fixed-point theorem, if \u03b3 \u2208 (0,1), there exists a unique maximally (minimally) optimal state value function v* (v*) satisfying (19).\nCorollary 2. If \u03b3\u2208 (0,1), then there exist deterministic maximally and minimally optimal policies for the symbolic model ED. In particular, these policies are given by,\n\u03c0*(s) = arg max q*(s,a), \u03c0*(s) = arg max q*(s,a). (20)\na\u0395\u0391\na\u0395\u0391"}, {"title": "Theorem 3. Under Assumption 4 is satisfied.", "content": "Theorem 3. Under Assumption 4 is satisfied. Let \u03be \u2208 S", "satisfies": "nq(k) (s", "q(0)": "by means of Assumption 4 the statement is true. Assume that q(k-1) (s", "have": "nq(k)(s", "inequality": "n-1) (s', a') \u2264 max -1) (\u03be', v')\n\u03c5EA("}]}