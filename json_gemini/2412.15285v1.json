{"title": "Maximize Your Data's Potential: Enhancing LLM Accuracy with Two-Phase Pretraining", "authors": ["Steven Y. Feng", "Shrimai Prabhumoye", "Kezhi Kong", "Dan Su", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro"], "abstract": "Pretraining large language models effectively requires strategic data selection, blending and ordering. However, key details about data mixtures especially their scalability to longer token horizons and larger model sizes remain under-explored due to limited disclosure by model developers. To address this, we formalize the concept of two-phase pretraining and conduct an extensive systematic study on how to select and mix data to maximize model accuracies for the two phases. Our findings illustrate that a two-phase approach for pretraining outperforms random data ordering and natural distribution of tokens by 3.4% and 17% on average accuracies. We provide in-depth guidance on crafting optimal blends based on quality of the data source and the number of epochs to be seen. We propose to design blends using down-sampled data at a smaller scale of 1T tokens and then demonstrate effective scaling of our approach to larger token horizon of 15T tokens and larger model size of 25B model size. These insights provide a series of steps practitioners can follow to design and scale their data blends.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLM) are typically pre-trained on large amounts of data in the order of billions (B) or trillions (T) of tokens derived from multiple data sources such as web crawl, books, papers, patents, mathematical and legal documents, and so forth (Brown et al., 2020; Parmar et al., 2024b; Team et al., 2024b; Dubey et al., 2024a; Nvidia et al., 2024). To develop a state-of-the-art model, it is critical to understand the nature of these data sources and to make informed decisions about optimal data blending (how different data sources are weighed during pretraining) and training strategies. These decisions typically involve running multiple large-scale experiments to empirically investigate the optimal training data blend(s) and ordering of data.\nMost advanced models (OpenAI et al., 2024; Dubey et al., 2024b) do not divulge information on the data blends that are used, nor the ablation studies informing the data mixing and ordering decisions. Recent works (Blakeney et al., 2024; Groeneveld et al., 2024; Dubey et al., 2024b; Snowflake, 2024) provide high-level data blend information about a small portion of pretraining by encouraging the upsampling of certain domains towards the end. In general, there exists a knowledge gap regarding how to craft and choose an optimal data blend(s) for the entire training process, and the generalizability of data blends and ordering strategies to larger token horizons and model sizes.\nIn this work, we address the above knowledge gap by understanding optimal data blends and ordering strategies for training LLM. We formalize and extensively explore a two-phase training approach (Figure 1) that balances diversity and quality: phase-1 emphasizes diverse, high-quality web crawl data, while phase-2 focuses on high-quality data sources such as math, code, and wiki data. Specifically, in this work we propose to use down-sampled data to prototype and explore multiple blends at a smaller scale of 1T tokens. We craft"}, {"title": "2 A Two-Phase Approach to Pretraining", "content": "In this work, we explore a two-phased approach to pretraining: phase-1 (P1) then phase-2 (P2). Figure 1 demonstrates our two-phased approach. In each phase, we explore different data blends based on the quality and number of epochs to be seen of a data source. In phase-1 (P1), we explore a general data distribution which consists of a mix of web crawl data, medium-quality data, and low amounts of high-quality data. In phase-2 (P2), we explore a blend which includes task data and emphasizes high-quality datasets such as math, code, and high-quality web crawl (\u00a75.1). As seen in Figure 1, our model sees the first general data blend during P1 for the majority of training, then a different data blend focused on high quality data during the shorter P2 of training.\nThe steps to create blends for P1 and P2 are: 1) Downsample a data source by a factor of f, 2) Estimate the quality of a data source (\u00a75.1), 3) Estimate the epochs to be seen in the whole pre-training (\u00a75.2) and finally 4) distribute the epochs appropriately in P\u2081 and P2 (\u00a73.2). The downsampling factor $f$ is based on the final total token budget which we assume to be 15T similar to Dubey et al. (2024b). Hence, for us $f = 1/15$ i.e for each data source, the number of tokens available for pretraining is 1/15th of the total token in that dataset. Downsampling helps to observe the impact of epochs of datasets at a smaller scale of 1T tokens and then can be used to scale the blend to a longer token horizon of 15T tokens using the full data.\nBaselines: Since our blends are based on quality and epoch based analyses of the data as well as the ordering of the data in the two phases, we consider the following two baselines: 1) Natural Distribution Blend (BASE-ND): This blend is based on ratio of the number of tokens available in each data source. The weight for each dataset is equal to the total number of tokens in that dataset divided by the sum of tokens available in all the datasets. This weighting is neither based on quality nor the epochs to be seen for the dataset. 2) Random Order Pretraining (BASE-RO): This blend is based on quality and epochs of each dataset but does not use two phases to train the model. The weight for each dataset here is the same as our two-phase approach but the order in which the the dataset is seen during pretraining is random."}, {"title": "3 Experimental Setup", "content": "3.1 Data Sources\nOur pretraining corpus spans a vast range of text data sources that cover several domains, types of data, and languages. We broadly divide our datasets into the following categories and their token counts in billions is shown in Table 1."}, {"title": "3.2 Data Blends for Each Phase", "content": "The final blends in P\u2081 and P2 are based on quality and epoch based ablations shown in \u00a75.1 and \u00a75.2. The insights from these studies are incorporated in Table 2 and 3.\nIn P1, we encourage diversity in data by including a high percentage of web crawl data which consists of high, medium, and low-quality crawl. We want to introduce a limited amount of high-quality data such as math, code, and wiki in P\u2081. In P2, the emphasis is primarily on high-quality datasets and only includes a limited amount of medium-quality data. For example, in P2, we only use high-quality crawl instead of medium or low-quality (see \u00a75.1).\nTable 2 details the five blends explored in P1. These blends are designed to compare the proportion of high-level categories with each other. The difference between Blend1 and Blend2 is that Blend2 has less code and more medium-quality datasets compared to Blend1. Blend3 has less web crawl and more medium-quality datasets compared to Blend1. Blend4 has less web crawl and more high-quality datasets compared to Blend1. Blend5 is designed to have majority web crawl at the cost of code and medium-quality data.\nTable 3 outlines the five blends explored in P2. In P2, we use more epochs and higher proportions of high-quality data such as high-quality web crawl, math, wiki, and code data. Blend3 has more code and less medium-quality datasets compared to Blend1, and Blend4 has more high-quality web crawl and less medium-quality datasets compared to Blend1. Blend2 has a more balanced distribution among the data categories, while Blend5 upsamples math data more heavily.\n3.3 Model Specifications\nWe experiment using the Megatron (Shoeybi et al., 2020) model, an autoregressive causal left-to-right LLM, with the Tiktokenizer (OpenAI, 2023). We downsample all our data by factor $f = 1/15$. Hence, only 1/15 of the tokens shown in Table 1 will be available for pretraining. We perform all our investigations using an 8 billion parameter model trained on 1 trillion total tokens. Furthermore, we test our two-phase approach by scaling along two dimensions: (1) we scale the token horizon to 1.7T tokens on a 8B model, and (2) we scale the parameters of the model to 25B and train on 1T tokens. Additionally, we train a 8B model on 15T tokens on full data (not downsampled) to observe if decisions made with downsampled data scales. Specifics on model architecture and hyperparameters are shared in Appendix A.\n3.4 Evaluation Suite\nTo comprehensively assess our models, we use various benchmarks that evaluate different capabilities."}, {"title": "4 Results for Two-Phase Pretraining", "content": "Findings\n\u2022 A two-phase approach for pretraining\nis effective.\n\u2022 Phase-1 should focus on data diversity\nand phase-2 on high-quality data.\nWe compare our best blends P1-Blend4-P2-Blend1 using two-phase training with two baselines: 1) BASE-ND: the weights are determined by the tokens available in each dataset and are not based on quality, and 2) BASE-RO: the weights for all the datasets are the same in this and P1-Blend4-P2-Blend1. The only difference is the order in which the data is presented during training (random or two-phased). Table 4 illustrates that using a quality and epoch based blend is on average 13.2% better than natural distribution blend (compare BASE-RO vs BASE-ND) across downstream tasks. It also presents that using our two-phase training approach noticeably improves average accuracy by 3.4% compared to BASE-RO and 17% compared to BASE-ND. This empirically demonstrates that the strategy of two-phase training is useful and tasks such as code and math are sensitive to the ordering of high-quality data in the second phase.\nWe scale our best blend P1-Blend4-P2-Blend1 to 15T tokens and use the full dataset to train a 8B model. All the previous experiments are performed on downsampled data and 1T scale. This means that the number of epochs is constant in both the runs. Table 5 shows that blends crafted at smaller scale can generalize to longer token budgets if the quality and epochs of the datasets are maintained at scale. This shows the generalizability of our two-phase approach to pretraining as well as quality- and epoch-based approach to designing blends.\n4.1 Determining Blends\nAs discussed in \u00a73.2, we explore five different blends for phase-1. We train an 8B model on downsampled data for 1T tokens for all five blends and eliminate blends based on a separately held-out validation split. Fig. 2 illustrates the validation loss for all five blends. As we can see, Blend5 and Blend2 had 2.8% and 2.1% higher validation loss, respectively, relative to Blend4 at approx. 250B tokens. Hence, we discontinue these two blends at"}, {"title": "4.2 Scaling", "content": "Findings\n\u2022 Two-phase approach is scalable and ro-\nbust to token horizon and model scale.\n\u2022 Data blends need adjusting at longer to-\nken horizons based on epoch count to\navoid high-quality data overexposure.\nWe further explore scaling our best blend along two dimensions: (1) a longer token horizon of 1.7 trillion tokens and (2) larger model size of 25B parameters. For a longer token horizon, we aim to assess whether the blend can be used as is or if adjustments are necessary to prevent overfitting (observed in \u00a75.2). Note that this is different from scaling to 15T token where we use the full data. Here we still use the downsampled data and scale to 1.7T tokens and hence the epochs seen of each dataset would be higher. Since high number of epochs of high-quality datasets are primarily seen in P2 of pretraining, we create a new blend, P2-Blend64, which is an epoch-adjusted version of P2-Blend1 to ensure that we do not see more than 8 epochs of certain high-quality data sources like math and task data. Table 8 shows the comparison of scaling from 1T to 1.7T total tokens. We see that P1-Blend4-P2-Blend6 is on average 2.2% better than P1-Blend4-P2-Blend1, illustrating that we need to adjust our blends according to the epoch counts of high-quality data for optimal results. Both the 1.7T models are better than 1T, demonstrating that we can still obtain higher downstream accuracies"}, {"title": "5 Ablations", "content": "This section details quality-based data blending, epoch study and percentage of phase-2 to be conducted in the pretraining. Additional fine-grained analyses of data blends and study on learning rate schedule to be used in phase-2 is shown in Appendix \u00a7D and \u00a7E."}, {"title": "5.1 Quality-Based Data Blending", "content": "Insights\n\u2022 Upsampling high quality and not using\nlow quality CC data is most effective.\n\u2022 CCdv, papers and books are similar in\nquality to CC-Medium-High.\nThe data blends of our two-phase approach are mainly based on the assessment of each data source's quality. Hence, we carry out extensive experiments to find an optimal data blend for web crawl documents. While previous work (Dubey et al., 2024a; Yang et al., 2024; Team et al., 2024a) mentions that web crawl documents like Common Crawl (CC) form a large majority of their pretraining data, none of them share a recipe on how to mix different slices of CC. Some recent work on constructing crawl-based pretraining datasets (Penedo et al., 2024b; Li et al., 2024a) directly use the high quality crawl documents in pretraining but provide no specific data mixing strategy. In this section, we provide comprehensive details on how to create a data blend for CC documents and use it effectively in our phase-1 and phase-2 of pretraining. Additionally, we provide a quality assessment of other datasets like CCav, papers, books and our high quality datasets. We compare them with medium, and high quality web crawl to position them optimally in our P\u2081 and P2 blends.\nQuality-Based Blending for Web Crawl: Each document in our web crawl data is classified into one of five quality categories: High, Medium-High, Medium, Medium-Low, and Low using the classifier from Su et al. (2024)."}, {"title": "5.2 Epoch-Based Analysis", "content": "Insights\n\u2022 We recommend 6 epochs of high-\nquality crawl and 8 epochs of math\nand task data for data mixing.\nWe take the number of epochs of high quality datasets into account while creating our P\u2081 and P2 blends. We experiment with different numbers of epochs for high-quality crawl, math, and task data. Since, majority of web crawl is used in P1, we pretrained an 8B model with 1T tokens, using different epochs of high-quality crawl tokens in the data mix, and evaluate each model's MMLU score. Note that we keep the overall percentage of web crawl the same in all the experiments. As we can see in Figure 4, increasing the number of high-quality tokens increases the MMLU score until 6 epochs. We primarily present MMLU score because these experiments do not include high amount of math or code data.\nSince, majority of math and task-data is seen in P2, Table 13 presents results for different numbers"}, {"title": "5.3 Optimal Duration of Phase-2", "content": "Insights\n\u2022 Pretraining with the P2 blend for the\nfinal 40% gives the best results.\nWe investigate the percentage of phase-2 to use in the whole pretraining regime. We experiment with 0 to 50% of P2 in the whole of pretraining. The longer the duration of P2, the shorter the duration of P1. We use the P1-Blend4-P2-Blend1 blend combination that we found best in \u00a74.\nTable 14 illustrates that a higher percentage of P2 until 40% is better overall, especially in math and code. Going above this, e.g. to P2 as 50% of training, downstream accuracies start to degrade across the board, potentially due to overfitting."}, {"title": "6 Related Work", "content": "Selecting and structuring pretraining datasets is important to improve model generalization and efficiency. Dubey et al. (2024b) emphasize openness and accessibility of models, Computer (2023); Soldaini et al. (2024) assemble an open corpus of trillions of tokens for large-scale training, and Groeneveld et al. (2024) release a truly Open Language Model, including its framework, training data, and code. Studies such as Li et al. (2024b); Penedo et al. (2024a) demonstrating that refined data selection impacts model accuracies more significantly than simply the quantity of data. But these studies are primarily aimed at CC data and they do not suggest any data mixing strategies for pretraining. Parmar et al. (2024a) provide a systematic approach to building effective LLM pretraining datasets with ablations on data attributes, and existing curation, selection, and sampling methods. In our work, we provide a systematic approach to craft data blends and to order the data in pretraining.\nStrategic weighting and timing of data usage can also noticeably impact model accuracies. Techniques like domain upsampling (Blakeney et al., 2024; Dubey et al., 2024b) towards the end of training have been shown to be effective. Snowflake (2024); Groeneveld et al. (2024) provide details about high level blends for their pretraining process. In contrast, our work provides fine grained details about the data blend creation process along with actionable steps that model developers can use to develop data blends and order. Prior work (Shen et al., 2023; Longpre et al., 2024; Mindermann et al., 2022; Xie et al., 2023a,b; Shao et al., 2024) investigates optimizing data mixtures based on clustering methods, manually designed domain composition weights, proxy models or reference models to determine data composition weights and sample-level data selection. Our work primarily focuses on data ordering and scaling of data blends in pretraining and can be used in conjunction with other data sampling techniques.\nCurriculum learning approaches inspired by human learning offer an ordered way to introduce data gradually to enhance model learning. Martinez et al. (2023); Wang et al. (2022); Feng et al. (2024) investigate cognitively-motivated curriculum-based training including vocabulary, and objective curricula, and outline and the challenges and potential solutions for designing effective curricula. Our work shows that ordering of data based on quality in pretraining LLMs has a significant impact of downstream accuracies."}, {"title": "7 Conclusion", "content": "In conclusion, through extensive experiments, we demonstrate the effectiveness of a two-phase pretraining approach for LLM. For the initial training phase, a more general data distribution consisting of mainly of web crawl proves most effective, while phase two benefits from a comprehensive data blend, with additional focus on math, code, and task data. Phase-two for the last \u224840% of training yields the best results, and over-extending it leads to diminishing returns. Increasing model size and token horizon further enhances accuracy, demonstrating the scalability of our approach. Importantly, we also show that considering both the quality of the data (including web crawl) and the number of epochs of each data source is crucial to attain optimal results and prevent overfitting."}, {"title": "Limitations", "content": "Some limitations of our work include our present suite of models and evaluation benchmarks. We can extend our work and show the effectiveness of two-phase pretraining approach on more LLM architectures such as Mamba (Gu and Dao, 2023), other hybrid SSM based architectures (Glorioso et al., 2024; Lieber et al., 2024) and mixture of experts (Shazeer et al., 2017). While our evaluation benchmarks are quite comprehensive, we could potentially expand to an even broader range of evaluations, including nuanced domain-specific or interactive tasks, or more theory of mind and developmental psychology-inspired benchmarks. This includes assessing capabilities such as analogical reasoning (Webb et al., 2023). Further, our scaling experiments could be expanded. Scaling up to hundreds of billions of parameters or significantly longer training may yield additional insights. Lastly, while our work focuses on two-phase training and shows its efficacy, we can potentially investigate multi-phase training, and the impact of the order of the phases. However, we believe this is more suited for future work. Overall, these are directions to potentially improve and expand upon our work. Despite these potential limitations, we feel that our current work is an insightful and useful contribution to the research community."}, {"title": "Ethical Considerations", "content": "Our research uses publicly available and commonly used datasets in LLM development. These sources, including Common Crawl, Wikipedia, and code repositories, are widely adopted in the research community. We examined the quality and origins of our data, prioritizing high-quality, domain-relevant data sources to improve LLM capabilities in a responsible manner. However, web crawl data may inherently contain biases or inappropriate content despite filtering efforts. We used established data cleaning and quality assurance procedures but acknowledge that potential biases may persist and impact model behavior in certain circumstances.\nWe recognize that scaling models and exploring data blending strategies require significant computational resources, which may raise environmental concerns. To mitigate this, we focused on efficient training strategies, such as two-phase training, to improve accuracy without excessively increasing resource usage. Future studies could benefit from exploring energy-efficient training methods to further minimize the environmental impact.\nOur models, data blends, and accompanying publication are intended solely for research purposes, with no intended real-world application without additional safety evaluations. We caution against deploying models based on our methods without thorough testing, as they may carry unknown risks, particularly when applied to tasks involving sensitive or personal information. Our work aims to advance the understanding of LLM training strategies, and we feel that it is an important contribution to the research community. We encourage researchers to expand upon our work while further investigating the ethical and societal implications of LLM."}]}