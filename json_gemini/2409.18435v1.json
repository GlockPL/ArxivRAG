{"title": "MULTI-AGENT REINFORCEMENT LEARNING FOR DYNAMIC DISPATCHING IN MATERIAL HANDLING SYSTEMS", "authors": ["Xian Yeow Lee", "Haiyan Wang", "Takaharu Matsui", "Daisuke Katsumata", "Chetan Gupta"], "abstract": "This paper proposes a multi-agent reinforcement learning (MARL) approach to learn dynamic dispatching strategies, which is crucial for optimizing throughput in material handling systems across diverse industries. To benchmark our method, we developed a material handling environment that reflects the complexities of an actual system, such as various activities at different locations, physical constraints, and inherent uncertainties. To enhance exploration during learning, we propose a method to integrate domain knowledge in the form of existing dynamic dispatching heuristics. Our experimental results show that our method can outperform heuristics by up to 7.4% in terms of median throughput. Additionally, we analyze the effect of different architectures on MARL performance when training multiple agents with different functions. We also demonstrate that the MARL agents' performance can be further improved by using the first iteration of MARL agents as heuristics to train a second iteration of MARL agents. This work demonstrates the potential of applying MARL to learn effective dynamic dispatching strategies that may be deployed in real-world systems to improve business outcomes.", "sections": [{"title": "Introduction", "content": "Material handling systems are integral to warehousing and logistics operations across industries, playing a pivotal role in ensuring efficient material flow [Bhandari et al., 2023]. Achieving optimal performance metrics, such as maximizing throughput, within these systems can have cascading effects on downstream business processes, resulting in streamlined operations and improved efficiency. Dynamic dispatching that involves real-time task allocation and resource management is crucial to achieving optimal performance. Traditionally, heuristics dispatching rules, such as shortest route, and nearest location, are usually employed. However, these rules are often sub-optimal in complex material handling systems due to various real-world challenges, such as inherent uncertainties at the input, output, and system dynamics, interconnected sub-processes that leads to complex interactions between the sub-processes, and system changes (such as change in layout or cycle times) due to business expansion or reduction.\nReinforcement Learning (RL) offers a promising avenue for overcoming the challenges and enhancing dynamic dispatching, allowing algorithms to adapt and optimize decisions in real-time scenarios [Kayhan and Yildiz, 2023, Shyalika et al., 2020]. However, training RL algorithms requires a simulator to mimic real-world complexities for algorithm development and testing, as it is often cost-prohibitive and infeasible to train RL algorithms in actual systems. In this work, we aim to develop a framework to train event-based multi-agent RL (MARL) strategies to improve the key performance index (KPI)s of the material handling systems and contribute to the following: We :1) formulate the dynamic dispatching aspect of material handling systems as an optimization problem, which may be solved using RL, 2): develop an environment that mimics the characteristics and reflects the complexity of an actual conveyor material"}, {"title": "Dynamic Dispatching for Material Handling Systems", "content": "In this section, we introduce an instance of a generic material handling system that is widely used in a variety of industries and would benefit from optimized dynamic dispatching strategies. We consider a system that consists of a conveyor belt that transports goods from multiple points to multiple destinations. This system consists of three types of points (incoming points, storage points, and outgoing points) and two major processes (receiving and shipping processes). As an example, we refer readers to a simplified example in Fig. 1. As shown in the figure, material handling systems typically have input points through which the goods enter the system. We represent these as incoming points, illustrated by white circles. At these incoming points, goods are loaded onto pallets which are transported via the conveyor to one of the many storage points, represented by gray circles. The transportation of the goods from the input points to the storage points constitutes the receiving process. Simultaneously, there is a shipping process, which consists of transporting selected goods from the storage points to the outgoing points on pallets. The selection of the goods from the storage to the outgoing points is stochastic and based on the demand of the shipping process. In this system, there is a fixed number of pallets that are constantly circulating in the system. As such, this results in pallets that either carry goods to the storage or outgoing points or empty pallets that are required at the incoming or storage points for loading the goods.\nWe consider two main classes of decisions that have to be made in these systems, illustrated by green diamonds in Fig. 1. The first class of decisions are the dispatching decisions made at incoming points, where the system decides which storage points to send the incoming goods to. The second class of decisions represents the dispatching decision made at junctions, where the system decides which direction to send empty pallets, which will subsequently affect the number of empty pallets available to feed the incoming and storage points. Thus, the goal of the dynamic dispatching problem here is to make decisions at each of the decision points such that the total receiving throughput (number of pallets entering the storage points) and total shipping throughput (number of pallets entering the outgoing points) are maximized. The main challenge of this problem emerges from the fact that all three points (incoming, storage, and outgoing) are interconnected via a shared and limited resource: the pallets. Furthermore, the complexity of the problem is often exacerbated by constraints imposed by the system's physical design, such as a limit on the number of pallets allowed on certain sections of the conveyor. Additionally, while pallets with goods are routed from their source to destination via the shortest route in the ideal case, sub-optimal decisions could cause the shortest route to be congested, making a longer route more efficient. Thus, an optimal dynamic dispatching strategy must coordinate between different decision points to achieve a high overall throughput."}, {"title": "Problem Formulation", "content": "In this section, we first formalize the dynamic dispatching problem as an optimization problem, then posit it as a MARL problem. Based on the description above, the dynamic dispatching problem for material handling can be posed as:\n$\\begin{aligned}\n&\\text{Max:} & \\sum_{t=0}^{T} F(x_t, y_t) \\\\\n&\\text{subject to:} & C_x(x_t) \\le 0, \\\\\n& & C_y(y_t) \\le 0, \\\\\n& & C_{x_t, y_t} = H(x_{t-1}, y_{t-1})\n\\end{aligned}$\nwhere $F(.)$ denotes the objective function, $x_t, y_t$ denote the optimization variables (in this case, representing the two class of decisions), $C_x(.), C_y(.)$ the constraints on $x_t, y_t$, where $C_x(.), C_y(.)$ are a consequence of a previous"}, {"title": "Related works and discussions", "content": "We briefly discuss existing related works from multiple perspectives: 1) works that are based on heuristic/optimization approaches for dynamic dispatching, 2) works that leverage (MA)RL for dynamic dispatching, and 3) research areas related to ideas of this work. Traditionally, dynamic dispatching has depended mainly on manually but expertly designed rules [Rajendran and Holthaus, 1999, \u0110urasevi\u0107 and Jakobovi\u0107, 2018, Yoon and Albert, 2021]. These works are not necessarily limited to material handling systems and are generalizable to many industries, but they often require the expertise of a subject matter expert, which has become an increasing challenge due to labor shortages in many industries. Beyond manually-designed rules, there are also efforts to develop optimization-based methods [Jia et al., 2017, Gohareh and Mansouri, 2022, Wang et al., 2023]. These methods often employ some version of evolutionary or swarm-based optimization, coupled with simulations, to generate dynamic dispatching policies. Despite the challenges of applying RL to dynamic dispatching [Khorasgani et al., 2020], the research community has strive to develop RL-based approaches due to RL's potential to generalize and handle uncertainties. We refer interested readers to the following papers for a detailed review of RL-based approaches for dynamic dispatching across various applications Kayhan and Yildiz [2023], Shyalika et al. [2020], Panzer et al. [2021], Bahrpeyma and Reichelt [2022]. Last but not least, the proposed idea in this paper of using heuristics alongside the MARL policies is just one way to aid exploration during training. We highlight that similar ideas have been explored in works which leverages expert demonstrations [Ram\u00edrez et al., 2022] and there are also numerous heuristic-agnostic methods that focus on improving exploration, such as the methods discussed in the following works by Ladosz et al. [2022], Yang et al. [2021], Hao et al. [2023]."}, {"title": "Environment", "content": "To evaluate the feasibility of a MARL-based dynamic dispatching approach, we developed a Python-based simulator that serves as a training platform. We benchmarked the simulator by implementing several heuristics and validated that the simulated KPIs reflect the KPIs of an actual proprietary material handling system, thus ensuring the simulator has sufficiently high fidelity. We then develop a training environment following the convention of PettingZoo [Terry et al., 2021].\nIn our experiments, we consider a three-loop material handling system with a conveyor belt that transports material from the incoming points to the storage and from the storage to the outgoing points, as described above. The system consists of four incoming points, twenty storage points, and six outgoing points, with 500 available pallets. The demand at the outgoing points is modeled according to the statistics of the actual system and is significantly non-uniform. Furthermore, we imposed additional rules on the environment to reflect actual constraints due to the design of the material handling system. Specifically, each incoming, storage, and outgoing point has a designated buffer for the number of pallets that can be in the queue to be processed. If the point's buffer is full, then pallets would be rerouted around the conveyor belt until the buffer is available. Additionally, there is a limit on the maximum number of pallets that can be present on the conveyor belt section connecting the different loops. If the number of pallets is exceeded, the junction points that control the flow of pallets between two different loops may either 1) stop the flow of pallets if both downstream segments of the conveyor are full, thus causing potential congestion upstream or 2) send the pallets on a"}, {"title": "Methods", "content": "In this section, we propose a framework to train MARL-based dynamic dispatching policies that outperform several manually designed heuristics. We conjectured and empirically observed that training vanilla MARL policies would be challenging due to a large combinatorial space of decisions, asynchronous event-based decision-making, and environmental constraints that may override the dispatching decision. On the other hand, businesses and domain experts who operate material handling systems often have invested time into developing heuristics based on their experiences from operating the system. While these heuristics may not be optimal, they are often better than the performance of training a MARL-based policy from scratch. As such, we propose to leverage these heuristics to inject domain knowledge into the learning process of the MARL training. Specifically, during training, we systematically interleave the heuristic's actions with the actions of the MARL policies and store all the actions and associated rewards in memory. Consequently, the MARL policies are trained using transition tuples consisting of its actions and associated rewards and heuristic actions and their associated rewards. By interleaving heuristics' actions into the training process, we utilize the existing domain knowledge as an exploration tool that could potentially guide the MARL policies into a regions of high-performing policies. To concretize our framework, we used a Monte-Carlo version of multi-agent proximal policy optimization (PPO) [Yu et al., 2022] with decentralized actors and a centralized critic to train the dynamic dispatching policies. Note that the choice of using PPO in the multi-agent setting is mainly driven by the simplicity of implementation, as compared to more advanced methods that requires the instantiation of multiple classes of networks such as Q-mix [Rashid et al., 2020], and the observed empirical effectiveness of PPO over more simple value-based methods such as IQL Tan [1993]. We refer readers to more discussions on the choice of the MARL algorithm in"}, {"title": "Experiments and Results", "content": null}, {"title": "Comparing MARL-based policies with existing heuristics", "content": "For our initial experiments, we consider a simpler version of the dynamic dispatching problem, where we only optimize the decisions at the incoming points to dispatch pallets to storage points rather than jointly optimizing the decisions at the incoming and junction points. For comparison, we implemented three heuristics that were designed manually by experts. The first heuristic sends the pallets to random storage points in the same loop. This is based on the principle that storage points within the same loop are closer in distance to storage points in other loops. The second heuristic we consider is a set of rules that select the optimal storage point based on the number of pallets in each storage point's buffer, the distance to each storage point, and the storage point's number of incoming and outgoing pallets. Finally, the third heuristic we implemented is similar to the second one, with an additional rule that considers the congestion at the junctions between the loops. The algorithms on the three heuristics developed are presented in Appendix B. We denote the first, second, and third heuristics as 'Low', 'Medium' and 'High' based on their respective evaluated performance. To ensure fair benchmarking, we had systematically fine-tune the hyperparameters associated with each heuristic (if any) to ensure that the performance exhibited by each heuristic is the maximized. For the dispatching decisions at the junctions, we implemented a heuristic that sends empty pallets into the direction of the conveyor loops, which have the least number of pallets. We used these three heuristics in our framework to train three instances of MARL-based dynamic dispatching policies. In each experiment, the material handling system consists of four incoming"}, {"title": "Comparing different architectures for MARL-policies with different functions", "content": "Having validated that MARL policies trained with heuristics can outperform a pure heuristic, we experimented with replacing both classes of dispatching decisions with MARL agents, specifically one class of agents that dispatches at the incoming points and one class that dispatches at the junctions between loops. In this scenario, we trained eight agents, four at the incoming points and four at the junction points. While the state space of these two types of agents is the same, the action spaces are different. Since the functions of both types of agents are different, we explore two"}, {"title": "Decoupling MARL policies from influences of heuristics", "content": "Motivated by the observation that the presence of the heuristic potentially limits the MARL's performance during evaluation, we conducted another set of experiments where we trained a second iteration of MARL policies from scratch. However, in this iteration, we use the first iteration of MARL policies as the heuristics by freezing the weights of the policies rather than the original set of heuristics. Our intuition is that since the first iteration of MARL agents performs better without heuristics during evaluation, they act as better exploration tools than the original set of heuristics. We performed this set of experiments using the same experimental parameters as before. Fig. 4 illustrates the evaluation results, where we show the progression from the best heuristic to the application of MARL policies at the incoming points, then to the application of MARL policies at both receiving and junction points and finally to the second iteration of MARL policies that were trained with previous iterations of trained policies, denoted as 'MARL + MARL*'. As shown in the figure, we observed that once again, removing heuristic decisions during evaluation enables a higher total throughput, and training the MARL policies with a previous iteration of MARL policies results in marginally higher throughput. Overall, by using the initial best heuristic to train the first iteration of MARL policies and then repeating the process to train a second iteration of MARL policies, we increased the median throughput from 4552 to 4712, representing a 3.51% increase in total throughput when compared with the best heuristic, and also increased the median throughput from 4349, representing an 8.34% improvement when compared to the random dispatching strategy. For detailed statistics of the results and an analysis of the decisions made by the MARL and heuristic policies, please refer to Table 3 and Fig. 6 in the Appendix."}, {"title": "Conclusion", "content": "Dynamic dispatching strategies are critical to increasing throughput in material handling systems, which are widely used in many industries. In this work, we propose an event-based MARL framework to learn an instance of a dynamic dispatching strategy. Using a MARL framework enables us to potentially scale to arbitrary systems sizes. To enhance the performance of the MARL algorithm, we proposed a method to leverage existing domain knowledge in the form of heuristics to improve the exploration capability and to yield a multi-agent strategy that outperforms the best heuristic by 3% in an environment that reflects the complexity of an actual material handling system. We also showed that the trained MARL policies could be used as fixed heuristics to train a newer set of policies that are independent of the original heuristics. Future work will focus on integrating more sophisticated exploration methods and solving the challenges of deployment in an actual system."}, {"title": "Appendix", "content": null}, {"title": "Background on PPO and MARL", "content": null}, {"title": "Proximal Policy Optimization (PPO)", "content": "Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. The agent's goal is to learn a policy $\\pi(a|s)$, which maps states s to actions a, in a way that maximizes the cumulative reward over time. The agent receives feedback from the environment in the form of rewards, which it uses to update its policy. Traditional methods in RL include value-based methods, such as Q-learning, and policy-based methods, such as policy gradient algorithms.\nPolicy gradient methods directly parameterize the policy $\\pi_{\\theta}(a|s)$ using parameters @ and optimize these parameters by maximizing the expected cumulative reward. One of the simplest forms of policy gradient is the REINFORCE algorithm, which updates the policy parameters @ in the direction of the gradient of the expected reward:\n$\\nabla_{\\theta} J(\\theta) = E_t [\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) R_t]$,\nwhere $R_t$ is the return, representing the cumulative reward from time step t. Although effective, traditional policy gradient methods can suffer from high variance and instability due to large updates to the policy.\nIn this work, we have used the Proximal Policy Optimization (PPO) as the choice of algorithm within our MARL framework. PPO is a popular algorithm in the field of RL and an improvement over traditional policy gradient methods, designed to optimize the policy in an efficient and stable manner.\nPPO operates by iteratively improving a stochastic policy $\\pi_{\\theta}(a|s)$, where $\\theta$ denotes the parameters of the policy network, s represents the state, and a represents the action. The key innovation in PPO is the use of a clipped surrogate objective"}, {"title": "Multi-Agent Reinforcement Learning (MARL)", "content": "Multi-Agent Reinforcement Learning (MARL) extends the principles of RL to environments where multiple agents interact and learn concurrently. Each agent in a MARL setting aims to maximize its own cumulative reward, which often depends on the actions of other agents, leading to a dynamic and interactive learning process. In the context of our work, the reward function (the total throughput) is also a shared reward function, hence creating a cooperative scenario. These complexities introduces challenges such as non-stationarity, where the environment's dynamics change as other agents learn and adapt.\nIn a MARL environment, each agent i has its own policy $\\pi_{\\theta_i}(a_i|s_i)$, where $\\theta_i$ denotes the parameters of agent i's policy network, $a_i$ represents the action taken by agent i, and $s_i$ represents the state observed by agent i. The goal of each agent is to maximize its own expected return $J_i(\\theta_i)$:\n$J_i(\\theta_i) = E \\left[ \\sum_{t=0}^{T} \\gamma^t r_t^i \\right]$,\nwhere $r_t^i$ is the reward received by agent i at time step t and $\\gamma$ is the discount factor."}, {"title": "Centralized Training with Decentralized Execution (CTDE)", "content": "A popular paradigm in MARL is Centralized Training with Decentralized Execution (CTDE). This approach leverages the advantages of centralized information during training while allowing agents to operate independently during execution. During the training phase, agents can access the global state and the actions of other agents, facilitating more coordinated and efficient learning. However, during execution, each agent acts based solely on its local observations, ensuring scalability and robustness in decentralized settings.\nMathematically, let S denote the global state space and Oi denote the observation space of agent i. During training, each agent's policy $\\pi_{\\theta_i}$ can condition on the global state s \u2208 S and the actions of other agents ${a_j}_{j\u2260i}$. The centralized value function $V(s, {a_i}_{i=1}^N)$ can be used to estimate the joint value of the state and actions, leading to better-informed policy updates.\nDuring execution, each agent i uses its decentralized policy $\\pi_{\\theta_i}(a_i|o_i)$, where $o_i \u2208 O_i$ is the local observation. This ensures that agents can operate independently and react to their local environment without requiring centralized coordination, which is crucial for real-world applications where communication may be limited or costly. CTDE strikes a balance between leveraging global information to enhance learning and maintaining the practicality of decentralized decision-making, making it a powerful approach in multi-agent systems. In our implementation, due to the asynchronous aspects of the environment, we only condition each policy on the global, shared state $s_t$ and not on the actions of other policies."}, {"title": "Heuristic's details", "content": "In this section, we detail the three heuristics that were used as baselines in the paper. These heuristics were developed manually developed based on the domain's experts intuition and further fine-tuned using the simulation model to achieve the best results.\nTo begin, we define several auxiliary variables and functions that may be used in each of the heuristics respectively. In general, each storage point in the system belongs to one of the many \"loops\" of the conveyor system. As such, for any pair of storage and incoming point, we can define if the storage point belong to the same loop as the incoming point or a different loop. Hence, we define $S_{same}$ as the set of all storage points that belong to the same loop as the incoming point, $S_{other}$ as the set of all storage points that belong to the other loops, and $S_{all}$ as the set of all storage points, where $S_{all} = S_{same} \\cup S_{other}$. Furthermore, we also define $I_n(s)$ as a function that returns the number of incoming pallets assigned to a given storage point, and $O_{ut}(s)$ returns the number of outgoing pallets being retrieved from the storage point from all the outgoing points. Additionally, we also define $X_{same}$ as the number of pallets assigned to all storage points in the same loop for a given incoming point, and $X_{other}$ as the number of pallets assigned to all storage points in the other neighbouring loops for a given incoming point. Finally, in Heuristic 2, we also defined a cost function $minCost(L)$. Specifically, the cost function is defined as:\n$\\text{minCost}(L_i) = \\frac{X_{same} - X_{min}}{X_{max} - X_{min}} + C_L$,\nwhere $X_{max}$ and $X_{min}$ denotes the number of pallets in the loop with the maximum/minimum number of pallets in the entire system and $C_{L}$ is a constant scalar cost value that is proportional to the distance it takes to send a pallet from loop i to loop j. Note that $C_{L}$ is a parameter that can be tuned by the user. Additionally, the Medium and High heuristics also requires an initialization of several parameters, which can be fine-tuned by the user depending on the system characteristics 1."}, {"title": "Visualization of heuristic v.s. MARL decisions", "content": "In Figure 6, we visualize and compare the decisions made by the best MARL policy (Hybrid MARL + MARL*, Separate Critic and Non-Assisted) with the decisions of the best heuristic (H). An interesting observation is that in the first four subplots, representing the dispatching at the incoming points, we see that the heuristics decisions are relatively stable. In contrast, the decisions of the MARL policies are significantly more erratic and there are almost no agreements between the MARL and heuristic in terms of the decisions made at the same time, except for a few events. This observation implies that a policy that results in a better total throughput would most likely require the design of a very complex heuristic, which may not be feasible to be designed manually. From the bottom four subplots, we observe that surprisingly, the heuristics consistently makes the same dispatching decision. Nevertheless, in three of the subplots, the MARL policies essentially imitates the decisions of the heuristic, while in the last subplot, the MARL policy sometimes deviates from the heuristic. We take this observation as an additional validation that the MARL policies are capable of making a stable, consistent decisions and only make decisions that deviate from the heuristic when it results in a higher reward."}, {"title": "Broad discussions on deployment and generalizations", "content": "In this section, we discuss the broader implications of our proposed framework on various industries and some practical challenges in deploying such a solution. Although we utilized a conveyor system in warehouses as a specific case study to demonstrate the strengths of the proposed method, we envision that the proposed method of combining Multi-Agent Reinforcement Learning (MARL) with existing heuristics to learn a dispatching policy is broadly generalizable to any material handling system that consists of multiple, possibly asynchronous, dynamic decision-making agents. This generalization implies that our method is broadly applicable to numerous industries, such as mining and agriculture, where raw and intermediary materials often need to be transported to several downstream locations. The timing of these transportation activities is often stochastic and dependent on numerous upstream processes. It is worth noting that even a small improvement in efficiency in many of these applications can result in a significant positive impact due to the sheer volume of material being transported.\nRegarding deployment challenges, several aspects need to be considered. One main limiting factor or assumption is the existence of a simulator, as it is often impractical to run MARL algorithms in actual scenarios. However, this assumption is not limited to MARL methods but also applies to other optimization-based and heuristic approaches, as domain experts usually validate their methods on a simulator before deploying any heuristics. In terms of computational requirements, the most computationally intensive part of developing the proposed approach is the training phase. We have demonstrated that we can train MARL policies efficiently for a material handling system equivalent in scale to actual systems on conventional consumer hardware. While this is just one example, we do not believe that computational requirements are a limiting factor in deploying such solutions, given recent advancements in computing capabilities.\nTwo remaining aspects to consider are integration with existing infrastructure and synchronization of the multi-agent policies. Regarding integration, we limited the state space of the MARL agents to information previously used by existing heuristics, so we do not foresee the need for additional infrastructure or sensors for the input. On the output side, while different applications and industries use different frameworks and infrastructures, we note that during deployment, the trained policy is simply a set of numerical weights that can be instantiated in Python, which can be easily wrapped with commercial APIs to communicate with existing infrastructure.\nConventional MARL approaches often assume a synchronous decision-making step among the different agents, which can be unrealistic when deployed. In our work, we explicitly designed the framework to be asynchronous, circumventing the issue of synchronous decision-making. Additionally, as described in the main paper, using the Centralized Training with Decentralized Execution (CTDE) training paradigm allows us to run the multiple agents independently in a decentralized fashion during deployment, further simplifying the task of deployment. Essentially, once the multi-agent RL policies are sufficiently trained together, each policy can be wrapped in an API that communicates with the existing infrastructure to receive sensor information when queried and returns a dispatching decision.\nFinally, it is worth noting that in our specific application, we assumed communication latency is not an issue since the frequency of dispatching decisions is several magnitudes slower than the latency to transfer the relatively low-dimensional data streams. However, it is important to consider latency during deployment if the state space requires higher-dimensional data such as image or video input."}]}