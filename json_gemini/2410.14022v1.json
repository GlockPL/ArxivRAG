{"title": "Vision-Language-Action Model and Diffusion Policy Switching\nEnables Dexterous Control of an Anthropomorphic Hand", "authors": ["Cheng Pan", "Kai Junge", "Josie Hughes"], "abstract": "To advance autonomous dexterous manipulation,\nwe propose a hybrid control method that combines the rela-\ntive advantages of a fine-tuned Vision-Language-Action (VLA)\nmodel and diffusion models. The VLA model provides language\ncommanded high-level planning, which is highly generalizable,\nwhile the diffusion model handles low-level interactions which\noffers the precision and robustness required for specific objects\nand environments. By incorporating a switching signal into the\ntraining-data, we enable event based transitions between these\ntwo models for a pick-and-place task where the target object\nand placement location is commanded through language. This\napproach is deployed on our anthropomorphic ADAPT Hand 2,\na 13DoF robotic hand, which incorporates compliance through\nseries elastic actuation allowing for resilience for any interactions:\nshowing the first use of a multi-fingered hand controlled with\na VLA model. We demonstrate this model switching approach\nresults in a over 80% success rate compared to under 40% when\nonly using a VLA model, enabled by accurate near-object arm\nmotion by the VLA model and a multi-modal grasping motion\nwith error recovery abilities from the diffusion model.", "sections": [{"title": "I. INTRODUCTION", "content": "Dexterous manipulation requires not only a robot hand\nwith the physical capacities, but also advanced task planning,\nwrist trajectory generation, contextual selection of grasp types\nand precise hand control [1]. This requires trained models or\ncontrollers with different goals or capabilities to seamlessly\nintegrate together to achieve a start-to-end solution. Recent\ndevelopments in Large Language models [2] and Visual-\nLanguage-Action Models for Robots (VLAs) [3] have shown\nthat they offer an effective approach for high-level planning\nbased upon language inputs. These models require large\ntraining data-sets and make us of robotic manipulation data-\nsets such as the open-X-embodiement which contains 1.5M\ntrajectories [4], or others with an order of 60-70k trajectories\n[5], [6]. The trained models have been effectively applied to\na number of pinch grippers with different embodiments to\nallow translation of text input to motion planning [4], [7]. The\ncomplexity of these generalist models for robotic manipulation\nis growing, from the notable RT-1 transformer model in 2022\n[8], to the RT-2 model [9] in 2023 with 55 Billion parameters,\nand recently the generalised manipulation Octo model with\n93 Million parameters [10], However, VLAs have been only\ndemonstrated its use with 1 DoF pinch grippers. For more\ndexterous, anthropomorphic manipulators it is unclear if these\nlarge models can provide the precision and complex actuation\nsignals required.\nThere are a number of existing methods which could be\ncombined with the high level planning capabilities of VLAs\nfor dexterous manipulators. The use of visual servoing [11] is\na widely applied method for approaching and grasping objects,\nand has been used effectively for three finger hands and anthro-\npomorphic hands. However, this approach struggles to utilize\ncontextual or environmental information to inform a grasping\nstrategy or policy. One approach for dexterous control of more\ncomplex manipulators is the use of diffusion models [12]\nwhich are well suited for robot motion learning and planning.\nThese have been successfully applied to manipulation [13],\n[14], and extended to work on a multi-fingered hand [15], [16].\nThese models typically require data-collection and training for\na specific task, however, they have the capacity for smooth\nand precise control, capturing of contextual information and\ndirect generation of motor control signals. The advantages of\ndiffusion models correspond to some of the weaknesses of\nVLAs, such that a hybrid approach could be advantageous for\nanthropmorphic dexterous manipulation.\nWe propose a hybrid framework (Fig. 1) for dexterous ma-\nnipulation which combines a fine-tuned openVLA pre-trained\nmodel [7] for language conditioned arm and hand positioning,\nwith a diffusion policy for dexterous grasping [12] that is able\nto apply on a multi-fingered hand. Although we propose this"}, {"title": "II. METHODS", "content": "In this section, we describe the methodology of the proposed\nhybrid VLA-diffusion model and experimental setup used to\nperform an autonomous pick-and-place task involving multiple\nobjects and placement locations."}, {"title": "A. VLA & Diffusion Model Switching Framework", "content": "Our framework switches between VLA and diffusion mod-\nels, to leverage the benefits of both. Due to the differences and\nstructural limitations of both, this is non-trivial. For high level\ntext to robot arm motion, our framework uses openVLA, a\nmodel pre-trained on robotic arms with pinch grippers [7].\nGiven a language input at the beginning, based on visual\nfeedback this model outputs 6 values for the robot arm end\neffector pose, and one signal to control the grasp percentage.\nFor grasping control, a diffusion policy model [12] is used,\ncapable of learning motion controllers from arbitrary sensor\ndata input to an arbitrary number of output commands. As\nsuch, the two models have different inputs and outputs and\nproviding different robot control signals. Secondly, to integrate\nthe two methods, the switching between the two models must\nbe automatic. This requires a 'stopping condition' signal to\nmark the end of the motion execution from each model: not\na property of both models a priori.\nFig.2 illustrates how the two models are used synergistically\ntogether and how they are switched during execution by\ntracking the evolution of a float parameter, the event signal\n$\\sigma$. The VLA is used to generate the movement of the arm,\nto move the hand to the proximity to the object. The scalar\noutput of the VLA is repurposed to indicate different events\nin the time sequence; to indicate both when the hand is in\nproximity to the target location for both picking and placing.\nFor the diffusion policy, an additional signal was recorded\nduring training to indicate the successful end of grasp, and\nthe transition back to the VLA. When combined, the robot\nshould move autonomously following a input command, as\nillustrated in Fig.1.\nOne limitation of this hybrid system is directly tied to\nthe limitation of diffusion policy controllers, where a unique\ndiffusion policy per object is necessary. In this work, a lookup\ntable is used to select one based on the language input to the\nVLA (see Fig.2)."}, {"title": "B. Robotic setup", "content": "1) ADAPT Hand 2: To demonstrate our framework's ca-\npabilities to perform dexterous manipulation on a multi-\nfinger robotic hand, we use the custom built ADAPT Hand\n2 [17]; an anthropomorphic robotic hand controlled with 13\nmotors. The hand incorporates series compliance at the base\njoints (metacarpophalangeal, carpometacarpal) which allow\nfor safe interaction with the environment [18]. This provides\nsome physical filtering to potentially noise actuation signals\ngenerated by the transformed based controllers. Fig.3 show\nthe ADAPT Hand 2 and its notable features including the\nanatomical kinematic design and continuous skin.\n2) Experimental Setup & Teleoperation: Fig.4 depicts the\nrobotic setup used for recording training data and execute the\nlearned motions. The ADAPT Hand 2 is attached directly to\nthe Universal Robot 5 (UR5) robot arm to perform a pick and\nplace task. The data collection is purely performed with the\nreal robot, using a custom teleoperation system developed in\n[17]. The teleoperation system uses the hand tracking system\nof the Apple Vision Pro headset to capture the users hand\nposes, and performs a joint-to-joint transfer of the arm and\nhand to translate this to robot motion. This is made possible\nby the highly anthropomorphic design of the hand. To capture\nthe visual data which is used as sensor signals used to control\nthe hand the setup also includes two cameras: static to the\nworld (cam1) and on the hand (cam2).\nWe focus on the pick-and-place of three representative\nobjects with varying geometry (red pepper, tape, and piece\nof paper) which are to be be placed in one of two locations\n(yellow or purple plate) shown in the image on the right.\nAlthough this is a small sub-set of objects is reflects a range\nof different grasp types or modalities.\nTo run both the trained VLA and diffusion models, we use\nthe Nvidia RTX 4090 GPU to achieve a control frequency of\napproximately 5 Hz for the respective models."}, {"title": "C. Data Collection", "content": "The data collection process must capture the relevant data\nfor the two different models and the switching framework.\nShown in Fig.5, the data used to train the VLA and the\ndiffusion policy is captured separately. For the VLA model\n(Fig.5A) the robot is teleoperated to perform the full pick-\nand-place task, while the operator manually records the event\nsignal. However, only the motion before and after the grasping\ntask (as indicated by the event signal) is used to train the\nmodel. A key feature in this process is to deliberately close"}, {"title": "D. Model training", "content": "The VLA model is fine-tuned based the pre-trained open-\nVLA model [7] on the dataset excluding the grasping periods.\nInstead of using images from a single camera as shown\noriginally [7], we combine two images from cam1 and cam2.\nThe two images are resized to 224x144 and 224x80, to then be\nvertically concatenated into a single image(see Fig.1). We use\nthe default fine-tuning parameter settings of openVLA model\nbut with a batch size of 22, and set image augmentations\nof fine-tuning as False (primary camera position and light\ncondition are kept same during data collection and testing).\nThe fine-tuning continues until the training action accuracy\nexceed 95 % and converges. The fine-tuning is loaded on a\ncluster virtual machine with a single A100-80GB.\nThe training of diffusion policy model is based on the\ncollected grasping demonstration. Images from two cameras\nare resized into 320x240 as inputs of the model, during\ntraining, images are randomly cropped images into 288x216\nas data augmentation. The training parameter settings are same\nas in the CNN-based diffusion policy in [12]. The model is\ntrained for 1500 epochs on a custom GPU."}, {"title": "E. Experimental Plan", "content": "A series of experiments are conducted to evaluate the\nproposed framework, starting with assessing the VLA and\ndiffusion policy individually before combining altogether for\none autonomous motion. The VLA is evaluated on its ability"}, {"title": "III. EXPERIMENTAL RESULTS", "content": ""}, {"title": "A. VLA Performance", "content": "To demonstrate the limitations of a VLA to perform the\nentire grasping sequence, its success rate is evaluated using\ntwo of the test objects. The blue paper (most challenging\nobject to grasp) was excluded from this test as when fine-\ntuning using data of all three objects, the training fails to\nconverge effectively (with action accuracy reaching only 80\n%). The VLA was fine-tuned until it achieves an action\naccuracy over 95% (requiring 50k fine-tuning steps), and we\nevaluate its performance on the entire grasping and placing\ntask for 5 repeats for each combination, where the objects are\nplaced in random within the testing area and commanded to\nplace on a specific color plate. As shown in Table. I, the VLA\nwith scalar output for grasping can only occasionally grasp\nthe pepper and is unable to grasp tape, which requires more\nprecise and dexterous manipulation. The score achieved pre-\ndominatnly relates to the VLA's ability to move the arm to the\ncorrect object.\nDespite the limitations of the VLA to perform the entire\ngrasping sequence, through the appropriate training it can\nenable the hand to reach to a location near the object based\nupon the text input. To evaluate the 'reaching' precision of\nthe VLA, the offset between the centre of the hand and the\nobject is recorded in the x-y plane (i.e. parallel to the desk)\nafter running the VLA. The position is recorded when the\nevent signal is given by the VLA to switch to the diffusion\nmodel. This is repeated for pepper and tape objects which\nare randomly placed within the test area. The VLA is fine-\ntuned using a dataset that excludes the grasping process,\nwith training continuing until the action accuracy exceeds\n95% (50k fine-tuning steps). To emphasize the advantage"}, {"title": "B. Diffusion Model", "content": "To highlight the capabilities of the diffusion policy model to\nperform precise manipulation and learn multi-modal behavior,\nwe perform grasping tests only using the diffusion policy\nmodel. To demonstrate the multi-modal behavior and capacity\nto leverage the environment for grasping, we focus on picking\nup the tape and the blue-block. The diffusion model policies\nhave been trained with human demonstration of picking the\nobjects using both slide & pick and direct pick (see Fig. 7 (b))\ndemonstrations, with an equal number of trajectories for each\ngrasping type. The slide & pick motion moves the object to\nthe edge of table, making it becomes easier for thumb to hold\nthe thin object from bottom, while the direct pick requires all\nfingers precisely align with the height of the object to achieve\na successful grasp. In the grasping test, we place the object\nat varying distances from the table edge and the grasp type\nevaluated for 5 repeats at each location. Fig. 7 (a) shows that\nas the object is placed closer to the table edge, the probability\nof using slide & pick grasping type increases; this reflects\nhow as a human you might slide an object close to the edge\nof a table, but pick it, if it is in the middle. For all grasping\ntrials, 5 trials for each distance, both objects succeed for all\nlocations, highlighting the effectiveness of diffusion policy in\nchallenging grasping tasks.\nAlthough diffusion policy can control the hand precisely,\nit requires that the hand is located nearby the objects before\nexecute the manipulation (the valid area depends on the cov-\nerage of demonstration for training). To show this limitation,\nand hence the need for the VLA, we offset the arm from the\ncenter of object in a x-y plane parallel to the desk as use\nthis as the starting position of grasping tests. We perform the\ngrasping tests for pepper and tape. Fig. 8 shows that if offset\nbetween hand and object exceeds 15 cm, the grasping success\nrate drops below 50% for both objects. This emphasizes the\nnecessity of VLA for moving the robot hand close enough to\nthe target object, and that the VLA typical error of 6cm is\nwithin the tolerance of the diffusion model.\nFig.9 shows the ability of the diffusion policy to recover\nfrom failure. Here, the robot moves to pick up the paper, but\nfails to fully grasp until the second attempt. The two attempts\ncan be observed by the two peaks in the blue signal showing\nthe angle of the metacarpophalangeal joint (base joint) of the\nindex finger. Importantly, the event signal stays zero on the\nfirst attempt when the robot fails, resulting in an automatic\nreattempt. When the grasp is successful, the event signal does\nthen increase to 1. This shows not only the diffusion policy's\nrecovery ability but the stability of the event signal."}, {"title": "C. Combined VLA & Diffusion Model", "content": "Fig.10 shows the result of the combined pipeline of the\nVLA and diffusion policy. Fig.10A shows three of the six total"}, {"title": "IV. DISCUSSION & CONCLUSION", "content": "In this work we introduce a framework for dexterous\nmanipulation which combines the relative advantages of a\nVLA for language input to high level planning, with that of\na diffusion policy model for contextual, precise generation of\nmotor signals applicable for a multi-fingered hand. Focusing\non a limited set of objects we demonstrate the need for\nboth of these models and the means by which they can\nbe effectively integrated. We deploy this framework on our\ncompliant anthropomorphic hand.\nOne of the key limitations of this work is the need to train a\ndiffusion model for the addition of each object. Developing a\nmore generalized approach for this, or leveraging existing data-\nsets to train the diffusion model could enable a more general-\nized approach. Another limitation is on the openVLA model\nbeing solely pre-trained on robots with 1DoF pinch grippers,\nwhich limits the use of such models for multi-fingered hands.\nTo diversity the types of end-effectors, fundamental changes\nto the models or large-scale data collection on various robotic\nhands could be explored.\nOpportunities exist to explore how hardware can enhance\nlearning-based controllers. In this work, the compliant hand\nnot only provided collision resilience but also improved ro-\nbustness in grasping tasks. Incorporating variable stiffness as\na control input could further enhance grasping robustness,\nparticularly in learning-based methods that struggle with re-\npeatable precise motions. On the sensor feedback, a natural\nprogression follows that of integrating tactile feedback into the\nfingers in addition to the existing cameras for better control\non the interaction forces."}]}