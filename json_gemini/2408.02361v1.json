{"title": "Dialogue Ontology Relation Extraction via Constrained Chain-of-Thought Decoding", "authors": ["Renato Vukovic", "David Arps", "Carel van Niekerk", "Benjamin Matthias Ruppik", "Hsien-Chin Lin", "Michael Heck", "Milica Ga\u0161i\u0107"], "abstract": "State-of-the-art task-oriented dialogue systems typically rely on task-specific ontologies for fulfilling user queries. The majority of task-oriented dialogue data, such as customer service recordings, comes without ontology and annotation. Such ontologies are normally built manually, limiting the application of specialised systems. Dialogue ontology construction is an approach for automating that process and typically consists of two steps: term extraction and relation extraction. In this work, we focus on relation extraction in a transfer learning set-up. To improve the generalisation, we propose an extension to the decoding mechanism of large language models. We adapt Chain-of-Thought (CoT) decoding, recently developed for reasoning problems, to generative relation extraction. Here, we generate multiple branches in the decoding space and select the relations based on a confidence threshold. By constraining the decoding to ontology terms and relations, we aim to decrease the risk of hallucination. We conduct extensive experimentation on two widely used datasets and find improvements in performance on target ontology for source fine-tuned and one-shot prompted large language models.", "sections": [{"title": "1 Introduction", "content": "State-of-the-art task-oriented dialogue (TOD) systems still rely on a fixed ontology to model their scope (Nguyen et al., 2023; Hude\u010dek and Dusek, 2023). A TOD ontology comprises three levels of hierarchy: domains, slots and values. Domains are general topics of interest, slots are types of information about entities in a domain, and values are concrete instantiations of slots. Ontology thus forms a hierarchy: it is a directed graph where slots belong to domains and values in turn belong to slots. Note that slots can be shared across domains, and so can values. An ontology is typically a prerequisite for generating API calls that access the underlying databases for entity retrieval. Further, the ontology defines the dialogue state, which is tracked by the system to determine the next actions given the evolving discourse.\nThe dependency on an ontology poses a significant challenge in transferring existing TOD systems to new domains and use cases. Although ontology-agnostic approaches do exist, their transfer capabilities are limited and their performance remains sub-par on novel data (Heck et al., 2022).\nLarge quantities of domain-specific TOD data, e.g. customer service recordings, are frequently available, but tend to come without annotation, rendering direct use for system development difficult (Brusco and Gravano, 2023). Manual labelling is error-prone, does not scale well and quickly becomes prohibitively expensive (Eric et al., 2020; Rosenbaum et al., 2022; Gung et al., 2023). Despite topical or domain mismatch, existing annotated datasets may provide information about TOD that can be leveraged to harness new data. For this reason, we are interested in utilising existing labelled TOD datasets to automatically generate a full ontology for new, yet-unlabelled, data.\nAutomatic dialogue ontology construction typically consists of two steps, dialogue term extraction (Vukovic et al., 2022) and hierarchy establishment. Although hierarchy establishment is often done via clustering (Hude\u010dek et al., 2021; Yu et al., 2022) we approach it via relation extraction (RE), which is more similar to common information extraction pipelines (Genest et al., 2022; Xu et al., 2023). We call this task dialogue ontology relation extraction (DORE). A hierarchy is established by inferring in which level extracted terms lie, and by connecting terms across levels.\nAlthough large language models (LLMs) have demonstrated considerable task transfer abilities (Brown et al., 2020; Ouyang et al., 2022),"}, {"title": "2 Related Work", "content": "Dialogue Ontology Construction We divide dialogue ontology construction into term extraction and relation extraction. Vukovic et al. (2022) improve out-of-domain generalisation of a dialogue term extraction model by making use of topological properties of the language model embedding space. Nguyen et al. (2023) improve phrasal segmentation of ontology terms via language model probing and contrastive learning. Since we evaluate the hierarchy on a global level based on relations, our approach is not directly comparable to clustering-based approaches such as Hude\u010dek et al. (2021); Yu et al. (2022). In contrast to these methods, we view hierarchy establishment in isolation as a relation extraction task.\nYu et al. (2020) present DialogRE, a popular dataset for RE in short chit-chat dialogues. Closest to our approach, Albalak et al. (2022) jointly optimise RE and explanation generation to improve performance with a model-agnostic framework. Xu and Chen (2023) propose a zero-shot approach for extracting trigger words for dialogue relation extraction on DialogRE. However, these works focus on chit-chat dialogues, which do not include ontology relations.\nRelation Extraction with LLMS LLMS show promising transfer capabilities out of the box (Laskar et al., 2023). Direct application to our task however is not promising, as it has been shown that aligned LLMs such as ChatGPT (OpenAI, 2022) do not perform well on extracting multiple relations at once (Lilong et al., 2024). This shortcoming has been linked to the influence of pre-training data distribution on downstream task performance (McCoy et al., 2023). RE data in particular amounts to a mere 0.5% of instruction-tuning datasets, and is hardly utilised for model selection (Wang et al., 2022; Zhang et al., 2023).\nTraditionally, RE is performed in a pairwise manner (Zhang et al., 2023), resulting in quadratic complexity given the number of terms. This becomes intractable for generative LLMs when querying the LLM separately for each pair of terms. Alternatively, one may extract all relations present in a given input with a single LLM query, as is common in multi-relation extraction tasks such as document-level RE. For example, Lilong et al. (2024) extract relations by either predicting relations directly, or first predicting possible head entities in a document. Zhang et al. (2023) align LLMs for zero-shot RE by transforming RE into a question answering (QA) task, which is more frequent in the instruction-tuning data.\nConstrained Decoding Constrained decoding limits the tokens that can be generated. It is typically applied to LLMs to improve downstream task performance, reduce hallucination and ensure certain output formats. Bogoychev and Chen (2023) constrain decoding for translation to ensure that certain terminology is used. Roy et al. (2024) use constrained decoding with a lookahead heuristic to speed up adaptation of LLMs to plan generation according to a given API in TOD. We want to force the model to use its inherent task knowledge while transferring abilities to new data.\nChain-of-Thought Reasoning LLM performance on complex reasoning tasks improves when the model generates a chain of thought (CoT). Wei et al. (2022) include examples of multi-step reasoning in the prompt, and Kojima et al. (2022) prompt the model in a zero-shot fashion to \u201cthink step by step\". Reasoning capabilities can be further enhanced via specific training on CoT-data (Chung et al., 2024), or via teaching the model to reason (Zelikman et al., 2022). In contrast to this, we focus on eliciting model-inherent reasoning capabilities, without the need for specific prompts or training. As described in Sec. 3.2, we leverage the fact that a top-k decoding beam usually contains a CoT (Wang and Zhou, 2024).\""}, {"title": "3 Constrained Chain-of-Thought Decoding for Ontology Relation Extraction", "content": "3.1 Problem Definition\nDialogue ontology relation extraction (DORE) aims at extracting all relations between different terms in a TOD dataset. As seen in Figure 1, for each dialogue paired with a list of ontology terms, the output is a set of relations similar to document-level relation extraction (Tan et al., 2022). However, we consider the joint relation prediction set accumulated from all dialogue-level predictions, rather than the dialogue-level performance. In the DORE task, the model receives as input a task-oriented dialogue D annotated with a list of ontology terms T present in this dialogue. The output are valid ontology relations RD,T between the terms, which includes predicting whether a term is a domain, slot, or value. A relation is denoted by a relational triplet with a head term, the relation and a tail term. Finally, the predicted relations for each dialogue are unified to form the final ontology relation set.\nWe consider 4 types of relation between ontology terms: domain-slot, slot-value, value-domain and equivalent term relations (see Table 1 for examples). Here, all relations except the equivalence relation are directed relations with a head and a tail term. Domains are general topics, such as hotel or restaurant, slots are types of information for entities in a domain, such as price range or area and values are concrete instantiations of slots, such as \"cheap\" or \"west\u201d. The equivalence relation connects terms from the same hierarchy level that point to the same ontological concept, e.g. \"expensive\" and \u201chigh-end\u201d both represent a high price. In the prompt and labels, we denote the relation types through different verbalisers, shown in Table 1. Verbalisers are descriptions of task-specific labels in natural language. They align the task closer with the pre-training distribution of the LLM (Schick and Sch\u00fctze, 2021; Mosbach et al., 2023).\nOur hypothesis is that the general definitions of the ontology hierarchy relations enable seamless transfer to new data in order to construct a similarly structured ontology on the new data. Based on these relations, we focus on transferring the structural information about ontologies from a source dataset to a target dataset. Here, we consider a one-shot and a fine-tuning approach.\n3.2 Chain-of-Thought Decoding\nCoT reasoning in LLMs has demonstrated improved performance in various complex reasoning tasks (Sec. 2). The results of Wang and Zhou (2024) show that LLMs inherently possess reasoning capabilities, which can be elicited without explicit prompting through Chain-of-Thought decoding. Concretely, they experiment on pre-trained and instruction-tuned versions of PaLM 2 (Anil et al., 2023) and Mistral-7B (Jiang et al., 2023). They observe that although the greedily decoded response might not always exhibit reasoning, one of the top-k beams usually contains a CoT. This CoT not only shows higher confidence in the answer, but also exhibits greater accuracy. They propose to consider the top-k probability tokens at the start of the predicted response. From there, k completions, called branches, are generated, resulting in k-times computational complexity during inference. The final response is chosen based on the confidence of the tokens that belong to the answer in each branch, i.e., the average confidence of the answer tokens. In logical reasoning, there is only one answer in each branch, which is a number. In that case, they identify the answer by prompting the model with \"So the answer is:\" at the end and match the following number to one in the preceding response. In our case, there are multiple answers per branch, which we identify based on the fact that relations are supposed to be predicted between brackets.\nCoT-Decoding for DORE In this paper, we extend CoT decoding to handle the multi-answer scenario in the DORE task. We compute the confidence of answer tokens by utilising their structure, which, in our case, involves predicting relational triplets in the format [headterm, relation, tailterm] and the notion of disparity. The disparity of a probability distribution is the difference between the probability of the most likely outcome and the next most likely outcome. The confidence for each answer token for a given branch is measured by the average disparity of its tokens. Formally this is given by\n\u0394i,a = \\frac{1}{nt}\\sum_{t \\in a} p(x_{t}^{op}|x^{<t}) - p(x_{t}^{next}|x^{<t}), (1)\nwhere a is an answer (in our case the triplet), i is a branch, xt are the answer tokens belonging to the answer in branch i, xop is the most likely token on position t and next the next most likely token on position t. x<t are the tokens in branch i on positions preceding t, i.e. the context so far.\nIn DORE, answer tokens are those that form terms and relations in the predicted relational triplets, which means there are three disparities per relation. This approach relies on detecting answer tokens in a generated response for confidence estimation, and we leave an extension to arbitrary answer structures to future work. The resulting triplet disparities are denoted as \u2206a = [\u2206h, \u2206r, \u2206t]. We explored mean, median, maximum, and minimum as aggregation strategies for relational triplet mentions, finding that all of them lead to similar results. For simplicity, we choose the mean to aggregate the disparity for a relational triplet in branch i, i.e. \u0394i,a = (Ah,i + \u2206r,i + \u2206t,i).\nWe select the branch with the highest average disparity over the relations predicted in each branch to get the final set of relation predictions for a dialogue. The average disparity for branch i is given by\n\u0394i = \\frac{1}{Na,i}\\sum_{a \\in Ri} \u0394i,a (2)\nwhere a is a relational triplet, Ri is the set of relations and na,i is the number of relations in branch i. The final set of predicted relations is then given by\nRargmax = {Ri | i = argmax{\u22060,..., \u2206k}} (3)\nWe also experiment with a confidence threshold based approach for relation selection. Here, the average disparity of a relation is computed across occurrences in different branches:\nAa = \\frac{1}{na}\\sum_{i \\in {1,...,k}} \u0394i,a (4)\nwhere Aia is the disparity of the answer a in the i-th branch and na is the number of occurrences of a across the different branches. The final set of predicted relations R\u25b3>\u25b3threshold is then\nRA>>threshold = {a | Aa > \u25b3threshold} (5)\n3.3 Constrained Decoding\nWe constrain the generation of the relation terms and relation types if the beginning of a relational triplet is predicted to ensure the structure and mitigate term and relation hallucination (see Figure 1). This means for a relational triplet, [h, r, t], we ensure that h, t \u2208 T and r \u2208 R, where T is the set of terms for the current dialogue and R is the set of relation types given in the prompt. Note that we only constrain the generation when an opening bracket is predicted by the model, and resume to non-constrained generation after the generated relational triplet."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nWe utilise the open-source Gemma 2B (Mesnard et al., 2024) instruction-tuned model with context size of 4096 for all experiments. In CoT-decoding we set k = 5. For a more thorough analysis of the impact of k in CoT decoding, resort to Wang and Zhou (2024). We always branch at the first token; branching at later tokens did not show improvements. For all CoT-decoding experiments, we select the relations from the branch with the highest disparity, as the threshold based method works worse and also adds a new hyperparameter. In the one-shot prompts, we use a combination of an instruction with simple natural language with a preceding example (Brown et al., 2020; Sahoo et al., 2024). For fine-tuning, we remove the example from the prompt.\nDatasets For the source dataset, we employ the MultiWOZ 2.1 dataset (Eric et al., 2020). It has 7 domains and over 10,000 dialogues. We use the training set for training and select from it one random dialogue with relation annotation as one-shot exemplar. The target dataset is the schema-guided dialogue (SGD) dataset (Rastogi et al., 2020). It comprises more than 20,000 dialogues and 20 domains. We use the SGD test split for evaluation in the main results, which contains 4,201 dialogues and 18 domains. In the test set, there are 134 domain-slot relations, 6,162 slot-value relations, 8,233 value-domain relations and 330 equivalence relations. It is worth noting the SGD test set contains dialogues from different domains than the SGD training set, as well as a significant amount of unseen ontology relations. We use ConvLab-3 (Zhu et al., 2023b) for loading all the datasets.\nTraining For both fine-tuning and one-shot prompting, we utilise the original Gemma prompt template (Mesnard et al., 2024). For training, we utilise Low-rank adaptation (LoRA, Hu et al., 2022) with the default parameters in the peft library (Mangrulkar et al., 2022). We train the model on a single NVIDIA RTX8000 GPU and do inference with all models on one NVIDIA RTX6000 GPU.\nWe only consider a one-shot approach due to context size constraints, as the relational triplets in the exemplars contain brackets. Brackets are considered individual tokens, increasing the number of tokens significantly. Because of this a maximum of three exemplars fits in the context size, which do not improve performance however, while increasing computational complexity. In the one-shot approach, we predict each relation type separately, since we found that the LLM struggles with jointly predicting all relation types. We also experimented with a zero-shot approach that performs significantly worse than one-shot.\nWe fine-tune the LLM via pattern-based fine-tuning (Schick and Sch\u00fctze, 2021; Ma et al., 2023) with a prompt for all relation types on the Multi-WOZ training split. We consider two upper bounds: an LLM trained on the SGD training split and a model utilising a one-shot exemplar from SGD.\n4.2 Evaluation\nIn evaluation, we only consider relations within dialogues in the ground truth, i.e., both terms of a relation occur in the same dialogue. Relations from equivalent terms to other terms have to be found at least once. If [term\u2081, refers to same concept as, term2] \u2208 Rgroundtruth, then [term\u2081,r,t] = [term2, r, t], where Rgroundtruth is the set of ground truth relations, r \u2260 'refers to same concept as' is another relation type and t \u2208 T is a third related term. E.g., the relations [price range, has value, high-end] and [price range, has value, expensive] are equivalent, since [expensive, refers to the same concept as, high-end]. Thus, the prediction of the former relation counts as a prediction for the latter and vice versa.\nTo compute the global micro F1 score, we compare the accumulated set of relations predicted from all the dialogues with the ground truth ontology relations. Note that we only consider exactly matching terms in relations to be correct.\nFor significance tests on the one-shot prompted models, we employ a pairwise t-test on dialogue level. For fine-tuned models, we use 5 random seeds for training and an independent t-test.\n4.3 Results\nTable 2 shows the full results on the target test set, see Appendix A for results for each relation type.\nSource One-Shot Approach We found that when predicting all relations at once in a one-shot fashion the model is completely unable to fulfil the task, so we resort to predicting one relation at a time. The one-shot approach is mainly improved through constrained decoding, although the combination with CoT-decoding is also significantly better than the baseline. Note that the source one-shot model is able to get closer to the performance of a model with a one-shot example from the target data with constrained CoT-decoding.\nSource Fine-tuning Approach For the source fine-tuned model, constrained CoT-decoding leads to significant improvements over the baseline. Furthermore, it significantly outperforms a model using a one-shot exemplar from the target data on all metrics. Constraining CoT-decoding helps performance, since the constraints mitigate overconfidence on the source data after fine-tuning.\nInterestingly, although the target fine-tuned model is the best model, it is not able to find all relations on the test set. As mentioned in Section 4.1, the SGD test set contains domains different to the SGD training set, which makes this task particularly difficult. In contrast to the excellent performance of LLMs on a variety of tasks, there is a lot of room for improvement on this task.\n4.4 Calibration Analysis\nIn Figure 2, we see that an absolute confidence threshold is not as meaningful and adds the problem of choosing the correct threshold as hyperparameter. Moreover, a high threshold leads to only a small increase in precision, while losing a significant amount of recall. Our results are in line with recent findings about instruction-tuned LLMs (Kapoor et al., 2024) being overconfident. We find that the model's confidence on predicted relations is generally on a high level, indicating overconfidence, as the significant changes in performance happen at high confidence thresholds. For lower thresholds, the performance remains unchanged, as most confidences are quite high and hence the set of predicted relations stays the same. Although this shows that the thresholds are less meaningful, the relative confidence of the branches is meaningful, since choosing the highest disparity branch leads to good performance.\n4.5 How useful are predictions from the additional branches?\nIn line with the findings from Wang and Zhou (2024), we find that for the instruction-tuned Gemma model, the gain in performance can be mainly attributed to the first additional branch (see Figure 3). While the F1 score is increased slightly up to k = 3, the jump in recall from k = 2 to k = 3 is more significant. This also shows that the branches from lower ranked first tokens lead to responses with higher total confidence across the relation predicted in the respective branch, which is why they are chosen in the highest disparity branch selection method.\n4.6 Qualitative Analysis\nWang and Zhou (2024) found that LLMs struggle to generate CoTs for less frequent tasks in the pre-training data. In our analysis, we found that higher confidence completions often follow a recap of the type of terms and relations that should be predicted. Illustrated in Figure 4 is an example of a response to the one-shot equivalence prediction prompt with constrained CoT-decoding (see Appendix B for completions of the other decoding approaches). Here, branches 0 and 1 contain a repetition of the information given in the prompt. In branch 1 however, the focus on the equivalence mentioned in the prompt is followed by a response that does not use the proper format for the answer to be parsed correctly. The last branch has the highest confidence and is chosen ultimately. Here, the focus on the task relation and the provided dialogue is part of the generated introduction to the response.\nAs seen in Table 3, for the fine-tuned models, there are no such reasonings observed, although the constrained CoT decoding significantly improves performance. We hypothesise that the additional branches together with the constraints in decoding force the model to use task knowledge from fine-tuning, rather than what it has learned about the source data distribution. This can be observed when comparing CoT-decoding with constrained CoT-decoding, where the unconstrained version mainly generates terms it has seen on MultiWOZ, such as the \"reference number\" slot that is not present in SGD. The constrained version on the other hand forces the model to use task knowledge instead of distributional knowledge, leading to a much better coverage of the terms mentioned in the dataset, if the correct branch is chosen based on confidence. When observing completions to other dialogues, we found that the qualitatively best branches are not necessarily those with the highest confidence, indicating that a more sophisticated branch selection strategy might boost performance further. We leave such an improvement to future work. When comparing constrained decoding with vanilla greedy decoding, it becomes apparent that constraining the generation greatly improves the output structure and the utilisation of mentioned terms in the target dataset."}, {"title": "5 Discussion", "content": "Although the performance of the fine-tuned model is improved by constrained CoT-decoding, it is not clear where the improvement comes from based on qualitative analysis alone, as this model generates no reasoning paths. The workings and explainability of constrained CoT-decoding for fine-tuned models have not been investigated yet, but are relevant future research directions.\nOur results imply that for tasks that are not frequently used in instruction-tuning data, it can be useful to utilise existing TOD data for training an LLM rather than annotating a few examples on the target data for the DORE task (see Section 4.3). Due to the length of examples in the DORE task, the amount of annotated examples that can fit in the prompt is highly limited, aggravating the applicability of few-shot approaches.\nThe results strengthen the finding that annotated data should be used if available (Zhu et al., 2023a). Constrained CoT-decoding only improves performance on the target data, where task knowledge is more useful than distributional knowledge about the source data. Mahowald et al. (2024) state that for learning tasks where language is used in a functional way to accomplish certain goals, such as DORE, self-supervised next-token prediction is not sufficient. Instead, the model needs to learn task-specific information via specialised fine-tuning to solve such tasks, which are not frequently present in pre-training data and involve task-specific reasoning. DORE can only be solved by knowing the specific relationship definition provided in the task descriptions, which the model cannot handle if it was not trained on the task. In summary, the presented results reinforce the observation that fine-tuning and specialised architectures are still needed to solve functional language-based tasks that cannot be solved by pattern matching alone.\nThe computational complexity of CoT-decoding is k times higher than regular greedy decoding, however, decoding of the different branches can be done in parallel. Nonetheless, one should note that only one run of ontology construction is needed, as the ontology can be repeatedly used for other dialogue modelling tasks such as state tracking. It is worth stressing that CoT-decoding only increases inference cost, while training cost remains the same. Compared to few-shot prompting, which also increases inference cost due to longer input context, there is no additional annotation cost."}, {"title": "6 Conclusion", "content": "We propose constrained chain-of-thought (CoT)-decoding, a new decoding mechanism for dialogue ontology generation (DORE) in a transfer set-up. An LLM using a one-shot example from the source data is significantly improved using the proposed constrained CoT-decoding mechanism. Fine-tuning an LLM on the source data and using constrained CoT-decoding for inference on the target data outperforms a one-shot target data model significantly.\nThe results warrant further research into DORE in particular, and into eliciting reasoning in LLMs by adapting the decoding mechanism in general. Moreover, we offer a method for applying LLMs to tasks that are underrepresented in pre-training and where the vanilla LLMs perform poorly. Our method is appealing as it does not necessitate labelling new examples. Future research directions include explainability of constrained CoT-decoding in fine-tuned LLMs and including CoT-decoding during fine-tuning."}, {"title": "7 Limitations", "content": "In this work we assume a pipeline approach, however with the raise of LLMs, end-to-end solutions tend to be more accurate. We leave the task of jointly extracting dialogue terms and relations for future investigation. Due to constraints in computational infrastructure, we were not able to run open-source LLMs with the size of ChatGPT, which might be promising however. We abstained from utilising proprietary models, such as ChatGPT, for increased transparency and reduced risk of training data contamination.\nFurthermore, the need for an annotated source dataset limits the application to low-resource languages and tasks. The reliance on a specific answer structure for confidence estimation limits application to less structured tasks.\nFinally, what we consider the upper bound, which was trained on the target dataset, can be argued to be a low bar too, reaching only an F1 of 37. This warrants more research on this task also on the same data setting."}, {"title": "8 Acknowledgements", "content": "RV and BMR are supported by funds from the European Research Council (ERC) provided under the Horizon 2020 research and innovation programme (Grant agreement No. STG2018 804636) as part of the DYMO project. CVN and HL are supported by the Ministry of Culture and Science of North Rhine-Westphalia within the framework of the Lamarr Fellow Network. MH is supported by funding provided by the Alexander von Humboldt Foundation in the framework of the Sofja Kovalevskaja Award endowed by the Federal Ministry of Education and Research. Computational infrastructure and support were provided by the Centre for Information and Media Technology at Heinrich Heine University D\u00fcsseldorf and Google Cloud. We want to thank the anonymous reviewers whose comments improved the quality of our paper."}]}