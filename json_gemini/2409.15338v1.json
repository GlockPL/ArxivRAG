{"title": "Explainable AI: Definition and attributes of a good explanation for health AI", "authors": ["Evangelia Kyrimi", "Scott McLachlan", "Jared M Wohlgemut", "Zane B Perkins", "David A. Lagnado", "William Marsh", "the ExAIDSS Expert Group"], "abstract": "Proposals of artificial intelligence (AI) solutions based on increasingly complex and accurate predictive models are becoming ubiquitous across many disciplines. As the complexity of these models grows, transparency and users' understanding often diminish. This suggests that accurate prediction alone is insufficient for making an Al-based solution truly useful. In the development of healthcare systems, this introduces new issues related to accountability and safety. Understanding how and why an Al system makes a recommendation may require complex explanations of its inner workings and reasoning processes. Although research on explainable AI (XAI) has significantly increased in recent years and there is high demand for XAI in medicine, defining what constitutes a good explanation remains ad hoc, and providing adequate explanations continues to be challenging. To fully realize the potential of Al, it is critical to address two fundamental questions about explanations for safety-critical Al applications, such as health-Al: (1) What is an explanation in health-Al? and (2) What are the attributes of a good explanation in health-Al? In this study, we examined published literature and gathered expert opinions through a two-round Delphi study. The research outputs include (1) a definition of what constitutes an explanation in health-Al and (2) a comprehensive list of attributes that characterize a good explanation in health-AI.", "sections": [{"title": "1. Introduction", "content": "Artificial Intelligence (AI) has potential for transformative impact in a number of fields and industries. The International Data Corporation (IDC) predicts annual Al spending will exceed $300bn by 2026 as more companies integrate these intelligent technologies into their product and service offerings [1]. Some of the key applications include:\n(i) Healthcare: for disease diagnosis and personalized treatment recommendation [2].\n(ii) Finance: for algorithmic trading, fraud detection and predicting investment decisions [3].\n(iii) Autonomous Vehicles: for advanced driver assistance systems and self-driving car technologies [4].\n(iv) Natural Language Processing (NLP): for language translation, speech recognition and voice assistants like Siri and Alexa [5].\n(v) Cybersecurity: for analysing patterns, identifying anomalies and automating security processes [6].\n(vi) Entertainment: for music and video generation, virtual reality simulations and game development [7], [8].\n(vii) Legal Services: for legal research, contract analysis and document review [8].\nDespite all the successes of Al, recent work shows that Al can unintentionally harm humans and that it is precisely the large-scale and wide introduction of Al technologies that hold enormous and unimagined potential for new types of unforeseen threats [9]. The absence of an understanding of how an Al works can allow both unintentional and deliberate bias to shape the recommendations, predictions and decisions Al may be used to make [9]. For example, Al can make unreliable decisions in safety-critical scenarios (e.g. in the medical domain) or undermine fairness by inadvertently discriminating against a group [10]. For this reason, explainable Al (XAI) is essential to ensure transparency, fairness, and ethical integrity in Al-driven decision-making in order to minimise the potential harms of unchecked Al implementation [11], [12], [13]. By providing insights into Al systems' inner workings, explainability empowers trust, understanding, and effective leveraging of Al technologies for positive societal impact [14], [15]. An increasing requirement for an explanation has arisen due to the broad adoption of machine learning (ML) approaches, where the reasoning task is often performed in what are known as blackbox systems where it is unclear why a specific result has been reached [16], [17]. Increasing use of these systems in healthcare, where important decisions about humans are made, raises new issues for accountability and safety [14]. Explainable and trustworthy Al have also been included in the European Commission's ethics guidelines [18].\nWithout a solution to the problem of trustworthy Al and user acceptance of healthcare technologies generally, the benefits of these systems will never be realised and all our efforts to develop accurate health-Al will be in vain [19]. This research aimed to answer two fundamental questions of explanation in health-Al that remain unanswered:\n1) What is an explanation in health-Al?\n2) What are the attributes of a good explanation in health-Al?\nTo achieve the above objectives, we consulted experts from diverse backgrounds using a Delphi study. The research outputs are (1) a definition and (2) a global list of attributes of a good explanation in health-Al. By understanding the attributes of a good explanation, improved explanation algorithms will be developed. Generating explanations is not enough, it is also crucial to evaluate how good these explanations are. Thus, the proposed list of attributes will serve as a means to produce an evaluation process, which is lacking with respect to formalised measures.\nThe remainder of this paper is organized as follows: Background information is presented in section 2. The methodology and results are explained in Sections 3 and 4, respectively. Finally, a detailed discussion and a study conclusion are presented in Sections 5 and 6."}, {"title": "2. Background and Motivation", "content": ""}, {"title": "2.1 Law and Ethics", "content": "Medical Al is considered a high-risk Al application in the proposed European legislation [14]. Therefore, having an explanation for why and how some conclusions were reached by the system is extremely important [20]. In healthcare, XAI is urgently needed for many purposes"}, {"title": "2.2 Related work on Explainable AI (XAI)", "content": "There is an extensive body of literature reviewing different aspects of XAI. Much of this literature focuses heavily on how we can classify explanation methods [31], [32], [33], [34] as well as how XAI can impact affected parties [35], [36], [37]. Others also review evaluation methodologies for explainable systems [24]. Papers on foundational aspects of an explanation are scarce. Defining what an explanation is and what constitutes a good explanation remains ad hoc [15], [23].\nThe majority of the published papers that focus on XAI definition either review existing literature [30], [38], [39] or seek definitions inspired by existing literature and their own expertise [16], [17], [33], [40], [41], [32], [42], [43], [44], [45], [46]. Existing definitions of XAI often fail to provide specific guidance. These definitions, which may be at least in part correct, tell us very little about what an explanation is. They omit aspects such as the semantic entity of an explanation, the aim of the explaining inference, the target audience, etc. In addition, an increasing number of contributions tend to rely on their own, often intuitive, notions of \u201cexplainability\u201d. This can lead to a failure to provide satisfactory explanations. Finally, defining what explainability in Al is should be an interdisciplinary task, yet the existing work focuses on a single discipline.\nDetermining the criteria for a good explanation is an area of interest in various scientific fields [47]. Many researchers propose a list of attributes for a good explanation based merely on literature synthesis [20], [23], [39], [48], [49], [50], [51], [52] [53], [54], [55], [56]. Others focus"}, {"title": "3. Methodology", "content": "To achieve this study's objectives, published literature and expert opinions, using a Delphi study of participants with a diverse research background, were analysed (Figure 1)."}, {"title": "3.1 Scoping review", "content": "A search of major databases including PubMed, MedLine, ScienceDirect, Scopus, DOAJ and Elsevier was performed using the following search query:\n[(XAI OR \u201cexplainable AI\u201d) AND (definition OR criteria OR desiderata OR attribute OR characteristic OR feature OR notion)]\nDue to the high number of returned articles, further scrutiny was necessary to identify the most relevant articles for this study. We selected papers for inclusion where the described keywords were present in the title or abstract. Additional screening was performed to exclude papers: (i) published before 2018 (when GDPR came into effect in the EU); (ii) not published in English; (iii) where full access to the paper was not possible; or (iv) articles that had not been through peer review. The remaining papers were those meeting the inclusion (IC) without triggering the exclusion (EC) criteria. These criteria are presented in Table 1. The results from the scoping review were used to form the questionnaire for the first round of Delphi study."}, {"title": "3.2 Delphi Study", "content": "A Delphi process comprising two rounds was used to reach expert consensus [65]. The questionnaire for Round 1 is available at https://exaidss.com/wp-content/uploads/2024/04/Questionnaire-Round-1.pdf. The questionnaire for round 2 is available at https://exaidss.com/wp-content/uploads/2024/04/Questionnaire-Round-2.pdf.\nParticipants from the following three groups were invited:\nGroup 1 End user decision makers: health professionals, clinicians.\nGroup 2 Al Developers: engineers/computer scientists, data scientists, statisticians, implementation scientists, human-computer interaction specialists.\nGroup 3 XAI theorists: psychologists/ cognitive scientists, social scientists, philosophers, legal theorists.\nThe participants were recruited using: (1) an invitation email to experts recommended by the authors; (2) an invitation email to authors of the publications identified through the initial literature search; (3) a call to contribute that was published in a short report (https://exaidss.com/publications/); and (4) the consideration of any expert contacting the lead author on their own initiative. Educational material on XAI and the Delphi study was sent to each participant."}, {"title": "Objective 1 - An explanation definition for health- Al", "content": "The first objective was explored using an iterative process that started with definition fragments identified in the literature and finished using a taxonomical approach, identifying themes in participants' feedback received during the Delphi study. More specifically, in the first round, we divided generic definitions of XAI, identified from the literature, into three components: (1) the semantic entity of an explanation; (2) the aim of the explaining process; and (3) the explanation purpose. Participants were asked to rate on a 1 \u2013 7 Likert scale their agreement on the proposed list of published definitions. Further details can be found at https://exaidss.com/wp-content/uploads/2024/04/Questionnaire-Round-1.pdf Based on the responses from round 1, a more abstract definition was prepared for the second round, asking participants again to rate their agreement using the same scale. More details about the abstract definition can be found at https://exaidss.com/wp-content/uploads/2024/04/Questionnaire-Round-2.pdf. To arrive at the proposed definition of an explanation in health-Al we reviewed participants' feedback obtained during round 2 using a taxonomical approach as applied by McLachlan, et al. in [66]."}, {"title": "Objective 2 \u2013 Attributes of a good explanation in health-Al", "content": "In the second round, the participants were asked to perform the same scoring exercise in a revised item list based on the data gathered from the first round. Only attributes that more than 70% of the participants rated as important (scores 5, 6 and 7) were included in the second round. In addition, participants were presented with two different medical case studies. In both cases participants were provided with two different explanations that varied based on complexity and content. Further details about the explanations provided can be found at https://exaidss.com/wp-content/uploads/2024/04/Questionnaire-Round-2.pdf. Participants were asked to select the explanation they preferred and to justify their choice by identifying reasons from a reasonably extensive pre-defined list of explanation attributes - the same list of attributes that participants reviewed in the previous step. Finally, participants were asked to provide any additional attributes that they would like to see in the provided explanation."}, {"title": "4. Results", "content": ""}, {"title": "4.1 Scoping Review", "content": "Initially, 1052 papers were identified where the prescribed keywords were present in their title or abstract, published after 2018, and in English. After removing duplicates and those papers that did not meet the IC of the study (Table 1), 61 papers were reviewed. Around half of these works included some definition or a list of attributes for a good explanation in health-Al. This was not a formal literature review, but rather a scoping review. The questionnaire for the first round of the Delphi study was developed based on the knowledge drawn from these papers (available at https://exaidss.com/wp-content/uploads/2024/04/Questionnaire-Round-1.pdf)."}, {"title": "4.2 Delphi Study", "content": "From the 135 participants invited to participate in the Delphi study, 39 (29%) participated, of whom: (i) 7 (18%) participated only in the first round; (ii) 14 (36%) participated only in the second round; and (iii) 18 (46%) participated in both rounds. The participants were almost equally distributed between Group 1 \u2013 end-user decision makers and Group 2 \u2013 Al developers, with fewer participants belonging to Group 3 \u2013 XAI theorists (Figure 3). Most had two to six years of experience working on XAI (Figure 4)."}, {"title": "Objective 1 - An explanation definition for health- Al", "content": "During the first round of the Delphi study, many participants found it difficult to rate their agreement with the proposed definition fragments found in the literature (available at https://exaidss.com/wp-content/uploads/2024/04/Questionnaire-Round-1.pdf). The main criticism was that providing a unique definition for an explanation of health-Al is challenging as it is highly depending on who needs the explanation and for what reasons. Thus, a more abstract definition was proposed in the second round (can be found at https://exaidss.com/wp-content/uploads/2024/04/Questionnaire-Round-2.pdf). As illustrated in Figure 5, 69% of the participants rated their agreement on the proposed abstract definition with rates 5-6. A consistent theme among participants' feedback is that the abstract definition should be less vague and distinguish better between the Al and its output and between the Al and the explanation purpose. Using a taxonomical approach, during which we identified keywords among the participants' feedback, we developed the following definition to answer the first objective ('What is an explanation in health-Al?'):\nAn explanation is a tool intended to assist a user with insights relevant to the function of an AI/ML model, designed for a specific purpose, and the reason for this particular output."}, {"title": "Objective 2 \u2013 What are the attributes of a good explanation in health-Al", "content": "The attributes of a good explanation were divided into three groups: (1) attributes related to the \"focus\" of the explanation, (2) attributes related to the \u201ccontent\u201d of the explanation and (3) attributes related to the \"output\" of the explanation. Participants rated the importance of each as shown in Figures 6,7 and 8, respectively."}, {"title": "5. Discussion", "content": "We often measure the performance of Al systems by metrics to determine if it achieves an acceptable performance [12]. However, in healthcare, as in many other fields, high predictive performance is not the only requirement and several unmet needs remain. These unmet needs include lack of explanations in clinically meaningful terms, coping with the unknown medical conditions, and transparency of the system's limitations. All these unmet needs stress the importance of the explainability, transparency, and interactions between the practitioners and the Al systems.\nA common theme within the XAI field is to determine when an explanation is proper [38]. Currently, this is hard to achieve as there in no universally agreed definition and list of attributes of a good explanation in Al. A reason for this is that properties of an explanation depend highly on the application domain, the intended purpose of the explanation and user's characteristics. For that reason, we studied expert opinions from a diverse range of interested parties to answer two fundamental questions of explanation in health-Al that remain unanswered: (1) What is an explanation in health-Al? And (2) What are the attributes of a good explanation in health-Al? The research outputs are (1) a definition and (2) a global list of attributes of a good explanation in health-Al.\nRegarding objective 1 \u2013 what is an explanation in health-Al? \u2013 we developed our proposed definition using an iterative process that started using definition fragments identified in the literature and finished using a taxonomical approach, identifying themes in participants' feedback received during the Delphi study. Our proposed abstract definition, though not exactly the same, follows the reasoning process described by Saeed et Omlin [17]. They provide an abstract definition of explainability that has three main components: insights, targeted audience and need. Their abstract definition, \u201cexplainability aims to help the targeted audience to fulfil a need based on the provided insights from the explainability techniques used", "definition": "an explanation is a tool intended to assist a user with insights relevant to the function of an AI/ML model designed for a specific purpose and the reason for this particular output\". This definition was developed for health-Al, but it can be applied to other disciplines as well.\nRegarding objective 2 \u2013 what are the attributes of a good explanation in health-AI? \u2013 our proposed list of attributes is based on the literature and participants. All the critical and\"\n    },\n    {\n      \"title\": \"5.1 Lessons learned from experts\",\n      \"content\": \"Objective 1 \u2013 What is an explanation in health-Al?\nDuring the first round of the Delphi study, we divided the definition of explainable AI (XAI) into three components (based solely on knowledge identified during the literature review): (1) the semantic entity of an explanation; (2) the aim of the explaining process; and (3) the explanation purpose. For each component, we present published definition fragments and asked participants to rate their agreement. The overwhelming impression from the participants was this part was incomprehensible as it was wrongly assumed that there is single definition. The participants highlighted the fact that the definition of an explanation is influenced by who is using the explanation and for what reason. Therefore, in the second round, we opted for an abstract definition such as \\\"an explanation of Al is an output that assists the user to achieve his/her purpose.\\\", while:\nUser: who is intended to use the explanation, such as: doctor, patient, model expert, lawyer, regulator etc.\nPurpose: the purpose for requiring an explanation, such as: increase the trust in model's recommendation, improve the understanding of model's outcome, debug the model, ensure fair and unbiased decisions, inspect the model's properties, etc.\nOutput: the type of explanation output, such as: a counterfactual statement, a justification, a list of relevant inputs, etc.\nParticipants agreed much more with this abstract definition and they provided us with valuable feedback. Some generic comments were provided, such as \u201cwhile I like the abstract definition, I find it a little vague", "as": "n\"The definition needs to include reference to where the output comes from\"\n\"It feels like there needs to be some clarification that the explanation is often an additional output or an extension to the output of the Al\u201d\n\"The relationship between the explanation and what is being explained\"\n\"I think the focus should be more on the fact that an explanation should assist the user to understand what and how the Al made the particular prediction/decision and less about what the user might do with that decision\"\n\"This definition is so general that it seems to make no distinction between the prediction (whether probabilistic or not) and the explanation\"\nUsing a taxonomical approach, we identified common themes among the participants' feedback, which resulted in our proposed definition.\nObjective 2 \u2013 What are the attributes of a good explanation in health-AI?"}, {"title": "5.2 Implications for theory and practice", "content": "We believe that investigating the two fundamental questions that have been neglected: 1- What is an explanation in health-Al? and 2- What are the attributes of a good explanation in health-Al?, will have important implications for different affected parties, such as the:\nXAI research community: The output of this study will significantly impact the XAl research community by providing consistent fundamental elements for XAI. More specifically, by understanding the attributes of a good explanation, new algorithms for developing improved and more holistic explanations can be developed. However, generating explanations is not enough, it is also crucial to evaluate how good these explanations are. Thus, the proposed list"}, {"title": "5.3 Objections on the need of XAI and counterarguments", "content": "Despite the increased research interest and recent regulation in favour of XAI, many researchers object to the imposition of XAI [72]. Some of the main criticisms against XAI include:\nLoss of accuracy: One of the main criticisms is that the requirement for explainability might lead to a preference for simpler models, resulting in a loss of accuracy [73]. However, it is also possible that the advocacy for transparency and explainability may lead to a general performance improvement for three reasons: (i) it will help ensure impartiality in decision- making, i.e. to highlight bias in the training dataset; (ii) it will act to improve robustness by highlighting potential adversarial perturbations that could change the prediction; and finally, (iii) it can help ensure that only meaningful variables influence the output, i.e., guaranteeing that an underlying truthful causality exists in the model reasoning [50].\nLoss of Al power: Expecting humans to review and understand Al reasoning would undermine the key benefits of using Al. Al may yet be far from capable of performing at the level of human cognition and emotional intelligence. However, demanding that the operation of Al \u2018slow down' so people can follow along in each use case could defeat the entire purpose of using Al [74]. Unfortunately, this perception of XAI is misleading. It is known that Al is powerful as a (self) learning system as it can continuously ingest new information and search for solutions in multiple different and nonlinear ways that may not always be understood by the user. That is why XAI should aim to provide a glimpse into the Al reasoning and not a fully detailed description of how the Al works.\nNeither necessary, nor sufficient to establish trust in Al: There is a belief that XAI is not necessary for having a trustworthy Al and that a merely accurate Al that is externally validated to show robustness and generalisability is sufficient [75]. However, XAl does not aim to replace model validation (either internal, temporal or external). Both aspects, XAI and validation, are crucial steps needed to establish trust and enable Al usability and integration into the existing workflows [11], [76]. Another belief is that when an Al model produces accurate predictions that aid clinicians to better treat their patients, that model may be useful even without detailed explanations [46]. For example, some Al tools are used to read, interpret and report medical images. However, when Al models are used in an automated fashion, laws and regulations should require an explanation of Al decisions to ensure that they are transparent, fair and capable of reasoned defeasibility [12].\nNot a prerequisite for legitimate and responsible Al: Finally, few researchers believe that XAI is not a prerequisite for responsible Al. They believe that there are many aspects in our life that we do not fully understand how they work, but we accept that they do work. They believe Al will be viewed very similarly in the near future [74]. However, we believe this is a simplistic"}, {"title": "5.4 Limitations and future research", "content": "The first strength of this study is that it aims to answer a fundamental question related to XAI by synthesising published literature and expert opinions. The second strength is that expert opinion was collected using a well-structured Delphi study that comprised two rounds and was supervised by a steering group of suitably qualified academics. Third and finally, the main strength of this study is that the reviewed literature and expert participants in the Delphi study were not limited solely to healthcare, but were selected from three diverse groups: (i) end user decision makers in healthcare; (ii) Al developers; and (iii) XAI theorists. This allowed us to answer the question using an interdisciplinary approach that was lacking in the literature alone, and this work profits from the opinions of these diverse disciplines.\nThis study has several limitations. First, even if a diverse and sufficient number of participants were included in the Delphi study, having more expert participants would have strengthened our conclusions. It would have also allowed us to study differences between the three types of responders with regard to their rating of attribute importance. While more than 100 participants were invited to participate in this study, only 39 consented and took part in our study. In addition, key decision-makers of regulators, such as administrators/hospital management and policymakers were lacking from our participant cohort. This expert group was initially considered in the study design and 17 regulators were invited to participate, but none responded. Finally, patients were not considered in this study's design but could potentially have offered useful insights regarding what it means for Al to be explainable to their experience as a healthcare consumer. Both regulators and patients should be included in future Delphi studies. Our future research will also seek to go deeper and clarify the unique attributes of a good explanation for different subdomains of medicine (e.g. acute medicine, chronic conditions) and different explanation purposes."}, {"title": "6. Conclusion", "content": "While the demand for XAI in healthcare is high, determining what constitutes a good explanation is ad hoc, and providing adequate explanations remains challenging. Without a solution to the problem of trustworthy Al and user acceptance of healthcare technologies generally, the undeniable benefits of these systems will never be realised, and all our efforts to develop accurate health-Al will be in vain. This research aimed to shed light on two fundamental questions of explanation in health-Al that remain unanswered; (1) What is an explanation in health-Al?, And (2) What are the attributes of a good explanation in health-Al? In this study, we synthesise for the first-time published literature and expert opinions, using a Delphi study, from a diverse research background. The research outputs are (1) a definition and (2) a global list of attributes of a good explanation in health-Al."}]}