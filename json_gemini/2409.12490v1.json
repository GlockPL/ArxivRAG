{"title": "CRITIPREFILL: A SEGMENT-WISE CRITICALITY-BASED APPROACH FOR PREFILLING ACCELERATION IN LLMS", "authors": ["Junlin Lv", "Yuan Feng", "Xike Xie", "Xin Jia", "Qirong Peng", "Guiming Xie"], "abstract": "Large language models have achieved notable success across various domains, yet efficient inference is still limited by the quadratic computation complexity of the attention mechanism. The inference consists of prefilling and decoding phases. Although several attempts have been made to accelerate decoding, the inefficiency of the prefilling phase, especially for long-context tasks, remains a challenge. In this paper, we observe a locality in query criticality during the pre-filling phase of long-context processing: adjacent query tokens tend to focus on similar subsets of the past Key-Value (KV) cache. Based on this observation, we propose CritiPrefill, a criticality-based segment-wise prefilling method. This method partitions the input sequence's queries and KV cache into segments and blocks, utilizing a segment-wise algorithm to estimate the query criticality. By pruning non-critical computations between query segments and cache blocks in the self-attention mechanism, the prefilling process can be significantly accelerated. Extensive evaluations on multiple long-context datasets show up to 2.7x speedup on Llama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100 GPU, with minimal quality degradation.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) have been widely applied across various fields, showcasing im-pressive capabilities in handling long-context tasks such as long text comprehension (Laban et al.,2023), multi-turn dialogue QA (Yang et al., 2018), in-context learning (Dong et al., 2022), and agenttasks(Wang et al., 2024). The rapid advancement of open-source LLMs has significantly reduced thetraining costs for downstream applications. However, a key challenge remains in the quadratic in-ference cost of the self-attention mechanism in Transformer layers while processing long sequences(Wan et al., 2024; Dong et al., 2023; Zhou et al., 2024).\nTypically, LLMs are composed of multiple Transformer layers, each containing a self-attention layerthat significantly contributes to the inference cost in long-sequence scenarios (Wang et al., 2020).Inference within each self-attention layer consists of two phases: the prefilling phase and the decoding phase. During the prefilling phase, LLMs calculate the Key-Value (KV) cache for all inputtokens and predict the first output token, which takes the majority of the computation cost dur-ing inference (Qin et al., 2024a). In the subsequent decoding phase, the model generates tokensautoregressively by leveraging the most recent query token along with the entire KV cache fromall previous steps. While this phase involves less computation, it is primarily constrained by theinput/output (I/O) latency of the KV cache (Leviathan et al., 2023), resulting in a memory-boundbottleneck.\nTo address this challenge, plug-and-play (P&P) methods have been extensively explored for seam-less integration into any LLM, to reduce inference costs without the need of substantial fine-tuning"}, {"title": "2 METHOD", "content": ""}, {"title": "2.1 QUERY CRITICALITY IN ATTENTION MECHANISM", "content": "Modern LLMs are generally constructed using multi-layer Transformer blocks, with the self-attention mechanism as the central operation. During the prefilling phase, the computation in eachlayer's self-attention is heavily dependent on the hidden states $X \\in \\mathbb{R}^{n \\times d}$ of all input tokens fromthe previous layer, which are transformed into Query $Q \\in \\mathbb{R}^{n \\times d}$, Key $K \\in \\mathbb{R}^{n \\times d}$, and Value"}, {"title": "2.2 LOCALITY PATTERN OF QUERY CRITICALITY", "content": "To leverage query criticality for computation pruning, we conduct a deliberated study focusing on thelong-context question answering (QA) task. Our findings reveal a locality pattern in query criticalityduring the prefill process. Specifically, for a 4K-length QA context on the Llama3-8B model, wefirst identify the critical KV cache subset ${K_i, V_i}$ for each query $q_i \\in Q$:\n(Ki, Vi) = (K[indexi], V[indexi])\nwhere indexi = argtopk(softmax(qiKT), k = 512)\nFigure 3 visualizes the similarity of critical cache subsets for different queries $q_i$ and $q_j$, using themetric $$\\frac{|I_i \\cap I_j|}{|I_i \\cup I_j|}$$. The results show that nearby queries tend to share critical KV cache subsets, asevident by the higher similarity near the diagonal. As the distance between queries grows, thissimilarity declines. This criticality locality pattern suggests that adjacent queries show closer criti-cality, allowing for collective prediction of query segments. Thus, by leveraging the locality, it cansignificantly reduce the computational cost of criticality estimation, while exploiting the structuredsparsity from segment-wise criticality."}, {"title": "2.3 PROPOSED METHODS", "content": "In this section, we present CritiPrefill, a segment-wise criticality-based method for accelerating theprefilling process by leveraging the locality pattern in query criticality. This approach improvesefficiency by estimating criticality at the segment level and utilizing structured sparsity to prunecomputations within the self-attention mechanism."}, {"title": "2.3.1 ESTIMATING SEGMENT-WISE QUERY CIRITICALITY", "content": "As described in Algorithm 1, CritiPrefill estimates segment-wise query criticality segment-wise,significantly outperforming token-wise approaches in terms of efficiency. First, the Query and KVCache are firstly divided into segments and blocks. Then, representative queries and keys are se-lected for each segment and block via element-wise max and min operations. Lines 9-13 calculateattention scores for these representatives, producing criticality scores between segments and blocks.Additionally, a layer-fusion mechanism is employed in line 14 to refine the estimation, drawing oninsights from prior works on inter-layer similarity (Elhoushi et al., 2024)."}, {"title": "2.3.2 PRUNING SELF-ATTENTION MECHNISUM", "content": "Based on the estimated segment-wise criticality scores, CritiPrefill applies structured pruning to theself-attention mechanism (Algorithm 2). Rather than standard dense attention, it eliminates non-critical KV Cache blocks for the computation of each query segment, thereby reducing redundantcomputation operations and speeding up the prefilling phase. Thus, the selective attention mecha-nism accelerates the overall process, while maintaining generation quality."}, {"title": "3 EXPERIMENT", "content": "In this section, we conduct experiments on datasets with various sequence lengths and scenarios todemonstrate the accuracy and acceleration effects of our approach."}, {"title": "3.1 SETTINGS", "content": ""}, {"title": "3.1.1 DATASET", "content": "We conduct thorough evaluations across four datasets in both Single-hop (Ko\u010disk\u00fd et al., 2018) andMulti-hop QA (Yang et al., 2018) scenarios. For Single-hop QA, where answers come from a sin-gle piece of evidence, we assess the Loogle Short-dependency QA (SD) (Dasigi et al., 2021) andMultiFieldQA (MF) (Bai et al., 2024) datasets. For Multi-hop QA, which requires integrating in-formation from multiple sources, we assess the Multiple Information Retrieval (MIR) (Yuan et al.,2024) and Comprehension and Reasoning (CR) tasks in Loogle datasets. Following the method-ology of (Yuan et al., 2024), we categorize each dataset into two groups based on context length:64k and 128k for evaluation under different context lengths. Additionally, the widely-used \"Needle-in-a-Haystack\" (Liu et al., 2024) test, a synthetic QA task designed to assess long-context retrievalcapabilities, is employed to facilitate a detailed evaluation across various context lengths."}, {"title": "3.1.2 BASELINE", "content": "We compare the CritiPrefill method with vanilla LLMs to demonstrate that our approach signifi-cantly accelerates prefilling with minimal quality loss. Additionally, a CritiPreill method withoutthe layer-fusion mechanism is also included as an ablation study to illustrate that this mechanismeffectively enhances the accuracy of segment-wise criticality estimation. All methods are based ontwo open-source LLMs, Llama3-8B-1M (Touvron et al., 2023), Yi-9B-200K (AI et al., 2024), whichare widely used for long context evaluation due to their moderate parameter size and remarkable ca-pability of long context processing(Xiong et al., 2023)."}, {"title": "3.1.3 PARAMETER SETTINGS", "content": "In all experiments, the query segment size is set to 512, the key-value block size to 32, and thebudget size to 1024. The layer-fusion hyperparameter, a, is fixed at 0.25. All experiments employFlash Attention (Dao, 2023) and FP16 for efficient computation on Nvidia A100 GPUs. For moredetails, please refer to our code at https://github.com/66RING/CritiPrefill."}, {"title": "3.2 EVALUATIONS CROSS MULTIPLE SCENARIOS", "content": "The test results for each model across all datasets are summarized in Table 1. Overall, our CritiPrefillmethod achieves an average speedup of over 3x across all datasets, with minimal degradation inquality. Specifically, on the Llama3-8B model, CritiPrefill achieves a 3.0x speedup with only aslight drop in quality score from 25.69 to 25.24. The Yi-9B model attains a 3.4x speedup, whilethe quality score remains nearly unchanged, shifting from 13.13 to 13.14. This demonstrates thatCritiPrefill's segment-wise estimation algorithm not only effectively approximates query critical-ity but also successfully implements structured attention pruning, resulting in tangible accelerationgains. Furthermore, when examining datasets with varying sequence lengths, CritiPrefill's accel-eration becomes more pronounced with longer sequences. For instance, on the SD dataset usingLlama3-8B, CritiPrefill achieves speedups of 2.2x and 3.3x at the 64K and 128K sequence lengths,respectively. This is primarily because, with longer sequences, CritiPrefill can prune more irrelevantoperations during AttentionPrefill, leading to greater performance improvements.\nFurthermore, comparing CritiPrefill with CritiPrefill without layer-fusion shows that while there isalmost no difference in speed, our method without layer-fusion shows a significantly lower gener-ation quality. This indicates that the layer-fusion mechanism effectively enhances the accuracy ofcriticality estimation without introducing excessive computational overhead."}, {"title": "3.3 EVALUATIONS ON NEEDLE-IN-A-HAYSTACK TEST", "content": "The Needle-in-a-Haystack test, as shown in Figure 4, is conducted to thoroughly evaluate the impactof the information retrieval capabilities across varying lengths. Both the Llama-3 and Yi modelsexhibit nearly lossless performance in the Needle-in-a-Haystack test, ranging from 0 to 128K contextlengths when utilizing CritiPrefill methods. As illustrated in Figure 5, the acceleration provided byCritiPrefill becomes increasingly significant as sequence lengths grow. It effectively reduces thequadratic prefilling time to linear without compromising performance at lower sequence lengthslike 8K-16K."}, {"title": "4 CONCLUSION", "content": "In this work, we propose CritiPrefill, a plug-and-play method to accelerate the prefilling phase inLLMs. Our key insight is identifying a locality pattern where neighboring query tokens rely on simi-lar critical KV cache. This enables efficient segment-wise criticality estimation and structured prun-ing of self-attention computation, significantly reducing the computational load during the prefillingphase. By introducing a layer-fusion mechanism to refine criticality across layers, CritiPrefill en-hances generation quality without compromising efficiency. Experiments on long-context QA tasksdemonstrate up to 3.0x speedup with minimal accuracy loss on models such as Llama3-8B and Yi-9B. CritiPrefill provides an effective solution to the long-sequence inference bottleneck, requiringno architectural changes or fine-tuning, making it highly adaptable for practical applications."}]}