{"title": "Efficient Deep Learning Board: Training Feedback Is Not All You Need", "authors": ["Lina Gong", "Qi Gao", "Peng Li", "Mingqiang Wei", "Fei Wu"], "abstract": "Current automatic deep learning (i.e., AutoDL) frameworks rely on training feedback from actual runs, which often hinder their ability to provide quick and clear performance predictions for selecting suitable DL systems. To address this issue, we propose EfficientDL, an innovative deep learning board designed for automatic performance prediction and component recommendation. EfficientDL can quickly and precisely recommend twenty-seven system components and predict the performance of DL models without requiring any training feedback. The magic of no training feedback comes from our proposed comprehensive, multi-dimensional, fine-grained system component dataset, which enables us to develop a static performance prediction model and comprehensive optimized component recommendation algorithm (i.e., \\(\\alpha\\beta\\)-\u0392\u039f search), removing the dependency on actually running parameterized models during the traditional optimization search process. The simplicity and power of EfficientDL stem from its compatibility with most DL models. For example, EfficientDL operates seamlessly with mainstream models such as ResNet50, MobileNetV3, EfficientNet-BO, MaxViT-T, Swin-B, and DaViT-T, bringing competitive performance improvements. Besides, experimental results on the CIFAR-10 dataset reveal that EfficientDL outperforms existing AutoML tools in both accuracy and efficiency (approximately 20 times faster along with 1.31% Top-1 accuracy improvement than the cutting-edge methods). Source code, pretrained models, and datasets are available at https://github.com/OpenSELab/EfficientDL.", "sections": [{"title": "INTRODUCTION", "content": "EEP learning is showcasing its immense potential across various domains, including computer vision and natural language processing. However, can practitioners truly harness the full power of deep learning models? Researchers and practitioners often face the challenge of investing substantial time and computational resources to manually select suitable model architectures, tune hyperparameters (such as learning rate, batch size, and number of epochs), and augment data to align with the specific characteristics of their datasets. As a result, achieving optimal performance with deep learning models can be particularly daunting for beginners.\nRecently, automatic machine learning (i.e., AutoML) techniques have emerged to streamline the design of deep learning systems, including tools like AutoML-CFBO [1], AutoKeras [2], and AutoFormer [3]. These techniques enhance the performance of machine learning models by automatically discovering optimal model architectures and hyperparameters for specific tasks. As a result, AutoML has proven to be invaluable for deep learning developers, enabling them to more easily apply deep learning to their particular applications.\nHowever, the optimization rules of these AutoML methods rely on the models' training feedback under given parameters. For example, upon receiving a new set of parameters, LEAF [4] requires training the deep learning model for three epochs under the current parameters to assess their impact, while Auto-Pytorch [5] utilizes the deep learning model trained for fifty epochs for evaluation. Consequently, the time and resource savings from AutoML methods are also limited, especially when dealing with large training datasets. For example, as illustrated in Fig. 1, Auto-Pytorch took 10 hours to discover optimal model architectures and hyperparameters for image classification task on CIFAR-10.\nInspired by current shortcomings that rely on training feedback from actual runs in AutoML, we explore into the following two questions:"}, {"title": "", "content": "1) Can AutoML be endowed with the ability to foresee model performance, thereby circumventing the need for performance evaluation?\n2) How can we ensure the accuracy of performance forecasts?\nWe argue that the resolution of the above questions pivots on the construction of a comprehensive, multi-dimensional, fine-grained system component dataset. Such a dataset serves a dual purpose: 1) it facilitates the development of a static performance prediction model in a data-driven manner, thereby endowing the AutoML methods with predictive capabilities. 2) The diversity and richness of the dataset contribute to the robustness and reliability of the performance prediction model. However, current AutoML methods mainly focus on a single constrained search space, typically involving fixed model architectures and specific hyperparameters within a limited range. For instance, ProxylessNAS [6] is designed for the automated construction and optimization of convolutional neural networks, concentrating on key parameters such as kernel size, number of channels, network depth, and skip connections. Similarly, LiteTransformerSearch [7] automates the design and optimization of architectures within the GPT family, emphasizing parameters like layer count, model dimension, adaptive embedding size, feedforward network dimensions within transformer layers, and the number of attention heads per layer. Such benchmarks may struggle to reach their full potential when tackling more complex tasks, particularly those that require a holistic consideration of multiple factors, data variations, and diverse hardware environments.\nTo this end, we propose a large-scale deep learning system component set comprising twenty-seven adjustable components. These components can be categorized into three dimensions: model and training components (including model architecture, training optimization, regularization and generalization, and framework), data components (e.g., size of training set, Input length, and Similarity between training and testing set), and hardware components (e.g., GPU type and number of GPUs). Each component includes a diverse array of candidate values, facilitating the configuration of DL systems across various model architectures, data augmentation techniques, and hardware setups.\nBased on the proposed system component dataset, we further introduce EfficientDL, an innovative deep learning board designed for automatic performance prediction and component recommendation. EfficientDL is powered by two key elements: a static performance prediction model and an optimized component recommendation algorithm, which enable EfficientDL to swiftly and precisely recommend twenty-seven system components and predict the performance of DL models without requiring any training feedback. Specifically, we first present a static performance prediction model using Random Forest (i.e., RF) regression to eliminate reliance on actual running of parameterized models during the optimization search. Importantly, we enhance the component recommendation process through an improved Bayesian Optimization method, called \\(\\alpha\\beta\\)-\u0392\u039f search, to efficiently identify well-performing configurations within a reduced search space.\nExperimental results on the CIFAR-10 dataset show that EfficientDL can rapidly recommend a deep learning system for image classification tasks, significantly outperforming state-of-the-art AutoML methods in both speed and accuracy (See in Figure 1). Compared to existing state-of-the-art AutoDL frameworks (particularly Auto-PyTorch), our EfficientDL reduces the time by at least 20-fold while identifying a model with fewer parameters and superior performance, achieving a Top-1 accuracy of 91.31%. Furthermore, the simplicity and power of EfficientDL stem from its compatibility with most DL models. For example, tests conducted on six popular model architectures, including ResNet50, MobileNetV3, EfficientNet-B0, MaxViT-T, Swin-B, and DaViT-T, demonstrate that EfficientDL achieves superior performance compared to handcrafted state-of-the-art DL systems. For instance, EfficientDL enhances Top-1 accuracy by 0.13% to 0.69%. Additionally, when employing the pre-trained models generated by our recommended components as backbones for object detection tasks, EfficientDL improves the mean Average Precision (i.e., mAP) by more than 0.49 for Swin-B and 0.22 for MobileNetV3. We have released a replication package in the GitHub repository to provide transparency into our research process.\nIn summary, the main contributions of our study are fourfold:\n1) We propose a large DL system component set covering twenty-seven changeable components of DL, including model and training, data, and hardware dimensions.\n2) We construct ImageClassEval, a multi-dimensional, fine-grained dataset tailored for computer vision tasks.\n3) We propose EfficientDL, an innovative deep-learning board designed for automatic performance prediction and component recommendation without relying on training feedback from the actual run in search optimization."}, {"title": "RELATED WORK", "content": "To assist developers in discovering suitable DL systems for their specific tasks, DL researchers primarily focus on automated DL techniques and well-designed benchmarks. Next, we will present the related work.\nAutomated Deep Learning (i.e., AutoDL) Techniques: AutoDL techniques [8\u201312] refer to a suite of methods and tools designed to automate various stages of the deep learning pipeline, reducing the need for human intervention and expertise. Currently, these techniques tend to focus more on neural architecture search (i.e., NAS), and hyperparameter tuning.\nNAS [13] automates the process of designing neural network architectures, which explores a predefined search space to find the optimal architecture for a given task with different search strategies (e.g., Reinforcement Learning (RL) [14, 15], Evolutionary Algorithms [16], and one-shot weight-sharing strategy [3]). For example, Tan et al. [17] optimize the combination of MBConv and Fused-MBConv modules by NAS on EfficientNetV2, which improves the model efficiency and reduces the model size by 6.8 times. Gupta et al. [9] apply the NAS to search the convolution Block, convolution kernel, and activation function components. However, current NAS methods are mainly designed to search for a specific structure space, requiring users to predefine a search space as a starting point.\nHyperparameter tuning involves finding the best set of hyperparameters (e.g., learning rate, batch size) that maximize the performance of a model architecture with grid search, random search, or Bayesian optimization [18]. For example, Diaz et al. [19] apply a derivative-free optimization tool to automatically and effectively search the appropriate parameters for neural networks. However, currently, most hyperparameter optimization methods are only treated as a subsequent step to NAS, ignoring the interaction between hyperparameters and choice of architecture."}, {"title": "MULTI-DIMENSIONAL SYSTEM COMPONENT DATASET", "content": "The performance of DL models is influenced by a multitude of factors, which we regard as components of DL systems and categorize into three groups: model and training components (including architecture, training optimization, regularization and generalization, and framework), data components, and hardware components on which the DL models run. Based on this categorization, we design a comprehensive and fine-grained set of deep learning components."}, {"title": "System component design", "content": "Model and training components encompass a diverse set of modules, strategies, and parameter configurations that are essential for constructing, training, and optimizing deep learning models. These components work together to define the model's architecture, learning capacity, generalization performance, and task-specific results. Broadly, they can be classified into four primary categories: model architecture, training optimization, regularization and generalization, and framework.\nModel architecture components refer to the fundamental building blocks and overall design of the model. These components define how the model processes input data, how information is passed through the model, and how output is generated. Our study mainly focuses on Convolutional Neural Networks (e.g., ResNet [25], MobileNetV2 [26], and AlexNet [27]) and Vision Transformers (e.g., Swin Transformer [28] and DeiT [29]), so the main model architecture components include:\n\u2022\tNormalization layer: It helps address issues such as internal covariate shift, where the distribution of inputs to each layer changes during training, impeding network convergence. The list of normalization methods includes Layer Normalization, Batch Normalization, Spectral Normalization, Weight Normalization, etc.\n\u2022\tInitialization: Initialization impacts deep learning performance by affecting gradient flow, optimization efficiency, and convergence speed, leading to better training and generalization. The list of initialization methods includes Kaiming Initialization, Xavier Initialization, Fixup Initialization, etc.\n\u2022\tConvolution: It involves a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. The list of convolution operations includes Ordinary Convolution, Depthwise Convolution, Pointwise Convolution, Grouped Convolution, etc.\n\u2022\tSkip connection: Skip connections enhance model performance by improving gradient flow and information transfer,"}, {"title": "Model and training components", "content": "leading to better training stability and accuracy. The list of skip connections includes Residual connection, concatenated skip connection, Zero-padded shortcut connection, and Deactivable Skip connection, etc.\n\u2022\tActivation function: Activation functions impact model performance by introducing non-linearity, enabling the network to learn complex patterns and improving its ability to make accurate predictions. The list of activation functions includes ReLU, GELU, Sigmoid Activation, Tanh Activation, Leak ReLU, etc.\n\u2022\tPooling operation: Pooling operations are crucial for DL model performance as they extract key features, reduce spatial dimensions and computational complexity, improve robustness through translation invariance, and minimize overfitting by reducing parameters. The list of pooling operations includes Max Pooling, Average Pooling, Global Average pooling, Center Pooling, etc.\n\u2022\tFeedforward network: Feedforward networks are the foundation for many deep learning models, and their simplicity makes them easier to understand and implement compared to more complex networks like recurrent or convolutional neural networks. The list of feedforward networks includes Dense Connections, Linear Layer, Position-Wise Feed-Forward Layer, Feedforward Network, Adapter, etc.\n\u2022\tAttention mechanism: Attention mechanism assigns different levels of importance (weights) to different input elements based on their relevance to the current task, allowing the model to prioritize certain information over others. The list of attention mechanisms includes Scaled Dot-Product Attention, Multi-Head Attention, Fixed Factorized Attention, Additive Attention, etc.\n\u2022\tOutput function: Output function shapes the output format, optimization, gradient flow, and performance metrics, ensuring the model's suitability for various tasks. The list of output functions includes Softmax, Heatmap, Mixture of Logistic Distributions, Adaptive Softmax, PAFs, etc.\nTraining optimization components refer to the various techniques, algorithms, and hyperparameters used to improve the performance, efficiency, and convergence of DL models during the training process. They are crucial for ensuring that the model learns effectively from the data, avoids issues like overfitting or underfitting, and converges to a solution in a reasonable amount of time. These components include:\n\u2022\tLearning rate schedule: It controls the rate of learning over time, impacting convergence speed, training stability, and the model's ability to reach an optimal solution. The list of learning rate schedules includes Linear Warmup, Cosine Annealing, Step Decay, Linear Warmup with Decay, Linear Warmup with Cosine Annealing, etc.\n\u2022\tOptimization algorithm: The optimization algorithm determines how these parameters are updated, thereby influencing the model's convergence speed and overall performance. The list of optimization algorithms includes Adam, SGD, Adafactor, RMSProp, AdaGrad, AdamW, SGD with Momentum, etc.\n\u2022\tSize of parameter: A higher number of parameters increases the model's expressive power but also adds computational complexity and the risk of overfitting.\n\u2022\tBatch size: Larger batch sizes provide more stable gradient estimates but require more computational resources; smaller batch sizes allow for more frequent updates, which may help escape local minima."}, {"title": "", "content": "\u2022\tLearning rate: determines the step size for updating the model's parameters in each iteration. An excessively high learning rate can lead to unstable training and non-convergence, while an overly low learning rate can result in slow convergence or getting stuck in local minima.\n\u2022\tEpochs: More epochs enable the model to fit the data better but can also increase the risk of overfitting.\nRegularization and generalization components refer to techniques and strategies used in machine learning to improve a model's ability to generalize to unseen data and avoid overfitting during training. These components help the model perform well not only on the training data but also on new, previously unseen data. These components include:\n\u2022\tRegularization: It helps prevent overfitting, improving the model's ability to generalize to new, unseen data. The list of regularization strategies includes Dropout, Label Smoothing, Attention Dropout, R1 Regularization, Stochastic Depth, L1 Regularization, etc.\n\u2022\tData augmentation: Data augmentation would expand the dataset without incurring additional data collection costs, thereby improving the generalization ability of machine learning models, reducing the risk of overfitting, and enhancing the model's robustness in different scenarios. The data augmentation list includes image rotation, scaling, translation, flipping, cropping, color adjustment, noise addition, etc.\nFramework refers to a collection of tools and libraries designed to build, train, and deploy deep learning models. It provides a set of efficient and user-friendly programming interfaces and pre-built functional modules, simplifying the development process of deep learning models. The framework list includes PyTorch, Tensorflow, Caffe, Caffe2, etc."}, {"title": "Data components", "content": "Data components refer to the characteristics and structure of the dataset used to train, validate, and test DL models. These components are critical as they directly influence the models' performance, the design of the learning tasks, and how well the models generalize to new data. Key data components include:\n\u2022\tSize of training set: refers to the total number of data instances used during model training. The size has a direct impact on the effectiveness of the model's training: a larger training set typically provides richer information, enhancing the model's learning capacity in handling new data. However, an excessively large training set may extend training duration and require more computational resources. Conversely, a training set that is too small cannot provide sufficient information, impeding the model's ability to generalize effectively.\n\u2022\tSize of testing set: refers to the total number of data instances used to evaluate the model's performance after training, aimed at verifying the model's generalization ability and effectiveness in practical applications. The size of the testing set influences the stability and reliability of the evaluation results.\n\u2022\tInput length: refers to the specific dimension or sequence length of each input sample that a deep learning model receives, determining the input shape and processing method of the model. The input length can vary depending on the type of data, such as, for image data, input length usually refers to the width and height of the image (in pixels) and the number of channels (e.g.,"}, {"title": "", "content": "3 channels for an RGB image). For text data, input length usually refers to the number of words or characters in a text sequence.\n\u2022\tOutput length: refers to the specific dimension or sequence length of the output generated by a deep learning model, determining the model's output shape and designed according to the task's requirements. The output length depends on the type of model and the application scenario, closely aligning with the model's intended task. For instance, for classification tasks, the output length typically equals the number of classification categories. For regression tasks, the output length is usually 1, representing the prediction of a single continuous value. However, in multi-dimensional regression, the output length may be greater than 1.\n\u2022\tSimilarity between training and testing set: refers to the degree of resemblance between the training data and the test data in terms of feature space or distribution. High similarity usually indicates that the training set and test set come from similar distributions, which can lead to better model performance on the test set because the inputs it encounters during testing are similar to those seen during training. In our study, cosine similarity [30], Jensen-Shannon (JS) [31], and L2 distance [32] serve as the metrics for assessing the similarity between training and testing sets."}, {"title": "Hardware components", "content": "Hardware components refer to the physical computing resources and devices that perform model training and inference tasks. The performance and configuration of the hardware have a direct impact on the speed, efficiency, scalability, and overall effectiveness of deep learning models. In our study, we would focus on the GPU devices, which have a significant impact on the performance of deep learning models, primarily in accelerating the training process, reducing training time, and improving model scalability.\n\u2022\tType of GPU: refers to the different kinds of GPUs used for handling deep learning tasks. Various types of GPUs offer different computational power, memory size, power consumption, and architectural design, all of which determine their performance in deep learning models. Selecting the appropriate GPU type is crucial for enhancing model training and inference efficiency. The type of GPU list includes V100, GeForce GTX 1080 Ti, NVIDIA RTX 2080 Ti, NVIDIA GTX980, etc.\n\u2022\tNumber of GPUs: refers to the number of GPUs used to execute deep learning tasks. The number of GPUs directly impacts the training speed, parallel computing capability, and overall performance of a deep learning model. Proper configuration of the GPU quantity can significantly enhance model training efficiency, reduce training time, and enable large-scale deep learning tasks to be completed within an acceptable timeframe.\nThe configuration space for each fine-grained component is presented in Table 5 of Appendix. From Table 5, we can observe that most of model architecture components and Hardware components are discrete values, while data components and training optimization components are continuous values. Furthermore, not all components exhibit unique values, as they may represent a combination of several discrete values. For example, Pooling Operations used in ResNet [33] include Average Pooling, Global Average Pooling, and Max Pooling."}, {"title": "Multi-dimensional dataset construction", "content": "Drawing upon the designed component set, our objective is to construct a multi-dimensional dataset to facilitate the development of the static performance prediction method, aiming to eliminate the dependency on actual training and running current parameterized models in search optimization. Given that the designed metrics encompass various model structures, parameters, and GPU types, the cost of conducting these experiments is substantial. As a result, we focus on gathering multi-dimensional datasets from existing published research papers that have been generally recognized to be the top peer-reviewed and influential publication. Specifically, the construction of multi-dimensional dataset is mainly divided into four steps:\nSearching for primary studies on specific tasks using DL models. We first search google scholar\u00b2 for primary studies concentrating on the application of DL models in addressing specific tasks (e.g., image classification, text classification) with the terms of specific tasks or deep learning. We then go through the abstract of all the searched papers to focus on the specific task with the DL models. Subsequently, we perform a forward and reverse snowball search [34], tracing all the papers cited by each selected study. We carefully review these references to identify additional relevant studies. This process is repeated recursively for each newly collected paper until all pertinent studies are exhausted.\nExcluding studies without code and parameters. We manually review each searched primary study by step 1 to find whether this primary study includes the code and parameters of pre-trained models. We then only retain these primary studies that include both code and parameters to construct the multi-dimensional dataset.\nExtracting values of fine-grained components and performance for each DL model included in primary studies. We primarily use a combination of automated and manual methods to extract values of fine-grained components and performance for each DL model. Meanwhile, it is important to note that a primary study would include a specific DL model with different values of components and different compared methods, which are treated as multiple instances in our study. Specifically, we first apply regular expressions to automatically identify keywords and corresponding table information (e.g., model parameters, hyperparameters, and performance) in the primary studies to extract values of components. In case where certain components are not explicitly described in the paper, we check the code of DL models to locate the corresponding values. Finally, to ensure the accuracy of the extracted values, we conduct a manual examination of the relevant papers and code, supplementing any missing component values as necessary. Additionally, components that are neither documented in the papers nor the code are recorded as missing values.\nEncoding fine-grained component to form multi-dimensional dataset for the specific task. As presented in Section 3.1, many components, particularly those related to model architecture, are composed of discrete values, with some being different combinations of multiple discrete values. For example, data augmentation component can be a combination of Random Horizontal Flip, Random Resized Crop, and Mixup. Therefore, we apply distinct encoding methods (i.e., one-hot encoding, label encoding, and polynomial encoding) to transform discrete values into a numerical form based on the nature of components. Specifically,"}, {"title": "ImageClassEval: A dataset for performance evaluation and component recommendation of DL models on image tasks", "content": "Followed by the process of constructing a multi-dimensional dataset in Section 3.2, we focus on image classification tasks and develop the ImageClassEval dataset. Next, we provide a detailed overview of the ImageClassEval dataset construction, highlighting its properties, including scale, hierarchy, and diversity."}, {"title": "Constructing ImageClassEval", "content": "ImageClassEval is a comprehensive, multi-dimensional, fine-grained system component dataset for image tasks, which enables researchers and practitioners to develop a static performance prediction model and optimized component recommendation algorithm in a data-driven manner, removing the dependency on actually running parameterized models during the traditional optimization search process. We describe here the process of constructing the ImageClassEval dataset:\nCollecting and cleaning candidate primary studies on image classification tasks with DL models. We first collect 1232 primary studies related to image classification tasks with different deep learning models from 2016 to 2023. We then clean candidate primary studies to gather a total of 342 candidate primary studies with code and parameters of pre-trained models.\nExtracting values of fine-grained components and performance for each DL model. We construct the final ImageClassEval dataset by extracting values of fine-grained components and performance for each DL model. Meanwhile, in our ImageClassEval dataset, we use Top-1 accuracy as the metric for evaluating DL models, since most of the candidate primary studies report this metric.\nThrough automatically identifying keywords and tables in papers and code, we gather a total of 2,433 DL models. Subsequently, after a manual review process, 741 DL models lacking fine-grained components or performance metrics are excluded. Finally, we construct the ImageClassEval dataset with 1,655 DL models."}, {"title": "Properties of ImageClassEval", "content": "ImageClassEval exhibits several key attributes that significantly enhance its utility and effectiveness for research and application, which can serve as a foundational resource, offering a comprehensive and well-structured collection of data points that enable researchers and practitioners to better predict the performance and optimize deep learning models across a wide array of configurations. We briefly explain below the various key attributes that our ImageClassEval exhibits.\nScale. ImageClassEval is designed to offer the most comprehensive and diverse representation of image datasets available. At present, ImageClassEval encompasses 30 distinct image datasets. To the best of our knowledge, ImageClassEval constitutes the largest dataset for performance estimation and component recommendation of deep learning models, specifically tailored for the vision and AutoML research communities. This scale is reflected not only in the sheer number of image datasets but also in the inclusion of highly detailed, fine-grained components.\nHierarchy. ImageClassEval is meticulously organized to reflect the hierarchical nature of deep learning model architectures, with a particular focus on Convolutional Neural Networks (i.e., CNNs) and Transformer-based models.\nFor CNNs, the dataset captures the hierarchical design inherent to these models, encompassing mid-level architectural variations, such as the incorporation of residual connections (as seen in ResNet), the depth of the network. Meanwhile, it includes lower-level details such as the types of pooling operations (e.g., max-pooling or average-pooling) and the application of techniques like batch normalization In the context of Transformer-based architectures, ImageClassEval documents variations across models such as MaxViT, DaViT, and Swin Transformer. This includes differences in layer depth, positional encoding strategies, and the size of feedforward networks.\nThis hierarchical organization not only facilitates a granular analysis of how different architectural components influence overall model performance but also enables meaningful comparisons across different deep learning paradigms. By providing a structured framework, ImageClassEval supports both detailed technical"}, {"title": "EFFICIENTDL: AN INNOVATIVE DEEP LEARNING BOARD", "content": "In this section, we introduce EfficientDL, an innovative deep learning platform built on the comprehensive, multi-dimensional, fine-grained system component dataset outlined in Section 3.1. EfficientDL is designed for automatic performance prediction and component recommendation without relying on training feedback. As shown in EfficientDL includes a static performance prediction model and a comprehensive optimized component recommendation algorithm. The static performance prediction model removes the need for training feedback from actual runs during the optimization process, while the optimized recommendation algorithm efficiently identifies well-performing configurations within a reduced search space."}, {"title": "Static performance prediction model", "content": "Existing AutoDL methods rely on actual runs to assess the effectiveness of recommended DL systems, significantly increasing the time costs associated with identifying the optimal system components. Fortunately, our constructed dataset outlined in Section 3 introduces a paradigm shift. It enables the development of a static performance prediction model in a data-driven manner, capable of directly predicting model performance without the need for actual training iterations. By leveraging this advanced framework, we can avoid the time-consuming training processes inherent in traditional methods and achieve an immediate evaluation of the recommended systems' effectiveness, substantially accelerating existing AutoDL processes in identifying the optimal system components.\nSpecifically, we employ the random forest model as the performance prediction model and train it on our multi-dimensional system component dataset. The diversity and comprehensiveness of the proposed dataset maximize the reliability of the performance prediction model. Besides, to further optimize the performance of this prediction model, we use a grid search strategy to tune the hyperparameters of Random Forest learner, with the range of \"n_estimators\" being {5,10,30,50, 80, 100, 150, 180, 200, 250, 280, 300} and the range of \"max_depth\" being {3,5, 10, 15}. The choice of Random Forest for building a performance prediction model on deep learning model evaluations is motivated by its reduced susceptibility to overfitting and efficient handling of high-dimensional data. Meanwhile, comparing with neural network, Random Forest can provide feature importance scores, helping to interpret which features most significantly impact the model's predictions."}, {"title": "Comprehensive optimized component recommendation algorithm", "content": "Our comprehensive optimized component recommendation algorithm aims to efficiently identify optimal component configurations. This advanced algorithm is powered by two key elements: component recommendation confirmation and Well-performing configurations recommendation."}, {"title": "Component Recommendation Confirmation", "content": "As outlined in Section 3.1, the performance of DL systems would be impacted by various system components. While our proposed static performance prediction model substantially mitigates the time costs in performance evaluation, the simultaneous optimization of all twenty-seven components introduces an expansive search space, potentially increasing the computational burden of the optimization algorithm. Moreover, the complete component recommendation may be unnecessary in certain scenarios where specific components are predetermined due to constraints or user preferences. For instance, when the system is restricted to an NVIDIA RTX 2080 Ti GPU, there is no need to recommend the GPU type. To address these considerations, our EfficientDL provides two strategies for component recommendation confirmation: one strategy involves manually specifying the components and their applicable ranges, while the other strategy automatically identifies the necessary components through an analysis of their importance. To automatically identify the necessary recommended component, we apply permutation feature important [35] to evaluate the importance of components on our static performance prediction model and recommend the Top-5 important components. For the range of each recommended component, we use the searched space outlined in Table 5 of the Appendix."}, {"title": "Well-performing configurations recommendation for DL system components", "content": "After confirming the key components required for optimization in Section 4.2.1, our EfficientDL employs an improved Bayesian optimization strategy, namely \\(\\alpha\\beta\\)-\u0392\u039f search, to recommend well-performing configurations for these essential DL system components. Bayesian optimization [36] enables the search process to rapidly converge to an optimal solution by dynamically balancing the exploration of untested regions with the exploitation of already identified promising areas. Typically, it is an iterative method that can efficiently explore and optimize objective functions by constructing a surrogate model, including three steps: building the surrogate model, selecting the next sampling point, and updating the surrogate model iteratively. Specifically, it begins by constructing a probabilistic model, typically a Gaussian Process, based on the existing sample data to approximate the distribution of the objective function. Then, an acquisition function is used to assess the sampling value of different points, balancing the exploration of new areas with the exploitation of known promising regions. In each iteration, the acquisition function evaluates potential sampling points and selects the next one, typically in the region most likely to improve the objective function. Once the sampling point is selected, the objective function's value is evaluated, and the surrogate model is updated, gradually narrowing the search for the optimal solution. This process is iterated until convergence to the optimal solution of the objective function or until the predefined evaluation limit is reached.\nHowever, within the current Bayesian optimization framework, the use of Expected Improvement (EI) or Probability of Improvement (PI) as acquisition functions often prioritizes points with the highest expected values or the highest probabilities of improvement. While this strategy can be effective, it may inadvertently overlook regions of the search space with significant potential, such as points with lower expected values but higher probabilities, or those with higher expected values but lower probabilities. This limitation becomes especially pronounced when dealing with complex or multimodal objective functions, where the optimization process is at risk of becoming trapped in local optima, thereby failing to thoroughly explore the entire search space.\nTo address these limitations, we propose \\(\\alpha\\beta\\)-\u0392\u039f search, an advanced optimization algorithm designed to explore the search space more effectively using a novel acquisition function and exploration strategy. The \u03b1\u03b2-BO Search method is conceptually straightforward and is implemented based on the Expected Improvement (EI) acquisition function, which can be seamlessly integrated with most surrogate models. This method enhances the exploration of previously uncharted regions by focusing on areas with a higher probability of yielding optimal values that have not been explored yet, while still building upon the information from previously explored areas. Additionally, to mitigate the risk of local convergence, \u03b1\u03b2-BO Search incorporates a probability parameter, \u03a9, which facilitates random searches throughout the exploration cycle.\nThe formula of probability parameter (\u03a9) for random search is as:\n\\(\u03a9 = 1 - k \\times P\\) (1)\nIn Eq. 1, k represents the number of repeated random explorations, and if one exploration is not random, k is reduced to 1. P denotes the user-defined likelihood of random exploration, where a higher value corresponds to a lower probability of random exploration. Through Eq. 1, we can observe that over time, \u03a9 decreases, refining the search focus.\nMeanwhile, the \\(\\gamma\\)EI represents an improved algorithm for the El acquisition function in our \\(\\alpha\\beta\\)-BO search. The formula of the \\(\\gamma\\)EI acquisition function is as:\n\\(\u03b3EI(x) = \u03b1 \\cdot \\int_{-\\infty}^{\\infty} (f_{best} - f(x)) \\cdot p(f(x)|x, D) df(x) + \u03b2 \\cdot P(f(x) > f_{best})\\) (2)\nIn Eq. 2, f(x) be a multi-dimensional Gaussian process as:\n\\(f(x) \\sim GP(m(x), k(x, x'))\\) (3)\nm(x) is the mean function, and k(x,x') is the covariance function. Additionally, in equation 2, \\(f_{best}\\) is the current best observed target value, \u03c3(x) is the standard deviation of the Gaussian surrogate model at position x, and p(f(x)|x, D) is the probability distribution of the target value at position x given the observed data D.\nIndeed, within a multi-dimensional Gaussian surrogate model, the derivation of the \\(\\gamma\\)EI acquisition function is inherently complex, as it involves the cumulative distribution function (CDF) and probability density function (PDF) of the multivariate normal distribution. Therefore, we define a vector Z as:\n\\(Z = \\frac{(f_{best} - m(x))}{\u03c3(x)}\\) (4)\nThen, the formula of the \\(\\gamma\\)EI acquisition function can be further simplified to:\n\\(\u03b3EI(x) = \u03b1 \\cdot (\u03c3(x) \\cdot Z + (f_{best} - m(x)) \\cdot \u03a6(Z)) + \u03b2 \\cdot (P(Z > \\frac{f_{best} - m(x)}{\u03c3(x)}))\\) (5)\nIn Eq. 5, \u03a6(Z) is the cumulative distribution function (CDF) of the multivariate normal distribution. By applying the CDF of the standard normal distribution, this expression can be further transformed into the following form:\n\\(\u03b3EI(x) = \u03b1 \\cdot (\u03c3(x) \\cdot Z + (f_{best} - m(x)) \\cdot \u03a6(Z)) + \u03b2 \\cdot (1 - \u03a6(\\frac{f_{best} - m(x)}{\u03c3(x)}))\\) (6)\nThe detailed process of our \u03b1\u03b2-BO search method is shown in the Algorithm 1 of Appendix B."}, {"title": "EXPERIMENT", "content": "In this section, we first conduct experiments to evaluate the effectiveness of our EfficientDL in recommending DL models for image classification tasks, comparing their performance against the state-of-the-art AutoDL tools. We then do experiments to evaluate the effectiveness of the important components recommended for the"}]}