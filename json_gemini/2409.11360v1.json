{"title": "Al Suggestions Homogenize Writing Toward Western Styles and Diminish Cultural Nuances", "authors": ["DHRUV AGARWAL", "MOR NAAMAN", "ADITYA VASHISTHA"], "abstract": "Large language models (LLMs) are being increasingly integrated into everyday products and services, such as coding tools and writing assistants. As these embedded AI applications are deployed globally, there is a growing concern that the AI models underlying these applications prioritize Western values. This paper investigates what happens when a Western-centric AI model provides writing suggestions to users from a different cultural background. We conducted a cross-cultural controlled experiment with 118 participants from India and the United States who completed culturally grounded writing tasks with and without AI suggestions. Our analysis reveals that AI provided greater efficiency gains for Americans compared to Indians. Moreover, Al suggestions led Indian participants to adopt Western writing styles, altering not just what is written but also how it is written. These findings show that Western-centric AI models homogenize writing toward Western norms, diminishing nuances that differentiate cultural expression.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) are increasingly shaping online discourse and are being integrated into everyday applications to improve user productivity and experience. These include chatbots that aid in content compre- hension\u00b9, code completion systems that streamline programming\u00b2, and autocomplete tools that facilitate faster writing [17]. Writing, in particular, has become a prominent area where LLMs offer substantial assistance, sup- porting tasks such as scientific writing [30], storytelling [83], and journalism [69]. These applications provide inline writing suggestions, which users can accept or reject in real time. Such Al writing suggestions have become integral to email clients (e.g., Gmail Smart Compose [17]), note-taking applications (e.g., Notion), and word processors (e.g., Google Docs). Their growing utility is evident from their integration into popular web browsers like Google Chrome and Microsoft Edge, which now offer autocomplete as a native feature, enabled by default\u00b3. While LLM-embedded applications like text autocomplete have gained widespread global adoption, these technologies also carry significant risks, such as perpetuating harmful stereotypes about marginalized groups and underserved communities. For instance, LLMs have been shown to depict Muslims as terrorists [2] and reinforce negative stereotypes about disabled individuals [27, 89]. Additionally, research has revealed that LLMs prioritize Western norms and values in their interactions [15, 43], resulting in representational harms for diverse non-Western cultures [9, 72, 80]. While explicit cultural stereotyping shown by prior work is deeply problematic, an even more insidious issue lies in the subtle biases LLMs can introduce through their suggestions. For example, recent work shows that autocomplete suggestions can change users' language [39], writing [70], and even attitudes about social topics [42, 90]. Furthermore, unlike chat-based applications such as ChatGPT, LLM-embedded applications do not allow users to prompt and fine-tune the model to suit their cultural preferences. This creates situations where users and AI may have different cultural values, potentially leading to conflicts over whose norms should be expressed. While prior studies have explored cultural harms in LLMs using open-ended prompts, no research has yet examined such cultural clashes in embedded AI applications. In this paper, we examine how AI suggestions influence user-generated content when the AI and users share the same cultural identity versus when they do not. In particular, we ask:\nRQ1: Does writing with a Western-centric AI provide greater benefits to users from Western cultures, compared to those from non-Western cultures?\nRQ2: Does writing with a Western-centric Al homogenize the writing styles of non-Western users toward Western styles?\nTo answer these questions, we conducted a cross-cultural experiment with 118 users from India and the US recruited through Prolific, an online crowdsourcing platform. Participants from both cultures were asked to complete writing tasks. We designed the task prompts using Hofstede's Cultural Onion framework [38], which allowed us to elicit cultural practices ranging from explicit (e.g., food) to implicit (e.g., rituals). Each participant was randomly assigned either to the AI condition receiving autocomplete suggestions from GPT-40 while writing or the No AI condition writing organically without AI assistance. Subsequently, we compared the essays written by participants in the four experimental groups (Indian and American users writing with and without AI suggestions).\nOur analysis yielded two key results. In response to RQ1, we found that while AI boosts productivity for both Indian and American participants, the gains are higher for American participants. This discrepancy raises concerns about quality-of-service harms [80] for non-Western users, wherein they need to put in more effort to achieve similar benefits. In response to RQ2, we found that AI caused Indian participants to write more like"}, {"title": "2 RELATED WORK", "content": "We first review scholarly work exploring cultural bias in AI models, emphasizing how LLMs prioritize Western norms and values. Given our focus on AI-based writing suggestions, we then situate our work within the emerging HCI scholarship on designing, developing, and evaluating AI technologies for writing support."}, {"title": "2.1 Cultural Bias in Al Models", "content": "A large body of HCI and AI scholarship has examined harmful social biases reflected by Al models. For example, researchers have identified biases in AI models across various demographic dimensions, including gender [49, 56, 81], race [36, 63], religion [2], and their intersections [46]. Researchers have also examined how these biases can cause a wide range of real-world harms, including representational, allocative, quality-of-service, and other kinds of harms [8, 80]. Researchers have also begun to examine the issues of AI alignment and representational harms specifically in generative AI technologies like LLMs and text-to-image models. However, most initial examinations to date have primarily focused on dimensions that are either universal (e.g., gender [56], disability [89], age [23]) or relevant only in Western cultures (e.g., race [36]) [63, 67], thereby overlooking biases prevalent in other cultures (e.g., caste in South Asia).\nAn emerging body of HCI scholarship has examined the cultural composition of generative AI technologies [15, 43], and found that these models center western norms and values [15]. For example, Qadri et al. [72] showed that text-to-image models fail to generate cultural artifacts, amplify hegemonic cultural defaults, and perpetuate cultural tropes when depicting non-Western cultures. Even within the West, these AI models tend to prioritize US-centric values and artifacts more [9, 77]. For instance, Johnson et al. [43] showed that GPT-3 interpreted the Australian National Firearms Act, which prioritizes communal safety through gun control, as a loss of liberty and freedom, reflecting the dominant view in the US. Furthermore, Naous et al. [64] showed that LLMs trained in non-Western languages (e.g., Arabic) also show Western bias [64]. While some models reflect cultural differences when probed in local languages, the cultural values they reflect are different from those of people of these cultures [6]. Taken together, these studies show the harms of culturally incongruent AI models [71], including cultural hegemony, erasure, and stereotyping [15, 72], suggesting the critical need to account for these cultural biases.\nIdentifying and addressing these cultural biases is crucial not only to prevent representational, allocative, and quality-of-service harms [4, 80] but also to protect against culture clashes and conflicts that could arise when the values embedded in AI models are significantly different from the values that users possess [35, 71]. Early work has acknowledged these complications of value plurality, for example, what does it mean to imbibe human values in Al when those values are diverse across cultures [86], and which values to use when producing a singular"}, {"title": "2.2 Al-Based Writing Support", "content": "Advances in computing have been extensively used to provide writing support, starting with tools like spell- check [68] and grammar-check [51] to improve writing efficiency [14]. As natural language generation technolo- gies evolved, researchers began applying them to support creative writing pursuits such as story writing [22]. Some examples include providing next-word or next-sentence suggestions [19, 57, 74], and generating creative content (e.g., poems) based on a given topic or writing style [26, 31]. However, early AI models struggled to capture the intent of the writer, rendering these tools as passive ideation tools rather than active writing as- sistants [29]. As a result, subsequent efforts focused on supporting specific writing sub-tasks, such as finding contextual synonyms [28] and generating metaphors for creative expression [16, 29].\nHowever, recent advancements in natural language generation, driven by LLMs, have led to a resurgence of tools to support open-ended writing [22], such as story generation [52, 83, 91, 92] and screenplay writing [60]. Beyond creative writing, HCI researchers have also proposed tools for argumentative writing, including journalism [69], scientific writing [30], email writing [45], and crafting self-introductions for networking [40]. These tools are designed to work in situ, as active collaborators, offering writing suggestions through pop-ups, side panels, or inline predictions [17]. While text remains the most common modality for writing support, other modalities have also been explored, such as sketches [18], images [83], and games [47].\nResearchers have also examined the broader implications of AI-powered writing suggestions, including their effect on users and the resulting text [14]. For instance, Arnold et al. [5] identified a trade-off between efficiency and ideation: while short single-word suggestions enhance efficiency without inspiring creativity, long multi-word suggestions offer inspiration but can be distracting. As a result, studies have shown mixed results regarding the efficiency benefits of writing suggestions [12, 22, 73], indicating that such gains may be context-dependent. Despite the potential to improve writing quality and productivity, writing suggestions often lead to reduced user satisfaction and a loss of ownership over the text produced [22, 44]. For example, Kadoma et al. [44] found that users experience a loss of agency when they rely heavily on AI suggestions.\nFurthermore, there has been growing interest in the risks of writing support tools, especially as they are powered by language models that reflect social biases. For instance, Jakesch et al. [42] showed that co-writing with opinionated language models shifts users' views to align with the model's biases. Kadoma et al. [44] highlighted the differential impact of writing assistants on users from minoritized genders. We extend this line of work by presenting the first cross-cultural study of writing suggestions, specifically designed to measure the cultural harms that may arise from these tools. The most closely related work in this space is Buschek et al. [14], who investigated the differential impact of multi-word suggestions on native and non-native English speakers. However, their study focused on a coarse-grained comparison of English language proficiency without controlling for cultural differences. In contrast, we present a systematic study that explicitly investigates cross-cultural differences in the context of AI-generated writing suggestions, measuring the impact of Al on users' writing when they work with Al systems that are culturally distant from them."}, {"title": "3 METHODOLOGY", "content": "We conducted a controlled experiment with 118 participants, comprising 60 Indian and 58 American users, recruited through the online crowdsourcing platform Prolific. Participants completed short writing tasks designed to elicit cultural values and artifacts. We had a standard 2 \u00d7 2 study design in which participants from India and the US completed the tasks with or without AI suggestions. Subsequently, we compared the essays written by participants from the four groups. Below, we describe our methodology in detail."}, {"title": "3.1 Experiment Design", "content": "To examine the impact of writing with a culturally incongruent model, we simulated a \u201ccultural distance\u201d between the users and the model. Since AI models are often aligned with Western cultures and values [10, 43, 61, 72], we conducted the study with participants from the US (representing a smaller cultural distance) and India (representing a larger cultural distance). Participants from both cultures were randomly assigned to complete writing tasks with or without AI suggestions, resulting in a standard 2 \u00d7 2 between-subjects study design. Hence, each participant was assigned to one of four experimental groups: Indians writing with or without AI, and Americans writing with or without AI\u00b9.\nThe two control groups (without AI) helped us capture the natural writing styles of participants from both cultures, forming a baseline for comparison with the treatment groups. The treatment groups (with AI) were designed to investigate whether writing styles changed when AI suggestions were introduced."}, {"title": "3.2 Writing Tasks", "content": "Each participant was required to complete four writing tasks. The task topics were designed to elicit various aspects of culture defined by Hofstede in his \u201cCultural Onion\" [38] (Figure 2). This framework uses the metaphor of a layered onion to model culture. Thus, it emphasizes the need for an outsider to work their way through layers of explicit cultural practices (symbols, heroes, rituals) before truly understanding the core of the culture, its implicit values.\nSymbols are words and objects that hold a certain meaning in a culture, including language, art, clothes, etc. Symbols are not fixed and evolve but are observable by outsiders to the culture. Heroes are people idolized by a culture, often because they possess characteristics that are valued within the culture. They may be real (e.g., APJ Abdul Kalam in India) or fictional (e.g., Rocky Balboa in the US). Rituals are collective activities deemed important by the people of the culture. This may include holidays and festivals, ways of greeting, or other social customs (e.g., sauna). These three layers are collectively referred to as practices and can be observed and practiced by outsiders even without necessarily understanding their underlying cultural meanings. Finally, at the core of the onion are the values, which are preferences for certain states of affairs (e.g., individualism or collectivism). They are static and so ingrained that even people within the culture may be unaware of them."}, {"title": "3.3 Study Procedure", "content": "We published the study on Prolific, a widely used crowdsourcing platform. The study consisted of two parts: completing the writing tasks on our study portal, and subsequently filling out a demographic survey. Participants had to complete both parts to complete the study and receive payment. Below, we describe these two parts of the study."}, {"title": "3.3.1 Study Portal.", "content": "The task instructions on Prolific directed participants to an external study portal, which we built using React and hosted at a public link. The landing page of the portal welcomed participants and provided a brief description of the study. It also displayed a link to the informed consent form and a check box for providing consent. Participants who provided consent were randomly assigned to the AI or No AI condition. Those in the AI condition were then directed to a tutorial, which was a pop-up-style walkthrough demonstrating how to accept or reject AI suggestions. After participants accepted and rejected a few suggestions, they proceeded to the main writing tasks.\nEach participant needed to complete four writing tasks (plus the attention check), presented sequentially. The interface for each task was the same and is shown in Figure 3. At the top, there were instructions. We enforced a minimum word requirement (50 words) to encourage participants to engage meaningfully with the task prompt [14, 42]. For the AI condition, the instructions included guidance on accepting or rejecting suggestions; we provided these instructions as a reminder despite the tutorial at the beginning.\nBelow the instructions was a textbox for their response. In the AI condition, the textbox fetched an inline autocomplete suggestion from GPT-40 if the user paused typing for 100 ms. While some related work used longer delays [14, 42], participants in those studies complained of long loading times [14]. Hence, we chose a shorter delay, especially as the model itself took ~500 ms to respond with a suggestion. To prevent participants from"}, {"title": "3.3.2 Retrieving Autocomplete Suggestions.", "content": "We used the following prompt to retrieve autocomplete suggestions from OpenAI's latest language model at the time, GPT-40. We refined this prompt through iterative prompt engineering and qualitatively evaluating the suggestions it produced.\nYou are an Al autocomplete assistant. You need to provide short autocomplete suggestions to help people with writing. Some guidelines:\nYour suggestion should make sense inline (it will be shown to the user as ghost text).\n- If the user has just completed a word, add a space before the suggestion.\nSuggestions should be <=10 words\nThe user is writing about the topic: \"<task prompt from Table 2>\"\nOutput a JSON of the following format: {{\"suggestion\": \"<your suggestion here>\"}}\nWe embedded the essay topic in the LLM prompt to provide contextually relevant suggestions to users [14, 42]. We also considered explicitly directing the model to generate Western-oriented suggestions. While this approach would enhance the robustness of our study by controlling the model's behavior, it would make it less realistic as users may not specify such culturally biased prompts in the real world. To explore this, we performed a preliminary evaluation. We added the instruction \u201cGive American suggestions\u201d to the above prompt and observed"}, {"title": "3.3.3 Demographic Survey.", "content": "After completing all the writing tasks, participants were directed to a demographic survey via a unique link. The demographic survey was a two-page survey designed on Qualtrics. We asked participants their age, gender, country of birth and residence, years lived in the country, level of education, occupation, and languages spoken. These questions helped us triangulate a participant's cultural background, e.g., we only kept data from participants who were born and had lived in the same country their entire lives. The demographic details also helped us ensure diversity within each culture on axes such as age and gender.\nWe selected India and the US as the target cultures for this study due to their different cultural compositions. However, since Prolific predominantly operates in the West, our Indian participants might be more familiar with American culture than the average Indian, due to their exposure to international crowdsourcing tasks. This could reduce the cultural distance between the AI model and our participants, potentially skewing the generalizability of our findings. To ensure that our participants from India and the US were culturally different from each other, we administered the Short Schwartz Value Survey (SSVS) [54] as part of our demographic survey. The SSVS consists of ten questions designed to measure individual and cultural differences across ten distinct values: power, achievement, hedonism, stimulation, self-direction, universalism, benevolence, tradition, conformity, and security. It is a shortened version of the original Schwartz Value Survey [76], a widely used survey comprising 57 questions to measure the same values.\nUpon completing the survey, the participants were redirected back to Prolific to mark the study as complete. At the backend, the research team tallied the logs to ensure that each participant had completed both parts of the study (the writing tasks and the demographic survey) and accordingly approved the payment ($2 per participant)."}, {"title": "3.4 Participants", "content": "In total, we collected responses from 118 participants: 60 from India and 58 from the US. The scale of our study was limited by the small number of Indian participants on Prolific. To get equal participation across the two cultures, we initially launched the study in India and allowed the participant count to stabilize before extending the study to the same number of participants in the US. The demographic details of the participants are summarized in Table 3.\nAs discussed previously, we also administered the Short Schwartz Value Survey (SSVS) to our participants to measure their cultural differences. Indian and American participants had significantly different scores (p < 0.05 on an independent samples t-test) on eight of the ten values measured by the survey (e.g., power, hedonism, tradition, conformity, etc.). The two values with no significant difference were benevolence and self-direction."}, {"title": "3.5 Analysis", "content": "We quantitatively analyzed the data collected from our online experiment by comparing task-level metrics across the four experimental groups: Indians and Americans, both with and without AI. We analyzed two sets of data: interaction logs captured on the study portal, and the final essays written by the participants. Below, we summarize the metrics used in our analyses."}, {"title": "3.5.1 Interaction Logs.", "content": "The portal recorded data such as the time taken to complete a task and the number of suggestions seen, accepted, and rejected. Based on this data, we defined the following metrics.\n(1) Suggestion Acceptance Rate: This measures the proportion of suggestions accepted in a task [14]. Thus, it quantifies engagement with the AI suggestions. It is computed as follows:\nAcceptance Rate = $\\frac{Number\\ of\\ suggestions\\ accepted}{Number\\ of\\ suggestions\\ shown}$\nIt ranges from 0 to 1, where 0 means that no suggestion was accepted in completing a task. Thus, it can be interpreted as the percentage of suggestions accepted in a task. However, this is a coarse measure as a user may modify or delete a suggestion after accepting it. The next two measures account for this possibility.\n(2) AI Reliance: This metric quantifies the proportion of the final essay generated by AI [14, 44]. For each character in the final essay, we determined whether it originated from an Al suggestion or was authored directly by a participant. Then, AI reliance for each task is calculated as follows:\nAl reliance = $\\frac{Number\\ of\\ AI-suggested\\ chars}{Total\\ number\\ of\\ chars\\ in\\ the\\ essay}$\nThis metric ranges from 0 to 1. Lower values indicate lesser reliance on AI (i.e., greater human contribution), while higher values indicate a stronger reliance on AI-generated content. Unlike the acceptance rate, this metric is sensitive to suggestions that were accepted but later modified or deleted.\n(3) Suggestion Modification: Users may accept a suggestion and modify it to better suit their requirements. For each task, we computed a boolean indicating whether a suggestion was modified during that task. We measure the existence of a modification, rather than the rate or frequency [14] because as little as one suggestion modification early in the essay can make subsequent suggestions culturally relevant. To capture modification, we checked if an accepted suggestion appeared in the final essay. If it was not found as-is, we assumed it was modified (or deleted).\n(4) Writing Productivity: This is the number of words written per unit time spent completing the task [22]. It can also be interpreted as the typing speed of the participant. It is computed as:\nProductivity = $\\frac{Number\\ of\\ words\\ in\\ the\\ essay}{Time\\ taken\\ to\\ finish\\ the\\ essay\\ (in\\ seconds)}$"}, {"title": "3.5.2 NLP Metrics.", "content": "We also used established techniques from the NLP literature to analyze the essays written by the participants.\n(1) Type-Token Ratio (TTR): TTR is a common measure of lexical diversity in text [7]. It is calculated as:\nTTR = $\\frac{Number\\ of\\ unique\\ words\\ (types)}{Total\\ number\\ of\\ words\\ (tokens)}$\nThe value is bounded between 0 and 1; a low TTR represents more repetition of words and therefore less linguistic variation, and a high TTR shows more linguistic variation. Note: Although TTR penalizes longer"}, {"title": "3.6 Ethics and Positionality", "content": "This study was approved by the IRB of our institution, which included a careful review of the risks of exposing study participants to generative AI applications. Since culture is closely tied to one's identity, we were careful in designing a culturally appropriate study. For example, the third essay prompt asked the participants to write about their favorite holidays, but the equivalent term in India was \u201cfestivals\u201d, so we framed the prompt to include both terms.\nOur team has multiple members who have lived in both India and the US. This exposure to both cultures provided us with valuable context for designing this study and interpreting its results. Finally, we approached this work from an action research mindset, aiming to study the harms of working with culturally incongruent AI models. Building on insights generated from this work, in the future we aim to take action toward reducing these harms."}, {"title": "4 FINDINGS", "content": "Overall, 118 participants completed four tasks each, resulting in a dataset of 472 essays. Among these participants, 65 received AI-generated suggestions to aid their writing-36 in India and 29 in the US. These participants saw a total of 12,015 suggestions, accepting 1,476 of them (12.3%). Of the rejected suggestions, only 0.6% were explicitly rejected by pressing the escape key; the rest were dismissed by continuing to type on. Further, a majority of the rejected suggestions (68.8%) were dismissed within 500 ms of appearing on the screen, indicating that they were dismissed in the flow of writing, perhaps without being fully considered by the user. These phases represent bursts of focused manual typing where AI suggestions may not be desired [14]. The most common suggestions provided by the model are shown in Table 4; there is a noticeable Western bias in the suggestions for the food and festival tasks.\nParticipants were more likely to accept suggestions as the task progressed: of the suggestions they saw in the first third of the task, they accepted 9.2%, in the second third they accepted 12.1%, and in the final third of the task they accepted 15.9%. This may be because suggestions become more useful with more context. In the rest of this section, we present a cross-cultural analysis of the data."}, {"title": "4.1 Does Al impact Indians and Americans differently?", "content": "We measure engagement using two metrics defined previously: AI reliance and suggestion acceptance rate. In short, AI reliance measures the proportion of an essay written using AI suggestions, while the acceptance rate measures the proportion of suggestions the user accepts in a task. Section 3.5 provides detailed descriptions of these metrics.\nFigure 4(a) illustrates the AI reliance scores for essays written by Indian and American participants. We observe that Indians exhibited a higher reliance on AI than Americans. The mean AI reliance score for Indians was 0.53 (SD = 0.2), suggesting that approximately half of their essays were AI-written, while the average for Americans was 0.42 (SD = 0.25). The Mann-Whitney U test revealed that this difference was statistically significant (U = 10614.0, p < 0.001) with a small effect size (Cliff's delta D = 0.27).\nFigure 4(b) shows a similar plot for the suggestion acceptance rate. Again, on average, Indians accepted a higher proportion of the suggestions shown to them in a task. On average, Indian participants accepted 25% (SD = 19) of the suggestions in a task, while American participants accepted 19% (SD = 19). The Mann-Whitney U test confirmed that this difference was statistically significant (U = 10748.0, p < 0.001) with a small effect size (Cliff's D = 0.29).\nOverall, these results indicate that Indians showed a higher engagement with AI suggestions than Americans on both metrics. This disparity might stem from differences in how AI is perceived or used in different cultural contexts. For example, non-native English speakers might find in-situ AI suggestions more appealing than native speakers, as shown by Buschek et al. [14]."}, {"title": "4.1.2 Suggestion Modification Behavior.", "content": "Users may accept a suggestion and then modify it to better suit their context. Since the suggestions were Western-biased (see Table 4), modification may be required to make them culturally relevant for Indians. To analyze the modification behavior, we compared the percentage of tasks in which Indians and Americans modified at least one suggestion.\nFigure 4(c) shows the distribution of the proportion of tasks where Indians and Americans modified at least one suggestion. To produce each distribution, we conducted a bootstrap sampling procedure over 100 iterations. In each iteration, we sampled 80% of the tasks (with replacement) and calculated the percentage of tasks in which Indians and Americans modified at least one suggestion. This produced a distribution of percentages (one for each of the 100 bootstrap iterations) for Indians and Americans, which we visualized using a density plot.\nWe observe that the distribution for Indian participants (blue curve) is shifted to the right. On average, Indian participants modified suggestions in 63.5% of the tasks (SD = 4.2) whereas American participants modified"}, {"title": "4.1.3 Value Derived from Al Suggestions.", "content": "Next, we compare the value derived from AI suggestions by Indian and American participants, focusing on the task completion time and the productivity measure defined in Section 3.5. Figure 5(a) illustrates the task completion duration for both Indian and American participants. Both cohorts showed a reduction in completion times when AI suggestions were used. For Indians, the mean task completion time decreased from 278.4 seconds without AI (SD = 134.5) to 179.6 seconds with AI (SD = 105.8), representing a 35% decrease (~1.5 minutes faster). This decrease was statistically significant (U = 10356.0, p < 0.001) with a large effect size (Cliff's D = 0.5). Similarly, Americans experienced a decrease, from 232.0 seconds without AI (SD=240.1) to 161.1 seconds with AI (SD = 117.7), a 30% decrease (~1.2 minutes faster). This reduction was also significant (U = 8936.0, p < 0.001) with a medium effect size (Cliff's D = 0.33). While descriptively it seemed that Indians gained slightly more from AI (35% decrease vs 30% for Americans), a Difference-in-Difference analysis using an Ordinary Least Squares (OLS) regression model showed that the reduction in duration was similar in both cohorts (p = 0.34, 95% CI=[-85.25, 29.58]).\nNevertheless, the descriptive difference in reduction prompted us to further investigate potential productivity gains. Figure 5(b) presents the productivity of participants (words written per second) in all four groups. Both Indian and American participants experienced a significant productivity boost when using AI sugges- tions (p < 0.001 for both cohorts, measured by the Mann-Whitney U test). However, the effect size was larger for Indians (Cliff's D = -0.48 compared to D = -0.33 for Americans), suggesting a slightly higher productivity gain for Indians. However, as shown previously, this gain came with a higher suggestion acceptance rate. This raised the question: Was the difference in productivity substantial enough to justify the higher acceptance rate?\nTo answer this, we calculated the value derived per suggestion by normalizing the productivity by the number of suggestions accepted5. However, this metric may understate the value derived by Indians, as they accepted more suggestions, and suggestions tend to offer diminishing returns (each additional suggestion provides less"}, {"title": "4.2 Does Al homogenize writing within cultures?", "content": "In the previous section, we outlined how AI impacts Indian and American users differently. In this section, we show that AI homogenizes writing styles within the same culture. This will set the stage for our main investigation of cross-cultural homogenization in the next section."}, {"title": "4.2.1 Similarity Analysis.", "content": "We conducted a similarity analysis to investigate whether AI standardizes writing styles for participants within their respective cultures. The similarity metric we used was cosine similarity, computed on the essay embeddings obtained from OpenAI's text-embedding model.\nSetup: For this experiment, we computed similarity scores within each of our four experimental groups and compared the four distributions. For example, there were n = 24 Indian participants in the No AI condi- tion, and each participant wrote four essays. So, we computed pair-wise similarity scores between all essays written by these 24 participants, grouped by the essay topic. This yielded a distribution of 4 essay topics \u00d7 (24) comparisons per topic = 1104 similarity scores. Similarly, we computed similarity distributions for the other three experimental groups (Indians with AI, Americans without AI, and Americans with AI). The means and 95% confidence intervals of these distributions are shown in Figure 6(a). The blue line shows how the within-culture similarity among Indian participants changes from the No AI condition to the AI condition, while the orange line represents this change for American participants.\nResults: First, we notice that without AI (left side of the graph), Indian participants exhibited slightly higher within-culture similarity than American participants, suggesting that Indians naturally had more homogeneous writing styles than Americans. This natural difference between the Indian and American cohorts was significant, t(2726) = 4.04, p < 0.001, though the effect size was negligible (Cohen's d = 0.16).\nNext, when AI suggestions were introduced, the similarity increased within both Indian and American cohorts. A t-test revealed that this increase was significant within both cohorts; t(3622) = \u22127.22, p < 0.001 for Indians (blue line) with a small effect size (Cohen's d = \u22120.26), and t(3246) = -9.45, p < 0.001 for Americans (orange line) with a small effect size (Cohen's d = -0.33). This means that AI caused both Indians and Americans to write more similarly within their cohorts. Visually, the magnitude of the increase looks slightly higher for Americans (the orange line is steeper than the blue line). However, a Difference-in-Difference analysis using a regression model showed that the increase was statistically similar in both cohorts (p = 0.30, 95% CI=[\u22120.018, 0.006]). This means that the magnitude of within-culture homogenization was similar in both cohorts.\nIn sum, the increasing similarity in both cohorts suggests that AI made writing more homogeneous, potentially by guiding users towards common phrasing, structure, or content. Thus, AI suggestions had a net homogenizing effect, suggesting that some level of cross-cultural homogenization was to be expected. However, as we demonstrate next, the extent of this homogenization was even more pronounced across cultures."}, {"title": "4.3 Does Al homogenize writing across cultures?", "content": "We also investigated if AI might homogenize writing styles across cultures, i.e., do Indians start writing like Americans (or vice versa) when they use AI suggestions? We now show evidence for this effect."}, {"title": "4.3.1 Similarity Analysis.", "content": "In this experiment", "embeddings.\nSetup": "First", "used.\nResults": "Figure 6(b) compares the means and 95% confidence intervals of the two distributions. It shows that without AI, the mean similarity between Indians and Americans was 0.48 (SD = 0.11), but with AI, the similarity score rose to 0.54 (SD = 0.13). A t-test revealed that this increase was statistically significant, t(6958) = -17."}]}