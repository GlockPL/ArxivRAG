{"title": "ABC3: Active Bayesian Causal Inference with Cohn Criteria in Randomized Experiments", "authors": ["Taehun Cha", "Donghun Lee"], "abstract": "In causal inference, randomized experiment is a de facto method to overcome various theoretical issues in observational study. However, the experimental design requires expensive costs, so an efficient experimental design is necessary. We propose ABC3, a Bayesian active learning policy for causal inference. We show a policy minimizing an estimation error on conditional average treatment effect is equivalent to minimizing an integrated posterior variance, similar to Cohn criteria (Cohn, Ghahramani, and Jordan 1994). We theoretically prove ABC3 also minimizes an imbalance between the treatment and control groups and the type 1 error probability. Imbalance-minimizing characteristic is especially notable as several works have emphasized the importance of achieving balance. Through extensive experiments on real-world data sets, ABC3 achieves the highest efficiency, while empirically showing the theoretical results hold.", "sections": [{"title": "1 Introduction", "content": "The major goal of causal inference is to estimate the treatment effect which is a relative effect on the treatment group compared to the control group. In randomized experiments, practitioners allocate treatment to subjects to estimate the treatment effect. Randomized experiments free practitioners from various theoretical problems prevailing in an observational study, e.g. unmeasured confounders. However, a randomized experiment is usually more expensive than an observational study, as a result, an efficient experiment design is desirable.\nTo achieve efficiency in randomized experiments, Efron (1971) first introduced a biased-coin design, and several works tried to minimize the estimation bias by achieving a balance between treatment and control groups (Atkinson 2014). Antognini and Zagoraiou (2011) extended this to covariate-adaptive design to achieve a balance, not only between treatment-control groups but also within sampling strata. Several recent works target the same goal using an adaptive Neyman allocation (Dai, Gradu, and Harshaw\n2023) or Pigeonhole design (Zhao and Zhou 2024). However, these lines of work assume the experimental subjects are given, not actively choosable.\nActive learning is a framework where a practitioner can choose unlabeled data points and ask an oracle to label them (Settles 2009). We can further rationalize the experimental design by adopting an active learning framework. For example, internet companies can choose a member whom they implement the A/B test, by utilizing the personal information they have gathered. Pharmaceuticals can choose a subject based on their personal information after the applicants are gathered to save a budget.\nTo develop a sound active learning method for randomized experiments, (1) it should not violate the standard assumptions in causal inference. Also, (2) the method should achieve a balance between observation and control groups to make a sound conclusion. We introduce ABC3, a novel active learning policy for randomized experiments to remedy these issues. Using the Gaussian process, our policy targets minimizing the error of individual treatment effect estimation. Our contributions are three folds:\n\u2022 We theoretically show minimizing the estimation error on individual treatment effect is equivalent to minimizing the integrated predictive variance in a Bayesian sense.\n\u2022 ABC3, the policy minimizing the variance, theoretically minimizes the imbalance between treatment and control groups and the type 1 error rate.\n\u2022 With extensive experiments, we empirically verify ABC3 outperforms other methods while showing theoretical properties hold.\nAfter examining related works in Section 2, we formalize our problem in Section 3. Then we introduce ABC3 and its theoretical properties in Section 4. We empirically verify the performance and properties of ABC3 in Section 5. Then we conclude our paper with several discussions and limitations in Section 6 and 7."}, {"title": "2 Related Works", "content": "There are several works exploring an active learning policy for observational data (Sundin et al. (2019), Jesson et al. (2021), and Toth et al. (2022)). Especially, Sundin et al. (2019) proposed an active learning policy for decision mak-ing, when treatment-control groups are imbalanced. They"}, {"title": "3 Problem Formulation", "content": "Let $X \\in \\mathcal{X}, Y \\in \\mathcal{Y}$ and $A \\in \\{0,1\\}$ be random variables. $X$ is a covariate representing each subject, $Y$ is an outcome, and $A$ represents a binary treatment. Following the Neyman-Rubin causal model (Rubin 1974), additionally define $Y^0$ and $Y^1$, potential outcomes for either control or treatment. Unlike the usual supervised learning settings, a practitioner can observe only one of $Y^0$ and $Y^1$ in a causal inference setting. $x, y, a, y^0$ and $y^1$ denotes the realizations of each random variable.\nLet $\\mathcal{D}_\\Omega = \\{(x_i, y_i^0, y_i^1)\\}_{i=1}^N$ be a subject pool with covariate information $x_i$ and potential outcomes $y_i^0, y_i^1 \\in \\mathbb{R}$. Let $\\mathcal{D}^a_t = \\{(x_i, y_i)\\}_{i \\in \\mathcal{I}^a_t}$ be an observed control group data set at time $t$ with an index set $\\mathcal{I}^a_t$. Likewise, define $\\mathcal{D}^1_t = \\{(x_i, y_i)\\}_{i \\in \\mathcal{I}^1_t}$ for a treatment group. Let $\\mathcal{X}_\\Omega, \\mathcal{X}^0_t$ and $\\mathcal{X}^1_t$ be sets of $x$s in each data set, $\\mathcal{D}_\\Omega, \\mathcal{D}^0_t$ and $\\mathcal{D}^1_t$.\nOur quantity of interest is the conditional average treatment effect (CATE), $\\text{CATE}(x) = \\mathbb{E} [Y^1 - Y^0 | X = x]$ for each subject $x$. Then we can train an estimator $\\widehat{\\text{CATE}}_t(x) = \\hat{y}^1_t(x) - \\hat{y}^0_t(x)$, where $\\hat{y}^a_t$ are regressors trained on each data set $\\mathcal{D}^a_t, a \\in \\{0,1\\}$.\nTo evaluate the trained estimator, we use a standard metric, expected precision in estimation of heterogeneous effect (PEHE, Hill (2011)),\n$\\begin{equation}\\text{CPEHE}(\\widehat{\\text{CATE}}_t) = \\mathbb{E}_{X} [(\\widehat{\\text{CATE}}_t(x) - \\text{CATE}(x))^2] = \\int_{\\mathcal{X}} (\\widehat{\\text{CATE}}_t(x) - \\text{CATE}(x))^2 d\\mathbb{P}(x),\\end{equation}$\nwhere $\\mathbb{P}$ is a probability distribution over whole covariates. In the usual case, we can treat $\\mathbb{P}$ as a discrete distribution"}, {"title": "4 Proposed Method", "content": "Gaussian process (GP, Rasmussen and Williams (2006)) is a non-parametric machine learning model based on Bayesian statistics. It is a powerful tool as it allows flexible function estimation depending on a pre-defined kernel function. Set priors on $f \\sim \\text{GP}(m(x), k(x,x'))$, where $m(x)$ is a mean prior and $k$ is a kernel function. Assume a data set $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$ with noisy observation $y_i = Y(x_i) + \\epsilon_i, \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_\\epsilon)$. We can obtain a posterior distribution of $f(x^*)$ given data set $\\mathcal{D}$ by computing $f(x^*)|\\mathcal{D}$ as\n$\\begin{equation}f(x^*)|\\mathcal{D} \\sim \\mathcal{N}(m'(x^*), \\sigma^2(x^*)) \\text{ where }\\end{equation}$\n$\\begin{aligned}m'(x^*) &= m(x^*) + \\mathbf{k}_*(x^*)^T [\\mathbf{K} + \\sigma^2_{\\epsilon}\\mathbf{I}]^{-1} \\mathbf{y}, \\\\ \\text{cov}(x, x^*) &= k(x, x^*) - \\mathbf{k}_*(x)^T [\\mathbf{K} + \\sigma^2_{\\epsilon}\\mathbf{I}]^{-1} \\mathbf{k}_*(x^*), \\\\ \\sigma^2(x^*) &= \\text{cov}(x^*, x^*), \\\\ \\mathbf{k}_*(x^*) &= [k(x_i, x^*)]_{i=1}^N, \\mathbf{K} = [k(x_i, x_j)]_{i,j=1}^N, \\\\ \\mathbf{y} &= [y_i]_{i=1}^N .\\end{aligned}$\nWe can observe that the posterior variance $\\sigma^2(x^*)$ does not depend on $\\mathbf{y}$. Adopting the notations from above, we define $\\mathbf{k}^a_*, \\mathbf{K}^a, \\mathbf{y}^a$ and $\\text{cov}^a$ with observed data set $\\mathcal{D}^a_t$ at time $t$. We also assume zero-mean prior, i.e. $m(x) = 0, \\forall x$.\nThe following theorem identifies our active learning policy with only posterior variance terms.\n$\\begin{equation}\\text{arg min}_{x_{t+1}, a_{t+1}} \\mathbb{E}_{t+1} [\\text{EPEHE}(\\widehat{\\text{CATE}}_{t+1})] = \\text{arg min}_{x_{t+1}, a_{t+1}} \\int_{\\mathcal{X}} \\mathbb{V}_{t+1} [Y^1 (x)] + \\mathbb{V}_{t+1} [Y^0(x)] d\\mathbb{P}(x)\\end{equation}$\nComputing the inverse of all hypothetical covariance ma-trix $\\mathbf{K}^a_{t+1}$ for all $x$ is computationally infeasible. However, we can efficiently find the minimizer as $\\mathbf{K}^a_t$ is a principal submatrix of $\\mathbf{K}^a_{t+1}$."}, {"title": "Theoretical Analysis", "content": "Balancing Treatment-Control Groups Unlike usual supervised learning, causal inference requires precise estimation of both functions for treatment and control groups. As a result, the balance between the two groups is crucial to obtain a sound estimation. For example, consider a study on the causal effect of online lectures. Assume our treatment group is concentrated on undergraduate students while our control group is concentrated on graduate students. Then it would be difficult to make a sound conclusion with statistical tools, as the two groups are highly imbalanced.\nSeveral researchers theoretically analyzed the effect of the imbalance on causal inference. Shalit, Johansson, and Sontag (2017) showed that the generalization error on CATE is upper bounded by the imbalance. Sundin et al. (2019) defined Type S Error, assigning a different sign (+ or -) to CATE, and showed the probability of Type S Error is bounded by the imbalance. Both works utilized Maximum Mean Discrepancy (MMD, Gretton et al. (2012)) to quantify and measure the imbalance.\n$\\begin{equation}\\text{MMD}(\\mathbb{P}, \\mathbb{Q}, \\mathcal{F}) = \\underset{f \\in \\mathcal{F}}{\\text{sup}} \\mathbb{E}_{x \\sim \\mathbb{P}(x)} [f(x)] - \\mathbb{E}_{y \\sim \\mathbb{Q}(y)} [f(y)]\\end{equation}$"}, {"title": "Minimizing Error", "content": "We measure the $\\widehat{\\text{EPEHE}}$ (not $\\text{EPEHE}$) when observing every 10% of population. We run 100 experiments for every data set. We mark the mean of measured $\\widehat{\\text{EPEHE}}$.\nABC3 shows the best performance, i.e. the lowest $\\widehat{\\text{EPEHE}}$ for most time steps. We can verify ABC3 succesfully minimizes the population $\\text{EPEHE}$, though optimization target of ABC3 is $\\widehat{\\text{EPEHE}}$. In most cases, when ABC3 observes only half of the population, it achieves $\\widehat{\\text{EPEHE}}$ level which is achieved with full observation by other policies. Especially for Boston, after 20%, ABC3 achieves $\\widehat{\\text{EPEHE}}$ which Naive policy cannot achieve even with full observation.\nACE policy shows comparable results to ABC3. ACE slightly outperforms ABC3 for a 20-40% interval of Lalonde data set. However, ABC3 outperforms ACE in most cases, though ABC3 has no access to the test data set, unlike ACE.\nLeverage temporarily outperforms ABC3 for the beginning part of IHDP and the last part of Lalonde. However, for ACIC, Leverage significantly underperforms other policies. The result may imply the vulnerability of linearity assumption in real-world data sets.\nMackay underperforms even Naive policy most times. It is interesting as Mackay utilizes the same uncertainty information as ABC3. The result may imply the importance of proper utilization of the same information.\nIn summary, ABC3 is a promising Bayesian active learning policy, which efficiently and robustly achieves the best performance among the others."}, {"title": "Balancing Treatment-Control Groups", "content": "Theorem 4.5 states that ABC3 theoretically minimizes the maximum mean discrepancy between treatment and control groups. A policy can benefit by minimizing MMD from several theoretical aspects, as introduced in Section 4.2. We empirically verify the property. We measure MMD between $\\mathbb{P}^0_t$ and $\\mathbb{P}^1_t$ for every 10% observation and applied the same setting as the previous section."}, {"title": "Minimizing Type 1 Error", "content": "Theorem 4.7. states ABC3 minimizes the upper bound of the integrated type 1 error probability, $\\int_{\\mathcal{X}} \\mathbb{P}_{t+1} [\\text{Type 1 Error}(x)] d\\mathbb{P}(x)$. Here we verify the property empirically.\nWe use Boston and Lalonde data sets which assume no treatment effect, i.e. $Y^0 = Y^1$. To measure the type 1 error rate, we compute a mean and standard deviation of $\\widehat{CATE}(x)$ for every $x$ in the test data set. Then we implement the Z test with a significance level of 5%. (i.e. $\\alpha = 1.96$). Type 1 error occurs when the absolute value of the Z-statistic is bigger than $\\alpha$. We compute the percentage of $x$ where the type 1 error occurs."}, {"title": "Hyperparameter Sensitivity", "content": "To implement ABC3, we need to determine uncertainty-quantifying kernel, kernel parameters, and observation error $\\sigma^2_\\epsilon$ as hyperparameters. As usual machine learning models utilizing the kernel method, selecting an appropriate kernel and parameters is crucial for obtaining precise estimation. We present hyperparameter sensitivity analysis for ABC3.\nFor the kernel, we test RBF kernel (utilized throughout this paper), Matern kernel, and Exp-Sine-Squared kernel (Sine). RBF kernel has one parameter, lengthscale, which determines how 'local' the output function would be. The smaller the lengthscale, the more local and wiggler the resulting function is. Matern kernel is a generalization of RBF kernel and has two parameters, lengthscale, and smoothness. Sine kernel assumes that our data shows a periodic pattern. it has two parameters, lengthscale, and periodicity. Here we test only lengthscale, with setting periodicity as 1."}, {"title": "Measuring Computation Time", "content": "Here we present the time to sample the whole train data set of Boston data. We compute the mean and standard deviation of the computation time by iterating 10 times. As a result, Leverage shows nearly the same computation time as Naive, while ABC3 shows a time comparable to Mackay. ACE requires nearly twice as much time than ABC3. However, most policies require less than 1 second to sample the whole data set. The result shows that ABC3 is a feasible policy with reasonable computation time."}, {"title": "Empirical Validation of Assumption", "content": "In Theorem 4.5, we introduced the following assumption: $\\epsilon^*(I_n) \\le 2\\delta^*(I_n), \\forall I_n$ where $M = \\sum_{i,j=1}^N k(x_i, x_j)$, $\\delta^*(I_n) = \\sum_{i \\in I_n} \\int_{\\mathcal{X}} k(x_i, x) d\\mathbb{P}(x)$ and $\\epsilon^*(I_n) = M - \\sum_{i,j \\in I_n} k(X_i, x_j)$. To verify the empirical satisfaction of the assumption, we compute and plot $\\epsilon^*(I_n)$ and $2\\delta^*(I_n)$ for data sets used in our paper. As computing all $\\epsilon^*(I_n)$ and $\\delta^*(I_n)$ for all $I_n$ is computationally infeasible, we compute the value of the leading principal submatrix by randomly permuting 100 times and present points with a minimum value of $2\\delta^*(I_n) - \\epsilon^*(I_n)$. The result, shown in Figure 6, supports the empirical satisfaction of the assumption."}, {"title": "6 Discussion & Limitation", "content": "ABC3 algorithm utilizes Gaussian process at its heart, hence the improvements pertaining to Gaussian process also happen in ABC3. For example, as multiple researchers attempted to extend the Gaussian Process to large data sets,"}, {"title": "7 Conclusion", "content": "We present ABC3, an active learning based sampling policy for causal inference. ABC3 minimizes the expected error of CATE estimation from a Bayesian perspective, without violating the key assumptions in causal inference. Using maximum mean discrepancy, we prove that ABC3 minimizes the upper bound of imbalance between observed treatment and control groups. Moreover, ABC3 theoretically minimizes the upper bound of type 1 error probability. ABC3 empirically outperforms other active learning policies, and its theoretical properties as well as empirical robustness are also validated to give additional support to the general applicability of ABC3. We expect ABC3 and its extensions will deepen theoretical insights and general applicability of active learning on casual inference tasks."}]}