{"title": "Packing Analysis: Packing Is More Appropriate for Large Models or Datasets in Supervised Fine-tuning", "authors": ["Shuhe Wang", "Guoyin Wang", "Jiwei Li", "Eduard Hovy", "Chen Guo"], "abstract": "Packing, initially utilized in the pre-training phase, is an optimization technique designed to maximize hardware resource efficiency by combining different training sequences to fit the model's maximum input length. Although it has demonstrated effectiveness during pre-training, there remains a lack of comprehensive analysis for the supervised fine-tuning (SFT) stage on the following points: (1) whether packing can effectively enhance training efficiency while maintaining performance, (2) the suitable size of the model and dataset for fine-tuning with the packing method, and (3) whether packing unrelated or related training samples might cause the model to either excessively disregard or over-rely on the context.\nIn this paper, we perform extensive comparisons between SFT methods using padding and packing, covering SFT datasets ranging from 69K to 1.2M and models from 8B to 70B. This provides the first comprehensive analysis of the advantages and limitations of packing versus padding, as well as practical considerations for implementing packing in various training scenarios. Our analysis covers various benchmarks, including knowledge, reasoning, and coding, as well as GPT-based evaluations, time efficiency, and other fine-tuning parameters. We also open-source our code for fine-tuning and evaluation and provide checkpoints fine-tuned on datasets of different sizes, aiming to advance future research on packing methods.", "sections": [{"title": "Introduction", "content": "Supervised fine-tuning (SFT) refers to adapting a pre-trained model to perform specific tasks by training it on a labeled conversation dataset consisting of (instruction, answer) pairs (Ouyang et al., 2022; Muennighoff et al., 2022; Wang et al., 2022a;\nPacking addresses these challenges by combining multiple training samples into a single sample. Originally used during the pre-training phase, packing extends each training sequence to the model's maximum input length, optimizing hardware resource usage, such as GPUs, and improving training efficiency (Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022; OpenAI, 2023; Touvron et al., 2023; Dubey et al., 2024). Despite its proven effectiveness during the pre-training phase, for SFT, a thorough analysis is still lacking on: (1) whether packing can effectively enhance training efficiency while maintaining performance, (2) the suitable size of the model and dataset for fine-tuning with the packing method, and (3) whether packing unrelated or related training samples might cause the model to either excessively disregard or over-rely on the context.\nTo address these concerns, this paper provides a thorough analysis of packing during the supervised fine-tuning (SFT) stage. Specifically, we perform extensive comparisons between supervised fine-tuning (SFT) methods using padding and packing, covering SFT datasets ranging from 69K to 1.2M and models from 8B to 70B. Our comparisons include various benchmarks, such as knowledge, reasoning, and coding, GPT-based evaluations, time efficiency, and other fine-tuning parameters, concluding that:\n\u2022 Models using packing generally perform better on average compared to those using"}, {"title": "Related Work", "content": "Supervised fine-tuning (SFT) in large language models (LLMs) involves additional training on a dataset of (instruction, answer) pairs. This approach helps align the LLMs' training goal of predicting the next word with users' expectations for the models to follow human instructions more accurately (Mishra et al., 2021; Wei et al., 2021; Rosenbaum et al., 2022; Ouyang et al., 2022; Wang et al., 2022b; Dwivedi-Yu et al., 2022; Longpre et al., 2023; Zhang et al., 2023b; Qi et al., 2023; Chung et al., 2024; Liu et al., 2024).\nThe initial step in SFT is to create annotated data, but current SFT datasets are often constrained by their limited quantity, diversity, and creativity (Mukherjee et al., 2023; Xu et al., 2023; Lu et al., 2023; Song et al., 2024; Wang et al., 2023b; Zhou et al., 2024). To address this issue and rich resources for research, in one line, researchers distilled data from powerful large models (e.g., GPT-4 (OpenAI, 2023)) (Chiang et al., 2023; Ding et al., 2023; Zhao et al., 2024). On the other line, some researchers are working on methods to enable pre-trained models to self-generate useful SFT data (Wang et al., 2022a; Xu et al., 2023; Sun et al., 2024).\nOnce high-quality SFT datasets are created, the next step is to use them for fine-tuning pre-trained models. Many studies are dedicated to minimizing the costs of fine-tuning, including GPU usage and time, while maintaining performance, such as lightweight fine-tuning (Hu et al., 2021; Zaken et al., 2021; Zhang et al., 2023a; Dettmers et al., 2024), speeding up attention algorithm for transformer-based LLMs (Dao et al., 2022), and efficient distributed fine-tuning (Rajbhandari et al., 2020). In this paper, we are analyzing one of the efficient fine-tuning techniques: packing, which packs multiple training samples into a single sample to maximize the utilization of hardware resources and enhance fine-tuning efficiency, providing a comprehensive understanding of its effectiveness and potential risks.\nIn this paper, we explore a new fine-tuning technique called packing, which gained prominence during the pre-training phase. This method involves combining multiple training samples into one to optimize hardware resource usage and improve fine-tuning efficiency. Below, we will thoroughly analyze its effectiveness and potential risks."}, {"title": "Methods: Padding and Packing", "content": "Padding and packing are two distinct methods to organize training samples, shown in Figure 1. In this section, we first define a set of mathematical symbols, followed by a detailed explanation of one padding method and two packing methods: random packing and greedy packing."}, {"title": "Symbols", "content": "We assume that $\\{C\\} = \\{c^1,...,c^N\\}$ denotes the training conversations, where N is the size of the training conversation, and $c^i = \\{(x_1^i, y_1^i), ..., (x_{m_i}^i, y_{m_i}^i)\\}, m_i \\geq 1$ denotes an instruction x and answer y pair of length $m_i$. The SFT process tunes a pre-trained LLM f to create a more human-like model g that adheres to user"}, {"title": "Padding", "content": "Padding refers to extending the length of training conversations to a consistent size to match the input requirements of the pre-trained LLM, following are its detailed process, strengths, and weaknesses:"}, {"title": "Process of Padding", "content": "Specifically, for a training sample comprising m-turn (m > 1) conversations $\\{(x_1,y_1),..., (x_m, y_m)\\}$, we start by concatenating each turn of the conversation into a single sequence, using the special token [EOS] to separate each instruction x and response y:\n$x_1y_1[EOS]x_2y_2[EOS]\\cdots x_my_m[EOS] = w_1w_2\\cdots[EOS]w_tw_{t+1}\\cdots\\omega_\\tau[EOS]$\nwhere $w_i, 1 < i < T$ denotes the word of the concatenated sequence, and T denotes the number of words in this sequence.\nDuring the training stage, LLMs often process data in batches, and each batch must consist of input sequences of equal length. This uniformity is essential because the fundamental mathematical operations required, such as matrix multiplications, necessitate tensors of consistent sizes. In this condition, for one batch with l concatenated sequences:\n$w_1^1 w_2^1...[EOS] w_{t_1}^1 w_{t_1+1}^1...\\omega_\\tau^1[EOS]$\n$w_1^2 w_2^2...[EOS] w_{t_2}^2 w_{t_2+1}^2...\\omega_\\tau^2[EOS]$\n$\\cdots$\n$w_1^l w_2^l...[EOS] w_{t_l}^l w_{t_l+1}^l...\\omega_\\tau^l[EOS]$\nwe must ensure that all sequences in this batch are limited to the minimum of the maximum input length of the pre-trained model (for example, 8192 for LLaMA-3-8B-base (Dubey et al., 2024)) and the length of the longest sequence within the batch:\n$T = min(Maximum Input Length, arg max T_i)$\n$1<i<l$"}, {"title": "Strengths or Padding", "content": "Padding mainly has two strengths, in one line, padding is straightforward to implement, allowing for its use across various training frameworks. In the other line, by truncating longer sequences and padding shorter ones, padding ensures that all training data conforms to the model's architecture, preventing errors associated with varying sequence lengths and speeding up underlying matrix computations."}, {"title": "Weaknesses of Padding", "content": "While necessary, excessive padding can lead to inefficiencies. If there are many padding tokens relative to actual data, it can lead to increased computation without corresponding benefits in learning. In order to alleviate this weakness, below, we introduce two packing-based methods: Random Packing and Greedy Packing."}, {"title": "Random Packing", "content": "Unlike the padding method, which extends shorter sequences with the special token [PAD], random packing combines multiple training conversations into a single sequence randomly, to maximize the model's learning efficiency and effectiveness:"}, {"title": "Process of Random Packing", "content": "Firstly, we concatenate all training conversations $\\{C\\} = \\{c^1,..., c^N \\}$ into one single sequence with the special token [EOS] to separate each instruction x and response y:\n$C_1[EOS]C_2[EOS] \\cdots [EOS]C_N[EOS] = x_1y_1[EOS]x_2y_2\\cdots [EOS]x_ny_n[EOS] = w_1w_2 \\cdots [EOS]w_tw_{t+1}\\cdots\\cdots w_m[EOS]$\nwhere n represents the total number of training instructions x and responses y, $w_i, i \\leq i \\leq m$ denotes each word in the concatenated sequence, and m signifies the total number of words in that sequence.\nSecondly, assuming that the maximum input length for the pre-trained model is T, we adjust to this limit by dividing the concatenated sequence into M subsequences, each with a length of T:\n$w_1^1w_2^1...[EOS]w_{t_1}^1w_{t_1+1}^1\\cdots w_{+T}^1[EOS]$\n$w_1^2w_2^2...[EOS]w_{t_2}^2w_{t_2+1}^2\\cdots w_{T+T}^2[EOS]$\n$\\cdots$\n$w_1^Mw_2^M...[EOS]w_{t_M}^Mw_{t_M+1}^M\\cdots w_{T+T}^M[EOS]$\nwhere T' < T, the special [PAD] token will be appended when T' <T.\nIn the end, we randomly pack these sequences into batches with each size of l, forming like:\n$w_1^1w_2^1...[EOS]w_{t_1}^1w_{t_1+1}^1\\cdots w_{+T}^1[EOS]$\n$w_1^2w_2^2...[EOS]w_{t_2}^2w_{t_2+1}^2\\cdots w_{T+T}^2[EOS]$\n$w_1^Mw_2^M...[EOS]w_{t_M}^Mw_{t_M+1}^M\\cdots w_{T+T}^M[EOS]$"}, {"title": "Strengths of Random Packing", "content": "Compared to the padding method, random packing enhances computational efficiency by densely packing each training batch, minimizing unused space and optimizing the use of the model's capacity. Furthermore, this approach potentially boosts the model's ability to generalize by exposing it to a broader range of contextual combinations in each training sample, thereby providing a more diverse array of scenarios."}, {"title": "Weaknesses of Random Packing", "content": "There are two potential weaknesses of random packing, one is that it can lead to the concatenation of two distinct or similar samples, which may cause the model to either excessively ignore or rely on the context. We have put more analyses about this issue in Section 5, here, briefly, the packing method does not cause the model to focus on the packed context overly. There are primarily two reasons for this: (1) the likelihood that a random approach leads to two similar training samples being packed together is quite low, and even if this occurs, it represents a minor proportion of the entire training set, thus not significantly influencing the model's capabilities; (2) the special token [EOS] effectively allows the model to differentiate between two adjacent training samples."}, {"title": "Greedy Packing", "content": "Instead of random packing that might result in a single conversation being split across two different sequences, greedy packing starts by sorting and selecting training conversations based on their length:"}, {"title": "Process of Greedy Packing", "content": "Formally, for a m-turn (m > 1) training conversation $\\{(x_1,y_1),..., (x_m, y_m)\\}$, we first use the special token [EOS] to concatenate all instructions x and answers y into one single sequence s:\n$x_1y_1[EOS] \\cdots [EOS]x_my_m[EOS] = w_1w_2\\cdots [EOS]w_tw_{t+1}\\cdots w_T[EOS]$\nwhere $w_i, 1 < i < T$ denotes the word of the concatenated sequence, and T denotes the number of words in this sequence.\nThen, we sort all of the concatenated sequences s, iterating from the longest sequence, and, in a greedy way, we pack as many sequences as possible without exceeding the maximum input length allowed by the pre-trained model. The full process is present in Algorithm 1, which results in M packed sequences S:\n$w_1^1 w_2^1...[EOS] w_{t_1}^1 w_{t_1+1}^1...\\omega_\\tau_1^1[EOS]$\n$w_1^2 w_2^2...[EOS] w_{t_2}^2 w_{t_2+1}^2...\\omega_\\tau_2^2[EOS]$\n$\\cdots$\n$w_1^M w_2^M...[EOS] w_{t_M}^M w_{t_M+1}^M...\\omega_\\tau_M^M[EOS]$\nFinally, similar to the padding method, we truncate packed sequences that exceed the maximum length allowed by the pre-trained model and pad shorter sequences with the special token [PAD] to randomly batch the packed sequences."}, {"title": "Strengths of Greedy Packing", "content": "Greedy packing mainly serves as a modification of random packing, designed to reduce the risk of dividing relevant contexts across different batches. Simultaneously, it preserves the benefits of the packing method: enhancing the model's generalization capabilities by exposing it to a wider variety of contextual combinations within each training sample, thus encompassing a more diverse set of scenarios."}, {"title": "Weaknesses of Greedy Packing", "content": "In addition to the issue associated with the packing method: it may cause the model to either excessively ignore or rely on the context by packing two distinct or similar training samples into one sequence. Another potential concern is the break in the random sampling of training data. Since greedy packing entails sorting and organizing data prior to batching, it naturally diminishes the randomness in the distribution of sequences across batches. This can affect the diversity of data within each batch, as it is not entirely random but instead guided by the specific criteria (sequence length), for packing. However, despite these concerns in subjective analysis, our analysis and a series of experimental results in Section 5 have shown that using models trained with the greedy packing method does not result in any performance loss across various downstream benchmarks and GPT-based evaluations."}, {"title": "Experimental Setups", "content": "In this section, we sequentially describe our \u201cTraining Setups\" in 4.1 and \"Evaluation Setups\" in 4.2. For results and analysis, we put them in Section 5."}, {"title": "Training Setups", "content": "The development of packing methods was primarily aimed at maximizing hardware resource utilization and minimizing training duration. To demonstrate these training differences between packing and padding, we analyze four SFT datasets with different sizes:"}, {"title": "Training Datasets", "content": "WildChat (GPT-4). WildChat (Zhao et al., 2024) is a corpus comprising roughly 652k real-world interactions between users and ChatGPT, noted for its variety of languages and diverse user prompts. This dataset was created by providing users free access to ChatGPT and GPT-4 in return for their consent to collect chat histories. WildChat (GPT-4) is the smallest dataset in our experiments consisting of approximately 69k real-world interactions, selected specifically to include interactions between users and the GPT-4 model.\nTULU. TULU (Wang et al., 2023b) is a dataset consisting of around 326k conversations, sourced both from real-world interactions between users and open large LLMs as well as from manually annotated dialogues. As a synthetic dataset, TULU (Wang et al., 2023b) aims to combine the benefits of various open resources, enhancing the performance of models fine-tuned on it to deliver the highest general performance.\nWildChat (Full). WildChat (Full) includes the entire 652k training conversations from the WildChat (Zhao et al., 2024) corpus. Utilizing such a large dataset allows us to confirm that the performance differences between padding and packing methods are statistically significant and not merely random fluctuations. Additionally, it provides an opportunity to assess the scalability and consistency of the padding and packing methods as the dataset is processed over time.\nOpen-source 1M. The larger the dataset, the more reliable the conclusions that can be drawn from the experiments, particularly in terms of how each method handles memory and computational resources at varying scales. To facilitate this analysis, we create a large data mixture named \"open-source 1M\", which consists of approximately 1.2M conversations collected from several high-quality open resources such as ShareGPT (Chiang et al., 2023), FLAN V2 (Longpre et al., 2023), Alpaca (Taori et al., 2023), among others. A complete list of these resources is detailed in Table 1."}, {"title": "Model Training Details", "content": "Pre-trained Models. In this paper, our experiments predominantly utilize the LLaMA-3-8B and LLaMA-3-70B (Dubey et al., 2024) models, which are among the largest and most advanced pre-trained models currently accessible to the research community:\nChat Template. Following Dubey et al. (2024), we format all datasets to follow a chat template to"}, {"title": "Evaluation Setups", "content": "Following the argument in (Wang et al., 2023b) that general-purpose models should be able to perform some core tasks before they can generalize to satisfy various practical needs, we first assess the core capabilities of our fine-tuned models using a set of specific benchmarks. Subsequently, we"}, {"title": "Specific Benchmarks", "content": "We evaluate our models on the following benchmarks:\nMMLU. Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2020) consists of 14079 questions covering 57 tasks including elementary mathematics, US history, computer science, law, and more. The wide range of subjects and complex questions make MMLU suitable for testing the model's language comprehension and decision-making capabilities.\nMATH and GSM8K. MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021) are two distinct mathematical datasets utilized for evaluating different aspects of model capabilities. The MATH (Hendrycks et al., 2021) dataset comprises 12,500 complex competition-level mathematics problems, primarily designed to assess the ability of models to tackle challenging and advanced mathematical questions typically encountered at the college level. Conversely, the GSM8K (Cobbe et al., 2021) dataset contains 8,500 high-quality elementary school math problems, aimed at testing the basic mathematical reasoning abilities of models.\nBBH. BBH, short for BIG-Bench Hard (Suzgun et al., 2022), is a subset of the BIG-Bench (Srivastava et al., 2022) dataset comprising 23 challenging tasks. These tasks were selected because they consistently proved too difficult for current large language models to handle effectively. Requiring complex, multi-step reasoning, the BBH dataset is primarily utilized to assess the general reason-"}, {"title": "Evaluations Based on GPT-4", "content": "While human-based evaluation provides important insights into user preferences, it suffers from significant drawbacks like high labor costs and lack of real-time feedback. To address these limitations, we employ WildBench (Lin et al., 2024), an automated evaluation framework based on GPT-4. WildBench consists of 1,024 tasks manually selected from over one million human-chatbot conversation logs. It employs advanced LLMs (e.g., GPT-4-turbo) alongside specifically tailored checklists to systematically evaluate models' outputs and provide structured explanations supporting scoring and comparisons.\nFor settings, we use WildBench-v2 as the test"}, {"title": "Results and Analysis", "content": "In this section, we provide our experimental results as well as analysis based on them."}, {"title": "Analysis on Various Benchmarks", "content": "Table 3 and Table 4 show results of different size models and datasets on various benchmarks, from that we can observe that:\n(1) Models using packing generally perform better on average compared to those using padding across various benchmarks. Compared to the padding method, the packing method exposes models to a wider variety of contextual combinations within each training sample, offering a more diverse set of scenarios and enhancing the models' ability to generalize. For example, 61.50 (Padding) v.s. 65.97 (Random Packing) on the model LLaMA-3-70B for the WildChat (GPT-4) dataset.\n(2) As the model size grows, the performance gap between padding and packing-based models on the benchmark increases. This is due to enhanced contextual efficiency. As the model size increases, its ability to effectively utilize extended contexts improves, thereby magnifying the advantages of the diverse contextual combinations brought by packing. For example, on the WildChat (GPT-4) dataset, the average score is 49.58"}, {"title": "Analysis on Training Time", "content": "Table 5 shows the training time of different size models on various training datasets, where we can find that:\n(1) Compared to padding, the packing method greatly reduces training time, making it possible to fine-tune large models on large datasets. The packing method significantly decreases training time by efficiently utilizing the available computational resources, for example, 9533s (Padding) v.s. 3749s (Random Packing) on the model LLaMA-3-70B for the WildChat (GPT-4) dataset, and 40735s (Padding) v.s. 9758s (Random Packing) on the model LLaMA-3-70B for the TULU dataset. This reduction is particularly beneficial for scaling up model training and enables the effective handling of larger models and more extensive datasets, for example, from 97893s (Padding) significantly dropping to 25124s (Greedy Packing) on the model LLaMA-3-70B for the WildChat dataset, and from 184709s (Padding) significantly dropping to 27426s (Greedy Packing) on the model LLaMA-3-70B for the Open-source 1M dataset, thus enhancing the overall training throughput and allowing for more complex and comprehensive model fine-tuning.\n(2) Using longer training samples increases the time required for the packing method to process each sample, making it less suitable for training on particularly small datasets. Compared to padding, the packing method results in a lower number of samples processed per second, for example, 21.13 (Padding) v.s. 20.48 (Greedy Packing) for the WildChat (GPT-4) dataset based on the model LLaMA-3-8B, and 2.162 (Padding) v.s. 1.895 (Random Packing) for the TULU dataset based on the model LLaMA-3-70B. Therefore, if your goal is to fine-tune a small model (e.g., 6B, 8B, or 9B) on a small dataset (e.g., 20K or 30K), using the padding method might be more time-efficient."}, {"title": "Other Analysis", "content": "In addition to the analysis provided, we conducted additional experiments and concluded that:\n(1) In packing mode, the batch size is no longer directly proportional to the learning rate. Previous research indicates that when increasing the batch size by a factor of k, the learning rate should also be multiplied by k to maintain a constant variance in the gradient expectation (Goyal, 2017). This raises the question of whether the linear relationship between batch size and learning rate still holds when using the packing method. To determine this, we compare the padding method and the random packing method by separately fine-tuning the LLaMA-3-8B model on the TULU dataset using different linear combinations of batch size and learning rate. Results are shown in Figure 2, where the IFEval (Zhou et al., 2023) score is the primary evaluation metric. The results reveal that while the batch size and learning rate adhere to a linear relationship in the padding method, this is not the case with the packing method. This discrepancy is due to the nature of packing: it does not ensure that each training sample consistently contains the same number of training conversations. Consequently, when the batch size is increased by a factor of k, the actual number of training conversations is not necessarily scaled up by the same factor, disrupting the linear relationship between batch size and learning rate.\n(2) Applying packing to datasets with only single-turn conversations may lead to a significant decrease in performance on few-shot benchmarks. In packing methods, training samples that lack contextual connections may be combined to create what could be considered \"fake\" multi-turn conversations. When the training dataset includes multi-turn conversations, this approach allows the model to learn when to consider the context and when not to. However, if the training dataset only consists of single-turn conversations, there's a risk that the model might become less effective at utilizing context, potentially leading to a decline in performance on few-shot benchmarks. To investigate"}, {"title": "Conclusion", "content": "In this paper, we conduct a thorough comparison of SFT methods using padding and packing, analyzing datasets from 69K to 1.2M and models ranging from 8B to 70B. This provides the first detailed examination of the advantages and limitations of packing versus padding, as well as practical considerations for implementing packing in various training scenarios. Our evaluation spans a range of benchmarks, including knowledge, reasoning, and"}]}