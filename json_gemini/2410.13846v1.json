{"title": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction", "authors": ["Xuan Zhang", "Cunxiao Du", "Chao Du", "Tianyu Pang", "Wei Gao", "Min Lin"], "abstract": "Recent advancements in large language models (LLMs) have extended their capabilities to handle long contexts. However, increasing the number of model layers and the length of input sequences significantly escalates the memory required to store key-value (KV) cache, posing challenges for efficient inference. To mitigate this issue, we present SimLayerKV, a simple yet effective method that reduces inter-layer KV cache redundancies by selectively dropping cache in identified lazy layers. Our approach is based on the observation that certain layers in long-context LLMs exhibit \"lazy\" behavior, contributing less to modeling long-range dependencies compared to non-lazy layers. By analyzing attention weight patterns, we find that the behavior of these lazy layers is consistent across tokens during generation for a given input. This insight motivates our SimLayerKV, which identifies lazy layers and reduces their KV cache accordingly. SimLayerKV is training-free, generalizable, and can be implemented with only seven lines of code. We conduct extensive experiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and Mistral-7B across 16 tasks from the LongBench benchmark. The results demonstrate that SimLayerKV achieves a KV cache compression ratio of 5x with only a 1.2% performance drop when combined with 4-bit quantization. Our code is available at https://github.com/sail-sg/SimLayerKV.", "sections": [{"title": "1 INTRODUCTION", "content": "Transformer-based autoregressive large language models (LLMs) have demonstrated exceptional performance across a wide range of tasks, such as question answering and arithmetic reasoning (Wei et al., 2022; Wang et al., 2022; Zhou et al., 2022; Yao et al., 2023). Recent advancements have extended their capabilities to handle long contexts, with models like Llama-3.1 supporting context lengths up to 128K tokens (Dubey et al., 2024) and Gemini-Pro-1.5 handling up to 1 million tokens (Reid et al., 2024). A critical component of these models during inference is the key-value (KV) cache, which stores precomputed key and value tensors for each token in the language sequence to avoid recomputing them for each attention layer. However, as the number of model layers and input lengths increases, the memory required for storing the KV cache grows significantly, posing challenges for inference efficiency (Zhang et al., 2024b; Wang et al., 2024; Li et al., 2024). For example, with an input sequence length of 128K tokens, the memory required for the KV cache in Llama2-7B amounts to approximately 62.5 GB GPU memory, which is significantly larger than the 13.2 GB needed for the model parameters.\nTo address the challenge, various methods have been introduced to reduce the KV cache storage (Zhang et al., 2024b; Li et al., 2024; Hooper et al., 2024; Dong et al., 2024a; Yang et al., 2024c). One approach is quantization (Hooper et al., 2024; Dong et al., 2024a; Yang et al., 2024c; Dong et al., 2024b; Kang et al., 2024; Liu et al., 2024c; Sheng et al., 2023), which stores the KV cache in low-bit formats. Another approach resorts to eviction (Zhang et al., 2024b; Li et al., 2024; Zhang et al., 2024a; Yang et al., 2024b), which only preserves the most important tokens selected based on"}, {"title": "2 RELATED WORK", "content": "Due to the autoregressive architectures of transformer-based LLMs, the key and value states of previously generated tokens can be stored as the KV cache, which facilitates the generation of subsequent tokens without redundant computations. However, despite its benefits, caching introduces a significant bottleneck during inference as it must reside in GPU memory. Several works (Prabhu et al., 2024; Kwon et al., 2023; Lin et al., 2024; Ye et al., 2024) have focused on optimizing KV cache memory at the system level. Other research has investigated reducing KV cache memory requirements by modifying model architectures (Shazeer, 2019; Brandon et al., 2024; Goldstein et al., 2024; Nawrot et al., 2024; Wang et al., 2024; Yu et al., 2024). For example, grouped-query attention (GQA) (Ainslie et al., 2023) divides the query heads into multiple groups, with each sharing its own set of keys and values. However, these techniques typically need to be applied during pre-training, which can be resource-intensive.\nA different line of research focuses on reducing the KV cache memory usage post pre-training. Some techniques (Xiao et al., 2023; Li et al., 2024; Wang et al., 2024; Zhang et al., 2024b; Liu et al., 2024b; Yang et al., 2024b; Zhang et al., 2024a) identify redundant tokens within each attention layer and evict their associated KV cache, thereby effectively lowering memory usage. Other methods (Hooper et al., 2024; Dong et al., 2024a; Yang et al., 2024c; Dong et al., 2024b; Kang et al., 2024; Sheng et al., 2023) reduce memory consumption by quantizing KV cache from full precision to lower bit values. However, these methods primarily exploit intra-layer KV cache redundancies while overlooking those across layers. These techniques are orthogonal to our approach and can potentially be combined for further improvements.\nA distinct line of research (Rajput et al., 2024; Brandon et al., 2024; Wu & Tu, 2024; Liao & Vargas, 2024; Wu & Tu, 2024; Liu et al., 2024a; Ge et al., 2024), more closely aligned with our focus, explores the inter-layer KV cache redundancies. For instance, CLA (Brandon et al., 2024) reduces overall KV cache storage by reusing the KV cache from the current layer in subsequent layers. Mix Attention (Rajput et al., 2024) integrates cross-layer cache sharing with sliding window attention, which retains only a small subset of recent tokens in the KV cache, thereby further reducing memory usage. LongGen (Ge et al., 2024), Inheritune (Sanyal et al., 2024), and Gemma 2 (Team et al., 2024) employs a predefined mixture of full attention and sliding window attention across different layers during training. However, these approaches rely on a fixed, predefined structure and lack adaptability to the input data. In contrast, our method dynamically identifies lazy layers based on their attention allocation patterns. In addition, these methods require additional training, which is computationally demanding. In contrast, MiniCache (Reid et al., 2024) offers a tuning-free solution by merging every two adjacent layers through spherical interpolation, assuming equal contribution from all layers within the merged set. Our SimLayerKV approach differs by selectively trimming lazy layers, based on the observation that not all layers contribute equally to the overall generation."}, {"title": "3 PRELIMINARY", "content": "Before introducing SimLayerKV, we formalize our notation and provide a brief overview of the generative inference in autoregressive LLMs, which is the key background knowledge for our method. We denote the input prompt $X = \\{x_0,\\ldots,X_{m-1}\\}$, representing a sequence of tokens, where $m$ is the number of tokens in the input prompt, indicating the sequence length. The total number of tokens, including both the input prompt and the generated responses, is denoted as $n$. The key and value cache for token $x_i$ are represented by $K_{x_i}$ and $V_i$, respectively."}, {"title": "4 OBSERVATIONS", "content": "In this section, we analyze the attention patterns during the prefilling and decoding phase in long-context LLMs, providing insights that motivate our approach to reducing KV cache based on the layer-specific roles in attention allocation. The study is conducted on the LLaMA3-8B-Instruct model (Dubey et al., 2024) using random samples from the LongBench (Bai et al., 2023) benchmark. Our key findings are as follows:\nLayer behavior in long context LLMs during decoding. Previous research (Xiao et al., 2023) has shown that a large portion of attention in LLMs tends to focus on semantically unimportant tokens (e.g., the first few tokens) and the most recent tokens. We refer to this pattern as lazy behavior, where the model \u201ctakes shortcuts\u201d by primarily attending to the beginning and end of the sequence, similar to someone skimming a paper by only reading the first few words in the abstract and the conclusion. Although this phenomenon is also known as \u201cattention sink\u201d (Xiao et al., 2023), we choose to call it \"lazy behavior\" in our context to better highlight the model's tendency to overlook the middle portions of the sequence, emphasizing the shortcut nature. However, in our experiments (See Table 1 and Table 3), we find that when KV cache are retained for only these tokens across all layers, the long-context capabilities of LLMs degrade sharply. This raises an important question: does this lazy behavior disappear when processing long texts?\nThrough our analysis, we observe that even when handling long texts, many layers continue to exhibit this lazy behavior during decoding (e.g., about 55% in LLama3-8B-Instruct in LongBench benchmark). Figure 2 presents the attention patterns across four different layers (0, 10, 20, and 30). We observe that some layers (e.g., layer 0) do not follow a clear pattern in attention weight distribution, while others (e.g., 20) show a clear lazy behavior pattern. Based on this observation, we define a lazy layer as one that primarily attends to a limited subset of tokens, including both the initial tokens $X_{initial} = \\{x_0, x_1,x_2,x_3\\}$ and recent w tokens $X_{recent}$, while allocating minimal attention to the rest of the tokens in the sequence during decoding stage. Intuitively, this suggests that in these lazy layers, most of the KV cache can be dropped, retaining only the portions the model relies on during its \"shortcut\" behavior, i.e., $X_{initial}$ and $X_{recent}$.\nLazy layer is less important than non-lazy layer. Although attention scores in lazy layers are concentrated on certain tokens, this does not necessarily indicate that these layers are unimportant for long-context capability. To investigate this further, we conduct experiments on 6 random selected tasks from the LongBench benchmark (Bai et al., 2023), including Qasper (Dasigi et al., 2021), Dureader (He et al., 2017), Musique (Trivedi et al., 2022), GovReport (Huang et al., 2021), MultiFieldQA-en (Bai et al., 2023), and HotpotQA (Yang et al., 2018). We test the effect of trimming most of the KV cache, retaining only the cache for $\\{X_{initial}, X_{recent}\\}$ in two scenarios: (1) lazy"}, {"title": "5 METHODOLOGY: SIMLAYERKV", "content": "In this section, we introduce our method SimLayerKV for reducing inter-layer KV cache usage in LLMs by leveraging the concept of lazy layers to optimize memory efficiency across layers. Empirical observations in Section 4 reveal that in certain layers, LLMs tend to take shortcuts by predominantly allocating attention weights to the initial and most recent tokens, denoted as $X_{initial}$ and $X_{recent}$, respectively. We refer to these layers as lazy layers because they contribute less to modeling long-range dependencies compared to non-lazy layers. Notably, whether a layer functions as lazy remains relatively consistent given a specific input sequence. This consistency suggests that attention patterns can be predicted from the allocation during the generation of previous tokens, enabling early identification of lazy layers in the generation process."}, {"title": "5.1 IDENTIFYING THE LAYER FUNCTION", "content": "To apply SimLayerKV, the first step is to identify which layers function as lazy layers based on their attention allocation patterns. Once these layers are identified, we can proceed to trim their KV cache to optimize memory usage. In the following, we detail our strategies for identifying the layer function. Corresponding to the two stages of the inference process (i.e., prefilling and decoding), we propose two different identification strategies.\n1) Last tokens in prefilling: We analyze the attention weight allocation when processing the last $w_{last}$ processed tokens $X_{last} = \\{X_{m-w_{last}+1},\\ldots, x_m\\}$ to identify lazy layers during prefilling. For each layer $l$, we calculate the average attention weights directed toward the $X_{initial}$ and $X_{recent}$ for all tokens in $X_{last}$. If this average exceeds a predefined threshold $\\delta$, we classify the layer $l$ as lazy; otherwise, it is considered non-lazy. This can be formalized as:\nFunction[l] =\n\\begin{cases}\nlazy \\text{layer}, & \\text{if } \\frac{1}{w_{last}} \\sum_{z \\in X_{last}} (\\sum_{x \\in \\{X_{initial}, X_{recent}\\}} A_l(z, x)) > \\delta, \\\\\nnon\\text{-lazy layer}, & \\text{otherwise,}\n\\end{cases}\n(1)\nwhere $A_l(x,x)$ represents the attention weight from token $\\hat{x}$ to token $x$ in layer $l$ and the threshold $\\delta$ is a predefined hyper-parameter.\n2) First token in decoding: We assess the attention weight distribution when generating the first token $x_{m+1}$ during the decoding phase to identify lazy layers. Specifically, for each layer $l$, if the attention weights directed toward $\\{X_{initial}, X_{recent}\\}$ when generating $x_{m+1}$ exceed $\\delta$, we classify the layer as lazy; otherwise, it is not considered lazy. This can be formalized as:\nFunction[l] =\n\\begin{cases}\nlazy \\text{layer}, & \\text{if } \\sum_{x \\in \\{X_{initial}, X_{recent}\\}} A_l(x_{m+1}, x) > \\delta, \\\\\nnon\\text{-lazy layer}, & \\text{otherwise.}\n\\end{cases}\n(2)\nRemark. During the prefilling stage, flash attention (Dao, 2023) is commonly used to accelerate computations. However, flash attention does not return explicit attention weights, making it challenging to apply the lazy layer identification strategy without recomputing the attention scores, which would introduce extra computational overhead. In contrast, during the decoding stage, tokens are generated one at a time without using flash attention, making the attention weights readily available. This allows us to apply our identification strategy without extra computation. In our experiment (See Table 6), we find that the two strategies perform comparably, with no significant differences."}, {"title": "5.2 CACHE STRATEGY", "content": "Once lazy layers have been identified, we proceed to trim the KV cache for these specific layers. Lazy layers are characterized by their significant attention allocation to a limited subset of tokens, namely $\\{X_{initial}, X_{recent}\\}$. Thus we retain only the KV cache corresponding to these tokens within lazy layers. This selective retention strategy is similar to approaches used in methods like Gemma 2 (Team et al., 2024), which also retain KV cache for recent tokens in predefined layers.\nSpecifically, for any lazy layer $l$, we trim its KV cache by retaining only those of tokens in $\\{X_{initial}, X_{recent}\\}$. Otherwise, we retain the full cache. This process can be expressed as:\nCache[l] = \\begin{cases}\n\\{K_{initial}, V_{initial}, K_{recent}, V_{recent}\\}, & \\text{if Function[l] = lazy layer}, \\\\\nfull, & \\text{otherwise,}\n\\end{cases}\n(3)\nwhere Cache[l] represents the KV cache for layer $l$."}, {"title": "6 EXPERIMENTS", "content": "In this section, we empirically validate that SimLayerKV can accelerate decoding while maintaining long-text capabilities and uncover several insightful findings."}, {"title": "6.1 SETTINGS", "content": "Baselines. To evaluate the effectiveness of our proposed SimLayerKV, we compare it against the following baselines: 1) Full KV (Full): A method that retains KV cache for all tokens at each layer during generation. 2) Streaming LLM (Str.) (Xiao et al., 2023): An intra-layer KV cache reduction technique that keeps only the KV cache for the first four tokens and the most recent w tokens at each attention layer during generation. 3) MiniCache (Mini.) (Liu et al., 2024a): An inter-layer KV cache reduction method that merges KV cache of every two adjacent layers after the model's midpoint using spherical interpolation while retaining important tokens to reduce cache storage. Additionally, for both MiniCache and our SimLayerKV, we evaluate their performance when combined with 4-bit quantization (Liu et al., 2024c) to assess their compatibility with quantization techniques.\nDatastes and evaluation metrics. To evaluate SimLayerKV's performance on tasks with long-context inputs, we test it on the LongBench benchmark (Bai et al., 2023) and compare the results with baseline methods. LongBench is a multi-task benchmark designed to assess the long-context capabilities of LLMs, consisting of datasets that span various tasks such as single-document QA (Ko\u010disk\u1ef3 et al., 2018; Dasigi et al., 2021), multi-document QA (Yang et al., 2018; Ho et al., 2020; Trivedi et al., 2022; He et al., 2017), summarization (Huang et al., 2021; Zhong et al., 2021; Fabbri et al., 2019; Wu et al., 2023), few-shot learning (Joshi et al., 2017; Gliwa et al., 2019; Joshi et al., 2017; NLPCC, 2014), synthetic tasks (Raffel et al., 2020), and code generation (Guo et al., 2023; Liu et al., 2023). For evaluation, we use the metrics recommended by LongBench. Additionally, we provide the compression ratios for both the number of layers and memory usage of the KV cache. For layers, the ratio is calculated as the total number of layers divided by the number of"}, {"title": "6.4 ABLATION STUDIES & ANALYSIS", "content": "Impact of threshold on lazy layer identification. To assess the impact of the threshold $\\delta$ in identifying lazy layers, we conduct an ablation analysis using the LLama3-8B-Instruct model, varying $\\delta$ from 0, 0.2, up to 1. As illustrated in Figure 5, we observe that as the threshold increases, the model's performance shows little to no change or only slow improvement initially. However, after exceeding 0.6, the performance improves rapidly, and by 0.9, it approaches the performance seen when the threshold equals 1 in most tasks. This indicates that as the threshold increases, the likelihood of accurately identifying and trimming truly lazy layers increases, allowing the model to maintain high performance while reducing unnecessary computations.\nEffect of different strategies for dropping KV cache at layer level. As shown in Figure 6 (a-d), we experiment with four different strategies. We ensured the same number of dropped KV cache for each strategy, except for Full. The results shown in Figure 6 (e-f) indicate significant reductions for Pyramid and Random strategies, suggesting that the predefined expectations about each layer's function may not fully align with their actual roles. Moreover, the performance difference between SLKV-prefill and SLKV-decode strategies is minimal, with only slight reductions compared to the full KV cache (0.20% and 0.28% on average, respectively). This indicates that both approaches are effective in reducing cache usage while maintaining performance, regardless of whether lazy layers are identified during the prefilling or decoding stages."}, {"title": "7 CONCLUSION", "content": "In this work, we introduced SimLayerKV, a simple yet effective method for compressing the KV cache in LLMs. By identifying lazy layers and trimming their KV cache, SimLayerKV effectively reduced inter-layer KV cache redundancies. Experiments on three different LLMs across 16 datasets from the LongBench benchmark demonstrated that SimLayerKV, with only seven lines of code, achieves a KV cache compression ratio of 5\u00d7 with only a 1.2% drop in performance when combined with 4-bit quantization. For future work, we aim to combine our inter-layer KV cache compression method, SimLayerKV, with other powerful intra-layer compression methods like H2O (Zhang et al., 2024b) to further enhance performance and efficiency."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 LIMITATION", "content": "While our SimLayerKV has demonstrated significant advantages in inter-layer KV cache compression, we have primarily focused on combining it with quantization, as quantization is one of the most widely used techniques. However, there are many other KV cache optimization methods, such as intra-layer eviction, which are orthogonal to our approach. In this study, we have not explored the potential of integrating our method with these techniques. In the future, we aim to combine our method with other optimization strategies, to further improve performance and efficiency. This will help validate the effectiveness of our method in a broader framework and potentially lead to even greater performance gains. Meanwhile, for simplicity, we have only explored KV cache redundancies across layers in this work. In the future, we plan to extend our approach to consider redundancies across attention heads as well."}, {"title": "A.2 PSEUDO CODE", "content": "The pseudo-code for SimLayerKV-prefill and SimLayerKV-decoding are in Table 4 and Table 5 respectively."}, {"title": "A.4 EXAMPLES ABOUT LAYER BEHAVIOR ACROSS TOKENS", "content": "Additional examples of layer behavior across tokens for a given input can be found in Figure 7. The examples are randomly chosen from LongBench benchmarks. The analysis is conducted using LLama3-8B-Instruct."}]}