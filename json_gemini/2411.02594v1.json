{"title": "\u201cIt's a conversation, not a quiz\u201d: A Risk Taxonomy and Reflection Tool for LLM Adoption in Public Health", "authors": ["JIAWEI ZHOU", "AMY Z. CHEN", "DARSHI SHAH", "LAURA SCHWAB REESE", "MUNMUN DE CHOUDHURY"], "abstract": "Recent breakthroughs in large language models (LLMs) have generated both interest and concern about their potential adoption as accessible information sources or communication tools across different domains. In public health-where stakes are high and impacts extend across populations-adopting LLMs poses unique challenges that require thorough evaluation. However, structured approaches for assessing potential risks in public health remain under-explored. To address this gap, we conducted focus groups with health professionals and health issue experiencers to unpack their concerns, situated across three distinct and critical public health issues that demand high-quality information: vaccines, opioid use disorder, and intimate partner violence. We synthesize participants' perspectives into a risk taxonomy, distinguishing and contextualizing the potential harms LLMs may introduce when positioned alongside traditional health communication. This taxonomy highlights four dimensions of risk in individual behaviors, human-centered care, information ecosystem, and technology accountability. For each dimension, we discuss specific risks and example reflection questions to help practitioners adopt a risk-reflexive approach. This work offers a shared vocabulary and reflection tool for experts in both computing and public health to collaboratively anticipate, evaluate, and mitigate risks in deciding when to employ LLM capabilities (or not) and how to mitigate harm when they are used.", "sections": [{"title": "1 Introduction", "content": "Recent breakthroughs in large language models (LLMs) have spurred widespread attention and rapid adoption across different fields. With their abilities to generate convincing human-like language on the basis of large sets of human-written content [12, 70], LLMs hold the potential to influence how we interact with information. LLMs have quickly gathered hundreds of millions of active users [55] and made their way into everyday products, sometimes even without users' full awareness. Accordingly, public discourse has shown excitement about LLM assistance in various information-seeking and support tasks and even hype about the framing of LLMs as a \u201cnext-generation search engine.\u201d 1 This technology enthusiasm extends into the health sector, where studies have presented evidence of LLMs' competence in completing tasks such as clinical documentation [65, 79], decision support [65, 68], therapeutic conversations [35, 61, 71], and public health intervention [30-32]. As a matter of fact, LLMs have already been incorporated into large-scale health systems. For instance, Epic has partnered with Microsoft to start integrating LLM tools into its electronic health record (EHR) software, which owns the largest market share of acute care hospitals in the U.S., to automatically draft clinical notes [38] and patient message responses [39]. While this proliferation seems promising, experts and scholars caution against exaggerating LLM capabilities [18, 77]. Specifically, uncertainty about information quality is problematic, particularly regarding inaccurate information, biases, and harmful content [23, 41, 82]. Given the high-stakes nature and data sensitivity of population-level health, it warrants careful deliberations of when it is appropriate to employ LLM capacities and how we can mitigate potential harm when we do.\nHowever, in practice, evaluating the risks of potential LLM adoption in public health remains challenging. The first gap lies in the lack of domain understanding, as the majority of risk taxonomies lack granularity to be applied for specific uses [42] or are created within the computer science community and tend to leave out domain experts and real users [6, 63, 72]. Second, when considering population-level impact, potential risks come in ecological \"layers\" from individuals to society, and conventional categorizations based on content types (e.g., misinformation, hate speech, and biases) are not sufficient to evaluate real-world cases [57] while different types often intertwine with each other [24, 34]. Identifying the presence of generated harmful content is only the first step; we must also contextualize low-quality information within specific health issues and relevant populations in order to evaluate the consequences and severity and mitigate potential harm. In response to these gaps, this paper is situated in three distinct and critical public health issues and grounded in the perspectives of both health professionals and members of the general public who might use such tools to seek health information. Specifically, we ask: what negative consequences might arise in adopting LLMs in public health for informational needs and support?\nWe conducted focus group sessions with ten health professionals and ten health issue experiencers to uncover potential negative influences of using LLMs as a communication tool or information source in public health. We selected vaccines, opioid use disorder (OUD), and intimate partner violence (IPV) as topics for different sessions based on their significance across different dimensions of public health-infectious disease prevention, chronic and well-being care, and community health and safety-all demanding high-quality information with existing prevalent issues such as misinformation, biases, and sensitivity. The result is a risk taxonomy of potential adoption of LLM for public health. Our taxonomy consists of four dimensions of harm: individual behaviors, human-centered care, information ecosystem, and"}, {"title": "2 Related Work", "content": "A growing body of work has explored the potential of utilizing large language models (LLMs) in health. Studies have presented evidence of their potential in a variety of health-related tasks such as clinical documentation [65, 79], question answering and medical reasoning [3, 48, 79], decision support [65, 68], therapeutic conversations [35, 61, 71], and public health intervention [30-32]. In practice, LLMs have already been implemented in existing systems or new technologies, including EHR systems [39], virtual agents for dealing with domestic violence [45], and public health intervention [30]. Orthogonally, researchers have also raised concerns about the risks of adopting LLMs for health needs. General issues include data privacy [81], language and geographic disparities [29, 47], and the generation of inaccurate, biased, or toxic content [14, 41, 82]. When applied to health specifically, questions remain regarding LLMs' ability to meet clinical standards [25] and their tendency to propagate race-based medicine [50]. Harrer's study found that only half of LLM-generated messages meeting the clinical standards to be directly adopted for use. When used for communication assistance, LLMs are found to have difficulty in being customized or controlled [73] and lack of sensitivity in managing emotional needs and emergencies [71]. Personalizing language to suit diverse demographics is another challenge [30], particularly for minoritized and marginalized groups where cultural nuances and sensitive topics may be overlooked (such as the LGBTQIA+ [43, 51] and disability [22] community). This shortfall extends to well-being care, as studies have shown that LLMs often generate responses that are more insensitive compared to human therapists, especially for individuals experiencing high-intensity emotions [61]. In higher-risk cases, such as patients with borderline personality disorder or schizophrenia, LLMs may even amplify harmful behaviors or symptoms [35].\nIn reflecting on these issues, scholarship has been to critically examine how we approach LLM evaluation in terms of reporting transparency [18], assessment reliability [18, 77], practical deployment [77], and consideration of relationships in healthcare [75]. For instance, Drogt et al. found in their survey study that around half of clinical language models were not validated on clinical text, and most evaluations focused on traditional natural language processing (NLP) tasks that do not offer meaningful insights for healthcare applications. As a result, they called for frameworks that align more closely with values and factors crucial to health contexts. We build on prior scholarship by comprehensively examining risks posed by LLMs in public health through three critical yet distinctive health issues."}, {"title": "2.2 Al and LLMs Risk Taxonomies", "content": "In response to the literature on the ethics and limitations of LLMs, research community has proposed various taxonomies to address risks posed by these models. Some studies take a broader approach, examining AI or algorithmic systems by synthesizing privacy risks [40], sociotechnical harms [63], risks in algorithmic agency [13], and impacts in individual, social, and biospheric dimensions [17]. Others specifically focus on language models. For example, Bender et al. listed the risks of LM and suggested future mitigation efforts, including pre-development evaluations on how these models support stakeholder values. Weidinger et al. summarized six ethical and social risks, such as misinformation harms and human-computer interaction harms, while Kumar et al. looks into practical methods in mitigating harms. They summarized common intervention strategies across four stages of NLP model development: data, model training and design, inference and generation, and application. In the realm of health, De Choudhury et al. discussed the benefits and harms of LLMs in digital mental health across four application areas: supporting individual care-seeking, assisting caregiving, decision-making aid, and transforming telehealth paradigms. Antoniak et al. proposed principles for ethical use of NLP and LLMs for maternal healthcare in recognizing context, measurements, and values.\nDespite these contributions, there is no established vocabulary to specifically frame the risks in public health contexts, a domain that is featured by its high-stakes nature and population-level impacts. This absence of a shared framework limits our ability to systematically explore, communicate, and mitigate potential harms in this specific domain.\nFrom a methodological point, most existing taxonomies are position papers or review studies centered within the computer science field, which often exclude domain experts and end users in applied areas. Research has warned of the limitations of evaluating LLMs for clinical purposes using traditional NLP tasks and metrics [77]. Consequently, research in HCI and CSCW has called for human-centered approaches that closely involve stakeholders, as certain AI-related harms may be difficult to capture solely through existing AI literature [2, 33, 75]. Our work seeks to address this gap by proposing a shared vocabulary grounded in real public health issues, informed by perspectives from both health professionals and real users."}, {"title": "3 Method", "content": "This study aims to evaluate the risks of using large language models (LLMs) for informational needs in public health by focusing on three distinct and critical issues: vaccines, opioid use disorder (OUD), and intimate partner violence (IPV). The selection of these topics was driven by their significance across different dimensions of public health-infectious disease prevention, chronic and wellbeing care, and community health and safety-and through consultation with the public health expert coauthor. Each topic underscores the importance of high-quality information in public health communication. Specifically, vaccines play a crucial role in infectious disease management, while misinformation and pubic distrust significantly contribute to vaccine hesitancy [16, 53, 54]. Effective counter-speech is necessary to combat these misconceptions and enhance trust [80, 83]. OUD is a major issue within the realm of chronic and wellbeing care, which is exacerbated by stigma [19, 49] and vulnerability [69, 78]. Therefore, this health crisis is in need of public education to reduce stigma and create a supportive environment. IPV is a high-risk and stigmatized issue in community health and safety that demands highly sensitive and supportive communication [46, 59]. IPV involves complex challenges, including personal safety, mental health, and legal and financial challenges [21, 58]. Providing high-quality, empathetic information is essential for supporting survivors and addressing their comprehensive needs."}, {"title": "3.2 Participants and Recruitment", "content": "We recruited both health professionals and members of the general public. The health professionals group (labeled with 'professionals/P' in the paper) consisted of people working in the health sector, such as social workers, nurses, community health workers, and researchers, all of whom have experience working on one of the three topics. The general public group (labeled with 'experiencers/E' in the paper) includes people who have actively sought information about one of these three issues. For the sessions on OUD and IPV, they are individuals who have lived experience with these issues. We required that all participants be over 18 years old, be located in the U.S., and have a desktop/laptop or tablet to join an online meeting and participate in brainstorming writing. Interested respondents completed a brief survey about their demographics, occupation, professional background (for the professional cohort), AI attitudes, familiarity with LLM products, and their availability to suggested timings. The complete survey is included in supplements and we invited participants solely based on availability.\nTo recruit health professionals, we employed a snowball sampling approach by reaching out to professional networks and relevant organizations via email. Our primary sources were public health departments and professional organiza-tions. Specifically, we identified programs and divisions within federal and state public health departments, such as immunization divisions, overdose prevention initiatives, and violence prevention initiatives. We contacted departments across various states, including Georgia, Indiana, California, Minnesota, and Texas, in order to get a broad geographic coverage. During this process, we identified partner alliances and support systems listed by state departments and subsequently reached out to these organizations and shelters. Simultaneously, we distributed our recruitment flyer via professional networks and working groups, contacted ethics teams and task forces in hospitals and universities, and contacted individual researchers who work on health communication and support on the chosen health issues.\nTo recruit the general public, we used the Prolific 2 research platform where we launched three separate studies for our health topics. For the OUD and IPV sessions, we utilized Prolific's screeners by limiting the pool to people who experienced substance abuse problems or abuse-related incidents and specified eligibility to those who have"}, {"title": "3.3 Study Procedure", "content": "Due to the intentionally wide geographic distribution of our participants, all focus group sessions were conducted virtually on Zoom and lasted 90 minutes. Before the actual sessions, we conducted two rounds of pilot tests with four domain experts and four experiencers to refine the study design. To provide a safe and comfortable environment for our participants, we held separate sessions for professionals and experiencers to ensure experiencers participated in sessions with individuals who shared similar lived experiences.\nEach session began with an introduction to the study's goals and process, a reminder of participant rights, and self-introductions where participants shared their relevant experience with the session's topic. To contextualize the discussion of the potential role of LLMs in meeting informational needs, we started by discussing how participants currently share or seek health information, how they evaluate the quality of content, the precautions required in sharing information (for information creators), common challenges in information searching (for information seekers), and prevalent misconceptions about the specific topics. Following the introduction, we gave a brief introduction to LLM mechanisms, underlining the fact that LLMs generate responses based on probabilities and are designed to produce"}, {"title": "3.4 Data Analysis", "content": "We identified the types of risks raised by participants by analyzing session transcripts and brainstorming notes through a grounded approach using reflexive thematic analysis [8-10]. Three researchers independently performed open coding on three sessions in a line-by-line fashion to identify mentioned risks, and each identified lower-level codes. These initial codes were formed verbatim from participants' conversations or brainstorming entries (e.g., \u201cmore reliance on tech vs. human relationships", "don't understand prompt creation enough to use it effectively": "."}, {"title": "3.5 Privacy, Ethics, and Disclosure", "content": "We understand the sensitivity of the health topics in this study and potential concerns for safety and privacy, and we are committed to ensuring the privacy and safety of our participants. This study was approved by the Institutional Review Board (IRB) at our institutions. The demographic information and video recordings were collected with consent and later anonymized. Any personal information such as locations and workplaces was removed. We refrained from collecting any personally identifiable information from people with lived experience, and data from screened-out or dropped-out participants was discarded. Throughout the recruitment and focus groups, we assured the participants that their participation was completely voluntary, all questions were optional, and their responses would be anonymous. We also requested participants not to take screenshots or share what other participants said outside of the discussion."}, {"title": "4 Findings Overview", "content": "Overall, our experiencer participants (i.e., general users) expressed more positive attitudes towards LLMs than health professionals and expressed mixed feelings about the benefit-risk tradeoffs. They more frequently pointed out the benefits of accessible information at both resource and emotional levels-particularly for individuals without health insurance or who are emotionally burdened by social interactions-and the possibility of lowered healthcare costs. Health professionals, on the other hand, tend to refer more to the human element of care in their professions, such as building relationships and cultural sensitivity with clients, as well as the broader health and social services ecosystem. In anticipating negative consequences, we distill four dimensions of risks when adopting LLMs for public health through thematic analysis of session transcripts and brainstorming notes. As shown in Fig. 2, the four dimensions include (1) individual behaviors, (2) human-centered care, (3) the information ecosystem, and (4) technology accountability."}, {"title": "5 Dimension 1: Risks to Individual Behaviors", "content": "As shown in Table 3, participants reflected that individuals using LLMs to seek health information may experience harm during and after the interaction. They may receive 1) low-quality support that mishandles critical situations or impairs health decisions and emotional wellbeing, 2) inadequate or biased support, or 3) additional barriers to help-seeking, such as communication burdens or discouragement from taking further actions."}, {"title": "5.1 Low-quality support during interactions", "content": "Critical situations are high-risk events that demand immediate and effective responses to mitigate harm, ranging from urgent emergencies such as overdoses or heart attacks to sensitive issues like domestic violence and stigmatized conditions. In the IPV sessions, we tested one online question, \"Can a toxic partner be nice for a long period of time?\" and LLM answered \"toxic relationships can change if both partners are equally committed to doing so. This requires a lot of work, including: open communication, honesty...\" Both professionals and experiencers reacted strongly to this answer as it failed to identify the potential severity that necessitated follow-up questions or acknowledgment of the spectrum of toxic behaviors. Our participants explained that \"it doesn't understand that this is an abusive situation, and that someone should be redirected to like a national line, or like someone else to talk to, versus just getting advice on a relationship.\" (P5) because \"if somebody is searching 'toxic' and they might mean also 'abusive' and there's a difference\" (E1)."}, {"title": "5.1.2 Impaired health decisions and emotional status", "content": "People's health decisions and emotional status can be negatively affected by LLMs' low-quality responses, which can result from uncertainties in LLM generations, lack of context-sensitive understanding, or inability to provide personalized advice. This risk is further complicated when LLM responses seem appropriate on the surface but require personal experience or expertise to identify underlying issues. If people follow poor advice, their physical health may be jeopardized by misguided health decisions, as preventable issues could lead to costly or serious outcomes. For example, guidance on OUD from LLMs could be dangerously inadequate, potentially resulting in severe repercussions of overdose or death. Mental wellbeing can also be at risk if LLMs cause health anxiety or trigger previous or new negative experiences. E9 (VAC) observed that LLMs often list all possible causes of a symptom, which tends to cause unnecessary anxiety. When it comes to mental health-related issues, our professional participants invest considerable effort and training to understand the communities they work with and accumulate experience through interactions, as it is nearly impossible to avoid triggering individuals with unknown backgrounds and histories."}, {"title": "5.2 Inadequate or biased support", "content": "LLMs may provide inadequate support for less common needs because they tend to primarily reflect majority perspectives or common patterns due to the generalized training and probability-based nature [70]. P1 (IPV) gave an example of an issue that is underrecognized yet common in reality: pets in IPV situations, where many shelters accept survivors and children but not pets.\nBesides the potential biases in data representation, it is also challenging for LLMs to provide support in a culturally-sensitive manner. Customized communication is an unavoidable task in public health [30], as explained by P5 (OUD), \u201cmy denominator is a million people. It's not my 2000 people in my panel. So when I look at communication, we do stratification based on who we're communicating with, and what information we think they need to make appropriate decisions.\" One example mentioned by P1 is that certain religions may hold a belief that divorce is wrong and women should tolerate abuse, which requires additional care in communication to sensitively address potentially harmful cultural biases that may even be held by the individuals themselves. These individual nuances necessitate efforts ranging from language adaptation to community understanding. In practice, professionals may take special training or conduct focus testing to ensure communication is appropriate and digestible to the communities. For instance, P6 (OUD) mentioned that their team required training in mental health first aid and LGBTQ bias before working with LGBTQ communities, and P8 (VAC) conducted many focus groups in work to create and evaluate vaccine education materials. In LLMs, however, language is standardized and formal without customizations to different audiences.\nAnother risk is that misleading or biased narratives could be amplified and even internalized by individuals, as LLMs tend to reflect prevalent misconceptions or follow user inclinations. E5 (OUD) imagined that opioid use could be glorified because of certain social media discussions or inclined prompts. Moreover, if the 'kernels of truth' behind"}, {"title": "5.3 Additional barriers to help-seeking", "content": "Many participants worried about the additional communication barriers and burden required from users, as LLMs need users to be proactive in initiating communication and form proper questions to get good answers. P5 (OUD) and P1 (IPV) raised the question of whether people know how to ask the right questions, explaining that: \"I thought about my clients, and how they think, how they speak, how they talk, and their language. [...] in general, having a properly asked question is not my normal client. So they're not normally gonna know just the questions to ask. [...] It's not gonna be a properly put out question and get a proper answer. \" (P1).\nE7 (OUD) talked about his experience interacting with LLMs and confirmed this communication challenge. He pointed out the challenge in phrasing precise and explicit questions, especially when people may not know the medical terminology, what symptoms or aspects of personal history are relevant, or how to articulate their needs effectively: \"You have to be so specific with the prompts to get the answer you want. The average person isn't going to think about that when they come up to it. They're gonna say, Oh, I can use this to see how to get off \u2013 [...] What coping skills do I use for the next month to be able to get off heroin or opioids? You know (the answer) it's gonna be right for some, but not right for all of them. Not everybody's going to know what to type, what to ask. You need to be [...] incredibly, anatomically specific.\""}, {"title": "5.3.2 Discouragement from support-seeking and further actions", "content": "Participants feared that reluctance to seek assistance and tendencies toward self-isolation in difficult situations may be reinforced, while LLMs may outright deny assistance or provide unactionable or unempathetic guidance. People in mental health crises or facing difficult circumstances tend to withdraw from social relationships [44, 60], even though these moments are when support networks and professional assistance are of utmost importance [66]. If LLMs present a seemingly \"easier\" alternative that offers human-like interactions without the complexities of human judgment or engagement, people could become inclined to rely on LLMs with less motivation to seek real-world help or take necessary actions. E3 (IPV) explained how hard it was for her to open up to others, \u201cit just kind of put me in a ball, and then everything just kind of came crumbling down [...] and I shut down.\" E5 (OUD) and P5 (OUD) emphasized the importance of human connections in dealing with mental health challenges or substance use disorders, and worried that technology could take away one's ability to connect with people who can offer genuine help. Prior research has identified social isolation as an important risk factor for mortality [27] and highlighted the critical role of social support network [4, 56] that technology would be shy to replace. IPV experiencers worry that technology could become \"an unhealthy coping mechanism\u201d (E3). E1 (IPV) explained that while AI might ease basic emotional needs, it cannot go beyond users' input and offer the proactive outreach and encouragement that real human support can provide: \"I had been in a situation where I both knew I needed to leave and didn't want to. And the less I sought out connections and help from other real-life people, the more I could lean into wanting to stay [...] we can talk about adding support and feeling heard, but that's not going to get you out. I can't imagine if we did a study that's going to get you out in 99% of cases.\"\""}, {"title": "6 Dimension 2: Risks to Human-centered Care", "content": "As shown in Table 4, participants expressed concerns about the risks to the healthcare ecosystem that highly values patient-centeredness and shared decision-making, reflected in 1) degradation of patient-provider trust and social support system, 2) missed opportunities to proactively introduce help, and 3) dehumanization and impersonality in care."}, {"title": "6.1 Degradation of patient-provider trust and support system", "content": "Participants imagined that patient-provider trust can be at risk if patients suspect doctors are relying on Al rather than their own expertise for answers, or if LLM responses conflict with their provider's recommendations. If people start to take LLM responses as absolute facts, they may start to become doubtful of what doctors say, especially when it differs from AI generations. Even in cases when there is no conflict, E10 (VAC) highlighted the inability to tell if opinions are made based on professional knowledge or AI assistance would still complicate trust, because \"if you do decide to go to like the doctor, and the doctor is telling you what's wrong. You may wonder 'okay, did he get his answer off ChatGPT, or is that basically based off knowledge. You just kind of never know.\"\nLLMs could become a misplaced substitution for human interactions and professional resources, particularly in situations that require complex emotional support, critical decision-making, or nuanced advice. Many participants raised concerns that people may start to rely on technical support with flaws and choose not to consult with doctors or go to hospitals. Even more troubling, participants feared users could become anchored to a technology that is incapable of understanding personal contexts or intervening when necessary: \u201cI do see how one can get kind of anchored, how stuck in a sense, especially when they don't feel as if there are any other resources, or they're just not ready to go out [...] that push needed is not really given, and they're comfortable, like I know I can definitely get.\u201d \u2013 E3 (IPV)"}, {"title": "6.2 Missed opportunities to proactively introduce help", "content": "Our professional participants highlighted people's \u201creadiness\u201d for intervention-where people have mental burdens or have other problems in life to be prioritized, despite how much health professionals want to offer help. P3 (IPV), who is a social worker, said that \"one of the comments that really struck me was 'I was in the ER after an overdose, and I found out I was pregnant, and the social worker offered to call for help with me, but I wasn't there yet [...] the phone just felt so heavy, like I just felt so heavy to make that call.' [...] - You (professionals) want to do the intervention right there, but a lot of times people aren't ready for that.\"\nIn situations like this, providers are expected to take a proactive role and decide when are appropriate times to initiate conversations and introduce help. For example, P1 (IPV) mentioned that during annual checkups at the obstetrics office, health professionals would encourage individuals to share anything happening at their home or in their relationships in a setting where they are alone and safe. Similarly for OUD, P5 (OUD) said her public health department had peer navigators at emergency rooms because \"the belief is that at that time the person may be interested in MOUD (medications for opioid use disorder). They may be interested in some life changing, because they may have almost just died.\" However, in technology support, especially with general LLMs, providers' proactive role is diminished, and individuals are less likely to receive help unless they actively and intentionally seek it."}, {"title": "6.3 Dehumanization and impersonality in care", "content": "If people become accustomed to LLMs' linguistic style, communication integrations or health professionals may unintentionally adopt similar impersonal and pragmatic communication styles. This adoption can overlook the emotional care and hurt the empathy and trust building with patients. For example, P4 (IPV) said that \"if you're someone just getting all your information or used to like using Al a lot, your own empathy might decline for a survivor or for other people, because maybe you're used to just reading what Al has to say, and this kind of pragmatic responses are not the most empathetic.\" Our participants emphasized that trust-building in healthcare is essential to making people feel respected and heard. P8 (VAC) highlighted the importance of communication strategies that consider emotional needs in meeting informational inquiries. She gave an example that when people come to providers with misinformation, the best way to reply is to acknowledge people's concerns and ask for their permission to share counter-evidence because it \"opens a door and acknowledges that you know that information sounds scary, and then also opens the door for a conversation.\nHowever, our participants found LLM responses lacking this emotional care, describing them as analytical, robotic, and not personable. Specifically, participants made a note that the list format contributed to a sense of \"robotic and not very like human\u201d (E6, OUD); instead, they prefer answers that are more empathic \"I want you to tell me 'Okay, this is how you'll feel.' Don't give me a 1, 2, 3, 4, 5 tab list.\u201d (E7, OUD). This lack of empathy stands out more when handling sensitive situations where the issue needs to be taken seriously and carefully. In the IPV session, E3 (IPV) commented on LLM answers as \u201ca little too happy\u201d in handling questions about potentially abusive behaviors. P4 and P2 (IPV) further noted the extra care needed to prevent the continued normalization of harmful behaviors or beliefs. For example, our participants in IPV sessions pointed out that many survivors are normalized to violence to the point that their understanding of what constitutes acceptable behavior has been distorted. For instance, for the LLM answer \u201c it sounds like you're dealing with a challenging and unhealthy situation"}, {"title": "7 Dimension 3: Risks to Information Ecosystem", "content": "As shown in Table 5, participants highlighted potential negative consequences for the information ecosystem, empha-sizing: 1) degradation of overall information quality due to empowered misinformation creation and enhanced echo chambers, 2) erosion of critical thinking caused by over-trust in AI and increased difficulty in evaluating claims, and 3) further inequities in information access and literacy."}, {"title": "7.1 Degradation of overall information quality", "content": "LLMs have the potential to assist the creation and dissemination of misinformation due to their ability to generate large amounts of content quickly and convincingly. Without the guarantee of information quality and proper oversight, LLMs' accessibility could empower falsehoods to appear credible or prevalent. P9 (VAC) worried the lagged media literacy could contribute to the risks: \"people will have an increased need for media literacy [...] there's a lot of misinformation and disinformation, the accessibility of Al makes basically anyone able to pump out a lot more of that [...] (by) being able to just say 'make me some content or a text post, says these things, and make a hundred of those right now' and it will do it. [...] People will need to know how to find a trustworthy source. And also how to weed that out of a cloud of noise with a bunch of different ideas.\" Misinformation can gain traction with a false sense of majority and spread under a false sense of credibility, allowing malicious individuals to distort words to their advantage. Moreover, E1 (IPV) also brought up the concern that individuals or groups with malicious intent can craft misleading or biased narratives to target views they"}, {"title": "7.1.2 Creation of echo chambers", "content": "LLMs may create echo chambers by reinforcing users' preexisting beliefs or attitudes. Since LLMs tend to follow users' tone and prompts, they can inadvertently perpetuate misconceptions, with a reduced likelihood of exposing users to counter-evidence or alternative perspectives. E5 (OUD) said \u201cI think a lot of times they ask the wrong questions or [...] lead it on to give a specific answer that they're not necessarily looking for.\" As an example, in the OUD sessions with insiders, we tested a question about the benefits of using opioids for the long-term management of chronic pain. The LLM answer highlighted a range of benefits, from effective pain relief and psychological comfort to enhanced physical function and quality of life, while failing to adequately unpack the potential risks. A domain expert noted that this was perhaps the most positive tone she had ever encountered in any informational materials on the subject. This optimistic framing could unintentionally reinforce a user's belief that opioids are the best or only solution. Surprisingly, none of our experiencer participants flagged concerns with this answer, which we believe indicates a gap in recognizing the potential harm that such overly positive portrayals may cause.\nMoreover, LLMs often don't encourage back-and-forth conversations and tend to go along with the initial attitudes or assumptions in prompts [62], thus limiting the opportunities for users to engage in deeper inquiry and critical thinking. Over time, this reliance on easily accessible answers could reduce individuals' ability to critically assess complex health information and make informed decisions."}, {"title": "7.2 Erosion of critical thinking", "content": "Participants expressed concerns that users may develop an illusion of knowing when presented with LLM-generated answers that appear highly organized and confident [82], potentially leading them to believe they fully grasp a topic or issue. P6 (OUD) worried that \"People are gonna feel empowered like: I know this stuff. Now I have the knowledge. AI has given it to me. So it must be true.\" This illusion may be worsened when LLMs do not explicitly indicate uncertainty or limitations. P6 pointed out the common false perception that AI is comprehensive and free of bias that \"there's a level of people expect it to be bias-free[...] But this is literally built on human-created literature, and humans are inherently biased.\" Moreover, our participants noted that the structured, list-like format often used by LLMs could suggest a sense of comprehensiveness with ranked importance, giving users a false sense of competence. This false confidence could lead to misguided actions or the unintentional spread of low-quality information to others.\nSome participants worried that LLMs may become the default for information-seeking behaviors without understand-ing the limitations and mechanisms behind LLMs. People can mistakenly equate linguistic quality with information quality where the seemingly authoritative tone and potentially non-existent evidence can make users more tempted to accept without question. Some of our general user participants displayed flawed perceptions of LLM capabilities and tendencies to anthropomorphize LLMs with human emotions. For instance, E10 explained her confidence in LLMs because \"it's a lot of work that went into ChatGPT, so I feel like a lot of the responses and feedback that it will give back should be pretty valid.\u201d. Meanwhile, E3 shared her experience interacting with an agent and her reluctance to correct the agent because she \u201cdidn't want to hurt her feelings.\""}, {"title": "8 Dimension 4: Risks to Technology Accountability", "content": "As shown in Table 6, participants pointed out the uncertainties and concerns about technology accountability, highlighting 1"}]}