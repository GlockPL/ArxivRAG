{"title": "BugSpotter: Automated Generation of Code Debugging Exercises", "authors": ["Victor-Alexandru P\u0103durean", "Paul Denny", "Adish Singla"], "abstract": "Debugging is an essential skill when learning to program, yet its instruction and emphasis often vary widely across introductory courses. In the era of code-generating large language models (LLMs), the ability for students to reason about code and identify errors is increasingly important. However, students frequently resort to trial-and-error methods to resolve bugs without fully understanding the underlying issues. Developing the ability to identify and hypothesize the cause of bugs is crucial but can be time-consuming to teach effectively through traditional means. This paper introduces BugSpotter, an innovative tool that leverages an LLM to generate buggy code from a problem description and verify the synthesized bugs via a test suite. Students interact with BugSpotter by designing failing test cases, where the buggy code's output differs from the expected result as defined by the problem specification. This not only provides opportunities for students to enhance their debugging skills, but also to practice reading and understanding problem specifications. We deployed BugSpotter in a large classroom setting and compared the debugging exercises it generated to exercises hand-crafted by an instructor for the same problems. We found that the LLM-generated exercises produced by BugSpotter varied in difficulty and were well-matched to the problem specifications. Importantly, the LLM-generated exercises were comparable to those manually created by instructors with respect to student performance, suggesting that BugSpotter could be an effective and efficient aid for learning debugging.", "sections": [{"title": "1 Introduction", "content": "Debugging is an essential skill for programming, yet there is little consistency in how it is taught [41]. A landmark review by McCauley et al. covered various educational perspectives on debugging, highlighting that it is both difficult for novices to learn and challenging for computer science educators to teach [25]. Similar work has explored common difficulties faced by students when learning debugging [12]. For example, Whalley et al. observed that novice programmers often employ vague and imprecise methods for hypothesis generation and verification, relying heavily on guesswork rather than using a systematic approach [41].\nWith large language models (LLMs) capable of automatically generating code, the ability for students to identify and debug errors becomes even more critical [11]. Educators must ensure that students are equipped with the skills to critically evaluate and correct LLM-generated code. Traditional debugging tasks typically involve presenting students with buggy code (i.e., code containing bugs) and asking them to identify and fix the issues. Frameworks for teaching debugging have also been proposed, providing more structured approaches to enhance debugging instruction [21]. Recent work by Ma et al. also highlights the importance of training students to hypothesize the causes of code errors, encouraging them to develop and test hypotheses about code defects systematically [23].\nCreating a diverse range of debugging exercises that offer automated feedback could make debugging instruction more consistent and frequent. In this paper, we present a new tool called BugSpotter which is designed to generate debugging exercises using LLMs. BugSpotter generates buggy code from problem descriptions and verifies these bugs with a test suite. Students interact with the tool by designing failing test cases, which not only helps them practice debugging but also improves their ability to read and understand problem specifications. This method aligns with work on metacognitive scaffolding which has demonstrated the value of test case generation for helping with problem understanding [10].\nFigure 1 illustrates a debugging exercise generated by BugSpotter, with more information about how these exercises are created in Section 3. We deployed BugSpotter in a large classroom setting and conducted a comparative analysis between LLM-generated debugging exercises and those manually created by instructors. We evaluate our experience of using BugSpotter by answering the following questions:\n\u2022 RQ1: How do experts evaluate the quality of the debugging exercises generated by BugSpotter?\n\u2022 RQ2: How does the difficulty of the generated exercises vary as measured by students' performance on the exercises and expert-classified difficulty?\n\u2022 RQ3: How does student success on LLM-generated debugging exercises compare to that on instructor-designed exercises?"}, {"title": "2 Related Work", "content": "The importance of debugging. Previous works have emphasized the importance of students acquiring debugging skills [4, 16, 41]. There are various strategies that students can use [12, 24, 27], yet they usually rely on guesswork [41]. Explicitly teaching students debugging strategies has been demonstrated to be an effective method for promoting a more systematic approach [17]. Despite this, both learning and teaching debugging remain challenging [41], thus novel frameworks for teaching debugging are necessary [21]. Notably, the process of fixing the bug when its location is known is easily carried out [12], but locating the bug and understanding the functionality of the buggy code are considered difficult tasks [25]. Our work focuses on automatically generating buggy codes for which students design failing test cases, practicing their skills for problem understanding, bug localization, and code comprehension.\nReducing educator workload. Reducing the workload of educators is an ongoing endeavor in the education literature. One early initiative at curating a large reusable repository of multiple-choice questions for computer science is the Canterbury QuestionBank [37]. Other approaches include student sourcing [9], or designing tools to help educators create a range of programming-related exercises [3, 22]. A recent line of research focuses on the automation of exercise creation, especially in introductory block-based visual programming domains [1, 13, 32, 33]. Finally, in the era of LLMs, a suite of new tools leveraging generative models have been proposed [14, 15, 35, 38]. For example, HypoCompass [23] makes LLMs act as novices seeking help, placing students in the role of tutors. In contrast, BugSpotter generates debugging exercises focusing on the creation of failing test cases, allowing students to practice problem and code comprehension, along with bug localization.\nLLMs for programming education. Besides exercise creation, LLMs have been explored for other educational purposes [7, 26, 35], such as generating natural language feedback for students [18, 20, 34, 36]. Moreover, Nguyen et al. focus on generating buggy student attempts for block-based programming [28]. Recently, the concept of prompt problems was proposed as a paradigm shift in this field [8, 39]. BugSpotter uses LLMs to address a less-explored topic: the generation of debugging exercises."}, {"title": "3 BugSpotter: Methodology and Tool", "content": "The BugSpotter tool aims to help students read and understand problem specifications, reason about buggy code, and design test cases that reveal bug(s). When interacting with BugSpotter, a student is shown a code debugging exercise, comprising a problem specification and a buggy code for the problem. To solve this, the student has to design a test case that will reveal the error in the code (see Figure 1). In the following, we introduce debugging exercises and then present our approach for generating such exercises using LLMs.\n3.1 Code Debugging Exercises\nWe now describe the typical flow of a code debugging exercise generated with BugSpotter. Let us first define a problem P, a buggy code CB, and its fixed version CF. A problem P comprises a problem specification and a test suite. A buggy code CB fails at least one test case in P's test suite. Its fixed version CF can be obtained by making small modifications to CB in order to pass P's entire test suite.\nA student is presented with a specification of a problem P and corresponding buggy code CB. The problem specification requires implementing a single function, and CB represents a buggy version of this function. Figures 1a and 1b show the specification for the \"Sum Positives\" problem along with an example buggy code CB. The student's objective is to provide a failing test case that reveals why CB is incorrect. The test case should include an input to the function in CB, the incorrect output from the buggy function, and the expected correct output as defined by the problem specification. The student successfully solves the exercise if the following criteria are met:\n(1) the outputs of CB and CF are different when run on the provided input (i.e., the test case is indeed a failing test case);\n(2) the output of CF when run on the provided input matches the provided correct output;\n(3) the output of CB when run on the provided input matches the provided buggy output."}, {"title": "3.2 Exercise Generation Pipeline", "content": "We now give details about BugSpotter's debugging exercise generation process, which comprises two stages: (a) code generation using LLMs and (b) validation of the exercise through execution. The generation process starts with a problem P, encompassing its problem specification and its test suite, as illustrated in Figure 2.\nCode generation stage. We leverage an LLM for generating the buggy code. We provide the LLM with P's problem specification, and prompt the LLM to reason about possible bugs that students may introduce while working on the problem's solution code. We then ask the LLM to generate a buggy code CB, its fixed version CF, and an explanation of the bug. The motivation to ask for reasoning, explanation, and fixed code CF draws inspiration from Chain-of-Thought [40], encouraging the LLMs to carefully reason about the buggy code and how it differs from a correct code. Additionally, CF will also play a role during validation. We enforce JSON formatting for easily extracting the information from the LLM's response. Figure 3 illustrates the prompt used in the tool for generating buggy codes.\nExercise validation stage. As we focus on delivering semantic bugs well-matched to problem specifications, the validation stage is aimed at filtering out syntactic bugs and buggy codes unrelated to the problem. First, we filter out instances where CF or CB do not successfully compile. Second, we check whether CF can correctly solve the given problem using the test suite provided in P. The main intuition behind checking the correctness of CF is to filter out the instances where the LLM was inconsistent and could not generate a fixed version, potentially leading to confusing buggy code that does not match the given problem. Third, we run CB using the same test suite. For CB to be valid for the debugging exercise, it should fail at least one of the test cases in the test suite, either by raising a runtime error or by producing an output different from the expected one. Otherwise, CB would be a correct implementation and we will not use it as part of an exercise. Fourth, we filter out instances that run longer than a given time limit. Finally, if both CF and CB are successfully validated, we deliver the exercise as output. Implementation details. It is desirable to have a diverse set of debugging exercises for a student to work on. For this, we ask the LLM to generate 10 tuples consisting of a buggy code, its fixed version, and corresponding explanation, as illustrated by the prompt shown in Figure 3. We use these tuples to create 10 exercises and keep only those that pass the validation stage. We deliver to the student the first exercise that passes validation, while caching the rest"}, {"title": "4 Evaluation Procedure", "content": "In this section, we describe our evaluation of BugSpotter which includes an expert assessment of generated exercises and a large-scale classroom deployment to compare exercises generated by BugSpotter with instructor-created exercises. The evaluation is done over 3 different problems, shown in Figures 1a, 7a, and 8a.\n4.1 Expert Evaluation for RQ1\nOur first research question, RQ1, involves expert-based assessment to evaluate the quality of exercises produced by BugSpotter. We assess the quality w.r.t. several attributes reflecting their suitability for being used in the classroom. For this, we created the following rubric grounded in literature [23, 35], though adapted to capture more features of the exercises generated with BugSpotter. SuccOf10 (0 to 10) reports how many of the 10 generated exercises successfully pass validation. DiverseCodes (0 to 10) reports how many of the generated exercises that pass the validation stage contain unique bugs. BugProbRelated (binary) is 1 when the buggy code CB is an attempt at solving problem P, and 0 when it is not related to P (i.e., trying to solve a different problem); we compute this attribute only for the exercises passing validation. NbBugs (positive number) reports the number of conceptually different bugs present in CB; we compute this attribute only for the exercises passing validation. EditTokens (non-negative number) captures the edit distance between strings obtained by tokenizing CB and CF using the Pygments library [2]; we compute this attribute only for the exercises passing validation. BugType (a one-hot encoded vector) characterizes the kind of behavior CB exhibits when executed on problem P's test suite; we compute this attribute only for the exercises passing validation. In particular, BugType categorizes exercises into the following three types:\n\u2022 Type 1 means that buggy code CB passes at least one test case from problem P's test suite, and does not result in a run-time error or division by 0 on any test case (see Figure 7).\n\u2022 Type 2 means that buggy code CB does not pass any of the test cases from problem P's test suite, and does not result in a run-time error or division by 0 on any test case (see Figure 1).\n\u2022 Type 3 means that running buggy code CB results in a run-time error or division by 0 on a test case (see Figure 8).\nMetrics SuccOf10, EditTokens, and BugType can be computed au-tomatically using scripts with no manual annotations. For the rest of the attributes, two human experts (evaluators) independently rated the generated exercises - these two evaluators are experts in programming, with one evaluator having experience tutoring introductory-level programming courses. We obtained a Cohen's kappa reliability value greater than 0.7 for each rated attribute, indicating substantial agreement between evaluators [6]. All results are reported based on average across annotations of the two evaluators."}, {"title": "4.2 Classroom Evaluation for RQ2 and RQ3", "content": "Our next two research questions, RQ2 and RQ3, involve assessing students' success on exercises generated with BugSpotter. We aim to understand the alignment between expert-classified difficulty of debugging exercises and the students' performance on the same exercises. Furthermore, we analyze whether there are significant differences between exercises generated with BugSpotter and exercises designed by instructors. To enable this, we deployed a version of BugSpotter as part of a laboratory task in a large introductory C programming course involving 741 students, taught at the University of Auckland. Students in this course typically have no prior programming experience. The lab was conducted towards the end of the course, and students were assigned 3 debugging exercises based on the 3 different problems we used throughout our study.\nFor the classroom deployment, we first generated exercises with the BugSpotter pipeline employing various LLMs from the GPT family [31]. Then, we pre-selected 5 exercises (instead of using real-time generation) to control the generation quality and avoid impacting the students' learning experience - the pre-selection was done to ensure all the exercises are of high-quality and diverse based on the rubric established above. For RQ3, an instructor created an additional 5 exercises by modifying reference solution code to include realistic bugs. As part of the deployment, students were assigned randomly to one of these 10 exercises (5 LLM-generated and 5 instructed-created). To explore the impact of both difficulty and source of buggy code on student performance, we collected student responses (i.e., test cases) and automatically computed their success rates on debugging exercises for our evaluation."}, {"title": "5 Results", "content": "In this section, we discuss the results of the study centered around the research questions (RQs) introduced in Section 1.\n5.1 RQ1: Expert-assessed Quality and Diversity\nWe present results in terms of quality and diversity of generated exercises. Figure 4 shows the performance of BugSpotter when using GPT3.5 [29] and GPT40 [30] as LLMs for generating buggy codes. We conduct 3 independent runs to report averaged results as mean (stderr) for all attributes based on annotations by two evaluators. Our results show that GPT3.5 is comparable with GPT40 in terms of creating a diverse set of debugging exercises that are well-matched to the problem. Furthermore, GPT3.5-generated exercises pass the validation step more often than those created by GPT40. Next, we see that GPT40 predominantly creates exercises with buggy codes that pass at least one of the problem's test cases (i.e., Type 1), while GPT3.5 has a higher tendency to create buggy codes that pass none or result in an error, bringing more diversity w.r.t. the types of produced bugs. In terms of diversity of exercises passing validation, both LLMs are comparable for creating sets of diverse buggy codes. Finally, we notice that GPT3.5 has a slightly higher number of conceptually different bugs per buggy program than GPT40. This leads to GPT3.5-produced buggy code requiring more edits to fix, signaling bugs that are more difficult.\nThese results highlight that both LLMs are suitable for the debugging exercise generation pipeline. Surprisingly, GPT3.5, the cheaper alternative, can produce exercises comparable in quality to GPT40. This means that GPT3.5 can be used to power BugSpotter as a scalable solution in terms of cost, without compromising quality.\n5.2 RQ2: Expert-assessed Difficulty and Student Performance on Debugging Exercises\nFor each problem, we analyze the 5 pre-selected exercises generated by LLMs as described in Section 4. Difficulty is often a key factor in deciding whether exercises should be assigned to students [19], so it is important for BugSpotter to produce a range of difficulties from which students can choose. We study this diversity from both"}, {"title": "5.3 RQ3: LLM-Generated vs. Instructor-Created Debugging Exercises", "content": "Next, we compare students' performance on the debugging exercises containing buggy codes generated by LLMs from the GPT family with those handcrafted by an instructor. We aim to explore whether there is any significant difference in students' performance in solving debugging exercises, depending on the source of the buggy code. Again, we analyze students' performance in terms of their success rate. Figure 6 shows students' success rate for each problem and aggregated over all problems. The results show that the success rates for debugging exercises with codes created by the instructor are slightly higher, indicating that they are less difficult. To check whether this slight difference is significant, we compare students' performance for LLM-generated exercises with instructor-created exercises via the $\\chi^2$ test [5], using contingency tables with two rows (source) and two columns (741 data points per problem mapped to successful/unsuccessful). As shown in Figure 6, all p-values exceed 0.05, with the lowest p-value being 0.065, indicating no significant difference between the two sources.\nWe believe that these results strongly suggest that debugging exercises generated with the help of LLMs are comparable in difficulty to those handcrafted by instructors. This shows the potential of using LLMs to automate the creation of such exercises in class, reducing the workload on educators. The slight difference in success rate, while not significant, may indicate that buggy codes created by the instructor are more naturalistic, aligning more closely with the types of errors students encounter while coding on their own."}, {"title": "5.4 Web Application", "content": "A free web application showing the capabilities of BugSpotter can be found at https://bugspotter.netlify.app/. This web application provides a demo similar to the format of the illustrative examples shown in Figures 1, 7 and 8. After providing a test case, a student can submit their answer for automatic validation. They can also request additional debugging exercises with the press of a button - after a short wait, BugSpotter delivers a new exercise. This demo application supports Python debugging exercises and we plan to add support for C debugging exercises in the future."}, {"title": "5.5 Limitations", "content": "Next, we discuss a few limitations of our study. Firstly, our classroom investigation was done using pre-selected exercises (instead of using real-time generation) to control the generation quality for the laboratory task. For future studies, it would be more informative to analyze students' performance on buggy exercises generated with LLMs in real-time. Secondly, we did not correlate students' success rates for debugging exercises with their long-term course performance. This would be a good indicator of the exercises' suitability for being included in classroom assignments. Thirdly, our study did not capture how BugSpotter could help educators in terms of reducing their workload in creating debugging exercises. This would be needed to fully understand the practical utility of BugSpotter."}, {"title": "6 Concluding Discussion", "content": "We developed BugSpotter, a tool for debugging practice with LLM-generated buggy code for problems that require implementing a single function. We tested the generated exercises' quality via both expert-based and classroom evaluations. As a first finding, our results show that with minor differences, both GPT40 and GPT3.5 produce diverse sets of buggy codes that can be leveraged by BugSpotter's debugging exercise generation pipeline. Our second finding is that there is a high alignment between expert-assessed difficulty and students' performance on generated exercises. This underlines that LLMs are suitable for generating diverse sets of exercises with varying difficulty. Our third finding obtained through further investigation of the classroom setting shows that exercises generated by LLMs are comparable in difficulty to those created by instructors. Statistical analysis shows no significant difference between the"}]}