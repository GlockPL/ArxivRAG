{"title": "RecSys Arena: Pair-wise Recommender System Evaluation with Large Language Models", "authors": ["Zhuo Wu", "Qinglin Jia", "Chuhan Wu", "Zhaocheng Du", "Shuai Wang", "Zan Wang", "Zhenhua Dong"], "abstract": "Evaluating the quality of recommender systems is critical for algorithm design and optimization. Most evaluation methods are computed based on offline metrics for quick algorithm evolution, since online experiments are usually risky and time-consuming. However, offline evaluation usually cannot fully reflect users' preference for the outcome of different recommendation algorithms, and the results may not be consistent with online A/B test. Moreover, many offline metrics such as AUC do not offer sufficient information for comparing the subtle differences between two competitive recommender systems in different aspects, which may lead to substantial performance differences in long-term online serving. Fortunately, due to the strong commonsense knowledge and role-play capability of large language models (LLMs), it is possible to obtain simulated user feedback on offline recommendation results. Motivated by the idea of LLM Chatbot Arena, in this paper we present the idea of RecSys Arena, where the recommendation results given by two different recommender systems in each session are evaluated by an LLM judger to obtain fine-grained evaluation feedback. More specifically, for each sample we use LLM to generate a user profile description based on user behavior history or off-the-shelf profile features, which is used to guide LLM to play the role of this user and evaluate the relative preference for two recommendation results generated by different models. Through extensive experiments on two recommendation datasets in different scenarios, we demonstrate that many different LLMs not only provide general evaluation results that are highly consistent with canonical offline metrics, but also provide rich insight in many subjective aspects. Moreover, it can better distinguish different algorithms with comparable performance in terms of AUC and nDCG. Our codes are publicly available at https://github.com/anonyProjects/RecSys-Arena.", "sections": [{"title": "1 INTRODUCTION", "content": "Accurate and comprehensive evaluation of recommendation algorithms is essential in practical recommender system design and optimization [5, 54]. However, recommender system evaluation is very challenging due to the complexity of user feedback and rapid shift of data distribution. Although online experiments such as A/B testing can give direct assessments about the overall performance of different recommendation algorithms, it is relatively time-consuming to accumulate sufficient user behaviors to obtain confident results. Moreover, many mainstream metrics such as click-through rate (CTR) do not fully reflect user satisfaction and long-term user experience.\nTo help system designers optimize their algorithms, researchers usually evaluate the performance of recommender systems based on offline user behavior logs to obtain preliminary assessments [6]."}, {"title": "2 METHODOLOGY", "content": "In this article, we propose a novel and practical approach, called RecSys Arena, to utilize the LLM to conduct pair-wise evaluations of the two recommender systems. The entire evaluation process consists of four steps. First, two recommender systems generate recommendation result lists for the same user. Next, user information, recommendation results, and descriptions of the evaluation aspects are integrated to construct a prompt. The prompt is then input into a large language model to obtain qualitative analyses and quantitative comparison results for each evaluation aspect. Finally, an evaluation report is produced."}, {"title": "2.1 Problem Formulation", "content": "We use $U = (u_1, u_2, ..., u_u)$ to denote the set of users in a recommender system (RS). Input to the RS includes the user's personal attribute information S and viewing histories H of users respectively. The RS recommend multiple items to each user u, which are defined as $I_u = (i_1, i_2, ..., i_{\\vert I_u \\vert})$.\nGiven two RSS $R_A$ and $R_B$, we use $I_{R_A}$ and $I_{R_B}$ to represent the corresponding recommendation result lists generated by systems $R_A$ and $R_B$, respectively. Let f(.) represent the evaluation method. The pair-wise evaluation results of systems $R_A$ and $R_B$, as provided by the LLM, can be expressed as:\n$f_{LLM} (U, R_A, R_B) = LLM(S_U, H_U, I_{R_A}, I_{R_B}, P)$ (1)\nwhere P denotes the prompt template.\nThe primary goal of pair-wise LLM-based evaluation is to 1) measure, from the user's perspective, which recommender system, $R_A$ or $R_B$, has better overall performance for the same user. Note that the overall performance refers to the comparison results that take into account multiple evaluation dimensions (to be introduce in 2.2). Along with the measurable overall performance, 2) LLM-based evaluation method also reports a detailed, interpretable qualitative analysis explaining the evaluation reasons for each dimension, which could facilitate the developer to further make targeted improvements to the recommender system."}, {"title": "2.2 Evaluation Dimensions", "content": "To address the issue that existing offline evaluation metrics cannot evaluate the quality of recommendation results from the user's perception, we primarily focus on user experience when designing the evaluation dimensions. We assume that users of the recommender system serve as the most accurate evaluators of the recommendation results. We consider both mainstream dimensions of concern (e.g., accuracy, satisfaction) and dimensions that traditional offline metrics cannot evaluate (e.g., inspiring content, positive impact).\nTherefore, for the paired recommendation result $I_{R_A}$ and $I_{R_B}$, the LLM is asked to give a comparative evaluation result from the following 6 aspects:\nAccuracy: This recommendation result list aligns well with my interests.\nSatisfaction: I am satisfied with the recommendation results provided by this recommender system.\nInspiration: The recommended items inspire me to think, promote further exploration, and enhanced my willingness to interact with the recommendation platform.\nContent Quality: The recommended items are of high quality.\nTransparency: The recommendation results are associated with one of my personal information or an interaction history, and it is evident which feature is relevant.\nImpact on users: The impact of this recommendation result list on me is positive."}, {"title": "2.3 Prompt Construction", "content": "In the section, we mainly introduce how we construct the prompt P. The prompt is designed to guide LLM to relatively evaluate the quality of pair-wise recommendation results from specific aspects, based on the user profile and viewing histories. As shown in Figure 2, the prompt P includes five key components.\nIn the first part of the prompt, we leverage the role-play capability of LLMs to facilitate a personalized evaluation of recommendation results. To do this, we provide the LLM with the user's personal attribute information, including age, occupation, gender, and other relevant details. The second part of the prompt consists of the user's viewing history, such as information on movies they have watched or news they have clicked on. This content is included to allow the LLM to perceive the user's preferences, enabling it to conduct subsequent evaluations from the user's perspective. Please note that the MIND dataset does not contain any personal user information. Instead, we provide the historical records of news articles that users have browsed, allowing the LLM to understand the user profile. This approach also helps the LLM gain insights into user preferences and behaviors. Next, the recommendation results from the two systems, $R_A$ and $R_B$, are presented to the LLM via the prompt. These recommendation results will include specific item information, such as the titles and genres of the movies. In the evaluation section of the prompt, we list the descriptions of each evaluation dimension to assist the LLM in understanding the specific content that requires assessment for each dimension. Additionally, the evaluation dimensions in this prompt template can be dynamically adjusted, further enhancing the scalability of the evaluation framework. This flexibility allows researchers to tailor the evaluation criteria to suit different contexts and objectives, making it applicable across a wide range of scenarios. The main objective of this section is to allow the LLM to make evaluative judgments based on its analysis of each evaluation aspect. We aim to guide the LLM through a step-by-step thought process, similar to the Chain-of-Thought [48]. This method encourages deeper reasoning and enhances the quality of the evaluation by building on prior insights. Finally, the LLM is asked to output the qualitative analysis for each dimension, along with an overall comparative evaluation of the pair-wise recommendation results from systems $R_A$ and $R_B$."}, {"title": "2.4 LLM Evaluator Construction", "content": "We utilize pre-trained LLMs to provide comparative evaluations for paired recommender systems. The LLM receives the user's personal attribute information, viewing histories, and recommendation results from the two recommender systems $R_A$ and $R_B$ under test, accompanied by the prompt P to describe the evaluation instruction. LLMs trained on massive corpora of unlabelled data possess a wealth of general knowledge, which aids them in understanding recommended items, such as movies. The reason for their strong power can be concluded as they do not need task-specific training data and can be pre-trained on tremendous in-the-wild data in a self-supervised manner (a.k.a. pre-training), so that sufficient domain knowledge can be captured [8, 16, 40].\nPrevious research on evaluation based on LLMs has mostly involved absolute evaluation [27, 56], where the LLM assigns scores to specified evaluation content. Our approach differs from previous studies in that we ask the LLM to conduct a comparative evaluation of two recommendation results, thereby providing a relative assessment. LLMs perform better on pair-wise tasks [14]. On one hand, using relative evaluation allows the LLM to simultaneously access information from two recommendation results, facilitating a more nuanced comparative analysis. This enables the LLM to uncover subtle differences and assess how well each result aligns with user preferences. On the other hand, absolute scoring evaluations often provide limited context, making it challenging for the model to identify and distinguish between the merits of individual recommendations. By leveraging relative evaluation, we enhance the LLM's capacity to perform finer-grained assessments, ultimately leading to more accurate and personalized recommendations.\nWe conduct a statistical analysis of the evaluation results generated by the LLM. To measure the degree of victory between the two models more precisely, we designed the quantile Q metric. Specifically, we calculate the quantile Q using the following formula:\n$Q = \\frac{(N_{win} + N_{tie})}{(N_{lose} + N_{tie})}$ (2)\nwhere $N_{win}$ denotes the number of samples in the test set where the RS is deemed to have won, $N_{tie}$ indicates the number of samples where the RSs tied, and $N_{lose}$ represents the number of samples in which the RS lost. A larger value of Q indicates a greater degree of victory for the RS."}, {"title": "3 EXPERIMENTAL SETUP", "content": ""}, {"title": "3.1 Research Questions", "content": "The goal of our study is to investigate the effectiveness of large language models on recommendation performance evaluation. To this end, we propose to answer two main research questions.\nRQ1: What is the overall performance of the LLM-based evaluation method for recommender systems? As the very first RQ, we aim at investigating the feasibility of using LLMs for comparative evaluation of the overall effectiveness of two recommender systems. Additionally, we assess the effectiveness of the LLM-based evaluation by determining whether it aligns with offline usage prediction metrics (i.e., AUC).\nRQ2: What is the performance of the LLM-based evaluation method for recommender systems in different evaluation sub-dimensions? Due to the absence of subjective user labels (e.g., user satisfaction, whether the impact on users is negative or positive, etc.) in the datasets, we are unable to directly assess the effectiveness of LLMs in evaluating sub-dimensions. We selected the dimensions of inspiration and transparency, and designed an indirect meta evaluation strategy. First, among the currently evaluable offline metrics, there is overlap in the content assessed for diversity and inspiration. Therefore, we considered whether the evaluation results for inspiration could align with the offline metrics of diversity. Second, we assessed whether LLMs could correctly evaluate transparency by observing whether LLMs could capture differences in feature selection during training."}, {"title": "3.2 Evaluation Metrics", "content": "We can initially assess the effectiveness of evaluation based on LLMs by determining whether the results provided by the LLMs align with offline metrics. We consider the three popular metrics: AUC (Area under the Curve) [31], nDCG@k (Normalized Discounted Cumulative Gain for the top k recommendations) [24], URD (User Recommendation Diversity) [39]. The first two metrics measure recommendation accuracy, while the last metric measure recommendation diversity.\nAUC [31] measures the area under the receiver operating characteristic curve, plotting the true positive rate against the false positive rate. It is widely used to evaluate recommender systems, indicating how often relevant items are ranked higher than irrelevant ones. A higher AUC reflects better recommendation accuracy.\nnDCG@k [24] is determined using the formula $nDCG@k = \\frac{DCG@k}{IDCG@k}$. Here, DCG@k is calculated as $DCG@k = \\sum_{i=1}^{k} \\frac{2^{p(i)}-1}{log_2 (i+1)}$, where p(i) represents the preference score of the user u for the ith item in the generated recommendation list. IDCG@k represents the DCG value of the ideal k-item recommendation list.\nURD [39] measures the degree of recommendation diversity. It is calculated based on the intra-list similarity [59], using the formula: URD(rl) = 1 \u2212 $\\frac{2}{n(n-1)} \\sum_{i_a \\epsilon r_l} \\sum_{i_b \\neq i_a \\epsilon r_l} Sim(i_a, i_b)$ represents the similarity between item ia and item ib."}, {"title": "3.3 Recommender Systems and Datasets", "content": "In this study, we use two content recommendation datasets (i.e., Movielens and MIND) released by the previous study [22, 50] as our evaluation datasets, which have been widely used in the existing studies [3, 28, 30, 49, 53]. More specifically, MovieLens [22] comprises data from 1 million movie ratings provided by 6,040 users across 3,883 movies. Within this dataset, user's attributes include gender, age, and occupation. MIND [50] is a news dataset and was collected from the user behavior logs of Microsoft News. The dataset contains one million users who had at least five news click records during six weeks from October 12 to November 22, 2019. It contains 161,013 news and 24,155,470 reading records.\nIn our study, we use 5 categories of recommender systems as the subject of the evaluation in total. We considered different types of recommendation models under test: Factorization Machines, the content-based model, the graph-based model, the sequence recommendation model, and the ID-based model. We describe the specific information of the recommendation models as follow:\n\u2022 FM. Rendle et al. propose factorization machine (FM) [41], which improve upon logistic regression models by addressing the challenge of training model parameters in sparse data scenarios. Furthermore, FMs incorporate second-order feature interactions, compensating for the limited expressive power of logistic regression.\n\u2022 NRMS. NRMS [49] is a representative of content-based recommendation models that employs multi-head self-attention mechanisms for encoding content, such as news titles.\n\u2022 LightGCN. LightGCN [23] is a model that simplifies Graph Convolutional Networks (GCNs) by focusing solely on the core component of neighborhood aggregation for collaborative filtering.\n\u2022 SASRec. SASRec [25] is a sequence recommendation model based on self-attention mechanisms.\n\u2022 DeepFM. DeepFM [20] is an extension of Wide&Deep that synergistically integrates factorization machines for recommendation and deep learning for feature learning, emphasizing both low- and high-order feature interactions."}, {"title": "3.4 LLMs", "content": "We conduct experiments with three LLMs, including the open-source LLM and proprietary LLM.\n\u2022 GPT-40 [1]. GPT-40 is a proprietary LLM released by OpenAI. With targeted optimizations, GPT-40 delivers superior performance in generating accurate and contextually relevant responses, benefiting from refined training techniques and updates.\n\u2022 DeepSeek-V2.5 [15]. DeepSeek-V2.5 is a strong Mixture-of-Experts (MoE) langage model that excels in writing and instruction-following. It comprises 236B total parameters.\n\u2022 Llama3.1-8B-Instruct [2]. LLaMA3-8b-Instruct is fine-tuned specifically to follow and execute user instructions more accurately, making it better at handling tasks that involve clear directives or specific commands.\nTo make the output as deterministic as possible, we set temperature=0 when calling the API."}, {"title": "4 EXPERIMENTAL RESULTS", "content": ""}, {"title": "4.1 RQ1: The Overall Performance of Evaluation", "content": "Our main results are displayed in Table 2. Specifically, we conducted pair-wise evaluations by separately comparing the ID-based recommendation model (i.e., DeepFM), content-based recommendation model (i.e., NRMS), sequential recommendation model (i.e., SASRec), and graph network-based recommendation model (i.e., LightGCN) with FM. First, we present the proportions of \"Win\", \"Tie\", and \"Lose\" for each RS relative to FM in the evaluation results provided by the LLM. Then, in Table 2, we calculated the quantile Q (i.e., $(N_{win} + N_{tie})/(N_{lose} + N_{tie})$) shown in Column \"Q\" and listed the rankings of the quantiles Q in Column \"Rank\".\nTo investigate the overall effectiveness of LLMs in evaluating recommendation quality, we use the AUC from traditional metrics as a reference. Specifically, we examine whether the ranking results of recommendation quality provided by LLMs are consistent with the ranking results based on traditional accuracy metric (i.e., AUC).\nImpact of different LLMs. In our experiments, we employed three LLMs: GPT-40, DeepSeek-V2.5-236B, and Llama3.1-8B-Instruct,"}, {"title": "Answer to RQ1:", "content": "The overall evaluation results obtained from the pair-wise evaluation method based on the LLM are consistent with the trends of offline metrics. Therefore, the pair-wise evaluation method based on the LLM can produce reliable evaluation results. Additionally, compared to offline metrics, LLM-based pairwise evaluations provide better discrimination."}, {"title": "4.2 RQ2: The Multiple-Aspect Performance of Evaluation", "content": "The multiple-dimensions we designed in Section 2.2 are intended to assist LLMs in deriving an overall evaluation result. Specifically, they guide the LLM to first consider the comparative evaluation results of these 6 sub-dimensions, and then synthesize them to arrive at an overall evaluation result. This step-by-step thought process is more conducive to obtaining reliable and accurate evaluation results. However, we still need to verify the effectiveness of the sub-dimension evaluations. We chose to validate the dimensions of inspiration and transparency."}, {"title": "4.2.1 Inspiration.", "content": "Table 4 presents the pair-wise evaluation results provided by GPT-40 and DeepSeek-V2.5 in terms of inspiration aspect. Table 4 shows that the evaluation trends provided by the LLM in terms of inspiration align well with the offline metric URD. Furthermore, in specific sub-dimensions (such as inspiration), the LLM's evaluation results also demonstrate better differentiation."}, {"title": "4.2.2 Transparency.", "content": "The transparency metric is primarily used to measure whether the model has utilized sufficient and correct features to infer and predict the recommended item. We assess whether the LLM can effectively evaluate the transparency metric by investigating whether it can perceive changes in the features used during training. Specifically, we trained the DeepFM model using different features. One model was trained considering only the features user_id and item_id. For the training of another model, we included more user features such as age, gender, and occupation. In the Case 1, given a user aged 45-49, the recommendation results from recommender system B (a model trained only with user_id and item_id) included movies in the category of \"Children's\". GPT-40 provided an evaluation on the transparency dimension, stating \"A wins.\" The rationale for this assessment is that Recommender System A's recommendation results are more aligned with the user's age. This example shows that LLMs are capable of distinguishing differences in recommendation results produced by recommendation models trained on different features. Furthermore, the LLM can pinpoint which specific feature is responsible for the observed differences.\nIn the Case 2, the user's specific characteristics are: age 25-34, occupation as a writer, and gender female. However, Recommender System B recommended movies categorized as \"Horror\" to her, a genre that had not appeared in her historical viewing list. From the results of GPT-40, it is evident that LLMs can analyze and perceive that recommending movies categorized as \"Horror\" to this user is inappropriate based on her personal attributes."}, {"title": "Answer to RQ2:", "content": "For sub-dimensions, the LLM-based pairwise evaluation method can provide reasonable and discriminative assessment results."}, {"title": "4.3 Relative and Absolute Evaluation", "content": "To investigate the differences in differentiation between relative and absolute evaluation using LLMs, we conducted a study in which we employed GPT-40 for the absolute evaluation of four recommender systems (i.e., NRMS, LightGCN, SASRec, and DeepFM) with similar AUC values trained on the MovieLens dataset. For each recommendation result list, we assigned a score between 0 and 1 using GPT-40. In Figure 6, we present the normalized results for the offline metric (i.e., AUC), the pair-wise evaluation quantiles Q, and the absolute evaluation scores for the four recommendation models: NRMS, LightGCN, SASRec, and DeepFM. Figure 6 reveals that the LLM-based relative assessment provides better differentiation among these four models. For example, for the two most closely matched models in terms of recommendation accuracy, LightGCN and SASRec, the quantiles Q derived from the LLM-based pair-wise evaluation still provide a better way to distinguish them, capturing the subtle differences between them."}, {"title": "5 RELATED WORK", "content": ""}, {"title": "5.1 Recommender Systems Evaluation", "content": "Evaluation is a well-established core part of the recommendation field. Generally, it aims to measure how well a recommender system can produce a list of ranked items that match the user's preferences. Currently, researchers typically use user clicks and ratings to quantify user preferences. The traditional evaluation process is divided into three steps: sampling evaluation data, designing evaluation metrics, and implementing evaluation methods.\nSampling. For evaluation data, Li et al. [29] study the relationship between sampling and the global top-k Hit-Ratio (HR, or Recall). Li et al. demonstrate theoretically and experimentally that sampling top-k Hit-Ratio provides an accurate approximation of its global exact counterpart and can consistently predict the correct winner. Krichene et al. argue that the sampled metrics can be viewed as high-bias, low-variance estimators of the exact metrics [28]. Therefore, they correct the sampled metrics point-by-point by minimizing criteria that trade-off bias and variance.\nMetrics. Avazpour et al. [4] made a comprehensive summary of the evaluation metrics of recommender systems from 16 dimensions, including correctness, coverage, diversity, trustworthiness, confindence, novelty, serendipity, utility, risk, robustness, learning rate, usability, scalability, stability, privacy and user preference. They also analyzed the positive and negative correlations between the various metrics. To better study the concepts and metrics of recommendation evaluation, it is important to consider what constitutes user satisfaction [43]. The ultimate goal of recommender system is to satisfy the interest of users. Therefore, on this basis, Kim et al. [26] designed a user satisfaction metric that comprehensively considered accuracy and diversity. Zhao et al. [57] studied the recommendation for different domains, the difference and correlation between model effectiveness. Maksai et al. [35] propose a set of evaluation metric system for news recommender system.\nEvaluation Approaches. Evaluation methods can be divided into two categories, evaluation using collected data (i.e., offline test) [12, 19, 37] and evaluation using real user interaction [7, 32, 33, 42, 55]. In addition to using data and calculating metrics in various dimensions, some researchers have introduced methods like preference graphs [12] and counterfactual evaluation [19, 37] to better mine evaluation information. A/B testing is a common test method to verify whether a change will have a significant impact on core metrics. Usually, in industry, we split the traffic (i.e., experimental users) in equal proportions, then apply two sets of schemes, and finally compare the metric results. However, this method of online a/b testing is difficult to implement in academic research. McInerney et al. [37] propose a new counterfactual estimator using interaction causal relationships to reduce variance. Interaction-based evaluation methods can be further subdivided into interaction evaluation methods with real users (i.e., online test) [32, 42] and user simulation [7, 33, 55]. Lu et al. [32] propose a method to manually annotate user preferences for the evaluation of recommender systems. Combining data visualization techniques, \u0160afa\u0159\u00edk et al. [42] proposed a set of highly interactive approaches for the evaluation of recommender systems. However, real user interaction is usually difficult to obtain. The researchers propose a series of simulators of user behavior [7, 33, 55]. Luo et al. [33] leverage reinforcement learning to simulate users' click decisions."}, {"title": "5.2 LLM-based Evaluation", "content": "Recent advances in LLMs have demonstrated impressive capabilities on a broad range of tasks [45, 51, 52]. Previous works have considered using LLMs for evaluation tasks [27, 38, 47, 56]. Kocmi et al. [27] propose using LLMs to evaluate the translation quality. They found that the effectiveness of the LLM-based evaluation is unexpectedly high, even surpassing all existing metric-based evaluation methods. Zhang et al. [56] proposed that certain zero-shot LLMs can achieve comparable or even better evaluation accuracy compared to traditional methods in the task of evaluating recommendation explanation quality. In addition, they claim that using the voting results of multiple LLMs can improve the accuracy of evaluations. Wang et al. [47] proposed utilizing the role-play capability of LLMs and using them as user simulators to evaluate conversational recommendation systems. Recently, there have also been research works to evaluate the quality of other LLMs using LLMs [9]. Additionally, some studies have utilized LLMs for relevance annotation in an IR context [11, 18, 34, 44].\nDifferent from them, 1) evaluating recommendation tasks involves a larger amount of information, requiring a comprehensive consideration of both user and item information. The complexity of user features and the dynamics of user preferences increase the difficulty of assessing the quality of recommendations. 2) In addition to accuracy, recommendation quality assessment involves more dimensions. Since recommender systems are user-facing software, the evaluation results tend to be more subjective, focusing on user experience. Such evaluation reports are more useful for the platform's development and more valuable for guiding improvements."}, {"title": "6 CONCLUSION", "content": "In this paper, we propose a practical LLM-based pair-wise evaluation method for recommender systems. Our experiments primarily investigate the feasibility of using LLMs for comparative evaluation in this context. We assess both the overall effectiveness and the sub-dimensional effectiveness of LLMs in the pair-wise evaluation task, examining their performance across various LLMs. Furthermore, we find that the LLM-based pair-wise evaluation method not only produces results that align with the trends of offline metrics but also offers improved discrimination, capturing finer distinctions between different recommendation models."}]}