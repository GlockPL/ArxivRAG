{"title": "A Bayesian Mixture Model of Temporal Point\nProcesses with Determinantal Point Process Prior", "authors": ["Yiwei Dong", "Shaoxin Ye", "Yuwen Cao", "Qiyu Han", "Hongteng Xu", "Hanfang Yang"], "abstract": "Asynchronous event sequence clustering aims to group similar event sequences in\nan unsupervised manner. Mixture models of temporal point processes have been\nproposed to solve this problem, but they often suffer from overfitting, leading to\nexcessive cluster generation with a lack of diversity. To overcome these limita-\ntions, we propose a Bayesian mixture model of Temporal Point Processes with\nDeterminantal Point Process Prior (TP2DP2) and accordingly an efficient posterior\ninference algorithm based on conditional Gibbs sampling. Our work provides a\nflexible learning framework for event sequence clustering, enabling automatic iden-\ntification of the potential number of clusters and accurate grouping of sequences\nwith similar features. It is applicable to a wide range of parametric temporal point\nprocesses, including neural network-based models. Experimental results on both\nsynthetic and real-world data suggest that our framework could produce moderately\nfewer yet more diverse mixture components, and achieve outstanding results across\nmultiple evaluation metrics.", "sections": [{"title": "1 Introduction", "content": "As a powerful tool of asynchronous event sequence modeling, the temporal point process (TPP)\nplays a crucial role in many application scenarios, such as modeling trading activities in financial\nmarkets [7, 9], analyzing information dissemination within social networks [12, 18, 11], and depicting\nthe propagation of infectious diseases [37, 34]. Traditional TPPs such as the Poisson process [6] and\nHawkes process [14] offer straightforward and intuitive parametric models for building interpretable\ngenerative mechanisms of event sequences. Recently, TPPs with neural network architectures have\nbeen developed to capture the intricate relationship among events in real-world sequences, in which\nevents may either excite or inhibit each other's occurrence and exhibit both short-term and long-term\nimpacts. In particular, leveraging architectures like recurrent neural network (RNN) [19, 10], attention\nmechanism [35, 39, 38, 20], and normalizing flow [24, 5], these neural TPPs have proven highly\neffective in capturing the complex temporal dynamics of event occurrences.\nIn practice, event sequences often demonstrate clustering characteristics, with certain sequences\nshowcasing greater similarities when compared with others. For instance, event sequences of patient\nadmissions may exhibit clustering patterns in response to specific medical treatments, and individuals\nsharing the same occupation often display similar shopping behaviors. Being able to accurately cluster\nevent sequences would bring benefits to many fields, including medical health and personalized\nrecommendations. In recent years, researchers have built mixture models of TPPs to tackle the event\nsequence clustering problem [30, 28, 36]. However, these models often suffer from overfitting during"}, {"title": "2 Preliminaries & Related Work", "content": "Temporal Point Processes TPP is a kind of stochastic process that characterizes the random occur-\nrence of events in multiple dimensions, whose realizations can be represented as event sequences, i.e.,\n{(ti, di)}=1, where ti \u2208 [0,T] are time stamps and di \u2208 D = {1, ..., D} are different dimensions\n(a.k.a. event types). Typically, we characterize a TPP by conditional intensity functions:\n*(t) = \u2211d=1 *(t), and (t)dt = E[dNa(t) | Ht].\n(1)\nHere, (t) is the conditional intensity function of the type-d event at time t, Na(t) denotes the\nnumber of the occurred type-d events prior to time t, and Ht denotes the historical events happening\nbefore time t. Essentially, \u5165(t) represents the expected instantaneous rate of event occurrence at\ntime t given the past event information.\nThe simplest TPP is Poisson process [6], whose events happen independently at a constant rate, i.e.,\n(t) = da, vd \u2208 D. Hawkes process [14] introduces triggering patterns among different event types,\nwhose conditional intensity function is \u5165(t) = \u00b5\u03b1 + \u03a3\u03c4; <t add\u2081 e-\u03b2(t-ti), where p\u00e5 represents the\nbase intensity of the type-d event, add, encodes the infectivity of the type-di event on the type-d event,\nand \u1e9e controls the decay rate of the infectivity over time. Neural TPPs set {\\*(t)}=1 = fe(t, Ht),\nwhere fe is a neural network parameterized by 0, which can be recurrent neural networks [19, 10] or\nTransformer architectures [35, 39, 20]. Given an event sequence s = {(ti, di)}{=1, the likelihood\nfunction of a TPP can be derived based on its conditional intensity functions:\nL(s) = \u220fi=1 (i) exp(-(7)dr).\n(2)\nBy maximizing the likelihood in Eq. (2), we can learn the TPP model to fit the observed sequence.\nMixture Models of TPPs Given multiple event sequences belonging to different clusters, i.e.,\n{sn}=1, we often leverage a mixture model of TPPs to describe their generative mechanism, leading\nto a hierarchical sampling process:\n1) Determine cluster: m ~ Categorical(\u03c0), 2) Sample sequence: s ~ TPP(0m),\n(3)\nwhere \u03c0 = [\u03c01,...,\u03c0\u03bc] \u2208 \u2206\u039c\u22121 indicates the distribution of clusters defined on the (M \u2013 1)-\nSimplex, TPP(0m) is the TPP model corresponding to the m-th cluster, whose parameters are\ndenoted as Om. In [30], the Dirichlet mixture of Hawkes processes is proposed and is the first model-based clustering method for event sequences, yet the mixture components of this model are restricted\nto Hawkes processes. The work in [36] extends the mixture model for neural TPPs. A reinforcement\nlearning-based clustering algorithm has also been proposed [28] for neural TPPs, but it only applies\nto unidimensional event sequences. In addition, these methods require setting a large cluster number\nin advance, along with the imposition of a hard threshold to gradually remove excessive clusters\nthroughout the training process. As mentioned above, they often suffer from overfitting and lead to\nexcessive and non-diverse cluster generation.\nDeterminantal Point Processes DPP [16] is a stochastic point process characterized by the unique\nproperty that its sample sets exhibit determinantal correlation. The structure of DPP is captured\nthrough a kernel function [17, 4], which determines the similarity or dissimilarity between points\nin the sample space. Denote the kernel function by \u043a : X \u00d7 X \u2192 R, where X represents a sample\nspace. The probability density function for samples X1, ..., XM \u2208 X in one realization of DPP is:\np(x1,...,xM) x det{K(x1,...,x)}, \n(4)\nwhere K (x1,...,xM) = [k(xi, xj)] is a M \u00d7 M Gram matrix corresponding to the samples.\nGiven arbitrary two samples xi and xj, we have p(xi,Xj) = k(Xi, Xi)K(Xj, Xj) \u2013 K(Xi, Xj)\u00b2 =\np(xi)p(xj) \u2013 \u043a(xi, xj)\u00b2 \u2264 p(xi)p(xj). Therefore, DPP manifests the repulsion between xi and xj"}, {"title": "3 Proposed TP2DP2 Model", "content": "The mixture model in Eq. (3) reveals that each event sequence s obeys a mixture density, i.e.,\n\u2211Mm=1mL(s | 0m), where Mis a random variable denoting the number of clusters, \u03c0 =\n[\u03c01, ..., \u03c0\u039c] \u2208 \u2206\u039c\u22121 specifies the probability of each cluster component (a TPP), and L(s | 0m) is\nthe likelihood of the m-th TPP parametrized by em. Given N event sequences S = {sn}n=1, we\ndenote cluster allocation variables of each sequence c = [C1, ..., CN] \u2208 {1, ..., M}N, where each set\n{Sn | Cn = m} contains the sequences assigned to the m-th cluster. Accordingly, we derive the joint\ndistribution of all variables, i.e., p(M, \u0398, \u03c0, c, S), as\np(M)p( | M)p(\u03c0 | M) p(c | \u03c0)p(S | \u0398, c),\n\u220f1 \u03c0\u03b5\u03b7 L(sn|Ocn) \n(5)\nwhere = {0m}M=1\u2208RP.p(M), p(\u0398 | M) and p(\u03c0 | M) are prior distributions of M, O and\n\u03c0, respectively. By Bayes Theorem, the posterior distribution p(\u039c, \u0398, \u03c0, c | S) is proportional to\nEq. (5).\nThe exact sampling from p(M, \u0398, \u03c0, c | S) is often intractable because the parameters of the TPPs\nin practice (especially those neural TPPs [19, 10, 35, 39, 20]) are too many to perform full Bayesian\nposterior calculation. To overcome this issue, we conduct posterior inference only on a subset of\nmodel parameters (i.e., the \u201csubnetwork\u201d of the whole model). In particular, we approximate the full\nposterior of the TPPs' parameters as\np(\u0398 | S) \u2248 p (\u0117s | S) \u03b4(OR \u2013 \u00d4R) = p(U | S)p(W | S)8(OR \u2013 \u00d4R),\n(6)\nwhere we split the model parameters into two parts, i.e., Os and OR, respectively. Os =\n{0s,m}m=1 corresponds to the subnetworks of the TPPs in the mixture model, which we set to\nbe last few layers in nerual TPPs, while OR = {0R,m}M=1 denotes the remaining parameters.\nIn Eq. (6), p(\u0398 | S) is decomposed into the posterior of the subnetworks p(Os | S) and a\nDirac delta function on the remaining parameters R, in which OR is estimated by their point\nestimation R = {0R,m}m=1, e.g., the maximum likelihood estimation achieved by stochastic\ngradient descent. Theories and experiments have proven that a partial Bayesian network can still\nensure model expressiveness and predictive performance and even outperforms their full Bayesian\ncounterparts [8, 23, 15].\nUnlike existing work, in Eq. (6), we further decompose the parameters in the subnetworks into two\nparts, i.e., Os = {U, W}, where U = {m}m=1 and W = {wm}m=1, respectively. For the\nm-th TPP in the mixture model, \u00b5m corresponds to the \u201ccentral\" parameters of their conditional\nintensity functions, which significantly impacts the overall dynamics of event occurrence. In Hawkes\nprocesses [14], the base intensity signifies the traits of the background process. In neural TPPs [19,\n10, 35, 39, 20], the bias term of their last linear layer functions in a similar way. Therefore, we denote\neach \u00b5m as the base intensity in Hawkes process and the bias term of the last linear layer in neural\nTPP, respectively.2 Accordingly, the other \u201cnon-central\u201d parameters in each subnetwork are denoted\nas wm, which are contingent upon specific architectures of different models.\nImposing the conditional independence on the central and non-central parameters, i.e., p(\u0472s|M) =\np(U|M)p(W|M), we have\np(\u039c, \u0398, \u03c0, c|S) x p(M)p(U|M)p(W|M)p(\u03c0|\u039c) \u220f\u039d \u03c0cnL(Sn|0S,Cn, OR,cn), \n(7)\nn=1\n2Assuming the exchangeability in p(\u03bc\u2081,\u00b7\u00b7\u00b7, \u03bc\u00f1) for any fixed number M, then U is a finite point process\nwhich specifies both the random cluster number M and the value of central parameters. Additionally, we have\nU\u2208\u03a9 = Um=0Nm, where Om denotes the space of all finite subsets of cardinality m. The related measure\ntheoretical guarantee of U can be found in our Appendix.\""}, {"title": "4 Posterior Inference Algorithm", "content": "Because TP2DP2 have a large number of parameters in general, we first use the stochastic gradient de-\nscent algorithm to pretrain the model and then use conditional Gibbs samplers to infer the parameters\nof the selected subnetworks. Since the number of clusters changes dynamically as these algorithms\nproceed, it is helpful to further partition model parameters into parameters of allocated clusters and\nthose of non-allocated clusters when applying posterior sampling. In particular, we partition U into\ntwo sets according to cluster allocations c: one comprising cluster centers currently used for data\nallocation, denoted as U(a) = {\u03bcc\u2081,\u06f0\u06f0\u06f0, \u00b5cm }, and the other containing cluster centers not involved\nin the allocation, denoted as U(na) = U \\ U(a). Note that the product measure d\u00b5 \u00d7 d\u00b5 in \u03a9 \u00d7 \u03a9\nlifted by the map (x, y) \u2192 x\u222ay results in the measure du, so the prior density of (U(a), U(na))\nis equivalent to p(U(a),U(na)) = p(U(a) \u222aU(na)), which follows the DPP density. W and r are\npartitioned in the same way.\nAs (U,\u03c0, W, c) and (U(a), r(a), W(a), U(na), r(na), W(na), c) are in a one-\nto-one correspondence, we can refer to Eq. (9) and obtain the posterior of"}, {"title": "5 Experiments", "content": "To comprehensively evaluate the effectiveness of our TP2DP2 model and its inference algorithm, we\ntest our method on both synthetic and real-world datasets and compare it with state-of-the-art event\nsequence clustering methods. For each method, we evaluate its clustering performance by clustering\npurity [30] and adjusted rand index (ARI) [26] and its data fitness by the expected log-likelihood per\nevent (ELL) [32].\nIn addition, we report the expected posterior value of the number of clusters (M) in real-world dataset,\nwhich reveals the inferred number of components given data. The code of TP2DP2 is available at\nhttps://anonymous.4open.science/r/TP2DP2/."}, {"title": "5.1 Experiments on Mixtures of Hawkes Processes", "content": "We first investigate the clustering capability of TP2DP2 and demonstrate the rationality of DPP\npriors on the synthetic datasets generated by mixture models of Hawkes processes, in which each\nHawkes process generates 100 event sequences with 3 event types. All Hawkes processes apply\nthe same triggering function, and their base intensities are set to \u00b5m = (0.5 + \u0431\u0442)13, where\n13 is a three-dimensional column vector, with each element equal to 1. dm = \u03b4 \u00b7 (m - 1) for\nm\u2208 {1,2,3,..., KGT}, where KGT denotes the true number of clusters. In other words, these\nHawkes processes exhibit distinct temporal patterns because of their different base intensities. The\nexperiments are carried out for KGT = 4,5 for multiple datasets, each dataset having different d\nvalues, \u03b4\u2208 {0.1, 0.15, 0.2, 0.25, 0.3, \u2026\u2026\u2026,1}.\nWe compare our TP2DP2 with the Dirichlet mixture model of Hawkes processes (DMHP) learned\nby the variational EM algorithm [30]. For a fair comparison, we use the same Hawkes process\nbackbone as in DMHP, ensuring identical parametrization, and we consider all model parameters in\nthe Bayesian inference phase. For each method, we initialize the number of clusters randomly in the\nrange [KGT - 1, KGT + 1]. The averaged results in five trials are presented in Figure 3.\nWhen the disparity in the true base intensity among different point processes is minimal, the inherent\ndistinctions within event sequences are not apparent, as shown in Figure 6. In this case, TP2DP2 tends\nto categorize these event sequences into fewer groups than the ground truth, resulting in a relatively\nmodest purity when d is small. As d increases, TP2DP2 exhibits increasingly robust clustering\ncapabilities, consistently outperforming DMHP when \u03b4 > 0.55."}, {"title": "5.2 Experiments on Mixtures of Hybrid TPPs", "content": "Our model is compatible with various TPP backbones, which can detect clusters and fit event\nsequence data originating from a mixture of hybrid TPPs. To verify our claim, we generate a set\nof event sequences based on five different TPPs, including 1) Homogeneous Poisson process, 2)\nInhomogeneous Poisson process, 3) Self-correcting process, 4) Hawkes process, and 5) Neural\nHawkes process [19]. Based on the sequences, we construct three datasets with the number of mixture\ncomponents ranging from three to five. For each dataset, we learn a mixture model of TPPs and set\nthe backbone of the TPPs to be 1) the classic Hawkes process [14], 2) the recurrent marked temporal\npoint process (RMTPP) [10], and 3) the Transformer Hawkes process (THP) [39], respectively. The\nlearning methods include the variational EM of Dirichlet mixture model [36] and our TP2DP2. The\nresults in Table 1 show that our method achieves competitive performance. Especially when the\nbackbone is Hawkes process, applying our method leads to notable improvements, which means\nthat our method is more robust to the model misspecification issue. In addition, learning RMTPP"}, {"title": "5.3 Experiments on Real-World Datasets", "content": "To examine the performance of our method on real-world data, we use the following two benchmark\ndatasets: 1) Amazon [33]. This dataset comprises time-stamped user product review events spanning\nfrom January 2008 to October 2018, with each event capturing the timestamp and the category of\nthe reviewed product. Data is pre-processed according to the procedure in [32]. The final dataset\nconsists of 5,200 most active users with 16 distinct event types, and the average sequence length is\n70. 2) BookOrder 3. This book order dataset comprises 200 sequences, with two event types in each\nsequence. The sequence length varies from hundreds to thousands."}, {"title": "6 Conclusion", "content": "In this paper, we propose the Bayesian mixture model TP2DP2 for event sequence clustering. It is\nshown that TP2DP2 could flexibly integrate various parametric TPPs including the neural network-based TPPs as components, achieve satisfying event sequence clustering results and produce more\nseparated clusters. In the future, we plan to study the impact of alternative repulsive priors on event\nsequence clustering, and develop event sequence clustering methods in high-dimensional and spatio-temporal scenarios. It is also meaningful to explore the effect of DPP on mixture of intensity-free\nTPP models."}, {"title": "A Derivation of the posterior of TP2DP2", "content": "Temporal Point Processes (TPPs) are stochastic processes that characterize the random occurrences\nof events over multiple dimensions, where realizations can be represented as event sequences\n{(ti, di)}=1. Here, t\u2081 \u2208 [0, T] denotes timestamps and d\u00bf \u2208 D = {1, . . ., D} represents different\nevent types. To describe the characteristics of a TPP, we typically employ the conditional intensity\nfunction:\n*(t) = \u2211d=1 *(t), where x(t)dt = E[dNa(t) | Ht].\nHere, (t) denotes the conditional intensity function for event type d at time t, Na(t) indicates the\ncount of type-d events up to time t, and Ht represents the history of events prior to time t. Essentially,\n(t) captures the expected instantaneous rate of occurrence of type-d events at time t, given the\ninformation of the previous events.\nTo derive the likelihood function of temporal point processes, we first need to establish the survival\nfunction S(t), which gives the probability that no events occur up to time t. Assuming that the event\ntime T is a random variable with a probability density function fr(t), the cumulative distribution\nfunction Fr(t) represents the probability that an event occurs before time t, i.e. S(t) = 1 \u2013 Fr(t).\nGiven the definition of the conditional intensity function, the probability that an event occurs in a\nsmall interval dt can be approximated by X*(t)dt. Hence, the probability that no events occur in\nthe small interval dt following time t is 1 \u2013 A*(t)dt. This implies that the probability of no events\noccurring up to time t + dt can be expressed as S(t + dt) = S(t)(1 \u2013 A*(t)dt).\nTaking the limit as dt \u2192 0, we obtain the differential equation:\nds(t)/dt = -x*(t)S(t).\nTo this differential equation, we separate variables and integrate both sides:\nds(t)/s(t)= -*(t) dt, s dS(t)/s(t)= -* (t) dt,\nand then we have\nlog S(t)= - + C,\nwhere C is the integration constant. Using the initial condition S(0) = 1, we find C = 0. Thus,\nthe survival function is S(t) = exp(-\u222bx*(u) du), which expresses the probability that no events\noccur up to time t. Consequently, the probability that no events occur in the interval [ti, ti+1) is\nexp(- ft+1 *(u) du).\nNext, we derive the likelihood function for the entire event sequence. The probability density of ob-\nserving each event (ti, di) is given by the conditional intensity function d\u2081(t\u2081). Thus, the likelihood\nof observing the event sequence {(ti, di)}{=1 is the product of these probabilities \u03a0=1(t).\nTo account for the probability that no events occur between observed events, we incorporate the\nsurvival probabilities for the intervals [ti, ti+1), i.e. \u041f\u2081=0 exp(- ft+1 *(u) du),"}, {"title": "B Measure Theoretical Details and Construction of DPP Priors", "content": "We first elaborate on the measure theoretical details related to the DPP prior \u03bc. Let \u03a9 = Um=0Nm\nbe the space of all finite subsets of R\u00ba where \u03a9m = {{\u03bc1,\u2026\u2026,\u03bcm}} denotes the space of all\nfinite subsets of cardinality m, q is the dimension of each uh, and 20 = {0}. In order to define a\nmeasure on the set of sets, we need to induce a well-defined measure by taking advantage of the\nmeasure on the set of real numbers. It is feasible to construct mappings, which map pairwise distinct"}, {"title": "C Conditional Gibbs Sampler for TP2DP2", "content": "we provide the comprehensive summary of the complete TP2DP2 posterior sampling procedure in\nAlgorithm 1."}, {"title": "D Additional Experiment Details", "content": "We use the same parameterization of the components for both DMHP and TP2DP2, as in [30]. For\nthe Hawkes process model which constitutes the m-th cluster component, its intensity function of the\ntype-d event at time t is given by\n(t) = \u03bc\u03b1 + \u2211diti<tad (t-ti)\n= \u03bc\u03b1 + \u2211\u22119j (t-ti),\nwhere Nm is the number of basis functions for triggering functions, add\u025bj is the infectivity coefficient\nof j-th basis function gj, deciding the infectivity of previous type-di event on the current event. We\nset all basis functions to be the Gaussian kernel functions and set them to be the same for both DMHP\nand TP2DP2."}, {"title": "D.2 Additional Details of Experiments on Synthetic Mixture of Hybrid Point Processes\nDatasets and Real-World Datasets", "content": "We use the following point processes to generate event sequences, and mix the obtained event\nsequences to construct a dataset: 1) Homogeneous Poisson process (Homo). This process involves\nevents that occur at a constant rate independently of each other. 2) Inhomogeneous Poisson process\n(Inhomo). In this process, the event rate varies over time, allowing for a non-constant rate of\noccurrence. 3) Self-correcting process (Sc). Within this process, the occurrence of one event\ninhibits the others according to the self-correcting mechanism. 4) Hawkes process (Hks). The\noccurrence of one event triggers the likelihood of subsequent events, which manifests the self-exciting\ncharacteristic. 5) neural point process (Npp): we utilize the Neural Hawkes Process (NHP) [19], a\nneural network model that can flexibly capture complex sequential dependencies between events\n(excitatory, inhibitory, time-varying, etc.) to generate sequences. By randomizing the parameters of\neach network layer, we obtain an instantiation of NHP. Subsequently, the Ogata's modified thinning\nalgorithm [21] is applied to simulate the event sequence based on the intensity function modeled by\nthe NHP."}, {"title": "E Additional Exmerimental Results", "content": "In Figure 6, each panel presents a t-SNE plot of the Hawkes processes with different base intensities,\nand different panels illustrates different levels of base intensity disparities across clusters. The ground\ntruth label is illustrated with different colors. It can be seen that when the disparity in the true\nbase intensity among different point processes is minimal (as seen in the upper two subfigures), the\ninherent distinctions within event sequences are not apparent. In such cases, clustering them into\nexcessive categories is futile."}]}