{"title": "When Synthetic Traces Hide Real Content: Analysis of Stable Diffusion Image Laundering", "authors": ["Sara Mandelli", "Paolo Bestagini", "Stefano Tubaro"], "abstract": "In recent years, methods for producing highly real-istic synthetic images have significantly advanced, allowing the creation of high-quality images from text prompts that describe the desired content. Even more impressively, Stable Diffusion (SD) models now provide users with the option of creating synthetic images in an image-to-image translation fashion, mod-ifying images in the latent space of advanced autoencoders. This striking evolution, however, brings an alarming consequence: it is possible to pass an image through SD autoencoders to reproduce a synthetic copy of the image with high realism and almost no visual artifacts. This process, known as SD image laundering, can transform real images into lookalike synthetic ones and risks complicating forensic analysis for content authenticity verification. Our paper investigates the forensic implications of image laundering, revealing a serious potential to obscure traces of real content, including sensitive and harmful materials that could be mistakenly classified as synthetic, thereby under-mining the protection of individuals depicted. To address this issue, we propose a two-stage detection pipeline that effectively differentiates between pristine, laundered, and fully synthetic images (those generated from text prompts), showing robustness across various conditions. Finally, we highlight another alarming property of image laundering, which appears to mask the unique artifacts exploited by forensic detectors to solve the camera model identification task, strongly undermining their performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Over the last few years, we have witnessed an escalation in methods for producing increasingly realistic synthetically generated images, which exhibit high quality and realism, easily fooling the human eye [1]\u2013[5]. In particular, Diffusion Models (DMs) [6] have been dominating the scene due to continuous improvements that have brought astonishing gen-eration results [7]\u2013[9]. Latent Diffusion Models (LDMs) [2] have been lately introduced to enhance the visual fidelity of generated images and reduce the training complexity compared to standard DMs. These models allow to generate synthetic images starting from a noisy signal and a text prompt that describes the desired characteristics for the final generated image. Both input signal and text are encoded into latent repre-sentations using powerful pretrained autoencoders. Subsequent denoising steps in the latent space, followed by a decoding stage, produce the final synthetic image.\nMore recently, LDMs have evolved into Stable Diffusion (SD) models, thanks to even more advanced autoencoder structures trained on vast amounts of data [10]\u2013[13]. SD has expanded the options available to users, allowing them to build creations starting from actual input images. Indeed, SD enables users to provide an image as input to the generation pipeline. This image is superimposed with noise, encoded into the latent space and modified through multiple denoising steps influenced by the user's text prompt instructions. The similarity of the final decoded image to the input one can be adjusted by the user using a specific strength parameter.\nAt one extreme, the user can adjust this strength parameter to select the maximum similarity between the input and output images. As a result, the input image will be encoded in the latent space without any noise injections; the latent represen-tation will not undergo any denoising steps, and it will be directly decoded to produce the final image. This \"generated\" image will almost perfectly reproduce the semantic content of the input one, with very few details, barely visible to the human eye, that can change according to the specific autoencoder architecture.\nThis process can be applied to any input image, including real content like photographs or video frames. In the forensic community, this technique has come to be known as image laundering, meaning that real content can be transformed through a chain of SD encoding and decoding to wash out its original traces and simulate synthetic generation [14]. Some examples of real photographs and their laundered versions are shown in Fig. 1. Notice the extremely high realism of the laundered copies and the absence of noticeable artifacts upon visual inspection. However, in principle, laundered images are synthetic and not real photographs.\nThis paper proposes a forensic analysis of the image laun-dering process, demonstrating that it represents a serious issue for assessing content authenticity on the web. In fact, the laundered copies of real images carry common artifacts of"}, {"title": "II. STABLE DIFFUSION AND IMAGE LAUNDERING ISSUE", "content": "A. Image-to-image synthesis through Stable Diffusion\nIn a \"standard\" text-to-image generation through LDMs, the model is fed with random noise [2]. This input signal can be modeled as a noise-free image super-imposed to a strong noise which hides the scene content depicted on it. The provided text prompt acts as a hint for what the original image (before noise corruption) should look like. To produce the noise-free image, the LDM converts the noise into a latent representation through an encoder and performs several denoising steps in this latent space. This process is completed with cross-attention mechanisms that allow a latent representation of the text prompt to influence the denoising stage [2]. The process runs iteratively for a number of steps, such to gradually remove a portion of the noise at a time. The final image is estimated by decoding the output of multiple denoising steps.\nIn the last period, LDMs have started being known as SD models. This happened after considering advanced image autoencoders and text prompt encoders trained on a huge amount of data [10]. SD allows to generate images from a text prompt, but it allows as well to synthesize images in an image-to-image translation fashion. In detail, image-to-image emulates the standard image synthetis process, apart from the fact that the input image is no more a random noise but it is an actual image, like a real photograph or a cartoon illustration [10]. To generate a synthetic image, the input original image is superimposed to an additive noise term to generate a noisy version of it. This noisy sample is passed through SD generation and the output is another image that resembles more closely the original supplied one with respect to the random noise input case.\nTo control how much the output is affected by the input, SD is provided with a \u201cstrength\u201d parameter \\(s \\in [0,1]\\). This parameter is related to the amount of noise added at the beginning, and to the number of denoising steps that SD will run. Higher values of \\(s\\) will deviate more the output image from the input original one.\nB. Image laundering through Stable Diffusion\nIn the image-to-image synthesis process, the strength param-eter s has a considerable impact on the semantic consistency between the input and the output images. In this context, \\(s = 1\\) and \\(s = 0\\) describe two extreme scenarios of the possible generation outcomes. In case \\(s\\) is 1, the noise addition step does not completely destroy the input image, even if this allows for lots of variations and semantics different from the input one. In case of \\(s = 0\\), no noise is added to the input image and no denoising steps are run. In this setup, the text prompt is completely irrelevant for the final generation. The image is passed only through SD encoder and decoder, and the output image is highly connected to the original one. The semantic content is completely replicated, and very few details (barely visible at the human eye) can be changed, according to the specific autoencoder used (see some examples in Fig. 1).\nThis last procedure can be applied to any input image, including real content like photographs or video frames. When applied to real images, this process has started been known as image laundering, meaning that real content can be trans-formed through a chain of SD encoding and decoding to wash out its original traces and simulate a synthetic generation [14]. The available synthetic image detectors in the state-of-the-art risks to be fooled by this process, detecting the laundered images as being synthetic [14]. As a result, original images showing sensitive or harmless content could proliferate online without being identified as real, risking oversight and under-ming individuals protection. In the next lines, we detail the proposed solution to deal with such alarming problem."}, {"title": "III. IMAGE LAUNDERING DETECTION", "content": "A. Problem formulation\nGiven an image \\(I\\) under analysis, we aim at investigating whether \\(I\\) can be correctly classified as being real, fully synthetic or the result of a laundering operation applied to a real image. For clarity's sake, we define an image \\(I\\) to be \u201creal\u201d if its pixel content generally comes from a photograph; it can have undergone post-processing operations like compression, cropping or resizing, but its original content has been acquired by a digital camera sensor. On the contrary, we say that \\(I\\) is \"synthetic\" if it is the result of a synthetic generation model applied to an input signal."}, {"title": "B. Forensics analysis of laundered images", "content": "Backbone detector. To perform our investigations, we exploit a detector built upon the one proposed in [15], which has shown excellent performances for the real versus synthetic image detection task. This detector is based on the extraction of small squared patches from the query image and their subsequent aggregation to assign a single score per image. It proved very robust to compression and resizing operations, thanks to a long list of augmentations included in the train-ing process. Furthermore, the possibility of extracting small patches from the query image enables to be less dependent on the semantic content depicted in the image and to focus on the actual synthetic generation artifacts.\nA sketch of our proposed detector is shown in Fig. 2. As done in [15], we consider the EfficientNet-B4 Convolutional Neural Network (CNN) architecture [16], but we further sim-plify the patch extraction and aggregation strategy. Differently from [15], we always extract in random locations \\(N = 800\\) color patches \\({P_i}\\sqrt{N}\\) with size 96 \u00d7 96 pixels, independently on the input image size. These numbers have been selected to enable good robustness and generalization properties even on large input images (~ 3000 \u00d7 3000 pixels). Every patch \\(P_i\\) is associated with a detection score \\(s_i\\), which is greater than 0 if the patch is detected as positive (i.e., \u201csynthetic\u201d), and negative (i.e., \"real\") otherwise. After analyzing each patch through our detector, we aggregate the patches' scores \\({s_i}\\) by simply selecting the \\(M\\) highest scores corresponding to the uppermost 75% (in this case, \\(M = 600\\)) and computing their arithmetic mean. The final image score is defined as \\(s_I = \\sum_{i=1}^M s_i / M\\)."}, {"title": "IV. EXPERIMENTAL ANALYSIS", "content": "A. Datasets\nTraining datasets. The first detector (real versus synthetic) is trained over real and synthetic human faces collected from several state-of-the-art datasets. We train over a huge amount of data to ensure good robustness and generalization against different data and synthetic generators. Overall, the training set includes more than 200K images (equally balanced among real and fake samples) with minimum size 256 \u00d7 256 pixels.\nReal faces have been selected from FFHQ [21], Cele-bAHQ [22] and from the pristine dataset released in [23].\nSynthetic faces have been generated through state-of-the-art models for synthetic image generation. These include GAN-based generators like StyleGAN2 [24], StyleGAN3 [1], StarGAN-v2 [25], FaceVid2Vid [26] and Taming Transformers [27], as well as DM-based generators like Score-based mod-els [8], LDMs and SD models [2], DALL-E3-based Image Creator [3], Adobe Firefly [4] and Imagine from Meta AI [5]. We use images from SD-1.5, SD-2.1, SD-XL and SD-XL-turbo in both laundered and text-prompt modes; the laundered images are synthetic copies of the considered real data, while text-prompt mode includes only images generated from noise (what we defined to be \"fully synthetic\"). Notice that, in the preliminary experiments shown in Section III, laundered images were not included during training.\nThe second detector (fully synthetic versus laundered) is trained over synthetic images only, including all the synthetic data used at the first training stage.\nTest dataset. We evaluate results on real and synthetic images selected from different datasets than training ones.\nPristine images belong to the dataset presented in [17], consisting of 1081 real human faces with size 600\u00d7600 pixels. Synthetic images are of three different categories: (i) laun-dered versions of pristine data computed through SD-1.5, SD-2.1, SD-XL and SD-XL-turbo (1081 images each); (ii) fully synthetic faces generated from text prompts through the same generators (~ 300 images per generator); (iii) 3000 uncontrolled synthetic data selected from the DiffusionDB dataset [28], which was built by scraping user-generated images on the official SD Discord server and depicts various semantic contents. This last dataset is uncontrolled, since we do not have precise information on the image generation process, i.e., images could have been laundered or generated from images or noise via text prompts. To be sure of testing realistic samples resembling actual photographs, DiffusionDB data have been filtered out by removing cartoon-like samples.\nB. Training details\nFollowing the approach in [15], we initialize the network weights with those trained on the ImageNet database. We em-ploy cross-entropy loss and the Adam optimizer with default parameters, training for up to 500 epochs. The learning rate starts at 0.001 and is reduced by a factor of 10 if the loss does not decrease for 10 epochs. Training is halted if the validation loss fails to improve for over 20 epochs, and the model with the best validation loss is selected. To enhance robustness and generalization, we include a consistent amount of training data augmentations, including random resizing, compression and color corrections as suggested in [15].\nC. Experimental results\nSynthetic image detection. In this phase, we pass the entire test set through our real vs synthetic detector. The achieved image scores distributions are depicted in Fig. 6.\nIt is worth noticing that the True Positive Rate at threshold 0 (TPR@thr=0) is extremely high for both laundered and fully"}, {"title": "V. ANONYMIZATION EFFECTS OF IMAGE LAUNDERING: ANALYSIS OF CAMERA MODEL IDENTIFICATION", "content": "As a final experiment, we investigate the anonymization capabilities of image laundering in the well known forensic problem of camera model identification, i.e., identifying the source camera model of a query image. In the forensic community, it is widely recognized that images taken with the same camera model exhibit a distinct set of artifacts that can differentiate them from images captured by other cameras [30]. Identifying the camera model that produced an image can assist forensic investigators in tracing the original creator of images shared online.\nOver the years, the community has developed manifold solutions to attribute an image to its source camera model, considering model-based and data driven approaches, even if deep learning-based solutions represent now the standard to deal with such tasks [31], [32]. For instance, a simple end-to-end learning approach was deployed in [31], where it is shown that standard CNN-based detectors can attribute the original camera model with an accuracy greater than 90%.\nIn this section, we investigate if image laundering can affect performances of state-of-the-art camera model identification detectors. To do so, we test the detector proposed in [31] over images passed through SD-based laundering. As pristine data, we use the same test set considered in [31]: images are selected from the well known Vision dataset [33]; they have a common size of 512 \u00d7 512 pixels and come from 28 different camera models. With the EfficientNetB0-based detector proposed in the original paper, the achieved camera model identification accuracy on the original test images results 96.15%."}, {"title": "VI. CONCLUSIONS", "content": "This paper explores the forensic implications of SD image laundering, which involves passing an image through SD autoencoders to reproduce a synthetic copy of it with high re-alism and minimal visual artifacts. Our experimental campaign shows that image laundering has a significant potential to obscure traces of real content, including sensitive and harmful materials that might be mistakenly identified as synthetic, thus compromising the protection of the individuals depicted.\nTo combat this issue, we propose a simple yet effective two-stage detection pipeline that reliably distinguishes between pristine, laundered, and fully synthetic images (those generated from text prompts), proving to be a powerful solution under various testing conditions. Eventually, we highlight another concerning aspect of image laundering, which reveals capable to conceal unique artifacts that forensic detectors rely on for camera model identification, thereby severely diminishing their performance. To the best of our knowledge, this is the first paper investigating in depth the laundering issue. We believe our thorough analysis can offer valuable insights to the forensics community, paving the way for more comprehensive understanding and containment of this problem."}]}