{"title": "EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios", "authors": ["Lu Qiu", "Yuying Ge", "Yi Chen", "Yixiao Ge", "Ying Shan", "Xihui Liu"], "abstract": "The advent of Multimodal Large Language Models (MLLMs), leveraging the power of Large Language Models, has recently demonstrated superior multimodal understanding and reasoning abilities, heralding a new era for artificial general intelligence (AGI). However, achieving AGI necessitates more than just comprehension and reasoning. A crucial capability required is effective planning in diverse scenarios, which involves making reasonable decisions based on complex environments to solve real-world problems. Despite its importance, the planning abilities of current MLLMs in varied scenarios remain underexplored, leaving a significant gap in our understanding of their full potential. In this paper, we introduce EgoPlan-Bench2, a rigorous and comprehensive benchmark designed to assess the planning capabilities of MLLMs across a wide range of real-world scenarios. EgoPlan-Bench2 encompasses everyday tasks spanning 4 major domains and 24 detailed scenarios, closely aligned with human daily life. EgoPlan-Bench2 is constructed through a semi-automatic process utilizing egocentric videos, complemented by manual verification. Grounded in a first-person perspective, it mirrors the way humans approach problem-solving in everyday life. We evaluate 21 competitive MLLMs and provide an in-depth analysis of their limitations, revealing that they face significant challenges in real-world planning. To further improve the planning proficiency of current MLLMs, we propose a training-free approach using multimodal Chain-of-Thought (CoT) prompting through investigating the effectiveness of various multimodal prompts in complex planning. Our approach enhances the performance of GPT-4V by 10.24% on EgoPlan-Bench2 without additional training. Our work not only sheds light on the current limitations of MLLMs in planning, but also provides insights for future enhancements in this critical area. We have made data and code available at https://qiulu66.github.io/egoplanbench2/.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid development of Multimodal Large Language Models (MLLMs) [1], [2], [3], [4], [5], [6], [7] has demonstrated remarkable comprehension and generalization capacities, opening new possibilities for achieving the ultimate goal of artificial general intelligence (AGI) [8], [9], which aims to match or surpass human performance in most tasks. By plugging efficient visual encoders into pretrained Large Language Models (LLMs) [10], [11], [12], [13] and learning alignments between vision and language [14], MLLMs have excelled in various multimodal tasks such as image captioning [15], [16], visual question answering [16], [17], [18], [19], [20], [21], [22], mathematical reasoning [23], [24], crossing-modality grounding [25], etc. However, achieving AGI requires more than just advanced comprehension and reasoning. A crucial milestone is attaining human-level task planning capabilities, which involve making informed decisions in complex environments. This capability is essential for developing a versatile intelligent assistant that can assist humans in tackling a wide array of real-world challenges in daily life.\nWhile the comprehension capabilities of MLLMs have been extensively evaluated in previous benchmarks [26], [27], [20], [28], [29], the evaluation of the planning abilities of current MLLMs in various scenarios remains underexplored. A comprehensive benchmark specifically designed to assess the planning capabilities of MLLMs across a wide range of real-world scenarios is highly demanded to uncover the potential of MLLMs in serving as versatile assistants in the real world. Previous egocentric video question answering (QA) benchmarks [30], [31] also evaluate model performance in everyday life, but they primarily assess comprehension rather than planning, where a model answers questions based on the spatial and temporal understanding of the entire video. Most relevant to addressing this issue is EgoPlan-Bench [32], which evaluates the planning abilities of MLLMs from an egocentric perspective. However, it is constrained to single kitchen scenarios, lacking a comprehensive evaluation across a variety of real-world contexts.\nIn this paper, we introduce EgoPlan-Bench2, a benchmark designed to rigorously assess the planning capabilities of MLLMs across a broad range of daily scenarios. EgoPlan-Bench2 is founded on three principal tenets: a) Rich and diverse real-world scenarios. It includes 1,321 high-quality multiple-choice QA pairs sourced from 1,113 videos, covering 4 major life domains: Work, Daily life, Hobbies and Recreation. These domains are further subdivided into 24 detailed scenarios, ranging from everyday household tasks to specialized activities such as laboratory work, blacksmith and mechanical repairs as shown in Fig. 1. In addition to scenario settings, EgoPlan-Bench2 features 284 distinct verbs in task goals and 434 in candidate options, along with 742 and 1,113 unique objects respectively. The duration of task progress videos varies from a few seconds to five minutes. The rich and diverse scene setups, coupled with a variety of actions, objects and video lengths, ensure a comprehensive evaluation of MLLMs' planning capabilities across various contexts. b) Egocentric perspective. We choose Ego4D [33] as the"}, {"title": "II. RELATED WORK", "content": "Building upon the impressive achievements of LLMs, MLLMs have also experienced a revolutionary transformation. MLLMs typically consists of an image encoder (e.g., CLIP [34]) to extract visual information, a language model (e.g., LLaMA [11], Vicuna [12]) to decode multimodal or text sequence and a trainable align module (e.g., Q-Former [35], gated cross-attention layer [36]) to integrate visual features into the language embedding space. Groundbreaking models like BLIP-2 [35], LLaVA [37], Flamingo [36] and PaLM-E [38] have made early attempt to integrate LLMs into vision-language pre-training and have demonstrated remarkable multimodal understanding and reasoning capabilities. Recent research interest has increasingly shifted towards multimodal understanding and generation that incorporates videos as visual signals [39], [40], [41], [42], [43]. These methods (e.g., VideoChat [39], VideoChatGPT [40], Valley [44]) try to enhance MLLMs' instruction-following capabilities by generating video instruction-tuning data. Video-LLaMA [45] encodes individual frames through a ViT [46] and an image Q-Former and then apply temporal process through a video Q-Former. VideoChat2 [18] encodes video frames through a video transformer, and Q-former is employed to compress video tokens. Expect these models mentioned above, a considerable number of video-based MLLMs [47], [48], [49], [50] have been proposed, demonstrating notable generalization and reasoning abilities across a wide range of tasks."}, {"title": "III. CONSTRUCTING EGOPLAN-BENCH2", "content": "To simulate how MLLMs function as versatile AI assistants in managing complex tasks, the proposed EgoPlan-Bench2 is founded on three essential design principles: a) diverse scenarios reflective of real-world human life, b) an egocentric perspective, and c) a focus on evaluating planning tasks.\nOur methodology begins with the collection of a comprehensive set of egocentric videos that cover 4 fundamental domains of human life, contributing to the properties of egocentric perspective and diverse scenarios of EgoPlan-Bench2. In terms of the last principle, we design a semi-automatic dataset construction pipeline to generate high-quality QA pairs focusing on planning tasks. Finally, we provide the detailed data statistics of EgoPlan-Bench2."}, {"title": "IV. EXPERIMENTS", "content": "In this study, we conduct the evaluation on 21 MLLMs, including GPT-4V [3], Video-LLaMA2 [50], ShareGPT4Video [47], LLaVA-NeXT-Video [48], VILA [80], VideoChat2 [18], LongVA [49], Video-LLaVA [79], Video-ChatGPT [40], BLIP-2 [35], InstructBLIP [76], InstructBLIP Vicuna [76], Yi-VL [70], Qwen-VL-Chat [78], Valley [44], DeepSeek-VL [77], LLaVA1.5 [72], mPLUG-Owl-2 [75], MultiModal-GPT [71], InternVL-1.5 [73], [74] and InternVL-2 [73], [74]. For video MLLMs, we adhere to their official configurations, including the number of frames. We crop the task progress video as the visual input and modify the sampling function to ensure inclusion of both the first frame and the last frame (representing the current observation image). For image MLLMs, we consistently use 8 key frames, reducing the number if necessary to prevent inference issues. These key frames are uniformly sampled from the provided video clips and saved in advance for model evaluation.\nFor the evaluation, we use a common prompt as: Select the best answer to the following multiple-choice question based on the video. Respond with only the letter (A, B, C, or D) of the correct option. Considering the progress shown in the video and my current observation in the last frame, what action should I take next in order to [task goal]? [candidate choices]. Following the evaluation strategy in Video-MME [19], the accuracy is calculated by matching the output of the model with the real one, without introducing any third party model such as GPT."}, {"title": "V. TOWARDS HUMAN-LEVEL PLANNING WITH MULTIMODAL CHAIN-OF-THOUGHT PROMPTING", "content": "In the realm of natural language processing, Chain-of-Thought (CoT) reasoning empowers language models to tackle complex tasks by informing them to generate intermediate rationales. Numerous recent studies [81], [82], [83], [84], [85] have significantly enhanced the performance of MLLMs through the application of multimodal CoT prompting, which integrates CoT reasoning with additional multimodal prompts.\nWhen evaluated on EgoPlan-Bench2, MLLMs encounter significant challenges in making direct planning decisions. In this section, we propose a flexible and effective multimodal CoT prompting approach aimed at improving model performance and analyzing their bottlenecks in task planning. In Sec. IV-B, we have discussed three dominant sources of challenges in EgoPlan-Bench2: two types of visual information including current observation image and historical task progress video, and the integrated planning process. Our objective is to improve the model's planning capabilities through multimodal CoT prompting without any additional training, focusing on these three aspects. Specifically, we begin with a preliminary study addressing performance bottlenecks through auxiliary multimodal prompts tailored to these two types of visual information, as shown in Fig. 13. For the integrated planning process, we introduce a prompt-based reasoning strategy. We employ CoT reasoning with GPT-4V to generate step-by-step rationales, facilitating better integration of multimodal input alongside auxiliary prompts. Finally, a multi-iteration"}, {"title": "VI. CONCLUSION", "content": "In this research, we introduce EgoPlan-Bench2, a benchmark specifically designed to evaluate the task planning capabilities of MLLMs across a variety of real-world scenarios. We construct EgoPlan-Bench2 based on three primary principles: the inclusion of diverse real-world scenarios, an egocentric perspective, and a focus on evaluating planning capacity. EgoPlan-Bench2 encompasses everyday tasks spanning four major domains and 24 detailed scenarios that closely reflect human daily life. EgoPlan-Bench2 is developed through a semi-automatic process that utilizes egocentric videos, supplemented by manual verification to ensure accuracy. The evaluation of 21 MLLMs reveals that EgoPlan-Bench2 poses significant challenges to existing models. Using GPT-4V as a case study, we analyze the reasons behind its shortcomings in real-world task planning and provide insights that could guide the future development of MLLMs toward achieving human-level task planning capabilities. To enhance the planning proficiency of current MLLMs, we propose a novel, training-free multimodal Chain-of-Thought (CoT) prompting method. This approach significantly improves the planning performance of GPT-4V by generating intermediate reasoning chains and leveraging various effective prompts."}, {"title": "A. Prompts for Historical Task Progress", "content": "In EgoPlan-Bench2, historical task progress is presented in an egocentric video format, with durations ranging from a few seconds to five minutes. Previous experiments detailed in Sec. IV-E reveal that while MLLMs can comprehend the general scene depicted in the video, they are inclined to overlook crucial actions and demonstrate poor temporal perception and cognition. To mitigate the deficiencies in historical task progress understanding, we employ the following prompts:\n\u2022 Action sequence (Action-seq). Following the long-term memory extraction method described in [86], we uniformly sample four frames from each action video segment and utilize GPT-4V to condense them into a concise phrase that encapsulates the action content. This process is repeated for all completed actions, resulting in the generation of a text sequence of actions (Action-seq-GPT) that captures the historical task progress. For example, an action sequence might read: \u201cplace golf ball on tee, swing the golf club, pick up a golf ball\u201d. We also extract raw annotations to construct a ground-truth action sequence (Action-seq-GT) for comparison purposes.\n\u2022 Video-level description (Description-video). GPT-4V is"}, {"title": "B. Prompts for Current Observation State", "content": "Another critical visual cue is the current observation image, which reflects the spatial relationships, interactions, and statuses between the camera wearer and manipulated objects. This information can impact the rationality of the subsequent action and the execution success rate, thus offering valuable insights for planning. In this section, we analyze the following prompts:\n\u2022 Image description (Description-img). This includes a detailed description of the current observation image, noting the activity the camera wearer is engaged in, the objects being interacted with, the visibility of task-related objects mentioned in the options, and their status if visible.\n\u2022 Bounding box of human hands (BoundingBox-hand). The movements and positions of human hands are indicative of interactions between the camera wearer and the manipulated objects. Using Grounding DINO, we delineate bounding boxes around the hands in the current observation image, which are then input into the planning model alongside the egocentric video as a visual prompt.\n\u2022 Bounding box of key objects (BoundingBox-obj). To minimize confusion from excessive bounding boxes, we instruct GPT-4 to identify no more than five key objects based on the question and options. These objects guide Grounding DINO for precise object marking, with only the most confident bounding box maintained for each key object. This prompt also emphasizes interactions between humans and objects and spatial relationships among different objects.\n\u2022 Image-based status of key objects (Status-obj). We crop key objects from the current observation image according to the bounding boxes, resize and pad them to uniform size, and concatenate them into an image grid. Compared to the current observation image with bounding boxes, this visual prompt composed of the cropped key objects focuses on the states of the key objects but loses spatial and interaction features.\nScene graph (SG). Scene graph is a formalization of objects and their relations and attributes that has been extensively used as a bridge between the visual and textual domains. We follow the steps of [84] to summarize"}, {"title": "C. Prompt-Based Reasoning Strategy", "content": "Through prior sections, we identify effective prompts for planning tasks, including the action sequence for the historical task progress and two types of bounding boxes for the current observation. In this section, we employ the CoT reasoning approach to facilitate step-by-step task planning and integrate various effective multimodal prompts, as demonstrated in Fig. 14. GPT-4V is instructed to: a) analyze completed actions and historical task progress, b) describe the current observation state based on visual input, c) assess the suitability of options as the next action relative to the task progress and determine their feasibility in the current state, d) choose the best answer from candidate choices. Using only object-related bounding boxes as additional prompts, we compared the results of direct action prediction with those generated through CoT reasoning. Direct prediction achieves an accuracy of 37.63%, whereas incorporating CoT reasoning improves accuracy to 39.82%. This demonstrates the importance of generating intermediate reasoning chains in planning tasks.\nFinally, we explore two types of multi-iteration decision approaches to reinforce answer consistency. The first involves a self-refinement approach, where GPT-4V iteratively corrects and refines the reasoning steps and answers from previous rounds until it confirms the correctness of the previous response. The second strategy employs a self-consistency mechanism, wherein GPT-4V generates answers for five times and selects the most frequently produced option among multiple answers. By integrating prompts of Action-seq-GPT and BoundingBox-obj with multimodal CoT reasoning and self-consistency, GPT-4V achieves a peak accuracy rate of 43.04%."}]}