{"title": "TAGE: Trustworthy Attribute Group Editing for Stable Few-shot Image\nGeneration", "authors": ["Ruicheng Zhang", "Guoheng Huang", "Yejing Huo", "Xiaochen Yuan", "Zhizhen Zhou", "Xuhang Chen", "Guo Zhong"], "abstract": "Generative Adversarial Networks (GANs) have emerged as a prominent research focus for image editing tasks,\nleveraging the powerful image generation capabilities of the GAN framework to produce remarkable results. However,\nprevailing approaches are contingent upon extensive training datasets and explicit supervision, presenting a significant\nchallenge in manipulating the diverse attributes of new image classes with limited sample availability. To surmount this\nhurdle, we introduce TAGE, an innovative image generation network comprising three integral modules: the Codebook\nLearning Module (CLM), the Code Prediction Module (CPM) and the Prompt-driven Semantic Module (PSM). The CPM\nmodule delves into the semantic dimensions of category-agnostic attributes, encapsulating them within a discrete codebook.\nThis module is predicated on the concept that images are assemblages of attributes, and thus, by editing these category-\nindependent attributes, it is theoretically possible to generate images from unseen categories. Subsequently, the CPM\nmodule facilitates naturalistic image editing by predicting indices of category-independent attribute vectors within the\ncodebook. Additionally, the PSM module generates semantic cues that are seamlessly integrated into the Transformer\narchitecture of the CPM, enhancing the model's comprehension of the targeted attributes for editing. With these semantic\ncues, the model can generate images that accentuate desired attributes more prominently while maintaining the integrity\nof the original category, even with a limited number of samples. We have conducted extensive experiments utilizing the\nAnimal Faces, Flowers, and VGGFaces datasets. The results of these experiments demonstrate that our proposed method\nnot only achieves superior performance but also exhibits a high degree of stability when compared to other few-shot image\ngeneration techniques.", "sections": [{"title": "1. INTRODUCTION", "content": "Few-Shot Image Generation [8, 37, 6] is an important research direction in the field of computer vision and deep\nlearning, and its goal is precisely to learn the potential patterns captured from a very limited number of examples, and\nutilize this information to generate diversified and realistic new images of the corresponding categories or other unseen\ncategories. This technique is particularly suitable for those cases where only a small amount of labeled or exemplar data\nis available, and is important for solving the problems of model generalization ability and adaptability when there is\ninsufficient data. In practical applications, such as personalization [24], art creation [23], medical image analysis [19, 5,39],\nimage enhancement [26, 25, 4] and dealing with rare category object recognition, few-sample image generation shows\ngreat potential and value.\nCurrent few-shot image generation methods are broadly classified into three categories: optimization-based, fusion-\nbased, and transformation-based. Optimization based approaches [6, 21] learn a set of generalized basic models through\nmeta-learning and fine tunes them for different tasks to achieve the goal. However, the quality of images generated by\nthese methods is not high. Fusion based methods [17, 16, 12] extract features from different input images and fuse them\ninto new categories of images in the latent space, the limitation of this method is that the input images need to be relatively\nsimilar and are quantitatively demanding. Transformation based methods [32] want to find intra-category transformations\nand apply these transformations to unseen category samples to generate more images of the same category, the\ndisadvantage is that the transformations are complex and training is unstable.\nIn contrast to the previous three methods, editing-based methods model the generation of few-shot images as attribute\nediting problems, which allows us to avoid complex and unstable transformation structures during training, and achieve\nhigh-quality image generation. The first edit-based approach proposed is AGE [8], which has high production quality.\nHowever, there are still many problems. As shown in Figure 1, the images generated by AGE may result in the\ndisappearance and collapse of organs, significantly impacting the perceived image quality. In pursuit of the primary\nobjective to generate training data for a handful of downstream applications, the quality improvements achieved thus far\ndo not meet the expected standards of satisfaction. In this paper, we further study the generation of new category images\nbased on codebook and text prompts, with the goal of enhancing the model's stability and attribute editing controllability.\nIn this paper, we propose TAGE, an image generation model that leverages pre-trained Generative Adversarial\nNetworks (GANs) to mine semantic directions and perform attribute editing without direct supervision. TAGE introduces\ndictionary learning into few-shot image generation using three key modules: the Codebook Learning Module (CLM), Code\nPrediction Module (CPM), and Prompt-driven Semantic Module (PSM). The CLM uses unlabeled images to identify\nsemantic directions for both category-related and unrelated attributes, constructing a sparse dictionary for generating\nunseen category images by recombining known attributes. The CPM enhances control and stability by predicting latent\ncodes that ensure accurate attribute editing, even under limited data or high diversity conditions. The PSM generates\nsemantic prompts that guide the CPM, enabling fine-grained control over attribute manipulation while preserving\ncoherence. Together, these modules allow TAGE to extract semantic information from pre-trained GANs and achieve\nflexible, high-quality image generation and editing, particularly in few-shot scenarios.\nOur contributions can be summarized as follows:\n1. We propose a few-shot image generation method called TAGE, including Codebook Learning Module (CLM),\nCode Prediction Module (CPM) and Prompt-driven Semantic Module (PSM). The method identifies category-independent\nediting directions without explicit supervision and enables more stable attribute editing.\n2. In few-shot image generation scenarios, a limited small-scale potential space helps to improve image quality. Our\nproposed CLM achieves this by limiting the potential space and storing high-quality reconstruction elements.\n3. To address the dilemma of lower input quality and reduced diversity, our proposed CPM enables better code\nprediction using global combinatorial information and long-range dependencies to improve the diversity of the generated\nimages.\n4. The Prompt-driven Semantic Module (PSM) facilitates few-shot learning by injecting semantically-guided prompts\ninto the transformer layers, enabling better attribute understanding and manipulation for higher-quality image generation\nand editing with scarce data.\n5. The experimental results from the Animal Faces, Flowers, and VGG Faces datasets demonstrate that our proposed\nnetwork can generate higher quality images with notable improvements in performance."}, {"title": "2. RELATED WORK", "content": "2.1 Few-shot Image Generation\nAs shown in Figure 0, few-shot image generation research can be grouped into three paradigms: optimization-based,\nfusion-based, and transformation-based methods. Optimization-based methods [6, 21] use meta-learning to train\ngeneralized base models that are fine-tuned for different tasks, but they often generate lower-quality images due to\ninsufficient detail capture. Fusion-based methods [17, 16, 12] combine features from multiple input images in the latent\nspace to create new image classes, yet they rely on high similarity among inputs and are computationally expensive.\nTransformation-based approaches [32] apply intra-class variations to unseen categories to synthesize images, but their\ncomplexity and instability during training are significant limitations.\n2.2 Codebook Learning\nSparse dictionaries have proven effective in image tasks like super-resolution and denoising. The VQ-VAE\nframework [30, 11] learns discrete codebooks in latent space, addressing \"posterior collapse\" and enhancing model\nperformance. VQGAN [10] further improves perceptual quality through adversarial training, while CodeFormer [38]\nreplaces Nearest-Neighbor Matching with a Transformer-based network for better codebook prediction. Leveraging these\nstate-of-the-art methods, we use discrete codebooks for few-shot image generation to improve image quality and robustness.\n2.3 Text-driven Image Generation\nInitially dominated by GANs [36, 33], text-driven image generation has shifted toward diffusion models [2, 18],\nwhich integrate advanced text processing for more precise image synthesis. For instance, DAELL2 [31] and StyleCLIP\n[29] combine CLIP embeddings with image generation models for high-fidelity results. Transformer-based models like\nCogView2 [9] and Muse [3] have also shown strong performance. Unlike these methods, our approach performs\nunsupervised semantic editing in StyleGAN's latent space for few-shot image generation, incorporating prior text\nembeddings to enhance image quality without requiring extensive labeled data or complex loss functions."}, {"title": "3. THE PROPOSED METHOD", "content": "3.1 The overview of our method\nWe use the seen category $c_s$ as training set and unseen category $c_u$ as testing set, where the number of images in $c_u$\nis small. Our goal is to generate unseen category images by editing category-irrelevant attributes, whose direction is\nextracted from a large number of seen category images without explicit supervision. Our method framework is shown in\nFigure 3. In the training stage, we embed the image into latent space and distinguish category-irrelevant attributes from\ncategory-relevant attribute vectors. Inspired by the idea of dictionary learning [13], we use CLM to discretize and store the\nsemantic directions of category-irrelevant attributes in a dictionary model, and employ a Code Prediction Module and a\nPrompt-driven Semantic Module to predict the code combination to achieve stable attribute group editing.\n3.2 Attribute Factorization\nTo realize attribute editing, the real image has to be firstly mapped to the latent space. This step is very important,\nbecause the performance of the image editing largely relies on the quality of the latent code. We use pSp as our encoder to\nfind the latent code of real images in the latent domain. pSp use feature pyramid as backbone to encode image into three\nlevels feature maps, which correspond to the coarse, medium and fine details in StyleGAN.\n$w_i = pSp(x_i)$ (1)\nwhere the $x_i$ is the input image and $w_i \\in R^{18 \\times 512}$ is the latent code that corresponds to input image.\nAssuming we already have latent code in $w +$ space, the next thing to do is to separate a set of category-relevant\nattribute and category-irrelevant attribute directions. As mentioned above, theoretically a large number of images of unseen\ncategories can be generated by editing category-irrelevant attributes. But without explicit supervision finding the category-\nirrelevant attribute directions is difficult. Since the category-related vectors of the same category are similar, if a large\nnumber of embedding vectors $w_i^{cm}$ of category $c_m$ are given, their average vectors approximate the category-related\nvectors $\\overline{w}^{cm}$ we need.\n$\\overline{w}^{cm} = \\frac{1}{N_m} \\sum_{i=1}^{N_m} w_i^{cm}$ (2)\nwhere $N_m$ is the number of samples in category $c_m$. Then, for a seen category, the latent code of image can be\ncomposed of the category-relevant vectors $\\overline{w}^{cm}$ plus the category-irrelevant attribute vectors $\\Delta w^i$ that we want to obtain.\n$w^{cm}_{ir} = \\overline{w}^{cm} + \\Delta w^i$ (3)\nWith this algorithm we are able to obtain a large number of category-irrelevant attribute vector $\\Delta w^i$, which provides\na stable data source for our next step training. To increases the diversity and stability of attribute editing, we want to\nemploy a Code Prediction Module to predict the category-irrelevant attribute vector. We first incorporate the idea of\ndictionary learning, using a pre-trained encoder to obtain a discrete codebook. Our method's training is divided into two\nstages accordingly.\n3.3 Codebook Learning Module\nThe goal of the first stage of training is to train a context-rich codebook [20], given a category-irrelevant attribute\nvector, we joint optimize a global dictionary $A \\in R^{18 \\times 512 \\times l}$ which contains category-irrelevant directions and a sparse\nrepresentation $\\eta_i$ following the AGE training.\nThen replace each \u201cpixel\u201d of the reconstructed category-irrelevant vector $\\Delta w_{ir}$ that generated by dictionary $A \\in\n$[R^{18 \\times 512 \\times l}$ with the closest part in the codebook $C \\in \\{c_k \\in R^{512}\\}_{k=0}^n$ to obtain a new reconstructed category-irrelevant\nvector $\\Delta \\overline{w}^{i}_{ir}$.\n$\\overline{v}^{ir}(i, j) = arg \\min_{k=0}^{n} ||\\Delta w^{ir}(i,j) - c_k||_2$ (4)\nThe reason why we still need a discrete codebook when we already have a global dictionary A is that compare to the\ncontinuous infinite space, small finite proxy space shows superior robustness and reconstruct quality. When input an\nunseen category image, the embedding modules such as pSp probably generate an ambiguous latent code, which will"}, {"title": "3.4 Code Prediction Module", "content": "Since the unseen categories of images are varied, sometimes Nearest-Neighbor(NN) Matching usually fails to find\nthe accurate editing code, making the edited images have a lower degree of perceived quality. Besides, diversity is\nimportant To alleviate this problem, we employ a Transformer module and some linear layers to predict the category-\nirrelevant attribute vector. we insert a Transformer [34] module which contains nine self-attention blocks following the\ndictionary module. The structure is shown in Figure 1. At this stage we freeze all modules except the Code Prediction\nModule, The i-th self-attention block of Transformer computes as the following:\n$X_{i+1} = \\sigma(Q_iK_i)V_i + X_i$ (7)\nwhere the $X_0 = \\Delta w_{ir}^i$, The query Q, key K, and value V are obtained from $X_i$ through linear layers.\nGenerally speaking, CPM use the edited image vector $w_{ir}^i$ as an input to predict n layer code sequence $s \\in \\{0,\\dots, N\n- 1\\}^n$ represent the probability of the N code items. Then, based on the predicted code sequence s, n individual code items\nare retrieved from the codebook to form the reconstruct vector.\nIn stage II, since we only train CPM, we don't need the four losses mentioned above. We only need two code-level\nlosses: 1) cross-entropy loss $L_{cross-entropy}$ for code prediction supervision, and 2) L2 loss $L_{code}^{dict}$ for the prediction code\n$\\Delta \\hat{w}_{ir}^i$ close to the embedding of the edited image $\\Delta w_{ir}^i$.\n$X_{i+1} = \\sigma(Q_iK_i)V_i + X_i$ (8)\nwhere $s_i$ is the ground truth code sequence s is obtained from the stage I and $\\hat{s}_i$ is predicted, n represent the pixel number\nin $\\Delta w_{ir}^i$.\nThe overall loss function is:\n$L_{code}^{dict} = ||\\Delta w_{ir}^i - sg(\\Delta \\hat{w}_{ir}^i)||_2$ (10)\n$L_{iit} = L_{cross-entropy} + L_{code}^{dict}$\nIn the inference phase, the same as AGE, we sample an arbitrary $\\hat{x}_i$ from $N (\\mu, \\Sigma)$ and apply editing to unseen\ncategory images. When we get the embedding of the edited image, we put it to the Code Prediction Module to predict the\ncode sequence and generate the image like stage II.\n3.5Prompt-driven Semantic Module\nIn this module, we first need to define the structure of cue words, a system of cue words containing category\ninformation, color features, shape features, and environment or background elements. Second, we need to construct a\nvocabulary $V$ that covers all predefined cue words. Next, we utilize the text encoder in CLIP to extract word embedding\nvector $v_i = ECLIP(t_i)$ for each word $t_i \\in V$ in the glossary. Where $ECLIP$ denotes the pre-trained CLIP image encoder.\nFor the input image, the model randomly selects a set of relevant cue words $\\{t_1, t_2, ..., t_n\\}$ from the vocabulary list. Each\nword in the cue word sequence is transformed into a corresponding word embedding vector to form a cue word vectors $V_i$.\nThe Transformer structure accepts both a sequence of edited image vector $\\Delta \\hat{w}$ and the cue word embedding vectors $V_i\nfor interactive computation, with the following formula:\n$Q_i = v_i W_Q$ (11)"}, {"title": "4. EXPERIMENT", "content": "4.1 Implementation Details\nWe use pre-trained StyleGAN generator and pre-trained pSp encoder. The MLP is 5 layers with Leaky-ReLU\nactivation function. We set the length $l$ of dictionary A to 100 and the codebook size N to 10000. Our method is trained\nusing two NVIDIA RTX 3090 GPUs with the PyTorch framework.\n4.2 Datasets\nWe conduct experiment on three few-shot image datasets: Animal Faces [22], Flowers [27], VGGFaces [28], each\ndataset is split into two parts: seen category for training and unseen category for testing.\nAnimal Faces: The dataset consists of images of 149 carnivore categories from ImageNet [7]. The dataset contains\n117,574 carnivore images. We divided these classes into a source class set and a target class set, containing 119 and 30\nanimal classes, respectively.\nFlowers: The dataset is an image classification dataset mainly used to test the performance of the algorithm in complex\nscenarios. It contains 102 different flower classes, which are mainly some common species in the UK. Each category\ncontains images ranging from 40 to 258 images, totalling 8189 images.\nVGGFaces: The dataset contains 3.31 million images from 9,131 celebrities spanning a wide range of races and\nprofessions. The dataset is divided into two parts: one for training with 1802 classes and the other for evaluation (testing)\nwith 552 classes.\n4.3 Metric\nWe evaluate our method by FID [14] and LPIPS [35] metrics. FID is a commonly used metric for evaluating the\ngenerative model, it measures the performance of the generative model by comparing the distance between the distribution\nof the generated image and the distribution of the real image, if the value of FID is smaller, it means that the distance\nbetween the generative model and the real distribution is smaller, and the quality of generation is better. LPIPS is used to\nevaluate the perceptual similarity to measure the similarity of images, it can be used to measure the image with similarity\nby simulating the human eye's perception of the picture, we adopt LPIPS to measure the diversity of generated images.\n4.4 Quantitative Evaluation\nWe compare our network with other few-shot image generation methods in Animal Faces, Flowers, VGGFaces\ndatasets. We randomly select from each unseen category image and generate 128 fake images, which are denote as $S_{fake}$\nAnd extract the equal number images from each unseen category image as $S_{real}$. We calculate the FID between $S_{real}$ and\n$S_{fake}$, and only use $S_{fake}$ to calculate the LPIPS score, the result of different methods is showed in Table 1. Our model\nhas achieved some improvements in both FID and LPIPS. We achieved the best LPIPS scores on both datasets and top\ntwo on FID. And our method requires only one image as input and produces more diverse images. Compared to AGE, our\nmethod is more stable, as will be illustrated in the qualitative evaluations below.\n4.5 Qualitative Evaluation\nIn qualitative evaluation, we also compare with the one-shot image generation method AGE on three datasets: Animal\nFaces, Flower, VGGFaces. It can be seen that both AGE and TAGE have some generalization ability to generate images\nwith different attributes. For example, we can generate dogs and flowers with different positions, man with sad or smile\nexpression.\nHowever, the indicators of a successful redevelopment are not just about diversity, perceptual is also a important\nmetric. Imagine we want to reconstruct a picture of a person, one picture perfectly reconstructs all the details except for a\nmissing nostril, and the other one may be missing some details but is complete, which one would be better? We believe is\nthe latter one, because it looks more like real picture. Compare with AGE, the images generated by TAGE are more in\naccord with human perception compared to AGE. For instance, like Figure 4, the images generated in AGE sometimes\nlose some of the organs such as the eyes and nose, causing the image to collapse. However, this is rarely the case in TAGE,\nthanks to the discrete codebook structure, Code Prediction Module and Prompt-driven Semantic Module, which makes the\nperceptual quality of TAGE generated images higher.\n4.6Ablation Study\nIn order to verify the effectiveness of the proposed module, we perform ablation experiments on Animal Faces and\nFlowers dataset. The result is shown in Table 2 and figure 5.\n1) Importance of Codebook Module: A codebook can be considered as a discrete dictionary, the discretization is done\nto force the separation of different attributes, the disentanglement of attributes is very important to improve the quality of\nimage editing. The images generated only by the codebook can store more details such as the head and ear colour, which\nmakes it perform better in the FID score. On the other hand, although the FID score is high, the images generated only by\ncodebook are similar to the input images. For example, the dogs' position is same and mouth open similar way, which\nmeans they lack of diversity. These results show that when the dictionary become discrete, the quality of the image editing\nis improved and the diversity is falling. This is also foreseeable because compare to the continuous dictionary, the discrete\ncodebook is more concentrate on the optimal elements for high quality image reconstruction. For the task of few-shot\nimage generation, we need to combine the Code Prediction Module to improve the diversity of generation, and it is not\nmeaningful to use the codebook alone.\n2) Important of Code Prediction Module: To enhance the diversity of image editing and reduce image collapse, we\ndesign a Code Prediction Module to replace the Nearest-Neighbour(NN) matching. The Code Prediction Module is\ncomposed of liner layer and Transformer, aiming to predict better embedding of the edited image. In Figure 5, we can see\nthe diversity is much higher than generated only by codebook, and still retain the input images' features such as ears and\nmouth shape. As the Table 2 shows, our FID down to 74.41 and LPIPS improve to 0.5501. Most importantly, the organs\nmissing phenomenon sharply decrease with the Code Prediction Module, which is necessary for a stable image editing.\n3) Important of Prompt-driven Semantic Module: To improve model understanding and precise attribute manipulation,\nwe introduce the Semantic Prompt Module (PSM). PSM generates prompts injected into the Code Prediction Module's\n(CPM) Transformer layers. Images with PsM show more prominent and coherent editing of target attributes. In animal\nface editing, PSM preserves species traits while accentuating edited attributes like age or expression. As the Table 2 shows,\nour FID improves to 70.13 and LPIPS improve to 0.5582. PSM addresses attribute distortion or missing regions in complex\nvisual editing, ensuring coherent structures and faithful attribute modifications even with limited data.\n4.7 User Study\nTo further verify the stability and the realism of our generated images, we conduct a user study for our method. In\naddition to our method, we chose AGE that has excellent performance as our opponent. We randomly select three images\nof unseen categories from different validation datasets as the evaluation set. For each input image, we separately generated\nthree edited images as a set using AGE and TAGE. A total of 50 participants was then invited to discern and choose the\nset of images that they perceived as appearing more natural following the editing process. As suggested in Figure 6, TAGE\ndemonstrated superior performance in terms of both perceptual quality and stability. This signifies that the images\ngenerated by TAGE were deemed more visually appealing and stable by the study participants."}, {"title": "5. CONCLUSION", "content": "We propose Trustworthy Attribute Group Editing (TAGE), a novel method for unsupervised attribute group editing\nthat mitigates the crash phenomenon seen in attribute group editing. TAGE comprises three key components: (1) the\nCodebook Learning Module (CLM) that learns a discrete codebook to constrain the latent space and improve semantic\ndirection for better image generation, (2) the Code Prediction Module (CPM) that replaces Nearest-Neighbor matching to\nenhance diversity and prevent crashes caused by low-quality latent codes, and (3) the Prompt-driven Semantic Module\n(PSM) that allows the model to understand target attributes more effectively, enabling clearer attribute generation with\nfewer samples. Extensive experiments demonstrate TAGE's stability and diversity in few-shot image generation. While\nTAGE excels in many areas, some limitations remain. Occasionally, generated images may shift categories, such as\nchanges in a dog's color or petal count affecting the perceived category. Future research will focus on advanced strategies\nto disentangle category-irrelevant attributes to minimize these unintended shifts."}]}