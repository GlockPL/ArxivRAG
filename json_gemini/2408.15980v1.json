{"title": "In-Context Imitation Learning\nvia Next-Token Prediction", "authors": ["Letian Fu", "Huang Huang", "Gaurav Datta", "Lawrence Yunliang Chen", "Will Panitch", "Fangchen Liu", "Hui Li", "Ken Goldberg"], "abstract": "We explore how to enhance next-token prediction models to perform\nin-context imitation learning on a real robot, where the robot executes new tasks\nby interpreting contextual information provided during the input phase, without\nupdating its underlying policy parameters. We propose In-Context Robot Trans-\nformer (ICRT), a causal transformer that performs autoregressive prediction on\nsensorimotor trajectories without relying on any linguistic data or reward func-\ntion. This formulation enables flexible and training-free execution of new tasks\nat test time, achieved by prompting the model with sensorimotor trajectories of\nthe new task composing of image observations, actions and states tuples, collected\nthrough human teleoperation. Experiments with a Franka Emika robot demonstrate\nthat the ICRT can adapt to new tasks specified by prompts, even in environment\nconfigurations that differ from both the prompt and the training data. In a multi-\ntask environment setup, ICRT significantly outperforms current state-of-the-art\nnext-token prediction models in robotics on generalizing to unseen tasks. Code,\ncheckpoints and data are available on https://icrt.dev.", "sections": [{"title": "1 Introduction", "content": "Learning-based single and multi-task robot policies have become increasingly capable [1, 2, 3, 4,\n5, 6, 7, 8, 9, 10]. This improvement in robot capabilities can largely be attributed to progress in\nrelated fields, particularly in vision and language modeling. Inspired by the recent development\nof large language models (LLMs) and large vision models (LVMs) [11, 12, 13], which formulate\nnatural language processing and vision problems all as next-token-prediction, recent works also\nhave formulated robot learning as next-token-prediction problems and achieved state-of-the-art\nperformance [7, 8, 14, 15]. Concurrently, there has been a surge in collecting large-scale robot\ndatasets [16, 17, 18, 19, 20, 21, 22, 23] and pre-training models on these datasets [24, 25, 26, 27, 15].\nDespite being pre-trained on large datasets and showing some generalization ability, it is still\nchallenging to teach these models to perform unseen tasks in different environments without additional\ntraining. New human demonstrations via teleoperation or new data collected from hand-crafted motion\nprimitives, as well as another round of model-finetuning, are often needed to complete the new tasks.\nThis process adds complexity to the workflow, making it challenging to apply these methods in\nreal-world environments. Ideally, given one or a few demonstrations, the robot should be able to\nperform the task immediately. In their respective domains, LLMs and LVMs [11, 12, 13] have\nexhibited a similar ability, named in-context learning: a capability allowing the model to rapidly\nadapt to and recognize the task corresponding to the prompt provided at inference time without\nadditional training."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Imitation Learning for Robotics", "content": "Imitation learning is a popular and effective paradigm for equipping robots with various skills. The\nsimplest algorithm in this domain, behavior cloning, has been successful across a wide range of\ntasks [28, 29, 30]. In recent years, alternative architectures such as energy-based models [31] and"}, {"title": "2.2 In-Context Learning", "content": "Despite the effort of training on large datasets, these policies still struggle with novel tasks or\nenvironments and often require fine-tuning. Several works have explored ways to bypass the need\nfor model fine-tuning or to increase sample efficiency when generalizing to new tasks, leading to\nadvances in zero-shot and few-shot imitation learning. Some approaches in meta-learning [41, 42, 43]\nenable few-shot imitation learning after training on a wide range of tasks, but still require fine-tuning\nin each new domain. Other works don't require fine-tuning model parameters for generalizing to new\ntasks. Brown et al. [33] refers to this as \"in-context learning\" to differ from works that fine-tune the\nmodel parameters.\nMany in-context learning methods often employ contrastive learning to train context encoders, which\nidentify the most similar training tasks to the test task in the latent space [37, 44]. However, how\nto effectively integrate these methods within the next-token-prediction framework remains unclear.\nValassakis et al. [45] achieved one-shot in-context learning by training a visual servoing network\nto align the robot's end-effector with the object's relative pose during the demonstration, but this\napproach requires an additional object segmentation model. Di Palo and Johns [46] introduced\nKeypoint Action Tokens, demonstrating in-context imitation learning using a large language model\nby representing demonstration trajectories as 3D coordinates with few-shot prompting. Unlike\nthese approaches, ICRT operates without additional perception modules, processing raw image\nobservations directly. Additionally, Vid2Robot [47] developed an encoder-decoder transformer that\nuses a demonstration video of a human and the current robot state as the prompt to generate robot\nactions. However, this method requires many auxiliary losses while ICRT uses a simple next-token\nprediction loss.\nIn this paper, we focus on enhancing next-token-prediction models to perform real-world in-context\nimitation learning with robots. ICRT bypasses the need for additional context encoders by directly us-\ning robot sensorimotor trajectories from new tasks as prompts for the transformer-based model. ICRT\nis closely related to the seminal work, One-Shot Imitation Learning [48] and Prompting Decision\nTransformer [49]. [48] predicts the next action by applying cross-attention between a demonstration\nsequence on a new task and the current observation, while [49] employs a short trajectory prompt\nto encode task-specific information for guiding policy generation in offline reinforcement learning.\nHowever, neither of these approaches considers image observations as inputs, nor do they extend\nbeyond tasks in simulation. In contrast, ICRT does not model rewards, utilizes a significantly longer\ncontext window, and demonstrates in-context learning capabilities in physical experiments using\nimage observations."}, {"title": "3 Problem Statement", "content": "We investigate in-context imitation learning on a real-robot in a continuous control setting. The\nobjective is to train a model with in-context learning capabilities using a multi-task dataset. At test\ntime, the model can perform an unseen task in a new environment configuration by taking a few new\nhuman-teleoperated robot demonstrations as a prompt. We define environment configuration as the\nobjects in the scene and their locations. Importantly, this is accomplished without any additional\ntraining on the new demonstrations.\nWe define motion primitives as distinct robot motions used to complete different tasks. Each task\nis characterized by 1) a motion primitive and 2) the set of objects the robot interacts with using\nthat primitive. By varying the test-time environment configuration from the one in the prompt, we\nevaluate the model's ability to determine the appropriate motion primitive and identify the correct\nobject to interact with. In this work, we consider new tasks to be tasks involving unseen objects but\nusing motion primitives from the training data (for example, training on picking up a tiger toy and\ntesting on picking up a cube).\nWe make the following assumptions for ICRT experiments:\n1. The model is trained on a dataset consisting of diverse demonstrations of a single robot.\nEach demonstration trajectory contains observations from an RGB camera at a fixed position\nand a wrist-mounted RGB camera, proprioception, action, and the associated task type.\n2. The task tested on the robot can be completed by human teleoperating the robot and is thus\nwithin the reachable workspace of the robot."}, {"title": "4 Approach", "content": "In this section, we first introduce the data composition to facilitate in-context imitation learning. We\nthen introduce the transformer-based policy and its training formulation to leverage the data."}, {"title": "4.1 Data Formulation", "content": "For model training, we consider a dataset D of visuomotor trajectories T. Each trajectory of\nlength t is a sequence of camera images it, proprioceptive robot states st, and actions at: T =\n(i1, s1, a1, ..., it, st, at). We use the absolute end-effector pose as the robot's proprioceptive state and\nthe delta robot end-effector pose between time steps as the action, which consists of delta translation,\ndelta rotation and the continuous gripper action (see Appendix Section 8.4 for more detail). We\nassume a known grouping of the trajectories so that the dataset can be partitioned into disjoint sets of\ntasks D = \\bigcup_{K}^{k=1} S_k, with Sk \u2229 Sl = \u00d8, k \u2260 l, where Sk = {Tk1, \u2026, Tkn}. In practice, this grouping\ncan be retrieved from the semantic labels of the dataset. In this work, we utilize the existing large\nrobotic dataset DROID [50] and a multi-task dataset manually collected in our robot setup, which we\nname ICRT-Multi-Task (ICRT-MT).\nDROID [50] is a joint effort from different organizations and contains 76k real-world demonstrations.\nWe randomly sample 10k demonstrations from DROID after filtering out demonstrations shorter\nthan 30 steps and longer than 450 steps. DROID dataset labels the task through human-specified\nlanguage instructions, which may be different for the same task. We organized the DROID data by\ngrouping demonstrations based on their language instructions CLIP text embedding cosine similarity.\nSpecifically, we use a threshold of 0.9 for grouping demonstrations. To further facilitate in-context\nlearning, we make sure that each task group contains at least 4 trajectories so that there are sufficient\ntrajectories to serve as prompts for each other. This results in roughly 2k trajectories that we use for\npre-training ICRT.\nMany trajectories in the DROID dataset are collected in a single-task setup, where only one task can\nbe completed in the given environment. In such setup, the model can learn the shortcut of performing\nthe task just conditioned on the current state and observation and never looks at the prompt, even"}, {"title": "4.2 Model Architecture", "content": "To facilitate in-context learning in a robotics setting, the model should have a sufficiently long context\nwindow to support prompting by providing robot trajectories as demonstrations. We construct the\nICRT model with three parts: a pre-trained vision encoder, a series of projectors for each input\nmodality, and a causal transformer backbone (Figure 2).\nVision Encoder The model processes multi-view image observations through a pre-trained vision\ntransformer. However, most visual pre-trained networks are trained on ImageNet or human videos [27,\n51, 52, 24], which exhibit a significant domain gap when compared to typical images from robot\ndatasets, where the images frequently include robots or grippers. To minimize the domain gap,\nwe pre-train a vision transformer [53] (ViT-Base) on an equal mix of ImageNet [54] and Open\nX-Embodiment [40] data, using CrossMAE as an efficient pre-training method [55]. During the\ntraining of the ICRT model, we freeze the vision encoder for efficiency. The vision encoder outputs\nthe entire feature map for each of the cameras and is then fed into the proprioception projector\n(Figure 2 left).\nModality-Specific Projectors To project image observations, the robot's proprioceptive state, and\nactions into a shared latent space for sequence modeling, we design modality-specific projectors.\nAt each timestep, the model takes as input a token representing either the robot's state or an action."}, {"title": "5 Experiments", "content": "In this section, we design an experimental setup to evaluate the in-context learning capabilities of the\nproposed models for continuous robot control and compare them against several baselines. Instead of\nfocusing on the difficulty of learning a specific task primitive, we design the experiments to assess the\npolicy's ability to accomplish unseen tasks among all executable options from the provided prompt\ntrajectories.\nExperiment Design We consider two action primitives: a pick-and-place primitive and a poking\nprimitive. For each action primitive, we design six unseen tasks (as defined in Section 3), with\nthree tasks evaluating in-domain object generalization and three evaluating on objects unseen during\ntraining (selected from radish, blue sponge, grey dog, and black dog, see Table 1 and Table 2).\nEach task is designed with five tiers of difficulty. In the pick-and-place primitive, the model is tasked\nwith identifying which object to grasp and where to place it in a multi-object or multi-placement\nsetting. The tiers are: 1) pick and place the object without any distractors, 2) with one distractor\nobject, 3) with two distractor objects, 4) with three distractor objects, and 5) with one distractor\nplacement position. For the poking primitive, the robot must close the gripper, poke the object, lift\nthe end-effector, and open the gripper. The five tiers of difficulty involve the target object presented\nwith 0-4 distractors in the scene.\nThe pick-and-place primitive is evaluated by assigning a partial credit of 0.5 if the robot correctly\npicks up the object. A successful placement results in a total score of 1. The poking task is evaluated\nby whether the model pokes the correct object; if an incorrect object is poked, the trial is marked as a\nfailure. The model is allowed retries within a time limit of 25 seconds (or 375 steps). Each tier of\ndifficulty is performed once, and we report the average success rate per task, as well as the average\nsuccess rate and standard deviation per action primitive across the six tasks.\nAlgorithms The default ICRT model is a randomly initialized Llama2-Base model pretrained on\nDROID and fully fine-tuned on ICRT-MT. We evaluate the impact of model initialization and training\ndatasets by introducing the following three variants: 1) ICRT-Llama2, a pre-trained Llama2-7B\nlanguage model fine-tuned on ICRT-MT with LoRA; 2) ICRT (DROID), a randomly initialized"}, {"title": "6 Ablations", "content": "In this section, we provide additional experiments presented Table 1 and Table 2 that ablate on a few\ncore design choices. We provide more ablation studies in Section 8.2."}, {"title": "6.1 Model Initialization", "content": "We conducted ablation studies to examine the impact of using a pretrained Llama2 on language\ndata and fine-tune it for robot sensorimotor sequence modeling. The results, presented in Table 1\nand Table 2, show that although ICRT-Llama2-7B achieves a lower training loss, its performance is\nworse compared to its smaller counterparts. This discrepancy may be attributed to a lower inference\nfrequency of ICRT-Llama2. We suggest that future work should focus on optimizing the inference\nspeed of ICRT-Llama2."}, {"title": "6.2 Training Dataset", "content": "We find that training on the DROID subset (see Section 4.1) is insufficient for successfully completing\nany of the test tasks; the policy (ICRT (DROID)) shows no progress across all tasks. This suggests\nthat although the DROID subset may offer greater visual diversity, the unique structure of ICRT-\nMT-where multiple tasks are performed from the same initial observation\u2014is particularly beneficial\nin developing the in-context learning capabilities of a next-token prediction robot model.\nICRT (MT) shows similar performance to ICRT that is pretrained on DROID, especially for the\npick-up and place primitive, even surpassing ICRT on the put radish in grey bowl task. However,\nICRT (MT) does not perform as well on the poking primitive. The results suggest that it may be\nbeneficial to pre-train the autoregressive model on a large dataset, as a diverse dataset may help the\ntransformer to perform better alignment between visual features and control."}, {"title": "6.3 No Prompt Loss", "content": "Following the design of many multi-turn conversation large language models or vision language\nmodel fine-tuning works [34, 62, 63, 64], we do not calculate the loss for the predicted action in\nthe prompt trajectories but only do so on the predictions after the prompt trajectories. We mark the\nmodel that calculates loss on the prompt as ICRT +Prompt Loss and the default model as ICRT.\nThe results are shown in Table 3 and Table 4. We find that by letting the model only predict the\ntrajectories after the designated prompt trajectories, the model's performance improves significantly.\nWe hypothesize that in the situation where there is a loss on the prompt trajectories, the model is\nforced to do un-conditional generation based on the current environment observation, especially when\nthere are multiple possible tasks available. This may cause the model to stop paying attention to the\nprompt."}, {"title": "7 Limitations and Conclusion", "content": "This method has a few limitations. While results suggest that ICRT can generalize the primitive to\nunseen objects and certain primitives that resemble the ones in training (see Section 8.2.3), it is still\nunclear how to generalize to unseen primitives. Future works should investigate how scaling model\ncapacity and scaling dataset can help with primitive-level generalization. In addition, ICRT assumes\na fixed robot morphology with a fixed impedance controller. Future works can also investigate how\nto facilitate transfer between different robot morphologies by learning a unified policy on different\nrobots. ICRT-Llama2 has a low inference frequency which may contribute to its low performance.\nWe hope to speed up ICRT-Llama2 at inference time in the future.\nIn summary, we present ICRT, where we study in-context imitation learning on a real robot. We\ndo so by training a causal transformer model on sequences of robot trajectories, where trajectories\nof the same task are combined to serve as the context for performing the task. We also present a\ncorresponding multi-task dataset to help facilitate this in-context learning. We find that by using robot\nsensorimotor trajectories as the context, the model can generalize the learned primitives to unseen\nobjects and different environment configurations, especially in environments where more than one\ntask is present."}, {"title": "8 Supplementary Material", "content": ""}, {"title": "8.1 Scene Illustrations", "content": "We provide an illustrations on the prompt trajectories and test scenes for the pick up the black dog\nand place in the pink bowl task in Figure 5. As mentioned in Section 5, we collected 3 types of\nprompt trajectories and test ICRT on 5 tiers of scenes that are different from the scenes in the prompt\ntrajectories."}, {"title": "8.2 Ablation Studies", "content": "In this section, we provide additional ablation experiments on a few core design choices and different\nprompting strategies."}, {"title": "8.2.1 Repeatability Experiments", "content": "We conduct experiments to evaluate the repeatability of the performance of ICRT. We conduct a pick\nup the black dog and place in the pink bowl task and a poke blue sponge task for 5 rollouts, where\neach rollout contains 5 trials as in Section 5, resulting a total of 25 trials. We calculate the average\nand the standard deviation of the success rate. Results are shown in Table 5. The low std from Table 5\nsuggests that the ICRT can reliably achieve the task."}, {"title": "8.2.2 Prompt Trajectories", "content": "We conduct experiments on different prompt types to evaluate the effect of different prompt trajectories\non task performance. We consider the task of picking up a black dog and placing in a pink bowl. We\nhave three prompt trajectories of different types: one with no distractors, one with one distractor and\none with one distractor placement, as shown in Appendix Figure 5 top. All three prompts trajectories\nare collected by human teleoprating the robot. The object locations and the placement locations at test\ntime are different from that in all three prompts. As in Section 5, for each prompt type, we conduct\nthe task with 5 trials as shown in Appendix Figure 5 bottom. The average success rates are reported\nin Table 6. We conduct experiments with one prompt trajectory of different types (the first three\ncolumns in Table 6), two prompt trajectories and three prompt trajectories. All prompt types result in\nsimilar performance, indicating ICRT is not sensitive to the prompt trajectory types. We hypothesize\nthis is because during the training, ICRT has seen different types and numbers of prompts."}, {"title": "8.2.3 Unseen Primitives", "content": "We evaluate the generalization capability of ICRT on primitives that are unseen during the training\nbut resemble the training primitives. We consider two such unseen primitives: grasp and drop an\nobject and put object A to the right of object B. We consider three tasks: grasp and drop a toy tiger,\ngrasp and drop a blue sponge (unseen objects during training) and put the blue sponge to the right of\nthe toy tiger. As in Section 5, we conduct 5 tiers for each task. Experiment results are summarized in\nTable 7, where ICRT shows decent success rate on all three tasks, suggesting that ICRT can generalize\nto some unseen primitives that resemble the training primitives."}, {"title": "8.2.4 Co-training", "content": "For training ICRT, we opt to separate the training into two stages: a pre-training phase where the\nmodel is pre-trained on the DROID dataset [50], and a fine-tuning phase where the model is trained\non the ICIL-MT dataset. In this ablation, we experiment with whether these two can be combined\ninto a single stage, where the policy is end-to-end trained with DROID and ICIL-MT. To balance the\ntwo datasets, we first calculate the median number of trajectories per task across the two datasets,\nthen for each epoch, sample each task with the median number of trajectories. This allows each task\nto be equally represented in each epoch. We train the model for the same number of epochs as for\nICRT fine-tuning and report the results in Table 8 and Table 9. The results indicate that the model\ndoes not converge as quickly in the combined stage and fails to respond to prompts and complete\ntasks effectively. We hypothesize two reasons for this: firstly, the dataset is heavily biased towards\nDROID, which contains 200 tasks compared to only 26 tasks in ICIL-MT, making it difficult for the\nmodel to learn the tasks as effectively as in the separate stage training. Future works can analyze the\ndata mixture and how to train with large-scale datasets more effectively."}, {"title": "8.3 Hyperparameters", "content": "We provide the hyperparameters for both the pre-training and fine-tuning phase in Table 10 and Ta-\nble 11."}, {"title": "8.4 Parameterization", "content": "Proprioception The proprioception space is parameterized by the absolute end effector translation\n(x, y, z), a 6DoF rotation vector, and a continuous end-effector gripper state. This results in a\n10-dimensional proprioception representation. The 6DoF rotation vector is flattened from the SO(3)\nrotation's matrix's first two rows.\nAction We use delta end effector pose as the action parameterization. At each prediction step,\nthe model predicts t actions. Given absolute end effector action transforms in T1, T2,..., Tt in a\ntrajectory and the current end-effector pose $T_{ee}$, we define the relative transforms that the model needs\nto predict as $T_{ee}^{-1}T_1, T_{ee}^{-1}T_2,\u2026\u2026T_{ee}^{-1}T_t$. We then append the continuous absolute gripper position to\neach delta action. Similar to proprioception, we present the delta action by the relative end effector\ntranslation and a 6DoF rotation. This results in a 10-dimensional action representation. When rolling\nout the predicted actions, in addition to temporal ensembling [2], we also use receding horizon\ncontrol [1], and select an action horizon of 10 steps."}, {"title": "8.5 System Information", "content": "All models are trained on 4 NVIDIA A100 80GB GPUs. ICRT pre-training on DROID takes 56\nminutes and fine-tuneing on ICRT-MT takes 18 hours. ICRT-Llama7B takes roughly 28 hours to\nfinetune. We report the inference speed of ICRT and ICRT-Llama2 in Table 12 averaged over 100\nsteps. All tests are performed on a workstation with NVIDIA RTX 3090Ti and Intel i5-12400F with\n64GB memory. We find that using the proposed formulation, which can leverage the KV cache, we\ncan run ICRT-Llama2 at 10Hz naively."}]}