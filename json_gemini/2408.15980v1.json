{"title": "In-Context Imitation Learning via Next-Token Prediction", "authors": ["Letian Fu", "Huang Huang", "Gaurav Datta", "Lawrence Yunliang Chen", "Will Panitch", "Fangchen Liu", "Hui Li", "Ken Goldberg"], "abstract": "We explore how to enhance next-token prediction models to perform in-context imitation learning on a real robot, where the robot executes new tasks by interpreting contextual information provided during the input phase, without updating its underlying policy parameters. We propose In-Context Robot Transformer (ICRT), a causal transformer that performs autoregressive prediction on sensorimotor trajectories without relying on any linguistic data or reward function. This formulation enables flexible and training-free execution of new tasks at test time, achieved by prompting the model with sensorimotor trajectories of the new task composing of image observations, actions and states tuples, collected through human teleoperation. Experiments with a Franka Emika robot demonstrate that the ICRT can adapt to new tasks specified by prompts, even in environment configurations that differ from both the prompt and the training data. In a multi-task environment setup, ICRT significantly outperforms current state-of-the-art next-token prediction models in robotics on generalizing to unseen tasks. Code, checkpoints and data are available on https://icrt.dev.", "sections": [{"title": "1 Introduction", "content": "Learning-based single and multi-task robot policies have become increasingly capable [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. This improvement in robot capabilities can largely be attributed to progress in related fields, particularly in vision and language modeling. Inspired by the recent development of large language models (LLMs) and large vision models (LVMs) [11, 12, 13], which formulate natural language processing and vision problems all as next-token-prediction, recent works also have formulated robot learning as next-token-prediction problems and achieved state-of-the-art performance [7, 8, 14, 15]. Concurrently, there has been a surge in collecting large-scale robot datasets [16, 17, 18, 19, 20, 21, 22, 23] and pre-training models on these datasets [24, 25, 26, 27, 15].\nDespite being pre-trained on large datasets and showing some generalization ability, it is still challenging to teach these models to perform unseen tasks in different environments without additional training. New human demonstrations via teleoperation or new data collected from hand-crafted motion primitives, as well as another round of model-finetuning, are often needed to complete the new tasks. This process adds complexity to the workflow, making it challenging to apply these methods in real-world environments. Ideally, given one or a few demonstrations, the robot should be able to perform the task immediately. In their respective domains, LLMs and LVMs [11, 12, 13] have exhibited a similar ability, named in-context learning: a capability allowing the model to rapidly adapt to and recognize the task corresponding to the prompt provided at inference time without additional training."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Imitation Learning for Robotics", "content": "Imitation learning is a popular and effective paradigm for equipping robots with various skills. The simplest algorithm in this domain, behavior cloning, has been successful across a wide range of tasks [28, 29, 30]. In recent years, alternative architectures such as energy-based models [31] and diffusion models [1] have also been proposed. Typically, these approaches require training a separate model for each task, although multi-task policies can be distilled from these task-specific models after training [32].\nRecent advancements have shown that using transformers for next-token prediction in sequence modeling has been particularly effective in both language and vision domains, especially for multi-task learning [33, 12, 34]. This approach has also been extended to robotic learning, where robot action planning is framed as a next-token prediction task using transformer-based architectures [35, 36, 7, 8, 14, 15]. In these models, observations are used to predict the next robot actions. In addition, in pursuit of developing generalist agents and robust robot policies, recent research has demonstrated that training policies on large, diverse datasets encompassing multiple tasks can lead to more robust and generalizable models [37, 38, 39, 40, 15, 5, 7, 36]. Octo [15] and OpenVLA [14] are trained on large robotic datasets, and are the state-of-the-art policies conditioning on goal images and language instructions (Octo) or just language instructions (OpenVLA). Octo employs a transformer with a diffusion head, which processes the goal conditions\u2014language instructions or goal images\u2014and the current image observation to predict robot actions. OpenVLA fine-tunes a pre-trained vision-language model to predict robot actions given vision observations and language instructions."}, {"title": "2.2 In-Context Learning", "content": "Despite the effort of training on large datasets, these policies still struggle with novel tasks or environments and often require fine-tuning. Several works have explored ways to bypass the need for model fine-tuning or to increase sample efficiency when generalizing to new tasks, leading to advances in zero-shot and few-shot imitation learning. Some approaches in meta-learning [41, 42, 43] enable few-shot imitation learning after training on a wide range of tasks, but still require fine-tuning in each new domain. Other works don't require fine-tuning model parameters for generalizing to new tasks. Brown et al. [33] refers to this as \"in-context learning\" to differ from works that fine-tune the model parameters.\nMany in-context learning methods often employ contrastive learning to train context encoders, which identify the most similar training tasks to the test task in the latent space [37, 44]. However, how to effectively integrate these methods within the next-token-prediction framework remains unclear. Valassakis et al. [45] achieved one-shot in-context learning by training a visual servoing network to align the robot's end-effector with the object's relative pose during the demonstration, but this approach requires an additional object segmentation model. Di Palo and Johns [46] introduced Keypoint Action Tokens, demonstrating in-context imitation learning using a large language model by representing demonstration trajectories as 3D coordinates with few-shot prompting. Unlike these approaches, ICRT operates without additional perception modules, processing raw image observations directly. Additionally, Vid2Robot [47] developed an encoder-decoder transformer that uses a demonstration video of a human and the current robot state as the prompt to generate robot actions. However, this method requires many auxiliary losses while ICRT uses a simple next-token prediction loss.\nIn this paper, we focus on enhancing next-token-prediction models to perform real-world in-context imitation learning with robots. ICRT bypasses the need for additional context encoders by directly us-ing robot sensorimotor trajectories from new tasks as prompts for the transformer-based model. ICRT is closely related to the seminal work, One-Shot Imitation Learning [48] and Prompting Decision Transformer [49]. [48] predicts the next action by applying cross-attention between a demonstration sequence on a new task and the current observation, while [49] employs a short trajectory prompt to encode task-specific information for guiding policy generation in offline reinforcement learning. However, neither of these approaches considers image observations as inputs, nor do they extend beyond tasks in simulation. In contrast, ICRT does not model rewards, utilizes a significantly longer context window, and demonstrates in-context learning capabilities in physical experiments using image observations."}, {"title": "3 Problem Statement", "content": "We investigate in-context imitation learning on a real-robot in a continuous control setting. The objective is to train a model with in-context learning capabilities using a multi-task dataset. At test time, the model can perform an unseen task in a new environment configuration by taking a few new human-teleoperated robot demonstrations as a prompt. We define environment configuration as the objects in the scene and their locations. Importantly, this is accomplished without any additional training on the new demonstrations.\nWe define motion primitives as distinct robot motions used to complete different tasks. Each task is characterized by 1) a motion primitive and 2) the set of objects the robot interacts with using that primitive. By varying the test-time environment configuration from the one in the prompt, we evaluate the model's ability to determine the appropriate motion primitive and identify the correct object to interact with. In this work, we consider new tasks to be tasks involving unseen objects but using motion primitives from the training data (for example, training on picking up a tiger toy and testing on picking up a cube).\nWe make the following assumptions for ICRT experiments:\n1. The model is trained on a dataset consisting of diverse demonstrations of a single robot. Each demonstration trajectory contains observations from an RGB camera at a fixed position and a wrist-mounted RGB camera, proprioception, action, and the associated task type.\n2. The task tested on the robot can be completed by human teleoperating the robot and is thus within the reachable workspace of the robot."}, {"title": "4 Approach", "content": "In this section, we first introduce the data composition to facilitate in-context imitation learning. We then introduce the transformer-based policy and its training formulation to leverage the data."}, {"title": "4.1 Data Formulation", "content": "For model training, we consider a dataset D of visuomotor trajectories T. Each trajectory of length t is a sequence of camera images \\(i_t\\), proprioceptive robot states \\(s_t\\), and actions \\(a_t\\): \\(T = (i_1, s_1, a_1, ..., i_t, s_t, a_t)\\). We use the absolute end-effector pose as the robot's proprioceptive state and the delta robot end-effector pose between time steps as the action, which consists of delta translation, delta rotation and the continuous gripper action (see Appendix Section 8.4 for more detail). We assume a known grouping of the trajectories so that the dataset can be partitioned into disjoint sets of tasks \\(D = \\cup_{k=1}^K S_k\\), with \\(S_k \\cap S_l = \\emptyset\\), \\(k \\neq l\\), where \\(S_k = \\{T_{k1}, \u2026, T_{kn}\\}\\). In practice, this grouping can be retrieved from the semantic labels of the dataset. In this work, we utilize the existing large robotic dataset DROID [50] and a multi-task dataset manually collected in our robot setup, which we name ICRT-Multi-Task (ICRT-MT).\nDROID [50] is a joint effort from different organizations and contains 76k real-world demonstrations. We randomly sample 10k demonstrations from DROID after filtering out demonstrations shorter than 30 steps and longer than 450 steps. DROID dataset labels the task through human-specified language instructions, which may be different for the same task. We organized the DROID data by grouping demonstrations based on their language instructions CLIP text embedding cosine similarity. Specifically, we use a threshold of 0.9 for grouping demonstrations. To further facilitate in-context learning, we make sure that each task group contains at least 4 trajectories so that there are sufficient trajectories to serve as prompts for each other. This results in roughly 2k trajectories that we use for pre-training ICRT.\nMany trajectories in the DROID dataset are collected in a single-task setup, where only one task can be completed in the given environment. In such setup, the model can learn the shortcut of performing the task just conditioned on the current state and observation and never looks at the prompt, even though the prompt trajectories are similar to the current task to be performed. Therefore multi-task data is crucial for the model to learn from the prompt. We manually collected a multi-task dataset ICRT-Multi-Task (ICRT-MT) using the DROID setup (Figure. 4). This dataset has 1098 trajectories in total, and contains 29 tasks with 6 primitives: picking, pick-and-place, stacking, pushing, poking, opening and closing drawers. Objects used in the data collection and examples of the primitives are shown in Figure. 4. In ICRT-MT, each environment is set so that there exist more than 2 possible tasks for the current observation so that the model has to distinguish and learn the motion from the prompt.\nDuring the training, for each trajectory, we independently apply vision augmentation on the image observations by augmenting the brightness and contrast. We additionally apply random crops and scaling to the side camera observation. We also apply proprioception noise sampled from a normal Gaussian distribution \\(\\mathcal{N}(0, 0.005)\\). For each epoch, we randomly shuffle the order of trajectories from each task and concatenate them to form the training sequence. For each batch, we subsample for a subsequence of length \\(L = 512\\) as the input to the model, where L is the sequence length defined as the number of observation, state, and action tuples. In practice, 512 steps usually contain up to 5 trajectories from the same task. We randomly select the first k trajectories and label them as the prompt within the sequence. At least one complete trajectory is included in the prompt. This data grouping aims to capture inter-trajectory patterns, encouraging the model to generate action conditioned on the prompt trajectories. This approach differs from traditional behavior cloning methods, which typically use short input sequences that focus on modeling intra-trajectory behaviors."}, {"title": "4.2 Model Architecture", "content": "To facilitate in-context learning in a robotics setting, the model should have a sufficiently long context window to support prompting by providing robot trajectories as demonstrations. We construct the ICRT model with three parts: a pre-trained vision encoder, a series of projectors for each input modality, and a causal transformer backbone (Figure 2).\nVision Encoder The model processes multi-view image observations through a pre-trained vision transformer. However, most visual pre-trained networks are trained on ImageNet or human videos [27, 51, 52, 24], which exhibit a significant domain gap when compared to typical images from robot datasets, where the images frequently include robots or grippers. To minimize the domain gap, we pre-train a vision transformer [53] (ViT-Base) on an equal mix of ImageNet [54] and Open X-Embodiment [40] data, using CrossMAE as an efficient pre-training method [55]. During the training of the ICRT model, we freeze the vision encoder for efficiency. The vision encoder outputs the entire feature map for each of the cameras and is then fed into the proprioception projector (Figure 2 left).\nModality-Specific Projectors To project image observations, the robot's proprioceptive state, and actions into a shared latent space for sequence modeling, we design modality-specific projectors.\nAt each timestep, the model takes as input a token representing either the robot's state or an action."}, {"title": "5 Experiments", "content": "In this section, we design an experimental setup to evaluate the in-context learning capabilities of the proposed models for continuous robot control and compare them against several baselines. Instead of focusing on the difficulty of learning a specific task primitive, we design the experiments to assess the policy's ability to accomplish unseen tasks among all executable options from the provided prompt trajectories.\nExperiment Design We consider two action primitives: a pick-and-place primitive and a poking primitive. For each action primitive, we design six unseen tasks (as defined in Section 3), with three tasks evaluating in-domain object generalization and three evaluating on objects unseen during training (selected from radish, blue sponge, grey dog, and black dog, see Table 1 and Table 2).\nEach task is designed with five tiers of difficulty. In the pick-and-place primitive, the model is tasked with identifying which object to grasp and where to place it in a multi-object or multi-placement setting. The tiers are: 1) pick and place the object without any distractors, 2) with one distractor object, 3) with two distractor objects, 4) with three distractor objects, and 5) with one distractor placement position. For the poking primitive, the robot must close the gripper, poke the object, lift the end-effector, and open the gripper. The five tiers of difficulty involve the target object presented with 0-4 distractors in the scene.\nThe pick-and-place primitive is evaluated by assigning a partial credit of 0.5 if the robot correctly picks up the object. A successful placement results in a total score of 1. The poking task is evaluated by whether the model pokes the correct object; if an incorrect object is poked, the trial is marked as a failure. The model is allowed retries within a time limit of 25 seconds (or 375 steps). Each tier of difficulty is performed once, and we report the average success rate per task, as well as the average success rate and standard deviation per action primitive across the six tasks.\nAlgorithms The default ICRT model is a randomly initialized Llama2-Base model pretrained on DROID and fully fine-tuned on ICRT-MT. We evaluate the impact of model initialization and training datasets by introducing the following three variants: 1) ICRT-Llama2, a pre-trained Llama2-7B language model fine-tuned on ICRT-MT with LoRA; 2) ICRT (DROID), a randomly initialized Llama2-Base model trained only on the DROID dataset; and 3) ICRT (MT), a randomly initialized Llama2-Base model trained only on the ICRT-MT dataset.\nWe consider 3 baseline algorithms. We train a goal-conditioned policy, where in each sample of the dataset, the goal observation and state pair are always prepended to the sequence, and in each sequence, there exists only one trajectory. This resembles the normal goal-conditioned imitation learning setup. Additionally, we finetune Octo [15], the state-of-the-art goal-image and language conditioned policy, and OpenVLA [14], the state-of-the-art language conditioned multi-task imitation learning algorithm. Octo is fine-tuned using their official fine-tuning recipe. We incorporate action chunking into OpenVLA by asking it to predict the next 16 actions, which performs better than vanilla OpenVLA which predicts only the next step. Both of these methods are representative of robot policies that use next-token prediction objectives.\nPrompt Generation For each task, we collect 3 demonstrations (with zero, one distractor object, a distractor placement for pick-and-place, or two distractor objects for poking) as the prompt in total before running the experiment. Please refer to the Appendix 8.1 Figure 5 for a visual example. During testing, a random demonstration is drawn as a prompt to assess the model's ability to generalize to different prompts. It's important to note that the environment setup during policy rollout differs from the prompts' setup, ensuring that the evaluation measures the model's understanding of task-relevant information from the prompt, rather than simply copying actions from it.\nResults We present the results in Table 1 and Table 2. For the pick-and-place primitive, we observe that the goal-conditioned policy generally succeeds in identifying the correct object to grasp when no distractor objects are present. However, its performance degrades significantly as the number of distractors increases. When the goal image only specifies the task but not the specific way to achieve it in the current environment, goal-conditioned policies often fail to execute the task effectively.\nOcto struggles with determining which object to interact with and where it should be placed, high- lighting the challenges posed by our experimental setup for multi-task policies. OpenVLA, while often moving towards the correct object, frequently fails in grasping the object or mistakenly performs the wrong task (e.g., grasping instead of poking, and vice versa). This indicates that OpenVLA may require a greater number of demonstrations (more than 50) per task to achieve better performance, and that relying solely on language conditioning may not be sufficient for generalization to new tasks.\nThe results suggest that ICRT outperforms the goal-conditioned policy in identifying the correct object to pick up and the appropriate placement location. The poking task presents a significant challenge for the goal or language-conditioned policies, as the goal position often closely resembles the start configuration. However, after conditioning on the prompt trajectory, ICRT is able to correctly identify the task as poking, and the results indicate that it consistently reaches the correct target object while ignoring distractors. Despite this, we do observe some failure modes with ICRT, such as missing the grasp of the target object, grasping the wrong object, or placing objects in incorrect locations. Specifically, when a distractor object shares the same color but has a different shape, the model struggles to accurately determine which object to grasp. This implies that additional fine-tuning of the vision encoder might be required to enhance model performance, a conclusion also reached by OpenVLA [14]."}, {"title": "6 Ablations", "content": "In this section, we provide additional experiments presented Table 1 and Table 2 that ablate on a few core design choices. We provide more ablation studies in Section 8.2."}, {"title": "6.1 Model Initialization", "content": "We conducted ablation studies to examine the impact of using a pretrained Llama2 on language data and fine-tune it for robot sensorimotor sequence modeling. The results, presented in Table 1 and Table 2, show that although ICRT-Llama2-7B achieves a lower training loss, its performance is worse compared to its smaller counterparts. This discrepancy may be attributed to a lower inference frequency of ICRT-Llama2. We suggest that future work should focus on optimizing the inference speed of ICRT-Llama2."}, {"title": "6.2 Training Dataset", "content": "We find that training on the DROID subset (see Section 4.1) is insufficient for successfully completing any of the test tasks; the policy (ICRT (DROID)) shows no progress across all tasks. This suggests that although the DROID subset may offer greater visual diversity, the unique structure of ICRT- MT-where multiple tasks are performed from the same initial observation\u2014is particularly beneficial in developing the in-context learning capabilities of a next-token prediction robot model.\nICRT (MT) shows similar performance to ICRT that is pretrained on DROID, especially for the pick-up and place primitive, even surpassing ICRT on the put radish in grey bowl task. However, ICRT (MT) does not perform as well on the poking primitive. The results suggest that it may be beneficial to pre-train the autoregressive model on a large dataset, as a diverse dataset may help the transformer to perform better alignment between visual features and control."}, {"title": "6.3 No Prompt Loss", "content": "Following the design of many multi-turn conversation large language models or vision language model fine-tuning works [34, 62, 63, 64], we do not calculate the loss for the predicted action in the prompt trajectories but only do so on the predictions after the prompt trajectories. We mark the model that calculates loss on the prompt as ICRT +Prompt Loss and the default model as ICRT. The results are shown in Table 3 and Table 4. We find that by letting the model only predict the trajectories after the designated prompt trajectories, the model's performance improves significantly. We hypothesize that in the situation where there is a loss on the prompt trajectories, the model is forced to do un-conditional generation based on the current environment observation, especially when there are multiple possible tasks available. This may cause the model to stop paying attention to the prompt."}, {"title": "7 Limitations and Conclusion", "content": "This method has a few limitations. While results suggest that ICRT can generalize the primitive to unseen objects and certain primitives that resemble the ones in training (see Section 8.2.3), it is still unclear how to generalize to unseen primitives. Future works should investigate how scaling model capacity and scaling dataset can help with primitive-level generalization. In addition, ICRT assumes a fixed robot morphology with a fixed impedance controller. Future works can also investigate how to facilitate transfer between different robot morphologies by learning a unified policy on different robots. ICRT-Llama2 has a low inference frequency which may contribute to its low performance. We hope to speed up ICRT-Llama2 at inference time in the future.\nIn summary, we present ICRT, where we study in-context imitation learning on a real robot. We do so by training a causal transformer model on sequences of robot trajectories, where trajectories of the same task are combined to serve as the context for performing the task. We also present a corresponding multi-task dataset to help facilitate this in-context learning. We find that by using robot sensorimotor trajectories as the context, the model can generalize the learned primitives to unseen objects and different environment configurations, especially in environments where more than one task is present."}, {"title": "8 Supplementary Material", "content": ""}, {"title": "8.1 Scene Illustrations", "content": "We provide an illustrations on the prompt trajectories and test scenes for the pick up the black dog and place in the pink bowl task in Figure 5. As mentioned in Section 5, we collected 3 types of prompt trajectories and test ICRT on 5 tiers of scenes that are different from the scenes in the prompt trajectories."}, {"title": "8.2 Ablation Studies", "content": "In this section, we provide additional ablation experiments on a few core design choices and different prompting strategies."}, {"title": "8.2.1 Repeatability Experiments", "content": "We conduct experiments to evaluate the repeatability of the performance of ICRT. We conduct a pick up the black dog and place in the pink bowl task and a poke blue sponge task for 5 rollouts, where each rollout contains 5 trials as in Section 5, resulting a total of 25 trials. We calculate the average and the standard deviation of the success rate. Results are shown in Table 5. The low std from Table 5 suggests that the ICRT can reliably achieve the task."}, {"title": "8.2.2 Prompt Trajectories", "content": "We conduct experiments on different prompt types to evaluate the effect of different prompt trajectories on task performance. We consider the task of picking up a black dog and placing in a pink bowl. We have three prompt trajectories of different types: one with no distractors, one with one distractor and one with one distractor placement, as shown in Appendix Figure 5 top. All three prompts trajectories are collected by human teleoprating the robot. The object locations and the placement locations at test time are different from that in all three prompts. As in Section 5, for each prompt type, we conduct the task with 5 trials as shown in Appendix Figure 5 bottom. The average success rates are reported in Table 6. We conduct experiments with one prompt trajectory of different types (the first three columns in Table 6), two prompt trajectories and three prompt trajectories. All prompt types result in similar performance, indicating ICRT is not sensitive to the prompt trajectory types. We hypothesize this is because during the training, ICRT has seen different types and numbers of prompts."}, {"title": "8.2.3 Unseen Primitives", "content": "We evaluate the generalization capability of ICRT on primitives that are unseen during the training but resemble the training primitives. We consider two such unseen primitives: grasp and drop an object and put object A to the right of object B. We consider three tasks: grasp and drop a toy tiger, grasp and drop a blue sponge (unseen objects during training) and put the blue sponge to the right of the toy tiger. As in Section 5, we conduct 5 tiers for each task. Experiment results are summarized in Table 7, where ICRT shows decent success rate on all three tasks, suggesting that ICRT can generalize to some unseen primitives that resemble the training primitives."}, {"title": "8.2.4 Co-training", "content": "For training ICRT, we opt to separate the training into two stages: a pre-training phase where the model is pre-trained on the DROID dataset [50], and a fine-tuning phase where the model is trained on the ICIL-MT dataset. In this ablation, we experiment with whether these two can be combined into a single stage, where the policy is end-to-end trained with DROID and ICIL-MT. To balance the two datasets, we first calculate the median number of trajectories per task across the two datasets, then for each epoch, sample each task with the median number of trajectories. This allows each task to be equally represented in each epoch. We train the model for the same number of epochs as for ICRT fine-tuning and report the results in Table 8 and Table 9. The results indicate that the model does not converge as quickly in the combined stage and fails to respond to prompts and complete tasks effectively. We hypothesize two reasons for this: firstly, the dataset is heavily biased towards DROID, which contains 200 tasks compared to only 26 tasks in ICIL-MT, making it difficult for the model to learn the tasks as effectively as in the separate stage training. Future works can analyze the data mixture and how to train with large-scale datasets more effectively."}, {"title": "8.3 Hyperparameters", "content": "We provide the hyperparameters for both the pre-training and fine-tuning phase in Table 10 and Ta- ble 11."}, {"title": "8.4 Parameterization", "content": "Proprioception The proprioception space is parameterized by the absolute end effector translation (x, y, z), a 6DoF rotation vector, and a continuous end-effector gripper state. This results in a 10-dimensional proprioception representation. The 6DoF rotation vector is flattened from the SO(3) rotation's matrix's first two rows.\nAction We use delta end effector pose as the action parameterization. At each prediction step, the model predicts t actions. Given absolute end effector action transforms in \\(T_1, T_2,..., T_t\\) in a trajectory and the current end-effector pose \\(T_{ee}\\), we define the relative transforms that the model needs to predict as \\(T_{ee}^{-1}T_1, T_{ee}^{-1}T_2, \u2026\u2026, T_{ee}^{-1}T_t\\). We then append the continuous absolute gripper position to each delta action. Similar to proprioception, we present the delta action by the relative end effector translation and a 6DoF rotation. This results in a 10-dimensional action representation. When rolling out the predicted actions, in addition to temporal ensembling [2], we also use receding horizon control [1], and select an action horizon of 10 steps."}, {"title": "8.5 System Information", "content": "All models are trained on 4 NVIDIA A100 80GB GPUs. ICRT pre-training on DROID takes 56 minutes and fine-tuneing on ICRT-MT takes 18 hours. ICRT-Llama7B takes roughly 28 hours to finetune. We report the inference speed of ICRT and ICRT-Llama2 in Table 12 averaged over 100 steps. All tests are performed on a workstation with NVIDIA RTX 3090Ti and Intel i5-12400F with 64GB memory. We find that using the proposed formulation, which can leverage the KV cache, we can run ICRT-Llama2 at 10Hz naively."}]}