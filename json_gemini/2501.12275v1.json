{"title": "With Great Backbones Comes Great Adversarial Transferability", "authors": ["Erik Arakelyan", "Karen Hambardzumyan", "Davit Papikyan", "Pasquale Minervini", "Albert Gordo", "Isabelle Augenstein", "Aram H. Markosyan"], "abstract": "Advancements in self-supervised learning (SSL) for machine vision have enhanced representation robustness and model performance, leading to the emergence of publicly shared pre-trained backbones, such as ResNet and ViT models tuned with SSL methods like SimCLR. Due to the computational and data demands of pre-training, the utilization of such backbones becomes a strenuous necessity. However, employing such backbones may imply adhering to the existing vulnerabilities towards adversarial attacks. Prior research on adversarial robustness typically examines attacks with either full (white-box) or no access (black-box) to the target model, but the adversarial robustness of models tuned on known pre-trained backbones remains largely unexplored. Furthermore, it is unclear which tuning meta-information is critical for mitigating exploitation risks. In this work, we systematically study the adversarial robustness of models that use such backbones, evaluating 20000 combinations of tuning meta-information, including fine-tuning techniques, backbone families, datasets, and attack types. To uncover and exploit potential vulnerabilities, we propose using proxy (surrogate) models to transfer adversarial attacks, fine-tuning these proxies with various tuning variations to simulate different levels of knowledge about the target. Our findings show that proxy-based attacks can reach close performance to strong black-box methods with sizable budgets and closing to white-box methods, exposing vulnerabilities even with minimal tuning knowledge. Additionally, we introduce a naive \"backbone attack\", leveraging only the shared backbone to create adversarial samples, demonstrating an efficacy surpassing black-box and close to white-box attacks and exposing critical risks in model-sharing practices. Finally, our ablations reveal how increasing tuning meta-information impacts attack transferability, measuring each meta-information combination.", "sections": [{"title": "1. Introduction", "content": "Machine vision models pre-trained with massive amounts of data and using self-supervised techniques (Newell & Deng, 2020) are shown to be robust and highly performing(Goyal et al., 2021a; Goldblum et al., 2024) feature-extracting backbones (Elharrouss et al., 2022; Han et al., 2022), which are further used in a variety of tasks, from classification (Atito et al., 2021; Chen et al., 2020b) to semantic segmentation (Ziegler & Asano, 2022). However, creating such backbones incurs substantial data annotation (Jing & Tian, 2020) and computational costs (Han et al., 2022), consequently rendering the use of such publicly available pre-trained backbones the most common and efficient solution for researchers and engineers alike. Prior works have focused on analysing safety and adversarial robustness with complete, i.e. white-box (Porkodi et al., 2018) or no, i.e. black-box (Bhambri et al., 2019) knowledge of the target model weights, fine-tuning data, fine-tuning techniques and other tuning meta-information. Although, in practice, an attacker can access partial knowledge (Lord et al., 2022; Zhu et al., 2022; Carlini et al., 2022) of how the targeted model was produced, i.e. original backbone weights, tuning recipe, etc., the adversarial robustness of models tuned on a downstream task from a given pre-trained backbone remains largely under-explored. We refer to settings with partial knowledge of target model constructions meta-information as grey-box. This is important both for research and production settings because with an increased usage (Goldblum et al., 2023) of publically available pre-trained backbones for downstream applications, we are incapable of assessing the potential exploitation susceptibility and inherent risks within models tuned on top of them and subsequently enhance future pre-trained backbone sharing practices.\nIn this work, we systematically explore the safety towards adversarial attacks within the models tuned on a downstream"}, {"title": "2. Related Work", "content": "Self Supervised Learning With the emergence of massive unannotated datasets in machine vision, such as YFCC100M(Thomee et al., 2016), ImageNet(Deng et al., 2009), CIFAR (Krizhevsky et al., 2009) and others Self Supervised Learning (SSL) techniques (Jing & Tian, 2021) became increasingly more popular for pre-training the models (Newell & Deng, 2020). This prompted the creation of various families of SSL objectives, such as colorization prediction (Zhang et al., 2016), jigsaw puzzle solving (Noroozi & Favaro, 2016) with further invariance constraints (Misra & van der Maaten, 2020, PIRL), non-parametric instance discrimination (Wu et al., 2018, NPID, NPID++), unsupervised clustering (Caron et al., 2018), rotation prediction (Gidaris et al., 2018, RotNet), sample clustering with cluster assignment constraints(Caron et al., 2020, SwAV), contrastive representation entanglement (Chen et al., 2020a, SimCLR), self-distillation without labels (Caron et al., 2021, DINO) and others (Jing & Tian, 2021). Numerous architectures, like AlexNet (Krizhevsky et al., 2012), variants of ResNet(He et al., 2016) and visual transformers (Dosovitskiy et al., 2021; Touvron et al., 2021; Ali et al., 2021) were trained using these SSL methods and shared for public use, thus forming the set of widely used pre-trained backbones. We obtain all of these models trained with different self-supervised objectives from their original designated studies summarised in VISSL (Goyal et al., 2021b).\nAdversarial Attacks The availability of pre-trained backbones allows to test them for vulnerabilities towards adversarial attacks, which are learnable imperceptible perturbations generated to mislead models into making incorrect"}, {"title": "3. Methodology", "content": "Preliminaries For consistency, we employ the following notation. We denote each Dataset $D$ = ${x,y}$."}, {"title": "Meta-Information variations", "content": "We define the variations of the available meta-information about the target model $M$ during an adversarial attack as a unit of release $R$ = $R(M(D, W, B(WB), F(T, Z)))$. For example, if the target fine-tuning mode $Z_{target}$ and dataset $D^{target}$ are not known, the unit of release will be $R$ = $R(M(*, W, B(WB), F(T, *)))$. Note that the black-box setting will correspond to the unit of release $R(M(*,*,*,*,*))$ and the white-box setting to $R(M(D, W, B(WB),F(T, Z)))$, all the variations between these are considered grey-box. When discussing any experiments within the gery-box setup, we assume the minimal unit of release contains knowledge about at least the pre-trained backbone i.e. $R(M(*, *, B(WB), *)$."}, {"title": "Adversarial Attacks with Proxy Models", "content": "To test the adversarial robustness of the models trained from the same pre-trained backbone, we create a set of proxy models $M_{proxy} = {M^{proxy} . . . M^{proxy} }^v$ given the pre-trained backbone $B$, where $v$ is the number of all possible units of release between black-box and white-box settings that include the backbone. For each proxy model $M^{proxy}$ with its designated meta-information unit of release $R_i$, we use an adversarial attack $A$ to generate adversarial noise and further transfer it to the target model $M_{target}$. This means that given an example image $x$ with a label $y$, target and proxy models $M_{target}, M^{proxy}$ we want to produce a sample $x'$ that would fool the target model, such that $\\arg \\max M_{target}(x') \\neq y$. If we are using a targeted attack then we want $M_{target}(x') = t$ where $t$ is the targeted class different from the ground truth $t\\neq cgt$. After creating the adversarial attack for each sample in $D^{proxy}$ and $D^{target}$ we evaluate the success rate of the attack and the success rate of the transferability onto the target model. To measure the success and robustness of the adversarial attack and its transferability, we define the following metrics:\n\u2022 Attack Success Rate (ASR): This is the proportion of adversarial examples successfully fooling the proxy"}, {"title": "3.1. Backbone Attack", "content": "To test the vulnerabilities associated with publicly available pre-trained feature extractors, we designed a naive backbone attack, which only utilises the known backbone B of the model $M_{target}$. The aim, similar to the prior paragraph, is to create an adversarial attack from the B to transfer towards the target model $M_{target}$. To do this, we utilise a Projected Gradient Descent (Madry et al., 2018, PGD)-based method, where the attack iteratively perturbs the input images in order to maximise the distance between the feature representations of the clean input and the adversarial input, as derived from the backbone B. More formally, let $x$ and  represent the clean input and adversarial input, respectively. The attack iteratively refines 2 such that:\n$x_{t+1}$ = $Proj_S$ $(x_t + \\alpha \\cdot sign (\\nabla_{x_t}L_{B(x,t)}))$, (3)\nwhere $L_B$ is the loss function defined to measure the distance between the feature representations of the clean and adversarial inputs. The backbone representations $f_B$ are extracted as $f(x) = B(x)$, and the differentiable loss can be formulated as:\n$L_B(x,\\tilde{x})$ = 1 \u2212 cos $(f_B(x), f_B(\\tilde{x}))$, (4)\nwhere cos $(,)$ represents the cosine similarity between the two feature vectors. To prevent gradient computation from propagating to the clean representation $f(x)$, we utilize a stop-gradient operation $f(x)$ = SG$(f(x))$. The adversarial input $\\tilde{x}$ is initialized with a random perturbation within the $l_\\infty$ ball of radius $e$, and the updates are iteratively projected back onto this ball using the $Proj_S$ operator:\n$Proj_S (x)$ = clip $(x + \\delta, 0, 1)$,\nwhere $\\delta$ = clip $(x - \\tilde{x}, -e, e)$. (5)\nThe pseudo-code of the complete process can bee seen in Algorithm 1. In summary, the backbone attack focuses solely on the backbone B, without requiring any knowledge of the full target model $M_{target}$, thereby revealing vulnerabilities inherent to publicly available feature extractors."}, {"title": "4. Experimental Setup", "content": "Image classification datasets Through our study, we use 4 datasets covering both classical and domain-specific classification benchmarks, such as CIFAR-10 and CIFAR-100 (Beyer et al., 2020) and Oxford-IIIT Pets (Parkhi et al., 2012), Oxford Flowers-102 (Nilsback & Zisserman, 2008). We train the proxy and target model variation on each one of the datasets using the recipe from (Kolesnikov et al., 2020), reproducing the state-of-the-art model performance results (Dosovitskiy et al., 2020; Yu et al., 2022; Bruno et al., 2022; Foret et al., 2020)."}, {"title": "5. Results", "content": "5.1. What meta-information matters\nTo quantify the impact of each possible meta-information availability along with the backbone knowledge during adversarial attack construction, we compute the difference between the adversarial attack success rate (ASR) for the target model and the transferability success rate (TSR) from"}, {"title": "5.2. Backbone-attacks", "content": "To test the extent of the vulnerabilities that the knowledge of the pre-trained backbone can cause, we evaluate our naive exploitation method, backbone attack, that utilizes only the pre-trained feature extractor for adversarial sample construction. Our results in Figure 2 and Figure 4 show that backbone attacks are highly effective at producing transferable adversarial samples regardless of the target model tuning mode, dataset or classification layer depth. This naive attack shows significantly higher transferability compared to a strong black-box attack with a sizeable query and iteration budget and almost all proxy attacks. The results are consistent across all meta-information variations, showing that even a naive attack can exploit the target model vulnerabilities closely to a white-box setting, given the knowledge of the pre-trained backbone. Moreover, from Figure 3, we see that the adversarial samples produced from this attack, on average, cause a bigger shift in the model's decision-making compared to white-box attacks. This indicates that backbone attacks amplify the uncertainty in the target model's predictions, making them more disruptive than conventional white-box attacks, highlighting the inherent risks of sharing pre-trained backbones for public use. A concerning aspect of backbone attacks is their effectiveness in resource-constrained environments. Unlike black-box attacks, which often require extensive computation or iterative querying, backbone attacks can be executed with minimal resources, leveraging pre-trained models freely available in public repositories. This ease of implementation raises"}, {"title": "5.3. Knowing weights vs Knowing everything but the weights", "content": "To isolate the impact of pre-trained backbone knowledge in adversarial transferability, we train two sets of models from the same ResNet-50 SwAV backbone with identical meta-information variations but different batch sizes. This allows the production of two sets of models with matching training meta-information but varying weights; one set is chosen as the target, and the other as the proxy model. We aim to compare the adversarial transferability of the attacks from the set of proxies towards their matching targets with the backbone attacks. This allows us to simulate conditions where adversaries either know all meta-information but lack the weights or have access to the backbone weights alone. Our results in Figure 5 show that the knowledge of the pre-trained backbone is, on average, a stronger or at least an equivalent signal for producing adversarially transferable attacks compared to possessing all of the training meta-information without the knowledge of the weights. The results are consistent across all of the datasets, with domain-specific datasets showing marginal differences in adversarial transferability between the two scenarios. This means that possessing information about only the target model backbone is equivalent to knowing all of the training meta-information for constructing transferable adversarial samples."}, {"title": "6. Conclusions", "content": "In this paper, we investigated the vulnerabilities of machine vision models fine-tuned from publicly available pre-trained backbones under a novel grey-box adversarial setting. Through an extensive evaluation framework, including over 20,000 adversarial transferability comparisons, we measured the effect of varying levels of training meta-information availability for constructing transferable adversarial attacks. We also introduced a naive backbone attack method, showing that access to backbone weights is sufficient for obtaining adversarial attacks significantly better than query-based black-box settings and approaching white-box performance. We found that attacks crafted using only the backbone weights often induce more substantial shifts in the model's decision-making than traditional white-box attacks. We demonstrated that access to backbone weights is equivalent in effectiveness to possessing all meta-information about the target model, making public backbones a critical security concern. Our results highlight significant security risks associated with sharing pre-trained backbones, as they enable attackers to craft highly effective adversarial samples, even with minimal additional information. These findings underscore the need for stricter practices in sharing and deploying pre-trained backbones to mitigate the inherent vulnerabilities exposed by adversarial transferability."}]}