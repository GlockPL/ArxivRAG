{"title": "CAN IN-CONTEXT LEARNING REALLY GENERALIZE TO OUT-OF-DISTRIBUTION TASKS?", "authors": ["Qixun Wang", "Yifei Wang", "Yisen Wang", "Xianghua Ying"], "abstract": "In this work, we explore the mechanism of in-context learning (ICL) on out-of- distribution (OOD) tasks that were not encountered during training. To achieve this, we conduct synthetic experiments where the objective is to learn OOD mathematical functions through ICL using a GPT-2 model. We reveal that Transformers may struggle to learn OOD task functions through ICL. Specifically, ICL performance resembles implementing a function within the pretraining hypothesis space and optimizing it with gradient descent based on the in-context examples. Additionally, we investigate ICL's well-documented ability to learn unseen abstract labels in context. We demonstrate that such ability only manifests in the scenarios without distributional shifts and, therefore, may not serve as evidence of new-task- learning ability. Furthermore, we assess ICL's performance on OOD tasks when the model is pretrained on multiple tasks. Both empirical and theoretical analy- ses demonstrate the existence of the low-test-error preference of ICL, where it tends to implement the pretraining function that yields low test error in the testing context. We validate this through numerical experiments. This new theoretical result, combined with our empirical findings, elucidates the mechanism of ICL in addressing OOD tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Pretrained large language models (LLMs) can perform in-context learning (ICL) (Brown, 2020), where providing a few examples of input-output pairs and a query example improves the model's ability to generate the desired output, compared to zero-shot predictions. Understanding ICL's abil- ity to learn out-of-distribution (OOD) input-output relationships, which are unseen during training, has recently gained significant attention.\nRecent studies have demonstrated that ICL can tackle seemingly new tasks. For instance, Garg et al. (2022); Ravent\u00f3s et al. (2024); Zhang et al. (2023); Aky\u00fcrek et al. (2022) found that ICL can learn new linear regression weights after pretraining on a large set of weight vectors. Moreover, Pan (2023); Kossen et al. (2024); Vacareanu et al. (2024) revealed that real-world LLMs like Llama-2 (Touvron et al., 2023) and GPT-4 (Achiam et al., 2023) are capable of solving artificially constructed tasks likely unseen in their pretraining data, such as a classification task with abstract labels.\nHowever, another line of research (Yadlowsky et al., 2023; Ahuja & Lopez-Paz, 2023) has raised a contrasting view, showing that ICL struggles to generalize to OOD tasks where there are distri- butional shifts in either the input distribution P(X) or the input-label mapping P(Y|X). These findings raise several important questions:\nCan ICL really learn new input-output mappings from the context? What under- lying mechanism of ICL determines its performance on OOD tasks?"}, {"title": "2 TRANSFORMERS IMPLEMENTS FUNCTIONS CLASSES SEEN DURING PRETRAINING", "content": "Exploring the ICL performance on OOD tasks. To investigate the ICL performance on new tasks that are unseen during training, following Garg et al. (2022), we train a GPT-2 (Radford et al., 2019) from scratch on some simple functions and evaluate it on functions different from the training ones. Denote the Transformer model parameterized by @ as Me. The pretraining objective is:\n$\\min_\\theta \\frac{1}{T} \\sum_{i=1}^T E_{f \\sim F}[||M_\\theta(S_i \\oplus x_{i+1}) - f(x_i)||_2^2],\\qquad(1)$$\nwhere S\u1d62 = [x\u2081 \u2295 y\u2081 \u2295 x\u2082 \u2295 y\u2082 \u2295 ... \u2295 x\u1d62 \u2295 y\u1d62] \u2208 \\mathbb{R}^{d\u00d72i} is the context of length i, \u2295 denotes concatenation. x\u1d62 \u2208 \\mathbb{R}^{d} are sampled from a standard normal distribution \\mathcal{N}(0, I) with dimension d = 20. Let y\u1d62 = f(x\u1d62) represent the labels, with \\mathcal{F} denoting the hypothesis class to which f belongs. We train three separate GPT-2 models on three different function classes \\mathcal{F}: linear regression, quadratic regression (element-wise square followed by linear regression), and a 2-layer ReLU network (detailed descriptions are in Appendix A.1). We then evaluate their ICL performance on these three tasks. Note that even when the testing and training functions are i.i.d. sampled from the same task, the specific instances of the testing functions remain unseen during training. For comparison, we also train models within the corresponding \\mathcal{F} with gradient descent (GD) using the testing in-context examples (details in Appendix A.1).\nObservations. We plot the test error on the three tasks in Figure 1 and observe that: 1) when evaluated on the same task \\mathcal{F} as pretraining, ICL can reach near-zero test error, as observed in previous works (Garg et al., 2022). 2) when evaluated on a new task, ICL performs similarly to the corresponding model of the pretraining function class optimized by GD, given enough in-context examples. This indicates that the ICL prediction implicitly implements function classes seen during pretraining. 3) The models trained on linear and quadratic regression exhibit a double descent error"}, {"title": "3 LEARNING ABSTRACT LABELS MAY NOT BE A NEW-TASK-LEARNING ABILITY", "content": "Recent works (Pan et al., 2023; Kossen et al., 2024) have shown that LLMs can perform classification tasks in which the labels are \"abstract symbols\" with no semantic meaning. For instance, in the SST- 2 binary classification task, the labels 'positive' and 'negative' are substituted with abstract terms like 'foo' and 'bar', respectively. These tasks are likely not seen during pretraining. Pan et al. (2023) refer to this ability of ICL to perform such classification as \"task learning\" (TL). In this section, we explore whether the TL ability really reflects a new-task-learning capability of ICL or if it merely stems from the model having learned similar tasks during pretraining.\nThe retrieval ability can be gained by pretraining on a retrieval task with diverse input-label mappings. The classification of abstract labels can be approached by first retrieving an example"}, {"title": "3.1 CLASSIFICATION TASKS WITH UNSEEN ABSTRACT LABELS", "content": "with semantics similar to the query and then outputting the label of that example, as empirically demonstrated in previous research (Wang et al., 2023; Yu & Ananiadou, 2024). Therefore, the retrieval ability is a crucial prerequisite for performing abstract-label classification. To investigate whether ICL's retrieval capability can emerge from training on similar tasks, we construct a retrieval task. Specifically, we generate a predefined word embedding \\mathbf{E} \u2208 \\mathbb{R}^{N\u00d7d} and randomly sample x\u1d62\u2208 \\mathbb{R}^d from the first 5 rows of E. Let the index of x in E be I_x; (where I_{x_i} \u2208 [0,5)). The labels y\u1d62 are generated by mapping I_{x_i} to new indices I_{y_i} \u2208 [N] according to a mapping rule I_{y_i} = R_s(I_{x_i}) = I_{x_i} + s (with s \u2208 \\mathbb{N}), and then taking out E_{I_{y_i}} as y\u1d62. All in-context examples in a sequence share the same mapping rule R_s. To accomplish this task, the model must retrieve the same token as the query example from the context and output its subsequent token.\nWe train three models with three different ranges of s: s \u223c \\mathcal{U}(50,150), s \u223c \\mathcal{U}(50, 250), and s \u223c \\mathcal{U}(50,450) and evaluate on s \u223c \\mathcal{U}(50, 150), s \u223c \\mathcal{U}(10, 20), and s \u223c \\mathcal{U}(500, 600), where \\mathcal{U} denotes the uniform distribution. We plot the test error in Figure 2."}, {"title": "3.2 ABSTRACT LABEL CLASSIFICATION CAN ONLY BE ACHIEVED ON IN-DISTRIBUTION TASKS", "content": "A retrieval task with OOD testing functions & observations. One might question whether, once a task can be solved through retrieval, ICL can generalize beyond the training function class. To investigate this, we replace the testing functions of linear regression with quadratic regression while preserving linear regression as the pretraining task. The results in Figure 5 show that the generaliza- tion doesn't improves with training on more in-distribution functions. Combining observations from Figure 4, we conclude that only when the testing task is in distribution can ICL solve classification with unseen labels."}, {"title": "3.3 REAL-WORLD LLMS MAT NOT NECESSARILY IN-CONTEXT LEARN NEW TASKS", "content": "Evaluating Llama2 on an OOD synthetic word classification task. In this section, we assess whether real-world LLMs can tackle OOD tasks through ICL. We select the pretrained Llama-2-7b and evaluate it on a synthetic word classification task. To ensure the task is far from the pretraining distribution, we randomly sample x\u1d62 \u2208 \\mathbb{R}^d from the word embedding of Llama-2-7b (denoted as E_{Llama}) and generate random linear mappings w \u2208 \\mathbb{R}^{C\u00d7d} as task functions (where C = 10). The label words are created by mapping x\u1d62 to one of the ten label vectors in E_{Llama} using w. Experimental details are in Appendix A.3. To complete this task, the model must learn w in context.\nFor comparison, we also evaluate the ICL performance of Llama-2-7b on a retrieval version of this task, where we sample C classes of tokens from E_{Llama} to generate random sequences. The objective is to retrieve the same tokens as the query token from the context and output its subsequent token. The results of these two tasks are presented in Figure 6."}, {"title": "4 THE ALGORITHM SELECTION MECHANISM EXISTS BROADLY WHEN EVALUATED ON OOD TASKS", "content": "Real-world LLMs are pretrained on a huge corpus that could contain massive tasks. Bai et al. (2024); Yadlowsky et al. (2023) have empirically found that the ICL performance of Transformers trained on multiple tasks approaches the optimal pretraining function when evaluated on one of the training tasks. In this section, we will show that this algorithm-selection phenomenon of ICL persists even"}, {"title": "4.1 THEORETICAL ANALYSIS", "content": "A mixed Gaussian pretraining dataset of multiple tasks. In this section, we theoretically analyze the algorithm selection mechanism of ICL on OOD tasks, based on the theoretical framework of Lin & Lee (2024). Consider a noisy linear regression pretraining dataset with the inputs and task weights following the mixed Gaussian distribution:\nAssumption 4.1. (Mixed Gaussian pretraining data) Input distribution: $P(x|\\mu) = \\mathcal{N}(x|\\mu, \\sigma^2I)$, label distribution: $P(y|x,w) = \\mathcal{N} (y|x^\\top w, \\sigma^2)$. The input means and task weights are sampled from a mixed Gaussian distribution: $P(\\mu, w) = \\sum_{m=1}^M \\pi_m\\mathcal{N}(\\mu; \\mu_m, \\sigma^2I) \\cdot \\mathcal{N} (w; w_m, \\sigma_I^2)$, where $\\sum_{m=1}^M \\pi_m = 1$, $0 < \\pi_m < 1$ and $\\|\\mu_m \\|_2 = \\|w_m \\|_2 = 1, \\forall m$. Define $\\delta_\\mu = \\frac{\\sigma}{\\sigma_I}$ and $\\delta_w = \\frac{\\sigma_y}{\\sigma_I}$. Each training sequence $S_T = [x_1 \\oplus y_1 \\oplus ... x_T \\oplus y_T]$ is constructed by first sampling the input mean and the task weight according to $P(\\mu, w)$ and then sampling $x_i$ and $y_i$ according to $P(x|\\mu)$ and $P(y|x, w)$, respectively. Denote this pretraining distribution as $P_{tr}$.\nThe lemma below states that the closed-form prediction of the model trained on the pretraining data under Assumption 4.1, given the testing context, remains a Gaussian mixture of the reweighted pretraining task weights.\nLemma 4.2. (Corollary 2. of Lin & Lee (2024), closed-form ICL prediction of the pretrained model) Denote the model M* that minimizes the risk on the pretraining data of Assumption 4.1, i.e., $\\mathcal{M}^* \\in \\arg \\min_\\mathcal{M} \\frac{1}{T}\\sum_{S_i \\sim P_{tr}}[||\\mathcal{M}(S_i \\oplus x_{i+1}) - y_{i+1}||^2]$, then the prediction on any sequence $S_T \\oplus x_{i+1}$ by $\\mathcal{M}^*$ is as follows: $\\mathcal{M}^* (S_i \\oplus x_{i+1}) = (x_{i+1}, \\sum_{m=1}^M \\bar{\\pi}_m \\bar{w}_m)$. where $\\bar{\\pi}_m$, and $\\bar{w}_m$ depending on both the pretraining task and the downstream context example are given in Lemma 1 of Lin & Lee (2024).\nBased on the closed-form ICL prediction, we now analyze how the downstream context affects \\bar{\\pi}, which determines how ICL selects the pretraining functions. First, we introduce Lemma 4.3 that characterizes the ratio of the reweighted weight of two pretraining tasks:"}, {"title": "4.2 EMPIRICAL VALIDATION OF THE ALGORITHM-SELECTION MECHANISM OF ICL", "content": "Now we validate our theoretical findings regarding ICL's algorithm-selection mechanism in OOD tasks by conducting numerical experiments following Lin & Lee (2024). In Figure 8 and 9, the train- ing data is a Gaussian mixture with four components (see Assumption 4.1), while the test function is a two-layer ReLU network (Appendix A.1). Both the training and the test data are in ICL format. We compute the test error of using each pretraining task function to predict the downstream function (the first row of Figure 8 and 9), the weights for each pretraining function during ICL inference (the second row), and the test error of the pretrained ICL model (the third row). We evaluate five different noise levels (\u03b4\u03b1 = \u03b4\u03c9 \u2208 {1/81,9/1,1,9, 81}) and consider two settings described below.\nLow-test-error preference of ICL. To validate Theorem 4.4, we ensure that the distributional dis- tances between the inputs of each training task and the test data remain consistent. Specifically, all x\u1d62 in both training data and test data are sampled from N([0,0,0], \u03c3\u00b2I). The task weights for different pretraining tasks vary, as detailed in the top half of Table 1. In this setup, only the test error of the pretraining functions influences algorithm selection. From the top two rows of Figure 8, we"}, {"title": "5 COMPARISON WITH RELATED WORKS", "content": "Abilities to learn new tasks of ICL. Besides studies indicating that ICL can learn new weights of linear regression (Garg et al., 2022; Ravent\u00f3s et al., 2024; Zhang et al., 2023; Aky\u00fcrek et al., 2022), other research has found that LLMs can tackle tasks that are unlikely to have been encoun- tered during pretraining. For example, Pan (2023) showed that LLMs perform better than random guessing on classification tasks with meaningless labels. Kossen et al. (2024) demonstrated that ICL can identify authorship based on writing style in private communication messages not included in the pretraining corpus. Additionally, Vacareanu et al. (2024) found that large-scale LLMs can learn various linear and non-linear functions from context. We argue that these findings do not contradict our work. While the LLMs may not have seen exactly the same tasks, there is no guarantee that they haven't encountered tasks from a similar distribution in their pretraining corpus. For instance, the LLMs could have been pretrained on a corpus containing authorship identification tasks or on"}, {"title": "6 CONCLUSION", "content": "In this work, we empirically find that Transformers struggle to generalize beyond the pretraining function classes when given downstream in-context examples of OOD tasks. Instead, ICL tries to seek a near-optimal solution within the pretraining function classes. However, ICL performs well in retrieval tasks where the shift in the input-label mapping is only caused by replacing the in-context label tokens with new ones while the underlying function distribution retains. We also examine ICL's performance on OOD tasks after pretraining on multiple tasks. Our theoretical and empirical analysis reveals ICL's preference for low-test-error functions, i.e., ICL tends to implement pretraining function classes with low test error in the test context. This finding, alongside previous work (Lin & Lee, 2024), highlights two key factors that determine how ICL will implement the prediction function based on the testing context and pretraining tasks: the distance between the training and testing input distributions, and the ability of a pretraining function to solve the test task."}, {"title": "7 LIMITATIONS", "content": "1) Most experimental results are based on a GPT-2 model pretrained on a limited set of mathematical functions. It is challenging to assess whether modern large-scale language models like GPT-4 and Claude 3 Opus face similar difficulties in generalizing beyond their pretraining corpus, given the vast range of tasks and content in their pretraining data. Nevertheless, our findings highlight the limitations of ICL for smaller models like Llama2-7b. 2) The models are trained on ICL data, while real-world LLMs are trained autoregressively. However, the ICL pretraining objective is also next-token prediction, so there is no clear gap between these two pretraining objectives."}, {"title": "8 REPRODUCIBILITY", "content": "In the main text and Appendix A, we've stated all setups for reproducing our experimental results. For the theoretical part, we've included the assumptions (Assumption C.2) and proofs in Appendix C."}]}