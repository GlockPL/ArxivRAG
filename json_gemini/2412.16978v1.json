{"title": "PromptDresser: Improving the Quality and Controllability of Virtual Try-On via Generative Textual Prompt and Prompt-aware Mask", "authors": ["Jeongho Kim", "Hoiyeong Jin", "Sunghyun Park", "Jaegul Choo"], "abstract": "Recent virtual try-on approaches have advanced by fine-tuning the pre-trained text-to-image diffusion models to leverage their powerful generative ability. However, the use of text prompts in virtual try-on is still underexplored. This paper tackles a text-editable virtual try-on task that changes the clothing item based on the provided clothing image while editing the wearing style (e.g., tucking style, fit) according to the text descriptions. In the text-editable virtual try-on, three key aspects exist: (i) designing rich text descriptions for paired person-clothing data to train the model, (ii) addressing the conflicts where textual information of the existing person's clothing interferes the generation of the new clothing, and (iii) adaptively adjust the inpainting mask aligned with the text descriptions, ensuring proper editing areas while preserving the original person's appearance irrelevant to the new clothing. To address these aspects, we propose PromptDresser, a text-editable virtual try-on model that leverages large multi-modal model (LMM) assistance to enable high-quality and versatile manipulation based on generative text prompts. Our approach utilizes LMMs via in-context learning to generate detailed text descriptions for person and clothing images independently, including pose details and editing attributes using minimal human cost. Moreover, to ensure the editing areas, we adjust the inpainting mask depending on the text prompts adaptively. We found that our approach, utilizing detailed text prompts, not only enhances text editability but also effectively conveys clothing details that are difficult to capture through images alone, thereby enhancing image quality. Extensive experiments demonstrate that our method outperforms the baselines significantly while showing versatile manipulation capabilities based on text prompts. Our code is available at https://github.com/rlawjdghek/PromptDresser.", "sections": [{"title": "1. Introduction", "content": "Virtual try-on [21] provides an advanced technique for personalized fashion previews, removing the need for physical trials. Moreover, the capability to control wearing styles, such as tucking style or fit, can enhance overall user experience by allowing users to visualize different outfit options.\nThanks to the advanced generative capabilities of diffusion models [14, 25, 41], virtual try-on have shown significant improvements compared to earlier generative adversarial network-based approaches [3, 11, 17\u201319, 22, 29, 47, 52]. These recent models leverage the powerful priors embedded in large-scale diffusion models [9, 39, 42, 44], resulting in improvements in clothing detail representation and generalization performance. Furthermore, recent studies have enhanced generative quality by incorporating text inputs with advanced text-to-image (T2I) models [12, 34] and have added an editing capability via click or drag [10]. However, although these approaches are built on text-based generative models [39, 42], they typically use simple textual prompts of clothing attributes, such as \u2018short sleeve t-shirt.' The potential for using rich textual information to achieve higher performance and extensive text-driven editability is still largely unexplored.\nIn this paper, we tackle a text-editable virtual try-on task that changes the clothing item based on the provided clothing image and edits the wearing style according to the text descriptions. Text-editable virtual try-on has unique and challenging aspects. (i) It is crucial to construct rich and well-aligned text descriptions for wearing the new clothing to the person image, (ii) avoiding textual conflicts between the original and new clothing during sampling. Since the wearing styles (e.g., untucked, fully tucked in) vary according to the the text descriptions, (iii) an adaptive mask aligned with the text prompt is necessary to preserve regions irrelevant to the clothing and to minimize the influence of the original clothing shape.\nTo address these challenges, we introduce PromptDresser, a novel virtual try-on model that leverages rich textual information to achieve high-quality and versatile manipulation. To obtain the text prompts for paired person-clothing data, we instruct the large multimodal models (LMMs) to describe the person and clothing images, respectively. However, LMMs often produce excessively diverse text descriptions on the provided images, leading to potential misalignment with the images or insufficient details. Therefore, we leverage in-context learning that conditions expressive layouts and clothing, effectively specifying attributes to focus on the inpainting regions with minimal human cost. This approach significantly outperforms the model that relies on holistic descriptions of the entire person and clothing image, providing a scalable and efficient solution for versatile virtual try-on. Furthermore, we found that detailed text descriptions produced by LMMs not only enhance text editability but also effectively maintain clothing details that are difficult to capture through clothing images alone, thereby generating high-fidelity images.\nFollowing the previous approaches [10\u201312, 28, 29, 38], we also utilize an inpainting mask to determine the preserved and generated regions. The traditional inpainting mask often constrains the model to the shape of the existing clothing, leading to unnatural results when adapting to different clothing types. To mitigate such constraints, we applied random dilation mask augmentation, enabling the model to learn from a wide range of mask sizes, from broad to narrow. Expanding the masked area effectively removes details related to the original clothing's length and shape, but it also poses the risk of erasing areas that should be preserved (e.g., pants when generating a top). Therefore, we propose to use an expanded inpainting mask to obtain an approximate clothing region aligned with the text prompt. Next, we generate a refined mask by combining this expanded mask with a fine mask that removes areas where clothing will be applied (e.g., arms or torso) to preserve as much of the original person and background as possible. By employing a refined mask that is agnostic to the existing clothing and aligned with the text prompts, our model preserves details irrelevant to the worn clothing, enabling a wide range of text-based manipulation. Through extensive experiments, we demonstrate that PromptDresser achieves superior image quality compared to the existing virtual try-on methods while effectively controlling the wearing styles using diverse text prompts.\nIn summary, our contributions are as follows:\n\u2022 We propose a text-editable virtual try-on model that, with the assistance of a large multimodal model (LMM) with in-context learning, achieves rich, well-aligned text descriptions for both person and clothing, ensuring no textual conflicts with any clothing provided.\n\u2022 To mitigate the issue of following the original clothing's attributes, such as shape and length, we propose random dilation mask augmentation. Our prompt-aware mask generation enhances diversity in virtual try-on results while effectively preserving the person's original appearance.\n\u2022 Our approach achieves state-of-the-art performance across multiple datasets with and enables versatile manipulation capabilities, highlighting the effectiveness of generative textual prompt for virtual try-on."}, {"title": "2. Related Work", "content": "2.1. Image-based Virtual Try-On\nEarly approaches often relied on two-stage frameworks, combining explicit warping networks with GAN-based generators [3, 11, 17, 29, 50, 52]. However, these methods continue to struggle with error accumulation across multiple stages, leading to a shift towards diffusion models which have shown impressive generative capabilities across various domains [6, 43, 54, 55, 58]. Due to the challenges of dataset acquisition and the advantages of leveraging prior knowledge, most approaches build upon large-scale text-to-image diffusion models [9, 39, 42, 44], inherently benefiting from their robust inpainting capabilities [19] or utilizing textual inversion techniques [38]. Notably, recent research [12, 28] propose attention-based, end-to-end virtual try-on models that preserve fine details of clothing while achieving the generalization performance.\nAnother line of progress in image-based virtual try-on is controllability. LC-VTON [53], for instance, introduced new segmentation labels to incorporate clothing length, enabling the generation of high-fidelity images. Additionally, some studies [10, 31] have utilized landmarks to introduce point-based controllability. While these spatial condition-based approaches offer a fine degree of controllability, they are limited in their ability to perform comprehensive editing such as adjusting fit and overall appearance. On the other hand, recent research leveraging text-based controllability has tried to address such challenges. However, most existing studies still incorporate text but fail to consider the existing areas (e.g., background) that need to be preserved [59] or are limited to captioning solely for clothing [12, 34].\nIn this paper, we propose a virtual try-on model that uses rich text descriptions to harness the capabilities of large-scale text-to-image diffusion models. By independently extracting captions for both the person and the clothing with LMMs, we enhance generalization performance and enable manipulation. Moreover, we propose a novel adaptive mask for further flexible manipulation while preserving the original person's appearance.\n2.2. LMMs for Multimodal Data Augmentation\nRecent advancements in large multimodal models (LMMs) [2, 30, 35, 46, 57] have demonstrated their powerful visual understanding [36, 48], achieving impressive performance across various vision tasks. Harnessing the capabilities of LMMs, they are utilized to tune image editing models [7] and to enhance captioning performance though image-caption fusion [4]. However, their application to virtual try-on remains still underexplored. In this paper, we leverage off-the-shelf LMMs to augment image captions from virtual try-on datasets, enabling the generation of higher-fidelity images. Furthermore, we allow users to wear diverse styles through text-based manipulation according to their preferences."}, {"title": "3. Method", "content": "3.1. Preliminary: Latent Diffusion Model\nOur approach builds on pre-trained text-to-image latent diffusion models (LDMs) [39, 42], which consist of three main components: a variational auto-encoder (VAE) with an encoder E(\u00b7) and decoder G(\u00b7), a text encoder \u03c4(\u00b7) and main U-Net \u03f5\u03b8(\u00b7). The pre-trained VAE encodes an image x into a low-dimensional latent space as z0 = E(x) and reconstructs it back into RGB space as x = G(z0). The main U-Net is trained to predict the z0 from the perturbed latent variable zt, defined as $z_t = \\mathcal{N}(z_t; \\sqrt{\\bar{\\alpha}_t}z_0, (1 - \\bar{\\alpha}_t)I)$. Here, $\\bar{\\alpha}_t = \\prod_{s=0}^{t}(1 - \\beta_s)$, where $(\\beta_t)_{t=0}^T$ is a decreasing sequence [25]. The loss function of LDMs is given by:\n$\\mathcal{L}_{LDM} = \\mathbb{E}_{z_0\\sim E(x), \\epsilon\\sim \\mathcal{N}(0,1), y, t} [||\\epsilon - \\epsilon_\\theta(z_t, t, \\tau(y))||^2]$ (1)\nThis objective function directs the model to reduce the difference between the added noise, \u03f5, and the noise predicted by the U-Net. The predicted noise is then used in the reverse diffusion process to approximate the original latent representation.\n3.2. Overall Framework\nOur method, PromptDresser, aims to enable text-editable virtual try-on of reference clothing x0 onto a target person xp with additional generative textual prompt provided by LMMs. Following the existing virtual try-on works [19, 28, 38], we adopt an inpainting framework, which reconstructs the target person image from a masked version, conditioned on the reference clothing image. Specifically, the main U-Net generates the target person image based on the input including a noise image (zt), a resized dilated clothing-agnostic mask (R(md)), and a latent agnostic map (E(xp)). Here, the dilated clothing-agnostic mask md is produced by a random dilation mask augmentation.\nTo preserve fine clothing details, we leverage a frozen U-Net as a feature extractor [39, 45], referred to as the reference U-Net. We then integrate the clothing features of reference U-Net by concatenating the key and value from self-attention layers of the reference U-Net with those of corresponding layers in the main U-Net [51].\nTo obtain rich text descriptions, we introduce an LMM-driven captioning mechanism. Merely providing a detailed description on person images can result in redundancy, as it may encompass areas that remain unmasked. Furthermore, this approach fails to address the unique nature of unpaired person-clothing data in virtual try-on, where the information of the person's original clothing and the new clothing can become entangled. Therefore, we designed our approach to separate information by pre-defining distinct attributes for the person and clothing.\nThe existing clothing-agnostic person representation [11, 29] fails to precisely remove structural features like clothing length, causing the generated images to conform closely to the shape of the original clothing, which limits the flexibility for manipulation. On the other hand, overly expanded mask regions make it difficult to accurately reconstruct the original person's information. Therefore, we introduce random dilation mask augmentation, enabling the model to learn a range of mask images from coarse to fine. This approach allows for a prompt-aware mask generation (PMG) during the inference, providing preservation of the original person's appearance irrelevant to the reference clothing.\n3.3. LMM-driven Virtual Try-on Captioning\nIn this work, we propose to improve the quality and controllability of virtual try-on via generative textual prompt. The primary challenges are: 1) ensuring that the text description focuses on the masked region and 2) excluding textual information about the existing clothing in the inference process. A naive approach would be to instruct the LMM to describe the person image, which results in detailed information about visible features in the unmasked regions, such as expression and hairstyle, as in \u201cThe image showcases a bold fashion statement... large hoop earrings and curly, voluminous hair enhance the overall stylish and confident look.\u201d. Moreover, describing the entire image in this way can introduce textual conflicts when virtually fitting new clothing, as information about the person's existing outfit may also be included.\nTo address this, we propose designing pre-defined attributes and tasking the LMM with generating captions based on these attributes. Using the LMM, we listed the attributes of both person and clothing images and carefully selected $n_{{p,c}}$ representative attributes $A_{{p,c}} = {a_1^{{p,c}}, ..., a_{n_{{p,c}}}^{{p,c}}}$. Specifically, for person-related attributes, we prioritized those within the masked region, including hand pose, body shape, and tucking style, along with attributes that support editability. Due to the limited token length of the backbone's text encoder (i.e., CLIP), we focused on selecting global clothing features, such as category and material, rather than local details like logos. The main advantages of using the LMM to generate captions based on pre-defined attributes for both person and clothing are: 1) it enables the generation of informative text descriptions specifically for the inpainting areas and 2) the separation of information for person and clothing allows us to create adequate prompts aligned to unpaired person-clothing scenarios, enabling descriptions of individuals wearing new clothing. Furthermore, we instruct the LMM to provide a detailed description of the pose. By using pose descriptions instead of DensePose [20], we retain the text-to-image backbone architecture and effectively reduce the errors associated with pose networks used in previous works, particularly in complex samples.\nAs depicted on the left of Fig. 2, we then utilize the in-context learning capability [8] of LMM models to generate rich, free-form captions for pre-defined attributes. Based on the observation that multi-modal models are proficient at predicting head categories such as gender [33], we carefully select $N$ few-shot exemplar images, each exhibiting different subtle details such as tucking or rolling style. Human annotators then label each caption to capture these specific details accurately.\nTherefore, for an arbitrary person or clothing image $x \\in {x_p, x_c}$ and a LMM $M$, the predicted captions $K_{{p,c}}$ is obtained through in-context learning as:\n$K_{{p,c}} = {k_1^{{p,c}}, ..., k_{N}^{{p,c}}} = M(x\\vert \\mathcal{P}, D_{{p,c}}^{ex}, T_{{p,c}})$, (2)\nwhere $\\mathcal{P}$ is the input prompt, $D_{{p,c}}^{ex}$ is the in-context learning dataset consisting of few-shot examples for captioning, and $T_{{p,c}}$ is the task description.\nUsing the predicted captions, we construct textual prompts for both the reference U-Net and the main U-Net. For the reference U-Net, the reference prompt $y^r$ includes only the clothing-specific captions $K_c$. In contrast, the main prompt $y^m$ combines both the person-specific captions $K_p$ and the clothing-specific captions $K_c$. In practice, we use the following format as the main prompt: \u201ca {body shape (a1)} {gender (a2)} wears {cloth category (a3)}, {material (a4)}, ..., with {hand pose (an)}.\u201d, where green and blue color denote person and clothing attributes, respectively. Therefore, our approach can generate the main prompt for the resulting image, even when arbitrary clothing is provided.\nEach prompt is processed through a text encoder before being input to its respective U-Net.\n3.4. Enhancing Adaptability via Mask Refinement\nRandom Dilation Mask Augmentation. We train a virtual try-on model via generative textual prompt that allows for text-based editing. However, we note the limitations in the commonly used masking approach, known as clothing-agnostic person representation [11], frequently used in virtual try-on methods. While such a masking approach effectively preserves the original person's appearance, it also retains certain features from the original clothing such as length and fit. This constrained mask region causes the reference clothing to fit too closely to the mask boundaries during training, making the new clothing mimic the shape of the original clothing and creating potential conflicts during manipulation.\nTo address these issues, we propose random dilation mask augmentation. As illustrated in Fig 2, we introduce a coarse mask $m_c$ and a fine mask $m_f$ to enable learning across a diverse range of masked images. We randomly dilate the fine mask, ensuring it does not extend beyond the boundaries of the coarse mask. The dilated mask $m_d$ used for training is represented as follows:\n$m_d = (m_f \\oplus b) \\cap m_c$, (3)\nwhere $\\oplus^n$ denotes n-iterated dilation with a structuring element $b$ [23], up to a sufficiently large but finite $n$.\nPrompt-aware Mask Generation. For the inference stage, we introduce a novel coarse-to-fine generation approach to effectively preserve the original person's appearance while allowing flexible text-based image manipulation. As illustrated on the left side of Fig. 3, we begin inference with a coarse mask, aiming to create an initial approximation of the clothing region aligned with the text prompt. For efficiency, we apply early stopping in the denoising process, running it only from timestep $T$ to $\\sigma T$, where $\\sigma\\in [0,1)$.\nWe then approximate $z_0$, decode it to $\\hat{x}_0$, and segment the region of interest using an off-the-shelf human parsing model. By taking the union of this mask with an existing clothing-agnostic person representation (fine mask), we acquire a refined mask, which subsequently serves as the in-painting mask for generating the final output. This method achieves a balance between precision and efficiency, improving the alignment of the output with the text prompt."}, {"title": "4. Experiment", "content": "Benchmarks. We train PromptDresser separately on VITON-HD [11] and DressCode [37] datasets, and SHHQ-1.0 is used to evaluate the generalizability of the model trained on VITON-HD [28].\nWe compare our model to two GAN-based models (HR-VITON [29], GP-VTON [50]) and four diffusion-based models (LADI-VTON[38], DCI-VTON [19], Stable-VITON [28], and IDM-VTON [12]). We use pre-trained weights if available; otherwise, we re-implement them using official code. LADI-VTON, DCI-VTON, and Stable-VITON, all based on Stable Diffusion 1.5, generate images at 512\u00d7384 resolution. To ensure a fair comparison, we upscale the outputs to 2\u00d7 using Real-ESRGAN [49].\nImplementation Details. We utilize a frozen SDXL [39] and SDXL inpainting model [26] as the reference and main U-Net, respectively. During inference, we set the denoising step as 30 with \u03c3 set to 0.5 for prompt-aware mask generation. To maintain overall pose consistency, we retain hand and foot details within the inpainting mask by Sapiens [27]. Additionally, we use GPT-40o [1] to automatically generate high-quality captions for pre-defined attributes across all experimental datasets.\n4.1. Comparison with Baselines\nQualitative Results Fig. 4 shows a comparative analysis of our model and baseline models trained on the VITON-HD [11] dataset (first row), evaluated both on the VITON-HD test dataset and the SHHQ-1.0 [16] dataset (second row). The baselines either closely follow the original clothing shape or lack detailed clothing features (especially on SHHQ-1.0). In contrast, our model accurately captures the details and shape of the reference clothing while preserving the features of the original person image, such as background and pose. Additionally, we compared images of the upper body, lower body, and dresses on the DressCode [37] dataset in Fig. 5. Across these images, the baselines tend to follow to the original clothing shape. Specifically in the lower body, baselines generate unnatural jeans by following the existing pants shape even when provided with a short skirt; similarly, for dresses, models including OOTD-iffusion and IDM-VTON produce excessively long dresses. This demonstrates that well-aligned and rich textual information enhances model generalization performance, and our adaptive mask disregard the original clothing shape but retain the person's appearance.", "4.2. Further Analyis on PromptDresser": null}, {"title": "Ablation Study", "content": "Evaluation on Text Alignment To validate our method's editing capability, we generated edited versions of 2,032 test images from VITON-HD by fixing a specific attribute (e.g., \"tucking style\") to a caption (e.g., \"untucked\") and then evaluated whether the captions generated by the LMM for these edited images matched the intended caption. We conducted experiments with two settings: (i) setting the tucking style to \u201cuntucked\u201d (i.e., the top is worn outside the pants) and (ii) setting the clothing fit to \u201ctight fit\u201d. The \u201cBase Ratio\u201d is the proportion of the 2,032 test images in which the LMM (i.e., GPT-40) identified a specific caption. For example, if the LMM identifies 1,016 out of the 2,032 test images to have an \u201cuntucked\u201d tucking style, the base ratio for 'untucked' would be 50%. In addition to two baselines, LADI-VTON and IDM-VTON, we compared our method with OurSpose to examine the effect of DensePose information on text-editability with the same text prompt. Table 3 shows that our model achieved significantly higher accuracy for both attributes. Notably, the baseline models scored 50.78% and 46.31% for the \u201cuntucked\u201d, performing similarly to the base ratio. This result underscores the limitations of conventional agnostic masks that restrict manipulation to pre-defined areas, especially for adjusting clothing length. Furthermore, for the \u201ctight fit\u201d attribute, IDM-VTON and ourspose both achieved around 44%, while our method reached 66.98%, indicating that DensePose can hinder accurate text-based editing. Therefore, the results demonstrate that our approach addresses the limitations of conventional agnostic mask, enabling for accurate manipulation based on text prompts.\nAblation Study We investigate the key design choices of our method through ablation studies on the VITON-HD dataset, as shown in Fig. 6 and Table 4. Our comparisons include: (i) using a holistic text description (i.e., a overall description of the image in detail) for both person and clothing images with the LMM and (ii) excluding the prompt-aware mask generation (PMG) during inference.\nFig. 6 shows that using a holistic description can fail to capture complex poses accurately. Moreover, our method with holistic description shows the lowest FID and KID scores in Table 4. These results indicate that text prompts describing the images using LMM without in-context learning are insufficient to represent fine-grained details including poses. Since Ours without PMG employs only the coarse masks, the regions that need to be preserved are often generated differently, as shown in Fig. 6 (e.g., a belt absent on the target person). Table 4 demonstrate that PMG improves the performance significantly in all metrics.\nUser Study We further assess our method and other baselines through a user study involving 40 participants, using VITON-HD and DressCode datasets. As shown in Fig. 7 (a) and (b), we first compare five and six models on VITON-HD and DressCode datasets, respectively. The participants selected the best results based on three aspects: (i) clothing shape, (ii) clothing detail, and (iii) overall quality. As shown in Figure 7, our method shows a significantly high preference across three aspects. To evaluate the text editability of the models, we compare our method with IDM-VTON which uses SDXL-based architecture, and OurSpose (with added pose information). We asked participants to select the sample that best matched the conditions of \u201cuntucked\u201d, \u201ctight fit\u201d, and \u201csleeve rolled up\u201d for each model. As shown in Fig. 7 (c), our model achieved an 84% preference, visually demonstrating superior text editability.\nLimitations and Future Work This paper demonstrated the utility of generative text prompts using LMMs with in-context learning. However, due to the limitation of SDXL that accepts only up to 77 text tokens, we manually constructed in-context dataset. Inspired by recent studies [32] on configuring in-context sequences to enhance in-context learning performance, exploring in-context data construction for virtual try-on can be a promising research direction. Stable Diffusion 3 [15] has employed a T5 text encoder [40] capable of covering long context lengths. Utilizing well-configured in-context learning on such models is expected to further improve the performance of virtual try-on."}, {"title": "5. Conclusion", "content": "This paper introduces a novel virtual try-on model that leverages generative text prompt and advanced masking methods. Along with rich text descriptions from a large multimodal model, our approach not only improves performance but also enables versatile manipulation of virtual try-on results. Our dilated mask addresses the issue of generated clothing following too closely to the person's existing clothing, and allows for a more natural overlay. We propose a prompt-aware mask generation technique, which enhances diversity while preserving the person's original appearance. Our method achieves state-of-the-art results across multiple datasets, highlighting the effectiveness of generative textual prompt for virtual try-on."}]}