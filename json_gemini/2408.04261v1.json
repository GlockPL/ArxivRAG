{"title": "Unveiling Hidden Visual Information: A Reconstruction Attack Against Adversarial Visual Information Hiding", "authors": ["Jonggyu Jang", "Hyeonsu Lyu", "Seongjin Hwang", "Hyun Jong Yang"], "abstract": "This paper investigates the security vulnerabilities of adversarial-example-based image encryption by executing data reconstruction (DR) attacks on encrypted images. A representative image encryption method is the adversarial visual information hiding (AVIH), which uses type-I adversarial example training to protect gallery datasets used in image recognition tasks. In the AVIH method, the type-I adversarial example approach creates images that appear completely different but are still recognized by machines as the original ones. Additionally, the AVIH method can restore encrypted images to their original forms using a predefined private key generative model. For the best security, assigning a unique key to each image is recommended; however, storage limitations may necessitate some images sharing the same key model. This raises a crucial security question for AVIH: How many images can safely share the same key model without being compromised by a DR attack? To address this question, we introduce a dual-strategy DR attack against the AVIH encryption method by incorporating (1) generative-adversarial loss and (2) augmented identity loss, which prevent DR from overfitting an issue akin to that in machine learning. Our numerical results validate this approach through image recognition and re-identification benchmarks, demonstrating that our strategy can significantly enhance the quality of reconstructed images, thereby requiring fewer key-sharing encrypted images. Our source code to reproduce our results will be available soon.", "sections": [{"title": "I. INTRODUCTION", "content": "MACHINE learning has evolved from a groundbreaking innovation to a widely adopted and promising technology across numerous fields. One key characteristic of machine learning is its dependency on data; machine learning models are trained on data and often require additional user data for their application services. Recently, this dependency on data has raised significant privacy concerns [1], [2]. In the most straightforward case, storing and processing facial or body images in public cloud services for machine learning applications can expose these images to unauthorized access and misuse [3].\nIn such an inference scenario, a simple solution is to abstain from transmitting the data and conduct on-device computing [2]. However, due to the limited computing power and battery capacity of mobile devices, it is not feasible to fully compute services locally. Henceforth, there is an increasing need for research into privacy-preserving machine learning techniques to address practical scenarios.\nBackground. This work focuses on security and privacy issues in cloud-based image recognition systems [4], [5], as depicted in Fig. 1. In the cloud-based system, the gallery dataset should be accessible at the cloud server because image recognition tasks involve comparing gallery images with target images, where gallery images contain sensitive information and are often unencrypted. Such leakage of the gallery dataset has a potential privacy vulnerability, where it is susceptible to simple cyber threats, as malicious attackers can directly access the sensitive face images of all users [1]. Hence, the local server aims to conceal the visual information before sending gallery images to cloud servers.\nDefensive methods. Several defensive methods have been proposed to counteract the security and privacy risks associated with gallery datasets, including: 1) hiding visual information in noisy images [1], [6], 2) perceptual encryption [7], [8], and 3) homomorphic encryption [9], [10]. Homomorphic encryption can guarantee the utility of DNN computation with strong security; however, it requires excessive computation time, making it impractical for cloud-based systems. Additionally, perceptual encryption necessitates re-training the service DNN, which significantly degrades inference quality after encryption. Conversely, the AVIH (Adversarial Visual Information Hiding) encryption method [1] encrypts gallery images into noisy images while preserving the output of the service model, as depicted in Fig. 1. Therefore, the AVIH method maintains the performance of the service without any speed degradation. For these reasons, our focus is on the AVIH encryption method [1]. However, despite its significance, there has been no work analyzing the effectiveness of AVIH in securing information.\nResearch question. In the AVIH method, assigning a unique key to each image is recommended for optimal security; however, storage limitations may necessitate some images sharing the same key model. Throughout this paper, we address the following research question regarding the practical use of the AVIH method for cloud-based inference systems:"}, {"title": "RQ: How securely can adversarial visual information hiding truly conceal visual information?", "content": "In the remainder of this paper, we aim to solve the above research question by executing a data reconstruction attack against the AVIH method. Examples of the data reconstruction attack are depicted at the bottom of Fig. 1.\nData reconstruction attacks. In previous studies [11]\u2013[18], data reconstruction attacks have been proposed for recovering training data from trained neural networks. This concept arises from the widespread belief that trained DNNs can retain information about their training data, with a simple example being inferring the membership of data [14]. Beyond merely inferring membership, researchers have developed data reconstruction attack methods for simple classifiers, though these reconstructed images often lack photo-realism [15]\u2013[17]. With the advent of generative models, new techniques have emerged to produce photo-realistic images using methods such as advanced identity loss leveraging logits [11], supervised inversion [12], and adversarial examples [13], where those methods focus on finding appropriate latent vectors. Our study differentiates itself by not relying on pre-trained DNNs for finding latent vectors, leading to imperfect reconstructed images. Instead, we train an attacker key model that mimics the original key model, aiming to exactly reconstruct the original images."}, {"title": "A. Our Findings", "content": "In this paper, we aim to show that leveraging adversarial examples for visual information hiding [1] is unsafe, as depicted in the right part of Fig. 2. To this end, we propose a data reconstruction attack on the encrypted gallery set without access to the original key model. In the proposed method, we first randomly initialize an attacker key model. Then, we train the attacker key model to recover an image that consistently matches the service DNN output while ensuring photo-realism. As a simple method, one can train the attacker key model with the identity loss between the service model outputs of encrypted data and the attacker key model's output. However, with only standard identity loss, the attacker key model's outputs are not similar to the original images. This is our main challenge in the generalization of the attacker key model, where overfitting (a concept similar to ordinary machine learning) occurs if few data share the same key model.\nOur salient contributions are summarized as follows:\n\u2022 Exact reconstruction attack. To the best of the authors' knowledge, our work is the first to demonstrate that an exact data reconstruction attack works in practical scenarios.\n\u2022 Resolving overfitting 1 (augmented identity loss). To alleviate the overfitting issue in data reconstruction attacks, we propose augmented identity loss, which helps generalize the trained attacker key model.\n\u2022 Resolving overfitting 2 (generative-adversarial loss). In addition to augmented identity loss, we propose a GAN-based data reconstruction attack to improve the generalization and photo-realism of the attacker outputs. Unlike previous studies [11]\u2013[13] that use pre-trained DNNs to find an appropriate latent vector, we train an auxiliary key model that mimics the original key model while constraining local patch-level similarity with an auxiliary dataset.\n\u2022 Vulnerabilities of the AVIH method. We validate that images encrypted by the AVIH method can be reconstructed by the proposed method for various tasks such as face recognition, human re-identification, and vehicle identification. For example, Fig. 3 shows the results of the proposed method by changing the number of key-sharing images for a face recognition model. Furthermore, our ablation studies show that the proposed method can enhance the quality of data reconstruction, highlighting the necessity of stronger privacy-preserving methods."}, {"title": "B. Organization", "content": "The remaining parts of this paper are organized as follows. In Sec. II, we provide comprehensive reviews of existing visual information hiding methods and corresponding security and privacy attacks. Section III details the proposed approach for data reconstruction against the AVIH method, including a detailed review of the AVIH method. Next, in Sec. IV and Appendix A, we present our experimental setup and results for face recognition scenarios and re-identification scenarios, respectively. Finally, Sec. V concludes the paper with a discussion on the conclusion, limitations, extensibility, and future research directions."}, {"title": "II. RELATED WORKS ON HIDING VISUAL INFORMATION", "content": "Several studies have focused on hiding visual information in machine learning tasks, particularly during the inference stage.\nHomomorphic encryption. Homomorphic encryption (HE) is a cryptographic technique that allows computations to be performed on encrypted data, maintaining privacy while still producing an encrypted result that, when decrypted, matches the result of operations performed on the original data. In [20], HE for deep neural networks was proposed. Extending HE to deeper neural networks, a low-complexity encryption method for DNNs was introduced in [9], [21]. Although HE effectively prevents privacy leakage, the state-of-the-art HE remains extremely slow for computing large neural networks.\nPerceptual encryption. In the inference stage, perceptual encryption (PE) has emerged as a promising method for finding a suitable encrypted domain for visual images. In [22], a cycle-GAN-based PE method was proposed for medical images, requiring encryption/decryption keys. In [23], a more advanced method eliminated the need for these keys, using the cycle-GAN model itself as a unique encryption key. However, the purported security of PE is questionable. Numerous studies have demonstrated that PE is highly vulnerable to data reconstruction attacks, which can effectively restore original images even against pixel-based encryption [24] and learnable encryption [25]. This vulnerability underscores a critical flaw in PE methods, challenging their viability for robust security in practical applications.\nAdversarial examples. Adversarial examples are widely used in privacy-related deep learning technologies due to their versatility, such as for inserting watermarks in foundation models [26], [27]. For hiding visual information, unlike HE and PE, steganography [28] and AVIH [1] are practical approaches, as they can hide information or recover the original data with low computational complexity. More specifically, AVIH can guarantee the correctness of computational results. The variance-consistency loss used in AVIH can efficiently encrypt visual information while preserving the correctness of DNN computations without requiring any re-training process, which could lead to additional privacy leakage. However, privacy-preserving aspects of AVIH have been discussed heuristically and do not meet stringent privacy constraints. In this work, we present the first approach that executes data reconstruction attacks on the AVIH method."}, {"title": "III. METHODOLOGY", "content": "In this work, our primary focus is to demonstrate that sharing the same key models for hiding visual information poses potential security and privacy vulnerabilities. In this section, we first briefly review the AVIH method in Sec. III-A and then propose our method in Sec. III-B."}, {"title": "A. Preliminary: AVIH Encryption", "content": "Consider a remote computing scenario where clients provide a gallery dataset and a target service model to a cloud server. If the original gallery dataset is sent to the server without encryption, the sensitive and private visual images that clients wish to keep confidential can be accessed by the server administrator. To address this, the AVIH method is proposed to secure the gallery dataset images while maintaining the service quality of the target service model. To ensure the reconstruction of the original data from the encrypted data, clients have their private and secure key model, accessible only to them. For image recognition tasks, such as face recognition [29], [30] or human re-identification [5], the encrypted gallery datasets can be used without a loss of accuracy. Additionally, while humans cannot recognize the original image from the encrypted version, the secure key model can accurately reconstruct the original image.\nOverall loss function. In the AVIH method, the primary goal is to generate images that 1) preserve the service model's output, 2) destroy the image's structural information, and 3) guarantee recovery of the original image with a key model. From the original image $x_i$, we generate the image $x_i'$, where $i$ denotes the index of the gallery images. The generated image $x_i'$ is designed to minimize the following loss function:\n$L_{AVIH}(x_i, x_i') = L_t(x_i, x_i') - \\lambda_1 L_d(x_i, x_i') + \\lambda_2 L_v(x_i') + \\lambda_3 L_r(x_i, x_i').$  \nThe details of the AVIH loss function (1) are introduced in the following sections. Fig. 4 illustrates the schematic diagram of this method.\nTask loss (Lt). The task loss is related to the preservation of the service model's output. With the service model $f_s(\\cdot)$, the loss function is defined as follows:\n$L_t(x_i, x_i') = || f_s(x_i) - f_s(x_i')||_2,$"}, {"title": null, "content": "where the distance metric is defined using $l_2$ norm; howeer, other distance metrics such as $l_1$ and $l_\\infty$ norms can be also used.\nDifference loss (Ld). The difference loss is related to destroying the image's structural information. Motivated by Type-I adversarial attacks, this loss function is defined by the $l_1$ or $l_2$ distance between $x_i$ and $x_i'$ as follows:\n$L_d(x_i, x_i') = ||x_i - x_i'||_2.$\nRecovery loss (Lr). Let us define the key model as $f_k(\\cdot)$. To recover the original image, the recovery loss is defined by the $l_2$ distance between the original image $x_i$ and the recovered image $f_k(x_i')$ as follows:\n$L_r(x_i, x_i) = ||x_i - f_k(x_i')||_2.$\nVariance consistency loss (Lv). The variance consistency (VC) loss, proposed in [1], addresses the trade-off between protection quality and recovery quality. To formulate this loss function, we divide the encrypted image $x_i'$ into $N$ overlapping patches, each with height $h$ and width $w$. Let us define the sum of the pixel values of the n-th patch as $S_n$, where $S_n^{(r)}$, $S_n^{(g)}$, and $S_n^{(b)}$ denote the sum of pixel values for the red, green, and blue channels, respectively. The VC loss then measures the channel-wise variance of $S_n$ as follows:\n$L_v(x) = var(S^{(r)}) + var(S^{(g)}) + var(S^{(b)}),$  \nwhere $var(S^{(r)})$ denotes the empirical variance of the sequence $S_1^{(r)}, S_2^{(r)}, ..., S_N^{(r)}$.\nRemark 1. If we aim to obtain an image that is robust against potential attackers, we should set a higher weight on the VC loss. Consequently, the quality of the image recovered using the key model will be poor."}, {"title": "B. Proposed Data Reconstruction Attack", "content": "Here, we first introduce the threat scenario of the AVIH method. Then, we present the proposed data reconstruction attack for the gallery dataset, where the visual information is hidden in noise-like images.\nThreat scenario. In this work, we follow the system model in the AVIH method, where the target service type is image recognition. In an image recognition system, the class of a target image can be identified by comparing it with images in the gallery dataset. As shown in Fig. 4, the AVIH method transforms the images in the gallery dataset into noise-like images while preserving the output of the service DNN.\nLet us define the original gallery dataset as $G$ and the encrypted gallery dataset as $\\hat{G}$. We consider a threat scenario where a malicious attacker aims to reconstruct the hidden images from noise-like images with access to the 1) encrypted dataset and 2) service DNN model. According to [1], the key model can reconstruct the original gallery dataset from the encrypted gallery dataset and is only available on the client-side.\nMore specifically, without making strong assumptions, since the service DNN weights are available at the remote server, we assume a white-box access scenario, i.e., an attacker can access the weights of the service DNN model.\nMotivation. In this work, we focus on the fact that a private key model can reconstruct the original gallery dataset. This implies there is an unknown but specific relationship between the encrypted and original datasets. Motivated by this, we aim to mimic the functionality of the key model. However, we neither have access to the key model nor know how it was trained. To address this, we leverage the generative model [31], which is widely used for guaranteeing photo-realism. However, generative model inversion attacks typically focus on mimicking the distribution of the training dataset and cannot reconstruct an image corresponding to a specific DNN output. To directly reconstruct the original gallery dataset, we consider the attacker key model as $a_k(\\cdot; \\theta)$, where $\\theta$ denotes its weights. For simplicity, we use $a_k(\\cdot; \\theta)$ and $a_k(\\cdot)$ interchangeably.\nIdentity loss. With the given information, we can assume that the target key model also preserves the output of the service model. Additionally, since we have access to the weights of the service DNN, we can formulate the identity loss as follows:\n$L_I(\\hat{G}) = E_{x_i' \\sim \\hat{G}}||f_s(a_k(x_i'; \\theta)) - f_s(x_i)||_2,$  \nwhere $\\hat{G}$ denotes the encrypted images sharing the same key model. Instead of an $l_2$-based loss, the identity loss function can be formulated to maximize the log-likelihood of the target class c or the cosine similarity of two feature vectors.\nOverfitting in data reconstruction. In Fig. 5, we show examples of the original images and the encrypted images by the AVIH method. The figure also depicts the images recovered by the original key model and the attacker key model. As shown, the attacker model trained with identity loss in (6) recovers very noisy images compared to the original images or those recovered by the original key model. This issue is quite similar to overfitting in ordinary machine learning problems. More specifically, there is a true relationship between the encrypted images and the original images, represented by the original key model. However, a few encrypted images are not sufficient to demonstrate this relationship using the service model. Therefore, in the remainder of this section, we propose 1) augmented identity loss and 2) a GAN-based training scheme to secure the generalization of the trained attacker key model. The details of these methods are depicted in Fig. 6."}, {"title": "Augmented identity loss.", "content": "In typical machine learning model training, data augmentation is widely used to increase the validation/test accuracy of the trained model, i.e., for better generalization Similarly, to alleviate the overfitting issue in data reconstruction, we combine data augmentation with identity loss. Let us consider an image $x_i'$ drawn from the encrypted dataset $\\hat{G}$. Then, we may reconstruct the original image using $a_k(x_i'; \\theta)$. Unlike the canonical identity loss in (6), we apply random data augmentation before forwarding the image into the service model $f_s(\\cdot)$. Denoting the random augmentation process as $T(\\cdot)$, the identity loss in (6) can be redefined as follows:\n$L_I(\\hat{G}) E_{x_i' \\sim \\hat{G}}||f_s(T(a_k(x_i'; \\theta)))) \u2212 fs(xi)||2$  \nIn our work, we consider the following data augmentation methods: i) random horizontal flip, ii) random padding, and iii) random crop. In the right part of Fig. 6, we illustrate the concept of the augmented identity loss. For further generalization, we also experimented with randomized smoothing on the reconstructed data $a_k(x_i'; \\theta)$; however, it did not produce notable differences.\nGenerative model inversion attack. In this paragraph, we aim to resolve the overfitting issue using a GAN-based loss function. Intuitively, if we want to find an image that has the same output as the encrypted image $x_i'$, there would be many possible images, most of which are unnatural. By reducing the number of cases by restricting the unnatural images, we can resolve the overfitting issue.\nTo make the reconstructed images natural, we consider an optimization problem that minimizes the augmented identity loss with a Jensen-Shannon (JS) divergence constraint between the auxiliary dataset and the reconstructed images as follows:\n$\\min_{a_k(\\cdot)} L_I(\\hat{G}), s.t. D_{JS}(a_k(\\hat{G})||X) \\leq \\epsilon,$  \nwhere $X$ denotes the auxiliary dataset, and $D_{JS}(a_k(\\hat{G})||X)$ denotes the JS divergence between the reconstructed images $a_k(x_i'), x_i' \\sim \\hat{G}$ and the auxiliary images $x'' \\sim X$. From this optimization, we derive the Lagrangian of the problem (8) as follows:\n$L = L_I(\\hat{G}) + \\lambda(D_{JS}(a_k(\\hat{G})||X) - \\epsilon),$  \nwhere $\\mu \\geq 0$. Let us consider the minimizer $a_k(\\cdot; \\theta)$ of the problem as $a_k^*(\\cdot; \\theta)$. Then, for any constant $e$, there exists $\\mu > 0$ that makes $a_k^*(\\cdot; \\theta)$ a minimizer of the original problem (8). The intuition is that for the minimizer $a_k^*(\\cdot; \\theta)$, a larger value of $\\mu$ indicates a smaller $\\epsilon$ in (8). Hence, we aim to minimize the Lagrangian in (9).\nThe next step is to convert the function in (9) into a GAN formulation. As shown in [31], the JS divergence minimization problem can be replaced by the GAN optimization problem. For brevity, we use simpler notation: the probability density function of the recovered images $x^* \\sim a_k(\\hat{G}) = p(x)$ and the auxiliary images $x'' \\sim X = q(x)$. The JS divergence in (9) can then be rewritten as follows:\n$D_{JS}(a_k(\\hat{G})||X)) = D_{JS}(p||q)$\n$\\propto E_{x^* \\sim p(x^*)} \\left[log \\frac{2p(x^*)}{p(x^*) + q(x^*)} \\right] + E_{x'' \\sim q(x'')} \\left[log \\frac{2q(x'')}{p(x'') + q(x'')} \\right].$  \nBy defining a discriminator as $D(x)$, we can convert the JS divergence into a GAN formulation as follows:\n$D_{JS}(p||q) = \\max_D E_{x^* \\sim p} [log(D(x^*))] + E_{x'' \\sim q} [log(1 - D(x''))],$  \nwhere the optimal D is $\\frac{p(x)}{p(x)+q(x)}$. Then, the loss function for the attacker key model is defined by\n$L_{key}(\\hat{G}) = E_{x' \\sim \\hat{G}} [log D(a_k(x_i'; \\theta))] + \\lambda_I \\cdot L_I(\\hat{G}),$  \nwhere $D$ is the discriminator model, and the last layer is activated by hyperbolic tangent function. Similarly, the discriminator loss is defined as follows:\n$L_{dis}(\\hat{G}) = -E_{x' \\sim \\hat{G}} [log D(a_k(x_i'; \\theta))] - E_{x \\sim \\hat{G}_{aux}} [log(1 - D(x))].$  \nSince the original images and the auxiliary images are not identical but belong to the same category (e.g., face images), we use a patch-GAN model for our optimization, where the discriminator D classifies true and false patches of the images. In Fig. 6, the attacker aims to generate photo-realistic results by deceiving the discriminator.\nRemark 2 (JS divergence vs. KL divergence). In our problem formulation (8), we use the JS divergence between the recovered images and auxiliary images as our constraint for photo-realism. Another metric, KL divergence, is widely used to ensure similarity between two datasets. We have tried with the KL divergence formulation; however, it is closely related to variational inference, which requires a pre-trained GAN. Since a pre-trained GAN is not suitable for exact data reconstruction attacks, we use JS divergence as our constraint."}, {"title": "C. Ablation Study of Key Features", "content": "Before introducing our experimental results, we briefly present graphical examples of our attacker key model with and without our key features for resolving overfitting issues. In the previous subsection, we proposed the augmented identity loss and GAN-based training loss. In Fig. 5, we show our results on the AgeDB-30 dataset, using the CelebA dataset as the auxiliary dataset. As depicted in the fourth column, the results with the canonical identity loss recover images that are not very similar to the original ones. However, by leveraging augmented identity loss, the shape of the images can be recovered, though the colors are not realistic. On the other hand, using GAN-based training yields more photo-realistic recovered images. Moreover, by combining both methods, the quality of the recovered images is further enhanced."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we evaluate the proposed data reconstruction attack against adversarial visual information hiding. Since there has been no prior work on exact data reconstruction attack methods for deep neural network models, we measure the quality of the reconstruction using various metrics. Instead of comparing with other methods, we conduct an ablation study on our key contributions: 1) augmented identity loss and 2) GAN-based training."}, {"title": "A. Experimental Details", "content": "We conduct two main experiments: one for the face recognition scenario and another for the object re-identification scenario. Both experiments are performed on a workstation equipped with an AMD Ryzen R9 5950x 16-core CPU and an NVIDIA GeForce RTX 3090 GPU with 24GB of VRAM.\nFace recognition scenario. In the face recognition scenario, we perform data reconstruction attacks on three target face datasets: LFW [32], AgeDB-30 [33], and CFP-FP [34]. For the service model on the local server and cloud, we use ArcFace [29] and AdaFace [30] models with IR-18 and IR-50 backbones. For the data reconstruction attack, we use the Celeb-A [35] dataset as our auxiliary dataset for all target face datasets. As an evaluation metric for face recognition, we use TAR@FAR=0.01 for the reconstructed images via our method. For this accuracy evaluation, we use the AdaFace model with an IR-101 backbone.\nImplementation of AVIH. For the implementation of the AVIH method, we follow the hyperparameters used in the original paper [1]. For example, we update encrypted images (x') for 800 epochs, and the kernel size for the VC loss is set to 4. The weights for the difference loss, recovery loss, and VC loss are set to 0.03, 0.5, and 3.0, respectively. The key model is configured using the standard U-Net [36]. In our experiment, we slightly modify the gallery sets of the datasets to contain 2,000 images. We then run the AVIH method on these 2,000 gallery images, using 1,000 images for training our attacker key model and the remaining 1,000 images for evaluating the trained attacker key model.\nImplementation of our data reconstruction. In our data reconstruction attack, we train an attacker key model based on the U-Net structure. We trained our key model for 1,600 steps with a batch size of 32. The weight on the augmented identity loss is set to 30.0. For the augmented identity loss function, we use the following data augmentation methods: random horizontal flip, random padding of 5 pixels, and random cropping to the original size."}, {"title": "B. Accuracy and Similarity Metrics", "content": "In our experiments, before implementing our data reconstruction attack scheme, the AVIH [1] encrypts the gallery dataset of three face recognition datasets: AgeDB-30, LFW, and CFP-FP. We note that the encrypted gallery dataset successfully performs face recognition tasks for the target service model. For example, with the encrypted LFW gallery dataset, the cloud achieves a TPR accuracy of 98.40%, which is the same accuracy as with the original gallery dataset.\nAccuracy metrics. In Tab. I, we present the TPR performance metrics at FPR 0.01 across three datasets: AgeDB-30, LFW, and CFP-FP. The TPR accuracy values are provided for the proposed method (Ours) at different percentages of images sharing the same key model (1%, 3%, 10%, and 70%). We also compare the results of our method with the following: 1) original gallery dataset, 2) encrypted dataset, 3) dataset reconstructed by the key model, and 4) random face images. For all three datasets, TPR values of the proposed method gradually increase as more gallery images share the same key model. For instance, in the AgeDB-30 dataset, the proposed method achieves a TPR of 0.628 with 1% same-key data, which increases to 0.746 with 3%, 0.761 with 10%, and 0.924 with 70% same-key data.\nOn the other hand, with the original gallery datasets, the TPR accuracy values are sufficient for recognizing most of the query images. For example, the TPR value for the AgeDB-30 dataset is 98.0%. More importantly, the gallery dataset reconstructed by the original private key model performs almost the same as the original gallery dataset. The encrypted gallery set has significantly lower TPR values since the evaluation service model (IR-101 backbone) is different from the target service model (IR-18 backbone).\nTo summarize, the proposed method shows significant improvement in TPR by executing data reconstruction attacks against the AVIH encryption method."}, {"title": "Image similarity metrics.", "content": "In Tab. II, we present the evaluation results for the three face recognition datasets. The numbers in the table are computed using each similarity metric between the original and reconstructed images. Similar to the accuracy metric benchmark in Tab. I, we evaluate the proposed method at different percentages of images sharing the same key model (1%, 3%, 10%, and 70%).\nIn the AgeDB-30 and LFW datasets, all similarity metrics improve as more images share a common key model. Interestingly, if only 3% of the images share the same privacy key model, the quality of reconstructed images is comparable to that of the true key model. For example, in the AgeDB-30 dataset, the PSNR for 3% shared key model images is 11.847 compared to 17.525 for the true key model images, and the MSE for 3% shared key model images is 0.071 compared to 0.022 for the true key model images.\nFurthermore, although pixel-based metrics such as PSNR and MSE show some differences between different percentages of images sharing the same key model, other metrics like LPIPS and CLIP do not show significant differences. For instance, in the AgeDB-30 dataset, the LPIPS for 3% shared key model images is 0.450 compared to 0.367 for the true key model images, and the CLIP for 3% shared key model images is 0.746 compared to 0.837 for the true key model images.\nFor the CFP-FP dataset, the reconstruction quality is relatively lower compared to the other two datasets. This is because the reconstruction quality with the true key model serves as a performance cap for the replicated key model, where the true key model's reconstruction quality is relatively lower. However, similar to the other two datasets, perceptual quality metrics such as LPIPS and CLIP still perform well. For example, in the CFP-FP dataset, the LPIPS for the true key model is 0.435, which is comparable to 0.623 for 1% shared key model images. The CLIP metric also shows a consistent trend, with 0.825 for the true key model and 0.728 for 1% shared key model images."}, {"title": "Graphical results.", "content": "In Fig. 7, we present graphical examples of the proposed method. The first row with green borders shows the original images, while the subsequent rows display the reconstructed images with different percentages of leaked encrypted data (1%, 3%, 10%, and 70%).\nWith 1% leaked encrypted data, the reconstructed images are significantly distorted and blurred, making recognition difficult, which aligns with lower similarity scores. As the percentage increases to 3%, facial features become more distinguishable despite some blurring, showing noticeable improvement. At 10%, the images are clearer and more recognizable, and at 70%, the reconstructed images are very close to the original quality, supporting the highest similarity scores and demonstrating the method's effectiveness.\nThese results highlight that while pixel-based metrics like PSNR and MSE show improvement, perceptual similarity metrics such as LPIPS and CLIP also indicate significant enhancements in image quality (in Tab. II). This improvement in reconstruction quality directly correlates with an increase in TPR (in Tab. I), further validating the robustness of the proposed method in maintaining high image similarity and effective data reconstruction as more images share the same key model."}, {"title": "C. Results for Various Face Recognition Models", "content": "In Tab. III, we present the accuracy and similarity metrics for various backbones and face recognition schemes. Unlike the benchmarks in Tabs. I and II, the AdaFace and ArcFace face recognition models are used for evaluation, with their backbone configured as the IR-50 network. As shown in the table, similar to the previous results, all evaluation metrics of the proposed method improve as more encrypted images share the same key model. For example, if 70% of the gallery images share the same key model, the proposed method nearly achieves the reconstruction quality of the original key model. Interestingly, the proposed scheme can nearly achieve the perceptual similarity score of the original key model even when only 3% of the gallery images share the key model.\nThis experiment demonstrates that the proposed method can be generally applied to various face recognition models and backbone networks."}, {"title": "D. Ablation Study", "content": "In this subsection", "contributions": 1, "overfitting": "less data leads to higher overfitting.\nWhen the percentage of leaked encrypted data is higher, such as 10% and 70%, the performance improvements from ablation studies are relatively smaller but still notable. GAN-based training continues to enhance the performance metrics. For instance, at 70% leaked data, the TPR increases from 0.860 to 0"}]}