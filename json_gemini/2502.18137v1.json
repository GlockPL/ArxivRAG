{"title": "SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference", "authors": ["Jintao Zhang", "Chendong Xiang", "Haofeng Huang", "Jia Wei", "Haocheng Xi", "Jun Zhu", "Jianfei Chen"], "abstract": "An efficient attention implementation is essen-tial for large models due to its quadratic timecomplexity. Fortunately, attention commonly ex-hibits sparsity, i.e., many values in the attentionmap are near zero, allowing for the omissionof corresponding computations. Many studieshave utilized the sparse pattern to accelerate at-tention. However, most existing works focus onoptimizing attention within specific models byexploiting certain sparse patterns of the atten-tion map. A universal sparse attention thatguarantees both the speedup and end-to-endperformance of diverse models remains elu-sive. In this paper, we propose SpargeAttn,a universal sparse and quantized attention forany model. Our method uses a two-stage on-line filter: in the first stage, we rapidly and ac-curately predict the attention map, enabling theskip of some matrix multiplications in attention.In the second stage, we design an online softmax-aware filter that incurs no extra overhead and fur-ther skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image,and video generation, without sacrificing end-to-end metrics.", "sections": [{"title": "1. Introduction", "content": "As sequence lengths in large models become longer, such as\n45K-128K in video generation and language models (Yang\net al., 2025; Bao et al., 2024; Dubey et al., 2024), the\ntime consuming of attention occupies a significant portion\nof inference latency in large models (Zhang et al., 2025).\nFortunately, the attention map P = Softmax(QKT/\u221ad)\nexhibits inherent sparsity, as the softmax operation often\ncreates many values approaching zero (Deng et al., 2024).\nSparse attention methods exploit such sparsity to accelerate\nattention by (1) constructing a \u201csparse mask\u201d, which indi\ncates the important non-zero entries of the attention map\nP that should be computed, and (2) computing attention\nonly for the parts corresponding to the sparse mask. There\nare three distinct categories of sparse attention methods\nbased on how the sparse mask is generated. pattern-based\nmethod (Zhang et al., 2023; Xiao et al., 2024a; Fu et al.,\n2024; Zhu et al., 2024; Xiao et al., 2025; 2024b) relies on\nspecific sparsity patterns based on empirical observations,\ndynamic sparse attention (Ribar et al., 2024; Singhania\net al., 2024; Jiang et al., 2024; FlexPrefill, 2025; Gao et al.,\n2024) computes the mask on-the-fly based on the inputs, and\ntraining-based method (Kitaev et al., 2020; Pagliardini et al.,\n2023) directly train models with native sparse attention.\nLimitation. (L1. Universality) Though existing sparse at\ntention methods already demonstrate promising speedup on\nsome tasks, their universality is still limited. Existing works\nare typically developed for specific tasks, such as language\nmodeling, utilizing task-specific patterns such as sliding\nwindows or attention sinks. However, the attention pattern\nvaries significantly across tasks (see examples in Fig. 2),\nmaking these patterns hard to generalize. (L2. Usability)\nMoreover, it is difficult to implement both accurate and\nefficient sparse attention for any input. This is because ac\ncuracy demands precise prediction of the sparse regions in\nthe attention map, while efficiency requires the overhead of\nthis prediction to be minimal. However, current methods\nare difficult to effectively satisfy both of the requirements si-"}, {"title": "2. Related Work", "content": "Depending on how the sparsity mask is constructed, sparse\nattention methods can be divided into three types: (1) Pat-\ntern required methods rely on some fixed patterns of\nthe attention map, such as sliding windows or attention\nsinks (Xiao et al., 2024b). H2O (Zhang et al., 2023),\nInfLLM (Xiao et al., 2024a), and DUOAttention (Xiao\net al., 2025) rely on sliding window pattern. SampleAt-\ntention (Zhu et al., 2024), MOA (Fu et al., 2024), and\nStreamingLLM (Xiao et al., 2024b) rely on sliding window\nand attention sink pattern. DitFastAttn (Yuan et al., 2024)\nrelies on sliding window patterns and similarities between\ndifferent attention maps. Moreover, DitFastAttn is restricted\nto simple diffusion transformers, showing incompatibility\nwith language models and MMDiT models like Flux (Black\nForest Labs, 2023), Stable Diffusion3 and 3.5 (Stability AI,\n2023), and CogVideoX (Yang et al., 2025). As the pattern\nvaries across models, these methods may not universally\nwork for different models. (2) Dynamic sparse methods\ndynamically construct the sparse mask based on the input\nwithout the need of preset patterns, and are thus potentially\nmore universal. Existing works can be further categorized\ninto channel compression and token compression. Channel\ncompression methods include SparQAttn (Ribar et al., 2024)\nand LokiAttn (Singhania et al., 2024). They construct the\nmask by carrying full attention with reduced dimensionality.\nHowever, as the dimension is already small, e.g., 64, 128,\nin commonly used attention, the speedup potential might\nbe limited. Token compression methods include MInfer\nence (Jiang et al., 2024) and FlexPrefill (FlexPrefill, 2025).\nThey construct the mask by compressing each block of to-\nkens to a single token and compute attention on this shorter\nsequence. However, this approximation is too aggressive:\nmissing important blocks of P is possible if they do not\nhave a large attention score on the compressed sequence.\nSeerAttention (Gao et al., 2024) requires training of addi\ntional parameters for attention, which is expensive to use.\nMoreover, they are all designed for language models, and\ntheir applicability to other model types, such as diffusion\nmodels, remains uncertain. (3) Training-based methods\nmodify the attention computation logic, requiring retraining\nthe entire model, such as Reformer (Kitaev et al., 2020) and\nFastAttention (Pagliardini et al., 2023). These methods are\nmuch more expensive to use than training-free methods.\nThere are other ways to accelerate attention, such as opti\nmizing the kernel implementation (Dao et al., 2022; Dao,\n2024; Shah et al., 2024), quantization (Zhang et al., 2025),\ndistributing the workload (Liu et al., 2024a), and designing\nlinear time attention (Wang et al., 2020; Choromanski et al.,\n2021; Yu et al., 2022; Katharopoulos et al., 2020). They are\northogonal to our approach."}, {"title": "3. SpargeAttn", "content": "SpargeAttn contains a two-stage online filter to imple-\nment sparse FlashAttention. First, as shown in Step1 and\nStep2 in Fig. 3, we design a fast and accurate method\nto predict the sparse block in the attention map, thereby\nskipping the corresponding products of QiK and Pij Vj.\nSecond, as shown in Step3 in Fig. 3, we design a sparse\nonline softmax method to further skip the products of Pij Vj.\n3.1. Sparse FlashAttention\nSpargeAttn adopts the tiling strategy of FlashAtten-\ntion (Dao, 2024), and skip computing the blocks that\nare filtered out. Consider an attention operation S =\nQKT/\u221ad, P = \u03c3(S), \u039f = PV, where \u03c3(S)ij =\nexp(Sij)/\u03a3k exp(Sik) is the softmax operation. Let N\nbe the sequence length and d be the dimensionality of each\nhead; the matrices Q, K, and V each have dimensions\nN\u00d7 d, while the matrix S and P is N \u00d7 N. FlashAttention\nproposes to tile Q, K, and V from the token dimension into\nblocks {Q}, {K}, {V} with block sizes bq, bk, bk, respec-tively. Then, it uses online softmax (Milakov & Gimelshein,\n2018) to progressively compute each block of O, i.e., O\u017c:\nSij = QiK/\u221ad, (mij, Pij) = \u00f5(mi,j-1, Sij),\nlij = exp(mi,j-1 - mij )li,j-1 + rowsum(Pij),\nOij = diag (exp(mi,j\u22121 \u2013 Mij)) Oi,j\u22121 + PijVj (1)\nwhere mij and lij are bq \u00d7 1 vectors, which are initialized\nto -\u221e and 0 respectively. The \u1ee1() is an operator similar\nto softmax.: mij = max{mi,j\u22121,rowmax(Sij)}, Pi,j =\nexp(Sij - mij). Finally, the output O\u2081 can be computed by\nOi = diag(lij)-10ij.\nImplementing sparse FlashAttention is intuitive. By skip-ping certain block matrix multiplications of QiK and\nPijVj, we can accelerate the attention computation. We\nformulate sparse attention based on FlashAttention in the\nfollowing definitions.\nDefinition 1 (Block Masks). Let Mg and Mpu be binary\nmasks of dimensions [N/bg] \u00d7 [N/bk], where each value\nis either 0 or 1. These masks determine which computations\nare skipped in the sparse attention mechanism.\nDefinition 2 (Sparse FlashAttention). The computation\nrules for sparse FlashAttention based on the masks are de-\nfined as follows:\nQiK, Pij Vj are skipped if Mg [i, j] = 0. (2)"}, {"title": "3.2. Selective Token Compression for Sparse Prediction", "content": "Key idea. Although attention maps vary across models, we\nobserve that various models exhibit a common trait: Most\ncloser tokens in the query and key matrices of the atten-tion show high similarity (See Fig. 4). Consequently, for\nblocks composed of highly similar tokens, we can consoli-date these tokens into a single representative token for the\nblock. Based on this observation, we propose a pattern-free\nonline prediction method for identifying sparse blocks in\nP to skip some computation of Q\u00bfK and PijV; during\nthe FlashAttention process. Specifically, we first compress\nblocks exhibiting high self-similarity within Q and K into\ntokens. Then, we swiftly compute a compressed attention\nmap P using the compressed Q and K. Finally, we selec-tively compute {QK, Pij Vj } for those pairs (i, j) where\n{P[i, j]} accumulates a high score in the compressed atten-tion map. Importantly, compressing only the token blocks\nwith high self-similarity is crucial, as omitting computations\nfor non-self-similar blocks can result in the loss of critical\ninformation. This will be confirmed in Sec. 4 and A.2.\nPrediction. As shown in Step1 in Fig. 3, we first compute\na mean cosine similarity across tokens for each block of Q\nand K. Next, we compress each block into a single token\nby calculating a mean across tokens. Then, we compute a\ncompressed QKT using the compressed Q and K. Finally,\nto prevent interference from non-self-similar blocks, i.e.,\nthe block similarity less than a hyper-parameter 0, we set\nthe corresponding values in S to -\u221e, and then obtain a\ncompressed attention map through softmax. This algorithm\ncan be expressed as:\nq = {q} = {mean(Qi, axis = 0)}\nk = {kj} = {mean(Kj, axis = 0)}\nSqi = CosSim(Qi), Skj = CosSim(Kj)\n\u015c[i] = qik; \u015c[:, j] = \u2212\u221e, If Skj < \u03b8\nP[i] = Softmax(\u015c[i])\nwhere Qi \u2208 Rbqxd,qi \u2208 R1\u00d7d, Kj \u2208 Rbk\u00d7d, kj \u2208 R1\u00d7d\nand CosSim(X) = | max(XXT) measures the cosine-\nXX\u2122\nsimilarity within a block.\nFor each row of P, i.e., P[i], we select the positions of\nthe top values whose cumulative sum reaches \u03c4\u00b7\u2211P[i],\nwhere T is a hyper-parameter. These positions are set to 1\nin Mg[i,:], while all other positions are set to 0.\nMg[i, :] =TopCdf(P[i], T)\nwhere the TopCdf(P[i], 7) can be formulated as follows."}, {"title": "3.3. Masking of the First Stage", "content": "Masking. The Mg can be applied in FlashAttention di-rectly to saving some computation. In the inner loop ofFlashAttention, i.e., during computing attention between\na Qi and {K}, {V;}, we can skip {Q\u00bfKT, PijV;} when\nMg [i, j] = 0.\nSkip QiK and PijVj, If Mg [i, j] = 0\n3.4. Sparse Warp Online Softmax\n(6)\nKey idea. We can further identify the small enough values\nin the attention map during the online softmax process. If\nall values in Pij are close enough to zero, the PijV; will\nbe negligible and can be omitted.\nTo identify which Pij = exp(Sij \u2013 mi,j) (See Sec. 3.1)\ncontains values small enough to be omitted, we note that in\nevery inner loop of FlashAttention, the Oij will be scaled\nby exp(mi,j-1 \u2013 mij) and then plus the Pij Vj:\nmlocal =rowmax(Sij), mij = max{mi,j\u22121, Mlocal}\nOij =diag (exp(mi,j-1 - Mij)) Oi,j\u22121 + Pij Vj\nIf rowmax(Sij) < mij, then mij = Mi,j\u22121. Consequently,\nOij = Oi,j\u22121 + PijVj. Furthermore, if rowmax(Sij) <\nmij holds ture, then all values in Pij = exp(Sij \u2013 mij) are\nclose to 0. This results in all values in Pij Vj being close\nto 0. This condition implies that Pij Vj is negligible when\nrowmax(Sij) is significantly smaller than mij:\nOij \u2248 Oi,j\u22121, if max (exp(Sij \u2013 mij)) \u2192 0\nmax (exp(Sij - mij)) \u2192 0 \u21d4 max(mlocal - mij) < \u03bb\nThe above equivalence is satisfied when A is small enough.\nTherefore, based on the analysis above, we propose a sim-ple yet effective sparse method to further skip the Pij Vj\ncomputation. Specifically, in the inner loop of FlashAtten-tion, the Sij will be split by cw GPU warps to {Sij[w*bq :-"}, {"title": "3.5. Combined with SageAttention", "content": "To further accelerate our implementation of sparse attention,\nwe integrate our method into SageAttention (Zhang et al.,\n2025), which proposes a quantized method for accelerating\nattention. Since quantization operations and sparse oper-\nations are orthogonal, sparse computation can be directly\napplied to SageAttention. The complete algorithm is shown\nin Algorithm 1. Specifically, first, we need to add one judg-\nment at the beginning of the inner loop of SageAttention\n(Line 10 in Algorithm 1) to decide whether to skip the whole\ninner loop once. Second, we add another judgment before\nthe updating of Oij in the inner loop of SageAttention (Line\n15 in Algorithm 1) to decide whether to skip the compu-\ntation of PijVj. Moreover, to minimize the attention map\nprediction overhead, we implement the prediction using\nCUDA and adopt some kernel fusion techniques.\n3.6. Hyper-parameters Determination for Model Layer\nBased on the method description in Sec. 3.2 and 3.4, our\nmethod incorporates three hyper-parameters: \u0442\u2208 (0,1),\n\u03b8\u2208 (\u22121, 1), and \u5165 < 0. The parameter determination pro-\ncess for each attention layer in any model is straightforward.\nWe aim to identify a set of hyperparameters that not only\nmaximize attention sparsity but also constrain the attention\nerror across five different model inputs. To evaluate atten-\ntion accuracy, we employ a strict error metric, the Relative\nL1 distance, defined as L1 = \u03a3|0 \u2013 0'|/\u03a3|0|. The\nprocess begins by setting two L1 error thresholds 11 and 12,\ne.g., l\u2081 = 0.05, 12 = 0.06. We first conduct a grid search for\n7 and 0 to identify the optimal pair that maximizes sparsity\nwhile ensuring L1 < 11. Subsequently, we perform an-other grid search for A to find the optimal value that furthermaximizes sparsity while maintaining L1 < 12.\n3.7. HilbertCurve Permutation\nKey idea. Improving sparsity while maintaining accuracy\nis a key challenge in enhancing the performance of sparse\nattention. In our algorithm, increasing the self-similarity\nof key and query blocks can reduce the number of non-self-similar blocks. This allows more blocks to participatein TopCdf selection, thereby improving sparsity. Sinceattention is computationally invariant to token permutations,"}, {"title": "4. Experiment", "content": "4.1. Setup\nModels. We validate the effectiveness of SpargeAttn\nacross diverse representative models from language, im-\nage, and video generation. Specifically, we conduct exper-iments on Llama3.1 (8B) (Dubey et al., 2024) for text-to-text, CogvideoX (2B) and Mochi (Team, 2024) for\ntext-to-video, Flux (Black Forest Labs, 2023)(.1-dev) andStable-Diffusion3.5(large) (Stability AI, 2023) fortext-to-image.\nDatasets. The Text-to-text model is evaluated on four zero-shot tasks: WikiText (Merity et al., 2017) to assess themodel's prediction confidence, Longbench (Bai et al., 2024)and En.MC of InfiniteBench (Zhang et al., 2024b) for acomprehensive assessment of long context understandingcapabilities, and the Needle-in-A-Haystack task (Kamradt,2023) to assess the model's retrieval ability. Text-to-videomodels are evaluated using the open-sora (Zheng et al.,2024) prompt sets. Text-to-image models are assessed onCOCO annotations (Lin et al., 2014).\nEnd-to-end metrics. For Llama3.1, we use perplex-ity (ppl.) (Jelinek et al., 1977) for WikiText, Longbenchscore (Bai et al., 2024), and retrival accuracy for the Needle-in-A-Haystack task (Kamradt, 2023). For text-to-video mod-els, following Zhao et al. (2025), we evaluate the qualityof generated videos on five metrics: CLIPSIM and CLIP-Temp (CLIP-T) (Liu et al., 2024b) to measure the text-videoalignment; VQA-a and VQA-t to assess the video aestheticand technical quality, and Flow-score (FScore) for tempo-ral consistency (Wu et al., 2023). For text-to-image mod-els, generated images are compared with the images in theCOCO dataset in three aspects: FID (Heusel et al., 2017) forfidelity evaluation, Clipscore (CLIP) (Hessel et al., 2021)for text-image alignment, and ImageReward (IR) (Xu et al.,2024) for human preference.\nSpeed and sparsity metric. We use TOPS (tera operationsper second) to evaluate the speed of sparse attention meth-ods. Specifically, TOPS = O(attn)/t, where O(attn) rep-resents the total number of operations in a standard attentioncomputation, and t is the latency from a given (Q, K, V)to the output of attention. Note that this speed metric iscompletely fair. This is because the O(attn) is fixed for aset of inputs, and then the speed is determined by t, whichincludes the time spent predicting the sparse region of theattention map. We define Sparsity as the proportion of theMatmul of QiKj plus PV; that are skipped relative to thetotal number of QiK; plus PV; in a full attention required.\nImplementation and Hyper-parameters. We implement ourmethod using CUDA. As discussed in Sec. 3.6, we needto determine 11, 12 for models. We use (l1 = 0.08, 12 =0.09) for Llama3.1, (11 = 0.05, 12 = 0.06) forCogvideox and Mochi, and (l\u2081 = 0.07, 12 = 0.08) forStable-Diffusion3.5 and Flux.\nBaselines. Currently, sparse attention methods applicableacross different model types are limited. We choose block-sparse MInference (Jiang et al., 2024) and FlexPrefill (Flex-Prefill, 2025) as our baselines. To vary the sparsity of thesebaselines, we use 30% and 70% for MInference, and use\u03b3 = 0.95 and 0.99 for FlexPrefill according to their paper.\n4.2. Quality and Efficiency Evaluation\nEnd-to-end metrics. We assess the end-to-end metrics ofvarious models using SpargeAttn compared to using fullattention and baselines."}, {"title": "5. Conclusion", "content": "In this paper, we propose SpargeAttn, a universal sparse\nand quantized attention that executes attention efficiently\nand accurately for any input. Our method uses a two-stage\nonline filter: in the first stage, we rapidly and accurately\npredict the attention map, enabling the skip of some matrix\nmultiplications in attention. In the second stage, we design\nan online softmax-aware filter that incurs no extra overhead\nand further skips some matrix multiplications. Experiments\nshow that SpargeAttn accelerates diverse models, includ-ing language, image, and video generation models, withoutsacrificing end-to-end metrics."}, {"title": "A. Appendix", "content": "A.1. Detailed Explain and results of permutation ablation\nWe use five distinct prompts and pre-searched hyperparameters with l\u2081 = 0.05,l2 = 0.06 on both CogvideoX and\nMochi models. The permutation are performed separately in attention operation for Q, K, V after position embedding. To\nretain the original order of the input sequence, an inverse permutation is performed on the output of attention; for models\nusing visual-language joint self-attention(e.g., CogvideoX), we only permute the visual tokens. When evaluating block\nself-similarity, we choose a block size of 128 for query and 64 for key, which aligns with our kernel implementation. The\nprecision metric(L1) is evaluated using FlashAttention2 output as ground truth.\nWe choose different permutation methods to compare their impact on the performance of attention operations. Given a\n3D visual token tensor with shape T \u00d7 H \u00d7 W \u00d7 d, the permutation finally results in a tensor with shape L \u00d7 d, where\nL=T\u00d7H\u00d7 W. The permutation methods and their detailed descriptions are shown in Table 8.\nRandom permutation of tokens, the order is recorded to perform inverse permutation.\nPermutation following row-major order. Tokens are continuous along the W dimension.\nPermutation following column-major order. Tokens are continuous along the H dimension.\nPermutation following time-major order. Tokens are continuous along the T dimension.\nPermutation following a Hilbert curve.\nDetailed results of permutation ablation for the CogvideoX and Mochi models are presented in Table 9. The HilbertCurve\npermutation consistently achieves superior block self-similarity and sparsity, with only a marginal loss in precision. This\nsuggests that the HilbertCurve permutation effectively enhances block self-similarity and sparsity. It is worth noting that the\nrandom permutation retains the precision metrics but sacrifices sparsity. This indicates that our algorithm has the property of\ndynamically adjusting and robust to complex token sequences.\nA.2. Ablation Study of Self-Similarity Judge\nTo investigate the impact of the self-similarity judge on attention performance, we follow the experimental setting outlined\nin Sec. A.1 and conduct an ablation study by removing the self-similarity judge. In most cases, the presence of highly\nlocalized patterns results in a minimal number of non-self-similar blocks, leading to only minor differences in precision\nand sparsity when averaging across all tensor cases. To obtain more meaningful and interpretable insights, we specifically\nanalyze cases where the precision difference is statistically significant.\nTo this end, we apply a threshold-based selection criterion, retaining only those cases where the absolute difference between\nL1sim-judge (precision error with the self-similarity judge) and L1no-judge (precision error without the self-similarity\njudge) exceeds 0.05. This criterion results in approximately 2% of the tensor cases being retained for further analysis. We\nemploy precision (L1 error) and sparsity as evaluation metrics to assess the influence of the self-similarity judge on the\nattention output. The results are summarized in Table 10.\nThe findings demonstrate that the self-similarity judge effectively mitigates extreme precision loss while introducing only a\nmarginal reduction in sparsity. Furthermore, we observe that a significant proportion of cases exhibiting notable differences"}]}