{"title": "Grokking vs. Learning: Same features, different encodings", "authors": ["Dmitry Manning-Coe", "Jacopo Gliozzi", "Alexander G. Stapleton", "Edward Hirst", "Giuseppe De Tomasi", "Barry Bradlyn", "David Berman"], "abstract": "Grokking typically achieves similar loss to ordinary, \"steady\", learning. We ask whether these different learning paths - grokking versus ordinary training - lead to fundamental differences in the learned models. To do so we compare the features, compressibility, and learning dynamics of models trained via each path in two tasks. We find that grokked and steadily trained models learn the same features, but there can be large differences in the efficiency with which these features are encoded. In particular, we find a novel \"compressive regime\" of steady training in which there emerges a linear trade-off between model loss and compressibility, and which is absent in grokking. In this regime, we can achieve compression factors 25x the base model, and 5x the compression achieved in grokking. We then track how model features and compressibility develop through training. We show that model development in grokking is task-dependent, and that peak compressibility is achieved immediately after the grokking plateau. Finally, novel information-geometric measures are introduced which demonstrate that models undergoing grokking follow a straight path in information space.", "sections": [{"title": "1. Introduction", "content": "Grokking (Power et al., 2022) is a mode of training in which generalisation emerges suddenly after a long period of overfitting. We empirically address two important questions that this raises. First, are there fundamental differences between models learned by grokking and through ordinary\n\"steady\" training? Since grokking happens after a large number of epochs, it is expensive. Given this, it is natural to also ask: are there any compensating practical advantages?\nPrevious work has observed grokking in a variety of tasks (Liu et al., 2023a), and several mechanisms have been identified as its possible origin (Thilak et al., 2022; Liu et al., 2023a; Kumar et al., 2024; Tan and Huang, 2024; Imtiaz Humayun et al., 2024; Kozyrev, 2024). For modular addition, the workhorse of grokking studies, an exactly solvable model of grokking has been proposed (Gromov, 2024), and network interpretability measures can track how overfitting gives way to generalisation (Nanda et al., 2023).\nAll of these works contrast the generalising, post-grokking model to the memorizing, pre-grokking model of the same training run. This leaves it unclear whether grokking is simply a slower form of ordinary learning, or whether there are more fundamental differences between these two learning paths. Explanations of grokking typically revolve around two points. First, that there are multiple representations that can solve the task, and second, that the model generalises as a result of compressing to a more efficient representation. This raises two immediate questions: do grokking and conventional learning lead to different representations, and are there differences in compression between grokking and learning?\nWe hence compare the features learned and the compressibility of models trained via grokking and conventional learning. To make this comparison, we choose two paradigmatic tasks which are known to have interpretable features. Our first task is to classify snapshots of the two-dimensional Ising model (Ising, 1925; Baxter, 2007). This task can be interpreted in terms of physical variables - the energy and the magnetization of a snapshot (Hu et al., 2017; Suchsland and Wessel, 2018b). Our second task is modular addition. Networks trained on modular addition are well known to learn a Fourier representation of the problem, and previous work has explicitly quantified how sharply the model is localized in this basis (Nanda et al., 2023; Gromov, 2024; Doshi et al., 2024).\nAfter comparing the terminal models, we then ask whether we can identify differences in model development through training. To do so, we define summary measures of model development and track their evolution. These measures are task-dependent and must be constructed by hand. We then show that important features of model development can be seen in the information geometry of the model, which can be defined for a general task. Using the recently developed framework of Bayesian renormalization, (Amari et al., 2019; Berman and Klinger, 2024; Berman et al., 2023; 2024; 2023), we introduce novel measures based on the Fisher Information Metric (FIM) to study model development in our example tasks.\nWe draw the following conclusions:\n1. In both tasks, the features learned via grokking and steady learning are the same.\n2. The encodings, however, are different. In the modular addition task, we discover what we call a \"compressive regime\" of training. In this regime, tuning the weight scale at initialization results in a linear trade-off between the train loss of a model and its compressibility.\n3. The modular addition and Ising tasks have different model development trajectories. In modular addition, consistent with previous literature, our interpretability tools allows us to define \"progress measures\" in the grokking plateau. In the Ising task, however, we find no sign of model development before generalisation. This raises the question of whether we can define new progress measures that would indicate a coming jump in capability before generalisation.\n4. In both tasks, the grokking trajectory follows an approximately straight line in model space, as defined by the Fisher Information Metric.\nThe first two points are about the terminal model after training, and are addressed in Sections 4 and 5, respectively. The latter three points concern the dynamics during training and are the subject of Section 6. For convenience, an introduction to our two tasks is provided in Section 2, and our grokking methodology is outlined in Section 3."}, {"title": "2. Task Descriptions: Ising and Modular Addition", "content": "2.1. The Ising task\nOur first task is classifying the phases of the two-dimensional Ising model, perhaps the simplest physical system that exhibits a phase transition (Ising, 1925; Baxter, 2007). On each vertex of a two-dimensional lattice, the model hosts a vector called a \"spin\" that can either point up or down. This system has two distinct phases: an \"ordered\" phase where the spins align with each other, and a \"disordered\" phase where the spins are randomly oriented.\nIn our model, we consider a square grid where each site i contains a spin of value \u03c3\u2081 = \u00b11, corresponding to up and down. We call an assignment of \u03c3\u2081 = \u00b11 to every site i in the grid a \"snapshot\". A snapshot is hence just a binary image\nSnapshots are characterized by two physical quantities: energy and magnetization. The energy is defined as a sum over nearest-neighbour pairs of spins,\nE = - \u03a3 \u03c3\u1d62\u03c3\u2c7c,  (1)\n(i,j)\nwhere (i, j) are adjacent sites and periodic boundary conditions are used. The energy determines the probability distribution of different snapshot configurations (see Appendix B for more details). Based on Eq. (1), neighbouring spins that are misaligned \"cost\" positive energy. Disordered snapshots have many misaligned spins, and therefore a higher energy than ordered snapshots. The magnetization of a snapshot is the sum of all of its spins:\nM = \u03a3 \u03c3\u1d62 (2)\ni\nIn the disordered phase, opposite spins cancel in the sum and M is small compared to the number of spins. In the ordered phase, spins align with each other and |M| is large. Viewing a snapshot as an image, the magnetization quantifies its brightness and the energy captures the number of light to dark interfaces. Classifying snapshots is therefore analogous to the problem of image recognition.\nIn our first task, we train a Convolutional Neural Network (CNN) to classify a given snapshot of the Ising model as \"ordered\" or \"disordered\". Concretely, our input data x is a 16 \u00d7 16 square grid of \u00b11 values, and our labels are y = 0, 1 corresponding to whether the phase is ordered or disordered. For all Ising classification tasks in the main text, we use two convolutional layers and one fully connected layer, ReLU activations, and optimize the cross entropy loss with the Adam optimizer (Kingma and Ba, 2017). Our training data are 300 snapshots equally divided between disordered and ordered configurations. The snapshots are generated by running a local-update Monte Carlo simulation at different temperatures surrounding the critical point between the two phases (see Fig. 1).\nIn an infinite system at equilibrium, the magnetization of a snapshot uniquely characterizes its phase. However, because our snapshots are of a small system that may not be fully equilibrated due to the local update Monte Carlo simulations, the classification problem is more difficult than simply counting how many spins have a positive sign. In particular, ordered snapshots with an anomalously low magnetization can be misclassified if their energy is not taken into account."}, {"title": "2.2. Modular Addition", "content": "Our second task is modular addition. We train models to solve modular equations of the form c = (a + b) % P. Our input data is the set of P2 two-hot encoded vectors corresponding to the pair a and b, i.e. the length 2P vectors Xa,b = (01, ..., 1a, ..., 0p, 0P+1, ..., 1P+b, ..., 02P), and our output vectors are the one-hot encoded length P vectors corresponding to c, yc = (01, ..., 1c, ...,0p). We train a fully connected network with a single hidden layer and ReLU activation, minimizing the cross-entropy loss using the Adam optimizer for P = 113. Our training data is a fixed fraction (70% in the main text) of randomly chosen samples from the set of all possible pairs. The full details of the model and data for both tasks is in the Appendix table 1."}, {"title": "3. Inducing grokking", "content": "We first show that we can tune between grokking and conventional learning regimes while maintaining the same training data, architecture, and regularization. This allows us to generate comparable models in each regime which differ in whether they were trained via grokking or steady learning. In subsequent sections, we will compare the features, compression, and development of these models.\nWe use a pragmatic definition of the grokking time:\nDefinition 1 (Grokking time). The grokking time is the number of training epochs between when the model is within 5% of its maximum test accuracy, ttest, and when the model is within 5% of its maximum train accuracy, ttrain:\ntgrok = ttest - ttrain (3)\nfrom which we define grokking as:\nDefinition 2 (Grokking). A training run is said to grok if tgrok > ttrain. Otherwise, we say that the model \"steadily learns\".\nFollowing (Liu et al., 2023a), we induce grokking by simply multiplying the initialization weights by a scale factor wo, favouring overfitting during training. The resulting training curves for the Ising task are shown in Fig. 2(a) and for the modular addition task in Fig. 2(b). The curves are averaged over five and ten seeds (random initialization), respectively, and we summarize the distribution over individual seeds in Appendix C. Increasing the weight initialization scale wo smoothly interpolates between the grokking and steady learning regimes. According to our definition, grokking sets approximately at wo = 3 for the Ising task, and wo = 0.4 for the modular addition task."}, {"title": "4. Grokked and learned models have the same features", "content": "4.1. Learning Ising phases: energy and magnetization are encoded in the neuron activations\nWe now show that models reached via grokking and \"steady\" learning learn the same dataset features. Previous studies have established that the key features learned by models trained on the Ising task are the energy (Eq. (1)) and the magnetization (Eq. (2)) (Hu et al., 2017; Suchsland and Wessel, 2018b). In these earlier works, it was shown that models (trained via the conventional steady learning trajectory) learned a mix of both features. To make a cleaner comparison between grokked and learned models, it is helpful to have a single dominant learned feature. We thus deliberately design our setup to favour learning the energy. To achieve this, we include training snapshots that are in the ordered phase but have anomalously low magnetization. These snapshots are correctly classified by the energy but misclassified by the magnetization, and often appear as metastable states in local-update Monte Carlo simulations of the Ising model (Suchsland and Wessel, 2018a). Moreover, we use a more powerful network, including a CNN component to detect the interfaces associated with calculating energy.\nTo understand which of the three features the model learns, we correlate the pre-activations for all neurons in the networks with the energy, the magnetization, and the absolute value of the magnetization. In Fig. 3, we compare the result for models reached via grokking and steady learning trajectories. We find that, in both grokking and steady learning, every neuron in the final layer is either perfectly correlated or anti-correlated with the energy of the input. In Appendix C, we show how this develops in each layer of the model. For every seed, in both learning and grokking, the energy has the highest correlation of the three measures with the majority of neuron activations in the second CNN and in the fully connected layer. We hence conclude that both grokking and steady learning models classify snapshots by learning the same feature - the energy."}, {"title": "4.2. Learning modular addition: both models learn the Fourier coefficients", "content": "To better understand the task dependence of our comparison between grokking and learning, we also study the modular addition task. A number of studies have convincingly shown that models trained on the modular addition task typically learn a Fourier representation of the problem (Power et al., 2022; Nanda et al., 2023; Gromov, 2024). For a two layer MLP with square activation, (Gromov, 2024) showed that an ansatz of Fourier modes for the model weights solves the modular addition problem. Moreover, the inverse participation ratio (IPR) can be used as a measure of how well-localized the weights of the model are in the Fourier basis. This provides a quantitative comparison of the extent to which grokked and learned models learn the same features.\nThe distribution of neuron post-activations in the fully connected layer is shown in Fig. 4(a-b). We see that each neuron learns the sine and cosine components of a single frequency. To quantify the localization to a given frequency in a given neuron k, we first Fourier transform the input (output) weight matrices in the embedding (unembedding) dimension, and then use Gromov's Inverse Participation Ratio measure (Gromov, 2024) to quantify the localization to a given frequency:\nIPR(k) = \u03a3 |Wv,k|\u2074 / (\u03a3 |Wv,k|\u00b2)\u00b2 (4)\nv=1 D\nv=1 D\nwhere v is the index of the embedding (unembedding) dimension of the input (output) matrices to the hidden layer, and D = 2P (D = P) is its size. The results are shown in Fig. 4(c-d). First, we notice that the average IPR is around 1/2, consistent with each neuron predominantly processing the sine and cosine components of a given frequency. Second, we see comparable mean IPRs between the grokking and the steady learning cases; 0.43 and 0.53, respectively."}, {"title": "5. Learning models can be more compressible than grokking models", "content": "Having established that both grokking and steady learning lead to similar features, we now show that there can nevertheless be significant differences in the efficiency with which the features are encoded. To see this, we use magnitude pruning on the end model in both tasks. Our key result in this section is the emergence of a linear trade-off between end model training loss and compressibility in modular addition (Fig. 5). We call this parameter range the \u201ccompressive regime\u201d. The compressive regime is a particularly striking example of a general feature: despite learning the same dataset features, grokked and steadily learned models can exhibit large differences in compressibility.\nWe start by introducing our pruning scheme. In the main text, we use a global magnitude pruning scheme. In the appendix, we show the results of pruning each layer in the network individually. To account for differences in weight scales and layer architecture, we slightly modify naive global pruning. First, we rank all weights within each layer separately by their absolute values. Given a pruning fraction p, we set the smallest p fraction of the weights in each layer to zero, and then evaluate the model accuracy on the test set. To quantify model compressibility, we first integrate the accuracy a(p) at each pruning fraction p with respect to p - i.e. we find the area bounded by the pruning curve and the x-axis in Fig. 5(a-b). We then define the compressibility c as:\na = \u222b a(p)dp, c = a / (1 - a) (5)\nThis is analogous to finding the size of the pruned model at a fixed accuracy, but averaged over the entire range of pruning.\nWe show the results of pruning across weight multipliers for modular addition in Fig. 5(a). In the steady learning regime wo \u2264 0.4, model compressibility improves systematically with reducing weight multiplier. In the Fig. 5(c), we see that once the model has transitioned into this parameter regime there emerges a striking linear trade-off between encoding efficiency and end model training loss. In Appendix D, we provide data for the relationship between the compressibility and a range of other network measures. In particular, we emphasize that the there is no correlation between compressibility and the test loss. Surprisingly, we also find no relationship between the compressibility and the model degeneracy, as measured by the local learning coefficient (Hoogland et al., 2024; Chen et al., 2023; Lau et al., 2024).\nIn the grokking regime of modular addition, we see that there is no significant dependence of model compressibility on wo the red curves are on top of each other. Hence, although models trained in both grokking and steady learning learn the Fourier components of the input data, models trained in the compressive regime of steady learning are significantly more compressible. We emphasize that the existence of a compressive regime in steady learning is not a generic feature, but is parameter-dependent. In Appendix F, we show that this regime can be eliminated by increasing the batch size from 64 as used in the main text to 200. The compressive regime is thus a novel parameter regime of steady learning, rather than a generic feature.\nIn the inset of Fig. 5(a), we show that we can achieve extremely high compressions for a higher weight decay (3 \u00d7 10\u207b\u2074 vs 3 \u00d7 10\u207b\u2075). Pairing this with a low weight multiplier, we are able to cut 95% of the weights in the model for a 5% drop in accuracy, a compressibility 25x that of the original model, and 5x the highest compression achieved in at the lower weight multiplier used in the rest of the main text. In Appendix E, we explore this regime. We show that in this extremely compressed regime, the mean IPR IPR falls by close to an order of magnitude, indicating that the model's Fourier representation is breaking down. We also note that the model is itself close to breakdown - models with a 30% larger weight decay no longer learn at all.\nFinally, we show the model compressibility for the Ising task in Fig. 5(b,d). Here we see no clear relationship between compressibility and weight multiplier, and therefore no compressive phase. Furthermore, there is no meaningful trade-off between model compressibility and the end loss. This reinforces the view that the compressive phase found"}, {"title": "6. Model development in grokking and learning", "content": "Having established measures for both the dataset features learned and model compressibility, we now ask how they develop over the course of training. We present two main results. First, that the development of models in the pregrokking plateau is different in the Ising and modular addition tasks. In the modular addition task, consistent with previous literature on \"progress measures\u201d (Nanda et al., 2023; Gromov, 2024), we find that the model continues to develop during the plateau, despite the fact that the accuracy does not improve. For the Ising task, however, we find that the model does not develop in the grokking plateau. This suggests that model dynamics in grokking is task dependent. We support this through the introduction of novel FIM-inspired measures for trajectory analysis in model space, which provide another perspective for identifying model development.\n6.1. Feature Development\nTo measure the development of model features, we first define layer-wide summary averages for our interpretability measures. For the Ising model, we define a pre-activation weighted correlation coefficient for a layer L as the sum of the (absolute value of) the correlation coefficients for each neuron k in that layer, weighted by that neuron's share of the absolute pre-activation, Wk\nwL = \u03a3 |rh,L| Wk (6)\nkEL\nr = 1/|L| \u03a3 \u03a3kh,Lzh,i/(\u03a3iE|D|zk,i\u00b2)\u00b9/2,\n\u03a3iedEwhere zh,i is the pre-activation of the k-th neuron in layer L on the input vector i, |D| is the number of elements in the test dataset and |L| is the number of neurons in the layer. For modular addition, we simply take the mean of the IPR across neurons k in the hidden layer to define IPR as:\nIPR = 1/|L|\u03a3 IPR(k) (7)\nkEL\nWe summarize the behaviour of these measures during training for the modular addition task in Fig. 6 and for the Ising task in Fig. 7, and additional data is provided in Appendix G. The two tasks have markedly different behaviour in the grokking plateau. In the modular addition case, the model improves its localization in the Fourier basis throughout the plateau despite no improvement in model accuracy or loss. This is consistent with the \"Circuit formation\" picture described for a transformer in Nanda et al.(Nanda et al., 2023) and for a two layer fully-connected network by Gromov (Gromov, 2024). The model trained on the Ising task, however, does not improve its representation of the energy in the plateau - all three metrics are roughly constant across all layers in the plateau. Whether or not the model is developing features in the grokking plateau is hence task dependent. Interestingly, in both grokking and steady learning, the model continues to improve its representation of the dominant feature well after the accuracy and loss are saturated."}, {"title": "6.2. The dynamics of compressibility", "content": "We also consider the dynamics of model compressibility. The results for the Ising task are given in the middle row of Fig. 7(c-d). In the grokking case, we see that the compressibility peaks a short time after the accuracy saturates and then declines slightly. The results for the modular addition task are given in Fig. 6(c-d). Here we see a transient 50% larger than the steady state compressibility for the steady learning trajectory, and 67% larger in the grokking trajectory. Comparing to the model formation measures in Fig. 6(a-b), we note that after the compressibility peak, the compressibility decreases as the neuron IPR \u03b3 increases. We also note that we observed the transient peak in the compressibility in all our modular addition runs - even at larger batch sizes for which there is no compressive phase. This suggests that the transient behaviour in the compressibility is a distinct phenomenon from terminal model compression."}, {"title": "6.3. Information-Geometric Trajectory Analysis", "content": "Once a model architecture has been set, the space of parameters defines the space of functions the model can represent, known as the model space. This model space may have a non-trivial geometry, which can be measured by the Fisher Information Metric (FIM). We give a fuller account of this connection in Appendix F. The FIM can then be used to measure how the information geometry of a model changes in training.\nWe use this connection to introduce measures of model \"speed\" and \"direction\u201d in model space through training. The measures are the information-geometric generalisation of step magnitude and cosine similarity between consecutive update steps along the trajectory. Given a model space position 0e at epoch e, the model space step is defined as se := 0e+f \u2013 0e, where f is the frequency between samples which is 200 (1) for the Ising (modular addition) task. From this the step magnitude and cosine similarity between two steps s and s' are given by\n|s|FIM := \u221a(\u03a3gi,j s\u1d62 gi,j s\u2c7c ) \u2208 [0,00) , (8)\nSc-FIM(s, s') := \u03a3gi,j si,j s'/(|s|FIM|S'|FIM) \u2208 [-1,1], (9)\nwhere repeated indices are summed. Focus is placed on the cosine similarity, where values close to one indicate steps that are approximately parallel, while values close to zero indicate orthogonal steps. We track the step cosine similarities, averaged over the 10 seed runs, for both the Ising and modular addition tasks, and for both the grokking and steady learning. In particular, we compute the cosine similarity of consecutive steps, se and se+1, and show the results in Fig. 8(a-b)."}, {"title": "7. Conclusion", "content": "To compare the features learned in grokking and learning as closely as possible, we studied toy models with clearly measurable features. This immediately raises the question of which of our findings generalise to larger models. In general, our work suggests three main questions for further work.\nFirst, are there properties of grokking that are practically useful? There are two ways in which this could occur: a grokked model may learn different features or be more compressible than one trained through \"steady\" learning. In our case, the features learned were the same, and the steady learning regime had higher compression. If both of these conclusions are generally true, this suggests that grokking should be eliminated whenever possible. In our tasks, we accomplished this by tuning the weight scale at initialization. However, it is unknown if there are tasks which can only be grokked, or, vice versa, can only be steadily learned.\nSecond, what is the nature of the compressive regime, and can it be used to compress practical models? Fortunately, we discovered a compressive regime in ordinary learning and not in grokking. The very large compression achieved raises the interesting possibility of using such a training scheme to obtain more compressible models in general. In addition, the compressive regime allows us to tune the size of the effective model by changing the initial weight multiplier. Since models are known to compress features by superposition (Elhage et al., 2022), it would be particularly interesting to investigate in further work whether models in this regime display significantly higher superposition than models trained outside of it.\nFinally, can information geometry be used to help scale interpretability? Our interpretability measures allowed us to follow model development through training. Although this provided progress measures in which the model develops smoothly in the modular addition task, it does not do so for the Ising task, where generalisation still emerges suddenly. Crucially, these measures had to be constructed by hand for each task. By contrast, ouinformation geometric measures were general, and identified signatures of the absence of model development in the Ising task as well as the nontrivial pre-grokking dynamics of the modular addition task. It would be interesting to further explore how information geometric measures can be used for model interpretability."}, {"title": "A. Model parameters and seed distribution", "content": "In this work, we control grokking solely by the modification of the weight multiplier. For convenience, table 1 provides all other model parameters for the data generated in the main text."}, {"title": "B. The Ising classification problem", "content": "This section gives a more formal definition of the Ising model used in the main text. Generically, the Ising model is defined on a graph G (with a vertex set V and an edge set E), where each node is associated with a discrete variable \u03c3\u03b1 \u2208 {\u22121,1}, called a spin. The space of spin configurations (snapshots) is given by all possible combinations \u03a9 = {\u22121,1}V. The Ising model can be defined as a probability distribution over this space of snapshots \u03a9. More precisely, a snapshot {x}x\u2208\u03bd iw drawn from the Boltzmann distribution:\np({\u03c3\u03b1}) = e^(-\u0395({\u03c3\u03b5})/\u03a4) / Z(T) (10)\nwhere T is the temperature of the system, Z(T) is a normalization factor (the partition function), and E({x}) is the energy (cost) function:\n\u0395({}) = \u2212J \u03a3 \u03c3\u03af\u03c3 (11)\n(i,j)\u2208E\nHere the sum is over edges of the graph, and J > 0 is the interaction strength between spins, which in the main text is set to one for simplicity. The phase transition is determined by the competition between the interaction J, which favours ordering"}, {"title": "C. Feature development in the Ising CNN", "content": "Here we provide the by-layer breakdown, across all seeds, of the features learned by the model trained on the Ising task for the grokking and learning cases. We note first that features become better defined as we step through the network. In the first CNN layer, there appears to be no information about the energy or absolute magnetization. In the second CNN layer this information emerges, and then is processed by the hidden layer, where the absolute value of the correlation to the energy in each neuron is around 0.99 and to the absolute magnetization around 0.9. We note in passing that the fully connected layer can be considered to be acting as a non-linear probe on the second convolutional layer (Alain and Bengio, 2018), which supports the view that the information about the energy is already formed at this point in the network.\nWe also note that the representation is more noisy in the grokking than in the learning regime. In the learning regime fig. 13, every seed has almost all of it's fully connected layer neurons close to perfectly correlated or anti-correlated with the energy. In the grokking regime fig. 13, three of the seeds have significant deviation from perfect correlation with the energy, and instead correlate to the raw value of the magnetization."}, {"title": "D. Compressibility", "content": "We present additional results for compressibility. In the modular addition task, we identified a \u201ccompressive regime\" which is characterized by a linear trade-off between end model loss and compressibility. Whether or not this regime exists depends on a wider range of model parameters. Although we have not done an exhaustive parameter search for the compressive regime, we have found that it is increasing the batch size to 200 from 64 in the main text eliminates the compressive effect, as shown in fig. 14.\nTo better understand the nature of the compressive regime, we also provide data for the correlation between model compressibility and a range of other measures in fig. 15. We also provide equivalent data for the non-compressive, batch size 200, runs in fig. 16. We first note that, despite the linear relationship with the train loss in the compressive regime, there is no relationship with the test loss in either the compressive or non-compressive regime.\nSecond the compressive regime is associated with a significant reduction in the localization of neurons to a particular frequency. We see in the rightmost plots of the bottom row that for both input and output weight matrices to the hidden layer, the compressive regime leads to an improvement in compressibility through a decrease in Fourier basis localization. In the extremely compressed regime that we discuss in appendix E, this effect becomes very dramatic. The low weight multiplier runs are associated to an almost complete breakdown of Fourier basis localization (fig. 15). It is important to point out that the reduction in the Fourier basis localization is a property of the compressive regime and not a difference between grokking and learning generally. In fig. 16 we provide data for the same measures, but at the higher batch size. Here, there is no significant difference between the IPRs of grokking and learning."}, {"title": "E. 25x compression in modular addition", "content": "Conducting a rough parameter search, we found a range with extremely compressed models for weight decay 3 \u00d7 10-4, an order of magnitude larger than that considered in the main text. In fig. 18, we show the pruning curves for the same parameters as in the main text, but at the higher weight decay. We see a very large improvement in the pruning, that occurs suddenly as we transition into the compressive regime. Model compressibility increases suddenly from around 32% of the"}, {"title": "F. Fisher Information Geometry", "content": "This section provides a whistle-stop tour of the basics of information geometry. For a more complete treatment", "as": "ngFIM := Ex(lilj), = Ex(\u2202/\u2202\u03b8\u1d62 ln(fNN(x|\u03b8)) \u2202/\u2202\u03b8\u1d62 (14)\nThe expectation is theoretically an integral over the entire data space x, however practically we compute a discrete expectation as an average over samples from the training data. The metric changes depending on the position in model space since fNN is a non-linear function of \u03b8, and this is what defines the non-flat geometry of the space. The discrete derivatives are computed using pytorch functionality, via the nngeometry package (George, 2021).\nThe Fisher information metric provides a measure of the similarity between two models, well illustrated by the connection to the KL-divergence, DKL, which measures the relative entropy between probability distributions p(x|\u03b8)\nDKL(0'|0) = Ex[p(x|0)ln(p(x|0)/p(x|0'))"}]}