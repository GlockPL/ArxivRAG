{"title": "Power- and Fragmentation-aware Online Scheduling for GPU Datacenters", "authors": ["Francesco Lettich", "Emanuele Carlini", "Franco Maria Nardini", "Raffaele Perego", "Salvatore Trani"], "abstract": "The rise of Artificial Intelligence and Large Language Models is driving increased GPU usage in data centers for complex training and inference tasks, impacting operational costs, energy demands, and the environmental footprint of large-scale computing infrastructures. This work addresses the online scheduling problem in GPU datacenters, which involves scheduling tasks without knowledge of their future arrivals. We focus on two objectives: minimizing GPU fragmentation and reducing power consumption. GPU fragmentation occurs when partial GPU allocations hinder the efficient use of remaining resources, especially as the datacenter nears full capacity. A recent scheduling policy, Fragmentation Gradient Descent (FGD), leverages a fragmentation metric to address this issue. Reducing power consumption is also crucial due to the significant power demands of GPUs. To this end, we propose PWR, a novel scheduling policy to minimize power usage by selecting power-efficient GPU and CPU combinations. This involves a simplified model for measuring power consumption integrated into a Kubernetes score plugin. Through an extensive experimental evaluation in a simulated cluster, we show how PWR, when combined with FGD, achieves a balanced trade-off between reducing power consumption and minimizing GPU fragmentation.", "sections": [{"title": "I. INTRODUCTION", "content": "The rise of Machine Learning (ML) and large language models (LLMs) significantly amplifies the demand for GPUs in modern large-scale computing infrastructures, commonly called GPU datacenters. Indeed, the adoption of ML and LLMs is expected to rapidly grow further with larger models [17] having higher training and inference costs [15]. This, in turn, drives up operational costs and power consumption of GPU datacenters, thus exacerbating their environmental impact [1], [3], [13], [14]. In this work, we address the problem of online scheduling in GPU datacenters handling hybrid ML workloads. This requires scheduling tasks to nodes as they arrive without prior knowledge of future task arrivals. To optimize GPU utilization, GPU datacenters typically allow multiple tasks to share the same GPU within given resource constraints [18], [19], [22]: this is called GPU-sharing, and is realized via policies such as time-sharing or NVIDIA Multi-Process service. In this setting, we optimize and reconcile two distinct objectives: minimizing GPU fragmentation, and minimizing the power consumption."}, {"title": "II. PRELIMINARIES AND PROBLEM DEFINITION", "content": "We consider a GPU datacenter managed by the Kubernetes orchestration system and comprising N nodes, which may be heterogeneous in nature. Most of the nodes are equipped with both CPUs and GPUs, though the types of CPUs and GPUs may vary between nodes. However, within any given node, the CPUs and GPUs are of the same type. We assume that the datacenter employs some GPU-sharing policy, i.e., multiple tasks are allowed to share a GPU within the limits of its memory and computational resources. One of these policies is GPU time-sharing, where a single GPU is shared among multiple tasks by allocating each task a dedicated time slice for exclusive access [19]. The main drawback of this approach is the overhead associated with context switching. The other GPU-sharing policy is NVIDIA's Multi-Process Service\u00b9 (MPS) [22], which allows concurrent execution of kernels from different tasks on a single GPU with minimal context-switching overhead by using a shared GPU context. In this work, we assume the use of GPU time-sharing, but the results apply to other GPU-sharing policies such as MPS. We do not consider NVIDIA's Multi-Instance GPU (MiG) [22] a GPU-sharing policy, since it physically partitions the resources of a compatible GPU into isolated instances with their own memory, cache, and compute cores. This slightly reduces overall resource availability, and instances are less power-efficient than the full GPU2 [9].\nModelling Node Resources. Following the notation provided in [19], we describe the computational resources available on any node n using the node's unallocated resource vector:\nRn = (RCPU, RMEM, R PGPU,..., RGPU).\nIn this vector, $R^{CPU}$ denotes the number of virtual unallocated CPUs (note that $R^{CPU}$ is a real number because tasks can require the allocation of fractions of virtual CPUs) and $R^{MEM} > 0$ is the amount of unallocated RAM. Moreover, given the set of $G_n$ GPUs equipping node n, $R^{GPU}_g \\in [0,1]$ specifies the percentage of unallocated memory and computational resources on a specific GPU $g \\in \\{1,...,G_n\\}$. In a complementary way, we denote as $R_a^n$ the allocated resource vector for node n, where: $R_a^n = (R_a^{CPU}, R_a^{MEM}, R_a^{GPU_1},..., R_a^{GPU_{G_n}})$. Symmetrically to Rn, each value of $R_a^n$ represents the resources of node n currently allocated to some task. Finally, we denote by $type^{CPU}(n)$ and $type^{GPU}(n)$ the CPU and GPU model installed in node n."}, {"title": "Modelling tasks.", "content": "Let t be a task submitted to the datacenter. The resources required by the task are described by the task resource demand vector Dt and task constraint set Ct:\nDt = (DCPU, DMEM, DGPU), Ct = {CCPU,CGPU}.\n$D^{CPU}$ and $D^{MEM}$ are the CPU and memory requests to be satisfied by the corresponding capacities $R^{CPU}$ and $R^{MEM}$ in Rn, while $D^{GPU}$ represents the GPU resources required by t. As in [19] we assume that $D^{GPU} \\in [0, 1) \\cup Z\u207a$, i.e., a task can demand no GPU (0 case), partially use a GPU ((0, 1) case), exclusively use one or multiple GPUs ($Z^+$ case), but cannot demand to both (1) share a GPU with other tasks and (2) fully utilize one or multiple GPUs. Furthemore, a task might require to be executed on specific CPU or GPU models; if so, such information is indicated in $C^{CPU}$ and $C^{GPU}$. Moreover, we assume that if a task is using a fraction of a GPU's resources, and no other tasks are currently using the same GPU, then the task can opportunistically use all the computational resources of that GPU while remaining within the memory limit specified by $D^{GPU}$. Finally, we introduce a scalar function un, used to determine if a node n has sufficient GPU resources to execute some task, i.e. $u_n = \\mathbb{1} [\\lfloor R^{GPU} \\rfloor] + max_{g \\in \\{1,...,G_n\\}} (R^{GPU}_g - \\lfloor R^{GPU}_g \\rfloor)$.\nEstimating the Power Consumption. To estimate the power consumption of a GPU datacenter, we limit ourselves to those components that significantly contribute to the overall consumption and have variable power usage, i.e., the CPUs and GPUs. Focusing on the CPUs, recall that $R^{CPU} (R_a^{CPU})$ represents the node's virtual unallocated (allocated) CPUs. In modern GPU datacenters, 2 virtual CPUs are typically mapped to a CPU physical core. Thus, $R^{CPU}$ can be used to directly determine the number of physical CPUs being currently used in a node. Let $P_{max}(type^{CPU}(n))$ and $P_{idle} (type^{CPU}(n))$ be functions that return the maximum and minimum power consumption of a physical core of the specific CPU model present in n. Moreover, let also $ncores(type^{CPU}(n))$ be the number of physical cores present in the CPU type equipping node n. Then, we estimate the total power consumed by the node's CPUs as:\n$P_{CPU}(n) = \\sum_{i=1}^{G_n} \\frac{P_{max} (type^{CPU}(n)) \\cdot \\lceil \\frac{R_a^{CPU}}{2 \\cdot ncores(type^{CPU}(n))} \\rceil + P_{idle} (type^{CPU}(n)) \\cdot \\lfloor \\frac{R^{CPU}}{2 \\cdot ncores (type^{CPU}(n))} \\rfloor} {Gn}$. (1)\nAccording to this modeling, CPUs that are even minimally used (first term of the sum, with the ceil operator) are assumed to consume maximal power, while those that are idle (second term, with the floor operator) are assumed to consume the lowest possible power. While this formula is bound to overestimate the power consumption of the CPUs of a node, we argue that it provides a safe and reasonable estimation when used in a simulator or when it is not possible to measure the actual power consumption in real-time. Similarly, for the GPUs of a node, we define $P_{max}(type^{GPU}(n))$ and $P_{idle} (type^{GPU}(n))"}, {"title": "GPU Fragmentation.", "content": "Modelling GPU fragmentation requires to first define the concept of target workload M derived from historical data [19]. A target workload describes how tasks are categorized based on the amount of CPU and GPU resources they require, effectively classifying tasks into different classes. For example, a target workload M may include a task class m that requests resources $D_m = (D^{CPU} = 8, D^{GPU} = 2)$, and another class that requests $D_{m'} = (D^{CPU} = 32, D^{GPU} = 8)$, among others. Each class m is also associated with a popularity score pm, which represents the probability of tasks from that class appearing in the workload M. The GPU fragmentation of node n for class m, denoted as Fn(m), measures how many of the node's unallocated GPU resources cannot be used by a task from class m. For further details on the two cases that determine how Fn(m) is computed, we refer the reader to [19]. Once the GPU fragmentation of a node is computed for each task class $m \\in M$, the expected GPU fragmentation of a node n, with respect to a task randomly sampled from the target workload M, can be estimated as: $F_n(M) = \\sum_{m \\in M} p_m F_n(m)$. The expected GPU fragmentation Fn is thus a simple statistical measure reflecting how fragmented the GPU resources of node n are, on average, for tasks across all classes in the workload. In turn, the expected GPU fragmentation of a datacenter for a target workload M can be computed as:\nFdatacenter = \u2211n=1 Fn(M) (4)"}, {"title": "Problem Definition.", "content": "We now consider the online scheduling problem, which involves assigning tasks to nodes as they arrive, without prior knowledge of future task arrivals. Each task scheduling is an atomic operation, i.e., a new scheduling decision starts only after the previous one has completed. The goal is to assign an incoming task t to a suitable node \u03b7 \u2208 {1,..., N}. We denote this assignment as t \u2192 n. As before, let M represent the target workload, modeled as discussed earlier. The objective is to determine the optimal assignment of task t to the node n of the datacenter that minimizes the following objective function:\narg min \u03b1 Pdatacenter + (1 \u2212 \u03b1)\u00b7 Fdatacenter, (5)\nt\u2192n\nwhere \u03b1 is a weighting parameter that balances between the estimated power consumption and GPU fragmentation. This optimization is subject to the following constraints, assuring that the task's demands do not exceed the resources available in a node:\n(Cond. 1) DCPU < RCPU\n(Cond. 2) DMEM < RMEM\n(Cond. 3)\nDOPU\n\u2264 un if DGPU \u2208 Z+\nU \u2264 (Un - [Un]) if DGPU \u2208 (0,1)\n(Cond. 4) if CCPU \u2260 0 then typeCPU(n) \u2208 CCPU\n(Cond. 5) if CGPU \u2260 0 then typeGPU (n) \u2208 CGPU"}, {"title": "III. RELATED WORK", "content": "In this work, we address the problem of minimizing power consumption in the context of scheduling policies for GPU datacenters dealing with hybrid ML workloads. Existing research in this area can be broadly categorized into two approaches: batch scheduling and online scheduling. Batch scheduling processes batches of tasks at predetermined intervals, while online scheduling assigns tasks to nodes as they arrive, without knowledge of future task arrivals. While batch scheduling often leads to better optimization outcomes, online scheduling typically relies on greedy algorithms, which may result in suboptimal solutions. Research focusing on batch scheduling often aims to optimize multiple objectives with integer linear programming, or by using evolutionary algorithms, bio-inspired methods, machine learning techniques, or combinations of these [11], [16], [10], [8], [22]. Batch scheduling approaches generally rely on several assumptions about task characteristics, such as fixed task arrival times and known resource requirements. Consequently, hereinafter we focus on works that address online scheduling, which offers greater adaptability and is more aligned with the objectives of this study.\nThe recent seminal work by [19] introduces a formal framework for characterizing online task scheduling in modern GPU datacenters. Their model assumes that a GPU datacenter must handle hybrid ML workloads and enable GPU time-sharing for maximizing the utilization of GPU resources. Their work does not focus on power consumption, and in this paper we extend their framework to deal effectively with both GPU fragmentation and power consumption.\nHu et al. [7] introduce the Cluster Energy Saving (CES) service, which operates within an orchestrator and uses a gradient-boosted decision tree model trained on various node features to predict future cluster behavior. Specifically, the model predicts how many idle nodes can be powered down using Dynamic Resource Sleep (DRS) while maintaining cluster usability. This solution is orthogonal to ours. More broadly, we believe that our contribution, focused on power-aware"}, {"title": "IV. POWER-AWARE SCHEDULING POLICY: PWR", "content": "As described in Equation 3 from Section II, we model power consumption using the estimated power consumption, denoted as Pdatacenter. The pseudocode in Algorithm 1 shows how the Kubernetes scheduling framework operates when the score plugin implementing our power-aware scheduling policy PWR is used. The scheduler takes in input the task t and the current status of the datacenter, including detailed information about its nodes, their specifications, and their (un)allocated resources. The variables S and n* are then initialized (lines 1-2). Here, S is an initially empty set of pairs, where each pair represents the increase in power consumption if task t were scheduled on a particular node n. The variable n*, initially undefined, represents the node on which t will eventually be scheduled. The parallel for loop, between lines 3 and 8, evaluates the impact on the datacenter's power consumption if task t were scheduled on each node in N. For each node n, the scheduler first checks whether the node has sufficient resources and meets all task constraints (line 4). This check is actually performed by the filter plugin within the Kubernetes scheduling framework. If node n satisfies both conditions, the scheduler applies the logic of PWR through the score plugin. It begins by hypothetically assigning task t to node n using the HYPASSIGNTONODE function (line 6). This function simulates the assignment by creating an updated copy of n's allocated and unallocated resource vectors, represented by the variable nh. Next, the scheduler calculates the increase in n's total estimated power consumption, denoted as \u0394, if task t were to be scheduled on n (line 7). This increase is computed using the power consumption model introduced in Section II, which takes into account the resource vectors of both n and nh. The result is then stored in S (line 8). Finally, the Kubernetes scheduling framework assigns task t to the node n* that meets the resource requirements and results in the smallest increase \u0394 in estimated power consumption, if any (lines 9-10).\nA. Combining PWR and FGD: a power- and GPU fragmentation-aware scheduling policy\nRecall that minimizing a GPU datacenter's power consumption is part of the broader problem we defined in Equations 5 and 6 in Section II, where minimizing GPU fragmentation is also crucial. Ideally, we aim to combine the benefits of both PWR and FGD in a single scheduling policy. Fortunately, the Kubernetes scheduling framework allows to linearly combine the normalized scores of multiple scoring plugins, effectively combining their behaviors provided that an appropriate coefficient \u03b1 is used. Hence, we use this capability to linearly combine the scores of PWR with those of FGD. In the experimental evaluation (Sections V and VI), we explore different \u03b1 values, identifying those that yield the most effective power- and GPU fragmentation-aware scheduling policies."}, {"title": "V. EXPERIMENTAL SETTING", "content": "We implemented the PWR scheduling policy as a Kubernetes score plugin written in Go. The plugin is integrated within the customized version of Alibaba's event-driven opensimulator used in [19]. Our code and the related documentation is publicly available in a GitHub repository\u00b3. We compare our scheduling policy, PWR, and its combination with FGD, against five heuristic policies that support GPU-sharing and have already been implemented in the simulator:\n1) Fragmentation Gradient Descent (FGD) is the scheduling approach proposed in [19].\n2) Best-fit (BestFit) [6] assigns tasks to the node with the least remaining resources, computed as a weighted sum over all resource dimensions.\n3) Dot-product (DotProd) [4] allocates tasks to the node with the smallest dot-product between the node's available resources and the task's requirements.\n4) GPU Packing (GpuPacking) [18] prioritizes task assignment first to occupied GPUs, then to idle GPUs on active nodes, and lastly to idle nodes, aiming to preserve resources for multi-GPU tasks.\n5) GPU Clustering (GpuClustering) [21] packs tasks with similar GPU requirements together to avoid heterogeneous resource distribution on the same node.\nA. Traces and generation of workloads\nTABLE I: Distribution of tasks in the Default trace.\nGPU Request per Task\n0\n(0, 1)\nTask Population (%)\n13.3\n37.8\nTotal GPU Reqs. (%)\n0\n28.5\nWe consider the 2023 Alibaba GPU trace dataset introduced in [19]. The dataset includes the Default trace, consisting of 8,152 tasks collected from an Alibaba production-grade GPU datacenter without GPU constraints. The distribution of task profiles and GPU requests in the Default trace is outlined in Table I. Additionally, from the dataset we consider other three types of traces, which were derived from Default as follows.\nMulti-GPU traces: the amount of GPU resources requested by tasks that use 1 or more entire GPUs is increased by 20%, 30%, 40%, and 50% compared to the Default trace. This is achieved by increasing the total number of multi-GPU tasks while keeping their internal distribution fixed. The numbers of CPU-only and sharing-GPU tasks remain unchanged.\nSharing-GPU traces: the percentage of GPU resources requested by sharing-GPU tasks is set at 40%, 60%, 80%, and 100% of the total GPU resources requested by GPU tasks. This is done by adjusting the number of sharing-GPU and multi-GPU tasks, while keeping intra-class distributions fixed and maintaining the same percentage of CPU-only tasks.\nConstrained-GPU traces: the percentage of GPU tasks that request specific GPU models is set at 10%, 20%, 25%, and 33%. All other characteristics match those of Default.\nWorkload generation: to evaluate the datacenter's capacity under a given scheduling policy, we use a Monte Carlo workload inflation approach for each trace. Tasks are randomly sampled from the traces with replacement and submitted for scheduling until the cluster reaches full capacity.\nB. The simulated GPU datacenter\nThe trace data in Section V-A was collected from an Alibaba production-grade GPU datacenter with 1213 nodes, 310 of which do not have any GPU. The datacenter includes a total of 107,018 virtual CPUs and 6,212 GPUs. Table II lists the GPU models, along with the per-model number of GPUs, their idle power consumption (in Watt, mapped to $P_{idle}$ in Eq.2), and their Thermal Design Power (TDP) representing the power consumption under maximum theoretical load. TDP, measured in Watt, is mapped to $P_{max}$ in Eq.2. The datacenter includes"}, {"title": "C. Evaluation Metrics", "content": "The metrics considered in the experiments are:\n1) The Estimated Overall Power Consumption (EOPC) of the datacenter accounts for both CPU and GPU power consumption (in Watt). It is modeled as described in Equation 3 from Section II.\n2) GPU Resource Allocation Ratio (GRAR): the ratio between the sum of GPU resources allocated to scheduled tasks and the sum of GPU resources requested by arrived tasks. This measure is a proxy to assess how well each scheduling strategy manages GPU fragmentation (as defined in Equation 4 from Section II) when the datacenter nears saturation. Low ratios indicate many task failures and thus poor task scheduling due to GPU fragmentation. Each batch of experiments is repeated 10 times. For each metric, we report the average value relative to the cumulative GPU resource requests made by tasks arrived to the datacenter."}, {"title": "VI. EXPERIMENTAL EVALUATION", "content": "The experimental evaluation is divided into four parts. First, in Section VI-A, we provide an overview of the power consumption for the simulated GPU datacenter, establishing a baseline for later discussions on power savings. In Section VI-B, we assess the power savings of PWR and its linear combinations with FGD, compared to plain FGD, focusing on minimizing both power consumption and GPU fragmentation. Section VI-C evaluates the power savings achieved by selected PWR and FGD combinations, as well as other competitors from Section V, relative to FGD. Finally, in Section VI-D, we evaluate the GRAR metric to confirm that the power savings are genuine and not due to task scheduling failures when the datacenter is not near saturation. We also show that the GRAR scores of the selected PWR and FGD combinations are close to those of plain FGD.\nA. The FGD EOPC baseline\nThis first batch of experiments has the purpose of setting the stage for subsequent discussions on the power savings achieved by various competitors compared to FGD. To this end, we consider the EOPC achieved by FGD when considering workloads from the Default trace. The plot in Figure 1 presents the results, shown in the form of a stacked plot, where the CPU and GPU components of EOPC are highlighted.\nFrom the figure, we observe that FGD EOPC starts just above 200kW and peaks at about 1.4MW as the simulated GPU datacenter approaches saturation. Additionally, by observing the dashed line associated with the right y-axis in the plot, we note that the estimated GPU power consumption consistently represents between 72% and 76% of the EOPC. Finally, we report that similar figures and trends are observed when estimating EOPC for workloads from the other traces.\nB. Combining PWR with FGD\nThese experiments aim to identify the most effective linear combinations of PWR's and FGD's scores for minimizing both power consumption and GPU fragmentation recall that the Kubernetes scheduling framework allows the linear combination of scores from multiple scoring plugins. Following Equation 5 in Section II, we denote such combinations by a \u00b7 PWR + (1 \u2212 a) \u00b7 FGD, with a \u2208 [0,1]. Note that in all the plots, the values of a and (1 \u2212 a) are shown multiplied by 1000. To evaluate the combinations considered in the experiments, we focus on the two metrics introduced in Section V-C. We use EOPC to assess the power savings achieved by PWR and its combinations with FGD compared to plain FGD. At the same time, GRAR helps us determine whether the power savings are genuine (i.e., not due to task scheduling failures) and identify which combinations minimize power consumption and GPU fragmentation. We report only the results of experiments conducted with workloads generated from the Default trace, as similar behaviors have been observed with the other traces.\nWe begin by examining the power savings achieved by PWR and its linear combinations with FGD compared to"}, {"title": "VII. CONCLUSIONS", "content": "In this paper, we addressed the online scheduling problem in GPU datacenters, reducing power consumption and minimizing GPU fragmentation. We introduced PWR, a power-aware scheduling policy that optimizes power usage by selecting power-efficient GPU and CPU combinations. This is achieved through a simplified power consumption model integrated into a new Kubernetes score plugin. By combining our policy with the Fragmentation Gradient Descent (FGD) one, we achieve a balanced tradeoff between reducing power consumption and minimizing GPU fragmentation. Extensive experiments, conducted using traces with different characteristics from the 2023 Alibaba GPU trace dataset, show that our approach offers substantial power savings compared to FGD alone. Specifically, the power savings vs plain FGD exceed 20% for moderate workloads while maintaining efficient GPU utilization as a datacenter nears saturation. Our approach is a scalable and practical solution for future GPU datacenters, where usage and power efficiency will be both essential. In future work, we plan to refine the power model for greater accuracy, and possibly extend the approach to other resource-intensive environments. Another research direction involves studying under which conditions dynamically adjusting the coefficient \u03b1 can improve power savings and GPU fragmentation. Furthermore, we plan to integrate the notion of target workload into PWR to estimate the expected increase in power consumption when scheduling tasks, to further increase power savings. Finally, we aim to extend our approach to batch scheduling, where optimizing across groups of tasks can further benefit power savings and GPU fragmentation. This extension could also consider factors such as carbon footprint and energy costs into the optimization problem, potentially contributing to a more holistic approach to sustainability in GPU datacenters."}]}