{"title": "On Occlusions in Video Action Detection: Benchmark\nDatasets And Training Recipes", "authors": ["Rajat Modi", "Vibhav Vineet", "Yogesh Singh Rawat"], "abstract": "This paper explores the impact of occlusions in video action detection. We fa-\ncilitate this study by introducing five new benchmark datasets namely O-UCF\nand O-JHMDB consisting of synthetically controlled static/dynamic occlusions,\nOVIS-UCF and OVIS-JHMDB consisting of occlusions with realistic motions and\nReal-OUCF for occlusions in realistic-world scenarios. We formally confirm an\nintuitive expectation: existing models suffer a lot as occlusion severity is increased\nand exhibit different behaviours when occluders are static vs when they are moving.\nWe discover several intriguing phenomenon emerging in neural nets: 1) transform-\ners can naturally outperform CNN models which might have even used occlusion as\na form of data augmentation during training 2) incorporating symbolic-components\nlike capsules to such backbones allows them to bind to occluders never even seen\nduring training and 3) Islands of agreement can emerge in realistic images/videos\nwithout instance-level supervision, distillation or contrastive-based objectives\u00b2(eg.\nvideo-textual training). Such emergent properties allow us to derive simple yet\neffective training recipes which lead to robust occlusion models inductively satis-\nfying the first two stages of the binding mechanism (grouping/segregation). Mod-\nels leveraging these recipes outperform existing video action-detectors under oc-\nclusion by 32.3% on O-UCF, 32.7% on O-JHMDB & 2.6% on Real-OUCF in\nterms of the vMAP metric. The code for this work has been released at https:\n//github.com/rajatmodi62/OccludedActionBenchmark.", "sections": [{"title": "1 Introduction", "content": "Deep learning[41] has led to significant advances in object detection/segmentation for both image[16,\n19] and video domain [84]. Such deep neural networks are in turn widely used in self-driving cars and\nsafety critical scenarios. A key concern for such applications is whether they are able to perform well\nwhen encountering realistic occlusions: e.g. are they able to reliably localize a pedestrian even when\nan occluder (say a dog) comes in front of him. However, one major limitations being existing dataset\ntest split doesn't contain such occlusions. This raises a concern, whether these models will be robust\nto real-world occlusions or not.\nOne would suspect that the inherent inductive biases of these architectures would be enough to induce\nnatural occlusion robustness. To verify the hypothesis, we run two preliminary setups: (Fig. 1)\n(A) Firstly, superimposing a single occluder (eg bus) over the actor, and, (B) Then, we analyze the\nperformance if the occlusion is shifted to background (i.e. no occlusion over actor at all). We observe\na relative drop of 20-50% across multiple existing state-of-the-art approaches [46, 13]. This verifies"}, {"title": "2 Related Work", "content": "Spatio-Temporal Action Detection (STVAD): Involves localizing an actor in each video frame and\nalso predicting an action class . Existing methods[66, 58, 11] adopt a tubelet- based approach: action\ntrajectories can be thought of as a sequence of actor bounding boxes spread over time. These tubes\ncan be predicted by progressively refining predictions over a center-keyframe[46, 77] or directly\nlearning a tubelet as a feature volume[86]. However, these methods require additional post-processing\nby linking these smaller tubelets across time. Other architectures like VideoCapsuleNet [13] frame\naction-detection as a per-pixel localization task.\nOcclusions in Videos: Recently, OVIS dataset[54] was proposed for Video Instance Segmenta-\ntion (VIS) under realistic occlusions. Methods like [76, 32, 3] introduce occlusion robustness by\ntemporally averaging actor-centric embeddings in DETR-like architectures[5]. However, instance-\nrecognition during VIS can be performed by object-detection in a single frame whereas action-\nclassification in STVAD requires more than one frame. Approaches like [44, 70] explore occlusions"}, {"title": "3 The Video Occlusion Datasets", "content": "The aim of this study is to study occlusions in spatio-temporal video action detection. We present\nfive different benchmark datasets to study this problem. Two of them (O-UCF and O-JHMDB)\nare synthetically generated to systematically study this problem by manipulating a single occlusion\nparameter at a time. Similarly, OVIS-UCF, OVIS-JHMDB consist of occluders exhibiting realistic\nmotions. Furthermore, we also present a real-world dataset (Real-OUCF) to validate our findings.\nWe use three different parameters to create our synthetic datasets, 1) Indoor/Outdoor Occluders 2)\nSeverity of Occlusion 3) Static/Dynamic motions of occluders."}, {"title": "3.1 Occluder Selection", "content": "To keep the generated occlusions realistic, we first crop out several objects from the Pascal-VOC[14]\nimages using ground-truth pixel-level annotations. Out of 2413 cropped instances, we shortlist\n900 possible occluders which correspond to commonly-occurring objects. Next, these objects are\ndistributed among two separate categories namely indoor objects (eg, chair, table) and outdoor\nobjects (eg, aeroplane, ship etc.). Finally, we softly blend these occluders over the video pixels using\nRGBD maps of these objects. This blending correctly simulates a very real object (occluder) being\npresent over in the frame, without having to resort to an intensive data collection procedure."}, {"title": "3.2 Severity of Occlusion", "content": "In a video, an actor occupies certain portion of the frame as it moves over time. Formally, we define\nsuch an actor-region (FG) as the tightest bounding box enclosing all the ground truth boxes of the\nactor's trajectory over time. The remaining region is termed as background(BG).\nOcclusion severity is defined as the fraction of the total area which is occupied by an occluder during\nocclusion. We experiment with three severity levels, namely 1 (0-20%), 2 (20-40%), and 3 (40-60%)\nrespectively. Occlusion levels in the actor region are prefixed by FG, i.e. FG1/2/3, whereas occlusions\nin the background are prefixed by BG, i.e. BG 1/2/3. A combination of these actor/background\nseverity levels yields 9 levels of severity."}, {"title": "3.3 Types of Occlusion", "content": "Static Occlusions: Refer to the occlusions where the occluders occupy a fixed position in the frame\nand don't move over time. Our dataset consists of 9 severity combinations of actor/background\nregions as explained in the previous section.\nDynamic Occlusions: Existing occlusion-based datasets are mostly image-based and are therefore\nunable to study the effects of the movement of occluders in the video. For such dynamic cases, we\npick a fixed severity level (i.e. FG2, BG3) and vary the motion of the occluders from a particular\nstarting point. Our test set consists of two types of motions, i.e. circular and sinusoidal. The training\nset consists of linear, zoom-in, zoom-out, or random motions. Note that the motion in the train/test\nset is mutually-exclusive, i.e. one type of motion in one dataset split is not present in another for\nfairness. Similarly, an occluder moving in the actor region never mixes over to the background region\nbut instead gets wrapped around over the course of its trajectory."}, {"title": "3.4 Benchmark datasets", "content": "The statistics for five proposed benchmark datasets are illustrated in Table 1. O-UCF contains\nof 24 action classes, along with 20306 testing samples. Similarly, O-JHMDB contains 21 action\nclases with 928 samples. Both of these datasets consist of static occlusions in 9 levels of severity, 4\ncontrolled occluder trajectories in train set, and 2 trajectories in the test set. Our OVIS-UCF/OVIS-\nJHMDB datasets consist of occluders with realistic motions from OVIS superimposed on top of\noriginal UCF/JHMDB datasets. Note that we term this dataset as semi-realistic because although\nthe occluders themselves are synthetically placed on the frames, occluder trajectories are naturally\noccuring as observed in OVIS dataset. Real-OUCF consists of 1743 fully-realistic occlusion videos\nwhich were hand-picked from Youtube for 24 action classes. These videos were then cropped\ntemporally using LossLessCut[2], to precisely localize start/end time step of each action. Such shorter\nduration clips then need to be spatio-temporally annotated. Therefore, we feed-forward all such\nclips through auto-label generator of GroundedSAM[48] in order to localize \"person\" class with an\nappropriately constructed textual query. Since SAM[38] is a foundational model, it segments all the\npersons in an image. However, we are only concerned with the people who are actually performing\nactions. In order to remove such excessive false positives predicted by GroundedSAM[48], we\nmanually suppress/refine per-frame instance-level masks using the CVAT Annotation Tool[1]. Finally,\nwe end up with 64.1% of annotated instances being occluded."}, {"title": "4 Experiments and Analysis", "content": "Studied models: We experiment on three most recent SOTA methods in video action detection.\nNamely, we pick MOC[46],YOWO[39] and VideoCapsuleNet[13] whose official Github implementa-\ntions and evaluation protocols have been fully open-sourced for both UCF and JHMDB datasets. All"}, {"title": "4.1 Results and Analysis", "content": "We present the result of our benchmark on the baselines and analyse several interesting trends.\nIncreasing occlusion severity over actor region reduces performance. In Tabs2,3, we occlude\nactor region with multiple occluders. It can be clearly observed that as the occlusion severity is\nincreased, the performance of all the action-detectors is reduced. The notable thing is that VCAPS is\nmost robust to severe occlusion (i.e. FG3) (in terms of absolute scores) on the larger O-UCF dataset\n(i.e. 36.9%), and YOWO is most robust to occlusions on smaller JHMDB dataset (i.e. 46.2%). The\nlarger performance difference in MOC and YOWO could be attributed to the treatment of action-\ndetection as a box-regression problem in MOC[46]/YOWO[39] vs dense-semantic mask prediction\nin the VideoCapsuleNet[13]. Regressing the 4 coordinates of a box has a minimum probability of\nerror as $\\frac{1}{4}$ = 0.25, whereas dense-prediction reduces this error to $\\frac{1}{n}$, where n is the no of pixels\nbeing predicted and $n >> 4$. As long as at least four pixels are predicted along the boundaries of the\nbox, the method performs relatively well\u00b3. Also capsules have been shown to be extremely robust to\nocclusions on Multi MNIST[60], and we believe similar inductive bias extends to videos.\nAre backbones with more parameters necessarily more robust? To answer this question, we\ntried CNN/Transformer-based backbones on two of our best performing methods, i.e. YOWO &\nVCAPS. In Table 5, it is evident that Mvitv2-S with 50% less parameters (35M) can outperform\nother 3D backbones like ResNext using large as 89M parameters (i.e. 67.3% on O-UCF & 86.8% on\nO-JHMDB). This shows that the internal attention mechanism in transformers doesn't necessarily\nneed more parameters to improve robustness. Note that VCAPS with Mvitv2-S backbone (67.3,\nO-UCF) largely outperforms YOWO which also uses Mvitv2-S backbone (31.2, O-UCF). A crucial\narchitectural difference is that the VCAPS-Mvitv2 uses 2 additional layers of capsules, whose\nqualitative effects shall be revealed in a later section."}, {"title": "5 Improving Robustness under Occlusion", "content": "Next, we investigate if we can make these models robust against occlusion. Augmentation is a well\nstudied technique to induce robustness in models against distribution shifts. We experimented with\nsynthetic occlusions as augmentations to make the existing models robust. We generate augmented\nsamples by superimposing occluders at random locations, various severity levels and motion. Training\nexisting baselines on these augmented samples allows to improve occlusion robustness (Table 6)."}, {"title": "5.1 Importance of Capsules", "content": "Here, we explore the importance of capsules towards occlusion modelling. In Tab9a, we remove\nall the capsule layers from VCAPS-Mvitv2. Note that the architecture then reduces to a standard\n3D Unet[10], and the performance drops from 67.3% to 64.1%. Therefore, capsules are a crucial\ncomponent for the performance of VCAPS-Mvitv2.\nEmergent Object/Occluder separation in capsules: Next, we qualitatively explore the behaviour\nof capsule-based models towards occlusions. We choose our best-performing VCAPS-MvitV2\nmodel which has never seen occlusions during training and feed-forward an occluded sample during\ninference in Fig5. The activation maps of some of the capsules in the primary layers have been\nvisualized. Each capsule has an activation map of size H\u00d7W, where each value denotes the confidence\nabout the location of a particular entity (object). Applying a threshold of 0.7 results in a binary mask.\nIt can be seen that some of the capsules selectively look at the objects (i.e. actors), and other capsules\nfocus on occluders (eg, chair, sofa). Note that the capsules have never seen occluders during training,\nbut are still able to focus on them. Therefore, architectural constraints like binding[17] object-specific\npixels to particular slots (eg, capsules) [36] allows behaviour like unsupervised object discovery to\nemerge[49]."}, {"title": "5.2 GLOM: Islands of agreement can emerge from token representations.[26]", "content": "We qualitatively explore the effectiveness of our VCAPS-Mvitv2 model presented in Tables7,9b.\nSpecifically, we assume that each of the output token representation from Mvitv2 is equivalent\nto a column-based activity vector [26]. We perform t-sne reduction [64] of each vector to three\ndimensions, and linearly project obtained components to the RGB range of [0, 256) in Fig6. Note that\nobtained maps are not simple patch-based correlations among token-representations[52], but rather\nlower dimensional clustering of vectors themselves plotted as identical colors[81]. Furthermore, our\nVCAPS-Mvitv2 has only received actor-level annotations/ action-label supervision and not any form\nof occluder masks [83], contrastive-textual supervision[68] or distilled knowledge[6] during training.\nThe only used prior is Kinetics pretrained weights learnt from action-recognition.\nThis suggests that the lower layers in our VCAPS-Mvitv2 (i.e. transformer layers) perform semantic\ngrouping of pixels into objects[75] (object discovery). Next, higher layers (capsule layers) selectively\nsegregate objects/occluders into instance-specific slots 5. Thus transformers and capsules complement\neach other to achieve first two stages of the binding process[17]. Finally, our decoder reasons to\nsuppress occluder-specific features and focus/infill on actor-centric regions during localization, i.e.\nFig 7. From this section, we conclude that two capsule layers stacked on top of transformer-based\nbackbones can help improve occlusion-robustness. The neuro-symbolic advantage of capsules"}, {"title": "5.3 Token masking", "content": "Till now, we have seen that architectures possessing a transformer based backbone with a few capsule\nlayers on top are highly robust to occlusions. Now, we propose token-masking to improve their\ninnate robustness. Given a video of dimensions T \u00d7 H \u00d7 W, a transformer creates L spatio-temporal\npatches each of which is projected to a common dimensionality D during input. Our idea is that\nrandomly masking (blacking out) some of the tokens during training introduces additional occlusion\nrobustness in the model. Mathematically, we generate L iid bernoulli random variables\n$M_i \\sim Bernoulli(p) \\forall i \\in [1, L]$\nwhere p is the masking probability. Each Mi is repeated D times, so that a particular token P is fully\nmasked before input. Next, $M \\bigodot P$ is multiplied element wise with the input sequence to give final\nmasked input I' = I $\\bigodot$ M where I,M are input sequence vector and masking vector of dimensions\n$\\mathbb{R}^{L \\times D}$ respectively, and $\\bigodot$ is element-wise hadamard operation. During inference, none of the tokens\nare masked, and the input occluded video sequence is directly feed-forwarded through the network.\nQuantitative Results In Table 7 it can be seen that such token-masking improves even the per-\nformance of models which have used occlusion as a data augmentation from 81.9% to 82.2% on"}, {"title": "6 Conclusion", "content": "We have conducted the first-ever benchmark study to evaluate the impact of occlusions in spatio-\ntemporal action detection. This study provides several interesting key insights including, i) Models\nperform better on static occlusions vs dynamic occlusions in realistic scenarios. ii) Transformer-\nbased models possess greater natural robustness compared to other models using occlusion as data\naugmentation. iii) Pre-training improves robustness of transformers more than CNN models. (iv)\nRobustness can be improved further by leveraging components like capsules and training jointly with\nocclusions as data augmentation. (v) Recipes like simply masking some input tokens of transformers\nduring training can introduce additional robustness and obtain state-of-the-art performance under\nrealistic occlusions. Our benchmark, datasets and code have been released at this link."}, {"title": "Limitations:", "content": "We have only focused on annotating visible regions of the occluded-objects in line with\nofficial COCO protocol[47]. Another direction could also focus on predicting missing regions, i.e.\namodal-segmentation. Classically, semantic-segmentation has assumed that one pixel can belong to\nonly one object[19]. However, this is not true for occlusions. Inductively, this symmetry issue has\nbeen resolved in works like Maskformer2[9] where multiple objects per pixel can be predicted by\nremoving the softmax assumption. We note that there is still a significant gap left to bridge in existing\nmodels for improving robustness to realistic occlusions9b."}, {"title": "8 Appendix", "content": ""}, {"title": "A1. Broader Impact Statement", "content": "A decade ago, [82] showed that successive filters of a convnet could act as general forms of edge,\nshape and texture detectors. In Fig 6, we now illustrate that higher layers of a neural net can learn to\ngroup pixels into objects at a semantic level without any explicit localization supervision/multi-modal\nalignment. This shows that we could learn non-parametric object-queries at the highest levels in the\narchitecture (via unsupervised-clustering), instead of directly injecting learnable parameters/proposals\ninto lower decoder-layers[5]. In nature, there can be a large number of objects present in the retinal\nframe (eg, leaves of a tree). Depending on the granularity of fixation (eg, separation between multiple\ntrees), we might care only about a subset of those classes[47]. However, the objects we don't care\nabout still exist, even though an object query might not bind to them. Therefore, the encoding process\nin a neural net should not prevent separate islands of finer objects (eg, leaves) from getting formed 7.\nThe representational collapse issue observed in Sec5.1 presents a memory-scaling bottleneck. One\nway to get around it is to consider the behaviour of a self-replicating cellular-automaton [71] like\nGLOM[24]: unfolding a singular embedding creates dynamic connectionist hardware on-the-fly[29].\nThis would also allow us to spiritually succeed the computationally-expensive distillation setup, i.e.\nsimply copying a singular embedding lesser number of times on a weaker hardware would allow the\nstudent model to exist. A key question that thus remains unanswered for our community is how to\ncompress the entire knowledge of a neural-net into this singular entity (eg, biological seed) and the\nmechanism behind its unfolding[35](which is an inverse of the protein-folding problem)."}, {"title": "A2. Mortal computation requires self-replicable embeddings", "content": "On the other hand if we want to pack similar levels of intelligence in a brain-like interface which\nconsumes less than 25 watts daily, we have to take an alternate biologically-plausible route[26]:\nintelligence can be encoded in a single cell (embedding) much like how the genetic code of a human\ngets encoded in the DNA. In nature, multiple copies of a zygote (cell) lead to emergence of an animal's\ninternal organs. Similarly, in our computers multiple copies of this single replicable embedding shall\nlead to networks whose structure gets discovered 'on-the-fly' according to local hardware constraints.\nIt has been well established in distillation literature that [27] a higher-parameterized teacher network\ncan teach a lower-parameter student network to obtain similar performance. If all the knowledge of\nthe teacher could be compressed into a singular embedding, then we won't need distillation at all.\nSimply copying the single embedding many times on a weaker student hardware would be enough to\nallow the student model to exist. Although the student model would be weak due to lesser copying,\nthe replication step for both student/teacher stems from a common learnt singular representations.\nThese 'connectionist-networks' [29] which start their existence from a single embedding and only\nremain in the memory as long as the hardware is powered on (hence the name mortal) would require\nlearning algorithms other than backpropogation: where the parameters of each layer could be updated\nwithout knowing the precise feed-forward mathematical-functions[25] and allow dynamic growing\nof the network on a smaller hardware[34]. We remain optimistic for the future that such mortal\ncomputation offers to humanity[25]."}, {"title": "A3. Additional Supplementary Material", "content": "This section discusses the supplementary materials in addition to the main manuscript. The supple-\nmentary material contains seven sections:\n\u2022 \u00a7A4. proposes a new task called action-segmentation involving instance-level localization\nof actor in a video and presents a benchmark to streamline further research in the field.\n\u2022 \u00a7A5. explores the importance of background context in action detection.\n\u2022 \u00a7A6. presents the full benchmark and analyzes the background bias property in existing\ndetectors."}, {"title": "A4. A New Instance Level Benchmark", "content": "Traditionally[46, 86], spatio-temporal action-detection has relied on predicting bounding boxes across\nan actor for each frame. A much harder task instead would be a finer-grained localization, i.e.\npredicting instance-level mask for an actor instead of only bounding boxes. Surprisingly, to the best\nof our knowledge, only one approach i.e. VideoCapsuleNet[13] is able to solve the much harder task\nof instance-level action segmentation by adapting the network trivially out-of-the-box.\nWe argue that research in the field of instance-level action-segmentation has largely been inhibited due\nto the lack of proper instance-level actor annotations for standard action-detection datasets[61]. While\nthe popular JHMDB[33] dataset consists of instance puppet masks, the larger UCF-24 dataset only\nhas bounding box annotations. To rectify this, we release the instance-level annotations for UCF-24\nsome of whose samples have been illustrated in Fig1213. Our insight is that these annotations will\nnow allow the existing research in action-detection and Video Instance Segmentation [72] to progress\nconcurrently due to inherently similar problem formulations 9. Finally, we note that the results of\nVCAPS on instance-level benchmark in Tab10 are significantly lower than on the bounding-box level\nbenchmark in Tab12. This indicates that instance-level localization task is significantly harder task\nthan isolating 'broader level' bounding boxes.\nAnnotation Procedure for UCF-24:We generate instance-level segmentation masks on UCF101-24\nvideos leveraging the recent SOTA in Video Instance segmentation [72]. To make sure that our\ninstance-level action tubes are temporally coherent, we perform inference over successive chunk\nsizes of 100 frames, with a temporal overlap of t = 30 frames. Next, we run a sliding window for\nsmoothening the predicted tube to remove any stray pixel-level artifacts. Finally, we manually refine\nthe obtained segmentation masks using the CVAT tool.[1]"}, {"title": "A5. Preliminary Experiments", "content": "In Figl of the main manuscript, we had run a set of preliminary experiments which served as a\nmotivation for this benchmark study. One notable result has been illustrated in Table 11. Distractor\nrefers to the setting when there is just one single occluder in the background of a video. No Context\nrefers to the setting when all the background pixels of the video have been blackened, thereby\nmaking it easier for the network[46, 13] to classify the remaining pixels as an actor. It can be clearly\nobserved that the highest drops (i.e. dr for No Context case) are observed when the background is\nentirely masked. This shows that existing networks are highly biased to the background information\nwhile trying to reason about an actor location. We note that this dependency on background is\ncounter-intuitive to the desirable behaviours of learning object(actor)-centric representations[5, 86]."}, {"title": "A6. Full Benchmark Analysis", "content": "In Tables 12, 13, we present the full benchmark of the proposed O-UCF and O-JHMDB datasets\nacross the 9 severity levels of static occlusions and circular/sinusoidal dynamic motions."}, {"title": "A7. Motion Analysis", "content": "In the main manuscript, we had experimented with realistic motion on the OVIS-dataset. We had also\nexperimented with synthetic motions on O-UCF and O-JHMDB datasets, and show those results in\nTable 14. We observe that in case of MOC using the 2D DLA34 backbone, the performance is better\nin case of static occlusions, (i.e. 28.7 vs 27.3 on O-UCF). This shows that 2D backbones work well\nif occluder is static. On the other hand, YOWO uses a combination of 2D backbone (YOLO) and\na 3D backbone (ResNext). This shows a slight positive improvement of 1.2% in UCF and 0.6% in\nJHMDB, thereby indicating that 3D backbone seems to be helping. The best improvement is evident\nby purely switching to a 3D based backbone as in the case of VCAPS, where gains as much as 4.1%\ncan be observed. From this, it can be seen that 3D backbones might help networks reason about\nactor locations even under the challenging scenarios when the occluders might be moving. However,\nwe note that these differences are close within a small margin of error. Also, our experiments with\ndifferent type of trajectories in Tab1213, i.e. circle and sinusoids show inconsistent- trends thereby\nindicating that there is no particular motion type to which networks are more sensitive."}, {"title": "A8. Real-OUCF dataset", "content": "We have curated Real-OUCF dataset for realistic occlusion scenarios.\nVideo Selection Process: The original videos in UCF-24 dataset were mostly of sporting events, but\nwere of significantly lower resolution, i.e. 240 by 320. Furthermore, they consisted of only a single\nactor. Now, we have significantly upgraded that test set to reflect multiple actors which mutually\nocclude each other with a high degree of overlap as high as 99%. First, we scraped the videos\nmatching keywords like \"riding bike\" etc. One curious way we were able to get such overlap ratios"}, {"title": "A9. Dataset Samples", "content": "In this section, we show some qualitative samples from our proposed-datasets and benchmarks.\nSpecifically, Fig1011 shows the realistic occlusion dataset which we have collected for evaluating\nrobustness to real-world occlusions. Next, Fig1415contains the synthetic dataset samples from\nO-UCF and O-JHMDB consisting of controlled occlusions. Fig1213 contains the exhaustive instance-\nlevel annotations of UCF-24 videos we have released to facilitate our proposed benchmark of\naction-segmentation."}, {"title": "A10. Datasheets For Datasets", "content": "Motivation\nFor what purpose was the dataset created? Was there a specific task in mind? Was\nthere a specific gap that needed to be filled? Please provide a description.\nO-UCF, O-JHMDB were created to perform systematic benchmark study of occlusions in video\naction detection. Real-OUCF was created to evaluate sota action-detectors on real-world occlusions.\nWho created this dataset (e.g., which team, research group) and on behalf of which\nentity (e.g., company, institution, organization)?\nThese were created by a research-group whose identity shall be released after the review process.\nComposition\nWhat do the instances that comprise the dataset represent (e.g., documents, photos,\npeople, countries)? Are there multiple types of instances (e.g., movies, users, and ratings;\npeople and interactions between them; nodes and edges)? Please provide a description.\nDataset contains videos of several people performing actions.\nIs there a label or target associated with each instance? If so, please provide a\ndescription.\nFor each instance, we provide an instance-level segmentation mask along with our datasets.\nAre there recommended data splits (e.g., training, development/validation, testing)? If\nso, please provide a description of these splits, explaining the rationale behind them.\nThe idea for a real-world occlusion dataset is that it is easy to train an existing network using\nsynthetic occlusions as a data augmentation. However, there is a need of consistent real-world tet\nset to benchmark all the future approaches against, and that is the test set which our Real-OUCF\nprovides.\nAre there any errors, sources of noise, or redundancies in the dataset? If so, please\nprovide a description.\nA very detailed care has been taken to make our annotations noise free. In the rare case that a\ncorrection is needed, the updated annotations will be posted on our official link (Shared on top of this\nmanuscript).\nIs the dataset self-contained, or does it link to or otherwise rely on external resources\n(e.g., websites, tweets, other datasets)? If it links to or relies on external resources, a)\nare there guarantees that they will exist, and remain constant, over time; b) are there official\narchival versions of the complete dataset (i.e., including the external resources as they\nexisted at the time the dataset was created); c) are there any restrictions (e.g., licenses,\nfees) associated with any of the external resources that might apply to a future user? Please\nprovide descriptions of all external resources and any restrictions associated with them, as\nwell as links or other access points, as appropriate.\nAll the videos of our dataset were obtained after hand picking from YOUTUBE, and then scraping\nthem. Due to offline availability of the dataset, our dataset is now self-contained.\nDoes the dataset relate to people? If not, you may skip the remaining questions in this\nsection.\nYes, it is spatio-temporal action detection. It primarily considers human actors."}]}