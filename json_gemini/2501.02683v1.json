{"title": "From Superficial Patterns to Semantic Understanding: Fine-Tuning Language Models on Contrast Sets", "authors": ["Daniel Petrov"], "abstract": "Large scale pretrained language models have demonstrated high performance on standard datasets for natural language inference (NLI) tasks. Unfortunately, these evaluations can be misleading, as although the models can perform well on in-distribution data, they perform poorly on out-of-distribution test sets, such as contrast sets. Contrast sets consist of perturbed instances of data that have very minor, but meaningful, changes to the input that alter the gold label, revealing how models can learn superficial patterns in the training data rather than learning more sophisticated language nuances. As an example, the ELECTRA-small language model achieves nearly 90% accuracy on an SNLI dataset but drops to 75% when tested on an out-of-distribution contrast set. The research performed in this study explores how a language models' robustness can be improved by exposing it to small amounts of more complex contrast sets during training to help it better learn language patterns. With this approach, the model regains performance and achieves nearly 90% accuracy on contrast sets, highlighting the importance of diverse and challenging training data.", "sections": [{"title": "1 Introduction", "content": "This paper focuses on investigating and reducing biases in language models in Natural Language Inference (NLI) tasks. In NLI, the target is to determine the relationship between a premise and a hypothesis and classify whether the premise entails, contradicts, or is neutral compared to the hypothesis. Being able to correctly understand such relationships is fundamental in understanding and building upon natural language. Modern state-of-the-art deep learning models, particularly transformer based-architectures, have scored high on such tasks, often times even higher than humans. However, recent research has raised the question are these models being evaluated properly? Standard test sets primarily evaluate in-distribution generalization, and if these datasets have systematic gaps (e.g., annotation artifacts), then the evaluations are misleading as the model may simply be learning decision rules that may perform well on the test set and are not really creating an understanding of language nuances (Gardner et al., 2020). In other words, the model may be learning spurious correlations or superficial patterns in the training data that can be carried over into the test set, ultimately inflating accuracy values. Such reliance on shallow heuristics limits the broader applicability of these models, especially in real-world, out-of-distribution scenarios.\nA more rigorous evaluation approach is to use contrast sets a collection of minimally perturbed data examples built from the training set. The perturbations are created in a way that alter the correct label of the original example without introducing significant syntactical changes to the input data. The goal of such a set is to evaluate the robustness and generalization capability of the model and test if it can handle slight variations in the input and still produce correct results. As expected, when testing the trained model on a contrast set, the accuracy drops to 74.9%, showing that the SNLI test set alone is not sufficient to measure performance. To enhance the model's generalization capabil-"}, {"title": "2 Model & Dataset", "content": "ities, subsets of the contrast dataset were used to fine tune the pre-trained model which was then evaluated on the remaining portion of the contrast set. By exposing the model to more complex examples, the model becomes more flexible. The results are promising, increasing accuracy to 90.7%.\nThis analysis highlights how critical the training data diversity is in developing models that can not only excel on standard benchmarks, but also demonstrate a robust understanding of nuanced language contexts. By using contrast sets, we highlight a path toward building models that are both high-performing and resilient when faced with complexity and variability."}, {"title": "2.1 Model", "content": "This paper trains the ELECTRA-small model, a popular model with 14M parameters that is widely used to pre-train transform networks designed specifically for natural language processing tasks (Clark et al., 2020). It is computationally efficient, and also much smaller than its base and large counterparts (110M and 335M parameters, respectively). The model was trained on a custom machine with an NVIDIA GeForce RTX 4070 GPU."}, {"title": "2.2 Training and Testing Data", "content": "The popular Standford Natural Language Inference (SNLI) corpus was used for training and initial testing (Bowman et al., 2015), which is a large collection of 570k human-written English sentence pairs that were manually labeled entailment, contradiction, or neutral.\nEach example in SNLI comes with a premise sentence, a hypothesis sentence, and a gold label (entailment, contradiction, or neutral). Examples of inputs can be seen in Table 1. The goal of the model is to determine whether the premise entails the hypothesis or contradicts it. In some cases, it may also be neutral. The data was split with 550k examples being used for training and 10k used for testing."}, {"title": "2.3 Contrast Set", "content": "The contrast set was automatically generated using Linguistically-Informed Transformations (LIT) (Li et al., 2020) where labels y \u2260 y'. A total of 14,363 contrastive examples were generated where 20% of those examples were used to fine tune the existing model and the remaining 80% was used for validation testing. The contrast set contains perturbed examples from the original SNLI data that changes the hypothesis in a way to change the resulting entailment label. Examples can be including antonyms, adding negations, synonyms, semantic changes, etc, and can be seen in Figure 1."}, {"title": "3 Method", "content": "The ELECTRA-small model was trained for NLI tasks on the SNLI dataset. During validation of the model on the SNLI test set, it scored a respectable accuracy of 89.8%. However, after performing the same evaluation on the contrast set, the accuracy dropped significantly by 14.9 points to a total of 74.9%.\nIt is important to understand the performance discrepancy and why the model misclassifies examples much more often on the contrast set vs. on the original SNLI data, particularly finding which language structures the model finds difficult. A comprehensive error analysis was performed where 20 incorrectly classified examples were studied and grouped into different categories. The following error categories were used, taken from prior research (Naik et al., 2018).\n1. Word Overlap \u2013 High lexical overlap between the premise and hypothesis could trick the model into predicting entailment even though they are unrelated or contradictory.\n2. Negation \u2013 Words such as \u201cno\u201d or \u201cnot\" in the hypothesis could cause the model to incorrectly predict contradiction in cases where it is actually neutral or entailed.\n3. Length Mismatch \u2013 The premise contains more words than the hypothesis which could confuse the model and create distractions, leading to incorrect classification.\n4. Ambiguity \u2013 Instances where the correct answer is unclear, even to the annotators.\n5. Unknown It is unclear what caused the model to misclassify."}, {"title": "4 Model Improvement", "content": "After training the model on the original SNLI dataset and evaluating on both the test contrast sets, the objective was to enhance the model's robustness and generalization capabilities in order to achieve better results on the contrast set while also getting better or similar results on the original SNLI data. These improvements are essential for addressing limitations in language models that rely on patterns in training data as opposed to developing a deeper understanding of language.\nThe proposed method is to go through a fine tuning process where the pre-trained model continues to be trained using a small set of examples taken from the contrast set. The aim was to expose the model to more complex and nuanced language scenarios, allowing it to better learn semantic relationships and context-dependent reasoning. During training, the accuracy on the contrast set was calculated at regular intervals, as shown in Figure 2. We can see an exponential boost in performance when increasing the number of contrast examples used in training. At approximately 1500 examples used, the accuracy level begins to level off, showing how that even using as little as 10% of the contrast set during training can lead to tremendous improvements in performance.\nWhen evaluating the fine-tuned model, the number of errors related to superficial patterns in the data (i.e. word overlap, negation) reduces significantly, as shown in Table 3. This underscores the initial observation that the model was primarily relying on patterns in the data rather than its genuine understanding of the language. The errors the model faced after tuning are more related to ambiguity, which is more of an issue with the annotated contrast set rather than with the model itself."}, {"title": "5 Conclusion", "content": "This paper gives an analysis on how to evaluate language models and highlights the importance of addressing biases and why contrast sets are difficult for models to handle. The ELECTRA-small model initially showed strong performance on in-distribution data, but then significantly dropped on the contrast set, highlighting its susceptibility to spurious correlations in the training data.\nFine-tuning the pre-trained model with a subset of contrast set examples showed significant gains in accuracy on more complex test sets. The improvement suggests that training sets with diverse examples can enhance a model's ability to handle more nuanced language examples, ultimately reducing its bias and increasing its generalization abilities."}, {"title": "Limitations", "content": "This method assumes the contrast set contains a variety linguistic examples with a balanced label distribution. A skewed dataset could result in the model simply predicting the majority label, regardless of context. Nevertheless, this paper highlights the proof of concept of training on complex data examples to ensure models are robust and able to generalize well."}]}