{"title": "Zero-shot Concept Bottleneck Models", "authors": ["Shin'ya Yamaguchi", "Kosuke Nishida", "Daiki Chijiwa", "Yasutoshi Ida"], "abstract": "Concept bottleneck models (CBMs) are inherently interpretable and intervenable neural network models, which explain their final label prediction by the intermediate prediction of high-level semantic concepts. However, they require target task training to learn input-to-concept and concept-to-label mappings, incurring target dataset collections and training resources. In this paper, we present zero-shot concept bottleneck models (Z-CBMs), which predict concepts and labels in a fully zero-shot manner without training neural networks. Z-CBMs utilize a large-scale concept bank, which is composed of millions of vocabulary extracted from the web, to describe arbitrary input in various domains. For the input-to-concept mapping, we introduce concept retrieval, which dynamically finds input-related concepts by the cross-modal search on the concept bank. In the concept-to-label inference, we apply concept regression to select essential concepts from the retrieved concepts by sparse linear regression. Through extensive experiments, we confirm that our Z-CBMs provide interpretable and intervenable concepts without any additional training. Code will be available at https://github.com/yshinya6/zcbm.", "sections": [{"title": "1. Introduction", "content": "One of the primary interests of the deep learning research community is developing a human-interpretable model. Concept bottleneck model (CBM, Koh et al. (2020)) is an inherently interpretable neural network model, which aims to explain their final prediction via the intermediate concept predictions. CBMs are trained on a target task to learn the input-to-concept and concept-to-label mappings in an end-to-end manner. A concept is composed of high-level semantic vocabulary for describing objects of interest in input data. For instance, CBMs can predict the final label \u201capple\u201d from the linear combination of the concepts \u201cred sphere,\u201d \u201cgreen leaf,\u201d and \u201cglossy surface.\u201d These intermediate concept predictions not only provide interpretability but also intervenability in the final prediction by editing the predicted concepts.\nIn the original CBMs (Koh et al., 2020), a concept set for each class label is defined by manual annotations, incurring massive labeling costs greater than ones of the class labels. To reduce the costs, Oikarinen et al. (2023) and Yuksekgonul et al. (2023) automatically generate the concept sets by large language models (LLMs, e.g., GPT-3 (Brown et al., 2020a)) and use the multi-modal embedding space of vision-language models (VLMs, e.g., CLIP (Radford et al., 2021)) to learn the input-to-concept mapping through similarities in the multi-modal feature space. Although modern CBMs are free from manual pre-defined concepts, we argue that the practicality is still restricted by the requirements of training input-to-concept and concept-to-label mappings on target datasets. In other words, CBMs are not available without manually collecting target datasets and additional training of model parameters on them.\nTo overcome this limitation, this paper tackles a new problem setting of CBMs in a zero-shot manner for target tasks. In this setting, we can access pre-trained VLMs, but we cannot know the concepts composing target data in advance. This setting forces models to perform two-stage zero-shot inference of input-to-concept and concept-to-label for unseen input samples. The zero-shot input-to-concept inference can not be solved by a na\u00efve application of VLMs as the ordinary zero-shot classification of input-to-label because it is required to infer a subset of relevant concepts, not a single label, from the large set of all concepts. Furthermore, the zero-shot concept-to-label inference is difficult because the concept-to-label mapping is not obvious without target data and training, which are unavailable in this setting. Therefore, we aim to answer the following research question: how can we provide interpretable and intervenable concepts by the zero-shot input-to-concept/concept-to-label inference without target datasets and training?\nWe present a novel CBM class called zero-shot concept bottleneck models (Z-CBMs). Z-CBMs are zero-shot interpretable models that employ off-the-shelf pre-trained VLMs"}, {"title": "2. Related Work", "content": "CBMs (Koh et al., 2020) are inherently interpretable deep neural network models that predict concept labels and then predict final class labels from the predicted concepts. In contrast to the other explanation styles such as post-hoc attribution heatmaps (Lundberg & Lee, 2017; Selvaraju et al., 2017; Sundararajan et al., 2017), CBMS provide semantic ingredients consisting the final label prediction through the bilevel prediction of input-to-concept and concept-to-label. The original CBMs have the challenge of requiring human annotations of concept labels, which are more difficult to obtain than target task labels. Another challenge is the performance degradation from backbone black-box models (Zarlenga et al., 2022; Moayeri et al., 2023; Xu et al., 2024) due to the difficulty of learning long-tailed concept distributions (Ramaswamy et al., 2023). Post-hoc CBMs (Yuksekgonul et al., 2023), Label-free CBMs (Oikarinen et al., 2023), and LaBo (Yang et al., 2023) addressed these challenges by automatically collecting concepts corresponding to target task labels by querying LLMs (e.g., GPT-3 (Brown et al., 2020b)) and leveraging multi-modal feature spaces of pre-trained VLMs (e.g., CLIP (Radford et al., 2021)) for learning the input-to-concept mapping. Subsequently, the successor works have basically assumed the use of LLMs or VLMs, further advancing CBMs (Panousis et al., 2023; Rao et al., 2024b; Tan et al., 2024; Srivastava et al., 2024). In particular, Panousis et al. (2023) and Rao et al. (2024a) are related to our work in terms of using sparse modeling to select concepts for input images. However, all of these existing CBMs still require training specialized neural networks on target datasets, incurring additional target data"}, {"title": "3. Zero-shot Concept Bottleneck Models", "content": "In this section, we formalize the framework of Z-CBMS, which perform a zero-shot inference of input-to-concept and concept-to-label without target datasets and additional training (Fig. 1). Z-CBMs are composed of concept retrieval and concept regression. Concept retrieval finds a set of the most input-related concept candidates from millions of concepts by querying an input image feature with a semantic similarity search (Fig. 2a). Concept regression estimates the importance scores of the concept candidates by sparse linear regression to reconstruct the input feature (Fig. 2b). Finally, Z-CBMs provide the final label predicted by the reconstructed vector and concept explanations with importance scores."}, {"title": "3.1. Problem Setting", "content": "We inherit the problem setting of existing vision-language-based CBMs (Oikarinen et al., 2023) except for not updating any neural network parameters. The goal is to predict the final task label \\(y \\in Y\\) of input \\(x \\in X\\) based on K interpretable textual concepts \\({c_i \\in C}_{i=1}^K\\), where \\(X, Y, C\\), and T are the input, label, concept, and text space, respectively. To this end, we predict the final task label by the bi-level prediction \\(h \\circ g(x)\\), where \\(g : X \\rightarrow C^K\\) is a concept predictor and \\(h : C^K \\rightarrow Y\\) is a label predictor. This setting allows to access a vision encoder \\(f_V : X \\rightarrow \\mathbb{R}^d\\) and a text encoder \\(f_T : T \\rightarrow \\mathbb{R}^d\\) provided by a VLM like CLIP (Radford et al., 2021), and a concept bank \\(C = \\{c_i\\}_{i=1}^{|C|}\\). The concept bank C is composed of unique concepts from arbitrary sources, including manually collected concepts and automatically generated concepts by LLMs like GPT-3 (Brown et al., 2020a)."}, {"title": "3.2. Zero-shot Inference", "content": "Concept Retrieval. We first find the most semantically closed concept candidates to input images from the large spaces in a concept bank (Fig. 2a). Given an input x, we retrieve the set of K concept candidates \\(C_x \\subset C\\) by using image and text encoders of pre-trained VLMs \\(f_V\\) and \\(f_T\\) as\n\\[C_x = \\text{Ret}_K(f_V(x), f_T(c)) = \\text{Top-}K \\text{ Sim}(f_V(x), f_T(c)),\\]\nwhere Top-K is an operator yielding top-K concepts in \\(C\\) from a list sorted in descending order according to a similarity metric Sim. Throughout this paper, we use cosine similarity as Sim by following (Conti et al., 2023). Thanks to the scalability of the similarity search algorithm (Johnson et al., 2019; Douze et al., 2024), Eq. (1) can efficiently find the concept candidates in an arbitrary concept bank C, which contains millions of concepts to describe inputs in various domains.\nConcept Regression. Given a concept candidate set \\(C_x = \\{c_1, ..., c_K\\}\\), we predict the final label \\(\\hat{y}\\) by selecting essential concepts from \\(C_x\\). Conventional CBMs infer the mapping between \\(C_x\\) and \\(\\hat{y}\\) by training neural regression parameters on target tasks, which incurs the requirements of target dataset collections and additional training costs. Instead, we solve this task with a different approach leveraging the zero-shot performance of VLMs. As shown in the previous studies (Radford et al., 2021; Jia et al., 2021), VLMs can be applied to zero-shot classification by inferring a label \\(\\hat{y}\\) by matching input x and a class name text \\(t_y \\in T\\) in the multi-modal feature spaces as follows.\n\\[\\hat{y} = \\arg \\max_{y \\in Y} \\text{Sim}(f_V(x), f_T(t_y)).\\]\nIf the feature vector \\(f_V(x)\\) can be approximated by \\(C_x\\), we can achieve the zero-shot performance of black-box features by interpretable concept features. Based on this idea, we approximate \\(f_V(x)\\) by the weighted sum of the concept features \\(F_{C_x} = [f_T(c_1), ..., f_T(c_K)] \\in \\mathbb{R}^{d \\times K}\\) with an importance weight \\(W \\in \\mathbb{R}^K\\). To obtain W, we solve the linear regression problem defined by\n\\[\\min_{W} ||f_V(x) - F_{C_x}W||_2^2 + \\lambda ||W||_1.\\]\nThrough this objective, we can achieve W not only for approximating image features but also for effectively estimating the contribution of each concept to the label prediction owing to the sparse regularization \\(||W||_1\\). Since \\(C_x\\) is retrieved from large-scale concept bank C, it often contains noisy concepts that are similar to each other, undermining interpretability due to semantic duplication. In this sense, the sparse regularization enhances interpretability since it can eliminate unimportant concepts for the label prediction (Hastie et al., 2015).\nFinal Label Prediction. Finally, we compute the output label with \\(F_{C_x}\\) and W in the same fashion as the zero-shot classification by Eq. (2), i.e.,\n\\[\\hat{y} = \\arg \\max_{y \\in Y} \\text{Sim}(F_{C_x}W, f_T(t_y)).\\]"}, {"title": "4. Implementation", "content": "In this section, we present the detailed implementations of Z-CBMs, including backbone VLMs, concept bank construction, concept retrieval, and concept regression.\nVision-Language Models. Z-CBMs allow to leverage arbitrary pre-trained VLMs for \\(f_V\\) and \\(f_T\\). We basically use the official implementation of OpenAI CLIP (Radford et al., 2021) and the publicly available pre-trained weights. Specifically, by default, we use ViT-B/32 as \\(f_V\\) and the base transformer with 63M parameters as \\(f_T\\) by following the original CLIP. In Section 5.6.1, we show that other VLM backbones (e.g., SigLIP (Zhai et al., 2023) and Open-CLIP (Cherti et al., 2023)) are also available for Z-CBMs.\nConcept Bank Construction. Here, we introduce the construction protocols of the concept bank C of Z-CBMs. Since Z-CBMs can not know concepts of input image features in advance, a concept bank should contain sufficient vocabulary to describe the various domain inputs. To this end, we extract concepts from multiple image caption datasets and integrate them into a single concept bank. Specifically, we automatically collect concepts as noun phrases by parsing each sentence in the caption datasets including Flickr-30K (Young et al., 2014), CC-3M (Sharma et al., 2018), CC-12M (Changpinyo et al., 2021), and YFCC-15M (Thomee et al., 2016); we use the parser implemented in nltk (Bird, 2006). At this time, the concept set size is \\(|C| \\approx 20\\text{M}\\). Then, we filter out nonessential concepts from the large base concept set according to policies based on Oikarinen et al. (2023); please see Appendix A. Finally, after filtering concepts, we obtain the concept bank containing \\(|C| \\approx 5\\text{M}\\) concepts. We also discuss the effect of varying caption datasets used for collecting concepts in Sec. 5.5 and 5.6.2.\nSimilarity Search in Concept Retrieval. Concept retrieval searches the concept candidates from input feature vectors. To this end, we implement the concept search component by the open source library of Faiss (Johnson et al., 2019; Douze et al., 2024). First, we create a search index based on the text feature vectors of all concepts in a concept bank C using \\(f_T\\). At inference time, we retrieve the concept vectors via similarity search on the concept index by specifying the concept number K. We set K = 2048 as the default value and empirically show the effect of K in Appendix C.4.\nSparse Linear Regression in Concept Regression. In concept regression, we can use arbitrary sparse linear regression algorithms, including lasso (Tibshirani, 1996), elastic net (Zou & Hastie, 2005), and sparsity-constrained optimization like hard thresholding pursuit (Yuan et al., 2014). The efficient implementations of these algorithms are publicly available on the sklearn (Pedregosa et al., 2011) and skscope (Wang et al., 2024) libraries. The choice of sparse linear regression algorithm depends on the use cases."}, {"title": "5. Experiments", "content": "We evaluate Z-CBMs on multiple visual classification datasets and pre-training VLMs. We conduct experiments on two scenarios: zero-shot and training head; the former uses pre-trained VLMs for inference without any training, while the latter learns only the classification heads."}, {"title": "5.1. Settings", "content": "Datasets. We used 12 classification datasets of various image domains: Aircraft (Air) (Maji et al., 2013), Bird (Welinder et al., 2010), Caltech-101 (Cal) (Fei-Fei et al., 2004), Car (Krause et al., 2013), DTD (Cimpoi et al., 2014), EuroSAT (Euro) (Helber et al., 2019), Flower (Flo) (Nilsback & Zisserman, 2008), Food (Bossard et al., 2014), ImageNet (IN) (Russakovsky et al., 2015), Pet (Parkhi et al., 2012), SUN397 (Xiao et al., 2010), and UCF-101 (Soomro, 2012). They are often used to evaluate the zero-shot generalization performance of VLMs (Radford et al., 2021; Zhou et al., 2022). In the training head scenario, we randomly split a training dataset into 9: 1 and used the former as the training set and the latter as the validation set. For ImageNet, we set the split ratio 99: 1.\nZero-shot Baselines. Since no zero-shot baselines of CBMs exist, we compare our Z-CBMs with the zero-shot inference of a black-box VLM and variants of Z-CBMs in terms of regression algorithms and concept banks. For more details, please see Appendix B.\nTraining Head Baselines. To compare Z-CBMs with existing vision-language-based CBMs, we evaluated models in a relaxed setting where the models are trained on target datasets. In this setting, we applied Z-CBMs to linear probing of VLMs, i.e., fine-tuning only a linear head layer on the feature extractors of VLMs; we refer to this pattern LP-Z-CBM. As the baselines, we used Lable-free CBM (Oikarinen et al., 2023), LaBo (Yang et al., 2023), and CDM (Panousis et al., 2023). We performed these methods based on their publicly available code repositories.\nEvaluation Metrics. For evaluating predicted concepts, we measured CLIP-Score (Radford et al., 2021; Hessel et al., 2021), which is the cosine similarity between image and text embeddings on CLIP, i.e., higher is better. CLIP-Score"}, {"title": "5.2. Quantitative Evaluation of Predicted Concepts", "content": "We first quantitatively evaluate the predicted concepts of Z-CBMs from the perspective of their factuality to represent image features. We measured averaged CLIP-Score and concept coverage across the 12 datasets.\nOn the other hand"}, {"title": "5.3. Evaluation of Human Intervention", "content": "Human intervention in the output concept is an essential feature shared by the CBM family for debugging models and modifying the output concepts to make the final prediction accurate. Here, we evaluate the reliability of Z-CBMs through two types of intervention: (i) concept deletion and (ii) concept insertion."}, {"title": "5.4. Qualitative Evaluation of Predicted Concepts", "content": "We demonstrate the qualitative evaluation of predicted concepts by Label-free CBMs and Z-CBMs when inputting the ImageNet validation examples in Fig. 5; we also show the results of Z-CBMs using linear regression to compute the importance coefficients instead of lasso. Overall, Z-CBMs tend to accurately predict realistic and dominant concepts that appear in input images even though they are not trained on target tasks.\nFor the comparison"}, {"title": "5.5. Zero-shot Image Classification Performance", "content": "Table 3 summarizes the averaged top-1 accuracy across the 12 image classification datasets. It also shows the ablation"}, {"title": "5.6. Detailed Analysis", "content": "We show the impacts on Z-CBMs when varying backbone VLMs. Since vision-language models are being intensively"}, {"title": "5.6.3. Effects of Concept Regressor", "content": "Z-CBMs allow users to choose arbitrary sparse linear regression algorithms according to their demands, as discussed in Sec. 4. Here, we compare the performance of Z-CBMs with multiple sparse linear regression algorithms: lasso (Tibshirani, 1996), elastic net (Zou & Hastie, 2005), and sparsity-constrained optimization with HTP (Yuan et al., 2014). Fur- therm"}, {"title": "6. Conclusion", "content": "In this paper, we presented zero-shot CBMs (Z-CBMs), which predict input-to-concept and concept-to-label mappings in a fully zero-shot manner. To this end, Z-CBMs first search input-related concept candidates by concept retrieval, which leverages pre-trained VLMs and a large-scale concept bank containing millions of concepts to explain outputs for unseen input images in various domains. For the concept-to-label inference, concept regression estimates the importance of concepts by solving the sparse linear regression approximating the input image features with linear combinations of selected concepts. Our extensive experiments show that Z-CBMs can provide interpretable and intervenable concepts comparable to conventional CBMs that require training. Since Z-CBMs can be built on any off-the-shelf VLMs, we believe that it will be a good baseline for zero-shot interpretable models based on VLMs in future research."}, {"title": "Impact Statements", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Details of Concept Filtering", "content": "We basically follow the policies introduced by (Oikarinen et al., 2023), which removes (i) too long concepts, (ii) too similar concepts to each other, and (iii) too similar concepts to target class names (optional). However, the second policy is computationally intractable because it requires the O(|C|2) computation of the similarity matrix across all concepts. Thus, we approximate this using a similarity search by Eq. (1) that yields the most similar concepts. We retrieve the top 64 concepts from a concept and remove them according to the original policy."}, {"title": "B. Details of Settings", "content": "Zero-shot Baselines. For the black-box baseline, according to the previous work (Radford et al., 2021), we construct a class name prompt ty by the scheme of \"a photo of [class name]\", and make VLMs predict a target label \\(\\hat{y}\\)\nby Eq. (2). ConSe is a zero-shot cross-modal classification method that infers a target label from a semantic embedding composed of the weighted sum of concepts of the single predicted ImageNet label. For Z-CBMs, we selected \\(\\lambda\\) by searching from \\({1.0 \\times 10^{-2}, 1.0 \\times 10^{-3}, 1.0 \\times 10^{-4}, 1.0 \\times 10^{-5}, 1.0 \\times 10^{-6}, 1.0 \\times 10^{-7}, 1.0 \\times 10^{-8}}\\)\n to choose the minimum value achieving over 10% non-zero concept ration when using K = 2048 on the subset of ImageNet training set. We used the same \\(\\lambda\\) for all experiments. To make"}, {"title": "C. Additional Experiments", "content": "Table 7, 8, and 9 shows all of the results on the 12 datasets omitted in Table 1, 2, and 3, respectively."}, {"title": "C.2. Analysis on Modality Gap", "content": "In Section 5.5, Table 3 shows that Z-CBMs improved the zero-shot CLIP baselines. We hypothesize that the reason is reducing the modality gap (Liang et al., 2022) between image and text features by the weighted sum of concept features to approximate fv (x) by Eq. 3. To confirm this, we conduct a deeper analysis of the effects of Z-CBMs on the modality gap"}, {"title": "C.3. Effects of A", "content": "Here, we discuss the effects when changing \\(\\lambda\\) in Eq. (3). We varied \\(\\lambda\\) in \\({1.0 \\times 10^{-2},1.0 \\times 10^{-3}, 1.0 \\times 10^{-4}, 1.0 \\times 10^{-5}, 1.0 \\times 10^{-6}, 1.0 \\times 10^{-7}, 1.0 \\times 10^{-8}}\\). Figure 7 plots the accuracy and the sparsity of predicted concepts on ImageNet. Using different lambda varies the sparsity and accuracy. Therefore, selecting appropriate \\(\\lambda\\) is important for achieving both high sparsity and high accuracy."}, {"title": "C.4. Effects of K in Concept Retrieval", "content": "As discussed in Sec. 4, the retrieved concept number K in concept retrieval controls the trade-off between the accuracy and inference time. We assess the effects of K by varying it in [128, 256, 512, 1024, 2048] and measuring the top-1 accuracy and averaged inference time for processing an image. Note that we set 2048 as the maximum value of K because it is the upper bound in the GPU implementation of Faiss (Johnson et al., 2019). Figure 8 illustrates the relationship between the accuracy and total inference time. As expected, the size of K produces a trade-off between accuracy and inference time. Even so, the increase in inference time with increasing K is not explosive and is sufficiently practical since the inferences can be completed in around 55 milliseconds per sample. The detailed breakdowns of total inference time when K = 2048 were 0.11 for extracting image features, 5.35 for concept retrieval, and 49.23 for concept regression, indicating that the computation time of concept regression is dominant for the total. In future work, we explore speeding up methods for Z-CBMs to be competitive with the existing CBMs baseline that require training (e.g., Label-free CBMs, which infer a sample in 3.30 milliseconds).\nEthics Statement. A potential ethical risk of our proposed method is the possibility that biased vocabulary contained in the concept bank may be output as explanations. Since the concept bank is automatically generated from the caption dataset, it should be properly pre-processed using a filtering tool such as Detoxify (Hanu & Unitary team, 2020) if the data source can be biased."}]}