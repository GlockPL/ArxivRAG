{"title": "Vision-Language Models Can Self-Improve Reasoning via Reflection", "authors": ["Kanzhi Cheng", "Yantao Li", "Fangzhi Xu", "Jianbing Zhang", "Hao Zhou", "Yang Liu"], "abstract": "Chain-of-thought (CoT) has proven to improve the reasoning capability of large language models (LLMs). However, due to the complexity of multimodal scenarios and the difficulty in collecting high-quality CoT data, CoT reasoning in multimodal LLMs has been largely overlooked. To this end, we propose a simple yet effective self-training framework, R\u00b3V, which iteratively enhances the model's Vision-language Reasoning by Reflecting on COT Rationales. Our framework consists of two interleaved parts: (1) iteratively bootstrapping positive and negative solutions for reasoning datasets, and (2) reflection on rationale for learning from mistakes. Specifically, we introduce the self-refine and self-select losses, enabling the model to refine flawed rationale and derive the correct answer by comparing rationale candidates. Experiments on a wide range of vision-language tasks show that R\u00b3V consistently improves multimodal LLM reasoning, achieving a relative improvement of 23% to 60% over GPT-distilled baselines. Additionally, our approach supports self-reflection on generated solutions, further boosting performance through test-time computation.", "sections": [{"title": "Introduction", "content": "Humans often rely on intuitive Chain-of-Thought (CoT) to perform complex reasoning (Ericsson and Simon, 1980). Previous studies have shown that this CoT capacity also emerges in Large Language Models (LLMs) (Wei et al., 2022). Through simple prompting or fine-tuning (Cobbe et al., 2021; Kojima et al., 2022; Hsieh et al., 2023), CoT enhances the reasoning performance of LLMs while providing insights into their decision-making process. Recently, OpenAI ol further advances reasoning by producing long internal CoT sequences, taking LLMs intelligence to a new level.\nWhile CoT reasoning has significantly advanced LLMs in textual domains, extending CoT to multimodal settings remains an open problem. Unlike the abundant, unsupervised text-based CoT in pre-training corpora (Kojima et al., 2022; Wei et al., 2022), multimodal CoT resources are scarce in the text-dominated internet collections (Dai et al., 2023), hindering the full realization of Multimodal LLMs' (MLLMs) reasoning potential.\nRecent studies show that open-sourced MLLMS struggle to integrate visual cues into their reasoning process, resulting in weak CoT performance (Zhang et al., 2024a; Shi et al., 2024). Consistent with our observations in Figure 1, CoT prompting provides minimal gains over direct prediction (Chen et al., 2024a) and falls far behind GPT-40. One potential solution is to construct multimodal CoT annotations for post-training; however, manual annotation is prohibitively expensive and hard to scale. This raises our first research question: can MLLMs self-improve the reasoning capabilities through bootstrapping on CoT samples?\nOrthogonal to fine-tuning on curated CoT annotations, relying solely on positive samples can lead to suboptimal policy due to insufficient exploration of reasoning paths. Inspired by human thinking, another promising direction involves learning from trial-and-errors (Yuan et al., 2024; Song et al., 2024), where mistakes are not failures but key opportunities to enhance reasoning. A few multimodal approaches use corrupted prompts to create negative samples for preference learning, aiming to improve image comprehension (Wang et al., 2023; Deng et al., 2024). However, these methods fail to generate reasoning-aligned positive and negative CoT solutions, making them unsuitable for complex multimodal reasoning tasks. Thus, it remains unaddressed: how can MLLMs efficiently learn from mistakes to improve their reasoning skills?\nTo address the above two questions, this paper proposes R\u00b3V, a self-training framework that enables the model to Reflect on bootstrapped COT Rationales, thereby strengthening its Vision-Language Reasoning. Firstly, we leverage MLLM's pre-existing but weak CoT ability to bootstrap both rationales and answers for a given question, enabling the collection of a large number of positive and negative solutions based on answer correctness. Secondly, we introduce a reflection mechanism on negative solutions to help the model learn from mistakes. Specifically, we design self-refine and self-select losses that guide the model to correct flawed rationales and derive the correct answer by comparing rationale candidates, respectively. The above synergistic process can be repeated, with improved samples boosting MLLM's reasoning and the enhanced model further improving rationale generation. Additionally, through self-select training, our model can derive the superior solution from multiple samples, further boosting performance via test-time computation.\nWe conduct experiments across a wide range of multimodal reasoning benchmarks, including charts, geometry, commonsense, science, mathematics, etc. R\u00b3V progressively enhances the reasoning ability of MLLMs, delivering a 23%-60% relative accuracy improvement compared to GPT distillation, and consistently outperforming the strong self-training baseline, STaR (Zelikman et al., 2022). Moreover, our test-time selection is robust and effective, consistently surpassing Pass@1 and majority voting, even in OOD scenarios.\nOur main contributions are as follows:\n\u2022 We introduce an iterative self-training framework R\u00b3V that leverages CoT bootstrapped by MLLM itself for self-improvement. To our knowledge, this is the first attempt to apply self-training in vision-language reasoning.\n\u2022 We propose learning from mistakes through self-reflection, with support for test-time computation to further improve reasoning performance.\n\u2022 We perform extensive evaluations across 6 different multimodal domains to validate the effectiveness of R\u00b3 V. Our analysis reveals the key factors driving the success of multimodal self-training."}, {"title": "Related Work", "content": "Vision-Language Reasoning Beyond the extensively studied unimodal reasoning (Cobbe et al., 2021; Sun et al., 2023), multimodal reasoning has recently attracted significant interest as an essential part of human intelligence (Yue et al., 2024; Lu et al., 2023). Although MLLMs perform well on general vision-language benchmarks (Liu et al., 2024; Chen et al., 2024b), integrating visual cues into the reasoning process poses unique challenges, especially for open-source models (Zhang et al., 2024a; Chen et al., 2024a). Several studies have explored using rationale datasets to fine-tune models and enhance visual-language reasoning capabilities. For example, (Gao et al., 2023; Zhang et al., 2024b) augmented existing mathematical datasets with rationales using GPT distillation, while (Yang et al., 2024) enhanced performance through manually collected CoT annotations. In this work, we advocate for MLLMs to self-improve, reducing reliance on resource-heavy rationale annotations.\nSelf-Training Methods Self-training helps the model learn from its own generated outputs, reducing the need for labor-intensive human annotations (Yuan et al., 2024; Chen et al., 2024c). Prior works have focused on enhancing the reasoning capacity of LLM. The typical approach involves sampling multiple rationales and filtering positive and negative solutions based on the answers. The LLM is then fine-tuned on the positive samples (Zelikman et al., 2022; Hosseini et al., 2024; Yuan et al., 2023) or improved using preference learning (Wang et al., 2024b; Mitra et al., 2024), such as DPO (Rafailov et al., 2024). Recent advances have also extended self-training to agents (Song et al., 2024) and neural symbolic (Xu et al., 2024) scenarios. In this paper, we pioneer the exploration of self-training in vision-language reasoning, investigate the failure of DPO in multimodal settings, and address these challenges with our R\u00b3V framework."}, {"title": "Methodology", "content": "Our self-training framework consists of two alternating components: (1) bootstrapping a large number of positive and negative CoT solutions for multimodal questions (Section 3.1); (2) using the above-sampled solutions to reflect on rationales and learn from mistakes (Section 3.2). This iterative process turns the MLLM from weak to strong. The overall framework is illustrated in Figure 2.\nIn visual-language reasoning, given an image I and a question x, a multimodal large language model is required to integrate information from both the image and the question for reasoning, generating a CoT rationale r and then deriving the final answer a. However, due to the difficulty in collecting high-quality rationale data, constructing large-scale (I,x,r,a) pairs presents significant challenges. This hinders the enhancement of MLLM reasoning capacities through fine-tuning.\nTo overcome this limitation, we propose leveraging the MLLM's pre-existing but weak CoT capability to iteratively augment (I, x, r, a) pairs from the widely available vision question answering data (I, x, a), enabling the model to self-improve.\nFollowing STaR (Zelikman et al., 2022), the MLLM self-training process involves iteratively fine-tuning on its self-generated rationale data. In each iteration t, given a question x from training set D = {(I, x, \u00e2)}, the MLLM M first generate a CoT rationale r along with an answer a, formulated as {(ri, ai)}. These intermediate outputs are then combined with the original training set, resulting in an augmented dataset that includes rationales:\n$D_r = \\{(I_i, x_i, r_i, a_i)\\}$                      (1)\nAssuming that rationales leading to correct answers are of higher quality compared to those that do not, we can divide Dr into positive and negative sample sets based on the correctness of the answers:\n$D^+_r = \\{(I_i, x_i, r_i, a_i) | a_i = \\hat{a_i}\\}_{i=1}$                                       (2)\n$D^-_r = \\{(I_i, x_i, r_i, a_i) | a_i \\neq \\hat{a_i}\\}_{i=1}$                                        (3)\nWe then fine-tune model M on the filtered positive CoT samples $D^+_t$ using supervised fine-tuning (SFT) with a negative log-likelihood objective:\n$L_{SFT} = -\\sum log M(y | x, I),$   (4)\n$(I,x,y) \\sim D_t$\nwhere the y = (r, a) is the solution generated by the model. We continue repeating the above process, generating new rationales with the newly fine-tuned model, until performance plateaus."}, {"title": "R3V: Reflection on Rationales", "content": "The above self-improvement process strengthens the model using positive solutions, while negative ones are typically discarded. However, negative samples comprise a large portion of the sampled solutions and offer valuable insights for further model enhancement (An et al., 2023; Hosseini et al., 2024). In our preliminary experiments, we found that the noisy nature of CoT in multimodal scenarios leads to suboptimal performance when using DPO (Rafailov et al., 2024). Inspired by the error-driven learning of humans, we introduce reflection on rationales, teaching the model to correct its own mistakes and reflect on multiple reasoning paths to identify the correct solution. Specifically, we propose additional self-refine (Section 3.2.1) and self-select (Section 3.2.2) losses for multitask learning. Our framework harnesses the continuous production of positive and negative samples in self-training, offering a robust and effective solution for learning from mistakes. Appendix E provides examples of different components in R\u00b3V."}, {"title": "Self-Refine", "content": "Upon failing to solve a problem, human students will analyze the errors in their solutions and reflect on how to correct them. Inspired by this, we designed the self-refine mechanism to encourage the model to correct flaws in its generated solutions. Multiple positive and negative solutions sampled during self-training can be viewed as the model's repeated reasoning on the same problem, making them well-suited for self-refine training. Specifically, we construct dataset for self-refine as follows:\n$D_{REF} = \\{(I_i, x_i, y^+_i, y^-_i) | y^-_i \\in Y_i, \\exists y^+_i \\in Y_i\\}_{i=1},$  (5)\nwhere $y^+$ and $y^-$ are positive and negative samples obtained from preceding iterations. Next, the self-refine loss is employed to guide the model in correcting errors in its self-generated answers:\n$L_{REF} = -\\sum log M(y^+ | y^-,x, I)$   (6)\n$(I,x,y^-,y^+) \\sim D_{REF}$\nThroughout the self-training iterations, samples for self-refine are continuously updated to incorporate higher-quality positive solutions and harder negative solutions."}, {"title": "Self-Select", "content": "Our early explorations reveal a key challenge in MLLM reasoning: current MLLMs frequently make simple errors such as misreading chart numbers or calculation mistakes, however, the autoregressive model has no mechanism to correct them, leading to suboptimal performance. In contrast, human reasoners implicitly simulate multiple reasoning paths, check for errors, and select the best one. Inspired by this, we introduce the self-selection mechanism, guiding MLLMs to derive the correct answer from multiple candidate solutions.\nGiven a set of sampled rationales, the model is required to analyze their differences and finally select the correct answer. Specifically, we construct the self-select dataset as follows:\n$D_{SEL} = \\{(I_i, x_i, \\hat{a_i}, C_i) | C_i\\}_{i=1},$ (7)\nwhere $\\hat{a}$ is the ground truth and $C_i = \\{y^1_i, y^2_i, ..., y^N_i\\}$ is a set of N sampled rationale-answer pair. In our experiments, N is set to 3 by default. We ensure that the candidate set C contains at least one positive solution $y^+$, allowing the model to select the final correct answer. Then, the self-select loss is defined as:\n$L_{SEL} = - \\sum log M(\\hat{a}|x, I, C)$ (8)\n$(I,x,\\hat{a},C) \\sim D_{SEL}$\nFinally, our framework combines three loss functions in a multi-task training setup to enhance MLLM reasoning (see algorithm in Appendix D):\n$L_{R^3V} = L_{SFT} + L_{REF} + L_{SEL}$ (9)"}, {"title": "Test-Time Selection", "content": "Through self-select training, our framework enables MLLMs to reflect on their self-generated solutions and select the final answer from multiple reasoning paths. During inference, given a question x and corresponding image I, we first sample multiple reasoning solutions to form the candidate set C. Next, the MLLM is prompted to select the best answer from these candidate solutions: $a = M(x, I,C)$.\nTest-time selection offers a novel approach for MLLMs to tackle complex multimodal reasoning. Instead of directly generating an answer, the model applies an elimination method by comparing different reasoning paths and checking for errors (e.g., visual recognition, calculation, or reasoning mistakes) to identify the most likely correct solution. In this way, our approach further boosts reasoning performance through test-time computation."}, {"title": "Experiments", "content": "In our experiments, we focus on a diverse and comprehensive set of vision-language reasoning tasks to demonstrate the effectiveness of R\u00b3V. We begin by outlining the benchmarks (Section 4.1) and experimental setup (Section 4.2), followed by the main results of R\u00b3V on six widely used datasets (Section 4.3). We also evaluated the improvements achieved by our framework in out-of-distribution (OOD) scenarios (Section 4.4)."}, {"title": "Datasets", "content": "We validate our framework's self-improvement on six vision-language reasoning benchmarks, which require integrating visual information into complex, multi-step reasoning. Refer to Appendix B for detailed information of these benchmarks.\nTabMWP (Lu et al., 2022): A dataset for table-based math word problems requiring reasoning and numerical calculation.\nChartQA (Masry et al., 2022): Focuses on reasoning and calculations within real-world charts.\nCLEVR-Math (Lindstr\u00f6m and Abraham, 2022): Compositional reasoning over abstract figures.\nMiniWob (Shi et al., 2017): A widely-used multimodal web navigation benchmark requiring models to generate multi-step actions.\nGeoQA (Chen et al., 2021): A geometry problem benchmark requiring complex reasoning.\nM\u00b3CoT (Chen et al., 2024a): A recently introduced dataset featuring multi-domain, multi-step multi-modal reasoning problems."}, {"title": "Experimental Setings", "content": "We primarily compare our framework with three categories of methods to comprehensively assess its effectiveness. All experiments are conducted under the same parameters to ensure a fair comparison.\nZero-shot Methods. We evaluated the MLLMS' zero-shot performance under the direct prompt (where the model tends to provide an immediate answer (Liu et al., 2024)) and the CoT prompt using \"Let's think step by step.\" GPT-40 was also chosen as a strong baseline for comparison.\nSupervised Fine-tuning Baselines. Since the self-training requires existing (I, x, a) datasets, we provide the results of fine-tuning MLLMs using direct prompts on these question-answer pairs. We also include a GPT distillation baseline, where GPT-40 annotates CoT rationales for a small subset of each dataset, and then the open-source MLLMs are fine-tuned for CoT reasoning.\nSelf-Training Methods. We employ the aforementioned GPT-distilled, warmed-up MLLM as the starting point for self-training, iteratively sampling positive and negative rationales from training samples for continuous self-improvement. We then compare R\u00b3V with the well-known self-training baselines STaR (Zelikman et al., 2022), which iteratively fine-tunes on self-generated positive solutions for model improvement."}, {"title": "Main Results", "content": "Table 1 presents the evaluation results of R\u00b3V on various multimodal reasoning tasks, including logical and numerical reasoning, agentic tasks, geometry, and multi-domain scenarios. The evolution progress of self-training is illustrated in Figure 3.\nSelf-training effectively converts MLLMs from weak to strong. Open-source MLLMs struggle with complex vision-language reasoning tasks. CoT reasoning with the \"Let's think step by step\" prompt (Zero-shot CoT) proves ineffective, with performance even worse than direct prompting (Zero-shot QA). In this situation, the self-training method leverages MLLMs' pre-existing but weak CoT capabilities to bootstrap multimodal CoT data for self-improvement. This process progressively elevates MLLMs' CoT reasoning, as shown in Figure 3, taking it to the next level on top of the GPT-distilled baseline. As an example with Qwen-VL, our self-training framework R\u00b3V delivers an average 32.8% relative performance improvement over the GPT-distilled baseline (48.47 \u2192 64.37). This result highlights the remarkable potential of MLLMs to enhance their reasoning capabilities through self-training on synthetic data.\nR3 V further enhances self-training efficiency by learning from mistakes. Instead of discarding valuable negative samples, our R\u00b3V framework leverages carefully designed self-refine and self-select mechanisms to learn from negative solutions, surpassing the strong self-training baseline STaR by a large margin (average 59.28 \u2192 64.37 on Qwen-VL). As shown in Figure 3, R\u00b3V demonstrates swift adaptation across different multimodal scenarios, achieving notably higher gains in the first iteration compared to the STaR baseline, highlighting the efficiency of our method. These results underscore the value of learning from mistakes in multimodal reasoning and demonstrate the effectiveness of our reflection-based methodology."}, {"title": "Out-of-Distribution Evaluation", "content": "Beyond the success of the R\u00b3V framework on in-domain benchmarks, we are curious whether its reasoning improvements can generalize to out-of-distribution (OOD) and more difficult vision-language tasks. To this end, we aggregated the CoT rationales self-generated by R\u00b3V across in-domain benchmarks and constructed positive and negative pairs for continual training on Qwen-VL. For a fair comparison, we also included a baseline that uses only GPT-distilled positive CoT annotations. We conducted evaluations on three challenging benchmarks: (1) MMMU (Yue et al., 2024), a multi-discipline dataset designed to evaluate various aspects of multimodal reasoning; (2) MathVista (Lu et al., 2023), which focuses specifically on mathematical reasoning in multimodal contexts; (3) VCR (Zellers et al., 2019), a cognition-level visual understanding benchmark that requires reasoning based on common sense and visual content.\nR3V also strengthens multimodal reasoning in OOD scenarios. As shown in Table 2, after incorporating R\u00b3V's self-generated CoT reasoning data, Qwen-VL significantly outperforms both the zero-shot and GPT-distilled baselines. This demonstrates that the CoT annotations synthesized by our framework not only enhance MLLM in-domain reasoning but also generalize to OOD and more challenging vision-language tasks.\nTest-time selection generalizes to unseen tasks. Somewhat surprisingly, we found that the test-time selection ability does generalize to unseen tasks. For example, on MMMU, sampling three times during inference combined with our self-select mechanism (see Section 3.2.3) led to further improvement (35.63 \u2192 38.48). This suggests that through our self-select training, the MLLM has learned to compare multiple reasoning paths, identify errors (e.g., recognition or calculation mistakes), and eliminate incorrect options to arrive at the correct answer."}, {"title": "Analysis", "content": "This section analyzes the key factors behind the success of the R\u00b3V, as well as the potential challenges of self-training in multimodal reasoning tasks."}, {"title": "Ablation Studies", "content": "Reflection on self-generated CoT facilitates learning from mistakes. To validate the effectiveness of each part of our framework, we independently ablated the self-refine and self-select losses, denoted as w/o self-refine and w/o self-select. As shown in Table 3, both self-refine and self-select play a crucial role in improving performance. This highlights the value of negative samples, while our R3V framework's reflection mechanism (i.e., self-refine and self-select losses) serves as an effective method for learning from mistakes.\nIterative training process is crucial for self-improvement. Next, we ablated iterative training as w/o iteration: instead of iteratively sampling and training, we sampled a large batch at once. For example, iterative self-training samples three times per round over four rounds, while w/o iteration samples 3 \u00d7 4 = 12 times in a single pass. This approach is similar to Rejection Sampling Fine-tuning (RFT; Yuan et al. (2023)), but includes our self-refine and self-select losses. The results in Table 3 demonstrate the importance of iteratation. Although w/o iteration produces a large number of positive and negative samples (comparable to R\u00b3V by our statistics), the progressive training process yields higher-quality, more diverse samples, which boosts self-training performance."}, {"title": "Test-time Compute", "content": "Test-time self-selection boosts performance through sampling. One key advantage of R3V framework lies in its capacity to enhance performance by scaling test-time computation: during inference, we sample multiple candidate solutions and apply self-select to choose the answer. Figure 4 compares self-selection with Test@1 and majority voting with a sample size of 3. Our self-selection method consistently outperforms Test@1 and majority voting across all tasks. While majority voting reduces noise by aggregating results, self-selection goes further by deeply comparing reasoning paths, eliminating incorrect options, and ultimately analyzing to reach the correct answer.\nR3V consistently benefits from the scaling of sampling size. An open question is the scalability of our test-time selection. We conducted experiments with Qwen-VL on the TabMWP and CLEVR-Math benchmarks, comparing the performance of self-select and majority voting as the sample size increases. As shown in Figure 5, scaling the sample size consistently improves the performance of test-time selection, achieving both higher efficiency and accuracy compared to majority voting. Due to limitations in input length and capability of current MLLMs, performance plateaus with excessive sample size, which we believe stronger base models could address.\nOur self-training framework requires no manual annotation, instead synthesizing large-scale positive and negative CoT rationales through sampling, equipping the model with the capacity for self-reflection during reasoning. It also opens up new opportunities for boosting MLLM reasoning performance by scaling test-time computation."}, {"title": "The Noisy Nature of Multimodal CoT", "content": "In our preliminary study, we found that the widely-used preference learning method DPO (Rafailov et al., 2024) struggles to leverage positive and negative solutions for further improvement in multimodal settings. As shown in Table 4, equipping STaR with DPO training yields minimal improvement and falls short of our R\u00b3 V.\nTo investigate DPO's failure, we closely examined the positive and negative samples self-generated by the MLLM (see details and case study in Appendix G). For each task, we randomly selected 100 positive solutions based on answer correctness and manually categorized their CoT fidelity as correct, partially correct, or incorrect. As shown in Figure 6, unlike natural language reasoning tasks (e.g., Logic, Math), multimodal CoT contains significant noise, with the proportion of fully correct CoT ranging from 8% to 70%. This stems from MLLM's limited recognition capabilities, leading to flawed CoT despite correct answers, such as OCR errors. As a result, faulty reasoning in noisy CoT is often misjudged as better solutions, making it challenging for DPO to distinguish between correct and incorrect reasoning paths and ultimately reducing performance (Chowdhury et al., 2024). In contrast, our reflection method avoids encouraging the generation of faulty solutions, instead guiding the model to select the correct answer through elimination, demonstrating greater efficiency in noisy multimodal CoT scenarios."}, {"title": "Generalization to Stronger Backbone", "content": "To demonstrate generalizability, we applied R\u00b3V to the latest advanced MLLM, Qwen2-VL (Wang et al., 2024a), evaluating its ability to self-improve in solving geometric problems (Chen et al., 2021). As shown in Figure 7a, even without GPT-distilled warmup, R\u00b3V achieves significant self-improvement by leveraging the model's pre-existing CoT abilities, demonstrating the R\u00b3V's generalizability across backbones. More impressively, we found that test-time selection demonstrates superior scalability on Qwen2-VL, markedly outperforming majority voting, as illustrated in Figure 7b. We hypothesize that the enhanced general capabilities of the base model further amplify the effectiveness of self-select, which we leave for future exploration."}, {"title": "Conclusion", "content": "The scarcity of multimodal CoT data limits the reasoning capabilities of current MLLMs. In this paper, we take the first step toward enabling MLLMS to self-improve for better vision-language reasoning. We propose an iterative self-training framework, R\u00b3V, which continuously bootstraps positive and negative solutions and improves reasoning through reflection on self-generated CoT rationales. Meanwhile, R\u00b3V enables MLLMs to self-reflect on their generated solutions, offering new opportunities for boosting performance through test-time computation. Extensive experiments and analyses demonstrate the effectiveness of our framework and the key factors behind its success."}, {"title": "Limitations", "content": "As discussed in Section 5.3, due to the limitations of current MLLMs, the CoT annotations generated by R\u00b3V often contain noise. While our framework can self-improve performance on noisy multimodal CoT, we believe that higher-quality CoT will further enhance reasoning ability. Due to computational constraints, our main experiments were conducted on two well-known MLLMs, LLaVA and Qwen-VL. Expanding to larger and more advanced MLLMs could yield interesting results, which we plan to explore in future work."}, {"title": "Additional Related Work", "content": "Multimodal Large Language Models and Multimodal Reasoning Driven by the advancement of Large Language Models (LLMs), the multimodal research community has recently witnessed a domain shift from Vision-Language Models (VLMs) (Radford et al., 2021; Li et al., 2022; Cheng et al., 2023) to Multimodal Large Language Models (MLLMs) (Achiam et al., 2023; Liu et al., 2023; Chen et al., 2023; Cheng et al., 2024). Unimodal reasoning has a strong research foundation, such as in mathematics (Hendrycks et al., 2021) and code generation (Sun et al., 2024). Multimodal reasoning requires models to integrate visual cues into the reasoning process (Zhang et al., 2023), presenting new challenges. Recent studies have explored synthesizing table or chart data and leveraging GPT to annotate CoT, aiming to enhance MLLM reasoning capabilities (Han et al., 2023; Jia et al., 2024). For instance, Huang et al. (2024) utilizes GPT to generate chart code and render it to obtain diverse chart reasoning samples. In this work, we do not rely on stronger models to synthesize new reasoning samples; instead, we enable MLLMs to achieve self-improvement from self-generated CoT data.\nSelf-Training Methods Self-training, especially integrated with reinforcement learning from its own outputs, offers a promising avenue for model self-improvement (Huang et al., 2022; Gulcehre et al., 2023). Recent studies have applied self-training to MLLMs with the goal of enhancing image comprehension, particularly in mitigating hallucinations (Zhou et al., 2024; Gunjal et al., 2024; Zhao et al., 2023). Deng et al. (2024) proposes constructing positive and negative sample pairs by perturbing images and prompts, and enhances alignment through DPO training. In contrast, this work focuses on complex reasoning in multimodal scenarios, which requires integrating visual cues to generate step-by-step reasoning CoT. To our knowledge, we are the first to explore self-training in the context of vision-language reasoning."}, {"title": "Vision-Language Reasoning Benchmarks", "content": "TabMWP (Lu et al., 2022) Tabular Math Word Problems (TabMWP) is benchmark containing open-domain grade-level problems that require mathematical reasoning and calculation on table figures. We use the standard train/test split provided by the author.\nChartQA (Masry et al., 2022) We used the human-written version as the self-train benchmark, which contains more reasoning-intensive questions compared to the augmented split. This subset contains 7,398 chart figures and question pairs, comprising both free-text and multiple-choice questions.\nCLEVR-Math (Lindstr\u00f6m and Abraham, 2022) The CLEVR-Math dataset consists of multimodal math word problems that combine text and images, where questions are posed about the state of the scene after a sequence of actions (like addition or subtraction of objects) have been applied. We randomly sampled 10000 instances for training and used the original test set.\nMiniWob (Shi et al., 2017) MiniWob asks MLLM to interact with a simulated Web environment. As shown in Figure 8, the model is provided with an image of the web interface along with the html as input. It is then asked to generate Python code to simulate keyboard and mouse actions and complete the given task.\nGeoQA (Chen et al., 2021) GeoQA contains 4,998 multiple-choice geometric problems from Chinese middle school exams and annotated with solving programs. We use human translated English version provided by UniGeo (Chen et al., 2022).\nM\u00b3CoT (Chen et al., 2024a) M\u00b3CoT is a manually verified multimodal, multi-domain, multi-step visual-language reasoning dataset. We use the official train/test splits in our R\u00b3V self-training process.\nEvaluation For structured outputs like GPT-distilled and self-train methods, we use the benchmark's default evaluation script to calculate metrics. For free-form outputs like the zero-shot CoT baseline, we employ GPT-40-mini as the evaluator to assess accuracy. For MiniWob, the simulated web environment provides an automatic reward of"}]}