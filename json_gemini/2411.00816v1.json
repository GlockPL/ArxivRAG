{"title": "CYCLE-RESEARCHER: IMPROVING AUTOMATED RESEARCH VIA AUTOMATED REVIEW", "authors": ["Yixuan Weng", "Minjun Zhu", "Guangsheng Bao", "Hongbo Zhang", "Jindong Wang", "Yue Zhang", "Linyi Yang"], "abstract": "The automation of scientific discovery has been a long-standing goal within the research community, driven by the potential to accelerate knowledge creation. While significant progress has been made using commercial large language models (LLMs) as research assistants or idea generators, the possibility of automating the entire research process with open-source LLMs remains largely unexplored. This paper explores the feasibility of using open-source post-trained LLMs as autonomous agents capable of performing the full cycle of automated research and review, from literature review and manuscript preparation to peer review and paper revision. Our iterative preference training framework consists of CycleResearcher, which conducts research tasks, and CycleReviewer, which simulates the peer review process, providing iterative feedback via reinforcement learning. To train these models, we develop two new datasets, Review-5k and Research-14k, reflecting real-world machine learning research and peer review dynamics. Our results demonstrate that CycleReviewer achieves a 26.89% improvement in mean absolute error (MAE) over individual human reviewers in predicting paper scores, indicating that LLMs can surpass expert-level performance in research evaluation. In research, the papers generated by the CycleResearcher model achieved a score of 5.36 in simulated peer reviews, surpassing the preprint level of 5.24 from human experts and approaching the accepted paper level of 5.69. This work represents a significant step toward fully automated scientific inquiry, providing ethical safeguards and advancing AI-driven research capabilities. The code, dataset and model weight are released at http://github/minjun-zhu/Researcher.", "sections": [{"title": "1 INTRODUCTION", "content": "Automating general scientific discovery has been a long-standing ambition of the research community, dating back to the late 1970s and 1980s (Lenat, 1977; 1983; Langley, 1987) with the advent of computer science. In the field of AI, researchers have envisioned automating scientific research using AI itself (Hutter, 2001; Radensky et al., 2024). The recent emergence of large language models (LLMs) has opened new possibilities for this endeavor (Wang et al., 2023a; Lu et al., 2024), demonstrating their capacity to not only process but also contribute meaningfully to scientific research. Most current efforts have relied on commercial LLMs to build agents that propose research ideas (Wang et al., 2023b; Yang et al., 2023; Radensky et al., 2024; Baek et al., 2024; Liu et al., 2024b), as an assistant to conduct experiments (Du et al., 2024; Yang et al., 2024b; Li et al., 2024), or act as an Al scientist capable of generating automated open-ended scientific publications (Lu et al., 2024; Taniguchi et al., 2024). To date, the challenge of automating the entire scientific discovery process remains largely unresolved, particularly when it comes to generating and refining research outputs that meet the high standards of peer-reviewed work. To date, existing AI-driven research agents have been unable to consistently achieve the necessary depth in key areas such as soundness,"}, {"title": "", "content": "presentation, and contribution. Moreover, few efforts address the integration of iterative feedback, which is essential for maintaining academic soundness and novelty. Current models often struggle to adapt across the full spectrum of research stages, highlighting gaps in their ability to conduct comprehensive, multi-step scientific discovery.\nCentral to the scientific process is the iterative cycle of submission, peer review, and revision \u2013 an established mechanism that maintains the quality and integrity of academic work (Smith, 2006; Boughton et al., 2018). Feedback from reviewers and peers plays a critical role in this cycle, offering insights that help researchers refine their work and improve its rigor and impact. Drawing inspiration from this cyclical process, we propose a novel framework that post-trains LLMs as autonomous agents to simulate the full loop of the scientific discovery process. Our approach, built entirely on open-source models, aims to replicate the real-world dynamic of research development and peer review processes. By leveraging trainable models, we enable the utilization of the iterative preference training mechanism (Yuan et al., 2024) using sampling examples through reinforcement learning. Our objective is to determine whether LLMs can actively contribute to each stage of scientific inquiry, from literature review and idea generation to experimental design, manuscript preparation, peer review and paper revision.\nAutomating the entire research lifecycle \u00d6berg et al. (2022) presents a significant challenge to current agent-based methods (Lu et al., 2024; Si et al., 2024; Yang et al., 2024b), which predominantly rely on commercial models. Consequently, these methods cannot be effectively modeled as policy optimization problems using reinforcement learning. While self-correction methods (Weng et al., 2023; Yuan et al., 2024; Lee et al., 2024) have been developed to enhance reasoning performance by assessing the quality of LLM outcomes and providing feedback, they have not yet been adopted in the domain of paper writing, which demands more complex evaluations from multiple perspectives. Our research addresses this gap by introducing an iterative post-training framework. The central research question we pose is: \u201cHow can we automate the Research-Review-Revision process by post-training LLMs?\u201d So that automated research can be improved according to feedback from automated reviews.\nWe build a novel iterative training framework (Pang et al., 2024) that contains two core components: the policy model (namely CycleResearcher) and the reward model (namely CycleReviewer) ranging in size - from 12B to 123B \u2013 based on Mistral (Jiang et al., 2023) and Qwen 2.5 (Yang et al., 2024a; Team, 2024). This framework enable interaction between the policy and reward models for complete research cycle. In particular, the policy model performs a variety of research tasks \u2013 ranging from generating hypotheses and designing experiments to conducting literature reviews and preparing manuscripts - for paper generation. The reward model, on the other hand, simulates the peer review process, evaluating the quality of the research output and providing feedback that informs reinforcement learning rewards. For instance, when exploring the topic of \u201cHacking Rewards of VLMs,\u201d we first fed fine-tuned CycleResearcher with a set of relevant published papers to inspire it to propose novel ideas. After generating a batch of first-round papers corresponding with those ideas, fine-tuned CycleReviewer evaluates them to generate pairwise preference samples, which are used to optimize the policy model using SimPO (Meng et al., 2024), this process is repeated.\nFor training our models, we construct two large-scale, publicly available datasets: Review-5k and Research-14k (described in Section 2). Which contains peer review and accepted papers in several ML conferences (e.g., ICLR, ICML, NeurIPS). For testing, we take both subjective human evaluation and objective model-based evaluations for assessing the quality of CycleReviewer and CycleResearcher. For CycleReviewer, experimental results demonstrate that LLMs when trained with our framework, can even outperform human reviewers. For CycleResearcher, both human evaluation and CycleReviewer evaluation results demonstrate that our method can provide more consistent and stable research evaluation compared to API-based agents (Lu et al., 2024). We also acknowledge that the generalizability across research domains remains a challenge for current LLMs.\nOur contributions are:\n\u2022 We introduce an iterative reinforcement learning framework that automates the entire research lifecycle, which mirrors the real-world Research-Rebuttal-Revision cycle. Our framework includes CycleResearcher, a policy model for research tasks, and CycleReviewer, a reward"}, {"title": "2 DATASET CONSTRUCTION", "content": "In this section, We present an overview of how we collect a substantial corpus of academic papers and organize them into the Review-5k and Research-14k training dataset. As illustrated in Figure 1, we introduce structured outline extraction and segmentation to assist the LLM in planning before generating research papers. Importantly, we will only make our datasets publicly available for those papers, which receive written consent from publishers."}, {"title": "2.1 REVIEW-5K", "content": "In order to collect a high-quality review dataset, we first gather paper information (including title, abstract, and PDF data) along with the corresponding review comments from ICLR 2024. This ensures that all papers are evaluated according to a consistent standard. We then attempt to retrieve the permitted LaTeX files from ArXiv. If the LaTeX files are unavailable, we use MagicDoc to convert the retrieved PDFs into markdown format. Then, inspired by the traditional peer review process, where a group of reviewers evaluates a paper, followed by a senior reviewer who synthesizes their feedback and makes the final decision, we collect each data point including key components: 1) summary of the work, 2) identified strengths and weaknesses, and 3) questions for clarification, along with 4) numerical scores for soundness, presentation, contribution, and an overall rating. Finally, we filter the blank data points and leave a dataset named Review-5k, containing 4,970 papers collected from ICLR 2024, comprising over 16,000 reviewer comments. Finally, we split our dataset into mutually exclusive training/testing sets, we keep 4,189 paper reviews for training and 781 samples for testing."}, {"title": "2.2 RESEARCH-14K", "content": "The research-14k dataset aims to capture structured outlines and detailed main text from academic papers. The data construction process involves three steps: (1). we first compile a list of accepted papers from major international machine learning conferences, such as ICLR, NeurIPS, ICML, ACL, EMNLP, CVPR, and ICCV, spanning from 2022 to 2024. Using Semantic Scholar\u00b3, we retrieve the corresponding ArXiv links and LaTeX-format files for each paper, gathering a total of 7,725 papers. The main text of these papers is then pre-processed using rule-based filtering to remove irrelevant content such as comments (\u201c%\u201d) and acknowledgments. (2). Since the academic value of a research paper depends on its background, we also use the Semantic Scholar API to retrieve the cited works from the bib file and add their abstracts to it. (3). Finally, we organize the main body of each paper into outlines and separate sections to help the model better understand the research process. We use the Mistral-Large-2 model (Jiang et al., 2023) to extract outline information from the paper, following the outline structure shown in Figure 1, and concatenate each outline with its corresponding section. These components form the complete fine-tuning dataset, where the input consists of detailed reference files and the output contains the paper outlines and main text.\nAfter filtering papers that do not meet the requirements, the final dataset, Research-14k, includes 12,696 training samples and 802 test samples. It covers nearly all significant machine learning papers from the past three years and ensures that all collected papers are open-access. The training and test sets are split chronologically, with test papers published later than the training ones. This dataset is used for supervised fine-tuning to enable the LLM to generate well-structured academic papers. Additionally, Research-14k is a long-output dataset, with an average output length of 28K tokens."}, {"title": "3 ITERATIVE TRAINING FRAMEWORK", "content": "We use the iterative Simple Preference Optimization (SimPO) (Meng et al., 2024) framework to replicate the Research-Review-Revision cycle typical in academic research. As mentioned in the introduction, we primarily focus on the development of ideas and the writing process, while the execution of actual experiments (Liu et al., 2024b; Zhu & Zhou, 2024; Hu et al., 2024) is beyond the scope of this work. The process begins with initializing two models: a baseline language model fine-tuned for academic writing (the CycleResearcher), and an LLM specialized in evaluating research papers (the CycleReviewer).\nAs illustrated in Figure 2, each iteration encompasses two primary phases: (1) The CycleResearcher model simulates key research steps, including literature review, hypothesis formulation, experimental"}, {"title": "3.1 REWARD MODEL: CYCLEREVIEWER", "content": "We train CycleReviewer as the Generative Reward model on the Review-5k Dataset. To accurately reflect the academic peer review process, we establish a streamlined evaluation workflow:\nPaper \u2192 R1, R2, ..., Rn \u2192 SR,\nwhere the research paper (Paper) is reviewed by multiple reviewers (R1, R2, ..., Rn). Each reviewer's opinion is then summarized by a Senior Reviewer (SR), forming the final decision.\nThe input to the CycleReviewer model is a complete research paper. Upon receiving the paper, the model generates sequential feedback and scores for key aspects including Strengths, Weaknesses, Soundness, Presentation, Contribution, and an Overall Score. The Overall Score is rated on a scale from 1 to 10, where 1 represents the lowest score and 10 the highest, with 5 indicating the paper is borderline for rejection and 6 suggesting it is near acceptance. The output of the model includes both the Overall Score and a recommendation labelled as the \u201cFinal Suggestion.\" The CycleReviewer simulates the review process across multiple reviewers, producing a set of Overall Scores. The final output is the average of these scores, representing the overall evaluation of the system.\nSettings. We use the Mistral-Large-2 model with LoRA-GA on an 4x 8x H100 80G cluster, with a learning rate of le-5 and a batch size of 4x8, for 12 epochs on the Reviewer-5k dataset. To ensure diversity in the generated reviews, CycleReviewer starts by simulating the feedback from the reviewer with the lowest rating, gradually progressing to the highest-rated reviewer. This approach ensures that a range of perspectives, from more critical to more favorable, are considered before the senior reviewer delivers the final assessment, highlighting key strengths and weaknesses and providing an objective decision."}, {"title": "3.2 POLICY MODEL: CYCLERESEARCHER", "content": "The CycleResearcher model is trained on Research-14k, and the process begins with a literature review, where the input bib file contains all references and their corresponding abstracts. After gaining a comprehensive understanding of the research background, the model moves on to manuscript preparation. In this stage, it alternates between generating outlines and main text to ensure a logical flow. First, the model generates the motivations and main ideas in the outline and then follows up by producing the title, abstract, introduction, and method sections in the main text. Next, it outlines the experimental setup and results, and subsequently generates the experimental design and simulated results in the main text, where it also incorporates discussions. In the virtual RL environment, to accelerate training, we require the \u201cexperimental results\" to be fabricated instead of conducting actual experiments. Finally, the model analyzes the experimental results and formulates the conclusion. Once all sections of the main text are generated, they are combined into a complete paper in LaTeX format. Notably, each part of the research paper in Research-14k is precisely segmented. Finally, the generated paper P is evaluated using the CycleReviewer, as described in Section 3.1.\nSettings. To build the policy model, we select widely used open-source LLMs: Mistral-Nemo-12B, Qwen2.5-Instruct-72B, and Mistral-Large-2 123B. All models are trained using 8x H100 80G GPUs and DeepSpeed + ZeRO2 (Rajbhandari et al., 2020; Rasley et al., 2020) for optimization. We maximized context length by setting the 12B model to 32K tokens, while the 72B and 123B models were set to 24K tokens. During training, we apply FP8 quantization (Kuzmin et al., 2024) to model weights and use LoRA-GA (Hu et al., 2022; Wang et al., 2024a) for training. Given memory constraints, samples exceeding the preset context length are randomly truncated. We use a batch size of 2 \u00d7 8, a learning rate of 4e-5, and train for a total of 12,000 steps. These models, all instruction-tuned, support context windows up to 128K tokens, making them suitable for planning research projects and writing research papers. In response, we contribute three versions of policy models:"}, {"title": "3.3 ITERATIVE SIMPO", "content": "We design an Iterative preference optimization alignment method (Xiong et al., 2024; Liu et al., 2024a) that simulates the peer rebuttal process as a reward mechanism. To construct a preference-pair dataset, we first collected 4,152 recent machine learning papers published on arXiv, retaining only the reference sections as the knowledge base. Then we sampled three times from the CycleResearcher with a temperature of 0.4 and processed the results into standard LaTeX-style texts M1, M2, M3. Next, the CycleReviewer model simulated discussions among multiple reviewers, providing detailed evaluations of various aspects of the papers (e.g., novelty, methods, experimental design, result analysis). The average score r\u2081 from all simulated reviewers was assigned to each output Mi. We then selected the output with the highest reward value as the positive sample yw and the one with the lowest reward value as the negative sample y\u2081, forming a preference-pair dataset Do = (x, yw, y\u2081).\nPolicy Optimization. Instead of using the iterative DPO training framework (Pang et al., 2024), we adopt the SimPO as the base method for saving computational costs. To mitigate overfitting, we sample one-third of the full dataset in each round. Then, we generate a series of models P\u2081, ..., \u0420\u0442, where each model Pt+1 is created using the preference data Dt generated from the model Pt. With the preference-pair dataset, we trained a new policy model \u03c0\u03b8 from Pt to Pt+1. P\u2081 was initialized from the original fine-tuned CycleResearcher model using instruction tuning.\nSimPO builds upon DPO (Rafailov et al., 2023), which is one of the most common offline preference optimization methods. It introduces a length-normalized reward function aligned with the generation target, thereby eliminating dependence on a reference model ref, which reduces memory and computation requirements. The reward function for SimPO is as follows:\nrSimpo(x, y) = \\frac{\\beta}{|y|} \\log \\pi_{\\theta}(y | x) = \\frac{\\beta}{|y|} \\sum_{i=1}^{|y|} \\log \\pi_{\\theta}(y_i | x, y_{<i}),\nwhere \u03c0\u03b8 is the policy model, |y| represents the length of the generated sequence, and \u03b2 is a constant controlling the scaling of reward differences. SimPO also introduces a target reward margin \u03b3 > 0 to help differentiate between winning and losing responses. The objective for SimPO is as follows:\nLSimPO(\u03c0\u03b8) = -E(x,yw,y1)~D \\log \\sigma \\left[ \\frac{\\beta}{|y_{w}|} \\log \\pi_{\\theta}(y_{w}|x) - \\frac{\\beta}{|y_{l}|} \\log \\pi_{\\theta}(y_{l}|x) - \\gamma \\right],\nConsidering that the models used in the research process may involve complex reasoning and mathematical calculations, we combine the SimPO loss learned from preference pairs with the negative log-likelihood (NLL) loss to stabilize training (Pang et al., 2024). The loss function for each preference pair is as follows:\nLour(\u03c0\u03b8) = -E(x,yw,y1)~D \\log \\sigma \\left[ \\frac{\\beta}{|y_{w}|} \\log \\pi_{\\theta}(y_{w}|x) - \\frac{\\beta}{|y_{l}|} \\log \\pi_{\\theta}(y_{l}|x) - \\gamma \\right] - E_{(x,y)~DNLL} [\\log \\pi_{\\theta}(y_{w} | X)] .\nHere, the hyperparameter \u039b balances the two loss terms. Each round of training resamples and optimizes based on the previous round's results, enabling an approximate online policy optimization process, which allows the CycleResearcher to continuously adapt to evolving publication standards."}, {"title": "3.4 SAFEGUARD ACADEMIC INTEGRITY", "content": "Beyond automating the research process, we are also concerned with safeguarding academic integrity. We aim to prevent the misuse of LLMs in the research community. To achieve that, we adopt the Fast-DetectGPT (Bao et al., 2024), which aims to use the metric of conditional probability curvature to determine whether the paper submission is generated by LLMs."}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENTS OF PAPER REVIEW GENERATION\nEvaluation Metrics. Evaluating reviewer performance is inherently difficult because the true quality of submissions is unknown. To address this challenge, we use Proxy Mean Squared Error (Proxy MSE) and Proxy Mean Absolute Error (Proxy MAE) to assess the accuracy of individual review scores (Su et al., 2024), detailed in Appendix C. For each paper, the conventional MSE and MAE for a review score r are defined as E [(r \u2013 ground truth)\u00b2] and E [|r \u2013 ground truth|], which are unobservable due to the unknown true quality. Therefore, we introduce a proxy evaluation method using an independent, unbiased estimator as a stand-in for the ground truth score. Assuming we have n human experts with scores R = r1,2,...,rn, we treat each reviewer's score ri as an unbiased estimator of the true quality. We define r' = mean(R \\ ri), which serves as an unbiased estimator excluding ri. Thus, we measure the quality of ri using Proxy MSE = (ri - r')\u00b2 and Proxy MAE = |ri \u2013 r'|. Simply put, for each submission, we use the average of the other n 1 reviewers' scores as an estimator of the true score.\nOur evaluation of the Reviewer-5k test set treats each human reviewer as an independent evaluator, using the average of the remaining reviewers' scores as a proxy for the true quality. The average rating across this dataset is 5.53. This approach provides an unbiased approximation. We assess Proxy MAE and Proxy MSE for human experts using this proxy and provide two proxy methods. In the n - 1 mode, we randomly select one reviewer from the group, using the average score of the remaining n 1 reviewers as the proxy for ground truth, allowing for fair comparison with human experts. Building upon our evaluation of human reviewers, we also evaluated a series of closed-source models built on the AI Scientist review system (Lu et al., 2024), including a one-shot review based on ICLR guidelines, five rounds of self-reflection (Shinn et al., 2023), and five ensembled reviews.\nCycleReviewer introduces better quality of peer review. Table 1 presents the performance comparison across various models. CycleReviewer clearly outperforms both proprietary systems and human experts in peer review tasks. Specifically, it achieves a 48.77% (2.34 -> 1.43) reduction in Proxy MSE and a 26.89% (1.16 -> 0.92) reduction in Proxy MAE compared to human reviewers. With a decision accuracy of 74.24%, our model demonstrates a significant lead over other closed-source systems. These results indicate that our model consistently generates scores more aligned with the true quality of submissions, surpassing individual human reviewers and AI Scientist systems (Lu et al., 2024) in delivering reliable, expert-level evaluation scores."}, {"title": "4.2 THE IMPORTANCE OF RESEARCH LIFECYCLE SIMULATION", "content": "Table 2 presents the results of CycleResearcher, which simulates a program committee review process, evaluating papers across the entire score range and ultimately providing a final acceptance decision. We report the average scores for the lowest-scoring reviewer, the highest-scoring reviewer, and the"}, {"title": "5 RELATED WORK", "content": "LLMs for Research. In recent years, several studies have explored using language models for creative tasks in research, such as multi-agent collaborative writing (Baek et al., 2024) and multi-module retrieval (Yang et al., 2023) to improve research idea generation. These works aim to boost the novelty and diversity of AI in creative tasks. Si et al. (2024) conducted a comprehensive human evaluation of the task of idea generation by language models. Wang et al. (2024b) proposed using LLMs to automatically write survey papers. Additionally, LLMs have been used to automate the research process: Huang et al. (2024) introduced a benchmark for evaluating LLMs in coding solutions for machine learning problems; Wang et al. (2023b) proposed a method leveraging LLMs for scientific literature retrieval. The AI Scientist project (Lu et al., 2024) introduced a fully automated, prompt-driven research pipeline. However, prompt-based methods often fail to generate ideas that are both diverse and practical, limiting their real-world application. To address this, we developed an iterative self-rewarding framework that enables the LLM to refine its ideas continuously, enhancing both diversity and practicality in research proposal generation.\nLLMs for Science Discovery. The tradition of AI-assisted scientific discovery (Langley, 1987; 2024) has a long history. As early as the last century, AI was applied in fields such as chemistry (Buchanan & Feigenbaum, 1981), synthetic biology (Jumper et al., 2021; Hayes et al., 2024), material discovery (Pyzer-Knapp et al., 2022; Merchant et al., 2023), and mathematics (Romera-Paredes et al., 2024). With the development of neural networks (LeCun et al., 2015), more researchers have focused on AI4Science (AI4Science & Quantum, 2023; LI, 2024; Yakaboski et al., 2023). AI is mainly used for data analysis within a single domain, playing a passive role without driving scientific discovery. The key challenge is enabling AI to go beyond analysis and actively contribute to generating new research ideas, which demands advanced reasoning and creativity. Our work builds on AI's historical role in science, aiming to shift AI from a supporting tool to a leader in scientific discovery.\nAutomated Evaluation of Research Papers. The use of AI tools in the scientific publishing process has garnered widespread attention (Bao et al., 2021; Liu & Shah, 2023; Liang et al., 2024; D'Arcy et al., 2024; Jin et al., 2024), including summarizing research paper content (Collins et al., 2017), detecting inaccuracies (Nuijten et al., 2016), and identifying fairness disparities (Zhang et al., 2022). Hosseini & Horbach (2023) conducted small-scale qualitative experiments to evaluate the effectiveness of ChatGPT in the peer review process, while Robertson (2023) invited 10 participants to assess the benefits of GPT-4 in assisting with peer review. Lu et al. (2024) and Tyser et al. (2024) used GPT-4 to evaluate full-text PDFs of scientific papers. However, when LLMs act as judges, even the most advanced models, such as GPT-4 (Achiam et al., 2023) and Gemini (Reid et al., 2024), still lag behind reward models specifically trained for the task, as seen in RewardBench (Lambert et al., 2024). This gap highlights the challenge of achieving human-level judgment and reasoning in AI-driven peer reviews. In contrast, we train a Generative Reward Model (Zhang et al., 2024) to simulate a comprehensive peer review. Our CycleReviewer simulates reviewers with varying perspectives, documenting summaries, strengths, and weaknesses. In the final stage, a primary reviewer consolidates these insights to deliver the final decision."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduced a novel framework for automating the entire research lifecycle using large language models (LLMs). Our approach combines CycleResearcher, a policy model designed to autonomously conduct scientific research, and CycleReviewer, a reward model that simulates the peer review process. Through the integration of Iterative SimPO, we enable the models to self-improve over multiple research-review-revision cycles. To facilitate this, we constructed two new datasets, Review-5k and Research-14k, which capture the complexities of peer review and research paper writing in machine learning. Our experimental results demonstrate significant advancements in research evaluation, with CycleReviewer surpassing human experts in scoring consistency. Moreover, the CycleResearcher models showed notable progress in generating research papers that align closely with human standards, achieving an acceptance rate comparable to human-generated submissions. These results indicate the feasibility of using LLMs to contribute meaningfully to both the scientific discovery and peer review processes. As we move forward, the potential of LLMs to transform research practices is vast. We hope this work sparks further investigation into how AI can assist researchers, while maintaining the highest standards of academic integrity and ethical responsibility."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We have made extensive efforts to ensure the reproducibility of all results presented in this paper. Firstly, the models discussed in this work, including CycleResearcher and CycleReviewer, will be made available as open-source, along with detailed documentation for setup and usage (See in Section 3.1, Section 3.2, and Appendix D). We provide the training datasets-Review-5k and Research-14k-which will be made publicly accessible to enable researchers to replicate the training process. Each dataset is accompanied by clear instructions regarding its collection, preprocessing steps, and structure (See in Section 2).\nAdditionally, we have included a thorough description of the model architectures, training procedures, and hyperparameters used in our experiments. Furthermore, we conducted all experiments using publicly available hardware and commonly used deep learning frameworks such as DeepSpeed. To further enhance transparency, we have included a detailed breakdown of evaluation metrics, such as Proxy MAE and Proxy MSE, to ensure that our performance claims can be independently verified. All code, datasets, and model weights will be released with a clear license to promote widespread reproducibility and ethical usage."}, {"title": "ETHICAL CONSIDERATIONS", "content": "While our primary objective in this work is to advance the automation of the research process via large language models (LLMs), it is crucial to clarify that we are not advocating for the use of LLMs in the malicious creation of academic papers, nor do we endorse using these models in ways that would undermine the integrity of the academic community. Recognizing the potential ethical risks associated with CycleResearcher and CycleReviewer models, we have taken deliberate measures to mitigate their misuse and protect the community from potential harm.\nFirstly, we have implemented a high-performance detection tool capable of identifying whether a submission is generated by our distributed models with an accuracy exceeding 95%. This tool provides the community with a means of distinguishing human-written papers from AI-generated content, ensuring transparency and accountability. In addition, we have embedded explicit watermarking into all model-generated outputs. For instance, papers generated with the assistance of CycleResearcher will carry a clear disclosure, such as:\nThis paper was written with the assistance of CycleResearcher, including but not limited to the introduction, related work, experimental design, and experimental results sections. A portion of the content may have been generated using large language models (LLMs).\nThis watermarking ensures that model use is always disclosed, maintaining honesty about the role LLMs play in the research process.\nSecondly, we have conducted multiple rounds of red-team exercises to rigorously test the boundaries of potential risks posed by the models. These exercises evaluated the extent to which CycleResearcher could be exploited to assist in harmful activities, such as acting as an agent in cyber-attacks, developing algorithms to compromise computer systems, or providing knowledge about the creation and enhancement of chemical, biological, radiological, nuclear, and explosive weapons. To prevent the model from providing such information unlawfully, we implemented a robust safety mechanism, SafetyLock (Zhu et al., 2024), prior to releasing any open-source weights. SafetyLock ensures that the model complies with legal and ethical standards, preventing the use of these models for any illegal or unethical purposes.\nLastly, in preparation for the open-source release, we have established a stringent licensing agreement. This agreement requires researchers to disclose their institutional affiliations when requesting access to the model weights, which we will treat with strict confidentiality. Furthermore, the license obligates users to agree not to utilize the models for official peer reviews or submissions without full disclosure of AI involvement, safeguarding the integrity of the academic publishing process.\nBy implementing these measures, we aim to contribute positively to the research community, fostering innovation while ensuring ethical responsibility in the development and application of LLMs for scientific discovery."}, {"title": "A LIMITATIONS", "content": "Our models are text-only transformer models (Vaswani et al., 2017), focused on handling LaTeX-style text content, but it does not include specific image information. While our models focus on the processes of research planning and academic writing, the envisioned LLM-led scientific discovery does not imply an isolated deployment. Instead, it should function as part of an integrated system. Crucially, human researchers or other agents are still needed to execute the experiments designed by the models and provide corresponding experimental details. We explicitly state that all experimental"}, {"title": "B ADDITIONAL EXPERIMENTS", "content": "B.1 FURTHER ANALYSIS OF MODEL PERFORMANCE"}, {"title": "B.2 LITERATURE REVIEW EXPERIMENT", "content": "In this subsection, we evaluate the ability of LLMs to generate research papers with substantial and relevant literature citations. Properly citing a wide range of references is essential in academic writing, as it not only demonstrates a comprehensive understanding of the field but also provides a basis for supporting claims and arguments. Therefore, we compare the quantity of references included in papers generated by CycleResearcher-12B and AI Scientist."}, {"title": "C PROXY MSE AS AN EVALUATION METRIC", "content": "In peer review, one of the key challenges is assessing the accuracy of review scores in estimating the true quality of a submission, since the ground truth is unknown. To overcome this limitation, we use proxy metrics such as Proxy Mean Squared Error (Proxy MSE) and Proxy Mean Absolute Error (Proxy MAE) to evaluate the performance of review scores, denoted as y. These proxy metrics provide a meaningful approximation by leveraging the assumption that multiple independent review scores for the same submission can act as unbiased estimators of its true quality."}, {"title": "C.1 PROXY MSE DERIVATION", "content": "Let y1, y2,..., yn represent the review scores given by n independent reviewers. For any submission, assume that y\u2081 is the review score of interest (i.e., the score we are evaluating), and that the average of the remaining scores y2, y3, ..., yn is a reasonable proxy for the \"true\" score of the submission. Denote this average as \u04ef', defined as:\n\u04ef' = \\frac{1}{n-1} \\sum_{i=2}^{n} Y_i"}, {"title": "C.2 UNBIASEDNESS AND BIAS OF PROXY MSE", "content": "Although y' is not the true ground truth", "as": "nE[(y1 - \u04ef')\u00b2", "E[\u1ef9'": "", "\u00b2": "Var(y')\nHere, the bias in Proxy MSE is equal to the variance of y', which we refer to as the \u201cnoisy target.\u201d This additional variance causes"}]}