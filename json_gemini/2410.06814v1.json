{"title": "DEFENDING MEMBERSHIP INFERENCE ATTACKS VIA PRIVACY-AWARE SPARSITY TUNING", "authors": ["Qiang Hu", "Hengxiang Zhang", "Hongxin Wei"], "abstract": "Over-parameterized models are typically vulnerable to membership inference attacks, which aim to determine whether a specific sample is included in the training of a given model. Previous Weight regularizations (e.g., l\u2081 regularization) typically impose uniform penalties on all parameters, leading to a suboptimal tradeoff between model utility and privacy. In this work, we first show that only a small fraction of parameters substantially impact the privacy risk. In light of this, we propose Privacy-aware Sparsity Tuning (PAST)\u2014a simple fix to the l1 Regular-ization-by employing adaptive penalties to different parameters. Our key idea behind PAST is to promote sparsity in parameters that significantly contribute to privacy leakage. In particular, we construct the adaptive weight for each parameter based on its privacy sensitivity, i.e., the gradient of the loss gap with respect to the parameter. Using PAST, the network shrinks the loss gap between members and non-members, leading to strong resistance to privacy attacks. Extensive experiments demonstrate the superiority of PAST, achieving a state-of-the-art balance in the privacy-utility trade-off.", "sections": [{"title": "1 INTRODUCTION", "content": "Modern neural networks are trained in an over-parameterized regime where the parameters of the model exceed the size of the training set (Zhang et al., 2021). While the huge amount of parameters empowers the models to achieve impressive performance across various tasks, the strong capacity also makes them particularly vulnerable to membership inference attacks (MIAs) (Shokri et al., 2017). In MIAs, attackers aim to detect if a sample is utilized in the training of a target model. Membership inference can cause security and privacy concerns in cases where the target model is trained on sensitive information, like health care (Paul et al., 2021), financial service (Mahalle et al., 2018), and DNA sequence analysis (Arshad et al., 2021). Therefore, it is of great importance to design robust training algorithms for over-parameterized models to defend against MIAs.\nThe main challenge of protecting against MIAs stems from the extensive number of model parameters, allowing to easily disclose the information of training data (Tan et al., 2022a). Therefore, previous works reduce the over-complexity of neural networks by weight regularization, like l\u2081 or l2 regularization. These regularization techniques impose uniform penalties on all parameters with large values, reducing the overfitting to the training data. However, if not all parameters contribute equally to the risk of leaking sensitive information, the uniform penalties can lead to a suboptimal tradeoff between model utility and privacy. The question is:\nAre all parameters equally important in terms of privacy risk?\nIn this work, we answer this question by an empirical analysis of parameter sensitivity in terms of privacy risk. In particular, we take the loss gap between member and non-member examples as a proxy for privacy risk and compute its gradient with respect to each model parameter. We find that only a small fraction of parameters substantially impact the privacy risk, whereas the majority have little effect. Thus, applying uniform penalties to all parameters is inefficient to defend against MIAS and may unnecessarily restrict the model's capacity."}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 BACKGROUND", "content": "Setup In this paper, we study the problem of membership inference attacks in K-class classification tasks. Let the feature space be X C Rd and the label space be y = {1, ..., K}. Let us denote by (x, y) \u2208 (X \u00d7 V) an example containing an instance \u00e6 and a real-valued label y. Given a training dataset S = {(xn, Yn)}=1 i.i.d. sampled from the data distribution P, our goal is to learn a model he with trainable parameters \u03b8\u2208 RP, that minimizes the following expected risk:\n$R(h_{\\theta}) = E_{(x,y)\\sim P}[L(h_{\\theta}(x), y)]$\nwhere $E_{(x,y)~p}$ denotes the expectation over the data distribution P and L is a conventional loss function (such as cross-entropy loss) for classification. In modern deep learning, the neural network he is typically over-parameterized, allowing to easily disclose the information of training data (Tan et al., 2022a).\nMembership Inference Attacks Given a data point (x, y) and a trained target model hs, attackers aim to identify if (x, y) is one of the members in the training set S, which is called membership inference attacks (MIAs) (Shokri et al., 2017; Yeom et al., 2018; Salem et al., 2019). In MIAs, it is generally assumed that attackers can query the model predictions he(x) for any instance \u0445. Here, we focus on standard black-box attacks (Irolla & Ch\u00e2tel, 2019), where attackers can access the knowledge of model architecture and the data distribution P.\nIn the process of attack, the attacker has access to a query set Q = {(zi, mi)}_1, where zi denotes the i-th data point (xi, Yi) and mi is the membership attribute of the given data point (xi, Yi) in the training dataset S, i.e., mi = [[(xi, Yi) \u2208 S]. In particular, the query set Q contains both member (training) and non-member samples, drawn from the data distribution P. Then, the attacker A can be formulated as a binary classifier, which predicts mi \u2208 {0,1} for a given example (xi, Yi) and a target model ho: A(xi, Yi; h\u00f8) \u2192 {0,1}."}, {"title": "3 METHOD: PRIVACY-AWARE SPARSITY TUNING", "content": "In this section, we start by analyzing the privacy sensitivity of model parameters and find that most parameters contribute only marginally to the privacy risk. Subsequently, we design a weighted l1 regularization that takes into account the privacy sensitivity of each parameter."}, {"title": "3.1 MOTIVATION", "content": "In this part, we aim to figure out whether the model parameters are equally important in terms of privacy risk. In particular, we perform standard training with ResNet-18 (He et al., 2016) on CIFAR-10 (Krizhevsky et al., 2009). We train the models using SGD with a momentum of 0.9, a weight decay of 0.0005, and a batch size of 128. We set the initial learning rate to 0.01 and decrease it using a cosine scheduler (Loshchilov & Hutter, 2017) throughout the training. In the analysis, we construct the datasets of members Sm and nonmembers Sn by randomly sampling two subsets with 10000 examples each from the training set and the test set, respectively."}, {"title": "3.2 METHOD", "content": "In the previous analysis, we demonstrate that the privacy risk can be alleviated by reducing the number of effective parameters with weight regularization techniques. Moreover, we show that most parameters contribute only marginally to the privacy risk, suggesting that the weight regularization can be focused on the most sensitive parameters. Thus, our key idea is to promote sparsity specifically within the subset of parameters that significantly contribute to privacy leakage.\nPrivacy-Aware Sparsity Tuning In this work, we introduce Privacy-Aware Sparsity Tuning (dubbed PAST), a simple fix to l\u2081 regularization that employs adaptive penalties to different pa-rameters in a deep neural network. Formally, the objective function of PAST is given by:\n$R_{PAST}(h_{\\theta}) = E_{(x,y)\\sim P}[L(h_{\\theta}(x), y)] + \\lambda R(h_{\\theta})$\n$ = E_{(x,y)\\sim P}[L(h_{\\theta}(x), y)] + \\lambda \\sum_{i} \\gamma_{i} |\\theta_{i}|$\nwhere X is the hyperparameter that controls the importance of the regularization term and Yi denotes the adaptive weight of the parameter 0\u2081. We expect larger weights for those parameters with higher privacy sensitivity, and smaller weights for those with lower sensitivity. Using the l\u2081 norm, the regularization can encourage those sensitive parameters to be zero, thereby improving the defense performance against MIAs.\nIn particular, we modulate the intensity of l\u2081 regularization for model parameters based on their privacy sensitivity, i.e., the gradient of the loss gap with respect to the parameters. Let Sm and Sn denote the subsets of members and non-members, respectively. For notation shorthand, we use Go to denote the loss gap G(Sm, Sn; ho) of the model ho on Sm and Sn. Then, we compute the normalized privacy sensitivity of each parameter \u03b8\u2081 in its associated module (e.g., linear layer):\n$\\gamma_{i} = \\frac{|M(\\theta_{i})|\\nabla_{\\theta_{i}}G_{0}}{\\sum_{\\theta_{j} \\in M(\\theta_{i})}|\\nabla_{\\theta_{j}}G_{0}|}$\nwhere M(0) denotes the associated module of the parameter 0\u2081. Equipped with the adaptive weight, the final regularization of PAST is :\n$R(h_{\\theta}) = \\sum_{i}\\gamma_{i}^{\\alpha} |\\theta_{i}|,$\nwhere a is the focusing parameter that adjusts the rate at which sensitive parameters are up-weighted. When a = 0, the regularization is equivalent to the standard l\u2081 regularization. As a"}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 SETUP", "content": "Datasets In our evaluation, we employ five datasets: Texas 100 (Texas Department of State Health Services, 2006), Purchase100 (Kaggle, 2014), CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), and ImageNet (Russakovsky et al., 2015). We split each dataset into six subsets, with each subset alternately serving as the training, testing, or inference set for the target and shadow models. The inference set was used by our method and adversarial training algorithms that incorporate adversary loss such as Mixup+MMD (Li et al., 2021) and adversarial regularization (Nasr et al., 2018). In our method, the inference set was used to obtain the comparison information between members and non-members. For other methods, the inference set is specifically utilized to generate adversary loss.\nTraining details We train the models using SGD with a momentum of 0.9, a weight decay of 0.0005, and a batch size of 128. We set the initial learning rate to 0.01 and drop it using a cosine scheduler (Loshchilov & Hutter, 2017) with Tmax = epochs. For CIFAR-10, we conduct training using an 18-layer ResNet (He et al., 2016), with 100 epochs of standard training and 50 epochs of sparse tuning. In the case of ImageNet and CIFAR-100, we employ a 121-layer DenseNet (Huang"}, {"title": "4.2 RESULTS", "content": "Can PAST improve privacy-utility trade-off? In Figure 3 and Figure 4, we plot privacy-utility curves to show the privacy-utility trade-off. The horizontal axis represents the performance of the target model, and the vertical axis represents the attack advantage defined in Equation (5). A salient observation is that our method drastically improves the privacy-utility trade-off. In particular, for these points that perform better than vanilla for utility (the area to the right of the dotted line), the privacy-utility curves of our methods are always below those of others. This means we can always obtain the highest privacy for any utility requirement higher than the undefended model. For example, on the CIFAR10, we focus on the hyperparameter a corresponding to the model with the lowest attack advantage with the constrain condition that test accuracy is better than vanilla, then our method with adaptive regularization can decrease the attack advantage of loss-metric-based from 14.8% to 5.2% compared with MixupMMD (the most powerful defense under our condition above).\n$Adv(A) := \\frac{Pr(A(h_{\\theta}(x), y) = 1|m = 1)}{Pr(A(h_{\\theta}(x), y) = 1|m = 0)}$\n$= 2Pr(A(h_{\\theta}(x), y) = m) - 1$\nwhere the notations are defined in Section 2.1. To assess the trade-off between utility and privacy, we utilize the P\u2081 score (Paul et al., 2021), which is defined as:\n$P_{1} = 2 \\times \\frac{Acc \\times (1 - Adv)}{Acc + (1 - Adv)}$\nwhere Acc denotes test accuracy and Adv denotes attack advantage on the target model."}, {"title": "5 RELATED WORK", "content": "Overparameterization in generalization and privacy Overparameterization, where models have significantly more parameters than training examples, has been shown to have a complex relation-ship with generalization and privacy. While traditional theories suggest that overparameterization increases overfitting and generalization error, recent research reveals that it can sometimes reduce error under certain conditions, such as in high-dimensional ridgeless least squares problems (Belkin et al., 2020). This phenomenon, known as \u201cdouble descent\u201d, suggests that beyond a critical point, increasing model complexity may lead to better generalization (Belkin et al., 2019; Dar et al., 2021; Hastie et al., 2022). However, from a privacy perspective, overparameterization has been empir-ically proven to increase vulnerability to membership inference attacks (MIAs) (Leemann et al., 2023; Dionysiou & Athanasopoulos, 2023). Large language models, in particular, are susceptible to these attacks, with attackers able to extract sensitive training data (Carlini et al., 2021; Mireshghal-lah et al., 2022). Theoretical evidence also indicates that there is a clear parameter-privacy trade-off, where an increase in the number of parameters amplifies the privacy risks by enhancing model memorization (Yeom et al., 2018; Tan et al., 2022b). Consequently, while overparameterization can sometimes improve generalization, its impact on privacy remains a significant concern, especially in the context of MIAs.\nOverparameterization in MIA defenses To mitigate the privacy risks associated with overpa-rameterization, several defense mechanisms have been proposed. One effective approach is network pruning, where unnecessary parameters are removed to reduce model complexity. Research shows that pruning not only preserves utility but also significantly reduces the risk of privacy leakage, in scenarios including MIA (Huang et al., 2020; Wang et al., 2021) and Unlearning (Hooker et al., 2019; Wang et al., 2022; Ye et al., 2022; Liu et al., 2024). Additionally, techniques combining prun-ing with federated unlearning have demonstrated effectiveness in protecting privacy by selectively forgetting specific data during the training process (Wang et al., 2022). Regularization methods, such as l2 regularization (Kaya et al., 2020), sparsification (Bagmar et al., 2021) and dropout (Galinkin, 2021), also play a critical role in defending against MIAs by discouraging the model from over-fitting to training data. Interestingly, while overparameterization generally increases privacy risks, when paired with appropriate regularization, it can maintain both utility and privacy (Tan et al., 2023). Furthermore, studies indicate that initialization strategies and ensemble methods can further alleviate privacy risks on over-parameterized model (Rezaei et al., 2021; Ye et al., 2024). These techniques illustrate that even in overparameterized models, privacy risks can be mitigated through careful design, preserving the balance between utility and privacy."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduce Privacy-aware Sparsity Tuning (PAST), a novel approach to mitigating membership inference attacks (MIAs) by adaptively regularizing model parameters based on the loss gap between member and non-member data. By promoting sparsity in parameters with large privacy sensitivity, the model shrinks the loss gap between members and non-members, leading to strong resistance to privacy attacks. Extensive experiments demonstrate that PAST effectively balances pri-vacy and utility, providing state-of-the-art performance in the privacy-utility trade-off. This method is straightforward to implement with existing deep learning frameworks and requires minimal mod-ifications to the training scheme. We hope that our insights into Privacy-aware regularization inspire further research to explore parameter regularization techniques for enhancing privacy in machine learning models.\nLimitations In this work, we focus on the popular black-box setting, where attackers can access the model outputs. So, the effectiveness of our method in defending against other types of MIAs (such as label-only attacks, white-box attacks) remains unexplored. Moreover, while our method can improve the MIA defense with high predictive performance, our method cannot fully break the trade-off between utility and MIA defense, which might be a potential direction for future work."}, {"title": "D FOR WHICH LAYERS AND MODULES ARE MORE EFFECTIVE?", "content": "As an example, we illustrate the average grad-gap dynamics during the PAST process across different modules of the ResNet18 model in Figure 9. It can be observed that the deeper layers are more effective than the earlier ones, and batch normalization (BN) and linear layers contribute more significantly than convolutional layers. Notably, the loss gap of all convolutional layers in the third and fourth blocks almost stabilizes at zero."}]}