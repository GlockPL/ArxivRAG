{"title": "Benchmark of Deep Learning-based Imaging PPG in Automotive Domain", "authors": ["Yuqi Tu", "Shakith Fernando", "Mark van Gastel"], "abstract": "Imaging photoplethysmography (iPPG) can be used for heart rate monitoring during driving, which is expected to reduce traffic accidents by continuously assessing drivers' physical condition. Deep learning-based iPPG methods using near-infrared (NIR) cameras have recently gained attention as a promising approach. To help understand the challenges in applying iPPG in automotive, we provide a benchmark of a NIR-based method using a deep learning model by evaluating its performance on MR-NIRP Car dataset. Experiment results show that the average mean absolute error (MAE) is 7.5 bpm and 16.6 bpm under drivers' heads keeping still or having small motion, respectively. These findings suggest that while the method shows promise, further improvements are needed to make it reliable for real-world driving conditions.\nIndex Terms-Imaging photoplethysmography, remote PPG, driver monitoring, deep learning", "sections": [{"title": "I. INTRODUCTION", "content": "In future vision of mobility safety [1], continuous mon- itoring of drivers' vital signs like heart rate is essential.\nTo achieve this goal, Imaging photoplethysmography (iPPG) is applied. iPPG estimates drivers' pulse signal in a non- contact manner by detecting subtle color changes in facial skin, captured by a camera installed in the vehicle. However, varying lighting conditions while driving significantly affect the results of RGB three-channel cameras largely. In contrast, a NIR camera with a specific optical density bandpass filter is more robust to light variation during driving[2], offering a promising alternative. Recent studies[3][4][5] have utilized NIR cameras in combination with using deep learning model to explore accurate drivers' heart rate estimation methods. To help understand the advances and challenges in drivers' heart rate estimation, we provide a benchmark of a representative deep learning-based approach[3] in this report on the MR- NIRP Car dataset[2]."}, {"title": "II. RELATED WORK", "content": "For deep learning-based iPPG using NIR camera, [3] pro- posed a novel U-net architecture[6]. Other approaches estimate heart rate (HR) using different techniques. In [4], an end-to- end method based on a 3D Convolutional Neural Network (3DCNN) and a Recurrent Neural Network (RNN) with im- ages as input is proposed. In [5], Li-Wen et al. proposed an approach training two deep learning modules, one encoder- decoder CNN-based model to estimate pulse waveform and a CNN-based HR estimator to derive HR from the estimated waveform. [7] modifies the 3DCNN from [4] and applies the maximum cross correlation (MCC) loss function to address the time offset existing between the estimated waveform and reference label.\nFor benchmark on iPPG of drivers, [8] evaluates various deep learning-based methods excluding [3]. According to the results in [8] and [3], [3] delivers a more accurate estimation. However, [3] only evaluates its performance under a few con- ditions by limited metrics. To provide a more comprehensive evaluation of the method in [3], this report presents a bench- mark demonstrating the method's performance under various conditions and using a broader set of metrics. In addition to the benchmark, we propose several variant methods and evaluate them, aiming to overcome the limitations we found in the approach of [3] and to explore alternative possibilities."}, {"title": "III. METHODOLOGY", "content": "Based on [3], this section introduces the deep learning ap- proach used for benchmarking. The approach consists mainly of two modules, a time series extraction module and deep neural architecture for PPG estimation. Details are explained as follows."}, {"title": "A. Time series extraction module", "content": "This module serves for i) feature extraction and ii) pro- cessing. Features are extracted from face videos. Different from [3], for each frame in the video, we localize 478 facial landmarks using MediaPipe[9] and select 23 regions of interest (ROIs) (where contain strongest PPG signals [10]) instead of 48."}, {"title": "B. Deep neural architecture for PPG estimation", "content": "U-net architecture[6] is applied in this module to do esti- mation. Figure.2 demonstrates the visualization of the archi- tecture. The deep neural network extracts the 300-length, 23- dimension time series, as described in section III. A.\nThis U-net model consists primarily of the downsampling section, the upsampling section, and the skip connection. The downsampling section is responsible for feature extraction and the upsampling section restores the resolution lost during downsampling. The skip connections link the downsampling and upsampling modules, aiding in the training process. In [3], it introduces a novel gated recurrent unit (GRU)-based, but such technique is not contained in this report due to its limited upgrade on performance.\nAs the frequency character is crucial for HR estimation, the goal for training can be interpreted as making the estimated time series linearly related to the ground-truth label. So, same as [3], the deep learning module trains to maximize the Pearson correlation between the estimated waveform and ground-truth label."}, {"title": "IV. EXPERIMENT", "content": "This section is composed of two parts. Experiment details will be elaborated in the experiment setup, including more information of the dataset, deep learning training configura- tion, and data augmentation. Experimental results demonstrate the experiment results and analysis. Following that are the complete results including motion scenarios. Analysis based on results is contained as well"}, {"title": "1) Experiment setup", "content": "Dataset: The MERL-Rice Near- Infrared Pulse (MR-NIRP) Car Dataset [2] is used for training and verification. This dataset contains different drivers' face video recorded with an NIR camera. The video frame rate is 30fps and is filtered with a 940 \u00b1 5 nm bandpass filter. There are 18 subjects in total and the video is recorded separately during two scenarios, either driving (city driving) or Garage (parked with engine running). Within each scenario, videos are separated in terms of head motion condition, this report contains test results of two motion levels, Still (driver's head keeps still), small motion. The ground-truth pulse waveform is collected by g a CMS 50D+ finger pulse oximeter recording at 60 fps and then downsampled to 30 fps to match face videos.\nTraining and test protocols: The train-test protocol we apply, as the same in [3], is leave-one-subject-out cross- validation. We trained the model for 8 epochs with batch size 96. Adam optimizer[11] is used with a learning rate $1.0\\times10^{-4}$ reducing after each epoch by a factor of 0.05.\nData Augmentation: Dataset contains HR ranging from 40 to 110 bpm, but it is unbalanced across HR. Most subjects' HR falls in the range [50,70] bpm. Considering the train-test protocol applied, once the extreme samples are put into the test dataset, performance suffers from the training set lacking such kind of data. To address such a problem, our implementation applies a data augmentation technique based on[3]. For both input time series and label, they are resampled with linear resampling rates 1+r and 1-r, where r is randomly chosen from the range [0.2,0.6]. This technique is also expected to compensate for the gaps in the distribution of subjects' HR.\nMetrics: The performance is evaluated in terms of 3 metrics. The first metric is PTE6 (percent of the time the error is less than 6 bpm). If the absolute error between estimation and ground-truth HR is less than 6, the estimation is regarded as a successful estimation. The threshold 6 is the expected frequency resolution of a 10-second window. The second metric is root-mean-squared error (RMSE), which is defined by calculating root-mean-squared error of each 10- second window and averaged over the test sequence. The third metric is mean-absolute error (MAE), which is defined by calculating the mean-absolute error of each 10-second window and averaging over the test sequence."}, {"title": "V. VARIANT METHODS", "content": "Taking [3] as the backbone, we implemented several vari- ants to improve the performance. Due to the time limit, we focus on Still condition in this report. Details are as follows.\nWindow Shift: Considering ground-truth labels and fea- tures used to estimate are extracted from different body parts (finger and face), there might be a time offset between these two signals [7]. To address this mismatch, we introduce a window shift technique. The goal of this method is to align the most linearly related sections of the label and the estimated waveform. Still using Pearson correlation, we apply the window shift and modify the loss function from [3] in two ways as explained below.\nI) After obtaining the estimated waveform in time series form, we truncate both the label and the estimated time series to the same length, denoted by x, where x iterates from 0 to 15 (other upper limits were also tested, yielding similar performance). However, the truncation starts from opposite ends for the two series, responsible for canceling out the offset. We then assess the linear relation by calculating the Pearson correlation between the label and the estimation for each value of x. When iteration and computation are done, the loss is then defined by the maximum correlation. This method is called WS-1.\nII) Still based on window shift, for this method, we enlarge the input series of the deep learning model to 350 frames, so the estimation series length is then also 350. However, the label series length remains the same. To match the label size, a moving 300-length window at stride 1 will iterate within the estimation series, similar to WS-1, and then it will select a 300- length continuous part of the estimation series which mostly linearly relates with the label series by comparing Pearson correlation and take the correlation value as loss. This method is proposed out of that method WS-1 might lead to inefficiency in dataset usage due to every window being truncated to a certain size. In addition, this method ensures that the HR is still estimated based on the same-length window as [3] all the time. We call this method as WS-2.\nFully Deep Learning-based (FDL) Estimation: We also explored a method that estimates HR completely by deep learning module instead of doing FFT following estimating pulse waveform. In the previous implementation, we found that loss is not always proportional to HR estimation perfor- mance, which can be explained by that correlation doesn't 100% inform the frequency characters of the waveform. Hence, training with this loss function might not be the most efficient option. To explore other possibilities, we tried training a deep learning module to directly estimate HR using another loss function. We first implemented this thought by simply adding a downsampling module composed of three fully con- nected layers after the U-net model described in section III.B and the loss function now is the mean square error (MSE). However the experiment result is much worse than what is shown in Table II. Considering the U-net model applied in III.B mainly aims to provide a high-resolution waveform and that the added downsampling module is not fully investigated, such performance is foreseeable. [5] proposed a deep learning architecture estimating HR whose input is pulse waveform, it's called CNN-Based HR Estimator and claims it's able to eliminate noise contained in the estimated waveform. To further investigate the performance of a fully Deep learning- based estimation method, we combine the HR estimator from [5] and the U-net model in section III.B. Since the output of the deep learning module is now HR, we apply MSE as loss function. Specifically, the loss is defined by MSE between a batch of estimated HR and a batch of label HR while label HR comes from the FFT result of the label pulse waveform. We tried training the U-net model and HR estimator together and only trained the HR estimator taking input from the trained U-net model. Only the latter method can provide comparable performance relative to the original implementation of [3] and it is shown in the next paragraph. We call this method FDL."}, {"title": "Experimental Results of Variant Methods", "content": "Table III compares three variant methods and original implementation (labeled with Benchmark). Data augmentation is not applied now due to it's its failure in previous implementation and the fact that it's a temporal solution to deal with a lack of data. As Table III implies, under Driving Still, WS-1 and WS- 2 provide better performance in terms of all three metrics, especially MAE. WS-1 and WS-2 reduce MAE by 12.6% and 9.2%, respectively. FDL also sees slight improvement in terms of RMSE and MAE, but PTE6 is 5.2% lower than the original implementation. In Garage Still, all methods provide comparable performance while the original implementation and WS-1 are slightly better in terms of PTE6.\nProposed variant methods also have limitations. For window shift-based methods, both WS-1 and WS-2 randomly initialize the parameters of the deep learning module. So it can be expected that time offset still exists, to be more specific, in that the estimated waveform of the very beginning stage is generated by initialization, selected label-estimation pair with the highest correlation is less meaningful and potentially misleading. This could cause the model to train in the wrong direction. For FDL, performance in terms of PTE6 under both conditions is noticeably lower than the benchmark, although the other two metrics remain comparable. This phenomenon can be explained by that large overlap between waveforms of neighboring windows leading to slow variation in FFT results, however, for the deep learning module of FDL, subtle changes in input could lead to large fluctuation on the estimation of HR, even if the average remains consistent, greater fluctuation increases the likelihood of more samples crossing the threshold set by PTE6."}, {"title": "VI. CONCLUSION", "content": "In this report, we provide a benchmark of deep learning- based imaging PPG during driving. We implement a U-net model-based method, propose variant methods, and test them under different situations. Analysis of the current method and dataset are also included. While estimation under certain conditions shows that deep learning-based imaging PPG is promising, performance in practical situations (driving on roads, drivers' heads moving, etc) still needs to improve."}]}