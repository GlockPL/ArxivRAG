{"title": "DASH: Warm-Starting Neural Network Training in Stationary Settings without Loss of Plasticity", "authors": ["Baekrok Shin", "Junsoo Oh", "Hanseul Cho", "Chulhee Yun"], "abstract": "Warm-starting neural network training by initializing networks with previously learned weights is appealing, as practical neural networks are often deployed under a continuous influx of new data. However, it often leads to loss of plasticity, where the network loses its ability to learn new information, resulting in worse generalization than training from scratch. This occurs even under stationary data distributions, and its underlying mechanism is poorly understood. We develop a framework emulating real-world neural network training and identify noise memorization as the primary cause of plasticity loss when warm-starting on stationary data. Motivated by this, we propose Direction-Aware SHrinking (DASH), a method aiming to mitigate plasticity loss by selectively forgetting memorized noise while preserving learned features. We validate our approach on vision tasks, demonstrating improvements in test accuracy and training efficiency.", "sections": [{"title": "1 Introduction", "content": "When training a neural network on a gradually changing dataset, the model tends to lose its plasticity, which refers to the model's ability to adapt to new information (Dohare et al., 2021; Lyle et al., 2023b; Nikishin et al., 2022). This phenomenon is particularly relevant in scenarios with non-stationary data distributions, such as reinforcement learning (Igl et al., 2020; Nikishin et al., 2022) and continual learning (Chen et al., 2023; Kumar et al., 2023; Wu et al., 2021). While requiring to overwrite outdated knowledge as the environment changes, models overfitted to previously encountered environments often struggle to cumulate new information, which in turn leads to reduced generalization performance (Lyle et al., 2023b). Under this viewpoint, various efforts have been made to mitigate the loss of plasticity, such as resetting layers (Nikishin et al., 2022), regularizing weights (Kumar et al., 2023), and modifying architectures (Lee et al., 2023; Lyle et al., 2023a; Nikishin et al., 2023).\nPerhaps surprisingly, a similar phenomenon occurs in supervised learning settings, even where new data points sampled from a stationary data distribution are added to the dataset during training. It is counterintuitive, as one would expect advantages in both generalization performance and computational efficiency when we warm-start from a model pre-trained on data points of the same distribution. For a particular example, when a model is pre-trained using a portion of a dataset and then we resume the training with the whole dataset, the generalization performance is often worse than a model trained from scratch (i.e., cold-start), despite achieving similar training accuracy (Ash and Adams, 2020; Berariu et al., 2021; Igl et al., 2020). Liu et al. (2020) report a similar observation: training neural networks with random labels leads to a spurious local minimum which is challenging to escape from, even when retraining with a correctly labeled dataset. Interestingly, Igl et al. (2020) found that pre-training with random labels followed by the corrected dataset yields"}, {"title": "1.1 Our Contributions", "content": "In this work, we aim to explain why warm-starting leads to worse generalization compared to cold- starting, focusing on the stationary case. We propose an abstract framework that combines the popular feature learning framework initiated by Allen-Zhu and Li (2020) with a recent approach by Jiang et al. (2024) that studies feature learning in a combinatorial and abstract manner. Our analysis suggests that warm-starting leads to overfitting by memorizing noise present in the newly introduced data rather than learning new features.\nInspired by this finding, we propose Direction-Aware SHrinking (DASH), which aims to encourage the model to forget memorized noise without affecting previously learned features. This enables the model to learn features that cannot be acquired through warm-starting alone, enhancing the model's generalization ability. We validate DASH using an expanding dataset setting, similar to the approach in Ash and Adams (2020), employing various models, datasets, and optimizers. As an example,"}, {"title": "1.2 Related Works", "content": "Loss of Plasticity. Research has aimed to understand and mitigate loss of plasticity in non-stationary data distributions. Lewandowski et al. (2023) explain that loss of plasticity co-occurs with a reduction in the Hessian rank of the training objective, while Sokar et al. (2023) attribute it to an increasing number of inactive neurons during training. Lyle et al. (2023b) find that changes in the loss landscape curvature caused by non-stationarity lead to loss of plasticity. Methods addressing this issue in non-stationary settings include recycling dormant neurons (Sokar et al., 2023), regularizing weights towards initial values (Kumar et al., 2023), and combining techniques (Lee et al., 2023) like layer normalization (Ba et al., 2016), Sharpness-Aware Minimization (SAM) (Foret et al., 2020), resetting layers (Nikishin et al., 2022), and Concatenated ReLU activation (Shang et al., 2016)."}, {"title": "2 A Framework of Feature Learning", "content": null}, {"title": "2.1 Motivation and Intuition", "content": "We present the motivation and intuition behind our framework before delving into the formal description. Our framework captures key characteristics of image data, where the input includes both label-relevant information (referred to as features, e.g., cat faces in cat images) and label-irrelevant information (referred to as noise, e.g., grass in cat images). A key intuition is that minimizing training loss involves two strategies: learning features and memorizing noise. This framework builds on insights from Shen et al. (2022) and integrates them into a discrete learning framework. We provide more detailed intuition on our framework, including the training process, in Appendix A.\nShen et al. (2022) consider a neural network trained on data with features of different frequencies and noise components stronger than the features. The negative gradient of the loss for each single data point aligns more with the noise than the features due to the larger scale of noise, making the model more likely to memorize noise rather than learn features. However, an identical feature appears in many data points, while noise appears only once and does not overlap across data points. Thus, if a feature appears at a sufficiently high frequency in the dataset, the model can learn the feature. Thus, the learning of features or noise depends on the frequency of features and the strength of the noise.\nInspired by Shen et al. (2022), we propose a novel discrete feature learning framework. This section introduces a framework describing a single experiment, while Section 3 analyzes expanding dataset scenarios. As our focus is on gradually expanding datasets, carrying out the (S)GD analysis over many experiments as in Shen et al. (2022) is highly challenging. Instead, we adopt a discrete learning process similar to Jiang et al. (2024) but propose a more intricate process reflecting key ideas from Shen et al. (2022). In doing so, we generalize the concept of plasticity loss and analyze it without assuming any particular hypothesis class for a more comprehensive understanding, whereas existing works are limited to specific architectures."}, {"title": "2.2 Training Process", "content": "We consider a classification problem with C classes, and data are represented as $(x, y) \\in \\mathcal{X} \\times [C]$, where $\\mathcal{X}$ denotes the input space. A data point is associated with a combination of class-dependent features $V(x) \\subset \\mathcal{S}_y$ where $\\mathcal{S}_c = {V_{c,1}, V_{c,2}, ..., V_{c, K}}$ is the set of all features for each class $c \\in [C]$. Also, every data point contains point-specific noise which is class-independent.\nThe model $f: \\mathcal{X} \\rightarrow [C]$ sequentially learns features based on their frequency. The training process is described by the set of learned features $\\mathcal{L} \\subset \\mathcal{S} \\equiv \\bigcup_{c\\in [C]} \\mathcal{S}_c$ and the set of data points with non-zero"}, {"title": "2.3 Discussion on Training Process", "content": "In our framework, the model selects features based on their frequency in the set of unclassified data points $\\mathcal{N}$. The intuition behind this approach is that features appearing more frequently in the set of data points will have larger gradients, leading to larger updates, and we treat $g(v; T, N)$ as a proxy of the gradient for a particular feature $v$. As a result, the model prioritizes sequentially learning these high-frequency features. However, if the frequency $g(v; T,N)$ of a particular feature $v$ is not sufficiently large, such that the total occurrence of $v$ is less than the strength of the noise, i.e., $|T| \\cdot g(v; T,N) < \\gamma$, the model will struggle to learn that feature. Consequently, the model will prioritize learning the noise over the informative features. When this situation arises, the learning procedure becomes sub-optimal because the model fails to capture the true underlying features of the data and instead memorizes the noise to achieve high training accuracy.\nThe threshold $\\tau$ determines when a data point is considered well-classified and acts as a proxy for the dataset's complexity. A higher $\\tau$ requires the model to learn more features for correct predictions, while a lower $\\tau$ allows accurate predictions with fewer learned features.\nRemark 2.1. We believe our analysis can be extended to scenarios where feature strength varies across data by treating the set of features as a multiset, where multiple instances of the same element are allowed. The analyses in these cases are nearly identical to ours; therefore, we assume all features have identical strengths for notational simplicity."}, {"title": "3 Warm-Starting versus Cold-Starting, and a New Ideal Method", "content": null}, {"title": "3.1 Experiments with Expanding Dataset", "content": "In this section, we set up the scenario where the dataset grows after each experiment in our learning framework, allowing us to compare warm-start, cold-start, and a new ideal method, which will be defined later in Sections 3.3 and 3.4.\nTo better understand the loss of plasticity under stationary data distribution, we consider an extreme form of stationarity where the frequency of each feature combination remains constant in each"}, {"title": "3.2 Prediction Process and Training Time", "content": "We provide a comparison of three initialization methods based on test accuracy and training time. To evaluate these metrics within our framework, we define the prediction process and training time.\nPrediction Process. The model predicts unseen data points by comparing the learned features with features present in a given data point $x$. If the overlap between the learned feature set $L$ and the features in $x$, denoted as $V(x)$, is at least $\\tau$, i.e., $|V(x) \\cap L| \\geq \\tau$, the model correctly classifies the data point. Otherwise, the model resorts to random guessing.\nTraining Time. Accurately measuring training time within our discrete learning framework is challenging. To address this, we introduce an alternative for training time of j-th experiment: the number of training data points with non-zero gradients at the start of j-th experiment, $|\\mathcal{N}^{(j,0)}|$. This represents the amount of \"learning\" required for the model to classify all data points correctly. We empirically validated this proxy in practical scenarios additionally, Nakkiran et al. (2021) observe that in real-world neural network training, when other components are fixed, the training time increases with the number of data points to learn."}, {"title": "3.3 Comparison Between Warm-Starting and Cold-Starting in Our Framework", "content": "Now we analyze the warm-start and cold-start initialization methods within our framework, focusing on test accuracy and training time. We note that, by definition, $L_{cold}^{(j,0)}$ and $M_{cold}^{(j,0)}$ are both empty sets, while $L_{warm}^{(j,0)} := L_{warm}^{(j-1)}$ and $M_{warm}^{(j,0)} := M_{warm}^{(j-1)}$, where $s_j$ denotes the last step of j-th experiment. Besides, we use a shorthand notation for step $s_j$ of the experiment $j$ that we drop s if $s = s_j$ (e.g., $L^{(j)} := L^{(j,s_j)}$). For the detailed algorithms based on our learning framework, see Algorithms 3 and 4 in Appendix E."}, {"title": "3.4 An Ideal Method: Retaining Features and Forgetting Noise", "content": "In Section 3.3, we observed a trade-off between warm-starting and cold-starting. Cold-starting often achieves better test accuracy compared to warm-starting, while warm-starting requires less time to converge. The results suggest that neither retaining all learned information nor discarding all learned information is ideal. To address this trade-off and get the best of both worlds, we consider an ideal algorithm where we retain all learned features while forgetting all memorized data points. For any experiment $J > 2$, if we consider the ideal initialization, learned features $L_{ideal}^{(J,0)} := L_{ideal}^{(J-1)}$ are retained, and memorized data points $M_{ideal}^{(J,0)}$ are reset to an empty set. Pseudo-code for this method is given in Algorithm 5, which can be found in Appendix E. We define $T_{ideal}^{(J)} = \\sum_{j \\in [J]} |\\mathcal{N}_{ideal}^{(j,0)}|$, as the training time with the ideal method, where $\\mathcal{N}_{ideal}^{(j,0)}$ represents the set of data points having a non-zero gradient at the initial step of the j-th experiment. Then, we have the following theorem:\nTheorem 3.6. For any experiment $J \\geq 2$, the following holds:\n$ACC (L_{cold}^{(J)}) = ACC (L_{ideal}^{(J)}), T_{warm}^{(J)} < T_{ideal}^{(J)} < T_{cold}^{(J)}$\nThe detailed proof is provided in Appendix D. The ideal algorithm addresses the trade-off between cold-starting and warm-starting. We conducted an experiment to investigate the performance gap between these initialization methods.\nSynthetic Experiment. To verify our theoretical findings in more realistic scenarios, we conducted an experiment that more closely resembles real-world settings. Instead of fixing the frequency of each feature set, we sampled each feature's existence from a Bernoulli distribution to construct $V(x)$. This ensures that the experiment is more representative of real-world scenarios. Specifically, for each data point $(x, y)$, we uniformly sampled $y \\in {0,1}$. From the feature set $S_y$ corresponding to the sampled class $y$, we sampled features where each feature's existence follows a Bernoulli distribution, $1 (v_{y,k} \\in V(x)) \\sim Ber(p_k)$, for all $v_{y,k} \\in S_y$. This approach allows us to model the variability in feature occurrence that is commonly observed in real-world datasets while still maintaining the core principles of our learning framework. We set the number of features, $K = 50$, with $p_k$ sampled from a uniform distribution, $U(0, 0.2)$. Each chunk contained 1000 data points with total 50 experiments, with $\\gamma = 50$, $\\tau = 3$. We sampled 10000 test data from the same distribution.\nAs shown in the results align with the above theorems. Random initialization, i.e. cold- starting, and ideal initialization achieve almost identical generalization performance, outperforming warm initialization. However, with warm initialization, the model converges faster, as evidenced by the number of non-zero gradient data points, which serves as a proxy for training time. Ideal initialization requires less time compared to cold-starting, which is also consistent with Theorem 3.6. Due to the sampling process in our experiment, we observe a gradual increase in the number of learned features and test accuracy in warm-starting, mirroring real-world observations. These findings remained robust across diverse hyperparameter settings"}, {"title": "4 DASH: Direction-Aware SHrinking", "content": "The ideal method recycles memorized training samples by forgetting noise while retaining learned features. From now on, we shift our focus to a practical scenario: training neural networks with real-world data. This brings up the question of whether such an ideal approach can be applied in real-world settings. To address this, we propose our algorithm, Direction-Aware SHrinking (DASH), which intuitively captures this idea in practical training scenarios. When new data is introduced, DASH shrinks each weight based on its alignment with the negative gradient of the loss calculated from the training data, placing more emphasis on recent data.\nIf the degree of alignment is small (i.e., the cosine similarity is close to or below 0), we consider that the weight has not learned a proper feature and shrinks it significantly to make it \"forget\" learned information. This allows weights to forget memorized noises and easily change their direction. On the other hand, if the weight and negative gradient are well-aligned (i.e., the cosine similarity is close to 1), we consider it learned features and we shrink the weight to a lesser degree to maintain the learned information. This method aligns with the intuition of the ideal method, as it allows us to shrink weights that have not learned proper information while retaining weights that have learned commonly observed features."}, {"title": "Algorithm 1 Direction-Aware SHrinking (DASH)", "content": "Require:\n\u2022 Model $f_{\\theta}$ with list of parameters $\\theta$ after the (j-1)- th experiment\n\u2022 Training data points $\\mathcal{T}_{1:j}$\n\u2022 Averaging coefficient $0 < \\alpha < 1$\n\u2022 Threshold $\\lambda > 0$\n1: Initialize:\n$G_{\\theta}^{(j)} \\leftarrow 0, \\forall \\theta \\in \\Theta$\n2: for i in 1: j do\n3: $\\mathcal{L} \\leftarrow Loss(f_{\\theta}, \\mathcal{T}_i)$\n4: $U_{\\theta} \\leftarrow -\\nabla_{\\theta}\\mathcal{L}(\\theta; \\mathcal{T}_{1:j})$\n5: end for\n6: for $\\theta \\in \\Theta$ do\n7: $G_{\\theta} \\leftarrow (1 - \\alpha) \\times G_{\\theta}^{(j-1)} + \\alpha \\times U_{\\theta}$\n8: end for\n9: for $\\theta \\in \\Theta$ do\n10: $se \\leftarrow CosineSimilarity(\\mathcal{G}_{\\theta}, \\theta)$\n11: $\\theta \\leftarrow \\theta \\times max{\\lambda, se}$\n12: end for\n13: return model $f_{\\theta}$, initialized for the j-th experiment\nThe shrinking is done per neuron, where the incoming weights are grouped into a weight vector denoted as $\\theta$. For convolutional filters, the height and width of the kernel are flattened to form a single weight vector $\\theta$ for each pair of input and output filters. DASH has two hyperparameters: $\\lambda$ and $\\alpha$. Hyperparameter $\\lambda$ is the minimum shrinkage threshold, as each weight vector is shrunk by $max{\\lambda, cosine \\_ sim}$, while $\\alpha$ denotes the coefficient of exponential moving average of per-chunk loss gradients. Lower $\\alpha$ value gives more weight to previous gradients, resulting in less shrinkage. This"}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Experimental Details", "content": "Our setup is similar to the one described in Ash and Adams (2020). We divided the training dataset into 50 chunks, and at the beginning of each experiment, a chunk is added to the existing training data. Models were considered converged and each experiment was terminated when training accuracy reached 99.9%, aligning with our learning framework. We conducted experiments with vanilla training i.e. without data augmentations, weight decay, learning rate schedule, etc. We evaluated DASH on Tiny-ImageNet, CIFAR-10, CIFAR-100, and SVHN using ResNet-18, VGG-16, and three-layer MLP architectures with batch normalization layer. Models were trained using Stochastic Gradient Descent (SGD) and Sharpness-Aware Minimization (SAM) (Foret et al., 2020), both with momentum.\nDASH was compared against baselines (cold-starting, warm-starting, and S&P) and methods addressing plasticity loss under non-stationarity and methods known to mitigate plasticity loss in reinforcement learning (Lee et al., 2023), were applied to both warm and cold-starting. Consistent hyperparameters were used across all methods, with details provided in Appendix C.3. S&P, Reset, and DASH were applied whenever new data was introduced. We report two metrics for both test accuracy and number of steps required for convergence: the value from the final experiment and the average across all experiments."}, {"title": "5.2 Experimental Results", "content": "We first experimented with CIFAR-10 on ResNet-18 to determine if methods from previous works for mitigating plasticity based on non-stationarity can be a solution to our incremental setting with stationarity. Appendix C.1 shows that L2 INIT, Reset, layer normalization, and reviving dead neurons, are not effective in our setting. Thus, we conducted the remaining experiments without these methods.\nAdditionally, Table 1 shows that warm-starting with SAM does not outperform cold-starting with SAM, indicating that SAM alone is not an effective method in our case. Table 1 shows that DASH surpasses cold-starting (Random Init) and S&P in most cases. Training times were often shorter compared to training from scratch, and when longer, the performance gap in test accuracy was more pronounced. Omitted results are in Tables 3-6 located in Appendix C.2. Additionally, we confirm that DASH is computationally efficient, with details on the computation and memory overhead comparisons provided in Appendix C.5.\nWe argue that S&P can cause the model to forget learned information, including important features, due to shrinking every weight uniformly and perturbing weights. This leads to increased training time and relatively lower test accuracy, especially in SoTA settings (see Appendix C.4.1). In contrast, DASH addresses these issues by preserving learned features with direction-aware weight shrinkage."}, {"title": "6 Discussion and Conclusion", "content": "In this work, we defined an abstract framework for feature learning and discovered that warm-starting benefits from reduced training time compared to random initialization but can hurt the generalization performance of neural networks due to the memorization of noise. Motivated by these observations, we proposed Direction-Aware SHrinking (DASH), which shrinks weights that learned data-specific noise while retaining weights that learned commonly appearing features. We validated DASH in real-world model training, achieving promising results for both test accuracy and training time.\nLoss of plasticity is problematic in situations where new data is continuously added daily, which is the case in many real-world application scenarios. Our research aimed to interpret and resolve this issue, preventing substantial waste of energy, time, and the environment. By elucidating the loss of plasticity phenomenon in stationary data distributions, we have taken a crucial step towards addressing challenges that may emerge in real-world AI, where the continuous influx of additional data is inevitable.\nWe hope our fundamental analysis of the loss of plasticity phenomenon sheds light on understanding this issue as well as providing a remedy. To generalize our findings to any neural network architecture, we treated the learning process as a discrete abstract procedure and did not assume any hypothesis class. Future research could focus on understanding the loss of plasticity phenomenon via optimization or theoretically analyzing it in non-stationary data distributions, such as in reinforcement learning."}]}