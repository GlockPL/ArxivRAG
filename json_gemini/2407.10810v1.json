{"title": "FabGPT: An Efficient Large Multimodal Model for Complex Wafer Defect Knowledge Queries", "authors": ["Yuqi Jiang", "Xudong Lu", "Qian Jin", "Qi Sun", "Hanming Wu", "Cheng Zhuo"], "abstract": "Intelligence is key to advancing integrated circuit (IC) fabrication. Recent breakthroughs in Large Multimodal Models (LMMs) have unlocked unparalleled abilities in understanding images and text, fostering intelligent fabrication. Leveraging the power of LMMs, we introduce FabGPT, a customized IC fabrication large multimodal model for wafer defect knowledge query. FabGPT manifests expertise in conducting defect detection in Scanning Electron Microscope (SEM) images, performing root cause analysis, and providing expert question-answering (Q&A) on fabrication processes. FabGPT matches enhanced multimodal features to automatically detect minute defects under complex wafer backgrounds and reduce the subjectivity of manual threshold settings. Besides, the proposed modulation module and interactive corpus training strategy embed wafer defect knowledge into the pre-trained model, effectively balancing Q&A queries related to defect knowledge and original knowledge and mitigating the modality bias issues. Experiments on in-house fab data (SEM-WaD) show that our FabGPT achieves significant performance improvement in wafer defect detection and knowledge querying.", "sections": [{"title": "1 INTRODUCTION", "content": "The intersection of visual and language models [1-3] has significantly propelled the revolutionary advancement of artificial intelligence (AI), which makes models understand and interpret the world similarly to humans. Since Large Multimodal Models (LMMs) [4-6] possess the capability to reason about visual images, they have attracted considerable attention in defect detection tasks. However, current LMMs are primarily applied to visual tasks [7-9] in basic scenarios and lack sensitivity to the knowledge of specialized domains. This limits their efficiency in wafer defect knowledge query in the field of integrated circuits (IC) fabrication.\nIn the semiconductor industry, the manufacturing process is intricate, with each step potentially introducing random defects. These defects impact the reliability of electronic devices [10-13], making it essential to detect defects on the wafer surface and perform thorough question-and-answer (Q&A) analysis to deepen engineers\u2019 understanding of these defects and IC questions."}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 IC Wafer Defect Analysis", "content": "Wafers are the fundamental material for manufacturing IC, and the quality of their surface directly impacts the reliability of the chips. By identifying and understanding the various defects on the wafer surfaces detected by Scanning Electron Microscopes (SEM) [10, 11, 13], engineers can learn about the attributes such as type, location, and cause of the defects, which is crucial for optimizing processes and quality control. Traditional and CNN-based analysis methods utilize large datasets for feature learning to detect defect regions. For example, Zontak et al. [21] utilized the periodicity of wafer patterns to manually construct defect features, thus detecting defects. Gomez et al. [22] proposed a detection method based on Support Vector Machines (SVM), which separates data points of different classes in high-dimensional space using defined hyperplanes. Cheon et al. [15] proposed a CNN model that can extract effective features for defect classification. These models achieve significant progress in classifying and segmenting wafer defects. However, they perform poorly when faced with scarce data and have not yet developed a model that integrates classification, segmentation, and analysis. There is an urgent need for an automated tool capable of detecting and inferring wafer defects with minimal data."}, {"title": "2.2 Collection and Processing of Dataset", "content": "Due to the absence of publicly available datasets for wafer surface defects, we collect an in-house dataset (SEM-WaD) using SEM techniques from various IC manufacturing steps and products. It comprises 1,226 defect-free images and 1,182 images with four common types of defects (holes: 250, particles: 500, pattern deformities: 250, scratches: 182) from our fab partners. The wafer images in SEM-WaD contain diverse and complex backgrounds, with each defect type exhibiting unique morphologies and features. Consequently, we meticulously annotate each image in the dataset with details such as image IDs and production steps."}, {"title": "2.3 Large Multimodal Models", "content": "LMMs [4-6] are the large and complex models capable of processing various types of data (images, spectra, sound, etc.) and utilize large-scale datasets during training to understand and generate descriptions of visual content. Their powerful comprehension and transfer abilities make them excel in various tasks such as image description generation and visual question answering. For example, [4] utilized a linear layer for aligning the frozen video encoder of the BLIP-2 [23] and the LLM Vicuna [24] to enable image-text Q&A. [5] adopted a contrastive learning approach to embed the semantic information of images and text into the same space to achieve zero-shot transfer. DALL-E [6] generated images related to text descriptions by encoding text with Transformer and using a Generative Adversarial Network (GAN). These models have a strong ability to understand complex image-text data, however, they face challenges in robustness when adapting to new domains."}, {"title": "2.4 Fine-Tuning Methods", "content": "Fine-tuning methods primarily include full [25, 26] and partial fine-tuning [19, 20, 27], which involves retraining parts of a model pre-trained on large datasets to adapt to specific tasks. In full fine-tuning, all pre-trained model layer weights are trainable, enabling flexible adaptation to new data features by adjusting all parameters. For example, BERT [25] is trained on large corpora adjusting all parameters in its bidirectional Transformer structure to optimize tasks such as sentiment analysis question answering and text summarization. ViT [26] adjusts its Transformer structure by specialized visual datasets, efficiently adapting to tasks such as image classification and object detection. Full fine-tuning enhances model adaptation to specific downstream tasks but also poses a higher risk of overfitting and increasing computational costs, limiting its wide applications.\nIn partial fine-tuning [19, 20, 27], it adjusts only partial parameters in pre-trained models and keeps the rest fixed. Recent popular partial fine-tuning strategies include Adapters, Prompt Learning, and LoRA. For example, Rebuffi et al. [27] introduced residual adapters to neural networks, optimizing only these modules to reduce fine-tuning costs due to their fewer parameters. Lai et al. [20] applied the LoRA method to consistently update the bottom embeddings and top linear head of the pre-trained model, while randomly updating a few intermediate self-attention layers to understand input text for precise segmentation. Gu et al. proposed AnomalyGPT [19], which fine-tunes an LLM using embedded prompt instructions to identify types and locations of defects.\nDespite their advancements, new models often lose their normal Q&A capabilities after fine-tuning. This occurs because the model becomes excessively focused on image inputs while neglecting user queries, whether or not these queries are related to the images. This phenomenon is termed \"modality bias\". Our model effectively integrates complex wafer defect knowledge and is designed to alleviate the modality bias."}, {"title": "3 PROPOSED METHOD", "content": ""}, {"title": "3.1 Network Architecture", "content": "As shown in Figure 3, our FabGPT is a conversational LMM designed for querying wafer defect knowledge, and it consists of a foundational stage for modal enhancement and two functional stages for detection and Q&A.\nGiven a query image \\(x \\in R^{H\u00d7W\u00d73}\\), text marks are extracted from x using Optical Character Recognition (OCR) technology [28]. The image x, its text marks, and the label set are encoded into initial vectors \\(v_{clip}^{img}\\), \\(v_{clip}^{txt}\\), \\(v_{clip}^{lab}\\), and \\(v_{clip}^{clip}\\) through pre-trained image and text encoders [29]. \\(V_{img}\\) \\(V_{txt}\\) and \\(V_{lab}\\) are fed into the Prediction Module (PM) to predict the defect category \\(P_{n}\\). Then, \\(P_{n}\\) is used to multiply with \\(V_{img}\\) and \\(V_{txt}\\), generating the vectors \\(V_{img}\\) and \\(V_{txt}\\). Visual and textual adapters further process these vectors into the information-rich image token \\(T_{img}\\) and text token \\(T_{txt}\\). In the detection stage, \\(T_{img}\\) and \\(T_{txt}\\) are fed into the detection head to obtain supervised detection masks. In the Q&A stage, the Modulation Module aligns \\(T_{img}\\), \\(T_{txt}\\), mask-projected token \\(T_{mas}\\), and the user's question token \\(T_{que}\\) into a unified visual token \\(aT_{vis}\\). Finally, it is concatenated with \\(T_{mas}\\) and \\(T_{que}\\) to serve as prompt instructions for fine-tuning PandaGPT [2]."}, {"title": "3.2 Modal Enhancement Stage", "content": "When analyzing the minor defects on the wafer surface, distinguishing between complex background and foreground features is challenging, which hinders queries on defect-related knowledge. To address this, we develop the modal enhancement stage, consisting of the PM and two enhancement branches, designed to highlight relevant defect features and minimize the impact of irrelevant features.\nPrediction Module (PM):\nThe pre-trained encoder captures pixel-level detail features in the latent space, but its repeated down-sampling operations result in the loss of semantic features. As shown in Figure 4, we design the PM to predict defect categories in the image and enhance semantic features by embedding the expected result \\(p_{n}\\) into the initial vectors. Specifically, we count a label set containing all defect categories found in wafer images to support automated classification tasks. First, the linear layer and the activation function are applied to reshape the dimensions of \\(V_{lab}^{clip}\\) and \\(V_{img}^{clip}\\), formulated as:\n\\[f_{img} = \u03c3(W_{img}^{clip}v_{img}^{clip} + b_{i}),\\]\n\\[f_{lab} = \u03c3(W_{lab}^{clip}v_{lab}^{clip} + b_{i}),\\]\nwhere \\(W_{i}\\) represents the weight matrix, \\(b_{i}\\) represents the bias vector, and \u03c3 represents the ReLU function. Then, the cosine of the angle between each category \\(f_{lab}\\) and \\(f_{img}\\) is computed to assess their similarity, and the cross-entropy loss function is used to constrain the selection of the corresponding category \\(p_{n}\\). This process is formulated as:\n\\[p_{i} = Cosine(f_{img}, f_{lab}),\\]\n\\[p_{n} = max(Softmax(p_{i})),\\]\n\\[= max(\\frac{exp(p_{i})}{\\sum_{j=1}^{N} exp(p_{j})}),\\]\nwhere Cosine represents the cosine similarity calculation. \\(p_{n}\\) is matrix-multiplied with \\(V_{img}^{clip}\\) and \\(V_{lab}^{clip}\\), resulting in vectors \\(V_{img}\\) and \\(V_{txt}\\), which are enriched with semantic features.\nTwo Enhancement Branches:\nAlthough semantic features related to defect attributes are enhanced, the detailed representation of defect features remains essential. In the visual and textual enhancement branches, adapters based on prompt learning are deployed, utilizing the extra prompts of pre-trained experts for adaptive feature optimization. The experts are initialized under the guidance of \\(V_{img}^{clip}\\) and \\(V_{img}^{clip}\\), which enable them to acquire knowledge from image and text modals. During training, they adaptively update parameters and interact with \\(V_{img}\\) and \\(V_{txt}\\) after each update, effectively controlling the direction and quality of the feature flow. The final outputs \\(T_{img}\\) and \\(T_{txt}\\) are generated, with \\(T_{img}\\) being represented by the following formula (the same applies to \\(T_{txt}\\)):\n\\[V_{fe} = (V_{img}^{clip}) \u2192 z_{i},\\]\n\\[T_{img} = Concat(V_{fe}, V_{img}),\\]\nwhere \u2192 represents the feature-guiding operation, \\(z^{i}\\) and \\((V_{img}^{clip})_{i}\\) are the i-th elements of the random vector z and \\(V_{img}^{clip}\\) respectively, and \\(V_{fe}\\) is the prompt feature of the pre-trained expert."}, {"title": "3.3 Detection Stage", "content": "Manually setting segmentation thresholds is not necessarily optimal while continuous adjustments should be made for specific tasks. We design the detection head that autonomously learns the specific thresholds for each pixel at feature positions to generate pixel-level masks \\(Mask \u2208 R^{H\u00d7W}\\). It first fuses complementary information from \\(T_{img}\\) and \\(T_{txt}\\) through matrix multiplication, then maps them back to high-dimensional features through four up-sampling operations of the trainable decoder, and normalizes the output into the mask image through the softmax function. The detection head matches multimodal information and supervises detail features, enabling precise defect detection in complex wafer backgrounds. This process is formulated as:\n\\[Mask =Softmax((T_{img} \\& T_{txt}) \u2191),\\]\nwhere \u2191 represents performing four times up-sampling."}, {"title": "3.4 Q&A Stage", "content": "In fine-tuning the large models based on prompt instructions, the commonly used embedded instruction format is:\n\\[INS = Concat(x, T_{img}, T_{que}).\\]\nHowever, this embedding format may lead to modality bias, where the model is dominated by image inputs and fails to respond to questions appropriately. For example, the latest defect Q&A model AnomalyGPT [19] embeds industrial defect knowledge into its pre-trained model based on Equation (5), it can only answer questions related to defects, such as:\nHowever, it fails to answer general questions not closely related to the images, such as:\nThis phenomenon indicates that while the model gains new information, it loses the understanding of its original knowledge, thereby diminishing its ability to analyze general knowledge effectively.\nWe suggest that this phenomenon occurs because the model fails to judge the correlation between the query image and the user's question adequately. The keys to resolving this issue and ensuring accurate model responses are: 1) Improving the quality of visual instructions; 2) Enhancing the ability to assess the relevance between visual prompt instructions and user query ones; 3) Optimizing the training strategies of the corpus."}, {"title": "Modulation Module:", "content": "The quality of prompt instructions significantly affects the model's ability to understand knowledge and respond to queries. Thus, it is necessary to align enhanced multimodal features to capture visual information and alleviate the model's fine-tuning burden. Inspired by Q-Former [23], a bidirectional self-attention in Figure 5 allows \\(T_{img}\\) to absorb semantic and detailed information from \\(T_{mas}\\) to obtain \\(\\tilde{f_{img}}\\), facilitating interaction within the same modality, formulated as:\n\\[M_{img} = Softmax(((T_{img} * k_{1})(T_{img} * k_{2})^{T})/\\sqrt{d_{k}}),\\]\n\\[M_{mask} = Softmax(((T_{mask} * k_{1})(T_{mask} * k_{2})^{T})/\\sqrt{d_{k}}),\\]\n\\[\\tilde{f_{img}} = \\frac{(S_{i} + S_{m})}{2} T_{img},\\]\nwhere * represents convolution operations, \\(k_{i}\\) represents different kernels, and \\(d_{k}\\) represents the dimension of the feature vector. Next, aligning the fine-grained information between visual features \\(\\tilde{f_{img}}\\) and textual tokens \\(T_{txt}\\) through cross-attention [30] allows for the sharing of complementary knowledge across multimodalities, the result \\(\\tilde{f_{imgt}}\\) can be formulated as:\n\\[M_{\\tilde{imgt}} = Softmax((\\tilde{f_{img}} * k_{1})(T_{txt} * k_{2})^{T})/\\sqrt{d_{k}}),\\]\n\\[\\tilde{f_{imgt}} = M_{\\tilde{imgt}}T_{txt}.\\]\nFinally, to maintain semantic consistency between the LLM and outputs of the modal enhancement stage, the feed-forward network maps the unified features \\(\\tilde{f_{imgt}}\\) to a high-dimensional space and outputs high-quality prompt instructions \\(T_{vis}\\) through the activation of nonlinear layers.\nSince the content of user queries involves knowledge of different tasks, we must assess the relationship between query and visual instructions before fine-tuning the LLM. We set a scaling factor a, dynamically adjusting its value through learning the association between instructions (the higher the value, the stronger the association). A learnable corrector that is generated under the guidance of \\(T_{vis}\\) is introduced, and its similarity score with the query is calculated to simulate the value of a. It can be formulated as:\n\\[a = \\frac{f_{c}^{T}T_{que}}{||f_{c}||||T_{que}||},\\]\nwhere represents the dot product and \\(V_{fc}\\) represents the prompt features of the learnable corrector. We assign a as the coefficient to \\(T_{vis}\\). The updated \\(aT_{vis}\\) along with \\(T_{mas}\\) and \\(T_{que}\\) serve as our input instructions, formatted as follows:\n\\[INS = Concat(aT_{vis}, T_{mas}, T_{que}).\\]"}, {"title": "Corpus Training Strategy:", "content": "During the corpus training process, the alternating training strategy balances learning new and old knowledge. We establish two corpora: Corpus-A, which includes 15 Q&A pairs for each category related to defect type, quantity, location, description, and analysis (e.g., Q: What type of defect is in the image? A: The defect in the image is object.), and Corpus-B, which contains 100 Q&A pairs unrelated to defect knowledge (e.g., Q: What is the capital of China? A: The capital of China is Beijing.). Our model trains alternately on these corpora at a 2:1 ratio to prevent it from favoring the retrieval of new knowledge when understanding questions."}, {"title": "3.5 Loss Functions", "content": "We employ three loss functions to constrain the detection and dialogue processes. Focal Loss [31] and Dice Loss [32] are used to improve the model's segmentation and localization abilities, and Cross-Entropy Loss is used to improve the model's classification and Q&A ones.\nFocal Loss:\nFocal Loss [31] aims to address the issue of class imbalance. It introduces a modulation factor \u03b3 to reduce the relative loss of correctly classified pixels and focus on hard-to-classify and misclassified pixels. It is realized as Equation (10):\n\\[L_{focal} = \\frac{1}{H \u00d7 W} \\sum_{i=1}^{H \u00d7 W}(1 - p_{i})^{\u03b3} log(p_{i}),\\]\nwhere \\(p_{i}\\) represents the probability of the pixel belonging to the correct category, and based on [19], we set the \u03b3 to 2.\nDice Loss:\nDice Loss [32] aims to maximize the overlap between outputs and actual labels, encouraging the model to learn to produce results closer to the ideal segmentation. It is realized as Equation (11):\n\\[L_{dice} = 1 - \\frac{\\sum_{i=1}^{H \u00d7 W} y_{i}\u0177_{i}}{\\sum_{i=1}^{H \u00d7 W} y_{i}^{2} + \\sum_{i=1}^{H \u00d7 W} \u0177_{i}^{2}},\\]\nwhere \\(y_{i}\\) is the output of the decoder, and \\(\\hat{y}_{i}\\) is the truth labels.\nCross-Entropy Loss:\nCross-entropy loss measures the difference between predicted and actual categories in the PM and between output texts of the language model and target texts in the Q&A task. It is realized as Equation (12):\n\\[L_{ce} = - \\sum_{i=1}^{C} y_{i} log(p_{i}),\\]\nwhere c represents the number of categories and tokens in classification and Q&A tasks, \\(y_{i}\\) represents the truth label and \\(p_{i}\\) represents the predicted label.\nThe overall loss function is:\n\\[L = \u03b1L_{focal} + \u03b2L_{dice} + \u03b4\\hat{L_{ce}} + \u03b5L_{ce}.\\]\nWe set the coefficients \u03b1, \u03b2, \u03b4 and \u03b5 to 1, by default."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 Experimental Setups", "content": "Datasets:\nOur experiment is conducted on the in-house SEM-WaD dataset. The dataset comprises images with a resolution of 480 \u00d7 480, each accompanied by a corresponding mask image and related textual descriptions and analyses. We divide the data into training and test sets in a 7:3 ratio, where both sets consist of good and defective images.\nImplementation Details:\nOur model uses PandaGPT [2] as the foundational LMM, composed of the Vicuna-7B [24] as the language model and ImageBind Huge [29] as the frozen encoder. During training, we employ the AdamW optimizer (\u03b2\u2081 = 0.9, \u03b2\u2082 = 0.999) [33] with an initial learning rate of 1e-4, gradually reducing to 1e\u00af6, using the cosine annealing [34] strategy. The model is trained on three 4090Ti GPUs, with a batch size of 24 and an epoch of 50.\nEvaluations:\nIn the detection task, we evaluate model performance using four metrics: Image-AUC (the Area Under the Receiver Operating Characteristic Curve), Pixel-AUC, Per-Region Overlap (PRO), and Average Precision (AP). Image-AUC and Pixel-AUC assess the model's ability to judge the presence of defects in images, while PRO and AP measure the precision of the model in identifying and locating defects. In the Q&A task, we conduct 15 questions for each of the four defect types, including inquiries about the presence, category, location, quantity, appearance description, and root cause analysis of defects. Additionally, we pose 50 questions that are unrelated to defects and not included in the corpus (IC-related or IC-unrelated general questions) to validate the modality bias issue. We use the percentage of correct answers as a relevant metric to evaluate the Q&A capability of the model. To demonstrate the outstanding performance of our FabGPT, we compare it with many representative previous arts:\nThese baselines are also tuned on our SEM-WaD dataset, following their publicly available implementations and models. Among these baselines, Lisa and AnomalyGPT are LMMs that support Q&A while the others do not."}, {"title": "4.2 Quantitative Results", "content": "The quantitative results report the accuracy of the supervised defect detection and the correctness of the knowledge Q&A responses in detail.\nDefect Detection Task:\nTable 1 and Table 2 report the Image-AUC, Pixel-AUC, PRO, and AP values for different methods across 4 categories within the SEM-WaD dataset. It can be observed that our model outperforms all other methods in four evaluation metrics for most defect categories. For example, compared to AnomalyGPT [19], which also employs prompt learning for fine-tuning, our model achieves a higher Image-AUC by 1.85% and a higher Pixel-AUC by 3.03%. Compared to the traditional detection model PRN [17], our model surpasses it by 10.12% in PRO and 14.91% in AP.\nQ&A Task:\nTable 3 reports the accuracy of different language models in answering defect-related and -unrelated questions where our model achieves state-of-the-art results. For example, compared to the powerful capabilities of GPT-4, our model achieves a comparable performance in the general questions and a 7.50% higher accuracy in defect analysis answers. Compared to Lisa [20], which embeds new tasks to pre-trained models, our model achieves a 78.00% higher accuracy in answering questions unrelated to the new task."}, {"title": "4.3 Qualitative Results", "content": "We produce comparison figures for detection results and dialogue diagrams for Q&A results on the SEM-WaD dataset. Intuitive detection results are validated using heatmaps and mask images to demonstrate the effectiveness of our model in the supervised detection task."}, {"title": "4.4 Ablation Studies", "content": "In this section, we conduct ablation studies to demonstrate the importance of each component introduced at every stage and the operations within them.\nStage Components:\nWe study the proposed components in each stage as shown in Table 4. It confirms the importance of each individual component and the best way to structure the sub-stage. It can be seen that the prediction of the PM and pre-trained experts significantly improved defect detection in the modal enhancement stage, and the operations in the modulator played a crucial role in addressing bias issues in the Q&A stage.\nDesign for PM Operation:"}, {"title": "5 CONCLUSIONS", "content": "In this paper, we introduce a novel large multimodal language model, FabGPT, for defect knowledge querying in the IC field, including defection, analysis, Q&A, etc. It employs three stages to gradually achieve the functionality of defect detection and high-quality dialogue. Enhanced feature tokens aid the model in automatically conducting high-precision detection. Dynamically aligned and corrected tokens fine-tune the LLM, enabling not only attribution analysis of defect regions and correct responses regarding defect type, location, quantity, etc., but also mitigate the modality bias issues within conversations. We validate the effectiveness of FabGPT on the SEM-WaD dataset and 100 questions. Our work provides great convenience for the semiconductor industry and also offers insights for further LMM research."}]}