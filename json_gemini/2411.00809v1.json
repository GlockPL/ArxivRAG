{"title": "ADAPTIVE DENSE REWARD: UNDERSTANDING THE GAP BETWEEN ACTION AND REWARD SPACE IN ALIGNMENT", "authors": ["Yanshi Li", "Shaopan Xiong", "Gengru Chen", "Xiaoyang Li", "Yijia Luo", "Xingyao Zhang", "Yanhui Huang", "Xingyuan Bu", "Yingshui Tan", "Chun Yuan", "Jiamang Wang", "Wenbo Su", "Bo Zheng"], "abstract": "Reinforcement Learning from Human Feedback (RLHF) has proven highly effective in aligning Large Language Models (LLMs) with human preferences. However, the original RLHF typically optimizes under an overall reward, which can lead to a suboptimal learning process. This limitation stems from RLHF's lack of awareness regarding which specific tokens should be reinforced or suppressed. Moreover, conflicts in supervision can arise, for instance, when a chosen response includes erroneous tokens, while a rejected response contains accurate elements. To rectify these shortcomings, increasing dense reward methods, such as step-wise and token-wise RLHF, have been proposed. However, these existing methods are limited to specific tasks (like mathematics). In this paper, we propose the \"Adaptive Message-wise RLHF\u201d method, which robustly applies to various tasks. By defining pivot tokens as key indicators, our approach adaptively identifies essential information and converts sample-level supervision into fine-grained, subsequence-level supervision. This aligns the density of rewards and action spaces more closely with the information density of the input. Experiments demonstrate that our method can be integrated into various training methods, significantly mitigating hallucinations and catastrophic forgetting problems while outperforming other methods on multiple evaluation metrics. Our method improves the success rate on adversarial samples by 10% compared to the sample-wise approach and achieves a 1.3% improvement on evaluation benchmarks such as MMLU, GSM8K, and HumanEval et al.", "sections": [{"title": "INTRODUCTION", "content": "Recently, large language models have achieved remarkable strides across varied fields, sparking widespread speculation about the emergence of artificial general intelligence and agents surpassing human capabilities. Reinforcement learning plays a crucial role in today's large model training. The numerous advantages of reinforcement learning methods make it an essential path toward agents more powerful than humans, such as obtaining annotation capabilities superior to human annotators through model interaction and better modeling of complex objectives that are difficult to quantify(Yang et al. (2024a); Dubey et al. (2024)).\nReinforcement learning from human feedback (RLHF) significantly improved the performance of large AI models(Ouyang et al. (2022)). Its impressive results and relatively expensive process have inspired other methods like DPO(Rafailov et al. (2024b)) and rejection sampling(Touvron et al. (2023)), which can be flexibly used in online or offline modes. These mainstream methods predominantly rely on sample-level supervision signals, which, from the perspective of classic reinforcement learning and control theory, exhibit inconsistency between reward signals and action spaces. In large language model training, this phenomenon is often conceptualized as an adaptive credit assignment in multi-armed bandits(Dud\u00edk et al. (2015); Rafailov et al. (2024a)), which frequently introduces systematic errors during training. For instance, when dealing with negative samples, colloquialisms or special format characters should not have their logits excessively reduced. Another example can be found in online on-policy iterative training that positive samples may contain processes that are largely correct but have minor issues. These small imperfections can lead to an increase in hallucinations, and make this training method sometimes less effective than iterative training(Dubey et al. (2024)).\nDenser supervision methods, such as step-by-step verified methods(Lightman et al. (2023b); Pan et al. (2023); Uesato et al. (2022b)) and step-DPO(Lai et al. (2024b)), face limitations in scaling to diverse dialogue generation tasks. In dialogue generation tasks, dividing by specific symbols often fails to capture truly useful steps, and may even result in lower accuracy than sample-level supervision signals(figure:1). For instance, contexts and inversions often appear in human conversations, such as \"A bee is a kind of fruit, which is incorrect\"; In code completion tasks, the critical information is concentrated at the specific completion locations.\nThe densest supervision methods, such as token-level DPO(Zeng et al. (2024b); Rafailov et al. (2024a)), demonstrate significant shortcomings on tasks that challenge the model's capabilities. Because rewards at the token level often have variances 1-2 orders of magnitude higher than sample-level supervision signals. These limitations become extremely apparent in tasks where the model lacks proficiency, such as complex mathematical problems or humor. Additionally, this method relies heavily on knowledge consistency between the reward model and actor model. Even subtle stylistic differences between them can lead to dramatic fluctuations in token-level scoring, making it almost impossible to perform off-policy or offline training."}, {"title": "PRELIMINARIES", "content": "In this section, we present an overview of predominant RLHF methods. Additional, we also define the symbols that will be consistently employed throughout this paper, as these notations will be integrated into our framework."}, {"title": "RL ENVIRONMENT", "content": "The Reinforcement Learning from Human Feedback (RLHF) paradigm can be formalized as a Markov Decision Process (MDP) with a dense reward structure derived from preference models. Traditionally, the Bradley-Terry model has been employed to estimate preferences between pairs of sequences:\n$P(Y_1 > Y_2 | X, Y_1, Y_2) = \\sigma(r(x, y_1) \u2013 r(x, y_2))$  (1)\nwhere $\\sigma$ is the sigmoid function, x is the input context, and $y_1, y_2$ are candidate responses. To refine this approach for token-level optimization, we decompose the reward function into individual token contributions:\n$P(Y_1 > Y_2 | X, Y_1, Y_2) = \\sigma(\\sum_{h=1}^{H} r(s_h^1, a_h^1) - \\sum_{h=1}^{H} r(s_h^2, a_h^2))$ (2)\nwhere $s_h$ and $a_h$ represent the state and action at step h for sequence i, respectively. This decomposition allows the definition of both sequence-level and token-level reward functions:\n$r_s(x, y) = log \\pi^*(y|x),   r_t((x, y_{1:h-1}), y_h) = log \\pi^*(y_h|x, y_{1:h-1})$. (3)\nHere, $r_s$ denotes the sequence-level reward, while $r_t$ represents the token-level reward. $\\pi^*$ is the optimal policy derived from human preferences. This formulation enables a more granular approach to RLHF, allowing for token-by-token optimization. This effectively bridges the gap between preference learning and reinforcement learning, providing a dense reward signal that can guide policy improvement at a finer scale. This approach not only enhances learning efficiency but also potentially leads to more coherent and contextually appropriate language generation. The token-level reward structure aligns with recent advancements in selective token methods (Yang et al. (2024b), Lin et al. (2024), Zeng et al. (2024a)), which focuses on optimizing the most relevant tokens. This synergy between dense reward modeling and selective token optimization presents a promising direction for improving the efficiency and effectiveness of RLHF in large language models."}, {"title": "RL METHODS", "content": "The evolution of Reinforcement Learning from Human Feedback (RLHF) has led to several methodological variants, each addressing specific aspects of the learning process. This section outlines key approaches in the RLHF paradigm."}, {"title": "CLASSICAL RLHF", "content": "The traditional RLHF objective function is formulated as:\n$L_{PPO}(\\theta) = max_\\theta E_{x \\sim D, y \\sim \\pi_\\theta(y|x)}[r_\\theta(x,y)] \u2013 \\beta D_{KL}[\\pi_\\theta(y|x)||\\pi_{ref}(y|x)]$, (4)\nwhere $r_\\theta (x, y)$ is the learned reward function, $\\pi_\\theta$ is the policy being optimized, $\\pi_{ref}$ is a reference policy, and $\\beta$ controls the strength of the KL-divergence regularization."}, {"title": "DIRECT PREFERENCE OPTIMIZATION (DPO)", "content": "DPO (Rafailov et al. (2024b)) reformulates RLHF as a preference learning problem:\n$P(y_1 > y_2 | x) = \\frac{exp(r(x, y_1))}{exp(r(x, y_1)) + exp(r(x, y_2))}$ (5)\nThis leads to the DPO loss:\n$L_{DPO}(\\pi_\\theta; \\pi_{ref}) = \u2212E_{(x, y_w, y_l) \\sim D} [log \\sigma(\\beta log \\frac{\\pi_\\theta(y_w | x)}{\\pi_{ref}(y_w | x)} - \\beta log \\frac{\\pi_\\theta(y_l | x)}{\\pi_{ref}(y_l | x)})]$; (6)\nwhere $y_w$ and $y_l$ represent winning and losing completions respectively."}, {"title": "REJECTION SAMPLING", "content": "An alternative approach uses rejection sampling (Touvron et al. (2023)), optimizing:\n$L_{Rejection Sampling} (\\theta) = \u2212E_{x, y_w \\sim D}[log \\pi_\\theta(y_w|x) \u2013 \\beta KL(\\pi_\\theta||\\pi_{ref})]$ (7)\nThis method directly optimizes the policy to generate preferred outputs while maintaining proximity to the reference policy."}, {"title": "STEP-WISE RLHF", "content": "Step-wise RLHF (Lai et al. (2024b)) applies the DPO principle to individual steps in a reasoning process:\n$L_{Step-DPO}(\\theta) = -E_{(x, y_{win}, y_{lose}) \\sim D} [log \\sigma (\\beta log \\frac{\\pi_\\theta(Y_{win}|x)}{\\pi_{ref} (Y_{win}|x)} - \\beta log \\frac{\\pi_\\theta(Y_{lose}|x)}{\\pi_{ref} (Y_{lose}|x)})]$ (8)\nThis approach is particularly effective for tasks requiring step-by-step reasoning, such as mathematical problem-solving."}, {"title": "TOKEN-LEVEL RLHF", "content": "Token-level RLHF (Zeng et al. (2024b)) extends the preference learning to the granularity of individual tokens:\n$L(\\pi_\\theta, D) = \u2212E_{(\\tau_w, \\tau_l) \\sim D}  log \\sigma(\\sum_{t=0}^{M-1} \\beta log \\frac{\\pi_\\theta(a_t^w | s_t^w)}{\\pi_{ref}(a_t^w | s_t^w)} - \\sum_{t=0}^{N-1} \\beta log \\frac{\\pi_\\theta(a_t^l | s_t^l)}{\\pi_{ref}(a_t^l | s_t^l)})]$ (9)\nwhere $\\tau_w$ and $\\tau_l$ represent winning and losing trajectories, and $a_t^w, s_t^w, a_t^l, s_t^l$ are the actions and states at each step for winning and losing trajectories respectively. These methods represent a spectrum of RLHF approaches, ranging from sequence-level to token-level optimization. Token-level methods offer enhanced alignment precision and granular control, potentially addressing issues of sparse rewards and inconsistent behavior. While sequence-level approaches may provide computational efficiency, token-level optimization enables more nuanced preference capture and potentially more coherent outputs. The selection of method depends on task requirements, computational constraints, and the desired level of control over language model outputs, with a growing trend towards finer-grained optimization techniques."}, {"title": "CONTEXTUAL DUELING BANDITS FOR RLHF", "content": "RLHF can be formulated as a contextual dueling bandits problem, extending the standard bandit setting to incorporate context and pairwise comparisons. Let X be the context space (e.g., input prompts) and Y the action space (e.g., model responses). The RLHF problem is defined by a set of policies $\\Pi: X \\rightarrow Y$, a distribution D over contexts $x \\in X$, and a preference function $P: Y \\times Y \\times X \\rightarrow [0, 1]$. In each round, a context $x_t \\sim D$ is sampled, two policies $\\pi_t, \\rho_t \\in \\Pi$ are selected, actions $y_t = \\pi_t(x_t)$ and $y'_t = \\rho_t(x_t)$ are generated, and feedback $r_t \\sim Bernoulli(P(y_t, y'_t, x_t))$ is observed. The goal is to maximize the expected preference over T rounds:\n$max_{\\pi \\in \\Pi} E_{x \\sim D, \\rho \\in \\Pi}[P(\\pi(x), \\rho(x), x)]$ (10)\nThis formulation captures key aspects of RLHF: the contextual nature of language tasks, preference-based feedback, and policy optimization without explicit reward modeling. Solutions to this problem typically involve learning policies that perform well in pairwise comparisons across a range of contexts, effectively maximizing the probability of winning duels against other policies in the space."}, {"title": "APPROACH", "content": "In this section, we introduce our Adaptive Message-wise RLHF approach. In section 3.1, we first explain how we obtain key tokens through a reward model or annotators. In section 3.2, we will describe how our message-wise framework integrates with the Reinforcement Learning from Human Feedback(RLHF) framework, and how it combines with the bandit capabilities derived from the large model's inherent knowledge. We will present adaptive message-wise forms of Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), and rejection sampling(RJ)."}, {"title": "ADAPTIVE DENSITY REWARDS", "content": "As mentioned in section 2.1, the Bradley-Terry model, along with its augmented variant, the Plackett-Luce model, can be represented as a Markov Decision Process (MDP), allowing for fine-grained reward acquisition.\nLeveraging this feature, researchers have explored step-wise and token-wise alignment methods. However, these approaches still face some unresolved issues. Step-wise methods, which rely on artificially defined step divisions, struggle to generalize across diverse tasks, particularly in writing and RAG tasks with lower information density. This leads to difficulties in obtaining meaningful steps and significantly reduces reward accuracy. Token-wise methods, which directly supervise using token-level rewards, are confined to online on-policy frameworks. This limitation makes effective training challenging in other scenarios and hinders generalization to domains where models are less robust, such as telling humorous jokes or solving complex mathematical problems.\nFundamentally, step-wise methods face a mismatch between the density of supervisory signals and the sampled supervised sequence in the action space. Token-wise methods, on the other hand, suffer from excessive supervisory signal density, resulting in noisy rewards with high variance, and fail to fully utilize the model's inherent knowledge.\nTo address these issues, we identify key tokens in sampled examples using signals of significant difference. We then divide the samples based on this critical information, thereby leveraging the model's inherent knowledge more effectively. Practical evidence shows that this approach often outperforms human annotators in identifying key information and dividing steps.\nAdaptive Mask as the most basic form of implementing this framework, is used here to express our method. We can divide the sequence into multiple subsequences through adaptive masking.\nIn RLHF training, we sample from the policy and categorize the samples into preferred and non-preferred based on rewards or advantages. The training process then involves pulling the logits towards the preferred samples while pushing them away from the non-preferred ones. This approach is widely adopted in various methods such as PPO (Proximal Policy Optimization), GRPO (Generalized Reward-Penalized Policy Optimization), DPO (Direct Preference Optimization), and KTO (KL-constrained Trust-region Optimization).To more accurately identify inappropriate elements within preferred samples or reasonable parts within non-preferred samples, we employ an adaptive masking technique. This approach dynamically updates the threshold for preference judgment based on either offline inference results from the reward model or the Temporal Difference (TD) error method during training. This technique allows for more flexible adjustment in how the model processes different samples, thereby enhancing both the efficiency and effectiveness of the training process. The following is the expression for adaptive mask:\n$M(x,y) = \\begin{cases} 1, & \\text{if } (y \\in Y \\text{ and } R(x, y) > b) \\text{ or } (y \\in Y' \\text{ and } R(x, y) \\leq b) \\\\ 0, & \\text{otherwise} \\end{cases}$ (11)"}, {"title": "OPTIMIZING SIGNAL ACCURACY THROUGH CYBERNETIC METHODS", "content": "Schmitt trigger approach exploits the hysteresis characteristic of the Schmitt trigger by introducing the offset value $\\delta$ to create a \"neutral zone,\" which helps reduce frequent classification changes due to small variations in rewards, thus making the classification more stable and reliable.\n$G = \\{t | r_t > b + \\delta\\}, B = \\{t | r_t < b \u2212 \\delta\\}, N = \\{t | b \u2212 \\delta \\leq r_t \\leq b + \\delta\\}$. (15)\nIn the equation, the set G represents good tokens, defined as those for which $r_t > b + \\delta$; the set B signifies bad tokens, satisfying $r_t < b - \\delta$; and the set N corresponds to neutral tokens, defined as $b - \\delta \\leq r_t \\leq b + \\delta$. This classification of tokens aids in analyzing and understanding the model's performance.\nAccording to equation 15:\n$M(t) = \\begin{cases} 1, & \\text{if } r_t > b + \\delta \\\\ 0, & \\text{if } b - \\delta < r_t < b + \\delta \\\\ -1, & \\text{if } r_t < b - \\delta \\end{cases}$ (16)\nM(t) is Mask value,$r_t$ is the reward for the t-th token, b be the baseline value, and $\\delta$ be the offset value."}, {"title": "EXPERIMENTS", "content": "In this section, we describe our experimental setup, training strategy and testing methods. We conducted experiments using our open-source model, Qwen2-7b(Yang et al. (2024a)). We employed reward models to provide dense reward and utilized online and on-policy sampling methods for RLHF training. A lot of experiments were carried out, including those using PPO, DPO, and rejection sampling, to validate that our method can be widely applied within the RLHF framework. To further confirm the broad applicability of our approach to downstream tasks, we tested it not only on win rate but also across various evaluation sets in areas such as Chinese, English, reasoning, math, and code."}, {"title": "EXPERIMENTAL SETTINGS", "content": "Model and Datasets. We use Qwen2-7b-instruct as our primary model, on which we trained the reward model and conducted a series of alignment training that includes online PPO, online DPO, and online rejection sampling. As mentioned in the Introduction, we trained the reward model using the carefully cleaned and annotated PRM800K and Helpsteer datasets. Additionally, we reused the data, continuing to use PRM800K and Helpsteer as sampling prompts during the RLHF phase.\nTraining Strategy. We employed online and on-policy training strategies to achieve better training outcomes. During the training process, we sampled simultaneously from both the policy model and the reference model, dynamically updating the reference model throughout the training phasesLiu et al. (2024c). For a detailed description of our sampling strategies and training parameters, please refer to the appendix B.\nEvaluation. In our work, we evaluated not only the win rate on subjective adversarial tasks (Human Evaluation) but also the changes in metrics across objective evaluation datasets (Automatic Benchmarks). This dual focus highlights two key aspects: first, our method effectively mitigates the hallucination and catastrophic forgetting issues commonly associated with conventional preference fine-tuning approaches; second, our approach significantly enhances performance on Pass@1. The prompts used for GPT-4 evaluation are detailed in Appendix C.The benchmarks are presented below:\n\u2022 C-Eval: Pass@1 scores on 5-shot. (Huang et al. (2023))\n\u2022 C3: Pass@1 scores on 0-shot. (Sun et al. (2019))\n\u2022 MMLU:Pass@1 scores on 0-shot.(Hendrycks et al. (2021))\n\u2022 CommonsenseQA: Pass@1 scores on 0-shot.(Talmor et al. (2019))\n\u2022 Race:Pass@1 scores on 0-shot. (Lai et al. (2017))\n\u2022 ARC-C:Pass@1 scores on 0-shot.(Clark et al. (2018))\n\u2022 ARC-E: Pass@1 scores on 0-shot.(Clark et al. (2018))\n\u2022 BBH:Pass@1 scores on 0-shot. (Suzgun et al. (2022))\n\u2022 HellaSwag: Pass@1 scores on 0-shot.(Zellers et al. (2019))\n\u2022 WindoGrande: Pass@1 scores on 0-shot.(Sakaguchi et al. (2019))\n\u2022 GSM8K: Pass@1 scores on 0-shot.(Cobbe et al. (2021))\n\u2022 HumanEval: Pass@1 scores on 3-shot.(Chen et al. (2021))"}, {"title": "RESULT", "content": "Win Rate We extracted a total of 1,000 carefully annotated and cleaned samples from Helpsteer Wang et al. (2023b) and PRM800K Lightman et al. (2023a), based on the data ratio, specifically as a test set. During the evaluation, three annotators along with GPT-4(Appendix C) will collaboratively perform the annotations. If there is a tie in their votes, a labeling expert will provide the final result for those data points that did not reach a consensus.Figure 5 shows that our method can typically improve the win rate by about 10% in evaluations on the test set compared to conventional direct methods.The training monitoring shown in Figure 6 aligns very well with the final evaluation results. Additionally, during the experiments, our method exhibited a more stable loss compared to sparse supervision methods(Figure 7).\nObjective evaluation metrics. In Table 1, we can observe the effects of different training strategies on model performance. Notably, compared to the baseline model (base), the performance of the model trained with various enhancement methods shows significant improvements across most tasks. Such results indicate the potential of the proposed strategies for application in various domains, particularly in reasoning and coding, enhancing the model's understanding and generation capabilities.\nOur methods not only rely solely on win rate, our approach additionally evaluates performance on objective datasets. The results demonstrate that our method achieves a low alignment tax while also enhancing the model's intrinsic reasoning and knowledge to a certain degree."}, {"title": "RELATED WORKS", "content": ""}, {"title": "REINFORMENT LEARNING FROM HUMAN FEEDBACK(RLHF)", "content": "In the era of large language models(LLM), self-supervised language models of increasing scale have demonstrated remarkable capabilities in handling zero-shot(Radford et al. (2019)) or few-shot prompts(Brown (2020); Narayanan et al. (2021); Chowdhery et al. (2023)) across a wide range of generation tasks Li et al. (2024a); Feng et al. (2022); Peng et al. (2023). Concurrently, researchers pay more attention to align model answers with human preference. By fine-tuning the language model using human-generated demonstrations and subsequent output rankings, researchers developed InstructGPT(Ouyang et al. (2022)), which demonstrated a marked preference for substantially larger models in human evaluations(Mishra et al. (2021); Sanh et al. (2021); Thoppilan et al. (2022); Bai et al. (2024)). Moreover, InstructGPT exhibited improved indicators of authenticity and reduced toxicity in its outputs. Consequently, RLHF has garnered widespread attention Liu et al. (2024b); Li et al. (2024b); Liu et al. (2024a). RLHF is a fusion of two research areas. First, the reward model is optimized based on human preferences, ensuring that the model,such as the Bradley-Terry model(Bradley & Terry (1952)),aligns closely with the preferences exhibited in human-preferred datasets. Subsequently, reinforcement learning algorithms, proximal policy optimization (Schulman et al. (2017)) are employed to fine-tune the language model to maximize the given reward. The cornerstone of most RLHF is PPO algorithm, which is known for its effectiveness. Its appeal lies in its iterative approach, alternating between data sampling through environmental interactions and optimization of the surrogate objective function via stochastic gradient ascent. However, using reinforcement learning for large language models still has relatively high costs and problems for fine-tuning and practical use."}, {"title": "DENSER REWARD METHODS", "content": "Exploration of methods to provide denser rewards in RLHF has yielded promising results. Hindsight Experience Replay (HER) was introduced as a technique enabling learning from sparse, binary rewards by treating failures as successes for alternative goals, significantly improving sample efficiency in multi-goal tasks (Andrychowicz et al. (2018)). In the context of language models, the step-wise method(Lightman et al. (2023b); Pan et al. (2023); Uesato et al. (2022b)) was proposed which apply preference optimization to individual reasoning steps rather than entire solutions, and achieve state-of-the-art performance on math benchmarks (Lai et al. (2024a); Wu et al. (2024)).\nA study comparing process-based and outcome-based approaches for training language models to solve math word problems found that process-based supervision or reward models that emulate it is necessary for low trace error rates (Uesato et al. (2022a))."}, {"title": "SELECTIVE TOKEN METHODS", "content": "Several studies have explored selective token methods to improve efficiency and performance in language model training and optimization. Selective Preference Optimization (SePO) was introduced, which uses DPO to estimate a token-level reward function, thereby enabling efficient selection and optimization of key tokens (Yang et al. (2024b)). Selective Language Modeling (SLM) was proposed as a novel approach that focuses on training language models using only high-value tokens identified by a reference model, thereby achieving state-of-the-art results with significantly fewer tokens (Lin et al. (2024)). Token-level Direct Preference Optimization (TDPO) was developed to optimize policy at the token level for better alignment with human preferences, incorporating forward KL divergence constraints for each token and utilizing the Bradley-Terry model for token-based rewards"}, {"title": "CONCLUSION", "content": "In this paper, we propose a new RLHF method that partitions sequences into different sub-subsequences, which we call adaptive message-wise RLHF. Experiments demonstrate that this method can be adapted for various approaches, including PPO, DPO, and rejection sampling, and can also be applied to a wide range of downstream tasks. Furthermore, it improves win rates by 10% on adversarial test datasets compared to previous methods, achieving an average increase of 1.6% across multiple objective evaluation datasets. This indicates that we have successfully unified various fine-grained supervision methods within a single framework.\nIn the future, we plan to make two improvements based on this work. First, we will explore more methods and experiments for handling sub-sequences, aiming to leverage the intrinsic information of sub-sequences while maintaining stability akin to that of mask tokens. Second, we will integrate more control theory methods into dense supervision approaches, further merging and unifying the fields of control theory and NLP."}, {"title": "MORE DETAILS TO PRMS", "content": null}, {"title": "TRAINING DETAILS", "content": "We fine-tune our PRMs by adjusting the policy model to estimate scores based on sequences of labeled steps. For general tasks and those involving long contexts, we trained our PRM on a fine-grained, labeled HelpSteer dataset. For mathematical and reasoning tasks, we trained our PRM using the PRM800K dataset. All of our PRMs underwent training for two epochs.\nOur PRM training methodology follows the approach outlined in Lightman et al. (2023b)."}, {"title": "DATASET", "content": "We conducted fine-grained annotations on HelpSteer(Wang et al. (2023a)) at the sentence level, scoring each sentence across 5 dimensions (includes: helpfulness, correctness, coherence, complexity, and verbosity.). This allows for step-by-step supervision similar to PRM800K.The data example is as follows\nCriterion 1:\nInstruction\n\"Summarize the following conversation between a high school teacher and the soccer team coach. Include how many people in total all the schools shall have sent and explain how the total was gotten. Teacher: Are you well prepared for the soccer tournament?\nCoach: Yes, we are well prepared.\nTeacher: How many schools are competing in the tournament?\nCoach: There are a total of 5 schools in the tournament.\nTeacher: How many teams does our school have?\nCoach: We have one girls team and one boys team.\nTeacher: How many players are on each team?\nCoach: Each team has 11 players.\nTeacher: Alright. How many teams do the other schools have?\nCoach: The other teams also have two teams each, with 11 players per team. Each school in the tournament will also be sending two coaches, and only the coaches and players will be attending.\nTeacher: Okay, I will communicate this with the head of department so that we can have proper arrangements for the tournament. Continue preparing so that we can win. I wish you all the best. Coach: Thank you.\"\nChosen\n\"The teacher inquired about the soccer tournament and wants to know if the school is well-prepared. The coach assures them that they are well prepared for the tournament. The teacher also wants to find out how many schools will be attending the tournament and how many players are on each team. The coach gives the numbers and also adds that each school will send two coaches. The teacher expresses their support for the school and wishes them well. The total number of people in the tournament will be:\nEach school has one girl's team and one boy's team, with 11 players on each team. 2*11 =\n22.\nEach school has two coaches; therefore, there will be 22 players + 2 coaches = 24 people per school.\nThere are five schools in the tournament; therefore, 24*5 = 120.\nAll the schools will have sent 120 people. This total includes both the players and coaches from all five schools.\"\nRejected"}]}