{"title": "JPC: Flexible Inference for Predictive Coding Networks in JAX", "authors": ["Francesco Innocenti", "Will Yun-Farmbrough", "Ryan Singh", "Paul Kinghorn", "Miguel De Llanza Varona", "Christopher L. Buckley"], "abstract": "We introduce JPC, a JAX library for training neural networks with Predictive Coding. JPC provides a simple, fast and flexible interface to train a variety of PC networks (PCNs) including discriminative, generative and hybrid models. Unlike existing libraries, JPC leverages ordinary differential equation solvers to integrate the gradient flow inference dynamics of PCNs. We find that a second-order solver achieves significantly faster runtimes compared to standard Euler integration, with comparable performance on a range of datasets and network depths. JPC also provides some theoretical tools that can be used to study PCNs. We hope that JPC will facilitate future research of PC. The code is available at www.github.com/thebuckleylab/jpc", "sections": [{"title": "1 Introduction", "content": "In recent years, predictive coding (PC) has been explored as a biologically plausible learning algorithm that can train deep neural networks as an alternative to backpropagation [14, 8, 7, 12]. However, with a few recent notable exceptions [6, 10], there has been a lack of unified open-source implementations of PC networks (PCNs) which would facilitate research and reproducibility\u00b9.\nIn this short paper, we introduce JPC, a JAX library for training neural networks with PC. JPC provides a simple, fast and flexible API for training a variety of PCNs including discriminative, generative and hybrid models. Like JAX, JPC follows a fully functional paradigm close to the"}, {"title": "2 Predictive coding: A primer", "content": "Here we include a minimal presentation of PC necessary to get started with JPC. The reader is referred to [14, 8, 7, 12] for reviews and to [1] for a more formal treatment.\nPCNs are typically defined by an energy function which is a sum of squared prediction errors across layers, and which for a standard feedforward network takes the form\n$F = \\sum_{l=1}^{L} ||z_l - f_l(W_l z_{l-1})||^2$ (1)\nwhere $z_l$ is the activity of a given layer and $f_l$ is some activation function. We ignore multiple data points and biases for simplicity.\nTo train a PCN, the last layer is clamped to some data, $z_L := y$. This could be a label for classification or an image for generation, and these two settings are typically referred to as discriminative and generative PC. The first layer can also be fixed to some data serving as a \"prior\", $z_0 := x$, such as an image in a supervised task. In unsupervised training, this layer is left free to vary like any other hidden layer.\nThe energy (Eq. 1) is then minimised in a bi-level fashion, first w.r.t. the activities (inference) and then w.r.t. the weights (learning)\nThe inference dynamics are generally first run to convergence until $\\Delta z_l \\approx 0$. Then, at the reached equilibrium of the activities, the weights are updated via common neural network optimisers such as stochastic GD or Adam (Eq. 3). This process is repeated for every training step, typically for a given data batch. Inference is typically performed by standard GD on the energy, which can be seen as the Euler discretisation of the gradient system $\\dot{z_l} = - \\frac{dF}{dz_l}$. JPC simply leverages well-tested ODE solvers to integrate this gradient flow."}, {"title": "3 Runtime efficiency", "content": "A comprehensive benchmarking of various types of PCN with GD as inference optimiser was recently performed by [10]. For this reason, here we focus on runtime efficiency, comparing standard Euler integration of the inference gradient flow dynamics with Heun, a second-order explicit Runge-Kutta method. Note that, as a second-order method, Heun has a higher computational cost than Euler; however, it could still be faster if it requires significantly fewer steps to converge.\nThe solvers were compared on feedforward networks trained to classify standard image datasets, with different number of hidden layers $H \\in \\{3,5,10\\}$. Because our goal was to specifically"}, {"title": "4 Implementation", "content": "JPC provides both a simple, high-level application programming interface (API) to train and test PCNs in a few lines of code (\u00a74.1) and more advanced functions offering greater flexibility as well as additional features (\u00a74.2). It is built on top of three main JAX libraries:\n\u2022 Equinox [5], to define neural networks with PyTorch-like syntax,\n\u2022 Diffrax [4], to leverage ODE solvers to integrate the gradient flow PC inference dynamics (Eq. 2), and\n\u2022 Optax [2], for parameter optimisation (Eq. 3).\nBelow we provide a sketch of JPC with pseudocode, referring the reader to the documentation and the example notebooks for more details."}, {"title": "4.1 Basic API", "content": "The high-level API allows one to update the parameters of essentially any Equinox network with PC in a single function call jpc.make_pc_step.\nAs shown above, at a minimum jpc.make_pc_step takes a model, an Optax optimiser and its state, and some data. For a model to be compatible with PC updates, it needs to be split into callable layers (see the example notebooks). Also note that an input is actually not needed for unsupervised training. In fact, jpc.make_pc_step can be used for classification and generation tasks, for supervised as well as unsupervised training (again see the example notebooks).\nUnder the hood, jpc.make_pc_step:\n1. integrates the gradient flow PC inference dynamics (Eq. 2) using a Diffrax ODE solver (Heun by default), and\n2. updates the parameters at the numerical solution of the activities (Eq. 3) with a given Optax optimiser.\nDefault parameters such as the ODE solver and a step size controller can all be changed. One has also the option of recording a variety of metrics including the energies and activities at each inference step (see the documentation for more details).\nImportantly, jpc.make_pc_step is already \"jitted\" for performance, and the user only needs to embed this function in a data loop to train a neural network. We also provide convenience, already-jitted functions for testing specific PC models, including jpc.test_discriminative_pc and jpc.test_generative_pc.\nA similar API is provided for hybrid PC (HPC) models [see 13] with make_hpc_step"}, {"title": "4.2 Advanced API", "content": "While convenient and abstracting away many of the details, the basic API can be limiting, for example if one would like to jit-compile some additional computations within each PC training step. Advanced users can therefore access all the underlying functions of the basic API as well as additional features.\nCustom step function. A custom PC training step would look like the following.\nThis can be embedded in a jitted function with any other additional computations. One has also the option of using any Optax optimiser to perform inference. In addition, the user can access other initialisation methods for the activities, the standard energy functions for PC and HPC, and the activity as well as parameter gradients used by the update functions. In fact, this is all there is to JPC, providing a simple framework to extend the library."}, {"title": "Theoretical tools", "content": "JPC also comes with some analytical tools that can be used to both study, and potentially diagnose issues with, PCNs. We now demonstrate this with an example. For deep linear networks (where $f_l$ is the identity at every layer), [3] showed that the energy (Eq. 1) at the inference equilibrium $V_zF = 0$ has the following closed-form solution as a rescaled mean squared error loss\n$F^* = \\frac{1}{2N} \\sum_{i=1}^{N} (y_i - W_{L:1}x_i)^T S^{-1}(y_i - W_{L:1}x_i)$ (4)\nwhere the rescaling is $S = I_{d_y} + \\Sigma_{l=2}^L (W_{L:l})(W_{L:l})^T$, and we use the shorthand $W_{k:l} = W_k ... W_l$ for $l, k \\in 1,..., L$.\n[3] found Eq. 4 to perfectly predict the energy (Eq. 1) at numerical convergence when $\\nabla_z F \\approx 0$. Figure 2 suggests that the theoretical energy can also help determine whether sufficient inference has been performed, in that more inference steps seem to correlate with better test accuracy on MNIST.\nSimilar results are observed on Fashion-MNIST (see Figure 7)."}, {"title": "5 Conclusion", "content": "We introduced JPC, a new JAX library for training a variety of PCNs. Unlike existing frameworks, JPC is extremely simple (<1000 lines of code), completely functional in design, and leverages well-tested ODE solvers to integrate the gradient flow inference dynamics of PCNs. We showed that a second-order solver can provide significant speed-ups in runtime over standard Euler integration across a range of datasets and networks. As a straightforward extension, it would be interesting to integrate stochastic differential solvers, which recent work associates with better generation performance [15, 9]. We hope that, together with other recent PC libraries [10, 6], JPC will help facilitate research on PCNs."}]}