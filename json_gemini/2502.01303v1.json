{"title": "Partial Channel Network: Compute Fewer, Perform Better", "authors": ["Haiduo Huang", "Tian Xia", "Wenzhe zhao", "Pengju Ren"], "abstract": "Designing a module or mechanism that enables a network\nto maintain low parameters and FLOPs without sacrific-ing accuracy and throughput remains a challenge. To ad-dress this challenge and exploit the redundancy within fea-ture map channels, we propose a new solution: partialchannel mechanism (PCM). Specifically, through the splitoperation, the feature map channels are divided into differ-ent parts, with each part corresponding to different oper-ations, such as convolution, attention, pooling, and iden-tity mapping. Based on this assumption, we introduce anovel partial attention convolution (PATConv) that canefficiently combine convolution with visual attention. Ourexploration indicates that the PATConv can completely re-place both the regular convolution and the regular visualattention while reducing model parameters and FLOPs.Moreover, PATConv can derive three new types of blocks:Partial Channel-Attention block (PAT_ch), Partial Spatial-Attention block (PAT_sp) and Partial Self-Attention block(PAT_sf). In addition, we propose a novel dynamic partialconvolution (DPConv) that can adaptively learn the pro-portion of split channels in different layers to achieve bettertrade-offs. Building on PATConv and DPConv, we proposea new hybrid network family, named PartialNet, whichachieves superior top-1 accuracy and inference speed com-pared to some SOTA models on ImageNet-1K classificationand excels in both detection and segmentation on the COCOdataset.", "sections": [{"title": "1. Introduction", "content": "Designing an efficient and effective neural network has re-mained a prominent topic in computer vision research. Todesign an efficient network, many prior works adopt depth-wise separable convolution (DWConv) [13] as a substitutefor regular dense convolution. For instance, some CNN-based models [27, 30] leverage DWConv to reduce themodel's FLOPs and parameters, while Hybrid-based mod-els [11, 26, 37] employ DWConv to simulate self-attentionoperations to decrease computation complexity. However,some studies [5, 20] have revealed that DWConv may suf-fer from frequent memory access and low parallelism dur-ing inference [3], which leads to low throughput.\nConsidering the substantial redundancy between featuremaps [3, 8], to further reduce the computational cost andparameters while improving the model's inference speedand accuracy, we introduce the partial channel mechanism(PCM) to fully exploit the value of feature map channels.PCM primarily splits the feature maps into different partsby a split operation, with each part undergoing differentoperations, followed by an integration by a concatenationoperation. Our insight is that by reasonably allocating dif-ferent operations and using cheaper, efficient operators topartially replace costly, dense operators, it is possible to im-prove the inference speed of a model while further enhanc-ing its accuracy. Specifically, our approach involves replac-ing computationally intensive convolutions with computa-tionally cheaper visual attention to enhance the model's rep-resentation capability and improve inference speed. Basedon this idea, we introduce a novel Partial Attention Convo-lution (PATConv), which can replace both regular convo-lutions and regular visual attention while also reducing themodel's parameters and FLOPs. This approach primarilyintegrates partial convolution with partial visual attention,"}, {"title": "2. Related Work", "content": "Efficient CNNs and ViTs. DWConv is widely adoptedin the design of efficient neural networks, such as Mo-bileNets [12, 27], EfficientNets [30, 31], MobileViT [21],and EdgeViT [23]. Because of its efficiency limitations onmodern parallel devices, numerous works have aimed to im-prove it. For example, RepLKNet [5] uses larger-kernelDWConv to alleviate the issue of underutilized calculations.PoolFormer [38] achieves strong performance through spa-tial interaction with pooling operations alone. Recently,FasterNet [3] reduces FLOPs and memory accesses simul-taneously by introducing partial convolution. Nevertheless,FasterNet does not outperform other vision models in ac-curacy. In contrast, our proposed PartialNet addresses thislimitation by integrating the visual attention into convolu-tion to enhance the accuracy of models.\nVisual Attention. The effectiveness of Vision Trans-formers (ViTs) mainly attributes their success to the roleof attention mechanisms [24, 25]. In visual tasks, attentionmechanisms are commonly categorized into three types:"}, {"title": "Channel Attention, Spatial Attention, and Self-Attention.", "content": "Some works [2, 22, 26, 29] employ various techniques toimplement the Self-Attention mechanism efficiently, e.g.,Linear Attention [2, 32]. Furthermore, the effectiveness ofChannel Attention and Spatial Attention has already beenvalidated in SRM [17], SE-Net [14] and CBAM [34]. Sim-ilarly, we have incorporated attention to the same feature,but in a parallel way with partial attention to mitigate theimpact of element-wise multiplication on overall inferencespeed."}, {"title": "3. Methodology", "content": "In this section, we start by introducing the motivation be-hind enhancing partial convolution with visual attention andpresent our novel Partial Attention Convolution (PATConv)mechanism, which leverages attention on a subset of featurechannels to balance computational efficiency and accuracy.Next, we detail three innovative blocks within PATConv:the Partial Channel-Attention block (PAT_ch), which inte-grates Conv3\u00d73 with channel attention for global spatialinteraction; the Partial Spatial-Attention block (PAT_sp),which combines Conv1\u00d71 with spatial attention to effi-ciently mix channel-wise information; and the Partial Self-Attention block (PAT_sf), which applies self-attention se-lectively to extend the model's receptive field. We fur-ther introduce a learnable dynamic partial convolution (DP-Conv) with adaptive channel split ratio for improved modelflexibility. Finally, we describe the overall PartialNetarchitecture, structured in four hierarchical stages withPATConv-integrated blocks, aimed at achieving a robustspeed-accuracy trade-off across model variants."}, {"title": "3.1. Partial Channel Mechanism", "content": "Generally, designing an efficient neural network necessi-tates comprehensive consideration and optimization fromvarious perspectives, including fewer FLOPs, smallermodel sizes, lower memory access, and better accuracy.Recently, some works (e.g., MobileViTv2 [22] and Effi-cientVit [2]) attempt to combine depthwise separable con-volutions with the self-attention mechanism to reduce themodel parameters and latency. Other works (e.g., Shuf-fleNetv2 [20] and FasterNet [3]) try to reduce FLOPs andimprove inference speed by performing feature extractionusing only a subset of the feature map channels. However,it does not exhibit a noticeable accuracy advantage whencompared to models with similar parameters or FLOPs.Among, FasterNet only uses partial convolution, achievingexceptional speed across various devices. However, we findthat FasterNet simply performs convolution operations onpartial channels, which can reduce FLOPs and latency butleads to limited feature interaction and lack of global infor-mation exchange.\nIn contrast, we comprehensively exploit the potentialvalue within the channels of the entire feature map. Fordifferent partials, we use different operations to further re-duce the model FLOPs while improving accuracy. It canbe called the partial channel mechanism. Based on this, wepropose a new type of convolution that replaces computa-tionally expensive dense convolution operations with cost-effective visual attention, called Partial Attention Convolu-tion (PATConv). Previous research [3, 8] has demonstratedthat redundancy exists among feature channels, making attention operations applied to partial channels a form ofglobal information interaction.\nUnlike regular visual attention methods, our PATConvis more efficient due to using only a subset of channelsfor the computationally expensive element-wise multiplica-tion. Indeed, running two operations in parallel on separatebranches allows for simultaneous computation, optimizingresource utilization on the GPUs [16]. Suppose the inputand output of our PATConv is denoted as $F \\in R^{h\\times w \\times c_{in}}$ and$O\\in R^{h\\times w \\times c_{out}}$ respectively, where $c_{in}$ and $c_{out}$ representthe number of input and output channels, h, w is the heightand width of a channel, respectively. Suppose $C_{in} = C_{out}$,PATConv can be defined as\n$O = PATConv(F) = Conv(F_{in}r_p) \\cup Atten(F_{in}(1-r_p))$ (1)\nwhere the symbol $\\cup$ and Atten denote the concatenation op-eration and the visual attention operation respectively. Therp is a hyperparameter representing the split ratio of thechannels and can be learned adaptively.\nIn addition, PATConv can apply channel-wise andspatial-wise mixing to enhance global information and in-tegrate self-attention mechanisms to expand the model's re-ceptive field to derive three blocks, proving to be highlyeffective.\nPAT_ch: We first propose to integrate Conv3\u00d73 andchannel attention involving global spatial information inter-action, and using an enhanced Gaussian-SE module com-pute channels' mean and variance to squeeze global spatialinformation. Unlike SENet [14], it only considers the meaninformation of the channel and ignores the statistical infor-mation of std. Considering that the feature maps obey anapproximately normal distribution [15] during training, wefully utilize the Gaussian statistical to express the channel-wise representation information, as shown in Fig. 3 (b).\nPAT_sp: Secondly, we integrate spatial attention withConv1\u00d71 because both operations mix channel wise infor-mation. Our spatial attention employs a point-wise convolu-tion to squeeze global channel information into tensor withonly one channel. After passing through a Hard-Sigmoidactivation, this tensor serves as the spatial attention map toweight features. We position PAT_sp after the MLP layer,enabling the Conv1\u00d71 component of PAT_sp to merge withthe second Conv1\u00d71 in the MLP layer during inference, asshown in Fig. 3 (c). This setup further minimizes the impactof attention on inference speed, and its details of the merge"}, {"title": "3.2. Learnable Dynamic Partial Convolution", "content": "For the PATConv, the split ratio rp is a critical hyperparam-eter that significantly influences the parameters and latencyof a model. A too-large rp causes PATConv to degener-ate into a regular convolution, rendering the visual atten-tion component ineffective at capturing global information.Conversely, a too-small rp results in PATConv lacking es-sential local inductive bias information.\nAchieving higher accuracy and throughput at similarcomplexity often necessitates extensive experimentation toidentify an optimal rp. In FasterNet, a default split ra-tio of 1/4 is for all variants. In contrast, we propose adynamic partial convolution (DPConv) in which the rp islearnable. This approach allows a model to adaptively de-termine the optimal rp for different layers during training.The strategies can be modeled by a binary relationship ma-trix $U \\in \\{0,1\\}^{C_{in} \\times C_{out}}$ [39]. The entire matrix U can bedecomposed in into a set of K small matrix $U_k \\in \\{0,1\\}^{2\\times 2}$,where $U_k$ either equal to a 2-by-2 constant matrix of ones 1or equal to 2-by-2 identity matrix I, i.e.,\n$U = U_1 \\otimes U_2... \\otimes U_K$ (2)\n$U_k = g_k1 + (1 \u2212 g_k)I, \\forall g_k \\in g, g = Sign(g)$ (3)\nwhere $\\otimes$ denotes a Kronecker product [1] and \u011f\u2208 RKis a learnable gate vector taking continues value, and g \u2208{0,1}K is a binary gate vector derived from \u011f. Since theSign function is not differentiable, the gate parameters areoptimized using a straight-through estimator [4], similar tothe binary network quantization method, to ensure conver-gence. Suppose Cin = Cout, So K = log2 Cin. DPConv canbe defined as\n$O = DPConv(F) = U \\odot m \\odot W$ (4)\nwhere $\\odot$ denotes elementwise product, the m\u2208 Rein is avector to mask the useless part of U. Since the result ofthe Kronecker product is a power of 2 matrix, the numberof channels in each layer of the model also needs to be 2K."}, {"title": "So, it is not hard to deduce that", "content": "$r_p = \\frac{2\\sum_{i=0}^{K-1}(1-g_k)}{C_{in}} = \\frac{m_i}{ \\begin{cases}\n1 & \\text{if } i < r \\\n0 & \\text{if } i \\geq r\n\\end{cases}}$ (5)\nwhere the gk indicates the k-th component of g. The spe-cific generation process of DPConv is shown in Fig. 4.\nConsidering the constraints of model deployment, e.g.,the parameters and FLOPs, it is necessary to limit thelearned rp to avoid being too large. Therefore, we design aresource-constrained training scheme for DPConv. To sim-plify the calculation, assuming that we only consider thecase of partial convolution. We propose a regularizationterm denoted as \u03da to constrain the computational complex-ity by\n$\\zeta = \\sum_{l=1}^{L} \\zeta_l, \\zeta_l = \\sum_{i=1}^{C_{in}} \\sum_{j=1}^{C_{out}} U_{ij}, U_{ij} \\in U$ (6)\nwhere L denotes the number of DPConv layers, and uij de-notes an element of U. The term \u0123\u012b represents the number ofnon-zero elements in U, measuring the number of activatedconvolution weights of the l-th DPConv layer. Thus, \u03da cantbe treated as a measurement of the model's computationalcomplexity. In fact, it can be deduced that the sum of eachrow or each column of U can be calculated as $\\prod_{k=1}^{K}(1+g_k)$.Substituting it to Eq. (6) gives us\n$\\zeta = \\sum_{l=1}^{L}(\\prod_{k=1}^{K}(1+g_k^l) \\cdot \\prod_{k=1}^{K}(1+g_k^l)) = \\sum_{l=1}^{L} 2^{2 \\cdot \\sum_{k=1}^{K} g_k^l}$ (7)\nwhere ge and K\u00b9 indices gk and K in the l-th layer, respec-tively. Here we suppose ci = Cin = Cout. Let \u03ba =\u03a3Ll=12represent the desire computational complexity of the entirenetwork. By setting \u03b1, we can control the overall complex-ity of the PartialNet. For example, when \u03b1=4, which is equalto the complexity of rp=1/4 in FasterNet. So, a weightedproduct [] to approximate the Pareto optimal problem, \u03b1is a constant value by empirically set. And we have \u03b1 = 0if \u03da < \u03ba, implying that the complexity constraint is satis-fied. Otherwise, \u03b1 = \u22120.01 is used to penalize the modelcomplexity when \u03da > \u03ba.\nAdditionally, the split ratio mask is closely related to thegate vector g. A reasonable Kronecker product can only begenerated when the elements in g are ordered such that allelements of 1 come before those of 1 for DPConv. There-fore, it is necessary to constrain g by incorporating a regu-larization loss term 4, which can be computed by\n$\\psi = \\sum_{l=1}^{L} \\psi_l, \\psi_l =  \\begin{cases}\n  \\sum_{i=0}^{K} g_i & \\text{if } g = 1 \\text{ and } g_{i+1} = 0,\n   0 & \\text{otherwise}.\n\\end{cases}$ (8)\nFinally, our objective is to search a PartialNet model that\n$\\begin{cases}\n \\text{minimize } L(\\{w\\}_{l=1}^L, \\{g\\}_{l=1}^L) \\cdot []^\\alpha + \\psi \\beta, \\\n \\text{subject to } \\zeta < \\kappa.\n\\end{cases}$ (9)\nwhere \u03b2\u2208 (0, 1] is the penalty factor of 4, default is 0.9."}, {"title": "3.3. PartialNet Architecture", "content": "The overall architecture of PartialNet is depicted in Fig. 3,consists of four hierarchical stages, each of which precedesan embedding layer (a regular Conv4\u00d74 with stride 4) or amerging layer (a regular Conv2\u00d72 with stride 2). These lay-ers serve for spatial downsampling and channel number ex-pansion. Each stage comprises a set of PartialNet blocks. Inthe first three stages of the PartialNet, we employ \"Partial-Net Block v1\" including PAT_ch block and PAT_sp block,as shown in Fig. 3 (a). Similarly, we employ \"PartialNetBlock v2\" by replacing PAT_ch with PAT_sf in the last stageand modifying the shortcut connection way to achieve sta-ble training, as shown in Fig. 3 (d).\nIn addition, we maintain normalization or activation lay-ers only after each intermediate Conv1\u00d71 to preserve fea-ture diversity and achieve higher throughput. We also in-corporate batch normalization into adjacent Conv layers toexpedite inference without sacrificing performance. For theactivation layer, the smaller PartialNet variants uses GELU,while the larger PartialNet variants employs ReLU. The lastthree layers consist of global average pooling, Conv1\u00d71,and a fully connected layer. These layers collectively servefor feature transformation and classification. We offer tiny,small, medium, and large variants of PartialNet, which aredenoted as PartialNet-T0/1/2, PartialNet-S, PartialNet-M,and PartialNet-L. These variants share a similar architecturebut differ in depth and width. For detailed specificationsplease refer to Tab. 3 of the appendix."}, {"title": "4. Experiments", "content": "4.1. PartialNet on ImageNet-1k Classification\nSetup. ImageNet-1K is one of the most extensively useddatasets in computer vision. It encompasses 1K common"}, {"title": "4.2. PartialNet on Downstream Tasks", "content": "Setup. We utilize the pre-trained PartialNet as the backbonewithin the Mask-RCNN [10] detector for object detection"}, {"title": "4.3. Ablation Studies", "content": "Partial Attention vs. Full Attention. To prove the su-periority of our PATConv over full attention, we conductcomparative experiments on the PartialNet-T2, as shownin Tab. 3. Specifically, we replace PATConv with corre-sponding regular full attention for comparison. Full atten-tion involves conducting visual attention calculations on allchannels of the input feature map, which is the commonway of conventional visual attention mechanism. The re-sults indicate that our PATConv achieves a superior bal-ance between inference speed and performance comparedto the full attention counterpart. In addition, we adopt Grad-CAM [28] to visualize the attention. Results in Fig. 6 showthat partial visual attention can focus on the target objects.It is feasible to perform attention operations on part chan-nels and confirm the effectiveness of our improved partialchannel attention mechanism.\nEffect of three PATConv blocks. To confirm the indi-vidual effects of our proposed three PATConv blocks, weconducted ablation studies by progressively adding eachblock one by one, as indicated in Tab. 4. The results in-dicate that the three proposed PATConv blocks consistentlyenhance model performance. Additionally, Tab. 5 also pro-"}, {"title": "5. Conclusion", "content": "Feature selection theory shows that there may be a certaindegree of redundancy and correlation between features.While this redundancy does not provide additional infor-mation gain, it can increase computational complexity andheighten the risk of overfitting. Our research builds onthis theory from an implementation perspective, achievinga balance of optimal performance and computationalefficiency. Specifically, we introduce the partial channelmechanism and propose Partial Attention Convolution,which strategically integrates visual attention into the con-volution process to enhance feature utility. Furthermore,we present Dynamic Partial Convolution, an adaptiveapproach that learns optimal split ratios for channels acrossdifferent layers in the model. With these innovations,we develop the PartialNet architecture, which surpassesrecent efficient networks on ImageNet-1K classificationas well as COCO detection and segmentation tasks.This underscores the effectiveness of the partial channelmechanism in achieving an optimal balance between"}, {"title": "6. Overview", "content": "In this supplementary material, we present more explana-tions and experimental results.\n\u2022 Firstly, we provide detailed explanations of our experi-mental setup, the specifics of the three PATConv blocks,and the different PartialNet variants.\n\u2022 Secondly, we present a comprehensive comparison ofthe classification task on the ImageNet-1k benchmark, aswell as object detection and instance segmentation taskson the COCO dataset.\n\u2022 Finally, we provide additional ablation studies for ourproposed Partial Attention Convolution (PATConv) andshow the training process of ConvNext-tiny with andwithout PATConv (i.e., PAT_ch)."}, {"title": "7. Clarifications on Experimental Setting", "content": "Firstly, we provide the ImageNet-1k training and evalua-tion settings in Tab. 8. These settings can be used to repro-duce our main results in Figure 1 of the main paper. Differ-ent PartialNet variants vary in the magnitude of regulariza-tion and augmentation techniques. The magnitude increasesas the model size increases to alleviate overfitting and im-prove accuracy. It is worth noting that most of the workscompared in Figure 1 of the main paper, such as Mobile-ViT, FastNet, ConvNeXt, Swin, etc., also adopt such ad-vanced training techniques (ADT), with some even heavilyrelying on hyper-parameter search. For other models with-out ADT, such as ShuffleNetV2, MobileNetV2, and Ghost-Net, although the comparison is not entirely fair, we include"}]}