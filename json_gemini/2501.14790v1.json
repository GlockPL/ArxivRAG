{"title": "Towards Dynamic Neural Communication and Speech Neuroprosthesis Based on Viseme Decoding", "authors": ["Ji-Ha Park", "Seo-Hyun Lee", "Soowon Kim", "Seong-Whan Lee"], "abstract": "Decoding text, speech, or images from human neural signals holds promising potential both as neuroprosthesis for patients and as innovative communication tools for general users. Although neural signals contain various information on speech intentions, movements, and phonetic details, generating informative outputs from them remains challenging, with mostly focusing on decoding short intentions or producing fragmented outputs. In this study, we developed a diffusion model-based framework to decode visual speech intentions from speech-related non-invasive brain signals, to facilitate face-to-face neural communication. We designed an experiment to consolidate various phonemes to train visemes of each phoneme, aiming to learn the representation of corresponding lip formations from neural signals. By decoding visemes from both isolated trials and continuous sentences, we successfully reconstructed coherent lip movements, effectively bridging the gap between brain signals and dynamic visual interfaces. The results highlight the potential of viseme decoding and talking face reconstruction from human neural signals, marking a significant step toward dynamic neural communication systems and speech neuroprosthesis for patients.", "sections": [{"title": "I. INTRODUCTION", "content": "Brain-computer interface (BCI) is a technology that interprets and conveys the human mind by translating brain signals that encapsulate various aspects of user intention and cognition [1]-[5]. Recent advances in signal processing and generative technologies are being integrated with BCI, giving rise to a novel form of intuitive neural communication represented by speech BCI [6]. This technology aims to decode speech-related neural signals and directly generate text or speech [7], [8]. Speech BCIs are an important and rapidly emerging field with significant potential, not only as assistive technologies for individuals with speech impairments, such as those with aphasia but also as a groundbreaking mode of neural communication for general users [2], [9]\u2013[11].\nAt the same time, advancements in computer vision and speech processing methods have significantly enhanced the generation of realistic talking faces that synchronize with users' speech and facial expressions, providing immersive and interactive visual experiences [12]-[16]. The convergence of these technologies with BCI may open new avenues of exploring how visual speech intentions, such as facial movements and expressions, can be decoded directly from brain signals and transformed into dynamic visual outputs [17]-[19]. This opens up the possibility of expressive neural communication for general users, and significant speech neuroprostheses for language-impaired patients that can provide speech-visual feedback to facilitate rehabilitation [9], [10].\nDespite these promising potentials, most studies have focused on generating fragmented or abstract outputs from neural signals, as challenges remain in translating these signals into dynamic and unconstrained forms [20]. Especially, extracting relevant features to reconstruct complex lip movements or subtle facial expressions from noisy neural signals poses a significant challenge with great potential for advancement. While signal processing techniques using invasive measurements have demonstrated their potential for robust decoding of speech intentions, their reliance on surgical procedures limits their application [10], [19]. Non-invasive methods such as electroencephalography (EEG) remain underexplored due to their low signal-to-noise ratio (SNR), despite offering significantly broader applicability and potential benefits [21]\u2013[23]. Therefore, developing techniques that can effectively process non-invasive neural signals despite low SNR is crucial for adaptable neural communication and speech neuroprosthesis [17]. To achieve this, a novel approach capable of robustly capturing subtle information is necessary to enhance both the fidelity and the ability to generate unconstrained visual representations from non-invasive brain signals.\nIn this paper, we introduce a diffusion model-based viseme decoding framework that effectively learns visual speech intentions from speech-related EEG signals. The viseme-level decoding demonstrates the potential to generate dynamic outputs from noisy and limited non-invasive neural signals, advancing beyond conventional word- or sentence-level decoding, and offering potential applications for patients with progressive paralysis. Our generative model-based framework goes beyond predefined class decoding, enabling robust decoding through phoneme-level clustering while extending to unconstrained reconstruction by learning small fragments of speech."}, {"title": "II. METHODS", "content": "A. Model Architectures\n1) Denoising Diffusion Probabilistic Models: The overall structure of the model consists of denoising diffusion probabilistic models (DDPMs), a conditional autoencoder (CAE), and a classifier (Fig. 1) [24]. DDPMs are based on a time-conditional U-Net architecture, designed to learn how to represent EEG data by progressively adding noise in a stepwise manner [25]. Unlike the conventional technique of training a model with a noisy $x_t$ and predicting the noise it contains by training network $\\epsilon_\\theta(x_t,t)$, our model is trained to reach the original signal $x_0$:\n$\\mathcal{L}_{DDPM}(\\theta) = ||x_0 - \\epsilon_\\theta(x_t, t)||$"}, {"title": "2) Conditional Autoencoder:", "content": "The information loss of the DDPM is identified and corrected by the CAE [26]. The CAE is composed of an encoder and decoder, denoted as $E_\\phi$ and $D_\\psi$, respectively (Fig. 1). Instead of connecting to the output of $E_\\phi$, $D_\\psi$ is skip-connected with the DDPM layers, enabling $D_\\psi$ to be indirectly influenced by the corruption stage of the DDPM. Additionally, to improve the reconstruction of $\\mathcal{L}_{DDPM}$, the original signal $x_0$ and the DDPM output $x_\\theta(\\epsilon_\\theta(x_t, t))$ are skip-connected to the final layer of $D_\\psi$. To ensure that $E_\\phi$ effectively derives meaningful representations related to visemes, channel attentions are applied after each layer block of $E_\\phi$. This structure learns the importance of each channel in the input features, calculates channel weights, and multiplies them with the original input to generate a weighted output. By doing so, the CAE can generate more accurate representations of the original signals.\n$\\mathcal{L}_{CAE}(\\psi, \\phi) = ||\\mathcal{L}_{DDPM}(\\theta) - D_\\psi(E_\\phi(x_0), x_\\theta(\\epsilon_\\theta(x_t, t)))) ||$"}, {"title": "3) Viseme Decoding Process:", "content": "The output from $E_\\phi$ is compressed into a one-dimensional representation $z$ through an adaptive average pooling layer. This latent vector is then passed to the linear classifier $C_\\rho$. $C_\\rho$, is composed of Kolmogorov-Arnold Networks (KAN), which incorporate linear layers and group normalization [27]. $C_\\rho$ is co-trained with the CAE to distinguish between class representations and perform classification. Only $E_\\phi$ and $C_\\rho$ are employed to classify the signals. Finally, the objective function of the CAE is integrated and the total classification loss is defined as follows:\n$\\mathcal{L}_{total}(\\psi, \\phi, \\rho) = \\mathcal{L}_{CAE}(\\psi, \\phi) + \\alpha||\\hat{y} - Y||^2$"}, {"title": "4) Sentence-level Reconstruction:", "content": "The model was fine-tuned to capture the continuous viseme sequences from the sentence-level EEG signals. The model, initially optimized on isolated trials, is subsequently trained with EEG signal segments that are epoched based on phoneme units derived from the recorded audio signals of spoken sentences [28]. During this process, the weights of the final down-sampling layer of the $E_\\phi$ are updated, while other components remain frozen. This preserves the reconstruction capability of the pre-trained DDPM and $D_\\psi$ for EEG signals while allowing the fine-tuned $E_\\phi$ to extract more salient and meaningful features."}, {"title": "B. Experimental Details", "content": "1) Data Recordings: EEG signals were recorded using a high-density EEG cap with active Ag/AgCl electrodes arranged in 128 channels at a sampling rate of 1,000 Hz. Fifteen distinctive viseme classes were defined, corresponding to 39 phonemes [29]. The dataset comprised 8,100 isolated trials and 7,629 trials from sentence-level windowing from three subjects, each isolated trials lasting 1.5 seconds for each viseme class (Fig. 2). To leverage sentence-level EEG data for viseme training, ten different sentences were recorded, with each sentence lasting 3 seconds and repeated 5 times across the trials. Each subject participated in a series of overt, mimed, and imagined speech. We collected EEG data along with corresponding audio and video, capturing the neural, auditory, and visual components related to visemes. The experiment was conducted in accordance with the Declaration of Helsinki, approved by the Korea University Institutional Review Board [KUIRB-2022-0104-03].\n2) Signal Preprocessing and Training Details: We applied a 5th-order Butterworth bandpass filter in the range of 30-499 Hz containing speech-related information in brain signals [7], [30]. Additionally, notch filtering was employed at harmonics of 60 Hz to remove the line noise. Signal preprocessing was conducted in Python and Matlab, using the OpenBMI Toolbox [31], BBCI Toolbox [32], and EEGLAB Toolbox [33]. To overcome the limitations of training visemes on restricted datasets and short fragments, we leveraged sentence-level EEG data for viseme training. This approach enabled the capture of visemes from continuous EEG at the sentence level, providing a more robust method for viseme decoding. The dataset was randomly split into 8:2 in training and test sets and trained with a fixed random seed for consistency. The backbone architecture training details followed Kim et al. [24]."}, {"title": "III. RESULTS AND DISCUSSION", "content": "A. Viseme Decoding\nTable I displays the average decoding performance across all subjects in overt, mimed, and imagined speech. The evaluation metrics include the viseme error rate (VER), F1-score, and area under the curve (AUC). Overt speech achieved the lowest VER of 34.07%, showcasing strong viseme decoding, while mimed speech, solely relying on lip shapes without auditory output, reached 48.33%. For imagined speech, without external articulation, a VER of 77.96% highlighted the potential to decode internal speech from brain signals. Our method consistently outperformed established networks, such as EEGNet [34], DeepConvNet [35], and ShallowConvNet [35], which have been reported to show robust performance in EEG signal decoding. The ability to decode fragmented segments of EEG signals into small viseme units suggests potential for unconstrained communication [36]. Also, the ability to decode visemes without overt articulation underscores the system's capability to capture and interpret internal cognitive processes related to speech tasks, paving the way for further advancements in silent communication [37], [38].\nB. Phoneme-Viseme Clustering\nFig. 3 presents the t-SNE projection of phoneme encodings from EEG signals into a 2D space, revealing clear clustering patterns based on the corresponding viseme classes. The 39 phonemes were grouped together by their shared lip shapes when articulated, supporting the coherence of these phonetic groupings. Notably, viseme classes with subtle differences were positioned relatively close together, reflecting their phonetic similarity, whereas classes with more pronounced differences in articulation, such as rounded versus unrounded lip shapes, were clearly separated with minimal overlap. These results provide qualitative validation for the classification of 15 distinct viseme categories, indicating that the reduced set of viseme classes adequately captures the variability in lip shapes associated with different phonemes. Furthermore, the clustering patterns underscore that EEG signals effectively encode speech-related lip shape information, demonstrating the model's capacity to distinguish between varying viseme classes based on neural activity. Fig. 4 shows the confusion matrices for each speech task, illustrating the classification performance across different viseme classes. The balanced classification performance across all classes further implied that our study consistently reads user visual speech intentions through viseme decoding across varying levels of speech, providing a foundation for future development in EEG-based visual speech interfaces [39].\nC. Face-to-face Neural Communication\nWe extended our approach to continuous sentence-level data, moving beyond isolated trial classification (Fig. 5). In natural sentence articulation, lip movements change rapidly, therefore, we inferred these changes in short time-step segments, sequentially combining the classification outputs for each segment and reconstructing continuous viseme sequences. By aligning the decoded visemes with the timing and context of actual sentence articulation, our system successfully produced outputs that were not only visually consistent but also highly realistic in terms of lip synchronization and expressiveness. This demonstrated that the proposed framework holds the potential to decode and reconstruct speech-related movements from continuous brain signals, even for previously unseen sentences [11]. For the predefined sentences used in the experiment, the system successfully inferred them in their complete form, proving that multi-output generation in text or audio formats could be feasible [23]. While further research is needed, these findings provide evidence for the feasibility of face-to-face neural communication, demonstrating that our approach can serve as a visual interface capable of accurately capturing and expressing users' visual speech intentions directly from brain signals [19]."}, {"title": "IV. CONCLUSION", "content": "We proposed a diffusion-based viseme decoding framework that learns visual speech intentions from non-invasive speech-related brain signals. By mapping phonemes into viseme units, our model could capture subtle lip movements and articulation from noisy EEG signals. This allowed for realistic sentence-level reconstruction, generating continuous visemes. The viseme-level decoding showed potential for dynamic and unconstrained output generation from limited neural signals, moving beyond conventional word or sentence decoding. Our study advances the potential of neural signal-based communication systems, particularly for face-to-face interaction using speech-related non-invasive brain signals. The findings may mark a step toward developing intuitive neural communication systems and speech neuroprostheses."}]}