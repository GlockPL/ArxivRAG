{"title": "Decoding AI Judgment: How LLMS Assess News Credibility and Bias", "authors": ["Edoardo Loru", "Jacopo Nudo", "Niccol\u00f2 Di Marco", "Matteo Cinelli", "Walter Quattrociocchi"], "abstract": "Large Language Models (LLMs) are increasingly used to assess news credibility, yet little is known about how they make these judgments. While prior research has examined political bias in LLM outputs or their potential for automated fact-checking, their internal evaluation processes remain largely unexamined. Understanding how LLMs assess credibility provides insights into AI behavior and how credibility is structured and applied in large-scale language models. This study benchmarks the reliability and political classifications of state-of-the-art LLMs-Gemini 1.5 Flash (Google), GPT-40 mini (OpenAI), and LLaMA 3.1 (Meta) against structured, expert-driven rating systems such as NewsGuard and Media Bias Fact Check. Beyond assessing classification performance, we analyze the linguistic markers that shape LLM decisions, identifying which words and concepts drive their evaluations. We uncover patterns in how LLMs associate credibility with specific linguistic features by examining keyword frequency, contextual determinants, and rank distributions. Beyond static classification, we introduce a framework in which LLMs refine their credibility assessments by retrieving external information, querying other models, and adapting their responses. This allows us to investigate whether their assessments reflect structured reasoning or rely primarily on prior learned associations.", "sections": [{"title": "Introduction", "content": "In a digital environment where information is constantly produced and consumed [1, 2] and users interact and discuss [3, 4], assessing the credibility of sources is a key challenge [5-7]. The way reliability is determined influences public trust [8, 9], shapes social and political discussions [4, 10, 11], and affects decision-making in critical areas like public health [12, 13]. While human evaluators rely on structured criteria to assess credibility [14-16], the rise of Large Language Models raises new questions about how these systems process, interpret, and replicate such judgments.\nNews rating agencies like NewsGuard and Media Bias Fact Check (MBFC) provide structured, expert-driven assessments of news reliability. These assessments are based on rigorous evaluation criteria, such as factual accuracy, transparency, and editorial independence, and are developed through years of systematic work by human evaluators [17]. These benchmarks serve as operational gold standards in media assessment, widely used by researchers, platforms, and policymakers [18]. However, their reliance on human expertise makes them costly and time-consuming [19, 20].\nOn the other hand, LLMs, including GPT-40 (OpenAI) [21], Gemini 1.5 Flash (Google) [22], and LLaMA 3.1 (Meta) [23], have demonstrated advanced capabilities in tasks such as text classification [24-27], sentiment analysis [28], and fact-checking [29-32]. Moreover, recent research has increasingly focused on how human heuristics and biases manifest in artificial intelligence models [33-35]. Beyond surface-level bias detection, studies are also investigating whether LLMs encode psychological traits and value orientations, shedding light on the broader implications of their training data and decision-making processes [36-38].\nThis raises a fundamental question: to what extent do LLMs replicate, diverge from, or even reveal new dimensions of these structured human evaluations? Indeed, little is known about how these models internally process information and build their evaluations. To what extent do LLMs reflect human-driven evaluations' biases, priorities, and heuristics? How do their decision-making processes differ from or align with those of human experts?\nThis study examines how Large Language Models (LLMs) make decisions when evaluating the reliability and political orientation of a sample of 2,302 news outlets. Instead of merely assessing their alignment with expert evaluations, we focus on how these models build them. We address the underlying patterns shaping their reasoning by analyzing the linguistic markers, heuristics, and contextual cues that factor into their classifications. Through a systematic comparison with structured human evaluations (i.e., NewsGuard and MBFC), we explore whether LLMs rely on similar principles or develop distinct strategies for credibility assessment.\nWe also investigate how LLMs behave within an agentic workflow in which models can refine their assessments by retrieving additional information, querying external sources, or interacting with other AI systems. This approach allows us to examine whether LLMs can self-correct, reinforce biases, or adapt their reasoning when faced with new inputs. By integrating these dynamics, we move beyond mere classification and lay the groundwork for a structured human-AI comparison to provide deeper insights into the cognitive mechanisms underlying credibility judgments."}, {"title": "Results and Discussion", "content": "We investigate how three state-of-the-art LLMs-Gemini 1.5 Flash, GPT-40 mini, and LLAMA 3.1 405B-encode and apply credibility assessments by comparing their outputs to expert human benchmarks from NewsGuard and MBFC. Rather than merely measuring classification accuracy, we aim to address the underlying processes guiding their evaluations. To ensure a diverse and representative dataset, we select 7,715 English-speaking news domains, evenly split between those labeled as Reliable and Unreliable by NewsGuard. These sources span multiple countries and include outlets with national or international focus. We retrieve a snapshot of each domain's homepage, filtering out nonessential elements (e.g., scripts, styling) to isolate relevant textual components, such as news headlines and descriptions. This pre-processing step ensures that all LLMs are evaluated based on the same contextual information a human assessor might use. The final dataset consists of 2,302 active domains with sufficient content for classification. By analyzing not only the final classification labels assigned by the LLMs but also the process behind their assessments, we aim to provide deeper insights into how these models encode the notion of reliability. A detailed breakdown of the data collection and processing is provided in Methods.\nWe begin our assessment by querying each model using a zero-shot, closed-book approach, meaning no prior examples or explicit definitions of reliability are provided. This ensures that the models, without further context, solely rely on their internalized knowledge and learned heuristics to classify news outlets. By doing so, we aim to investigate these models' interpretative framework and assess how their reliability assessments mirror or diverge from structured human evaluations. To this end, beyond a simple binary classification (Reliable or Unreliable), we prompt the models to assign a political orientation label to each news outlet and to justify their assessment by generating explanatory keywords. This additional layer of analysis allows us to explore how LLMs construct reliability assessments, whether their justifications align with human evaluators, and whether emerging discrepancies can be observed in their decision-making. Further, as detailed in Methods, we use the same prompt for all three LLMs to allow for a direct comparison between models. Finally, we introduce an agentic framework where LLMs refine their credibility assessments by retrieving external information, interacting with other models, and adapting their responses. This approach allows us to examine whether LLMs apply structured reasoning beyond their internalized priors and sets up the conditions for a direct comparison between human and AI-driven evaluation strategies."}, {"title": "LLMs vs. Expert-Driven Assessments", "content": "In Fig. 1A, we illustrate how the classifications of each model compare with the reliability ratings assigned by NewsGuard. It is important to note that NewsGuard's ratings are not arbitrary judgments but the result of a structured, operationalized evaluation framework, developed through rigorous, systematic assessments of news outlets. At the same time, LLMs operate without explicit knowledge of these guidelines, meaning their decisions emerge from their internal processes, rather than from strict adherence to predefined criteria."}, {"title": "Explaining Reliability Ratings with Keywords", "content": "We now investigate the main factors driving LLMs' reliability ratings and how they relate to the content of a news outlet's homepage. To achieve this, we analyze three distinct sets of keywords generated by the models for each outlet, alongside their reliability ratings and political orientation. By examining what keywords are used and how they relate to reliability and political orientation, we aim to gain further insights into the mechanisms these models employ to reach their reliability evaluation. Unlike human evaluators, LLMs do not explicitly follow predefined scoring guidelines, so exploring the patterns they exhibit when assigning reliability ratings is essential.\nFor all domains, each LLM is tasked to provide three types of keywords. The first set of keywords, referred to as \"classification keywords\", reflects the model's rationale behind its classification and summarizes its rating. The second set, \"determinant keywords\", comprises terms extracted directly from the domain's homepage that were critical for the model's reliability judgment. The final set, \"summary keywords\", includes terms that broadly summarize the contents of the domain's homepage. Before analysis, we convert all keywords to lowercase. Importantly, we do not give the models any constraints on the number of keywords to output. By omitting this constraint, we can observe the typical number of keywords each model associates with a given input and examine whether this number varies between \"reliable\" and \"unreliable\" sources or across different models. Furthermore, imposing such a limit may hinder the explainability of each model's reliability ratings by reducing their expressive power."}, {"title": "Agentic Framework for Investigating LLM Decision-Making", "content": "Our analysis shows that LLMs often produce reliability ratings that closely align with expert evaluations from NewsGuard and MBFC. This suggests that these models have developed internal heuristics that approximate human assessments, despite not having explicit access to structured evaluation criteria. However, a critical question remains: how do LLMs actually reach these conclusions?\nA key observation emerges when we prompt the models with nothing more than the URL of a domain, without any extracted content from its homepage [29]. Even in this minimal setting, the models generate reliability ratings that broadly align with those assigned by human evaluators. For instance, Gemini achieves an Fl-score of 0.78-slightly lower than the 0.85 obtained with the HTML homepage and GPT an F1-score of 0.77, instead of 0.79. This raises an important issue: are LLMs actively analyzing information, or are they simply recalling prior associations learned during training? If models can classify a news source without even seeing its content, it suggests that their evaluations may be shaped more by pre-existing knowledge than by real-time evaluation. This makes it difficult to determine whether their classifications are based on an actual assessment of the content or just on patterns they have already internalized.\nTo address this, we introduce a structured agentic workflow designed to probe how LLMs interpret and classify news outlets, which we test with Gemini and GPT on a sample of 71 news outlet domains. Rather than treating these models as black boxes that output a binary reliability label, we create an agent that can actively gather and analyze information before providing a final assessment. We equip this agent with three"}, {"title": "Conclusions", "content": "This study investigates how Large Language Models (LLMs) evaluate news outlet reliability, comparing their judgments to structured human benchmarks provided by NewsGuard and Media Bias Fact Check (MBFC). While prior research has often treated LLMs as potential tools for automating reliability assessments, our findings suggest a broader question: how do these models construct their reasoning, and how does it compare to human evaluative frameworks?\nOur results reveal a strong alignment between LLM classifications and human expert ratings, particularly in identifying \"unreliable\" sources. The models consistently flag domains associated with conspiracy theories, sensationalism, and bias, echoing key criteria used in expert evaluations. However, their classification of \"reliable\" sources is less consistent, revealing differences in how they interpret credibility when contextual signals may be limited. Interestingly, when analyzing how errors in reliability classification are distributed across the political spectrum, we find that right-leaning news outlets tend to be consistently misclassified as \"unreliable\", while the center and the right-leaning as \"reliable\". These results raise critical questions about whether LLMs inherit biases from training corpora, how these biases interact with structured evaluative frameworks, and whether their reasoning patterns reflect genuine assessment or learned associations. This is further corroborated by the models producing similar ratings even when prompted only with domain URLs, rather than the scraped domain homepage.\nBy analyzing keyword usage via their rank-frequency distributions, we further explore how LLMs operationalize reliability. Our findings indicate that all models consistently use certain terms to explain their ratings, as shown by the characteristic heavy-tailed behavior of the distributions. Overall, we find that keywords referring to local news, factual reporting, or neutral language are typically associated with \"reliable\" domains. Conversely, \"unreliable\" domains are often characterized by terms relating to sensationalism, controversies, or bias, which reflect commonly used markers employed by human evaluators to identify low credibility sources. Additionally, our results show that keywords that summarize the contents of the webpage are often common to both reliable and unreliable news outlets, pointing toward the role of tone and framing in the models' reliability evaluations.\nMoving beyond simple classification, we introduce an agentic workflow to investigate how LLMs structure their evaluation procedure when given tools to actively seek information. By equipping an AI agent with a webpage scraper, a search engine, and the possibility to query a LLM for content analysis, we gain a more granular view of how these models reach their conclusions. Analyzing whether the criteria the models decide to evaluate change with the final reliability rating, we find that \u2018Transparency' and 'Bias' emerge as the more commonly evaluated criteria for both reliable and unreliable domains, while 'Language or Tone' or 'Correction Policy' are not as employed. Overall, for both Gemini and GPT we observe no substantial differences between reliable and unreliable news outlets in terms of what criteria are prioritized. Conversely, discrepancies emerge when exploring the relationship between the criteria and the final political orientation label. In this case, certain criteria such as 'Language"}, {"title": "Methods", "content": "All data was collected by downloading the HTML homepages of domains rated by NewsGuard as \u201creliable\u201d or \u201cunreliable\u201d, using the requests library available on Python. These domains have been selected among outlets reported by NewsGuard as English-speaking, based in an English-speaking country (US, GB, CA, AU, NZ), and with a National or International focus. Not all domains could be downloaded, as many were either no longer active at the time of downloading, only accessible from specific regions, or designed in such a way to render automatic scraping difficult.\nThe downloaded pages are then filtered to retain only the information relevant to the LLMs to evaluate the reliability of each domain. This process involves removing unnecessary HTML components, such as scripts, styles, and navigation elements, using the beautifulsoup4 library on Python. The resulting document almost exclusively includes textual content, particularly related to news or information about the outlet. While domain-specific pre-processing techniques can also be implemented, we opted for a more conservative, universal approach that can easily be applied to any news outlet's homepage. The final dataset employed includes a total of 2,302 domains with at least 100 characters of content, with 1,196 rated by NewsGuard as \"reliable\" and the remaining 1,106 as \"unreliable\"."}, {"title": "Model prompting", "content": "We follow a zero-shot, closed-book approach to prompting for all three LLMs we experiment with, by providing no example classifications and no definitions of \"reliability\", respectively. Our prompt, which we use for all models, is split into two parts.\nThe first part gives the model basic instructions regarding the classification task, providing no context to the concept of \"reliability\" beyond mentioning the terms \"reliable\" and \"unreliable\". In the second part, we provide the models with specific"}, {"title": "Agentic workflow", "content": "We implemented the agentic workflow for outlet reliability classification with smolagents, a library for Python developed by Hugging Face. In particular, we implement a so-called Code Agent, which is an agent that performs actions via code writing [40]. By allowing an agent to write its actions in code and providing it with a set of tools that can be utilized via code, we obtain a model that is capable of designing"}]}