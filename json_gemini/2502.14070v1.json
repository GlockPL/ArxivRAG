{"title": "DiffExp: Efficient Exploration in Reward Fine-tuning for Text-to-Image Diffusion Models", "authors": ["Daewon Chae", "June Suk Choi", "Jinkyu Kim", "Kimin Lee"], "abstract": "Fine-tuning text-to-image diffusion models to maximize rewards has proven effective for enhancing model performance. However, reward fine-tuning methods often suffer from slow convergence due to online sample generation. Therefore, obtaining diverse samples with strong reward signals is crucial for improving sample efficiency and overall performance. In this work, we introduce DiffExp, a simple yet effective exploration strategy for reward fine-tuning of text-to-image models. Our approach employs two key strategies: (a) dynamically adjusting the scale of classifier-free guidance to enhance sample diversity, and (b) randomly weighting phrases of the text prompt to exploit high-quality reward signals. We demonstrate that these strategies significantly enhance exploration during online sample generation, improving the sample efficiency of recent reward fine-tuning methods, such as DDPO and AlignProp.", "sections": [{"title": "Introduction", "content": "Reward fine-tuning (Black et al. 2024; Fan et al. 2024; Clark et al. 2024; Prabhudesai et al. 2023; Lee et al. 2023) has recently emerged as a powerful method for improving text-to-image diffusion models (Podell et al. 2023; Rombach et al. 2022). Unlike the conventional optimization strategy of likelihood maximization, this framework focuses on maximizing reward scores that measure the quality of model outputs, such as image-text alignment (Kirstain et al. 2024; Lee et al. 2023; Wu et al. 2023; Xu et al. 2024) and image fidelity (Schuhmann et al. 2022). Several methods including policy gradient (Black et al. 2024; Fan et al. 2024) and direct backpropagation (Clark et al. 2024; Prabhudesai et al. 2023) have been studied for reward maximization. These methods have shown promising results in improving image-text alignment (Black et al. 2024; Fan et al. 2024), reducing undesired biases (Fan et al. 2024), and removing artifacts from generated images (Wu et al. 2023). \nSince this reward-based fine-tuning involves online sample generation, the reward optimization depends on what samples are produced during the generation process. In other words, if samples with good reward signals are not obtained during the generation process, the model will converge slowly due to the lack of these signals. To demonstrate this, we conduct reward-based fine-tuning using the single prompt \u201ca dolphin riding a bike\u201d, which is known to be challenging for reward optimization (Black et al. 2024). In this experiment, we fine-tune Stable Diffusion 1.5 (Rombach et al. 2022) using the policy gradient method (i.e.,"}, {"title": "Related Work", "content": "Fine-tuning diffusion models using reward functions. \nOptimizing text-to-image diffusion models on rewards has been proven effective in supervising the final output of a diffusion model by a given reward. They are especially useful when training objectives are difficult to define given a set of images, such as human preference. Early studies (Lee et al. 2023) use supervised approaches to fine-tune the pretrained models on images, which are weighted according to the reward function. As they are not trained online on examples, recent works (Black et al. 2024; Fan et al. 2024) further explore optimizing rewards using policy gradient algorithms by formulating the denoising process as a multistep decision-making task. Though these RL approaches can flexibly train models on non-differentiable rewards, they might lose useful signals as analytic gradients of many reward functions are, in practice, available. AlignProp (Prabhudesai et al. 2023) and DRaFT (Clark et al. 2024) explored optimizing diffusion models with differentiable rewards of human preference (Kirstain et al. 2024; Wu et al. 2023), effectively improving the image generation quality. In this work, we explore another strategy by focusing on an exploration technique toward a more efficient reward optimization process."}, {"title": "Efficient reward fine-tuning with exploration.", "content": "In reinforcement learning (RL), where agents learn to make decisions by interacting with the environment, exploration is a crucial aspect. Exploration directly impacts the discovery of optimal policies, as it allows agents to gather diverse experiences. Intrinsic Curiosity Module (ICM) (Pathak et al. 2017) motivates exploration by rewarding actions that lead to unseen states, promoting discovery of unexplored areas of the environment. Soft Actor-Critic (SAC) (Haarnoja et al. 2018) enhances exploration by employing entropy regularization, encouraging a wider range of actions. These works promote exploration by driving the agent to gather new experiences. There are works that investigate efficient reward fine-tuning using generative diffusion models as well. A concurrent work (Uehara et al. 2024a) proposed to endorse exploration by promoting diversity, formulating the fine-tuning of diffusion models as entropy-regularized control against pretrained diffusion model. Another work (Uehara et al. 2024b) facilitates exploration by integrating an uncertainty model and KL regularization into the diffusion model tuning process. Inspired by the success of exploration strategy in RL literature and the reward fine-tuning of diffusion models, we propose an effective exploration method for reward fine-tuning of text-to-image diffusion models."}, {"title": "Preliminaries", "content": "Text-to-image diffusion models. Diffusion models (Ho, Jain, and Abbeel 2020) are a class of generative models that represent the data distribution p(x) by iteratively transforming Gaussian noise into data samples through a denoising process. These models can be extended to model the conditional distribution p(x|c) by incorporating an additional condition c. In this work, we consider text-to-image diffusion models that utilize textual conditions to guide image generation. The models consist of parameterized denoising function \\( \\epsilon_{\\theta} \\) and are trained by predicting added noise \\( \\epsilon \\) to the image \\( x \\) at timestep \\( t \\). Formally, given dataset D com-"}, {"title": "Method", "content": "In this section, we introduce DiffExp (DiffusionExplore): an exploration method for efficient fine-tuning of text-to-image diffusion models using rewards. We first describe the problem setup regarding reward optimization of text-to-image diffusion models. Then, we present our approach, which promotes exploration in sample generation by scheduling CFG scale and randomly weighting a phrase of prompt."}, {"title": "Problem Formulation", "content": "We consider the problem of fine-tuning a text-to-image model using a reward model. The goal is to maximize the expected reward r(x0, c) for image xo generated by the model \\( p_{\\theta} \\) using text prompt c. Formally, it can be formulated as follows: \n\\begin{equation} \\max_{\\theta} \\mathbb{E}_{p(c)} \\mathbb{E}_{p_{\\theta}(x_0|c)} [r(x_0, c)], \\end{equation} \nwhere p(c) is the training prompt distribution and \\( p_{\\theta}(c) \\) is the sample distribution for the generated image xo. To achieve this goal, we consider online optimization methods that continuously generate image samples while optimizing models with rewards. Specifically, we utilize two types of reward fine-tuning methods: the policy gradient method (Fan et al. 2024; Black et al. 2024) and direct reward backpropagation method (Clark et al. 2024; Prabhudesai et al. 2023). In the policy gradient method, the denoising process is defined as a multi-step Markov Decision Process, and the transition distribution \\( p_{\\theta} (x_{t-1} | x_t, c) \\) (described in Equation 3) is treated as a policy. Based on this framework, the gradient of the objective in Equation 1 is computed as follows to update the model: \n\\begin{equation} \\mathbb{E}_{p(c), p_{\\theta}(x_0|c)} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log p_{\\theta}(x_{t-1}|x_t, c) r(x_0, c) \\end{equation}"}, {"title": "Dynamic Scheduling of CFG Scale", "content": "To effectively discover good reward signals, it is crucial to generate diverse image samples through exploration. To achieve this, we propose dynamically scheduling the CFG scale of the denoising process during online optimization. As detailed in the preliminaries, the CFG scale controls the trade-off between sample quality and diversity: a high CFG scale yields high-quality but low-diversity samples, whereas a low CFG scale promotes diversity at the cost of reduced quality (see Figure 3). To optimize this balance, we start with a low CFG scale during the early stages of the denoising process and switch to a high CFG scale after the tthres denoising step, as follows: \n\\begin{equation} \\tilde{\\epsilon} _{\\theta} (x_t, t, c) = \\epsilon_{\\theta} (x_t, t) + w(t) \\cdot (\\epsilon_{\\theta} (x_t, t, c) - \\epsilon_{\\theta} (x_t,t)), \\end{equation} \nwhere \n\\begin{equation} w(t) = \\begin{cases} \\omega_{l} & \\text{if } t > t_{\\text{thres}}, \\\\ \\omega_{h} & \\text{if } t \\leq t_{\\text{thres}}, \\end{cases} \\end{equation} \nand \\( \\omega_{l}, \\omega_{h} (\\omega_{l} < \\omega_{h}) \\) are the CFG scale values. Note that in the denoising process, we get Gaussian noise at t = T while the final image is at t = 0. We set \\( \\omega_{l} \\) to an extremely low value and \\( \\omega_{h} \\) to an ordinary CFG value (i.e., 5.0 or 7.5). This dynamic scheduling adaptively balances between image quality and diversity, allowing for generating diverse image samples without sacrificing overall sample quality."}, {"title": "Random Prompt Weighting", "content": "To further promote diverse sample generation, we propose an additional exploration method that alters text prompts. Specifically, we increase the weight of a random word (i.e. token) in the text prompt embedding. This adjustment emphasizes the selected word in the generated image, leading to variations where different elements are highlighted. Consider the text prompt \\\"A dolphin riding a bike\\\" for example, which typically results in images featuring only a dolphin, with the bike often omitted. By increasing the weight of the word \\\"bike\\\", there is a high chance that text-to-image diffusion models generate the image containing a bike. This leads to more complete representations of the original prompt, thereby achieving higher prompt alignment rewards and enhancing image diversity. \nFormally, given a text embedding c with Nwords words, we choose a random word with index \\( 0 < i < Nwords \\). Then, we increase the weight of the chosen word as follows: \n\\begin{equation} c[i] \\leftarrow c_{\\text{null}} + w_{\\text{prompt}} * (c[i] - c_{\\text{null}}), \\end{equation} \nwhere \\( c_{\\text{null}} \\) is the text embedding that corresponds to the empty text (\\\"\\\"), and \\( w_{\\text{prompt}} \\) is the prompt weight. We find that sampling \\( w_{\\text{prompt}} \\) randomly from U(1, 1.2) every time is generally successful."}, {"title": "Experiments", "content": "We design our experiments to investigate the following questions:\n1. Can our exploration method enhance the sample-efficiency of reward fine-tuning methods?\n2. Can our exploration method improve the quality of generated samples?\n3. Can our exploration method exhibit generalization ability for unseen prompts that are not used during fine-tuning?\nBaselines. As discussed in the Problem Formulation section, we evaluate the effectiveness of our exploration method using two types of state-of-the-art reward fine-tuning methods: policy gradient method (Fan et al. 2024; Black et al. 2024) and direct reward backpropagation method (Clark et al. 2024; Prabhudesai et al. 2023). We combine our approach with each of these methods and compare the performance with the original methods. Specifically, we use (1) DDPO (Black et al. 2024) for the policy gradient method and (2) AlignProp (Prabhudesai et al. 2023) for direct reward backpropagation method.\nReward functions. To evaluate our method across different types of rewards, we conduct experiments using two distinct reward functions. First, we utilize an Aesthetic Score (Schuhmann et al. 2022), which is trained to predict the aesthetic quality of images. Following the baseline (Black et al. 2024; Prabhudesai et al. 2023), we use 45 animal names as training prompts for the aesthetic quality task. Second, in order to improve image-text alignment, we employ PickScore (Kirstain et al. 2024), an open-source reward model trained on a large-scale human feedback dataset. Based on the baseline (Black et al. 2024), we use a total of 135 prompts for the image-text alignment task, combining 45 different animal names with 3 different activities (e.g., \"a monkey washing the dishes\"). We provide the entire set of prompts used for training in the supplementary materials."}, {"title": "Conclusion", "content": "In this work, we propose DiffExp, an exploration method for promoting diverse reward signals during the reward optimization of text-to-image models. We demonstrate that adjusting the CFG scale of denoising process and randomly weighting certain phrase within prompts can serve as effective exploration strategy by improving the diversity of online sample generation. We conduct extensive experiments to verify that DiffExp improves both sample efficiency and generated image quality, and demonstrate this across various reward fine-tuning methods such as DDPO or AlignProp. Furthermore, we conduct analysis using more advanced prompt sets such as DrawBench, and apply our method to the more advanced diffusion models such as SDXL, both of which result in significant performance improvements."}]}