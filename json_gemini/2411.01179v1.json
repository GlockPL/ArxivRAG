{"title": "Hollowed Net for On-Device Personalization of Text-to-Image Diffusion Models", "authors": ["Wonguk Cho", "Seokeon Choi", "Debasmit Das", "Matthias Reisser", "Taesup Kim", "Sungrack Yun", "Fatih Porikli"], "abstract": "Recent advancements in text-to-image diffusion models have enabled the person-\nalization of these models to generate custom images from textual prompts. This\npaper presents an efficient LoRA-based personalization approach for on-device\nsubject-driven generation, where pre-trained diffusion models are fine-tuned with\nuser-specific data on resource-constrained devices. Our method, termed Hollowed\nNet, enhances memory efficiency during fine-tuning by modifying the architecture\nof a diffusion U-Net to temporarily remove a fraction of its deep layers, creating a\nhollowed structure. This approach directly addresses on-device memory constraints\nand substantially reduces GPU memory requirements for training, in contrast to\nprevious methods that primarily focus on minimizing training steps and reducing\nthe number of parameters to update. Additionally, the personalized Hollowed Net\ncan be transferred back into the original U-Net, enabling inference without addi-\ntional memory overhead. Quantitative and qualitative analyses demonstrate that our\napproach not only reduces training memory to levels as low as those required for\ninference but also maintains or improves personalization performance compared to\nexisting methods.", "sections": [{"title": "1 Introduction", "content": "Recent research on text-to-image (T2I) diffusion models [1, 2], which generate high-resolution images\nfrom text prompts, has increasingly focused on personalizing and customizing these generative models\neffectively [3\u20137]. A primary approach, termed subject-driven generation [5], involves fine-tuning\npre-trained diffusion models with a few user-specific images to generate varied representations of a\nsubject using simple text prompts. This allows users to create personalized images of specific subjects,\nsuch as family, friends, pets, or personal items, with preferred appearances, backgrounds, and styles.\nSuch capabilities enable creative applications including art renditions, property modifications, and\naccessorization.\nFrom a practical standpoint, implementing subject-driven generation on-device offers significant\nbenefits in efficiency and privacy. By operating independently of congested cloud servers or networks,\nusers can generate personalized images anywhere at no additional cost and do not need to compromise\ntheir privacy as all data and personal information remain on the device."}, {"title": "2 Related Works", "content": null}, {"title": "2.1 Efficient Personalization of T21 Diffusion Models", "content": "Recent research on the personalization of T2I diffusion models has introduced various methods\nto fine-tune the models for generating diverse images of user-specific subjects from a few given\nimages. Two foundational works in this area are Textual Inversion and DreamBooth [3, 5]. Textual\nInversion [3] aims to learn new text embeddings to represent a given subject, while DreamBooth [5]\nproposes fine-tuning an entire diffusion model to align the subject with a unique token.\nBuilding on these foundational works, recent research has focused on enhancing the efficiency of this\npersonalization process, primarily through two approaches. The first approach involves decreasing\nthe number of training steps, mostly by utilizing an additional large pre-trained model. A popular\nmethod is to use a pre-trained image/multi-modal encoder to generate personalized text embeddings\nor image prompts from a user-specific image [9, 12, 10]. Other recent works [8, 11] propose utilizing\na set of pre-optimized LoRA parameters or millions of fine-tuned expert models to pre-initialize for\nefficient fine-tuning or enable zero-shot generation with in-context learning. While these models\ndemonstrate significant reductions in the number of training steps, the requirement for additional large\npre-trained models limits their application to on-device settings. Moreover, models with zero-shot\npersonalization capacities [11, 12, 10] cannot be a one-size-fits-all solution for addressing different\ntypes of user-subject prompts. These models often struggle with flexibility in editing subjects or\nmaintaining subject fidelity, and in these cases, additional fine-tuning with specific subjects is needed\nto further enhance their personalization capacity [11, 12].\nOn the other hand, another stream of work adapts parameter-efficient fine-tuning (PEFT) approaches.\nThese methods demonstrate significant reductions in the number of training parameters by limiting\nupdates to a small subset of model weights in cross-attention layers [7] or further reducing the\nupdating parameters by applying singular vector decomposition to weight matrices [14]. However,\nthese methods are still limited in environments with extremely low computational resources, as they\nrequire backpropagation over large diffusion models and do not reduce memory usage from the model\nweights. Therefore, it is crucial to explore new approaches for personalizing T2I diffusion models in\nresource-limited settings, as we propose with our novel method, Hollowed Net. Notably, our method\ncan be integrated with previously discussed techniques to further improve efficiency based on specific\nresource constraints."}, {"title": "2.2 Fine-Tuning with Side Networks", "content": "The idea of of side-tuning has been introduced by Zhang et al. [15], proposing the training of\na lightweight \"side\" network instead of directly fine-tuning a pre-trained network for adaptation.\nIn terms of efficiency, Cai et al. [16] has demonstrated an additional lightweight residual module\ncan reduce memory overhead associated with the activations of the original network. Similarly,\nAuxAdapt [17] has shown that a small auxiliary network can be fine-tuned to adjust the main\nnetwork's decisions, enabling efficient test-time adaptation for video semantic segmentation tasks.\nIn the context of generative models, LST [18] has demonstrated the effectiveness of side networks for\ndifferent NLP tasks with LLMs by introducing a small side network that takes intermediate activations\nof the main network as input via shortcut connections. However, directly applying LST to diffusion\nU-Nets poses challenges due to varying spatial dimensions, channel sizes, and skip-connections across\nblocks, unlike the consistent dimensions in transformer layers of LLMs. Furthermore, the structural\npruning and specific weight initialization required to construct side-tuning networks complicate\nLST's adaptability for personalized tasks across a range of subjects and domains."}, {"title": "2.3 Layer Pruning of Large Generative Models", "content": "Several concurrent works demonstrate that layer-pruning methods can be applied to generative\nmodels, particularly for NLP tasks. Gromov et al. [19] suggest that for fine-tuning LLM models, up\nto 40% of deep layers can be removed, while still achieving comparable results. The authors propose\nthat the optimal block of layers to prune can be selected based on similarity across layers. Similarly,\nKim et al. [20] also propose a depth-pruning approach by evaluating block-level importance.\nThese approaches differ from ours due to the distinct characteristics of LLMs versus diffusion U-Nets.\nThe aforementioned approaches involve the complete removal of deep layers for both fine-tuning\nand inference, considering that those layers store less critical knowledge. However, our study finds\nthat the deep layers of diffusion U-Nets may be less involved with personalization but still contain\ncrucial high-level image features for generating high-fidelity images. Thus, their removal can lead\nto severe performance degradation, even with additional pre-training [21], as shown in Appendix A.\nThis highlights the importance of our two-stage fine-tuning strategy, which excludes layers during\nfine-tuning to reduce memory overhead while preserving the knowledge from these excluded layers\nthroughout both training and inference stages."}, {"title": "3 Preliminaries", "content": "In this section, we describe some preliminaries on T2I diffusion models. First, we discuss the basics\nof Stable Diffusion (SD) model and how they can be used for fine-tuning. The SD model is a large"}, {"title": "4 Methodology", "content": "In this section, we describe the details of our novel memory-efficient personalization technique,\nHollowed Net, and its fine-tuning strategy. We begin by identifying less significant layers for\npersonalization from diffusion U-Nets. Based on these observations, we explain how to construct\nHollowed Net from a pre-trained U-Net. Next, we present our fine-tuning and inference processes for\nmemory-efficient personalization of T2I diffusion models."}, {"title": "4.1 Analysis of the LoRA Weight Changes per Block of U-Net", "content": "To achieve the goal of reducing the required memory for fine-tuning a diffusion model, we first\nidentify less significant layers in the diffusion U-Net for personalization. Similar to Li et al. [22],\nKumari et al. [7] and Shah et al. [6], we analyze the LoRA weight changes $AW$ in the fine-tuned\nmodel for each block:"}, {"title": "4.2 Hollowed Net", "content": "Based on the aforementioned observations, we propose fine-tuning a layer-pruned U-Net, which\nwe refer to as Hollowed Net, instead of directly fine-tuning the entire diffusion model. The core\nconcept of Hollowed Net involves removing deep layers that are not vital for personalization from\na pre-trained diffusion U-Net. This strategy decreases the need to store the entire model in GPU\nmemory, thereby reducing the memory cost associated with the model's weights.\nHowever, unlike transformer layers in large language models, where input and output maintain\nthe same data structure, the alterations in spatial and channel dimensions in U-Net architectures\ncomplicate the removal of its deep layers in the middle. To address this, we utilize the symmetrical\n\"U-shape\" architecture of the diffusion U-Net, where each down-block layer's output is concatenated\nwith a corresponding up-block layer's input via a skip-connection. This design permits us to select any\nup-block layer skip-connected to a down-block layer and hollow out the middle layers between the\npair, ensuring that the processed information from the remaining down-blocks can still be transferred\nto the remaining up-blocks without the need for additional projection layers to adjust for dimensional\ndifferences. The missing input for the upper layer, due to the removal of the middle layers, is replaced\nwith the pre-computed output from the full diffusion U-Net, which is illustrated in the next section."}, {"title": "4.3 LORA Personalization with Hollowed Net", "content": "To optimize GPU memory utilization, we propose a two-stage fine-tuning strategy: (1) pre-computing\nintermediate activations of the original diffusion U-Net and (2) fine-tuning the Hollowed Net using the\npre-computed activations, as shown in the upper and bottom half of Fig. 1, respectively. Initially, we"}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Experimental Settings", "content": "We conduct experiments following the protocol proposed in DreamBooth [5]. We use a total of 131\nsubjects for experiments, utilizing both the DreamBooth [5] and CustomConcept101 [7] datasets. The\nDreamBooth dataset includes 30 image sets from 15 different classes, each containing 4-6 images of\na given subject. The subjects are divided into living subjects and objects, and 25 different prompts are\nassigned based on this division. Meanwhile, the CustomConcept101 dataset includes 101 image sets,\neach containing 3-15 images of a given subject. The subjects consist of 15 different large categories,\nwith 20 unique prompts assigned to each category. For evaluation, four images with different fixed\nrandom seeds are generated per subject per prompt for both datasets."}, {"title": "5.2 Results", "content": "In this section, we present the results of our proposed Hollowed Net to evaluate its effectiveness\nin terms of both memory efficiency and personalization performance. We conduct experiments\nwith Hollowed Net, applying a hollowed fraction of 39.2%. Architectural details are provided in\nAppendix B. Ablation studies on different fractions of hollowed layers can be found in Sec. 5.3. In\nthe main results, the rank of Hollowed Net is fixed to 128. Experimental results on different ranks are\npresented in Appendix C."}, {"title": "5.3 Ablation Study on Fractions of Hollowed Layers", "content": "Based on the symmetrical \"U-shape\" architecture of the diffusion U-Net, we can design different\nHollowed Net architectures by selecting a different up-block layer skip-connected to a down-block\nlayer and hollowing out the middle layers between the pair. Figure 5 presents experimental results\nacross different fractions of hollowed layers, ranging from around 10% to 85% of layers removed. In\nFig. 5, we observe the peak GPU memory usage decreasing linearly with layer removal, as fewer\nmodel weights need to be stored on the GPU during backpropagation. Analyzing the DINO and\nCLIP-I scores in Fig. 5 (b) and (c), we find that the model's capacity to preserve subject fidelity\nremains comparable to or slightly better than LoRA until around 39.2% of layers are removed,\nwhere memory cost reduces nearly to the level of inference. Beyond this threshold, however, subject\nfidelity significantly diminishes, as fewer layers essential for personalization are included in the\nHollowed Net. This effect of hollowed layer fractions is also visible in the qualitative results in Fig. 6.\nMeanwhile, the CLIP-T score does not exhibit a general trend, except in cases of very high hollowed\nfractions, where the model is not capable of personalization, and thus generates images solely based\non a given prompt. However, note that the increase in CLIP-T remains marginal, as Hollowed Nets\nwith low hollowed fractions also maintain a high capacity for text-image alignment."}, {"title": "6 Conclusion", "content": "In conclusion, our paper introduces a novel approach for on-device personalization through memory-\nefficient fine-tuning with Hollowed Net. Hollowed Net effectively leverages the architecture of the\ndiffusion U-Net, enabling fine-tuning with significantly reduced memory costs by minimizing the\nmodel's size during fine-tuning without requiring any additional processes such as structural pruning\nor pre-training on large-scale datasets. However, we observe that, due to the use of non-personalized\nprompts with the original network, the model's performance can be sensitive to the granularity of\nclass token definitions. For example, the DreamBooth dataset contains \"poop emoji\" images, for\nwhich the class token is very coarsely defined as \"toy\". In this case, non-personalized intermediate\nactivations generated with prompts using \"toy\" struggle to effectively correlate and generate \"poop\nemoji\" image. Therefore, a careful choice of fine-grained class tokens is necessary for the effective\napplication of Hollowed Net.\nAdditionally, it is worth noting that our methodology is orthogonal to existing different PEFT\nmethods [24, 25] and quantization methods [26, 27]. Thus, our approach offers substantial potential\nfor further memory reduction, which is crucial for training under constrained computational resources.\nFurthermore, while our primary focus in this paper has been on image generation tasks, our method\nis not limited to diffusion models and can be seamlessly extended to various NLP tasks with LLMs,\nwhich we leave for future work. We anticipate that Hollowed Net will be applied to a wide range\nof tasks requiring constrained computational resources, serving as an efficient solution for various\non-device applications."}, {"title": "A Experiments with Layer-Pruned Diffusion Models", "content": "As shown in recent work [21], layer pruning involves the complete removal of selected layers, which\nnecessitates extensive pre-training on large datasets to recover lost information and restore model\nfunctionality. However, diffusion models often suffer from substantial performance degradation\npost-pruning, as the lost information may not be fully recoverable through pre-training.\nIn contrast, Hollowed Net does not completely remove deep layers and requires no additional\npre-training. Instead, we temporarily exclude selected layers during fine-tuning while preserving\nessential information through a pre-computation stage. Notably, despite this added stage, the overall\ncomputational load for training Hollowed Net can remain more efficient than LoRA fine-tuning, as\ndiscussed in Sec. 5.2 and Appendix D."}, {"title": "B Architectural Details of Hollowed Net", "content": "In this section, we present the architectural details of Hollowed Net. We leverage the skip connections\ninherent in the U-Net architecture to determine which layers to be removed during fine-tuning\n(hollowed). For our main results, we choose the third block of the down_blocks.2 (block 3-3), the\nentire down_blocks.3 (blocks 4-1 and 4-2), the entire mid_block (blocks 5-1 and 5-2), and the entire\nup_blocks.0 (blocks 6-1, 6-2, 6-3, and 6-4) to be hollowed, which corresponds to around 39.2% of\nthe U-Net's parameters, as described in Fig. 7.\nSimilarly, Hollowed Net with different fractions of hollowed layers can be achieved as follows:\n\u2022 11.5% removed: blocks 5-1 and 5-2 are hollowed.\n\u2022 20.8% removed: blocks 4-2, 5-1, 5-2, and 6-1 are hollowed."}, {"title": "C Experiments with Different Ranks", "content": "In Table 5, we present the results using LORA and Hollowed Net with different ranks (4 and 16)\nusing the DreamBooth dataset. While the default rank of 4 in the diffusers library is often used, we\nhave found that it often oversimplifies personalization details or fails to effectively handle a range\nof challenging subjects and prompts. Increasing the rank from 4 to 16 improves subject fidelity.\nHowever, to achieve personalization quality comparable to full fine-tuning across all subjects and\nprompts, we find that the rank of 128 is necessary."}, {"title": "D Further Analysis on Computational Costs", "content": "In Fig. 8, we provide ablation studies on the impact of varying the number of samples on both\nquantitative and qualitative results. The findings indicate that using only 200 pre-computed samples\nresults in minimal performance degradation compared to using 1000 pre-computed samples.\nAdditionally, we present a detailed analysis of computational loads and space consumption in Table 6\nfor different numbers of precomputed samples and different fractions of hollowed layers, which"}, {"title": "E Experiments with SDXL", "content": "To demonstrate the scalability of Hollowed Net, we present additional analysis and qualitative\nexamples using SDXL [23]. Figure 9 shows that similar patterns of weight changes are observable\nwith SDXL, as displayed in Fig. 2. In Fig. 10, we present qualitative examples of Hollowed Net and\nLORA FT with the samples from the DreamBooth dataset using SDXL. Hollowed Net is applied by\nremoving the entire mid_block layers (410M parameters) of SDXL. The results show that Hollowed\nNet achieves high-fidelity personalization results comparable to LORA FT."}]}