{"title": "LOCAL VS GLOBAL CONTINUAL LEARNING", "authors": ["Giulia Lanzillotta", "Sidak Pal Singh", "Benjamin F. Grewe", "Thomas Hofmann"], "abstract": "Continual learning is the problem of integrating new information in a model while retaining the knowledge acquired in the past. Despite the tangible improvements achieved in recent years, the problem of continual learning is still an open one. A better understanding of the mechanisms behind the successes and failures of existing continual learning algorithms can unlock the development of new successful strategies. In this work, we view continual learning from the perspective of the multi-task loss approximation, and we compare two alternative strategies, namely local and global approximations. We classify existing continual learning algorithms based on the approximation used, and we assess the practical effects of this distinction in common continual learning settings. Additionally, we study optimal continual learning objectives in the case of local polynomial approximations and we provide examples of existing algorithms implementing the optimal objectives.", "sections": [{"title": "INTRODUCTION", "content": "Given the present trend toward training gigantic, foundational models (Achiam et al., 2023), effective continual learning solutions hold the key to reducing the computational costs of ever integrating new information into the model without forgetting older information. The alternative of retraining on the entire data to update the model is not an option at this scale, especially in the case where the models are expected to adapt quickly to a dynamic environment.\nMcCloskey & Cohen (1989) were the first to observe that neural networks trained on a sequence of tasks perform poorly on inputs from temporally antecedent tasks, a phenomenon termed catastrophic forgetting. Several algorithms have since been developed to limit catastrophic forgetting in deep neural networks (De Lange et al., 2021; Khetarpal et al., 2022). However the solutions developed so far struggle in real world scenarios, where satisfying either compute or memory constraints is crucial (Kontogianni et al., 2024; Garg et al., 2023). We believe that a better understanding of the problem of continual learning is needed to design algorithms that can address catastrophic forgetting effectively. Moreover, studying the failure cases of existing continual learning algorithms can point the way towards principled solutions.\nIn this work, we view continual learning as the problem of approximating the multi-task loss and we study existing continual learning algorithms in this light. In particular, we are interested in distinguishing between local and global approximations. Said simply, local approximations rely on information about the state of the network after learning each task, whereas global approximations don't. This characteristic implies that algorithms employing local approximations need to satisfy a restrictive assumption, which we bring to the surface. We classify continual learning algorithms as either local or global, based on the properties of the underlying approximation, and we evaluate experimentally the practical consequences of the two approximation schemes.\nContributions and paper structure. After covering some background (Section 2) we introduce the formulation of continual learning from the perspective of loss approximation (Section 3.1). In Section 3.2 we discuss two mutually exclusive approximation strategies, namely local and global approximations and we define the locality assumption, which is unavoidable for any algorithm employing a local approximation. Next (Section 4), we study the case of continual learning algorithms using polynomial local approximations, and we derive optimal objectives for the quadratic case. In Section 5 we consider a few classic examples from the literature and we classify them as either local or global algorithms. Further, we show formally that orthogonal gradient descent (Farajtabar et al., 2020) implements the optimal objective for local quadratic approximations derived in Section 4. Finally, we provide experimental evidence of our claims in Section 6, and we evaluate the practical implications of local and global approximations in common continual learning settings."}, {"title": "BACKGROUND", "content": "Consider a set of supervised learning tasks $T_1,..., T_T$, with the data associated to the t-th task $T_t$ being $D_t = {\\{(x,y) \\in X_t \\times Y_t\\}}$. Continual learning algorithms learn the tasks sequentially, whereby at each learning step they utilize the present task data to update the model, which is typically a neural network with parameters $\\theta$. In order to avoid forgetting the old tasks while learning a new one, the algorithm can often access an external memory, which is also updated after every task. The content of this memory may differ across algorithms. The performance on each task is measured by a task-specific loss function $l_t$, and the objective of each task is to minimise the expected loss: $L_t(\\theta) = \\langle l_t(x, y, \\theta) \\rangle_{P_t}$, where $\\langle \\cdot \\rangle_{P_t}$ denotes the average over the task distribution $P_t$. In practice, the expectation is approximated by an average over the dataset, which is called the task empirical loss $L_t(\\theta) = \\langle l_t(x, y, \\theta) \\rangle_{D_t}$. The continual learning problem is to minimise the multi-task loss:\n$\\frac{1}{t}L_{MT}(\\theta) = \\langle L_1(\\theta) + ... + L_t(\\theta) \\rangle$  (1)\nwhile only having direct access to the current task data $D_t$ and the external memory. Other objectives may be considered instead of the multi-task loss, such as the average lifetime performance. We choose to use the multi-task loss objective in order to conform with the historically prevalent practice in the literature.\nNotation & Metrics. We use $\\theta \\in \\Theta_t$ to refer to a generic vector in the parameter space and $\\Theta_t$ to the value of the network parameters after t learning steps. We index the current task with t and any single old task with o. Given a current parameter vector $\\theta$ catastrophic forgetting on task $T_o$ may be measured by the signed difference in expected loss $\\mathcal{E}_o(\\theta) = L_o(\\theta) - L_o(\\theta_o)$. Similarly, the empirical approximation of $\\mathcal{E}_o(\\theta)$ is the signed change in the empirical task loss $E_o(\\theta) = L_o(\\theta) - L_o(\\theta_o)$. Alternatively, forgetting can be measured in terms of the task test accuracy as $\\mathcal{E}_{acc}(\\theta) = ACC_o(\\theta_o) - ACC_o(\\theta)$. Notice that in both cases a lower value of forgetting is better. Additionally, we denote the average forgetting after t steps by $\\mathcal{E}(t) = \\sum_{o=1}^{t-1} \\mathcal{E}_o(\\theta_t)$ and the average empirical forgetting as $E(t)$. Likewise, the average accuracy after t steps by $ACC(t) = \\frac{1}{t}\\sum_{o=1}^{t-1} ACC_o(\\theta_t)$. We refer the reader to Section 9 (Appendix A) for a full overview of the notation used."}, {"title": "LOCAL AND GLOBAL APPROXIMATIONS IN CONTINUAL LEARNING", "content": "3.1 PROBLEM FORMULATION\nIn this work, we view continual learning algorithms from the perspective of loss approximation, which we introduce in this section. Consider the problem of minimizing the multi-task expected loss in Equation (1). If hypothetically all the data were available, one could approximate the distribution multi-task loss $L_{MT}(\\theta)$ by its empirical version $\\hat{L}_{MT}(\\theta)$ and use it as the optimization objective, as is common practice in machine learning. However, a continual learning algorithm can only access the current task data $D_t$ and the content of its external memory $M_t$, therefore, its objective should be, strictly speaking, a function of $D_t$ and $M_t$ alone: $obj_t(D_t, M_t)$.\nIn this work, we are interested in the way in which the (explicit or implicit) objective of a continual learning algorithm $obj_t(D_t, M_t)$ approximates the true objective, i.e. the multi-task loss. In particular, we consider the relation between $L_{MT}(\\theta)$ and $obj_t(D_t, M_t)$ in order to focus on the locality (or non-locality) of the approximation, and we set aside a discussion of the generalization gap (i.e., whether $\\hat{L}_{MT}(\\theta) \\approx L_{MT}(\\theta)$) . Accordingly, we view the continual learning problem as follows:\n$\\min_{\\Theta \\in \\Theta} obj_t (D_t, M_t)(\\theta) \\quad s.t. \\quad obj_t(D_t, M_t)(\\theta) \\approx L_{MT}(\\theta)$  (2)\nTo conform with existing algorithms, we hereafter study approximate objectives $obj_t(D_t, M_t)$ of the form:\n$obj_t (D_t, M_t) = \\hat{L}_{MT}(\\theta) := \\frac{1}{t} (\\hat{L}_1(\\theta) + ... + \\hat{L}_{t-1}(\\theta) + L_t(\\theta))$  (3)\nIn short, each previous task loss is approximated separately as $L_i(\\theta) \\approx \\hat{L}_i(\\theta)$. We want to stress that such an approximation $\\hat{L}_i(\\theta)$ need not always be explicit. For some cases, we infer the approximation used from the objective which the algorithm effectively minimizes at each learning step. This distinction will become clear in Section 5, where we review examples from the literature.\n3.2 LOCAL AND GLOBAL APPROXIMATIONS\nWe are now ready to introduce the central point of our discussion. Starting from the formulation of continual learning introduced in the previous section (Equation (2)), we are interested in asking whether the task loss approximation $L_t(\\theta)$ is local or global."}, {"title": "CASE STUDY: LOCAL POLYNOMIAL APPROXIMATIONS", "content": "A general example of local approximation of $L_t(\\theta)$ is the Taylor expansion around $\\theta_t$:\n$\\hat{L}_t(\\theta) = L_t(\\theta) + (\\theta - \\theta_t)^T \\nabla L_t(\\theta_t) + \\frac{1}{2} (\\theta - \\theta_t)^T \\frac{\\partial^2 L_t(\\theta)}{\\partial \\theta^2} (\\theta - \\theta_t) + ...$  (5)\nThe approximation error for a p-order approximation is upper bounded by $O(||\\theta - \\theta_t||^P)$, and thus the $\\epsilon$-region is simply a p-norm ball around $\\theta_t$ with radius proportional bounded by $\\epsilon$: $\\Omega = {\\theta : C \\cdot ||\\theta - \\theta_t||^P < \\epsilon}$. A convenient property of this approximation is that it allows us to express forgetting in terms of the loss derivatives in $\\theta_t$. Indeed forgetting is simply $\\mathcal{E}_t(\\theta) = L_t(\\theta) - L_t(\\theta_t)$.\n4.1 QUADRATIC LOCAL APPROXIMATIONS\nA wealth of studies have shown that for over-parameterized networks the loss tends to be very well-behaved and almost convex in a reasonable neighborhood around the minima (Saxe et al., 2013; Choromanska et al., 2015; Jacot et al., 2020). Therefore in such cases a quadratic approximation of $L_t(\\theta)$ might be accurate.\nIf we write the Hessian matrix $\\frac{\\partial^2 L_t(\\theta)}{\\partial \\theta^2}$ as $H_t$ and denote the change in parameters due to learning a new task $\\theta_t - \\theta_{t-1}$ as $\\Delta_t$, following Equation (5) we can write the forgetting on task o after learning t as:\n$E_o(\\theta_t) = (\\theta_t - \\theta_o)^T \\nabla L_o(\\theta_o) + \\frac{1}{2} (\\theta_t - \\theta_o)^T H_o^* (\\theta_t - \\theta_o)$  (6)"}, {"title": "LOCAL AND GLOBAL ALGORITHMS IN THE LITERATURE", "content": "We now review some existing continual learning algorithms, classifying them into local and global algorithms. We select a few well known exemplars, representing different families of algorithms according to popular taxonomies of the literature (De Lange et al., 2021).\n5.1 GLOBAL ALGORITHMS\nExperience Replay (ER) is one of the oldest (Robins, 1995) and still one of the most effective (Buzzega et al., 2021) algorithms for continual learning. Although several variants have been proposed (Rebuffi et al., 2017; Lopez-Paz & Ranzato, 2017; Shin et al., 2017; Van de Ven et al., 2020) for now we consider its simplest form. For each task, a random subset of the dataset $S_t \\subset D_t$ is stored in an external bufferto approximate the task loss $L_t(\\theta)$ as follows:\n$\\hat{L}_t(\\theta) = \\frac{1}{|S_t|} \\sum_{x,y \\in S_t} l_t(x, y, \\theta)$  (13)\nThe objective of each learning step is that of Equation (3). As long as the buffer sampling strategy does not depend on the network parameters after learning the task, the approximation used by experience replay is global. The approximation error is a function of the buffer size and it has been observed that in most cases the algorithm is effective also when the buffer size is small (Buzzega et al., 2020; 2021).\nGradient Episodic Memory (GEM) (Lopez-Paz & Ranzato, 2017; Chaudhry et al., 2018) employs the same task loss approximation in a different way. Instead of directly minimizing the multi-task loss, the objective of GEM is:\n$\\min_{\\Delta_t \\in \\Theta} L_t(\\theta_{t-1} + \\Delta_t) \\quad s.t. \\quad L_o(\\theta_{t-1} + \\Delta_t) \\leq \\hat{L}_o(\\theta_{t-1}) \\quad \\forall o < t$  (14)\nIn other words, the parameter update $\\Delta_t$ does not minimise the approximate task loss $\\hat{L}_o$ but it does not increase it (thus avoiding catastrophic forgetting). In order to enforce the constraint in Equation (14), GEM uses a local linearization of the old task loss, which is updated after each optimization step. The linearization may be inaccurate when learning with large gradient step sizes, and result in reduced performance. Nevertheless, the task loss approximation is based on the current state of the network rather than its state after learning the task, which makes this algorithm global.\nSynaptic Intelligence (SI) (Zenke et al., 2017) uses a quadratic approximation of the old task loss, centered around the previous task solution. For a current task t the approximation of $L_o$ is:\n$\\hat{L}_o(\\theta) = (\\theta - \\theta_{t-1})^T H_o (\\theta - \\theta_{t-1})$  (15)\nwhere $H_o$ is a diagonal matrix updated at each gradient step, which roughly estimates the task Hessian matrix evaluated at $\\theta_{t-1}$. The objective of each learning step is that of Equation (3). The approximation of $L_o$ is updated after each task based on the current state of the network $\\theta_t$, therefore SI also belongs to the group of global algorithms. Similarly to GEM, SI is sensitive to large step sizes, as the quadratic approximation may be inaccurate if the distance travelled in the parameter space is large.\nProgressive Neural networks (PNN) (Rusu et al., 2016) belong to the category of network expansion methods, which dynamically allocate new parameters of the neural network to each task. Specifically, PNN subsequently adds 'columns' (parametrized by feed-forward networks) to the neural network with unilateral connections between them. Although the whole network is used to produce outputs, only the parameters of the last column are trained on the current task, while the others are frozen. In more general terms, let $\\Theta^1, . . . , \\Theta^t$ denote the parameter subspace associated to each task. By design the modified derivative of the task loss $L_t$ is:\n$\\frac{\\partial \\hat{L}_t(\\theta)}{\\partial \\theta_i} =  \\begin{cases} \\frac{\\partial L_t(\\theta)}{\\partial \\theta_i} \\quad \\text{if } \\theta_i \\in \\Theta^t \\\\  0 \\quad \\text{if } \\theta_i \\notin \\Theta^t \\end{cases}$  (16)"}, {"title": "EXPERIMENTS", "content": "Experimental setup. With our experiments we want to evaluate the practical side of the theoretical distinction between local and global algorithms. We take several classic algorithms in the literature which are representative of the different families of algorithms, namely: Expericence replay (ER), Averaged GEM (A-GEM, a computationally cheap variant of GEM), Elastic Weight Consolidation (EWC), Synaptic Intelligence (SI), iCarl and Orthogonal Gradient Descent (OGD), which were discussed in Section 5. We do not include Progressive Neural networks (PNN) in the experiments because we are interested in forgetting, which by design is always 0 for PNN."}, {"title": "CONCLUSION", "content": "In summary, in this work, we view continual learning from the point of view of the multi-task loss approximation and we study the differences between local and global approximations. Based on this analysis, we provide a classification of existing algorithms into local and global algorithms, and we evaluate the practical consequences of our theoretical distinction through extensive experiments. We believe our results are of interest to both empirical and theoretical research in continual learning. In general, we find that the loss approximation viewpoint has not been sufficiently developed in the literature, and with this work, we aim to demonstrate that it offers powerful abstractions of continual learning algorithms."}]}