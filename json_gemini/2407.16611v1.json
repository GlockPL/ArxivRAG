{"title": "LOCAL VS GLOBAL CONTINUAL LEARNING", "authors": ["Giulia Lanzillotta", "Sidak Pal Singh", "Benjamin F. Grewe", "Thomas Hofmann"], "abstract": "Continual learning is the problem of integrating new information in a model while retaining the knowledge acquired in the past. Despite the tangible improvements achieved in recent years, the problem of continual learning is still an open one. A better understanding of the mechanisms behind the successes and failures of existing continual learning algorithms can unlock the development of new successful strategies. In this work, we view continual learning from the perspective of the multi-task loss approximation, and we compare two alternative strategies, namely local and global approximations. We classify existing continual learning algorithms based on the approximation used, and we assess the practical effects of this distinction in common continual learning settings. Additionally, we study optimal continual learning objectives in the case of local polynomial approximations and we provide examples of existing algorithms implementing the optimal objectives.", "sections": [{"title": "1 INTRODUCTION", "content": "Given the present trend toward training gigantic, foundational models (Achiam et al., 2023), effective continual learning solutions hold the key to reducing the computational costs of ever integrating new information into the model without forgetting older information. The alternative of retraining on the entire data to update the model is not an option at this scale, especially in the case where the models are expected to adapt quickly to a dynamic environment.\nMcCloskey & Cohen (1989) were the first to observe that neural networks trained on a sequence of tasks perform poorly on inputs from temporally antecedent tasks, a phenomenon termed catastrophic forgetting. Several algorithms have since been developed to limit catastrophic forgetting in deep neural networks (De Lange et al., 2021; Khetarpal et al., 2022). However the solutions developed so far struggle in real world scenarios, where satisfying either compute or memory constraints is crucial (Kontogianni et al., 2024; Garg et al., 2023). We believe that a better understanding of the problem of continual learning is needed to design algorithms that can address catastrophic forgetting effectively. Moreover, studying the failure cases of existing continual learning algorithms can point the way towards principled solutions.\nIn this work, we view continual learning as the problem of approximating the multi-task loss and we study existing continual learning algorithms in this light. In particular, we are interested in distinguishing between local and global approximations. Said simply, local approximations rely on information about the state of the network after learning each task, whereas global approximations don't. This characteristic implies that algorithms employing local approximations need to satisfy a restrictive assumption, which we bring to the surface. We classify continual learn-ing algorithms as either local or global, based on the properties of the underlying approximation, and we evaluate experimentally the practical consequences of the two approximation schemes.\nContributions and paper structure. After covering some background (Section 2) we introduce the formulation of continual learning from the perspective of loss approximation (Section 3.1). In Section 3.2 we discuss two mutually exclusive approximation strategies, namely local and global approximations and we define the locality assumption, which is unavoidable for any algorithm employing a local approximation. Next (Section 4), we study the case of continual learning algorithms using polynomial local approximations, and we derive optimal objectives for the quadratic case. In Section 5 we consider a few classic examples from the literature and we classify them as either local or global algorithms. Further, we show formally that orthogonal gradient descent (Farajtabar et al., 2020) implements the optimal objective for local quadratic approximations derived in Section 4. Finally, we provide experimental evidence of our claims in Section 6, and we evaluate the practical implications of local and global approximations in common continual learning settings."}, {"title": "2 BACKGROUND", "content": "Consider a set of supervised learning tasks T\u2081,..., TT, with the data associated to the t-th task Te being Dt = {(x,y) \u2208 Xt \u00d7 Yt}. Continual learning algorithms learn the tasks sequentially, whereby at each learning step they utilize the present task data to update the model, which is typically a neural network with parameters \u03b8. In order to avoid forgetting the old tasks while learning a new one, the algorithm can often access an external memory, which is also updated after every task. The content of this memory may differ across algorithms. The performance on each task is measured by a task-specific loss function lt, and the objective of each task is to minimise the expected loss: Lt(0) = \\langle lt(x, y, 0)\\rangle_{p_t}, where \\langle\u00b7\\rangle_{p_t} denotes the average over the task distribution Pt. In practice, the expectation is approximated by an average over the dataset, which is called the task empirical loss L_t(0) = \\langle l_t(x, y, 0)\\rangle_{D_t}. The continual learning problem is to minimise the multi-task loss:\n\nL_{MT}(0) = \\frac{1}{t} (L_1(0) + ... + L_t(0)) \\tag{1}\n\nwhile only having direct access to the current task data Dt and the external memory. Other objectives may be considered instead of the multi-task loss, such as the average lifetime performance. We choose to use the multi-task loss objective in order to conform with the historically prevalent practice in the literature.\nNotation & Metrics. We use \u03b8\u2208 \u0398t to refer to a generic vector in the parameter space and Ot to the value of the network parameters after t learning steps. We index the current task with t and any single old task with o. Given a current parameter vector \u03b8 catastrophic forgetting on task To may be measured by the signed difference in expected loss \u0190\uff61(0) = L\uff61(0) \u2013 L\uff61(0\uff61). Similarly, the empirical approximation of \u0190(0) is the signed change in the empirical task loss Eo(0) = L\uff61(0) - L\uff61(0\uff61). Alternatively, forgetting can be measured in terms of the task test accuracy as E^{acc}_o(0) = ACC_o(0_o) \u2013 ACC_o(0). Notice that in both cases a lower value of forgetting is better. Additionally, we denote the average forgetting after t steps by E(t) = \\frac{1}{t}\\sum_{o=1}^{t-1}Eo(\u03b8_t) and the average empirical forgetting as \\overline{E}(t). Likewise, the average accuracy after t steps by ACC(t) = \\frac{1}{t}\\sum_{o=1}^{t-1} ACC_o(\u03b8_t). We refer the reader to Section 9 (Appendix A) for a full overview of the notation used."}, {"title": "3 LOCAL AND GLOBAL APPROXIMATIONS IN CONTINUAL LEARNING", "content": "We are now ready to introduce the central point of our discussion. Starting from the formulation of continual learning introduced in the previous section (Equation (2)), we are interested in asking whether the task loss approximation Lt(0) is local or global."}, {"title": "3.1 PROBLEM FORMULATION", "content": "In this work, we view continual learning algorithms from the perspective of loss approximation, which we introduce in this section. Consider the problem of minimizing the multi-task expected loss in Equation (1). If hypothetically all the data were available, one could approximate the distribution multi-task loss LMT (0) by its empirical version \\widehat{L}_{MT}(0) and use it as the optimization objective, as is common practice in machine learning. However, a continual learning algorithm can only access the current task data Dt and the content of its external memory Mt, therefore, its objective should be, strictly speaking, a function of Dt and Mt alone: objt(Dt, Mt).\nIn this work, we are interested in the way in which the (explicit or implicit) objective of a continual learning algorithm objt (Dt, Mt) approximates the true objective, i.e. the multi-task loss. In particular, we consider the relation between LMT (0) and obj(Dt, Mt) in order to focus on the locality (or non-locality) of the approximation, and we set aside a discussion of the generalization gap (i.e., whether \\widehat{L}_{MT}(0) \u2248 L_{MT}(0)) . Accordingly, we view the continual learning problem as follows:\n\n\\min_{\\Theta \\in \\Theta} \\operatorname{obj}_t(D_t, M_t)(0) \\quad \\text{s.t.} \\quad \\operatorname{obj}_t(D_t, M_t)(0) \u2248 \\widehat{L}_{MT}(0) \\tag{2}\n\nTo conform with existing algorithms, we hereafter study approximate objectives obj\u2081(Dt, Mt) of the form:\n\n\\widehat{L}_{MT} (0) := \\frac{1}{t} (\\widehat{L}_1(0) + ... + \\widehat{L}_{t-1}(0) + L_t(0)) \\tag{3}\n\nIn short, each previous task loss is approximated separately as L\u2081(0) \u2248 \\widehat{L}_i(0). We want to stress that such an approximation \\widehat{L}_i(0) need not always be explicit. For some cases, we infer the approximation used from the objective which the algorithm effectively minimizes at each learning step. This distinction will become clear in Section 5, where we review examples from the literature."}, {"title": "3.2 LOCAL AND GLOBAL APPROXIMATIONS", "content": "We are now ready to introduce the central point of our discussion. Starting from the formulation of continual learning introduced in the previous section (Equation (2)), we are interested in asking whether the task loss approximation Lt(0) is local or global.\nIn general, a local approximation of a function f(x) makes use of information about the function at a particular point 20 to produce a good approximation of f in a neighborhood of 20. Correspondingly, we say that the task loss approximation \\widehat{L}_t(0) is local when it uses information about the function at the task solution \u03b8t and it is accurate in a neighborhood of Ot. Conversely, we say that the approximation is global when it is independent of Ot, meaning that modifying Ot would not change the approximation. We formalise the notion of local and global approximations in Definition 3.1.\nDefinition 3.1 (Local and global task loss approximation.). Let I(X; Y) denote the mutual information of the pair of random variables (X, Y). We say that the task loss approximation \\widehat{L}_t(0) is local when I(\\widehat{L}_t(0); 0_t) > 0 \u2200 \u03b8\u2208 \u0398, and that it is global when I(\\widehat{L}_t(0); 0_t) = 0 \u2200 \u03b8\u2208 \u0398 \\backslash {0t}.\nFor illustrative purposes, consider the constant function approximation \\widehat{L}_t(0) = C. A local approximation could be \\widehat{L}_t(0) = L_t(0_t), and a global approximation could be \\widehat{L}_t(0) = 0. Although it is intuitively clear that the former approximation relies on information of the task solution Ot, in general one could evaluate D_{KL}(P(\\widehat{L}_t(0_t), 0_t)||P(\\widehat{L}_t(0_t)) \u00b7 P(0_t)) > 0, given any distribution P(0t).\nLet \u03be_t(0) = |\\widehat{L}_t(0) \u2013 L_t(0)| be the approximation error in \u03b8. We define the e-region of the approximation as \u03a9_e = {\u03b8 \u2208 \u0398 : \u03be_t(\u03b8) < e}. For local approximations \u03a9_e is always a neighborhood of Ot (this may be another definition of local approximations), while it may be a disjoint set of points for a global approximation. Notice that the global approximation is not necessarily more accurate than a local approximation. Depending on the case, the e-region of a local approximation may have more volume than the e-region of a global approximation, and thus be 'less wrong' on average.\nThis brings us to state the main assumption for continual learning algorithms using local approximations. Hereafter, we say a continual learning algorithm is local if it uses a local approximation of the task loss function, and global if it uses a global approximation instead. It follows that local continual learning algorithms effectively reduce forgetting only with the additional assumption that learning is localised, which we dub the locality assumption.\nAssumption 3.2 (Locality assumption). Given a local approximation with e-region \u03a9_e for task t and arbitrary e, the following condition holds while learning task t + 1:\n\n\\bigcap_{o\u2264t} \\Theta_o \\subseteq \\Omega_e \\tag{4}\n\nSimply put, for a continual learning algorithm producing the sequence of solutions 01,..., 0t, the locality assumption requires that all the solutions lie relatively close to each other. The error of a local task loss approximation is higher the farther away from the task solution. Thus, for a given error tolerance \u20ac the locality assumption is broken when the distance between the task solutions is \u201ctoo high\". In Section 6, we break the locality assumption by artificially increasing the distance between task solutions, and we show that local algorithms struggle in this setting.\""}, {"title": "4 CASE STUDY: LOCAL POLYNOMIAL APPROXIMATIONS", "content": "A general example of local approximation of Lt (0) is the Taylor expansion around 0t:\n\n\\widehat{L}_t(0) = L_t(0_t) + (0 \u2013 0_t)^T \\nabla L_t(0_t) + \\frac{1}{2}(0 \u2013 0_t)^T \\frac{\\partial^2 L_t(0)}{\\partial 0^2} (0 \u2013 0_t) + ... \\tag{5}\n\nThe approximation error for a p-order approximation is upper bounded by O(||0 \u2013 0t||P), and thus the e-region is simply a p-norm ball around Ot with radius proportional bounded by \u0454: \u03a9_e = {0 : C \u00b7 ||0 \u2013 0t||P < e}. A convenient property of this approximation is that it allows us to express forgetting in terms of the loss derivatives in Ot. Indeed forgetting is simply Et(0) = Lt(0) \u2013 Lt(0t)."}, {"title": "4.1 QUADRATIC LOCAL APPROXIMATIONS", "content": "A wealth of studies have shown that for over-parameterized networks the loss tends to be very well-behaved and almost convex in a reasonable neighborhood around the minima (Saxe et al., 2013; Choromanska et al., 2015; Jacot et al., 2020). Therefore in such cases a quadratic approximation of Lt (0) might be accurate.\nIf we write the Hessian matrix \\frac{\\partial^2 L_t (0)}{\\partial 0^2} as H* and denote the change in parameters due to learning a new task Ot - 0t-1 as \u2206t, following Equation (5) we can write the forgetting on task o after learning t as:\n\nE_o(0_t) = (0_t \u2013 0_o)^T \\nabla L_o(0_o) + \\frac{1}{2} (0_t \u2013 0_o)^T H_o^* (0_t \u2013 0_o) \\tag{6}\n\nIn order to highlight the contribution of the latest parameter update to the average forgetting \\overline{E}(t) = \\frac{1}{t-1} \\sum_{o=1}^{t-1} E_o(\u03b8_t), we can formulate it in a recursive fashion:\n\n\\overline{E}(t) = \\frac{t-1}{t} \\overline{E}(t-1) + \\frac{1}{t} \\Delta_t^T \\bigg(\\sum_{o=1}^{t-1} \\nabla L_o(0_o) \\bigg) + \\frac{1}{2t} \\Delta_t^T \\bigg(\\sum_{o=1}^{t-1} H_o^* \\bigg) \\Delta_t + \\frac{1}{t} v^T \\Delta_t \\tag{7}\n\nwhere v denotes the vector \\sum_{o=1}^{t-1} (\u03b8_{t-1} \u2013 \u03b8_o) H_o^*.\nExample. Let us consider the case of t = 2 as an example, for which v = 0. The forgetting of the first task after learning the second is simply: E_1(\u03b8_2) = \\Delta_2^T \\nabla L_1(0_1) + \\frac{1}{2} \\Delta_2^T H_1^* \\Delta_2. Putting everything together, the objective when learning the second task is:\n\n\\min_{\\Delta_2 \\in \\Theta} \\bigg\\{ L_2(0_1 + \\Delta_2) + \\Delta_2^T \\nabla L_1(0_1) + \\frac{1}{2} \\Delta_2^T H_1^* \\Delta_2 \\bigg\\} \\tag{8}\n\nNotice that minimizing this objective with respect to A2 is equivalent to minimizing the multi-task loss L2 + L1. If we now assume that \u03b8\u2081 is a local minima of L1, we get \u2207L1(01) = 0 and positive semi-definite Hessian H\u2081 \u2265 0, from which it follows that E(2) \u2265 0. In this case the objective Equation (8) can be rewritten as:\n\n\\min_{\\Delta_2 \\in \\Theta} L_2(0_1 + \\Delta_2) \\quad \\text{s.t.} \\quad \\Delta_2^T H_1^* \\Delta_2 = 0 \\tag{9}\n\nRepeating the same procedure for every following task (t > 2) we can write the optimal learning objective for any new task under a quadratic local approximation of the loss. We state our result in Theorem 4.1.\nTheorem 4.1 (Optimal quadratic local continual learning). For any continual learning algorithm producing a sequence of parameters 01,...,\u0472t such that 0; is a local minima of Li and \\sup_{\\theta_i,\\theta_k} ||\u03b8_i \u2013 \u03b8_k||_3 < \u0454 the following relationship holds:\n\nE(1), ..., E(t \u2212 1) = 0 \u21d2 E(t) = \\overline{E}(t) = \\frac{1}{2} \\Delta_t^T \\bigg(\\sum_{o=1}^{t-1} H_o^* \\bigg) \\Delta_t \u2265 0 \\tag{10}\n\nMoreover, if E(1), . . ., E(t \u2212 1) = 0 the optimal learning objective for task t is:\n\n\\min_{\\Delta_t \\in \\Theta} L_t (0_{t-1} + \\Delta_t) \\quad \\text{s.t.} \\quad \\Delta_t^T \\bigg(\\sum_{o=1}^{t-1} H_o^* \\bigg) \\Delta_t = 0 \\tag{11}\n\nThere is an intuitive explanation of Theorem 4.1: when a quadratic approximation to the loss is accurate enough and each task solution is a local minimum, the parameter updates must be taken along directions where the multi-task loss landscape is (absolutely) flat in order to prevent forgetting. More formally, the constraint in Equation (11) is forcing the update At to lie in the null-space of the average Hessian matrix Ht := \\sum_{o=1}^{<t} H_o^*.\nAt this point, it is natural to ask whether there exists a solution to the objective given by Theorem 4.1. Previous studies (Sagun et al., 2016; 2017) have observed that the loss landscape of deep neural networks is mostly degenerate around local optima, implying that most of the eigenvalues of the loss Hessian lie near zero. In other words, the rank of H is rather small, and as shown theoretically in the case of deep linear networks (Singh et al., 2021), this is of the order square-root the number of parameters. In the Appendix (Figure 4) we plot the effective rank of the multi-task loss Hessian matrix as a function of t and we find in practice that the rank of Ht grows sub-linearly in t.\nNevertheless, there may be settings for which the condition in Equation (11) is unsatisfiable (e.g. the multi-task loss Hessian matrix is full rank) or leads to poor task solutions (thereby violating the assumptions of Theorem 4.1). Equation (7) describes forgetting in a more general case with no other assumptions than locality. Generally, for quadratic local approximations of the loss, the optimal learning objective for a given task is:\n\n\\min_{\\Delta_t \\in \\Theta} \\bigg\\{ L_t (0_{t-1} + \\Delta_t) + \\Delta_t^T (\\nabla L_{<t} + v) + \\frac{1}{2t} \\Delta_t^T H^* \\Delta_t \\bigg\\} \\tag{12}\n\nwhere we have introduced the abbreviation \u2207L_{<t} := \\sum_{o=1}^{t-1} \u2207L_o(0_o). Notice that Equation (12) also favours parameter updates which lie in the null space of the multi-task loss Hessian matrix. However while Equation (11) employs a hard constraint effectively reducing the space of solutions, Equation (12) employs a soft constraint, potentially trading-off forgetting with better performance on the task. In Section 5, we review examples of existing algorithms that implement the hard and soft constraints.\nThe role of the multi-task loss Hessian matrix in the local learning objectives (Equations (11) and (12)) explains the observation in the literature that \"flatter local minima\" (Keskar et al., 2016) result in reduced forgetting (Deng et al., 2021; Mirzadeh et al., 2020a). Under a local quadratic approximation, the flatter the previous task minima, the larger the space of solutions to the current task where forgetting is 0 or close to 0. Therefore continual learning algorithms favouring flatter landscapes such as (Deng et al., 2021) implicitly rely on a local approximation of the task loss, and thereby they are successful strategies only when learning is localised."}, {"title": "5 LOCAL AND GLOBAL ALGORITHMS IN THE LITERATURE", "content": "We now review some existing continual learning algorithms, classifying them into local and global algorithms. We select a few well known exemplars, representing different families of algorithms according to popular taxonomies of the literature (De Lange et al., 2021)."}, {"title": "5.1 GLOBAL ALGORITHMS", "content": "Experience Replay (ER) is one of the oldest (Robins, 1995) and still one of the most effective (Buzzega et al., 2021) algorithms for continual learning. Although several variants have been proposed (Rebuffi et al., 2017; Lopez-Paz & Ranzato, 2017; Shin et al., 2017; Van de Ven et al., 2020) for now we consider its simplest form. For each task, a random subset of the dataset St C Dt is stored in an external bufferto approximate the task loss \\widehat{L}_t (0) as follows:\n\n\\widehat{L}_t(0) = \\frac{1}{|S_t|} \\sum_{x,y \\in S_t} l(x,y,\u03b8) \\tag{13}\n\nThe objective of each learning step is that of Equation (3). As long as the buffer sampling strategy does not depend on the network parameters after learning the task, the approximation used by experience replay is global. The approximation error is a function of the buffer size and it has been observed that in most cases the algorithm is effective also when the buffer size is small (Buzzega et al., 2020; 2021).\nGradient Episodic Memory (GEM) (Lopez-Paz & Ranzato, 2017; Chaudhry et al., 2018) employs the same task loss approximation in a different way. Instead of directly minimizing the multi-task loss, the objective of GEM is:\n\n\\min_{\\Delta_t \\in \\Theta} L_t (0_{t-1} + \\Delta_t) \\quad \\text{s.t.} \\quad \\widehat{L}_o(0_{t-1} + \\Delta_t) \u2264 \\widehat{L}_o(0_{t\u22121}) \\quad \u2200o<t \\tag{14}\n\nIn other words, the parameter update \u25b3\u2081 does not minimise the approximate task loss \\widehat{L}_o but it does not increase it (thus avoiding catastrophic forgetting). In order to enforce the constraint in Equation (14), GEM uses a local linearization of the old task loss, which is updated after each optimization step. The linearization may be inaccurate when learning with large gradient step sizes, and result in reduced performance. Nevertheless, the task loss approximation is based on the current state of the network rather than its state after learning the task, which makes this algorithm global.\nSynaptic Intelligence (SI) (Zenke et al., 2017) uses a quadratic approximation of the old task loss, centered around the previous task solution. For a current task t the approximation of Lo is:\n\n\\widehat{L}_o(0) = (0 \u2013 0_{t-1})^T H_o (0 \u2013 0_{t-1}) \\tag{15}\n\nwhere Ho is a diagonal matrix updated at each gradient step, which roughly estimates the task Hessian matrix evaluated at Ot-1. The objective of each learning step is that of Equation (3). The approximation of Lo is updated after each task based on the current state of the network Ot, therefore SI also belongs to the group of global algorithms. Similarly to GEM, SI is sensitive to large step sizes, as the quadratic approximation may be inaccurate if the distance travelled in the parameter space is large.\nProgressive Neural networks (PNN) (Rusu et al., 2016) belong to the category of network expansion methods, which dynamically allocate new parameters of the neural network to each task. Specifically, PNN subsequently adds 'columns' (parametrized by feed-forward networks) to the neural network with unilateral connections between them. Although the whole network is used to produce outputs, only the parameters of the last column are trained on the cur-rent task, while the others are frozen. In more general terms, let \u0398\u00b9, . . ., Ot denote the parameter subspace associated to each task. By design the modified derivative of the task loss Lt is:\n\n\\frac{\\partial \\widehat{L}_t(0)}{\\partial \u03b8_i} = \\begin{cases} \\frac{\\partial L_t(0)}{\\partial \u03b8_i} \\quad \\text{if } \u03b8_i \u2208 \u0398^t \\\\ 0 \\quad \\text{if } \u03b8_i \\notin \u0398^t \\end{cases} \\tag{16}"}, {"title": "5.2 LOCAL ALGORITHMS", "content": "Second-order regularization. Yin et al. (2020) have recently demonstrated that a large group of continual learning algorithms which use quadratic regularizers to prevent forgetting effectively employ a second order local approximation of the tasks loss function (discussed in Section 4), which shows that these algorithms are local. More specifically, Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017) and Structured Laplace Approximation Ritter et al. (2018) minimize the optimal objective of Equation (12), and differ in their assumptions on the hessian matrix.\niCarl (Rebuffi et al., 2017) uses experience replay to avoid catastrophic forgetting. There are several elements contributing to the final form of the algorithm, such as the use of network-generated targets instead of labels to approximate the task loss, and the non-parametric classifier based on the network features. Crucially, the samples in the task replay buffer are selected after the task has been learned in order to best approximate the true feature class mean. The selection procedure, based on herding, inevitably depends on the value of the network parameters after learning the task, as the network features are determined by the parameters. Substantial transformations of the feature map will impact the task-loss approximation accuracy. Therefore, iCarl belongs to the set of local algorithms due to this prioritized buffer selection procedure. In Section 6 we experimentally verify our claim by changing the algorithm's sampling strategy.\nOrthogonal gradient descent (OGD) (Farajtabar et al., 2020) avoids catastrophic forgetting by enforcing orthogonality between the parameter update and the previous tasks output-gradients. In order to do so efficiently, a set of the task output-gradient vectors is stored in a buffer at the end of the task. By doing so, the algorithm uses a local approximation of the task loss function. In Theorem 5.1 we show that OGD implements the optimal continual learning objective under local quadratic approximations of the loss (Theorem 4.1).\nTheorem 5.1. Let \u22061, ..., \u2206t be the sequence of updates produced by Orthogonal Gradient Descent on a sequence of t tasks and let 01,...,0\u1e6d be the corresponding parameters. If \\sup_{\\theta_i, \\theta_k} ||\u03b8_i \u2013 \u03b8_k||_3 < e, then \u2206t satisfies the constraint in Equation (11) and Et = 0 for all learning steps t.\nThis result establishes a direct link between OGD and second-order regularization methods such as EWC: in settings where a quadratic approximation of the task loss is accurate, both methods minimize the optimal learning objective, with the difference that OGD relies on a hard constrain (Equation (11)) and regularization methods use soft constraints (Equation (12)). As a consequence, OGD implements a more conservative approach, potentially sacrificing perfor-mance on new tasks in order to maintain performance on old tasks, whereas EWC and the like strike a balance between new and old tasks performance which is determined by the regularization strength hyperparameter."}, {"title": "6 EXPERIMENTS", "content": "Experimental setup. With our experiments we want to evaluate the practical side of the theoretical distinction between local and global algorithms. We take several classic algorithms in the literature which are representative of the different families of algorithms, namely: Expericence replay (ER), Averaged GEM (A-GEM, a computationally cheap variant of GEM), Elastic Weight Consolidation (EWC), Synaptic Intelligence (SI), iCarl and Orthogonal Gradient Descent (OGD), which were discussed in Section 5. We do not include Progressive Neural networks (PNN) in the experiments because we are interested in forgetting, which by design is always 0 for PNN."}, {"title": "6.1 LOCAL VS GLOBAL", "content": "Our main experiment consists in comparing local and global algorithms in settings where the locality assumption (Assumption 3.2) holds and settings where it doesn't. Recall that for a local approximation of the task loss Lt, the e-region is a neighborhood of the task solution 0t. For example, for a polynomial approximation of degree p, the e-region is a p-norm ball around Ot of radius e. Roughly, the higher the distance travelled between tasks in the parameter space, the higher the loss approximation error for local approximations. And we say that the locality assumption is broken when this error is higher than a given tolerance of e.\nWe break the locality assumption by artificially increasing the Euclidean distance between task solutions, i.e. ||At ||2. There are a number of factors which determine || At ||2, including the learning rate, the number of epochs, the batch size and the dataset size. We choose to adjust the learning rate while keeping all the other factors constant. More explicitly, we repeat all our experiments for different learning rates, which are approximately equidistant on a logarithmic scale. By varying the learning rate in a wide range, we smoothly interpolate between a local learning setting and a non-local learning setting.\nWe verify our methodological choice by plotting the distance travelled in the parameter space as a function of the learning rate in Figure 1. For all our configurations we see that higher learning rates indeed result in a higher distance from initialization, and, consequently higher || At||2. Interestingly, for low learning rates the curves look homogeneous"}, {"title": "6.2 ROBUSTNESS OF LOCAL APPROXIMATIONS", "content": "Finally, we take a closer look at the geometry of the parameter space around the task optima. We wish to understand how in practice the e-region of a task loss approximation behaves.\nTo this end, we evaluate the size of the area around a local minima where a second order approximation of the loss is accurate. Recall that around a local minima the second order approximation of the loss (Equation (5)) mostly depends on the hessian matrix, evaluated at the minima. The spectrum of this matrix is often dominated by a few, principal eigenvalues in practice. This means that, when close to the minima, moving along a principal eigenvector direction leads to the sharpest increase in the loss. However, far from the minima, where the second order approximation of the loss is no longer accurate, moving along the same direction will be no different than moving along any other direction, on average.\nWe observe the transition in the correctness of the second order approximation by looking at the loss function as we move away from the minima along the principal eigenvectors. We obtain the task local optima 01,..., \u04e8\u0442 by training on each task sequentially with an SGD optimizer. We then evaluate the effect of perturbing the optima along a principal eigenvector through the following score:\n\ns(r) = \\mathbb{E}_{\u03bc'} \\frac{L_t(0_t + r \u00b7 v_i) - L_t(0_t)}{L_t(0_t+r\u00b7 \u03bc') \u2013 L_t (0_t)} \\tag{17}\n\nIn Equation (17) vi represents the i-th principal eigenvector, and \u03bc' is a Gaussian random vector scaled to have unit norm. The scalar r controls the distance from the optima. In Figure 3 we plot s(r) and the loss Lt (0t + r \u00b7 vi) as we vary r in the range [10-3, 106] for all 5 tasks of the Split CIFAR-10 challenge. We see that the score increases"}, {"title": "7 RELATED WORK", "content": "We are not the first to view the continual learning problem from the loss approximation perspective. Several existing algorithms (Zenke et al.", "regime": "our results regard instead the equivalence between OGD and the optimal objective under local quadratic approximations.\nThis work is also related to existing surveys of the continual learning literature, such as (De Lange et al., 2021; Parisi et al., 2019; Khetarpal et al., 2022; Qu et al., 2021; Awasthi & Sarawagi, 2019), which catalogue the existing continual learning into different 'families' and rank them according to different metrics. However, our classification of the literature into local and global algorithms reflects the implicit limitations of the algorithms rather than their surface-level characteristics, such as the presence of regularisation or rather the storing of input-output pairs in an external memory bank.\nFinally, our work shares the spirit of existing theoretical studies of catastrophic forgetting and continual learning algorithms, such as (Mirzadeh et al., 2020b;a; Farquhar & Gal, 2019; Ramasesh et al., 2020; Verwimp"}]}