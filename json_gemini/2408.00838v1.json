{"title": "Calibrating Bayesian Generative Machine Learning for Bayesiamplification", "authors": ["S. Bieringer", "S. Diefenbacher", "G. Kasieczka", "M. Trabs"], "abstract": "Recently, combinations of generative and Bayesian machine learning have been intro-\nduced in particle physics for both fast detector simulation and inference tasks. These neural networks\naim to quantify the uncertainty on the generated distribution originating from limited training statis-\ntics. The interpretation of a distribution-wide uncertainty however remains ill-defined. We show a\nclear scheme for quantifying the calibration of Bayesian generative machine learning models. For a\nContinuous Normalizing Flow applied to a low-dimensional toy example, we evaluate the calibra-\ntion of Bayesian uncertainties from either a mean-field Gaussian weight posterior, or Monte Carlo\nsampling network weights, to gauge their behaviour on unsteady distribution edges. Well calibrated\nuncertainties can then be used to roughly estimate the number of uncorrelated truth samples that\nare equivalent to the generated sample and clearly indicate data amplification for smooth features\nof the distribution.", "sections": [{"title": "1 Introduction", "content": "The upcoming high-luminosity runs of the LHC will push the quantitative frontier of data taking to\nover 25-times its current rates. To ensure precision gains from such high statistics, this increase in\nexperimental data needs to be met by an equal amount of simulation. The required computational\npower is predicted to outgrow the increase in budget in the coming years [1, 2]. One solution to\nthis predicament is the augmentation of the expensive, Monte Carlo-based, simulation chain with\ngenerative machine learning. A special focus is often put on the costly detector simulation [3, 4].\nThis approach is only viable under the assumption that the generated data is not statistically\nlimited to the size of the simulated training data. Previous studies have shown, for both toy data [5]\nand calorimeter images [6], that samples generated with generative neural networks can surpass the\ntraining statistics due to powerful interpolation abilities of the network in data space. These studies\nrely on comparing a distance measure between histograms of generated data and true hold-out data\nto the distance between smaller, statistically limited sets of Monte Carlo data and the hold-out set.\nThe phenomenon of a generative model surpassing the precision of its training set is also known as\namplification. While interesting in theory and crucial for the pursuit of the amplification approach,"}, {"title": "2 Bayesian Neural Networks", "content": "In contrast to traditional, frequentist deep neural networks, in a Bayesian phrasing of deep learning, a\ndistribution on the network weights is applied. This distribution encodes the belief in the occurrence"}, {"title": "3 Bayesian Continuous Normalizing Flows", "content": "Generative models of various flavours have been applied for fast simulation of detector effects [3, 4].\nMeanwhile, Normalizing Flows, both block-based [22] and continuous [23], can be connected to\nBayesian machine learning straight-forwardly, as the log-likelihood of the model is accessible. Due\nto the recent success of diffusion-style models in detector emulation [24\u201329] and their high data\nefficiency, we combine both and concentrate on Continuous Normalizing Flows (CNF) in this study."}, {"title": "3.1 Variational Inference Bayes", "content": "The parameters of an approximation $\\tilde{\\pi}(\\theta)$ of the posterior distribution $\\pi(\\theta|D)$ can be inferred, by\nminimizing their Kullback-Leibler (KL) divergence using stochastic gradient descent methods [17].\nAs the posterior is not analytically accessible, Bayes' theorem (2.1) is employed to rewrite the KL\ndivergence in terms of the log-likelihood and the distance to the prior\nThe log-likelihood of the data under the CNF can be directly employed here. However, calculating\nthe log-likelihood of a CNF is costly as the ODE (3.1) needs to be solved for every point in the"}, {"title": "3.2 Markov Chain Monte Carlo", "content": "A competing approach to variational inference-based Bayesian deep learning is MCMC sampling.\nOur approach to MCMC sampling for neural networks, AdamMCMC [21], uses the independence of\nthe sampled invariant distribution to the starting point to initiate the sampling from CFM-trained\nmodel parameters $ \\theta_0 $. This drastically reduces the optimization time over the joint optimization\nof Section 3.1, and makes employing the costly log-likelihood for the consequent uncertainty\nquantification feasible.\nFor every step of the chain, the ODE (3.1) is solved to determine the negative log-likelihood\n$L_{NLL}$ of the data to construct a chain drawn from a proposal distribution around an Adam [32] step\nIn combination with a proposal distribution that is elongated in the direction of the step\nthis algorithm handles high dimensional sampling for neural networks very efficiently and results\nin a high acceptance rate in a subsequent stochastic Metropolis-Hastings (MH) correction with\nacceptance probability\nfor a large range of noise parameter settings. Both the inverse temperature parameter $\\lambda$ and the\nnoise parameter $\\sigma_\\bigtriangleup$ tune the predicted uncertainties. In theory small $\\lambda$ and high $\\sigma_\\bigtriangleup$ will result in high\nerror estimates, albeit in practice the dependence on the inverse temperatures is very weak. We thus\nlimit ourselves to adapting the noise parameter to align the generated uncertainties.\nAfter an initial burn-in period, which can be skipped when initializing from a pretrained model,\nrepeatedly saving the network parameters after gaps of length $l$ ensures approximately independent\nparameter samples. The set of sampled parameters\nfollows the tempered posterior distribution, due to Bayes' theorem and the resulting proportionality"}, {"title": "4 Toy Setup", "content": "Similar to previous studies on data amplification [5], we employ the CNF on a low-dimensional\nring distribution. Within this paper, we limit the study to two dimensions for illustrative purposes\nand to reduce computational costs. Nevertheless, the calibration can be executed analogously for\nhigher dimensional distributions. We generate samples from a ring distribution with an unsteady\nedge at a radius of r = 4, by sampling in spherical coordinates from\nwith parameters $\\alpha = \\beta = 2$ for the Gamma distribution. Per training, we use an independent sample\nof N = 10,000 points. Before passing the data to the CNF, we transform into Cartesian coordinates\nto obtain the ring shape shown in Figure 1. This construction allows us to estimate the behaviour of\nthe uncertainties at distribution edges and simultaneously prevents divergences of the probability\ndistribution in (x, y) = (0, 0)."}, {"title": "4.1 Gamma Function Ring", "content": "Similar to previous studies on data amplification [5], we employ the CNF on a low-dimensional\nring distribution. Within this paper, we limit the study to two dimensions for illustrative purposes\nand to reduce computational costs. Nevertheless, the calibration can be executed analogously for\nhigher dimensional distributions. We generate samples from a ring distribution with an unsteady\nedge at a radius of r = 4, by sampling in spherical coordinates from\n$\\phi \\sim \\text{uniform}(0, 2\\pi)$ and $r-4 \\sim \\Gamma(\\alpha, \\beta)$\nwith parameters $\\alpha = \\beta = 2$ for the Gamma distribution. Per training, we use an independent sample\nof N = 10,000 points. Before passing the data to the CNF, we transform into Cartesian coordinates\nto obtain the ring shape shown in Figure 1. This construction allows us to estimate the behaviour of\nthe uncertainties at distribution edges and simultaneously prevents divergences of the probability\ndistribution in (x, y) = (0, 0)."}, {"title": "4.2 Hyperparameter Choices", "content": "Due to the low dimensionality of the toy example, we do not need to employ complicated archi-\ntectures to obtain a good approximation of the vector-field $v_t(\\cdot, \\theta)$. Based on a small grid search,\na Multi-Layer Perceptron with 3 layers of 32 nodes and ELU activation is deemed to be sufficient.\nEach of the 3 layers takes the time variable t as an additional input. The neural network part of the\nCNF thus totals a mere 2498 parameters.\nWhen parameterizing the weight posterior approximation $\\tilde{\\pi}(\\theta)$ as an uncorrelated Normal\ndistribution, as is standard in VIB [17], the number of parameters consequently doubles. For VIB\nwe train using the Adam optimizer [32] at a learning rate of 10^{-3} for up to 250k epochs of 10 batches\nof 1000 datapoints each. To prevent overfitting, we evaluate the model at the earliest epoch after\nconvergence of the KL-loss term. This point depends on the choice of k and varies between 75k for"}, {"title": "4.3 Quantiles", "content": "As in Reference [5], we evaluate the generated data in histogram bins of equal probability mass.\nWe will refer to these bins as quantiles $Q_j$, their count as $q_j$ and the set of all quantiles as\n$Q = \\{Q_1, ..., Q_{n_Q}\\}$. To construct bins with the same expected occupancy, we use spherical\ncoordinates. In angular direction, the space can simply be divided into linearly spaced quantiles,\nwhile in radial direction we use the quantiles of a 10M generated truth dataset to gauge the boundaries\nof the quantiles. To guaranty even population, we always choose the same number of quantiles in\nboth dimensions. Figure 1 illustrates the construction and occupancy for 5\u00d75 quantiles in Cartesian\ncoordinates.\nFor correlated data, quantiles can be constructed by iteratively dividing a truth set into sets of\nequal size [6]. The binning is however not relevant for the discussion of calibration and analogous\narguments can be made for arbitrary histograms. The advantage of quantiles over other binning\nschemes is the clear definition of the number of bins without an offset by an arbitrary amount of\ninsignificant bins in the sparsely or unpopulated areas of the data space. This allows us to show the\nbehaviour of calibration and amplification over the number of bins in Section 5 and 6."}, {"title": "5 Calibration", "content": "To align the uncertainty quantification, for AdamMCMC we generate 10M points from the CNF for the\n$n_{mcmc} = 10$ parameter samples in $\\Theta_{mcmc}$. We obtain a set of points $G^{(i)}$ per parameter sample\n$\\theta^{(i)}$, with the corresponding count\nin quantile $Q_j$. Each count corresponding to a parameter sample thus constitutes one drawing of a\nrandom variable $G_j$ whose distribution is induced by the posterior.\nAnalogously, for VIB we draw a set $\\Theta_{VIB}$ of parameters from the posterior approximation $\\tilde{\\pi}(\\theta)$,\ngenerate 10M samples from each and calculate the quantile counts to generate drawings of $G_j$. As\nthe training cost does not depend on the number of draws for VIB, we use $n_{VIB} = 50$ samples for\nbetter accuracy.\nUsing the quantile values $g_j^{(i)}$, we approximate the cumulative distribution function (CDF)"}, {"title": "5.1 Scaling with the Number of Quantiles", "content": "To further investigate the calibration of our Bayesian generative neural networks, we pick the\nseemingly best calibrated parameter settings for both methods. For AdamMCMC this is $\\sigma = 0.1$ and\nfor VIB $k = 10$. We generate $n_{MCMC} = n_{VIB} = 50$ samples from the posterior for both methods\nnow and evaluate the scaling with the number of quantiles in more detail.\nAs we do not want to evaluate one calibration plot for each quantile, we reduce the diagonal\ncalibration plots by calculating the mean (absolute) deviation between empirical and nominal\ncoverage"}, {"title": "5.2 Calibration at Sharp Features in Radial Direction", "content": "The right panel of Figure 4 displays the the marginal empirical coverage in radial direction $\\bar{c}_{j_r}$ for\n200 \u00d7 200 quantiles and both BNN methods. We can see a distinct difference in the uncertainty\nquantification.\nWhile the VIB prediction seems very well calibrated in total, in the radial direction, the VIB\nunderestimates its bias for the steeply rising part of the data distribution between $r \\in [4.0, 5.0]$.\nFor the same interval, the MCMC prediction is well calibrated and less underconfident than for\nhigher radii. For $r > 5$ both models slightly overestimate the uncertainty and show very similar\ncalibration.\nIn terms of absolute uncertainty, both methods actually predict very similar results. However,\nthe mean prediction of the VIB-CNF is strongly biased by the prior KL-loss term, resulting in large\nunderpopulation of the generated density due to oversmoothing for $r < 4.5$ and a corresponding\noverpopulation in $r \\in [4.5, 5.0]$. We have tested the predictions for k = 50 and the behaviour"}, {"title": "6 Bayesiamplification", "content": "Based on the previous discussion of both the total and marginal calibration, we can confidently\nsay that our AdamMCMC-CNF is well calibrated, albeit slightly underconfident for some areas of\ndata space and small numbers of bins. It is, however, important to note that truth information was\nneeded to evaluate the calibration of the BNN. In a practical application, this would require either\na validation region or a large hold-out set, the latter of which would partially defeat the purpose of\ndata amplification in fast detector simulation. However, for applications with validations regions,\nsuch as generative anomaly detection [33, 34], precision improvements through data amplification\ncan be realized.\nWith a well calibrated BNN, we can try and develop a measure of the statistical power of the\ngenerated set from the uncertainties. We do so by relating the uncertainty to the statistics of an\nuncorrelated set of points T from the truth distribution. For $n_{bins}$ arbitrary bins, we expect the count\nin the j-th bin to be approximately Poisson distributed with mean and variance $t_j$. For the same"}, {"title": "6.1 Checking Amplification with Jensen-Shannon Divergence", "content": "To test how well the sum over all bin estimates\nactually gauges the size of an equivalent independent data set, we calculate the Jensen-Shannon\n(JS) divergence\nbetween the histogram estimation of the density in our quantiles and the known data distribution.\nThe JS divergence is bounded by 0 and log 2, with smaller values indicating similarity between the\ncompared distributions."}, {"title": "7 Conclusion", "content": "In the previous chapters, we present a novel evaluation of the uncertainty provided by a Bayesian\ngenerative neural network in a histogram. To this end, we propose constructing confidence intervals\nper histogram bin and compare the nominal coverage of the constructed interval to the empirical\ncoverage obtained from a small ensemble of BNNs.\nWe observe a strong dependence of the calibration on the parameters of both a VIB-CNF and\nan MCMC-sampled CNF. Furthermore, we find a strong tendency to oversmooth with strong priors\nleading to underestimation of the data density and corresponding error at the non-differentiable inner\nedge of our toy distribution. While present in both approaches, this behavior was predominantly\ndisplayed by the VIB-CNFs."}]}