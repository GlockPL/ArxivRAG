{"title": "Calibrating Bayesian Generative Machine Learning for Bayesiamplification", "authors": ["S. Bieringer", "S. Diefenbacher", "G. Kasieczka", "M. Trabs"], "abstract": "Recently, combinations of generative and Bayesian machine learning have been introduced in particle physics for both fast detector simulation and inference tasks. These neural networks aim to quantify the uncertainty on the generated distribution originating from limited training statistics. The interpretation of a distribution-wide uncertainty however remains ill-defined. We show a clear scheme for quantifying the calibration of Bayesian generative machine learning models. For a Continuous Normalizing Flow applied to a low-dimensional toy example, we evaluate the calibration of Bayesian uncertainties from either a mean-field Gaussian weight posterior, or Monte Carlo sampling network weights, to gauge their behaviour on unsteady distribution edges. Well calibrated uncertainties can then be used to roughly estimate the number of uncorrelated truth samples that are equivalent to the generated sample and clearly indicate data amplification for smooth features of the distribution.", "sections": [{"title": "1 Introduction", "content": "The upcoming high-luminosity runs of the LHC will push the quantitative frontier of data taking to over 25-times its current rates. To ensure precision gains from such high statistics, this increase in experimental data needs to be met by an equal amount of simulation. The required computational power is predicted to outgrow the increase in budget in the coming years [1, 2]. One solution to this predicament is the augmentation of the expensive, Monte Carlo-based, simulation chain with generative machine learning. A special focus is often put on the costly detector simulation [3, 4].\nThis approach is only viable under the assumption that the generated data is not statistically limited to the size of the simulated training data. Previous studies have shown, for both toy data [5] and calorimeter images [6], that samples generated with generative neural networks can surpass the training statistics due to powerful interpolation abilities of the network in data space. These studies rely on comparing a distance measure between histograms of generated data and true hold-out data to the distance between smaller, statistically limited sets of Monte Carlo data and the hold-out set. The phenomenon of a generative model surpassing the precision of its training set is also known as amplification. While interesting in theory and crucial for the pursuit of the amplification approach,\n\u2022 we introduce a technique for quantifying the calibration of Bayesian uncertainties on generative neural networks based on the mean coverage of the prediction.\n\u2022 We then develop an estimate of the number of simulated truth events matching the generated set in statistical power and validate this estimate.\nFor applications where the uncertainty calibration can be ensured, for example by evaluating on a validation region, this approach gives an inherent quantification of the significance of a generated set.\nIn Bayesian neural networks (BNNs) and beyond, calibrating uncertainty quantification is crucial for correct application of the prediction results [8]. While we prefer the uncertainties to align perfectly with the prediction error, overconfident predictions will lead to inflated significance values and false discoveries. Underconfident predictions on the other hand will obscure findings, but not lead to false results and can thus be tolerated to small extend.\nBayesian generative machine learning is inherently different from other BNNs in particle physics applications such as regression [9] or classification [10, 11]. Notably, in generative modeling, a low density region of data cannot be understood as low training statistics, but rather as a feature of the data that has to reproduced by the network. The uncertainty estimate thus behaves similarly to a low-dimensional, parameterized fit [12] introducing high error estimates at steep features of the data distribution or whenever the function class induced by the network architecture is not sufficient to reproduce the data. In a subsequent study of the quality of event generators [13], the authors also connect low uncertainty to good performance of the posterior mean in terms of a classifier test, but find that the weight distribution of a classifier is more sensitive to diverse failure modes than the Bayesian uncertainty.\nIn Section 2, we will explain the basic concepts of BNNs, while the connection to generative machine learning will be made in Section 3. We introduce the toy data, as well as the employed binning in Section 4 and use them to evaluate the calibration of two different classes of BNNs in Section 5. The idea of employing the Bayesian uncertainties for amplification is developed and deployed in Section 6, before we conclude in Section 7."}, {"title": "2 Bayesian Neural Networks", "content": "In contrast to traditional, frequentist deep neural networks, in a Bayesian phrasing of deep learning, a distribution on the network weights is applied. This distribution encodes the belief in the occurrence\n$\\pi(\\theta|D) = \\frac{\\pi(D|\\theta) \\pi(\\theta)}{\\pi(D)}$\n(2.1)\nis formed from our prior beliefs $\\pi(\\theta)$ and the likelihood $\\pi(D|\\theta)$ of the data $D$ under the model. While the likelihood gives the probability of the data given its modelling through the network and thus encodes the data inherent distribution (aleatoric uncertainty), the posterior distribution provides the uncertainty due to a lack of data (epistemic uncertainty) [14].\nMultiple methods of accessing the posterior distribution exist. For a broad overview over the existing techniques, we refer the readers to [8, 14\u201316]. They can mostly be classified as either approximating or sampling the posterior.\nOne popular option is approximating the posterior as an uncorrelated Gaussian distribution by learning a mean and a standard deviation per network weight. These parameters of the approximation are then inferred with (stochastic) variational inference. This technique is also referred to as 'Bayes-by-Backprop' [17] or within High-Energy Physics often understood as 'Bayesian Neural Networks'. We will refer to it as 'Variational Inference Bayes' (VIB).\nFor sampling the posterior, Markov Chain Monte Carlo (MCMC) methods are employed, with full Hamiltonian Monte Carlo (HMC) often considered the gold-standard [18]. To adapt this class of methods to the large datasets and high dimensional parameter spaces of deep learning stochastic and gradient-based chains have been developed. Most notably among them are stochastic gradient HMC [19] and its variations. Due to its easy application to different machine learning tasks and great performance on previous generative applications [20], we use AdamMCMC [21] as one instance of MCMC-based Bayesian inference of network weights.\nWith access to the posterior distribution of a neural network $f_\\theta(x) = y$, we can generate the network prediction as the posterior mean prediction and its uncertainty prediction as\n$\\hat{y} = \\int d\\theta \\pi(\\theta|D) f_\\theta(x) \\text{ and } \\sigma^2 = \\int d\\theta \\pi(\\theta|D) [f_\\theta(x) - \\hat{y}]^2$.\n(2.2)\nHere, the integration is approximated as a summation over an ensemble of network weights obtained from the posterior directly via sampling or from its approximation.\nFor generative machine learning, a per-sample uncertainty cannot be evaluated due to the unsupervised setup of the problem. We thus generate sets of data with every network weight instance in the ensemble, calculate histograms for each set and report the mean and standard deviation per bin over all sets. This allows us to compare against the expected truth values in each histogram bin."}, {"title": "3 Bayesian Continuous Normalizing Flows", "content": "Generative models of various flavours have been applied for fast simulation of detector effects [3, 4]. Meanwhile, Normalizing Flows, both block-based [22] and continuous [23], can be connected to Bayesian machine learning straight-forwardly, as the log-likelihood of the model is accessible. Due to the recent success of diffusion-style models in detector emulation [24\u201329] and their high data efficiency, we combine both and concentrate on Continuous Normalizing Flows (CNF) in this study.\nFollowing [23], we first introduce the flow mapping $\\Phi_t : [0, 1] \\times R^d \\rightarrow R^d$ parameterized by a time parameter $t \\in [0,1]$. In analogy to the application of multiple blocks in a coupling-block flow [22], the change of the flow mapping between target and latent space is determined by an ordinary differential equation (ODE)\n$\\frac{d}{dt} \\Phi_t(x) = v_t (\\Phi_t(x)), \\Phi_0(x) = x,$\n(3.1)\nthrough a time dependent vector-field $v_t : [0,1]\\times R^d \\rightarrow R^d$. For Diffusion Models this differential equation is promoted to a stochastic differential equation through the addition of time-dependent noise. For both cases, the vector-field is approximated using a deep neural network\n$v_t (\\cdot, \\theta) \\approx v_t$.\nBy convention, the flow is constructed to model latent data from a standard Gaussian at $t = 0$ and detector/toy data at $t = 1$. This defines the the boundaries of the probability path induced by the flow mapping\n$p_t(x) = p_0 \\left(\\Phi_t^{-1}(x)\\right) \\text{det} \\left(\\frac{\\partial \\Phi_t^{-1}(x)}{\\partial x}\\right)$.\n(3.2)\nTo circumvent solving the ODE to calculate the likelihood of the input data during training, we employ Conditional Flow Matching (CFM) [30]. Instead of the arduous ODE solving, the CFM loss objective matches the neural network predictions $v_t (x; \\theta)$ to an analytical solution $u_t$, by minimizing their respective mean-squared distance\n$L_{\\text{CFM}}(\\theta) = E_{t,q(x_1),p_t(x|x_1)} ||u_t (x|x_1) - v_t (x; \\theta)||^2$.\n(3.3)\nThe expectation value is calculated by sampling $t \\sim U(0, 1)$, $x_1 \\sim q$ and $x \\sim p_t(x|x_1)$, with $q$ the probability distribution of the detector/toy data. An efficient and powerful choice of $u_t$ is the optimal transport path [30]. By applying a Gaussian conditional probability path the CFM loss objective reduces to\n$L_{\\text{CFM}}(\\theta) = E_{t,q(x_1),p(x_0)} || t(x_1 - (1 - \\epsilon_{\\text{min}}) x_0) - v_t (\\sigma_t x_0 + \\mu_t; \\theta)||^2$.\n(3.4)\nHere, we use the conventions $\\mu_t = tx_1$ and $\\sigma_t = 1 - (1 - \\epsilon_{\\text{min}})t$, as well as the Gaussian latent distribution $p(x_0) = \\mathcal{N}(0, 1)$ and a small parameter $\\epsilon_{\\text{min}}$, that mimics the noise level of the training data."}, {"title": "3.1 Variational Inference Bayes", "content": "The parameters of an approximation $\\tilde{\\pi}(\\theta)$ of the posterior distribution $\\pi(\\theta|D)$ can be inferred, by minimizing their Kullback-Leibler (KL) divergence using stochastic gradient descent methods [17]. As the posterior is not analytically accessible, Bayes' theorem (2.1) is employed to rewrite the KL divergence in terms of the log-likelihood and the distance to the prior\n$L_{\\text{VIB}} = D_{\\text{KL}} [\\tilde{\\pi}(\\theta), \\pi (\\theta|D)] = - \\int d\\theta \\tilde{\\pi}(\\theta) \\text{log} \\pi (D|\\theta) + D_{\\text{KL}} [\\tilde{\\pi}(\\theta), \\pi(\\theta)] + \\text{constant}.$\n(3.5)\nThe log-likelihood of the data under the CNF can be directly employed here. However, calculating the log-likelihood of a CNF is costly as the ODE (3.1) needs to be solved for every point in the training data. The authors of [7] thus propose, to substitute the log-likelihood with the CFM loss (3.4) and attribute for the difference by a tunable factor $\\kappa$\n$L_{\\text{VIB-CFM}} = E_{\\tilde{\\pi}(\\theta)} L_{\\text{CFM}} + \\kappa D_{\\text{KL}} [\\tilde{\\pi}(\\theta), \\pi(\\theta)]$.\n(3.6)\nSimilar to changing the width of the prior $\\pi(\\theta)$, varying $\\kappa$ adjusts the balance of the CFM-loss to the prior and thus both the bias and variance of the predicted distributions. Trainings at low values of $\\kappa$ produce better fits at smaller uncertainties, while higher values impact the fit performance by imposing higher smoothness at the trade-off of higher estimated uncertainties. In our experience, promoting a CFM model to a BNN this way increases the training time considerably, due to the low impact and thus slow convergence of the KL-loss term. Possible ways to mitigate this include initiating the prior distribution and the variational parameters from the a pretrained deterministic neural network [31]."}, {"title": "3.2 Markov Chain Monte Carlo", "content": "A competing approach to variational inference-based Bayesian deep learning is MCMC sampling. Our approach to MCMC sampling for neural networks, AdamMCMC [21], uses the independence of the sampled invariant distribution to the starting point to initiate the sampling from CFM-trained model parameters $\\theta_0$. This drastically reduces the optimization time over the joint optimization of Section 3.1, and makes employing the costly log-likelihood for the consequent uncertainty quantification feasible.\nFor every step of the chain, the ODE (3.1) is solved to determine the negative log-likelihood $L_{\\text{NLL}}$ of the data to construct a chain drawn from a proposal distribution around an Adam [32] step\n$\\tilde{\\theta}_{i+1} = \\text{Adam}(\\theta_i, L_{\\text{NLL}}(\\theta_i))$.\n(3.7)\nIn combination with a proposal distribution that is elongated in the direction of the step\n$\\tau_i \\sim q(\\cdot|\\theta_i) = \\mathcal{N}(\\tilde{\\theta}_{i+1}, \\sigma^2 I + \\sigma_{\\triangle} (\\tilde{\\theta}_{i+1} - \\theta_i)(\\tilde{\\theta}_{i+1} - \\theta_i)^T),$\n(3.8)\nthis algorithm handles high dimensional sampling for neural networks very efficiently and results in a high acceptance rate in a subsequent stochastic Metropolis-Hastings (MH) correction with acceptance probability\n$\\alpha = \\frac{\\text{exp}(-L_{\\text{NLL}} (\\tau_i)) q(\\theta_i|\\tau_i)}{\\text{exp}(-L_{\\text{NLL}}(\\theta_i)) q(\\tau_i|\\theta_i)}$\n(3.9)\nfor a large range of noise parameter settings. Both the inverse temperature parameter $\\lambda$ and the noise parameter $\\sigma$ tune the predicted uncertainties. In theory small $\\lambda$ and high $\\sigma$ will result in high error estimates, albeit in practice the dependence on the inverse temperatures is very weak. We thus limit ourselves to adapting the noise parameter to align the generated uncertainties.\nAfter an initial burn-in period, which can be skipped when initializing from a pretrained model, repeatedly saving the network parameters after gaps of length $I$ ensures approximately independent parameter samples. The set of sampled parameters\n$\\Theta_{\\text{MCMC}} = {\\theta^{(1)}, ..., \\theta^{(n_{\\text{MCMC}})}} := {\\theta_{I\\cdot 1}, ..., \\theta_{I\\cdot n_{\\text{MCMC}}}} $\n(3.10)\nfollows the tempered posterior distribution, due to Bayes' theorem and the resulting proportionality\n$\\pi_{\\lambda}(\\theta|D) \\propto \\text{exp} \\left(-\\lambda L_{\\text{NLL}}(\\theta)\\right) \\pi(\\theta)$.\n(3.11)"}, {"title": "4 Toy Setup", "content": "Similar to previous studies on data amplification [5], we employ the CNF on a low-dimensional ring distribution. Within this paper, we limit the study to two dimensions for illustrative purposes and to reduce computational costs. Nevertheless, the calibration can be executed analogously for higher dimensional distributions. We generate samples from a ring distribution with an unsteady edge at a radius of $r = 4$, by sampling in spherical coordinates from\n$\\phi \\sim \\text{uniform}(0, 2\\pi) \\text{ and } r-4 \\sim \\Gamma(\\alpha, \\beta)$\nwith parameters $\\alpha = \\beta = 2$ for the Gamma distribution. Per training, we use an independent sample of $N = 10,000$ points. Before passing the data to the CNF, we transform into Cartesian coordinates to obtain the ring shape shown in Figure 1. This construction allows us to estimate the behaviour of the uncertainties at distribution edges and simultaneously prevents divergences of the probability distribution in $(x, y) = (0, 0)$."}, {"title": "4.1 Gamma Function Ring", "content": "Similar to previous studies on data amplification [5], we employ the CNF on a low-dimensional ring distribution. Within this paper, we limit the study to two dimensions for illustrative purposes and to reduce computational costs. Nevertheless, the calibration can be executed analogously for higher dimensional distributions. We generate samples from a ring distribution with an unsteady edge at a radius of $r = 4$, by sampling in spherical coordinates from\n$\\phi \\sim \\text{uniform}(0, 2\\pi) \\text{ and } r-4 \\sim \\Gamma(\\alpha, \\beta)$\nwith parameters $\\alpha = \\beta = 2$ for the Gamma distribution. Per training, we use an independent sample of $N = 10,000$ points. Before passing the data to the CNF, we transform into Cartesian coordinates to obtain the ring shape shown in Figure 1. This construction allows us to estimate the behaviour of the uncertainties at distribution edges and simultaneously prevents divergences of the probability distribution in $(x, y) = (0, 0)$."}, {"title": "4.2 Hyperparameter Choices", "content": "Due to the low dimensionality of the toy example, we do not need to employ complicated archi-tectures to obtain a good approximation of the vector-field $v_t (\\cdot, \\theta)$. Based on a small grid search, a Multi-Layer Perceptron with 3 layers of 32 nodes and ELU activation is deemed to be sufficient. Each of the 3 layers takes the time variable $t$ as an additional input. The neural network part of the CNF thus totals a mere 2498 parameters.\nWhen parameterizing the weight posterior approximation $\\tilde{\\pi}(\\theta)$ as an uncorrelated Normal distribution, as is standard in VIB [17], the number of parameters consequently doubles. For VIB we train using the Adam optimizer [32] at a learning rate of $10^{-3}$ for up to 250k epochs of 10 batches of 1000 datapoints each. To prevent overfitting, we evaluate the model at the earliest epoch after convergence of the KL-loss term. This point depends on the choice of $\\kappa$ and varies between 75k for $\\kappa = 50$ and 250k for $\\kappa = 1. We do 5 runs each for multiple values of $\\kappa \\in [1, 5, 10, 50]$ to regulate the uncertainty quantification.\nFor the AdamMCMC sampling, we start the chain from a pretrained model. The model is first optimized for 2500 epochs (Adam with learning rate of $10^{-3}$) using only the CFM-loss (3.4). For the deterministic model, this is enough to converge. We then run the sampling at a the same learning rate as the optimization with $\\sigma_{\\triangle} \\approx 50$ and $\\lambda = 1.0$. We add a sample to the collection at intervals of 100 epochs, to ensure the independence of the sampled weights. To adjust the calibration, we scan the noise value at four points $\\sigma \\in [0.01, 0.05, 0.1, 0.5]$. Once again, we calculate 5 chains per noise parameter setting."}, {"title": "4.3 Quantiles", "content": "As in Reference [5], we evaluate the generated data in histogram bins of equal probability mass. We will refer to these bins as quantiles $Q_j$, their count as $q_j$ and the set of all quantiles as $Q = {Q_1, ..., Q_{n_Q}}$. To construct bins with the same expected occupancy, we use spherical coordinates. In angular direction, the space can simply be divided into linearly spaced quantiles, while in radial direction we use the quantiles of a 10M generated truth dataset to gauge the boundaries of the quantiles. To guaranty even population, we always choose the same number of quantiles in both dimensions. Figure 1 illustrates the construction and occupancy for 5\u00d75 quantiles in Cartesian coordinates.\nFor correlated data, quantiles can be constructed by iteratively dividing a truth set into sets of equal size [6]. The binning is however not relevant for the discussion of calibration and analogous arguments can be made for arbitrary histograms. The advantage of quantiles over other binning schemes is the clear definition of the number of bins without an offset by an arbitrary amount of insignificant bins in the sparsely or unpopulated areas of the data space. This allows us to show the behaviour of calibration and amplification over the number of bins in Section 5 and 6."}, {"title": "5 Calibration", "content": "To align the uncertainty quantification, for AdamMCMC we generate 10M points from the CNF for the $n_{\\text{mcmc}} = 10$ parameter samples in $\\Theta_{\\text{mcmc}}$. We obtain a set of points $G^{(i)}$ per parameter sample $\\theta^{(i)}$, with the corresponding count\n$g_j^{(i)} = \\#{x' \\in Q_j | x' \\in G^{(i)}}$\nin quantile $Q_j$. Each count corresponding to a parameter sample thus constitutes one drawing of a random variable $G_j$ whose distribution is induced by the posterior.\nAnalogously, for VIB we draw a set $\\Theta_{\\text{VIB}}$ of parameters from the posterior approximation $\\tilde{\\pi}(\\theta)$, generate 10M samples from each and calculate the quantile counts to generate drawings of $G_j$. As the training cost does not depend on the number of draws for VIB, we use $n_{\\text{VIB}} = 50$ samples for better accuracy.\nUsing the quantile values $g_j^{(i)}$, we approximate the cumulative distribution function (CDF)\n$F_{G_j,\\theta} (g_j) \\approx F_{G_j} (g_j) = P (G_j \\leq g_j)$,\n(5.1)\nfrom its empirical counterpart using linear interpolation. We leave the set $\\Theta$ general, without a subscript, for now. From the approximated CDF, we construct symmetric confidence intervals for a given confidence level $c$ from its inversion\n$I_{j,\\theta}(c) = [F_{G_j}^{-1}(0.5 - \\frac{c}{2}), F_{G_j}^{-1}(0.5 + \\frac{c}{2})]$.\n(5.2)\nThe chosen confidence level $c$ corresponds to the expected or nominal coverage.\nTo evaluate the observed coverage, we draw 5 different training sets from the Gamma ring distribution and calculate a VIB- and AdamMCMC-CNF ensemble each\n$\\Theta_{\\text{MCMC}}^s \\text{ and } \\Theta_{\\text{VIB}}^s \\text{ for } s\\in {1, .., 5}$.\nFor every model, we construct a confidence interval and evaluate the number of intervals containing the expected count of the truth distribution, i.e. $1/n_Q$. The ratio of models with an interval containing the truth value over the total number of models gives the empirical coverage per bin\n$\\hat{c}_j = \\frac{\\# \\{ 1/n_Q \\in I_{j,\\theta^s} (c) | s \\in {1, .., 5} \\}}{5}$\n(5.3)\nwhere we again keep the subscript on the set of parameters unspecified. For one quantile this coverage estimate is very coarse as it can only take on one of six values. Since we want to check the agreement of nominal and empirical coverage for multiple nominal coverage values, we report the mean empirical coverage\n$\\bar{c} = <\\hat{c}_j>_{j \\in {1,...,n_Q}}$\n(5.4)\nover all quantiles. The range of possible mean values is big enough to compare to a fine spacing in nominal coverage.\nThis also allows us to judge the agreement of nominal and empirical coverage in the full data space in a single figure. However, it also introduces the possibility for over- and underconfident areas to cancel each other out. This issue will be treated in more detial in Section 5.1 and 5.2."}, {"title": "5.1 Scaling with the Number of Quantiles", "content": "To further investigate the calibration of our Bayesian generative neural networks, we pick the seemingly best calibrated parameter settings for both methods. For AdamMCMC this is $o = 0.1$ and for VIB $\\kappa = 10$. We generate $n_{\\text{MCMC}} = n_{\\text{VIB}} = 50$ samples from the posterior for both methods now and evaluate the scaling with the number of quantiles in more detail.\nAs we do not want to evaluate one calibration plot for each quantile, we reduce the diagonal calibration plots by calculating the mean (absolute) deviation between empirical and nominal coverage\nMD = (\\bar{c} - c)_{c\\in[0,1]} \\text{ and } MAD = <|\\bar{c} - c|>_{c\\in [0,1]}$,\n(5.5)"}, {"title": "5.2 Calibration at Sharp Features in Radial Direction", "content": "The right panel of Figure 4 displays the the marginal empirical coverage in radial direction $\\bar{c}_{j_r}$ for 200 \u00d7 200 quantiles and both BNN methods. We can see a distinct difference in the uncertainty quantification.\nWhile the VIB prediction seems very well calibrated in total, in the radial direction, the VIB underestimates its bias for the steeply rising part of the data distribution between $r \\in [4.0, 5.0]$. For the same interval, the MCMC prediction is well calibrated and less underconfident than for higher radii. For $r > 5$ both models slightly overestimate the uncertainty and show very similar calibration.\nIn terms of absolute uncertainty, both methods actually predict very similar results. However, the mean prediction of the VIB-CNF is strongly biased by the prior KL-loss term, resulting in large underpopulation of the generated density due to oversmoothing for $r < 4.5$ and a corresponding overpopulation in $r \\in [4.5, 5.0]$. We have tested the predictions for $\\kappa = 50$ and the behaviour"}, {"title": "6 Bayesiamplification", "content": "Based on the previous discussion of both the total and marginal calibration, we can confidently say that our AdamMCMC-CNF is well calibrated, albeit slightly underconfident for some areas of data space and small numbers of bins. It is, however, important to note that truth information was needed to evaluate the calibration of the BNN. In a practical application, this would require either a validation region or a large hold-out set, the latter of which would partially defeat the purpose of data amplification in fast detector simulation. However, for applications with validations regions, such as generative anomaly detection [33, 34], precision improvements through data amplification can be realized.\nWith a well calibrated BNN, we can try and develop a measure of the statistical power of the generated set from the uncertainties. We do so by relating the uncertainty to the statistics of an uncorrelated set of points $T$ from the truth distribution. For $n_{\\text{bins}}$ arbitrary bins, we expect the count in the $j$-th bin to be approximately Poisson distributed with mean and variance $t_j$. For the same bin, the set of $n_{\\text{MCMC}} = 50$ AdamMCMC-CNF posterior samples gives a mean prediction\n$\\bar{g}_j = <g_j^{(i)}>_{i \\in {1,...,n_{\\text{MCMC}}}} \\text{ and variance } \\sigma_{g_j}^2 = <(g_j^{(i)} - \\bar{g}_j)^2>_{i \\in {1,...,n_{\\text{MCMC}}}}$\nWe will now use the posterior mean and variance to construct an estimator $\\hat{t}_j$ of the Poisson equivalent to the per-bin predictions. Using only the mean $\\hat{t}_j = \\bar{g}_j$, the equivalent will simply be the generated statistics. Thereby, we would disregard the correlations in the generated data through limited training data completely.\nBy instead equating the variance of the BNN to that of the equivalent uncorrelated set $\\frac{t_j}{\\sqrt{\\bar{g}_j}} = \\frac{\\sigma_{g_j}}{\\bar{g}_j}$, we would introduce an unwanted dependence on the uncertainty prediction. Overestimated uncertainties would lead to an overestimation of the statistical power.\nAs we do not want to overestimate the generative performance, we aim to have undercertain predictions to lead to an underestimation of the uncorrelated equivalent. Such a behaviour can be constructed using the coefficient of variation\n$\\frac{1}{\\sqrt{\\hat{t}_j}} = \\frac{\\sigma_{g_j}}{\\bar{g}_j} \\implies \\hat{t}_j = \\frac{\\bar{g}_j^4}{\\sigma_{g_j}^2}.$\n(6.1)\nThe equivalent uncorrelated statistics now decreases for overestimated $\\sigma_{g_j}$. Both the predictions from the absolute and from the relative error give the similar estimates for well calibrated errors in our tests.\nWe calculate the equivalent truth set size for both the VIB-CNF and AdamMCMC-CNF and the quantiles from Section 5. In Figure 5, we report the amplification as the ratio of the sum over all bin estimates and the training statistics\n$\\sum_j \\hat{t}_j / N$\nin the left panel, as well as the mean estimate over all bins on the right.\nSince the amplification contains the sum over all quantiles of our setup and $\\hat{t}_j$ depends on the fluctuations of the individual predictions $g_j^{(i)}$ around the posterior mean prediction only, we expect it to scale linearly in the number of bins. This seems in good agreement with the figure. For large numbers of quantiles, where the BNNs are best calibrated, the average amplification per bin converges to a constant value. Fitting a exponential linear function $\\text{exp}(a + b \\cdot \\text{log}(x)) = a' \\cdot x^b$ to these last 8 points of Figure 5 using least squares, we indeed find no significant deviations from $b = 1$. We estimate $a' = (4.3 \\pm 2.9) \\cdot 10^{-3}$ and $b = 0.99 \\pm 0.06$ for the VIB-CNF and $a' = 0.012 \\pm 0.004$ and $b = 0.99 \\pm 0.04$ for the AdamMCMC-CNF. At lower numbers, the deviations of the model output for different parameters in the Bayesian set integrate over large intervals of the data space leading to smaller error estimates and increased amplification per bin.\nThis behaviour is consistent with the previous studies [5, 6] and the observation that one can not improve the estimation of low moments of the distribution, like the distribution mean, by oversampling with a generative neural network. From Figure 5, we can also estimate the minimum amount of bins to leverage the amplification. For the MCMC sample, evaluating at 100 bins is expected to yield an improved density estimation over using only the training set. This number could decrease for a less underconfident model. For highly granular binning, we find amplification estimates of more than a factor 100.\nFor smaller training statistics, we expect a higher initial amplification at low numbers of bins, while the corresponding larger uncertainty estimate will result in a flatter slope. The number of quantiles where an amplification larger than 1 first occurs will be smaller in such a case. Higher training statistics on the other hand will lead to a steeper slope and a later trade-off point. The results of [5] imply, that amplification effects are stronger in larger data spaces, due to the reduced density of the training data.\nSimilar calculations can be done for arbitrary binnings to justify the use of generative machine learning in a specific analysis. The evaluation of the Bayesian uncertainty prediction however requires the calculation of multiple sets of fast-simulation data points. This reduces the speed benefits of applying generative machine learning over more classical tools like MCMC simulation or inference."}, {"title": "6.1 Checking Amplification with Jensen-Shannon Divergence", "content": "To test how well the sum over all bin estimates\n$N = \\sum_j \\hat{t}_j$\nactually gauges the size of an equivalent independent data set, we calculate the Jensen-Shannon (JS) divergence\n$D_{\\text{JS}}(p,q) = \\frac{1}{2} \\sum_j \\left( p_j \\text{ log } \\left( \\frac{2 p_j}{p_j + q_j} \\right) + q_j \\text{ log } \\left( \\frac{2 q_j}{p_j + q_j} \\right) \\right)$\n(6.2)\nbetween the histogram estimation of the density in our quantiles and the known data distribution. The JS divergence is bounded by 0 and $\\text{log } 2$, with smaller values indicating similarity between the compared distributions."}, {"title": "7 Conclusion", "content": "In the previous chapters, we present a novel evaluation of the uncertainty provided by a Bayesian generative neural network in a histogram. To this end, we propose constructing confidence intervals per histogram bin and compare the nominal coverage of the constructed interval to the empirical coverage obtained from a small ensemble of BNNs.\nWe observe a strong dependence of the calibration on the parameters of both a VIB-CNF and an MCMC-sampled CNF. Furthermore, we find a strong tendency to oversmooth with strong priors leading to underestimation of the data density and corresponding error at the non-differentiable inner edge of our toy distribution. While present in both approaches, this behavior was predominantly displayed by the VIB-CNFs.\nWe further use the calibrated errors to estimate the statistical power of the generated data in terms of the size of an equivalent independently sampled data set. This estimate correctly quantifies the performance of the BNNs mean prediction when the errors are well calibrated and assigns a concrete number to the data amplification in dependence of the employed binning. For a correct amplification estimate, it is crucial that the errors are well calibrated in the full data space.\nSimilar calibration checks can be applied wherever a generative neural network is used for inference or generation with a sufficient validation set or for interpolation into hold-out regions of the data."}]}