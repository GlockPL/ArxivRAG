{"title": "PatchFinder: A Two-Phase Approach to Security Patch Tracing for Disclosed Vulnerabilities in Open-Source Software", "authors": ["Kaixuan Li", "Jian Zhang", "Sen Chen", "Han Liu", "Yang Liu", "Yixiang Chen"], "abstract": "Open-source software (OSS) vulnerabilities are increasingly prevalent, emphasizing the importance of security patches. However, in widely used security platforms like NVD, a substantial number of CVE records still lack trace links to patches. Although rank-based approaches have been proposed for security patch tracing, they heavily rely on handcrafted features in a single-step framework, which limits their effectiveness.\nIn this paper, we propose PatchFinder, a two-phase framework with end-to-end correlation learning for better-tracing security patches. In the initial retrieval phase, we employ a hybrid patch retriever to account for both lexical and semantic matching based on the code changes and the description of a CVE, to narrow down the search space by extracting those commits as candidates that are similar to the CVE descriptions. Afterwards, in the re-ranking phase, we design an end-to-end architecture under the supervised fine-tuning paradigm for learning the semantic correlations between CVE descriptions and commits. In this way, we can automatically rank the candidates based on their correlation scores while maintaining low computation overhead. We evaluated our system against 4,789 CVEs from 532 OSS projects. The results are highly promising: PatchFinder achieves a Recall@10 of 80.63% and a Mean Reciprocal Rank (MRR) of 0.7951. Moreover, the Manual Effort@10 required is curtailed to 2.77, marking a 1.94 times improvement over current leading methods. When applying PatchFinder in practice, we initially identified 533 patch commits and submitted them to", "sections": [{"title": "1 INTRODUCTION", "content": "Open-source software (OSS) is fundamental to the industrial applications and software community. This widespread adoption, however, comes with security challenges. The open and accessible nature of OSS has inadvertently led to a surge in security vulnerabilities [24, 49, 51]. The notorious vulnerabilities such as Log4Shell [8] and Spring4Shell [6] have put millions at risk of data theft and service denials, reducing trust in the OSS ecosystem.\nTo facilitate understanding and remediation for vulnerabilities, public security platforms, including the Common Vulnerabilities and Exposures (CVE) and the National Vulnerability Database (NVD), provide details (e.g., descriptions [15]) on disclosed software vulnerabilities and links to relevant patches for mitigation. However, a recent study revealed that almost 57% of CVEs lack trace links to patches, and only 12% of commits in OSS reference the corresponding CVE-IDs [38]. One fact is that the maintainers (e.g., CVE Numbering Authorities, CNAs) may not update the trace link on CVE/NVD even though the vulnerability has been fixed."}, {"title": "2 BACKGROUND AND MOTIVATION", "content": ""}, {"title": "2.1 Large Language Models (LLMs)", "content": "Pre-trained language models like BERT [7], GPT [32], Llama2 [41], and T5 [33] have significantly advanced NLP tasks. These models adopt a pre-training and then fine-tuning paradigm to develop transferable language representations. This paradigm has been adapted to programming languages with models such as CodeBERT [9], CodeLlama [36], CodeT5 [44], and CodeReviewer [22]. These models have demonstrated remarkable effectiveness, achieving SOTA"}, {"title": "2.2 Problem Definition", "content": "Since it is labor-intensive to trace the security patches for disclosed software vulnerabilities of CVEs, our objective is to design a model that can automatically identify the patches from OSS projects. We view the process as a ranking problem that ranks the commits in OSS projects based on their correlations with reported CVEs. Ideally, when provided with a CVE, the model should rank the associated security patch as high as possible. The model could take the CVE description, commit messages, and code changes (diffs) as input. It then generates a ranked list of commits for the given CVE, indicating the likelihood of each commit being the relevant patch. Input Data: CVE Descriptions and Commits. Let D be the set of CVE descriptions, such that D = {d1, d2, . . ., d|1| }. For each description di in D, we have a corresponding set of commits Ci = {C1, C2, ..., CC\u2081| }. Each commit cj in C\u012f is represented as a tuple containing its message and code diff, i.e., cj = (msgj, diffj).\nOutput Data: Ranked Commits. For a given CVE description di and its associated commits in C\u2081, the model produces a ranking vector Ri = [r1, 2, ..., r|c\u2081|]. This vector indicates the likelihood of each commit being the patch for di, allowing the commits to be sorted based on their rankings."}, {"title": "2.3 Motivating Example", "content": "The example in Listing 1 illustrates the challenge of associating a CVE description with its corresponding patch commit [3]. The description for CVE-2015-1867 [29] (Line 2) hints at a vulnerability in pacemaker for versions below 1.1.13, but lacks specifics such as its exact location including the function name or file name. Moreover, the commit message (Lines 5-6) provides a hint about the root cause but expresses it differently and does not mention the exploitation of the vulnerability. Existing SOTA tools like PatchScout and VCMatch predominantly rely on commit messages, predefined vulnerability type mappings, and handcrafted features [38, 42]. Such an approach can be limiting, especially when faced with ambiguous CVE descriptions that do not directly match commit messages. Solely relying on token-based features without considering the semantic nuances present in the message and diff can lead to inaccuracies."}, {"title": "3 APPROACH", "content": ""}, {"title": "3.1 Overview", "content": "In this section, we present an overview of our approach designed to trace security patches for disclosed vulnerabilities from the NVD/CVE websites. As highlighted in earlier discussions, the primary challenge lies in the implicit correlations between CVE descriptions and commits, which necessarily require understanding the semantics of both sides. Theoretically, we recognized the challenges posed by direct retrieval using a fine-tuned LLM in extensive"}, {"title": "3.2 Initial Retrieval via Hybrid Retriever", "content": "In the vast landscape of open-source repositories, developmental commits overwhelmingly outnumber security patches. To illustrate, the renowned Linux kernel project [40] has amassed 1,215, 313 commits as of 15th September 2023. Yet, throughout its history, it has been associated with only 4, 165 CVEs [4]. While existing ranking-based methods such as PatchScout [38] have made notable strides, they predominantly lean on handcrafted features to pinpoint security patches. Given the overwhelming number of commits, these methods might not fit well to consistently attain the desired precision. To address this, we incorporate initial retrieval into the security patch tracing task. Specifically, we utilize a hybrid approach to combine a lexical-based TF-IDF [2] retriever and a semantic-based CodeReviewer (pretrained) retriever to take both lexical and semantic information into account. This is because, prior works [18, 48] show that sparse and dense retrievers can complement each other for more robust text retrieval. Due to the existence of large commits and the length constraint of CodeReviewer (maximum of 512 tokens), we preprocess diff files by extracting only the lines that involve code changes and then limit the scope to the first 1,000 lines. The statistics show that it can get good coverage (98.6%) of the patch samples on our dataset."}, {"title": "3.2.1 Lexical-based Retriever", "content": "TF-IDF [2] stands out for its efficiency and its well-recognized capability to capture lexical similarities. At this stage, our objective is not to definitively locate the security patches but to considerably narrow down the pool of potential commits. By harnessing the capabilities of TF-IDF, we can effectively filter out commits less likely to be security patches, paving the way for a more in-depth analysis in the following stages of our approach.\nFormally, in our task, the term t represents individual words or tokens present in CVE descriptions or commits, which includes both commit messages and code diffs. Given a CVE description di \u2208 D, both di and the corresponding commits cj \u2208 C\u012f are treated as separate documents. The entire set of commits associated with a particular CVE, denoted as Ci, forms our corpus for di.\nThe TF-IDF score for a term t in a document d (either a CVE description di or a commit cj) within the corpus C\u012f is given by:\n$$TF-IDF(t, d, C\u2081) = TF(t, d) \\times IDF(t, C_i)$$\nHere TF(t, d) is the term frequency of t in d, calculated as the number of times t appears in d divided by the total number of terms in d. IDF(t, Ci) is the inverse document frequency of t in Ci, calculated as the logarithm of the total number of documents in Ci divided by the number of documents containing t.\nTo measure the similarity between the TF-IDF vectors of a given CVE description di and a code commit c (c\u2208 Ci), we employ"}, {"title": "3.2.2 Semantic-based Retriever", "content": "Inspired by [50], we adopt a pre-trained CodeReviewer model to retrieve relevant patches by measuring their semantic similarity. Specifically, to encode the CVE description and commits, we use a CodeReviewer encoder to map each CVE description and commit pair (di, cj) (where di \u2208 D, cj \u2208 Ci) to a fixed-size dense vector, leveraging its proficiency in analyzing code changes and understanding the semantics of defects such as vulnerabilities. Specifically, given a CVE description di =< d, d... ddi >and a candidate commit cj =< c},c}, >, we use contextual embeddings to represent the tokens and compute matching using cosine similarity (as shown in Figure 2).\nToken Representation. We use contextual embeddings to represent the tokens in the CVE description di and candidate commit cj, since its better semantic capturing when compared with word embeddings [50]. Contextual embeddings can generate different vector representations for the same word in different sentences depending on the surrounding words, which form the context of the target word. Specifically, We use a shared pretrained CodeReviewer (abbr. CRP) to separately encode the CVE description di in D and each commits candidate cj in Ci. We prepend a special token of [CLS] into its tokenized sequence and employ the final layer hidden state of the [CLS] token as the patch representation. We format each of the commits as {[CLS], diff j, [MSG], msgj}. Then the CVE description di and each commit cj \u2208 Ci are fed separately into the CRP encoder to obtain the sequences of token vectors, which can be formulated as: S(di) = CRPencoder (di), and S(cj) = CRPencoder(msgj; diffj) respectively.\nSimilarity Calculation. The token representation facilitates a soft measure of similarity instead of exact-string or heuristic matching in lexical-based methods. For each token vector in the CVE description, we denote them as dm \u2208 S(di) and commit d \u2208 S(cj),"}, {"title": "3.2.3 Hybrid Retriever", "content": "To take both lexical and semantic information into account, we utilize a hybrid approach following [18] to combine TF-IDF and CodeReviewer. The fusion of lexical and semantic similarities leverages their complementary analysis perspectives-lexical for word-based similarity and semantic for conceptual alignment (e.g., synonyms). Additionally, they share the same value space ([0,1]), facilitating straightforward additive fusion. The parameter A adjusts the emphasis on these features, allowing for a unified similarity metric. The similarity score is computed as fp(di, cj) = sim(di, cj) + \u03bb \u00b7 cosine(di, c'j), where a is a weight to balance the two retrievers. After conducting a parameter tuning process including a grid search over various values (from 0.1 to 10 with a step of 0.05), we found that \u03bb = 1 in our experiment delivers optimal effectiveness among them. Nevertheless, we retain the parameter 1 to facilitate adaptation to different datasets."}, {"title": "3.3 Re-ranking via Fine-tuning CodeReviewer", "content": "As mentioned above, we have refined the list of commits to the top-k most relevant candidates for each CVE. For this phase, we opt for CodeReviewer [22], a state-of-the-art large language model pre-trained on the foundation of CodeT5 [44]. There are two considerations for this choice. 1) Encoder Specialization: CodeReviewer's encoder is designed to deeply understand commit behaviors and issues, a feature not necessarily present or optimized in other models. This encoder specialization ensures that the model comprehends the intricate relationships between code changes and potential security implications, vital for matching commits to CVE descriptions. 2) Downstream Task Optimization: Although our focus is not on generating code reviews, the fact that CodeReviewer's decoder is optimized for such downstream tasks indicates its ability to link code changes to descriptive text, a parallel to our objective of linking commits to CVE descriptions. Given these advantages, we fine-tune CodeReviewer on the top-k candidate commits, aiming to re-rank them based on their relevance to the respective CVE descriptions.\nWe only use the pre-trained encoder of CodeReviewer (abbr. CR) since our task can be basically viewed as a binary classification problem in the re-ranking phrase. Specifically, given a CVE description di, and the top-k commits represented as Ck = {(msgj, diffj)}}=1'\nwe format each commit as {[CLS], diff j, [MSG], msgj}. Then the CVE description d and each commit cj \u2208 Ck are encoded separately using the CR encoder to yield two sequences of vectors:\n$$E(d) = CR_{encoder} (d)$$\n$$E(c_j) = CR_{encoder} (msg_j; diff_j)$$\nWe obtain the vector representations of di and cj by extracting the hidden state in the last layer of the special token [CLS] at the beginning of E(di) and E(cj) respectively. The encoded vectors of the CVE description and the commit are concatenated:\n$$V_j = [E(d_i); E(c_j)]$$\nWe apply a linear classifier to the concatenated vector V; for estimating the correlations:\n$$y_j = \\sigma(W \\cdot V_j + b)$$\nwhere W is the weight matrix, b is the bias term, and o denotes the sigmoid function ensuring the output lies between 0 and 1.\nWe utilize labeled data containing known CVE-commit pairs during the fine-tuning phase. The training goal is to minimize the"}, {"title": "4 EVALUATION", "content": ""}, {"title": "4.1 Research Questions", "content": "We aim to answer the following research questions:\n\u2022 RQ1: Effectiveness Analysis. How effective is PatchFinder when compared with existing baselines in tracing security patches?\n\u2022 RQ2: Ablation Analysis. What impact does each component of PatchFinder have on the overall performance?\n\u2022 RQ3: Distribution Analysis. Does PatchFinder exhibit notably high or low accuracy for certain vulnerability types or severity?\n\u2022 RQ4: Practicality Analysis. How effective is PatchFinder in real-world applications, particularly when detecting security patches for CVEs without associated trace links?"}, {"title": "4.2 Dataset", "content": "For the training and evaluation of our model, we compiled a dataset that encompasses both OSS vulnerabilities and their corresponding security patches. The dataset was assembled in two primary steps: 1) Initial Data Collection: We began by collecting data from Wang et al. [42] and Tan et al. [38]. We thereby obtained 1,669 unique CVEs from 10 OSS projects. 2) Dataset Supplement: To augment our dataset, we crawled vulnerabilities and their associated patches from multiple sources, including the official CVE, NVD, and Snyk [37] vulnerability databases. This initial collection yielded 3,585 unique CVEs from 532 OSS projects. After eliminating duplicate entries, our finalized dataset comprises 4,789 unique CVEs and 4,870 distinct security patch commits, which involve 532 unique OSS projects, covering various programming languages including C/C++, Java, JavaScript, and PHP. This makes our dataset the most extensive collection of CVEs and security patches available to date. Each dataset entry includes the CVE-ID along with its textual description and the corresponding security patch links. We also extracted related commits including commit messages and code diffs, primarily from GitHub [11] and GitLab [12].\nTo create a robust training set, we followed the practices in prior works [38, 42] to sample 5,000 non-patch commits as negative samples for each CVE. However, in scenarios where a repository contained fewer than 5,000 commits in total, we included all available non-patch commits as negative samples. Finally, we got 21,781,044 commits in total. To the best of our knowledge, this is the largest dataset specific for the security patch tracing problem."}, {"title": "4.3 Experiment Setup", "content": "We randomly split the 4,789 unique CVEs along with their corresponding commits in the proportion of 8:1: 1 to keep the same split settings as baselines [38, 42]. The maximum token length for CodeReviewer is set at 512, which represents its upper limit for processing capacity. Given the preprocessing of code diffs as detailed in Section 3, this token length is actually sufficient for our purposes. To preprocess data, we use the NLTK toolkit and the BPE tokenizer of CodeReviewer to tokenize CVE descriptions and commits. For the initial retrieval phase, we retrieve the top 100 commits from the initial 5,000 commits for each CVE. This threshold k enables us to obtain a good balance between the coverage of true patches and noisy commits. During fine-tuning, the batch size is set to 32 and the maximum epoch is 20. We adopt the widely-used Adam [21] as the optimizer with a learning rate of 5e-5 for training our model. All the above hyper-parameters are determined based on the validation set by selecting the best ones among some alternatives. All experiments ran on a server with 48 CPU cores (Intel\u00ae Xeon\u00ae Silver 4214 CPU @ 2.20GHz), 252 GB RAM, and 4 NVIDIA RTX 3090 GPUs (24 GB memory each)."}, {"title": "4.4 Baselines", "content": "We benchmark our approach against state-of-the-art works in security patch tracing as presented in [38, 42]. Due to the absence of available replication packages of PatchScout [38], we implemented it independently by adhering to the default settings unless specified otherwise. Additionally, we consider two renowned baselines frequently used in the information retrieval domain for our evaluation: BM25 [35], a classic method for sparse retrieval [31], and ColBERT [19], known for dense retrieval [18]."}, {"title": "4.5 Evaluation Metrics", "content": "To ensure a fair comparison between PatchFinder and the baselines, we utilize three metrics: Recall@K, Mean Reciprocal Rank (MRR) [14], and Manual Efforts@K. Recall@K and Manual Efforts@K have been previously employed in previous studies [38, 42]. We also incorporate MRR into our evaluation, given its established significance in ranking systems."}, {"title": "4.5.1 Recall@K", "content": "Recall@K refers to the ratio of the number of security patches traced in the top-K results to the number of all security patches for a given K. Hence, a higher Recall@K score means better performance."}, {"title": "4.5.2 Mean Reciprocal Rank (MRR)", "content": "MRR is a widely used evaluation metric for ranking systems, particularly in the domain of information retrieval. It emphasizes the importance of the position of the first relevant result in a list of retrieved documents, making it especially relevant for security patch tracing where we typically seek a single commit. MRR is defined mathematically as follows:\n$$MRR = \\frac{1}{|D|} \\sum_{i=1}^{|D|} \\frac{1}{rank_i}$$\nIn this equation, |D| represents the total number of CVEs, and rank\u012f denotes the position of the first security patch for the i-th CVE. The MRR values range between 0 and 1, with higher values indicating better retrieval performance. By considering the inverse"}, {"title": "4.5.3 Manual Efforts@K", "content": "In the pursuit of tracing security patches within OSS, the Manual Efforts@K metric emerges as a classic metric. It represents the manual inspection effort required to locate the correct patch within the top-K results. If the desired security patch is found within these results, the effort corresponds to its rank. However, if the patch is not within the top-K, the effort is K, indicating a comprehensive search without success. Drawing from related work [38], the metric is mathematically expressed as:\n$$Manual\\ Efforts@K = \\frac{\\sum_{i=1}^{n} min(rank_i, K)}{n}$$\nA lower Manual Effort@K score is indicative of a more efficient and effective method for tracing security patch commits. This aids NVD security experts in mitigating the extensive manual work associated with tracing security patches, reducing inspection time, and enhancing patch detection accuracy."}, {"title": "5 RESULTS AND DISCUSSION", "content": "We investigate the following research questions to provide a thorough analysis of the experimental results."}, {"title": "5.1 RQ1: Effectiveness Analysis", "content": "Table 1 and Table 2 show the effectiveness of different approaches in terms of Recall@K, MRR, and Manual Efforts@K, with the best one of each metric marked in bold. Table 1 reveals that PatchFinder significantly outperforms all the SOTA approaches across different values of K, for the Recall@K metric. The superiority of PatchFinder is most prominent at K = 1 where it achieves a Recall of 79.23%, markedly higher than PatchScout's 46.25%, VCMatch's 55.93%, BM25 's 11.88%, and ColBERT's 26.29%. This trend continues as K increases, showcasing the consistent effectiveness of PatchFinder in locating security patch commits within the top-K results. Notably, the Recall@K for PatchFinder remains above 79% for all values of K, highlighting its robustness in tracing relevant security patches. The MRR further confirms the effectiveness of PatchFinder with a score of 0.7951, significantly outpacing the 0.6195 and 0.3824 attained by VCMatch and PatchScout, respectively. This superior performance can be attributed to our two-phase approach that combines"}, {"title": "5.2 RQ2: Ablation Analysis", "content": "In our proposed approach, we integrate two primary components: an initial retrieval using a hybrid retriever consisting of TF-IDF and pre-trained CodeReviewer (Phase-1), followed by a subsequent re-ranking using a fine-tuned CodeReviewer model (Phase-2). The input to our system encompasses the CVE description, commit messages, and code diffs. To dissect their effectiveness, we conducted a detailed ablation study by examining the performance of PatchFinder under various configurations: 1 Using only the"}, {"title": "5.3 RQ3: Distribution Analysis", "content": "This RQ focuses on a detailed investigation of the outcomes from PatchFinder. The primary aim is to explore if there exists a correlation between the types of vulnerabilities and the true patches identified by PatchFinder. Specifically, we are interested in examining whether PatchFinder's retrieval accuracy varies across different vulnerability categories and their respective severity levels.\nTo conduct this analysis, we categorized the vulnerabilities in our test dataset based on their Common Weakness Enumeration (CWE) identifiers [5] and Common Vulnerability Scoring System (CVSS) V2 scores [10]. Our analysis reveals that PatchFinder is particularly effective in tracing security patches for specific types of"}, {"title": "5.4 RQ4: Practicality Analysis", "content": "In evaluating the practical utility of PatchFinder, we initially curated a dataset of 212, 074 CVE entries from NVD as of April 2023. However, we encountered a significant challenge in accurately identifying CVEs without patches since the \"patch\" tags in the NVD are often imprecise [38]: some entries with patches lack the corresponding tag, while others may be inaccurately tagged as having a patch but in fact, they do not. Hence, we opted to focus on a more reliable subset: CVEs affecting OSS and known to lack patches. To this end, we leveraged the OSS project list maintained"}, {"title": "5.5 Case Study", "content": "Among the 533 patches we traced in RQ4, a particularly illustrative instance is CVE-2022-31814 from the \"pfSense-pfBlockerNG\u201d project. As shown in Listing 2, this vulnerability allows remote attackers to execute arbitrary OS commands via specific manipulations. The associated patch commit, with its seemingly innocuous commit message \"Update index.php\" does not directly indicate its relevance to the CVE. This ambiguity poses challenges for tools like PatchScout and VCMatch who failed to recognize it. Their reliance on direct textual correlations (such as \"# shared file names\", \"# shared function names\", and \"# shared words\", etc.) between CVE descriptions and commits can be limited, especially when descriptions and commits employ synonyms or different phrasings.\nIn contrast, PatchFinder outperforms in such situations since harnessing the strengths of TF-IDF and CodeReviewer. Unlike other tools, it effectively deals with commits having limited information. Specifically, the patch commit in Listing 2 initially ranked 47th in our lexical-based retriever due to its brief message. However, the semantic-based retriever recognized its relevance, where the addition of escapeshellarg at Lines 23-24 crucially sanitizes shell metacharacters in the HTTP Host header, directly addressing the vulnerability. Such intricate changes, often missed by other tools, are accurately identified by PatchFinder due to its outstanding semantic analysis of code and text. As a result, our hybrid retriever improved its rank to 23rd. In the subsequent phase, the top-100 results, including this commit, were analyzed further. Here, the fine-tuned CR, adept at understanding code semantics, elevated its rank to 7th, placing it within the top-10 results."}, {"title": "5.6 Discussion on False Negatives", "content": "We further analyzed the missed patch commits of 95 CVEs and summarized them into three main causes.\n\u2022 Low-Quality CVE Descriptions (28/95): Some CVE descriptions lack sufficient detail for effective patch tracing. Notably,"}, {"title": "6 THREATS TO VALIDITY", "content": "External Threats. A primary external threat pertains to the reproducibility of the baselines. While we endeavored to faithfully implement PatchScout based on its published methodology, the absence of its source code posed challenges. To ensure a robust comparison with state-of-the-art methods, we also incorporated BM25 and ColBERT, which are notable sparse and dense retrieval models, respectively, as additional baselines.\nInternal Threats. Our dataset's quality and scope could introduce internal threats. To minimize it, we initially built our dataset upon datasets from prior studies [38, 42], and followed their practice to source CVEs and security patches from various public advisories. Despite our efforts to curate a broad and diverse dataset, biases from these sources might persist. In the future, we will consider"}, {"title": "7 RELATED WORK", "content": "There are numerous works focusing on tracing security patches, which can be divided into two categories: tracing security patches for disclosed vulnerabilities [26, 27, 38, 42, 47] and identifying silent security patches [1, 43, 46, 52-55].\nTracing security patches for disclosed vulnerabilities. Xu et al. [47] conducted an empirical study to understand the quality and characteristics of patches for disclosed vulnerabilities in two industrial vulnerability databases, thereby proposing to track patches from the CVE reference links across multiple knowledge sources (e.g., Debian). Their work focuses on analyzing reference links provided by security analysts, instead of directly tracing patches from OSS repositories. Tan et al. [38] conducted the most related work with PatchFinder. They designed a ranking-based tool named PatchScout to locate the patch commits by using RankNet on manually defined features from the CVE description and commits. Similarly, VCMatch [42], which directly classifies one commit as related or unrelated to the CVE description by fusing the features from PatchScout and extracted vectors from Bert. Unlike PatchScout and VCMatch, PatchFinder introduces a novel two-phase framework designed to overcome the challenges posed by large search spaces, and enables an end-to-end fine-tuning, to fully exploit the natural correlation between CVE descriptions and commits.\nIdentifying silent security patches. Besides, several works [1, 43, 46, 52-55] have delved into silent security patch identification. These efforts discern security patches but do not correlate them with specific vulnerabilities they rectify. In contrast, our focus is on tracing security patches tailored to a particular vulnerability, as defined by its CVE description."}, {"title": "8 CONCLUSION", "content": "In this paper, we present PatchFinder, an end-to-end and LLM-enhanced two-phase approach for effectively tracing security patches for disclosed vulnerabilities in OSS. The first phase employs a hybrid retriever for the initial retrieval of relevant commits, significantly narrowing down the candidate pool. The second phase leverages a fine-tuned CodeReviewer model to re-rank these commits, achieving a high degree of accuracy. Our extensive evaluations demonstrate that PatchFinder consistently outperforms state-of-the-art methods in Recall@K, MRR, and manual efforts, setting PatchFinder as a new benchmark in the field of security patch tracing."}]}