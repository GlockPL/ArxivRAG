{"title": "PatchFinder: A Two-Phase Approach to Security Patch Tracing for Disclosed Vulnerabilities in Open-Source Software", "authors": ["Kaixuan Li", "Jian Zhang", "Sen Chen", "Han Liu", "Yang Liu", "Yixiang Chen"], "abstract": "Open-source software (OSS) vulnerabilities are increasingly prevalent, emphasizing the importance of security patches. However, in widely used security platforms like NVD, a substantial number of CVE records still lack trace links to patches. Although rank-based approaches have been proposed for security patch tracing, they heavily rely on handcrafted features in a single-step framework, which limits their effectiveness.\nIn this paper, we propose PatchFinder, a two-phase framework with end-to-end correlation learning for better-tracing security patches. In the initial retrieval phase, we employ a hybrid patch retriever to account for both lexical and semantic matching based on the code changes and the description of a CVE, to narrow down the search space by extracting those commits as candidates that are similar to the CVE descriptions. Afterwards, in the re-ranking phase, we design an end-to-end architecture under the supervised fine-tuning paradigm for learning the semantic correlations between CVE descriptions and commits. In this way, we can automatically rank the candidates based on their correlation scores while maintaining low computation overhead. We evaluated our system against 4,789 CVEs from 532 OSS projects. The results are highly promising: PatchFinder achieves a Recall@10 of 80.63% and a Mean Reciprocal Rank (MRR) of 0.7951. Moreover, the Manual Effort@10 required is curtailed to 2.77, marking a 1.94 times improvement over current leading methods. When applying PatchFinder in practice, we initially identified 533 patch commits and submitted them to the official, 482 of which have been confirmed by CVE Numbering Authorities.", "sections": [{"title": "1 INTRODUCTION", "content": "Open-source software (OSS) is fundamental to the industrial applica-tions and software community. This widespread adoption, however,comes with security challenges. The open and accessible natureof OSS has inadvertently led to a surge in security vulnerabili-ties [24, 49, 51]. The notorious vulnerabilities such as Log4Shell [8]and Spring4Shell [6] have put millions at risk of data theft andservice denials, reducing trust in the OSS ecosystem.\nTo facilitate understanding and remediation for vulnerabilities,public security platforms, including the Common Vulnerabilitiesand Exposures (CVE) and the National Vulnerability Database(NVD), provide details (e.g., descriptions [15]) on disclosed soft-ware vulnerabilities and links to relevant patches for mitigation.However, a recent study revealed that almost 57% of CVEs lacktrace links to patches, and only 12% of commits in OSS referencethe corresponding CVE-IDs [38]. One fact is that the maintainers(e.g., CVE Numbering Authorities, CNAs) may not update the tracelink on CVE/NVD even though the vulnerability has been fixed."}, {"title": "2 BACKGROUND AND MOTIVATION", "content": "Pre-trained language models like BERT [7], GPT [32], Llama2 [41],and T5 [33] have significantly advanced NLP tasks. These modelsadopt a pre-training and then fine-tuning paradigm to develop trans-ferable language representations. This paradigm has been adaptedto programming languages with models such as CodeBERT [9],CodeLlama [36], CodeT5 [44], and CodeReviewer [22]. These mod-els have demonstrated remarkable effectiveness, achieving SOTA"}, {"title": "2.1 Large Language Models (LLMs)", "content": "Pre-trained language models like BERT [7], GPT [32], Llama2 [41],and T5 [33] have significantly advanced NLP tasks. These modelsadopt a pre-training and then fine-tuning paradigm to develop trans-ferable language representations. This paradigm has been adaptedto programming languages with models such as CodeBERT [9],CodeLlama [36], CodeT5 [44], and CodeReviewer [22]. These mod-els have demonstrated remarkable effectiveness, achieving SOTA"}, {"title": "2.2 Problem Definition", "content": "Since it is labor-intensive to trace the security patches for disclosedsoftware vulnerabilities of CVEs, our objective is to design a modelthat can automatically identify the patches from OSS projects. Wewiew the process as a ranking problem that ranks the commitsin OSS projects based on their correlations with reported CVEs.Ideally, when provided with a CVE, the model should rank theassociated security patch as high as possible. The model could takethe CVE description, commit messages, and code changes (diffs) asinput. It then generates a ranked list of commits for the given CVE,indicating the likelihood of each commit being the relevant patch.Input Data: CVE Descriptions and Commits. Let D be the set ofCVE descriptions, such that D = {d\u2081, d\u2082,..., d\u2081D\u2081}. For each de-scription d\u1d62 in D, we have a corresponding set of commits C\u1d62 ={c\u2081, c\u2082,..., c|C\u1d62|}. Each commit c\u2c7c in C\u1d62 is represented as a tuplecontaining its message and code diff, i.e., c\u2c7c = (msg\u2c7c, diff\u2c7c).Output Data: Ranked Commits. For a given CVE description d\u1d62 andits associated commits in C\u1d62, the model produces a ranking vectorR\u1d62 = [r\u2081, r\u2082,..., r|C\u1d62|]. This vector indicates the likelihood of eachcommit being the patch for d\u1d62, allowing the commits to be sortedbased on their rankings."}, {"title": "2.3 Motivating Example", "content": "The example in Listing 1 illustrates the challenge of associatinga CVE description with its corresponding patch commit [3]. The de-scription for CVE-2015-1867 [29] (Line 2) hints at a vulnerability inpacemaker for versions below 1.1.13, but lacks specifics such as itsexact location including the function name or file name. Moreover,the commit message (Lines 5-6) provides a hint about the root causebut expresses it differently and does not mention the exploitation ofthe vulnerability. Existing SOTA tools like PatchScout and VCMatchpredominantly rely on commit messages, predefined vulnerabilitytype mappings, and handcrafted features [38, 42]. Such an approachcan be limiting, especially when faced with ambiguous CVE descrip-tions that do not directly match commit messages. Solely relyingon token-based features without considering the semantic nuancespresent in the message and diff can lead to inaccuracies."}, {"title": "3 APPROACH", "content": "In this section, we present an overview of our approach designedto trace security patches for disclosed vulnerabilities from theNVD/CVE websites. As highlighted in earlier discussions, the pri-mary challenge lies in the implicit correlations between CVE de-scriptions and commits, which necessarily require understandingthe semantics of both sides. Theoretically, we recognized the chal-lenges posed by direct retrieval using a fine-tuned LLM in extensive"}, {"title": "3.1 Overview", "content": "In this section, we present an overview of our approach designedto trace security patches for disclosed vulnerabilities from theNVD/CVE websites. As highlighted in earlier discussions, the pri-mary challenge lies in the implicit correlations between CVE de-scriptions and commits, which necessarily require understandingthe semantics of both sides. Theoretically, we recognized the chal-lenges posed by direct retrieval using a fine-tuned LLM in extensive"}, {"title": "3.2 Initial Retrieval via Hybrid Retriever", "content": "In the vast landscape of open-source repositories, developmentalcommits overwhelmingly outnumber security patches. To illustrate,the renowned Linux kernel project [40] has amassed 1,215, 313commits as of 15th September 2023. Yet, throughout its history,it has been associated with only 4, 165 CVEs [4]. While existingranking-based methods such as PatchScout [38] have made notablestrides, they predominantly lean on handcrafted features to pinpointsecurity patches. Given the overwhelming number of commits,these methods might not fit well to consistently attain the desiredprecision. To address this, we incorporate initial retrieval into thesecurity patch tracing task. Specifically, we utilize a hybrid approachto combine a lexical-based TF-IDF [2] retriever and a semantic-based CodeReviewer (pretrained) retriever to take both lexical andsemantic information into account. This is because, prior works [18,48] show that sparse and dense retrievers can complement eachother for more robust text retrieval. Due to the existence of largecommits and the length constraint of CodeReviewer (maximumof 512 tokens), we preprocess diff files by extracting only the linesthat involve code changes and then limit the scope to the first 1,000lines. The statistics show that it can get good coverage (98.6%) ofthe patch samples on our dataset."}, {"title": "3.2.1 Lexical-based Retriever", "content": "TF-IDF [2] stands out for itsefficiency and its well-recognized capability to capture lexical similarities. At this stage, our objective is not to definitively locatethe security patches but to considerably narrow down the pool ofpotential commits. By harnessing the capabilities of TF-IDF, wecan effectively filter out commits less likely to be security patches,paving the way for a more in-depth analysis in the following stagesof our approach.\nFormally, in our task, the term t represents individual words ortokens present in CVE descriptions or commits, which includesboth commit messages and code diffs. Given a CVE description d\u1d62\u2208 D, both d\u1d62 and the corresponding commits c\u2c7c \u2208 C\u1d62 are treated asseparate documents. The entire set of commits associated with aparticular CVE, denoted as C\u1d62, forms our corpus for d\u1d62.\nThe TF-IDF score for a term t in a document d (either a CVEdescription d\u1d62 or a commit c\u2c7c) within the corpus C\u1d62 is given by:TF-IDF(t, d, C\u1d62) = TF(t, d) \u00d7 IDF(t, C\u1d62) (1)\nHere TF(t, d) is the term frequency of t in d, calculated as thenumber of times t appears in d divided by the total number ofterms in d. IDF(t, C\u1d62) is the inverse document frequency of t in C\u1d62,calculated as the logarithm of the total number of documents in C\u1d62divided by the number of documents containing t.\nTo measure the similarity between the TF-IDF vectors of a givenCVE description d\u1d62 and a code commit c (c\u2208 C\u1d62), we employ"}, {"title": "3.2.2 Semantic-based Retriever", "content": "Inspired by [50], we adopt a pre-trained CodeReviewer model to retrieve relevant patches by measuring their semantic similarity. Specifically, to encode the CVE description and commits, we use a CodeReviewer encoder to map each CVE description and commit pair (d\u1d62, c\u2c7c) (where d\u1d62 \u2208 D, c\u2c7c \u2208 C\u1d62) to a fixed-size dense vector, leveraging its proficiency in analyzing code changes and understanding the semantics of defects such as vulnerabilities. Specifically, given a CVE description d\u1d62 = \u27e8d\u2081, d\u2082,..., d|d\u1d62|\u27e9 and a candidate commit c\u2c7c = \u27e8c\u00b9, c\u00b2,..., c|c\u2c7c|\u27e9, we use contextual embeddings to represent the tokens and compute matching using cosine similarity (as shown in Figure 2).\nToken Representation. We use contextual embeddings to represent the tokens in the CVE description d\u1d62 and candidate commit c\u2c7c, since its better semantic capturing when compared with word embeddings [50]. Contextual embeddings can generate different vector representations for the same word in different sentences depending on the surrounding words, which form the context of the target word. Specifically, We use a shared pretrained CodeReviewer (abbr. CRP) to separately encode the CVE description d\u1d62 in D and each commits candidate c\u2c7c in C\u1d62. We prepend a special token of [CLS] into its tokenized sequence and employ the final layer hidden state of the [CLS] token as the patch representation.\nWe format each of the commits as {[CLS], diff\u2c7c, [MSG], msg\u2c7c}.\nThen the CVE description d\u1d62 and each commit c\u2c7c \u2208 C\u1d62 are fed separately into the CRP encoder to obtain the sequences of token vectors, which can be formulated as: S(d\u1d62) = CRPencoder(d\u1d62), and S(c\u2c7c) = CRPencoder(msg\u2c7c; diff\u2c7c) respectively.\nSimilarity Calculation. The token representation facilitates a soft measure of similarity instead of exact-string or heuristic matching in lexical-based methods. For each token vector in the CVE description, we denote them as dm \u2208 S(d\u1d62) and commit d \u2208 S(c\u2c7c), respectively. Then we calculate their cosine similarity to consider token relations between them. To reduce the calculation cost to theinner product dmc, we use pre-normalized vectors. While thismeasure considers tokens in isolation, the contextual embeddingscontain information from the rest of the sentence.\nBased on this, we calculate the complete score that matches eachtoken in CVE description d\u1d62 to a token in candidate commit c\u2c7c tocompute Recall and each token in c\u2c7c to a token in d\u1d62 to computePrecision. We use greedy matching to maximize the matching simi-larity score, where each token is matched to the most similar tokenin the other sentence. Finally, we combine precision and recall tocompute an F1 measure. For a CVE description d\u1d62 and its candi-date commit c\u2c7c, the Recall (R), Precision (P), and F1 score (F1) arecalculated as:\nR = 1/|d\u1d62| \u03a3d\u2098\u2208d\u1d62 max c\u2c7c\u2208c\u2c7c d\u2098c\u2c7c (3)\nP = 1/|c\u2c7c| \u03a3c\u2091\u2208c\u2c7c max d\u2098\u2208d\u1d62 d\u2098c\u2091 (4)\nsim(d\u1d62, c\u2c7c) = F1 = 2\u22c5R\u22c5P/R+P (5)\nThe F1 similarity score ranges between 0 and 1, indicating thesemantic similarity sim(d\u1d62, c\u2c7c) between the given CVE description d\u1d62and the commit c\u2c7c. In this way, commits that are more semanticallysimilar to the CVE description will have a higher F1 score."}, {"title": "3.2.3 Hybrid Retriever", "content": "To take both lexical and semantic infor-mation into account, we utilize a hybrid approach following [18]to combine TF-IDF and CodeReviewer. The fusion of lexical andsemantic similarities leverages their complementary analysis per-spectives-lexical for word-based similarity and semantic for con-ceptual alignment (e.g., synonyms). Additionally, they share thesame value space ([0,1]), facilitating straightforward additive fusion.\nThe parameter \u03bb adjusts the emphasis on these features, allowingfor a unified similarity metric. The similarity score is computed asfp(d\u1d62, c\u2c7c) = sim(d\u1d62, c\u2c7c) + \u03bb \u00b7 cosine(d\u1d62, c\u2c7c), where \u03bb is a weight tobalance the two retrievers. After conducting a parameter tuningprocess including a grid search over various values (from 0.1 to10 with a step of 0.05), we found that \u03bb = 1 in our experimentdelivers optimal effectiveness among them. Nevertheless, we retainthe parameter \u03bb to facilitate adaptation to different datasets."}, {"title": "3.3 Re-ranking via Fine-tuning CodeReviewer", "content": "As mentioned above, we have refined the list of commits to thetop-k most relevant candidates for each CVE. For this phase, we optfor CodeReviewer [22], a state-of-the-art large language model pre-trained on the foundation of CodeT5 [44]. There are two considerations for this choice. 1) Encoder Specialization: CodeReviewer'sencoder is designed to deeply understand commit behaviors and is-sues, a feature not necessarily present or optimized in other models.This encoder specialization ensures that the model comprehendsthe intricate relationships between code changes and potential se-curity implications, vital for matching commits to CVE descriptions.2) Downstream Task Optimization: Although our focus is noton generating code reviews, the fact that CodeReviewer's decoderis optimized for such downstream tasks indicates its ability to linkcode changes to descriptive text, a parallel to our objective of linkingcommits to CVE descriptions. Given these advantages, we fine-tuneCodeReviewer on the top-k candidate commits, aiming to re-rankthem based on their relevance to the respective CVE descriptions.\nWe only use the pre-trained encoder of CodeReviewer (abbr. CR)since our task can be basically viewed as a binary classification prob-lem in the re-ranking phrase. Specifically, given a CVE descriptiond\u1d62, and the top-k commits represented as Ck = {(msg\u2c7c, diff\u2c7c)}k\u2c7c=\u2081,we format each commit as {[CLS], diff\u2c7c, [MSG], msg\u2c7c}. Then theCVE description d and each commit c\u2c7c \u2208 Ck are encoded separatelyusing the CR encoder to yield two sequences of vectors:\nE(d\u1d62) = CRencoder(d\u1d62) (6)\nE(c\u2c7c) = CRencoder(msg\u2c7c; diff\u2c7c) (7)\nWe obtain the vector representations of d\u1d62 and c\u2c7c by extractingthe hidden state in the last layer of the special token [CLS] at thebeginning of E(d\u1d62) and E(c\u2c7c) respectively. The encoded vectors ofthe CVE description and the commit are concatenated:\nV\u2c7c = [E(d\u1d62); E(c\u2c7c)] (8)\nWe apply a linear classifier to the concatenated vector V\u2c7c for estimating the correlations:\ny\u2c7c = \u03c3(W \u00b7 V\u2c7c + b) (9)\nwhere W is the weight matrix, b is the bias term, and \u03c3 denotes thesigmoid function ensuring the output lies between 0 and 1.\nWe utilize labeled data containing known CVE-commit pairsduring the fine-tuning phase. The training goal is to minimize thebinary cross-entropy loss:\nL = -1/k \u03a3k\u2c7c=\u2081 [ytrue,\u2c7c log(yj) + (1 - ytrue,\u2c7c) log(1 - yj)] (10)\nwhere ytrue,\u2c7c is the ground truth label of the jth sample, indicatingwhether commit c\u2c7c is related to the CVE description d\u1d62.\nAfter fine-tuning, for any new CVE and set of commits, the modelcan compute the relevance scores. Commits can then be re-rankedbased on the scores concerning the given CVE. This method ensuresthat the CR encoder understands both generic textual semantics andthe specific indicators that tie a commit to the given CVE, makingthe approach specially tailored for our challenge."}, {"title": "4 EVALUATION", "content": "We aim to answer the following research questions:\n\u2022 RQ1: Effectiveness Analysis. How effective is PatchFinderwhen compared with existing baselines in tracing security patches?\n\u2022 RQ2: Ablation Analysis. What impact does each componentof PatchFinder have on the overall performance?\n\u2022 RQ3: Distribution Analysis. Does PatchFinder exhibit notablyhigh or low accuracy for certain vulnerability types or severity?\n\u2022 RQ4: Practicality Analysis. How effective is PatchFinder inreal-world applications, particularly when detecting securitypatches for CVEs without associated trace links?"}, {"title": "4.1 Research Questions", "content": "We aim to answer the following research questions:\n\u2022 RQ1: Effectiveness Analysis. How effective is PatchFinderwhen compared with existing baselines in tracing security patches?\n\u2022 RQ2: Ablation Analysis. What impact does each componentof PatchFinder have on the overall performance?\n\u2022 RQ3: Distribution Analysis. Does PatchFinder exhibit notablyhigh or low accuracy for certain vulnerability types or severity?\n\u2022 RQ4: Practicality Analysis. How effective is PatchFinder inreal-world applications, particularly when detecting securitypatches for CVEs without associated trace links?"}, {"title": "4.2 Dataset", "content": "For the training and evaluation of our model, we compiled a datasetthat encompasses both OSS vulnerabilities and their corresponding security patches. The dataset was assembled in two primarysteps: 1) Initial Data Collection: We began by collecting datafrom Wang et al. [42] and Tan et al. [38]. We thereby obtained 1,669unique CVEs from 10 OSS projects. 2) Dataset Supplement: Toaugment our dataset, we crawled vulnerabilities and their asso-ciated patches from multiple sources, including the official CVE,NVD, and Snyk [37] vulnerability databases. This initial collectionyielded 3,585 unique CVEs from 532 OSS projects. After eliminat-ing duplicate entries, our finalized dataset comprises 4,789 uniqueCVEs and 4,870 distinct security patch commits, which involve532 unique OSS projects, covering various programming languagesincluding C/C++, Java, JavaScript, and PHP. This makes our datasetthe most extensive collection of CVEs and security patches avail-able to date. Each dataset entry includes the CVE-ID along withits textual description and the corresponding security patch links.We also extracted related commits including commit messages andcode diffs, primarily from GitHub [11] and GitLab [12].\nTo create a robust training set, we followed the practices in priorworks [38, 42] to sample 5,000 non-patch commits as negative sam-ples for each CVE. However, in scenarios where a repository con-tained fewer than 5,000 commits in total, we included all availablenon-patch commits as negative samples. Finally, we got 21,781,044commits in total. To the best of our knowledge, this is the largestdataset specific for the security patch tracing problem."}, {"title": "4.3 Experiment Setup", "content": "We randomly split the 4,789 unique CVEs along with their corresponding commits in the proportion of 8:1:1 to keep the samesplit settings as baselines [38, 42]. The maximum token length forCodeReviewer is set at 512, which represents its upper limit for processing capacity. Given the preprocessing of code diffs as detailedin Section 3, this token length is actually sufficient for our purposes.To preprocess data, we use the NLTK toolkit and the BPE tokenizerof CodeReviewer to tokenize CVE descriptions and commits. For theinitial retrieval phase, we retrieve the top 100 commits from theinitial 5,000 commits for each CVE. This threshold k enables us toobtain a good balance between the coverage of true patches andnoisy commits. During fine-tuning, the batch size is set to 32 andthe maximum epoch is 20. We adopt the widely-used Adam [21] asthe optimizer with a learning rate of 5e-5 for training our model.All the above hyper-parameters are determined based on the validation set by selecting the best ones among some alternatives. All experiments ran on a server with 48 CPU cores (Intel\u00ae Xeon\u00aeSilver 4214 CPU @ 2.20GHz), 252 GB RAM, and 4 NVIDIA RTX3090 GPUs (24 GB memory each)."}, {"title": "4.4 Baselines", "content": "We benchmark our approach against state-of-the-art works in security patch tracing as presented in [38, 42]. Due to the absence ofavailable replication packages of PatchScout [38], we implementedit independently by adhering to the default settings unless specified otherwise. Additionally, we consider two renowned baselinesfrequently used in the information retrieval domain for our evaluation: BM25 [35], a classic method for sparse retrieval [31], andColBERT [19], known for dense retrieval [18]."}, {"title": "4.5 Evaluation Metrics", "content": "To ensure a fair comparison between PatchFinder and the baselines, we utilize three metrics: Recall@K, Mean Reciprocal Rank(MRR) [14], and Manual Efforts@K. Recall@K and Manual Ef-forts@K have been previously employed in previous studies [38, 42].We also incorporate MRR into our evaluation, given its establishedsignificance in ranking systems."}, {"title": "4.5.1 Recall@K", "content": "Recall@K refers to the ratio of the number ofsecurity patches traced in the top-K results to the number of allsecurity patches for a given K. Hence, a higher Recall@K scoremeans better performance."}, {"title": "4.5.2 Mean Reciprocal Rank (MRR)", "content": "MRR is a widely used evaluation metric for ranking systems, particularly in the domain ofinformation retrieval. It emphasizes the importance of the positionof the first relevant result in a list of retrieved documents, makingit especially relevant for security patch tracing where we typicallyseek a single commit. MRR is defined mathematically as follows:\nMRR = 1/|D| \u03a3|D|i=1 1/rank\u1d62 (11)\nIn this equation, |D| represents the total number of CVEs, andrank\u1d62 denotes the position of the first security patch for the i-thCVE. The MRR values range between 0 and 1, with higher valuesindicating better retrieval performance. By considering the inverse"}, {"title": "4.5.3 Manual Efforts@K", "content": "In the pursuit of tracing security patcheswithin OSS, the Manual Efforts@K metric emerges as a classicmetric. It represents the manual inspection effort required to locatethe correct patch within the top-K results. If the desired securitypatch is found within these results, the effort corresponds to itsrank. However, if the patch is not within the top-K, the effort is K,indicating a comprehensive search without success. Drawing fromrelated work [38], the metric is mathematically expressed as:\nManual Efforts@K = \u03a3\u207f\u1d62=\u2081 min(rank\u1d62, K)/n (12)\nA lower Manual Effort@K score is indicative of a more efficientand effective method for tracing security patch commits. This aidsNVD security experts in mitigating the extensive manual workassociated with tracing security patches, reducing inspection time,and enhancing patch detection accuracy."}, {"title": "5 RESULTS AND DISCUSSION", "content": "We investigate the following research questions to provide a thorough analysis of the experimental results."}, {"title": "5.1 RQ1: Effectiveness Analysis", "content": "Table 1 and Table 2 show the effectiveness of different approachesin terms of Recall@K, MRR, and Manual Efforts@K, with the bestone of each metric marked in bold. Table 1 reveals that PatchFindersignificantly outperforms all the SOTA approaches across differentvalues of K, for the Recall@K metric. The superiority of PatchFinderis most prominent at K = 1 where it achieves a Recall of 79.23%,markedly higher than PatchScout's 46.25%, VCMatch's 55.93%, BM25's 11.88%, and ColBERT's 26.29%. This trend continues as K increases, showcasing the consistent effectiveness of PatchFinder inlocating security patch commits within the top-K results. Notably,the Recall@K for PatchFinder remains above 79% for all values of K,highlighting its robustness in tracing relevant security patches. TheMRR further confirms the effectiveness of PatchFinder with a scoreof 0.7951, significantly outpacing the 0.6195 and 0.3824 attainedby VCMatch and PatchScout, respectively. This superior performance can be attributed to our two-phase approach that combines"}, {"title": "5.2 RQ2: Ablation Analysis", "content": "In our proposed approach, we integrate two primary components:an initial retrieval using a hybrid retriever consisting of TF-IDFand pre-trained CodeReviewer (Phase-1), followed by a subsequentre-ranking using a fine-tuned CodeReviewer model (Phase-2). Theinput to our system encompasses the CVE description, commitmessages, and code diffs. To dissect their effectiveness, we conducted a detailed ablation study by examining the performanceof PatchFinder under various configurations: 1 Using only the"}, {"title": "5.3 RQ3: Distribution Analysis", "content": "This RQ focuses on a detailed investigation of the outcomes fromPatchFinder. The primary aim is to explore if there exists a correlation between the types of vulnerabilities and the true patchesidentified by PatchFinder. Specifically, we are interested in examining whether PatchFinder's retrieval accuracy varies across differentvulnerability categories and their respective severity levels.\nTo conduct this analysis, we categorized the vulnerabilities inour test dataset based on their Common Weakness Enumeration(CWE) identifiers [5] and Common Vulnerability Scoring System(CVSS) V2 scores [10]. Our analysis reveals that PatchFinder is par-ticularly effective in tracing security patches for specific types of"}, {"title": "5.4 RQ4: Practicality Analysis", "content": "In evaluating the practical utility of PatchFinder, we initially curated a dataset of 212, 074 CVE entries from NVD as of April 2023.However, we encountered a significant challenge in accuratelyidentifying CVEs without patches since the \"patch\" tags in theNVD are often imprecise [38]: some entries with patches lack thecorresponding tag, while others may be inaccurately tagged ashaving a patch but in fact, they do not.\u00b9 Hence, we opted to focuson a more reliable subset: CVEs affecting OSS and known to lackpatches. To this end, we leveraged the OSS project list maintained"}, {"title": "5.5 Case Study", "content": "Among the 533 patches we traced in RQ4, a particularly illustrative instance is CVE-2022-31814 from the \"pfSense-pfBlockerNG\u201dproject. As shown in Listing 2, this vulnerability allows remoteattackers to execute arbitrary OS commands via specific manipulations. The associated patch commit, with its seemingly innocuouscommit message \"Update index.php\" does not directly indicate itsrelevance to the CVE. This ambiguity poses challenges for toolslike PatchScout and VCMatch who failed to recognize it. Their reliance on direct textual correlations (such as \"# shared file names\",\"# shared function names\", and \"# shared words\", etc.) betweenCVE descriptions and commits can be limited, especially whendescriptions and commits employ synonyms or different phrasings.In contrast, PatchFinder outperforms in such situations sinceharnessing the strengths of TF-IDF and CodeReviewer. Unlike othertools, it effectively deals with commits having limited information.Specifically, the patch commit in Listing 2 initially ranked 47thin our lexical-based retriever due to its brief message. However,the semantic-based retriever recognized its relevance, where theaddition of escapeshellarg at Lines 23-24 crucially sanitizes shellmetacharacters in the HTTP Host header, directly addressing thevulnerability. Such intricate changes, often missed by other tools,are accurately identified by PatchFinder due to its outstandingsemantic analysis of code and text. As a result, our hybrid retrieverimproved its rank to 23rd. In the subsequent phase, the top-100results, including this commit, were analyzed further. Here, thefine-tuned CR, adept at understanding code semantics, elevated itsrank to 7th, placing it within the top-10 results."}, {"title": "5.6 Discussion on False Negatives", "content": "We further analyzed the missed patch commits of 95 CVEs andsummarized them into three main causes.\n\u2022 Low-Quality CVE Descriptions (28/95): Some CVE descrip-tions lack sufficient detail for effective patch tracing. Notably,"}, {"title": "6 THREATS TO VALIDITY", "content": "External Threats. A primary external threat pertains to the reproducibility of the baselines. While we endeavored to faithfullyimplement PatchScout based on its published methodology, theabsence of its source code posed challenges. To ensure a robustcomparison with state-of-the-art methods, we also incorporatedBM25 and ColBERT, which are notable sparse and dense retrievalmodels, respectively, as additional baselines.\nInternal Threats. Our dataset's quality and scope could introduceinternal threats. To minimize it, we initially built our dataset upondatasets from prior studies [38, 42], and followed their practice tosource CVEs and security patches from various public advisories.Despite our efforts to curate a broad and diverse dataset, biasesfrom these sources might persist. In the future, we will consider"}, {"title": "7 RELATED WORK", "content": "There are numerous works focusing on tracing security patches,which can be divided into two categories: tracing security patchesfor disclosed vulnerabilities [26, 27, 38, 42, 47] and identifying silentscurity patches [1, 43, 46, 52\u201355].\nTracing security patches for disclosed vulnerabilities. Xu etal. [47] conducted an empirical study to understand the quality andcharacteristics of patches for disclosed vulnerabilities in two indus-trial vulnerability databases, thereby proposing to track patchesfrom the CVE reference links across multiple knowledge sources(e.g., Debian). Their work focuses on analyzing reference linksprovided by security analysts, instead of directly tracing patchesfrom OSS repositories. Tan et al. [38] conducted the most relatedwork with PatchFinder. They designed a ranking-based tool namedPatchScout to locate the patch commits by using RankNet on manually defined features from the CVE description and commits. Simi-larly, VCMatch [42], which directly classifies one commit as relatedor unrelated to the CVE description by fusing the features fromPatchScout and extracted vectors from Bert. Unlike PatchScout andVCMatch, PatchFinder introduces a novel two-phase frameworkdesigned to overcome the challenges posed by large search spaces,and enables an end-to-end fine-tuning, to fully exploit the naturalcorrelation between CVE descriptions and commits.\nIdentifying silent security patches. Besides, several works [1,43, 46, 52\u201355] have delved into silent security patch identification.These efforts discern security patches but do not correlate themwith specific vulnerabilities they rectify. In contrast, our focus ison tracing security patches tailored to a particular vulnerability, asdefined by its CVE description."}, {"title": "8 CONCLUSION", "content": "In this paper, we present PatchFinder, an end-to-end and LLM-enhanced two-phase approach for effectively tracing security patchesfor disclosed vulnerabilities in OSS. The first phase employs a hybridretriever for the initial retrieval of relevant commits, significantlynarrowing down the candidate pool. The second phase leverages afine-tuned CodeReviewer model to re-rank these commits, achiev-ing a high degree of accuracy. Our extensive evaluations demonstrate that PatchFinder consistently outperforms state-of-the-artmethods in Recall@K, MRR, and manual efforts, setting PatchFinderas a new benchmark in the field of security patch tracing."}, {"title": "ACKNOWLEDGMENTS", "content": "This work is supported by the National Key R&D Program of China under grant 2021ZD0114501, the RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contributions from the industry partner(s), the National Research Foundation, Singapore, and the Cyber Security Agency under its National Cybersecurity R&D Programme (NCRP25-P04-TAICEN). Any opinions, findings and conclusions, or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore and Cyber Security Agency of Singapore."}]}