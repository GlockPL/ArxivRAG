{"title": "Multi-Agent Deep Reinforcement Learning for Distributed and Autonomous Platoon Coordination via Speed-regulation over Large-scale Transportation Networks", "authors": ["Dixiao Wei", "Peng Yi", "Jinlong Lei", "Xingyi Zhu"], "abstract": "Truck platooning technology enables a group of trucks to travel closely together, with which the platoon can save fuel, improve traffic flow efficiency, and improve safety. In this paper, we consider the platoon coordination problem in a large-scale transportation network, to promote cooperation among trucks and optimize the overall efficiency. Involving the regulation of both speed and departure times at hubs, we formulate the coordination problem as a complicated dynamic stochastic integer programming under network and information constraints. To get an autonomous, distributed, and robust platoon coordination policy, we formulate the problem into a model of the Decentralized-Partial Observable Markov Decision Process. Then, we propose a Multi-Agent Deep Reinforcement Learning framework named Trcuk Attention-QMIX (TA-QMIX) to train an efficient online decision policy. TA-QMIX utilizes the attention mechanism to enhance the representation of truck fuel gains and delay times, and provides explicit truck cooperation information during the training process, promoting trucks' willingness to cooperate. The training framework adopts centralized training and distributed execution, thus training a policy for trucks to make decisions online using only nearby information. Hence, the policy can be autonomously executed on a large-scale network. Finally, we perform comparison experiments and ablation experiments in the transportation network of the Yangtze River Delta region in China to verify the effectiveness of the proposed framework. In a repeated comparative experiment with 5,000 trucks, our method average saves 19.17% of fuel with an average delay of only 9.57 minutes per truck and a decision time of 0.001 seconds.", "sections": [{"title": "I. INTRODUCTION", "content": "Platooning technology enables intelligent vehicles to travel closely together in a column in transportation networks while maintaining a fixed distance between each other [1]. This approach enhances transportation system efficiency and capacity, particularly beneficial in heavy-duty industries like truck transport [2]. Notably, platooning can contribute to fuel efficiency and emission reduction. For example, [3] demonstrated a 13% fuel consumption saving when three trucks formed a platoon with a 10-meter gap. Furthermore, if 40% of trucks in the transportation network adopt platooning, it can result in a 2.1% reduction in total carbon dioxide emissions. Platooning technology has been developing rapidly [4]-[6]. Particularly, platoon control technology has been extensively studied. Examples include the platoon safety issues in emergencies [7], platooning around curves [8], and the control of vehicle merging in platoons [9]. Concurrently, vehicle-to-vehicle communication technology [10] and highway information architecture [11] have also garnered widespread attention. With the ongoing development of platoon control [12] and communication technologies [13], there is a promising prospect of forming a large-scale truck platoon traffic network in the future.\nPlatooning coordination studies how a group of vehicles forms a platoon, to maximize the benefits of platooning within various task and traffic network constraints, and it remains an active and challenging research topic [14]. [15] focused on planning the departure time at hubs to wait for other trucks to form a platoon, and formulated the coordinated as an integer optimization problem to save fuel and enhance traffic efficiency simultaneously. In [16], the authors explored platoon coordination for trucks sharing a common starting point but diverging at different endpoints from the departure hub. They formulated the problem as a non-cooperative game, wherein each truck seeks to maximize individual benefits within the platoon. Considering the computational load of large-scale transportation networks, [17] introduced distributed model predictive control to regulate truck waiting times at hubs. Additionally, the authors presented a distributed approximate dynamic programming scheme to address the coordination by waiting at hubs, ensuring compliance with hours-of-service regulations [18]. Moreover, [19]-[21] studied the problem of coordination for multi-fleet platoons. [22] considered coordination under uncertain travel times but requires knowledge of the distribution of travel times in advance. All these works assumed that the travel time between two hubs is fixed and the coordination is fulfilled by planning the waiting time at hubs. This is not practical for a practical traffic network.\nOn the other hand, path selection [23] and speed regulation [24] can also be used for platoon coordination. [25] introduced a heuristic algorithm for pre-planning the path scheduling problem with platooning coordination level as an optimization objective. [26] explored a pairing algorithm for regulating speeds to form platoons. [27], [28] discussed the combination"}, {"title": "II. PROBLEM FORMULATION", "content": "This section introduces platoon formation and the total platoon coordination optimization problem.\n**A. Transportation Network and Platoon Formation**\nWe consider the platoon coordination within a transportation network (V, E). Here, V represents the set of hubs where trucks can wait to form platoons with other trucks, while E represents the set of edges that connect the hubs in V. We consider a discrete-time setting and define $t \\in \\{1,2,..., T_e\\}$ with a time step $\\Delta t$, where $T_e$ represents a maximal length allowing the last truck to reach its ending point. The set of trucks in the transportation network is represented by $N = \\{1,2,...,N\\}$.\nEach truck $i \\in N$ has a starting point, an endpoint, and a pre-scheduled departure time $d_i \\in \\{1,2,...,T_e\\}$ from the starting point. It has a fixed path $E_i = \\{(i_1,i_2), (i_2, i_3), ..., (i_{n_i-1}, i_{n_i})\\} \\subseteq E$ from the starting point $i_1$ to the endpoint $i_{n_i}$ by passing through hubs $V_i = \\{i_1,i_2,...,i_{n_i}\\}\\subseteq V$. We denote $i_k$ as the k-th hub on the journey of truck i for $k \\in \\{1,2,...,n_i\\}$. Meanwhile,"}, {"title": "B. Platoon Coordination Problem", "content": "We form the platoon coordination problem by defining its decision variable, feasible set, and objective functions. The trucks can coordinate by changing their speed to form a platoon. If a truck i or a truck group is at the hub, it can decide to wait by setting its speed to 0 for forming a platoon with incoming trucks. Meanwhile, when the trucks are on the edge, they can decide to accelerate or moderate to rendezvous to form a platoon. But notice that the feasible action of changing speed is also constrained by its state as\n$v \\in A_i(s) =\\begin{cases}\\ \\{u_l, u_m, u_h, 0\\}, & \\text{if } s_i^t,1 = i_k, \\\\ \\{u_l, u_m, u_h \\}, & \\text{if } s_i^t,1 = (i_k, i_{k+1}). \\end{cases}$\n$A_i(s)$    (2)\nThe objective function is to maximize the saving of fuel consumption in the entire transportation network meanwhile to minimize the time delay for the completion of transportation tasks. Next follows how to evaluate the time delay and fuel consumption under a given decision.\nTime delay: As shown in Fig. 1, for truck $i \\in N$ is at hub $i_{k+1}$, its departure time from hub $i_{k+1}$ is defined as\n$t_d(i_{k+1}) = t_a(i_{k+1}) + \\tau_i(i_{k+1}),$   (3)\nwhere $t_a(i_{k+1})$ denotes the arrival time of truck i at hub $i_{k+1}$ and $\\tau_i(i_{k+1}) \\in \\mathbb{N}$ is the waiting time of truck i at hub $i_{k+1}$.$\\tau_i(i_{k+1}) = |\\{v : v = 0, s_i^t,1 = i_{k+1}, t \\in \\{1,2,..., T_e\\}\\}|$. In particular, $t_d(i_1) = d_i + \\tau_i(i_1)$. The arrival time of truck i at hub $i_{k+1}$ is given by\n$t_a(i_{k+1}) = t_d(i_{k}) + T_i(i_k, i_{k+1}),$    (4)\nwhere $T_i(i_k, i_{k+1})$ represents the travel time of truck i on edge $(i_k, i_{k+1})$. Let $L(i_k, i_{k+1})$ denotes the length of edge $(i_k, i_{k+1})$ and $v_i^t$ the speed of truck i at time t. We assume"}, {"title": "III. DEC-POMDP FORMULATION FOR PLATOON COORDINATION", "content": "In this section, we reformulated this problem as a Dec-POMDP for training a distributed and adaptive policy with MADRL. The Dec-POMDP problem is composed of a tuple $G = (N, S, A, T, R, Z, O)$ [35]. In the following, we give G at every time t in detail.\nState Space: State space $S_t = \\{s_1^t,s_2^t,...,s_\\sqrt{N}^t\\}$, where each $s_i^t$, $i \\in N$ is defined in II-A.\nAction Space: At time t, truck i can take the action $a_i^t$ of changing its speed $v \\in A_i(s_i^t)$ with the constraint Eq. (2). Due to the noise $\\xi$, it is often difficult for trucks to be in the same position. After trucks rendezvous in the same position, we assume that these trucks have achieved the intention to form a platoon, that is trucks in the same position can only travel at a speed of $v_m$ and form a platoon. Let $X_i(S_t) = \\{j | j \\in N, j \\neq i, s_i^t = s_j^t\\}$ represents a set of trucks that are in the same platoon with truck i. Thus, the action $a_i^t$ is constrained by\n$a_i^t \\in A_i(s_i^t) =\\begin{cases}\\ \\{u_l, u_m, u_h, 0\\}, & \\text{if } s_i^t,1 = i_k, \\\\ \\{u_l, u_m, u_h \\}, & \\text{if } s_i^t,1 = (i_k, i_{k+1}), X_i(S_t) = \\emptyset, \\\\ \\{v_m\\}, & \\text{if } s_i^t,1 = (i_k, i_{k+1}), X_i(S_t) \\neq \\emptyset. \\end{cases}$   (15)\nThe joint action space of all trucks is defined as $A_t = v_t = \\{a_1^t, a_2^t,..., a_\\sqrt{N}^t\\}$, $A = v = \\{A_1, A_2, ..., A_T\\}$.\nState Transition Function: $T(S_t, A_t,\\xi_t) : S_t \\times A_t \\rightarrow S_{t+1}$ represents the function that $S_t$ transfers to $S_{t+1}$ after performing the joint action $A_t$. The state transition function is defined in Eq. (1).\nObservation Space and Observation Function: The observation of a truck depends on the information it receives from other trucks. The trucks are assumed to have limited communication scope within large-scale transportation networks. At time t, each truck $i \\in N$ can communicate with its neighboring trucks through a hub $h_i(s_i^t)$ defined as (16).\n$h_i(s_i^t) =\\begin{cases}\\ i_k, & \\text{if } s_i^t,1 = i_k, \\\\ i_{k+1}, & \\text{if } s_i^t,1 = (i_k, i_{k+1}), \\end{cases}$   (16)\nThis implies that each truck communicates with the trucks having the same next hubs. For example, as shown in Fig. 3, the green truck j located at hub $j_i$ will communicate through hub $i_k$, while the blue truck i driving on edge $(i_k, i_{k+1})$ will communicate through hub $i_{k+1}$.\nThe communicated information of truck i through hub $h_i(s_i^t)$ is denoted by $m_i(s_i^t) = (m_{i,1}(s_i^t), m_{i,2}(s_i^t))$, where"}, {"title": "A. Representation for State and Observation", "content": "Neural networks usually require fixed-length of inputs. Since the dimensions of $S_t = \\{s_1^t, s_2^t,...,s_\\sqrt{N}^t\\}$ and $Z_t = \\{z_1^t, z_2^t,...,z_\\sqrt{N}^t\\}$ are determined by the number of trucks, they can not be input directly to the neural network. Considering the variability of the transportation network scale, we need to design a representation of the state and observation into a fixed-size input.\nThe representation of the state is denoted as $S_t = (S_{edge}^t, S_{co}^t)$. At time t, $S_{edge}^t$ represents the location information of trucks on the edge of the transportation network. $S_{co}^t$ represents the information about the successful cooperation of the truck.\nFirst, for each edge $e_m \\in E$, we discretize the length $L(e_m)$ into $\\alpha$ segments as follows.\n$L(e_m) = (L(e_m[1]), L(e_m[2]), ..., L(e_m[\\alpha])),$   (26)\nwhere\n$L(e_m[j]) =\\frac{(j-1)}{\\alpha}L(e_m), \\frac{(j+1)-1}{\\alpha}L(e_m)$   $j \\in \\{1, 2, ..., \\alpha\\}.$\nWe let the state of edge $e_m$ be the number of trucks driving on each segment of the edge at time t, i.e.,\n$S_m^t = (|M(e_m[1])|, |M(e_m[2])|, ..., |M(e_m[\\alpha])|)$.   (27)\nHere, $M(e_m[j])$, $j \\in \\{1,2,...,\\alpha\\}$ denotes the set of trucks driving on the edge $e_m$, for which the remaining length belongs to the interval $L(e_m[j])$, i.e., $M(e_m[j]) = \\{i : i \\in N, s_i^t,1 = e_m, s_i^t,2 \\in L(e_m[j])\\}$. Hence the state of all edges is denoted as $S_{edge}^t = (S_1^t, S_2^t, ..., S_\\sqrt{E}^t)$, which has a fixed size of $\\alpha \\sqrt{E}$.\nSecond, $S_{co}^t = (S^t[1], S^t[2],...,S^t[\\sqrt{N}])$ is a binary vector of length $\\sqrt{N}$, where each element is either 0 or 1. N is the maximum number of trucks carried in the transportation network. At time t, if truck i forms a platoon, $S^t[i] = 1$. Clearly, for $i \\in \\{1, 2, . . ., |\\sqrt{N}|\\}$, $S^t[i]$ can be defined as\n$S^t[i] =\\begin{cases}1, & \\text{if } |\\Omega_i(S_t, A_t)| \\neq \\emptyset, \\\\ 0, & \\text{otherwise}. \\end{cases}$   (28)\nTherefore, $S_t$ has a fixed size of $\\alpha \\sqrt{E} + |\\sqrt{N}|$.\nThe observation $Z_i^t = O_i(s_i^t)$ changes with the number of potential partners that share information through hub $h_i(s_i^t)$, see (20). We define the representation of the observation as\n$Z_i^t = O_i(s_i^t) = (Z_{i, self}^t, Z_{i, delay}^t, Z_{i, others}^t)$.   (29)\nHere, $Z_{i, self}^t = (s_i^t, s_i^t,2)$ represents truck i's own location information. $s_i^t,1$ divides the truck state $s_i^t$ into four categories: at a hub, traveling alone on an edge, traveling in a platoon, and inactive. Eq. (30) shows the details.\ns't =\\begin{cases}0, & \\text{if } s_i^t,1 = i_k, \\\\ 1, & \\text{if } s_i^t,1 = (i_k, i_{k+1}), |X_i(S_t)| = \\emptyset, \\\\ 2, & \\text{if } s_i^t,1 = (i_k, i_{k+1}), |X_i(S_t)| \\neq \\emptyset, \\\\ 3, & \\text{otherwise}. \\end{cases}$   (30)"}, {"title": "IV. TA-QMIX ALGORITHM FOR TRAINING PLATOON COORDINATION POLICY", "content": "This section elaborates on the operational process of training a platoon coordination policy by multi-agent reinforcement learning algorithms."}, {"title": "B. Deep Policy Network Architeture", "content": "QMIX [36] is a widely recognized deep reinforcement learning algorithm designed to address collaboration tasks in multi-agent scenarios. The structure of QMIX cannot well represent the information exchanged between trucks, making it difficult to weigh the fuel consumption and time costs caused by acceleration and deceleration. Hence, directly training the model with QMIX is hard to converge. Therefore, we propose TA-QMIX combined with the TCA Block and the TSA Block to solve the training efficiency of platoon coordination policy as shown in Fig. 4.\nTA-QMIX comprises $N = |N|$ individual TCA Agent Networks denoted as $\\{Q_i\\}_1^\\sqrt{N}$ with parameters $\\{\\omega_i\\}_1^\\sqrt{N}$, and a TSA-Mixing Network denoted as f with parameters $\\omega_f$. Similar to the approach in QMIX, TA-QMIX adopts the target network method for training, incorporating target agent networks $\\{Q'_i\\}_1^\\sqrt{N}$ with parameters $\\{\\omega'_i\\}_1^\\sqrt{N}$, and a target mixing network denoted as $f'$ with parameters $\\omega'_f$.\n1) Truck Cross Attention Agent Network: TCA Block uses the attention method [37] to explicitly model the connection between the trucks' information and the observed nearby truck information, calculate the weight of different truck information, and promote cooperation between trucks. The attention method operates as follows: (i) create three learnable transformation matrices $W_q$, $W_k$, $W_v$ embedded within the network; (ii) perform matrix multiplication with the input $S_{in}$ to obtain Query, Key, and Value:\n$Query = S_{in} \\cdot W_q,$\t\t(33)\n$Key = S_{in} \\cdot W_k,$\t\t(34)\n$Value = S_{in} \\cdot W_v.$\t\t(35)"}, {"title": "V. EXPERIMENTS", "content": "In this section, we conduct experiments on the transportation network in the Yangtze River Delta region of China to illustrate the efficiency of the proposed method.\n**A. Experiment Setting**\nOur experiments are implemented with an Intel(R) Xeon(R) Gold 6240R CPU @ 2.40GHz, 4 NVIDIA 3090 GPUs, and Pytorch. Illustrated in Fig. 7, we built a transportation network simulation environment in the Yangtze River Delta to simulate large-scale truck freight scenarios. The simulation environment"}, {"title": "B. Baselines and Metrics", "content": "The proposed method is compared with the following baseline method, including the no-coordination and Nash equilibrium-based coordination method.\n*   No Coordination (NC): NC refers to trucks that are not guided by any coordinated strategy. The truck travels at a medium speed $u_m$ throughout the journey, without any waiting at the hub, relying solely on the same departure time to form a platoon.\n*   Nash equilibrium (NE)-based coordination: NE-based method [22] considers each truck as a non-cooperative agent and uses potential games to calculate the Nash equilibrium solution for all trucks in the large-scale transportation network. This method essentially uses the potential game to maximize each truck's benefit at the decision-making moment of the transportation network. Using settings similar to [22], trucks travel at a fixed"}, {"title": "VI. CONCLUSION", "content": "In this paper, we address the joint decision-making process for truck speed and departure time at hubs within the context of platoon coordination in large-scale transportation networks. This problem is a complex integer programming challenge within a dynamic and stochastic transportation environment. We propose a distributed communication framework that equips trucks with local information for informed decision-making. The problem is reformulated as a Dec-POMDP, with well-defined state transitions and dense rewards to facilitate training. Furthermore, we apply the TA-QMIX deep reinforcement learning framework to optimize speed and time planning for platoon coordination in extensive transportation networks. TA-QMIX leverages an attention mechanism to enhance the representation of environmental states and truck-specific information, thereby improving cooperative behavior among trucks in the platoon coordination problem. To validate our approach, we developed a transportation network simulator for the Yangtze River Delta region in China. Simulation experiments demonstrate the effectiveness and generalization of our proposed method. Our future work will focus on exploring dynamic path planning for trucks during transit, aiming to enrich the decision-making space and improve collaboration potential among trucks."}]}