{"title": "HA-FGOVD: Highlighting Fine-grained Attributes via Explicit Linear Composition for Open-Vocabulary Object Detection", "authors": ["Yuqi Ma", "Mengyin Liu", "Chao Zhu", "Xu-Cheng Yin"], "abstract": "Open-vocabulary object detection (OVD) models are considered to be Large Multi-modal Models (LMM), due to their extensive training data and a large number of parameters. Mainstream OVD models prioritize object coarse-grained category rather than focus on their fine-grained attributes, e.g., colors or materials, thus failed to identify objects specified with certain attributes. However, OVD models are pretrained on large-scale image-text pairs with rich attribute words, whose latent feature space can represent the global text feature as a linear composition of fine-grained attribute tokens without highlighting them. Therefore, we propose in this paper a universal and explicit approach for frozen mainstream OVD models that boosts their attribute-level detection capabilities by highlighting fine-grained attributes in explicit linear space. Firstly, a LLM is leveraged to highlight attribute words within the input text as a zero-shot prompted task. Secondly, by strategically adjusting the token masks, the text encoders of OVD models extract both global text and attribute-specific features, which are then explicitly composited as two vectors in linear space to form the new attribute-highlighted feature for detection tasks, where corresponding scalars are hand-crafted or learned to reweight both two vectors. Notably, these scalars can be seamlessly transferred among different OVD models, which proves that such an explicit linear composition is universal. Empirical evaluation on the FG-OVD dataset demonstrates that our proposed method uniformly improves fine-grained attribute-level OVD of various mainstream models and achieves new state-of-the-art performance.", "sections": [{"title": "I. INTRODUCTION", "content": "THE Open-Vocabulary Object Detection (OVD) [1] is based on [2], [3] or proposes an architecture of Large Multi-modal Models (LMM) [4], [5], characterized by its extensive training data and a large number of parameters. In contrast to classical object detectors limited to closed-set categories, OVD models detects objects further beyond the category scope of their training data [6]. Although there has been a notable emergence of researches on OVD in recent years [7]\u2013[9], major OVD models are both learned and evaluated on mainstream datasets with the typical labels as only a few category names. Therefore, they struggle in following natural user instruction with a more fine-grained attribute vocabulary than category names, which requires a stronger understanding of the fine-grained semantics of the language by aligning them with visual features.\nTo this end, recent studies have introduced a novel task, i.e., fine-grained open-vocabulary object detection (FG-OVD) [10]. As illustrated in Fig. 1, different from traditional closed-set fine-grained object detection (FG-OD) that focuses on inter-class variations (e.g., Labrador Retriever vs Golden Retriever)."}, {"title": "II. RELATED WORKS", "content": ""}, {"title": "A. Open Vocabulary Object Detection", "content": "The task of OVD is designed to leverage rich vision-language semantics to detect objects of unseen categories during training. Hence, large vision-language models are pre-trained on large-scale image-text datasets with a rich vocabulary base. For instance, CLIP [12], which was trained on 400 million (image, text) pairs, has demonstrated significant potential in learning representations that are transferable to OVD models [15] in various ways, including knowledge distillation [16], [17], serving as an encoder for visual or textual data [3], [18], and the generation of informative pseudo-labels [9], [19], [20]. Due to limited regional text alignment capability of CLIP, some OVD models are directly trained on large-scale datasets for various tasks, such as object detection [21]-[23], image captioning [24], [25], and phrase grounding [22], [26] etc., to enrich semantic understanding and improve the detection ability of OVD models. However, despite the inclusion of a rich attribute vocabulary in training datasets, recent studies [10] have shown that mainstream OVD models perform poorly in detecting objects with fine-grained attributes. Under the mainstream evaluation protocol for only category names, OVD models suppress the fine-grained attributes that are explicitly composited in global text features [27].\nConsequently, the development of specialized datasets and methods for fine-grained open vocabulary detection is crucial for advancing the precision and granularity of OVD models."}, {"title": "B. Fine-Grained Open Vocabulary Object Detection", "content": "Traditional fine-grained object detection focuses on inter-class differences [28]\u2013[31], such as distinguishing between Samoyed and Labrador dogs, and lack the capability to detect objects outside their training categories. In contrast, FG-OVD is more concerned with intra-class extrinsic attributes, such as \"brown dog\" or \"black dog\", offering greater generality. However, research on FG-OVD models is currently scarce. Most works focus on fine-grained categorization, e.g., DetCLIPv3 [32] generates and trains on hierarchical category labels to enhance category-level fine-grained capabilities, while real-world objects are more complex with attribute-level granularity.\nDifferently, this paper is dedicated to exploring attribute-level fine-grained open vocabulary detection methods. The study reveals that existing OVD models all incorporate text encoders based on the Transformer architecture [33], mainly based on CLIP text encoder architecture [2], [3], [8], [9], [16], [18], [34], [35], and BERT architecture [4], [5], [7]. Our proposed method can enhance text attribute features through linear combination to activate attribute features that exist but are suppressed by mainstream OVD models."}, {"title": "C. Large Language Models as Auxiliary Tools", "content": "LLMs [14], [36], due to their emergent comprehension and generation abilities to generate answers following user instructions, are increasingly being utilized to assist in various natural language processing tasks. For instance, OVD model DetCLIPv3 [32] employs an LLM to annotate images with rich hierarchical object labels. Similarly, the FG-OVD dataset [10] comprises both positive and negative captions generated by an LLM. In this paper, the capabilities of LLM are leveraged to extract attributes in input text, thereby aiding OVD models in focusing on and enhancing attribute features. This approach is designed to improve the sensitivity of OVD models to fine-grained attributes and their overall performance on fine-grained open-vocabulary object detection."}, {"title": "III. PROPOSED METHOD", "content": "As illustrated in Fig. 2, we propose a universal approach to enhance the attribute-level OVD capabilities of mainstream models by highlighting fine-grained attributes in an explicit linear space. To amplify the fine-grained attribute features that exist but are suppressed in the frozen OVD models, our architecture consists of three key process: Attribute Words Extraction, Attribute Feature Extraction and Attribute Feature Enhancement."}, {"title": "A. Attribute Words Extraction", "content": "To assist the OVD models in focusing on attribute words, we employed the LLAMA2 [14] LLM to extract attribute words from the input text. As shown in Fig. 3, to ensure the precision and standardization of the output, we configure the LLM within the system message to establish a general dialogue background which sets the model as the role of extracting the attribute words, and provides the definition of the attribute word as well as the output format. In addition, several in-context examples are supplied to aid the model in understanding the output rules. The specific output format is defined to include only attribute words, separated by spaces, without any other vocabulary, in the form of [attributel attribute2 ... attributeN]. When no attribute word is detected in the statement, the output should be [None].\nGiven the prompt instruction P and input text T, the LLM predicts the set of N attribute words AT = {ai \u2208 T}:\n$A_{T} = LLM(P, T) = \\{a_{i} \\in T | 0 \\leq i \\leq N\\}$  (1)\nThen the position of each attribute word is obtained by matching function with the input text T:\n$\\Phi_{A,T} = Match(Tokenize(T), Tokenize(A_{T}))$\n$= \\{\\phi_{i} \\in [0, L] | 0 \\leq i \\leq N\\}$ (2)"}, {"title": "B. Attribute Feature Extraction", "content": "As mentioned in Section II, all models we have surveyed include a text encoder structure and are based on variations of the Transformer architecture, specifically the CLIP [12] or BERT [37] models. These models utilize token attention masks to guide the text encoder in extracting text information from specified positions when encoding input text. Considering that the semantic information of attribute words depends on different contexts, for example, the contextual semantic information of \"blue\" in \"a blue dog\" and \"a blue umbrella\" is different, and in order to ensure consistency of the positional information between the global text features and attribute features, attribute features are not extracted from individual attribute words.\nInstead, we have highlighted attribute semantic information by retaining the token attention masks of attribute and masking non-attribute tokens. Subsequently, we combine the global text features with attribute features through a linear weighting composition to form an attribute-highlighted text feature, which is then utilized for object detection.\nThe method is applied following the original models' token attention mask structures and making adaptive adjustments.\nBERT architecture [37] employs a vanilla self-attention mechanism which allows each element in the sequence to interact with all other elements within the sequence. Due to the model's special handling of attention masks for special tokens, such as the initial [CLS], separator [SEP], and padding [PAD] tokens (as illustrated in Fig. 4), this algorithm operates only on the token attention masks between the [CLS] and [SEP] tokens.\nGiven an attention map QKT \u2208 R(L+3)\u00d7(L+3), its default 2D attention mask M\u2208 {\u2212\u221e,0}L\u00d7L is derived as:\n$M = \\Psi\\Psi^{T} + diag(1 - \\Psi),  M = [(1 - M) \\to \\infty]$. (3)\nwhich is further based on the 1D token mask as a vector \u03a8 = [\u03a80, \u03a81,..., \u03a8L+1, \u03a8L+2]with elements \u03a8i = 1 as text tokens and \u03a8i = 0 as special tokens [CLS], [PAD] and [SEP]:\n$\\Psi_{i} = \\begin{cases}\n0, & i=0, i>L\\\\\n1, & 1 \\leq i \\leq L\n\\end{cases}$ (4)\nTherefore, \u03a8\u03a8T means that 2D attention is computed across the tokens of input text, while diag(1 \u2013 \u03a8) handles the special tokens at \u03a8i = 0 positions in 1 \u2013 \u03a8 as a Diagonal Matrix.\nWith the attention mask M, global feature Uglobal is obtained via the self-attention mechanism with query Q, key K and value V features of the text and special tokens as:\n$U_{global} = softmax(\\frac{Q K^{T}}{\\sqrt{d}} + M)V$, (5)\nwhere each attention value added with \u221e in attention mask M yields e\u2212\u221e = 0 inside softmax() function for the lowest attention, and other value added with 0\nTo keep attribute-specific tokens, we employ the attribute position \u0424\u0410,\u0422 from LLM as a new 1D token mask with elements \u0398i:\n$\\Theta_{i} = \\begin{cases}\n0, & i \\in \\Phi_{A,T}\\\\\n1, & i \\notin \\Phi_{A,T}\n\\end{cases}$ (6)\nAttribute-specific feature Uattri is obtained similar to Uglobal via new M* to obtain M*:\nM* = \u0398\u03a8T + diag(1 \u2013 \u03a8) (7)"}, {"title": "C. Attribute Feature Enhancement", "content": "In the field of multimodality, embeddings of composite concepts can often be well-approximated as linear compositional structures [13], such as U(rainy morning) = U(rainy)+ U(morning). Leveraging the linear additivity of embeddings, we perform a weighted linear fusion of global text features and attribute features as two vectors, which can be mathematically represented as follows:\n$U_{new} = W_{global} U_{global} + W_{attri}U_{attri}$\n$\\Rightarrow U_{new} = W_{global} (U_{attri} + U_{cate}) + W_{attri}U_{attri}$\n$\\Rightarrow U_{new} = (W_{global} + W_{attri}) U_{attri} + W_{global}U_{cate}$ (10)\nwhere Uglobal and Uattri are the vectors for global text and the attribute words, respectively, while Wglobal and Wattri denote their corresponding weight scalars. According to the reasoning formula, the new features Unew are enhanced more towards Uattri, with Wglobal + Wattri higher than Wglobal for category features Ucate. Moreover, an extra bias can be further applied to Unew for a better learning of explicit linear composition:\n$U_{new} = W_{global} U_{global} + W_{attri}U_{attri} + bias$. (11)\nThe Unew signifies the attribute-highlighted embedding. Unew is the output of the text encoder for detection tasks. The scaler weight triplets are represented as (Wglobal, Wattri, bias)."}, {"title": "IV. EXPERIMENTS", "content": "We report the performance of applying our method to three models of different architectures, which are fine-tuned in the FG-OVD [10] training set and tested on this benchmark. In \"Experimental Settings\" Section, FG-OVD dataset and evaluation protocol is introduced. The three models are introduced in \"Baseline Methods\" Section. Then, we present the fine-tuning schemes and compare the fine-tuned FG-OVD performance with the baselines and previous works.\nSince only three parameter scalars for the explicit linear composition are fine-tuned, they have strong transferability. Hence, we report the test performance after transferring the best weight scalars triplets of the three models to each other."}, {"title": "A. Experimental Settings", "content": "FG-OVD benchmark: The FG-OVD dataset [10] is a comprehensive dataset with eight distinct scenarios, divided into difficulty-based and attribute-based benchmarks. Each subset consists of both positive and negative captions, which are essential for a balanced assessment. The difficulty-based benchmarks adjust the negative captions to make the lower or higher distinction between positive and negative captions, with the Trivial, Easy, Medium, and Hard subsets representing increasing levels of challenge. In details, negative captions of Trivial are arbitrarily selected from other objects, while Easy, Medium, and Hard are crafted by successively replacing 3, 2, and 1 attributes, in that order. On the other hand, the attribute-based benchmarks enable a more focused evaluation of OVD models to detect objects with a specific kind of attributes types, including Color, Material, Pattern and Transparency.\nEvaluation Protocol: The Evaluation Protocol for this paper strictly adheres to the settings of FG-OVD dataset [10], employing dynamic vocabularies for both training and inference. This means that for each detected object i, the model identifies a category from {cpos, c1neg, ..., cNneg}, Here, cpos signifies the positive description of the object i, while cjneg represents the first negative caption for object i, with a total of N. As depicted in the formula [40], traditional evaluation metrics do not penalize the presence of an incorrect prediction with higher confidence occurring at the correct location.\nFor instance, assuming a OVD model is tasked with detecting a category with a positive caption \u201cred house\u201d and a negative caption \"green house\", if the model fails to comprehend the semantic context of the text, it may detect two bounding boxes at the location of the ground truth \"red house\" and assign both these two labels accordingly."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a universal FG-OVD approach, called HA-FGOVD, which is a plug-and-play manner for mainstream frozen OVDs to enhance their fine-grained object detection capabilities by making use of a simple but explicit linear composition. Extensive experiments have demonstrated that our approach can effectively activate the suppressed attribute features, proving that there is potential for attribute features within the latent space of mainstream OVD models. Moreover, the weight scalar triplets in our approach can be transferred to other OVD models without additional training. The proposed approach achieves new state-of-the-art performance on the FG-OVD benchmark."}]}