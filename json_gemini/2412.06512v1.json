{"title": "The Fusion of Large Language Models and Formal Methods for Trustworthy Al Agents: A Roadmap", "authors": ["YEDI ZHANG", "YUFAN CAI", "XINYUE ZUO", "XIAOKUN LUAN", "KAILONG WANG", "ZH\u00c9 H\u00d3U", "YIFAN ZHANG", "ZHIYUAN WEI", "MENG SUN", "JUN SUN", "JING SUN", "JIN SONG DONG"], "abstract": "Large Language Models (LLMs) have emerged as a transformative AI paradigm, profoundly influencing daily life through their exceptional language understanding and contextual generation capabilities. Despite their remarkable performance, LLMs face a critical challenge: the propensity to produce unreliable outputs due to the inherent limitations of their learning-based nature. Formal methods (FMs), on the other hand, are a well-established computation paradigm that provides mathematically rigorous techniques for modeling, specifying, and verifying the correctness of systems. FMs have been extensively applied in mission-critical software engineering, embedded systems, and cybersecurity. However, the primary challenge impeding the deployment of FMs in real-world settings lies in their steep learning curves, the absence of user-friendly interfaces, and issues with efficiency and adaptability.\nThis position paper outlines a roadmap for advancing the next generation of trustworthy AI systems by leveraging the mutual enhancement of LLMs and FMs. First, we illustrate how FMs, including reasoning and certification techniques, can help LLMs generate more reliable and formally certified outputs. Subsequently, we highlight how the advanced learning capabilities and adaptability of LLMs can significantly enhance the usability, efficiency, and scalability of existing FM tools. Finally, we show that unifying these two computation paradigms-integrating the flexibility and intelligence of LLMs with the rigorous reasoning abilities of FMs- has transformative potential for the development of trustworthy AI software systems. We acknowledge that this integration has the potential to enhance both the trustworthiness and efficiency of software engineering practices while fostering the development of intelligent FM tools capable of addressing complex yet real-world challenges.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid advancement of modern AI techniques, particularly in the realm of Large Language Models (LLMs) like GPT [Achiam et al. 2023], Llama [Touvron et al. 2023], Claude [Claude 2024], Gemini [Gemini 2024], etc., has marked a significant evolution in human-level computational capabilities. These models are fundamentally reshaping tasks across a spectrum of applications, from natural language processing to automated content generation. Trained on vast text corpora,"}, {"title": "2 A ROADMAP TO TRUSTWORTHY LLM: COMBINING LLM AND FM", "content": "This section gives a roadmap for the development of a trustworthy LLM agent, specifically tailored to tasks involving FMs, emphasizing the integration of rigorous formal analysis techniques with the adaptability and learning capabilities of language models.\nThe proposed roadmap, depicted in Figure 1, adopts a twofold strategy. On the one hand, we advocate for the integration of formal analysis principles throughout the development and V&V phases. This aims to enhance the reasoning capabilities of LLM agents while ensuring their reliability and compliance with stringent certification standards, a strategy direction we term FMs for LLMs. On the other hand, we leverage the adaptability and learning capabilities of LLMs during the functionality development phase of FM-based tools. This is aimed at enhancing the flexibility and reasoning efficiency of established formal analysis methodologies, such as theorem proving and model checking. In this paper, we refer to this strategic direction as LLMs for FMs. Ultimately, we argue that the dynamic interplay between these two strategic directions will substantially advance the development of trustworthy LLM agents, particularly in the domain of formal methods.\nBuilding on these foundations, we outline the envisioned characteristics of a trustworthy LLM agent as follows. Firstly, an LLM agent tailored for formal analysis should be implemented directly at the client's premises, allowing them to learn from and adapt to the client's specific data and workflows, also allowing the client's domain-specific experts to use formal methods through LLM agents with a low entry bar. This localized learning approach should not only enable the agents to enhance their performance over time but also ensure that the entire process safeguards the client's privacy. Based on this, LLM agents will significantly reduce both the human learning curve and the requisite human resources needed to apply formal methods, thereby facilitating a more efficient integration of these methodologies into the client's operational framework. Secondly, each output suggested by an LLM agent should be both faithful and rigorously certified, particularly when the response is expected to be deterministic. This requirement ensures that the outputs not only align closely with the rules defined within the formal methods framework but also meet predefined accuracy and reliability standards. To achieve this, the outputs must undergo a thorough certification process, possibly involving techniques such as testing or verification, to validate that"}, {"title": "3 FM FOR LLM: TOWARDS RELIABILITY AND CERTIFICATION", "content": "In this section, we illustrate how formal methods can enhance the reliability and help certification of LLMs. Specifically, we explore this integration direction from three perspectives: (i) trustworthy LLMs with symbolic solvers, (ii) LLM Testing based on logical reasoning, and (iii) rigorous LLM behavior analysis. We argue that incorporating these FM-based techniques enables AI systems to become reliably secure, paving the way for developing trustworthy AI systems.\nNote that this section highlights the initial efforts to apply formal methods to enhance the reliability of LLMs, acknowledging that these approaches are not exhaustive. While a fully comprehensive methodology for integrating formal methods into LLMs remains elusive, we present our current implementations and future visions as preliminary steps toward advancing this direction."}, {"title": "3.1 SMT Solvers for LLMs", "content": "Satisfiability Modulo Theories (SMT) solvers are specialized tools designed to determine the satisfiability of logical formulas defined over some theories, such as arithmetic, bit-vectors, and arrays. They play a pivotal role in formal verification, program analysis, and automated reasoning, serving as essential components to ensure the correctness and reliability of complex software systems. Recent studies [Deng et al. 2024; Pan et al. 2023; Wang et al. 2024; Ye et al. 2024] have explored the integration of SMT solvers to enhance the accuracy and reliability of LLMs in logic reasoning tasks. These solver-powered LLM agents operate by translating task descriptions into formal specifications, delegating reasoning tasks to specialized expert tools for precise analysis, and subsequently converting the outputs back into natural language.\nWe have identified three main challenges within this research line. Firstly, while LLMs are capable of generating logical constraints or SMT formulas, they often produce suboptimal or overly verbose constraints, which can place an additional computational burden on the solver. Secondly, the outputs of LLMs lack guarantees of correctness or logical consistency, potentially introducing subtle inaccuracies or ambiguities in the generated SMT constraints. It can lead to invalid results or solutions that are challenging to interpret. Lastly, LLMs often lack domain-specific knowledge and may struggle to generate outputs that conform to the precise formal syntax required by SMT solvers. Consequently, they may generate formulas that are semantically sensible but syntactically invalid formulas, rendering them unprocessable by the solvers."}, {"title": "3.1.1 Our Insights", "content": "We present our insights and proposed strategies to address the three key challenges outlined above.\nStrategy 1. Multiple LLMs Debating. To address the challenge of LLMs generating suboptimal or overly verbose constraints, a potential strategy involves leveraging multiple LLMs in a collaborative or adversarial framework to critique, validate, and refine each other's outputs. In this approach, the system employs one or more LLMs to generate SMT code from natural language inputs, while other"}, {"title": "3.1.2 Case Study on LLMs with Z3", "content": "To illustrate our perspective, we give our recent exploration of the interaction between LLMs and Z3 in Python.\nZ3 [de Moura and Bj\u00f8rner 2008], a widely used SMT solver, accepts inputs in the form of simple-sorted formulas expressed in first-order logic (FOL). These formulas can include symbols with predefined meanings, defined by the underlying theories supported by the solver, and these theories encompass domains such as arithmetic, bit-vectors, arrays, etc., making Z3 particularly well-suited for reasoning about a wide range of logical constraints.\nExample. Consider a scenario where a user requests the LLM agent to solve a scheduling problem:\n\u201cCan you help plan a meeting for a team of three people-David, Emma, and Alex?\nDavid is free on Monday or Tuesday, Emma is free on Tuesday or Wednesday, and Alex is free only on Tuesday or Thursday. Find a common day when all three are available.\u201d\nWe now provide a detailed, step-by-step solution for this task:\nFormalization of Constraints. Given the above problem, the initial Z3 constraints in Python generated by the LLM are given as follows:\n# Define days of the week\ndays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\"]\nDavid_free = [Bool(f\"David_free_{day}\") for day in days]\nEmma_free = [Bool(f\"Emma_free_{day}\") for day in days]\nAlex_free = [Bool(f\"Alex_free_{day}\") for day in days]\n# Create a solver\nsolver = Solver()\n# Define constraints for each person's availability\nsolver.add(Or(David_free[0], David_free[1]))\nsolver.add(Or(Emma_free [1], Emma_free[2]))\nsolver.add(Or(Alex_free [1], Alex_free[3]))\n# Add constraints that ensure a common day for all three\ncommon_day_constraints = [And(David_free[i],\nEmma_free[i], Alex_free[i]) for i in range(len(days))]\nsolver.add(Or(common_day_constraints))"}, {"title": "3.2 Logical Reasoning for LLM on Test Case Generation", "content": "LLM Testing [Hendrycks et al. 2021; Huang et al. 2023; Zhong et al. 2024; Zhou et al. 2023] is primarily focused on establishing a comprehensive benchmark to evaluate the overall performance of the models, ensuring that they fulfill specific assessment criteria, such as accuracy, coherence, fairness, and safety, in alignment with their intended applications. Among these criteria, the phenomenon of \u201challucination\u201d is particularly noteworthy. This occurs when LLMs generate coherent but factually or contextually inaccurate or irrelevant output during tasks such as problem-solving.\nRecent research has introduced a variety of methodologies for detection and evaluation to mitigate hallucination issues in LLMs. A common and straightforward method is to create comprehensive benchmarks specifically designed to assess LLM performance. However, these methods, which often rely on simplistic or semi-automated techniques such as string matching, manual validation, or cross-verification using another LLM, has significant shortcomings in automatically and effectively testing Fact-conflicting hallucinations (FCH) [Li et al. 2024a]. This is largely due to the lack of dedicated ground truth datasets and specific testing frameworks. We contend that unlike other types of hallucinations, which can be identified through checks for semantic consistency, FCH requires the verification of content's factual accuracy against external, authoritative knowledge sources or databases. Hence, it is crucial to automatically construct and update factual benchmarks, and automatically validate the LLM outputs based on that.\nTo this end, we propose to apply logical reasoning to design a reasoning-based test case generation method aimed at developing an extensive and extensible FCH testing framework. Such a testing framework leverages factual knowledge reasoning combined with metamorphic testing principles to ensure a comprehensive and robust FCH evaluation of LLM outputs."}, {"title": "3.2.1 Overview", "content": "A reasoning-based test case generation framework for FCH in LLMs should comprise the following four key modules working in sequential:\nModule 1. Factual Knowledge Extraction. Utilizing extensive knowledge database dumps, the essential information and factual triples of verified entities are systematically extracted.\nModule 2. Logical Reasoning. This module enables employing reasoning rules for generating valid and diverse facts and establishing new ground truth knowledge.\nModule 3. Benchmark Construction. Focused on producing high-quality test case-oracle pairs from the newly generated ground truth knowledge, this module uses a straightforward metamorphic relation: Questions that align with the generated knowledge should receive a response of \u201cYES\u201d, while questions that contradict it should be answered with \u201cNO\u201d. This module also includes methods for generating or selecting effective prompts to facilitate reliable interaction with LLMs.\nModule 4. Response Evaluation. In the final module, the responses from LLMs are evaluated to detect factual consistency automatically. The module parses LLM outputs using NLP techniques to construct semantic-aware representations and assesses semantic similarity with ground truth. It then applies similarity-based oracles, using metamorphic testing to evaluate the consistency between LLM responses and established ground truth."}, {"title": "3.2.2 Factual Knowledge Extraction", "content": "This process focuses on extracting essential factual information from input knowledge data in the form of fact triples, which are then suitable for logical reasoning. Existing knowledge databases [Auer et al. 2007; Bollacker et al. 2007; Miller 1995; Suchanek et al. 2007] serve as valuable resources due to their extensive repositories of structured data derived from"}, {"title": "3.2.3 Logical Reasoning", "content": "This step focuses on deriving enriched information from previously extracted factual knowledge by employing logical reasoning techniques. The approach utilizes a logical programming-based processor to automatically generate new fact triples by applying predefined inference rules, taking one or more input triples and producing derived outputs.\nIn particular, to introduce variability in the generation of test cases, reasoning rules, commonly utilized in existing literature [Abboud et al. 2020; Liang et al. 2022; Zhou et al. 2019] for knowledge reasoning, are typically adopted, including negation, symmetric, inverse, transitive and composite. These rules provide a systematic framework for generating new factual knowledge, ensuring diverse and comprehensive test case preparation. The system applies all relevant reasoning rules exhaustively to the appropriate fact triples, enabling the automated enrichment of the knowledge base for further testing purposes."}, {"title": "3.2.4 Benchmark Construction", "content": "This process consists of two key steps: (i) generating question-answer (Q&A) pairs and (ii) creating prompts from derived triples, which together can significantly reduce manual effort in test oracle generation.\nStep 1. Question Generation. This step uses entity-relation mappings to populate predefined Q&A templates, aligning relation types with corresponding question structures based on the grammatical and semantic characteristics of predicates. For predicates with unique characteristics, customized templates are employed to generate valid Q&A pairs. To enhance natural language formulation, LLM can be used to refine the Q&A structures. Answers are derived directly from factual triples, with true/false judgments determined by the data. Mutated templates, leveraging synonyms or antonyms, diversify questions with opposite semantics, yielding complementary answers.\nStep 2. Prompt Construction. Prompts are designed to instruct LLMs to provide explicit judgments (e.g., yes/no/I don't know) and outline their reasoning in standardized formats. LLM analysts can utilize predefined instructions to ensure clarity and enable LLMs to deliver assessable and logically consistent responses. This method maximizes the model's reasoning capabilities within the structured framework of prompts and cues."}, {"title": "3.2.5 Response Evaluation", "content": "This step aims to enhance the factual evaluation in LLM outputs by identifying discrepancies between LLM responses and the verified ground truth in Q&A pairs. Since the \u201cyes\u201d or \u201cno\u201d answers from LLMs may be unreliable, the module emphasizes analyzing the reasoning process to assess factual consistency. The approach uses semantic similarity between parsed LLM responses and ground truth to detect inconsistencies systematically.\nThe process begins with preliminary screening, where responses indicating the LLM's inability to answer (e.g., \u201cI don't know\u201d) are classified as correct, as they align with the model's honesty principle. Suspicious responses proceed to response parsing and semantic structure construction, where statements in the reasoning process are parsed into triples. These triples are used to construct"}, {"title": "3.3 Rigorous LLM Behavior Analysis", "content": "While LLM Testing techniques can effectively provide broad assessments and reveal edge cases that may provoke unexpected responses, they are limited in their capability to give rigorous guarantees on LLM behaviors. LLM Verification, on the other hand, serves as a complementary mechanism. However, as LLMs grow more complex and tasks become increasingly sophisticated, traditional neural network verifiers lose relevance due to their limitations in accommodating diverse model architectures and their focus on single-application scenarios. Indeed, formal verification of LLMs poses intrinsic challenges due to three key factors:\nFactor 1. Non-Deterministic Responses. Responses from LLMs are non-deterministic, meaning their outputs may vary even with the same input. This inherent variability presents substantial challenges to providing deterministic guarantees regarding their behavior.\nFactor 2. High Input Dimensions. The high dimensionality of inputs in LLMs leads to exponential growth in the number of input tokens, rendering exhaustive verification across an infinite input space highly impractical.\nFactor 3. Lack of Formal Specification. While formal specifications are rigorous, they often lack the expressive capability of natural language, which makes it extremely difficult to precisely capture the nuanced and complex language behaviors expected from LLMs.\nHence, we propose that a specialized verification paradigm tailored specifically for LLMs should be considered to ensure reliable and rigorous certification for long-term applications.\nGiven these challenges, we argue that monitoring might serve as a viable long-term solution for reliable LLM behavior analysis. Positioned between testing and verification, monitoring of formalized properties at runtime enables rigorous certification of system behavior with minimal computation overhead by examining execution traces against predefined properties. This approach has already inspired several efforts to monitor LLM responses at runtime [Besta et al. 2024; Chen et al. 2024; Cohen et al. 2023; Manakul et al. 2023] (quite similar to another research line named guardrails). However, the specifications used in these methods remain ambiguous and informal. For example, they define the properties of low hallucination based on the stability of LLM outputs. More recently, an approach [Cheng et al. 2024] has been introduced to monitor the conditional fairness properties of LLM responses. The specifications in [Cheng et al. 2024] are informed by linear temporal logic and its bounded metric interval temporal logic variant, reflecting a shift toward formal methods for more precise and dependable monitoring of LLM behavior. Despite these progresses, further efforts are needed to enable rigorous monitoring of a wider range of properties to fit in more real-world settings, which will remain our primary focus in the future."}, {"title": "4 LLM FOR FM: SOLVING VERIFICATION TASKS INTELLIGENTLY", "content": "This section explores how LLMs can enhance formal methods by developing intelligent LLM agents for tasks such as theorem proving and model checking. These agents bring adaptability and efficiency to traditional formal verification processes, paving the way for more advanced and effective formal methods."}, {"title": "4.1 LLM for Autoformalization", "content": "Autoformalization is the process of automatically translating natural language based specifications or informal representations into formal specifications or proofs. This complex task demands a deep understanding of both informal and formal languages, along with the ability to generate accurate, machine-readable formal representations. The recent advances of LLMs have opened new avenues for automating the formalization process, which is traditionally a manual task. Recent research has demonstrated the effectiveness of LLMs in various autoformalization scenarios, including neural theorem proving [Jiang et al. 2022b], temporal logic generation [Murphy et al. 2024], and program specification generation based on source code [Ma et al. 2024b]. In this section, we explore the role of LLM agents in facilitating proof autoformalization, focusing on the challenges and opportunities in this line of research.\nInformal proofs, commonly found in textbooks, research papers, online forums, or even generated by LLMs, often omit details that human consider trivial or self-evident. However, to ensure rigorous verification by theorem provers, they need to be translated into formal proofs that adhere to a specific syntax, where all the details are explicitly provided. The challenge lies in bridging the gap between the informal proof sketches and the detailed rigor required in formal proofs.\nSpecifically, Figure 2 gives a motivating example from the miniF2F benchmark [Zheng et al. 2022]. The problem and its human-written informal proof are given, and it demonstrates a concise and correct inductive proof. However, some intermediate steps, particularly the algebraic manipulations, are omitted in the informal proof. Although the equation can be easily verified by human experts, it is challenging for LLMs to infer the missing details and generate a correct formal proof due to their limited symbolic reasoning capabilities. The following Isabelle/HOL proof snippet illustrates an attempt by ChatGPT-3.5-turbo to formalize the given informal proof. While the model correctly identifies the required transformations, it fails during the simplification of the term 2*(n+2)-(n+1),"}, {"title": "4.2 LLM for Theorem Proving", "content": "Among all formal analysis techniques, theorem proving stands out for its capability to handle complex state spaces, abstract specifications, and highly intricate systems. Unlike model checking (we will discuss subsequently) which is primarily designed for finite models and faces challenges with state space explosion, theorem proving excels in leveraging mathematical reasoning to establish properties that hold universally. This capability has been successfully demonstrated in critical systems, such as CompCert [Leroy 2009], a formally verified C compiler that guarantees the correctness of compiled code, and seL4 [Klein et al. 2009], a microkernel with rigorous proofs of memory safety, functional correctness, and security properties. These examples showcase the unmatched flexibility theorem proving offers in system verification, making it a powerful tool for ensuring system correctness. In this section, we explore how LLMs can augment each sub-task"}, {"title": "4.2.1 Premise Selection", "content": "Retrieving relevant facts from a large collection of lemmas is a critical task in theorem proving, and this process is known as premise selection. This task is typically done manually by explicitly specifying the used lemmas in the proof scripts, which often requires trial and error and deep domain knowledge, making it time-consuming and error-prone. Some powerful automation techniques in interactive theorem provers (ITPs) also need premise selection to first filter out irrelevant lemmas from the large search space. For example, Sledgehammer [B\u00f6hme and Nipkow 2010], an effective tool for Isabelle/HOL [Paulson 1994], collects relevant facts from the background theories and sends them to external automatic theorem provers (ATPs) and SMT solvers to find proofs. This process involves premise selection to identify the most relevant lemmas that can help in proving the current goal. For example, Sledgehammer usually selects about 1,000 lemmas out of tens of thousands of premises. Some heuristics [Meng and Paulson 2009] and machine learning techniques [K\u00fchlwein et al. 2013] like naive Bayes are used in Sledgehammer for relevant fact selection. A recent work [Miku\u0142a et al. 2024] proposed using transformer models to learn the relevance of lemmas for premise selection, which improved the success rate of Sledgehammer by 13% on the miniF2F benchmark [Zheng et al. 2022].\nOur insight is that LLM agents can further improve the premise selection process by leveraging their code understanding capabilities. Premise selection is fundamentally different from other tasks like code retrieval, where cosine similarity is often used to identify candidates with high syntactic resemblance. While such approaches are effective in retrieval contexts, they fall short in theorem proving. Lemmas with similar syntactic structures to the proof goal may not necessarily be the most useful, and conversely, the most helpful facts may exhibit little syntactic similarity. This highlights the need for a retrieval approach grounded in the semantic understanding of lemmas, rather than relying solely on syntactic features as most current methods do.\nLLMs, with their strong code comprehension capabilities, offer a way to address this gap. They can infer the meaning of a lemma from its name, its definition, and its contextual information, mimicking the reasoning process of a human expert. For instance, a human expert can intuitively assess whether a given lemma is likely to be helpful for a particular proof goal. However, the sheer number of lemmas in large proof libraries makes it impossible for experts to manually evaluate and rank all possible candidates. By contrast, LLM agents can efficiently scale this process. We can first collect definitions and contextual information of lemmas and ask LLM agents to generate semantic descriptions in natural language for each lemma, forming a knowledge base for premise selection. Then, given a proof goal, LLM agents comprehend the goal and generate a semantic representation, which is used to query the knowledge base for relevant lemmas. This approach effectively simulates the expert's intuition across a vast number of possibilities, fully leveraging semantics information to guide the premise selection process."}, {"title": "4.2.2 Proof Generation", "content": "Proof step generation is the central task in theorem proving, where the objective is to predict one or more proof steps to construct a valid proof for a given theorem. Many pioneering works on LLM-based proof generation [Han et al. 2022; Polu et al. 2023; Polu and Sutskever 2020] approach this problem as a language modeling task and train LLMs on large-scale proof corpora to predict the next proof step. Various techniques have been developed to improve the quality of generated proofs. For instance, learning to invoke ATPs to discharge subgoals [Jiang et al. 2022a], repairing failed proof steps by querying LLMs with the error message [First et al. 2023], and predicting auxiliary constructions to simplify proofs [Trinh et al. 2024] have all demonstrated significant potential."}, {"title": "4.2.3 Case Study on LLMs with Coq", "content": "To illustrate our perspective, we illustrate our recent exploration of the interaction between LLMs and Coq. Coq [Huet et al. 1997] is a classic proof assistant based on constructive type theory, supporting functional programming and formal specification. The integration of Coq with an LLM agent involves several key steps:\nStep 1. Natural Language Understanding. The LLM agent receives natural language input from the user, typically in the form of a mathematical theorem, conjecture, or problem.\nStep 2. Formalization of the Problem. The LLM agent translates the natural language problem into Coq's formal language. This includes defining types, propositions, and functions necessary for the formulation of the theorem.\nStep 3. Proof Construction. The LLM agent collaborates with Coq to construct proofs, utilizing Coq's interactive features to propose proof steps that are subsequently verified or refined.\nStep 4. Proof Verification and Feedback. Once the proof is constructed, Coq verifies its correctness. The LLM agent translates the verified proof back into natural language, providing a comprehensible explanation of the result to the user.\nExample. Consider the following natural language query posed to the LLM agent:\n\u201cProve that the sum of two even numbers is even.\u201d"}, {"title": "4.3 LLM for Model Checking", "content": "Model checking is a formal verification technique that systematically explores a system's state space to check if the system satisfies specified properties, such as safety and liveness. It is particularly effective for finite-state systems, providing automated detection of logical errors like deadlocks or critical system property violations. However, traditional model checking faces great limitations, including scalability challenges for large systems and the complexity involved in system modeling and property formalization.\nIn this section, we illustrate how model checking agents can alleviate above limitations by combining the strengths of LLMs. Leveraging the respective advantages, a model checking agent accepts a system description in natural language from the user, generates corresponding processes using an LLM, and refines them iteratively based on feedback from the model checker. We note that this refinement can be facilitated by an Automated Prompt Engineer (APE) [Cheng et al. 2023; Zhou et al. 2022], which optimizes the interaction between the LLM and the model checker. This synergy not only streamlines the model checking process but also enhances its accessibility for users without extensive expertise in formal verification."}, {"title": "4.3.1 Overview", "content": "The overall workflow for building a model checking agent should include the following steps:\nStep 1. Natural Language Input. Users provide a description of a system in natural language, specifying desired behaviors or properties. For instance, they might describe a mutual exclusion protocol or outline expected system behaviors.\nStep 2. LLM Planning. An LLM with strong reasoning capabilities translates the natural language input into a detailed, structured plan. This plan breaks down the system description into specific instructions, including a one-to-one mapping of logical steps, such as defining variables, specifying state transitions, and drafting preliminary assertions.\nStep 3. LLM Code (Assertion) Completion. Following the structured plan, a specialized LLM trained in syntax and logic generates the code and assertions required to implement the system."}, {"title": "4.3.2 PAT Agent", "content": "Process Analysis Toolkit (PAT) is a formal verification tool designed to model, simulate, and verify concurrent and real-time systems. It supports the verification of key properties such as deadlock-freeness, reachability, and refinement, addressing critical correctness and reliability concerns in system design. With applications spanning domains such as vehicle and aircraft safety, resource optimization, and complex system analysis, PAT provides a robust foundation for developing a model checking agent.\nIn the following motivating example, we illustrate how the PAT Agent can be used to derive verified system constructs, starting from natural language instructions.\nExample. In the development of a car system, ensuring that the key is never locked inside the car is a critical requirement. This safeguard is essential for user convenience and safety, as locking the key inside could result in costly locksmith services, delays, or emergencies in dangerous areas. The system must ensure logical consistency in different operations to avoid such consequences.\nWe begin by asking an LLM (gpt-40-2024-08-06) to directly generate a formal model based on a high-level description. While the LLM has learned syntax rules and demonstrates effective planning capabilities, the generated model contains a critical logic flaw: it allows the key to be locked inside the car. This issue arises due to hallucination by GPT-40, which incorrectly assumes that placing the key into a currently locked car is a valid scenario, contradicting commonsense reasoning.\n[key == i && owner[i] == near]leavekey {key = incar;}\nConsequently, the resulting system design deviates from the intended behavior, compromising its reliability and safety.\nTo address this issue, we use PAT to formally verify the generated system. PAT identifies an error trace (i.e., a sequence of operations) that leads to the key being locked inside the car, providing feedback that highlights flaws in the logic governing key and door operations. By analyzing this error trace, the LLM identifies the aforementioned logical flaw and corrects it by imposing stricter restrictions on when the key can be left inside the car. Additionally, it establishes clear conditions for locking the door, ensuring alignment with the intended system behavior."}, {"title": "4.3.3 Other Model Checking Agents", "content": "We acknowledge that such an agent framework is highly generalizable and can be adapted for tools like Alloy Analyzer [Jackson 2000], PRISM [Kwiatkowska"}, {"title": "5 UNIFYING THE POWER OF FMS AND LLMS", "content": "This section highlights the synergy between FMs and LLMs, combining the rigor of formal methods with the flexibility and intelligence of LLMs. By integrating these two approaches, we can create robust systems that enhance both the trustworthiness and efficiency of software engineering processes while adapting to complex and real-world problems. We demonstrate this idea through an application: trustworthy code generation.\nWe acknowledge that using an LLM for program synthesis involves leveraging its ability to understand and generate code snippets, solve programming problems, and create software systems based on high-level requirements. This integration aims to leverage the LLM agent's ability to understand natural language, translate program specifications and code into formal properties, and assist in verifying correctness using automated verification tools."}, {"title": "5.1 Challenges of LLMs for Program Synthesis", "content": "LLMs have made rapid advancements in mathematics, reasoning, and programming [Romera-Paredes et al. 2023; Zhao et al. 2023], significantly reshaping the landscape of software development and computational problem-solving. Industrial-grade LLMs like GPT-4 [OpenAI and co. 2023] and tools like GitHub Copilot [GitHub 2023] have emerged as powerful assistants for programmers, automating coding-related tasks and achieving performances exceeding the 50th percentile in competitive programming contests [Pandey et al. 2024]. These achievements highlight the transformative potential of LLMs in enhancing productivity and reducing manual effort in coding.\nHowever, despite these advancements, LLMs face significant challenges, notably the issue of hallucination. This phenomenon undermines their reliability in critical contexts. User studies [Ding et al. 2023; Vaithilingam et al. 2022] have revealed that programmers often struggle to trust and debug LLM-generated code due to the opaqueness and lack of control over the generation process. Alarmingly, empirical research indicates that over half of ChatGPT's responses to programming-related queries contain inaccuracies [Kabir et al. 2024], exacerbating concerns about their utility in professional and high-stakes environments. Even worse, recently, mathematicians have proven that hallucinations in LLMs are unavoidable [Xu et al. 2024].\nThese challenges underscore the urgent need for strategies to mitigate hallucination and improve the transparency and trustworthiness of LLM-generated code, paving the way for their broader adoption in critical programming workflows."}, {"title": "5.2 Program Refinement", "content": "The task of program refinement has a rich history, dating back to foundational works such as [Dijkstra et al. 1976; Floyd 1993; Hoare 1969]. The related theories [Back and von Wright 1990; Morgan 1990] are based on Hoare logic and the calculus of weakest preconditions. Recent advancements have extended these foundational ideas by formalizing the refinement calculus within interactive theorem provers. Notable examples include the work in Isabelle [Foster et al. 2020] and Coq [Alpuim and Swierstra 2018; Sall et al. 2019], leveraging the expressive power of these tools to mechanize and verify program refinements. The Coq proof assistant, in particular, has been instrumental in encoding and verifying complex refinement relations [Barras et al. 1999]."}, {"title": "5.3 Integrating Formal Program Refinement with LLMs", "content": "The refinement calculus [Back and von Wright 1990; Carrington et al. 1998; Morgan 1990; Sall et al. 2019; Swierstra and Alpuim 2016] provides a rigorous framework for the stepwise refinement method of program construction. This approach involves specifying a program's required behavior as an abstract, non-executable specification, which is systematically transformed into an executable program through a series of correctness-preserving steps. By ensuring that each step maintains the program's intended semantics, the refinement calculus offers a sound methodology for constructing reliable software."}, {"title": "5.3.1 Technical Challenges", "content": "Despite its strengths, the refinement process is traditionally performed manually, making it a labor-intensive and error-prone task. The reliance on human effort to translate abstract specifications into executable code not only slows down development but also increases the risk of introducing subtle mistakes during transformation. This bottleneck has long been a barrier to scaling the benefits of refinement calculus to larger, more complex software systems.\nGiven the advancements in LLMs, integrating their powerful code-generation capabilities into the refinement process presents an exciting opportunity. LLMs can assist in automating parts of the transformation, bridging the gap between high-level specifications and executable implementations while adhering to the principles of correctness-preserving refinement. This integration could significantly reduce the manual effort required, streamline the refinement process, and open new avenues for leveraging LLMs in formal methods and program synthesis."}]}