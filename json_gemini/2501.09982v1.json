{"title": "RichSpace: Enriching Text-to-Video Prompt Space via Text Embedding Interpolation", "authors": ["Yuefan Cao", "Chengyue Gong", "Xiaoyu Li", "Yingyu Liang", "Zhizhou Sha", "Zhenmei Shi", "Zhao Song"], "abstract": "Text-to-video generation models have made impressive progress, but they still struggle with generating videos with complex features. This limitation often arises from the inability of the text encoder to produce accurate embeddings, which hinders the video generation model. In this work, we propose a novel approach to overcome this challenge by selecting the optimal text embedding through interpolation in the embedding space. We demonstrate that this method enables the video generation model to produce the desired videos. Additionally, we introduce a simple algorithm using perpendicular foot embeddings and cosine similarity to identify the optimal interpolation embedding. Our findings highlight the importance of accurate text embeddings and offer a pathway for improving text-to-video generation performance.", "sections": [{"title": "1 Introduction", "content": "Text-to-video models have developed rapidly in recent years, driven by the advancement of Transformer architectures [Vas17] and diffusion models [HJA20]. Early attempts at text-to-video generation focused on scaling up Transformers, with notable works such as CogVideo [HDZ+22] and Phenaki [VBK+22], which demonstrated promising results. More recently, the appearance of DiT [PX23], which incorporates Transformers as the backbone of Diffusion Models, has pushed the capabilities of text-to-video generation models to new heights. Models like Sora [Ope24], MovieGen [Met24], CogVideoX [YTZ+24], and Veo 2 [Goo24] have further showcased the potential of these approaches. Despite the impressive progress made in recent years, current state-of-the-art text-to-video generation models still face challenges in effectively following complex instructions in user-provided text prompts. For instance, when users describe unusual real-world scenarios, such as \"a tiger with zebra-like stripes walking on the grassland,\" the text encoder may struggle to fully capture the intended meaning. This results in text embeddings that fail to guide the video generation model toward producing the desired output. This issue is also observed in the text-to-image generation domain, where a notable work, Stable Diffusion V3 [EKB+24], addresses this challenge by incorporating multiple text encoders to improve understanding. Although their approach, which combines embeddings from different encoders, yields effective results, it comes at a significant computational cost due to the need to compute embeddings from multiple sources.\nIn this work, we take a different approach by exploring whether we can obtain a powerful text embedding capable of guiding the video generation model through interpolation within the text embedding space. Through empirical experiments, we demonstrate that by selecting the optimal text embedding, the video generation model can successfully generate the desired video. Additionally, we propose an algorithm that leverages perpendicular foot embeddings and cosine similarity to capture both global and local information in order to identify the optimal interpolation text embedding (Fig. 1 and Algorithm 2)."}, {"title": "2 Related Work", "content": "Text-to-Video Generation. Text-to-video generation [SPH+22, VJMP22, BRL+23], as a form of conditional video generation, focuses on the synthesis of high-quality videos using text descriptions as conditioning inputs. Most recent works on video generation jointly synthesize multiple frames based on diffusion models [SSDK+20, HJA20]. Diffusion models implement an iterative refinement process by learning to gradually denoise a sample from a normal distribution, which has been successfully applied to high-quality text-to-video generation. In terms of training strategies, one of the existing approaches uses pre-trained text-to-image models and inserts temporal modules [GNL+23, AZY+23], such as temporal convolutions and temporal attention mechanisms into the pre-trained models to build up correlations between frames in the video [SPH+22, GWZ+23, GYR+23]. PYoCo [GNL+23] proposed a noise prior approach and leveraged a pre-trained eDiff-I [BNH+22] as initialization. Conversely, other works [BRL+23, ZWY+22] build upon Stable Diffusion [RBL+22] owing to the accessibility of pre-trained models. This approach aims to leverage the benefits of large-scale pre-trained text-to-image models to accelerate convergence. However, it may lead to unsatisfactory results due to the potential distribution gap between images and videos. Other approaches are training the entire model from scratch on both image and video datasets [HCS+22]. Although this method can yield high-quality results, it demands tremendous computational resources.\nEnrich Prompt Space. In the context of conditional tasks, such as text-to-image and text-to-video models, prompts worked as conditions can have a significant influence on the performance of the models. For text-conditioned tasks, refining the user-provided natural provided natural language prompts into keyword-enriched prompts has gained increasing attention. Several recent works have explored the prompt space by the use of prompt learning, such as CoCoOp [ZYLL22], which uses conditional prompts to improve the model's generalization capabilities. AutoPrompt [SRLI+20] explores tokens with the most significant gradient changes in the label likelihood to automate the prompt generation process. Fusedream [LGW+21] manipulates the CLIP [RKH+21] latent space by using GAN [GPAM+14] optimization to enrich the prompt space. Specialist Diffusion [LTW+23] augments the prompts to define the same image with multiple captions that convey the same meaning to improve the generalization of the image generation network. Another work [LZY+23] proposes to generate random sentences, including source and target domain, in order to calculate a mean difference that will serve as a direction while editing. The iEdit [BGB+24]"}, {"title": "3 Preliminary", "content": "We first introduce some basic notations in Section 3.1. In Section 3.2, we introduce formal definitions of key concepts. Then, we introduce the formal definition of each module in the CogvideoX model in Section 3.3. Section 3.4 introduces the problem formulation."}, {"title": "3.1 Notations", "content": "For any $k \\in \\mathbb{N}$, let $[k]$ denote the set ${1, 2, \\ldots, k}$. For any $n \\in \\mathbb{N}$, let $n$ denote the length of the input sequence of a model. For any $d \\in \\mathbb{N}$, let $d$ denote the hidden dimension. For any $c \\in \\mathbb{N}$, let $c$ denote the channel of a video. For any $n_f \\in \\mathbb{N}$, we use $n_f$ to denote the video frames. For any $h \\in \\mathbb{N}$ and $w \\in \\mathbb{N}$, we use $h$ and $w$ to denote the height and width of a video. For two vectors $x \\in \\mathbb{R}^n$ and $y \\in \\mathbb{R}^n$, we use $\\langle x, y \\rangle$ to denote the inner product between $x, y$. Namely, $\\langle x, y \\rangle = \\sum_{i=1}^n x_i y_i$. For a vector $x \\in \\mathbb{R}^n$, we use $||x||_2$ to denote the $\\ell_2$ norm of the vector $x$, i.e., $||x||_2 := \\sqrt{\\sum_{i=1}^n x_i^2}$. Let $\\mathcal{D}$ represent a given distribution. The notation $x \\sim \\mathcal{D}$ indicates that $x$ is a random variable drawn from the distribution $\\mathcal{D}$."}, {"title": "3.2 Key Concepts", "content": "We will introduce some essential concepts in this section. We begin with introducing the formal definition of linear interpolation.\nDefinition 3.1 (Linear Interpolation). If the following conditions hold:\n\u2022 Let $x, y \\in \\mathbb{R}^d$ denote two vectors.\n\u2022 Let $k \\in \\mathbb{N}$ denote the interpolation step.\nFor $i \\in [k]$, we define the $i$-th interpolation result $z_i \\in \\mathbb{R}$ as follows:\n$z_i := \\frac{i}{k} \\cdot x + \\frac{k-i}{k} \\cdot y$\nNext, we introduce another key concept used in our paper, the simple yet effective cosine similarity calculator.\nDefinition 3.2 (Cosine Similarity Calculator). If the following conditions hold:\n\u2022 Let $X, Y \\in \\mathbb{R}^{n \\times d}$ denote two matrices.\n\u2022 Let $X_i, Y_i \\in \\mathbb{R}^d$ denote $i$-th row of $X, Y$, respectively.\nThen, we defined the cosine similarity calculator $cos(X, Y) : \\mathbb{R}^{n \\times d} \\times \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}$ as follows\n$cos(X, Y) := \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\langle X_i, Y_i \\rangle}{||X_i||_2 ||Y_i||_2}$"}, {"title": "3.3 Model Formulation", "content": "In this section, we will introduce the formal definition for the text-to-video generation video we use. We begin with introducing the formal definition of the attention layer as follows:\nDefinition 3.4 (Attention Layer). If the following conditions hold:\n\u2022 Let $X \\in \\mathbb{R}^{n \\times d}$ denote the input matrix.\n\u2022 Let $W_K, W_Q, W_V \\in \\mathbb{R}^{d \\times d}$ denote the weighted matrices.\n\u2022 Let $Q = X W_Q \\in \\mathbb{R}^{n \\times d}$ and $K = X W_K \\in \\mathbb{R}^{n \\times d}$.\n\u2022 Let attention matrix $A = Q K^T$."}, {"title": "3.4 Problem Formulation", "content": "In this section, we introduce the formal definition for finding the optimal interpolation embedding as follows:\nDefinition 3.9 (Finding Optimal Interpolation Embedding Problem). If the following conditions hold:\n\u2022 Let $P_a, P_b, P_c$ denote three text prompts.\n\u2022 Our goal is to generate a video that contains features mentioned in $P_a$ and $P_b$, and $P_c$ is a text description of the feature combination of $P_a$ and $P_b$.\n\u2022 Let $E_{t_a}, E_{t_b}, E_{t_c} \\in \\mathbb{R}^{n \\times d}$ denote the text embedding of $P_a, P_b, P_c$.\n\u2022 Let $f_\\theta(E_t, z)$ be defined in Definition 3.8.\nWe define the \u201cFinding optimal interpolation embedding\u201d problem as: According to $E_{t_a}, E_{t_b}, E_{t_c}$, find the optimal interpolation embedding $E_{opt}$ that can make the text-to-video generation model $f_\\theta(E_{opt}, z)$ generate video contains features mentioned in $P_a$ and $P_b$."}, {"title": "4 Main Results", "content": "In Section 4.1, we provide our rigorous theoretical analysis showing that word embedding space is not sufficient to represent all videos. In Section 4.2, we present our algorithm for finding the optimal interpolation embedding."}, {"title": "4.1 Word Embedding Space being Insufficient to Represent for All Videos", "content": "Since the text-to-video generation model only has a finite vocabulary size, it only has finite wording embedding space. However, the space for all videos is infinite. Thus, word embedding space is insufficient to represent all videos in video space. We formalize this phenomenon to a rigorous math problem and provide our findings in the following theorem.\nTheorem 4.1 (Word Embeddings being Insufficient to Represent for All Videos, informal version of Theorem A.4). If the following conditions hold:"}, {"title": "4.2 Optimal Interpolation Embedding Finder", "content": "In this section, we introduce our main algorithm (Algorithm 1 and Algorithm 2), which is also depicted in Fig. 1. The algorithm is designed to identify the optimal interpolation embedding (as defined in Definition 3.9) and generate the corresponding video. The algorithm consists of three key steps:\n1. Compute the perpendicular foot embedding (Line 9 in Algorithm 3)."}, {"title": "5 Experiments", "content": "Experiments Setup. Our experiments are conducted on the CogVideoX-2B [YTZ+24]. We investigate the performance of our optimal embedding finder algorithm in the following two scenarios:\nMixture of Features from Two Initial Prompts. As outlined in Definition 3.9, we conduct experiments where the goal is to generate a mixture of features described in two text prompts, $P_a$ and $P_b$. We construct a third prompt, $P_c$, to specify the desired features. Following Algorithm 2, we identify the optimal text embedding and use it for the text-to-video generation with our base model. We conducted experiments using a variety of text prompts. In Fig. 3 and 4, we investigate the mixture of features from different animals, demonstrating that a video containing the mixture"}, {"title": "6 Discussion", "content": "Identifying the Actual Bottleneck of Generative Models. Our work identifies that the primary bottleneck hindering text-to-video generation models from producing the desired videos is the text encoder's inability to generate accurate text embeddings. Through our proposed algorithm, we can guide the video generation model to produce the desired output. This insight helps the community identify the true bottleneck within cutting-edge generative models, allowing for improvements in model performance and capabilities.\nLimitations and Future Work. As discussed in Section 5, our algorithm achieves impressive results on various prompts. But there are still some failure cases, which we analyze in Section C. The reason for these failures is that the optimal text embedding may not lie within the linear interpolation space, preventing our algorithm from identifying the correct embedding in such cases. Additionally, leveraging information from $E_{t_c}$ to derive the optimal embedding based on $E_{t_a}$ and $E_{t_b}$ is a non-trivial task. Therefore, we consider this an important direction for future work."}, {"title": "7 Conclusion", "content": "In this work, we propose a novel algorithm designed to identify the optimal embedding that enables a video generation model to produce videos containing features specified in the initial prompts, which indicates that the primary bottleneck in text-to-video generation is due to the text encoder's inability to generate accurate text embeddings. These insights can guide future improvements in model performance by identifying and addressing the bottleneck modules."}, {"title": "A Word Embedding Space being Insufficient to Represent for All Videos", "content": "In this section, we provide detailed proofs for Theorem A.6, showing that word embeddings are insufficient for representing all videos. We begin with a 1 dimensional case, where we assume all weights in function $f(x)$ are integers.\nLemma A.1 (Integer function bound in 1 dimension). If the following conditions hold:\n\u2022 Let $V \\in \\mathbb{N}$ denote a positive integer.\n\u2022 Let $f : [V]^n \\rightarrow \\mathbb{R}$ denote a linear function where weights are all integers.\n\u2022 Let $x \\in [V]^n$ denote the input of function $f$.\n\u2022 Let $M := \\text{maxx } f(x), m := \\text{minx } f(x)$.\n\u2022 Let $ \\epsilon = 0.5$.\nThen we can show there exits a scalar $y \\in [m, M]$ such that for any $x \\in [V]^n, |f(x) \u2013 y| \\geq \\epsilon$.\nProof. Since $x \\in [V]^n$, all entries of $x$ are integers. Since function $f$ is a linear function where all weights are integers, the output $f(x) \\in \\mathbb{Z}$ can only be integer.\nTherefore, $m, M \\in \\mathbb{Z}$. We choose $y = m + 0.5$. Since for all $f(x)$ are integers, then we have $|f(x) - y| \\geq 0.5$.\nThen, we extend the above Lemma to $d$ dimensional case."}, {"title": "Lemma A.2 (Integer function bound in d dimension). If the following conditions hold", "content": "\u2022 Let $V \\in \\mathbb{N}$ denote a positive integer.\n\u2022 Let $f : [V]^n \\rightarrow \\mathbb{R}^d$ denote a linear function where weights are all integers.\n\u2022 Let $x \\in [V]^n$ denote the input of function $f$.\n\u2022 Let $M := \\text{maxx } ||f(x)||_2, m := \\text{minx } ||f(x)||_2$.\n\u2022 Let $ \\epsilon = 0.5\\sqrt{d}$.\nThen we can show there exits a vector $y \\in \\mathbb{R}^d$, satisfying $m < ||y||_2 \\leq M$, such that for any $x \\in [V]^n, ||f(x) \u2013 y||_2 \\geq \\epsilon$.\nProof. Let $x_{min} \\in [V]^n$ denote the vector which satisfies $f(x_{min}) = m$. Since all entries in $x$ and $f$ are integers, all entries in $f(x_{min})$ are all integers.\nFor $i \\in [d]$, let $z_i \\in \\mathbb{Z}$ denote the i-th entry of $f(x_{min})$.\nThen, we choose the vector $y \\in \\mathbb{R}^d$ as\n$y = \\begin{bmatrix} z_1 + 0.5 \\\\ z_2 + 0.5 \\\\ \\vdots \\\\ z_d + 0.5 \\end{bmatrix}$\nThen, since all entries of $f(x)$ are integers, we have $||f(x) \u2013 y||_2 \\geq 0.5\\sqrt{d}$.\nThen, we move on to a more complicated case, in which we do not make any assumptions about the function $f(x)$. We still begin by considering the 1 dimensional case."}, {"title": "Lemma A.3 (Any function bound in 1 dimension). If the following conditions hold", "content": "\u2022 Let $V \\in \\mathbb{N}$ denote a positive integer.\n\u2022 Let $f : [V]^n \\rightarrow \\mathbb{R}$ denote a function.\n\u2022 Let $x \\in [V]^n$ denote the input of function $f$.\n\u2022 Let $M := \\text{maxx } f(x), m := \\text{minx } f(x)$.\n\u2022 Let $ \\epsilon = (M \u2013 m)/(2V^n)$.\nThen we can show there exits a scalar $y \\in [m, M]$ such that for any $x \\in [V]^n, |f(x) \u2013 y| \\geq \\epsilon$.\nProof. Assuming for all $y \\in [m, M]$, there exists one $f(x)$, such that $|f(x) - y| < (M - m)/(2V^n)$. The overall maximum cover of all $V^n$ points should satisfy\n$2 \\cdot V^n \\cdot |f(x) \u2013 y| < (M - m)$\nwhere the first step follows from there are total $V^n$ possible choices for $f(x)$, and each choice has a cover with length less than $2|f(x) - y|$.\nEq (1) indicates the overall cover of $V^n$ points can not cover all $[m, M]$ range. We use $S$ to denote the union of covers of all possible $f(x)$.\nSince the length of $S$ is less than $M \u2013 m$, there exists at least one $y$ lies in $[m, M] \\setminus S$ such that $|f(x) - y| \\geq (M \u2013 m)/(2V^n)$.\nThen, we complete our proof."}, {"title": "Here, we introduce an essential fact that stating the volume of a $l_2$-ball in $d$ dimensional space", "content": "Then, we extend our 1 dimensional result on any function $f(x)$ to $d$ dimensional cases.\nTheorem A.4 (Word embeddings are insufficient to represent for all videos, formal version of Theorem 4.1). If the following conditions hold:\n\u2022 Let $n, d$ denote two integers, where $n$ denotes the maximum length of the sentence, and all videos are in $\\mathbb{R}^d$ space.\n\u2022 Let $V \\in \\mathbb{N}$ denote the vocabulary size.\n\u2022 Let $U = {u_1, u_2, \\ldots, u_V}$ denote the word embedding space, where for $i \\in [V]$, the word embedding $u_i \\in \\mathbb{R}^k$.\n\u2022 Let $ \\delta_{min} = \\text{mini,j}\\in [V], i\\neq j ||u_i \u2013 u_j||_2$ denote the minimum $l_2$ distance of two word embedding.\n\u2022 Let $f : \\mathbb{R}^{nk} \\rightarrow \\mathbb{R}^d$ denote the mapping from sentence space (discrete space ${u_1, \\ldots, u_V}^n$) to video space $\\mathbb{R}^d$.\n\u2022 Let $M := \\text{maxx } ||f(x)||_2, m := \\text{minx } ||f(x)||_2$.\n\u2022 Let $ \\epsilon = ((M^d \u2013 m^d)/V^n)^{1/d}$.\nThen, we can show that there exits a video $y \\in \\mathbb{R}^d$, satisfying $m \\leq ||y||_2 \\leq M$, such that for any sentence $x \\in {u_1, u_2, \\ldots, u_V}^n, ||f(x) - y||_2 \\geq \\epsilon$.\nProof. Assuming for all $y$ satisfying $m \\leq ||y||_2 \\leq M$, there exists one $f(x)$, such that $|f(x) - y| < ((M^d \u2013 m^d)/V^n)^{1/d}$.\nThen, according to Fact 3.3, for each $f(x)$, the volume of its cover is $\\frac{\\pi^{d/2}}{(d/2)!}((\\frac{M^d-m^d}{V^n}))$. There are maximum total $V^n f(x)$, so the maximum volume of all covers is\n$V^n \\cdot \\frac{\\pi^{d/2}}{(d/2)!} ((\\frac{M^d - m^d}{V^n})) < \\frac{\\pi^{d/2}}{(d/2)!} (M^d - m^d)$\nwhich indicates the cover of all $V^n$ possible points does not cover the entire space for $y$.\nTherefore, there exists a $y$ satisfying $m \\leq ||y||_2 \\leq M$, such that $||f(x) - y||_2 \\geq ((M^d \u2013 m^d)/V^n)^{1/d}$.\nThen, we complete our proof.\nDefinition A.5 (Bi-Lipschitzness). We say a function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^d$ is L-bi-Lipschitz if for all $x, y \\in \\mathbb{R}$, we have\n$L^{-1}||x - y||_2 \\leq ||f(x) \u2212 f(y)||_2 \\leq L||x \u2212 y||_2$.\nThen, we state our main result as follows\nTheorem A.6 (Word embeddings are insufficient to represent for all videos, with Bi-Lipschitz condition). If the following conditions hold:"}, {"title": "Let n, d denote two integers, where n denotes the maximum length of the sentence, and all videos are in $ \\mathbb{R}^d$ space", "content": "\u2022 Let $V \\in \\mathbb{N}$ denote the vocabulary size."}, {"title": "More Examples", "content": "In this section, we will show more experimental results that the video generated directly from the guidance prompt does not exhibit the desired mixed features from the prompts. In Fig. 9, we provide an example of an \"age\" case. In Fig. 10, we provide an example of an \"animal\" case. In Fig. 11, we provide an example of a fruit case. In Fig. 12, we provide an example of the \"plant\" case."}, {"title": "C Failure Cases", "content": "In this section, we show our failure cases that the video generated directly from the guidance prompt can exhibit desired mixed features from the prompts. In Fig. 13, 14, 15, and 16, we provide examples on \"color\" case. In Fig. 17 and 18, we provide example on \"plant\" case. In Fig. 19, we provide an example for the \"landscape\u201d case."}, {"title": "D Full Algorithm", "content": "In this section, we provide the algorithm for 3D attention in Algorithm 4."}]}