{"title": "Diffusion Models as Optimizers for Efficient Planning in Offline RL", "authors": ["Renming Huang", "Yunqiang Pei", "Guoqing Wang", "Yangming Zhang", "Yang Yang", "Peng Wang", "Hengtao Shen"], "abstract": "Diffusion models have shown strong competitiveness in offline reinforcement learning tasks by formulating decision-making as sequential generation. However, the practicality of these methods is limited due to the lengthy inference processes they require. In this paper, we address this problem by decomposing the sampling process of diffusion models into two decoupled subprocesses: 1) generating a feasible trajectory, which is a time-consuming process, and 2) optimizing the trajectory. With this decomposition approach, we are able to partially separate efficiency and quality factors, enabling us to simultaneously gain efficiency advantages and ensure quality assurance. We propose the Trajectory Diffuser, which utilizes a faster autoregressive model to handle the generation of feasible trajectories while retaining the trajectory optimization process of diffusion models. This allows us to achieve more efficient planning without sacrificing capability. To evaluate the effectiveness and efficiency of the Trajectory Diffuser, we conduct experiments on the D4RL benchmarks. The results demonstrate that our method achieves 3-10x faster inference speed compared to previous sequence modeling methods, while also outperforming them in terms of overall performance.", "sections": [{"title": "Introduction", "content": "Offline Reinforcement Learning (Offline RL) is a data-driven RL paradigm concerned with learning exclusively from static datasets of previously collected experiences [23]. In this setting, a behavior policy interacts with the environment to collect a set of experiences, which later can be used to learn a policy without further interaction. This paradigm is extremely valuable in settings where online interaction is impractical, either because data collection is expensive or uncontrollable. Learning optimal policies within a limited dataset presents significant challenges, the policy needs to effectively handle out-of-distribution (OOD) data and possess the capability to generalize to novel environments that are not encountered within the offline dataset [23]. Over the last few years, diffusion models [28, 29, 37] have achieved remarkable success in the field of conditional generative modeling [35,37], showcasing their strong generalization abilities. This sparks significant interest in applying diffusion models to decision-making research. Previous studies [2, 17] involve training generative trajectory models on offline datasets and employing them to generate trajectories based on conditions, which guide the decision-making process. Furthermore, some methods [24] focus on improving diffusion models by generating synthetic data and utilizing reward signals to enhance the performance and adaptability on both known and unknown tasks.\nThe use of diffusion models for trajectory planning undoubtedly achieves significant success. However, these methods [2,17,24,27] suffer from the common challenge that limits their practicality: The diffusion models require lengthy denoising processes during inference, rendering them unsuitable for tasks with rapidly changing environments, such as autonomous driving. While there are attempts to address this problem in previous works [29,40], they often sacrifice the quality of sampling.\nAfter an in-depth investigation into the denoising process, we have identified that the diffusion model employed for trajectory planning can be effectively decomposed into two distinct steps. First, there is the generation of a feasible trajectory, where the agent continuously optimizes the trajectory over time using the diffusion model. This initial optimization takes a substantial amount of computing time, gradually refining the trajectory into a reasonable configuration. Subsequently, in the last few steps of the diffusion process, the model explicitly optimizes the trajectory towards a higher-reward target, as illustrated in Figure 1. This observation consolidates our understanding of how diffusion models operate in trajectory generation and motivates us to devise a more efficient approach to accelerate the denoising process. Our goal is to reduce the time required for the initial trajectory generation while offering a quality trajectory initialization to maintain or even improve the quality of the trajectory generated.\nMotivated by the above observation, we propose a method to accelerate trajectory generation without sacrificing sampling quality, named Trajectory Diffuser, as illustrated in Figure 2. To accelerate sampling, we opt to employ a faster and simpler method to replace the first step of the process mentioned earlier. Drawing inspiration from the Decision Transformer [4], we utilize a transformer [41] model with contextual awareness generating a high-quality initial trajectory in an autoregressive manner and enjoying higher efficiency compared to the diffusion models. To guarantee trajectory quality, despite the preservation of the diffusion model as the trajectory optimizer, only the final one-tenth of the denoising steps are leveraged as the optimization process. Additionally, we use conditional information to guide and improve the quality of the trajectory.\nIn summary, our contributions are three-fold:\nWe decompose the inference process of the diffusion model into two steps: 1) generate a feasible trajectory, and 2) optimize the feasible trajectory. This decomposition allows us to separately consider the efficiency and quality of the generative model.\nWe introduce Trajectory Diffuser, a novel offline reinforcement learning algorithm that not only mitigates the computational challenges of inference using diffusion models for trajectory planning but also delivers notable performance improvements.\nWe evaluate the efficiency and performance of Trajectory Diffuser on the D4RL benchmark tasks [9] for offline RL. Compared to previous methods based on diffusion models, our approach achieves a 3-10\u00d7 improvement in efficiency while surpassing their performance."}, {"title": "Related Work", "content": "Offline Reinforcement Learning Offline reinforcement learning is a technique that enables learning optimal policies from pre-collected datasets without requiring additional interaction. However, the distributional shift between the learned policy and data collection policy poses a significant challenge to improving performance [10, 43]. This disparity can cause overestimation of out-of-distribution behavior and inaccurate policy evaluations, leading to performance degradation [10, 22]. Recent research in offline RL has identified two categories of methods to address this challenge: model-free and model-based approaches. Model-free techniques impose constraints on the learned policy and value function to prevent inaccurate estimation of unseen behavior and quantify the robustness of out-of-distribution behavior by introducing uncertainty [21, 22]. In contrast, model-based methods propose RL algorithms that learn the optimal policy through planning or synthetic experience generated from a learned dynamic model [13, 26]. Despite a variety of techniques being proposed, the distributional shift challenge in offline RL remains an active research area, and the development of new algorithms and techniques is crucial for improving performance in practical environments.\nDiffusion Models in Reinforcement Learning Diffusion models have shown promising results in the learning of generative models for both image and text data [28, 29, 37]. The iterative denoising process can be interpreted as optimizing the gradient of a parameterized data distribution to match a scoring objective [40], leading to the formation of an energy model [8, 12, 30]. Diffusion-QL [42] utilizes a diffusion model for policy regularization, effectively enhancing the expressive power of the policy by integrating guidance from Q-learning. Recent approaches extend diffusion models to trajectory generation and demonstrate their ability to model data distributions effectively [2,17]. Furthermore, AdaptDiffuser [24] enhances the performance and adaptability of the diffusion model on known and unknown tasks by generating synthetic data and employing reward guidance. MetaDiffuser [27] leverages the diffusion model as a conditional planner to achieve generalization across diverse tasks. However, these previous methods involved reverse diffusion starting from random noise, which result in high computational costs in terms of time. This limitation restricted their practicality.\nTransformers in Reinforcement Learning Transformers [41] are a type of neural network architecture that has been successfully applied to various natural language processing [6,33] and computer vision tasks [3,14]. However, their application in reinforcement learning has been relatively unexplored due to higher variance in training. [36,45] have shown that augmenting transformers with relational reasoning and iterative self-attention can improve performance in combinatorial environments and enhance the utilization of episodic memories by RL agents. Moreover, new approaches that cast decision-making in RL as a sequence generation problem [4,11,17,46], such as Decision Transformer (DT) [4] and Trajectory Transformer (TT) [18], have shown great potential in improving RL performance. DT generates action based on historical state and reward token sequences, while TT discretizes such tokens and generates sequences using beam search. These developments indicate that transformers have the potential to be combined with RL and improve its performance."}, {"title": "Preliminaries", "content": "Offline RL In the field of sequential decision-making, the problem is formulated as a discounted Markov Decision Process defined by the tuple $M = \\langle \\rho_0, \\gamma, S, A, T, R\\rangle$, where $\\rho_0$ represents the initial state distribution, $S$ and $A$ are the state and action spaces, $T: S \\times A \\rightarrow S$ is the transition function, $R: S \\times A \\times S \\rightarrow \\mathbb{R}$ gives the reward at any transition, and $\\gamma \\in [0,1)$ is a discount factor [32].The agent acts according to a stochastic policy $\\pi: S \\rightarrow \\Delta A$, which generates a sequence of state-action-reward transitions, or trajectory $\\tau = (s_k, a_k,r_k)_{k\\geq 0}$, with a probability $p_{\\pi}(\\tau)$. The return $R(\\tau) = \\sum_{k>0} \\gamma^k r_k$ for a trajectory $\\tau$ is defined as the sum of discounted rewards obtained along the trajectory. The standard objective in reinforcement learning is to find a policy $\\pi^* = \\arg \\max_{\\pi} \\mathbb{E}_{\\tau \\sim p_{\\pi}} [R(\\tau)]$ that maximizes the expected return. This involves finding a return-maximizing policy from a fixed dataset of transitions collected by an unknown behavior policy $\\mu$ [23]. Solving Offline RL using sequence modeling can be viewed as a trajectory optimization problem [17]. The objective is to find an optimal sequence of actions $a_{0:T}^*$ that maximizes (or minimizes) an objective function $I$, which is computed based on per-timestep rewards (or costs) $r(s_t, a_t)$:\n$a_{0:T}^* = \\arg \\max_{a_{0:T}} I(s_0, a_{0:T}) = \\arg \\max_{a_{0:T}} \\sum_{t=0}^{T} r(s_t, a_t), $\nwhere the objective value of a trajectory is represented as $I(T)$.\nDiffusion Probabilistic Models Diffusion Probabilistic models [15,39] are a type of generative model that learns the data distribution $q(x)$ from a dataset $\\mathcal{D} := {x_i}_{0<i<M}$. It represents the process of generating data as an iterative denoising procedure, denoted"}, {"title": "Trajectory Diffuser", "content": "In this section, we introduce Trajectory Diffuser, which reformulates the problem by decomposing the trajectory planning process of diffusion models into two steps: generate a feasible trajectory and trajectory optimization. Such design allows us to simultaneously ensure efficiency and performance while striking a balance, and we accordingly design a model consisting of two parts. In Section 4.2, we employ a transformer-based autoregressive model to generate the feasible trajectory, thereby enhancing the inference speed. In Section 4.3, we retain the diffusion model for trajectory optimization, ensuring trajectory quality. The overview of our framework is summarized in the Figure 3.\nTrajectory Representation The primary criterion for the selection of a trajectory representation is its capacity to facilitate meaningful pattern learning by the transformer model [4,18]. Decision Transformer [4] provides us with a trajectory representation method with the capacity of learning to maximize returns-to-go $R_t = \\sum_{t'=t}^T \\gamma^{t'}r_{t'}$. In Decision Transformer, the last $K$ timesteps are fed into the Decision Transformer, for a total of $3K$ tokens (one for each modality: returns-to-go, state, or action). However, in the reinforcement learning setting, directly modeling actions suffers from some practical issues [17]. First, while states are typically continuous in RL, actions are more varied and are often discrete. Furthermore, sequences over actions, which are often represented as joint torques, tend to be more high-frequency and less smooth, making them much harder to predict and model [2]. Therefore, our trajectory planning model excludes actions, resulting in the following trajectory representation $\\tau$ being amenable to autoregressive training and generation:\n$\\tau = \\begin{bmatrix} s_1 \\\\ R_1 \\end{bmatrix} \\begin{bmatrix} s_2 \\\\ R_2 \\end{bmatrix} ... \\begin{bmatrix} s_T \\\\ R_T \\end{bmatrix} $\nActing with Inverse-Dynamics. Merely sampling states from a diffusion model falls short of establishing a comprehensive controller. An inverse dynamics model [1,31] takes two consecutive states as input and outputs the action that likely led to the transition from the first state to the second. Mathematically, this can be represented as follows: Given two consecutive states $s_t$ and $s_{t+1}$, the inverse dynamics model $f_{\\phi}$ with learnable parameters $\\phi$ predicts the action $a_t$ that led to the transition from $s_t$ to $s_{t+1}$:\n$a_t = f_{\\phi}(s_t, s_{t+1})$.\nGenerate Feasible Trajectory with Transformer Since trajectory has a natural ordering pattern, it is common to factorize the joint probabilities over symbols as the product of conditional probabilities. We train an autoregressive transformer model to predict the next state given the previous states and returns-to-go. The autoregressive transformer model takes as input a sequence of tokens of the form $(s_{t-h:t}, R_{t-h:t})$ and predicts the next state and expected reward $(s_{t+1}, R_{t+1})$, where $h$ stands for the history length. The model is trained to minimize the negative log-likelihood of the ground-truth sequence of states and rewards:\n$\\mathcal{L} = -\\sum_{t=1}^T \\log p(s_{t+1}, R_{t+1} | s_{t-h:t}, R_{t-h:t})$, where $p(s_{t+1}, R_{t+1} | s_{t-h:t}, R_{t-h:t})$ is the conditional probability distribution over the next state and expected reward given the previous states and rewards.\nTo generate a trajectory using the trained Transformer model, we can start with history states $s_{t-h:t}$ and expected returns-to-go $R_{t-h:t}$, We then repeatedly"}, {"title": "Diffusion based Trajectory Optimization", "content": "Notation: To clarify the distinction between the two types of timesteps in this work, we employ a notation convention. Specifically, we use superscripts $k \\in {1, ..., K}$ to represent timesteps associated with the diffusion process, and subscripts $t \\in {1, ...,T}$ to represent timesteps pertaining to the trajectory in reinforcement learning.\nThe diffusion model offers the advantage of effectively modeling multi-modal data [42]. To preserve this desirable characteristic, we train the diffusion model to learn the forward process of all diffusion timesteps. However, since our dataset comprises a collection of sub-optimal trajectories, the diffusion model may be affected by such sub-optimal behaviors. Hence, we incorporate condition guidance in the diffusion model. Specifically, we employ classifier-free guidance [16] to directly train a conditional diffusion model. This approach, in comparison to classifier guidance [7], avoids the need for dynamic programming.\nIn the standard reinforcement learning task, our objective is to devise a policy that maximizes reward. Therefore, we employ the returns-to-go metric derived from the offline dataset as the conditioning factor denoted by $y(t)$. It is noteworthy that our approach diverges from Diffuser [17] and Decision Diffuser [2], we employ cumulative reward beyond the current timestep t as the returns-to-go: $R(t) = \\sum_{t'>t} \\gamma^k r_t$. We consider that trajectories with lower overall rewards"}, {"title": "Experiments", "content": "In the experiment, conclusions are drawn that our method can improve inference speed without sacrificing performance as summarized in Figure 4. We also conduct ablation experiments to demonstrate the necessity of trajectory improvement and the tradeoff between performance and efficiency. Specifically, we aim to answer the following questions:\nHow much speed improvement can be achieved by Trajectory Diffuser compared to prior naive diffusion model based methods?\nDoes generating the feasible trajectory using an autoregressive model affect the model's performance?\nWhether our method is applicable to other regression models?\nIs the process of trajectory improvement necessary? Where is the balance point between efficiency and performance?\nDataset. We use the D4RL dataset [9], which provides benchmark datasets for reinforcement learning. We consider three different domains of tasks in D4RL, including Gym-locomotion, Adroit, and Kitchen."}, {"title": "Evaluation on the Efficiency", "content": "The inference time of the model in the control domain is also an important issue. Previous sequence models based on diffusion achieve significant improvements in performance. However, in each inference of the model, a reverse process of the diffusion model is performed, which significantly increases the inference time. In this section, we focus on evaluating the efficiency of Trajectory Diffuser.\nExperimental Setup. In order to ensure the validity of the comparisons, we conduct a comparative analysis among four trajectory planning models, including Diffuser [17], Trajectory Transformer (TT) [18], Decision Diffuser (DD) [2] and Adapt Diffuser [24]. The planning horizon and diffusion step for the trajectory planning are consistent with the hyperparameters provided in the original paper. We choose Gym-locomotion task as the testbed, we sample 1000 transitions by interacting with the environment and calculate the corresponding seconds-per-step (SPS) for inference speed.\nResults. We show the inference time of generating an action taken by prior methods and our method in Table 1. Taking DD as the baseline, we are able to attribute the performance boost to specific techniques proposed, as visualized in Figure 4. Specifically, compared to the transformer-based TT model, the TT model uses beam search to obtain the optimal trajectory, while our model utilizes a diffusion model to optimize the trajectory, resulting in more than 10x increase in inference speed. Compared to naive diffusion models including DD, Diffuser, and AdaptDiffuser, our method achieves an inference speedup of 3-10\u00d7."}, {"title": "Evaluation on the Effectiveness", "content": "We are interested in investigating whether the Trajectory Diffuser achieves inference speed improvements while potentially causing a decline in model performance. Thus we conduct an evaluation of Trajectory Diffuser in Gym-locomotion, Adroit and Kitchen tasks and compare its performance against previous sequence-modeling methods. Gym-locomotion results are shown in Table 2. The results of Adroit and Kitchen tasks are shown in Table 3. Additionally, in the Appendix, we compare our approach with other methods based on Temporal Difference Learning and model-based methods.\nBaselines. we compare our approach with Behavior Cloning (BC) and sequence modeling methods include the Decision Transformer [4], Trajectory Transformer [18], diffusion-based models include Diffuser [17], Decision Diffusion [2] and AdaptDiffuser [24]. To provide a comprehensive evaluation of our method's performance, in Appendix, we compare our method with several existing offline RL methods, including CQL [22], BRAC [43], IQL [21], Diffusion-QL [42], and MoReL [19]."}, {"title": "Compare with More Autoregressive Models", "content": "Our method introduces the utilization of autoregressive models for generating feasible trajectories. Traditional autoregressive models, such as GRU [5], LSTM [38], and Transformer [41], are commonly employed in this domain. Through the incorporation of diverse autoregressive models, we establish the robust generalization capabilities of our proposed framework. Furthermore, we empirically validate that leveraging the Transformer model for trajectory initialization yields superior outcomes within our approach."}, {"title": "Ablation Study of Trajectory Diffuser", "content": "We provide empirical evidence in support of utilizing a diffusion model for trajectory enhancement through rigorous experimentation. By conducting ablation experiments, we evaluate the influence of various optimization steps on performance, clearly demonstrating that the optimization process primarily takes"}, {"title": "Conclusion", "content": "In this paper, we propose a decomposition approach for the inference process of diffusion models, consisting of generating feasible trajectories and optimizing them. By utilizing a more efficient autoregressive model for trajectory generation and leveraging the capabilities of the diffusion model for trajectory optimization, we achieve significant improvements in inference speed while maintaining high-quality results. The experimental results demonstrate the impressive speed improvements of our method compared to traditional diffusion models. Additionally, our approach outperforms prior sequence modeling methods in terms of effectiveness. Benefiting from the improvement in both efficiency and accuracy, we look forward to our approach as a practical framework for decision-making tasks, and also an example design on how to apply the diffusion model for solving offline reinforcement learning problems."}, {"title": "Algorithm 1 Planning With Trajectory Diffuser (for continuous actions)", "content": "Input: history length L, imporve steps K, planning steps C, reward-to-go rtg, guidance scale w\nRequirements: sequence model transformer, Noise model ee and inverse dynamics fo\n1: Initialize h\u2190 Queue(length = L), r \u2190 Queue(length = L), t \u2190 0  Maintain a history of length C\n2: while not done do\n3: Observe state st; h.insert(st);\n4: Returns-to-go rtg; r.insert(rtg);\n5: // Planning stage\n6: extract history states: states \u2190 h\n7: extract history rtgs: rtgs \u2190 r\n8: for c= C... 1 do\n9: next_state, next_rtg = transformer(states[-L:], rtgs[-L:])\n10: append next_state to states\n11: append next_rtg to rtgs\n12: end for\n13: // Improve stage\n14: if improve stage then\n15: Initialize K(T) \u2190 AddNoise(states[t : t + c])\n16: for k = K... 1 do\n17: K(T)[0] \u2190 states [t]  Constrain plan to be consistent with current state\n18: \u20ac \u2190 \u20ac\u03b8 (K(T), k) + \u03c9(\u03b5\u03b8(K(t), y, k) \u2013 \u20ac\u03b8(K(T), k))  Classifier-free guidance\n19: (\u03bc\u03ba\u22121, \u03a3\u03ba\u22121) \u2190 DeNoise(xk(T), \u20ac)\n20: K\u22121 ~ \u039d(\u03bc\u03ba\u22121, \u03a3\u03ba\u22121)\n21: end for\n22: states[t: t + c] \u2190 K0(T)\n23: end if\n24: Extract(St, St+1) from states\n25: Execute at \u2190 f(St, St+1); t \u2190t+1\n26: end while"}, {"title": "A.3 Pseudocode of Trajectory Diffuser", "content": "Pseudocode for trajectory planning with Trajectory Diffuser is shown in Algorithm 1.\nTo utilize the Trajectory Diffuser for planning purposes, an initialization step involves establishing two queues responsible for maintaining a record of historical states and rewards. Subsequently, a transformer model is employed to perceive the historical states, while trajectory planning is carried out through an autoregressive prediction technique. It is worth noting that the autoregressive approach is prone to error accumulation, leading to the emergence of suboptimal trajectories and out-of-distribution scenarios. In order to mitigate these challenges, a diffusion model is leveraged to capture the underlying distribution of trajectories, enabling their subsequent adjustment towards the optimal tra-"}, {"title": "Analyse From the Frequency Perspective", "content": "We first validate our assumption through a bandit case: The diffusion sampling process undergoes optimization in the final few steps (Figure A1(a)), which can be used as an optimizer to improve the results of other methods (e.g., BC-CVAE) (Figure A1(b)). Based on these assumptions, we explain how our method achieves performance advantages from a frequency perspective (Figure A2). In a 1D trajectory planning task, the train data consists of mixed frequency trajectories, planning with the transformer involves interfering frequencies, but the primary frequency is correct. By incorporating a small number of diffusion steps for optimization, the interference frequencies are significantly reduced while maintaining the primary frequency. We can conclude that introducing the diffusion model as an optimizer does not change the main distribution of the trajectory but helps obtain the optimal solution through optimization. These two experiments demonstrate that our method simultaneously benefits from both efficiency and performance. The improvement in efficiency comes from a significant reduction in sampling steps, while the performance is enhanced by introducing the transformer for good initialization and combining it with the diffusion model optimization."}, {"title": "More Discussion about Limitations", "content": "We summarize some potential limitations of Trajectory Diffuser:\nIt circumvents exploration by focusing on offline sequential decision-making, but online fine-tuning of the Trajectory Diffuser could be incorporated to enable exploration using state-sequence model entropy.\nThe model's applicability is currently limited to state-based environments, but extending it to image-based environments by performing diffusion in the latent space could enhance its capabilities.\nThe diffusion models, including the Trajectory Diffuser, are prone to overfitting in scenarios with limited data, potentially limiting their effectiveness in certain applications.\nWhile we proposed using the diffusion model as an optimizer for trajectory regulation, we did not redesign the structure of the diffusion model."}, {"title": "Additional Experiment Results", "content": "In this section, We present a comprehensive comparison with previous methods"}]}