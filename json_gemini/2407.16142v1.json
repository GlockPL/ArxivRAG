{"title": "Diffusion Models as Optimizers for Efficient Planning in Offline RL", "authors": ["Renming Huang", "Yunqiang Pei", "Guoqing Wang", "Yangming Zhang", "Yang Yang", "Peng Wang", "Hengtao Shen"], "abstract": "Diffusion models have shown strong competitiveness in offline reinforcement learning tasks by formulating decision-making as sequential generation. However, the practicality of these methods is limited due to the lengthy inference processes they require. In this paper, we address this problem by decomposing the sampling process of diffusion models into two decoupled subprocesses: 1) generating a feasible trajectory, which is a time-consuming process, and 2) optimizing the trajectory. With this decomposition approach, we are able to partially separate efficiency and quality factors, enabling us to simultaneously gain efficiency advantages and ensure quality assurance. We propose the Trajectory Diffuser, which utilizes a faster autoregressive model to handle the generation of feasible trajectories while retaining the trajectory optimization process of diffusion models. This allows us to achieve more efficient planning without sacrificing capability. To evaluate the effectiveness and efficiency of the Trajectory Diffuser, we conduct experiments on the D4RL benchmarks. The results demonstrate that our method achieves 3-10x faster inference speed compared to previous sequence modeling methods, while also outperforming them in terms of overall performance.", "sections": [{"title": "1 Introduction", "content": "Offline Reinforcement Learning (Offline RL) is a data-driven RL paradigm concerned with learning exclusively from static datasets of previously collected experiences [23]. In this setting, a behavior policy interacts with the environment to collect a set of experiences, which later can be used to learn a policy without further interaction. This paradigm is extremely valuable in settings where online interaction is impractical, either because data collection is expensive or uncontrollable. Learning optimal policies within a limited dataset presents significant challenges, the policy needs to effectively handle out-of-distribution (OOD) data and possess the capability to generalize to novel environments that are not encountered within the offline dataset [23]. Over the last few years, diffusion models [28, 29, 37] have achieved remarkable success in the field of conditional generative modeling [35,37], showcasing their strong generalization abilities. This sparks significant interest in applying diffusion models to decision-making research. Previous studies [2, 17] involve training generative trajectory models on offline datasets and employing them to generate trajectories based on conditions, which guide the decision-making process. Furthermore, some methods [24] focus on improving diffusion models by generating synthetic data and utilizing reward signals to enhance the performance and adaptability on both known and unknown tasks.\nThe use of diffusion models for trajectory planning undoubtedly achieves significant success. However, these methods [2,17,24,27] suffer from the common challenge that limits their practicality: The diffusion models require lengthy denoising processes during inference, rendering them unsuitable for tasks with rapidly changing environments, such as autonomous driving. While there are attempts to address this problem in previous works [29,40], they often sacrifice the quality of sampling.\nAfter an in-depth investigation into the denoising process, we have identified that the diffusion model employed for trajectory planning can be effectively decomposed into two distinct steps. First, there is the generation of a feasible trajectory, where the agent continuously optimizes the trajectory over time using the diffusion model. This initial optimization takes a substantial amount of computing time, gradually refining the trajectory into a reasonable configuration. Subsequently, in the last few steps of the diffusion process, the model explicitly optimizes the trajectory towards a higher-reward target, as illustrated in Figure 1. This observation consolidates our understanding of how diffusion models operate in trajectory generation and motivates us to devise a more efficient approach to accelerate the denoising process. Our goal is to reduce the time required for the initial trajectory generation while offering a quality trajectory initialization to maintain or even improve the quality of the trajectory generated.\nMotivated by the above observation, we propose a method to accelerate trajectory generation without sacrificing sampling quality, named Trajectory Diffuser, as illustrated in Figure 2. To accelerate sampling, we opt to employ a faster and simpler method to replace the first step of the process mentioned earlier. Drawing inspiration from the Decision Transformer [4], we utilize a transformer [41] model with contextual awareness generating a high-quality initial trajectory in an autoregressive manner and enjoying higher efficiency compared to the diffusion models. To guarantee trajectory quality, despite the preservation of the diffusion model as the trajectory optimizer, only the final one-tenth of the denoising steps are leveraged as the optimization process. Additionally, we use conditional information to guide and improve the quality of the trajectory.\nIn summary, our contributions are three-fold:\nWe decompose the inference process of the diffusion model into two steps: 1) generate a feasible trajectory, and 2) optimize the feasible trajectory. This decomposition allows us to separately consider the efficiency and quality of the generative model.\nWe introduce Trajectory Diffuser, a novel offline reinforcement learning algorithm that not only mitigates the computational challenges of inference using diffusion models for trajectory planning but also delivers notable performance improvements.\nWe evaluate the efficiency and performance of Trajectory Diffuser on the D4RL benchmark tasks [9] for offline RL. Compared to previous methods based on diffusion models, our approach achieves a 3-10\u00d7 improvement in efficiency while surpassing their performance."}, {"title": "2 Related Work", "content": "2.1 Offline Reinforcement Learning\nOffline reinforcement learning is a technique that enables learning optimal policies from pre-collected datasets without requiring additional interaction. However, the distributional shift between the learned policy and data collection policy poses a significant challenge to improving performance [10, 43]. This disparity can cause overestimation of out-of-distribution behavior and inaccurate policy evaluations, leading to performance degradation [10, 22]. Recent research in offline RL has identified two categories of methods to address this challenge: model-free and model-based approaches. Model-free techniques impose constraints on the learned policy and value function to prevent inaccurate estimation of unseen behavior and quantify the robustness of out-of-distribution behavior by introducing uncertainty [21, 22]. In contrast, model-based methods propose RL algorithms that learn the optimal policy through planning or synthetic experience generated from a learned dynamic model [13, 26]. Despite a variety of techniques being proposed, the distributional shift challenge in offline RL remains an active research area, and the development of new algorithms and techniques is crucial for improving performance in practical environments.\n2.2 Diffusion Models in Reinforcement Learning\nDiffusion models have shown promising results in the learning of generative models for both image and text data [28, 29, 37]. The iterative denoising process can be interpreted as optimizing the gradient of a parameterized data distribution to match a scoring objective [40], leading to the formation of an energy model [8, 12, 30]. Diffusion-QL [42] utilizes a diffusion model for policy regularization, effectively enhancing the expressive power of the policy by integrating guidance from Q-learning. Recent approaches extend diffusion models to trajectory generation and demonstrate their ability to model data distributions effectively [2,17]. Furthermore, AdaptDiffuser [24] enhances the performance and adaptability of the diffusion model on known and unknown tasks by generating synthetic data and employing reward guidance. MetaDiffuser [27] leverages the diffusion model as a conditional planner to achieve generalization across diverse tasks. However, these previous methods involved reverse diffusion starting from random noise, which result in high computational costs in terms of time. This limitation restricted their practicality.\n2.3 Transformers in Reinforcement Learning\nTransformers [41] are a type of neural network architecture that has been successfully applied to various natural language processing [6,33] and computer vision tasks [3,14]. However, their application in reinforcement learning has been relatively unexplored due to higher variance in training. [36,45] have shown that augmenting transformers with relational reasoning and iterative self-attention can improve performance in combinatorial environments and enhance the utilization of episodic memories by RL agents. Moreover, new approaches that cast decision-making in RL as a sequence generation problem [4,11,17,46], such as Decision Transformer (DT) [4] and Trajectory Transformer (TT) [18], have shown great potential in improving RL performance. DT generates action based on historical state and reward token sequences, while TT discretizes such tokens and generates sequences using beam search. These developments indicate that transformers have the potential to be combined with RL and improve its performance."}, {"title": "3 Preliminaries", "content": "3.1 Offline RL\nIn the field of sequential decision-making, the problem is formulated as a discounted Markov Decision Process defined by the tuple M = \u3008\u03c10, \u03b3, S, A, T, R\u3009, where \u03c10 represents the initial state distribution, S and A are the state and action spaces, T : S \u00d7 A \u2192 S is the transition function, R: S \u00d7 A \u00d7 S \u2192 R gives the reward at any transition, and \u03b3 \u2208 [0, 1) is a discount factor [32].The agent acts according to a stochastic policy \u03c0 : S \u2192 \\DeltaA, which generates a sequence of state-action-reward transitions, or trajectory \u03c4 = (sk, ak, rk)k\u22650, with a probability p\u03c0(\u03c4). The return R(\u03c4) = \\sumk>0 \u03b3krk for a trajectory \u03c4 is defined as the sum of discounted rewards obtained along the trajectory. The standard objective in reinforcement learning is to find a policy \u03c0\u2217 = arg max\u03c0 E\u03c4\u223cp\u03c0 [R(\u03c4)] that maximizes the expected return. This involves finding a return-maximizing policy from a fixed dataset of transitions collected by an unknown behavior policy \u00b5 [23]. Solving Offline RL using sequence modeling can be viewed as a trajectory optimization problem [17]. The objective is to find an optimal sequence of actions a0:T that maximizes (or minimizes) an objective function I, which is computed based on per-timestep rewards (or costs) r(st, at):\na*0:T = argmaxa0:T I(s0, a0:T) = argmaxa0:T \u2211Tt=0 r(st, at), (1)\nthe objective value of a trajectory is represented as I(T).\n3.2 Diffusion Probabilistic Models\nDiffusion Probabilistic models [15,39] are a type of generative model that learns the data distribution q(x) from a dataset D := {xi}0<i<M. It represents the process of generating data as an iterative denoising procedure, denoted"}, {"title": "4 Trajectory Diffuser", "content": "In this section, we introduce Trajectory Diffuser, which reformulates the problem by decomposing the trajectory planning process of diffusion models into two steps: generate a feasible trajectory and trajectory optimization. Such design allows us to simultaneously ensure efficiency and performance while striking a balance, and we accordingly design a model consisting of two parts. In Section 4.2, we employ a transformer-based autoregressive model to generate the feasible trajectory, thereby enhancing the inference speed. In Section 4.3, we retain the diffusion model for trajectory optimization, ensuring trajectory quality. The overview of our framework is summarized in the Figure 3.\n4.1 Trajectory Representation\nThe primary criterion for the selection of a trajectory representation is its capacity to facilitate meaningful pattern learning by the transformer model [4,18]. Decision Transformer [4] provides us with a trajectory representation method with the capacity of learning to maximize returns-to-go Rt = \u2211Tt'=t \u03b3t\u2032\u2212trt\u2032. In Decision Transformer, the last K timesteps are fed into the Decision Transformer, for a total of 3K tokens (one for each modality: returns-to-go, state, or action). However, in the reinforcement learning setting, directly modeling actions suffers from some practical issues [17]. First, while states are typically continuous in RL, actions are more varied and are often discrete. Furthermore, sequences over actions, which are often represented as joint torques, tend to be more high-frequency and less smooth, making them much harder to predict and model [2]. Therefore, our trajectory planning model excludes actions, resulting in the following trajectory representation \u03c4 being amenable to autoregressive training and generation:\n\u03c4 =( s1R1,s2R2,...,sTsT), (2)\nActing with Inverse-Dynamics. Merely sampling states from a diffusion model falls short of establishing a comprehensive controller. An inverse dynamics model [1,31] takes two consecutive states as input and outputs the action that likely led to the transition from the first state to the second. Mathematically, this can be represented as follows: Given two consecutive states st and st+1, the inverse dynamics model f\u03b8 with learnable parameters \u03b8 predicts the action at that led to the transition from st to st+1:\nat = f\u03b8(st, st+1). (3)\n4.2 Generate Feasible Trajectory with Transformer\nSince trajectory has a natural ordering pattern, it is common to factorize the joint probabilities over symbols as the product of conditional probabilities. We train an autoregressive transformer model to predict the next state given the previous states and returns-to-go. The autoregressive transformer model takes as input a sequence of tokens of the form (st\u2212h:t, Rt\u2212h:t) and predicts the next state and expected reward (st+1, Rt+1), where h stands for the history length. The model is trained to minimize the negative log-likelihood of the ground-truth sequence of states and rewards:\nL = - \u2211Tt=1 log p(st+1, Rt+1|st\u2212h:t, Rt\u2212h:t), (4)\nwhere p(st+1, Rt+1|st\u2212h:t, Rt\u2212h:t) is the conditional probability distribution over the next state and expected reward given the previous states and rewards.\nTo generate a trajectory using the trained Transformer model, we can start with history states st\u2212h:t and expected returns-to-go Rt\u2212h:t, We then repeatedly sample I states and returns-to-go from the distribution generated by the model:\n(st+1, Rt+1) \u223c p(st+1, Rt+1|st\u2212h:t, Rt\u2212h:t). (5)\nArchitecture. We employ the last H timesteps, which comprise two types of information: state and returns-to-go. Unlike the decision transformer approach, we do not treat state and returns-to-go as separate tokens. Instead, we consider them as a unified entity, implicitly binding returns-to-go with state. In order to obtain token embeddings, we employ a training process that involves training distinct linear layers for each modality. These linear layers are responsible for converting the raw inputs into embeddings of the desired dimension. Subsequently, layer normalization is applied to ensure appropriate normalization of the embeddings. The processed tokens are then fed into a GPT model [34], which utilizes autoregressive modeling to predict future states.\nTraining. We are provided with a dataset D of offline trajectories, we sample mini-batches of sequence length K from the dataset. To prevent information leakage, we apply an upper triangular mask matrix to mask out all information beyond the current token. The prediction head associated with the input token [st, Rt] is trained to predict the next token, which consists of two components: the next timestep\u2019s state st+1 and reward-to-go Rt+1. We employ two separate linear layers to predict the next state and reward-to-go for each timestep, respectively. The losses for each timestep are then averaged. Benefited from our bouding constraint on the state and returns-to-go together, improved results are then observed when simultaneously predicting the state and returns-to-go.\n4.3 Diffusion based Trajectory Optimization\nNotation: To clarify the distinction between the two types of timesteps in this work, we employ a notation convention. Specifically, we use superscripts k \u2208 {1, ..., K} to represent timesteps associated with the diffusion process, and subscripts t \u2208 {1, ...,T} to represent timesteps pertaining to the trajectory in reinforcement learning.\nThe diffusion model offers the advantage of effectively modeling multi-modal data [42]. To preserve this desirable characteristic, we train the diffusion model to learn the forward process of all diffusion timesteps. However, since our dataset comprises a collection of sub-optimal trajectories, the diffusion model may be affected by such sub-optimal behaviors. Hence, we incorporate condition guidance in the diffusion model. Specifically, we employ classifier-free guidance [16] to directly train a conditional diffusion model. This approach, in comparison to classifier guidance [7], avoids the need for dynamic programming.\nIn the standard reinforcement learning task, our objective is to devise a policy that maximizes reward. Therefore, we employ the returns-to-go metric derived from the offline dataset as the conditioning factor denoted by y(t). It is noteworthy that our approach diverges from Diffuser [17] and Decision Diffuser [2], we employ cumulative reward beyond the current timestept as the returns-to-go: R(t) = \u2211t'>t \u03b3krt. We consider that trajectories with lower overall rewards still contain locally better trajectories. This option ensures that our diffusion model is not confined solely to the optimal trajectories. Formally, we train our conditional generative model in a supervised manner. Given a trajectory \u03c4 \u2208 D from the dataset D, represented as \u03c4 = (st, st+1,..., sT ), it is labeled with the returns-to-go Rt which achieves starting from the current timestep. For each trajectory \u03c4 and condition y(t), we start by sampling noise \u03f5 from a standard normal distribution N(0, I), and a timestep k uniformly from the set {1, ..., K}. Then, we construct a noisy array of states xk(\u03c4) and predict the noise as \u03f5\u03b8 = \u03f5\u03b8(xk(\u03c4), y(t), k), using the reverse diffusion process p\u03b8 parameterized by the noise model \u03b8. The model is trained using the following loss function:\nL(\u03b8) = Ek,\u03c4\u2208D,\u03b7\u223cBern(p)[| \u03f5 - \u03f5\u03b8(xk(\u03c4), \u0177(\u03c4, \u03b7), k)|2], (6)\nHere, \u0177(\u03c4, \u03b7) = (1 \u2212 \u03b7)y(\u03c4) + \u03b7), Bern(p) represents a Bernoulli distribution with probability p, defined as:\nBern(p) = 1, p\n0, 1-p\nDuring sampling, we optimize our trajectory through the reverse process of a conditional diffusion model, given by\np\u03b8(xo(\u03c4)|y(\u03c4)) = p\u03b8(xo:k(\u03c4)|y(\u03c4)), (8)\nThe final trajectory of the reverse chain, x1(\u03c4), represents the improved trajectory. Generally, p\u03b8(xk\u22121|xk, y(t)) can be modeled as a Gaussian distribution:\np\u03b8(xk\u22121|xk, y(\u03c4)) = N(xk\u22121; \u00b5\u03b8(xk, y(\u03c4), k), \u03a3\u03b8(xk, y(\u03c4), k)), (9)\nWe follow [15] to parameterize p\u03b8(xk\u22121(\u03c4)|xk(\u03c4)) as a noise prediction model, with the covariance matrix fixed as \u03a3\u03b8(xk(\u03c4),y(\u03c4),k) = \u03b2kI, and the mean constructed as:\n\u00b5\u03b8(xk(\u03c4), y(\u03c4), k) = 1/\u221a\u03b1k (xk(\u03c4) \u2212 \u03b2k/\u221a1\u2212\u03b1k \u03f5\u03b8(xk(\u03c4), y(\u03c4), k)), (10)\nwhere \u03f5\u03b8(xk(\u03c4), y(t), k)) represents perturbed noise, following [16]. It is calculated as:\n\u03f5\u03b8(xk(\u03c4), y(\u03c4), k)) = \u03f5\u03b8(xk(\u03c4), \u00f8, k) + \u03c9 (\u03f5\u03b8(xk(t), y(t), k) \u2212 \u03f5\u03b8(xk(\u03c4), \u00f8, k)), (11)\nThe scalar \u03c9 is applied to the term \u03f5\u03b8(xk(\u03c4), y(\u03c4), k) \u2212 \u03f5\u03b8(xk(\u03c4), \u00f8, k), aiming to enhance and extract the most valuable parts of trajectories in the dataset that exhibit y(\u03c4).\nWith these components, trajectory optimization becomes similar to sampling from the diffusion model. First, we obtain a trajectory \u03c4, represented as xk(\u03c4) with k < K. Next, we sample states starting from timestep k using the diffusion process conditioned on y(t). Finally, we determine the action that should be taken to reach the predicted state with our inverse dynamics model. The process is iterated within a conventional receding-horizon control loop, as outlined in Algorithm 1."}, {"title": "5 Experiments", "content": "In the experiment, conclusions are drawn that our method can improve inference speed without sacrificing performance as summarized in Figure 4. We also conduct ablation experiments to demonstrate the necessity of trajectory improvement and the tradeoff between performance and efficiency. Specifically, we aim to answer the following questions:\nHow much speed improvement can be achieved by Trajectory Diffuser compared to prior naive diffusion model based methods?\nDoes generating the feasible trajectory using an autoregressive model affect the model\u2019s performance?\nWhether our method is applicable to other regression models?\nIs the process of trajectory improvement necessary? Where is the balance point between efficiency and performance?\nDataset. We use the D4RL dataset [9], which provides benchmark datasets for reinforcement learning. We consider three different domains of tasks in D4RL, including Gym-locomotion, Adroit, and Kitchen.\n5.1 Evaluation on the Efficiency\nThe inference time of the model in the control domain is also an important issue. Previous sequence models based on diffusion achieve significant improvements in performance. However, in each inference of the model, a reverse process of the diffusion model is performed, which significantly increases the inference time. In this section, we focus on evaluating the efficiency of Trajectory Diffuser.\nExperimental Setup. In order to ensure the validity of the comparisons, we conduct a comparative analysis among four trajectory planning models, including Diffuser [17], Trajectory Transformer (TT) [18], Decision Diffuser (DD) [2] and Adapt Diffuser [24]. The planning horizon and diffusion step for the trajectory planning are consistent with the hyperparameters provided in the original paper. We choose Gym-locomotion task as the testbed, we sample 1000 transitions by interacting with the environment and calculate the corresponding seconds-per-step (SPS) for inference speed.\nResults. We show the inference time of generating an action taken by prior methods and our method in Table 1. Taking DD as the baseline, we are able to attribute the performance boost to specific techniques proposed, as visualized in Figure 4. Specifically, compared to the transformer-based TT model, the TT model uses beam search to obtain the optimal trajectory, while our model utilizes a diffusion model to optimize the trajectory, resulting in more than 10x increase in inference speed. Compared to naive diffusion models including DD, Diffuser, and AdaptDiffuser, our method achieves an inference speedup of 3-10\u00d7.\n5.2 Evaluation on the Effectiveness\nWe are interested in investigating whether the Trajectory Diffuser achieves inference speed improvements while potentially causing a decline in model performance. Thus we conduct an evaluation of Trajectory Diffuser in Gym-locomotion, Adroit and Kitchen tasks and compare its performance against previous sequence-modeling methods. Gym-locomotion results are shown in Table 2. The results of Adroit and Kitchen tasks are shown in Table 3. Additionally, in the Appendix, we compare our approach with other methods based on Temporal Difference Learning and model-based methods.\nBaselines. we compare our approach with Behavior Cloning (BC) and sequence modeling methods include the Decision Transformer [4], Trajectory Transformer [18], diffusion-based models include Diffuser [17], Decision Diffusion [2] and AdaptDiffuser [24]. To provide a comprehensive evaluation of our method\u2019s performance, in Appendix, we compare our method with several existing offline RL methods, including CQL [22], BRAC [43], IQL [21], Diffusion-QL [42], and MoReL [19].\nExperimental Setup. We independently train a sequence model based on the transformer architecture, incorporating a state diffusion process and an inverse dynamics model. This training is carried out using publicly available D4RL datasets. We train the model for 200k iterations (100k iterations for Adroit tasks and Kitchen tasks) with a batch size of 32. The specific hyperparameters varied across different datasets and can be found in the supplementary material.\nResults. The experimental results in Table 2 and Table 3 demonstrate that our method achieves state-of-the-art performance among sequence-based models across a diverse range of offline reinforcement learning tasks. Particularly, in the more challenging D4RL Adroit and Kitchen tasks, which require strong policy regularization to overcome extrapolation errors [10] and complex credit assignment over long periods, our method outperforms Decision Diffusion and exhibits significant advantages.\n5.3 Compare with More Autoregressive Models\nOur method introduces the utilization of autoregressive models for generating feasible trajectories. Traditional autoregressive models, such as GRU [5], LSTM [38], and Transformer [41], are commonly employed in this domain. Through the incorporation of diverse autoregressive models, we establish the robust generalization capabilities of our proposed framework. Furthermore, we empirically validate that leveraging the Transformer model for trajectory initialization yields superior outcomes within our approach.\n5.4 Ablation Study of Trajectory Diffuser\nWe provide empirical evidence in support of utilizing a diffusion model for trajectory enhancement through rigorous experimentation. By conducting ablation experiments, we evaluate the influence of various optimization steps on performance, clearly demonstrating that the optimization process primarily takes"}, {"title": "6 Conclusion", "content": "In this paper, we propose a decomposition approach for the inference process of diffusion models, consisting of generating feasible trajectories and optimizing them. By utilizing a more efficient autoregressive model for trajectory generation and leveraging the capabilities of the diffusion model for trajectory optimization, we achieve significant improvements in inference speed while maintaining high-quality results. The experimental results demonstrate the impressive speed improvements of our method compared to traditional diffusion models. Additionally, our approach outperforms prior sequence modeling methods in terms of effectiveness. Benefiting from the improvement in both efficiency and accuracy, we look forward to our approach as a practical framework for decision-making tasks, and also an example design on how to apply the diffusion model for solving offline reinforcement learning problems."}, {"title": "A.1 Analyse From the Frequency Perspective", "content": "We first validate our assumption through a bandit case: The diffusion sampling process undergoes optimization in the final few steps (Figure A1(a)), which can be used as an optimizer to improve the results of other methods (e.g., BC-CVAE) (Figure A1(b)). Based on these assumptions, we explain how our method achieves performance advantages from a frequency perspective (Figure A2). In a 1D trajectory planning task, the train data consists of mixed frequency trajectories, planning with the transformer involves interfering frequencies, but the primary frequency is correct. By incorporating a small number of diffusion steps for optimization, the interference frequencies are significantly reduced while maintaining the primary frequency. We can conclude that introducing the diffusion model as an optimizer does not change the main distribution of the trajectory but helps obtain the optimal solution through optimization. These two experiments demonstrate that our method simultaneously benefits from both efficiency and performance. The improvement in efficiency comes from a significant reduction in sampling steps, while the performance is enhanced by introducing the transformer for good initialization and combining it with the diffusion model optimization."}, {"title": "A.2 Additional Experiment Results", "content": "In this section, We present a comprehensive comparison with previous methods (Table A1). We provide additional experimental results (Table A5), including"}, {"title": "A.3 Pseudocode of Trajectory Diffuser", "content": "Pseudocode for trajectory planning with Trajectory Diffuser is shown in Algorithm 1.\nTo utilize the Trajectory Diffuser for planning purposes, an initialization step involves establishing two queues responsible for maintaining a record of historical states and rewards. Subsequently, a transformer model is employed to perceive the historical states, while trajectory planning is carried out through an autoregressive prediction technique. It is worth noting that the autoregressive approach is prone to error accumulation, leading to the emergence of suboptimal trajectories and out-of-distribution scenarios. In order to mitigate these challenges, a diffusion model is leveraged to capture the underlying distribution of trajectories, enabling their subsequent adjustment towards the optimal tra-"}, {"title": "A.4 Implementation Details", "content": "In this section, we describe various architectural and hyperparameter details:\nFor the transformer model, we employ 4 transformer block layers and 4 attention heads. The embedding dimension is set to 256. Finally, we have utilized the Rectified Linear Unit (ReLU) activation function as the nonlinear activation layer.\nWe represent the noise model @ with a temporal U-Net [17], consisting of a U-Net structure with 6 repeated residual blocks. Each block consisted of two temporal convolutions, each followed by a group norm [44], and a final Mish nonlinearity [25]. Timestep and condition embeddings, both 64-dimensional vectors, are produced by separate 2-layered MLP (with 128 hidden units and Mish nonlinearity) and are concatenated together before getting added to the activations of the first temporal convolution within each block.\nWe represent the inverse dynamics f\u03b8 with a 2-layered MLP with 256 hidden units and ReLU activations.\nWe train @ and f\u03b8 using the Adam optimizer [20] and batch size of 32 for 2e6 train steps.\nWe choose the probability p of removing the conditioning information to be 0.5."}, {"title": "A.5 More Discussion about Limitations", "content": "We summarize some potential limitations of Trajectory Diffuser:\nIt circumvents exploration by focusing on offline sequential decision-making, but online fine-tuning of the Trajectory Diffuser could be incorporated to enable exploration using state-sequence model entropy.\nThe model's applicability is currently limited to state-based environments, but extending it to image-based environments by performing diffusion in the latent space could enhance its capabilities.\nThe diffusion models, including the Trajectory Diffuser, are prone to overfitting in scenarios with limited data, potentially limiting their effectiveness in certain applications.\nWhile we proposed using the diffusion model as an optimizer for trajectory regulation, we did not redesign the structure of the diffusion model."}]}