{"title": "Mitigating Domain Shift in Federated Learning via Intra- and Inter-Domain Prototypes", "authors": ["Huy Q. Le", "Ye Lin Tun", "Yu Qiao", "Minh N. H. Nguyen", "Keon Oh Kim", "Choong Seon Hong", "Kyung Hee University", "Vietnam-Korea University of Information and Communication Technology"], "abstract": "Federated Learning (FL) has emerged as a decentralized machine learning technique, allowing clients to train a global model collaboratively without sharing private data. However, most FL studies ignore the crucial challenge of heterogeneous domains where each client has a distinct feature distribution, which is common in real-world scenarios. Prototype learning, which leverages the mean feature vectors within the same classes, has become a prominent solution for federated learning under domain skew. However, existing federated prototype learning methods only consider inter-domain prototypes on the server and overlook intra-domain characteristics. In this work, we introduce a novel federated prototype learning method, namely I\u00b2PFL, which incorporates Intra-domain and Inter-domain Prototypes, to mitigate domain shifts and learn a generalized global model across multiple domains in federated learning. To construct intra-domain prototypes, we propose feature alignment with MixUp-based augmented prototypes to capture the diversity of local domains and enhance the generalization of local features. Additionally, we introduce a reweighting mechanism for inter-domain prototypes to generate generalized prototypes to provide inter-domain knowledge and reduce domain skew across multiple clients. Extensive experiments on the Digits, Office-10, and PACS datasets illustrate the superior performance of our method compared to other baselines.", "sections": [{"title": "1. Introduction", "content": "Federated Learning (FL) has emerged as a prominent distributed machine learning framework, enabling multiple clients to collaboratively train a model without leaking private data [19, 21, 25]. The widely used FL approach, FedAvg [25], ensures user privacy by sharing only model parameters with a central server. In recent years, FL has gained considerable attention and demonstrated promising results across various domains [2, 12, 24, 31]. Despite its potential, FL faces a critical challenge: data heterogeneity [20, 21, 26]. As such, the data distributions across clients are non-independently and identically distributed, i.e., non-iid, leading to the degradation of learning performance and fluctuations in the convergence of the global model performance. To address this challenge, recent FL methods have focused on enhancing local training through regularization techniques [13, 21] or novel aggregation schemes [9, 41, 42].\nHowever, most existing FL methods primarily address the label shift, assuming client data is derived from the same domain. In real-world scenarios, private data is often collected from multiple domains. For instance, images of a cat and sketches of a cat might share the same label but come from different domains, leading to heterogeneous feature distributions across clients. Unlike label skew, the impact of domain skew on FL has not been extensively explored. Under domain skew, a domain gap exists across different participating clients, causing local models to be domain-specific, leading to poor generalization of the global model. To overcome the challenge above, recent FL works on heterogeneous domain [10, 38] have considered prototypes, represented as the mean values of vectors within the same semantic class as the solution. These studies [37, 38] obtain the global prototype by averaging local prototypes, which are then used for regularizing the local model to resolve the label skew. Regarding domain skew, the authors in [10] propose a clustering approach to construct unbiased prototypes to provide diverse domain knowledge for multiple clients. This method has demonstrated effectiveness in addressing domain shifts in scenarios where client distributions across domains are non-identical, and it is considered state-of-the-art. However, these methods only consider constructing prototypes at the inter-domain level and overlook the intra-domain characteristics of local clients.\nUnlike existing federated prototype learning methods, we aim to tackle domain shifts in FL by considering prototypes from intra- and inter-perspectives, as shown in Table 1. In particular, previous works [37, 38] consider averaging the local prototypes that belong to the same class space to obtain the global prototypes. However, under the domain skew challenge, directly averaging prototypes can create biased global prototypes, similar to the problem in FedAvg. This results in poor generalization of the global model across different domains, as stated in [10].\nBuilding upon the issues identified in prior works, we rethink the concept of inter-domain prototypes on the server by proposing a prototype reweighting scheme. Specifically, on the server side, we first calculate the initial mean of prototypes from different clients within the same semantic class. Under domain skew conditions, the initial mean may be skewed toward the dominant domain due to the bias in client distribution across different domains. We assert that prototypes further from the initial mean need more weight than those closer to the mean. Therefore, reweighting scheme assigns more weights to a prototype as its distance from the mean increases. By doing so, we can obtain generalized prototypes that provide unbiased inter-domain knowledge for local training, consequently improving performance on challenging domains. It is important to note that our generalized prototype construction maintains privacy through multiple averaging operations [37]. In addition, we introduce the concept of intra-domain prototypes for local clients, enriching the local feature diversity during training. Unlike local prototypes that are sent to the server, we define the intra-domain prototypes as being stored and utilized locally on the client side. Specifically, inspired by the MixUp augmentation [44] technique, we create intra-domain prototypes with augmented prototypes for each client. By learning from augmented prototypes, local clients can extract more semantic information from their features, enhancing their generalization capability for subsequent prototype aggregation. To effectively handle domain shift in FL, it is essential to combine prototypes from both intra-domain and inter-domain perspectives. Intra-domain prototypes enhance the diversity within a single domain, improving local learning, while inter-domain prototypes facilitate knowledge transfer across multiple domains, enhancing global generalization.\nIn this paper, we propose Intra and Inter-Domain Prototype Federated Learning (I2PFL), which consists of two components: Generalized Prototypes Contrastive Learning (GPCL) and Augmented Prototype Alignment (APA). Our proposed I2PFL is illustrated in Fig. 1. Our approach simultaneously handles intra- and inter-domain prototypes under domain-skewed FL. First, Generalized Prototypes Contrastive Learning (GPCL) is proposed to guide the local model training with the inter-domain knowledge and alleviate the domain shift problem in FL. Specifically, we generate the generalized prototypes using the reweighting scheme to reduce the bias towards dominant domains at the inter-domain level. Inspired by the success of contrastive learning [3, 8, 29], GPCL encourages the alignment of local features with the generalized prototypes of the same semantic class while pushing them away from generalized prototypes of different classes. Additionally, the APA component increases local feature diversity and avoids overfitting on domain-specific data at the intra-domain level by encouraging alignment between local features and augmented prototypes using MixUp-based feature augmentation. By combining prototypes at multiple levels, our proposed method enhances the global model's robustness and mitigates negative impacts of domain shift. Our primary contributions are:\n\u2022 We focus on FL under the domain skew challenge and recognize that the existing methods are limited on constructing prototypes at the inter-domain level, while neglecting the intra-domain characteristics of local clients.\n\u2022 To tackle the challenge of domain skew in FL, we introduce a novel approach, I2PFL. Our method first introduces prototype learning at the intra-domain level to enhance feature diversity using MixUp-based augmented prototypes. We further construct generalized prototypes with a novel prototype reweighting scheme at the inter-domain level to provide inter-domain knowledge, achieving generalization performance across different domains.\n\u2022 We conduct extensive experiments on the Digits, Office-10, and PACS datasets. We demonstrate the superiority of our method over other baselines and validate the effectiveness of each component through ablation studies."}, {"title": "2. Related Work", "content": "FedAvg faces performance degradation when dealing with data heterogeneity. To address the non-iid challenge, some studies incorporate regularization terms to focus on improving the local training, such as FedProx [21] with a proximal term calculated by the distance between global and local models and SCAFFOLD [13] with control variates. Other methods, such as FedDyn [1] and pFedMe [36], also enhance local training through various regularization techniques. Another direction is to improve the aggregation phase. FedMA [41] utilizes a Bayesian non-parametric method to average model parameters in a layer-wise manner, while FedAvgM [9] incorporates a momentum-based global update at the server. However, these methods primarily consider scenarios with single domain data and label skew, overlooking the domain skew challenge in FL. Recently, methods like FedBN [22] and FPL [10] have been developed to address domain skew. Specifically, FPL proposes clustering prototypes to achieve unbiased prototypes, resulting in state-of-the-art performance. FedGA [45], FedDG [23], and gPerXAN [14] address the problem of domain generalization, aiming to improve the global model's ability to generalize to unseen domains, i.e., data domains not included in the training process. In contrast, our work tackles a different challenge, focusing on enabling the global model to handle distribution shifts across multiple clients. In this work, we introduce Intra- and Inter-Domain Prototype Federated Learning (I2PFL), which constructs intra- and inter-domain prototypes. Our focus is on enhancing the generalization of the global model under domain shift by utilizing generalized and local augmented prototypes in federated learning."}, {"title": "2.2. Prototype Learning", "content": "Prototypes [35] have achieved success in various applications, including few-shot learning [39, 43, 46] and unsupervised learning [4, 5, 17]. In FL, the concept of prototypes has been extended to address the data heterogeneity challenge [10, 30, 38]. FedProto [37] was among the first to introduce the use of prototypes in FL, proposing a communication method that exchange prototypes between clients and the server instead of model parameters. Recently, FPL [10] introduced a cluster-based prototype method to generate unbiased global prototypes, addressing the challenges in FL where the client distributions vary across domains. However, these methods primarily focus on constructing the prototypes at the global server, overlooking the intra-domain characteristics of the local clients. In contrast, our approach constructs intra-domain prototypes to increase local feature diversity and introduce a reweighting scheme to inter-domain prototypes, producing the unbiased generalized prototypes. The integration of intra- and inter-domain prototypes enables the model to leverage both components effectively: intra-domain prototypes enhance the local generalization within each domain, while inter-domain prototypes provide the shared knowledge across different domains, thus effectively aiding in the generalization of the global model."}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Overview", "content": "In this paper, we assume there are $M$ clients (indexed by $m$), each with private data $\\mathcal{D}_m = \\{(x_m, y_m)\\}$, where $x_m$ represents samples and $y_m$ denotes the corresponding labels. Under the domain shift, each client has private data with different feature distributions $P_m(x)$, but the label distributions $P_m(y)$ remain the same across multiple clients."}, {"title": "3.2. Prototype Reweighting Scheme", "content": "Prior research on federated prototype learning [37, 38] typically produce global prototypes by simply averaging the local prototypes from different clients. This can lead to a bias favoring the dominant prototypes and negatively impact performance in domain-skewed FL scenarios. This motivates us to rethink the concept of inter-domain prototypes by designing generalized prototypes that can reduce the bias in prototype averaging and enhance the global model\u2019s generalization. We first define the kth class local prototypes from client m as:\n$$\np_m^k = \\frac{1}{|\\mathcal{S}_m^k|} \\sum_{x_i \\in \\mathcal{S}_m^k} f(x_i) \\in \\mathbb{R}^d,\n$$\nwhere $\\mathcal{S}_m^k$ is the subset of $\\mathcal{D}_m$ belonging to class kth. Then, we further calculate the initial mean of prototypes from different clients within the same semantic class kth as:\n$$\n\\mu^k = \\frac{1}{M} \\sum_{m=1}^M p_m^k, \\quad \\mu = [\\mu^1, \\mu^2, ..., \\mu^K],\n$$\nwhere $\\mu^k$ denotes the initial mean of prototypes belonging to class $k \\in \\mathcal{K}$. Under the domain shift, this initial mean may be biased toward the dominant prototypes due to the differences in client distribution. We define the distance between the local prototype and the initial mean of prototypes within the same semantic class k as $d_m^k = ||p_m^k - \\mu^k||_2$. We assert that prototypes further from the initial mean need more weight than those closer to the mean to reduce the bias. By assigning more weight to these distant prototypes, the generalized prototypes reflect a more balanced and comprehensive view of different domains. Thus, we propose a reweighting scheme for each prototype within the same class based on its distance from the initial mean. Prototypes with a higher distance from the mean are assigned more weight, as illustrated in Fig. 2. We denote the generalized prototypes with our proposed reweighting scheme as follows:\n$$\ng^k = \\frac{\\sum_{m=1}^M d_m^k p_m^k}{\\sum_{m=1}^M d_m^k} \\in \\mathbb{R}^d, \\quad \\mathcal{G} = [g^1, g^2, ..., g^K],\n$$\nwhere $d_m^k = \\sum d_m^k$ denotes the sum of distances between the local prototype and the initial mean of prototypes from different clients, and $g^k$ denotes the generalized prototypes belonging to class $k \\in \\mathcal{K}$. To achieve more stable and consistent generalized prototypes, we apply the Exponential Moving Average (EMA) update to the generalized prototypes of the current communication round t + 1 from the previous round t. The formulation is as follows:\n$$\n\\mathcal{G}^{t+1} = \\beta \\mathcal{G}^{t+1} + (1 - \\beta)\\mathcal{G}^{t}.\n$$\nwhere \u03b2 is the decay rate of the EMA update. By applying an EMA update to generalized prototypes and assigning greater weight to past prototypes, we can maintain a balanced representation and mitigate performance fluctuations caused by domain shifts. Compared with the conventional prototype averaging method, our generalized prototypes $\\mathcal{G}$ achieve fair optimization on multiple domains and avoid bias toward the dominant domain, thus ensuring consistent guidance for the local training process."}, {"title": "3.3. Generalized Prototypes Contrastive Learning", "content": "The consistent generalized prototype could enhance the robustness of the global model under the domain shift and guide the local training with inter-domain knowledge. Thus, we apply the contrastive learning between the local features and generalized prototypes. We encourage local features of data samples to closely align with their corresponding generalized prototypes within the same semantic class while pushing away the generalized prototypes of different semantic classes. Regarding the data samples {xi, yi}, we first employ the feature extractor to generate the feature vectors h = f(x) \u2208 Rn. Let g be the corresponding generalized prototypes g \u2208 G, g+ denotes the generalized prototypes with the same semantic class from the local samples. Subsequently, inspired by InfoNCE loss [29], we design the Generalized Prototype Contrastive Learning (GPCL) loss as follows:\n$$\n\\mathcal{L}_{\\text{GPCL}} = - \\frac{1}{B} \\sum_{i=1}^B \\log \\frac{\\exp(s(h_i, g^+)/\\tau)}{\\sum_{g_k \\in \\mathcal{G}} \\exp(s(h_i, g^k)/\\tau)},\n$$\nwhere s(u, v) = $u^Tv/||u||||v||$ represents the cosine similarity between the local feature and the generalized prototypes, B denotes local batch size and \u03c4 is the temperature parameter. Our target of Eq. 6 is to encourage the local client from different domains to acquire inter-domain knowledge from the generalized prototypes, thereby enhancing the generalization and mitigating the domain skew\u2019s negative impact."}, {"title": "3.4. Augmented Prototypes Alignment", "content": "In federated learning, under the domain skew problem, the individual clients possess local data that is limited to a specific domain, which can lead to overfitting and poor generalization. Unlike previous works [10, 27, 37] that consider only the prototype construction at the inter-domain level on the server, we propose constructing the intra-domain prototypes at the local clients. To address the limitation of local training data diversity, we conduct the MixUp-based prototype augmentation. Unlike traditional input-level MixUp, which introduces variation in raw input, feature-level MixUp operates on embedding features, producing semantically richer and more stable, less domain-specific augmented prototypes. We first encode the local samples {xi, yi} into the feature vectors hi using the feature extractor f. Inspired by MixUp [44] augmentation technique, which generates synthetic instances by combining the features and labels of samples pairs through linear interpolation, we incorporate MixUp strategy to generate the augmented feature as follows:\n$$\nh_i = \\gamma h_i + (1 - \\gamma)h_j\n$$\nwhere \u03b3 ~ Beta(\u03b1,\u03b1) with \u03b1 \u2208 (0,\u221e), and hj is the feature of random data sample xj from different semantic class on Dm. This approach increases the diversity within local features and helps prevent overfitting to data specific to a particular domain. Similar to Eq. 2, we denote the augmented prototypes of local client mth as:\n$$\np_m^k = \\frac{1}{|\\mathcal{S}_m^k|} \\sum_{x_i \\in \\mathcal{S}_m^k} h_i,\n$$\n$$\n\\mathcal{P}_m = [p_m^1, p_m^2, ..., p_m^K],\n$$\nwhere $p_m^k$ denotes the augmented prototypes of mth client belonging to class $k \\in \\mathcal{K}$. By learning from the augmented prototypes, the local features become less class-specific, thereby improving the generalization capability of the local prototypes before they are sent to the server and enhancing the robustness of local model training against domain shift. Subsequently, we utilize l2 distance and introduce the Augmented Prototype Alignment (APA) as follows:\n$$\n\\mathcal{L}_{APA} = \\sum_k ||h_k - p_m^k||^2,\n$$\nwhere $h_k$ is the local features of k semantic class of client m. By establishing an alignment between the local representations and the augmented prototypes, we enhance the local feature diversity and avoid overfitting on domain-specific aspects at the intra-domain level. Moreover, it enhances the generalization of the model parameters and prototypes when the global model performs the aggregation on the server. By integrating the prototypes at intra- and inter-domain levels, our proposed scheme enhances the global model\u2019s robustness on multiple domains and alleviates the domain shift. We define the overall training objective for each client as follows:\n$$\n\\mathcal{L} = \\mathcal{L}_{\\text{CE}} + \\mathcal{L}_{APA} + \\mathcal{L}_{\\text{GPCL}}\n$$\nDuring the local training phase, each client trains the model on their private data using the loss function specified in Eq. 10.\nIn the previous work, for the inter-domain prototypes, FPL [10] utilized the clustering method to generate unbiased prototypes from local prototypes to reduce the bias towards the dominant domain. In contrast, our approach introduces a prototype reweighting mechanism, where prototypes with greater distances from the mean receive higher weights, yielding more generalized inter-domain prototypes and reducing single-domain bias. Additionally, we incorporate intra-domain prototypes at the local clients to further improve global model generalization under domain shift."}, {"title": "4. Experiment", "content": ""}, {"title": "4.1. Experimental Setup", "content": "Datasets. We conducted experiments using three image classification datasets: Digits [11, 15, 28, 33], Office-10 [6] and PACS [16]. The Digits dataset comprises four domains: MNIST (mt), USPS (up), SVHN (sv), and USPS (up), each presenting 10 categories with digit numbers from 0 to 9. The Office-10 includes four domains: Caltech (C), Amazon (A), Webcam (W), and DSLR (D) of 10 categories. The PACS dataset contains images across 7 categories from four domains: Photo (P), Art Painting (A), Cartoon (C), and Sketch (S).\nWe initialize 20, 10, and 10 clients for Digits, Office-10, and PACS, respectively, and assign domains to clients randomly, following [10], as shown in Table 2. We sampled a specific proportion from these domains for each client based on task difficulty and dataset size, with sampling rates set at 1%, 20%, and 30% for Digits, Office-10, and PACS, respectively. To ensure reproducibility, we fixed the seed.\nModel Architecture. For the Digits and Office-10 datasets, we used ResNet-10 [7] as the base model architecture, while for the PACS dataset, we employed ResNet-18 [7].\nImplementation Details. The communication round is set to 100, and the local training epoch is 10 for all datasets. We employ the SGD [32] optimizer with a weight decay of le-5 and a learning rate of 0.01 across all datasets. The training batch size is 32 for the Digits and Office-10 datasets, and 16 for the PACS dataset. The EMA \u03b2 is set as 0.99 for all datasets. We use Top-1 accuracy as the evaluation metric for all datasets. Each experiment is repeated three times, and we report the mean values from the last 5 communication rounds."}, {"title": "4.2. Comparison to SOTA methods.", "content": "Table 3 and Table 4 present the performance comparison of our proposed I2PFL with other SOTA methods on three datasets. As the results show, I2PFL consistently outperforms other baselines across multiple domains. The average accuracy depicts the effectiveness in achieving better generalization. For the Digits dataset, I2PFL demonstrates superior performance across all domains, with an average accuracy improvement of 0.92% compared to FPL. Regarding the Office-10 dataset, our method outperforms the state-of-the-art method FPL by a notable gap, illustrating an improvement of 2.95%. Specifically, we improve the performance on a challenging domain like DSLR, where I2PFL significantly outperforms other methods. In the PACS dataset, methods incorporating contrastive learning tend to achieve higher average accuracy across all domains. However, I2PFL still outperforms other approaches in most domains, with a 1.15% improvement in average accuracy compared to MOON. These results highlight our method's ability to achieve better generalization across multiple domains and different tasks. By integrating intra- and inter-domain prototypes, we enhance the generalization across multiple domains, effectively avoiding the bias toward any specific domain."}, {"title": "4.3. Ablation Study", "content": "Contributions of Key Components. To assess the effect of each component on I2PFL's performance, we perform an ablation study by selectively removing individual components, as detailed in Table 5. Our findings reveal that both GPCL and APA contribute to performance improvements over the baseline without any components, underscoring the effectiveness of both intra-domain and inter-domain prototypes. Notably, APA depicts more impact on the performance across all datasets, demonstrating the effectiveness in enhancing the feature diversity of our proposed intra-domain prototypes on the local side. These observations highlight the critical importance of leveraging both intra- and inter-domain prototypes to improve the generalization of the global model under domain shift. Additionally, we present the effect of different prototype components in the supplementary material.\nAnalysis on inter-domain prototypes. In Table 6, we evaluate the effectiveness of our proposed inter-domain prototypes with prototype reweighting scheme against the prototype averaging method and the FINCH [34] clustering method used in FPL [10]. The results clearly demonstrate the superior performance of utilizing our reweighting approach, showing improvements of 0.74%, 2.93%, and 2.35% on Digits, Office-10, and PACS datasets, respectively. This finding highlights the ability of our method to generate the generalized prototypes at the inter-domain level, thereby providing the inter-domain knowledge and improving generalization across different domains.\nEffect of MixUp on intra-domain prototypes. In Table 7, we evaluate the effect of MixUp on our intra-domain prototypes. The results show that by using MixUp at the feature level, our method achieves better generalization than other intra-domain prototype variations. This finding underscores that feature-level MixUp produces richer prototypes, helping to prevent overfitting to specific domains. Additionally, the consistent performance across domains emphasizes the robustness of our feature-level MixUp approach in capturing diverse semantic representations, thereby strengthening the overall model performance under domain shift.\nEffects of hyper-parameters. We illustrate the performance impact by temperature parameter \u03c4 and \u03b1 used for Beta distribution in Fig. 5. For the Digits dataset, optimal performance is achieved with \u03c4 = 0.07 and \u03b1 = 0.4. In the Office-10 dataset, the best results are obtained with \u03c4 = 0.02 and \u03b1 = 0.4. In the PACS dataset, optimal performance occurs with \u03c4 = 0.04 and \u03b1 = 0.2. These hyper-parameters are used by default in all experiments."}, {"title": "5. Conclusion", "content": "This paper introduces I2PFL, a novel prototype-based FL framework designed to mitigate domain shifts in FL. Our approach incorporates two key components: intra-domain prototypes and inter-domain prototypes. Specifically, we introduce the intra-domain prototypes with MixUp-based augmented prototypes. Moreover, we propose a novel prototype reweighting scheme for inter-domain prototypes to generate the generalized prototypes. We use contrastive learning with generalized prototypes to provide inter-domain knowledge and guide local training. Furthermore, we enhance local feature diversity by encouraging alignment between local features and the augmented prototypes. By integrating intra- and inter-domain prototypes, we significantly improve the generalization of the global model and address domain shifts in federated learning. Experiments on three image classification datasets demonstrate the superior performance of I2PFL compared to other state-of-the-art methods."}, {"title": "6. Pseudo Code for I\u00b2PFL", "content": "We provide a detailed algorithm of our proposed method in Alg. 1. In each communication round, clients receive the generalized prototypes and global model from the server. Then, clients conduct the local training process using augmented and generalized prototypes. After finishing the local training process, the updated local prototypes and local models are sent back to the server, which aggregates them to update the global model and generalized prototypes."}, {"title": "7. Additional Results", "content": ""}, {"title": "7.1. Performance comparison on different client distribution", "content": "In this experiment, we compare the performance of our proposed I2PFL method with other SOTA approaches across different client distributions. Specifically, we allocate 20, 12, and 12 clients for Digits, Office-10, and PACS datasets, respectively, and distribute an equal number of clients per domain. As shown in Table 8 and Table 9, the performance on Office-10 and PACS datasets improves compared to the default setting due to the increased number of clients. However, the Digits dataset shows a slight decrease in performance due to the smaller number of clients in challenging domains like SYN. Overall, I2PFL consistently outperforms other baselines across multiple domains, illustrating the adaptation to different client distributions."}, {"title": "7.2. Visualization", "content": "We illustrate the representations produced by our I2PFL using t-SNE [40] on Digits and Office-10 datasets, as shown in Fig. 8 and Fig. 9. We compare the representations extracted from the global model between our proposed method, FPL and FedAvg on Digits dataset with SVHN and SYN domains and Office-10 dataset with Amazon and Webcam domains. The figures show that the features generated by our method are more distinctly separated compared to those from other methods, illustrating the better generalization of the global model across different domains."}, {"title": "7.3. Effect of different prototype components", "content": "We present performance curves in Fig. 7 to illustrate the impact of different prototype components across all datasets. The results show that intra- and inter-domain prototypes contribute to the convergence of I2PFL, underscoring the effectiveness of combining these prototype types. Notably, intra-domain prototypes substantially impact performance across all datasets, highlighting the benefits of leveraging augmented prototypes locally. These observations confirm the importance of integrating intra- and inter-domain prototypes for optimal performance."}, {"title": "7.4. Effects of EMA updates on generalized prototypes", "content": "In addition, we evaluate the impact of using EMA updates for generalized prototypes, as shown in Table 10. The results indicate the improved performance, with increases of 0.87%, 3.80%, and 2.76% on Digits, Office-10, and PACS datasets, respectively. This improvement demonstrates that leveraging EMA updates enhances the stability and robustness of generalized prototypes. By smoothing the prototype updates over time, EMA reduces the impact of noise and fluctuations caused by varying domain distributions, resulting in more balanced and generalized prototypes."}, {"title": "7.5. Effects of EMA parameter \u03b2", "content": "We illustrate the performance impact by EMA parameter \u03b2 in Fig. 6. As the figure shows, the optimal performance on all datasets is achieved with \u03b2 = 0.99. The EMA parameter \u03b2 is used by default in all experiments."}]}