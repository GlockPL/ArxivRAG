{"title": "Aerial Vision-and-Language Navigation via Semantic-Topo-Metric Representation Guided LLM Reasoning", "authors": ["Yunpeng Gao", "Zhigang Wang", "Linglin Jing", "Dong Wang", "Xuelong Li", "Bin Zhao"], "abstract": "Aerial Vision-and-Language Navigation (VLN) is a novel task enabling Unmanned Aerial Vehicles (UAVs) to navigate in outdoor environments through natural language instructions and visual cues. It remains challenging due to the complex spatial relationships in outdoor aerial scenes. In this paper, we propose an end-to-end zero-shot framework for aerial VLN tasks, where the large language model (LLM) is introduced as our agent for action prediction. Specifically, we develop a novel Semantic-Topo-Metric Representation (STMR) to enhance the spatial reasoning ability of LLMs. This is achieved by extracting and projecting instruction-related semantic masks of landmarks into a top-down map that contains the location information of surrounding landmarks. Further, this map is transformed into a matrix representation with distance metrics as the text prompt to the LLM, for action prediction according to the instruction. Experiments conducted in real and simulation environments have successfully proved the effectiveness and robustness of our method, achieving 15.9% and 12.5% improvements (absolute) in Oracle Success Rate (OSR) on AerialVLN-S dataset.", "sections": [{"title": "I. INTRODUCTION", "content": "The Aerial Vision-and-Language Navigation (Aerial VLN) [1] emerges as a groundbreaking task. It enables unmanned aerial vehicles (UAVs) to interpret natural language instructions and corresponding visual information to navigate in outdoor environments. This technology can eliminate the necessity for manual UAV operation by human pilots, clearly mitigating the barriers to human-UAV interaction and potentially benefitting search, rescue, and delivery tasks.\nRecently, VLN tasks have been well-developed. Considering the powerful reasoning capability of large language models (LLMs) [2], [3], [4], [5], several VLN methods have started to use LLMs as agents to parse instructions and predict actions [6], [7], [8], [9]. Specifically, existing methods attempt to describe the visual observations with text to enhance the LLM's scene understanding ability. NavGPT [6] converted visual scene semantics into text description for the LLM to conduct high-level planning. MapGPT [10] introduced a topological graph described in language as text prompts to an LLM, guiding low-level planners to explore new nodes of a predefined graph of the environment.\nAlthough existing VLN methods have made significant progress, most of them are designed for indoor or ground-based outdoor environments and pay less attention to aerial VLN tasks. Practically, the rich semantics, complex spatial relationships, and the large-scale nature of outdoor 3D spaces make it challenging for existing VLN methods to adapt to aerial scenarios. As shown in Fig. 1 (a), the aerial scene can be quite complex, which may lead to incorrect reasoning for several reasons, such as an overemphasis on instruction-irrelevant objects, or a failure to capture the contextual relationships between different areas. Furthermore, the vast scale of aerial landscapes can further complicate navigation, making local-perspective-based VLN methods unable to comprehend the environment. Thus, the development of VLN methods that integrate both effective semantic information and precise spatial representations is urgently needed to enhance the adaptability and accuracy in aerial scenarios.\nTo overcome these challenges, we propose a zero-shot LLM-based aerial VLN framework, which takes natural language instructions, RGB images, and depth images as input, and directly makes action predictions (e.g., forward 10 meters) through single-step reasoning updates. Specifically, a Semantic-Topo-Metric Representation (STMR) is designed for LLMs prompting. We first extract instruction-related landmarks and obtain corresponding semantic masks by perception models, i.e., Grounding Dino [11] and Tokenize Anything [12]. After that, semantic masks obtained from each step are gradually projected into a top-down map using"}, {"title": "II. RELATED WORK", "content": "a) Vision-Language Navigation (VLN): VLN aims to enable autonomous agents to navigate complex environments by understanding and executing natural language instructions based on visual context. Early VLN methods use sequence-to-sequence LSTMs to predict low-level actions [13] or high-level actions from panoramas [14]. Several attention processes have been proposed [15], [16], [17] to enhance the process of learning visual textual correspondence. Reinforcement learning is also explored to improve policy learning [18], [19], [20]. Besides, transformer-based architecture have shown superior performance to long-distance contextual information [21], [22]. More recent works [6], [10] leverage the reasoning and dialogue capabilities of LLMs, achieving great progress. However, most of them are developed under a ground-based, discrete VLN setting, which simplifies navigation as traversing on a predefined graph on the ground. This limits the free movement space of UAVs in the real world. Aerial VLN remains challenging due to the large-scale and complex environments.\nb) UAV navigation: Unmanned Aerial Vehicle (UAV) navigation has seen a surge of interest over the past few decades. Guided with instructions, Blukis et al. [23] use imitation learning for low-level velocity command prediction. [24] predicts interpretable position-visitation distributions to guide UAV's control actions, using a combination of supervised and imitation learning for efficient training. Misra et al. [25] decompose instruction execution in two stages, separately using supervised learning for goal prediction and policy gradient for action generation. AerialVLN [1] contributes a much more challenging aerial VLN dataset focusing on large-scale environments, and provides a look-ahead guidance method as the baseline. Despite the progress, the generalizability and performance of these methods still require improvement.\nc) LLMs for robot planning and interaction: Most recently, LLMs have demonstrated impressive capabilities in understanding and reasoning. To leverage these capabilities, several promising methods have been proposed for applying LLMs in robotic systems. A few methods involve using LLM-generated rewards optimized in simulation to improve control [26], [27]. Others utilize LLM-selected subgoals as an abstraction to enhance policies for navigation [28], [29] and manipulation [30], [31]. Additionally, research has explored the use of LLMs to generate executable code for control and perception primitives [32], [33], [34]. Despite their potential, LLMs are still prone to confidently hallucinating outputs, such as referring to objects not observed in the scene [35]. In order to alleviate the hallucinating phenomenon, we propose a matrix-based representation containing topological, semantic, and metric information for better prompting the LLMs."}, {"title": "III. METHOD", "content": "In this paper, we introduce a novel zero-shot framework that leverages large language models (LLMs) for action prediction in aerial VLN tasks. As shown in Fig. 2, our framework mainly consists of three modules. The sub-goal extracting module decomposes language instructions into several sub-goals, facilitating step-by-step reasoning and navigation. The Semantic-Topo-Metric Representation (STMR) module represents the outdoor environment as a matrix containing semantic, topological, and metric information for the prompt to an LLM. Finally, the LLM planner outputs its current thoughts and actions by using the prompts from the aforementioned two modules along with the task description and history as inputs.\nA. Problem Formulation\nIn an aerial VLN task, the problem is formulated as a free-form language instruction guided navigation. At the beginning of each episode, the UAV is placed in an initial pose $P = [x, y, z, \u03c6, \u03b8, \u03c8]$, where $(x, y, z)$ denotes the UAV's position and $(4,0,\u03c8)$ represents pitch, roll, and yaw of the UAV's orientation. A natural language instruction $L$ is"}, {"title": "B. Semantic-Topo-Metric Representation (STMR)", "content": "Previous LLM-based VLN methods use natural language to describe current observations, or nodes and edges of a topological graph to model the spatial information of the environment. However, in open scenarios, simple directional words such as \"next to\" or \"aside\" are not enough to describe complex spatial relationships, which can easily introduce ambiguity into LLMs. To address this challenge, we in-troduce the Semantic-Topo-Metric Representation (STMR) to enhance the spatial-aware reasoning capability of LLMs. Specifically, STMR incrementally takes an RGB image $I_R$ and a depth map $I_P$ as input from each step, and generates a dynamically updated matrix representation with semantic, topological, and metric information as its output. The details of STMR are presented as follows.\na) 2D Visual Perception: Impressed by the powerful open-vocabulary detection capabilities of Grounding DINO $\\text{GD}(\\cdot)$, as well as the captioning and segmentation capabil-ities of Tokenize Anything model $\\text{TA}(\\cdot)$, we integrate these two models as our 2D visual perceptor, as illustrated in Fig. 3. Given a single RGB image $I_R$ and an instruction $L$ as input, we first obtain detailed landmark categories using a Landmark Extractor $\\text{LE}(\\cdot)$ driven by an LLM, and then identify the corresponding bounding box for each category through $\\text{GD}(\\cdot)$. Next, we employ $\\text{TA}(\\cdot)$ to take each bounding box as a prompt and output a set of 2D semantic masks $m^{(t)}$ and captions $z^{(t)}$ for the current RGB image $I_R$. The entire process can be described as:\n{m^{(t)}, z^{(t)}\\} = \\text{TA} (I_R, \\text{GD} (I_R, \\text{LE}(L))).\\tag{1}\nTo improve the simplicity of the semantic masks and reduce the misleading of LLM-reasoning caused by numer-ous open-vocabulary categories, we propose a text-matching method to mitigate over-classification. As shown in Fig. 3, our method involves vectorizing the landmarks extracted from the instruction and the captions $z^{(t)}$ generated in each $I_R$. Then, we calculate the cosine similarity between these vectorized landmarks and captions using TF-IDF [36]. If the similarity score exceeds the threshold \u30f6 >0.8, the landmark is classified as visible in the current view. By implementing this strategy, we effectively simplify semantic masks and ensure the LLM's reasoning focuses on relevant categories.\nb) Sub-goal-driven top-down map: Considering that the top-down view can better express spatial relationships than the first-person view, we further utilize the depth image $I_P$ to map semantic masks to 3D space, and then convert them into a top-down map. Specifically, with the RGB image $I_R$ processed by the 2D visual perceptor, we produce first-person view predictions by assigning semantic labels to each pixel, resulting in a segmented image with identified objects and regions. Subsequently, the depth image $I_P$ is converted into a 3D point cloud, where each pixel is mapped to a 3D point $(X, Y, Z)$ based on its depth value and camera parameters:\nX = \\frac{(u - c_x) I_P (u, v)}{f_x}, y = \\frac{(v - c_y) I_P (u, v)}{f_y}, z = I_P (u, v),\\tag{2}\nwhere $(u, v)$ are the pixel coordinates, $(c_x, c_y)$ are the camera's principal point coordinates, and $(f_x, f_y)$ are the focal lengths. The semantic labels from the segmented RGB image are mapped to the corresponding 3D point clouds, resulting in a point cloud with semantic information $(X, Y, Z, C_i)$, where $C_i$ are the semantic categories. Then the 3D point cloud is partitioned into discrete voxels, where each voxel aggregates its point clouds as one semantic category using max pooling described in [37]. Considering that UAVs usually fly above ground objects, for a specific coordinate $(x,y)$, the semantic label of the top one in a column of voxels will be projected into the top-down map:\n\\text{TopDownMap}(x, y) = \\text{Voxel}(x, y, z_{top}),\\tag{3}\nwhere $z_{top}$ means the highest z coordinate at location $(x, y)$ and $\\text{Voxel}(x, y, z_{top})$ denotes the corresponding semantic label. So, we get a top-down map with semantic information."}, {"title": "C. Designing Text Prompt for LLM Planning", "content": "Given that LLMs are well-trained in reasoning and plan-ning, we complement their spatial perception and scene understanding capability with formatted prompt and action space. This prompt involves two components. First, the task description is prepared which includes the definition of the environment and the UAV, as well as the format of input and output. We require an LLM to leverage Chain-of-Thought reasoning [38] after capturing the observation, thinking step by step in an order of observation-thought-planning-history to predict the next action. Regarding the input instruction, sub-goals are extracted by the LLM, and the landmarks given in the instructions will be updated in the task definition as the scene changes. Second, to improve the quality of prompts, we implemented a cyclic refinement process. This involves designing a new set of instructions for an LLM, including elements such as 'history action' and 'next planning' corresponding to the STMR module. The task description also emphasizes updating the status of planning, including three states, i.e., todo, in process, and completed. The STMR is gradually updated in this iterative process as the UAV explores to improve the accuracy and relevance of the prompts."}, {"title": "IV. EXPERIMENT SETUP", "content": "a) Datasets: We conduct experiments using a novel and complex dataset provided by [1], which we refer to as the \"AerialVLN-S\" dataset, to assess the performance of aerial VLN tasks. This dataset is designed to mimic real-world urban environments with more than 870 different kinds of objects and various scenarios including downtown cities, factories, parks, and villages. Besides, the AerialVLN-S dataset encompasses 8,446 flying paths recorded by experienced human UAV pilots who possess the AOPA (Aircraft Owners and Pilots Association) certificate, ensuring the paths' realism and quality. Several comparison methods are evaluated on this dataset, including state-of-the-art VLN models, to"}, {"title": "A. Experimental Results", "content": "a) Quantitative results in simulator: Table I compares our method with several other works. Random and Action Sampling respectively mean sampling actions randomly and according to the action distribution of the training set. LingUNet [25] assumes an agent can see the destination from the beginning, which is transformed into a step-wise paradigm by [1]. Seq2Seq [13] is a learning-based recurrent policy, directly predicting the next action. CMA is a baseline model in [1], considering attention between multi-modalities. LAG is the main method in [1], designing a look-ahead guidance to adjust paths. In comparison, our method significantly outperforms others in most metrics, demonstrating the superiority of our method. We found that instead of processing the entire long instruction and visual observations at each time step, the formatted decomposed sub-goal is more conducive to the fine-grained alignment of each action and landmark. Moreover, UAV has a better understanding of its own spatial position and other semantic constraints under the guidance of our STMR, which has obvious advantages in long instruction environments and leads to more robust execution.\nb) Quantitative results in real environment: To test whether our solution works well in the real world, we collect 10 outdoor scenes including street scenes and forests with ground-truth lengths ranging from 50m to 500m. Then, we apply our method on real UAVs and call cloud-hosted LLMS (GPT-4o is used) to navigate in these challenging environments. LLM-based VLN methods MapGPT [10] and NavGPT [6] are employed and compared in this experiment. From demo snapshot Fig. 4, we can see that our method can align visual and textual landmarks as well as understand commands. Finally, the UAV reaches the destination successfully. Table II also shows that the proposed method achieves much better results than most recent LLM-based VLN methods.\nc) Failure case analysis: In the simulation, we analyze two most common failures caused by incorrect planning or execution. The first typical failure is the misunderstanding of ambiguous instructions. In some instructions, there are a lot of continuous commands without any landmark, such as \"turn left, then move right, then go straight\", which have no objects for an LLM to refer to. This case will directly leads to the repeated execution of a step. The second typical error is caused by the inaccuracy of the visual perceptor. Although modern perception models show strong ability, they still need to be improved in identifying objects from different views. As a result, the key landmark may not be mapped to the matrix representation, causing the proper action not to be completed. This observation aligns with our real-world experiment, where most errors are from perceptions."}, {"title": "B. Comprehensive Analysis", "content": "We carry out several ablation studies to assess the effectiveness of the proposed framework. All ablation experiments are conducted on the AerialVLN-S task with the validation"}, {"title": "V. CONCLUSIONS AND FUTURE WORK", "content": "This paper addresses the challenging aerial VLN task by introducing an LLM-based end-to-end framework. To enhance the spatial reasoning ability of LLMs, we propose Semantic-Topo-Metric Representation (STMR). STMR first integrates instruction-related landmarks and their locations into a top-down map, and subsequently transforms this map into a matrix representation containing semantic, topological, and distance metric information. Taking the proposed STMR as a part of prompts of the LLM, we significantly improve the UAV's navigation capabilities. Our framework achieves state-of-the-art results on the AerialVLN-S dataset, demonstrating its effectiveness. In the future, we will explore flexible framework that will interact with humans to eliminate ambiguities in the instructions."}]}