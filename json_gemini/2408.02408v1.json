{"title": "Multi-weather Cross-view Geo-localization Using Denoising Diffusion Models", "authors": ["Tongtong Feng", "Qing Li", "Xin Wang", "Mingzi Wang", "Guangyao Li", "Wenwu Zhu"], "abstract": "Cross-view geo-localization in GNSS-denied environments aims\nto determine an unknown location by matching drone-view im-\nages with the correct geo-tagged satellite-view images from a large\ngallery. Recent research shows that learning discriminative image\nrepresentations under specific weather conditions can significantly\nenhance performance. However, the frequent occurrence of unseen\nextreme weather conditions hinders progress. This paper intro-\nduces MCGF, a Multi-weather Cross-view Geo-localization Frame-\nwork designed to dynamically adapt to unseen weather conditions.\nMCGF establishes a joint optimization between image restoration\nand geo-localization using denoising diffusion models. For image\nrestoration, MCGF incorporates a shared encoder and a lightweight\nrestoration module to help the backbone eliminate weather-specific\ninformation. For geo-localization, MCGF uses EVA-02 as a backbone\nfor feature extraction, with cross-entropy loss for training and co-\nsine distance for testing. Extensive experiments on University160k-\nW X demonstrate that MCGF achieves competitive results for geo-\nlocalization in varying weather conditions.", "sections": [{"title": "1 Introduction", "content": "Cross-view geo-localization [1] aims to determine an unknown lo-\ncation by matching drone-view images with the correct geo-tagged\nsatellite-view images from a large gallery, based on geographic\nfeatures in the images, as shown in Figure 1. This task is crucial\nfor accurate navigation and safe planning [2-4] in GNSS-denied\nautonomous drone flights. Recent advances in vision transformer\nhave led to significant breakthroughs in various cross-view geo-\nlocalization tasks, such as drone localization [5, 6] (matching drone-\nview query images with geo-tagged satellite-view images) and\ndrone navigation[7, 8] (using satellite-view query images to guide\ndrones to a target area). However, varying weather conditions,\nincluding fog, rain, snow, wind, light, dark, and combinations of\nmultiple weather types, reduce visibility, corrupt the information\ncaptured by an image, significantly complicate image geographic\nrepresentation, and lead to a sharp decline in task performance.\nThe major challenge lies in adaptively achieving unbiased image\ngeographic representation under diverse weather conditions."}, {"title": "2 Method", "content": "MCGF establishes a joint optimization between image restoration\nand geo-localization using denoising diffusion models. In image\nrestoration, MCGF uses a shared encoder and a lightweight restora-\ntion module to gradual denoising and obtain clearer drone-viewing\nimages. In geo-localization, MCGF uses a diffusive matching mod-\nule for cross-view matching, which can make the matching module\nrun at multiple granularities, resulting in more accurate matching\nresults. The overview structure of MCGF is shown in Figure 2."}, {"title": "2.1 Denoising Diffusion Models", "content": "The diffusion model is a probabilistic model that has attracted\nconsiderable interest in the computer vision community. It can\nremarkably approximate the original data distribution by gradually\nadding Gaussian noise to the training data and learning to reverse\nthis diffusion process.\nThe forward process is a fixed Markov Chain that sequentially\ncorrupts the data zo ~ qe (zo) at T diffusion time steps, by injecting\nGaussian noise according to a variance schedule \u03b21, ..., \u03b2\u03c4. Given\nthe clean drone-view images zo, the forward process at step t is\ndefined as:\n$qo (z_t | z_{t-1}) = N(z_t; \\sqrt{a_t}z_{t-1}, \u03b2_\u03b5I)$  (1)\n$q_\u03b8(z_{1:T} | z_0) = \\prod_{t=1}^{T} q_\u03b8(z_t | z_{t-1})$ (2)\n$q_\u03b8(z_t | z_0) = N(z_t; \\sqrt{\\bar{a}_t}z_0, (1 \u2013 \\bar{a}_t)I)$ (3)\nwhere at and \u1e9et are noise schedule parameters, \u0101t = \u03a0t=1 as and\nat = 1 - \u1e9et.\nThe reverse process attempts to remove the noise added in the\nforward process. The reverse process defined by the joint distribu-\ntion pe (zo:T) is a Markov Chain with learned Gaussian denoising\ntransitions starting at a standard normal prior pe (zy) = N(zy; 0; I).\nAt step t, the reverse process is defined as:\n$P_\u03b8(Z_{0:T}) = p(Z_T) \\prod_{t=1}^{T} Po(Z_{t-1} | Z_t)$ (4)\n$Po(Z_{t-1} | z_t) = N(z_{t-1}; \u03bc_\u03b8 (z_t, t), \u03a3_\u03b8 (z_t, t))$ (5)\nFor simplicity, we assume Ee is a known constant, thus the\nreverse process simplifies to:\n$Po(z_{t-1} | z_t) = N(z_{t-1}; \u03bc_\u03b8 (z_t, t), \u03c3^2I)$ (6)\nHere the reverse process is parameterized by a neural network\nthat estimates \u03bc\u03b8 (zt, t) and Ee (zt, t)). The forward process vari-\nance schedule \u1e9et can be learned jointly with the model or kept\nconstant, ensuring that z\u0142 approximately follows a standard normal\ndistribution.\nThe training objective of the denoising diffusion model is to max-\nimize the likelihood of the reverse process, which can be achieved\nby minimizing the variational lower bound (VLB) of the negative\nlog-likelihood. The VLB is given by:\n$LVLB = Eq [-log pe (zo) + SDKL]$ (7)\n$SDKL = \\sum_{t=1}^{T} DKL [q_\u03b8(Z_{t-1} | Z_t, Z_0) || Po(Z_{t-1} | Z_t)] ]$ (8)\nIn practice, this can be decomposed into reconstruction error and\nKL divergence terms for each step, which are optimized accordingly."}, {"title": "2.2 Shared Encoder", "content": "To enhance feature representation and improve subsequent image\nrestoration and geo-localization, we utilize the widely adopted state-\nof-the-art transformer-based model, Swin Transformer [20], as the\nshared encoder in our unified framework. The Swin Transformer\nis a hierarchical transformer that employs shifted windows, which\nrestricts attention computation to non-overlapping local windows,\nmaking it adaptable for modeling at various scales. To balance\ncomputational overhead and inference speed, we select the tiny\nversion of Swin Transformer as the default backbone."}, {"title": "2.3 Restoration Module", "content": "The restoration module utilizes a straightforward CNN-based en-\ncoder architecture, consisting of three deconvolutions, an upsam-\npling, and a Tanh activation function. It facilitates geo-localization\nby revealing clean features at multiple scales and produces weather-\nfree images. We adopt a simple Mean Squared Error (MSE) as the\nloss function of the restoration subnetwork.\n$L_{res} = \\frac{1}{n} \\sum_{i=1}^{n}(z_{0,i} - \\hat{z_{0i}})^2$ (9)\nwhere n denotes the patch size. It can minimize the pixel-wise\ndifference between the clean image zo,i and the estimated weather-\nfree image $ \\hat{z_0}$ ."}, {"title": "2.4 Diffusive Matching Module", "content": "Feature extraction. MCGF introduces the latest transformer-based\nvisual representation, EVA-02, as the backbone of Econtent (.) in the\nnetwork. In fact, EVA-02 has shown superior performance in most\nCV downstream tasks. EVA's architecture is a vanilla ViT encoder\nthat can be regarded as a student model, with a shape following ViT\ngiant and the vision encoder of BEiT-3. A big dataset, consisting\nof several typical and openly accessible datasets with 29.6 million\nimages in total, is used as pre-training data. After pre-training, EVA\nis scaled up to 1.0B parameters compared to CLIP. Based on the\ntheory of EVA, larger CLIP-like models will provide more robust\ntarget representations for masking image modeling.\nLoss calculation. Because the training set and the testing set do\nnot overlap in terms of image-matching categories, there are many\nnew categories in the test set. During training, since there is a\nlarge dataset, the cross-entropy loss function can better converge\nthe model. However, for new categories in the test set, the cosine\ndistance can solve this problem well. So the feature map extracted\nby Econtent (.) encoder is fed into a multilayer perceptron (MLP) to\ncalculate the cross-entropy loss for training or cosine distance for\ntesting. MLP includes 2 dense layers, a Batch Normalization (BN)\nlayer, a drop out layer, and a softmax activation function.\nOptimization. MCGF contains two loss functions, one is image\nrestoration loss Lres, and the other is matching loss Lmat. The joint\noptimization between image restoration and geo-localization is\nachieved through total loss function Lall. At every time t, denoised\nimages 2 and matching images m are obtained, and the joint\noptimization is achieved by minimizing the cumulative loss Lall.\n$L_{mat} = \\frac{1}{n} \\sum_{i=1}^{n} (m_{o,i} - \\hat{m_{oi}})^2$ (10)\n$Lall = L_{res} + L_{mat}$ (11)\nwhere n denotes the patch size. In the gradual denoising process of\nthe restoration module, the diffusive matching module can grad-\nually obtain clearer drone-viewing images as input. This process\nenables the matching model to run at multiple granularities, result-\ning in more accurate matching results."}, {"title": "3 Experiment", "content": "Dataset. University160k-WX[21] is a multi-weather cross-view geo-\nlocalization dataset, which extends the University-1652 dataset with\nextra 167,486 satellite-view gallery distractors. University160k-WX\nfurther introduces weather variants on University160k, including\nfog, rain, snow and multiple weather compositions. These distractor\nsatellite-view images have a size of 1024 \u00d7 1024 and are obtained\nby cutting orthophoto images of real urban and surrounding areas.\nMultiple weathers are randomly sampled to increase the difficulty\nof representation learning.\nImplement details. We employed the EVA-02 model, which is\nbased on the Vision Transformer, as the backbone for diffusive\nmatching module. This model has been trained and fine-tuned on\nmany large vision datasets. In our experiments, we resized each\ninput image to a fixed size of 448 \u00d7 448 pixels. During training, we\nused SGD as the optimizer with a momentum of 0.9 and weight\ndecay of 5 \u00d7 10 \u2013 4), with a mini-batch size of 16. The initial\nlearning rate was set to 0.01 for the backbone layer and 0.1 for the\nclassification layer. Our model was built using Pytorch.\nEvaluation metrics. The performance of our method is evaluated\nby the Recall@K (R@K) and the average precision (AP). R@K de-\nnotes the proportion of correctly localized images in the top-K list,\nand R@1 is an important indicator. AP is equal to the area under\nthe Precision-Recall curve. Higher scores of R@K and AP indicate\nbetter performance of the network."}, {"title": "3.1 Geo-localization results", "content": "We train MCGF with outstanding algorithms (including LPN[22],\nMBEG[23], and Muse-Net[17]) on the University-160k-WX train\nset until convergence and obtain optimal results. We test all trained\nmodels on the official unified test set provided by the competition\norganizer. All test results can be displayed and downloaded on the\ncompetition result submission platform. Table 1 shows that MCGF is\nsignificantly better than existing methods in all evaluation metrics.\nEspecially compared with the latest research Muse-Net, MCGF\ncan achieve a 67.75% performance improvement in the Recall@1\nindicator. MCGF shows considerable potential for geo-localization\nas a general framework."}, {"title": "3.2 Visualization", "content": "As shown in Figure 3, we visualise heatmaps and Top-5 matching\nresults generated by our method in 10 different weather conditions.\nSince the drone is flying around, the drone images is not only\ninterfered by weather but also by rotational posture. Therefore, we\nalso show the impact of drone posture changes on geo-localization\nin Figure 3. The heatmap shows that our method can accurately\nextract the shape and relative position of geographic targets under\nweather and pose interference. From the matching results shown,\nwe observe that our model obtains the true match in the Top-1 yet\nthe remaining matching results are inconsistent under 10 different\nconditions, which also indicates that the adjusted features still\ncontain a few discrepancies."}, {"title": "4 Conclusion", "content": "This paper presents a multi-weather cross-view geo-localization\nframework designed to dynamically adapt to unseen weather condi-\ntions, which establishes a joint optimization between image restora-\ntion and geo-localization using denoising diffusion models. In image\nrestoration, MCGF uses a shared encoder and a lightweight restora-\ntion module to gradual denoising and obtain clearer drone-viewing\nimages. In geo-localization, MCGF uses a diffusive matching mod-\nule for cross-view matching. The limitation of this method is that\nit needs to take a long time to train. For future research, it pro-\nvides a joint optimization framework based on the diffusion model,\nwhich can be applied to other tasks, such as matching watermarked\nimages, matching stained images, and matching occluded images."}]}