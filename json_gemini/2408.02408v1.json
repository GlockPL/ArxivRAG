{"title": "Multi-weather Cross-view Geo-localization Using Denoising Diffusion Models", "authors": ["Tongtong Feng", "Qing Li", "Xin Wang*", "Mingzi Wang", "Guangyao Li", "Wenwu Zhu*"], "abstract": "Cross-view geo-localization in GNSS-denied environments aims to determine an unknown location by matching drone-view images with the correct geo-tagged satellite-view images from a large gallery. Recent research shows that learning discriminative image representations under specific weather conditions can significantly enhance performance. However, the frequent occurrence of unseen extreme weather conditions hinders progress. This paper introduces MCGF, a Multi-weather Cross-view Geo-localization Frame-work designed to dynamically adapt to unseen weather conditions. MCGF establishes a joint optimization between image restoration and geo-localization using denoising diffusion models. For image restoration, MCGF incorporates a shared encoder and a lightweight restoration module to help the backbone eliminate weather-specific information. For geo-localization, MCGF uses EVA-02 as a backbone for feature extraction, with cross-entropy loss for training and cosine distance for testing. Extensive experiments on University160k-WX demonstrate that MCGF achieves competitive results for geo-localization in varying weather conditions.", "sections": [{"title": "1 Introduction", "content": "Cross-view geo-localization [1] aims to determine an unknown location by matching drone-view images with the correct geo-tagged satellite-view images from a large gallery, based on geographic features in the images, as shown in Figure 1. This task is crucial for accurate navigation and safe planning [2-4] in GNSS-denied autonomous drone flights. Recent advances in vision transformer have led to significant breakthroughs in various cross-view geo-localization tasks, such as drone localization [5, 6] (matching drone-view query images with geo-tagged satellite-view images) and drone navigation[7, 8] (using satellite-view query images to guide drones to a target area). However, varying weather conditions, including fog, rain, snow, wind, light, dark, and combinations of multiple weather types, reduce visibility, corrupt the information captured by an image, significantly complicate image geographic representation, and lead to a sharp decline in task performance. The major challenge lies in adaptively achieving unbiased image geographic representation under diverse weather conditions.\nA clean image without any weather degradation is desired in cross-view geo-localization. Early methods for weather removal using empirical observations [9], Convolutional Neural Networks (CNNs) based and transformer-based for deraining[10], dehazing[11], and desnowing [12]. Most of these methods achieve excellent performance, but these are not generic solutions for all adverse weather removal problems as the networks have to be trained separately for"}, {"title": "2 Method", "content": "MCGF establishes a joint optimization between image restoration and geo-localization using denoising diffusion models. In image restoration, MCGF uses a shared encoder and a lightweight restoration module to gradual denoising and obtain clearer drone-viewing images. In geo-localization, MCGF uses a diffusive matching module for cross-view matching, which can make the matching module run at multiple granularities, resulting in more accurate matching results. The overview structure of MCGF is shown in Figure 2."}, {"title": "2.1 Denoising Diffusion Models", "content": "The diffusion model is a probabilistic model that has attracted considerable interest in the computer vision community. It can remarkably approximate the original data distribution by gradually adding Gaussian noise to the training data and learning to reverse this diffusion process.\nThe forward process is a fixed Markov Chain that sequentially corrupts the data $z_0 \\sim q_e(z_0)$ at T diffusion time steps, by injecting Gaussian noise according to a variance schedule $\\beta_1, ..., \\beta_T$. Given the clean drone-view images $z_0$, the forward process at step t is defined as:\n$q_0(z_t | z_{t-1}) = N(z_t; \\sqrt{a_t}z_{t-1}, \\beta_tI)$ (1)\n$q_0(z_{1:T} | z_0) = \\prod_{t=1}^T q_0(z_t | z_{t-1})$ (2)\n$q_e(z_t | z_0) = N(z_t; \\sqrt{\\bar{a}_t}z_0, (1 - \\bar{a}_t)I)$ (3)\nwhere $a_t$ and $\\beta_t$ are noise schedule parameters, $\\bar{a}_t = \\prod_{i=1}^t a_i$ and $a_t = 1 - \\beta_t$.\nThe reverse process attempts to remove the noise added in the forward process. The reverse process defined by the joint distribution $p_\\theta(z_{0:T})$ is a Markov Chain with learned Gaussian denoising transitions starting at a standard normal prior $p_\\theta(z_T) = N(z_T; 0; I)$. At step t, the reverse process is defined as:\n$p_\\theta(z_{0:T}) = p(z_T) \\prod_{t=1}^T p_\\theta(z_{t-1} | z_t)$ (4)\n$p_\\theta(z_{t-1} | z_t) = N(z_{t-1}; \\mu_\\theta(z_t, t), \\Sigma_\\theta(z_t, t))$ (5)\nFor simplicity, we assume $\\Sigma_\\theta$ is a known constant, thus the reverse process simplifies to:\n$p_\\theta(z_{t-1} | z_t) = N(z_{t-1}; \\mu_\\theta(z_t, t), \\sigma^2I)$ (6)\nHere the reverse process is parameterized by a neural network that estimates $\\mu_\\theta(z_t, t)$ and $\\Sigma_\\theta(z_t, t))$. The forward process variance schedule $\\beta_t$ can be learned jointly with the model or kept constant, ensuring that $z_t$ approximately follows a standard normal distribution.\nThe training objective of the denoising diffusion model is to maximize the likelihood of the reverse process, which can be achieved by minimizing the variational lower bound (VLB) of the negative log-likelihood. The VLB is given by:\n$L_{VLB} = E_q [-log p_\\theta(z_0) + D_{KL}]$ (7)\n$D_{KL} = \\sum_{t=1}^T D_{KL} [q_0(z_{t-1} | z_t, z_0) || p_\\theta(z_{t-1} | z_t)]$ (8)\nIn practice, this can be decomposed into reconstruction error and KL divergence terms for each step, which are optimized accordingly."}, {"title": "2.2 Shared Encoder", "content": "To enhance feature representation and improve subsequent image restoration and geo-localization, we utilize the widely adopted state-of-the-art transformer-based model, Swin Transformer [20], as the shared encoder in our unified framework. The Swin Transformer is a hierarchical transformer that employs shifted windows, which restricts attention computation to non-overlapping local windows, making it adaptable for modeling at various scales. To balance computational overhead and inference speed, we select the tiny version of Swin Transformer as the default backbone."}, {"title": "2.3 Restoration Module", "content": "The restoration module utilizes a straightforward CNN-based encoder architecture, consisting of three deconvolutions, an upsampling, and a Tanh activation function. It facilitates geo-localization by revealing clean features at multiple scales and produces weather-free images. We adopt a simple Mean Squared Error (MSE) as the loss function of the restoration subnetwork.\n$L_{res} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{z}_{0,i} - z_{0,i})^2$ (9)\nwhere n denotes the patch size. It can minimize the pixel-wise difference between the clean image $z_{0,i}$ and the estimated weather-free image $\\hat{z}_{0,i}$."}, {"title": "2.4 Diffusive Matching Module", "content": "Feature extraction. MCGF introduces the latest transformer-based visual representation, EVA-02, as the backbone of $E_{content}(.)$ in the network. In fact, EVA-02 has shown superior performance in most CV downstream tasks. EVA's architecture is a vanilla ViT encoder that can be regarded as a student model, with a shape following ViT giant and the vision encoder of BEiT-3. A big dataset, consisting of several typical and openly accessible datasets with 29.6 million images in total, is used as pre-training data. After pre-training, EVA is scaled up to 1.0B parameters compared to CLIP. Based on the theory of EVA, larger CLIP-like models will provide more robust target representations for masking image modeling.\nLoss calculation. Because the training set and the testing set do not overlap in terms of image-matching categories, there are many new categories in the test set. During training, since there is a large dataset, the cross-entropy loss function can better converge the model. However, for new categories in the test set, the cosine distance can solve this problem well. So the feature map extracted by $E_{content}(.)$ encoder is fed into a multilayer perceptron (MLP) to calculate the cross-entropy loss for training or cosine distance for testing. MLP includes 2 dense layers, a Batch Normalization (BN) layer, a drop out layer, and a softmax activation function.\nOptimization. MCGF contains two loss functions, one is image restoration loss $L_{res}$, and the other is matching loss $L_{mat}$. The joint optimization between image restoration and geo-localization is achieved through total loss function $L_{all}$. At every time t, denoised images $\\hat{z_0}$ and matching images $m_0$ are obtained, and the joint optimization is achieved by minimizing the cumulative loss $L_{all}$.\n$L_{mat} = \\frac{1}{n} \\sum_{i=1}^n (m_{0,i} - \\hat{m}_{0,i})^2$ (10)\n$L_{all} = L_{res} + L_{mat}$ (11)\nwhere n denotes the patch size. In the gradual denoising process of the restoration module, the diffusive matching module can gradually obtain clearer drone-viewing images as input. This process enables the matching model to run at multiple granularities, resulting in more accurate matching results."}, {"title": "3 Experiment", "content": "Dataset. University160k-WX[21] is a multi-weather cross-view geo-localization dataset, which extends the University-1652 dataset with extra 167,486 satellite-view gallery distractors. University160k-WX further introduces weather variants on University160k, including fog, rain, snow and multiple weather compositions. These distractor satellite-view images have a size of 1024 \u00d7 1024 and are obtained by cutting orthophoto images of real urban and surrounding areas. Multiple weathers are randomly sampled to increase the difficulty of representation learning.\nImplement details. We employed the EVA-02 model, which is based on the Vision Transformer, as the backbone for diffusive matching module. This model has been trained and fine-tuned on many large vision datasets. In our experiments, we resized each input image to a fixed size of 448 \u00d7 448 pixels. During training, we used SGD as the optimizer with a momentum of 0.9 and weight decay of 5 \u00d7 10 \u2013 4), with a mini-batch size of 16. The initial learning rate was set to 0.01 for the backbone layer and 0.1 for the classification layer. Our model was built using Pytorch.\nEvaluation metrics. The performance of our method is evaluated by the Recall@K (R@K) and the average precision (AP). R@K denotes the proportion of correctly localized images in the top-K list, and R@1 is an important indicator. AP is equal to the area under the Precision-Recall curve. Higher scores of R@K and AP indicate better performance of the method."}, {"title": "3.1 Geo-localization results", "content": "We train MCGF with outstanding algorithms (including LPN[22], MBEG[23], and Muse-Net[17]) on the University-160k-WX train set until convergence and obtain optimal results. We test all trained models on the official unified test set provided by the competition organizer. All test results can be displayed and downloaded on the competition result submission platform. Table 1 shows that MCGF is significantly better than existing methods in all evaluation metrics. Especially compared with the latest research Muse-Net, MCGF can achieve a 67.75% performance improvement in the Recall@1 indicator. MCGF shows considerable potential for geo-localization as a general framework."}, {"title": "3.2 Visualization", "content": "As shown in Figure 3, we visualise heatmaps and Top-5 matching results generated by our method in 10 different weather conditions. Since the drone is flying around, the drone images is not only interfered by weather but also by rotational posture. Therefore, we also show the impact of drone posture changes on geo-localization in Figure 3. The heatmap shows that our method can accurately extract the shape and relative position of geographic targets under weather and pose interference. From the matching results shown, we observe that our model obtains the true match in the Top-1 yet the remaining matching results are inconsistent under 10 different conditions, which also indicates that the adjusted features still contain a few discrepancies."}, {"title": "4 Conclusion", "content": "This paper presents a multi-weather cross-view geo-localization framework designed to dynamically adapt to unseen weather conditions, which establishes a joint optimization between image restoration and geo-localization using denoising diffusion models. In image restoration, MCGF uses a shared encoder and a lightweight restoration module to gradual denoising and obtain clearer drone-viewing images. In geo-localization, MCGF uses a diffusive matching module for cross-view matching. The limitation of this method is that it needs to take a long time to train. For future research, it provides a joint optimization framework based on the diffusion model, which can be applied to other tasks, such as matching watermarked images, matching stained images, and matching occluded images."}]}