{"title": "Lifelong Sequential Knowledge Editing without Model Degradation", "authors": ["Akshat Gupta", "Phudish Prateepamornkul", "Maochuan Lu", "Ahmed Alaa", "Thomas Hartvigsen", "Gopala Anumanchipalli"], "abstract": "Prior work in parameter-modifying knowledge editing has shown that large-scale sequential editing leads to significant model degradation. In this paper, we study the reasons behind this and scale sequential knowledge editing to 10,000 sequential edits, while maintaining the downstream performance of the original model. We first show that locate-then-edit knowledge editing methods lead to overfitting on the edited facts. We also show that continuous knowledge editing using these methods leads to disproportionate growth in the norm of the edited matrix. We then provide a crucial insight into the inner workings of locate-then-edit methods. We show that norm-growth is a hidden trick employed by these methods that gives larger importance to the output activations produced from the edited layers. With this \u201cimportance hacking\", the edited layers provide a much larger contributions to the model's output. To mitigate these issues, we present ENCORE - Early stopping and Norm-Constrained Robust knowledge Editing. ENCORE controls for overfitting and the disproportionate norm-growth to enable long-term sequential editing, where we are able to perform up to 10,000 sequential edits without loss of downstream performance. ENCORE is also 61% faster than MEMIT and 64% faster than AlphaEdit on Llama3-8B.", "sections": [{"title": "1. Introduction", "content": "Knowledge editing is the task of editing specific facts that a model learns during pretraining in a data and computing efficient manner (De Cao et al., 2021; Yao et al., 2023; Hartvigsen et al., 2024). In this paper, we focus on a category of parameter-modifying knowledge editing methods called \"locate-then-edit\" methods, which modify only a small subset of the model parameters to add or update knowledge. These methods have been the focus of many recent works (Gu et al., 2024; Gupta et al., 2024a;c; Ma et al., 2024; Kolbeinsson et al., 2024; Fang et al., 2024). While prior work showed knowledge editing methods like ROME (Meng et al., 2022a) and MEMIT (Meng et al., 2022b) lead to a catastophic loss of downstream performance within a few hundred sequential edits (Gu et al., 2024; Gupta et al., 2024b), a recently introduced method called AlphaEdit (Fang et al., 2024) has been able to push this to 3000 edits. In this paper, we aim to enable large-scale sequential knowledge editing without causing model degradation. Towards this goal, we push sequential knowledge editing to what we propose as the next frontier - performing 10,000 sequential knowledge edits without loss of downstream performance.\nOur work is based on two main observations. Firstly, we show that existing locate-then-edit knowledge editing methods are prone to overfitting on the edited fact, where the output probability of an edited fact is unusually higher when compared to the confidence with which an unedited model predicts facts. Secondly, we show that sequential knowledge updates made to a model consistently lead to an increase in the norm of the weight matrix being edited, as shown in Figure 1. We show that this norm-growth is a secret trick used by locate-then-edit methods, which we call \"importance hacking\". The increasing norm of the edited matrix also increases the norm of the activation vectors produced by the edited layers on average. This allows the outputs produced from the edited layers to have a larger influence"}, {"title": "2. Background and Related Work", "content": "In this section, we provide a brief introduction to locate-then-edit knowledge editing methods and present them as a two-step fine-tuning process. For a more detailed introduction to these methods, we refer the reader to prior works (Meng et al., 2022a;b; Gupta et al., 2024c).\nLocate-then-edit family of methods like ROME (Meng et al., 2022a), MEMIT (Meng et al., 2022b) and AlphaEdit (Fang et al., 2024) are used to update facts that can be represented in the form of triplets of the form (subject, relation, object) or (s, r, o). Instead of updating all the weights of a model to incorporate new knowledge, these methods only update certain weight matrices that are most responsible for factual recall (Meng et al., 2022a). The location of an edit within a model is described by a two-dimensional address - (i) an intermediate layer to be edited and (ii) a token from the list of input tokens used to create the target representation. The exact layer to be edited is found using causal tracing (Meng et al., 2022a) or an empirical sweep over all decoder layers of the model (Hase et al., 2024; Yoon et al., 2024). Additionally, updating the second MLP layer in the FFN module of decoder layers has shown optimal knowledge editing performance (Geva et al., 2020; Meng et al., 2022a). This provides the first part of the editing address. Meng et al. (2022a) also showed that using the output representation of the subject token of a sentence produces the best editing results. This provides the second part of the editing address, where the edit is made using the position index of the last token of the subject. We explain this with an example below.\nGiven a fact to be edited, for example - \"The capital of Malaysia is Singapore\", the query phrase for the editing process is \"The capital of Malaysia is\u201d and the target phrase is \"Singapore\u201d. The first part of the editing address, the exact layer whose second MLP matrix gets edited, is decided before the editing begins. The second part of the editing address is the token index of the last subject token, which in this case would be the last subword-token in \u201cMalasiya\u201d. The intermediate hidden representation of this last subject token is used to make the edit."}, {"title": "3. Methods, Models, Datasets and Evaluation", "content": "In this paper, we focus on three \u201clocate-then-edit\" methods - ROME (Meng et al., 2022a), MEMIT (Meng et al., 2022b) and AlphaEdit (Fang et al., 2024). As discussed in section 2, each of these algorithms use gradient descent to find intermediate target representations followed by a second optimization step which is linear in the argument. ROME uses an equality-constraint to enforce memorization in place of the least square constraint as shown in equation 1, whereas equation 1 represents the objective function used in MEMIT. AlphaEdit adds a null-space projection term in the MEMIT objective.\nPrior work has shown model degradation during sequential editing with ROME and MEMIT (Gu et al., 2024; Gupta et al., 2024b), whereas AlphaEdit is a recent method that is able to perform sequential editing for up to 3,000 facts. In this paper, we adopt the experimental setting of AlphaEdit where they perform sequential edits in batches of 100 facts. This means that 100 facts are edited into the model with each weight update, and multiple such updates are performed sequentially. Since ROME only allows for one edit to be made at a time to the model, we used the batched generalization of ROME, called EMMET (Gupta et al., 2024c), which uses the same equality-constraint objective as ROME but generalizes it to the batched editing case.\nWe evaluate all algorithms on three representative models - GPT2-XL (Radford et al., 2019), Llama2-7B (Touvron et al., 2023) and Llama3-8B (Yoon et al., 2024). All experiments are performed on the CounterFact (Meng et al., 2022a) and zsRE (Mitchell et al., 2021) datasets, which are standard knowledge editing datasets. We present the results for Llama2-7B and Llama3-8B on CounterFact dataset in the main paper and present the remaining results in the appendix due to space constraints.\nIn this paper, we evaluate the editing algorithms along two dimensions - editing performance and downstream performance. The editing performance evaluates the success of the knowledge editing algorithm in making successful edits, while downstream performance evaluates the extent of model degradation following prior work (Fang et al., 2024; Gupta et al., 2024b; Gu et al., 2024).\nKnowledge Editing Metrics: To evaluate editing performance, we use five standard knowledge editing metrics (Meng et al., 2022a). (i) Efficacy Score (ES), which measures if an edit was successfully made, (ii) Paraphrase Score (PS), which measures if the model is able to recall edited facts in different scenarios, (iii) Neighborhood Score (NS), which measures if edited facts disturbs unrelated knowledge, (iv) Overall Score (S), which is the harmonic mean of ES, PS and NS, and (v) Generation Entropy (GE), which measures the fluency of a model. A detailed explanation of these"}, {"title": "4. Overfitting during Knowledge Editing", "content": "As discussed in section 2, locate-then-edit methods can be seen as a two-step fine tuning process. The first step uses gradient descent to create intermediate target representations. The gradient descent step minimizes average cross-entropy loss for predicting the target fact over the query phrase augmented by 'N' random prefixes. The random prefixes are supposed to represent different contexts in which the edited fact can be recalled, thus aiming to create a more general query representation. The average cross-entropy loss over all contexts is optimized as shown:\n$L(\\theta) = -\\frac{1}{j=N} \\sum_{j=1}logP_{\\theta}[o^*|x_j + p]$"}, {"title": "4.1. MPES and Knowledge Editing", "content": "To overcome this overfitting over a small subset of edited facts, we propose a variant of early stopping called \"most-probable early stopping\" (MPES). Conventionally, early stopping is used during training while monitoring validation loss, where training is halted when the validation loss stops improving. In MPES, we stop the gradient descent process in knowledge editing when the target fact becomes the most probable token for all 'q' query phrases used for optimization. While the gradient descent loss continues to go down and the target probability continues to increase, the rank of"}, {"title": "5. Norm Growth during Sequential Knowledge Editing", "content": "Past work has shown that sequential knowledge editing can lead to an increase in the norm of the edited matrix (Gupta et al., 2024b). This disproportionate growth of norm can be seen in Figure 1 and 4 when using AlphaEdit and MEMIT on Llama3-8B. Both AlphaEdit and MEMIT modify the second MLP matrix of layers 4 to 8. To put the norm growth in perspective with the rest of the model, we plot the norms of the edited matrices after 5k and 10k edits along with the norm of other MLP matrices of the model. We clearly see the disproportionate growth in the norm for the edited layers in Figure 4, where the norm grows to more than 10 times its original value when using MEMIT, and twice the original norm when using AlphaEdit, while the norm of the matrices before and after the edited layers is not changed. The growth of the norm of the edited matrix is a continuous process during knowledge editing. As shown in Figure 1, for not even one edit does the norm of the edited matrix remain constant or decrease during editing. The norm always increases!\nBut is this growing norm harmful to the model? We answer this question by analyzing the residual stream and reveal a very important property about the inner workings of knowledge editing methods."}, {"title": "5.1. The Secret Mechanics of Knowledge Editing", "content": "Due to residual connections in an LLM, the final output of a model can be written as a summation of the outputs of the individual sub-modules. Let h\u00b9 represent the intermediate hidden state vectors between each decoder layer. Then, the computations within a layer of a general decoder-only LLMs proceed as follows:\n$f' = LN1(h^{l-1})$\n$a' = Att(f')$\n$g' = LN2(h^{l-1} + a')$\n$m^l = W_{projo}(W_g + b'_{fc}) + b_{proj}$\n$h^l = h^{l-1} + a + m^l$\nThe intermediate hidden vector between each layer, h\u00b9, is also sometimes referred to as the residual stream. LN1 represents the first LayerNorm (or equivalently RMSNorm for Llama models) that acts just before the attention module and LN2 represents the second LayerNorm just before the MLP module. Att represents the self-attention module in an LLM whereas the action of a traditional MLP module is written in terms of the individual weight matrices (Wfc, Wproj). As the vectors computed in the attention and MLP modules get added back to the residual stream at each layer, the residual stream represents a summation of an increasing number of vectors as we go deeper into the model. A non-recursive formula for the output of the transformer just before unembedding is shown below:\n$h^L = h^0 + \\sum_{i=1}^{i=L} a^i + \\sum_{i=1}^{i=L} m^i$\nHere, L represents the total number of layers in a model and h\u00b9 represents the residual stream after the final layer. Note that he is the activation just before the application of the final layernorm and unembedding matrices. Thus, the output vector at the final layer is a summation of the outputs of individual self-attention and MLP sub-modules.\nNow, if the norm of the  matrix grows as disproportionately as shown in Figures 1 and 4, the norm of the vectors that are produced from those edited MLP sub-modules will also grow. As the norm of a few vectors in the summation in equation 8 grows, these vectors will begin to dominate the sum. Proof for this is shown in Appendix C.1, where we show that if the norm of a vector in a summation grows, the overall sum effectively tends towards that vector."}, {"title": "5.2. Introducing ENCORE", "content": "In the above discussion, we show how growing norm of edited matrices is detrimental to the functioning of edited models and hypothesize that it is the cause of loss of downstream performance. To test our hypothesis, we propose to add an additional term to the preservation-memorization objective to control this norm growth. Thus, we augment the MEMIT objective with a norm-constraint:\n$L(W) = \\lambda_p \\sum_{i=1}^{P} ||\\hat{W}K_0 - W_0K_0||^2 + \\sum_{j=1}^{B} ||\\hat{W}K_1 - V_i||^2  + \\lambda_n||\\hat{W} - W_0||$\nThe original weight matrix is represented by Wo and W represents the edited matrix. The above objective has a"}, {"title": "6. Conclusion", "content": "In this paper we present two major drawbacks of existing knowledge editing methods - (i) overfitting on edited facts and (ii) continuous and disproportionate norm-growth of edited matrix. We also present insights into the inner workings of locate-then-edit methods and show that they achieve edit success using the short-cut of norm-increase, which increases the importance of the output of edited matrices. We then present ENCORE, a faster and stronger knowledge editing method compared to its predecessors. ENCORE is able to perform 10,000 sequential knowledge edits without noticeable loss of downstream performance in about 40% of the time taken by other algorithms for Llama3-8B. While many recent works have shown limitations of knowledge editing methods at scale, we show that with a better understanding and appropriate regularizations, these methods can indeed be scaled. We hope this brings further research and excitement into the field of knowledge editing."}, {"title": "Impact Statement", "content": "This paper advances the field of knowledge editing in LLMs by enabling scalable, updates to a model's stored knowledge without significant model degradation. Such techniques could be beneficial for multiple practical and societal uses, including quick updating of time-sensitive facts such as changing a model's knowledge following natural disasters or newly published research.\nHowever, as with any technology, there are potential risks with this method. The ability to efficiently overwrite and retain specific facts raises concerns about malicious editing or the injection of wrong information. Further research on robust governance and the use of these frameworks is still needed. Overall, we believe this research can lead to more responsive, up-to-date, and ethically aligned LLMs provided that careful attention is paid to responsible use."}]}