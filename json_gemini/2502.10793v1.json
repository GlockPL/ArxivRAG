{"title": "Dynamic Influence Tracker: Measuring Time-Varying Sample Influence During Training", "authors": ["Jie Xu", "Zihan Wu"], "abstract": "Existing methods for measuring training sample influence on models only provide static, overall measurements, overlooking how sample influence changes during training. We propose Dynamic Influence Tracker (DIT), which captures the time-varying sample influence across arbitrary time windows during training. DIT offers three key insights: 1) Samples show different time-varying influence patterns, with some samples important in the early training stage while others become important later. 2) Sample influences show a weak correlation between early and late stages, demonstrating that the model undergoes distinct learning phases with shifting priorities. 3) Analyzing influence during the convergence period provides more efficient and accurate detection of corrupted samples than full-training analysis. Supported by theoretical guarantees without assuming loss convexity or model convergence, DIT significantly outperforms existing methods, achieving up to 0.99 correlation with ground truth and above 98% accuracy in detecting corrupted samples in complex architectures.", "sections": [{"title": "I. INTRODUCTION", "content": "Understanding training sample influence on deep learning models remains a key challenge for model interpretability and robustness. While influence functions [1] and subsequent research [2]-[4] can measure sample influence on models, they only work after model convergence, failing to capture the sample influence during training.\nThis limitation motivates two fundamental questions:\n1) How does sample influence evolve during training?\n2) How can we measure and utilize these time-varying dynamics to improve model training?\nAnswering these questions is challenging. First, analyzing non-converged models requires new theoretical frameworks beyond existing influence methods that rely on model convergence [5]. Second, measuring time-varying influence requires intensive computation of model updates, demanding high computational resources. Third, it is required to analyze multiple aspects of model behavior, such as parameter changes, loss changes, and prediction changes, to capture the complex interplay between samples and model learning dynamics.\nTo address these challenges, we propose Dynamic Influence Tracker (DIT), a query-based framework that measures the time-varying influence of training samples during training. DIT first estimates how removing individual samples affects model parameters in training windows. Then, DIT computes inner products between parameter changes and task-specific query vectors to efficiently measure sample influence on model behaviors.\nUsing DIT, we uncover several key insights about learning dynamics. First, samples show different time-varying influence patterns (Early Influencers, Late Bloomers, Stable Influencers, and Highly Fluctuating), revealing the limitations of tradi-tional static influence analysis methods. Second, sample influences show a weak correlation between early and late stages, revealing the multi-stage nature of deep learning, where learning priorities shift significantly over time. Third, it is more effective and efficient to identify corrupted samples by analyzing sample influence during the model convergence period than the entire training process.\nIn summary, DIT has the following advantages:\n\u2022 Accurate and Dynamic Influence Analysis. DIT achieves state-of-the-art accuracy with a 0.99 correlation to ground truth. As the first approach enabling influence analysis within any training window, DIT enables efficient corruption detection (>98% accuracy) in single-epoch analysis.\n\u2022 Query-Based Multi-faceted Influence Measure. DIT uses queries to evaluate how samples impact multiple aspects of model behavior, providing a comprehensive evaluation during training.\n\u2022 Robustness to Non-Convergence and Non-Convexity. DIT theoretically guarantees influence estimation for ar-bitrary training windows without requiring model con-vergence or loss convexity, supporting real-world deep learning applications."}, {"title": "II. PRELIMINARIES", "content": "Let $\\mathcal{Z} = \\mathcal{X} \\times \\mathcal{Y}$ denote the space of observations, where $\\mathcal{X} \\subset \\mathbb{R}^d$ is the input space and $\\mathcal{Y}$ is the output space. Given a training set $D = \\{z_i\\}_{i=1}^N$ of i.i.d. observations $z_i = (x_i, y_i) \\in \\mathcal{Z}$, a model $f: \\mathcal{X} \\times \\Theta \\rightarrow \\mathcal{Y}$ parameterized by $\\theta \\in \\mathbb{R}^P$, and a loss function $l: \\mathcal{Z} \\times \\Theta \\rightarrow \\mathbb{R}$, we formulate the learning problem as:\n$\\theta = \\underset{\\theta \\in \\Theta}{\\arg \\min} \\frac{1}{N} \\sum_{i=1}^N l(z_i; \\theta).$ (1)\nDefinition 1 (Stochastic Gradient Descent (SGD)). Let $g(z; \\theta) = \\nabla_{\\theta} l(z; \\theta)$, and SGD starts from $\\theta^{[0]}$. For mini-batch $S_t \\subseteq \\{1, ..., N\\}$ and learning rate $\\eta_t$ at step $t$, SGD updates:\n$\\theta^{[t+1]} = \\theta^{[t]} - \\frac{\\eta_t}{\\vert S_t\\vert} \\sum_{i \\in S_t} g(z_i; \\theta^{[t]}), 0 < t \\le T - 1, (2)$\nwhere T is the total number of SGD steps.\nDefinition 2 (Influence Function [1]). The influence function measures the effect of removing sample $z_j$ on optimal parameters $\\hat{\\theta}$, defined as $\\hat{\\theta}_{-j} - \\hat{\\theta}$ where $\\hat{\\theta}_{-j} = \\underset{\\theta}{\\arg \\min} \\sum_{i=1, i \\neq j}^N l(z_i; \\theta)$. For strongly convex losses, it can be approximated as:\n$\\hat{\\theta}_{-j} - \\hat{\\theta} \\approx -\\hat{H}^{-1} \\nabla_{\\theta} l(z_j; \\hat{\\theta}), (3)$"}, {"title": "III. PARAMETER CHANGE IN TIME WINDOW", "content": "A. Problem Formulation\nOur goal is to estimate the impact of training samples during an arbitrary time window $[t_1, t_2]$, where $0 < t_1 < t_2 < T$. We formalize this goal with a counterfactual question: how would the model's parameters change during the interval $[t_1, t_2]$ if a specific sample $z_j$ is not used?\nDefinition 5 (Parameter Change in Time Window). For a time window $[t_1, t_2]$ during SGD training, the parameter change estimates the contribution of a training sample $z_j$ as:\n$\\Delta\\theta^{[t_1, t_2]}_{-j} = (\\theta^{[t_2]} - \\theta^{[t_1]}) - (\\theta^{[t_2]}_{-j} - \\theta^{[t_1]}_{-j}), (5)$\nwhere $(\\theta^{[t_2]} - \\theta^{[t_1]})$ represents the parameter changes under standard SGD within $[t_1, t_2]$, and $(\\theta^{[t_2]}_{-j} - \\theta^{[t_1]}_{-j})$ represents the parameter changes over the same interval when excluding sample $z_j$.\nFor the special case $[0, t]$, starting from the beginning of training, this simplifies to:\n$\\Delta\\theta^{[0, t]}_{-j} = (\\theta^{[t]} - \\theta^{[0]}) - (\\theta^{[t]}_{-j} - \\theta^{[0]}_{-j}) = \\theta^{[t]}_{-j} - \\theta^{[t]}. (6)$\nFor brevity, we denote $\\Delta\\theta^{[t]} = \\Delta\\theta^{[0, t]}$.\nB. Estimation of Parameter Change in Time Window\nWe aim to estimate the parameter change due to the absence of sample $z_j$ over the time window $[t_1, t_2]$, where $0 < t_1 < t_2 \\le T$:\n$\\Delta\\theta^{[t_1, t_2]}_{-j} = (\\theta^{[t_2]}_{-j} - \\theta^{[t_1]}_{-j}) - (\\theta^{[t_2]} - \\theta^{[t_1]}) = (\\theta^{[t_2]}_{-j} - \\theta^{[t_2]}) - (\\theta^{[t_1]}_{-j} - \\theta^{[t_1]}). (7)$\nConsider the normal SGD update for step $t$:\n$\\theta^{[t+1]} = \\theta^{[t]} - \\frac{\\eta_t}{\\vert S_t\\vert} \\sum_{i \\in S_t} g(z_i; \\theta^{[t]}). (8)$\nConsider the SGD update excluding sample $z_j$:\n$\\theta^{[t+1]}_{-j} = \\theta^{[t]}_{-j} - \\frac{\\eta_t}{\\vert S_t\\setminus \\{j\\}\\vert} \\sum_{i \\in S_t \\setminus \\{j\\}} g(z_i; \\theta^{[t]}_{-j}). (9)$"}, {"title": "C. Estimation Error Analysis", "content": "To analyze the quality of this estimator, we derive theoretical bounds on the estimation error $\\Vert \\Delta\\theta^{[t_1, t_2]}_{-j} - \\widehat{\\Delta\\theta^{[t_1, t_2]}_{-j}} \\Vert$. Our analysis provides theoretical guarantees in non-convex settings without requiring model convergence, with bounds that hold for arbitrary training intervals. This makes DIT particularly suitable for real-world deep learning scenarios. The complete analysis can be found in Appendix B."}, {"title": "IV. DYNAMIC INFLUENCE TRACKER", "content": "While Section III establishes how to measure sample influence on model parameters, these parameter changes may not directly reflect their impact on different model behaviors.\nA. Query-Based DIT\nTo understand the impact on model behaviors (e.g., test loss, predictions), we propose Dynamic Influence Tracker (DIT) to project parameter changes onto meaningful directions using query vectors, where each query vector defines a direction of interested model behavior.\nDefinition 6 (Query-based Dynamic Influence Tracker). Let $q: [0, T] \\rightarrow \\mathbb{R}^P$ be a query function that maps time t to a query vector $q(t) \\in \\mathbb{R}^P$. The Query-based Dynamic Influence Tracker for a training sample $z_j$ over the time window $[t_1, t_2]$ is defined as:\n$Q^{[t_1, t_2]}_{-j}(q) = (q(t_2), \\Delta\\theta^{[t_2]}_{-j}) - (q(t_1), \\Delta\\theta^{[t_1]}_{-j}), (19)$\nwhere $\\Delta\\theta^{[t]}_{-j} = \\theta^{[t]}_{-j} - \\theta^{[t]}$ represents the parameter change at time t and $(\\cdot, \\cdot)$ denotes the standard inner product in $\\mathbb{R}^P$.\nUsing the test loss gradient as query vector, $q(t) = \\nabla_{\\theta} l(z_{\\text{test}}; \\theta^{[t]})$, we have:\n$\\begin{aligned} Q^{[t_1, t_2]}_{-j}(q) &= (\\nabla_{\\theta} l(z_{\\text{test}}; \\theta^{[t_2]}), \\Delta\\theta^{[t_2]}_{-j}) - (\\nabla_{\\theta} l(z_{\\text{test}}; \\theta^{[t_1]}), \\Delta\\theta^{[t_1]}_{-j}) \\\\ &\\approx [l(z_{\\text{test}}; \\theta^{[t_2]}_{-j}) - l(z_{\\text{test}}; \\theta^{[t_1]}_{-j})] - [l(z_{\\text{test}}; \\theta^{[t_2]}) - l(z_{\\text{test}}; \\theta^{[t_1]})]. \\end{aligned} (20)$\nDifferent choices of q enable analysis of various model char-acteristics. We can set $q = \\nabla_{\\theta} f(x_{\\text{test}}; \\theta^{[t]})$ measures prediction changes, $q = e_i$ (standard basis vector) examines individual parameter importance, and $q = \\nabla_{\\theta} l(z_j; \\theta^{[t]})$ assesses gradient alignments. A detailed analysis of these query vectors is in Appendix C.\nB. DIT Implementation\nTo compute $Q^{[t_1, t_2]}_{-j}(q)$, we propose Algorithm 1 for collecting essential information during model training to estimate parameter changes, and Algorithm 2 for efficient influence computation using these values."}, {"title": "VII. CONCLUSION", "content": "This paper introduces Dynamic Influence Tracker (DIT), a novel approach for fine-grained estimation of individual training sample influence within arbitrary time windows during training. Our method's query-based design enables multi-faceted analysis of sample influence on various aspects of model performance effectively. Our theoretical analysis pro-vides error bounds without assuming convexity and conver-gence. Extensive experimental results reveal patterns in in-fluence dynamics and show that DIT consistently outperforms existing methods in influence estimation accuracy, particularly for complex models and datasets."}]}