{"title": "Learning to Stop Overthinking at Test Time", "authors": ["Hieu Tran Bao", "Nguyen Cong Dat", "Nguyen Duc Anh", "Hoang Thanh-Tung"], "abstract": "Test time scaling is currently one of the most active research areas that shows promise after training time scaling has reached its limits. Deep-thinking (DT) models are a class of recurrent models that can perform easy-to-hard generalization by assigning more compute to harder test samples. However, due to their inability to determine the complexity of a test sample, DT models have to use a large amount of computation for both easy and hard test samples. Excessive test time computation is wasteful and can cause the \u201coverthinking\u201d problem where more test time computation leads to worse results. In this paper, we introduce a test time training method for determining the optimal amount of computation needed for each sample during test time. We also propose Conv-LiGRU, a novel recurrent architecture for efficient and robust visual reasoning. Extensive experiments demonstrate that Conv-LiGRU is more stable than DT, effectively mitigates the \u201coverthinking\u201d phenomenon, and achieves superior accuracy.", "sections": [{"title": "1 INTRODUCTION", "content": "Recurrent Neural Networks (RNNs) have proven to be highly effective in tackling machine reasoning tasks, demonstrating remarkable capability to manage problems of different complexity levels within a single task [Orvieto et al., 2023, De et al., 2024, Beck et al., 2024]. However, traditional RNNs struggle to autonomously generalize to more complex problems beyond those encountered during training. RNN's sequential architecture and limited memory capacity hinder parallel training and scalability, causing them to fall behind Transformers.\nDespite extensive research on the reasoning capabilities of recurrent models, they are mainly used for simple sequence processing tasks like prefix sum or sequence copying. Some studies [Eyzaguirre and Soto, 2020, Veerabadran et al., 2024] explored their use in visual reasoning but focus on time-dependent tasks like maze-solving or chess. Static environments without explicit reasoning steps like object recognition, remain underexplored. Recent studies in vision-language reasoning have sought to integrate visual understanding into large language models (LLMs) [Lin et al., 2024, Wang et al., 2024]. To enhance reasoning capability, these models employ Chain-of-Thought (CoT), generating step-by-step demonstrations of images, similar to methods used in LLMs [Dong et al., 2024, Thawakar et al., 2025]. However, such approaches often overlook the models' robustness to low-quality and noisy images. Training solely on curated datasets makes them highly vulnerable to inappropriate prompts. Furthermore, the discrete nature of language-based and tree-based reasoning models could lead to exponential complexity as the models try to imitate discrete search algorithms like DFS, BFS, and A* [Lehnert et al., 2024, Yao et al., 2023] In contrast, studies on latent language models have revealed promising signs of enhanced robustness [Hao et al., 2024], and the efficacy of inference in latent space has been confirmed in other fields [Rombach et al., 2022, Radford et al., 2021, Brooks et al., 2024].\nIn this study, we take the first steps toward exploring the reasoning capabilities of visual recurrent models in latent space, in parallel with CoT techniques in LLMs. Motivated by Deep Thinking [Schwarzschild et al., 2021, Bansal et al., 2022], our proposed model can generalize to tackle more complex problems at test time simply by iterating its recurrent units more times and no additional training is needed. Our approach enables zero-shot extrapolation to more challenging environments within the same task. The ability to handle problems under various conditions enables the development of robust and adaptable models, which are crucial for real-world applications and have the potential to apply to LLMs. We explore the effectiveness of RNNs in object recognition tasks using the CIFAR10-C and CIFAR100-C [Hendrycks and Dietterich, 2019] datasets."}, {"title": null, "content": "The key contributions of this study are:\n\u2022 We explore the extrapolation capabilities of recurrent model architectures for simple visual reasoning tasks, specifically object recognition. We demonstrate that recurrent models enable strong extrapolation while utilizing significantly fewer parameters compared to conventional feedforward networks.\n\u2022 We show that the early stopping heuristic in previous works-which forces the model to halt as soon as possible-limits the extrapolation capabilities of RNNs. To enhance performance, we propose using a self-supervised task to estimate the accuracy trend across iterations, allowing us to determine the optimal number of iterations for the main task.\n\u2022 We propose Conv-LiGRU, a novel recurrent model for effective and compute-optimal visual reasoning.\n\u2022 Extensive analysis and experiments show that Conv-LiGRU is more stable than Conv-GRU, better mitigates the \"overthinking\" phenomenon, and achieves superior accuracy compared to previous methods."}, {"title": "2 RELATED WORKS", "content": "Thinking in Language vs. Latent Space: Recent studies, most notably OpenAI-O1 OpenAI [2024], have demonstrated that large language models (LLMs) can handle more complex tasks by thinking longer before answering in natural language. While it brings large language models closer to human-like thinking, this approach activates all layers at any time. Furthermore, its performance depends largely on the thought sequence's length, making it challenging to perform more difficult tasks with fewer computations. Recently, Deepseek-R1 Guo et al. [2025] has utilized a fraction of the large model for inference, allowing adaptation to edge devices. In this study, we focus on implicit reasoning embedded within recurrent models. Reasoning in the latent space allows models to synthesize and refine instance interpretability, akin to a conventional deep model. Xu et al. [2024] extends the latent reasoning to In-Context Learning, showing greater robustness with a faster inference time. As opposed to CoT reasoning, Hao et al. [2024] demonstrates that the continuous space can represent multiple alternative reasoning steps, thereby significantly expanding the model's search space.\nTest-time Training (TTT): Another line of work coinciding with ours is learning at test time by directly updating model parameters on test data without supervision. Previous work has shown that TTT is robust to distribution shifts [Sun et al., 2020, Gandelsman et al., 2022], while Muennighoff et al. [2025] shows that a simple test-time training can beat OpenAI-O1 on the math question. The connection between the RNN update mechanisms, the attention mechanisms,"}, {"title": null, "content": "and the TTT has been highlighted in Sun et al. [2024], therefore a new TTT layer is proposed for the generation of long sequences. Our study leverages self-supervised tasks to estimate the optimal reasoning depth, improving the model's computational efficiency and reasoning performance.\nDeep Thinking: Schwarzschild et al. [2021] demonstrated that recurrent models trained on simple tasks can generalize to harder ones simply by repeating a set of layers more times during testing. However, Bansal et al. [2022] identified the \"overthinking\" problem, where longer inference time leads to worse performance. To mitigate this issue, they proposed Recall architecture and Progressive Loss. The \"Recall\" architecture adds residual connections from the original input to every recurrent input, effectively preventing \u201coverthinking\u201d due to vanishing gradient. Progressive Loss forces the outputs across iterations to be consistent, allowing the hidden representations to converge to a fixed point after some iterations. Bansal et al. [2022] used a very large number of steps for all difficulty levels to ensure that the representations converged because they could not determine the optimal number of steps. To address this, Veerabadran et al. [2024], Ballas et al. [2016] applied ACT Graves [2016], using a sigmoidal unit to decide when to stop iterating. Since stopping probabilities cannot be predetermined, ACT added a \"ponder cost\" to encourage early termination. Building on this, Banino et al. [2021] restricted the stopping probabilities to a predefined prior, allowing control over the termination point by adjusting the prior.\nSchwarzschild et al. [2021], Bansal et al. [2022], Veerabadran et al. [2024] conducted experiments on logical tasks such as prefix sum, maze solving, chess, and pathfinding, demonstrating the logical extrapolation capabilities of recurrent models. However, the extrapolation ability of recurrent models has yet to be explored in classical computer vision tasks like object recognition. Hendrycks and Dietterich [2019] developed corruption datasets CIFAR10-C, CIFAR100-C, and ImageNetC corruption datasets to generalize the model by introducing 15 types of corruption at five different severity levels. Noting the similarity between the experimental setup and the logical tasks, we show that recurrent models trained on lower noise level data can achieve robustness on harder noise levels. We further demonstrate that recurrent models can exhibit extrapolation capabilities in object recognition tasks, mirroring human perception. Under ideal, noise-free conditions, recognition is swift and effortless, whereas challenging conditions demand additional time for pattern identification."}, {"title": "3 METHODOLOGY", "content": null}, {"title": "3.1 DEEP THINKING MODEL OVERVIEW", "content": "We study the deep thinking network that processes input images $X \\in \\mathbb{R}^{C \\times H \\times W}$ in three main stages to explore the"}, {"title": null, "content": "behavior of recurrent models under adaptive computation and extrapolation:\nInput Transformation: The input image X is transformed via a convolutional layer, producing the initial state $h_0$:\n$h_0 = \\sigma(W_{in} * X + b_{in}),$ (1)\nwhere $W_{in}$ and $b_{in}$ are the weights and bias of the convolution, $*$ denotes the convolution operator, and $\u03c3(\u00b7)$ is an activation function such as ReLU.\nThinking Processing: The thinking process employs a recurrent layer with identical input and output shapes, allowing the model to iteratively apply this layer multiple times, similar to an RNN. With this architecture, the model can adjust the number of iterations based on the complexity of the task, a process we refer to as \"thinking\". We formulate this architecture as follows:\n$h_t = f_{rec}(h_{t-1}), t = 1, ..., T_{train}$ (2)\nwhere $f_{rec}(\u00b7)$ represents the recurrent function, and $T_{train}$ is the maximum number of \"thinking\" steps during training.\nTo ensure that the model does not \"forget\" the initial input, Bansal et al. [2022] proposed Recall, which concatenates the input X with $h_{t-1}$ at each \"thinking\" step, resulting in $h_t = f_{rec}([h_{t-1}, X])$. However, this approach requires the hidden feature map size to match the input X, increasing computational complexity. Instead, we integrate $h_{t-1}$ and $h_0$ at each iteration, with $f_{rec}$ designed as a recurrent unit, leading to $h_t = f_{rec}(h_{t-1}, h_0)$. The hidden feature map $h_t$ can be downsampled compared to X, reducing computational complexity compared to Recall. In this study, we experiment with different architectures for $f_{rec}$ and propose a novel design introduced in Section 3.3.\nOutput Prediction: The output at each iteration $\\hat{y}_t$ is generated by applying a readout function:\n$\\hat{y}_t = f_{out}(h_t),$ (3)\nwhere $f_{out}(\u00b7)$ maps the recurrent state to the desired task output.\nBy constraining a consistent target at each iteration, these stages enable us to examine the effect of varying the number of iterations t on the model's performance, highlighting the adaptability and extrapolation capacity of recurrent models."}, {"title": "3.2 ACCURACY-ITERATION RELATIONSHIP ESTIMATION", "content": "Since ground-truth labels are unavailable during testing, assessing the performance on the main task ($T_{main}$) across iterations (t) is nontrivial. To address this, we introduce a simple self-supervised auxiliary task ($T_{aux}$) as a proxy, leveraging its strong correlation with $T_{main}$ to estimate main task's performance. The auxiliary task should satisfy the following assumptions:\nCore Assumptions: 1. $T_{aux}$ shares semantic and structural similarities with $T_{main}$, ensuring a positive correlation between their accuracies over t:\n$corr \\ (Accuracy_{T_{aux}}(t), Accuracy_{T_{main}}(t)) > 0 \\ \\forall t$\n2. The difficulty of $T_{aux}$ positively correlates with $T_{main}$ under both in-distribution (ID) and out-of-distribution (OOD) conditions.\nAuxiliary Task Design: We use the rotation prediction task Balaji et al. [2018] as $T_{aux}$, a simple and effective pretext task that allows the model to learn features that are useful for many downstream tasks. Furthermore, we hypothesize that similar to the recognition task, rotation prediction becomes more difficult under more challenging conditions. The input image X is rotated by one of four angles {$0^\\circ$, $90^\\circ$, $180^\\circ$, $270^\\circ$}, and the model predicts the rotation angle as a four-class classification problem. The auxiliary loss with t iteration is:\n$L_{aux}^t = - \\sum_{k=1}^{4} y[k] log \\hat{y}_t[k],$ (4)\nwhere $y[k]$ is the probability for each angle.\nDuring training, at each update, we use the output of the last iteration $T_{train}$ to calculate the loss function. Combining with the main task loss $L_{main}^{T_{train}}$ which is the cross-entropy loss in classification, we obtain the total training loss:\n$L = L_{main}^{T_{train}} + L_{aux}^{T_{train}}$\nIteration Search for Main Task Improvement: During testing, $Accuracy_{T_{aux}}(t)$ is used to estimate the optimal iteration $t_{opt}$ for $T_{main}$. Given a fixed budget $T_{test}$,\n$t_{opt} = arg \\underset{t \\in [T_{test}]}{max} \\ (Accuracy_{T_{aux}}(t)).$ (5)"}, {"title": null, "content": "Algorithm 1 Testing Phase: Accuracy-Iteration Relationship Estimation\n1: Input: Trained model, test data with D batches, maximum iterations $T_{test}$\n2: Output: Optimal iteration $t_{opt}$\n3: Step 1: Initialize variables\n4: Initialize Correct as a zero array of length $T_{test}$\n5: Step 2: Compute $Accuracy_{T_{aux}}(t)$ during testing\n6: for i = 1, 2, ..., D do Iterate over D test batches\n7:  $h^{(0)} \\leftarrow InputTransformation(Batch)$\n8:  for t = 1, 2, ..., $T_{test}$ do Iterate over $T_{test}$ steps\n9:   $h_t \\leftarrow f_{rec}(h_{t-1}, h_0)$ Recurrent computation\n10:   $\\hat{y}_{main}$, $\\hat{y}_{aux} \\leftarrow f_{out}(h_t)$ Output predictions\n11:   Correct[t] += # Correct auxiliary samples\n12:  end for\n13: end for\n14: Accuracy(t) $\\leftarrow$ Correct[t]/TotalSamples\n15: Step 3: Estimate optimal iteration\n16: $t_{opt} \\leftarrow arg \\underset{t}{max} \\ (Accuracy_{T_{aux}}(t))$\n17: Return: $t_{opt}$\nHence, comparing to the theoretical optimal accuracy given $t^*$, i.e., $A^* = Accuracy_{Train}(t^*)$, our hypothesis estimates the optimal accuracy as\n$A^* = Accuracy_{T_{main}}(t_{opt}).$ (6)\nThis framework offers a scalable and adaptive method for estimating the accuracy of the main task and optimizing performance efficiently in diverse test scenarios. The inference pipeline is described at Algorithm 1."}, {"title": "3.3 CONV-LIGRU", "content": "Existing research has explored various recurrent architectures for image input, including Recursive Convolutional Networks LeCun [2014], GRU-based models Ballas et al. [2016], and Deep Thinking Recall. However, these architectures often fail to achieve stable accuracy across iterations or miss the long term memory.\nIn this study, we propose a novel GRU-based model, Conv-LiGRU, inspired by the light-gated recurrent unit (LiGRU) Ravanelli et al. [2018], which simplifies the GRU by removing the reset gate, replacing tanh with ReLU activation, and applying batch normalization.\nKey features of Conv-LiGRU include:\nRemoval of the Reset Gate: By eliminating the reset gate, Conv-LiGRU streamlines the gating mechanism, reducing the number of parameters, and improving computational efficiency. LiGRU was originally designed for audio data, where the authors argued that the reset gate in GRU might disrupt intermediate features, particularly for continuous data like audio. Similarly, the reasoning process consists of a sequence of thinking steps, where skipping even a single step can lead to incorrect conclusions. Therefore, removing the reset gate is a reasonable choice to ensure a stable flow of information and a consistent, uninterrupted thinking process.\nNormalization and Activation Function: Similar to LiGRU, Conv-LiGRU replaces tanh with ReLU to mitigate the vanishing gradient problem and enhance the model's ability to capture long-range dependencies. Additionally, instead of layer normalization, Conv-LiGRU employs batch normalization He et al. [2016] to ensure stable performance during iterative computations and to better suit image-based data and convolution operations.\nConvolutional State Transitions: Conv-LiGRU adapts LiGRU for image tasks by replacing fully connected layers with convolutions, preserving spatial structure and enhancing performance.\nThe state update equation for Conv-LiGRU is defined as:\n$h_t = z_t \\odot \\hat{h_t} + (1 - z_t) h_{t-1},$ (7)\n$z_t = \\sigma(U_z * h_{t-1} + BN(W_z * h_0)),$ (8)\n$\\hat{h_t} = RELU(BN(W_h * h_0) + U_h * h_{t-1})$ (9)\nwhere $z_t$ is the update gate, $\\hat{h_t}$ is the candidate state, and $\\odot$ denotes element-wise multiplication, $*$ denotes the convolution operation, and $W_z$, $W_h$, $U_z$, and $U_h$ are learnable parameters, RELU, BN are relu activation function and bach normalize layer perspective.\nThese enhancements allow Conv-LiGRU to maintain stability and achieve robust performance across ID and OOD scenarios. Reduced computational complexity and enhanced stability make Conv-LiGRU highly effective for iterative image-based tasks."}, {"title": "4 EXPERIMENT", "content": null}, {"title": "4.1 DATASETS", "content": "We train our models on the CIFAR10 and CIFAR100 and evaluate them using CIFAR10-C and CIFAR100-C datasets.\nCIFAR10 & CIFAR100 are standard benchmarks for image classification. CIFAR10 has 50,000 training and 10,000 testing images across 10 classes, while CIFAR100 spans 100 classes with the same dataset size. We use both for training and validation.\nCIFAR10-C & CIFAR100-C are corrupted versions of CIFAR10 and CIFAR100. They are used to assess models'"}, {"title": "4.2 MODELS AND TRAINING", "content": "Model Architecture. We kept the Image Transformation and Output Prediction components (Section 3.1) to evaluate extrapolation, varying the Thinking Processing between feed-forward and recurrent architectures.\nWe used ResNet [He et al., 2016] with 4 layers, each layer includes 6 convolution blocks, a total of 24 convolutional blocks for the feed-forward model, maintaining equal input-output channels. We compare 3 main architectures: Recall architecture, Conv-GRU, and Conv-LiGRU. All models used 128 feature channels, and recurrent models were trained for $T_{train}$ = 30 iterations, for testing we use the $T_{test}$ = 100.\nTraining used the Adam optimizer [Diederik P. Kingma, 2015] with a weight decay of 0.0002. Datasets were split 80%/20% for training and validation. Following Rusak et al. [2020], Gaussian noise was added to enhance generalization. For self-supervised learning, each input image was randomly rotated by the function $f_{rotate}$ at one of four angles (0\u00b0, 90\u00b0, 180\u00b0, 270\u00b0), so the final augmentation as:\n$x' = f_{rotate}(clip(x + \\delta))$ (10)\nwhere $\\delta \\sim \\mathcal{N}(0, \\sigma^2I)$, $\\sigma$ is the standard deviation of the Gaussian noise, and $x + \\delta$ is clipped to the input range [0,1]$^N$. We set $\\sigma$ = 0.04 equivalent to level-1 Gaussian noise corruption in CIFAR10-C.\nWe trained these models for 200 epochs on CIFAR10 and for 600 epochs on CIFAR100."}, {"title": "4.3 EXTRAPOLATION CAPABILITY OF RECURRENT MODELS", "content": "Recurrent networks are well-known for their generalization capabilities in logical tasks. In our study, we extend this observation to computer vision tasks. By training various recurrent models on clean datasets augmented from CIFAR10 and CIFAR100, and testing them on 15 corruption types from CIFAR10-C and CIFAR100-C, we found that recurrent models consistently outperform feed-forward networks in generalization.\nUnlike feed-forward networks, which operate with fixed computation costs, recurrent models adapt dynamically, iterating more when encountering challenging samples with"}, {"title": "4.3.1 Iterative Outputs", "content": "higher corruption levels. This \"thinking deeper\" ability enhances their generalization performance. Figure 2 highlights this, showing that Conv-GRU and Conv-LiGRU outperform ResNet on most corruption test sets. More results with CIFAR-10C are demonstrated in Appendix B.\nRecurrent models also demonstrate remarkable parameter efficiency, as shown in Table 1, using only one-sixth the parameters of feed-forward networks while achieving superior generalization. Figure 3 further illustrates their adaptability. For example, at noise level 3, the model requires just 6 iterations to achieve 60% accuracy, while at noise level 5, it needs 13 iterations to achieve the same accuracy. Notably, a recurrent model with 24 iterations outperforms a feed-forward network with equivalent computational depth, emphasizing the effectiveness of dynamic depth adjustment.\nBeyond adapting computation during inference, recurrent models learn invariant features that are reusable across iterations. This synergy of dynamic computation and feature reuse makes them powerful for tackling complex computer vision tasks."}, {"title": "4.4 ANALYSIS OF THE UNDERTHINKING PROBLEM", "content": "ACT aims to optimize the iteration count in recurrent models but often limits their reasoning ability. Overemphasis on the \"ponder cost\" can cause RNNs to halt prematurely, effectively reducing them to feedforward networks. To examine the impact of adaptive computation, we incorporated the ACT mechanism into Conv-GRU and Conv-LiGRU and compared its performance with our proposed self-supervised accuracy estimation method.\nWe trained Conv-GRU model by the ACT method with different values for the hyperparameters $\u03c4$ and $\u03f5$, where $\u03c4$ is the weight of the \"ponder cost\" term in the loss function. A larger $\u03c4$ encourages the model to minimize the number of \"thinking\" steps during training. Additionally, if the cumulative stopping probability at each \"thinking\" step exceeds 1 - $\u03f5$, the model terminates the thinking process. Therefore, a smaller $\u03f5$ leads to a longer thinking process. We provide a more detailed explanation of ACT and the hyperparameters $\u03c4$ and $\u03f5$ in the Appendix A. With $\u03c4$ = 0.5 and $\u03f5$ = 1e-5, the model halted after the third iteration (Figure 6a). Lowering $\u03c4$ to 2e-4 extended its iterations to 18 (Figure 6b). However, applying the same parameters to Conv-LiGRU still led to immediate halting after 2 iteration (Figure 6c).\nThis early termination significantly restricts the extrapolation capabilities of recurrent models. Instead, we propose allowing models to compute adaptively with an upper limit defined by $T_{test}$ and estimating the optimal iteration ($t_{opt}$) via a self-supervised task.\nOur approach allows recurrent models to \"think\" freely, leading to notable performance improvements. From Table 2, we observe that Conv-GRU and Conv-LiGRU, when not constrained by ACT, tend to take more thinking steps and achieve superior performance compared to when ACT re-"}, {"title": "4.5 EFFICIENCY OF CONV-LIGRU", "content": "Conv-LIGRU Mitigates Overthinking. Figure 6e shows that Conv-GRU suffers from overthinking on level-5 corruption test sets in CIFAR10-C, affecting both main and auxiliary tasks.\nTo address this, Conv-LiGRU removes the reset gate to better retain information across iterations. This is crucial for handling corrupted data, where feature extraction is more challenging. Figure 6f confirms that Conv-LiGRU significantly reduces overthinking across all corruption test sets, offering a robust solution.\nWhile Recall demonstrates resistance to overthinking (Figure 6d), it requires maintaining full feature map resolution, leading to higher computational costs. Furthermore, its lack of long-term memory results in inferior performance in CIFAR10-C compared to GRU-based models (Table 2)."}, {"title": "4.6 LIMITATION OF ROTATION PREDICTION TASK", "content": "Figure 7 illustrates the likelihood of different classes across iterations, using a sample (Figure 7a) from the \"Ship\" class with a rotation angle of 0 degrees. The sample's features, which include many edge features, allow the model to predict rotation effectively and maintain stability over iterations"}, {"title": "4.7 CONVERGE TO A FIXED POINT DOES NOT ENSURE TO MITIGATE \"OVERTHINKING\"", "content": "Bansal et al. [2022] highlights the relationship between changes in feature maps across iterations and the issue of \"overthinking\". they demonstrate that if $||h_t - h_{t-1}||_2$ converges to 0 in deep thinking models, the feature map reaches a fixed point, making later predictions unchanged and mitigating \"overthinking.\""}, {"title": null, "content": "However, we observe that this assumption is not entirely accurate. Figure 9a illustrates that $||h_t - h_{t-1}||_2$ of Conv-GRU converges to 0 after more than 20 iterations. Nevertheless, Figure 9b reveals that both the classification loss and self-supervised loss of the model exhibit divergence, corresponding to Conv-GRU encountering \"overthinking\" on corruption test sets, as shown in Figure 6e.\nIn contrast, Conv-LiGRU demonstrates greater stability. Specifically, not only does $||h_t - h_{t-1}||_2$ converge to 0 (Figure 9a), but both the main loss and auxiliary loss also converge smoothly to 0 (Figure 9b). Additionally, Figure 6f shows that Conv-LiGRU significantly mitigates the \"overthinking\" phenomenon compared to Conv-GRU.\nIn conclusion, the convergence of $||h_t - h_{t-1}||_2$ does not ensure that the model is free from \"overthinking.\""}, {"title": "5 CONCLUSION", "content": "This study highlights the efficiency and adaptability of recurrent models for object recognition tasks. We demonstrate their strong generalization with fewer parameters compared to feedforward networks. To address the challenge of selecting optimal iterations during testing, we propose a self-supervised method to estimate accuracy trends, enhancing extrapolation capabilities. Additionally, we introduce Conv-LiGRU, a stable and efficient model that mitigates the \"overthinking\" issue and achieves superior accuracy, making it a robust choice for vision-based tasks. These findings pave the way for further advancements in lightweight and adaptive architectures for real-world applications."}, {"title": "A BACKGROUND: ADAPTIVE COMPUTATION TIME (ACT)", "content": "ACT Graves [2016] is a mechanism designed to dynamically determine the number of recurrent steps required to process each input. Unlike its original formulation, which handles variable-length sequences, our work applies ACT to static inputs in visual reasoning tasks.\nAt each time step, the model generates a halting score pt through a learned convolutional layer. The cumulative sum of these scores, $P_t$, determines whether the computation should continue. When $P_t$ reaches a predefined threshold (1 \u2013 $\u03f5$), iteration stops, and the final hidden state is computed as a weighted sum of the intermediate states.\nA key component of ACT is the ponder cost, an auxiliary loss term that encourages the model to minimize the number of recurrent steps while maintaining accuracy. The total loss function consists of the task loss $L_{task}$(y, $\\hat{y}_{act}$) and the ponder cost, weighted by a hyperparameter $\u03c4$ (Equal 11). By tuning $\u03c4$, we control the trade-off between computational efficiency and performance. In our study, we analyze the limitations of ACT's early stopping heuristic and propose a self-supervised approach to better estimate the optimal number of iterations.\n$L = \\frac{1}{D}\\sum_{i=0}^{D} L_{task}(y, \\hat{y}_{act}^{i}) - \\epsilon \\sum_{t=1}^{t_{halt}-1} p_t$ (11)"}, {"title": "B ADDITIONAL SIMULATION RESULTS", "content": "Figure 10 compares the performance of feedforward (ResNet) and recurrent architectures (Conv-GRU, Conv-LiGRU) in deep thinking networks on 15 corruption types at level 5 from the CIFAR10-C test set. The results show that recurrent models outperform feedforward networks in 14 out of 15 corruption types. Detailed accuracy values are provided in Table 4.\nAdditionally, we compare feedforward and recurrent models on CIFAR100-C, with Table 5 listing the accuracy of ResNet, Conv-GRU, and Conv-LiGRU on 15 corruption types at level 5. Both tables confirm the superiority of recurrent models over feedforward networks. Furthermore, Conv-LiGRU surpasses Conv-GRU on most test sets (11 out of 15 on CIFAR10-C and 12 out of 15 on CIFAR100-C), demonstrating its effectiveness and suitability for deep thinking models.\nFigure 11 compares the performance of Conv-LiGRU with and without ACT. The results show that without ACT, Conv-LiGRU outperforms the ACT variant on 9 out of 15 level 5 test sets of CIFAR10-C. Detailed accuracy values for each test set are provided in Table 4. These findings reinforce that deep thinking models can achieve better performance when allowed to think freely rather than being constrained."}]}