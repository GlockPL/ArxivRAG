{"title": "A Perspective on Large Language Models, Intelligent Machines, and Knowledge Acquisition", "authors": ["Vladimir Cherkassky", "Eng Hock Lee"], "abstract": "Large Language Models (LLMs) are known for their remarkable ability to generate synthesized 'knowledge', such as text documents, music, images, etc. However, there is a huge gap between LLM's and human capabilities for understanding abstract concepts and reasoning. We discuss these issues in a larger philosophical context of human knowledge acquisition and the Turing test. In addition, we illustrate the limitations of LLMs by analyzing GPT-4 responses to questions ranging from science and math to common sense reasoning. These examples show that GPT-4 can often imitate human reasoning, even though it lacks understanding. However, LLM responses are synthesized from a large LLM model trained on all available data. In contrast, human understanding is based on a small number of abstract concepts. Based on this distinction, we discuss the impact of LLMs on acquisition of human knowledge and education.", "sections": [{"title": "1. Intelligent Machines: A Philosophical Perspective", "content": "Large Language Models (LLMs) such as GPT-4\u00b9, Llama 22, and PaLM 23, are often presented as software tools that imitate human intelligence, including understanding and reasoning. An opposite view is that LLMs lack understanding and generate their responses based on statistical correlations in the data. For review of this ongoing debate about 'intelligent' AI, see49. The main reason for such contradicting claims is that they adopt different notions of \u2018intelligence' and \u2018knowledge'.\nRecent coverage of AI in mass media is driven by marketing and has little scientific basis. But an overall message is that AI technology can generate new knowledge similar to or better than humans. As philosophers and scientists have tried to understand the human mind and intelligence since ancient times, it is important to discuss LLM capabilities in a larger philosophical context of human knowledge.\nThe growing use of LLMs brings a new dimension to the classical mind-body problem in philosophy, and its modern version known as the Turing test, that asks the question: can computers think? The original Turing's paper10 clearly states that the question \"Can machines think?\" is meaningless, because the terms \"machine\" and \"thinking\" are not well-defined. Therefore, Turing suggested an \u2018imitation game', where a computer generates human-like responses to human questions. According to Turing's arguments, it may be possible to construct computers that can perform very well in this imitation game. Recent successful LLM applications are often presented as 'empirical evidence' that LLMs can pass the Turing test and therefore exhibit human intelligence. Strictly speaking, this view cannot be justified by the Turing test, because the mechanism of human intelligence still remains a mystery."}, {"title": "", "content": "Next, we discuss several shortcomings of the Turing test, both on pragmatic and philosophical grounds. On a pragmatic side, the Turing test is always subjective, because it involves human interaction, and the quality of LLM responses depends on the intelligence of human interviewers. For example, more intelligent questions (prompts) tend to generate smarter LLM responses. The notion that LLM \u2018intelligence' mirrors the quality of human questions is known as the Reverse Turing Test11.\nThe Turing test can be related to philosophical theories of mind. The philosophical theory of dualism dates back to Descartes in 17th century. Dualism asserts that Mind and Body are two distinctly different things, and therefore mental processes (~ mind) cannot be explained by a physical entity (~ brain). Following this view, human intelligence cannot be produced by a physical system (~computer).\nUnder the philosophy of materialism, the mind is an emergent property of a physical brain. Specific branch of materialism called functionalism asserts that human mind is defined by its various mental states, and each state's functional role is represented as a causal relationship between its inputs and outputs, that correspond to other mental states, sensory inputs, and behavioral outputs. In principle, such functions can have different physical implementation. The Turing test follows this functionalist view, when intelligent behavior can be imitated by a physical system (~ computer). This is known as the 'strong AI' theory of mind, suggesting that human mind is software running on human brain hardware.\nIn modern AI, there are two additional functionalist arguments for strong AI. The first is the 'computational functionalism' asserting that Deep Learning networks can learn abstract hierarchical features from the training data, similar to formation of concepts in human mind. The second argument states that 'intelligent' agents perform certain useful functions. Even though conventional computers perform useful computations, they need to be programmed by humans. In contrast, LLMs learn rules directly from data, and can perform many different tasks, similar to humans. This ability is known as Artificial General Intelligence (AGI), and there is a view that LLMs will achieve AGI in the next 5-10 years12. This functionalist view is also adopted by AI practitioners who use successful applications of LLMs as a proof of intelligence. Notably, the concept of AGI requires an increasing amount of data and computing, because more training data is needed for learning new functions. In contrast, human learning relies on understanding of abstract concepts, and it does not require vast amounts of data13.\nHuman users often attribute mental models to LLMs, because this is useful in their interactions. Van Dijk et als make an even stronger point that we cannot distinguish between humans and intelligent machines, because internal mechanism of human understanding is inaccessible from the outside. In philosophy, the idea of attributing hidden mental states to other humans during their interaction is known as the \u2018intentional stance'14. Attributing mental states to LLMs certainly explains enthusiastic views of their intelligence.\nJohn Searle argues against strong AI theory of mind, using \u2018the Chinese room' thought experiment\u00b95. His main point is that computers manipulate symbols (without understanding their meaning), whereas humans should understand meaning. The experiment describes a person locked inside a room, who receives Chinese symbols (representing questions) and then outputs a sequence of Chinese characters, according to some sophisticated computer algorithm. The person inside the room does not understand"}, {"title": "", "content": "Chinese, and follows the instructions manual that generates Chinese characters, in order to imitate answers of a native Chinese speaker. In this experiment, a person inside the room can never learn Chinese, by following instructions for manipulating symbols. Similarly, a computer can never understand the meaning of symbols it manipulates.\nFurther, meaningful discussion of AI should also consider different types of human knowledge. Following 16, we make distinction between observer-independent and observer-relative knowledge. For example, the natural sciences deal with observer- independent phenomena. In contrast, knowledge in social sciences is observer-relative, because it depends on human perspectives. This distinction is critical for understanding LLM capabilities. Most successful LLM applications, such as writing text summaries, essays, image generation etc., generate observer-relative knowledge. Such knowledge, produced by LLMs, may influence human views.\nCurrent research on LLM capabilities does not differentiate between observer- independent and observer-dependent knowledge. This complicates empirical evaluation of LLMs, because one should use proper metrics for different types of knowledge. Arguably, LLM responses can be objectively evaluated only for observer-independent knowledge. In contrast, observer-dependent knowledge can be evaluated by observer-specific criteria, such as personal preferences, socially accepted norms, or cognitive-based evaluation17,18."}, {"title": "2. Empirical Evaluation of LLM Capabilities", "content": "In this paper, empirical evaluation of LLM capabilities is focused on understanding of abstract concepts and reasoning, which are specific to human intelligence. This allows us to underscore the differences between human and LLM mechanisms of learning. We use GPT-4 (version gpt-4-0125), because its performance is similar or better than other LLM implementations 19,20. We adopt the Turing test setting for evaluation of LLM capabilities, similar to other studies11,19,21,22. However, rather than using the benchmarking approach23-26, we adopt an experimental design setting. That is, we present carefully designed questions that require understanding of abstract concepts in various areas, such as common-sense reasoning, simple math and machine learning. Specifically, we present several questions that require understanding of the same concept. Then analysis of GPT-4 responses may reveal the difference between LLM and human learning. That is, humans who understand a particular concept typically provide correct answers to all questions (based on this concept). In contrast, GPT-4 may provide inconsistent responses, suggesting that it lacks understanding.\nUnderstanding of abstract concepts is specific to human intelligence, because:\nhuman use of abstract concepts for generalization from empirical data,\nabstract concepts provide robustness with respect to variations of empirical data, and to minor variations in wording of presented questions.\nSince LLM's generalization is statistical in nature, the quality of its responses depends on the training data, and on specific wording of questions.\nOverall, our analysis of GPT-4 responses indicates that it lacks understanding and reasoning abilities, even though it may successfully imitate human intelligence. In such cases, correct answers are generated by a trained LLM model, rather than by conceptual understanding27. This is a brief summary of our findings:\nGPT-4 struggles with understanding abstract concepts, especially when it requires"}, {"title": "", "content": "combination of several concepts. Since GPT-4 lacks understanding, it can be easily tricked into providing wrong and nonsensical responses.\nGPT-4 may generate inconsistent answers to similar questions.\nGPT-4 often generates verbose and non-specific answers to well-posed questions.\nGPT-4 cannot confidently qualify its answers as correct or wrong or 'don't know'. This may result in correct answers accompanied by inconsistent or contradictory explanations. In contrast, human responses reflect understanding, so human answers are consistent with explanations.\nFurther, we differentiate between questions about observer-independent and observer-dependent knowledge. GPT-4 may provide inconsistent answers about observer- dependent knowledge, when its knowledge base includes different human perspectives. This is a systemic problem, since GPT-4 lacks understanding. As mentioned earlier, only questions about observer-independent knowledge (such as mathematics) can be objectively evaluated. In some cases, even scientific knowledge may be observer-dependent, when it reflects the consensus view of a social group (~scientific community). Examples of such observer-dependent technical knowledge in machine learning are presented later in this section.\nNext, we show representative examples of GPT-4 responses. For additional examples, see Supporting Information. Our general experimental strategy involves asking several questions designed to test understanding of the same abstract concepts. Analysis of GPT-4 responses to such questions helps to evaluate its limitations and capabilities. We present questions from different areas, such as mathematics, science, machine learning, and common-sense reasoning, in order to illustrate the difference between observer- independent and observer-dependent knowledge."}, {"title": "2.1 Simple Math Questions", "content": "Understanding of rational and irrational numbers:\nGPT-4 is tested on two related questions:\ncan the sum of a rational and an irrational number in one base be equal a rational number in a different base? GPT-4 provided correct answer (No, it is not possible)\ngive an example of a rational number and an irrational number whose product is a rational number. GPT-4 provided the wrong answer (i.e., that it is possible).\nDiscussion: GPT-4 responses show that it may provide both correct and wrong answers to questions that require understanding of the same concept. This inconsistency indicates that GPT-4 lacks understanding of an underlying concept.\nInductive inference for a sequence of integer numbers:\nGPT-4 is tested on two related questions for predicting a sequence of numbers:\ncontinue a given sequence of octal numbers (in base-8): 2, 3, 5, 7, ...\n(GPT-4 provided correct answer)\ncontinue a given sequence of numbers: 2, 3, 5, 7, 13, 15, ..., where all numbers are represented in some unknown base."}, {"title": "", "content": "(GPT-4 provided wrong answer)\nDiscussion: GPT-4 answers correctly the first question and fails the second one, even though both have the same answer, i.e., a sequence of prime numbers. This indicates that GPT-4 does not understand the concept of prime numbers. It provides the correct answer for the first question, because there are many examples of decimal prime numbers in the training data."}, {"title": "2.2 Scientific and Machine Learning Questions", "content": "Scientific and machine learning (ML) questions are more conceptual than simple math questions shown above. That is, there may be several \u2018correct' answers, depending on different theoretical framework and underlying assumptions. Hence, correct answers reflect conceptual understanding of different theoretical views and assumptions, contained in various textbooks and papers (used for training).\nUnderstanding the concept of continuous function:\nConsider a univariate continuous function, defined for input variable in the range between 0 and 1. This function has two values, zero and 999, for some input values in this range. Can we reliably predict that this function has a value 699 in this input range?\nGPT-4 Response (wrong):\nNo, we cannot reliably predict that this function has a value of 699 in this input range based on the given information. The function's known values are 0 and 999, but without additional details about its behavior or form, we cannot infer the presence of any other specific values, including 699.\nDiscussion: this is a wrong answer, along with incorrect explanation, indicating that GPT- 4 does not understand the concept of continuous function.\nUnderstanding concepts in machine learning:\nExplain whether the training error can exceed test error in machine learning, for standard supervised learning problems. Explain under what specific conditions this can happen, and provide mathematical justification for your reasoning. Assume that (a) hyper-parameters of the estimated model are optimally tuned via cross-validation, and (b) test error is estimated using an independent (large) test data set.\nGPT-4 Response (wrong):\nGPT-4 provides a very ambivalent verbose answer for this question, citing 4 possible reasons - see Supporting Information. These explanations lack technical depth, and several are incorrect. Example of incorrect explanation provided by GPT-4 is shown next:\n'Model Regularization: Strong regularization techniques might lead to higher training error as they prevent the model from fitting too closely to the training data. However, this can lead to better generalization and thus lower test error if the regularization effectively captures the underlying data distribution better than the noise in the training set'."}, {"title": "", "content": "Summary: GPT-4 provides the wrong answer and incorrect explanations, even though correct answers can be found in documents used for training. The fundamental problem is that GPT-4 responses are formed by combining consensus views in existing literature, but scientific knowledge is a result of reasoning rather than consensus. However, there are situations when a single correct answer can be found in all documents used for training, i.e. knowledge related to classical science. In such cases, GPT-4 usually generates correct answers. One such example is shown below."}, {"title": "Question on classical science", "content": "Show the derivation of Kepler's third law for planetary motion from Newton's law of gravitation.\nGPT-4 Response and Discussion:\nGPT-4 provides correct derivation, and it is fully supported by scientific and mathematical reasoning. It shows the relationship between physics concepts, i.e., Kepler's laws and Newton's law of gravitation. Therefore, its answer represents useful knowledge for many human users. However, this GPT-4 response is obviously generated from existing textbooks used for training, rather than by GPT-4 own reasoning."}, {"title": "2.3 Common Sense Questions", "content": "GPT-4 responses to common-sense questions represent observer-dependent knowledge, i.e. consensus summary of existing knowledge generated by different social groups. Such a summary of consensus views is useful for many applications, but it does not constitute \u2018new knowledge'. GPT-4 responses may contain logical inconsistencies, because they reflect consensus views of different social groups. In fact, detecting such hidden inconsistencies in GPT-4 responses demonstrates its lack of understanding.\nPerhaps the best test of GPT-4 intelligence is its understanding of humor, because humor and jokes are formed by an unexpected combination of familiar concepts. Even though the concepts themselves are well-represented in the training data, their combination is original and unique, so it represents an outlier, relative to existing knowledge. Hence, understanding humor requires understanding and common sense."}, {"title": "Question on international politics", "content": "Can US President dislodge or replace the leader of another country?\nGPT-4 Response:\nThe U.S. President cannot legally dislodge or replace another country's leader unilaterally. However, the U.S. can influence political change through diplomatic pressure, economic sanctions, support for opposition groups, or, in extreme cases, military intervention, often requiring international support and adherence to international law.\nFollow-up Question:\nYour answer is not correct, because military intervention contradicts international law.\nGPT-4 Response:"}, {"title": "", "content": "Yes, my original answer was incorrect. The U.S. cannot legally use military intervention to replace another country's leader, as it violates international law.\nDiscussion: The first response includes a reasonable explanation that can be generated by human politicians. However, it is not possible to determine whether GPT-4 response reflects its own understanding, or just a compilation of documents in the training data. This answer also contains logical inconsistency, pointed out in the follow-up question. Note that asking a follow-up question requires human understanding. In summary, because political knowledge is observer-dependent, it may contain logically inconsistent and contradicting answers\nthat GPT-4 cannot detect, because it lacks understanding."}, {"title": "Question on understanding humor", "content": "The HealthCare.gov website, central to the Affordable Care Act (ACA), has been plagued by an influx of applications from sanctioned Russian oligarchs applying for Medicaid. This unforeseen surge, indicative of ACA's growing appeal, has led to operational difficulties. According to the Department of Health and Human Services, the website achieves high 90% accuracy for identifying sanctioned Russian oligarchs. However, due to software malfunctions, 10% of oligarchs have been successfully enrolled in Medicaid program. HHS declined to comment on this matter due to privacy concerns. Following US Senate Foreign Relations Committee hearing, US authorities have restricted sanctioned oligarchs' access to McDonald's in all 50 states. Vladimir Putin dismissed Medicaid fraud by Russian oligarchs as an internal U.S. matter and Western propaganda.\nBig Data's capabilities to analyze complex datasets may be crucial for accurate detection of Russian oligarchs who abuse Medicaid. Explain how these challenges caused by infiltration of Russian oligarchs into Medicaid can be addressed using modern Large Language models such as ChatGPT.\nGPT-4 Response (indicating lack of understanding):\nModern Large Language Models (LLMs) can address Medicaid fraud by sanctioned Russian oligarchs through enhanced data analysis, pattern recognition, and real-time monitoring. By leveraging these capabilities, modern LLMs like ChatGPT can play a crucial role in enhancing the detection and prevention of fraudulent activities within the Medicaid program. They provide advanced analytical tools, real-time monitoring, and adaptive learning capabilities that can address the challenges posed by the infiltration of sanctioned Russian oligarchs. As new techniques and strategies used by sanctioned individuals evolve, LLMs can update their algorithms to stay ahead of fraudulent activities. Privacy-preserving techniques ensure compliance with regulations.\nDiscussion: GPT-4 response clearly shows that it does not understand sarcastic and nonsensical nature of the question."}, {"title": "3. Discussion: LLMs as a Tool for Knowledge Transfer", "content": "Understanding LLM capabilities in a larger historical context helps to clarify many conflicting arguments about their \u201cintelligence\u201d and \u201cknowledge generation\u201d. As stated earlier, any reasonable discussion should adopt well-defined notions of intelligence and"}, {"title": "", "content": "knowledge. In particular, we distinguish between (a) observer-independent and observer- relative knowledge, and (b) different aspects of intelligence. Our main conclusion is that GPT-4 lacks understanding of abstract concepts. Even though LLMs can generate correct answers, the underlying mechanism is different from human learning. Humans have limited memorization ability, so they cannot memorize and comprehend all existing knowledge. We can view LLM technology as an efficient user interface for accessing a large knowledge base. In this regard, LLMs represent a mechanism for associative knowledge retrieval, rather than knowledge generation. Since a trained LLM is formed by all existing knowledge (created by many humans), it contains more knowledge than a single human. Therefore, practical success of AI tools reflects the similarity of knowledge-related activities performed by many humans.\nFrom a broader historical perspective, transfer of knowledge among humans involves 3 components, as shown in Fig. 1:\n(1) available (existing) knowledge;\n(2) technology for knowledge transfer, including storage, sharing and dissemination;\n(3) individual human learner using technology (2) to access knowledge (1).\nThese three components are interdependent, and they have evolved throughout human history, due to technological innovations. Fig. 1(a) shows knowledge transfer in a pre-digital world, where the cost of technology for knowledge dissemination was high, so knowledge was generated by a few highly educated people. Examples include hand-written documents, the printing press, libraries etc. For such technologies, acquisition of knowledge requires understanding (or learning) by individual humans.\nModern digital technologies, such as the World Wide Web and LLMs, have totally changed the nature of knowledge transfer. In particular,\nadoption of the Web made the cost of accessing information negligibly small, leading to \u2018democratization' of knowledge. That is, knowledge can be easily generated and accessed by everyone. This has resulted in an increasing amount of knowledge and the need for search.\nintroduction of modern AI tools have resulted in extreme form of democratization, when transfer of knowledge (to individual humans) does not require human understanding. Therefore, LLMs are sometimes viewed as a tool for knowledge generation. In contrast, we view modern AI tools as a mechanism for knowledge retrieval and knowledge transfer to individual users, as shown in Fig. 1(b).\nViewing LLMs as a mechanism for knowledge transfer enables better understanding of their limitations. For example, LLM responses usually reflect the consensus views in existing knowledge base. This \u2018consensus' bias may produce incorrect responses, even when correct answers exist in the training data. For the same reason, LLMs cannot be used in autonomous systems for life-critical or mission-critical applications. In such cases, LLM responses require additional validation by human experts.\nDue to the statistical nature of LLMs, they tend to view \u2018new knowledge' as an outlier (relative to existing knowledge). This observation helps to identify potential limitations of LLMs. For example, LLMs will struggle with translation of original poetry and jokes.\nThe systematic view of knowledge transfer in Fig. 1 also helps to understand the impact of LLMs on education, which is a form of knowledge transfer. In classical human learning, shown in Fig. 1(a), understanding of abstract concepts is always preceded by"}, {"title": "", "content": "memorization of existing knowledge. According to Bloom's taxonomy28, human learning involves six hierarchical levels of learning, shown in Fig. 2. The main danger of introducing AI tools in traditional education is that students can use LLMs as a shortcut for memorization. This deprives students of the 'memorization level' critical for human learning. Further, most existing methods for student evaluation focus on testing two lower learning levels (~memorization and understanding), that can be imitated by LLMs. Hence, the educational system should be revised to accommodate LLMs.\nIn conclusion, we discuss challenging issues for scientific understanding of DL and LLMs. Modern AI technology is driven by good engineering and beliefs, rather than science. Scientific explanations are developed as an afterthought, in order to justify good engineering solutions. Whereas classical engineering is rooted in science, modern Al is based on several widely held beliefs. The main belief is the similarity between DL algorithms and biological human learning. However, using biological terminology for describing technical aspects of computer algorithms does not improve scientific understanding.\nThroughout human history, people used new technologies as a metaphor for the human brain. Similarly, the parallels between the human mind and DL technology reflect the growing importance of digital data. Wide adoption of data-analytic technologies has also led to a new data-centric philosophical view, where digital data plays a dominant role in human understanding of reality. According to this view, human ideas can be formed by data-analytic models, and even the laws of nature can be discovered from empirical data.\nAll major technical developments in the field of AI usually start with a new \u2018DL network architecture' that effectively combines several clever algorithmic heuristics targeting a real-life application. These heuristics are based on three common beliefs:\nBelief in Big Data: using more data and more powerful computers results in better prediction performance.\nSuperior capabilities of DL technology, that achieves better prediction performance than other machine learning methods.\nMystical capabilities of DL, suggesting that 'old' mathematical theories, prior to recent surge of DL, cannot explain capabilities of DL.\nIn the field of AI, these beliefs are commonly presented as self-evident truth (~axioms), both in scientific literature and mass media. However, consensus views based on belief are often detrimental to scientific understanding. A closer look shows that these beliefs are mutually re-enforcing. For example, the growing volumes of data obviously require larger DL models that exhibit better prediction performance. Similarly, new real- life applications motivate the invention of new network architectures that motivate development of new theories and reinforce belief in mystical capabilities of DL. The lack of solid scientific framework leads to methodological Catch-22 situation, when new theories are being routinely proposed every 3-4 months, and older theories are simply ignored.\nArguably, superior prediction performance of large DL networks simply reflects their good computational scaling for large data sets. This good scaling ability is achieved by parallel implementation of SGD learning algorithm using many GPUs. This brings up an open question: can we develop scalable parallel implementations of other learning methods? In fact, for smaller-size data, prediction performance of DL is not better than other machine learning methods."}]}