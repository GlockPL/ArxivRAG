{"title": "Learning Disease Progression Models That Capture Health Disparities", "authors": ["Erica Chiang", "Divya Shanmugam", "Ashley N. \u0412\u0435\u0435\u0441\u0443", "Gabriel Sayer", "Nir Uriel", "Deborah Estrin", "Nikhil Garg", "Emma Pierson"], "abstract": "Disease progression models are widely used to inform the diagnosis and treatment of many progressive diseases. However, a significant limitation of existing models is that they do not account for health disparities that can bias the observed data. To address this, we develop an interpretable Bayesian disease progression model that captures three key health disparities: certain patient populations may (1) start receiving care only when their disease is more severe, (2) experience faster disease progression even while receiving care, or (3) receive follow-up care less frequently conditional on disease severity. We show theoretically and empirically that failing to account for disparities produces biased estimates of severity (underestimating severity for disadvantaged groups, for example). On a dataset of heart failure patients, we show that our model can identify groups that face each type of health disparity, and that accounting for these disparities meaningfully shifts which patients are considered high-risk.", "sections": [{"title": "1. Introduction", "content": "In many settings, observed data is used to model the progression of a latent variable over time. Models of human aging use a person's physical and biological characteristics to model progression of their latent \"biological age\" (Pierson et al., 2019); models of infrastructure deterioration use inspection results to model progression of a system's latent overall health (Madanat et al., 1995); and disease progression models, which we focus on in this paper, use observed symptoms to model progression of a patient's latent severity of a chronic disease (Wang et al., 2014). Disease progression models can help predict a patient's disease trajectory and thus personalize care, detect diseases at earlier stages, and guide drug development and clinical trial design (Mould et al., 2007; Romero et al., 2015). They have been applied to a wide variety of progressive diseases such as Alzheimer's disease (Holford and Peace, 1992) and cancer (Gupta and Bar-Joseph, 2008).\nFor the benefits of these models to apply to all patients equitably, it is crucial that they accurately describe progression for all populations of patients. However, disease progression models have typically failed to account for the fact that systemic disparities in the healthcare process can bias the observed data that they are trained on. For example, disparities have been shown to arise along axes such as socioeconomic status (Weaver et al., 2010; Miller and Wherry, 2017), race (Yearby, 2018), and proximity to care (Chan et al., 2006; Reilly, 2021). Accounting for such disparities is important because it can meaningfully shift estimates of disease progression. For intuition, imagine learning that a patient in the emergency room traveled three hours to get there; if their symptoms are ambiguous, this contextual information may increase our estimate of how severe their underlying condition is. Disease progression models have historically been unable to capture this type ofcontext and, as we show, this can lead to biased estimates of severity. To address this, we propose a method for learning disease progression models that interpretably capture three well-documented health disparities:\n1.  Disparities in initial severity. Certain patient groups may start receiving care only when their disease is more severe (Hu et al., 2024).\n2.  Disparities in disease progression rate. Certain patient groups may experience faster disease progression, even while receiving care (Diamantidis et al., 2021).\n3.  Disparities in visit frequency. Certain patient groups may visit healthcare providers for follow-up care less frequently, even at the same disease severity (Nouri et al., 2023).\nA core technical challenge we address is designing a model that is flexible enough to capture all three disparities but still identifiable. Identifiability is necessary for accurate estimates of disparities and disease progression. As such, our key contributions are: (1) we develop an interpretable Bayesian model of disease progression that accounts for multiple types of disparities but remains provably identifiable from the observed data; (2) we prove and show empirically that failing to account for any of these three disparities leads to biased estimates of severity; and (3) we characterize fine-grained disparities in a heart failure dataset. Our model reveals that non-white patients have more severe heart failure and face multiple types of health disparities: Black and Asian patients tend to start receiving care at more severe stages of heart failure than do White patients, and Black patients see healthcare providers for heart failure 10% less frequently than do White patients at the same disease severity level. Accounting for these disparities meaningfully shifts our estimates of disease severity, increasing the fraction of Black and Hispanic patients identified as high-risk. While we ground our work in healthcare, our method for learning progression models that account for disparities applies naturally to many other progression model settings where disparities are of interest, including infrastructure deterioration (Madanat et al., 1995) and human aging (Pierson et al., 2019)."}, {"title": "2. Related Work", "content": "Disease progression modeling. Disease progression models have been developed for many chronic diseases, including Parkinson's disease (Post et al., 2005), Alzheimer's disease (Holford and Peace, 1992), diabetes (Perveen et al., 2020), and cancer (Gupta and Bar-Joseph, 2008). A key feature of the progression models we consider, common in the machine learning literature, is that a latent severity $Z_t$ progresses over time and gives rise to the observed symptoms $X_t$. Models in this family include variants of hidden Markov models (HMMs) (Wang et al., 2014; Liu et al., 2015; Alaa and Hu, 2017; Sukkar et al., 2012; Jackson et al., 2003) and recurrent neural networks (RNNs) (Choi et al., 2016b; Lipton et al., 2017; Lim and van der Schaar, 2018; Choi et al., 2016a; Ma et al., 2017; Kwon et al., 2019; Alaa and van der Schaar, 2019). The existing literature has not focused on modeling disparities; we extend it by proposing a new approach to disease progression modeling that can interpretably characterize and account for multiple types of health disparities.\nHealth disparities. Disparities have been documented in many parts of the healthcare process. Factors such as distance from hospitals (Reilly, 2021), distrust of the healthcare system (LaVeist et al., 2009), or lack of insurance (Venkatesh et al., 2019) can result in underutilization of health services; biases in the judgements of healthcare providers can lead minority groups to receive later screening (Lee et al., 2021), fewer referrals (Landon et al., 2021), or generally worse care (Sch\u00e4fer et al., 2016); and issues such as limited health literacy or trust can create disparities in follow-through for appointments or the effectiveness of at-home care (Davis, 1968; Brandon et al., 2005).\nThe existing literature has shown that disparities emerge along the three axes that we capture in this paper: (1) how severe a patient's disease becomes before they start to receive care (Chen et al., 2021; Iqbal et al., 2015; Hu et al., 2024); (2) how quickly their latent severity progresses even while receiving care (Diamantidis et al., 2021; Suarez et al., 2018); and (3) how likely they are to visit a healthcare provider at a given severity level (Nouri et al., 2023). Our goal is to show how accounting for disparities along all three of these axes improves the severity estimates of disease progression models, while also learning more fine-grained descriptions of existing disparities.\nCapturing disparities with machine learning. We build upon a large body of past work that uses machine learning as a tool to capture and address health disparities, including models that estimate the relative prevalence of underreported medical conditions (Shanmugam et al., 2021), improve risk prediction for patients with missing outcome data (Balachandar et al., 2023), evaluate the impact of race corrections in risk prediction (Zink et al., 2023), assess disparate impacts of AI in healthcare (Chen et al., 2019), and quantify disparities in the performance of clinical prediction tasks (Zhang et al., 2020). The closest work to our own is Chen et al. (2021), which develops a clustering algorithm that accounts for the fact that some patients do not come in (and are therefore not observed) until later in their disease progression. While their work addresses one form of data bias that can arise due to health disparities, it differs from our own in two ways: it does not specifically document or study health disparities, and it focuses on clustering patients as opposed to modeling disease severity or progression. Our work proposes a model for capturing three types of health disparities in the disease progression setting in order to learn precise descriptions of multiple disparities and make severity estimates that exhibit less bias than existing disease progression models."}, {"title": "3. Model", "content": "We build on a standard setup for disease progression modeling, in which each patient has an underlying latent disease severity $Z_t$ that progresses over time and gives rise to a set of observed features $X_t$ (Klemera and Doubal, 2006; Levine, 2013).\nWe characterize each patient's severity $Z_t \\in \\mathbb{R}$ at time t by their initial severity $Z_0$ at their first observation (which we denote as t = 0) and their rate of progression R after that point:\n$Z_t = Z_0 + R \\cdot t$\nIf a patient visits a healthcare provider at time t, we observe some recorded set of features $X_t \\in \\mathbb{R}^d$ (e.g., lab results, imaging, symptoms). At any given visit, a clinician does not necessarily observe or record all features\u2014we model the features that are observed as a noisy function of the patient's latent severity $Z_t$:\n$X_t = f(Z_t) + \\epsilon_t$\n$\\epsilon_t \\sim N(0, \\Psi)$\nwhere the diagonal covariance matrix $\\Psi \\in \\mathbb{R}^{d \\times d}$ parameterizes feature-specific noise (accounting for both measurement error and variation in how the patient's physical state can fluctuate day-to-day). In our experiments, we specifically instantiate f as a linear function $f(Z_t) = F \\cdot Z_t + b$, where $F \\in \\mathbb{R}^d$ is a feature-specific scaling factor and $b \\in \\mathbb{R}^d$ is a feature-specific intercept, but our approach extends to more general parametric forms for f. We constrain the first feature $F_0 > 0$ using domain knowledge; this restriction is necessary for identifiability because it restricts the mapping between features and severity (Shapiro, 1985). We also observe a set of timesteps when a patient visits a healthcare provider; we encode this with a binary indicator $D_t \\in \\{0, 1\\}$ which is equal to 1 if a patient has a visit at time t.\nCapturing disparities. Our model captures the three types of health disparities discussed in \u00a72 by allowing model parameters to vary as a function of a patient's demographic feature vector A. For expositional clarity, we describe a setup where A encodes a single categorical label (e.g., a patient's race group), but our approach naturally extends to multiple categorical groupings or to continuous features.\n1.  Disparities in initial severity. Underserved patients may start receiving care only when their"}, {"title": "4. Theoretical analysis", "content": "In this section, we prove two main theoretical results. First, we show that our model is identifiable, a necessary condition for its parameters to be estimated from the observed data. Learning these parameter estimates is what allows us to characterize disparities. Second, we prove that failing to account for disparities produces biased estimates of severity. We summarize proof strategies in the main text and provide formal proofs in Appendices A and B."}, {"title": "4.1. Identifiability", "content": "We show that our model is identifiable, meaning different sets of parameters yield different observed data distributions (Bellman and \u00c5str\u00f6m, 1970), which is necessary to correctly estimate model parameters from the observed data. Learning a model of progression that is flexible enough to characterize multiple disparities but still identifiable is a core challenge our work addresses. In fact, if we added one more dependence on A in particular, adding an arrow from A to X in Figure 1 \u2014 the model would no longer be identifiable; without a shared interpretation across groups of how features map to severity, it would be impossible to identify disparities in disease progression.\nTheorem 1 All model parameters are identified by the observed data distribution $P(X_t, D_t | A)$.\nAs mentioned in \u00a73, the distribution of initial severity $Z_0$ is pinned to a unit normal for one demographic group $a_0$. This pinned distribution reduces the number of unknown latent parameters for group $a_0$, allowing us to show that $\\{F, b, \\Psi\\}$ are identified by $P(X_t | A = a_0)$. Having identified these, we show that the parameters $\\{\\mu_{Z_0}^{(a)}, \\sigma_{Z_0}^{(a)}, \\mu_R^{(a)}, \\sigma_R^{(a)}\\}$ are identified by $P(X_t | A = a)$ for all groups a. Finally, we show that given the previously identified parameters, $\\{\\beta_0, \\beta_z\\}$ are identified by $P(D_t | A = a_0)$ and $\\{\\beta_A^{(a)}\\}$ is identified by $P(D_t | A = a)$ for all other groups a. We provide a full proof in \u00a7A.1."}, {"title": "4.2. Bias in models that do not account for disparities", "content": "Next we show that, when any of the health disparities we discuss are present, a model that does not account for group-specific disparities will produce biased estimates of severity\u2014i.e., $E[Z_t | X_t, D_t] \\neq E[Z_t | X_t, D_t, A = a]$. These theoretical results hold whenever the model dependencies are encoded by the graph in Figure 1, a more general assumption than our full parametric model. For each proof, we analyze the effect of one disparity\u2014e.g., for disparities in initial severity, we assume that $P(Z_0 | A = a)$ differs across groups while keeping other distributions constant across groups. These results hold in the presence of multiple disparities as long as existing disparities disfavor or favor the same group, so as to not cancel each other out in their effects.\nTo quantify disparities, we use the strict Monotone Likelihood Ratio Property (MLRP) to reason aboutthe probability density functions of initial severity and progression rate for certain groups, relative to the overall population (Karlin and Rubin, 1956):\nDefinition 2 Two distributions characterized by probability density functions f(x) and g(x) have the strict monotone likelihood ratio property in x if $\\frac{f(x)}{g(x)}$ is a strictly increasing function of x.\nIntuitively, this means that as some variable x ($Z_0$ or R, in our case) gets larger, it is more likely to be drawn from f than g. The MLRP is a widely-used assumption across many settings (Gaebler and Goel, 2024; Anwar and Fang, 2006; Chemla and Hennessy, 2019); the normal, exponential, binomial, and Poisson families all have this property. For brevity, we say \"f(x) strictly MLRPs g(x)\" to mean that f(x) and g(x) satisfy the strict MLRP in x. We now prove for each disparity that any model that fails to account for the disparity will produce biased estimates of severity.\nTheorem 3 A model that does not take into account disparities in initial disease severity $Z_0$ will underestimate the disease severity of groups with higher initial severity and overestimate that of groups with lower initial severity. Specifically, if $P(Z_0 | A = a)$ strictly MLRPs $P(Z_0)$ for some group a, then $E[Z_t | X_t] < E[Z_t | X_t, A = a]$. Similarly, if $P(Z_0)$ strictly MLRPs $P(Z_0 | A = a)$ for some group a, then $E[Z_t | X_t] > E[Z_t | X_t, A = a]$.\nWe prove this by showing that $P(Z_0 | X_t, A = a)$ strictly MLRPs $P(Z_0 | X_t)$, which implies that $E[Z_t | X_t, A = a] > E[Z_t | X_t]; \u00a7B.1 provides a full proof.\nTheorem 4 Suppose disease severity increases linearly with some progression rate R. A model that does not take into account disparities in R will underestimate the disease severity of groups with higher progression rates and overestimate that of groups with lower progression rates. Specifically, if $P(R | A = a)$ strictly MLRPs $P(R)$ for some group a, then $E[Z_t | X_t] < E[Z_t | X_t, A = a]$. Similarly, if $P(R)$ strictly MLRPs $P(R | A = a)$ for some group a, then $E[Z_t | X_t] > E[Z_t | X_t, A = a]$.\nWe use a similar proof technique as for Theorem 3 and provide a full proof in \u00a7B.2.\nFinally, we analyze disparities in visit frequency. For this portion of the theoretical analysis, we consider discrete, non-infinitesimal time intervals (each starting at time $t_0$ = 0, $t_1$, $t_2$, ...) and whether or not"}, {"title": "5. Synthetic experiments", "content": "In this section, we validate our model and theoretical results in synthetic data simulations. We generate synthetic datasets according to the modeling assumptions in \u00a73 (with parameter values for each dataset drawn randomly from each parameter's prior distribution). For each dataset, we generate simulated data for two separate groups, differing in initial severity, progression rate, and visit frequency (characterized by different $\\mu_{Z_0}$, $\\mu_R$, and $\\beta_A$, respectively).\n5.1. Identifiability and Severity Estimation\nWe first verify Theorem 1 in simulations, showing that when we fit our model on synthetic data, it can accurately recover the true data-generating parameters. We do this by examining the concordance between the model's estimated parameters and the true, latent parameter values, a common approach in past work (Chang et al., 2021; Pierson et al., 2019). We find high correlation between the true parameters and our model's posterior mean estimates (mean Pearson's r 0.96 across all parameters; median 0.99), and good calibration (mean linear regression slope 1.0; median 1.0 when fit without an intercept term). We provide scatterplots of the true and estimated parameters in Appendix C. We also see that our model's mean severity estimates for each group are highly correlated and well-calibrated with ground truth, despite underlying differences in group severity distributions and visit rates (Figure 2).\n5.2. Bias in models that do not account for disparities\nWe now demonstrate in simulation that failing to account for disparities can lead to biased severity estimates, consistent with Theorems 3, 4, and 5. In each trial, we use the same data to fit four models: our full model, which accounts for all disparities, plus three ablated models that each fail to account for one of the"}, {"title": "6. Modeling health disparities in heart failure progression", "content": "We fit our model on a real-world dataset of heart failure patients in the New York-Presbyterian hospital system. Heart failure is a progressive disease that affects many people, requires both specialty and preventive care (Colucci et al., 2020), and has known health disparities (Lewsey and Breathett, 2021), making it a natural application setting for our model. In \u00a76.1 we summarize the dataset, and in \u00a76.2 we confirm that our model can learn meaningful low-dimensional representations of disease severity by evaluating its reconstruction and predictive performance compared to standard baselines. In \u00a76.3 we present our main results: we interpret our model's learned parameters to provide precise descriptions of health disparities in our setting, and we show that (as our theory predicts) failing to account for these disparities meaningfully shifts severity estimates.\n6.1. Data\nOur data comes from the New York-Presbyterian (NYP)/Weill Cornell Medical Center's electronic health record (EHR) system from 2012-2020. We analyze a cohort of N = 2,942 patients who (1) have a specific subtype of heart failure (heart failure with reduced ejection fraction), to ensure our cohort can be described by a single progression model, and (2) are likely to receive most of their cardiology care in the NYP system, to ensure we can reasonably estimate when they receive care.\nObserved feature data $X_t$ for each patient includes four types of measurements: left ventricle ejection fraction (LVEF), brain natriuretic peptide (BNP), systolic blood pressure (SBP), and heart rate (HR). LVEF and BNP have strong clinical associations with heart failure severity (in terms of both underlying physiological health and observed symptoms) (Murphy et al., 2020). SBP and HR are less informative (more prone to fluctuation and changes not related to heart failure), but they are still expected to show general trends over time as a patient's heart failure progresses. Since we must pin the sign of at least one scaling factor F for identifiability, and decreasing LVEF is strongly associated with increasing severity in the heart failure subtype we study, we pin the sign of the scaling factor between severity and LVEF values ($F_{LVEF} < 0$).\nMeasurements close in time are often from the same hospital visit, so we combine measurements within the same week (which has the additional benefits of increasing the speed of model fitting and allowing us to focus on capturing longer-term changes in severity). Specifically, for each week, we average together measurements of the same type and treat any measurements as if they occurred at the beginning of the week. We then analyze disparities across four self-reported race/ethnicity groups: White non-Hispanic patients, Black non-Hispanic patients, Hispanic patients, and Asian non-Hispanic patients (which we will hereby describe as White, Black, Hispanic, and Asian subgroups). A full description of our data processing can be found in Appendix D.\n6.2. Model validation\nWe first confirm that our model accurately fits the data: we verify that the model's inferred parameters are consistent with medical knowledge (\u00a76.2.1) and compare the model's reconstruction and predictive performance to standard baselines (\u00a76.2.2). Having done this, we then show in \u00a76.3, as our primary result, that our model provides insight into disparities in disease progression.\n6.2.1. CONSISTENCY WITH MEDICAL KNOWLEDGE\nFigure 3 plots our model's inferred parameters, all of which are consistent with existing medical knowledge. Specifically, (1) the model correctly learns that BNP and HR tend to increase with heart failure severity ($F_{BNP}, F_{HR} > 0$), while SBP tends to decrease ($F_{SBP} < 0$) (Murphy et al., 2020); (2) the model learns larger variance parameters for SBP and HR values ($\\Psi$), correctly inferring that these features are less informative about heart failure progression than BNP and LVEF (Murphy et al., 2020); and (3) the model estimates that $\\beta_z > 0$, meaning it learns that patients with higher disease severity tend to see healthcare providers more frequently, as expected.\n6.2.2. RECONSTRUCTION AND PREDICTIVE PERFORMANCE\nWe next evaluate the model's ability to reconstruct and predict patient features $X_t$. While predicting and reconstructing $X_t$ is not the primary goal of our model, the model performs generally well relative to standard baselines, validating its ability to meaningfully represent the data.\nBecause the model represents each patient visit in terms of a scalar severity $Z_t$, we do not expect the model to perfectly reconstruct the multi-dimensional $X_t$; rather, we hope for predictions that correlate significantly with $X_t$. Consistent with this, when fit on 3 years of data per patient, our model's predicted feature values correlate with true values both in- and out-of-sample. As we would hope, the model best represents the features that are most informative for heart failure progression\u2014LVEF (r = 0.81 in-sample,\nr = 0.51 out-of-sample) and BNP (r = 0.62 in-sample, r = 0.31 out-of-sample)\u2014as opposed to the less-informative features SBP (r = 0.42 in-sample, r = 0.24 out-of-sample) and HR (r = 0.17 in-sample, r = 0.03 out-of-sample; all p-values besides HR out-of-sample < 0.001).\nTo provide a more detailed assessment of performance, we evaluate our model's ability to reconstruct patient features $X_t$ in-sample and predict $X_t$ out-of-sample, in comparison to seven standard baselines. All of the baselines are designed to reconstruct or predict observed feature values ($X_t$), as opposed to additionally predicting whether patient visits will occur ($D_t$). Our model can predict the latter as well, but in order to provide a direct comparison of reconstruction and predictive performance, we compare only the feature prediction aspect of our model (so we do not fit any models using $D_t$ data) in this subsection. We use mean absolute percentage error (MAPE) to report a normalized measure of error across features.\nReconstruction performance. We compare our model's reconstruction performance to that of two standard dimensionality reduction baselines: principal component analysis (PCA) and factor analysis (FA). We compare our model to two variants of each. First, we compare our model to PCA and FA fit at the visit level: one component per patient visit, analogous to our model's $Z_t$. Second, we compare our model to PCA and FA fit at the patient level: two components for each patient, to capture the trajectory of feature values as we do with $Z_0$ and R (Table S1). We describe the implementation of these baselines with more detail in Appendix E.\nBecause both PCA and FA require input vectors of consistent size, all models are fit on feature values from the first three visits per patient. Compared to all baselines, we achieve equivalent or better reconstruction performance across all features, and better performance on the more informative features.\nPredictive Performance. While prediction is not the primary goal of our model (and models with relatively low predictive performance can still provide useful insights on disparities (Pierson et al., 2021)), we compare our model's predictive performance to three standard timeseries forecasting baselines as an additional validation of our model's ability to meaningfully represent the data. We compare our model to (1) a linear regression for each patient and feature; (2) a quadratic regression for each patient and feature; and (3) predicting values equal to those at"}, {"title": "7. Discussion", "content": "In this paper, we formalize three specific axes along which healthcare disparities emerge as biases in observed health data: underserved patients may (1) first receive care only when their disease is more severe, (2) progress faster even while receiving care, or (3) receive care less frequently even at the same disease severity. We develop a disease progression modeling approach to interpretably capture all three types of disparities while provably retaining identifiability. We prove that failing to account for any of these disparities leads to biased estimates of severity and show in a real-world heart failure dataset that accounting for health disparities does indeed meaningfully shift severity estimates by increasing the proportion of non-white patients identified as high-risk. By evaluating our model in a real healthcare setting, we validate its ability to learn fine-grained descriptions of health disparities and to make disease severity estimates that are accurate across diverse populations of patients. We thus urge future work in disease progression modeling to account for disparities in healthcare, and we lay a foundation for doing so.\nThere are several natural directions for future work. First, beyond heart failure, our approach could be applied to the many other progressive diseases, including Parkinson's (Post et al., 2005), Alzheimer's (Holford and Peace, 1992), diabetes (Perveen et al., 2020), and cancer (Gupta and Bar-Joseph, 2008). Second, an interesting technical direction is to extend our model to capture additional data modalities (e.g., medical images) or more flexible progression models (e.g., non-linear trajectories) while retaining its provable identifiability. Finally, our approach generalizes naturally to progression model settings beyond healthcare where disparities are of interest, including infrastructure deterioration (Madanat et al., 1995) and human aging (Pierson et al., 2019); these would be interesting domains for future work."}, {"title": "Appendix A. Proof of Identifiability", "content": "A.1. Proof of Theorem 1\nTheorem 1 All model parameters are identified by the observed data distribution $P(X_t", "lemmas": "nLemma 6 Parameters F", "1985)": "n$X_0 \\sim N(b", "same": "F(F)^T + \\Psi = \\tilde{F"}, "tilde{F})^T + \\tilde{\\Psi}$. Element-wise equality of the covariance matrix gives us the following, where subscripts i refer to the i-th element of each parameter vector:\n$F_iF_j = \\tilde{F_i}\\tilde{F_j} \\forall i, j, i \\neq j$\n$(F_i)^2 + \\Psi_i = (\\tilde{F_i})^2 + \\tilde{\\Psi_i}$\nUsing the equality constraint (1) for multiple pairs of indices, we have that for all assignments of distinct indices i, j, k:\n$(F_iF_j = \\tilde{F_i}\\tilde{F_j}) \\land (F_jF_k = \\tilde{F_j}\\tilde{F_k}) \\Rightarrow \\frac{F_i}{F_i} = \\frac{\\tilde{F_k}}{F_k}$\n$F_iF_k = \\tilde{F_i}\\tilde{F_k} \\Rightarrow \\frac{F_i}{\\tilde{F_i}} = \\frac{F_k}{F_k}$\nTogether, equations 3 and 4 give us:\n$\\frac{F_i}{\\tilde{F_i}} = \\frac{F_j}{F_j} \\Rightarrow (F_i)^2 = (\\tilde{F_i})^2 \\rightarrow F_i = a\\tilde{F_i}$\nwhere a \u2208 {-1, +1}. Since we have fixed $F_0 > 0$ for all factor loading matrices F, the sign of a is fixed:\n$F_0 = a\\tilde{F_0} \\Rightarrow a = 1 \\rightarrow F_i = \\tilde{F_i} \\forall i \\in [0, d)$,\nmeaning we have identified F.\nLastly, using equations (2) and (5) we get $F_i = \\tilde{F_i} \\rightarrow \\Psi_i = \\tilde{\\Psi_i}$. We have now shown that if two parameter sets induce the same distribution of X at time t = 0, they must have the same exact value assignments. Therefore F, b, \u03a8 are identified by $P(X_t | A = a_0)$.\nLemma 7 Global parameters F,b, \u03a8 and parameters $\\{\\mu_{Z_0}^{(a)}, \\sigma_{Z_0}^{(a)}, \\mu_R^{(a)}, \\sigma_R^{(a)}\\}$ for each group a are identified by $P(X_t | A)$.\nProof By Lemma 6, we know that F, b, \u03a8 are identified by $P(X_0 | A = a_0)$. We want to show that for any group a, if two parameter sets $\\{\\mu_{Z_0}^{(a)}, \\sigma_{Z_0}^{(a)}, \\mu_R^{(a)}, \\sigma_R^{(a)}\\}$ and $\\{\\tilde{\\mu}_{Z_0}^{(a)}, \\tilde{\\sigma}_{Z_0}^{(a)}, \\tilde{\\mu}_R^{(a)}, \\tilde{\\sigma}_R^{(a)}\\}$ yield the same observed data distribution $P(X_t | A = a)$, the parameter sets must be identical. In this proof we consider an arbitrary group a and omit the (a) superscript for brevity.\nWe model the following:\n$Z_0 \\sim N(\\mu_{Z_0}, \\sigma_{Z_0}^2)$\n$R \\sim N(\\mu_R, \\sigma_R^2)$\n$Z_t = Z_0 + R \\cdot t \\rightarrow Z_t \\sim N(\\mu_R \\cdot t + \\mu_{Z_0}, \\sigma_R^2 \\cdot t^2 + \\sigma_{Z_0}^2)$\n$X_t = F \\cdot Z_t + b + \\epsilon_t$, where $\\epsilon_t \\sim N(0, \\Psi)$\nWe see that equation (6) captures a factor analysis model with factor loading matrix F and diagonal covariance matrix \u03a8, meaning\n$X_t \\sim N(b + F(\\mu_R \\cdot t + \\mu_{Z_0}), F(\\sigma_R^2 \\cdot t^2 + \\sigma_{Z_0}^2)FT + \\Psi)$.\nRecalling that $F_0 > 0$, we first consider t = 0, where $X_0 \\sim N(b + F\\mu_{Z_0}, F(\\sigma_{Z_0}^2)FT + \\Psi)$. In order for the two"]}