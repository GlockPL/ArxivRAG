{"title": "Continuous Video Process: Modeling Videos as Continuous Multi-Dimensional Processes for Video Prediction", "authors": ["Gaurav Shrivastava", "Abhinav Shrivastava"], "abstract": "Diffusion models have made significant strides in image generation, mastering tasks such as unconditional image synthesis, text-image translation, and image-to-image conversions. However, their capability falls short in the realm of video prediction, mainly because they treat videos as a collection of independent images, relying on external constraints such as temporal attention mechanisms to enforce temporal coherence. In our paper, we introduce a novel model class, that treats video as a continuous multi-dimensional process rather than a series of discrete frames. We also report a reduction of 75% sampling steps required to sample a new frame thus making our framework more efficient during the inference time. Through extensive experimentation, we establish state-of-the-art performance in video prediction, validated on benchmark datasets including KTH, BAIR, Human3.6M, and UCF101.", "sections": [{"title": "1. Introduction", "content": "In the evolving landscape of machine learning and generative models, particularly in the domain of video representation  there exists a pivotal challenge in adequately capturing the dynamic transitions between consecutive frames. In this paper, we introduce a novel approach to video representation that treats the video as a continuous process in multi-dimensions. This methodology is anchored in the observation that transitions between consecutive frames in a video do not uniformly contain the same amount of motion. Modeling these transitions with a single-step process often leads to suboptimal quality in sampling. Our method, therefore, involves multiple predefined steps between two consecutive frames, drawing inspiration from recent advancements in diffusion models for image data. This multi-step diffusion process has been instrumental in better modeling image data, and we aim to extend this success to video data."}, {"title": "2. Related Works", "content": "Understanding and predicting future states based on observed past data  is a cornerstone challenge in the domain of machine learning. It is crucial for video-based applications where capturing the inherent multi-modality of future states is vital, such as in autonomous vehicles. Early methods in this field, as noted by Yuen et al. and Walker et al. primarily focused on matching past frames within datasets to extrapolate future states, although these predictions were constrained to either symbolic trajectories or directly retrieved future frames. The advent of deep learning has significantly propelled advancements in this area. One of the seminal works by Srivastava et al. leveraged a multi-layer LSTM network for deterministic representation learning of video sequences. Subsequent studies  have expanded the scope of this research by constructing models that account for the stochastic nature of future states, marking a notable shift from earlier deterministic approaches.\nRecent research in this domain has explored both implicit and explicit probabilistic modeling approaches. Implicit probabilistic modeling, typified by GAN  -based models, has a substantial history. Nonetheless, these models often grapple with training stability issues and mode collapse(where model only focuses on a few modes in the dataset) issues. On the other hand, explicit probabilistic modeling for video prediction encompasses a range of methodologies, including Variational Autoencoders (VAEs) , Gaussian processes, and Diffusion models. VAE-based video prediction methods  tend to average results to align with all potential future scenarios, which undermines the fidelity of predictions. Gaussian process-based models  exhibit proficiency with smaller datasets but encounter scalability issues owing to matrix inversion limitations when calculating training likelihood. While workarounds exist, they tend to compromise result fidelity.\nRecent advancements in diffusion models  have positioned them as the preferred choice for video prediction tasks. These multi-step models offer superior sample quality and are resilient to mode collapse. However, even with such lucrative advantages, modeling videos with these models tends to have downsides. Majorly methods falling under this category enforce temporal consistency using artificial external constraints such as the introduction of temporal attention blocks. This might be effective but comes at a cost of significant computing power.\nAnother class of popular video prediction models is hierarchical prediction models. These models are multistage models that decompose the problems into two stages. They first predict a high-level structure of a video, like a human pose, and then leverage that structure to make predictions at the pixel level. These models generally require additional annotation for the high-level structure for training, unlike ours that predicts future frames utilizing only the pixel-level information of context frames.\nWe also want to highlight some very recent works like InDI and Cold diffusion that provide an alternate approach to denoising diffusion models that is similar to our approach. However, their works only explored such formulation for image-based computational photography and image generation tasks."}, {"title": "3. Method", "content": "Instead of introducing noise iteratively to the frames until they conform to a Gaussian distribution, and adopting a reverse process such as denoising diffusion, a commonly employed technique for video prediction, we introduce a novel model category designed to depict videos as continuous processes. This section delves into the modeling of this continuous video process.\nSuppose we have a video sequence denoted by $V = {x_j}_{j=1}^N$ where $x_j \\in \\mathbb{R}^{c \\times h \\times w}$ is the frame at the timestep j. We represent this video sequence as a continuous process. The intermediate frames between $x = x^i$ and $y = x^{i+1}$ are given by the following equation.\n$x_t = (1 - t)x + ty - \\frac{t \\log(t)}{\\sqrt{2}} z$   (1)\nHere, $z \\sim \\mathcal{N}(0, I)$ denotes the white noise. From the above Eqn, it can be seen that at $t = 0$, we get the frame $x$ and at $t = 1$, we get the frame $x^{i+1}$. We utilize this continuous process of evolving $x \\rightarrow x^{i+1}$ given by Eqn. 1 and derive both the forward and reverse processes. For defining the forward process, we take steps in the direction $t : T\\rightarrow 0$ instead of the other way, which happens in"}, {"title": "4. Experiments", "content": "Video prediction task can be defined as given a few context frames, the model has to predict the subsequent future frames. In this section, we empirically demonstrate that our approach yields superior results in modeling the video prediction task."}, {"title": "4.1. Datasets", "content": "We chose 4 different types of datasets to demonstrate the efficacy of our approach. These are standard benchmarks for video prediction tasks. Dataset lists include KTH action recognition dataset , BAIR robot pushing dataset , Human3.6M  and UCF101 datasets. Training and architecture-specific details about the approach are included in the appendix.\nKTH Action Recognition Dataset. The KTH action dataset  consists of video sequences of 25 people performing six different actions: walking, jogging, running, boxing, hand-waving, and hand-clapping. The background is uniform, and a single person is performing actions in the foreground. The foreground motion of the person in the frame is fairly regular. The frames in the video for this dataset consist of a single channel. The spatial resolution of the frames in the video is downsampled to the size of 64 x 64.\nBAIR pushing Dataset. The BAIR robot pushing dataset contains the videos of table mounted sawyer robotic arm pushing various objects around. The BAIR dataset consists of different actions given to the robotic arm to perform. The spatial resolution of the frames in the video is kept to be 64 x 64.\nHuman3.6M Dataset. Human3.6M  dataset consists of 10 subjects performing 15 different actions. The pose information from the dataset was not used in predicting next frame. The background is uniform, and a single person is performing actions in the foreground. The foreground motion of the person in the frame is fairly regular. The frames in the video for this dataset consist of 'RGB' channels. The spatial resolution of the frames in the video is downsampled to the size of 64 x 64.\nUCF101 Dataset. This dataset  consists of 13,320 videos belonging to 101 different action classes. The video seems to have a variety of backgrounds and the frames of the video have three channels, namely 'RGB'. We reshape the resolution of frames from the original size of 320 \u00d7 240 down to 128 \u00d7 128 for our video prediction tasks. The downsampling is done utilizing the bicubic downsampling."}, {"title": "4.2. Metrics", "content": "We primarily use the FVD  metric to determine the best-performing baseline when evaluating a video prediction task. FVD metric evaluates a baseline on both terms, the reconstruction quality and diversity of the generated samples. FVD is calculated as the frechet distance between the I3D embeddings of generated video samples and real samples. The I3D network used for obtaining the embeddings for real and generated video is trained on the Kinetics-400 dataset."}, {"title": "5. Setup and Results", "content": "Below, we describe in detail how the setup for our experiment looks compared to baselines. We also showcase our findings about the performance of our method and comparison to baselines in this section."}, {"title": "KTH action recognition dataset:", "content": "For this dataset, we adhered to the baseline setup , which utilizes the first 10 frames as context frames. In baseline setup, these 10 frames are utilized to predict the subsequent 30 and 40 frames. A notable aspect of our experiment is we only used the last 4 frames from this sequence of 10 frames as context frames in our CVP model, while disregarding the information in the remaining 6 frames. This decision was taken to maintain consistency with the experimental setups used in prior baseline methodologies. The outcomes of this evaluation are summarized in Table. 1.\nIt can be observed from the Table. 1, our model's unique approach requires a significantly reduced number of frames for training. Contrary to other methods that train on an additional set of k frames (10[context frames]+k[future frames]), our model uses just one frame (effectively 4[context frames]+1[future frames]). We employ the 4 context frames to predict the immediate next frame and then autoregressively generate either 30 or 40 frames, depending on the evaluation requirement. This methodology is supported by our model's efficient handling of video sequences as continuous processes, which eliminates the need for external artificial constraints, such as temporal attention mechanisms.\nThe results, as shown in Table 1, clearly indicate that our method delivers state-of-the-art performance when compared to other baseline models. Additionally, the qualitative results for our CVP model on the KTH dataset can be observed in Fig. 3."}, {"title": "BAIR Robot Push dataset:", "content": "The BAIR Robot Push dataset is characterized by highly stochastic video sequences. In our study, we adhered to a baseline setup  with three main experimental settings: 1) using only one context frame to predict the next 15 frames, 2) employing two context frames to predict 14 future frames, and 3) utilizing two context frames to forecast the next 28 frames. The outcomes of these approaches are summarized in Table 2.\nAs observed in Table 2, a trend emerges where increasing the number of frames predicted at a time concurrently results in a degradation of prediction quality. This phenomenon is hypothesized to stem from an augmented disparity between the blocks of context frames and predicted future frames. Specifically, consider the scenario where two context frames are designated as x0:2, corresponding to x in the context of Eqn.1. Under the first experimental condition, where the model predicts a single frame at a time, the future frame prediction block is represented as x1:3, analogous to y in Eqn.1. Conversely, in the second condition, where two frames are predicted simultaneously, the future frame block extends to x2:4, again paralleling y in the equation. This setup implies that in the former setting, interpolation occurs between adjacent frames (i.e., the transition from x\u00ba \u2192 x\u00b9 and x\u00b9 \u2192 x\u00b2), while in the latter, interpolation spans a two-frame interval (i.e., the transition from x\u00ba \u2192 x\u00b2 and from x\u00b9 \u2192 x\u00b3). The expanded interval in the second scenario is posited as the causative factor for the observed reduction in predictive performance, particularly in configurations where k = 2 and P \\geq 2."}, {"title": "Human3.6M dataset:", "content": "Similar to the KTH dataset, the Human3.6M dataset features actors performing distinct actions against a static background. However, the Human3.6M dataset distinguishes itself by offering a greater variety of distinct actions within its videos and providing three-channel video frames, in contrast to the single-channel frames of the KTH dataset. For evaluating the Human3.6M dataset, we employed a similar setup to that used for the KTH dataset, where 5 frames are provided as context, and the model predicts the subsequent 30 frames based on these context frames. The results of this evaluation are summarized in Table 3.\nAn analysis of Table 3 reveals that our model, with its unique approach, requires a significantly lower number of frames for training, needing only a total of 6 frames per block to yield results that are considerably better than those of the baselines.\nThe results, as presented in Table 3, unequivocally demonstrate that our method outperforms other baseline models, establishing a new state-of-the-art on the Human3.6M dataset. Furthermore, the qualitative efficacy of our CVP model on the Human3.6M dataset is illustrated in Fig. 5, showcasing the model's ability to effectively capture and predict the dataset's varied actions."}, {"title": "UCF101 dataset:", "content": "The UCF101 dataset presents a greater level of complexity compared to the KTH or Human3.6M datasets, owing to its substantially higher number of action categories, diverse backgrounds, and significant camera movements. Notably, we only use information from the context frames for our frame-conditional generation task. No extra information, like class labels, was used for the prediction task. In evaluating the UCF101 dataset, we adopted an approach similar to that used for the Human3.6M dataset, where 5 context frames are provided, and the model is tasked with predicting the next 16 frames based on these. The outcomes of this evaluation are detailed in Table. 4.\nAn examination of Table. 4 reveals that our CVP model surpasses the performance of other baseline models, thereby setting a new benchmark for the UCF101 dataset. Additionally, the qualitative performance of our CVP model on the UCF101 dataset is depicted in Fig. 6. This illustration showcases the model's proficiency in accurately capturing and predicting the diverse range of actions featured in the dataset."}, {"title": "6. Ablation Studies", "content": "In this section, we present a series of ablation studies conducted to ascertain the impact of various components in our proposed methodology. These studies focus on three primary aspects: the modification of the noise schedule denoted as $g(t)$, the variation in the number of sampling steps, and the exploration of different strategies for sampling the timestep t. Our experimental framework utilizes the KTH dataset for these evaluations.\nThe outcomes of these experiments are systematically tabulated in Table. 6, offering a comprehensive view of the results. The key insights derived from these ablation studies are threefold. Firstly, our analysis underscores the criticality of sampling the timestep t from a uniform square root distribution, specifically $t \\sim \\sqrt{U[0, 1]}$. This approach appears to significantly influence the model's performance.\nSecondly, regarding the noise schedule $g(t)$, we find that the optimal formulation for the task of video prediction is given by $g(t) = \\frac{-t \\log(t)}{\\sqrt{2}}$. This particular noise schedule is characterized by a zero initial and final noise level, with a peak near t = 0. Such a configuration is advantageous for our application.\nThirdly, our results, as detailed in Table 6, indicate that an increase in the number of sampling steps beyond 25 does not substantially improve the outcome. Our method outperforms MCVD by producing higher-quality frames in just 25 sampling steps, a 75% reduction compared to its 100 steps. This efficiency is attributed to our CVP method, which retains information from preceding frames, eliminating the need"}, {"title": "7. Limitation", "content": "A primary limitation of our approach is its reliance on a limited context frame window for predicting the next frame. Specifically, when a context vector, denoted as x0:4, comprising 4 video frames is used, the prediction of the subsequent frame is entirely dependent on this four-frame window. This model architecture performs adequately in scenarios involving uniform video sequences. However, its efficacy diminishes in a setting that requires more context to predict the future frame. Addressing this limitation requires a more adaptive approach that can handle varying contextual information, a challenge we have earmarked for future research.\nAnother constraint lies in the computational efficiency of our model. Currently, it necessitates multiple steps to sample a single frame, which could become a significant bottleneck, especially when a larger number of frame predictions are required. Although our method is more efficient in terms of the number of steps needed for frame sampling compared to diffusion-based counterparts, further optimization is necessary to reduce the computational overhead associated with this process.\nAdditionally, our experimental setup was constrained by the computational resources available to us. The model was developed and tested using just two A6000 GPUs. This limitation raises questions about the potential improvements that could be achieved with a more powerful computational setup. A larger model with an increased number of parameters, trained on more advanced hardware, could potentially unveil further advancements in video prediction capabilities. We recognize this as an important area for investigation and encourage labs with more substantial resources to explore this avenue.\nIn summary, while our model represents a significant step forward in video prediction, these limitations highlight crucial areas for future research and development, paving the way for more robust and versatile video prediction models."}, {"title": "8. Broader Impact", "content": "We used this method for video prediction; however, such modeling can make a major impact on many computational photography tasks. Here, one end of the CVP can be a corrupted image and the other end be a clean ground truth image. Additionally, a larger model with an increased number of parameters, trained on more advanced hardware, could potentially have advanced video prediction capabilities. This can lead to a significant increase in the creation of high-quality artificially generated content, further compounding the problems of fake content. However, a positive contribution of this approach can help with its application in autonomous driving."}, {"title": "9. Conclusion", "content": "In this work, we have presented a novel model class designed specifically for video representation, marking a significant advancement in the field of video prediction tasks. Our comprehensive experimental evaluations across various datasets, including KTH, BAIR, Human3.6M, and UCF101, have not only validated the effectiveness of our model but also established new benchmarks in state-of-the-art performance for video prediction tasks.\nA notable aspect of our approach is its efficiency in terms of the required number of context and future frames for training. Moreover, our model's continuous video process capability uniquely operates without the need for additional constraints such as temporal attention, which are typically employed to ensure temporal consistency. This aspect of our model underscores its inherent ability to maintain temporal coherence, further simplifying the video prediction process while enhancing its effectiveness.\nIn conclusion, the innovations introduced in our model offer promising directions for future research in video representation and prediction. The achievements demonstrated in this paper not only contribute to the advancement of video prediction methodologies but also open avenues for exploring more efficient and effective ways of video representation in various real-world applications."}, {"title": "Supplementary Material", "content": null}, {"title": "A. Extended derivations of Eq. (8)", "content": "Below is a derivation of Eq. (8), the reduced variance variational bound for our CVP models.\n$L = E_q[-log p(x_0) - \\sum_{t \\geq 1} log \\frac{p_\\theta(x_t|x_{t-1})}{q(x_{t-1}|x_t)}]$\n$= E_q[-log p(x_0) - \\sum_{t \\geq 1} log \\frac{p_\\theta(x_t|x_{t-1})q(x_t|x_0, x_T)}{q(x_{t-1}|x_t)q(x_t|x_0, x_T)}]$\n$= E_q[-log p(x_0) - log \\frac{q(x_T|x_0, x_T)}{q(x_0|x_0, x_T)}]$\n$= E_q[-log p(x_0) - \\sum_{t \\geq 1} log \\frac{p_\\theta(x_t|x_{t-1})}{q(x_t|x_{t-1}, x_0, x_T)}]$\n$= E_q[log p(x_0) - \\sum_{t \\geq 1} log \\frac{p_\\theta(x_t|x_{t-1})}{q(x_t|x_{t-1}, x_0, x_T)}]$\n$= E_q[-log p(x_0) - \\sum_{t \\geq 1} log \\frac{p_\\theta(x_t|x_{t-1}) p(x_0|x_{t-1})}{q(x_t|x_{t-1}) q(x_t|x_0, x_T)}]$"}, {"title": "B. Extended derivation for Eq. (2)", "content": "Using Eq. (2) we can write the term $x_{t+\\triangle t}$ as follows,\n$x_{t+\\triangle t} = (1 - (t + \\triangle t))x + (t + \\triangle t)y - \\frac{(t + \\triangle t) log(t + \\triangle t)}{\\sqrt{2}} Z_{t+\\triangle t}$\nConsidering the term $(t + \\triangle t) log(t + \\triangle t)$ we simplify further,\n$(t+\\triangle t) log(t + \\triangle t) = t(1 + \\frac{\\triangle t}{t}) log t(1 + \\frac{\\triangle t}{t}) \\approx t \\log t (1 + \\frac{\\triangle t}{t}) \\quad (14)$\nif $\\triangle t$ is infinitesimally small we can write $(1 + \\frac{\\triangle t}{t}) \\approx 1$. Using this property we can rewrite $x_{t+\\triangle t}$ as,\n$x_{t+\\triangle t} = (1 - (t + \\triangle t))x + (t + \\triangle t)y - \\frac{t \\log(t)}{\\sqrt{2}} Z_{t+\\triangle t} \\quad (15)$\nNow, Subtracting $x_{t+\\triangle t}$(Eq. (16)) and $x_t$(Eq. (1)) we get,\n$x_{t+\\triangle t} - x_t \\approx (y - x)\\triangle t - \\frac{t \\log(t)}{\\sqrt{2}} (Z_{t+\\triangle t} - Z_t) \\quad (17)$\nFocusing on the term $(Z_{t+\\triangle t} - z_t)$. Here, $Z_t, Z_{t+\\triangle t} \\sim \\mathcal{N}(0, I)$. Hence, we can write,\n$(Z_{t+\\triangle t} - z_t) = \\sqrt{2} z \\quad where, z \\sim \\mathcal{N}(0, I). \\quad (18)$\nSubstituting this result back to Eq. (17) we get the following,\n$x_{t+\\triangle t} - x_t \\approx (y - x)\\triangle t - t \\log(t) z. \\quad (19)$\nRearranging the terms we get the Eq. (2)."}, {"title": "C. Training Details", "content": "For the optimization of our model, we harnessed the compute of two Nvidia A6000 GPUs, each equipped with 48GB of memory, to train our CVP model effectively. We adopted a batch size of 64 and conducted training for a total of 500,000 iterations. To optimize the model parameters, we employed the AdamW optimizer. Additionally, we incorporated a cosine decay schedule for learning rate adjustment, with warm-up steps set at 10,000 iterations. The maximum learning rate (Max LR) utilized during training was 5e-5."}]}