{"title": "Uncertainty Estimation of Large Language Models in Medical Question Answering", "authors": ["Jiaxin Wu", "Yizhou Yu", "Hong-Yu Zhou"], "abstract": "Large Language Models (LLMs) show promise for natural language generation in healthcare, but risk hallucinating factually incorrect information. Deploying LLMs for medical question answering necessitates reliable uncertainty estimation (UE) methods to detect hallucinations. In this work, we benchmark popular UE methods with different model sizes on medical question-answering datasets. Our results show that current approaches generally perform poorly in this domain, highlighting the challenge of UE for medical applications. We also observe that larger models tend to yield better results, suggesting a correlation between model size and the reliability of UE. To address these challenges, we propose Two-phase Verification, a probability-free Uncertainty Estimation approach. First, an LLM generates a step-by-step explanation alongside its initial answer, followed by formulating verification questions to check the factual claims in the explanation. The model then answers these questions twice: first independently, and then referencing the explanation. Inconsistencies between the two sets of answers measure the uncertainty in the original response. We evaluate our approach on three biomedical question-answering datasets using Llama 2 Chat models and compare it against the benchmarked baseline methods. The results show that our Two-phase Verification method achieves the best overall accuracy and stability across various datasets and model sizes, and its performance scales as the model size increases.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), such as GPT-4 and Llama 2, have demonstrated considerable potential in generating human-like text across a broad spectrum of fields, without additional domain-specific training. Their capabilities can be harnessed to provide assistance in the healthcare sector for a wide range of applications, including but not limited to disease diagnosis, clinical decision-making, and patient communication (Cascella et al., 2023). Despite the potential, the deployment of LLMs faces challenges. A prevalent concern is the tendency of LLMs to \u2018hallucinate', a term used to describe circumstances where the model generates plausible yet incorrect information, particularly when they are not able to provide an accurate response (Ji et al., 2023). In high-risk scenarios such as healthcare, where decisions can have direct impact on human lives, ensuring the reliability of LLMs becomes critical. This underscores the need for effective approaches to accurately estimate the uncertainty of generated responses and detect instances of hallucination."}, {"title": "2 Related Work", "content": "In medical settings, existing methods for quantifying uncertainty, including entropy-based methods (Kadavath et al., 2022; Kuhn et al., 2023) and fact-checking (Guo et al., 2022; Shuster et al., 2021), have demonstrated certain limitations. Entropy-based methods operate on the assumption that a model, when confident in its answer, generates a distribution of responses with a small entropy. On the contrary, if the model is unsure, it might hallucinate and produce a diverse range of responses, thus increasing the entropy (Kadavath et al., 2022). However, within the complexity of the medical domain, the model can often fabricate untruthful information with a high level of confidence. This results in a misleadingly low entropy, which fails to accurately represent the uncertainty embedded in the generated response. Fact-checking, another common approach for uncertainty estimation, validates the generated responses by comparing them with relevant truth retrieved from an external knowledge database. However, this method encounters limitations due to the scarcity of comprehensive and professional medical knowledge bases.\nIn this report, we benchmark several popular methods using different model sizes and datasets to establish a comparative understanding of their performance. These benchmarks reveal the challenges of uncertainty estimation in medical question-answering. We also propose Two-phase Verification, a probability-free approach based on the Chain-of-Verification (CoVe) concept (Dhuliawala et al., 2023). This approach operates independently of token-level probabilities and thus can be applied to black-box models. First, the model generates an explanation alongside its initial answer. Next, it formulates verification questions targeting the explanation, to which it provides independent answers. Two-phase Verification refines CoVe's inconsistency check process by prompting the model to answer the verification questions again, using the statement in question as a reference. The inconsistencies between the two sets of answers serve as a measure of uncertainty in the answer. The workflows of CoVe and Two-phase Verification are visualized in Figures la and 1b, respectively."}, {"title": "2.1 Entropy-based methods", "content": "Large Language Models generate output on a token-by-token basis based on the sequence so far. Token probabilities are a direct measure of a model's confidence in its next-token prediction. Xiao & Wang (2021) explore the link between hallucination in conditional language"}, {"title": "2.2 Self-assessment methods", "content": "LLMs possess the inherent potential to reflect on their outputs; however, the self-evaluation may not be robust as LLMs are inclined to find their own content credible (Kadavath et al., 2022). To enhance the calibration and confidence estimation of LLMs, researchers have developed techniques including fine-tuning and prompting. Lin et al. (2022) finetune GPT-3 by supervised learning to express its uncertainty in natural language. Their experiment demonstrates that GPT-3 can be trained to provide answers along with a corresponding confidence level. Similarly, Kadavath et al. (2022) investigate the self-awareness of LLMs by training them to estimate the likelihood that their generated responses are correct. Their research reveals that the effectiveness of self-evaluation improves with model size and few-shot prompting. Additionally, the process benefits from presenting the model with various answer samples before asking it to assess the validity of a single proposed response. Kojima et al. (2023) explore the zero-shot capabilities of LLMs, finding that chain-of-thought prompting boosts the reasoning abilities of LLMs, especially in arithmetic tasks. By instructing LLMs to generate intermediate steps explicitly before answering the questions, a simple prompt template provides performance gain. Manakul et al. (2023) introduce a zero-resource methodology for LLMs to self-check hallucinated generations based on the hypothesis that hallucinations tend to diverge. It operates by generating multiple responses to a prompt and then assessing the factual consistency between these responses."}, {"title": "2.3 External tools", "content": "Since knowledge gaps are a common cause of hallucinations, external knowledge retrieval is often utilized to mitigate hallucinations and produce more faithful generations (He et al., 2022; Shuster et al., 2021). For example, Chern et al. (2023) collect external evidence to validate the factuality of claims extracted from the LLM output. While prompting strategies enhance LLM performance in certain tasks, plausible explanations are often provided even when the final answer is wrong (Kojima et al., 2023). To overcome this limitation, Chen et al. (2023) propose a multi-turn conversation framework to integrate prompting and external tools including calculators and search engines, which reduces the mistakes made by LLMs and enhances the accuracy in complex reasoning tasks."}, {"title": "3 Methodology", "content": "In this section, we elaborate on our approach to estimating generation uncertainty, which leverages the idea from the Chain-of-Verification (CoVe) framework. Our approach is inspired by the foundational work of Dhuliawala et al. (2023) and extends it by integrating a measure for confidence level based on discovered inconsistencies. The primary goal is to identify the occurrence of possible hallucinations by incorporating a robust, unsupervised verification mechanism that operates independently of the model's initial outputs."}, {"title": "3.1 Generate step-by-step explanation", "content": "For each question, the LLM is required to generate a definitive answer followed by a step-by-step explanation. We perform the experiment on two types of questions: those that require a ternary response (affirmative, negative, or uncertain) and those that present multiple-choice options. The definitive answer will be in the form of \"yes,\u201d \"maybe,\" or \"no\" for the first type of questions, or a selection from the multiple-choice options for the second type. This is followed by generating a detailed step-by-step explanation for the chosen answer, which is critical for the subsequent verification chain. The step-by-step breakdown converts the model's reasoning into discrete units that can be independently verified for truthfulness and consistency, thereby enabling an estimation of the overall confidence in the response."}, {"title": "3.2 Plan verification", "content": "Upon generating the initial answer and step-by-step explanation, the model proceeds to formulate a set of verification questions, with each one targeting a single step in the explanation. These questions are purposefully designed to challenge the accuracy of particular factual claims within the individual steps of the explanation. The objective of these questions is to verify the truthfulness of each assertion without necessitating supplementary knowledge or additional context for their resolution. For example, in response to the statement \"Ringed sideroblasts are a characteristic feature of iron overload, particularly in the bone marrow\u201d, a potential verification question could be \u201cWhat condition are ringed sideroblasts typically indicative of?\u201d This question directly targets the factual claim made within the statement and is structured to elicit a response that either confirms or refutes the accuracy of the original statement.\nWhile the model is capable of formulating reasonable verification questions on a zero-shot instruction, the incorporation of a few-shot prompt significantly refines this procedure by enhancing the efficacy of the verification questions. A few-shot prompt presents the model with a set of carefully curated exemplary pairs of statements and corresponding verification questions. These examples serve as a template, showcasing the structure and purpose of a well-crafted verification question. Consequently, this few-shot prompt empowers the model to formulate questions that are not only relevant but also incisive in their ability to discern and test the validity of factual assertions."}, {"title": "3.3 Execute verification", "content": "Given the verification questions, in the next step, the model executes the verification procedure to self-check whether the explanation is accurate. We examined several different approaches for verification in our experiment."}, {"title": "3.3.1 Step verification", "content": "As a base for the verification procedure, we use the model to directly assess the truthfulness and the consistency related to the previous steps of each sentence in the explanation without utilizing the verification questions. For each sentence, the model is prompted to determine its truthfulness based on the prior sentences in the explanation, classifying it as true or false. This serves as a baseline measurement of the model's ability to self-validate its content.\nThis direct approach assumes that the language model is intrinsically capable of recognizing factual information. It provides a straightforward validation mechanism without the additional layer of complexity introduced by verification questions."}, {"title": "3.3.2 CoVe", "content": "In this approach, the LLM answers the verification questions independently to avoid the influence of the initial output. Next, the independent answer will be checked against the original statement being examined for consistency. This is performed by providing the model with both the answer and the statement and asking it to decide if they are consistent or not."}, {"title": "3.3.3 Two-phase verification", "content": "In this more sophisticated approach, the model is prompted to answer each verification question twice. First, the model answers the verification question independently, as in the previous approach. Next, the model is given the statement to be verified as the context and prompted to answer the verification question again. To evaluate whether the two answers are consistent, we adopt a method for checking semantic equivalency which uses a Deberta-large model (He et al., 2021) for a bidirectional entailment check (Kuhn et al., 2023). This process involves appending a special token between the answers and evaluating whether each answer can be inferred from the other, with equivalence determined by mutual \"entailment\" classifications by the model. An example of the Two-phase Verification procedure is illustrated in Figure 2.\nThe rationale for integrating the second verification question answering step responds to two significant challenges encountered during the consistency check in CoVe:\n1. Ambiguity in Consistency Checks: The instructions for a consistency check can themselves be ambiguous due to the different interpretations of \u201cconsistency\". Additionally, the model may fixate on superficial linguistic patterns rather than the underlying factual content. Various phrasings conveying the same meaning may not be recognized as consistent by the model, leading to false judgments in identifying consistency.\n2. Relevance and Information Discrepancies: The independent answer generated by the model could introduce additional information that is not strictly relevant to the initial explanation, or it could omit crucial details, making it difficult to accurately assess consistency. The answer might be factually correct in itself but still not align perfectly with the explanation due to differences in scope or detail level."}, {"title": "3.4 Uncertainty quantification", "content": "Following the completion of the verification steps, we translate the findings into a measurable indicator of uncertainty. This involves counting the statements identified as inconsistent in the verification phase, relative to the overall number of statements in the provided explanation. To express this quantitatively, we compute the Uncertainty Level (UL) using the formula below:\n$UL = \\frac{\\text{Number of Inconsistent Statements}}{\\text{Total Number of Statements in Explanation}}$"}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Experimental setting", "content": "Models We conduct the experiment on Llama 2 Chat (Touvron et al., 2023), which is a collection of open-source chat models fine-tuned for optimized dialogue use cases. Llama 2 Chat (7b) and Llama 2 Chat (13b) were examined in our experiment.\nDatasets We consider three biomedical question-answering (QA) datasets: PubMedQA (Jin et al., 2019), MedQA (Jin et al., 2021) and MedMCQA (Pal et al., 2022). PubMedQA is a biomedical research QA dataset designed to answer questions with a yes/no/maybe format. Each question comes with a context extracted from the corresponding abstract of a research paper and challenges models to reason over quantitative biomedical content. The expert-annotated questions were utilized in our experiment. MedQA is a free-form multiple-choice"}, {"title": "4.2 Results", "content": "We compare Two-phase Verification with several baseline methods on three medical datasets using two Llama 2 Chat models. The results are summarized in Table 1 and Figure 3.\nLexical Similarity (LS), which assesses uncertainty based on the overlap among sample responses, shows the lowest overall average AUROC. This suggests that lexical resemblances are insufficient indicators of certainty in the generated text, where semantic meaning is crucial. Semantic Entropy (SE) and Predictive Entropy (PE) demonstrate moderate improvements over LS. The two methods achieve similar AUROC scores, as they both estimate uncertainty from the entropy of sample responses. SE has slightly better overall results than PE, indicating that semantic clustering is an effective strategy in entropy-based methods. Length-normalized Entropy (LE) achieves the highest average AUROC for the Llama 2 Chat"}, {"title": "5 Discussion", "content": ""}, {"title": "5.1 Uncertainty Estimation in medical QA", "content": "Uncertainty Estimation is of paramount importance in the medical domain, where untruth-ful information can lead to severe consequences. In medical applications utilizing LLMs, such as Al medical chatbots, it is crucial to assess the trustworthiness of model outputs to ensure patient safety. In the cases where the model is less certain in its predictions, the user"}, {"title": "5.2 Limitations and future work", "content": "Verification question generation A critical stage of Two-phase Verification is to generate verification questions that effectively challenge the initial explanation. As explanation paragraphs are generated with linguistic coherence, sentences often use pronouns or references that rely on previous sentences. When verification questions are derived from discrete sentences, the model may miss essential context. Thus, the verification questions might not always incisively interrogate the key information presented. Although few-shot prompts aid question formulation, they can inadvertently inhibit the LLM's creativity, confining it to the patterns seen in these examples. In future work, it will be essential to enhance the generation of verification questions to be more context-aware and adaptable.\nDomain knowledge constraints Another constraint for Two-phase Verification is the knowledge capacity of the language model, which directly affects the quality of the answers to verification questions. Llama 2 Chat, as a general-purpose language model, possesses only a broad understanding of medical knowledge, lacking the depth required for specialized areas. To improve the model's responses to verification questions, we integrate dense retrieval techniques to source relevant information from external databases like Wikipedia. However, this method falls short as the retrieved results frequently have low relevance scores to the verification queries and fail to provide the necessary knowledge. Future improvements should focus on retrieving relevant information from professional medical datasets, such as research papers, medical textbooks, and expert-curated knowledge bases. By leveraging these domain-specific resources, the model can generate more accurate and reliable independent answers to verification questions, enabling more effective detection of hallucinations and uncertainties in medical explanations."}, {"title": "6 Conclusion", "content": "In this paper, we conduct an empirical study on the Uncertainty Estimation of LLMs in medical question-answering tasks. We find Uncertainty Estimation challenging in the medical domain, with existing methods performing poorly, especially with smaller model sizes. To address this challenge, we propose Two-phase Verification, a novel approach that integrates the concept of CoVe to assess the reliability of language model outputs. We show that the model is capable of detecting its own hallucinations by answering verification"}]}