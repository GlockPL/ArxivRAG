{"title": "Uncertainty Estimation of Large Language Models in Medical Question Answering", "authors": ["Jiaxin Wu", "Yizhou Yu", "Hong-Yu Zhou"], "abstract": "Large Language Models (LLMs) show promise for natural language gener-\nation in healthcare, but risk hallucinating factually incorrect information.\nDeploying LLMs for medical question answering necessitates reliable un-\ncertainty estimation (UE) methods to detect hallucinations. In this work,\nwe benchmark popular UE methods with different model sizes on medical\nquestion-answering datasets. Our results show that current approaches\ngenerally perform poorly in this domain, highlighting the challenge of UE\nfor medical applications. We also observe that larger models tend to yield\nbetter results, suggesting a correlation between model size and the reliabil-\nity of UE. To address these challenges, we propose Two-phase Verification, a\nprobability-free Uncertainty Estimation approach. First, an LLM generates\na step-by-step explanation alongside its initial answer, followed by formu-\nlating verification questions to check the factual claims in the explanation.\nThe model then answers these questions twice: first independently, and\nthen referencing the explanation. Inconsistencies between the two sets of\nanswers measure the uncertainty in the original response. We evaluate our\napproach on three biomedical question-answering datasets using Llama 2\nChat models and compare it against the benchmarked baseline methods.\nThe results show that our Two-phase Verification method achieves the best\noverall accuracy and stability across various datasets and model sizes, and\nits performance scales as the model size increases.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), such as GPT-4 and Llama 2, have demonstrated consid-\nerable potential in generating human-like text across a broad spectrum of fields, without\nadditional domain-specific training. Their capabilities can be harnessed to provide assis-\ntance in the healthcare sector for a wide range of applications, including but not limited\nto disease diagnosis, clinical decision-making, and patient communication (Cascella et al.,\n2023). Despite the potential, the deployment of LLMs faces challenges. A prevalent concern\nis the tendency of LLMs to \u2018hallucinate', a term used to describe circumstances where the\nmodel generates plausible yet incorrect information, particularly when they are not able\nto provide an accurate response (Ji et al., 2023). In high-risk scenarios such as healthcare,\nwhere decisions can have direct impact on human lives, ensuring the reliability of LLMs\nbecomes critical. This underscores the need for effective approaches to accurately estimate\nthe uncertainty of generated responses and detect instances of hallucination."}, {"title": "2 Related Work", "content": "In medical settings, existing methods for quantifying uncertainty, including entropy-based\nmethods (Kadavath et al., 2022; Kuhn et al., 2023) and fact-checking (Guo et al., 2022;\nShuster et al., 2021), have demonstrated certain limitations. Entropy-based methods operate\non the assumption that a model, when confident in its answer, generates a distribution of\nresponses with a small entropy. On the contrary, if the model is unsure, it might hallucinate\nand produce a diverse range of responses, thus increasing the entropy (Kadavath et al.,\n2022). However, within the complexity of the medical domain, the model can often fabricate\nuntruthful information with a high level of confidence. This results in a misleadingly low\nentropy, which fails to accurately represent the uncertainty embedded in the generated\nresponse. Fact-checking, another common approach for uncertainty estimation, validates\nthe generated responses by comparing them with relevant truth retrieved from an external\nknowledge database. However, this method encounters limitations due to the scarcity of\ncomprehensive and professional medical knowledge bases.\nIn this report, we benchmark several popular methods using different model sizes and\ndatasets to establish a comparative understanding of their performance. These benchmarks\nreveal the challenges of uncertainty estimation in medical question-answering. We also pro-\npose Two-phase Verification, a probability-free approach based on the Chain-of-Verification\n(CoVe) concept (Dhuliawala et al., 2023). This approach operates independently of token-\nlevel probabilities and thus can be applied to black-box models. First, the model generates an\nexplanation alongside its initial answer. Next, it formulates verification questions targeting\nthe explanation, to which it provides independent answers. Two-phase Verification refines\nCoVe's inconsistency check process by prompting the model to answer the verification\nquestions again, using the statement in question as a reference. The inconsistencies between\nthe two sets of answers serve as a measure of uncertainty in the answer. The workflows of\nCoVe and Two-phase Verification are visualized in Figures la and 1b, respectively."}, {"title": "2.1 Entropy-based methods", "content": "Large Language Models generate output on a token-by-token basis based on the sequence\nso far. Token probabilities are a direct measure of a model's confidence in its next-token pre-\ndiction. Xiao & Wang (2021) explore the link between hallucination in conditional language"}, {"title": "2.2 Self-assessment methods", "content": "LLMs possess the inherent potential to reflect on their outputs; however, the self-evaluation\nmay not be robust as LLMs are inclined to find their own content credible (Kadavath et al.,\n2022). To enhance the calibration and confidence estimation of LLMs, researchers have\ndeveloped techniques including fine-tuning and prompting. Lin et al. (2022) finetune GPT-3\nby supervised learning to express its uncertainty in natural language. Their experiment\ndemonstrates that GPT-3 can be trained to provide answers along with a corresponding\nconfidence level. Similarly, Kadavath et al. (2022) investigate the self-awareness of LLMs\nby training them to estimate the likelihood that their generated responses are correct.\nTheir research reveals that the effectiveness of self-evaluation improves with model size\nand few-shot prompting. Additionally, the process benefits from presenting the model\nwith various answer samples before asking it to assess the validity of a single proposed\nresponse. Kojima et al. (2023) explore the zero-shot capabilities of LLMs, finding that\nchain-of-thought prompting boosts the reasoning abilities of LLMs, especially in arithmetic\ntasks. By instructing LLMs to generate intermediate steps explicitly before answering the\nquestions, a simple prompt template provides performance gain. Manakul et al. (2023)\nintroduce a zero-resource methodology for LLMs to self-check hallucinated generations\nbased on the hypothesis that hallucinations tend to diverge. It operates by generating\nmultiple responses to a prompt and then assessing the factual consistency between these\nresponses."}, {"title": "2.3 External tools", "content": "Since knowledge gaps are a common cause of hallucinations, external knowledge retrieval\nis often utilized to mitigate hallucinations and produce more faithful generations (He et al.,\n2022; Shuster et al., 2021). For example, Chern et al. (2023) collect external evidence to\nvalidate the factuality of claims extracted from the LLM output. While prompting strategies\nenhance LLM performance in certain tasks, plausible explanations are often provided even\nwhen the final answer is wrong (Kojima et al., 2023). To overcome this limitation, Chen et al.\n(2023) propose a multi-turn conversation framework to integrate prompting and external\ntools including calculators and search engines, which reduces the mistakes made by LLMs\nand enhances the accuracy in complex reasoning tasks."}, {"title": "3 Methodology", "content": "In this section, we elaborate on our approach to estimating generation uncertainty, which\nleverages the idea from the Chain-of-Verification (CoVe) framework. Our approach is\ninspired by the foundational work of Dhuliawala et al. (2023) and extends it by integrating\na measure for confidence level based on discovered inconsistencies. The primary goal is to\nidentify the occurrence of possible hallucinations by incorporating a robust, unsupervised\nverification mechanism that operates independently of the model's initial outputs."}, {"title": "3.1 Generate step-by-step explanation", "content": "For each question, the LLM is required to generate a definitive answer followed by a step-by-\nstep explanation. We perform the experiment on two types of questions: those that require a\nternary response (affirmative, negative, or uncertain) and those that present multiple-choice\noptions. The definitive answer will be in the form of \"yes,\u201d \"maybe,\" or \"no\" for the first\ntype of questions, or a selection from the multiple-choice options for the second type. This\nis followed by generating a detailed step-by-step explanation for the chosen answer, which\nis critical for the subsequent verification chain. The step-by-step breakdown converts the\nmodel's reasoning into discrete units that can be independently verified for truthfulness\nand consistency, thereby enabling an estimation of the overall confidence in the response."}, {"title": "3.2 Plan verification", "content": "Upon generating the initial answer and step-by-step explanation, the model proceeds to for-\nmulate a set of verification questions, with each one targeting a single step in the explanation.\nThese questions are purposefully designed to challenge the accuracy of particular factual\nclaims within the individual steps of the explanation. The objective of these questions is to\nverify the truthfulness of each assertion without necessitating supplementary knowledge or\nadditional context for their resolution. For example, in response to the statement \"Ringed\nsideroblasts are a characteristic feature of iron overload, particularly in the bone marrow\u201d, a potential\nverification question could be \u201cWhat condition are ringed sideroblasts typically indicative of?\u201d\nThis question directly targets the factual claim made within the statement and is structured\nto elicit a response that either confirms or refutes the accuracy of the original statement.\nWhile the model is capable of formulating reasonable verification questions on a zero-shot\ninstruction, the incorporation of a few-shot prompt significantly refines this procedure by\nenhancing the efficacy of the verification questions. A few-shot prompt presents the model\nwith a set of carefully curated exemplary pairs of statements and corresponding verification\nquestions. These examples serve as a template, showcasing the structure and purpose of a\nwell-crafted verification question. Consequently, this few-shot prompt empowers the model\nto formulate questions that are not only relevant but also incisive in their ability to discern\nand test the validity of factual assertions."}, {"title": "3.3 Execute verification", "content": "Given the verification questions, in the next step, the model executes the verification pro-\ncedure to self-check whether the explanation is accurate. We examined several different\napproaches for verification in our experiment."}, {"title": "3.3.1 Step verification", "content": "As a base for the verification procedure, we use the model to directly assess the truthfulness\nand the consistency related to the previous steps of each sentence in the explanation without\nutilizing the verification questions. For each sentence, the model is prompted to determine\nits truthfulness based on the prior sentences in the explanation, classifying it as true or false.\nThis serves as a baseline measurement of the model's ability to self-validate its content.\nThis direct approach assumes that the language model is intrinsically capable of recogniz-\ning factual information. It provides a straightforward validation mechanism without the\nadditional layer of complexity introduced by verification questions."}, {"title": "3.3.2 CoVe", "content": "In this approach, the LLM answers the verification questions independently to avoid the\ninfluence of the initial output. Next, the independent answer will be checked against the\noriginal statement being examined for consistency. This is performed by providing the\nmodel with both the answer and the statement and asking it to decide if they are consistent\nor not."}, {"title": "3.3.3 Two-phase verification", "content": "In this more sophisticated approach, the model is prompted to answer each verification\nquestion twice. First, the model answers the verification question independently, as in the\nprevious approach. Next, the model is given the statement to be verified as the context\nand prompted to answer the verification question again. To evaluate whether the two\nanswers are consistent, we adopt a method for checking semantic equivalency which uses\na Deberta-large model (He et al., 2021) for a bidirectional entailment check (Kuhn et al.,\n2023). This process involves appending a special token between the answers and evaluating\nwhether each answer can be inferred from the other, with equivalence determined by\nmutual \"entailment\" classifications by the model. An example of the Two-phase Verification\nprocedure is illustrated in Figure 2.\nThe rationale for integrating the second verification question answering step responds to\ntwo significant challenges encountered during the consistency check in CoVe:\n1. Ambiguity in Consistency Checks: The instructions for a consistency check can\nthemselves be ambiguous due to the different interpretations of \u201cconsistency\".\nAdditionally, the model may fixate on superficial linguistic patterns rather than\nthe underlying factual content. Various phrasings conveying the same meaning\nmay not be recognized as consistent by the model, leading to false judgments in\nidentifying consistency.\n2. Relevance and Information Discrepancies: The independent answer generated by\nthe model could introduce additional information that is not strictly relevant to the\ninitial explanation, or it could omit crucial details, making it difficult to accurately\nassess consistency. The answer might be factually correct in itself but still not align\nperfectly with the explanation due to differences in scope or detail level."}, {"title": "3.4 Uncertainty quantification", "content": "Following the completion of the verification steps, we translate the findings into a measur-\nable indicator of uncertainty. This involves counting the statements identified as inconsistent\nin the verification phase, relative to the overall number of statements in the provided ex-\nplanation. To express this quantitatively, we compute the Uncertainty Level (UL) using the\nformula below:\n$UL = \\frac{\\text{Number of Inconsistent Statements}}{\\text{Total Number of Statements in Explanation}}$"}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Experimental setting", "content": "Models We conduct the experiment on Llama 2 Chat (Touvron et al., 2023), which is a\ncollection of open-source chat models fine-tuned for optimized dialogue use cases. Llama 2\nChat (7b) and Llama 2 Chat (13b) were examined in our experiment.\nDatasets We consider three biomedical question-answering (QA) datasets: PubMedQA (Jin\net al., 2019), MedQA (Jin et al., 2021) and MedMCQA (Pal et al., 2022). PubMedQA is a\nbiomedical research QA dataset designed to answer questions with a yes/no/maybe format.\nEach question comes with a context extracted from the corresponding abstract of a research\npaper and challenges models to reason over quantitative biomedical content. The expert-\nannotated questions were utilized in our experiment. MedQA is a free-form multiple-choice"}, {"title": "4.2 Results", "content": "We compare Two-phase Verification with several baseline methods on three medical datasets\nusing two Llama 2 Chat models. The results are summarized in Table 1 and Figure 3.\nLexical Similarity (LS), which assesses uncertainty based on the overlap among sample re-\nsponses, shows the lowest overall average AUROC. This suggests that lexical resemblances\nare insufficient indicators of certainty in the generated text, where semantic meaning is\ncrucial. Semantic Entropy (SE) and Predictive Entropy (PE) demonstrate moderate improve-\nments over LS. The two methods achieve similar AUROC scores, as they both estimate\nuncertainty from the entropy of sample responses. SE has slightly better overall results than\nPE, indicating that semantic clustering is an effective strategy in entropy-based methods.\nLength-normalized Entropy (LE) achieves the highest average AUROC for the Llama 2 Chat"}, {"title": "5 Discussion", "content": ""}, {"title": "5.1 Uncertainty Estimation in medical QA", "content": "Uncertainty Estimation is of paramount importance in the medical domain, where untruth-\nful information can lead to severe consequences. In medical applications utilizing LLMs,\nsuch as Al medical chatbots, it is crucial to assess the trustworthiness of model outputs to\nensure patient safety. In the cases where the model is less certain in its predictions, the user"}, {"title": "5.2 Limitations and future work", "content": "Verification question generation A critical stage of Two-phase Verification is to generate\nverification questions that effectively challenge the initial explanation. As explanation para-\ngraphs are generated with linguistic coherence, sentences often use pronouns or references\nthat rely on previous sentences. When verification questions are derived from discrete\nsentences, the model may miss essential context. Thus, the verification questions might not\nalways incisively interrogate the key information presented. Although few-shot prompts\naid question formulation, they can inadvertently inhibit the LLM's creativity, confining it\nto the patterns seen in these examples. In future work, it will be essential to enhance the\ngeneration of verification questions to be more context-aware and adaptable.\nDomain knowledge constraints Another constraint for Two-phase Verification is the knowl-\nedge capacity of the language model, which directly affects the quality of the answers to\nverification questions. Llama 2 Chat, as a general-purpose language model, possesses only a\nbroad understanding of medical knowledge, lacking the depth required for specialized areas.\nTo improve the model's responses to verification questions, we integrate dense retrieval\ntechniques to source relevant information from external databases like Wikipedia. However,\nthis method falls short as the retrieved results frequently have low relevance scores to the\nverification queries and fail to provide the necessary knowledge. Future improvements\nshould focus on retrieving relevant information from professional medical datasets, such as\nresearch papers, medical textbooks, and expert-curated knowledge bases. By leveraging\nthese domain-specific resources, the model can generate more accurate and reliable indepen-\ndent answers to verification questions, enabling more effective detection of hallucinations\nand uncertainties in medical explanations."}, {"title": "6 Conclusion", "content": "In this paper, we conduct an empirical study on the Uncertainty Estimation of LLMs in\nmedical question-answering tasks. We find Uncertainty Estimation challenging in the\nmedical domain, with existing methods performing poorly, especially with smaller model\nsizes. To address this challenge, we propose Two-phase Verification, a novel approach that\nintegrates the concept of CoVe to assess the reliability of language model outputs. We show\nthat the model is capable of detecting its own hallucinations by answering verification"}]}