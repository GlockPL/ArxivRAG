{"title": "Foundations for Unfairness in Anomaly Detection - Case Studies in Facial Imaging Data", "authors": ["Michael Livanos", "Ian Davidson"], "abstract": "Deep anomaly detection (AD) is perhaps the most controversial of data analytic tasks as it identifies entities that are then specifically targeted for further investigation or exclusion. Also controversial is the application of AI to facial imaging data. This work explores the intersection of these two areas to understand two core questions: \"Who\" these algorithms are being unfair to and equally important \"Why\". Recent work has shown that deep AD can be unfair to different groups despite being unsupervised with a recent study showing that for portraits of people: men of color are far more likely to be chosen to be outliers. We study the two main categories of AD algorithms: autoencoder-based and single-class-based which effectively try to compress all the instances with those that can not be easily compressed being deemed to be outliers. We experimentally verify sources of unfairness such as the under-representation of a group (e.g. people of color are relatively rare), spurious group features (e.g. men are often photographed with hats), and group labeling noise (e.g. race is subjective). We conjecture that lack of compressibility is the main foundation and the others cause it but experimental results show otherwise and we present a natural hierarchy amongst them.", "sections": [{"title": "Introduction", "content": "Anomaly detection (AD) is a central part of data analytics and perhaps the most controversial given that it is employed for high-impact applications that identify individuals for intervention, policing, and investigation. Its use is prevalent to identify unusual behavior in finance (transactions)(Huang et al. 2018; Zamini and Hasheminejad 2019), social media (posting and account creation)(Yu et al. 2016; Savage et al. 2014), and government services (medicare claims)(Zhang and He 2017; Bauder and Khoshgoftaar 2017).\nPerhaps one of the most controversial applications of AI is to facial imaging. This is due to our faces being uniquely identifying and personal. Further, the AI's ability to identify us and make decisions (without consent) crosses many cultural and legal barriers (Garvie, Bedoya, and Frankle 2016). Existing work on facial data has focused predominantly on facial recognition, that is, given a large collection of people in a known database, identify if any of them occur in an image. Though legislation and progress have been made towards regulating facial recognition technology (Almeida, Shmarko, and Lomas 2022) other technologies in particular AD involving facial images are starting to emerge which gives rise to new ethical considerations and understanding. Previous work (Zhang and Davidson 2021) has just begun to explore the unfairness at the intersection of AD applied to facial imaging data. For example, our previous work showed that applying AD to a collection of celebrity images overwhelmingly showed the anomalies being people of color and males (see Figure 1). However, our previous work was mainly focused on making AD algorithms fairer. We recreate our earlier results for not only the one-class AD method and the celebrity image dataset the authors used but also for the popular auto-encoder AD method and a more challenging dataset (Labeled Face In The Wild(Huang et al. 2007)).\nOur experimental section attempts to address the \"Who\" and \"Why\" questions. We create a measure of unfairness (Disparate Impact Ratio (DIR)) which measures how over-represented a protected group (or its complement) is in the anomaly set. We then experimentally investigate who these algorithms are being unfair to and more nuanced questions such as is the same group always being treated unfairly regardless of algorithm. We also explore why an unsupervised algorithm can be biased. We conjecture four main foundations of unfairness, propose metrics to measure them, and outline a series of experiments to test a hypothesis on how they are structured.\nThe contributions of this work as are as follows:\n\u2022 We study the \"Who\" and \"Why\" questions when anomaly detection is applied to facial imaging data a topic to our knowledge has not been addressed before.\n\u2022 Our experiments addressing the \"Who\" question show that group-level unfairness is due to an interaction between the dataset and the algorithm.\n\u2022 We conjecture four main reasons for the \"Why\u201d question: i) incompressibility, ii) sample size bias (SSB), iii) spurious feature variance (SFV) within a group, and iv) attribute/group labeling noise (ALN).\n\u2022 We postulate an intuitive structure to our conjectured reasons, showing it is not empirically verified, but our experimental results suggest an alternative structure.\nWe begin by discussing background and related work. We"}, {"title": "Background and Related Work", "content": "Applications of AD to Facial Data. AD algorithms have been used on imaging data for a variety of reasons. Perhaps the most ubiquitous is for data cleaning where anomalies are viewed as being \"noise\" (Ng and Winkler 2014) which are removed and then a downstream supervised algorithm is applied. However, if the AD algorithm is biased this creates an under-representation in the down-stream training tasks.\nAnother common use of AD is to view the outliers as \"signal\" and in doing so flag the outliers for extra attention. Examples include using AD to identify facial expressions to recognize emotions (Zhang et al. 2020) such as surprise. However, if the AD is biased towards some groups this will over-predict certain emotions for certain groups. Similarly, AD can be used to identify aggressive behavior (Cao et al. 2021). However, if the AD has a bias towards some groups this will incorrectly identify the group as being overly aggressive.\nSource of Bias. It has been well established that supervised learning algorithms can have bias due to a variety of reasons. In particular class labeling bias has been extensively studied in the context of the Compas dataset (Angwin et al. 2016). Even though features (e.g. race) associated with this bias are removed, deep learning offers the ability to learn surrogates (e.g. zip code)(Raghavan et al. 2020).\nThe work on fair AD starts in 2020 (Davidson and Ravi"}, {"title": "Four Reasons for Unfairness And Their Measurement", "content": "Here we outline our four premises for unfairness in AD and explain them at a conceptual level using Figure 1. We then describe how we measure them.\nIncompressability of Data\nWe begin by discussing how AD methods work in particular what causes an instance to be an outlier. Deep AD methods at their core employ compression either directly or indirectly. Instances that cannot be compressed well are deemed outliers and if a group is unusual in some sense it will be unfairly treated as it will be hard to compress and hence overwhelmingly flagged as an outlier.\nTo understand this further, we present a common taxonomy of anomaly detection algorithms(Pang et al. 2021).\nAutoencoder for Anomaly Detection. Let $\u03a6_e$ be the encoding network which maps the data X into the compressed latent space and \u03a6_a be the decoding network which maps the latent representation $\u03a6_e(X)$ back to the original feature space(Hinton 1989). Given the network parameters $\u03b8_e, \u03b8_a$ the standard reconstruction objective to train the autoencoder is:\n$argmin_{\u03b8_e,\u03b8_a}\\frac{1}{n} \\sum_{i=1}^{n} ||x_i - \u03a6_{\u03b8_a}(\u03a6_{\u03b8_e}(x_i))||^2 + R$\n(1)\nThe term R denotes the regularization to the encoder and decoder. The anomaly score s(x) for instance x is calculated from the reconstruction error:\ns(x) = ||x \u2013 $\u03a6_{\u03b8_a}(\u03a6_{\u03b8_e}(x))$||2\n(2)\nHere clearly an outlier is defined as being an instance that the AE cannot easily compress and hence cannot easily reconstruct(Japkowicz, Myers, and Gluck 1995).\nOne-Class/Cluster Anomaly Detection Next, consider one class anomaly detection which is still unsupervised. Given the training data of instances X \u2208 $R^{n\u00d7d}$, one class AD method such as the the popular deep SVDD (Ruff et al. 2018) network is trained to map all the n instances close to a fixed center c. Denote function \u03a6 as a neural network with parameters \u03b8 the training objective function is:\n$argmin_{\u03b8} \\frac{1}{n} \\sum_{i=1}^{n} ||\u03a6_{\u03b8}(x_i) - c||^2 + R$\n(3)"}, {"title": "Causes Beyond Incompressibility", "content": "The above states that outliers are inherently points that the deep learner cannot compress. Hence it is natural to consider reasons why a deep learner cannot compress a group as being a key issue for unfairness. Here we conjecture three main reasons with the view they are related to biased outliers as shown in Figure 2.\nGroup Underrepresentation. Here we have a group that is relatively rare in the dataset but has some unique properties so the deep learner cannot compress it well. For example in Figure 1 many outliers are African Americans as they only consist of under 15% of the dataset hence the deep learner uses its limited encoding space to encode more populous properties.\nSpurious Features for Groups. In this situation, the group has a property that is not critical for the outlier detection task but is highly variable. For example in Figure 1 many groups who are over-represented in the outliers wear different styles of hats.\nLabel Attribution Noise. Here the labeling of a group is inaccurate and hence can be a reason a group is labeled as being overly abundant in the outlier group. For example in Figure 1 the second to the bottom line of outliers all have the tag Male but this is erroneous."}, {"title": "Measurements of Unfairness and Four Properties", "content": "Before discussing our empirical results, we first define each of the properties and how anomaly unfairness is measured. Many of these metrics are the maximum between some expression and their reciprocal. This is because the presence of a tag is equally important as the absence of a tag: for example, disparate treatment of young people and disparate treatment of old (i.e. not young) people are equally important phenomena to study. We first describe how we measure unfairness for anomalies and then how we measure our four properties.\nAnomaly DIR: The unfairness of an AD algorithm's output for a particular group a is measured by the disparate impact ratio (DIR), which is (Feldman et al. 2015):\n$DIR(X, AD, a) = max(\\frac{P(AD(X) = 1|A = a)}{P(AD(X) = 1|A = \u00aca)}, \\frac{P(AD(X) = 1|A = \u00aca)}{P(AD(X) = 1|A = a)})$ (6)\nHere X is the dataset the AD algorithm (AD) has made predictions (normal vs anomaly) with AD(x) = 1 implying x is an anomaly and AD(x) = 0 implying it is a normal instance, and a is the group in question. This is a natural choice for anomaly detection as it compares the rate at which different attributes are flagged as anomalies, normalized by how often the rest of the data is considered anomalous. It is also the most widely used metric in fair unsupervised learning(Verma and Rubin 2018). The range for this metric is [1,\u221e) with the larger the number the more unfairly group a is treated.\nIncompressibility: To measure this feature, we extend the typical measure of reconstruction error into the novel metric of reconstruction ratio, which is defined:\n$RR(X, f, a) = max(\\frac{LOSSMSE(X, f(X)|A = a)}{LOSSMSE(X, f(X)|A = \u00aca)}, \\frac{LOSSMSE(X, f(X)|A = \u00aca)}{LOSSMSE(X, f(X)|A = a)})$ (7)\nHere X and a are the data used for AD and group again, with f being the autoencoder model (both encoder and decoder). The range of Equation 7 is therefore also [1,\u221e), where a higher number indicates that a group is harder to compress than the rest of the data. For example, a RR of 2 indicates that the attribute/group (or absence of the attribute/group) is twice as difficult to compress than the rest of the data."}, {"title": "Sample Size Bias (SSB)", "content": "SSB (sometimes referred to as representation bias) is determined by the proportion of that tag or lack in the dataset X and is measured as(Suresh and Guttag 2021):\n$SSB(X, a) = max(P(A = a|X), P(A = \u00aca|X))$ (8)\nWhere X and a are again the data and the group in question. Because all groups are binary (or encoded as one-hot encoding), the range of this metric is [0.5, 1], with 0.5 indicating perfect balance of the group (i.e. males and females are equally likely) and 1 indicating that the group is always on or always off. Most groups will fall between these two extremes.\nSpurious Feature Variance (SFV): SFV refers to the amount of variance in the background objects in the image and is measured as a proportion of the reconstruction error of the image:\n$SFV(X, f, a, b) = 1 - max(\\frac{LOSSMSE(X[b], f(X)[b]|A = a)}{LOSSMSE(X, f(X)|A = a)}, \\frac{LOSSMSE(X[b], f(X)[b]|A = \u00aca)}{LOSSMSE(X, f(X)|A = \u00aca)})$ (9)\nWhere X is the data, f is the autoencoder, a the tag, and b is a bounding rectangle around the foreground/focus of the image (i.e. the face), either provided by the data or estimated(Kumar et al. 2009). As the denominator is clearly always greater than or equal to the numerator, SFV ranges between [0, 1], where higher values indicate that more error comes from spurious features.\nLabel Attribute Noise (LAN): This is a metric of how noisy the labeling of a particular group is, as provided by the academic literature((Lingenfelter, Davis, and Hand 2022) for CelebA and (Kumar et al. 2009) for LFW). Some groups such as Gender tend to have very low LAN, whereas other tags have very high LAN such as Blurry(Kumar et al. 2009). We define LAN as:\n$ALN(X, a, a*) = 1 \u2212 (P(a = a*|X) + P(\u00aca = \u00aca*|X))$ (10)\nWhere X is the data, a the group in question, and a* the true label for the group. This property has a range [0, 1] where the higher the value the less reliable the group labeling."}, {"title": "Experimental Results - Who Is AD Unfair To?", "content": "Here we answer the question: Who are the groups of individuals most adversely affected? Following this, we explore more nuanced inquiries, such as whether the unfairness is attributable solely to the data, the algorithm, or a combination of both. In the subsequent section, we aim to investigate the underlying reasons for the unfairness inherent in AD.\nOur experiments consist of two core AD algorithms: A reconstruction based autoencoder anomaly detection algorithm (hereby referred to as AE) and Deep one-class SVDD"}, {"title": "The Algorithms are Overwhelming Fair to Most Groups.", "content": "In total amongst both the two algorithms and two datasets there are 222 groups and a frequency distribution shows that overwhelmingly the algorithms are fair with respect to over 70% of the groups as shown in Figure 3. A score of less than 1.2 indicates that the occurrence of the group in the anomalies is not more than 20% greater than the rate of all other groups (together) being labeled anomalies.\nHowever, there are significant examples of unfairness whose properties we now discuss.\nFew Groups Are Always Treated Unfairly. We found that there are several groups that are always (regardless of algorithm or dataset) treated unfairly but they are relatively rare. These include the groups centered around weight having the annotations Chubby, Double-Chin and those centered around very unusual image properties such as Wearing-Hats. This is not unexpected given a very rare"}, {"title": "Unfairness Varies Due to Both Algorithm and Dataset.", "content": "A more likely occurrence is that some groups are treated very unfairly but only for some datasets and some algorithms. Table 1 shows in bold groups treated unfairly (the Anomaly DIR is shown in parentheses) but only for that dataset and algorithm combination. For other algorithm-dataset combinations, they are treated fairly as the Table shows. This result is surprising and shows the strong interaction between the algorithm and the data. Consider that the AE method labeled No Beard (reported as \"Beard\") in the CelebA dataset at a rate over 3 times greater than the other groups. Yet, the SVDD algorithm on the very same dataset produced just a 1.27 DIR for the Beard group, and in the LFW dataset both algorithms the DIR was below 1.2.\nThe More Focused The Dataset The More Likely Unfairness Can Occur. When we aggregated all fairness DIR scores (see Appendix) for each group and all algorithms we found that the CelebA dataset (Mean DIR = 1.4) causes significantly more unfairness than the LFW dataset (Mean DIR = 1.13).\nThis is likely due to the CelebA dataset having a much more focused selection bias as it is limited to people who are overwhelmingly in the arts (film, television, music) whereas the LFW dataset consists of a larger representation of popular people. Hence, the definition of normality learned is very specific and there are many ways to deviate from the norm. Examples of groups that are found to be unfairly treated in the CelebA dataset but NOT the LFW dataset are: Wearing Hat, Big Nose, Eye-Glasses, Goatee, Wavy-Hair.\nThe More Focused The Algorithm The More Likely Unfairness Can Occur.\nSimilarly, the way the algorithm defines normality is in"}, {"title": "Experimental Results - Why is AD Unfair", "content": "Here we attempt to experimentally answer the following questions:\n\u2022 How strong are our four properties correlated to unfairness?\n\u2022 How are our four properties related to each other and in particular is there a hierarchical structure to them?\n\u2022 How can these properties be combined to create a model to explain unfairness in anomaly detection?\nRelationship between Unfairness and Each Property\nOur experiments (see Figure 5) demonstrate strong (Pearson) correlations and moderate to strong RSQ (R-squared values of the regression trendline) for each of the properties studied. Each plot shows the results for two datasets (CelebA and LFW) with each data point representing a group of individuals. A positive trend line indicates positive Pearson correlation (see sub-titles of plots for exact values) and we see that incompressability is the most strongest property correlated with unfairness, then Spurious features, then Attribute label noise, and finally Sample Size Bias. This is an interesting result as earlier seminal results showed that AD using facial images (Zhang and Davidson 2021) was unfair due to an under-representation of African Americans and Males in the underlying datasets.\nHowever, it is also clear that no individual property explains unfairness completely by itself. This is shown as each graph has points that not only do not fit the trendline, but are"}, {"title": "Hypothesis Testing of Relationship Claims", "content": "In order to test our claims, we create four hypotheses that we verify through hypothesis significance-testing. Those are:\n\u2022 H1: No individual property is sufficient to always explain unfairness.\n\u2022 H2: The properties, when combined into a multiple regression, are sufficient to explain unfairness.\n\u2022 H3: No properties of the multiple regression are redundant and all are needed.\n\u2022 H4: The results of H2 are significant in that when one property fails to predict unfairness, another does.\nNull hypothesised H10 \u2013 H40 are constructed straightforwardly. To create the significance test for H1, we perform an F-test on individual regression models crafted from the relationship between each property and DIR. The results of this F-Test (visualized in Figure 7) indicate that individual properties are reasonable though comparably weak predictors of unfairness, with P-values ranging from 0.0137-0.0986 for the AE model and 0.0279-0.0571 for Deep SVDD. Therefore, we reject the null hypothesis H10 and validate hypothesis H1.\nTo test hypotheses H2 and H3, we construct a multiple-regression model. Specifically, this is a stacked multiple regression where the meta-function selects the best individual model for the datum. To validate H2, we create such a multiple-regression using all four of the properties (the \"full\" model). This yields P-Values of 0.00589 for the AE model and 0.0127 for Deep SVDD, significantly lower than those of the respective single-regression models, and indicating that using all four properties is sufficient to explain how unfairness occurs. We reject the null hypothesis H20 and validate hypothesis H2.\nFor H3, we conduct a similar experiment except we leave one property out. In every case, the resulting multiple regression models were worse than the full model, with P-Values ranging from 0.00674-0.0109 for the AE model and 0.0138-0.0164 for Deep SVDD, all greater than that of the full model, indicating that every property is necessary and none are redundant. We reject the null hypothesis H30 and validate hypothesis H3.\nOne may object to the multiple-regression models used above, given that the model as described will monotonically increase in predictive power given more properties. It is important to note that this model matches the central claim of this paper - that unfairness with respect to a group occurs because of one of the four properties described, though one may still be wary of the statistical significance of the reported results given the technique. To resolve these concerns, we demonstrate that our model is not just combining the predictive power of four different already powerful predictors, but rather when one model fails it is because it is explained by one of the other properties.\nTo validate this claim, we construct fabricated distributions similar to those of Figure 5. Specifically, unfairness is kept the same, and we create distributions of random fake data which has the same correlation and RSQ as all of those shown. This is accomplished by, for each property, finding random points (sampled across a uniform distribution) along the X-axis, giving them fabricated values perfectly in line with the correlation, and then adding noise such that the correlation is maintained and the RSQ matches that of the actual measured properties. Then, we create the same full model of the multiple regression and measure the P-value. We repeat this process 10,000 times to get 10,000 such distributions.\nThe distributions therefore should be statistically similar to our real data, but there is no reason to believe that when one of the fabricated models fails, another will explain the unfairness. To validate hypothesis H4, we measure the number of times the fake distributions produce P-values under that of the real data. If the statically similar fabricated data"}, {"title": "A Proposed Model Of Unsupervised Unfairness Relationships", "content": "Given the resulting hypothesis tests, we craft our model of unfairness in unsupervised learning. Figure 8 provides a graphical representation of this model. Edges between prop"}, {"title": "Conclusion, Limitations, and Future Work", "content": "We study the intersection of the controversial deep AD algorithm with facial imaging data to address the \"Who\" and \"Why\" questions. We found that overwhelmingly both auto-encoder and one-class deep AD algorithms are fair to most groups. However, due to the compression-based focus, they are unfair to some sub-groups.\nWith regard to the \u201cWho\u201d question we found that it was rare to be consistently unfair to the one group and instead unfairness was due to the interaction of the data and the algorithm. In particular, the more focused the dataset and algorithm the more unfairness was found.\nOur study of the \"Why\u201d question aimed at developing a deeper understanding on the effect of data related factors on the fairness as well as detection performance of OD algorithms. We postulated four hypotheses and found all to be statistically significant by rejecting the null hypothesis. The first hypothesis is that no single property alone is sufficient to explain unfairness. The second hypothesis is when combined the properties can explain unfairness. The third hypothesis is that all properties are relevant and none are redundant and finally, the fourth hypothesis is that the combination of properties is meaningful beyond the predictive power of each individual property.\nLimitations. The use of groups may have varying degrees of applicability to real-world fairness scenarios. For example, some groups such as Male, Black and Young correspond to legally recognized protected classes (88th United States Congress 1964; 90th United States Congress 1967), while others such as Goatee, Wearing Hat and attractive may not. However, we believe that this study still provides meaningful insights into the mechanism of un"}]}