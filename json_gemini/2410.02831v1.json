{"title": "Skill Issues: An Analysis of CS:GO Skill Rating Systems", "authors": ["Mikel Bober-Irizar", "Naunidh Dua", "Max McGuinness"], "abstract": "The meteoric rise of online games has created a need for accurate skill rating systems for tracking improvement and fair matchmaking. Although many skill rating systems are deployed, with various theoretical foundations, less work has been done at analysing the real-world performance of these algorithms. In this paper, we perform an empirical analysis of Elo, Glicko2 and TrueSkill through the lens of surrogate modelling, where skill ratings influence future matchmaking with a configurable acquisition function. We look both at overall performance and data efficiency, and perform a sensitivity analysis based on a large dataset of Counter-Strike: Global Offensive matches.", "sections": [{"title": "I. INTRODUCTION", "content": "Counter-Strike: Global Offensive (CS:GO) is a multiplayer first-person shooter game where players work in teams of 5v5 to fight over objectives, and ultimately try to win a match of up to 30 rounds. As with many modern games, gameplay is focused around a central \"competitive matchmaking\" mode, where two teams of five with similar skill are pitted against each other, and the outcome of each match is used to update the players' skill ratings.\nWith millions of competitive matches played each day [1], having accurate skill ratings for each player and team is fundamental to producing fair matchups and allowing players to progress as their skill improves. The ever-rising popularity of matchmaking in multiplayer video games has shown that this format makes for an engaging experience.\nIn CS:GO, the developer Valve uses an unpublished variant of the Glicko2 rating system [2, 3]; the perceived inaccuracies of the skill rating system are a major point of contention in the community. The term \"elo hell\" is commonly used by players who feel that their skill rating doesn't reflect their real ability [4], and many players opt to use alternative matchmaking services such as FACEIT (which uses the Elo system [5, 6]) to form ratings instead. While the benefits of accurate skill ratings are clear, little comparative work has been done to understand the performance of each system in CS:GO.\nIn this work, we take a look at several systems available for skill ratings (including Elo and Glicko2), and apply it to a large dataset of professional CS:GO matches. As well as comparing the skill rating systems themselves, we explore the effect of different matchmaking algorithms on the accuracy of skill ratings. We achieve this through a surrogate modelling environment where the matchmaking system chooses which match to be played next (and therefore to be used for updating skill ratings before the next match), while imputing real data for the"}, {"title": "II. RELATED WORK", "content": "The history of skill rating systems for games is a rich one. The Elo system, first introduced in the 1950s, is widely used in many sports and remains the defining system the chess world accepts. The system assumes that each player has a fixed skill rating, and the probability of each player winning the match is a function of the difference in rating between the two players [5]. It was the first rating system developed that modelled the players' skill level probabilistically.\nIn 1995, Mark Glickman created the Glicko system, specifically to improve upon issues he saw with the Elo system [2, 7]. Glicko adds a confidence parameter RD, that is a measure of the system's confidence in its estimate of skill. This allows changes in skill rating to also be dependent on this confidence parameter, thus introducing an idea of \"information gained\" by a particular match being played. Glicko2 adds an additional parameter \u03c3, that measures the player's fluctuation in skill.\nHerbrich, et al. from Microsoft Research introduced TrueSkill in 2007, based on Bayesian inference [8]. TrueSkill address two concerns of online team based matchmaking:\n1) Match outcomes are team-based, but a skill rating for individuals is desired.\n2) Some games have multiple 'teams' playing, and the match outcomes are not binary win/loss, (e.g. \"free-for-all (FFA) deathmatch\" matches)\nTrueSkill uses Gaussian Processes (GPs) via a Factor Graph and Message Passing approach to model the players' skill and perform matchmaking. Microsoft analysed the performance of TrueSkill on a dataset collected in the beta testing of Halo 2, and found that TrueSkill substantially outperforms Elo [8]. Microsoft published a significant upgrade, TrueSkill 2, in 2018 [9]. Trueskill 2 is designed to address deficiencies in TrueSkill by analysing game-specific metrics such as player experience, the player's individual performance in the match, tendency to quit, and skill in other game modes to assist with matchmaking.\nSome evaluation of prediction performance has been previously studied. Dehpanah et al. analysed Elo, Glicko and TrueSkill on a set of 100,000 matches from PlayerUnknown's Battlegrounds [10]. They found that\nincorporation of new players into the model proved tricky, and deteriorated the performance of these rating systems.\nMakarov et al. analysed the performance of rating systems on two Valve games, focusing on Dota 2 as well as CS:GO. They analysed TrueSkill on CS:GO games and observed a 62% accuracy [11]. Both approaches analysed a static dataset of past matches, in contrast with our approach."}, {"title": "III. METHODS", "content": "For the experiments in this paper, we implemented a extensible Python library called skillbench, which allowed us to implement each component for comparison following a consistent interface. Skill rating systems such as TrueSkill are implemented as Emulators, with matchmaking algorithms implemented as Acquisition Functions. The overall training and evaluation is governed by a Simulator, which simulates the playing of games (as chosen by the acquisition function) based on the match dataset, and trains and evaluates the emulator.\nFigure 1 shows the overall architecture of the library. In the following sections, we talk about each component in much more detail."}, {"title": "B. Simulator", "content": "In lieu of a game environment populated by real players, we simulate the process of chosen teams competing against one another by selecting records from a dataset of historical matches (Section III-E). The key constraint to this approach is that the model's choice of matchups is limited to those present in the dataset - these limitations are discussed further in Section V-A.\nIn our implementation, we characterise the simulator as being responsible for the generic training process of emulators. That is to say, it is within the Simulator class that a matchup is chosen according to its acquisition score (computed by an Acquisition Function); the result of the match is then simulated (by popping a random result from the set of real matches between that pair of teams); before finally the Emulator is informed of the result and it is fit into its internal model.\nAt each iteration, the Simulator chooses 25 matches from the remaining training pool at random, and the emulator is fed whichever match has the highest score according to an Acquisition Function. After a given number of training matches, we evaluate the Emulator's predictions according to its accuracy against historical results. For external validity, we can split our dataset and make use of separate \"training\" and \"testing\u201d Simulators.\nWe summarise this Simulator-led training process below."}, {"title": "C. Emulators", "content": "We model each skill rating system as an Emulator, which can \"emulate\" (predict) the outcome of a match between two teams as a probability. Each emulator can also be updated based on the outcome of previous matches.\nWe implement five emulators within our framework, which are briefly described below.\n1) WinRate: WinRate is a baseline na\u00efve emulator which we introduce in this work, as a point of comparison. We keep track of each team's proportion of matches won so far, w(T), and then calculate a 'probability' of A winning a match against B:\n$E[A|B] = \\frac{1 + w(A) \u2013 w(B)}{2}$\nOne of the shortcomings of this approach is that a team's win rate is biased by the skill of the teams they have played against; this is addressed by the other emulators.\n2) Elo: Our Elo implementation is defined as follows:\nParameters: k, representing the 'k-factor' and \u00b5, the starting rating given to new players.\n$E[A|B] = \\frac{1}{1+10^{\\frac{R_B-R_A}{400}}}$ (1)"}, {"title": "", "content": "Equation 1 represents the expected score (probability of winning) of team A in a match against team B, where RT is the rating of team T.\nTo update the skill ratings, we multiply the difference between the actual outcome and the predicted outcome by k, and adjust the rating by this value.\n3) Glicko2: The Glicko2 implementation has the following parameters:\n\u2022 \u03bc = The default rating of player.\n\u2022 $ = The rating deviation (RD) of a player.\n\u2022 \u03c3 = The player's skill volatility.\n\u2022 $T$ = The system constant, which dictates the volatility over time.\nThe exact specifics of how to calculate the rating algorithm are detailed in [2]; we provide an overview here.\n1) Compute the estimated variance (v) of the team's rating based solely on the game outcome.\n2) Compute the estimated improvement (\u25b3) in rating by comparing the current rating to the rating based on the game outcome.\n3) Iteratively compute the new volatility.\n$v = [g(\\phi')^2 E(\\mu,\\mu', \\phi') \\cdot (1 \u2013 E(\\mu, \\mu', \\phi'))]^{-1}$ (2)\n$\\triangle = \\frac{vg(\\phi')(s \u2013 E(\\mu, \\mu', \\phi'))}{\\phi}$ (3)\nwhere \u03bc' \u03c6' represent the rating and RD of the opponent, and s is the actual outcome.\n4) TrueSkill: The TrueSkill implementation has the following parameters [8, 12]:\n\u2022 \u03bc = The rating of player.\n\u2022 \u03c3 = The player's skill volatility.\n\u2022 \u03b2 = The skill class width, if a player has B rating higher than another, then the player has an 80% chance to win.\n\u2022 T = The additive dynamics factor, the square of which is added to the player's variance on each skill update. This ensures that \u03c3 does not converge to 0, and thus a player's skill rating never becomes static.\nTrueSkill is a Gaussian Process over teams (or players), that is a joint distribution of infinitely many Gaussians. The TrueSkill algorithm attempts to minimise the Kullback-Leibler (K-L) Divergence between a 3D truncated Gaussian (created by the performance of the two teams) and the approximation of it.\nA deep dive in the maths behind TrueSkill is given in [12].\n5) TrueSkillPlayers: Similar to TrueSkill, but rather than tracking a per-team skill rating, each player's skill rating is tracked individually. We take advantage of TrueSkill's unique native support for any type of match, to update all 10 skill ratings for each player in one go."}, {"title": "D. Acquisition Functions (AFs)", "content": "One of the goals of this analysis is to determine how to select teams for matches that provide the most information to the skill rating system - i.e. how can we acquire all teams' skill ratings in as few games as possible? To do this, we use the notion of an acquisition function (AF) from surrogate modelling: a function which determines which data point to sample next to update the model. In this case, the data points are match outcomes between some pair of teams, and the model is the skill rating emulator. This can be thought of as an analogue to the matchmaking system which is used in practice for selecting matchups.\nAn acquisition function provides a heuristic quantification for how valuable any given match may be in the training of an emulator, according to:\n\u2022 the emulator's internal state;\n\u2022 which teams are involved in the match.\nMore formally, we can define an acquisition function as:\nAF: Emulator \u00d7 Team \u00d7 Team \u2192 R.\nWe will now discuss several approaches to designing an acquisition function.\n1) Expected Improvement: One particularly prevalent form of acquisition is Expected Improvement (EI), which adopts a greedy strategy to locate the global minimum in a search space. It achieves this by selecting the point that has the greatest probability of being lower than our current best estimate, as predicted by a surrogate model such as a Gaussian Process (GP):\n$E[u(x)|x, F] = E[max(0, S_F(x^*) \u2013 S_F(x))|x, F]$, (4)\nwhere F is our search space (F : X \u2192 Y), sf is our surrogate model approximating F, and x is our current best estimate for F's global minimum.\nTo translate this approach to the domain of CS:GO skill estimation, one could make the following correspondence:"}, {"title": "", "content": "However, there is an issue with this setup. The basic formulation of EI (equation 4) models the function F and search domain X as static, when in reality they should depend on our emulator's internal state. In other words, over the course of training, we don't want to find the most useful single matchup, but rather the most useful"}, {"title": "", "content": "This alters the premise of the Expected Improvement environment - we now have a dynamic search space, which depends on the history of matches Xt-1 that have already been observed at that timestep:\nBeing more explicit with regards to timesteps, we can now reformulate the Expected Improvement equation as:\n$E[u(x)|x, F] = E[max(0, S_{F_{xt-1}}(x_{t-1}) - S_{F_{x_t}}(x))|x, F]$. (5)\n2) Cheater's AF: With this EI environment, we can immediately notice a hypothetically optimal (greedy) choice of acquisition function, by making our choice of SFxt as inherently close to Fxt as possible. In practical terms, this means \"cheating\" and computing SF over unseen training data.\nThis way, the AF would be selecting its next matchup by training Xx emulators, and taking whichever achieves the lowest error score on the remainder of our training data:\nAFcheat (E, T1, T2) = -avg(err(E', x) for x in training data), (6)\nwhere E' is a copy of emulator E after being trained on the outcome of a match between T\u2081 and T2.\n3) Gaussian Process: With our EI assumptions, we can note two factors inhibiting the usefulness of a simple GP for surrogate model sf:\n\u2022 The dynamic between X and Y changes as t increments. A gaussian process could model this as variance in X, but doing so would fail to capture what should be a predictable dynamic (e.g. F(x2,x2)(x2) will almost certainly be higher than F(x1,x1)(x2)).\n\u2022 Our model cannot directly sample Y at each iteration, as computing Y depends on unseen future matches. Instead, the model can only sample the outcome of one match per iteration, and must approximate Y based on:\n- The information content of xt.\n- An assumption about how our emulator will make use of this information content.\n- An assumption about the distribution of matchups on which the emulator would be evaluated.\nWe can address both of the above by focusing our acquisition function's design goal into the following:\nHow can we compute the information content of a particular matchup, in the context of whichever matches the emulator has seen thus far?\n4) Information-Theoretic AF: Likeliest Draw: In designing an information-theoretic acquisition function, an intuitive option would be to take the matchups that our emulator believes are most likely to be draws, in order to settle uncertainties of skill between pairs of players. We can justify this as being the entropy of an individual matchup.\nLet us suppose that R is a discrete random variable (DRV) representing the emulator's predicted results for a particular matchup, having two outcomes: WT1 and WT2. Furthermore, suppose M is a DRV representing all matchups seen by the emulator. In this regard, H(RM = m) represents the entropy of the two possible results for a particular matchup according to our emulator.\nWe can derive a formula for the entropy in terms of the emulator's predictions p(wr1|m) and p(wr2|m):\n$\\theta = H(R|M = m) = -p(wr1|m)log(p(wr1|m))$\n$-p(wT2|m)log(p(wT2|m)).$\nTaking p(wT2|m) = 1 - p(wr1|m):\n$\\theta = H(R|M = m) = -p(wr1|m)log(p(wr1|m))$\n$-(1 \u2013 p(wr1|m))log(1 \u2013 p(wr1|m)).$\nWe can then show that the entropy is maximized when p(\u0448\u04421 m) = 0.5 by taking the derivative:\n$\\frac{d\\theta}{dp} = log(1 - p(\\omega_{T1}|m)) \u2013 log(p(\\omega_{T1}|m))$\n$0 = log(1 - p(\\omega_{T1}|m)) \u2013 log(p(\\omega_{T1}|m)$\n$1 = \\frac{1-p(\\omega_{T1}|m)}{p(\\omega_{T1}|m)}$\n$0.5 = p(\\omega_{T1}|m).$\nThus, we define acquisition function:\n$AF_{draw}(E, T1, T2) = -p(\\omega_{T1}|m)log(p(\\omega_{T1}|m))$\n$-(1 \u2013 p(\\omega_{T1}|m))log(1 \u2013 p(\\omega_{T1}|m)).$ (7)\n5) Information-Theoretic AF: Cross-Entropy: The likeliest-draw approach has a key limitation: it neglects to consider the number of prior encounters that an emulator has had with a specific matchup.\nThe failure here is to assume that the entropy within a match is reflective of the entropy within our emulator's understanding of the world. In actual fact, some teams will naturally have a new-draw winrate against one another, and sampling these matchups many times over won't be informative to the emulator as time goes on.\nTo rectify this, we propose reformulating the entropy calculation to instead compute the surprisal of each result in the context of all results that the emulator has previously seen. The assumption here is that more unexpected results will be more informative to the emulator."}, {"title": "", "content": "This is equivalent to the Cross-Entropy (CE) between the emulator's predicted distribution of results for a particular matchup (RM = m), and its predicted distribution of results across all known matchups (RM), derived as follows:\nCE((R|M = m), (R|M))\n= -E(R|M=m) [log(R|M)]\n= -$ -p(wr1|m) *log(p(wr1)) \u2013 p(wr2|m) *log(p(wt2)).$\nBy Bayes' theorem, we take p(\u0448\u0442\u2081) = p(\u0448\u0442\u2081|m)p(m):\nCE((R|M = m), (R|M))\n$\\- p(\\omega_{T1}|m) log(p(\\omega_{T1}|m)p(m))$\n$\\- p(\\omega_{T2}|m)log(p(\\omega_{T2}|m)p(m))$\n$\\- p(\\omega_{T1}|m) log(p(\\omega_{T1}|m)p(m))$\n$-(1 \u2013 p(\\omega_{T1}|m)) log((1 \u2013 p(\\omega_{T1}|m))p(m)).$\nHere, p(m) represents the probability of matchup m occurring according to the emulator. By assuming matchups are independently distributed between teams, we can find $p(m) = \\frac{c(T_1)}{\\sum_{T \\epsilon Team} c(T)} \\frac{c(T_2)}{\\sum_{T \\epsilon Team} c(T)}$, where c(T\u2081) is the number of times that the emulator has observed team T\u2081. This gives us a computable expression:\nAFCE(E, T1, T2) = CE((R|M = m), (R|M))\n$-(p(\\omega_{T1}|m) log p(\\omega_{T1}|m) \\frac{c(T_1)}{\\sum_{T \\epsilon Team} c(T)} \\frac{c(T_2)}{\\sum_{T \\epsilon Team} c(T)}$\n$-\\ (1 \u2013 p(\\omega_{T1}|m)) log ((1-p(\\omega_{T1}|m)) \\frac{c(T_1)}{\\sum_{T \\epsilon Team} c(T)} \\frac{c(T_2)}{\\sum_{T \\epsilon Team} c(T)})$ (8)\n6) Weighted AF: One could think of AFCE as a function of two factors: the estimated likelihood of a draw between two teams, and the number of times our emulator has seen those teams before. However, it is not clear how to parameterise these factors within AFCE.\nInstead, we propose a weighted acquisition function which models the two factors (draw probability, no. of times teams seen) explicitly:\n$AF_{weighted} (E, T1,T2) = \\alpha \\cdot draw\\_factor + \\beta \\cdot seen\\_factor$\n$=\\alpha \\cdot (1 - |p(\\omega_{T1}|m) \u2013 p(\\omega_{T2}|m)|$\n$+ \\beta  (\\frac{1}{(\\frac{c(T1)}{\\sum c(T)} + 1)} + \\frac{1}{(\\frac{c(T2)}{\\sum c(T)}+1)})$ (9)\n7) Other Acquisition Functions: For the sake of comparison and experimentation, we implemented several other simple acquisition functions:\n\u2022 LeastSeen: simply sum the number of times an emulator has seen each team (or, in the case of TSPlayers, players) and perform a negative logarithm on each result:\n$AF_{Unseen} = \\sum_{Tem} -log(c(T)).$ (10)\nThis is another information-theoretic approach that equates observations with bits of information, though neglects to take into account result predictions.\n\u2022 MostSeen: take the negative of LeastSeen.\nWe expect this AF to perform poorly, by the logic that more expected matchups will tend to have lesser information content for the Emulator.\n\u2022 LikeliestWin: take the negative of LikeliestDraw (7).\nWe expect this AF to perform poorly, by the logic that more \"obvious\" outcomes will also convey lesser information.\n\u2022 TSQuality: take TrueSkill's built-in quality() metric. The TrueSkill paper describes this as \u201cthe draw probability relative to the highest possible draw probability in the limit \u0454 \u2192 0\", which in practice can be thought of as the expectation of draw probability when treating team skill as a Gaussian Process. In other words, a trade-off between an Emulator's confidence in player skill and its confidence in match outcome."}, {"title": "E. Data", "content": "For our experiments, we scraped a large database of professional and semi-professional CS:GO matches from hltv.org. For the analysis, we used all matches with a greater than or equal to \"1 star\" team rating between 2017-2022, for a total of 9,929 matches.\nAs the modelling of skill ratings over time is out of scope of this work, we split the dataset into a training and evaluation set using a random 50/50 split. Matches used for fitting by the AFs are taken from the training set, and evaluation is always done by analysing emulator performance on the entire validation set."}, {"title": "F. Trueskill Sensitivity Analysis", "content": "Algorithms such as TrueSkill naturally have several parameters that control their behaviour and thus performance (in the case of Trueskill, there are four primary parameters as described above: \u03bc,\u03c3, \u03b2, \u03c4). While the defaults are generally considered to \"lead to reasonable dynamics\" [12], there is no current literature exploring the selection of these parameters.\nHence, we here perform a sensitivity analysis against our dataset to determine which parameters have the largest effect on performance, and whether the defaults provide reasonable results on our dataset. Since all parameters are set relative to \u03bc, we perform a logarithmic grid search across each pair of parameters in \u03c3,\u03b2,\u03c4, \u00b11 order of magnitude from the default.\nSince all other parameters are defined in terms of \u03bc, we leave the default \u03bc = 25, and vary two parameters at a time out of \u03c3, \u03b2, 7 in the form of a logarithmic grid search. We then use the GPy Python library [13], to fit a Gaussian Process with an RBF kernel to the results:\n$k(x,x') = exp(\\frac{||x - x'||^2}{0.5})$\nwith a mean performance prior of 60%. Using a GP allows us to smooth the noisy results without needing to repeat every run many times to get an average."}, {"title": "IV. RESULTS & ANALYSIS", "content": "Table I summarises the results across each combination of emulator and acquisition function. As expected, we see that as more matches are selected to fit the emulator, the overall performance of that emulator increases.\nThe Weighted acquisition function (parameterised with \u03b1 = 1, \u03b2 = 1) introduced in this work produces the best overall performance for all emulators based on existing rating systems, with the biggest gains seen after only 500 matches of training. The Random AF provides a baseline to which we can compare the other acquisition functions; we see that the MostSeen and LikeliestWin AFs provide significantly worse performance for the same number of training matches. This is inline with theory, as we are essentially feeding the emulator the results of matches it is already sure about. However, this highlights the importance of choosing a good acquisition function when minimal data is available.\nThe LikeliestDraw and CrossEntropy AFs select matchups which the emulator is unsure about, and this yields an improvement of 1-1.5% over random matchups from the dataset. However, we see that while the Weighted AF consistently provides the best performance for Elo, The more basic WinRate emulator favours the CrossEntropy AF.\nThe TSQuality AF, which uses TrueSkill's own internal match quality for selection, underperforms in comparison to other AFs, even when compared to random sampling.\nWe can also analyse the comparative behaviour of different Emulators. We find that among the team-based skill rating systems, Glicko2 provides better performance across the board, with greater gains with fewer training matches. This is surprising, as TrueSkill was introduced to address the shortcomings of Glicko2.\nHowever, one of the main benefits that TrueSkill brings is the ability to generalise beyond 1v1 matchups, and the benefit of this is seen when we use 5v5 matchups in TrueSkill with per-player skill ratings. The TSPlayers emulator beats all other approaches by around 1%, giving us the best achieved average accuracy of 64.1%.\nOverall, we see that the choice of acquisition function and emulator can be largely decoupled: player-based TrueSkill excels as an emulator throughout, and the Weighted acquisition function provides the best results across emulators."}, {"title": "A. Trueskill Sensitivity Analysis", "content": "Figure 2 shows us the performance of each of our TrueSkill-emulators, as the algorithm parameters are varied. We can draw a number of interesting conclusions from this:\n1) \u03b2 and $T$ seem like the most important parameters, with $\\sigma$ having a much smaller effect on performance. We confirm that the ratio between $\\beta$ and $\\sigma$ is very important ($\\beta = \\sigma/2$ is the default) - performance remains similar if the ratio is maintained. We find that the optimal ratio is $\\beta = \\sigma/0.5$ and $\\beta = \\sigma/1.6$ for the two emulators.\n2) In TrueSkillEmulator (with a single skill rating for a given team), the combination of a high $\\sigma$ and low \u03b2 dramatically reduces the performance of the algorithm. This effect is also seen (less drastically) in TrueSkillPlayersEmulator, which maintains a separate skill rating for each of the team's 5 players.\n3) The TrueSkillPlayersEmulator is much more robust to changes in parameters (a 1.3% range versus 7.5%). This suggests that the TrueSkill algorithm is more stable with 5v5 matches rather than 1v1 matches.\n4) That being said, we see that the default values given achieve close to optimal performance in both cases!"}, {"title": "B. Accuracy over training", "content": "Figure 3 shows the performance of emulators using the Random and Weighted acquisition function, and the emulators are trained until they run out of matches.\nWe find that the engineered AFs reach a plateau of performance much faster than picking matches at random: matches are picked that try to maximise the speed at which the emulators can learn. In the case of the Weighted AF, we actually see overall accuracy actually decreases towards the end of training. This is because a good acquisition function selects \u201chigh-quality\" matches (for some notion of high quality), which inevitably means that the 'low-quality' matches are all fed to the emulator at the end; potentially an unrepresentative sample that skews ratings. We see that this behaviour is much more prominent in the TrueSkillPlayers emulator.\nNote that by intentionally training on an AF-selected subset of the dataset, we avoid this behaviour in Table I, as we report results there only after training on a subset of the dataset. This effect is a limitation of our training method, which limits matchups to a subset of actually\nobserved matchups (rather than letting the model choose any possible matchup), as opposed to a fundamental property of the emulators."}, {"title": "V. DISCUSSION", "content": "While the goal of this work was to accurately model each method's performance within an environment where it can choose which match is played next, there are several limitations to our work:\n\u2022 Not every matchup is selectable by the emulator during training - it can only choose matchups that actually occurred in the dataset.\n\u2022 Matches are not presented to the algorithm in time-order, meaning that the ability of each emulator to model time-varying skill was not tested. This was done to avoid dramatically limiting match choice.\n\u2022 We test only on a single dataset, based on professional CS:GO matches. Therefore, it is unclear how well the results transfer to amateur matches, or other video games which rely on similar matchmaking systems.\n\u2022 We have only evaluated Emulators in their accuracy at predicting non-draw outcomes. It may be insightful to compute the log-loss of each emulator to reward stronger beliefs & to factor in drawed games.\n\u2022 Acquisition Functions are evaluated on their ability to produce accurate skill ratings (as measured by match prediction accuracy), but there are many other metrics. For example, the TrueSkill Quality metric [8] (which did not perform well in this work) is designed to try to maximise the probability of a draw, under the heuristic that evenly matched games are the most fun; the best match for players may not be the match that allows the skill rating system to learn the most."}, {"title": "B. Future Work", "content": "Perhaps the primary way in which our work could be extended is through the use of larger datasets (such as amateur matchmaking data if such data were obtainable), which would allow for greatly reduced aleatoric uncertainty in results. Another obvious extension would be to analyse several different games and determine whether the choice of system is game-specific or if e.g. TrueSkill is always superior to Elo.\nOther avenues include:\n\u2022 Parameter-tuned acquisition functions. To further improve the performance of our Weighted AF (9), we could consider tuning its parameters per emulator, such as through Bayesian optimisation. Furthermore, we could explore the use of highly parameterized AFs that make deeper assumptions about the emulator and data, such as modeling the impact of network effects on different teams; or even the relationship between factors like draw probability and team count as a Gaussian process to achieve closer to optimal results.\n\u2022 Exploring non-tournament matchmaking settings. To provide a more realistic matchmaking experience, we could simulate a setting whereby the matchmaking function must balance the usefulness of the emulator, fairness of the match, and queue waiting times, rather than assuming that all teams are always available for matches. This approach would offer a more dynamic and practical solution.\n\u2022 Next-generation emulators. Microsoft recently published TrueSkill2 [9], which claims to improve match prediction accuracy on Halo 5 from 52% to 68%. This was out of scope in this paper as no open-source implementations currently exist. Alternatives that could be evaluated include OpenSkill. [14]."}, {"title": "VI. CONCLUSION", "content": "In this work, we performed an thorough analysis on the performance of different skill rating systems and matchmaking algorithms when applied to a real-world dataset of professional CS:GO matches. The core novelty of our work is the use of a surrogate modelling architecture to evaluate a skill rating system's effect on matchmaking and its recursive effect on future skill ratings, while retaining the use of real-world data. Our architecture is released as a Python library to allow for future extension and evaluation.\nWe draw several conclusions from our analysis: that the default parameters of TrueSkill yield close to optimal results, that a selection of emulator and AF can be made largely independently, that a 5v5 TrueSkill and Weighted AF provide the best results, and that optimal performance can be reached with surprisingly few matches, with only marginal gains between skill rating systems. Larger datasets and an evaluation of generalisation across games could allow for more robust future analysis."}]}