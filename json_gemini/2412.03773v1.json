{"title": "MODULAR ADDITION WITHOUT BLACK-BOXES: COMPRESSING EXPLANATIONS OF MLPS THAT COMPUTE NUMERICAL INTEGRATION", "authors": ["Chun Hei Yip", "Rajashree Agrawal", "Lawrence Chan", "Jason Gross"], "abstract": "The goal of mechanistic interpretability is discovering simpler, low-rank algo-\nrithms implemented by models. While we can compress activations into fea-\ntures, compressing nonlinear feature-maps-like MLP layers is an open problem.\nIn this work, we present the first case study in rigorously compressing nonlin-\near feature-maps, which are the leading asymptotic bottleneck to compressing\nsmall transformer models. We work in the classic setting of the modular ad-\ndition models (Nanda et al., 2023), and target a non-vacuous bound on the be-\nhaviour of the ReLU MLP in time linear in the parameter-count of the circuit.\nTo study the ReLU MLP analytically, we use the infinite-width lens, which turns\npost-activation matrix multiplications into approximate integrals. We discover\na novel interpretation of the MLP layer in one-layer transformers implementing\nthe \"pizza\" algorithm (Zhong et al., 2023): the MLP can be understood as evalu-\nating a quadrature scheme, where each neuron computes the area of a rectangle\nunder the curve of a trigonometric integral identity. Our code is available at\nhttps://tinyurl.com/mod-add-integration.", "sections": [{"title": "1 INTRODUCTION", "content": "Neural networks' ability to generalize suggests they implement simpler, low-rank algorithms despite\nperforming high-dimensional computations (Olah et al., 2020). The field of mechanistic interpretabil-\nity has made tremendous progress in finding low-rank approximations of activations-for example,\ndiscovering interpretable features (Bricken et al., 2023; Cunningham et al., 2023). However, finding\nlow-rank approximations of feature-maps-particularly MLP layers\u2014is still an open problem (Elhage\net al., 2022).\nFinding low-rank approximations of nonlinear feature-maps becomes particularly important in light of\nrecent work that found evidence that sparse linear features do not fully capture the structure of frontier\nmodels (Marks et al., 2024; Engels et al., 2024). Interpretations that lack analyses of feature-maps\nmay not be usable for ambitious applications of mechanistic interpretability, like anomaly detection\nand worst-case guarantees. The expressivity of MLPs constitutes a large attack surface for perturbing\nthe model, so not compressing feature-maps leaves a lot of free parameters in the interpretation.\nThese free parameters diminish our ability to detect anomalous behaviour, or make strong guarantees.\nTo illustrate the difficulty of compressing MLPs, consider the following toy model comparing the\neffective parameter count of deep nonlinear networks and deep linear networks: adding or multiplying\nk matrices of shape m \u00d7 m. For linear operations, we need only $m^2$ parameters to completely describe\nthe input-output behaviour, regardless of the depth of network. However, introducing nonlinearities\nlike ReLU between these operations increases the effective parameter count to $km^2$, with the\ncomplexity growing exponentially with depth. While deep linear networks can be compressed to\nshallow networks of equivalent width without loss of expressivity, nonlinear networks resist such\ncompression.\nEven in the classic toy setting of modular addition models, the MLP layer is treated as a black box.\nNanda et al. (2023) find sparse features to describe the input and output to the MLP layer, however,\nthey do not tell us how the MLP layer processes the input features to generate the output features."}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 MECHANISTIC INTERPRETABILITY OF MODULAR ADDITION MODELS", "content": "Models trained on the modular addition task have become a classic testbed in the mechanistic\ninterpretability literature. Originally, Nanda et al. (2023) studied a one-layer transformer model. They\nfound low-rank features to describe all components of the model, and analysed the feature-map of the\nfinal linear layer. While they generated a human-intuitive algorithm of how the model works, they\ndid not explain how the MLP layers compute logits that fit with the form required by this algorithm.\nDespite the tremendous progress in analysing the non-MLP layers of the model, the ReLU MLP is\nstill treated as a black-box.\nZhong et al. (2023) extended the analysis to a family of architectures parameterized by attention\nrate. The architecture from Nanda et al. (2023) corresponds to attention rate 0, while attention rate"}, {"title": "2.2 EXPERIMENTAL SETUP", "content": "We study models which implement the 'pizza' algorithm from Zhong et al. (2023): a one-layer\nReLU transformer with four heads and constant attention $\\frac{1}{2}$ for all tokens, trained to compute\nM : (a, b, '=') \u2192 $(a + b)$ mod p. As in Zhong et al. (2023), we take p = 59. The model takes\ninput (a, b, '=') encoded as one-hot vectors, and we read off the logits logit(a, b, c) for all possible\nvalues (mod p) above the final sequence position '=', with the largest logit representing its predicted\nanswer.\nSince the attention is constant, the logits of the model given input a, b are calculated as:\n$x^{(0)} = W_e i_a + p_i$\nEmbedding\n$x^{(1)} = x^{(0)} + \\frac{1}{4} \\sum_{j=1}^{4} W_i W_j (x^{(0)} + x^{(0)})$\nPost attention residual stream\n$N = ReLU(W_{in}x^{(1)})$\nPost ReLU neuron activations\n$x^{(2)} = x^{(1)} + W_{out} N$\n$logits = W_u x^{(2)} = W_u (x^{(1)} + W_{out} ReLU(W_{in}x^{(1)}))$\nAs such, the model architecture considered has constant attention $\\frac{1}{2}$ throughout, so the model\nconsists of linear embeddings, followed by ReLU, followed by a further linear layer.\nAs noted in both Nanda et al. (2023) and Zhong et al. (2023), the contribution from the skip connection\naround the MLP to the logits is small. Combining this with the fact that the attention is uniform\nacross a, b, and using $W_L = W_u W_{out}$ for the neuron-logit map, the logits can approximately be\nwritten as the sum of the contributions of each of the $d_{mlp}$ neurons:\n$logit(a, b, c) \\approx \\sum_{i=1} ^{d_{mlp}} (W_L)_{ci} ReLU(OV(a)_i + OV(b))$"}, {"title": "2.3 THE \"PIZZA\" ALGORITHM", "content": "Zhong et al. (2023) demonstrated that small transformers using constant attention implement modular\narithmetic using the following algorithm, which they call the \u201cpizza\u201d algorithm:\n1. OV(a), OV (b) embed the one-hot encoded tokens a,b as (cos(ka), sin(ka)) and\n(cos(kb), sin(kb)) for a small handful of key frequencies k\n2. Before the MLP layer, the representation of the two tokens are averaged:\n(cos(ka) + cos(ka), sin(ka) + sin(ka))/2 = cos(k(a \u2013 b)/2)(cos(k(a + b)/2), sin(k(a + b)/2))\n3. The network then uses the MLP layer to \u201cdouble the frequencies\u201d:\n$N = |cos(k(a - b)/2)|(cos(k(a + b)), sin(k(a + b)))$"}, {"title": "3 COMPRESSING MLPS", "content": "Post-hoc mechanistic interpretability (Olah et al., 2020; Elhage et al., 2021; Black et al., 2022) can\nbe formalized as finding a compact explanation (Gross et al., 2024) of how the model computes its\noutputs on the entire input distribution. Gross et al. (2024) demonstrated that finding non-trivial\ncompact explanations, i.e. proving a meaningful bound instead of a null bound, necessarily requires\nmore mechanistic information about the model behaviour. Moreover, if some marginal mechanistic\nanalysis does not result in compression, then it does not reduce the free parameters of that component.\nGross et al. (2024) conduct a limited empirical\nstudy of models trained to compute the max-\nimum of k integers, which are attention-only\ntransformers with no MLP layers. Our work can\nbe seen as applying this rigorous formalization\nto a more challenging case study.\nFormally, we consider the computational com-\nplexity needed to check the behaviour of the\nMLP on all inputs. The lower the complexity is,\nthe better we have understood the MLP and the\nbetter we can compress our description of how\nthe MLP computes the function that it does.\nThe naive baseline (as provided by both Nanda\net al. (2023) and Zhong et al. (2023)) is to de-\nscribe the MLP's behaviour by evaluating it on\nevery possible input. For n such inputs and an\nMLP with width $d_{mlp}$ and input/output dimen-\nsion $d_{model}$, this requires checking the result of\n$O(n d_{mlp}d_{model})$ operations. This corresponds\nto a null interpretation providing zero under-\nstanding of how the MLP internally functions.\nIdeally, we hope that we can bound the MLP's behaviour by referencing only the model weights,\nwithout evaluating it on all inputs \u2013 in time $O(d_{mlp}d_{model} + n)$, linear in the parameter count of the\nMLP. As this is the information-theoretic limit, we target interpretations that formally bound MLP\nbehaviour in time linear in parameter count."}, {"title": "4 INTERPRETING \u201cPIZZA\u201d MLPS AS PERFORMING NUMERICAL INTEGRATION", "content": "Following Nanda et al. (2023), we focus on a particular (\u201cmainline\") model in the main body of the\nwork Appendix A. We confirm that our results generalize to another 150 transformers in Appendix C.\nWe identify new structure in the model by using the amplitude-phase form of the Fourier series, and\nfind an integral approximation of the MLP showing doubling of the frequency of the preactivations.\""}, {"title": "4.1 TRANSLATING INTO THE AMPLITUDE-PHASE FOURIER FORM", "content": "We first rewrite the algorithm in subsection 2.3 as an equation of the logits. Let F be the p$\\times$ p\ndiscrete Fourier transform matrix, then $W_1 = F W_L$, OV(a) = OVF_a, and OV(b) = OVF_b for\nsome matrices of Fourier coefficients $W_L$, OV. The logits can thus be written:\n$logit (a, b, c) \\approx F W_L ReLU(OV(F_a + F_b))$\nWe translate the algorithm into amplitude-phase Fourier form, because phase terms naturally capture\ncyclic relationships. We rewrite the matrices of Fourier coefficients $W_L$ and OV in polar coordinates.\nTaking $s = k(a + b)/2$ and $s' = k(a \u2013 b)/2$, the algorithm becomes\n1. Fa, Fr embed the one-hot encoded tokens a, b as (cos(ka), sin(ka)) and (cos(kb), sin(kb))\nfor a small handful of key frequencies k\n2. Before the MLP layer, the representation of the two tokens are averaged:\n(cos(ka) + cos(ka), sin(ka) + sin(ka))/2 = cos s' (cos s, sin s)\n3. The network then uses the MLP layer to \u201cdouble the frequencies\". For each output frequency\nk', we write\n$\\sum (W_L)_{k'i} ReLU (OV(F_a + F_b)) \\approx \\sum |r_i r_i cos s'|(cos \\psi_{k'i}, sin \\psi_{k'i}) ReLU (cos \\phi_i cos s + sin \\phi_i sin s) \\approx \\sum |r_i r_i cos s'|(cos \\psi_{k'i}, sin \\psi_{k'i}) ReLU (cos(s \u2013 \\phi_i)) \\approx \\sum_k' cos s'(cos(2s), sin(2s))$"}, {"title": "4.2 OBSERVATIONS", "content": "We find that each neuron has a primary fre-\nquency for both input and output. As seen in\nFigure 4, the majority of pre-activations for each\nneuron consist of terms from a single key fre-\nquency, as does the neuron-logit map WL. The\nlargest frequency component for the ReLU pre-\nactivation is almost always equal to the largest\nfrequency component for the corresponding row\nof WL. This allows us to divide the neurons into\nclusters Ik, each of which correspond to a single\nfrequency k.\nFurthermore, the output phase is double the\ninput phase. In Figure 3, we plot the input"}, {"title": "4.3 INTEGRAL APPROXIMATION OF THE MLP", "content": "Treating the MLP layer as approximately performing numerical integration allows us to make sense\nof the previous observations. Recall that we can write the logits of the model as\n$logit (a, b, c) \\approx \\sum_{i=1}^{d_{mlp}}(W_L)_{ci} ReLU (OV(a) + OV(b)) \\approx  \\sum_{k \\in k} \\sum_{i \\in Ik} cos (k(c + 2\\phi_i)) ReLU (cos (\\frac{k}{2}(a - b)) cos (\\frac{k}{2}(a + b + \\phi_i)))$\nFor each frequency k, we can write the contributions to the logits from neurons of frequency k as :\n$logit^{(k)}(a, b, c) = |cos (\\frac{k}{2}(a - b))| \\sum_{i \\in I_k}  cos (k(c + 2\\phi_i)) ReLU (\\sigma_k cos (\\frac{k}{2}(a + b + \\phi_i)))$\nwhere $\\sigma_k 1$ if $|cos (\\frac{k}{2}(a - b))| > 0$ and $-1$ otherwise.\nIgnoring the $|cos(k(a \u2013 b)/2)|$ scaling factor (which does not vary per neuron), we claim that the\nnormalized MLP outputs\n$\\sum_{i \\in I_K} w_i ReLU [\\sigma_k cos((\\frac{k}{2}(a + b) + \\phi_i))] cos(kc + 2\\phi_i)$\ncan we well-thought of as approximating the integral (see Appendix E):\n$\\frac{2}{\\pi} \\int_{0}^{\\pi} ReLU[cos((a+b) + \\phi)] cos(kc + 2\\phi) d\\phi = \\frac{2}{\\pi} cos(k(a + b - c)).$\nNote that the above integral is valid for both $\\sigma_k = \u22121,1$.\nThis gives us the desired form for the output logits:\n$logit(a, b, c) = \\sum |cos(k(a - b)/2)| cos(k(a + b - c))$"}, {"title": "5 VALIDATION VIA COMPACT GUARANTEES ON MLP PERFORMANCE", "content": "As evidence for the usefulness of the numerical integration interpretation, we use it to derive non-\nvacuous bounds on the output of the MLP on all inputs in time linear in the parameters."}, {"title": "5.1 COMPUTING NUMERICAL INTEGRATION ERROR", "content": "The validation of our interpretation as an integral then becomes a mathematical question of evaluating\nthe efficiency of the quadrature scheme\n$\\int_{-\\pi}^{\\pi} h_{a,b,c}(\\phi) d\\phi \\approx \\sum w_i h_{a,b,c}(\\phi_i)$\nwhere $h_{a,b,c}(\\phi_i) = ReLU[\\sigma_k cos((\\frac{k}{2}(a + b) + \\phi_i))] cos(kc + 2\\phi_i)$. The absolute error is given by\n$\\epsilon_s :=  \\int_{-\\pi}^{\\pi} h_{a,b,c}(\\phi) d\\phi -  \\sum h_{a,b,c}(\\phi_i)$\nand we can compute relative error by computing (the average value over a, b, c of)\n$\\epsilon_0 := | \\int_{-\\pi}^{\\pi} h_{a,b,c}(\\phi) d\\phi |$\nand then dividing Equation 7 by Equation 8 to give the relative error $\\epsilon_r := \\epsilon_s/\\epsilon_0$.\nFollowing Gross et al. (2024), if we can compute a non-vacuous error bound (i.e. a relative error\nthat is strictly less than 1) in time linear in the number of parameters of the computation, we can be\nassured that our interpretation has some validity. Since there are $d_{mlp}$ neurons and p points to evaluate\nour target is to compute an error bound in time $O(d_{mlp} + p)$. Thus, this part of the computation is\nsublinear in the number of parameters as desired.\nRecall the neuron contributions to the logits from Equation 5:\n$ReLU[\\sigma_k cos((\\frac{k}{2}(a + b) + \\phi))] cos(kc + 2\\phi)$\nWe can split ReLU as $ReLU(x) = (x + |x|)/2$. We shall see below that the $x/2$ part integrates\nto 0. As a result, a relative error bound would not give a meaningful result for this part. However,\nthis part of the network is linear, thus, we can still effectively compress the network behaviour here.\nFor example, we can compute a matrix A $\\in R^{P \\times P}$ such that the logit contribution of this part is\nA[:, a] + A[:, b] for inputs a and b. Thus, in the below section, we can restrict our attention to the\nabsolute value part, |x|/2.\nThis turns $ReLU[\\sigma_k cos((\\frac{k}{2}(a + b) + \\phi))] cos(kc + 2\\phi)$ into\n$|\\frac{1}{2} \\sigma_k cos((\\frac{k}{2}(a + b) + \\phi))| cos(kc + 2\\phi) +\\frac{1}{2} \\sigma_k cos((\\frac{k}{2}(a + b) + \\phi)) cos(kc + 2\\phi)$\nWe sort the phases $\\phi_i$ for each neuron, and turn the weights $w_i$ into widths of the rectangles, and the\nfunction calls h($\\phi_i$) into the heights of the rectangles corresponding to the function evaluated at $\\phi_i$.\nThis gives a picture similar to Figure 2. The absolute error is\n$\\epsilon_h := |\\int_{-\\pi}^{\\pi} h(x) - h(\\phi_i) dx|$\nA crude bound is:\n$\\int_{-\\pi}^{\\pi} |h(x) - h(\\phi_i)|dx$\n$\\le  sup_x |h'(x)| dx \\int |x - \\phi_i| dx$\n$\\le sup_x |h'(x)| \\sum_i \\int |x - \\phi_i| dx$\n$= sup|h'(x)| \\sum \\frac{1}{2} \\frac{((\\phi_i - \\phi_i)^2 - (\\phi_{i-1} - (\\phi_i)^2)| if \\phi_i \\in [\\phi_{i-1}, \\phi_i] }{(\\phi_i - \\phi_i)^2 + ((\\phi_{i-1} - \\phi_i)^2)  otherwise }$\nwhere the rectangle width goes from $\\phi_{i-1}$ to $\\phi_i$ and the function is evaluated at $\\phi_i$.\nFor h = $h_{a+b,c,k}$, we can bound\n$sup |h'(x)| \\le 2$"}, {"title": "5.2 EMPIRICAL VALIDATION", "content": "We now provide empirical validation of our in-\nterpretation.\nThe network is well-approximated as doing\nnumerical integration. We can compute the\nactual error $\\epsilon_f/\\epsilon_0$ empirically, by evaluating\nthe expression over all possible inputs, giving\nnumbers between 0.03 and 0.05, see Table 1.\nThe interpretation gives useful compres-\nsion because integral explanation gives com-\npact non-vacuous bounds. Our relative error\nbounds range from 0.48 to 0.7 (i.e. less than 1),\nsee Table 1.\nThe bounds are far away from actual error\nbecause we don't completely understand nu-\nmerical integration. Intuitively, we'd like to"}, {"title": "6 THE ROLE OF SECONDARY FREQUENCIES", "content": ""}, {"title": "6.1 REGRESSING MODEL LOGITS VERSUS \"CLOCK\" AND \"PIZZA\" LOGITS", "content": "As noted above, Nanda et al. (2023) claim that logits are of the form (\"clock logits\")\n$logit (a, b, c) \\propto cos(k(a + b - c))$\nwhile Zhong et al. (2023) suggests that logits can also be of the form (\"pizza logits\")\n$logit(a, b, c) \\propto |cos(k(a \u2013 b)/2)| cos(k(a + b \u2212 \u0441))$\nInterestingly, if we regress the logits against the factors |cos(k(a \u2013 b)/2)| cos(k(a + b - c)), which\ngives an $R^2$ of 0.86, while if we regress them against just cos(k(a + b \u2212 c)), we obtain an $R^2$ of\n0.98 - substantially higher. So overall, the \u201cclock logits\u201d give a more accurate expression, suggesting\nthat the analysis above is importantly incomplete. Interestingly, if we only consider the contribution\nto the logits from only the absolute value component of ReLU (Appendix I), the $R^2$ values become\n0.99 and 0.85 respectively.\nWhat this suggests is that the absolute value component of ReLU indeed carries out the \"pizza\"\nalgorithm and produces those logits. As we will demonstrate below, the discrepancy for the overall\nlogits is due to the effects of the substantially smaller non-primary frequencies. In particular, it is\nexplained by the action of the \u201csecondary frequency\" \u2013 the second largest Fourier component.\""}, {"title": "6.2 USING SECONDARY FREQUENCIES TO BETTER APPROXIMATE CLOCK LOGITS", "content": "For each of the neurons, the largest secondary frequency is almost always twice the primary frequency.\nFor example, for neurons of frequency 12, the largest secondary frequency is 24, while for neurons of\nfrequency 22, the largest secondary frequency is 15 (= 59 - 22 2, note that cosine is symmetric\nabout 0).\nNote that the input phase shift of the secondary frequency is approximately twice the input phase\nshift of the primary frequency plus \u03c0 (Figure 7).\nThe contribution of the doubled secondary frequency to the logits can thus be written as (compare to\nEquation 5, note we lose the factor in the pre-ReLU expression because the secondary frequency is"}, {"title": "7 DISCUSSION", "content": "We provide a first case study in rigorously compressing nonlinear feature-maps. We demonstrate\nthat interpreting feature-maps reveals additional insight about the model mechanism", "were": "splitting the input into orthogonal output-\nrelevant and output-irrelevant directions; decomposing the pre-activations as a product of functions\nof these axes; reindexing the neurons by the output-relevant direction so that the reindexed post-"}]}