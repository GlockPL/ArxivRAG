{"title": "COST-EFFICIENT CONTINUAL LEARNING WITH SUFFICIENT EXEMPLAR MEMORY", "authors": ["Dongkyu Cho", "Taesup Moon", "Rumi Chunara", "Kyunghyun Cho", "Sungmin Cha"], "abstract": "Continual learning (CL) research typically assumes highly constrained exemplar\nmemory resources. However, in many real-world scenarios especially in the\nera of large foundation models-memory is abundant, while GPU computational\ncosts are the primary bottleneck. In this work, we investigate CL in a novel setting\nwhere exemplar memory is ample (i.e., sufficient exemplar memory). Unlike prior\nmethods designed for strict exemplar memory constraints, we propose a simple\nyet effective approach that directly operates in the model's weight space through a\ncombination of weight resetting and averaging techniques. Our method achieves\nstate-of-the-art performance while reducing the computational cost to a quarter or\nthird of existing methods. These findings challenge conventional CL assumptions\nand provide a practical baseline for computationally efficient CL applications.", "sections": [{"title": "1 INTRODUCTION", "content": "Continual learning (CL) has attracted significant attention as a paradigm enabling machine learn-\ning models to adapt to sequential tasks while overcoming catastrophic forgetting of previously\nacquired knowledge (Wang et al., 2024). A fundamental challenge in CL is the stability-plasticity\ndilemma (Carpenter & Grossberg, 1987; Mermillod et al., 2013), where a model that maintains\nstability on previous tasks often struggles to integrate new knowledge, and vice versa. Among\nvarious approaches, exemplar-based methods that store representative samples from past tasks have\nbeen widely adopted, particularly in class-incremental learning (class-IL) (Masana et al., 2022).\nTraditionally, these methods have operated under strict exemplar memory constraints, assuming\nthat minimizing memory usage is critical for cost efficiency. However, recent studies suggest that\nmemory limitations may not be as critical as once thought, with computational costs\u2014such as GPU\nusage-emerging as a more dominant factor (Prabhu et al., 2023; Chavan et al., 2023). For instance,\nan AWS GPU server equipped with 8 A100 GPUs incurs an hourly cost of approximately $32.77,\nwhereas storing 1TB of data on Amazon S3 costs roughly $23.55 per month.\nBuilding on this, we revisit the class-IL setting under more realistic conditions, where exemplar\nmemory constraints are relaxed (i.e., sufficient exemplar memory is available). Our experiments reveal\nthat even the simplest exemplar-based approach\u2014such as naive finetuning (i.e., Replay)\u2014performs\ncomparably to state-of-the-art algorithms while incurring significantly lower training costs (e.g.,\nshorter training time). Motivated by this, we propose a simple yet effective approach that optimizes\ncomputational cost while maintaining strong performance. Our method is designed with two key\nobjectives: (1) outperforming other algorithms including Replay, and also (2) achieving computational\nefficiency, in the setting with sufficient exemplar memory. The core idea is to lower computational\noverhead by directly operating in the model's weight space. Specifically, we integrate weight resetting\nand averaging techniques to accelerate convergence while enhancing both stability and plasticity\nduring CL. Experimental results show that our method not only surpasses competing approaches but\nalso achieves superior performance while requiring only a quarter to a third of the computational cost\nof state-of-the-art algorithms. Our contributions are summarized as follows:\n\u2022 We investigate the impact of relaxed exemplar memory constraints on class-IL algorithms\nand demonstrate that even naive finetuning performs competitively under these conditions\nwhile significantly reducing the computational cost."}, {"title": "2 RELATED WORKS", "content": "Continual learning Continual learning (CL) has been actively studied in various scenarios and\nmethodological categories. Among the three scenarios of CL (Van de Ven & Tolias, 2019), class-\nincremental learning (class-IL) has been considered the most challenging and has been the most\nactively studied scenario (Masana et al., 2022). CL algorithms (including class-IL) can be categorized\ninto regularization-based approaches, which penalize changes to important parameters for past\ntasks (Kirkpatrick et al., 2017; Aljundi et al., 2018; Cha et al., 2020), rehearsal-based approaches,\nwhich store and replay exemplars from past tasks (Rebuffi et al., 2001; Cha et al., 2023), and expansion-\nbased approaches, which expand the model's capacity to balance the trade-off between stability and\nplasticity (Yan et al., 2021; Wang et al., 2022). Additional approaches focus on addressing classifier\nbias toward recent tasks while using the exemplars (Wu et al., 2019; Zhao et al., 2020). While\nexemplar-based methods have demonstrated state-of-the-art performance, they typically rely on strict\nmemory constraints, often limiting memory size to a small percentage of the dataset. Recent studies\nchallenge the necessity of these strict memory constraints, highlighting that the computational cost of\nmaintaining and processing memory-especially GPU usage-can far outweigh storage costs (Prabhu\net al., 2023; Chavan et al., 2023). This shift in perspective opens the door to relaxing memory limits\nin order to reduce training costs, which is the focus of our work.\nWeight space operations Weight space operations directly manipulate the model parameters for\nvarious goals e.g., multi-task learning (Yu et al., 2024) and continual learning (Marouf et al., 2025;\nMarczak et al., 2025). For instance, weight averaging is utilized to merge the knowledge of multiple\nmodels (Ilharco et al., 2022; Jang et al., 2025), while weight reset is used to enhance plasticity\n(Dohare et al., 2024) and improve parameter-redundancy (Yadav et al., 2024). For an overview of\nweight-space operations, please refer to Appendix A.6."}, {"title": "3 PROBLEM FORMULATION", "content": "Notation We generally follow the setting of class-incremental learning (class-IL) (Masana et al., 2022).\nWe consider a sequence of T tasks, each associated with distribution Pt. Let $D_t = \\{(x, y)_i\\}_{i=1}^{N_t}$\nwith (x, y) ~ Pt, be the training dataset for task t. The tasks are presented in order t = 1,\u2026\u2026 ,T.\nThe model F (its parameters $\\theta$) does not retain explicit access to previous task datasets Dj for j < t,\nexcept via a exemplar memory buffer M of a capacity of K. Thus, at the training step of task t, the\nmodel updates its parameters $\\theta$ using the data Dt U M1:t\u22121 and the cross-entropy loss l($\\theta$; x, y),\nwhere M1:t-1 includes selected samples from earlier tasks."}, {"title": "4 WEIGHT SPACE CONSOLIDATION", "content": "We introduce a simple but cost-effective method that\nleverages weight-space operations (e.g., weight-reset,\nweight-average) for continual learning with sufficient\nexemplar memory. The grounding idea of our ap-\nproach is that the sufficient exemplar memory regime\nallows the model to obtain an effective minimizer of\nall previous tasks (discussed in Section 3). Deriving\nfrom this, at the beginning of training each tth task,\nwe assume that we already have a good minimizer of\nthe previous tasks, and encourage the model to stay in\nits proximity in the weight-space.\nAlgorithm 1 shows pseudo-code of our method. Prior\nto the training of the tth task, the model parameters\n(i.e., minimizer of previous 1 : t - 1 tasks) are stored\nas Oprev (see Line 6 in Algorithm 1). Then, we begin\ntraining on the current tth task using the loss l($\\theta$; x, y)\n(see Line 8). After nwarm warming epochs, we apply\na rank-based weight resetting procedure to reset the\nparameters that were dormant during training (see Line\n11). We rank the dormant parameters by comparing\nthe current value in @ to the value of the previous task\nmodel prev. We tested multiple metrics to quantify\nthe dormancy and found that a simple distance |$\\theta[l]$-\n$\\theta_{prev}[l]|$ is sufficient to measure the drift for the [th\nparameter. Please refer to appendix A.3. Specifically,\nwe reset 80% of the parameters, following empirical\nresults of Yadav et al. (2024) (see Lines 12-14). Formally, the lth parameter of @ is reset using:\n$\\theta[l] = \\alpha \\cdot \\theta_{prev} [l] + (1 - \\alpha) \\cdot \\theta[l]$,\nwhere \u03b1 = 0.5 by default. This step is intended to reset parameters that did not contribute to learning\nthe current task t, allowing the model @ to remain in proximity to the previous task model @prev\u00b7\nAssuming the reset model is a good (if not optimal) minimizer of the current task t, and also retaining\nthe parameters of @prev, we apply a periodic weight-averaging over the learning trajectory (see Lines\n16-17) (Izmailov et al., 2018). The weight-averaging procedure is intended to accelerate convergence\nand improve generalization by aggregating diverse learning signals. We find this particularly beneficial\nunder sufficient exemplar memory settings, where the variance of data is significant. Please refer to\nappendix A.2 for a detailed elaboration of our method."}, {"title": "5 EXPERIMENT", "content": "Experimental settings We evaluate our algorithm on two standard class-incremental learning (class-\nIL) benchmarks (Masana et al., 2022) using the PyCIL framework (Zhou et al., 2023). CIFAR100\nis an image classification dataset with 100 classes, which is split into 10 sequential tasks, each\ncontaining 10 classes. ImageNet100 is another classification dataset with 100 classes, which is\nalso split into 10 sequential tasks, each containing 10 classes. We compare our approach with six\nexemplar-based baselines, including iCaRL (Rebuffi et al., 2001), BiC (Wu et al., 2019), WA (Zhao\net al., 2020), DER (Yan et al., 2021), and FOSTER (Wang et al., 2022). Additionally, we include"}, {"title": "6 CONCLUDING REMARKS", "content": "Our work shifts the focus in continual learning from memory efficiency to computational cost,\ndemonstrating that with sufficient exemplar memory, exemplar-based methods like Replay can\nachieve competitive performance at a fraction of the training cost. We introduce a simple, effective\nmethod that directly manipulates the model's weight space, outperforming existing algorithms in terms\nof both accuracy and efficiency. These results highlight the potential for optimizing computational\nefficiency in class-incremental learning and provide a promising direction for practical applications,"}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 AN INVESTIGATION INTO THE SUFFICIENT EXEMPLAR MEMORY REGIME", "content": "Defining the sufficient exemplar memory regime Most prior work assumes a strict exemplar\nmemory constraint: $K <= \\sum_{t=1}^T |D_t|$. Under such limited capacity, the replay buffer M can only hold\na small fraction of each older dataset in {D\u2081,\u2026\u2026, Dt\u22121}. Consequently, it provides only a partial\nrepresentation of the task distributions {$P_1$,\u00b7\u00b7\u00b7, $P_{t\u22121}$}, leaving the model prone to catastrophic\nforgetting (Kirkpatrick et al., 2017) between tasks (i.e., inter-task forgetting)\nBy contrast, we say the replay buffer M is sufficient if it can store enough samples to approximate\neach past distribution Pj for 1 < j < t. Formally, we let $K \\approx \\kappa\\sum_{j=1}^{t-1} |D_j|$, where 0 < \u043a \u2264 1\ndetermines what fraction of the earlier task data can be retained. A larger \u043a indicates that M preserves\na representative set of samples from past tasks - though not necessarily every sample. As a result of\nthis sufficient exemplar memory setting, the mixture distribution Ppast of previously encountered\ntasks at the training step of task t effectively becomes:\n$P_{past}(x, y) \\approx \\frac{\\sum_{j=1}^{t-1} \\pi_j P_j(x, y)}{\\sum_{j=1}^{t-1} \\pi_j}$,\nwhere \u03c0; denotes the relative importance (i.e., size) of each past task and Pj(x, y) the true data\ndistribution of task j. Hence, when learning task t, the effective training distribution is\n$P_{train}^{(t)}(x, y) \\approx \\alpha_t P_t(x, y) + (1 - \\alpha_t)P_{past}(x, y)$,\nwhere \u03b1t \u2208 [0, 1] is a balancing factor that weights the newly introduced and replayed data, usually\nemphasizing the current task."}, {"title": "Sufficient exemplar memory mitigates catastrophic forgetting", "content": "Next, we discuss how a suffi-\nciently large exemplar memory buffer M approximates previous tasks' distributions, thus reducing\ninter-task forgetting.\nIdeally, if we could train jointly on all tasks 1,..., t, the obtained model parameter $\\theta_{1:t}$ would\nminimize the risk:\n$R_{1:t}(\\theta) = \\sum_{j=1}^{t} E_{(x,y) \\sim P_j} [l(\\theta; x, y)]$,\nwhich minimizes inter-task forgetting by design. By joint-training on all task data up to task t, we\nwould find $\\theta_{1:t}$ = arg $min_{\\theta}$ $R_{1:t}(\\theta)$, with no conventional notion of forgetting between tasks.\nSimilarly, with a large enough exemplar memory, the replayed data for previous tasks closely\napproximates their true distributions (Pj \u2248 Pj for all j < t). Therefore training on Dt U M1:t\u22121\nyields a risk\n$R_{1:t}(\\theta) \\approx \\alpha_t E_{(x,y) \\sim P_t} [l(\\theta)] + (1-\\alpha_t) \\frac{1}{t-1} \\sum_{j=1}^{t-1} E_{(x,y) \\sim P_j} [l(\\theta)]$,"}, {"title": "A.2 METHOD (CONTINUED.)", "content": "In this section, we elaborate on our method, highlighting the role of each component. The proposed\nmethod is a combination of two weight-space operations. (1) Weight Reset (Yadav et al., 2024) and\n(2) Weight Averaging (Wortsman et al., 2022).\nWeight Reset The weight reset step selects and resets parameters that were redundant in adapting\nto the new task. The aim of this procedure is to find the minimal set of parameters that are capable of\nadapting to the new tth task, and reverting the redundant parameters to the value of the previous task\nmodel Oprev, which helps recover learned features. In this process, we use a simple distance metric to\nmeasure a model's contribution to the new task, formulated as:\n|$|\\theta[l] \u2013 \\theta_{prev} [l]|$,\nwhere 0[1] and Oprev[l] indicates the [th parameter of the current model @ and the previous task model\nOprev, respectively. Using this metric, we retain the top-Q% parameters that have largely drifted\nduring the current task and reset the dormant parameters using eq. (1). The idea of resetting model\nweights is not novel in the continual learning literature, but most focused on improving plasticity\n(Dohare et al., 2024). Conversely, we improve both plasticity and stability using a mixing technique"}, {"title": "A.3 MEASURING PARAMETER-DRIFT", "content": "In our work, we measure a parameter's contribution to learning a new task by comparing its parameter\ndrift (eq. (7)). However, there are a number of alternative approaches we could take. For instance, we\ncan differentiate between how a parameter changes (1) between tasks (2) within a task.\nInter-task Parameter Drift When transitioning from one task to the next, the change in the l-th\nparameter can be measured as\n$\u0394\u03b8_{inter}[l] = |\u03b8_t[l] \u2014 \u03b8_{t\u22121}[l]|,$\nwhere 0t [1] denotes the lth parameter after training on the current task t, and Ot\u22121[l] represents the\nIth parameter after training on the previous task t 1. A large value indicates that the parameter is\nhighly task-specific, while a small value suggests robustness across tasks.\nIntra-task Parameter Drift Alternatively, we can analyze how a parameter evolves during the\ntraining process of a single task. Let $\\theta_l^{(i)}$ denote the value of the lth parameter at the ith iteration\nduring training. Then, the intra-task parameter drift is given by\n$\u0394\u03b8_{intra}[l] = |[\\theta_l^{(i+1)}] \u2013 [\\theta_l^{(i)}]|$.\nThis measure captures the incremental updates of the parameter as the model optimizes its perfor-\nmance on the current task. By comparing both the inter-task and intra-task parameter changes, we\ngain a more comprehensive understanding of the role each parameter plays in adapting to new tasks\nas well as the dynamics of learning within a single task. In our future work, we will seek a more\nreliable metric to express a parameter's behavior in the weight space."}, {"title": "A.4 EXPERIMENTAL RESULTS AND ANALYSIS", "content": "In this section, we continue our discussion of the experimental results."}, {"title": "A.5 EXPERIMENTAL DETAILS", "content": "In this section, we report the experimental details of our experiments. In all our experiments, we have\nused the PyCIL (Zhou et al., 2023) library, which allows easy replication. We followed the standard\ntraining hyperparameters of the PyCIL library, which are fixed across experiments. Regarding unique\nhyperparameters, the average interval j was set as 5, and the warming epoch Nwarm was set as 25% of\nthe total training epochs (default set as 70 under the PyCIL setting). j and Nwarm were selected using\na grid search. Please refer to Section 4 for a better understanding of each hyperparameter. Lastly,\nregarding the computing resources, we used a single NVIDIA RTX 6000 GPU for all experiments.\nFor our experiments, we used the 2.2.1 version of Pytorch (Paszke et al., 2019)."}, {"title": "A.6 RELATED WORKS (CONTINUED.)", "content": "Weight Space Operations Recent works have shown that manipulating model parameters directly\nwith weight-space operations (e.g., model merging (Wortsman et al., 2022)) can handle multi-task\nlearning Yu et al. (2024) and continual learning (Marouf et al., 2025; Marczak et al., 2025) in a\nmore principled way. These techniques usually intervene post-training by merging the weights of\ndifferent models e.g., Yadav et al. (2024) suggested a selective merging algorithm that mitigates the\ninterference between different models, while Ilharco et al. (2022) showed that arithmetic operations\non the weight space can edit the model without training. Unlike these post-training interventions, our\napproach manipulates the model's weight space amidst training (Izmailov et al., 2018; Jang et al.,\n2025) without storing multiple model parameters, aiming for cost-effective editing of the continual\nlearner. Another relevant yet overlooked topic is the effect of weight-space operations on model\nattributes e.g., loss landscape (Li et al., 2018; Kaur et al., 2023) and plasticity (Dohare et al., 2024),\nthat contribute to continual learning and generalization. This work empirically investigates various\naspects of the model to study their effect on the model's ability to handle distribution shifts. In the\ncontinual learning literature, several works have adopted weight-space operations to obtain multi-task\nsolutions without joint training. For instance, Kozal et al. (2024) has suggested the use of weight\naveraging techniques for continual learning, and Marczak et al. (2025) has extended the idea using\ntask arithmetic. However, these approaches merge models post-training and requires the storage\nof multiple model weights during training. On the other hand, our approach utilizes weight-space\noperations amidst training, without the redundant storage of multiple model weights. We view this as\nan important difference in modern settings where the model size is exponentially growing."}]}