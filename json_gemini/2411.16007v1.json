{"title": "Performance Implications of Multi-Chiplet Neural\nProcessing Units on Autonomous Driving Perception", "authors": ["Mohanad Odema", "Luke Chen", "Hyoukjun Kwon", "Mohammad Abdullah Al Faruque"], "abstract": "We study the application of emerging chiplet-based\nNeural Processing Units to accelerate vehicular AI perception\nworkloads in constrained automotive settings. The motivation\nstems from how chiplets technology is becoming integral to emerg-\ning vehicular architectures, providing a cost-effective trade-off\nbetween performance, modularity, and customization; and from\nperception models being the most computationally demanding\nworkloads in a autonomous driving system. Using the Tesla\nAutopilot perception pipeline as a case study, we first breakdown\nits constituent models and profile their performance on different\nchiplet accelerators. From the insights, we propose a novel\nscheduling strategy to efficiently deploy perception workloads on\nmulti-chip AI accelerators. Our experiments using a standard\nDNN performance simulator, MAESTRO, show our approach\nrealizes 82% and 2.8\u00d7 increase in throughput and processing\nengines utilization compared to monolithic accelerator designs.", "sections": [{"title": "I. INTRODUCTION", "content": "The landscape of the automotive industry is being trans-\nformed through software-defined vehicles (SDVs) that enable\nintegrating sought-out features of Advanced Driver Assistance\nSystems (ADAS), autonomy and infotainment (AR/gaming)\n[1]\u2013[4]. To maintain flexibility and manage rising compute\ndemands, emerging vehicular systems are shifting towards new\ncross-domain, centralized Electrical/Electronic (E/E) architec-\ntures with a few powerful processing computers and zone\nECUs [1], enabling easier integration of new features and\nupdates, and allowing automakers to adopt customized chip\ndesign tailored to their needs. As a result, automakers like\nTesla, GM Cruise, Volkswagen have entertained the adoption of\ncustomized System-on-Chips (SoCs) for their automotive com-\npute requirements [5]\u2013[7]. Still, as automotive AI workloads\ncontinue to evolve and rise in complexity, advancements in\nautomotive SoC hardware also become a necessity, and that\npresents a considerable challenge given the long design cycle,\nhigh manufacturing costs, and Moore's law stagnation.\nChiplets technology presents a viable solution for the au-\ntomotive industry [2], [4], [8], [9]. Owing to chiplets' com-\nposability property, a scalable, customizable approach becomes\nfeasible supporting the integration of individual hardware mod-\nules on the package level, potentially from different technology\nnode generations, and enabling individual component updates\nto be incorporated with a faster turnaround time than monolithic\nSoC approaches. Even more so, recent advancements has seen\nNeural Processing Engines (NPUs) - integral to autonomous\ndriving systems \u2013 implemented through consolidating multiple\nsmall accelerator chiplets on the package to form a scalable\nAl inference engine [10]\u2013[15], providing flexible means to\ncontrol different design parameters (accelerator chips number,\nconnection topology, and heterogeneity) [13], [14].\nStill, NPUs constructed as multi-chiplet modules (MCM)\nremain largely understudied in the context of automotive AI\nworkloads despite the challenges and potential gains. On the\none hand, automotive AI workloads exhibit unique execution\nflows characterized by intricate dependencies, concurrencies,\nand feature fusion nodes [16]\u2013[18]. On the other, AI computing\nkernels exhibit varying affinities towards different accelerator\ntypes [13], [19]\u2013[22]. Though multiple works [23]\u2013[26] have\ninvestigated the architectural implications for improving auto-\nmotive Al workloads' performance, the architectural implica-\ntions of adopting chiplets technology remains to be studied.\nThis work aims to investigate such implications when an\nMCM AI accelerator is employed as the automotive NPU to\naccelerate AI perception workloads. Specifically, we follow the\narchitectural template of Tesla's FSD chip [27], and simulate\nan industry-grade MCM inference accelerator engine \u2013 Simba\n[10], [15] \u2013 for the system's NPU (Figure 1). For the automotive\nworkloads, we focus on those from the compute-intensive\nperception pipeline [16], [24], [28]. We implement and analyze\nsuch workloads following the Tesla Autopilot system [5] which\nentail HydraNets, spatio-temporal fusion, and multi-task heads\n(lane detection, occupancy networks). From the insights, we\ndevise a novel scheduling methodology to enhance performance\nefficiency of perception workloads on the MCM-NPU. In\nsummary, our key contributions are as follows:\n\u2022\nWe characterize the key workloads existing in a SOTA per-\nception pipeline (Tesla Autopilot), from the early feature\nextraction stage till the final multi-task heads models.\n\u2022\nWe conduct a thorough performance breakdown of per-\nception workloads to understand their execution properties\nand hardware acceleration affinities using the standard\nDNN performance simulator, MAESTRO [29], [30].\n\u2022\nFrom our analysis, we implement a low-cost scheduling\nalgorithm for mapping perception workloads onto MCM-\nNPUs given added Network-on-Package overheads and the"}, {"title": "II. BACKGROUND AND PRELIMINARIES", "content": "A. Anatomy of a self-driving platform architecture: Tesla FSD\nWe take the Tesla FSD (Full Self Driving) SoC as our\nreference self-driving architecture template. As illustrated in\nFigure 1, the FSD integrates the following units:\n\u2022\nCPU clusters. For general purpose compute. Each cluster\nconstitutes a quad-core Cortex A72 in the Tesla FSD.\n\u2022\nMemory. Main memory component of the chip (e.g.,\nLPDDR4 memory)\n\u2022\nCameras I/F and ISP. High speed camera serial interface\nand image signal processor for image preprocessing\n\u2022\nNPUs. Custom-designed hardware accelerators for effi-\ncient processing of AI workloads\n\u2022\nGPU. For light-weight post-processing\n\u2022\nOther. Video encoder, safety component.\nFor a chiplets technology variant, a system-in-package (SiP)\ncan be constructed from this template integrating multiple\nsmaller chiplets, each covering a subset of components and\ncommunicating over package via high-speed interconnects.\nMoreover, recent prototypes like Simba [10] have shown that\nthe NPU component can also be implemented as MCMS\nintegrating accelerator chiplets on the package level.\nB. Autonomous Driving System Pipeline\nAutonomous driving systems (ADS) map sensory input data\nto vehicle control outputs through 3 stages: perception for\ncontextual understanding and objects tracking in the driving\nscene; planner for trajectory paths generation; control for pro-\nviding the necessary control outputs. Perception represents the\nmost compute intensive stage [17], [24], and supports multiple\ndriving tasks (detection, lane prediction). Recent perception\nmodules have seen the adoption of the HydraNets architecture\n[16], with a shared backbone for extracting low-level, common\nfeatures, and specialized heads for driving tasks specialization.\nIn Figure 2, we illustrate the HydraNet architecture from the\nTesla Autopilot system [31] discussed in the following:\nSensor inputs. Tesla Autopilot perception relies on collect-\ning images from 8 installed cameras. Typically images can be\n720p resolution at around 30 FPS [24].\nStage 1: Feature Extraction (FE+BFPN). Each raw input\nimage is pre-processed and passed through a feature extractor\n(FE) followed by a bidirectional feature pyramid network\n(BFPN) [32], to generate multi-scale feature representations.\nFollowing [28], the FE can be a ResNet18 architecture with 4\nmultiscale features (90x160x256, 45x80x512, 23x40x1024, and\n12x20x2048) generated throughout its ResNet blocks, which\nare then passed through 2 Bi-directional FPN blocks (BFPN).\nAt the final output, multiscale features are concatenated from\neach FE+BFPN pipeline forming the 8x20x80x256 outputs.\nStage 2: Multi-cam Spatial Fusion (S_FUSE). Generated\nembeddings from each camera are fused onto a combined\nspatial representation in vector space (2D/3D grid map). A\ntransformer with multi-attention head is employed, computing\nattention scores between every grid cell and detected features\nfrom each camera. The attention module [33] comprises: QKV\nprojection to generate Query (Q), Key (K), and Value (V)\nvectors; Attention with two matrix multiplications for $(QK^T) \\cdot V$;\nFFN (Feed-forward Network) comprising two linear layers.\nFor a 20\u00d780 2D grid [28], [31], the output becomes a fused\nprojection of the 8 camera features onto a 1x20x80x256 grid.\nStage 3: Temporal Fusion (T_FUSE). The 1x20x80x256\nspatial representation is fed into a video queue to be fused with\na feature queue of N previous representations to accommodate\nnecessary temporal features (e.g., seen signs or lane markings),\nand telemetry information. Another attention module can be\nused with N=12 for temporal fusion [28], [31], each undergoing\nQKV projection, attention, and FFN transformation till the final\nfused spatio-temporal 1x20x80x300 representation.\nStage 4: Trunks and Heads (TR). The 1x20x80x300 rep-"}, {"title": "III. ANALYSIS OF PERCEPTION MODULE WORKLOADS", "content": "To derive design insights for MCM-based NPUs, We first\nanalyze the perception workloads' performance on various\naccelerator architectures from Section II-B using the analytical\nDNN cost model simulator, MAESTRO [29], [30] (known to\nhave achieved 96% accuracy compared to RTL simulations).\nWe consider the following:\n\u2022\nWe set the number of PEs in each accelerator to 256 PES.\nThat way, when accelerators are assimilated as chiplets into\na 6\u00d76 MCM like Simba [10], a total of 9,216 PEs will be\navailable equivalent to that of the original Tesla NPU [27].\n\u2022\nWe use the same operating frequency at 2 GHz [27].\n\u2022\nWe analyze the workloads on weight stationary (WS) and\noutput stationary (OS) accelerator types \u2013 like NVDLA [34]\nand Shidiannao [35]) \u2013 given their proven superiority over\nother accelerator types [13], [19], [36].\nA. Coarse-grained Performance Analysis\nIn Figure 3, we breakdown the contributions of the percep-\ntion stage models in terms of latency and energy on NVDLA-\nand Shidiannao-like accelerators and deduce the following:\nOS dataflow suits latency-critical workloads. Across all\nworkloads, the Shidiannao OS dataflow offers 6.85\u00d7 speedups\nover its WS counterparts, making it the prime candidate for\nlatency critical automotive workloads [24].\nWS offers energy efficiency opportunities. On average,\nthe WS dataflows can lead to 1.2\u00d7 energy efficiency gains\ncompared to OS. The gains are even more significant (1.55\u00d7)\nif we omit S_FUSE and T_FUSE from our analysis, which\nare more affine towards OS dataflow. This opens the door\nfor potential heterogeneous integration (mix of OS and WS\nchiplets) to realize interesting performance trade-offs.\nFusion Modules are computational bottlenecks. The\nS_FUSE and T_FUSE modules constitute respective 25%-\n28% and 52%-54% of the overall perception module latency\nunder both dataflow scenarios. This is attributed to the feature\naggregation from multiple sources (8 cameras and 12 temporal\nframes) onto a shared projection space (200\u00d780\u00d7256 grid).\nFE+BFPN Scaling. Evaluations for the FE+BFPN in Figure\n3 are for a single camera and to be multiplied by the 8 cameras.\nB. Fine-grained Performance Analysis\nWe analyze the individual layers affinities towards OS and\nWS from the various models in Figure 4, where we compare\nthe difference in values between the OS and WS dataflows,\n$\\Delta \\text{Value} = \\text{Value}_{OS} - \\text{Value}_{WS}$, for latency and energy (-ve\nvalues imply OS affinity and +ve values imply WS affinity).\nFE+BFPN tradeoffs. The FE+BFPN stage exhibits a direct\ntrade-off between latency and energy across all layers. Yet, the\nnon-uniform distribution of performance gains, the complex\ndependencies of the FE and BFPN networks, and the concurrent\nexecution requirement for 8 FE+BFPN models impose strict\nrequirements on accelerator chiplets assignment process.\nS_FUSE and T_FUSE tradeoffs. The negative evaluations\nof Latency and $\\Delta \\text{Energy}$ across all layers indicate a strong\naffinity towards the OS dataflow for the fusion modules. We\nalso observe significant performance bottlenecks exist at the\nself-attention layers (QKV calculations at layer ids 1 and 2).\nTrunks tradeoffs. The diversity of trunk models lead to\nvarying affinities: the lane prediction trunk is completely\nskewed towards the OS dataflow (due to attention modules),\nunlike other trunks which can provide exploitable trade-offs."}, {"title": "C. Design Insights.", "content": "Based on our analysis, we derive the following insights onto\nscheduling perception workloads onto MCM-based NPUs:\n\u2022\nAny scheduling configuration is to target maintaining\na consistent throughput (pipelining latency) across the\nvarious perception stages for streamlined input processing.\n\u2022\nHeterogeneous chiplets integration can be supported as\nlong as the latency constraint is not violated. Given the\ndominance of the OS dataflow with regards to execution\nlatency, we specify the pipelining latency of the OS\ndataflow as the reference latency constraint.\n\u2022\nAny optimizations for the FE+BFPN stage must be applied\nfor the 8 concurrent models for simultaneous execution.\n\u2022\nThe computational bottleneck of the fusion layers are\nconfined within a small number of layers (see Figure 4),\nmaking them targets for parallelization optimizations.\n\u2022\nHeterogeneous chiplets integration can be particularly ben-\neficial for the diverse trunk workloads to elevate efficiency."}, {"title": "IV. PROPOSED SCHEDULING METHODOLOGY", "content": "We propose to schedule the perception workloads on MCM-\nNPU through a throughput matching mechanism as follows:\n1) Specify initial execution latency estimates and chiplet\nallocation per layer in each stage\n2) Allocate chiplet resources across the various workload\nstages based on the latency estimates\n3) Alleviate bottleneck stages impact via data sharding\n4) Update latency estimates and chiplet assignments\n5) Repeat steps 2-4 until pipelining latencies are matched\nor no further sharding is possible\nIn Algorithm 1, we achieve this sequence through a nested\ngreedy algorithm whose outer loop alleviates bottlenecks across\nstages, and inner loop alleviates bottlenecks across layers in\nthe bottleneck stages. Line 2 demonstrates how chiplets are\ninitially allocated to each stage based on a function of the\nmodel execution latencies, the number of workloads, and the\nnumber of models per each stage workload. We initially adopt\nuniform partitioning of chiplet resources across the 4 stages,\nwhere each is assigned a separate quadrant of chiplets. The\nreasons being: (i) having 4 distinctive perception stages; (ii)\nhaving 8 processing parallel model instances of the FE+BFPN;\n(iii) the latency bottlenecks being in the fusion modules with\nsmall number of layers; (iv) the diversity of the trunk models.\nA. Choosing the FE+BFPN for the base pipelining latency\nGiven 8 model instances of the FE+BFPN stage where\neach possesing a compute latency around of the T_FUSE\nbottleneck (Figure 3), at least 8 chiplets need to be initially\nallocated to each model to maintain concurrent execution. From\nthe latency contribution breakdown in Figure 3 across the 4\nstages, assigning additional resources to the FE+BFPN would\nlead to suboptimal solution since it leaves just above half of\nthe total number of chiplets for processing >90% of the overall\nworkloads. Therefore, it becomes reasonable to specify the\ninitial base pipelining latency to that of the FE+BFPNs models\n(Latbase=82.7 ms) as shown in Figure 5."}, {"title": "B. Alleviating S_FUSE and T_FUSE bottlenecks", "content": "We alleviate the attention fusion bottlenecks through recur-\nsive sharding until the stages' latency equates that of Latbase.\nS_FUSE Bottleneck. This attention module operates on 8\nfeature input sets from the FE+BFPN stage outputs. Given\n200\u00d780\u00d7256 attention grid dimensions, S_FUSE stage incurs\n78.7 ms, 20.5 ms, and 236 ms latency overheads for the QKV\nprojection, self-attention, and Feed-Forward (FFN) layers when\neach is processed on an individual chiplet. Accordingly, the\nspatial FFN layer is sharded in a four-folded manner, replicating\nthe FFN weights on 4 chiplets, each processing features from\ntwo FE+BFPNs. This brings FFN latency close to the QKV\nprojection (78.7 ms) and Latbase (82.7 ms). As S_FUSE is\nstill left with 4 chiplets (one is surplus from the FE+BFPN\nstage), an additional sharding step can be performed, leading\nto the final configuration shown in Figure 6.\nT_FUSE Bottleneck. Prior to sharding the 12 frames, the\nT_FUSE took 165.6 ms, 36.4 ms, and 490.2 ms for the QKV\nprojection, self-attention, and FFN blocks, respectively. Our\nalgorithm in this case involves two inner loop iterations, first\ndistributing the FFN block workloads across 6 chiplets, leading"}, {"title": "C. Design Space Exploration for the Trunks stage", "content": "Given the diversity of the trunk models and their customiza-\ntion, we employ a design space exploration for this stage to\nmanage the mapping of workloads onto chiplets. Additionally,\nwe consider heterogeneous integration options based on the\nanalysis in Figure 4 to leverage potential energy efficiency\ngains. In particular, we consider two configurations, Het(2) and\nHet(4), which integrate 2 and 4 WS chiplets within the OS-\ndominated 3\u00d73 chiplets quadrant, respectively. We specify the\nfollowing scoring function for our search:\n$\\text{Score}(\\text{config}) =\\begin{cases} \\infty, & \\text{if } L_{max}(\\text{chiplet}) > L_{cstr} \\\\ -1 \\times \\text{EDP}, & \\text{otherwise} \\end{cases}$\nwhere a scheduling configuration (config) is only consid-\nered on the basis of its Energy-Delay Product evaluation as long\nas the pipeline latency ($L_{cstr}$) is not violated by any chiplet.\nGiven the relatively small size of the search space (9 chiplets,\n3 models, and < 100 layers), we perform a brute force search\nwith the results provided in Table I at $L_{cstr}$ = 85 ms.\nFrom the table, we can see that the heterogeneous con-\nfigurations, Het(2) and Het(4) realize performance efficiency\nimprovements in energy and EDP compared to the OS only\nconfiguration, achieving average 1.1% and 6.2% reductions\nin raw energy; 17.4% and 12.0% reductions in EDP, re-\nspectively. Upon careful inspection, we found the WS chiplets\nare predominantly assigned to the DET_TR layers, leading\nDET_TR independently to achieve a 35% reduction in energy."}, {"title": "D. NoP Cost Modeling", "content": "We model the NoP data movement overheads using a cost\nmodel built around MAESTRO [29], [30] and the microarchi-\ntecture parameters from Simba [10] scaled to 28 nm as follows:\n\u2022\nNoP Interconnect BW: 100 GB/s/chiplet\n\u2022\nNoP Interconnect Lat: 35 ns/hop\n\u2022\nNoP Interconnect Ergy: 2.04 pJ/bit\nTransmission latency is computed as a function of feature map\nsize over the NoP bandwidth, multiplied by the number of hops\nfrom source to destination. Whereas transmission energy is the\nmultiplication of feature map size, interconnect bit transmission\nenergy, and the number of hops.\nWe analyze the NoP data movement overheads in Figure 9\nacross the different layer workloads following the scheduling\nconfiguration of the previous subsections and make the fol-\nlowing observations: (i) Large feature map outputs (as from\nQKV_Proj layers) have high transmission costs (latency and\nenergy) and are placed in close proximity to their destinations\nto limit overheads (Figures 6 and 7); (ii) The gathering of\nsharded outputs from multiple far nodes (S_FFN) can cause\nrise in NoP traffic and the corresponding latency. (iii) Most\nimportantly, the overall perception latency bottleneck does not\nlie in NoP transmission overheads as they are at least two orders\nof magnitude less than the computational costs in Figure 3."}, {"title": "V. EVALUATION", "content": "We evaluate our MCM scheduling solution against baselines\nwith the same number of PEs:\n\u2022\nA single accelerator chiplet with 9,216 PEs (Regular)\n\u2022\nTwo accelerator chiplets with 4,608 PEs each\n\u2022\nFour accelerator chiplets with 2,304 PEs each\nWe consider two pipelining schemes for the evaluation: (1)\nstagewise, and (2) layerwise. We also analyze how our solution\ncan scale to two NPUs (2\u00d76\u00d76 Simba MCMs for a total of\n72 chiplets). Unless otherwise stated, we focus our analysis\non the multi-chiplet NPU with OS only dataflow. We perform\ncomparisons on the first 3 bottleneck stages of the perception\npipeline. A separate ablation study is performed for the trunks."}, {"title": "A. Comparison against Baselines", "content": "We show the comparison against the baselines in Table II.\nThough the 6\u00d76 solution achieves the best pipelining latency\nscores with 87 ms, it incurs additional energy consumption\nfrom the overhead associated with the NoP data transmission,"}, {"title": "B. Scaling to 2 Multi-chiplet NPUs", "content": "We explore how our solution scales to 72 chiplets if the two\nNPUs on the FSD are active and processing the same workloads\n(recall Fig 1). We assume in this analysis the number of trunks\nis doubled (assigned 2\u00d79 chiplets), and that they incur a fixed\nperformance overhead not becoming the the latency bottleneck.\nIn Figure 10, we show how our algorithm progresses to\nreduce the pipelining latency and assigns chiplet resources.\nFirst, our algorithm extends the sharding of the temporal\nprojection layer T_QKV from 2 to 4, reducing its pipelining\nlatency to 41.1 ms. Then, T_FFN layer is assigned an additional\n6 chiplets to bring its latency down to 40.8 ms. At this point, the\nsharding is exhausted for the T_FFN as each temporal frame is\nprocessed independently on a separate chiplet. The FE+BFPN\nlayer is then partitioned into two pipelining stages at the\nfourth convolutional ResNet-18 block, yielding two equivalent\npartitions with pipelining latency of 40.04 ms. Lastly, S_PROJ\nis divided across two chiplets to yield a latency at 39.4 ms.\nThe final pipelining latency is 41.1 ms, almost 2\u00d7 that of the\n36 chiplet case."}, {"title": "C. Ablation on Optimizing Trunk models Design", "content": "We performed additional ablation studies on the trunk models\nto pinpoint their performance bottlenecks.\nOccupancy Trunk. Deconvolution operations are the bottle-\nneck of the occupancy trunk. In Table III, we showcase the non-\nlinear increase in latency incurred in the occupancy network\nwith each added upsampling layer, with the final upsampling\nlayer contributing the most to the overall latency (~75%).\nThis underpins an exploitable trade-off between resolution\ngranularity and performance efficiency.\nLane Prediction Trunk. In the Tesla Autopilot System,\nthe lane prediction network adopts a form of context-aware\ncomputing for efficiency, where rather than running the lane\nprediction algorithm for every region, processing is only per-\nformed for relevant regions in the grid [37]. In Figure 11, we\nshow a similar pattern in our implementation where processing\nacross all regions leads latency to exceed the 82 ms pipelining\nlatency. Around 60% computing satisfies the latency constraint."}, {"title": "VI. RELATED WORKS", "content": "A. Multi-chiplet Modules\nAccelerating AI workloads through a chiplets system in-\nvolves combining on package a host system with an accel-\neration platform which can be based on GPU [38], [39],\nFPGA [40], or NPUs [10], [11], [15]. More sophisticated\narchitectures can entail the acceleration platform comprising\nmultiple smaller modules consolidated on package to form a\nmulti-chiplet module (MCM) [10]\u2013[13], [15], [38]."}, {"title": "B. Autonomous Driving Systems Efficiency", "content": "The works in [17], [26], [41], [42] study how distributed\nscheduling of ADS perception tasks across multiple platforms\nelevates efficiency. Other works in [23], [24] explore how\ndifferent hardware architectures and platforms (GPUs, FPGAs,\nASICs) influence efficiency in the autonomous driving SoC."}, {"title": "VII. CONCLUSION", "content": "We have studied the performance implications from adopting\nchiplet-based NPUs to accelerate perception AI workloads\nthrough the Tesla Autopilot use case. Our findings demonstrate\nthat the combination of throughput matching algorithm and\nheterogeneous integration offer desirable performance trade-\noffs from the multi-chiplet module despite the added NoP costs,\nmotivating further studies on adopting forthcoming chiplet-\nbased NPU architectures in the automotive setting."}]}