{"title": "Enhanced Speech Emotion Recognition with Efficient Channel Attention Guided Deep CNN-BiLSTM Framework", "authors": ["Niloy Kumar Kundu", "Sarah Kobir", "Md. Rayhan Ahmed", "Tahmina Aktar", "Niloya Roy"], "abstract": "Speech emotion recognition (SER) is crucial for enhancing affective computing and enriching the domain of human-computer interaction. However, the main challenge in SER lies in selecting relevant feature representations from speech signals with lower computational costs. In this paper, we propose a lightweight SER architecture that integrates attention-based local feature blocks (ALFBs) to capture high-level relevant feature vectors from speech signals. We also incorporate a global feature block (GFB) technique to capture sequential, global information and long-term dependencies in speech signals. By aggregating attention-based local and global contextual feature vectors, our model effectively captures the internal correlation between salient features that reflect complex human emotional cues. To evaluate our approach, we extracted four types of spectral features from speech audio samples: mel-frequency cepstral coefficients, mel-spectrogram, root mean square value, and zero-crossing rate. Through a 5-fold cross-validation strategy, we tested the proposed method on five multi-lingual standard benchmark datasets: TESS, RAVDESS, BanglaSER, SUBESCO, and Emo-DB, and obtained a mean accuracy of 99.65%, 94.88%, 98.12%, 97.94%, and 97.19% respectively. The results indicate that our model achieves state-of-the-art (SOTA) performance", "sections": [{"title": "1. Introduction", "content": "The intricate process of human communication includes the exchange of knowledge between individuals using a variety of modalities, including speech, gestures, and facial expressions. Individuals regularly interact with information and emotions through speech [1]. Speaking causes sound waves to be released into the environment, which the listener's ears pick up on. The listener's brain then decodes those sound waves to produce important details, such as the meaning of those words and their underlying emotions. Speech emotion recognition (SER) is an approach used to recognize and comprehend the emotional states of individuals by analyzing their speech, audio, and video. It works by extracting features from speech signals and mapping them to specific emotional states like happiness, sadness, anger, or fear [2]. SER provides various practical applications, such as improving interactions between humans and computers, otherwise known as human-computer interaction (HCI). Furthermore, in domains like psychology and healthcare, SER can be utilized to understand patients' emotional states and provide relevant interventions [3]. Despite it's potential benefits, SER remains a challenging task. Due to the intricate nature of human emotions and the diverse characteristics of vocal signals, achieving accurate recognition results poses a significant challenge.\nFeature extraction is a crucial part of building an accurate and robust SER model. The most essential and extensively used features in SER are acoustic features. In several studies, SER has been developed by incorporating a wide range of acoustic features, including prosodic features such as pitch, intonation, energy, and loudness, spectral features like centroid, flux,\nmel-frequency cepstral coefficients (MFCCs), and mel frequency magnitude coefficient, as well as voice-quality-related features such as jitter, shimmer, and harmonic-to-noise ratio [4, 5, 6]. Additionally, features like the energy\noperator have also been incorporated into this process [4]. These features are used to acquire both the local and global contextual dependencies for"}, {"title": "2. Related Works", "content": "This section presents recent findings and studies related to SER tasks.\nDeep learning is highly useful for data scientists who deal with collecting, ana- lyzing, and understanding huge amounts of data. Deep learning-based models have performed tremendously well in SER systems. Kwon et al. [32] pro- posed a model based on a one-dimensional dilated CNN with residual blocks using a skip connection to recognize the local features from each segment of the speech signal. The proposed model was evaluated by the IEMOCAP and Emo-DB datasets with a prediction accuracy of 73% and 90% respectively."}, {"title": "3. Methodology", "content": "As depicted in Figure 2, our initial step involved utilizing the SER dataset.\nIn Stage 1, we extracted MFCC, mel-spectrogram, RMS, and ZCR values, which were then represented as a feature vector. This feature vector was combined and the dimension is (3210, 150). Subsequently, we split the feature vector into training, validation, and testing sets with a ratio of 60:25:15 respectively. For model training, we fed both the training and validation"}, {"title": "3.1. Proposed Dual Channel SER Model", "content": "We have designed dual-channel attention-guided 1D CNN-BiLSTM networks- based deep neural networks. One channel is used for extracting high-level hidden local features. The other channel is utilized for contextual and se- quential global feature extraction. In addition, ECA-Net employs a parallel"}, {"title": "3.1.1. Attention Based Local Feature Blocks", "content": "The channel for extracting local features is made of four ALFBs. Each block has four distinct sections. The first part is the 1D CNN layer which contains 256 filters, a kernel size of five, padding = \u2018same', strides = 1, and the Rectified Linear Unit (ReLU) is used as an activation function. ReLU is used for solving the vanishing gradient problem in our model. The 1D CNN is effective in extracting local features from sequential data. It can learn spatial and temporal patterns in the data, capturing short-term patterns and structures that are beneficial for emotion recognition. Then the output of the 1D CNN layer is passed through the second part an ECA-Net block, which is responsible for channel attention.\nECA-Net is a lightweight attention-mechanism module derived from Squeeze and Excitation Network (SE-Net). The SE-Net is one of the representative models of the channel attention mechanism. It was the champion model of the final ImageNet classification competition in 2017. It performs three main operations: squeeze, excitation, and reweight. Figure 4 shows the SE-Net module.\nIn figure 4, X is the Input feature map, where C, H, and W are Channel dimensions, height, and width, respectively. To describe the outcome of a\ngiven transformation, $F_{tr}$: X \u2192 U,X \u2208 $R^{H'\u00d7W'\u00d7C'}$,U \u2208 $R^{H\u00d7W\u00d7C}$ and X represent the resulting feature map. Fsq represents the squeeze operation that reduces the spatial dimensions of each feature map to 1x1, resulting in a channel-wise representation. The calculation formula is given below:\n$F_{sq}(u_c) = \\frac{1}{H \\times W} \\sum_{i=1}^{H,W} u(i, j)$ (1)"}, {"title": "", "content": "$F_{ex}$ denotes the excitation operation. The goal here is to learn a set of weights for each channel of the input feature map X to capture the impor- tance of each channel. These weights are learned using a small neural network consisting of one or more fully connected layers. The calculation formula is:\n$F_{ex}(z,W) = \u03c3(g(z, W)) = \u03c3(W_2d(W_1z))$ (2)\nIn Eq 4, W1 and W2 represent the two fully connected layers. $F_{scale}$ denotes the scaling operation which is also known as reweighting. This is done by multiplying each channel of the original feature maps. For this reason, it gives more importance to the channels that are important and reduces the significance of less important channels. The result is a feature map that is focused on the most relevant information, which can be beneficial for improving the performance of deep neural networks.\nHowever, SE-Net also has some disadvantages. Sometimes, it loses im- portant information during the process of squeezing and capturing the de- pendencies of all channels. To overcome this issue, ECA-Net is proposed. It allows the network to selectively focus on the most informative features. For that reason, it can easily suppress the noise and irrelevant information. Figure 5 represents the diagram of the ECA-Net module."}, {"title": "", "content": "In Figure 5, X is the Input feature map, where C, H, and W are Channel dimensions (number of filters), height, and width, respectively. A global av- erage pooling (GAP) operation is applied to the input feature map X which compresses H \u00d7 W \u00d7 C data into 1 \u00d7 1 \u00d7 C without dimensionality reduc- tion and the features of each channel are aggregated. In ECA-Net, first, we adaptively select the kernel size K = 1 by a function of channel dimension C. After that, a one-dimensional convolution of size K is performed. Finally, a sigmoid function has been performed to learn channel attention. The con- volution kernel size (K), plays an important role in identifying the coverage of interaction within the data. This parameter is related to the channel dimension C, which is typically chosen to be a power of 2. The mapping relationship between these two parameters can be calculated as follows:\n$C = \u03c6(k) = 2^{(\u03b3\u2217k\u2212b)}$ (3)\nThe computational formula of the convolution kernel size K is given be- low.\n$k = \u03c8(C) = \\left| \\frac{log_2(C)}{\u03b3} + \\frac{b}{\u03b3} \\right|_{odd}$ (4)\nHere, $\\left| \\frac{log_2(C)}{\u03b3} + \\frac{b}{\u03b3} \\right|_{odd}$ indicates the nearest odd number of t, y is set to 2, and b to 1.\nAfter that, the output feature map from ECA-Net is reshaped so that it can pass through the third part of our model which is batch normalization (BN). It increased the training stability and speed of the network. As a result, it is passed through the final part 1D max-pooling layer with a pool size of two, a stride of one, and using the padding as 'same'. It is used to produce the same output length similar to the input sequence length while keeping the most important features. The following three ALFBs are employed, with kernel sizes of 5, 5, and 3 for the 1D convolution layers. The filters on these blocks are 256 except for the last block which is 32. Padding and stride configurations are the same as the previous blocks. In addition, ECA-Net is also performed after each 1D convolution layer. Batch normalization is also passed through these layers and a 1D max-pooling with the following three ALFBs with a pool size of 2, 2, 5, and a stride of two is performed. After that, we passed this input into a flattened layer."}, {"title": "3.1.2. Global Feature Blocks", "content": "The global feature blocks (GFBs) are responsible for global contextual feature extraction and consist of BiLSTM. It can capture how speech features change over time by processing speech frames or signals in both forward and backward directions. They take into account data from previous and forthcoming frames, allowing the model to comprehend the larger context and identify long-term patterns crucial for precise emotion recognition in speech. For the SER task, it is important because emotional cues in speech often occur over a longer period of time than just a few milliseconds. For that reason, it is influenced by the context of preceding and succeeding speech segments.\nThe LSTM unit has four fundamental components, including one input gate it with corresponding weight matrices $W_{xi}, W_{ci}, W_{h\u2081}$ and bias $b_i$, one forget gate $f_t$ with corresponding weight matrices $W_{xf}, W_{cf}, W_{hf}$ and bias\n$b_f$, one output gate $o_t$ with corresponding weight matrices $W_{xo}, W_{co}, W_{ho}$ and bias $b_o$, and the cell state $c_t$. Each of these gates calculates its output based on the current input $x_i$, the previous hidden state $h_{(t-1)}$, and the\nprevious cell state $C_{(t-1)}$. Simultaneously, the same input sequence is passed\nthrough the backward LSTM layer. It processes the sequence in reverse\norder. Additionally, it also maintains hidden states and memory cell states.\nThe computational formula is given below:\n$i_t = \u03c3(W_{xi}x_t + W_{hi}h_{(t-1)} + W_{ci}C_{(t-1)} + b_i)$ (5)\n$f_t = \u03c3(W_{xf}x_t + W_{hf}h_{(t-1)} + W_{cf}C_{(t-1)} + b_f)$ (6)"}, {"title": "", "content": "$g_t = tanh(W_{xc}x_t + W_{hc}h_{(t-1)} + W_{cc}C_{(t-1)} +b_c)$ (7)\n$O_t = \u03c3(W_{xo}x_t + W_{ho}h_{(t-1)} + W_{co}C_t +b_o)$ (8)\n$C_t = f_t \u00a9 C_{(t\u22121)} + i_t 9_t$ (9)\n$h_t = O_t \u00a9 tanh(c_t)$ (10)\nThe final output of the BiLSTM is calculated by concatenating the for- ward and backward hidden states at each time step. Eq. 11 represents the final output.\n$h_t = [\\overrightarrow{h_t},\\overleftarrow{h_t}]$ (11)\nIn our proposed model, BiLSTM has been performed using the input from the input layer with units of 512 and making the return sequence true. Then it is passed through a BN layer to improve the training stability and efficiency of the network. Furthermore, we have used dropout with a dropout rate of 0.1 to prevent over-fitting and improve the generalization performance of our model. The next GFB used the same configuration as the previous GFB except for the dropout rate. In this block, we incorporate the dropout rate is 0.2 in the dropout layer. After that, the output of the dropout layer is passed through the 1D global average pooling layer and merged with the output of the other channel.\nFinally, we concatenated both the final ALFB and GFB together to en- hance the model's performance with better feature representation consisting of hidden local emotional cues as well as long-term contextual relations and dependencies. After merging both the local and global features, it is passed through a dense layer with a unit of 32, followed by a batch normalization layer. The output layer utilizes the softmax activation function to differen- tiate between the emotions present in the speech audio."}, {"title": "4. Dataset and Feature Extraction", "content": "4.1. Dataset\nFor this study, we have used five different datasets: TESS, RAVDESS, BanglaSER, SUBESCO, and Emo-DB which cover English, Bengali, and German languages."}, {"title": "4.2. Data Augmentation", "content": "In the field of SER, data augmentation serves as a valuable technique for expanding the quantity and diversity of training data, which can enhance the effectiveness and generalization of ML-based models. By performing various transformations on the existing data, data augmentation involves producing new training samples. Due to the relatively low number of speech utterance records in each class, this study employs five types of audio data augmen- tation techniques., Additive White Gaussian Noise (AWGN) injection [48], pitch shifting, time-stretching, adding noise on the pitch shifting data, and adding pitch shifting on the time-stretching data. We added AWGN to the samples by using NumPy's uniform method with a rate of 0.015. Pitch shift was performed using the librosa library in Python by employing a factor of 0.7. Speed or Duration has been added without changing the pitch by stretch- ing the time using the method called time_stretch - python's librosa library with a factor of 0.8. We also added noise to the pitch-shifted audio files. In addition, we also shifted the pitch of the audio after stretching the time using the same procedure. Notably, these augmentation techniques were im- plemented without compromising the performance of the SER system. After performing data augmentations, the overall samples of TESS, RAVDESS, BanglaSER, SUBESCO, and Emo-DB datasets increased to 16799, 8637, 8796, 41991, and 3210, respectively."}, {"title": "4.3. Extracted Features", "content": "To enhance the performance of our model, we have extracted various spec- tral features as the initial feature set which include MFCC, mel-spectrogram, ZCR, and RMS values from the audio samples. MFCC is one of the widely used features for the SER task [7, 34]. We focused on MFCCs because they are often used in speech processing and are good candidates for speech feature extraction since they capture the spectrum features of audio signals. They are particularly useful for conveying phonetic and prosodic information be- cause they closely resemble the way the human auditory system reacts to sound. They can aid in capturing variations in speech that express emotion, like shifts in pitch, intonation, and timbre, in the process of recognizing emo- tions [49]. The extraction process relies on the inherent characteristics of the human auditory system, which serves as a natural reference for speech recog- nition. To obtain MFCC, the speech frame undergoes an initial application of a hamming window. Subsequently, the discrete spectrum is computed using"}, {"title": "", "content": "a discrete fourier transform (DFT) [50]. MFCC can be calculated through Eq. 12, where N is the sample length, h(n) is the hamming window and K\nis the length of the DFT.\n$Yi(k) = \\sum_{n=1}^{N} Yi(n)h(n)e^{\\frac{-j2\u03c0\u03ba\u03b7}{N}}; 1\u2264 k \u2264 N$ (12)\nThe human auditory system perceives frequencies on a logarithmic scale, and the mel-spectrogram is a logarithmic representation of signal frequen- cies that share this characteristic. Mel-spectrogram visualizes the power of frequency bands over time, showing the relative importance of different frequency bands, similar to how our ears perceive sound [51]. We use the mel-spectrogram to depict the spectrum features of speech. With MFCCs, it is a helpful function that is commonly used. Mel-spectrograms can highlight important frequency components and acoustic patterns in audio that point to emotional content. Mel-spectrogram helps to identify important information related to the spectral content and the frequency distribution of speech [52].\nThe correlation between the mel spectrum frequency ($f_{mel}$) and the signal frequency ($f_{Hz}$) can be characterized as follows:\n$f_{mel} = 2595. log_{10}(1+\\frac{f_{Hz}}{700})$ (13)\nZCR records the frequency at which the audio stream crosses the thresh- old of zero amplitude. We looked at it because of the information it can provide about how rapidly the audio signal changes and how it relates to speech tempo and prosody. ZCR shifts during emotive speech may be a reflection of rhythmic changes that might convey subtle feelings [53]. In ad- dition, ZCR divides the number of samples in a specific region of an audio frame. After that, it provides the number of zero crossings in that region. Mathematically, the ZCR can be calculated using Eq. 14, where x represents the length of N. For the positive sample amplitude, the a function provides 1 and 0 for the negative sample amplitude over a time frame (t).\n$ZCR_t = \\frac{1}{N-1} \\sum_{i=1}^{N-1} \u03b1 (X_tX_{t-1})$ (14)"}, {"title": "", "content": "where, a =\n[1, $X_iX_{i-1}$ \u22650\n0, $X_iX_{i-1}$ \u22640\nRMS is used to measure the loudness of the sound. RMS is widely used by researchers for the SER task that uses the magnitude of the audio signal [7, 54]. It provides information about the intensity of the speech signals. It measures the average power of an audio signal. It is practical for spotting variations in loudness and intensity, which may be a sign of various emotional states [55]. RMS can be calculated by considering the square root of the sum of the mean squares of the amplitudes of the sound samples. Eq. 15 represents the formula of RMS.\n$X_{rms} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} x_i^2}$ (15)\nIn this study, we extracted a total of 20 MFCC features, 128 Mel-spectrogram features, as well as one RMS and one ZCR feature. These features are com- bined to create a feature vector with a dimension of 3210 \u00d7 150."}, {"title": "5. Experimental Analysis", "content": "In this section, we discuss about the Experimental Setup, and Hyperpa- rameter Tuning for the model's optimization."}, {"title": "5.1. Experimental Setup", "content": "This section focuses on the setup of the proposed model. The specific details of our model are provided in Table 1. In order to train the model, we needed to augment the dataset as there were not enough training examples. We used a 60:25:15 ratio to partition the augmented dataset, with 60% of the data used for training the model, 25% for validation, and the remaining 15% for testing the model. We set the batch size to 32 and the learning rate at 0.0001 which is selected by the hyperparameter tuning. The model was trained for 100 epochs in each dataset. For training, the system used an NVIDIA Tesla P100 GPU, 16 GB of RAM, and the TensorFlow backend. The proposed model has a total of 9.14 million trainable parameters. To train the"}, {"title": "5.2. Hyperparameter Tuning", "content": "For fine-tuning our proposed model, we have chosen Keras Tuner [56] as the hyperparameter tuner. This library is specifically developed for opti- mizing hyperparameters in Keras. It provides a simple and flexible API for automatically searching the hyperparameters of the deep learning model to enhance its performance. Table 2 represents the hyperparameters used in the ALFBs."}, {"title": "6. Result Analysis", "content": "In this section, we analyze the achieved results of the proposed model and provide an in-depth comparative analysis with other SOTA existing methods."}, {"title": "6.1. Evaluation Metrics", "content": "In order to measure the performance of our model we have chosen preci- sion, recall, F1 score, and accuracy.\nPrecision: Precision is a performance measure that evaluates the effec- tiveness of a classification model [57]. It quantifies the ratio of true positive predictions to the total number of positive predictions made by the model."}, {"title": "", "content": "In the form of an equation, precision can be written as:\n$Precision = \\frac{\u03a4\u03a1}{TP + FP}$ (17)\nWhere TP stands for true positives (cases that were correctly labeled as\npositive) and FP stands for false positives (incorrectly classified positive in- stances).\nRecall: The concept of recall [58] can be represented mathematically as follows:\n$Recall = \\frac{\u03a4\u03a1}{TP+FN}$ (18)\nWhere TP stands for true positives (cases that were correctly labeled as positive), and false negatives, are indicated by FN (incorrectly classified neg- ative instances).\nF1-score: The F1-score [59] is a single metric that provides a balanced measure of the effectiveness of a classification model by combining both pre- cisions and recalls into a single value. The F1 score can be stated as an equation, which looks like this:\n$F1-score = \\frac{2 * precision * recall}{precision + recall}$ (19)\nWhere precision and recall are the respective values obtained from the con- fusion matrix of the classification model.\nK-fold cross-validation: K-fold cross-validation [60] is commonly used to evaluate ML models. It divides the data into k equal-sized segments, trains the model on k-1, and evaluates the remaining subsets. Each subset is tested once. Validation accuracy is used to monitor the performance of the model during training and to modify its hyperparameters. For this study, we have performed a 5-fold cross-validation to evaluate the model's overall performance. Mean accuracy can be expressed as an equation:\n$The mean accuracy = \\frac{1}{K} \\sum_{i=1}^{K} E_i$ (20)"}, {"title": "6.2. Performance Analysis", "content": "We have utilized five benchmark datasets such as TESS, RAVDESS, BanglaSER, SUBESCO, and Emo-DB. Table 5 represents the five folds cross- validation for each data set and reports validation accuracy, precision, re- call, F1-score, and mean accuracy for each dataset. We have taken the weighted average precision, recall, and F1-score as the evaluation metrics for our model. The mean validation accuracy for the TESS dataset is 99.65%, the mean precision is 99.65%, the mean recall is 99.65% and the mean F1- score is 99.65%. The mean validation accuracy for the RAVDESS dataset is 94.88%, the mean precision is 95.09%, the mean recall is 94.87%, and the mean F1-score is 94.89%. The mean validation accuracy for the BanglaSER dataset is 98.12%, mean precision is 98.21%, mean recall is 98.18%, and mean F1-score is 98.18%. The mean validation accuracy for the SUBESCO dataset is 97.94%, the mean precision is 97.70%, the mean recall is 97.69%, and the mean F1-score is 97.69%. The Emo-DB dataset's mean validation accuracy is 97.19%, it's mean precision is 97.4%, mean recall is 97.18% and it's mean F1-score is 97.18%. The mean accuracy of the proposed model in all the datasets is presented in Figure 7.\nThe confusion matrix represents the true positive and true negative values along the diagonal. In addition, the off-diagonal entries represent instances of misclassification or confusion between different classes. Figure 8, repre- sents the confusion matrix of the five benchmark datasets of our proposed model. Our model is comparatively better at classifying different emotion classes. The features of the TESS and Emo-DB datasets are classified well by the proposed model. In most of the datasets, Happy and Sad are perfectly classified by our proposed model. In the SUBESCO dataset, Disgust shows the highest misclassification rate which is confused with Angry and Surprise. The rest of the datasets also show a good classification result in the confusion matrix. Figure 9 illustrates, the training and validation accuracy achieved in our proposed dual-channel multi-lingual speech recognition model on five benchmark datasets. It has been observed that the training and validation accuracy are closely aligned, indicating the successful fitting of the model. The model has reached stability within 100 epochs, as demonstrated in the"}, {"title": "6.3. Comparison with State-of-the-art (SOTA) Approaches", "content": "Tables 6 to 10 present the results of a performance evaluation conducted on five different datasets. Our main objective is to compare the effectiveness of our proposed model against previous SOTA approaches. To ensure a fair comparison, we have prioritized the classification performance of the model over computational efficiency, taking into account factors such as the method- ology employed, the extracted features, and the accuracy of the approaches. Results presented in Table 6, demonstrate that our proposed method out- performs SOTA architectures on the TESS dataset, achieving an impressive classification accuracy of 99.65%. Our model surpasses all SOTA architec- tures in this dataset. In Table 7, when considering the RAVDESS dataset, the 1D-CNN-LSTM-GRU model [7] achieves the highest accuracy among the other models, reaching 95.62%. However, our model performs signifi- cantly better compared to the existing models in this dataset. Turning to the BanglaSER dataset in Table 8, our proposed model achieves significantly improved results compared to all SOTA architectures. For a fair compar- ison, we did not include several papers [69] [70] [71] that classified six or"}, {"title": "6.4. Ablation Study", "content": "We have conducted an extensive ablation study in the Emo-DB and RAVDESS datasets to examine the impact of incorporated components such as ECA-Net in the ALFBs, as well as the number of incorporated GFBs. Table 11, represents the experimental results from the ablation study. Here, the proposed model shows the mean validation accuracy with the mean pre- cision, recall, and F1 score. The rest of the experiments in the ablation study was run two times. Each result shows the mean value from the experiments. The study demonstrates that, if we remove the ECA-Net and GFBs from the proposed architecture, the performance of the proposed model tends to reduce. In the Emo-DB dataset, the proposed architecture without ECA- Net and GFBs achieved an accuracy of 95.04%. Removal of the ECA-Net module from ALFBs reduced the mean accuracy by 0.84%. This observation emphasizes the significance of the ECA-Net module in enhancing the model's performance. Furthermore, when the proposed model was tested without one"}, {"title": "7. Conclusion", "content": "In this paper, a dual-channel attention-guided SER architecture is pro- posed that tries to address the challenge of efficient feature representation in the presence of limited data availability. The proposed model's integra- tion of ECA-Net, 1D CNN, and BiLSTM network performs better in rep- resenting the local salient features as well as global contextual features of emotional speech utterances. Integration of ECA-Net allows the network on capturing salient and interrelated features between different channels. On the other hand, BiLSTM is integrated to capture global features from speech data. Data augmentation techniques were employed to mitigate the issue of data scarcity and enhance classification performance. Through comprehen- sive evaluations of five standard benchmark datasets from multiple languages,"}]}