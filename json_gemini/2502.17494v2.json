{"title": "External Large Foundation Model: How to Efficiently Serve Trillions of Parameters for Online Ads Recommendation", "authors": ["Mingfu Liang", "Xi Liu", "Rong Jin", "Boyang Liu", "Qiuling Suo", "Qinghai Zhou", "Song Zhou", "Laming Chen", "Hua Zheng", "Zhiyuan Li", "Shali Jiang", "Jiyan Yang", "Xiaozhen Xia", "Fan Yang", "Yasmine Badr", "Ellie Wen", "Shuyu Xu", "Hansey Chen", "Zhengyu Zhang", "Jade Nie", "Chunzhi Yang", "Zhichen Zeng", "Weilin Zhang", "Xingliang Huang", "Qianru Li", "Shiquan Wang", "Evelyn Lyu", "Wenjing Lu", "Rui Zhang", "Wenjun Wang", "Jason Rudy", "Mengyue Hang", "Kai Wang", "Yinbin Ma", "Shuaiwen Wang", "Sihan Zeng", "Tongyi Tang", "Xiaohan Wei", "Longhao Jin", "Jamey Zhang", "Marcus Chen", "Jiayi Zhang", "Angie Huang", "Chi Zhang", "Zhengli Zhao", "Jared Yang", "Qiang Jin", "Xian Chen", "Amit Anand Amlesahwaram", "Lexi Song", "Liang Luo", "Yuchen Hao", "Nan Xiao", "Yavuz Yetim", "Luoshang Pan", "Gaoxiang Liu", "Yuxi Hu", "Yuzhen Huang", "Jackie Xu", "Rich Zhu", "Xin Zhang", "Yiqun Liu", "Hang Yin", "Yuxin Chen", "Buyun Zhang", "Xiaoyi Liu", "Xingyuan Wang", "Wenguang Mao", "Zhijing Li", "Qin Huang", "Chonglin Sun", "Shupin Mao", "Jingzheng Qin", "Peggy Yao", "Jae-Woo Choi", "Bin Gao", "Ernest Wang", "Lei Zhang", "Wen-Yen Chen", "Ted Lee", "Jay Zha", "Yi Meng", "Alex Gong", "Edison Gao", "Alireza Vahdatpour", "Yiping Han", "Yantao Yao", "Toshinari Kureha", "Shuo Chang", "Musharaf Sultan", "John Bocharov", "Sagar Chordia", "Xiaorui Gan", "Peng Sun", "Rocky Liu", "Bo Long", "Wenlin Chen", "Santanu Kolay", "Huayu Li"], "abstract": "Ads recommendation is a prominent service of online advertising systems and has been actively studied. Recent studies indicate that scaling-up and advanced design of the recommendation model can bring significant performance improvement. However, with a larger model scale, such prior studies have a significantly increasing gap from industry as they often neglect two fundamental challenges in industrial-scale applications. First, training and inference budgets are restricted for the model to be served, exceeding which may incur latency and impair user experience. Second, large-volume data arrive in a streaming mode with data distributions dynamically shifting, as new users/ads join and existing users/ads leave the system. We propose the External Large Foundation Model (ExFM) framework to address the overlooked challenges. Specifically, we develop external distillation and a data augmentation system (DAS) to control the computational cost of training/inference while maintaining high performance. We design the teacher in a way like a foundation model (FM) that can serve multiple students as vertical models (VMs) to amortize its building cost. We propose Auxiliary Head and Student Adapter to mitigate the data distribution gap between FM and VMs caused by the streaming data issue. Comprehensive experiments on internal industrial-scale applications and public datasets demonstrate significant performance gain by ExFM.", "sections": [{"title": "1 Introduction", "content": "Ads recommendation is an important service provided by online advertising systems, whose model performance can impact user experience. It has been actively studied to enhance model performance by advanced designs Zhu et al. (2024); Zhang et al. (2022); Geng et al. (2022); Wang et al. (2021); Li et al.; Ying et al. (2018) and scaling-up model complexity Zhang et al. (2024a); Pan et al. (2024); Anil et al. (2022), especially after observing the remarkable performance of trillion-parameter models such as GPT-4 Achiam et al. (2023), LLaMa Touvron et al. (2023), etc. However, prior studies often neglect two fundamental challenges in industrial-scale applications:\n\u2022 C1: Restricted training and inference latency for the serving models. Industrial platforms usually have to recursively update the models, and evaluate the prediction score of O(100)~O(100K) ads for each request within a restricted latency. Exceeding latency may degrade model performance and impair user experience.\n\u2022 C2: Large-volume streaming data arrive with data distributions dynamically shifting. This is largely due to the evolving nature of advertising systems: new users/ads join, and existing users/ads leave the system as time passes. This implies that multi-pass training would risk over-fitting and is the most salient difference between industry and academia.\nTo overcome the restricted inference latency challenge in C1, multiple prior studies employ knowledge distillation (KD) Liu et al. (2022); Kang et al. (2021); Kweon et al. (2021); Zhu et al. (2020). As shown by Figure 1(a), a large teacher model is co-trained with a compact student model to be used for serving. The teacher's knowledge is continuously distilled into the student. Unfortunately, the co-training will increase the training computational cost of the serving model, which cannot meet the industrial restriction on the training latency of the serving model. Besides, ads recommendation often involves multiple serving models, each corresponding to one specific service or ranking stage (e.g., early or later/ranking stage). Building and maintaining a dedicated large teacher model for each serving model is too inefficient and non-scalable. To sum up, the ideal teacher model (1) is separately trained with the student model, i.e., external distillation, and (2) is 1-to-N, able to benefit multiple student models, like a Foundation Model (FM).\nThe streaming and non-stationary nature of recommendation data in C2 makes it challenging to realize the ideal teacher model. As shown by Figure 1(b), models need to be recursively trained to capture the up-to-date distribution shifting. Figure 2 illustrates an example of performance degradation under model staleness over internal datasets, where the model is trained with 74-billion examples and stops training with new examples since inference. Compared to the baseline being trained with new examples daily, its inference NE (normalized entropy) He et al. (2014) loss keeps enlarging as the delay inclines. Besides, since the teacher aggregates various students' training data from different services, the distribution of the teacher model's data will be more sophisticated. Those factors lead to two potential distribution gaps between the teacher model and the student models: (1) Cross-domain Bias. For an individual student, the teacher may carry bias due to being trained by not only that student's data but an aggregation of all students' data under distribution shifting. (2) Freshness Gap. The teacher predictions used in external distillation are from a teacher with a slight delay w.r.t. training data compared to the student under training.\nIn light of the above limitations, we propose the External Large Foundation Model (ExFM) framework to address the challenges overlooked by prior studies. Figure 1(c) presents a big picture of the ExFM framework. To avoid additional training or inference computational costs on the serving model, ExFM employs external distillation where the teacher model is separately trained, and the teacher's predictions are offline logged as external supervision for the student training. To amortize the resources needed for training and maintenance of the teacher model, ExFM aggregates the data from multiple student traffic as the training data of the"}, {"title": "2 Problem Formulation", "content": "We re-define the ads recommendation problem to match industrial-scale applications with two overlooked fundamental challenges:\n\u2022 Restricted training and inference latency for serving models. Ideally, the model enhancement should not incur any training or inference computational costs on serving models.\n\u2022 Training and inference data are large volumes of streaming data with continuously shifting data distribution. Models have to keep training on fresh examples to perform well.\nThe new ads recommendation problem is to predict user interest in ads based on their interaction data with dynamic data distribution shift constrained by restricted training and inference latency. Formally, let $F(t)$ be the FM parameters after training $\\Theta_F(t \u2212 1)$ on FM's new data $D_F(t)$ at time t, and $\\Theta_i(t)$ be VM i's parameters after training $\\Theta_i(t \u2212 1)$ on VM's new data $D_V^i(t)$, i = 1, . . ., n."}, {"title": "3 Methods", "content": "3.1 Overview\nFigure 1(c) illustrates the details of the ExFM framework with one model iteration as an example. Specifically, FM and VMs are recursively trained on streaming data $D_F(t)$ and ${D_V^i(t)}_{i=1}^n$ incrementally arrived at time t. To amortize the building and maintenance costs, the teacher model is built as an FM that can make prediction on multiple VMs' traffic. Correspondingly, the training data of FM $D_F(t \u2212 1)$ is obtained by aggregating the cross-traffic VM's training data ${D_V^i(t \u2212 1)}_{i=1}^n$. After training FM model $F(t \u2212 2)$ on $D_F(t \u2212 1)$, FM model parameters are updated to $\\Theta_F (t \u2212 1)$.\nTo not incur additional training or inference costs to VMs, external distillation, as shown in Figure 1(a), is employed. Specifically, Teacher/FM is trained on $D_F(t \u2212 1)$ separately from Students/VMs. After training, Teacher/FM inferences on Students'/VMs' training data ${D_V^i(t)}_{i=1}^n$ to generate predictions as external supervision to Students/VMs. The external supervision, along with the VMs' training data ${D_V^i(t)}_{i=1}^n$ will be"}, {"title": "3.2 Data Augmentation Service (DAS)", "content": "Data Augmentation Service (DAS) logs FM's supervision on the fly when preparing training data for VMs, fulfilled in two stages. As shown by Figure 3(a), first, all VMs' features and true labels are joined together into a shared dataset. There is a large time window to wait for the feedback (i.e., true label) for this training example (e.g., 5 to 90 minutes for CTR feedback, or 1 day for CVR feedback). During this time window, DAS will call FM to get FM's inference on VM's features, which usually has a large latency budget (in seconds level) sufficient to evaluate a large-scale FM. Then each VM picks its own datasets from the shared dataset"}, {"title": "3.3 Auxiliary Head (AH)", "content": "The baseline of knowledge distillation is through label smoothing of the ground-truth label y and the pseudo- label $\\hat{y}^F$ from FM. The model architecture of VM consists of two parts: the backbone and a serving head. We define the output vector of the backbone as x, and x will be sent to the serving head. The output vector of the serving head is defined as $\\hat{y}_S = \\phi(x)$, where $\\phi$ denotes the multi-layer perceptron (MLP). The training objective is binary cross-entropy and can be generally defined as\n$h(\\hat{y}, y) = y \\cdot log (\\sigma(\\hat{y})) + (1 \u2212 y) \\cdot log (1 \u2212 \\sigma(\\hat{y})),$ (1)\nwhere $\\sigma(x) = 1/(1 + exp(-x))$ denotes the Sigmoid activation function and $\\hat{y}$ denotes the prediction. Accordingly, the training loss of VM with knowledge distillation is defined as\n$L_{kd}(\\hat{y}_S, \\hat{y}_F,y) = h(\\hat{y}, y) + h(\\hat{y}_S, \\hat{y}^F)$ (2)\nwhere $\\hat{y}_S$ denotes the output of the serving head. The gradients with respect to y and $\\hat{y}_F$ are entangled in the serving head and flowing back to the backbone.\nUsing a single head to consume both labels may induce bias from FM to VM. Therefore, we propose the Auxiliary Head (AH) that leverages a separate head to consume the pseudo-label from FM. In this case, the model architecture of VM consists of the backbone, the serving head, and a distillation head as AH. The"}, {"title": "3.4 Student Adapter (SA)", "content": "output of the backbone x will be sent to different heads, and the output of AH is from the distillation head $\\hat{y}_D$, where $\\hat{y}_D$ is defined as $\\hat{y}_D = \\psi(x)$. $\\psi$ represents an additional MLP head which is built on top of the backbone. By disentangling the serving head and distillation loss, the serving head is only supervised by the ground-truth label y as\n$L_s(\\hat{y}_S,y) = h(\\hat{y}_S,y).$ (3)\nThe distillation head is supervised by the pseudo-label $\\hat{y}_F$ as\n$L_a(\\hat{y}_D, \\hat{y}_F) = h(\\hat{y}_D,\\hat{y}_F).$ (4)\nWe further provide the theoretical insight that, compared to adding the auxiliary loss to the serving task for distillation, we prove that integrating the auxiliary head for knowledge distillation (KD) can alleviate the bias transfer from FM to VMs.\nTheorem 3.1. (Informal) The VM will contain bias from the FM when the VM is trained with KD by a single serving head. In contrast, the VM is guaranteed to find the optimal solution for predicting $y_{gt}$ with KD by auxiliary heads.\nRemark. Calibration is one of the most critical metric in ads recommendation system. When we perform knowledge distillation with single serving head, the introduced bias will lead to mis-calibration issue, while separate head can guarantee the calibration will not be effected. More specifically, if the teacher model is mis-calibrated, the bias transfer from the teacher model from a single serving head can lead to mis-calibration in the student model with high probability. In contrast, distillation with an auxiliary head mitigates this issue by reducing the propagation of teacher model's bias through the separated serving head.\nThe proof of Theorem 3.1 in the Appendix Sec. A.2, where we prove by construction using a two-layer linear model to show that (1) the bias in the pseudo labels from the FM is partially compensated by the separated prediction head for KD, and (2) fewer \u201cbiased\" gradients will flow from the prediction head for KD to the shared backbone, leading to a better-generalized performance than the single serving head for KD.\nHowever, only isolating the supervision from $\\hat{y}_F$ to y is not enough to bring effective knowledge transfer in practice. This is due to the issues originating from the intrinsic properties of the training data of Ads engagement, i.e., the majority of engagement data does not convert to actions like clicking; thus, the FM primarily predicts majority of $\\hat{y}_F$ close to zero, making the KD task fitting to a difficult long-tailed distribution. These practical issues hinder the effectiveness of the KD with AH.\nTo remedy the above issue, we propose amplifying the FM's distillation effect by Gradient Scaling (GS), Label Scaling (LS), and Loss Weighting (LW). The gradient scaling scales the gradient of the distillation head to backbone with a hyperparameter \u03b2 during training to enlarge the impact of the distillation from the FM.\nFor label scaling, we multiply the $\\hat{y}$ with a constant a to enlarge its magnitude and clip the $a \\cdot \\hat{y}^F$ correspondingly to avoid the scaled label being out-of-bound, e.g., $a \\cdot \\hat{y}_{FM}$ needs to be smaller or equal to 1. Lastly, we apply the loss weighting to $L_a$ with a hyperparameter w. Accordingly, the final training loss $L_{ah}$ for VM with AH is:\n$L_{ah} = L_s(\\hat{y}_S, y) + w * L_a(\\hat{y}_D, a \\cdot \\hat{y}^F)$ (5)\nTo further alleviate the bias from the $\\hat{y}_F$, we propose to provide additional distillation using the adapted $\\hat{y}_F$ based on y, called Student Adapter (SA). The output of SA is defined as $\\hat{y}^{SA} = MLP(\\hat{y}^F)$. As shown in Figure 4 (b), SA is trained by the loss\n$L_{sta}(\\hat{y}^{SA}, y) = h(\\hat{y}^{SA}, y)$ (6)\nWhen training VM with SA, we first optimize $L_{sta}(\\hat{y}^{SA}, y)$ to update SA and get $\\hat{y}^{SA}$. To use $\\hat{y}^{SA}$ for training VM, we stop the gradient of $\\hat{y}^{SA}$, i.e., SG($\\hat{y}^{SA}$), to avoid the gradient of VM flowing back to SA, and we consume SG($\\hat{y}^{SA}$) with another distillation head to update VM with the following loss\n$L_{sa}(\\hat{y}_D, SG(\\hat{y}^{SA})) = h(\\hat{y}_D, SG(\\hat{y}^{SA})).$ (7)"}, {"title": "4 Experiments", "content": "We evaluate the proposed ExFM on both internal industrial-scale and public datasets to answer the following research questions:\n\u2022 Q1: How effective is the proposed ExFM in elevating the performance of a single VM and multiple VMs, e.g., from different tasks, domains or stages? (Sec. 4.2)\n\u2022 Q2: How effective are the proposed Auxiliary Head and Student Adapter? (Sec. 4.3)\n\u2022 Q3: How do the values of hyperparameters impact the ExFM performance? (Sec. 4.4)\nFor better readability, each part of discussion will first cover internal datasets, then public datasets. The caption of each figure and table starts from [Internal] if obtained on internal datasets and [Public] on public datasets.\n4.1 Experiment Setup\n4.1.1 Datasets\nWe conduct comprehensive experiments on internal industrial-scale datasets with billions of training examples, from ads CTR prediction tasks. Each training example contains O(1k) features for a user and ad pair, and"}, {"title": "4.2 Effectiveness of FM on VMs", "content": "enjoys more advanced and resource intensive model architecture such as an internal version of Interformer Zeng et al. (2024), SUMZhang et al. (2024b), Wukong Zhang et al. (2024a), DHEN Zhang et al. (2022), etc., while VM adopts a shrunk version of FM or less resource intensive structure such as DLRM Naumov et al. (2019).\nOn public datasets, we use the models that attain SOTA performance on TaobaoAd, Amazon Electronics, and Kuai Videos as FMs based on BARS Zhu et al. (2022) benchmark. Specifically, we use DMIN Xiao et al. (2020), DCN Wang et al. (2017), and DCNv2 Wang et al. (2021) as different variants of FM. Regarding VMs, Factorization Machine (FaM) Rendle (2010) and its two variants, i.e., FmFaM Sun et al. (2021) and DeepFaM Guo et al. (2017) are considered, as they are less complex and low latency to SOTA. For instance, the parameter of FaM is 4.23M on Amazon Electronics while DMIN is 5.94M, with a near 30% parameter decrease. Our purpose is to study the delta impact of FM on VMs under different choices of the two. More details of the models and training settings are in Appendix A.1.\n4.2.1 One FM for One VM\nOn internal datasets, we evaluated the impact of different-sized FMs on elevating VM performance. As shown by Figure 5, FM model size ranges from 30X to 1800X where 1X = 60M training FLOPs. FM model parameter number ranges from 0.8T to 3.2T where 1T is 1 Trillion. FM scale-up is employing advanced model arches with more features and larger embedding dimensions. VMs are all 30X and from past halves' real iterations, e.g., 23H1 implies the iteration of the first half in 2023. VMs are iterated by refreshing features and adopting micro model arch changes. The baseline of both FMs and VMs is the VM at 22H2. NE Gain % refers to the percentage of NE improvement attained vs. baseline. \"VM alone\" NE Gain % means the NE improvement of VM obtained without FM. Cumulative gain means the number includes the gain since 23H1. Both FM and VMs are trained with > 300B streaming data and inference on the next-day's new arriving data. Although both VMs and FM are changing from half to half, the value of FM on VM gain is obtained by apple-to-apple comparison. We observe that (1) FMs can generate consistent NE gains on VMs from half to half, (2) compared to the trend of the world without FM (blue trendline), the one with FM has steeper slope (red trendline), implying the role of FM on bending the trending curve.\nOn public datasets, we evaluated the impact of FM on VMs under different FM-VM pairs in Table 2 and 3. Data is described in Sec. 4.1.1 and VM is trained on the 5th day with FM's supervision and inference on the 6th day to get the AUC/LogsLoss reading. Note that the original performance of VMs without FM is limited, because the VMs' model is chosen not to have any SOTA and complex model architectures for the consistency with the problem settings. In the table, \u201cdistill w/ Lah + Lsa\" means w/ FM, and \"w/o distill\" means w/o FM. From the table, we observe consistent AUC and Logloss improvement from having FM across different FM-VM choices, demonstrating that the effectiveness of ExFM can hold in general.\n4.2.2 One FM for N VMs"}, {"title": "4.3 Effectiveness of of AH and SA", "content": "Early, and Later stages, each of which reduces ads candidates, from O(100k) to O(1k), from O(1k) to O(100), and from O(100) to O(10), respectively. Specicially, we use the 1000X, 3.2T FM in Figure 5 and VMs from three stages for experiments on internal dataset. Retrieval VM and Early stage VM employ the Two-Tower Sparse Network (TTSN) Wang et al. (2019), and Later stage VM uses DLRM Naumov et al. (2019) as the corresponding VM in Figure 5. Training and inference data preparation also follow those of Figure 5. In Figure 6, we observe that the FM brings 0.11% to 0.25% additional NE gain to VMs of different stages. The earlier a stage is, the more gains the FM generates.\nTo verify the benefits of FM to multiple VMs from different domains, we split the TaobaoAd data into different domains as described in Sec. 4.1.1. To verify the same thing for different tasks, we use the Kuai Video dataset where multiple types of feedback are available for prediction. We use the hard-parameter shared DMIN Navon (2024) as the FM and FaM Rendle (2010) dedicatedly trained from scratch for each domain or task as the VM. Table 4 and Table 5 illustrate the results for different domains and tasks separately. In both tables, we observe that w/ FM outperforms the w/o FM in all tasks and domains, demonstrating the effectiveness of FM for multiple VMs.\nTo understand the role of AH in the success of ExFM, on internal datasets, we compare the VM's NE after including AH (Eqn. 5) vs. w/o AH (Eqn. 2) under the same 1000X, 3.2T FM and corresponding VM from Figure 5. Figure 7 provides the comparison, where the x-axis is the number of streaming training examples and the value of NE Change at each point is obtained by using the current snapshot to inference on next-batch data, thus equivalent to the inference NE, but in a streaming mode. Since NE is the lower the better, the negative 'NE Change %' (-4%) implies that AH brings a large and stable performance gain compared to baseline. Similarly, to understand the role of SA, we compare the VM's NE after including SA vs. w/o SA under the same 1800X, 2.2T FM, and corresponding VM from Figure 5. Figure 8 shows the comparison, and we observe that with SA, the NE performance of VM on streaming data improves by 0.08%, with an enlarging trend, which is considered significant.\nTo reproduce the results on public datasets, we split the dataset as discussed in Sec. 4.1.1 by timestamps to simulate the streaming setting. We apply AH and SA over different pairs of public models on public datasets as described in Section 4.1.3, and the performance of VMs is reported in Table 2 and 3. Specifically\n\u2022 distill with $L_{kd}$ (Eqn. 2), i.e., the distillation loss\n\u2022 distill with $L_{ah}$ (Eqn. 5), i.e., the AH\n\u2022 distill with $L_{ah} + L_{sa}$ (Eqn. 7), i.e., AH and SA.\nWe observe that the AH and SA are consistently superior to the baseline with $L_{kd}$ among different pairs of FMs and VMs on various datasets, demonstrating their strong effectiveness in enhancing FM benefits on VMs. As the breakdown, it is found that (1) the vanilla distillation w/o AH or SA has very small benefits, e.g.,"}, {"title": "4.4 Impact of hyper-parameters", "content": "0.35% AUC gain for DMIN as FM and FaM as VM on TaobaoAd, (2) the major leap is by AH, e.g., boosting the AUC gain from 0.35% to 1.11%, and (3) SA further brings significant gains on top of AH, e.g., lifting the AUC gan from 1.11% to 1.35%.\nThe data points in Table 2 and 3 are obtained by one-time inference on the 6th day's data. We are curious about the performance of SA with a bigger freshness gap between FM and VM. To study this, we use DMIN as FM and FaM as VM, stop the FM from being trained on the new day's data, and compare their performance w/ and w/o SA in Figure 9. We observe that (1) the benefits of FM on VM diminish as the time that FM stops training on new data is longer, (2) SA can generate additional gain no matter whether FM updates or not, but (3) SA cannot revert the diminishing trend when the FM is not updated with new day's data.\nWe notice that the benefits of FM on VM may be sensitive to the choice of GS, LS, and LW as defined in Eqn. 5. On internal datasets, Figure 10 demonstrates the positive impact of increasing LW and LS. We observed that the benefits of FM on VM are improved correspondingly when we increase the value of LW and LS with others fixed. Figure 11 shows the impact of GS, where there seems to be a sweet spot. The NE gain first increases significantly when GS increases and then tends to plateau when GS is large enough.\nOn the internal dataset, the test for the combination effect of LW, LS, and GS is often restricted by the limited training resources. We explore that on the public dataset. Figure 12 illustrates our experiments on TaobaoAd: (1) The simple combination, i.e., (LS = 1, GS = 1, LW = 1), can only achieve moderate improvement on the VM. (2) Aggressive scaling, e.g., (LS = 10, GS = 10, LW = 10) or (LS = 1,GS = 1, LW = 100), will even be detrimental to the VM's performance. Both observations imply that an appropriate combination of these"}, {"title": "5 Related Works", "content": "Inspired by the success of large-scale model in NLP Achiam et al. (2023); Touvron et al. (2023), more and more recent studies have focused on scaling-up recommendation model capacity for better performance Zhang et al. (2024b,a); Fang et al. (2024); Pan et al. (2024); Shin et al. (2023); Anil et al. (2022). One fundamental challenge to employ them in industrial-scale applications is the restricted training and inference budget for serving models. To overcome that, multiple prior studies explored the adaption of knowledge distillation (KD) Hinton (2015) in recommendation problems, where a large teacher model continuously supervises a compact student model by co-training and finally only the student model is used for serving Kang et al. (2024, 2023); Chen et al. (2023); Liu et al. (2022); Kang et al. (2021); Kweon et al. (2021); Zhu et al. (2020); Tang and Wang (2018). One limitation of those studies is that they primarily focused on static environments, while in industrial applications, large-volume data continuously arrive. Lee et al. (2024) proposed to handle that by a continual learning framework. Another limitation is that those studies follow co-training based distillation. It will increase the training cost of the serving model, as well as the risk of model staleness due to iteration delay, implying performance loss when new incoming data has distribution shifting. Khani et al. (2024) proposed to handle that by external distillation where teacher training happens separately from supervising students.\nAmong existing studies, ours are closest to Lee et al. (2024) and Khani et al. (2024). Compared to Lee et al. (2024), our study has the following salient differences: (1) Different settings for the teacher model. To amortize building and maintaining resources, the teacher model in ExFM is an FM that uses an aggregation of student models' feature sets. As a result, the teacher itself often has feature gap and dimension mismatch when compared to an individual VM, so ExFM does not generate student models from the teacher model like Lee et al. (2024). (2) Teacher update does not depend on students. Lee et al. (2024) instead couples the teacher model's update with students' by replay learning. Such dependency will increase overhead of teacher update and bring in staleness, as illustrated by Figure 2, implying a risk of huge performance loss. (3) Consideration of data distribution shifting in an industrial streaming setting. We develop Student Adapter to handle that and provide theoretical guarantees. Compared to Khani et al. (2024), our differences and contributions are as follows. Its auxiliary distillation looks similar to our AH, but our AH is an isolated task arch that consumes supervision from FM in a dedicated fashion, not simultaneously consuming true labels like in Khani et al. (2024). Khani et al. (2024) lacks theoretical proof to justify its proposal and does not provide a solution to mitigate the distribution gap between FM and VMs due to data distribution shifting. Instead, we develop and prove Student Adapter can achieve the goal. Moreover, Khani et al. (2024) does not contain sufficient details and lacks comprehensive benchmark experiments on external datasets."}, {"title": "6 Conclusion", "content": "In this paper, we propose the ExFM framework to address two fundamental but overlooked challenges by prior studies on scaling-up Ads recommendation models \u2013 (C1) restricted training and inference budget and (C2) streaming data with distribution shifting. To overcome C1, ExFM employs the external distillation and the data augmentation service, where teacher training separates from student training and one teacher can supervise multiple VMs, like an FM. To alleviate the distribution gap between FM and VMs caused by C2, ExFM proposes Auxiliary Head (AH) and Student Adapter (SA), with mathematical guarantees on benefits provided. ExFM, including its core techniques such as AH and SA, achieved outstanding performance on industrial-scale datasets over multiple tasks and stages. It enabled the serving of a trillion-parameter model without increasing serving latency or negatively impacting user experience, and established the capability to serve LLM-scale ads model in the future. We also experimented on public datasets and were able to reproduce ExFM's outstanding performance, further demonstrating the effectiveness of the proposed framework."}, {"title": "A Appendix", "content": "A.1 Details of models and training configurations\nWe use the BARS benchmark Zhu et al. (2022) and the FuxiCTR Zhu et al. (2021) to implement all the public models. We follow the default training and model configurations (e.g.", "models": "for an input x, they output d - 1 soft labels as $w_k^T Zx$, k = 2,...,d. For simplicity, we assume that $w_k = u_1 + \\mu u_k$, $k \\in [d"}]}