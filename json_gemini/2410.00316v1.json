{"title": "Emoknob: Enhance Voice Cloning with Fine-Grained Emotion Control", "authors": ["Haozhe Chen", "Run Chen", "Julia Hirschberg"], "abstract": "While recent advances in Text-to-Speech (TTS) technology produce natural and expressive speech, they lack the option for users to select emotion and control intensity. We propose EmoKnob, a framework that allows fine-grained emotion control in speech synthesis with few-shot demonstrative samples of arbitrary emotion. Our framework leverages the expressive speaker representation space made possible by recent advances in foundation voice cloning models. Based on the few-shot capability of our emotion control framework, we propose two methods to apply emotion control on emotions described by open-ended text, enabling an intuitive interface for controlling a diverse array of nuanced emotions. To facilitate a more systematic emotional speech synthesis field, we introduce a set of evaluation metrics designed to rigorously assess the faithfulness and recognizability of emotion control frameworks. Through objective and subjective evaluations, we show that our emotion control framework effectively embeds emotions into speech and surpasses emotion expressiveness of commercial TTS services.", "sections": [{"title": "1 Introduction", "content": "The complexity of human communication extends far beyond mere verbal exchange. Vocal inflections and emotional undertones play pivotal roles in conveying meaning. While text alone can be ambiguous in meaning (Jenkins, 2020), different emotions in voices can articulate different messages in the same piece of text (Nygaard and Lunders, 2002). Consider Shakespeare's iconic phrase, To be or not to be. This line can express despair, contemplation, defiance, or resignation, depending on the speaker's emotional delivery, illustrating the profound impact of vocal emotions in communication.\nThe ultimate objective in the field of conversational systems is to develop intelligent agents capable of comprehending, deciding, and synthesizing speech with nuanced emotional undertones. While recent advances in Text-to-Speech (TTS) technology have achieved remarkable naturalness and expressiveness in synthesized voices(ElevenLabs; OpenAI, 2024b; Microsoft), these systems lack the capability for users to select and control the emotional tone and intensity. The emotion conveyed in the generated speech is solely determined by the text, without allowing for variability or intensity control.\nPrevious works on emotion control in speech synthesis primarily focus on a few simple emotion categories (Lei et al., 2022; Lorenzo-Trueba et al., 2018; Kang et al., 2023; Qin et al., 2024). These methods do not allow control of a more diverse array of emotions. Synthesis for more complex and heterogeneous emotions like charisma (Yang et al., 2020) and empathy (Chen et al., 2024) is not well studied.\nOur work leverages recent breakthroughs in foundation models for voice cloning (MetaVoice; Anastassiou et al., 2024; suno.ai, 2023; Casanova et al., 2024; Shen et al., 2018). By exploring the rich expressiveness in these models' latent embedding spaces, we develop methods to extract a representation for any emotion with just a few demonstrative samples. These representations are inherently synergistic with the speech generation capabilities of rapidly advancing voice cloning/TTS models, enabling us to generate high quality speech while applying fine-grained emotion controls. This approach proves effective for both simple and complex emotions and includes mechanisms to adjust emotional intensity with a scalar knob.\nOur framework's capability of applying fine-grained emotion control for any emotion with a few demonstrative examples enables us to propose two methods for applying emotion control based"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Foundational Model for TTS and Voice Cloning", "content": "Large foundational models have become the basis of many machine learning fields such as text (OpenAI et al., 2024) and images (Radford et al., 2021). These large foundational models are trained in an unsupervised manner with massive datasets and learn high quality representations of data, which are commonly used directly or through fine-tuning for downstream tasks.\nThe TTS domain also sees a rising trend in large, foundational models. These end-to-end models trained on large corpora provide natural speech rendering from text. MetaVoice trains a 1.2B parameter model with 100K hours of speech for TTS; Lajszczak et al. (2024) trains a 1B parameter model on 100K open-domain speech data. Many of these models are capable of replicating a speaker's voice in zero-shot or few-shots (MetaVoice; Anastassiou et al., 2024; suno.ai, 2023; Casanova et al., 2024; Shen et al., 2018). Our work explores how to leverage the high quality speaker representation learned by these foundational models to enhance voice cloning with few-shot fine-grained emotion control. In particular, we focus on manipulating the latent speaker embedding in MetaVoice."}, {"title": "2.2 Emotion and Style Control in Speech Synthesis", "content": "While models discussed in Section 2.1 and existing commercial services (OpenAI, 2024b; Microsoft; ElevenLabs) produce natural sounding speech, their speech output's emotions are primarily decided by input text, and the emotion strength cannot be controlled. Users thus cannot select arbitrary emotions for a piece of text. However, emotions expressed through acoustic-prosody serve an important additional channel for conveying information (Gobl and Chasaide, 2003; Patel et al., 2011; Laukkanen et al., 1997).\nPrevious work trains latent speech style space on small corpora and cannot generalize to style transfer beyond the training corpus (Zhang et al., 2019). In addition, existing labeled emotional speech datasets (Martinez-Lucas et al.; Poria et al., 2019) are limited to a few categories of basic emotions. Previous work thus commonly bases emotion controls on categorical emotion label inputs and is limited in types of emotions that can be controlled (Lei et al., 2022; Lorenzo-Trueba et al., 2018; Kang et al., 2023; Qin et al., 2024). Extending these methods to control new emotions require extensive retraining of models, preventing expressive emotion control over many emotions. These methods' requirement on large labeled datasets also prevents emotional control on more complex, nuanced emotions represented by more specialized, heterogeneous datasets such as charisma (Yang et al., 2020) and empathy (Chen et al., 2024).\nWhile Jo et al. (2023) uses domain adversarial training to achieve few-shot emotion transfer, their method requires training a style encoder built from scratch and is not compatible with existing and future large foundational models. Thus, it is unable to improve naturalness and expressiveness in current and future TTS model developments. Our work provides a training-free framework that leverages a foundation model's TTS capability for single/few-shot emotion control and is inherently synergetic with growing foundation speech models."}, {"title": "2.3 Open-ended Text Prompt Control on Voice", "content": "A recent strand of works use text description to control voices. Guo et al. (2022); Yang et al. (2023); Lacombe et al. (2024); Lyth and King (2024) allow users to describe qualities such as tone, pitch, gender, and emotions of a voice before synthesizing speech with the described voice. While existing speech datasets lack text descriptions of voices, this work bypasses this obstacle by creating synthetic text captions based on acoustic-prosodic features and speaker metadata. These methods do not generalize well to text descriptions beyond the format and the scope of the synthetic captions. The emotion control with these methods is limited to the categorical emotion labels in speaker metadata. These methods also do not allow voice cloning and emotion variation on an unseen speaker. Based on our method's capability to enhance voice cloning with single/few samples, we propose retrieval and synthetic data based frameworks for synthesizing expressive emotions with open-ended text descriptions."}, {"title": "3 Methods", "content": "We apply fine-grained emotion controls by manipulating the speaker embedding space of pre-trained foundation voice cloning models. This framework allows us to apply emotion control with few-shot emotional speech samples. The few-shot capability enables us to design two frameworks for applying control with emotion specified by arbitrary text descriptions."}, {"title": "3.1 Preliminaries: Pre-Trained Foundational Voice Cloning Model", "content": "Existing voice cloning models (MetaVoice; Anastassiou et al., 2024; suno.ai, 2023; Casanova et al., 2024; Shen et al., 2018) can be abstracted into a two-stage architecture with a speaker encoder E that takes in a speaker reference clip xs and outputs a speaker embedding us. A conditional text-to-speech decoder D then takes in input text I to output speech audio ys,I = D(us, I) that utters I replicating speaker's voice. We will manipulate the speaker embedding space (output space of E and conditional input space of D) to obtain an emotion representation and obtain few-shot emotion control."}, {"title": "3.2 Few-Shot Fine-Grained Emotion Control", "content": "We hypothesize that a pre-trained foundation voice cloning model's speaker embedding provides expressive representations for acoustic-prosodic qualities. Our framework disentangles how an speaker embedding represents speaker-specific qualities and speaker-independent emotions. We then use the speaker-independent emotions obtained to apply fine-grained emotion control on an arbitrary speaker representation. We show this process for few-shot fine-grained emotion control in Figure 2.\nWe disentangle speaker-specific qualities and speaker-independent emotion representations by using paired samples of emotional speech x and neutral speech xm from the same speaker. We encode representations u, un for these i-th pairs of samples in a speaker embedding space with the pre-trained speaker encoder E: $u_i^e$ = E(x), un = E(xm).\nWe hypothesize that taking their difference results in a speaker-independent emotion direction vector ve. In addition, we normalize $v_e^i$ for convenient fine-grained emotion strength control later:\n$\\hat{v}_e^i = \\frac{u_i^e - u_n^i}{||u_i^e - u_n^i||}$\nWe can obtain the emotion direction vector by averaging over many pairs of samples. We will show in experiments that single-shot (N = 1) suffices to produce high-quality emotion control in many cases:\n$v_e = \\frac{1}{N} \\sum_{i=1}^{N} \\hat{v}_e^i = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{u_i^e - u_n^i}{||u_i^e - u_n^i||}$\nGiven a new speaker reference sample xs, we hope to replicate the speaker's voice qualities while controlling emotions in an utterance. We first obtain the reference speaker's speaker embedding with us = E(xs). Then, we apply emotion control with\nus,e = us + \u03b1 \u00b7 ve\nwhere emotion control strength \u03b1 is a scalar that enables fine-grained control of emotion intensity. We hypothesize that larger \u03b1 values lead to more intense emotions in the speech produced.\nFinally, we use pre-trained decoder D to synthesize ys,I,e, a speech utterance of text I replicating speaker s's voice while conveying emotion e."}, {"title": "3.3 Towards Open-Ended Text Prompted Emotion Control", "content": "Our framework's ability to apply emotion controls with the few-shot demonstration allows us to design two frameworks that take in open-ended text description of an emotion and apply fine-grained control on output speech for the specified emotion. These frameworks allow synthesis of speech with emotions such as Romantic, full of desire and Grateful, appreciative, thankful, indebted, blessed that are nuanced in details and lack existing datasets. Both frameworks take advantage of recent development in LLMs to overcome the lack of a labeled emotional speech dataset."}, {"title": "3.3.1 Synthetic Data-Based Method", "content": "While existing TTS models and services do not allow emotion control, they produce expressive and accurate emotions for texts that obviously convey the emotions (OpenAI, 2024b; Microsoft; ElevenLabs). We leverage this quality to generate synthetic emotional samples that can be used for emotion control with our framework. We show this process in Figure 3(a).\nGiven a text description T of an emotion e, we prompt an LLM to generate N text samples Il...,N, that obviously convey the emotion: $I_{1,...,N}$ = LLM(T). Prompted with prompts such as Generate 10 sentences that someone would say when feeling [emotion], LLM generates emotional texts that conveys emotion e. Then, we use expressive commercial TTS services to obtain an emotional speech sample xi: $x_i^e$ = TTS(x).\nWe can obtain neutral audio samples with the same procedure by first prompting LLM with prompts such as Generate 10 simple fact statements to generate neutral texts. Then, we can obtain the neutral audio samples xm with the TTS services.\nWe can then use the emotional speech samples obtained $x_i^e$ and $x_i^m$ with our few-shot emotion control framework to apply fine-grained emotion control on new speakers."}, {"title": "3.3.2 Transcript Retrieval-Based Method", "content": "While a synthetic-data-based method enables open-ended emotion control while bypassing the lack of captioned emotion datasets, the high cost of expressive TTS services limits the framework's wide usage. In this section, we hypothesize that in existing datasets with speech-transcript pairs, transcripts that obviously convey an emotion are matched with audio clips that convey the emotion. We leverage recent developments of text embedding models and document retrieval pipeline to find emotional audio samples that we can use for few-shot emotion control. We show this pipeline in Figure 3(b).\nGiven a text description T of an emotion e and a text embedding model M, we retrieve transcript-audio pairs ($I_j^2$, $x_j^2$) in a dataset such that the transcript $I_j^2$ best matches the emotion description: j = arg maxj M($I_j^2$)M(T).\nWe can find neutral samples $I_n^2$ either with the same retrieval pipeline or neutral labels in the dataset, which are more widely available than diverse emotion labels."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Evaluation Metrics", "content": ""}, {"title": "4.1.1 Subjective Evaluations", "content": "Given the novelty of fine-grained emotion control in text-to-speech synthesis, there is not an established paradigm for examining this capability. To rigorously test the objective of providing fine-grained, faithful emotion control, we proposed the following subjective evaluation metrics:"}, {"title": "4.1.2 Objective Evaluation", "content": "Since our goal is to preserve source speaker identity and maintain accurate text-to-speech synthesis while conducting emotion control, we follow previous voice cloning work (Anastassiou et al., 2024; Shah et al., 2023) on measuring word error rate (WER) and speaker similarity (SIM). We use 100 texts from Common Voice dataset (Ardila et al., 2020) to calculate WER and SIM.\nFor WER, we first transcribe the generated clips with Whisper-large-v3 (Radford et al., 2022) and calculate WER with jiwer library (nikvaessen). We use the WER of audio generated without any emotion control (original voice cloning model) as a baseline of comparison. Similar WER between emotion-controlled audio and baseline suggests that our framework preserves the high quality TTS in base voice cloning models.\nFor SIM, we used spkrec-ecapa-voxceleb (Ravanelli et al., 2021) to measure the similarity between generated audio and a reference speaker clip. We use SIM between audio generated without any emotion control and a reference speaker clip as baseline. Similar SIM between the baseline and using emotion-controlled audio suggests our framework's faithful replication of reference speaker while applying emotion control."}, {"title": "4.2 Experiment Details", "content": "We use MetaVoice-1B (MetaVoice) as the base voice cloning model, while our framework can be easily extended to any embedding-conditioned voice cloning model. We conduct speech generation on a single NVIDIA L40 GPU. We use an additional NVIDIA L40 GPU for text retrieval in text-retrieval based open-ended emotion control. We provide an audio sample page at emoknob.cs.columbia.edu."}, {"title": "4.3 Single-Shot Control of Simple Emotions", "content": "We first show our framework's effectiveness on fine-grained emotion control with simple emotion"}, {"title": "4.4 Two-Shot Control of Complex Emotion", "content": "Our framework allows a few-shot transfer of emotion onto new speakers and bases such transfer on expressive representation of foundation voice cloning models. We show that these features enable previously not studied controls on more complex, composite, and nuanced emotions. Our experiments focus on two emotions with corresponding datasets: (1) charisma defined as conveying the personality of leadership and persuasiveness (Yang et al., 2020); and (2) compassionate empathy defined as understanding another's pain as if we are having it ourselves and taking action to mitigate problems producing it (Chen et al., 2024). For each emotion, we use two pairs of emotional and neutral speech from two speakers. We fix emotion strength \u03b1 = 0.4 for all samples.\nWe report the subjective and objective evaluation results in 4. Subjective evaluation results show that our framework produces recognizable, faithful emotion selection and enhancement, surpassing commercial TTS on uttering specified emotions. Speech produced from emotion control shows similar WER and SIM within uncertainty as the baseline of no emotion control, showing that our framework preserves accurate TTS of the base model and speaker identity while conducting emotion control."}, {"title": "4.5 Synthetic Data-Based Open-Ended Emotion Control", "content": "We experiment with our synthetic-data based framework for emotion control on arbitrary text emotion description with emotions that do not have previously collected labeled datasets for emotional speech synthesis: Desire, enviousness, romance, sarcasm. We use GPT4-0 (OpenAI) to generate emotional and neutral speech texts. We use ElevenLabs (ElevenLabs) to generate 10 pairs of samples (10 speakers) for each emotion. We fix emotion strength of \u03b1 = 0.4 for all samples.\nWe report the subjective and objective evaluation results in Table 5. Subjective evaluations indicate our recognizable and faithful emotion control in speech, outperforming commercial TTS in expressing specific emotions. Additionally, speech from our emotion control maintains similar WER and SIM to the baseline, confirming that our framework effectively preserves the base model's accuracy and speaker identity while controlling emotions."}, {"title": "4.6 Retrieval-Based Open-Ended Emotion Control", "content": "Since a text retrieval model works best with descriptive, detailed texts, we focus on longer emotion descriptions of three emotions that lack established labeled datasets shown in Table 6. We prefix the emotion descriptions with the retrieval prompt of Given a description, retrieve relevant transcript lines whose overall style/emotions matches the description to enable retrieval models focused on the overall emotion of the transcript and avoid keyword matching. We use SFR-Embedding-Mistral (Meng et al., 2024) as the text embedding model. We use 10 pairs of emotional and neutral samples for each emotion. We fix emotion strength \u03b1 = 0.5 for all samples.\nWe report the subjective evaluation results and objective evaluate results with a standard deviation in Table 6. The evaluation results show that our framework produces recognizable, faithful emotion selection and enhancement while preserving base model accuracy and reference speaker identity."}, {"title": "5 Ablation Studies", "content": "In this section, we conduct ablation studies that vary the shot (sample) number when obtaining the emotion direction vector and the emotion control strength \u03b1 when applying emotion control. These ablation studies help users decide how to select these hyper-parameters. We report in Table 4 SIM and WER with audio generated with 100 Common Voice texts while applying emotion control of simple emotions with varying shot numbers and emotion strengths. We observe that both SIM and WER are insensitive to shot number and degrades as emotion control strength increases. Users thus need to trade off between generating more emotional clip with higher emotion strength and accurate TTS and voice clone. However, a larger number of samples make the method more robust in larger emotion control strength. Users thus could employ a larger number of samples to compensate the TTS quality decrease while obtaining more emotional speech."}, {"title": "6 Conclusion and Future Works", "content": "We proposed EmoKnob, a framework that enables fine-grained emotion control in voice cloning with few-shot samples. We also propose a synthetic-data-based and a retrieval-based method to embed emotions described by open-ended text into speech synthesis. Given novelty of the emotion control domain, we proposed a set of metrics to rigorously evaluate faithfulness and recognizability of emotion control. Our method establishes a new way of extracting emotion representation in foundation speech models thus bypassing data limitations. Future works can further explore emotion control paradigms such as synthesizing emotions in conversation turns based on these representations."}, {"title": "Limitations", "content": "Naturalness and expressiveness of speech created by our framework is constrained by base voice cloning model. However, since we are seeing rapid advances in foundation speech models, and our method is inherently synergetic with these advances, speech produced by EmoKnob will naturally improve as voice cloning models scale up and improve."}, {"title": "Potential Risks", "content": "Risks in speech identity theft in voice cloning apply to our work. Practices such as voice cloning detection (Malik, 2019) and phasing out voice-based authentication systems (OpenAI, 2024a) help mitigate risks of our works."}]}