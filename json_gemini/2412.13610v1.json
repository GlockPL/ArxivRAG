{"title": "Faster and Stronger: When ANN-SNN Conversion Meets Parallel Spiking Calculation", "authors": ["Zecheng Hao", "Zhaofei Yu", "Tiejun Huang"], "abstract": "Spiking Neural Network (SNN), as a brain-inspired and energy-efficient network, is currently facing the pivotal challenge of exploring a suitable and efficient learning framework. The predominant training methodologies, namely Spatial-Temporal Back-propagation (STBP) and ANN-SNN Conversion, are encumbered by substantial training overhead or pronounced inference latency, which impedes the advancement of SNNs in scaling to larger networks and navigating intricate application domains. In this work, we propose a novel parallel conversion learning framework, which establishes a mathematical mapping relationship between each time-step of the parallel spiking neurons and the cumulative spike firing rate. We theoretically validate the lossless and sorting properties of the conversion process, as well as pointing out the optimal shifting distance for each step. Furthermore, by integrating the above framework with the distribution-aware error calibration technique, we can achieve efficient conversion towards more general activation functions or training-free circumstance. Extensive experiments have confirmed the significant performance advantages of our method for various conversion cases under ultra-low time latency. To our best knowledge, this is the first work which jointly utilizes parallel spiking calculation and ANN-SNN Conversion, providing a highly promising approach for SNN supervised training.", "sections": [{"title": "1 Introduction", "content": "Spiking Neural Network (SNN), as the third generation of neural networks [Maass, 1997], has become an academic focus in the domain of brain-inspired intelligence. Unlike traditional Artificial Neural Network (ANN), the network backbone of SNN is composed of alternating synaptic layers and neuron layers. Due to the superior biological plasticity and unique firing mechanism of the spiking neuron models, SNNs have great potential in the field of neuromorphic computing and the internal spike triggering events are extremely sparse. At present, SNNs can be effectively deployed on multiple neuromorphic hardwares and have demonstrated significant advantages in inference power consumption [Merolla et al., 2014, Davies et al., 2018, DeBole et al., 2019, Pei et al., 2019].\nHow to train effective spiking models remains a core topic faced by researchers in the SNN community. The current two mainstream training methods, Spatial-Temporal Back-propagation (STBP) [Wu et al., 2018, Neftci et al., 2019] and ANN-SNN Conversion [Cao et al., 2015, Bu et al., 2022], each have their own advantages and deficiencies, as described in Tab.1. Among them, one can obtain SNN models under ultra-low time latency (e.g. < 4 ~ 6 time-steps) through STBP training, but it requires significant costs in terms of training speed and GPU memory overhead [Yao et al., 2022]. Therefore, STBP will struggle greatly for network backbones with larger parameter scale and training time-steps. In addition, when STBP does not fuse global information in the time dimension or adopts"}, {"title": "2 Related Works", "content": "STBP training for SNNs. As a significant learning algorithm that can achieve relatively superior performance for SNNs within ultra-low time latency, Wu et al. [2018] and Neftci et al. [2019] integrated Back-propagation through Time (BPTT) with surrogate gradient to pioneer the concept of STBP. On this basis, researchers have successively combined STBP algorithm with novel BatchNorm (BN) layers [Zheng et al., 2021, Jiang et al., 2024], residual blocks [Fang et al., 2021, Hu et al., 2024], objective learning functions [Li et al., 2021b, Deng et al., 2022, Guo et al., 2022], multi-dimensional attention mechanisms Qiu et al. [2024], Transformer blocks [Zhou et al., 2023, Shi et al., 2024] and advanced spiking models [Yao et al., 2022, Hao et al., 2024a], thereby extending SNN models to various application scenarios [Ren et al., 2023, Liao et al., 2024, Yao et al., 2024]. In addition, to"}, {"title": "3 Preliminaries", "content": "Spiking Neuron Models. Leaky Integrate-and-Fire (LIF) neuron is the most commonly-used spiking foundation model in the domain of SNN supervised training. Within a simulation period consisting of T time-steps, $V_t \\in [1, T]$, the LIF neuron will undergo three phases: receiving input current $I_{l,t}$, firing spikes $s^l_t$, and resetting potential $V^{l,t}_{PRE}$, which can be described in the following equations:\n$v^{l,t}_{PRE} = \\lambda^l v^{l,(t-1)} + I^{l,t}, v^{l,t} = v^{l,t}_{PRE} - \\theta^l s^{l,t}$ \n$I^{l,t} = W^l s^{(l-1),(t-1)}, s^{l,t} = \\begin{cases} 1, \\ v^{l,t}_{PRE} \\geq \\theta^l\\\\ 0, \\ otherwise \\end{cases}$  (1)\nHere $V^{l,t}_{PRE}$ and $v^{l,t}$ respectively denote the membrane potential before and after firing spikes. $\\lambda^l$ and $\\theta^l$ regulate the potential leakage degree and firing threshold. IF neuron is a special form of the LIF neuron when $\\lambda^l = 1$. $W^l$ represents the synaptic weight. As the spike firing process of the LIF neuron depends on the value of the previous membrane potential, the calculation procedure in each layer is serial, which limits the inference speed of SNNs. To address this issue, Fang et al. [2023] proposed the concept of parallel spike computing:\n$v^{l,PRE} = \\Lambda^l \\cdot \\mathbb{I}^l, \\quad  \\Lambda^l = \\begin{bmatrix} 1 & \\lambda^l & (\\lambda^l)^2 & ... & (\\lambda^l)^{T-1}\\\\  & 1 & \\lambda^l & ... & (\\lambda^l)^{T-2}\\\\  &  & 1 & ... & (\\lambda^l)^{T-3}\\\\  &  &  & \\ddots & \\vdots\\\\  &  &  &  & 1 \\end{bmatrix}$    (2)\nHere $\\Lambda^l \\in \\mathbb{R}^{T \\times T}$. As the dynamic equation of the LIF neuron at the t-th time-step can also be rewritten as $v^{l,PRE}_t = \\sum_{i=1}^t (\\lambda^l)^{t-i} I^{l,i} - \\sum_{i=2}^{t} (\\lambda^l)^{t-i} s^{l,i}$, when we ignore the influence of the previous spike firing sequence $[s^{l,1}, ..., s^{l,(t-1)}]$ on the current time-step, the equation will degenerate into the form of Eq.(2), which can finish the calculation process of T time-steps in parallel at once."}, {"title": "4 Methods", "content": "4.1 Establishing an Equivalent Mapping Relationship between Spiking Parallel Inference and QCFS\nFor traditional conversion framework, spiking neurons generally require a higher time latency (i.e. larger T) to make the error term $(v^{l,T} - v^{l,0})/T$ in Eq.(4) approach zero, thereby achieving the"}, {"title": "4.2 Towards Universal Conversion Error Rectification", "content": "Theorem 4.1(ii) indicates that under the condition of receiving uniform data distribution, even if the simulated time latency $\\tilde{T}$ is not equal to the actual inference latency T, from the perspective of"}, {"title": "4.3 Optimizing the Calculation Overhead of Spiking Parallel Inference", "content": "In the previous discussion, we have pointed out that for the parallel conversion matrix in Eq.(8), the calculation intention of $\\Lambda^{PC}$ is to determine whether the total number of firing spikes within T time-steps is not less than $\\frac{T}{T} \\tilde{x} + 1$. That is to say, if parallel neurons emit spikes at the $\\tilde{x}$-th step, they will continue to emit spikes from the $\\tilde{x} + 1$-th step to the T-th step. Therefore, to further optimize the computational overhead and inference speed, we can leverage this sorting property and apply the binary search technique in the parallel inference stage.\nSpecifically, under the initial state, we respectively set lower-bound and upper-bound pointers ptr\u00b9, ptr\" for the search interval, where $ptr^\\ell = 1, ptr^\" = T$. In each subsequent search, we\nselect the mid = $\\frac{ptr^\\ell + ptr\"}{2}$ -th step and calculate $\\Lambda^{\\ell,mid}_{PC} + b^{\\ell,mid}$. If $s^{\\ell,mid} = 1$, the next search interval will be squeezed to $[ptr^\\ell, mid]$, otherwise it will be updated to $[mid + 1, ptr\"]$. Finally,\nwe will derive the time-step $t_{FIR}$ at which the first spike is emitted. Then, we can directly set $s^{\\ell,t_{FIR}:T} = 1, s^{\\ell,1:t_{FIR}-1} = 0$ and transmit it to the next synaptic layer."}, {"title": "5 Experiments", "content": "Consistent with previous conversion learning works, we conduct performance validation on CIFAR [Krizhevsky et al., 2009] and ImageNet [Deng et al., 2009] datasets by using two types of network backbones, VGG [Simonyan and Zisserman, 2014] and ResNet [He et al., 2016]. We selected multiple methods including STBP Training (Li et al., Dspike; Guo et al., RecDis; Yao et al., GLIF; Jiang et al.,"}, {"title": "5.1 Comparison with Previous state-of-the-art Works", "content": "In Tab.3, we choose QCFS ANNs as the pretrained base models, and the hyper-parameter settings of QCFS function are the same as [Bu et al., 2022] (T = 8 for CIFAR-100/ImageNet-1k, ResNet-20/34;\n$\\tilde{T}$ = 16 for ImagNet-1k, VGG-16; $\\tilde{T}$ = 4 for the remaining cases). One can note that when the inference latency T is equal to the simulation latency $\\tilde{T}$, the performance of converted SNNs is generally at the same level as that of the corresponding ANNs. When T \u226a$\\\tilde{T}$, especially for complex datasets and deeper network backbones, the additional utilization of layer-wise error calibration will further enhance the performance of SNNs under ultra-low latency.\nCompared with other types of learning methods, our approach has achieved significant advantages, even surpassing the memory-hungry STBP methods, which means that the parallel conversion scheme may open up a third path for the domain of SNN supervised learning besides STBP and ANN-SNN Conversion. For instance, we achieve 73.92% for ImageNet-1k, VGG-16 within 8 time-steps, which exceeds the performance of COS (T = 10) by 3.33% and is at least 3.31% higher than the reported accuracies of remaining methods even if extending the inference latency by 4x (i.e. T = 32)."}, {"title": "5.2 Performance Validation of Training-Free Parallel Conversion", "content": "We further investigate the parallel inference capability of our method under the condition of training-free conversion, as illustrated in Tab.4. One can find that our method can reduce the accuracy loss of conversion learning to < 1% within 32 time-steps and achieve better performance than previous schemes with only half of their inference latency. For example, we achieve accuracies of 66.26%(68.04%) on ResNet-18(34) within 16 steps, which exceeds the corresponding results of TBC within 32 steps by 15.61% and 9.01%, respectively. In addition, it is worth noting that even if the number of network layers increases to over 100 (e.g. ResNet-101), our training-free conversion framework can also rapidly squeeze the accuracy loss within the same time latency, preventing the conversion error from being exacerbated as the network becomes deeper."}, {"title": "5.3 Analysis of Parallel Inference Speed", "content": "As illustrated in Fig.2, we compare our parallel inference with the serial inference based on vanilla IF neuron. One can note that our scheme generally achieves 19 ~ 38\u00d7 acceleration ratio when T > 32, even for the very deep network backbone (> 100 layers). It is worth noting that the conversion error may be further amplified for complex network backbones or task scenarios, which leads to more severe time latency and performance degradation. Therefore, if we respectively consider the converted SNNs after adopting our method and traditional conversion framework at the same performance level, the actual advantages we achieve in terms of inference speed will become more remarkable. More experimental results can be found in Appendix."}, {"title": "6 Conclusion", "content": "In this paper, we propose a novel concept of parallel conversion and theoretically establish its mathematical equivalent relationship with the general activation function modules. Extensive experiments have validated that our scheme outperforms existing routes in SNN supervised learning in terms of inference performance and speed, which provides a brand-new approach for obtaining efficient SNN models."}, {"title": "A Appendix", "content": "A.1 Proof of Theorem 4.1\nTheorem 4.1. For a T-steps parallel inference in the l-th layer, we use b' to denote the corresponding shift term, here $b^\\ell \\in \\mathbb{R}^T$. When the pretrained ANN adopts QCFS function in Eq.(5), for the following cases, we will derive the optimal value of the shift term: $\\mathbb{b}^{\\ell} = [\\frac{T-x+1}{T}, ..., \\frac{T-x+1}{T}]^T$.\n(i) If T = $\\tilde{T}$, then we have $r^{\\ell,T} = r^{\\ell,T}_{QCFS}$.\n(ii) If T\u2260$\\tilde{T}$ and $\\psi^\\ell = \\theta^\\ell/2$, then we have $E (r^{\\ell,T} -r^{\\ell,\\tilde{T}}_{QCFS}) = 0$.\nProof. (i) For $I = W^\\ell r^{(l-1),\\tilde{T}} \\in [k\\theta^{\\ell}/T - \\psi^\\ell, (k + 1)\\theta^{\\ell} - \\psi^\\ell),\\forall k \\in [1,\\tilde{T}]$, from Eq.(5) we can derive that $r^{\\ell,T}_{QCFS} = k\\theta^{\\ell}/\\tilde{T}$.\nWhen we consider $s^\\ell = (\\Lambda I^\\ell + b^\\ell \\geq \\theta^\\ell)$, we will have:\n$s^\\ell = \\begin{bmatrix} \\frac{1}{T} & \\frac{1}{T} & \\frac{1}{T} & ... & \\frac{1}{T}\\\\ & \\frac{1}{T-1} & \\frac{1}{T-1} & ... & \\frac{1}{T-1}\\\\ & & \\ddots & & \\vdots\\\\ & & & & \\frac{1}{1} \\end{bmatrix} \\begin{bmatrix} k\\theta^{\\ell} - \\psi^{\\ell}\\\\  \\\\  \\\\  \\\\  \\\\ \\end{bmatrix} + \\begin{bmatrix} \\frac{T-1}{T} & \\frac{T-1}{T} & ... & \\frac{T-1}{T}\\\\ 0 &  & ... & \\\\\\ \\vdots &  & \\ddots & \\\\\\ 0 &  &  & \\\\\\  &  &  & \\\\\\  \\\\ \\end{bmatrix} \\geq \\theta^\\ell  \\quad  \\Leftarrow  \\frac{W^{i, \\ell} \\tilde{r}^{(i-1),\\ell} \\tilde{T}}{T- \\tilde{x} + 1} \\geq  \\theta^\\ell \\quad (S1)$\nCombining with $W^\\ell r^{(l-1),\\tilde{T}} \\in [\\frac{k\\theta^{\\ell} - \\psi^{\\ell}}{T - \\tilde{x} + 1}, \\frac{(k + 1)\\theta^{\\ell} - \\psi^{\\ell}}{T - \\tilde{x} + 1})$, we further have:\n$\\frac{k\\theta^{\\ell}}{\\tilde{T}} \\leq \\Lambda I^\\ell + b^\\ell < \\frac{(k + 1)\\theta^{\\ell}}{\\tilde{T}} \\qquad  \\Longrightarrow  \\qquad \\tilde{s}^\\ell = (\\Lambda I^\\ell + b^\\ell \\geq \\theta^\\ell)  \\Longrightarrow  \\tilde{s}^\\ell = [0 ... 1 ... 1]^T   \\quad  \\Rightarrow \\begin{cases} 1, \\\\\\ 0, \\end{cases} \\qquad (k = \\frac{\\tilde{x}}{T} + 1)\\quad (S2)$\nFinally, we will derive $r^{\\ell,T} = r^{\\ell,\\tilde{T}}_{QCFS} = k\\theta^{\\ell}/\\tilde{T}$.\n(ii) Since QCFS function has the property of $E (\\frac{r^{\\ell,T}}{\\tilde{T}} - \\frac{r^{\\ell,\\tilde{T}}_{QCFS}}{\\tilde{T}}) = 0$ when $\\psi^{\\ell} = \\theta^{\\ell}/2$, as mentioned in [Bu et al., 2022], combining with the conclusion of (i), we can have:\n$E (\\frac{r^{\\ell,T}}{\\tilde{T}} - \\frac{r^{\\ell,\\tilde{T}}_{QCFS}}{\\tilde{T}}) = E (\\frac{r^{\\ell,T}}{\\tilde{T}} - \\frac{r^{\\ell,\\tilde{T}}}{\\tilde{T}})  \\frac{r^{\\ell,\\tilde{T}}}{\\tilde{T}} + E (\\frac{r^{\\ell,\\tilde{T}}}{\\tilde{T}} - \\frac{r^{\\ell,\\tilde{T}}_{QCFS}}{\\tilde{T}})  = 0 + 0 = 0. (S3)$\nA.2 Detailed Experimental Configuration\nFor pretrained QCFS ANN models, we use SGD optimizer [Bottou, 2012], the optimization strategy of Cosine Annealing [Loshchilov and Hutter, 2017] and data augmentation techniques [DeVries and Taylor, 2017, Cubuk et al., 2019], the corresponding hyper-parameter settings are: lr = 0.1, wd =\n5 \u00d7 10\u22124 for CIFAR-10, lr = 0.02, wd = 5 \u00d7 10\u22124 for CIFAR-100 and lr = 0.1, wd = 1 \u00d7 10\u22124\nfor ImageNet-1k. The specific network structure is consistent with [Bu et al., 2022]. Regarding the error calibration technique, we utilize the training dataset as the calibration data to iterate for 1 epoch. The learning momentum \u03b1 mentioned in Algorithm 1 is set to 0.99.\nFor ReLU ResNet family in Tab.4, we replace all ReLU modules except for Stem with ClipReLU, DA-QCFS, and parallel spiking neurons in sequence. The inference speeds in Fig.2 and Fig.S1 are measured on a single NVIDIA RTX 4090 GPU. Among them, the inference speed of IF neuron is calculated on a subset of the test dataset (1000 images). In addition, experimental results reported in\nTab.4, Fig.2 and Fig.S1 utilize O(T) acceleration optimization in the charging phase."}]}