{"title": "Hierarchical Autoscaling for Large Language Model Serving with Chiron", "authors": ["Archit Patke", "Dhemath Reddy", "Saurabh Jha", "Chandra Narayanaswami", "Zbigniew Kalbarczyk", "Ravishankar Iyer"], "abstract": "Large language model (LLM) serving is becoming an increasingly important workload for cloud providers. Based on performance SLO requirements, LLM inference requests can be divided into (a) interactive requests that have tight SLOs in the order of seconds, and (b) batch requests that have relaxed SLO in the order of minutes to hours. These SLOs can degrade based on the arrival rates, multiplexing, and configuration parameters, thus necessitating the use of resource autoscaling on serving instances and their batch sizes. However, previous autoscalers for LLM serving do not consider request SLOs leading to unnecessary scaling and resource under-utilization. To address these limitations, we introduce Chiron, an autoscaler that uses the idea of hierarchical backpressure estimated using queue size, utilization, and SLOs. Our experiments show that Chiron achieves up to 90% higher SLO attainment and improves GPU efficiency by up to 70% compared to existing solutions.", "sections": [{"title": "1 INTRODUCTION", "content": "Background and Motivation. Large language models (LLMs) such as OpenAI GPT-4, Google Gemini, and Meta Llama have enabled novel capabilities in a wide range of AI applications such as chatbots and coding assistants. Consequently, serving LLMs for enterprise and consumer applications with latency-oriented service-level objectives (SLOs) has become increasingly critical. Based on their SLO requirements, requests can be divided into (a) interactive requests that have tight SLOs in the order of seconds, and (b) batch requests that have relaxed SLOs in the order of minutes to hours.\nTo serve these requests, LLM serving systems such as Llumnix and Ray Serve use an autoscaler that scales resources based on the underlying backpressure generated from memory utilization and queuing metrics. However, these autoscalers frequently overestimate backpressure at both local and global levels leading to excessive scale up actions and resource under-utilization. Specifically, we find sub-optimality in the following two scenarios:\nAt the local level, previous systems limit maximum batch sizes leading to sub-optimal serving throughput. Existing\nstate-of-the-art serving systems such as vLLM use optimizations like continuous batching and PagedAttention to improve the serving throughput of LLM serving instances. While such optimizations significantly improve throughput, they also lead to increased request preemptions and inflate the inter-token latency for interactive requests beyond the SLO value. To resolve this issue, in practice, operators limit the maximum batch size, diluting the benefits of PagedAttention. However, when similar limitations are applied to batch requests, it can lead to suboptimal utilization and reduced throughput.\nAt the global level, previous systems excessively scale up the number of instances with the arrival of batch requests. Previous systems do not distinguish between request with\nvarying SLOs when performing scaling actions. Consequently, as batch requests arrive there is an immediate scale up of instances as shown in Figure 1 (Left). Instead, batch requests can be maintained in a queue and scaling action can be delayed to allow for multiplexing with interactive requests and improve resource utilization (as shown in Figure 1 (Right)).\nOur Work. To address the above limitations, we propose Chiron, a pluggable autoscaler that works with exisiting LLM serving systems. Chiron uses the idea of hierarchical backpressure to determine scaling actions at the local and global serving instances.\nAt the local instance level, batch size controls number of requests served concurrently. If batch size increases throughput is higher, albeit at the cost of latency. Previous attempts at batch size optimization such as Andes and Vidur typically rely on offline profiling. However, such an approach is increasingly infeasible with the exponential configuration space for optimizations such as speculative decoding, prefix caching, chunked prefill, etc. Instead, Chiron uses a reactive autoscaling approach based on local backpressure generated from a combined latency throughput metric. If latency SLO violation or throughput degradation is observed, local backpressure increases and Chiron reduces the batch size to eliminate preemptions and meet the SLO (and vice-versa if SLOs are already met).\nAt the global cluster level, the number of serving instances determine the overall throughput and latency serving capabilities of the system. An ideal autoscaling policy would dynamically change the number of instances to match the incoming request workload. However, with LLM serving workloads, unlike previous cloud workloads, time to bring up new instances is much higher due to large model size. Therefore, the system would always have to be over-provisioned to ensure that SLOs for interactive requests are not missed during bursty arrivals. As such over-provisioning is necessary, it leads to spare capacity in the system that can be leveraged by batch requests to further improve utilization and throughput. To decide whether the spare capacity is sufficient to meet SLOs and scale instances accordingly, Chiron uses request waiting time estimation from QLM. Additionally, Chiron uses requests groups from SHEPHERD to minimize unnecessary scaling actions.\nResults. We demonstrate Chiron on vLLM as the backend LLM-serving system and GPU clusters with NVIDIA A100 GPUs. We adopt workloads from a real-world LLM dataset: ShareGPT using setups derived from our production requirements. Our experiments demonstrate the following major improvements with Chiron:\n(1) SLO Attainment: Depending on the arrival rate, Chiron achieves up to 90% higher SLO attainment compared to previous LLM serving systems like Llumnix.\n(2) GPU Efficiency Improvements: Chiron improves the request throughput up to 300% compared to previous systems. Such throughput increase leads to GPU savings up to 70% as shown in Figure 2 (Right).\n(3) Ablation Study and Robustness Analysis: We demonstrate that using both local and global backpressure contributes to SLO attainment and throughput improvement. Additionally, we present robustness analysis with varying SLO values and bursty arrival patterns."}, {"title": "2 BACKGROUND", "content": "2.1 LLM Inference\nInference Primer. An inference process starts from a request (prompt) with a list of input tokens from which the LLM generates a list of output tokens. Due to the autoregressive pattern, the LLM can only generate new tokens one by one, and the generation process of each new token depends on all the previous tokens in that sequence, specifically their key and value vectors. In this sequential generation process, the key and value vectors of existing tokens are cached for generating future tokens, known as KV cache.\nContinuous Batching. During LLM inference, the decoding stage is memory-bound, as loading model weights from memory takes longer than computation. Therefore, state-of-the-art LLM serving systems like vLLM, Orca, Tensor-RT and TGI employ continuous batching with iterative scheduling to enable dynamic addition of requests to a batch as soon as others have finished generation."}, {"title": "2.2 Definitions", "content": "Definition 2.1. SLO: Each LLM serving request has two key latency requirements: (a) time to first token (TTFT): the time required to complete the prefill step and generate the first token, and (b) inter-token latency (ITL): the time required to generate each subsequent token in the decode phase. These two latency requirements together form the service-level objective (SLO) for the request.\nDefinition 2.2. LLM Serving Instance: An LLM serving system is capable of hosting LLM models by providing the necessary infrastructure and resources to load the models into memory and respond to requests. Chiron is compatible with existing LLM serving systems such as vLLM and TGI. An LLM serving instance is composed of the LLM serving system and an LLM model that is being served.\nDefinition 2.3. Autoscaler: An autoscaler provisions resources based on incoming request arrival patterns. An ideal autoscaler (a) maximizes serving throughput while maintaining request SLOs, (b) quickly converges to op-\ntimal resource levels, and (c) avoids unnecessary scaling actions (referred to as hysteresis). Note that such an autoscaler also ensures high system utilization and reduces total resource requirement."}, {"title": "2.3 Motivation and Characterization", "content": "To design an SLO-aware autoscaler for LLM workloads, we investigate the following scenarios: (a) Request arrival patterns in a production cluster, (b) Limitations of existing autoscalers with varying SLO requirements, and (c) Throughput versus latency trade-off space in LLM serving systems. Besides data from the internal production cluster, we use state of the art LLM serving system, vLLM, to investigate these scenarios using the ShareGPT dataset.\nBurstiness in request arrivals leads to SLO violations in production clusters. An optimal autoscaling policy scales instances up and down to match the incoming request arrival rate while minimizing any under-utilized capacity. However, for LLM workloads, the time to scale up new instances is significant due to the large model sizes. Even with model loading optimizations , the model load time varies between 15 seconds and one minute. Due to these long model load times, new instances cannot be brought online quickly enough when the system is at capacity and the request arrival rate increases (i.e., a request spike occurs), leading to SLO violations.\nTo quantify this observation, we define a request spike as the ratio of the request arrival rate between two consecutive time intervals, where the interval length corresponds to the model load time. If an arrival spike is greater than one, and the system is at capacity (i.e., no GPU memory is available to process additional requests), an SLO violation will occur. We observe that such large arrival rate spikes are common in our internal production cluster traces, as shown in Figure 4. Over a two-month period, the p90 and p99 values of the arrival spikes are 1.6 and 3, respectively.\nAdditionally, we created a synthetic workload with Gamma arrival rates to simulate increasing burstiness levels by varying the coefficient of variance (CV) parameter. Figure 5 shows the necessary over-provisioning to meet SLOs across different percentiles. As burstiness increases, requests spikes increase and additional provisioning is necessary.\n\u2022 Resource over-provisioning is necessary to serve interactive requests and such over-provisioning is proportional to burstiness of request arrivals.\n\u2022 Design Consequence 1: Chiron uses extra capacity to serve batch requests via a global au-"}, {"title": "3 CHIRON DESIGN AND IMPLEMENTATION", "content": "Figure 7 provides an overview of Chiron. Chiron follows a hierarchical autoscaling design to meet TTFT and ITL SLOs while maximizing throughput by: (a) scaling the batch size of an individual instance via Local Autoscaler, and (b) scaling the interactive, mixed and batch instances\nLifecycle of a Request. All incoming requests are en-queued into a global queue and then served by the underlying LLM serving instances. Each serving instance is classified into one of three categories: (a) Interactive Instances serve interactive requests only, (b) Mixed Instances serve both interactive and batch requests, and (c) Batch Instances serve batch requests only. Each request is preferentially routed to it's own instance type (i.e., interactive requests to interactive instances and batch requests to batch instances) leading to non-uniform routing of requests in Chiron. If capacity is unavailable on its own instance type, they are routed to the mixed instances.\nRequest Multiplexing with Mixed Instances. Mixed instances enable multiplexing between interactive and batch requests and drive up overall cluster utilization. For interactive requests, the mixed instances can handle unpredictable spikes in request arrivals. For batch requests, the mixed instances provides additional running capacity when sufficient interactive requests are not present. To enable such multiplexing between interactive and batch requests while ensuring immediate execution of interactive requests, mixed instances are preemptible, i.e., interactive requests can evict out batch requests and send them back into the global queue. To prevent throughput drop from such eviction, we enable fast restart by saving the KV cache by migrating it to CPU memory."}, {"title": "4 LOCAL AUTOSCALER", "content": "The local autoscaler uses an online algorithm to determine the optimal maximum batch size for each instance using a local backpressure metric, that accounts for both latency and throughput degradation.\nWhy use an online algorithm? In production settings,\neach serving instance can be configured with multiple optimizations, such as speculative decoding, prefix caching, and chunked prefill. These configurations impact latency and throughput of the serving instance. However, the usual practice of estimating the optimal batch size per instance through offline profiling is prohibitively expensive due to exponential configuration space."}, {"title": "4.1 Local Backpressure", "content": "Increase in latency and throughput degradation at the local instance-level leads to local backpressure, that we define as follows:\n\u2022 Latency-based Backpressure (LBP): Empirically, we observe that the increasing the batch size increases the inter-token latency (ITL) due to increased request preemptions and higher self-attention computation costs. Such increase in inter-token latency can lead to SLO violations. To account for the ITL inflation, we define the latency-based backpressure (LBP) metric as the ratio between observed ITL and the ITL SLO value. When LBP is greater than one, ITL SLO violations are observed and the batch size should be decreased.\n\u2022 Throughput-based Backpressure (TBP): Increase in batch size can also increase throughput as more requests are processed simultaneously. However, surprisingly, we observe that increasing the batch size beyond an inflection point results in a decrease in throughput due to increased preemptions and higher self-attention costs (similar to inter-token latency increase). Figure 3 shows the inflection points for Llama 8B and 70B. To account for this drop in throughput, we define throughput-based backpressure (TBP) as the ratio between previously observed and current throughput. If TBP is greater than one, no throughput gain is observed from increasing the batch size and it should be decreased."}, {"title": "4.2 Batch Size Autoscaling", "content": "Algorithm 1 describes the overall autoscaling algorithm that varies the batch size based on local backpressure. If local backpressure is greater than one, the batch size is divided by half. Otherwise, the batch size is increased proportionally with exponentially weighted moving average (EWMA) to obtain throughput benefits. EWMA and proportional scaling allows for convergence to the optimal batch size value\u00b9. As the backpressure approaches one, the algorithm slows down the increase in batch size values. Note that the ITL SLO for the instance is the smallest\n\u00b9EWMA is widely used in resource control algorithms such as congestion control , operating system load estimation , and database query optimization ."}, {"title": "5 GLOBAL AUTOSCALER", "content": "The global autoscaler scales LLM serving instances based on the incoming request using a global backpressure metric that accounts for instance usage and queue formation."}, {"title": "5.1 Global Backpressure", "content": "Global backpressure can arise from both increased instance utilization and request queue formation as described below:\n\u2022 Interactive Backpressure (IBP): Interactive instances need to be overprovisioned to handle any unpredictable spikes in request arrivals, as shown earlier in Section 2.3. In the case of Chiron, mixed instances represent such over-provisioned capacity and request spikes are routed to the mixed instances, leading to increased backpressure as their usage increases. Therefore, we define interactive backpressure (IBP) as the ratio of instances running interactive requests to the total mixed and interactive instances. As IBP increases new mixed and interactive instances need to be added to maintain the level of over-provisioning.\n\u2022 Batch Backpressure (BBP): Batch requests can utilize the spare capacity available on the mixed instances. However, such spare capacity may not be sufficient to complete all batch requests and they would remain in the queue. Increase in queue by itself is not a problem if requests are not close to their TTFT SLO-based deadline. Hence, we define batch backpressure (BBP) as number of batch requests in the global queue that are close to their TTFT SLO (formally defined in Section 5.3). As BBP increases, the queue waiting time can exceed the TTFT SLO-based deadline and new batch instances need to be added."}, {"title": "5.2 Interactive Autoscaling", "content": "The interactive autoscaler attempts to maintain a constant level of over-provisioning (i.e. the ratio of instances running interactive requests to the total mixed and interactive instances) equal to \u0398. As requests arrive, the level of over-provisioning varies leading the IBP (i.e., the current over-provisioning level) to change. If IBP exceeds \u0398, interactive and mixed instances are added to maintain over-provisioning. Conversely, if IBP decreases below \u0398, mixed and interactive instances are removed. The specific value of is chosen based on historic arrival patterns in the system. For example, if the tail request arrival spike is three times the average arrival rate, \u0398 \u00b2 is set to 1/3."}, {"title": "5.3 Batch Instance Autoscaling", "content": "The batch instance autoscaler scales batch instances based on the requests in the queue using BBP as shown in Algorithm 2.3\nRequest Group Creation: BBP uses the idea of request groups, which are created by clustering requests in the queue with similar TTFT SLO values . Since requests within a request group have similar TTFT SLO requirements, Chiron treats the ordering of the requests within a group using a first-come-first-serve (FCFS) policy. These request groups are executed together, thus minimizing hysteresis as discussed in Section 2.3.\nEstimating Batch Backpressure (BBP): Chiron estimates the queue waiting time for each request group to determine BBP. If requests with waiting time is close to the TTFT SLO deadline, the BBP increases (and vice-versa). The waiting time estimates are generated with a statistical approach as described in QLM . We describe the estimation approach below for completeness. Chiron consider the token generation throughput (\u0398) to be constant throughout the token generation process due to statistical averaging effects of continuous batching. Therefore, the total waiting time for a single request can be represented by Equation 1 by dividing the number of tokens ahead ($O_i$) in the queue by the token generation throughput (\u0398) where i denotes each of the q \u2212 1 requests in the queue ahead of the request we model.\n$W_q = \\sum_{i=1}^{q-1} \\frac{O_i}{\\Theta}$ (1)\nNote that we do not know the number of output tokens ahead of time (that requires the knowledge of the output se-\n\u00b2In practice, to minimize hysteresis (i.e. constant addition/re-tirement of instances), we maintain IBP in a range equal to [\u0398 \u2013 \u03b4, \u0398 + \u03b4]\n\u00b3 Note that interactive requests follow a zero-queuing approach to minimize TTFT."}, {"title": "5.4 Interactive Autoscaling", "content": "The interactive autoscaler attempts to maintain a constant level of over-provisioning (i.e. the ratio of instances running interactive requests to the total mixed and interactive instances) equal to \u0398. As requests arrive, the level of over-provisioning varies leading the IBP (i.e., the current over-provisioning level) to change. If IBP exceeds \u0398, interactive and mixed instances are added to maintain over-provisioning. Conversely, if IBP decreases below \u0398, mixed and interactive instances are removed. The specific value of is chosen based on historic arrival patterns in the system. For example, if the tail request arrival spike is three times the average arrival rate, \u0398 \u00b2 is set to 1/3."}, {"title": "6 EVALUATION", "content": "We evaluate Chiron on the following dimensions:\n(a) SLO attainment and throughput improvements for an interactive workload (Section 6.1),\n(b) SLO attainment and throughput improvements for a combined interactive and batch workload (Section 6.2),\n(c) Time for autoscaler to converge,\n(d) Robustness analysis including accuracy of queue waiting time estimator, use of serving optimizations, vari-"}, {"title": "5.3 Batch Instance Autoscaling", "content": "The batch instance autoscaler scales batch instances based on the requests in the queue using BBP as shown in Algorithm 2.\nRequest Group Creation: BBP uses the idea of request groups, which are created by clustering requests in the queue with similar TTFT SLO values . Since requests within a request group have similar TTFT SLO requirements, Chiron treats the ordering of the requests within a group using a first-come-first-serve (FCFS) policy. These request groups are executed together, thus minimizing hysteresis as discussed in Section 2.3.\nEstimating Batch Backpressure (BBP): Chiron estimates the queue waiting time for each request group to determine BBP. If requests with waiting time is close to the TTFT SLO deadline, the BBP increases (and vice-versa). The waiting time estimates are generated with a statistical approach as described in QLM . We describe the estimation approach below for completeness. Chiron consider the token generation throughput (\u0398) to be constant throughout the token generation process due to statistical averaging effects of continuous batching. Therefore, the total waiting time for a single request can be represented by Equation 1 by dividing the number of tokens ahead ($O_i$) in the queue by the token generation throughput (\u0398) where i denotes each of the q \u2212 1 requests in the queue ahead of the request we model.\n$W_q = \\sum_{i=1}^{q-1} \\frac{O_i}{\\Theta}$ (1)\nNote that we do not know the number of output tokens ahead of time (that requires the knowledge of the output se-\n\u00b2In practice, to minimize hysteresis (i.e. constant addition/re-tirement of instances), we maintain IBP in a range equal to [\u0398 \u2013 \u03b4, \u0398 + \u03b4]\n\u00b3 Note that interactive requests follow a zero-queuing approach to minimize TTFT.\nAutoscaling Algorithm: The final autoscaling algorithm shown in Algorithm 2 adds the minimum instances required to make the value of BBP equal to zero. If BBP is zero and there are no active requests being served on the batch instances, then all batch instances are retired."}, {"title": "5.4 Interactive Autoscaling", "content": "The interactive autoscaler attempts to maintain a constant level of over-provisioning (i.e. the ratio of instances running interactive requests to the total mixed and interactive instances) equal to \u0398. As requests arrive, the level of over-provisioning varies leading the IBP (i.e., the current over-provisioning level) to change. If IBP exceeds \u0398, interactive and mixed instances are added to maintain over-provisioning. Conversely, if IBP decreases below \u0398, mixed and interactive instances are removed. The specific value of is chosen based on historic arrival patterns in the system. For example, if the tail request arrival spike is three times the average arrival rate, \u0398 \u00b2 is set to 1/3."}, {"title": "6 EVALUATION", "content": "We evaluate Chiron on the following dimensions:\n(a) SLO attainment and throughput improvements for an interactive workload (Section 6.1),\n(b) SLO attainment and throughput improvements for a combined interactive and batch workload (Section 6.2),\n(c) Time for autoscaler to converge,\n(d) Robustness analysis including accuracy of queue waiting time estimator, use of serving optimizations, vari-"}, {"title": "6.1 Interactive Autoscaling", "content": "To understand the efficacy of Chiron's interactive autoscaling, we run workload WA with varying interactive arrival rates and evaluate the impact on throughput and SLO satisfaction metrics. Figure 9 shows the average per-instance throughput and SLOs met across all autoscalers for the small, large, and mixed model configurations.\nRequest Throughput. For all configurations, we find that Chiron has equal or higher per-instance throughput compared to both versions of Llumnix. Specifically, Chiron performs better because: (a) Compared to Llumnix (Untuned), Chiron is able to adapt batch sizes across different models. For example, the small model can use a large batch size with higher throughput, whereas the large model would use a smaller batch size to maintain the same inter-token latency SLO. The local autoscaler in Chiron is able to consider these differences and maintains a 5\u00d7 higher batch size for the small model compared to the large one. (b) Compared to Llumnix (Tuned), Chiron adjusts batch size dynamically across time based on the token distributions. For example, when multiple short requests are pro-"}, {"title": "6.2 Batch Autoscaling", "content": "To understand the efficacy of Chiron's batch autoscaler, we run workload WB with varying batch request queues and evaluate the impact on request throughput and SLO satisfaction. In addition to the batch requests, we also serve interactive requests with constant arrival rate of 50 requests/s and 10 requests/s for 8B and 70B models respectively. Figure 10 shows the average per-instance throughput and SLOs met across all autoscalers for the small, large, and mixed model configurations.\nRequest Throughput. Similar to WA, we find that Chiron has higher per-instance throughput with WB across all configurations compared to other autoscalers. Specifically, we find that (a) Compared to Llumnix (Untuned),"}, {"title": "6.3 Chiron Robustness Analysis", "content": "Convergence Analysis. Chiron's local autoscaler dynamically adapts the batch size based on the underlying con-"}, {"title": "6.1 Interactive Autoscaling", "content": "To understand the efficacy of Chiron's interactive autoscaling, we run workload WA with varying interactive arrival rates and evaluate the impact on throughput and SLO satisfaction metrics. Figure 9 shows the average per-instance throughput and SLOs met across all autoscalers for the small, large, and mixed model configurations.\nRequest Throughput. For all configurations, we find that Chiron has equal or higher per-instance throughput compared to both versions of Llumnix. Specifically, Chiron performs better because: (a) Compared to Llumnix (Untuned), Chiron is able to adapt batch sizes across different models. For example, the small model can use a large batch size with higher throughput, whereas the large model would use a smaller batch size to maintain the same inter-token latency SLO. The local autoscaler in Chiron is able to consider these differences and maintains a 5\u00d7 higher batch size for the small model compared to the large one. (b) Compared to Llumnix (Tuned), Chiron adjusts batch size dynamically across time based on the token distributions. For example, when multiple short requests are pro-"}, {"title": "6.2 Batch Autoscaling", "content": "To understand the efficacy of Chiron's batch autoscaler, we run workload WB with varying batch request queues and evaluate the impact on request throughput and SLO satisfaction. In addition to the batch requests, we also serve interactive requests with constant arrival rate of 50 requests/s and 10 requests/s for 8B and 70B models respectively. Figure 10 shows the average per-instance throughput and SLOs met across all autoscalers for the small, large, and mixed model configurations.\nRequest Throughput. Similar to WA, we find that Chiron has higher per-instance throughput with WB across all configurations compared to other autoscalers. Specifically, we find that (a) Compared to Llumnix (Untuned),"}, {"title": "6.3 Chiron Robustness Analysis", "content": "Convergence Analysis. Chiron's local autoscaler dynamically adapts the batch size based on the underlying con-"}, {"title": "7 RELATED WORK", "content": "General ML Model-Serving Systems. Traditional model-serving systems provide functionalities such as scheduling, placement, batching, and autoscaling. Clipper, TensorFlow-Serving , MArk , InferLine , SHEPHERD , and Clockwork are some earlier work on serving traditional ML models like ResNet that are relatively small. INFaaS and Cocktail propose a model-less serving framework to automate the model selection and autoscaling to meet SLOs. However, they fail to consider the autoregressive property of LLMs leading to suboptimal throughput.\nLLM Scheduling Optimization. Existing state-of-the-art LLM serving systems vLLM , Orca , TGI ; triton adopts continuous batching and a first-come-first-serve (FCFS) scheduling policy that suffers from head-of-line (HOL) blocking, which we address in Chiron. FastServe proposes preemptive scheduling with a Multi-Level Feedback Queue. Andes defines Quality-of-Experience (QoE) for LLM serv-ing as token delivery speed, and proposes a preemptive scheduler that maximizes QoE. Chiron is the first autoscaling framework that optimizes SLO attainment while improving LLM-serving throughput and device utilization.\nLLM Serving Backend Optimization. Various LLM serving backend optimization techniques have been proposed to improve token generation throughput and memory cost while adapting to fine-tuning paradigms such StreamingLLM, Speculative Decoding, ChunkedAttention, FlashAttention and more.\nThese backend LLM-serving optimizations are complementary to Chiron."}, {"title": "8 CONCLUSION", "content": "We presented Chiron, a hierarchical autoscaling solution for LLM serving. Evaluation using real-world LLM serving datasets on GPU devices demonstrate that Chiron improves SLO attainment by up to 90%, serving throughput up to 300%, and reduces resource requirement up to 70%."}, {"title": "A APPENDIX", "content": "A.1 Impact of Model Loading Times\nWhen model loading times are greater than the TTFT SLO for interactive requests, over-provisioning is a must to meet SLOs. Given such over-provisioning exists, Chiron's global autoscaler can improve overall utilization by multiplexing the additional capacity with batch requests.\nIf model loading times are below the TTFT SLO (such as for models with less than 3B parameters), then Chiron can follow a completely elastic autoscaling approach similar to traditional cloud autoscaling. In such a case, the need for"}]}