{"title": "SF-GNN: Self Filter for Message Lossless Propagation in Deep Graph Neural Network", "authors": ["Yushan Zhu", "Wen Zhang", "Yajing Xu", "Zhen Yao", "Mingyang Chen", "Huajun Chen"], "abstract": "Graph Neural Network (GNN), with the main idea of encoding graph structure\ninformation of graphs by propagation and aggregation, has developed rapidly. It\nachieved excellent performance in representation learning of multiple types of\ngraphs such as homogeneous graphs, heterogeneous graphs, and more complex\ngraphs like knowledge graphs. However, merely stacking GNN layers may not im-\nprove the model's performance and can even be detrimental. For the phenomenon\nof performance degradation in deep GNNs, we propose a new perspective. Unlike\nthe popular explanations of over-smoothing or over-squashing, we think the issue\narises from the interference of low-quality node representations during message\npropagation. We introduce a simple and general method, SF-GNN, to address\nthis problem. In SF-GNN, we define two representations for each node, one is\nthe node representation that represents the feature of the node itself, and the other\nis the message representation specifically for propagating messages to neighbor\nnodes. A self-filter module evaluates the quality of the node representation and\ndecides whether to integrate it into the message propagation based on this quality\nassessment. Experiments on node classification tasks for both homogeneous and\nheterogeneous graphs, as well as link prediction tasks on knowledge graphs, demon-\nstrate that our method can be applied to various GNN models and outperforms\nstate-of-the-art baseline methods in addressing deep GNN degradation.", "sections": [{"title": "Introduction", "content": "Graphs are widely used to model structured and relational data [1]. There are homogenous graphs\nconsisting of a single type of nodes and edges such as document citation networks [2], heterogeneous\ngraphs consisting of several different types of nodes and edges such as web and social networks [3],\nand more complex graphs like Knowledge Graphs (KGs) [4, 5] containing hundreds or thousands of\nnode and relational edge types. Mining and analyzing graphs can enhance a wide range of real-world\napplications, such as node classification and link prediction. A common approach for graph mining is\nto map the nodes and edges in a graph into continuous vector spaces through graph representation\nlearning. These vector representations of nodes and edges can then be utilized for various tasks. In\nrecent years, Graph Neural Networks (GNNs) [6, 7, 8, 9, 10] have rapidly developed and become the\nleading approach to graph representation learning.\nThe core concept of GNNs is to learn rich node representations through feature propagation and\naggregation. Multi-layer GNNs typically serve as encoders to capture extensive graph structure\ninformation. They work in conjunction with a task-related decoder. For instance, in node classification\ntasks, a multi-layer perceptron (MLP) is often used as the decoder. In GNN-based knowledge graph\nembedding (KGE) models [11, 12, 13], conventional KGE methods [14, 15, 5] are employed as"}, {"title": "Related Work", "content": "Graph Neural Networks The graph neural network (GNN) works primarily on a message-passing\nmechanism [6], where each node updates its representation by aggregating messages propagated\nfrom its direct neighbors. Classical GNN variants such as GGNN [7], GAT [8], GCN [9] and\nGraphSAGE [10], are mainly different in the way each node aggregates the representation of its\nneighbors with its own representation. Learning heterogeneous graphs containing more than one\ntypes of nodes and edges has always been a thorny problem for GNN, and many methods for learning\nheterogeneous graphs have been gradually proposed [22, 23, 24, 19]. Knowledge graph (KG) is\na more complex and representative heterophily graph that may contain hundreds or thousands of\ntypes of entities and edges, where nodes and edges are also called entities and triples. with the\ndevelopment of GNNs, knowledge graph embedding learning methods based on GNN, namely\nGNN-based KGEs [11, 12, 25, 13], show good performance. These models are of encoder-decoder\narchitectures, where the encoder is a multi-layer GNN to enhance the node representation by capturing\nglobal or local structure and the decoder utilizes a traditional KGE such as TransE [14], DistMult [15],\nand ConvE [5] to predict triples. R-GCN [11] firstly applies GCN [26, 9] in KGE, where GCN is\nthe encoder to deal with the highly multi-relational data characteristic of KGs and DistMult is the\ndecoder. However, R-GCN lacks the learning of edge feature (relation representation), in which\nthe edge only serves as an attribute to label the class of the target node. CompGCN [12] takes the\ninformation of multi-type edges into the calculation of KGs, jointly learning the representations of\nentities and relations, and uses TransE, DistMult, or ConvE as the decoder. RGHAT [27] emphasizes\nthe importance of different relations and different neighbor entities with the same relation by relation-\nlevel and entity-level attentions, and it uses ConvE as the decoder. NBFNet [13] encodes the\nrepresentation of a pair of nodes as the generalized sum of all path representations between nodes\nand a path representation as the generalized product of edge representations in the path, then applies\na multi-layer perceptron (MLP) as the decoder. SE-GNN [25] focuses on semantic evidence (SE) for\nKGE models with strong extrapolation capability, the encoder models relation-level, entity-level, and\ntriple-level SE, its decoder is ConvE.\nDegradation of Deep GNNs Ideally, stacking GNN layers can expand the propagation range\nof messages, make nodes perceive richer graph structure information, and enhance the model\nperformance. However, recent studies have shown that stacking GNN layers can cause significant\nmodel degradation due to over-smoothing [16, 17, 19, 20] and over-squashing [18, 21] issues.\nOver-smoothing means that the node representation becomes indistinguishable when stacked in\nmultiple layers. Inspired by ResNet [28] and DenseNet [29] proposed for deep convolutional\nnetworks, DeepGCN [16] firstly handles the over-smoothing problem by increasing residual learning\nand connecting the output of GNN layers. DGN [20] is a group normalization method limiting the\nembedding distribution of each node set into a specific mean and variance. EGNN [17] constrains the\nDirichlet energy embedded by nodes of each GNN layer to avoid over-smoothing. Ordered GNN [19]\nis one of the state-of-the-art methods, it splits the node representation into several segments and\nencodes the information of the node's different-distance neighbors into them to avoid nodes' feature\nmixing within hops. Over-squashing is firstly mentioned in FA [18]: as GNN layers increase, the\nneighbors' information in the perceiving domain of a node increases exponentially, and a fixed-length\nnode representation cannot encode and learn long-range signals in graphs well. FA proposes a simple\nand effective way to solve it by changing the graph structure at the last GNN layer to a fully adjacent\ngraph. SDRF [21] alleviates over-squashing by adding some edges to improve the curvature of the\ngraph. Besides the above two views, DAGNN [30] thinks that deep GNN degradation is caused by\nthe entanglement of the transformation and propagation of node representations.\nWe think that the performance degradation of deep GNN may be caused by the interference brought\nby mediate nodes with low-quality representations. Many indirectly connected nodes in graphs rely\non intermediary nodes for message propagation in deep GNNs. When an intermediary node has a\nlow-quality representation, it can easily disrupt the flow of message passing along the nodes."}, {"title": "Method", "content": "In this section, we first introduce the general formalization of GNN models, then introduce our\nproposed SF-GNN and show how to apply SF-GNN to different GNNs."}, {"title": "General Formalization of GNN Models", "content": "In a graph $G = (V,E, \\mathcal{Y}, R)$, $V$ and $E$ are the node and edge set respectively. $\\mathcal{Y}$ is the set of node\nlabels (or node types). $R$ is the set of relations (or edge types). Each node $v \\in V$ has a label $y_v \\in \\mathcal{Y}$.\n$(u, r, v) \\in E$ denotes an edge from node $u \\in V$ to node $v \\in V$ connected by relation $r \\in R$. If $G$ is a\nhomogeneous graph, there is only one relation (or type of edges) in the graph, and the types of node\n$u$ and node $v$ in each edge $(u, r, v)$ are the same, i.e. $y_u = y_v$. Let $h_v^{(0)} \\in \\mathbb{R}^d$ and $h_r^{(0)} \\in \\mathbb{R}^d$ denote\nthe initial representation of node $v \\in V$ and relation $r \\in R$, respectively. In the model of a specific\ntask, the GNN plays the role of an encoder and combines with a task-related decoder, forming an\nencoder-decoder architecture."}, {"title": "GNN-based Encoder", "content": "The encoder is composed of a $L$-layer GNN and aims to enhance the node\nrepresentation by capturing structure information of graphs. In each GNN layer, each node transfers\na message to its neighbor nodes, and then each node receives and aggregates messages from its\nneighbor nodes. Finally, each node updates its node representation by fusing its old representation\nwith the aggregation result. More specifically, the l+1-th layer is a parameterized function $f_{JG}^{(l+1)}$\nthat updates the representation of node $v \\in V$ by acting on its node representation $h_v^{(l)}$ output by the\nl-th layer and the representations of node $v$'s neighbor nodes $h_u^{(l)}$ and associated relations $h_r^{(l)}$. The\nrepresentation of node $v$ output by the l+1-th GNN layer can be updated by\n$$h_v^{(l+1)} = f_{JG}^{(l+1)}(h_v^{(l)}; \\{ h_u^{(l)}; h_r^{(l)} | (u,r) \\in N_v \\}; \\theta^{(l+1)})$$"}, {"title": "Task-specific Decoder", "content": "The decoder is a task-specific function $f_{task}(h_v^{(L)}; h_r^{(L)})$ whose input is\nthe node and relation representations output by the encoder. Its role is to give the final result related\nto the specific task. For example, in the node classification task, it can be a multilayer perceptron\nnetwork to give the label distribution of nodes [19, 8, 10], and in the link prediction task, it can be a\nconventional KGE [14, 15, 5] model to give the confidence of the input edge [13, 11, 12]."}, {"title": "Our Method", "content": "The phenomenon of performance degradation with the increase of GNN layers has been studied in\ndeep GNN-related works, and there are currently two mainstream views: over-smoothing of the node\nrepresentation [16, 17, 19, 20] and over-squashing of the node information [18, 21].\nIn this work, we put forward a different point of view. We hypothesize that the performance\ndegradation of deep-layer GNNs is due to the interference brought by mediate nodes with low-quality\nrepresentations in the propagation process. Taking Figure 2(a) as an example, node A wants to\ntransmit its representation $h_A^{(l)}$ at the l+1-th layer. According to Equation (1), after node C receives\nrepresentation $h_A^{(l)}$ from node A, it fuses $h_A^{(l)}$ with its node representation $h_C^{(l)}$ output by the last layer,\nand updates its representation as the fusion result $h_C^{(l+1)}$, which will be propagated at the next layer.\nThis process will repeat along the path between A and the other nodes. Although $h_A^{(l)}$ can eventually\nbe propagated to other nodes in the graph, such as node D, multiple hops away by stacking multiple\nlayers of GNN, representation $h_A^{(l)}$ will be mixed with $h_C^{(l)}$ of the intermediate node C. Suppose $h_C^{(l)}$\nis of low-quality, $h_A^{(l)}$ may be affected and damaged. There are amounts of indirectly adjacent nodes\nin graphs, and these nodes rely on other nodes as intermediaries for information propagation. Thus,\nif the representation of an intermediary node $v$ is of low quality, it will interfere with the message\npropagation among those indirectly adjacent nodes via node $v$."}, {"title": "Metric for the Node Representation Quality Assessment", "content": "To distinguish the low-quality representations, we need a metric for node quality assessment. Since\nthe output of the decoder is the probability of the corresponding class of a node or the confidence of\nan edge, a high output value means that the input representation can enable the decoder to produce a\ncorrect prediction of node type or edge, and a low output value means the decoder cannot predict\ncorrectly based on input representation. Thus, we can use the output of the decoder to score the\nquality of the node representation.\nSpecifically, to quantify the quality of the node representation at the middle layer of GNN, we input\nthe node representation of the middle layer into the decoder and run the decoder again. For the\nrepresentation of node $v$ at the l-th GNN layer, we define its quality as\n$$qual(v) = f_{task}(h_v^{(l)}; h_r^{(l)})$$"}, {"title": "SF-GNN", "content": "To achieve the goal that information transmission between indirect adjacent nodes is less interfered\nwith by intermediate nodes' low-quality representation, we propose a simple, effective, and general\nmethod SF-GNN, with minor modification to the original GNN models.\nFor a node with low-quality representation, it is necessary not only to block the propagation of its own\nfeatures, but also to continue to propagate the information from its neighbors to other nodes. This\ncannot be achieved by a conventional GNN, which uses a single representation for each node, because\nblocking one node's representation would simultaneously block the information being transmitted\nfrom other nodes. Thus an independent representation to store the information that the node received\nand will propagate to other nodes is necessary. In SF-GNN, we define two types of representations\nfor each node $v$: 1) the node representation, denoted as $h_v^{(l)}$, represents the feature of the node itself\nat the 1-th layer, and 2) the message representation, denoted as $m_v^{(l)}$, represents the information that\nnode $v$ will transmit to its neighbors at the next layer."}, {"title": "Experiment", "content": "We evaluate the effect of SF-GNN on the node classification task and link prediction task, and we\nfocus on answering the following research questions: (RQ1) Does our method make GNN models\nachieve better completion results? (RQ2) Does our method delay the performance decrease when\nstacking GNN layers? (RQ3) Does our method slow the degree of performance decrease when\nstacking GNN layers?"}, {"title": "Experiment Setting", "content": ""}, {"title": "Dataset and Evaluation Metric", "content": "We use three homophily citation graphs [2]: Cora, CiteSeer, and PubMed, and six heterophily\nweb network graphs [32, 33]: Actor, Texas, Cornell, Wisconsin, Squirrel, and Chameleon for node\nclassification task following [19]. For the metric, we adopt the mean classification accuracy with the\nstandard deviation on the test nodes over 10 random data splits the same as [19].\nFor more complex graphs, we use two com-\nmon link prediction benchmark knowledge graphs\nWN18RR [4] and FB15K237 [5] shown in Ta-\nble 1, where nodes and edges are also called entities\nand triples. We adopt standard metrics MRR and\nHit@k (k = 1,3, 10). Given a test edge (or triple)\n(u, r, v), we first generate candidate triples by re-\nplacing head entity u with each entity in KG, then we calculate these candidate triples' scores and"}, {"title": "Baselines", "content": "For node classification task on homophily and heterophily graphs, we apply SF-GNN on several\nclassic and currently best-performing GNN models including GAT [8], GraphSAGE [10], and Ordered\nGNN [19]. For link prediction task on knowledge graphs, we apply SF-GNN on 3 commonly used\nGNN-based KGE models including R-GCN [11], CompGCN [12], and NBFNet [13].\nWe denote the original GNN-based model without any method to tackle the deep GNN degradation\nproblem as the \u201cbase\u201d model. We compare the original GNN model applying SF-GNN (denoted\nas \u201c+ SF-GNN\u201d) to that applying following 3 baseline methods proposed for solving deep GNN\ndegradation problem: 1) DenseGCN [16], denoted as \u201c+ DGCN\u201d, is inspired by DenseNet [29] and\nconnects all the intermediate GNN layer outputs to enable efficient reuse of features among layers; 2)\nFully Adjacent [18], denoted as \u201c+ FA\u201d, thinks deep GNN degradation is caused by over-squashing.\nFA solves this by setting the input graph for the last GNN layer to be fully connected with no\nchange to the model structure; and 3) Ordered GNN [19], denoted as \"+ OG\", is the state-of-the-art\nmethod solving over-smoothing in deep GNN. It alleviates the information confusion of different-hop\nneighbors by encoding them into different segments of the representations."}, {"title": "Implementation", "content": "For the node classification task, following [19], we report the mean classification accuracy with the\nstandard deviation on the test nodes over 10 random data splits as [32]. We use 8-layer GNN, which\nis already deeper than most popular GNN models, for each dataset and we set the representation\ndimension d = 256 for GNN models the same as [19]. For the link prediction task, we implement SF-\nGNN by extending NeuralKG [34], an open-source KGE framework based on PyTorch that includes\nimplementations of various GNN-based KGE models. We set the GNN layers L = {1, 2, 3, 4, 5} for\nR-GCN and CompGCN, and L = {2, 4, 6, 8, 10} for NBFNet according to their original GNN layers\n(2, 1 and 6 for R-GCN, CompGCN and NBFNet). Embedding dimension d = 200 for CompGCN,\nd = 300 on WN18RR and d = 500 on FB15K237 for R-GCN, and d = 32 for NBFNet. Batch size is\n2048 for CompGCN, 10000 on WN18RR and 40000 on FB15K237 for R-GCN, and 32 on WN18RR\nand 64 on FB15K237 for NBFNet. We generate 10 negative triples for each positive one by randomly\nreplacing its head or tail entity with another entity. We use Adam [35] optimizer with a linear decay\nlearning rate scheduler. The other implementation details of baseline methods are the same as their\noriginal papers. We perform a search on the initial learning rate in {0.0001, 0.00035, 0.001, 0.005}\nand report results from the best one. All experiments are performed on a single NVIDIA Tesla A100\n40GB GPU."}, {"title": "Node Classification on Homophily and Heterophily Graphs", "content": ""}, {"title": "Link Prediction on Knowledge Graphs", "content": ""}, {"title": "Results of Different GNN Layers", "content": ""}, {"title": "Analysis of SFM Output", "content": "To verify whether SF-GNN has effectively evaluated and filtered the low-quality representation of\nmediate nodes in the information propagation process, we analyze the middle output of SFM in the\n5-layer CompGCN applying SF-GNN at the entity level on the test data of WN18RR and FB15K237.\nFirst, for entity node v in test set,\nwe take the corresponding output\nof SFM at each GNN layer, that\nis $SFM(h_v^{(i)}), i=0, 1, 2, 3, 4$.\nWe then divide entities of test\nset into 6 classes, for which\nthe total times of $SFM(h_v^{(i)})=1$\nare 0,1,2,3,4,5, respectively,\nthat is $C_i = \\{vv \\in V_{test} \\mid$\n$\\sum_{i\\epsilon[0,4]} SFM(h_v^{(i)}) = i\\}$.\nIt is difficult to directly evaluate\nthe quality of entity node representation. There is a broad assumption in KGE that the score of\nthe positive triple is positively correlated with the quality of entity representation. A low score for\npositive triple (u, r, v) means that u's or v's node representation is low-quality. Thus, we calculate\nthe quality of v's node representation by averaging the prediction ranks of all test triples related to v,\ndenoted as the average triples rank $atr_v$=mean($\\{rank_{(*,r,v)}\\} \\cup \\{rank_{(v,r,*)}\\}$). We tally MRR as\nthe mean reciprocal $atr_v$ of entities of each class $C_i$, and Hit@k as the percentage of entities with\natr$_{-v}$<=k for class $C_i$ in Table 4.\nThis result is in line with our hypothesis and expectation that the more times an entity's node\nrepresentation is propagated, the better its node representation quality. The fewer propagation times\nof an entity's node representation means that the node representation has not been learned well\nenough in most GNN layers, which will harm the final node representation performance. Entities\nof the class $C_0$ have the worst performance, their node representations are filtered out and never\npropagated at each layer. In this case, SF-GNN limits the propagation of such poor-quality node\nrepresentations, reduces their damage to high-quality information in the message propagation process,\nand improves the overall performance of GNN models."}, {"title": "Conclusion and Future Work", "content": "In this work, we propose a new view about performance degradation of deep GNN that it's because\nlow-quality node representations interfere with and damage propagating messages, and we propose an\neasy, universal, and effective method SF-GNN to solve it. In SF-GNN, we define two representations\nfor each node: one is the node representation that represents the node's feature and the other is the\nmessage representation that is propagated to the neighbor nodes. A self-filter module is designed\nto evaluate the quality of node representation and filter out low-quality node representation from\nmessage propagation. The experimental results show the effectiveness and superiority of our method\nin mitigating the performance degradation of deep GNN. SF-GNN makes it feasible to improve GNN\nmodels by stacking GNN layers, but it also brings increasing model parameters. In the future, we\nhope to explore improving GNN while minimizing the number of model parameters."}, {"title": "GNN-based KGE", "content": "In this section, we first introduce the general formalization of GNN-based KGE models, then introduce our\nproposed SF-GNN and show how to apply SF-GNN to them."}, {"title": "GNN-based KGE", "content": ""}, {"title": "Encoder", "content": "The encoder function $f_{JG}^{(l+1)}$ in Equation (1) is the core of the encoder and it's significantly distinguishable in\ndifferent GNN-based KGE models.\nIn R-GCN [11], $f_{JG}^{(l+1)}$ is defined as\n$$h_v^{(l+1)} = \\sigma(\\sum_{r\\epsilon R} \\sum_{(u,r) \\epsilon N_v} \\frac{1}{c_{v,r}}W_{ent}^{(l+1)} h_u^{(l)}+W_{rel}^{(l+1)} h_r^{(l)})$$\nwhere $W_{ent}^{(l+1)}$ is parameter matrix and $W_{rel}^{(l+1)}$ is the relation-specific parameter matrix at the l+1-th layer, $c_{v,r}$\nis regularization coefficient, $\u03c3$ is activation function.\nIn CompGCN [12], $f_{JG}^{(l+1)}$ is\n$$h_v^{(l+1)} = \\sum_{(u,r) \\epsilon N_v} f_{JG}^{(l+1)}(h_v^{(l)}, \\phi(W_r^{(l+1)}h_r^{(l)}))$$\nwhere $W_r^{(l+1)}$ represents the parameter matrix of $r \\epsilon R$ at the l+1-th layer, $\u03bb_r$ has three values according\nto the incoming edge, the outgoing edge and the self-connected edge, $(\u00b7)$ is the combination operation of\nentity and relation representations and is subtraction function (TransE [14] as decoder), multiplication function\n(DistMult [15] as decoder) or circular-correlation function (ConvE [5] as decoder), relation representation in\neach layer is updated by $h_r^{(l+1)} = W_r^{(l+1)}h_r^{(l)}$, where $W_r^{(l+1)}$ is the relation-specific matrix.\nIn NBFNet [13], for a given source entity u and query relation q, $f_{JG}^{(l+1)}$ learn pair representation for pair (v, u)\nof source entity v and all u \u2208 V by\n$$h_q^{(l+1)}(v, u) = PNA( \\{MSG(h_q^{(l)}(*,x); (x, r, u))|(x, r, u) \\epsilon E\\})$$\nwhere $h_q^{(0)}(v, u) = IND(v, u, q)$, IND(\u00b7) initializes a query relation embedding $h_q$ on entity u if u equals to\nv and zero otherwise, MSG$(h_q^{(l)}(*, x); (x, r, u)) = h_q^{(l)}(*, x) \\bigodot (W_{rel}^{(l+1)} h_q + b_{rel}^{(l+1)})$, $W_{rel}^{(l+1)}$ and $b_{rel}^{(l+1)}$\nare relation-specific parameter matrix and bias vector at the 1+1-th layer, $ \\bigodot $ is element-wise multiplication,\nPNA(\u00b7) is the principal neighborhood aggregation [36] function.\nIn SE-GNN [25], $f_{JG}^{(l+1)}$ is defined as\n$$h_v^{(l+1)} = s_{rel}^{(l)} + s_{ent}^{(l)} + s_{tri}^{(l)} + h_v^{(l)}$$\n$$s_{rel}^{(l)} = \\sigma(\\sum_{(u,r) \\epsilon N_v} \\alpha_{v,r}^{(rel,l+1)} W_{rel}^{(l+1)} h_r^{(l)})$$\n$$s_{ent}^{(l)} = \\sigma(\\sum_{(u,r) \\epsilon N_v} \\alpha_{v,r}^{(ent,l+1)} W_{ent}^{(l+1)} h_u^{(l)})$$\n$$s_{tri}^{(l)} = \\sigma(\\sum_{(u,r) \\epsilon N_v} \\alpha_{v,r}^{(tri,l+1)} (h_r^{(l)}, h_u^{(l)}))$$"}, {"title": "Decoder", "content": "The decoder is mainly composed of a traditional KGE method such as TransE [14], DistMult [15], and ConvE [5].\nFor a given triple (u, r, v), firstly, the final entity representations $h_u$ and $h_v$ of entities u and v and relation\nrepresentation $h_r$ of relation r are easily calculated based on the output of the encoder, where $h_u = h_v^{(L)}$\nand $h_r = h_r^{(L)}$ in R-GCN [11]; $h_u = h_v^{(L)}$ and $h_r = h_r^{(L)}$ in CompGCN [12]; $h_u = h_q^{(L)}$ and $h_r =$\nWoutConcat([h(1), h(2), ..., h(4)]) in SE-GNN [25]. Then the decoder scores the authenticity of triple\n(u, r, v) through the scoring function $f_{task}$ of the KGE method, where $f_{task}(u, r, v) = -||h_u + h_r - h_v ||$ for"}, {"title": "Node representation quality in GNN-based KGE model", "content": "In GNN-based KGE models, the representation quality of an entity node x is calculated as the average edges'\nconfidence (triples' scores) of this entity:\n$$qual(v) = \\frac{1}{|E_x|} \\sum_{(u,r,v) \\epsilon E_x} f_{task}(u,r,v)$$\nwhere $E_x = \\{(u, r, v) u = x or v = x, (u, r, v) \\epsilon E\\}$ is the set of triples related to entity node x, $f_{task}$ has the\nsame form as the triple score function $f_{task}$ in Section A.1.2 but the input representations of the entity and relation\nare calculated based on the output of the encoder's first I layers, that is $h_u = h_v^{(l)}, h_r = h_r^{(0)}$ for R-GCN,$h_u =$\n$h_v^{(l)}, h_r = h_r^{(0)}$ for CompGCN, $h_u = h_q^{(l)}(e, v)$ for NBFNet, and $h_r = W_{out}Concat([h^{(1)}, h^{(2)}, ..., h^{(l)}])$ for\nSE-GNN. Higher qual(v) indicates higher representation quality of node x."}]}