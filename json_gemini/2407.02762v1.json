{"title": "SF-GNN: Self Filter for Message Lossless Propagation in Deep Graph Neural Network", "authors": ["Yushan Zhu", "Wen Zhang", "Yajing Xu", "Zhen Yao", "Mingyang Chen", "Huajun Chen"], "abstract": "Graph Neural Network (GNN), with the main idea of encoding graph structure\ninformation of graphs by propagation and aggregation, has developed rapidly. It\nachieved excellent performance in representation learning of multiple types of\ngraphs such as homogeneous graphs, heterogeneous graphs, and more complex\ngraphs like knowledge graphs. However, merely stacking GNN layers may not im-\nprove the model's performance and can even be detrimental. For the phenomenon\nof performance degradation in deep GNNs, we propose a new perspective. Unlike\nthe popular explanations of over-smoothing or over-squashing, we think the issue\narises from the interference of low-quality node representations during message\npropagation. We introduce a simple and general method, SF-GNN, to address\nthis problem. In SF-GNN, we define two representations for each node, one is\nthe node representation that represents the feature of the node itself, and the other\nis the message representation specifically for propagating messages to neighbor\nnodes. A self-filter module evaluates the quality of the node representation and\ndecides whether to integrate it into the message propagation based on this quality\nassessment. Experiments on node classification tasks for both homogeneous and\nheterogeneous graphs, as well as link prediction tasks on knowledge graphs, demon-\nstrate that our method can be applied to various GNN models and outperforms\nstate-of-the-art baseline methods in addressing deep GNN degradation.", "sections": [{"title": "1 Introduction", "content": "Graphs are widely used to model structured and relational data [1]. There are homogenous graphs\nconsisting of a single type of nodes and edges such as document citation networks [2], heterogeneous\ngraphs consisting of several different types of nodes and edges such as web and social networks [3],\nand more complex graphs like Knowledge Graphs (KGs) [4, 5] containing hundreds or thousands of\nnode and relational edge types. Mining and analyzing graphs can enhance a wide range of real-world\napplications, such as node classification and link prediction. A common approach for graph mining is\nto map the nodes and edges in a graph into continuous vector spaces through graph representation\nlearning. These vector representations of nodes and edges can then be utilized for various tasks. In\nrecent years, Graph Neural Networks (GNNs) [6, 7, 8, 9, 10] have rapidly developed and become the\nleading approach to graph representation learning.\nThe core concept of GNNs is to learn rich node representations through feature propagation and\naggregation. Multi-layer GNNs typically serve as encoders to capture extensive graph structure\ninformation. They work in conjunction with a task-related decoder. For instance, in node classification\ntasks, a multi-layer perceptron (MLP) is often used as the decoder. In GNN-based knowledge graph\nembedding (KGE) models [11, 12, 13], conventional KGE methods [14, 15, 5] are employed as"}, {"title": "2 Related Work", "content": "Graph Neural Networks The graph neural network (GNN) works primarily on a message-passing\nmechanism [6], where each node updates its representation by aggregating messages propagated\nfrom its direct neighbors. Classical GNN variants such as GGNN [7], GAT [8], GCN [9] and\nGraphSAGE [10], are mainly different in the way each node aggregates the representation of its\nneighbors with its own representation. Learning heterogeneous graphs containing more than one\ntypes of nodes and edges has always been a thorny problem for GNN, and many methods for learning\nheterogeneous graphs have been gradually proposed [22, 23, 24, 19]. Knowledge graph (KG) is\na more complex and representative heterophily graph that may contain hundreds or thousands of\ntypes of entities and edges, where nodes and edges are also called entities and triples. with the\ndevelopment of GNNs, knowledge graph embedding learning methods based on GNN, namely\nGNN-based KGEs [11, 12, 25, 13], show good performance. These models are of encoder-decoder\narchitectures, where the encoder is a multi-layer GNN to enhance the node representation by capturing\nglobal or local structure and the decoder utilizes a traditional KGE such as TransE [14], DistMult [15],\nand ConvE [5] to predict triples. R-GCN [11] firstly applies GCN [26, 9] in KGE, where GCN is\nthe encoder to deal with the highly multi-relational data characteristic of KGs and DistMult is the\ndecoder. However, R-GCN lacks the learning of edge feature (relation representation), in which\nthe edge only serves as an attribute to label the class of the target node. CompGCN [12] takes the\ninformation of multi-type edges into the calculation of KGs, jointly learning the representations of\nentities and relations, and uses TransE, DistMult, or ConvE as the decoder. RGHAT [27] emphasizes\nthe importance of different relations and different neighbor entities with the same relation by relation-\nlevel and entity-level attentions, and it uses ConvE as the decoder. NBFNet [13] encodes the\nrepresentation of a pair of nodes as the generalized sum of all path representations between nodes\nand a path representation as the generalized product of edge representations in the path, then applies\na multi-layer perceptron (MLP) as the decoder. SE-GNN [25] focuses on semantic evidence (SE) for\nKGE models with strong extrapolation capability, the encoder models relation-level, entity-level, and\ntriple-level SE, its decoder is ConvE.\nDegradation of Deep GNNs Ideally, stacking GNN layers can expand the propagation range\nof messages, make nodes perceive richer graph structure information, and enhance the model\nperformance. However, recent studies have shown that stacking GNN layers can cause significant\nmodel degradation due to over-smoothing [16, 17, 19, 20] and over-squashing [18, 21] issues.\nOver-smoothing means that the node representation becomes indistinguishable when stacked in\nmultiple layers. Inspired by ResNet [28] and DenseNet [29] proposed for deep convolutional\nnetworks, DeepGCN [16] firstly handles the over-smoothing problem by increasing residual learning\nand connecting the output of GNN layers. DGN [20] is a group normalization method limiting the\nembedding distribution of each node set into a specific mean and variance. EGNN [17] constrains the\nDirichlet energy embedded by nodes of each GNN layer to avoid over-smoothing. Ordered GNN [19]\nis one of the state-of-the-art methods, it splits the node representation into several segments and\nencodes the information of the node's different-distance neighbors into them to avoid nodes' feature\nmixing within hops. Over-squashing is firstly mentioned in FA [18]: as GNN layers increase, the\nneighbors' information in the perceiving domain of a node increases exponentially, and a fixed-length\nnode representation cannot encode and learn long-range signals in graphs well. FA proposes a simple\nand effective way to solve it by changing the graph structure at the last GNN layer to a fully adjacent\ngraph. SDRF [21] alleviates over-squashing by adding some edges to improve the curvature of the\ngraph. Besides the above two views, DAGNN [30] thinks that deep GNN degradation is caused by\nthe entanglement of the transformation and propagation of node representations.\nWe think that the performance degradation of deep GNN may be caused by the interference brought\nby mediate nodes with low-quality representations. Many indirectly connected nodes in graphs rely\non intermediary nodes for message propagation in deep GNNs. When an intermediary node has a\nlow-quality representation, it can easily disrupt the flow of message passing along the nodes."}, {"title": "3 Method", "content": "In this section, we first introduce the general formalization of GNN models, then introduce our\nproposed SF-GNN and show how to apply SF-GNN to different GNNs."}, {"title": "3.1 General Formalization of GNN Models", "content": "In a graph $G = (V,E, Y, R)$, $V$ and $E$ are the node and edge set respectively. $Y$ is the set of node\nlabels (or node types). $R$ is the set of relations (or edge types). Each node $v \\in V$ has a label $y_v \\in Y$.\n$(u, r, v) \\in E$ denotes an edge from node $u \\in V$ to node $v \\in V$ connected by relation $r \\in R$. If $G$ is a\nhomogeneous graph, there is only one relation (or type of edges) in the graph, and the types of node\n$u$ and node $v$ in each edge $(u, r, v)$ are the same, i.e. $Y_u = y_v$. Let $h_v^{(0)} \\in R^d$ and $h_r^{(0)} \\in R^d$ denote\nthe initial representation of node $v \\in V$ and relation $r \\in R$, respectively. In the model of a specific\ntask, the GNN plays the role of an encoder and combines with a task-related decoder, forming an\nencoder-decoder architecture.\nGNN-based Encoder The encoder is composed of a $L$-layer GNN and aims to enhance the node\nrepresentation by capturing structure information of graphs. In each GNN layer, each node transfers\na message to its neighbor nodes, and then each node receives and aggregates messages from its\nneighbor nodes. Finally, each node updates its node representation by fusing its old representation\nwith the aggregation result. More specifically, the l+1-th layer is a parameterized function $f_\\theta^{(l+1)}$\nthat updates the representation of node $v \\in V$ by acting on its node representation $h_v^{(l)}$ output by the\nl-th layer and the representations of node $v$'s neighbor nodes $h_u^{(l)}$ and associated relations $h_r^{(l)}$. The\nrepresentation of node $v$ output by the l+1-th GNN layer can be updated by\n$$h_v^{(l+1)} = f_\\theta^{(l+1)}(h_v^{(l)}; \\{h_u^{(l)}; h_r^{(l)}\\}(u,r) \\in N_v; \\theta^{(l+1)})$$\nwhere $N_v = \\{(u, r)|(u, r, v) \\in E\\}$ denotes the set of $v$'s neighbor node $u \\in V$ connected by relation\n$r \\in R$ in the graph. $h_v^{(l)}$, $h_u^{(l)}$ and $h_r^{(l)}$ are representations of node $v$, $u$ and relation $r$ output by\nthe l-th layer. $\\theta^{(l+1)}$ is the set of other parameters at the l+1-th layer, $f_\\theta^{(l+1)}$ is the propagation and\naggregation function which is different in different GNN models. Some examples are in the Appendix.\nTask-specific Decoder The decoder is a task-specific function $f_{task}(h_v^{(L)}; h_r^{(L)})$ whose input is\nthe node and relation representations output by the encoder. Its role is to give the final result related\nto the specific task. For example, in the node classification task, it can be a multilayer perceptron\nnetwork to give the label distribution of nodes [19, 8, 10], and in the link prediction task, it can be a\nconventional KGE [14, 15, 5] model to give the confidence of the input edge [13, 11, 12]."}, {"title": "3.2 Our Method", "content": "The phenomenon of performance degradation with the increase of GNN layers has been studied in\ndeep GNN-related works, and there are currently two mainstream views: over-smoothing of the node\nrepresentation [16, 17, 19, 20] and over-squashing of the node information [18, 21].\nIn this work, we put forward a different point of view. We hypothesize that the performance\ndegradation of deep-layer GNNs is due to the interference brought by mediate nodes with low-quality\nrepresentations in the propagation process. Taking Figure 2(a) as an example, node A wants to\ntransmit its representation $h_A^{(l)}$ at the l+1-th layer. According to Equation (1), after node C receives\nrepresentation $h_A^{(l)}$ from node A, it fuses $h_A^{(l)}$ with its node representation $h_C^{(l)}$ output by the last layer,\nand updates its representation as the fusion result $h_C^{(l+1)}$, which will be propagated at the next layer.\nThis process will repeat along the path between A and the other nodes. Although $h_A^{(l)}$ can eventually\nbe propagated to other nodes in the graph, such as node D, multiple hops away by stacking multiple\nlayers of GNN, representation $h_D^{(l)}$ will be mixed with $h_C^{(l)}$ of the intermediate node C. Suppose $h_C^{(l)}$\nis of low-quality, $h_D^{(l)}$ may be affected and damaged. There are amounts of indirectly adjacent nodes\nin graphs, and these nodes rely on other nodes as intermediaries for information propagation. Thus,\nif the representation of an intermediary node $v$ is of low quality, it will interfere with the message\npropagation among those indirectly adjacent nodes via node v."}, {"title": "3.2.1 Metric for the Node Representation Quality Assessment", "content": "To distinguish the low-quality representations, we need a metric for node quality assessment. Since\nthe output of the decoder is the probability of the corresponding class of a node or the confidence of\nan edge, a high output value means that the input representation can enable the decoder to produce a\ncorrect prediction of node type or edge, and a low output value means the decoder cannot predict\ncorrectly based on input representation. Thus, we can use the output of the decoder to score the\nquality of the node representation.\nSpecifically, to quantify the quality of the node representation at the middle layer of GNN, we input\nthe node representation of the middle layer into the decoder and run the decoder again. For the\nrepresentation of node v at the l-th GNN layer, we define its quality as\n$$qual(v) = f_{task}(h_v^{(l)}; h_r)$$\nwhere $f_{task}$ is the same as that in Section 3.1. A higher $qual(v)$ indicates a better quality of the\nnode representation $h_v^{(l)}$. Our intuition behind the metric $qual(v)$ for evaluating the quality of\nrepresentations across layers is as follows: In original GNN models, the decoder evaluates the output\nof the last GNN layer. When GNN layers with the same structure are stacked, node representations at\ndifferent layers aggregate information from neighbors at varying distances using identical propagation\nand aggregation functions. This means that, although the amount of information increases, the\npatterns of information across layers remain fundamentally similar. Therefore, the decoder can be\neffectively used to assess the quality of these representations."}, {"title": "3.2.2 SF-GNN", "content": "To achieve the goal that information transmission between indirect adjacent nodes is less interfered\nwith by intermediate nodes' low-quality representation, we propose a simple, effective, and general\nmethod SF-GNN, with minor modification to the original GNN models.\nFor a node with low-quality representation, it is necessary not only to block the propagation of its own\nfeatures, but also to continue to propagate the information from its neighbors to other nodes. This\ncannot be achieved by a conventional GNN, which uses a single representation for each node, because\nblocking one node's representation would simultaneously block the information being transmitted\nfrom other nodes. Thus an independent representation to store the information that the node received\nand will propagate to other nodes is necessary. In SF-GNN, we define two types of representations\nfor each node v: 1) the node representation, denoted as $h_v^{(l)}$, represents the feature of the node itself\nat the l-th layer, and 2) the message representation, denoted as $m_v^{(l)}$, represents the information that\nnode $v$ will transmit to its neighbors at the next layer."}, {"title": "4 Experiment", "content": "We evaluate the effect of SF-GNN on the node classification task and link prediction task, and we\nfocus on answering the following research questions: (RQ1) Does our method make GNN models\nachieve better completion results? (RQ2) Does our method delay the performance decrease when\nstacking GNN layers? (RQ3) Does our method slow the degree of performance decrease when\nstacking GNN layers?"}, {"title": "4.1 Experiment Setting", "content": "4.1.1 Dataset and Evaluation Metric\nWe use three homophily citation graphs [2]: Cora, CiteSeer, and PubMed, and six heterophily\nweb network graphs [32, 33]: Actor, Texas, Cornell, Wisconsin, Squirrel, and Chameleon for node\nclassification task following [19]. For the metric, we adopt the mean classification accuracy with the\nstandard deviation on the test nodes over 10 random data splits the same as [19].\nFor more complex graphs, we use two com-\nmon link prediction benchmark knowledge graphs\nWN18RR [4] and FB15K237 [5] shown in Ta-\nble 1, where nodes and edges are also called entities\nand triples. We adopt standard metrics MRR and\nHit@k (k = 1,3, 10). Given a test edge (or triple)\n(u, r, v), we first generate candidate triples by re-\nplacing head entity u with each entity in KG, then we calculate these candidate triples' scores and"}, {"title": "4.1.2 Baselines", "content": "For node classification task on homophily and heterophily graphs, we apply SF-GNN on several\nclassic and currently best-performing GNN models including GAT [8], GraphSAGE [10], and Ordered\nGNN [19]. For link prediction task on knowledge graphs, we apply SF-GNN on 3 commonly used\nGNN-based KGE models including R-GCN [11], CompGCN [12], and NBFNet [13].\nWe denote the original GNN-based model without any method to tackle the deep GNN degradation\nproblem as the \u201cbase\u201d model. We compare the original GNN model applying SF-GNN (denoted\nas \u201c+ SF-GNN\u201d) to that applying following 3 baseline methods proposed for solving deep GNN\ndegradation problem: 1) DenseGCN [16], denoted as \u201c+ DGCN\u201d, is inspired by DenseNet [29] and\nconnects all the intermediate GNN layer outputs to enable efficient reuse of features among layers; 2)\nFully Adjacent [18], denoted as \u201c+ FA\u201d, thinks deep GNN degradation is caused by over-squashing.\nFA solves this by setting the input graph for the last GNN layer to be fully connected with no\nchange to the model structure; and 3) Ordered GNN [19], denoted as \"+ OG\", is the state-of-the-art\nmethod solving over-smoothing in deep GNN. It alleviates the information confusion of different-hop\nneighbors by encoding them into different segments of the representations."}, {"title": "4.1.3 Implementation", "content": "For the node classification task, following [19], we report the mean classification accuracy with the\nstandard deviation on the test nodes over 10 random data splits as [32]. We use 8-layer GNN, which\nis already deeper than most popular GNN models, for each dataset and we set the representation\ndimension d = 256 for GNN models the same as [19]. For the link prediction task, we implement SF-\nGNN by extending NeuralKG [34], an open-source KGE framework based on PyTorch that includes\nimplementations of various GNN-based KGE models. We set the GNN layers L = {1, 2, 3, 4, 5} for\nR-GCN and CompGCN, and L = {2, 4, 6, 8, 10} for NBFNet according to their original GNN layers\n(2, 1 and 6 for R-GCN, CompGCN and NBFNet). Embedding dimension d = 200 for CompGCN,\nd = 300 on WN18RR and d = 500 on FB15K237 for R-GCN, and d = 32 for NBFNet. Batch size is\n2048 for CompGCN, 10000 on WN18RR and 40000 on FB15K237 for R-GCN, and 32 on WN18RR\nand 64 on FB15K237 for NBFNet. We generate 10 negative triples for each positive one by randomly\nreplacing its head or tail entity with another entity. We use Adam [35] optimizer with a linear decay\nlearning rate scheduler. The other implementation details of baseline methods are the same as their\noriginal papers. We perform a search on the initial learning rate in {0.0001, 0.00035, 0.001, 0.005}\nand report results from the best one. All experiments are performed on a single NVIDIA Tesla A100\n40GB GPU."}, {"title": "4.2 Node Classification on Homophily and Heterophily Graphs", "content": "Table 2: Node classification results on homophily and heterophily graphs. The base results are\nreported by [19]. The best result is in bold."}, {"title": "4.3 Link Prediction on Knowledge Graphs", "content": "Table 3: Link prediction results. Column L is the number of layers achieving the performance. The\nbest result(baseline) is in bold(underlined).\nTable 3 shows the best result of the GNN-based KGE model applying different methods with its\ncorresponding best layers number, exceeding which model performance begins to decline. On each\nKGE model, SF-GNN almost achieves the best performance among all methods, followed by OG. OG\nperforms the best among all baseline methods. It divides the node representation into several segments\nand encodes information of different-distance neighbor entities into different representation segments.\nSo low-quality representations of mediate nodes only affect the corresponding representation seg-\nments. FA works worse in most cases, which simply modifies the graph as fully connected at the last\nGNN layer, it introduces too much noise by connecting many completely unrelated nodes in KGs\nthat are usually complex and sparse. Though DGCN, FA, and OG can improve the final performance\ncompared to the base model, they barely delay performance degradation with stacking GNN layers\nexcept for increasing the optimal stacking layers by only 1 layer for R-GCN and CompGCN on\nWN18RR. Using our SF-GNN, the performance degradation of GNN models can be delayed until\nstacking over 4 and 10 layers for R-GCN and NBFNet on WN18RR.\nThe results show that by filtering out the interference of low-quality node representations, SF-GNN\nachieves excellent performance against deep GNN degradation, answering our research question RQ1\nthat SF-GNN successfully makes GNN models achieve better results and RQ2 that SF-GNN delays\nthe performance decrease when stacking GNN layers."}, {"title": "4.4 Results of Different GNN Layers", "content": "Figure 3 shows the MRR and Hit@ 10 results of different GNN layers on WN18RR and FB15K237.\nAn obvious phenomenon is that as GNN layers increase, the GNN model applying SF-GNN works"}, {"title": "4.5 Analysis of SFM Output", "content": "To verify whether SF-GNN has effectively evaluated and filtered the low-quality representation of\nmediate nodes in the information propagation process, we analyze the middle output of SFM in the\n5-layer CompGCN applying SF-GNN at the entity level on the test data of WN18RR and FB15K237.\nFirst, for entity node v in test set,\nwe take the corresponding output\nof SFM at each GNN layer, that\nis $SFM(h_v^{(l)})$, i=0, 1, 2, 3, 4.\nWe then divide entities of test\nset into 6 classes, for which\nthe total times of $SFM(h_v^{(l)})=1$\nare 0,1,2,3,4,5, respectively,\nthat is $C_i = \\{vv \\in V_{test}\\t\\sum_{l\\in[0,4]} SFM(h_v^{(l)}) = i\\}$.\nIt is difficult to directly evaluate\nthe quality of entity node representation. There is a broad assumption in KGE that the score of\nthe positive triple is positively correlated with the quality of entity representation. A low score for\npositive triple (u, r, v) means that u's or v's node representation is low-quality. Thus, we calculate\nthe quality of v's node representation by averaging the prediction ranks of all test triples related to v,\ndenoted as the average triples rank $atr_v$=mean($\\{rank(*,r,v)\\} \\cup \\{rank(v,r,*)\\}$). We tally MRR as\nthe mean reciprocal atr of entities of each class $C_i$, and Hit@k as the percentage of entities with\natrk for class $C_i$ in Table 4.\nThis result is in line with our hypothesis and expectation that the more times an entity's node\nrepresentation is propagated, the better its node representation quality. The fewer propagation times\nof an entity's node representation means that the node representation has not been learned well\nenough in most GNN layers, which will harm the final node representation performance. Entities\nof the class $C_0$ have the worst performance, their node representations are filtered out and never\npropagated at each layer. In this case, SF-GNN limits the propagation of such poor-quality node\nrepresentations, reduces their damage to high-quality information in the message propagation process,\nand improves the overall performance of GNN models."}, {"title": "5 Conclusion and Future Work", "content": "In this work, we propose a new view about performance degradation of deep GNN that it's because\nlow-quality node representations interfere with and damage propagating messages, and we propose an\neasy, universal, and effective method SF-GNN to solve it. In SF-GNN, we define two representations\nfor each node: one is the node representation that represents the node's feature and the other is the\nmessage representation that is propagated to the neighbor nodes. A self-filter module is designed\nto evaluate the quality of node representation and filter out low-quality node representation from\nmessage propagation. The experimental results show the effectiveness and superiority of our method\nin mitigating the performance degradation of deep GNN. SF-GNN makes it feasible to improve GNN\nmodels by stacking GNN layers, but it also brings increasing model parameters. In the future, we\nhope to explore improving GNN while minimizing the number of model parameters."}, {"title": "A GNN-based KGE", "content": "In this section, we first introduce the general formalization of GNN-based KGE models, then introduce our\nproposed SF-GNN and show how to apply SF-GNN to them."}, {"title": "A.1 GNN-based KGE", "content": "A.1.1 Encoder\nThe encoder function $f_\\theta^{(l+1)}$ in Equation (1) is the core of the encoder and it's significantly distinguishable in\ndifferent GNN-based KGE models.\nIn R-GCN [11], $f_\\theta^{(l+1)}$ is defined as\n$$h_v^{(l+1)} = \\sigma(\\sum_{r\\in R} \\sum_{(u,r) \\in N_v} \\frac{1}{c_{v,r}}W_{ent}^{(l+1)}h_u^{(l)}+W_{rel}^{(l+1)}h_r^{(l)})$$\nwhere $W_{ent}^{(l+1)}$ is parameter matrix and $W_{rel}^{(l+1)}$ is the relation-specific parameter matrix at the l+1-th layer, $c_{v,r}$\nis regularization coefficient, $\\sigma$ is activation function.\nIn CompGCN [12], $f_\\theta^{(l+1)}$ is\n$$h_v^{(l+1)} = \\sum_{(u,r) \\in N_v} \\lambda_{\\gamma}f_{\\theta}^{(l+1)}(\\phi(h_u^{(l)}, h_r^{(l)}))$$\nwhere $W_{r}^{(l+1)}$ represents the parameter matrix of $r \\in R$ at the l+1-th layer, $\\lambda_{\\gamma}$ has three values according\nto the incoming edge, the outgoing edge and the self-connected edge, $\\phi(\u00b7)$ is the combination operation of\nentity and relation representations and is subtraction function (TransE [14] as decoder), multiplication function\n(DistMult [15] as decoder) or circular-correlation function (ConvE [5] as decoder), relation representation in\neach layer is updated by $h_r^{(l+1)} = W_r^{(l+1)}h_r^{(l)}$, where $W_r^{(l+1)}$ is the relation-specific matrix.\nIn NBFNet [13], for a given source entity u and query relation q, $f_\\theta^{(l+1)}$ learn pair representation for pair (v, u)\nof source entity v and all u \u2208 V by\n$$h_q^{(l+1)}(v, u) = PNA(\\left\\{MSG(h_q^{(l)}(*,x); (x, r, u))|(x, r, u) \\in E\\}\\right)$$\nwhere $h_q^{(0)}(v, u) = IND(v, u, q)$, IND(\u00b7) initializes a query relation embedding $h_q$ on entity u if u equals to\nv and zero otherwise, MSG($h_q^{(l)}(*,x); (x, r, u)$) = $h_q^{(l)}(*,x) \\odot (W^{(l+1)}h_q + b^{(l+1)})$, $W^{(l+1)}$ and $b^{(l+1)}$\nare relation-specific parameter matrix and bias vector at the 1+1-th layer, $\\odot$ is element-wise multiplication,\nPNA(\u00b7) is the principal neighborhood aggregation [36] function.\nIn SE-GNN [25], $f_\\theta^{(l+1)}$ is defined as\n$$h_v^{(l+1)} = s_{rel}^{(l)} + s_{ent}^{(l)} + s_{tri}^{(l)} + h_v^{(l)}$$\n$$s_{rel}^{(l)} = \\sigma( \\sum_{(u,r) \\in N_v}  \\alpha_{v,u}^{(rel,(l+1))}W_{rel}^{(l+1)}h_r^{(l)})$$\n$$s_{ent}^{(l)} = \\sigma( \\sum_{(u,r) \\in N_v}  \\alpha_{v,u}^{(ent,(l+1))}W_{ent}^{(l+1)}h_u^{(l)})$$\n$$s_{tri}^{(l)} = \\sigma( \\sum_{(u,r) \\in N_v}  \\alpha_{v,u}^{(tri,(l+1))}\\phi(h_u^{(l)}, h_r^{(l)}))$$"}, {"title": "A.1.2 Decoder", "content": "The decoder is mainly composed of a traditional KGE method such as TransE [14], DistMult [15], and ConvE [5].\nFor a given triple (u, r, v), firstly, the final entity representations $h_u$ and $h_v$ of entities u and v and relation\nrepresentation $h_r$ of relation r are easily calculated based on the output of the encoder, where $h = h_v^{(L)}$\nand $h_r = h_r^{(L)}$ in R-GCN [11]; $h_u = h_v^{(L)}$ and $h_r = h_r^{(L)}$ in CompGCN [12]; $h_u = h_q^{(L)}$ and $h_r =$\n$W_{out}Concat([h^{(1)}, h^{(2)}, ..., h^{(4)}])$ in SE-GNN [25]. Then the decoder scores the authenticity of triple\n(u, r, v) through the scoring function $f_{task}$ of the KGE method, where $f_{task}(u, r, v) = -||h_u + h_r - h_v||$ for"}, {"title": "A.2 Node representation quality in GNN-based KGE model", "content": "In GNN-based KGE models, the representation quality of an entity node x is calculated as the average edges'\nconfidence (triples' scores) of this entity:\n$$qual(x) = \\frac{1}{|E_x|}\\sum_{(u,r,v) \\in E_x} f_{task}(u,r,v)$$\nwhere $E_x = \\{(u, r, v) | u = x \\text{ or } v = x, (u, r, v) \\in E\\}$ is the set of triples related to entity node x, $f_{task}$ has the\nsame form as the triple score function $f_{task}$ in Section A.1.2 but the input representations of the entity and relation\nare calculated based on the output of the encoder's first $l$ layers, that is $h = h_v^{(l)}$, $h_r = h_r^{(l)}$ for R-GCN, $h =\nh_v^{(l)}$, $h_r = h_r^{(l)}$ for CompGCN, $h = h_q^{(l)}(e, v)$ for NBFNet, and $h_r = W_{out}Concat([h^{(1)}, h^{(2)}, ..., h^{(l)}])$ for\nSE-GNN. Higher $qual(x)$ indicates higher representation quality of node x."}]}