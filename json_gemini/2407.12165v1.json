{"title": "Building Al Agents for Autonomous Clouds: Challenges and Design Principles", "authors": ["Manish Shetty", "Yinfang Chen", "Gagan Somashekar", "Minghua Ma", "Yogesh Simmhan", "Xuchao Zhang", "Jonathan Mace", "Dax Vandevoorde", "Pedro Las-Casas", "Shachee Mishra Gupta", "Suman Nath", "Chetan Bansal", "Saravan Rajmohan"], "abstract": "The rapid growth in the use of Large Language Models (LLMs) and AI Agents as part of software development and deployment is revolutionizing the information technology landscape. While code generation receives significant attention, a higher-impact ap- plication lies in using Al agents for operational resilience of cloud services, which currently require significant human effort and do- main knowledge. There is a growing interest in AI for IT Operations (AIOps) which aims to automate complex operational tasks, like fault localization and root cause analysis, thereby reducing human intervention and customer impact. However, achieving the vision of autonomous and self-healing clouds though AIOps is hampered by the lack of standardized frameworks for building, evaluating, and improving AIOps agents. This vision paper lays the ground- work for such a framework by first framing the requirements and then discussing design decisions that satisfy them. We also propose AIOPSLAB, a prototype implementation leveraging agent-cloud- interface that orchestrates an application, injects real-time faults using chaos engineering, and interfaces with an agent to localize and resolve the faults. We report promising results and lay the groundwork to build a modular and robust framework for building, evaluating, and improving agents for autonomous clouds.", "sections": [{"title": "1 INTRODUCTION", "content": "IT applications and services for enterprises and web-scale compa- nies are becoming more complex to develop, deploy, and maintain. The broad adoption of the micro-services pattern to compose com- plex applications and serverless deployment models hosted on the cloud has arguably simplified application development and scaling. But this has also introduced more moving parts that make their operations, reliability, fault diagnosis and recovery even harder [30]. Cloud services have set the expectation of five-9s of availability, and missing it can affect customer satisfaction. At the same time, failures in the clouds are becoming more frequent and with signifi- cant impact [73, 83, 91]. For instance, for one major recent outage, the estimated cost for one hour of service downtime for Amazon was approximately $100 million [91].\nSite Reliability Engineers (SRE) and DevOps engineers are tasked with deployment and operational maintenance of cloud services [79]. They typically follow four steps when rectifying faults: fault de- tection, triaging, localization, and mitigation. This scope can be broader when we include proactive fault predictions and other pre- emptive maintenance. Many tools help with monitoring, anomaly detection, incident triage, root cause analysis, etc. [13, 93]. However, SREs face information overload and complex decision-making in operating large-scale cloud environments. E.g., a large-scale study of outages in Microsoft Teams found that root-causing and mit- igation require significant manual effort, context awareness and technical expertise [31]. Hence, the need for AIOps Agents, which can automatically detect, localize and mitigate faults with minimal human intervention, is becoming critical. These AIOps agents are key to achieve the vision of Autonomous Clouds.\nWhile the concept of self-healing clouds has been proposed earlier [24, 55], the emerging adoption of AIOps, i.e., the use of AI to support IT Operations, is making this a reality [29, 36, 56, 63, 80, 81, 92, 99, 101, 105]. Here, Al algorithms and agents leverage the monitoring infrastructure to rapidly and efficiently track the health, identify failures and mitigate them within the operational environment [71]. More recently, agents are able to converse with Large Language Models (LLMs) using an observe-thought-action pattern to localize problems, explore possible causes, and enact solutions to achieve recovery in a semi-autonomous manner [22, 47, 78, 94, 102]. We are at the cusp of AIOps agents being able to independently carry out these tasks in a matter of minutes within production environments, compared to several hours taken even by experts [61, 90].\nThe design, development, evaluation and iterative improvement of AIOps agents are challenging. While the adoption of AI has seen rapid progress for coding and programming [6, 94] due to the avail- ability of platforms like WebArena [107], R2E [43] and benchmarks like HumanEval [21], LiveCodeBench [42], SWE-bench [46], the same is lacking for cloud operations. Existing tools address individ- ual aspects of the AIOps lifecycle: observability and introspection tools [35, 77], application suites [28, 52], chaos engineering and fault injection tools [16, 17, 70], agent-computer interfaces [94], etc., but do not cohesively integrate them.\nMoreover, recent prior works on leveraging AIOps agents for cloud operations like RCAgent [90], and Zhang et al. [103] use proprietary services and datasets. Other prior works use frame- works specific to the solutions that they are building [80], or ad hoc and static benchmarks and metrics [59] that fail to capture the dynamic nature of real-world cloud services. Furthermore, cur- rent approaches do not agree on standard metrics or a standard taxonomy for operational tasks. This calls for a standardised and principled framework for building, testing, comparing and improving AIOps agents. The framework should allow agents to interact with realistic service operation tasks in a reproducible manner. It must be flexible in extending to new applications, workloads, and faults."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "Next, we identify gaps in existing work for evaluating AIOps tools and agents in a standardized, realistic and reliable manner.\nFault Mitigation Lifecycle. A typical incident goes through four stages: (1) Detection [62, 97, 98]: When an anomalous system be- haviour is observed, an alert is raised by monitors or users of the service (internal engineers or external customers) and reported to the Incident Management System (IcM). (2) Triaging [13, 19, 20]: After the detection, the incident is notified to the On-Call Engi- neers (OCEs) to begin investigation and then assigned to the most appropriate engineering team. (3) Diagnosis [22, 61, 75, 102]: The assigned engineers inspect different aspects of the incident and have several rounds of back-and-forth communication to identify the root cause. (4) Mitigation [5, 44, 76]: Several actions are taken by the team to mitigate the incident and to restore service health. The outcome is also updated postmortem in the IcM.\nAIOps Benchmarks. Several benchmarks have been proposed to evaluate AIOps at various stages of the software lifecycle. For in- stance, ADBENCH [33] and ANOMALYBENCH [41] evaluate anomaly detection, and LOGPARSER [110] evaluates log parsing. However, these benchmarks focus only on single aspects of incident manage- ment. More recently, LLMs have powered a new wave of AIOps tools and autonomous agents, forcing evaluations to evolve. Gen- eral benchmarks [4, 58] like AGENTBENCH task LLMs to operate as autonomous agents in diverse environments, solving problems across domains like operating systems and databases.\nThat said, specialized AIOps evaluations have received less at- tention. OPSEVAL [59] attempts to address this with a question- answer-based evaluation for LLMs, but it disconnects from real- world operational challenges that require complex debugging, code understanding, and multi-step fault resolution. Our proposed work on AIOPSLAB bridges this gap. We believe using real services, work- loads, and faults to create problems and enable executing concrete"}, {"title": "3 AIOPSLAB: A PRINCIPLED VISION", "content": "In this section, we first discuss the principles that we believe should guide the envisioned framework (3.1), followed by a discussion of the design choices that can lead to such a framework (3.2).\n3.1 Requirements and Principles\n(R1) Modular Design for applications, workloads, and agents.\nAn effective evaluation system should seamlessly incorporate exist- ing and evolving resources on application workloads, fault injection models, and resilience techniques for fault detection and resolution. It must be flexible, supporting easy integration of new components through standard interfaces. This allows for varied use cases, in- cluding enterprise and end-user assessments of fault and recovery strategies, AIOps research on new agents using established error models and workloads, and red-team simulations of complex faults on synthetic applications to test mitigation effectiveness.\n(R2) Flexible Interface for human, digital, and Al agents. The framework must provide diverse interfaces to agents, spanning the"}, {"title": "4 SYSTEM ARCHITECTURE", "content": "This section expands on how the decisions discussed in \u00a7 3.2 lead to an prototype system architecture for AIOPSLAB. Figure 1 shows our system architecture consisting of 5 key pieces.\n4.1 Orchestrator\nAIOPSLAB strictly separates the Agent and the Application Service using an intermediate Orchestrator. It provides several interfaces for other system parts to integrate and extend. First, it establishes a session with an Agent to share information about benchmark prob- lems: (1) the problem description, (2) instructions (e.g., response for- mat), and (3) available APIs to call as actions. As shown in Figure 2, the APIs are a set of documented tools, e.g., get_logs, get_metrics exec_shell, designed to help the Agent solve a task. There are no restrictions on the Agent's implementation; the Orchestrator poses problems and polls it for the next action to perform given the previous result. Each action must be a valid API call, which the Orchestrator validates and carries out. The Orchestrator has privi- leged access to the deployment and can take arbitrary actions (e.g., scale-up, redeploy) using appropriate tools (e.g., helm, kubectl) to re- solve problems on behalf of the Agent. Lastly, the Orchestrator calls workload and fault generators to create service disruptions, which serve as live benchmark problems. AIOPSLAB provides additional APIs to extend to new services and generators.\n4.2 Service\nAIOPSLAB abstracts a diverse set of services to reflect the vari- ance in production environments. This includes live, running ser- vices implemented using various architectural principles, including microservices, serverless and monolithic. We also leverage open- sourced application suites such as DEATHSTARBENCH [28] as they provide artifacts, like source code and commit history, along with run-time telemetry. Adding tools like BLUEPRINT [10] can help scale to other academic [25, 52, 82, 88, 96, 109] and production ser- vices [85] and also seamlessly deploy new variants of these services.\n4.3 Workload Generator\nThe workload generator in AIOPSLAB plays a crucial role by cre- ating simulations of both faulty and normal scenarios. It receives specifications from the Orchestrator, such as the task, desired ef- fects, scale, and duration. The generator can utilize a model trained on real production traces, to generate workloads that align with these specifications. Faulty scenarios may simulate conditions like resource exhaustion, exploit edge cases, or trigger cascading fail- ures, inspired by real incidents. Normal scenarios mimic typical production patterns, such as daily activity cycles and multi-user interactions. When various characteristics (e.g., service calls, user distribution, arrival times) can lead to the desired effect, multiple workloads can be stored in the problem cache for use by the Orches- trator. In coordination with the Fault Generator (4.4), the workload generator can also create complex fault scenarios with workloads.\n4.4 Fault Generator\nAIOPSLAB has a novel push-button fault generator designed for generic applicability across various cloud scenarios. Our approach integrates application and domain knowledge to create adaptable"}, {"title": "5 CASE STUDY AND INSIGHTS", "content": "As a proof-of-concept, we implement a prototype of AIOPSLAB and evaluate an LLM agent on an AIOps incident mitigation task. Here, we aim to demonstrate the potential of AIOPSLAB to standardize evaluation for AIOps tools and uncover novel insights.\nSetup. We deploy the SocialNetwork application from DEATHSTAR- BENCH [28] on a Kubernetes cluster. We instantiate the fault gener- ator to induce a realistic misconfiguration fault at the virtualization layer: the target port of a microservice is misconfigured, causing connectivity issues with other microservices. We generate traffic by using an exponential workload pattern from the wrk tool. We study a REACT [95] agent with a GPT-4 [3] backend. REACT is a popular LLM-agent paradigm that leverages interleaved Thought-Action traces, utilizing LLMs' reasoning and planning abilities.\nEvaluation Goals. This case study aims to showcase several of the requirements in Section 3.1. We address (R4) by employing quanti- tative metrics (Success, Time-to-Detect (TTD), Time-to-Mitigate (TTM), and Efficiency) and qualitative analysis of the agent's ac- tions. We demonstrate (R8) by covering multiple stages: detection, root-cause analysis, and mitigation. (R9) is ensured through ap- plication observability (logs, metrics, traces), and the well-defined APIs for the agent, including a shell, fulfill (R10).\nKey Insights. Figure 2 illustrates the agent's trace while attempting to first detect a service anomaly. It then takes a series of actions, as shown in Figure 3, to identify the root cause and mitigate the fault. Overall, it successfully detected the problem in 14 secs (TTD) and mitigated it in 36 secs (TTM). We measured the average resource efficiency, and found that it takes 10-12 interactions costing around $0.25 Below, we distill key insights from this case study:\n\u2460 Importance of Observability. The agent's ability to detect the misconfiguration fault hinged on the detailed telemetry data provided by AIOPSLAB. For example, the agent identified repeated"}, {"title": "6 CONCLUSION", "content": "In this vision paper, we have framed the requirements and design principles for a framework to build, test, compare and improve Agents used for the operational management of cloud services. We also discuss our prototype implementation, AIOPSLAB, and report"}]}