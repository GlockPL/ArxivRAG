{"title": "Building Al Agents for Autonomous Clouds: Challenges and Design Principles", "authors": ["Manish Shetty", "Yinfang Chen", "Gagan Somashekar", "Minghua Ma", "Yogesh Simmhan", "Xuchao Zhang", "Jonathan Mace", "Dax Vandevoorde", "Pedro Las-Casas", "Shachee Mishra Gupta", "Suman Nath", "Chetan Bansal", "Saravan Rajmohan"], "abstract": "The rapid growth in the use of Large Language Models (LLMs) and AI Agents as part of software development and deployment is revolutionizing the information technology landscape. While code generation receives significant attention, a higher-impact ap- plication lies in using Al agents for operational resilience of cloud services, which currently require significant human effort and do- main knowledge. There is a growing interest in AI for IT Operations (AIOps) which aims to automate complex operational tasks, like fault localization and root cause analysis, thereby reducing human intervention and customer impact. However, achieving the vision of autonomous and self-healing clouds though AIOps is hampered by the lack of standardized frameworks for building, evaluating, and improving AIOps agents. This vision paper lays the ground- work for such a framework by first framing the requirements and then discussing design decisions that satisfy them. We also propose AIOPSLAB, a prototype implementation leveraging agent-cloud- interface that orchestrates an application, injects real-time faults using chaos engineering, and interfaces with an agent to localize and resolve the faults. We report promising results and lay the groundwork to build a modular and robust framework for building, evaluating, and improving agents for autonomous clouds.", "sections": [{"title": "1 INTRODUCTION", "content": "IT applications and services for enterprises and web-scale compa- nies are becoming more complex to develop, deploy, and maintain. The broad adoption of the micro-services pattern to compose com- plex applications and serverless deployment models hosted on the cloud has arguably simplified application development and scaling. But this has also introduced more moving parts that make their operations, reliability, fault diagnosis and recovery even harder [30]. Cloud services have set the expectation of five-9s of availability, and missing it can affect customer satisfaction. At the same time, failures in the clouds are becoming more frequent and with signifi- cant impact [73, 83, 91]. For instance, for one major recent outage, the estimated cost for one hour of service downtime for Amazon was approximately $100 million [91].\nSite Reliability Engineers (SRE) and DevOps engineers are tasked with deployment and operational maintenance of cloud services [79]. They typically follow four steps when rectifying faults: fault de- tection, triaging, localization, and mitigation. This scope can be broader when we include proactive fault predictions and other pre-emptive maintenance. Many tools help with monitoring, anomaly detection, incident triage, root cause analysis, etc. [13, 93]. However, SREs face information overload and complex decision-making in operating large-scale cloud environments. E.g., a large-scale study of outages in Microsoft Teams found that root-causing and mit- igation require significant manual effort, context awareness and technical expertise [31]. Hence, the need for AIOps Agents, which can automatically detect, localize and mitigate faults with minimal human intervention, is becoming critical. These AIOps agents are key to achieve the vision of Autonomous Clouds.\nWhile the concept of self-healing clouds has been proposed earlier [24, 55], the emerging adoption of AIOps, i.e., the use of AI to support IT Operations, is making this a reality [29, 36, 56, 63, 80, 81, 92, 99, 101, 105]. Here, Al algorithms and agents leverage the monitoring infrastructure to rapidly and efficiently track the health, identify failures and mitigate them within the operational environment [71]. More recently, agents are able to converse with Large Language Models (LLMs) using an observe-thought-action pattern to localize problems, explore possible causes, and enact solutions to achieve recovery in a semi-autonomous manner [22, 47, 78, 94, 102]. We are at the cusp of AIOps agents being able to independently carry out these tasks in a matter of minutes within production environments, compared to several hours taken even by experts [61, 90].\nThe design, development, evaluation and iterative improvement of AIOps agents are challenging. While the adoption of AI has seen rapid progress for coding and programming [6, 94] due to the avail- ability of platforms like WebArena [107], R2E [43] and benchmarks like HumanEval [21], LiveCodeBench [42], SWE-bench [46], the same is lacking for cloud operations. Existing tools address individ- ual aspects of the AIOps lifecycle: observability and introspection tools [35, 77], application suites [28, 52], chaos engineering and fault injection tools [16, 17, 70], agent-computer interfaces [94], etc., but do not cohesively integrate them.\nMoreover, recent prior works on leveraging AIOps agents for cloud operations like RCAgent [90], and Zhang et al. [103] use proprietary services and datasets. Other prior works use frame- works specific to the solutions that they are building [80], or ad hoc and static benchmarks and metrics [59] that fail to capture the dynamic nature of real-world cloud services. Furthermore, cur- rent approaches do not agree on standard metrics or a standard taxonomy for operational tasks. This calls for a standardised and principled framework for building, testing, comparing and improving AIOps agents. The framework should allow agents to interact with realistic service operation tasks in a reproducible manner. It must be flexible in extending to new applications, workloads, and faults.\nImportantly, it should go beyond just evaluating the Al agents and also enable users to improve the agents themselves, e.g., by providing sufficient observability and even serving as a training environment (\"gym\") to generate samples to learn on [103].\nIn this vision paper, we draw upon our experiences from devel- oping AIOps agents at Microsoft to make these contributions:\n(1) We envision the requirements for a holistic framework to en- able the design, development, evaluation, and enhancement of AIOps agents that, additionally, serves the purpose of repro- ducible, standardized, interoperable and scalable benchmarks. We also suggest key design decisions to build it (\u00a7 3).\n(2) We describe a prototype framework, AIOPSLAB, that adopts this design to combine workload and fault generators to mimic pro- duction incidents and an agent-cloud interface for orchestrating the service operation lifecycle (\u00a7 4). This is the foundation for our ongoing work on a more comprehensive framework.\n(3) We report a case study on using this preliminary framework to evaluate an LLM agent for two critical operations tasks: Fault Detection and Mitigation (\u00a7 5)."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "Next, we identify gaps in existing work for evaluating AIOps tools and agents in a standardized, realistic and reliable manner.\nFault Mitigation Lifecycle. A typical incident goes through four stages: (1) Detection [62, 97, 98]: When an anomalous system be- haviour is observed, an alert is raised by monitors or users of the service (internal engineers or external customers) and reported to the Incident Management System (IcM). (2) Triaging [13, 19, 20]: After the detection, the incident is notified to the On-Call Engi- neers (OCEs) to begin investigation and then assigned to the most appropriate engineering team. (3) Diagnosis [22, 61, 75, 102]: The assigned engineers inspect different aspects of the incident and have several rounds of back-and-forth communication to identify the root cause. (4) Mitigation [5, 44, 76]: Several actions are taken by the team to mitigate the incident and to restore service health. The outcome is also updated postmortem in the IcM.\nAIOps Benchmarks. Several benchmarks have been proposed to evaluate AIOps at various stages of the software lifecycle. For in- stance, ADBENCH [33] and ANOMALYBENCH [41] evaluate anomaly detection, and LOGPARSER [110] evaluates log parsing. However, these benchmarks focus only on single aspects of incident manage- ment. More recently, LLMs have powered a new wave of AIOps tools and autonomous agents, forcing evaluations to evolve. Gen- eral benchmarks [4, 58] like AGENTBENCH task LLMs to operate as autonomous agents in diverse environments, solving problems across domains like operating systems and databases.\nThat said, specialized AIOps evaluations have received less at- tention. OPSEVAL [59] attempts to address this with a question- answer-based evaluation for LLMs, but it disconnects from real- world operational challenges that require complex debugging, code understanding, and multi-step fault resolution. Our proposed work on AIOPSLAB bridges this gap. We believe using real services, work- loads, and faults to create problems and enable executing concrete actions (e.g., run commands) is necessary for the reliable evaluation of state-of-the-art AIOps solutions, and to enhance them.\nApplication Benchmark Suites. Relevant to this work are bench- mark suites for cloud applications [27, 34, 50, 52, 89]. Ferdman et al. [27] presented Cloudsuite to study the architectural impli- cations, and TailBench [50] proposes a methodology to analyze the performance of web servers and database services. Further, the emergence of microservices has prompted recent work to study their characteristics and requirements [51, 82, 86, 108]. The popular DeathstarBench [28] differentiates from these studies by focusing on diverse large-scale applications with tens of unique microser- vices, allowing studying effects that only emerge at scale, such as network contention and cascading Quality of Service (QoS) viola- tions due to dependencies between tiers. Beyond static application suites [28, 109], BLUEPRINT [10] provides the ability to reconfig- ure applications and iteratively generate variants. We integrate with such application workloads to enable benchmarking of AIOps solutions.\nChaos Engineering and Fault Injection. Prior work developed fault injection techniques aimed at applications and distributed backend systems, including storage and data processing [2, 7, 8, 12, 15, 18, 23, 32, 38, 48, 53, 60, 64, 66, 69, 72, 84, 100]. However, these existing techniques fall short in providing a generic, one- click fault generator that can be universally applied across var- ious microservices. The limitations are multifaceted: many rely on application or domain-specific knowledge to create policies and oracles, making them unsuitable for the diverse requirements of AIOps [18, 53, 60, 69, 84, 111]. Others offer mechanisms with- out automated policies or oracles beyond simple crashes, requir- ing developers to manually implement complex functionalities [1, 8, 23, 32, 38, 39, 48, 65, 68, 72, 74]. Additionally, fault injections at a single level (e.g., HTTP) do not adequately expose root-causes of failures due to: 1) their coarse-grained nature, 2) challenge of con- structing meaningful error objects, and 3) a lack of consideration for dependencies between microservices [23, 26, 40, 45, 66, 67, 100]."}, {"title": "3 AIOPSLAB: A PRINCIPLED VISION", "content": "In this section, we first discuss the principles that we believe should guide the envisioned framework (3.1), followed by a discussion of the design choices that can lead to such a framework (3.2).\n3.1 Requirements and Principles\n(R1) Modular Design for applications, workloads, and agents. An effective evaluation system should seamlessly incorporate exist- ing and evolving resources on application workloads, fault injection models, and resilience techniques for fault detection and resolution. It must be flexible, supporting easy integration of new components through standard interfaces. This allows for varied use cases, in- cluding enterprise and end-user assessments of fault and recovery strategies, AIOps research on new agents using established error models and workloads, and red-team simulations of complex faults on synthetic applications to test mitigation effectiveness.\n(R2) Flexible Interface for human, digital, and Al agents. The framework must provide diverse interfaces to agents, spanning the unique requirements of humans to LLM-based agents. Humans might use a web interface for log review and command execu- tion, digital agents need APIs for integration, and conversational LLM agents require prompt-based interaction for requests and responses.\n(R3) Scalability at various operational and temporal scales. Operating at diverse spatial (single VM to clusters) and tempo- ral scales (minutes to days) is paramount to make the framework amenable to different use cases and resource availability, e.g., from large enterprises with complex deployments and a wide fault sur- face to software engineering course assignment with a tiny sce- nario. Operating across time scales also allows faults that gradually emerge (e.g., memory leaks) or occur periodically to be detected, predicted, and preemptively mitigated.\n(R4) Reproducible Setup for reliable measurements. An evalu- ation framework needs consistent and, ideally, automated deploy- ments for reproducible and standardized assessment of mitigation strategies. Challenges may arise from non-deterministic elements in applications or faults, which should stem from external models, not the framework itself. While evaluation metrics can differ de- pending on the user or context, the framework should offer default objective metrics (e.g., accuracy, time to mitigate) and support the creation of more intricate measures with the data it provides.\n(R5) Versatility in operating environments. The operating en- vironments may vary based on the needs of the user. Chaos engi- neering [70], for instance, advocates for fault injections directly in production environments, necessitating a framework capable of integrating with live applications. Alternatively, a sandboxed deployment with a simpler variant of production applications may be preferred. Synthetic and emulated systems [11, 14] can also help evaluate large-scale what-if scenarios.\n(R6) Comprehensive and Cross-Layer fault support. The frame- work should enable the introduction of faults across the entire stack, including hardware, network, OS, middleware, application, and ex- ternal services. It must offer capabilities to simulate realistic faults inspired by real-world production incidents, which can cause cas- cading effects across distributed components, as well as synthetic faults for assessing potential future scenarios.\n(R7) Diverse and realistic workload conditions. Workloads across domains have different burstiness, interarrival times, and paral- lelism [87]. An effective benchmarking framework should allow for generating workloads that reflect these characteristics rather than a 'one-size-fits-all' approach. Current benchmarking often overfits well-known publicly available workload traces due to the scarcity of realistic workloads available [9]. Realism is essential for effectively testing and improving AI agents, as evidenced by practitioners at CompanyX who faced delays in AI agent deploy- ment due to the difficulty in obtaining genuine user interaction and traffic patterns.\n(R8) Coverage of the operations lifecycle. The incident manage- ment process can have diverse goals and scopes, and the framework should support the different stages that correspond to them, like fault detection, root-cause analysis, and mitigation. It should allow proactive strategies to predict and preempt failures and reactive strategies to detect and correct errors.\n(R9) Sufficient Observability into applications. Adequate visi- bility into the different aspects of the system and application en- vironment should be available to detect faults and their impact. It includes interfaces to access logs, telemetry, traces, KPIs, and docu- mentation such as incident reports, standard procedures, etc. Tools to explore configuration files and source code may be beneficial in acquiring the necessary context for localization and mitigation.\n(R10) Adequate controls for agent operations. Fault correction may require modifying configuration files or restarting VMs or services. Even fault detection may require the agent to run test suites to gain more visibility into the possible cause. The framework should enable such actions by having hooks into different layers.\n3.2 Design Decisions\nTo address the requirements outlined in Section 3.1 (annotated be- low), we recommend several key design choices to build AIOPSLAB, a new standardized evaluation framework for AIOps:\n3.2.1 Bring your Services Alive. We propose evaluating with live services, that are designed and implemented using different architectures at various scales (R3), and capable of operating in var- ious contexts, from sandbox to production (R5). Real-world cloud services present complex behaviours, challenging AIOps agents with variability in workload patterns, diversity of faults, and in- tricacies of service dependencies. This approach closely mirrors the operational and reliability challenges faced in production sys- tems, making the benchmark directly applicable to practitioners' needs. Further, we choose automation tools like Helm [37] and BLUE- PRINT [10] for repeatable and consistent setups (R4) and interfaces to extend to new services, preserving its applicability and rigour (R1). This approach provides a comprehensive, realistic, and rele- vant evaluation platform to advance AIOps research and practice.\n3.2.2 Real Faults, Real Challenges. We recommend incorporat- ing dynamic workload and fault generators to simulate real-world conditions accurately. These generators are designed to produce a wide range of realistic traffic patterns and operational challenges (R7), from typical user behaviour to peak loads and various fault scenarios, including kernel failures, network issues, and configu- ration bugs (R6). This approach not only tests adaptability and robustness but also mitigates the risk of \"training data contami- nation\" for LLM-powered tools. A key implication of this choice is that the state of a faulty service (say telemetry or logs) is only relevant and observed in the context of an injected fault - making it publicly unavailable to seep into training data.\n3.2.3 Evaluate Across the Board. The combination of live ser- vices (\u00a7 3.2.1) and real faults (\u00a7 3.2.2) allow us to reliably evaluate the impact of agents on various AIOps tasks and performance di- mensions. Here, one can use generated faults independently or in conjunction to create benchmark problems for agents. Notably, one can use a single fault (e.g., network misconfiguration) to evaluate tasks across the board - from detection to resolution (R8). Evalua- tion involves quantitative dimensions, such as performance, time, resource usage, dollar cost, and other metrics beyond accuracy [49]. Furthermore, to reliably compare agents, we emphasize qualitative evaluation of their traces by a human or LLM-as-a-Judge [106].\n3.2.4 Orchestrate the Agent-Cloud-Interface. Typically, ser- vice engineers operate cloud environments with various program- ming (e.g. APIs, database queries) and user interfaces (incident portals). Existing UI to the cloud are designed only for a human, and not amenable for LLMs and agents. E.g., humans reliably ignore extra information while the same can distract the context and harm performance for agents [57]. Inspired by human-computer inter- action (HCI), Yang et al. [94] introduce agent-computer-interface, finding that agents can similarly benefit from better-designed inter- faces for coding tasks. We posit the same for AIOps and envision an Agent-Cloud-Interface (ACI). The ACI is an Orchestrator be- tween the agent and the cloud (Figure 1) which specifies both the actions available and how the service's state is conveyed back to the agent as the observation of its actions (R2). It simplifies the action space into a concise list of APIs, each documented to ensure that agents can make meaningful progress towards objectives (R10). Also, the Orchestrator takes actions on behalf of the agent and returns high-quality feedback (e.g., outputs, error messages, etc.).\n3.2.5 Abstract Environments, not agents. Two sides to a real- time evaluation are the agent and the environment. For AIOps, the agent can be a DevOps engineer or an AIOps tool (e.g., LLM- powered agent). The environment is a deployed application (e.g., SocialNetwork) that the user interacts with. Here, we suggest pro- viding abstractions for the environment, not the agent. This choice maximizes flexibility in implementing and evaluating various kinds of tools and agents (R2). Consequently, AIOPSLAB provides suf- ficient information (task description, available APIs/actions, and additional instructions) to solve a problem in the benchmark.\n3.2.6 Observe Everything, Everywhere, All at Once. Observ- ability captures a system's internal states from its external outputs. It traditionally includes (1) traces detailing the end-to-end request paths through distributed entities; (2) application logs as textual records of runtime operations; and (3) metrics monitoring compo- nent health. We suggest an observability layer to collect not only canonical telemetry data but also other system indicators, such as cluster information, including logs, running status, and configu- rations (R9). But increased observability can overwhelm AIOps tools with data volume and complex data types. Therefore, we offer flexible APIs for users to select the specific information they need, ensuring tailored and comprehensive observability.\nSummary. In summary, these design decisions prioritize creating a framework that is:\n(1) Realistic: By using live services, workloads, and faults that mirror real-world operational challenges.\n(2) Scalable: Through dynamic workload and fault generators that can create new problem scenarios at varying scales.\n(3) Reliable: Evaluating tasks and performance dimension across the operations lifecycle.\n(4) Observable: Through rich telemetry data and actual service interactions, ensuring reliable evaluation.\n(5) Flexible: With the Agent-Cloud-Interface (ACI) that supports plugging a diverse range of tools.\n(6) Extensible: With the ability to easily incorporate new services, workloads, and faults."}, {"title": "4 SYSTEM ARCHITECTURE", "content": "This section expands on how the decisions discussed in \u00a7 3.2 lead to an prototype system architecture for AIOPSLAB. Figure 1 shows our system architecture consisting of 5 key pieces.\n4.1 Orchestrator\nAIOPSLAB strictly separates the Agent and the Application Service using an intermediate Orchestrator. It provides several interfaces for other system parts to integrate and extend. First, it establishes a session with an Agent to share information about benchmark prob- lems: (1) the problem description, (2) instructions (e.g., response for- mat), and (3) available APIs to call as actions. As shown in Figure 2, the APIs are a set of documented tools, e.g., get_logs, get_metrics exec_shell, designed to help the Agent solve a task. There are no restrictions on the Agent's implementation; the Orchestrator poses problems and polls it for the next action to perform given the previous result. Each action must be a valid API call, which the Orchestrator validates and carries out. The Orchestrator has privi- leged access to the deployment and can take arbitrary actions (e.g., scale-up, redeploy) using appropriate tools (e.g., helm, kubectl) to re- solve problems on behalf of the Agent. Lastly, the Orchestrator calls workload and fault generators to create service disruptions, which serve as live benchmark problems. AIOPSLAB provides additional APIs to extend to new services and generators.\n4.2 Service\nAIOPSLAB abstracts a diverse set of services to reflect the vari- ance in production environments. This includes live, running ser- vices implemented using various architectural principles, including microservices, serverless and monolithic. We also leverage open- sourced application suites such as DEATHSTARBENCH [28] as they provide artifacts, like source code and commit history, along with run-time telemetry. Adding tools like BLUEPRINT [10] can help scale to other academic [25, 52, 82, 88, 96, 109] and production ser- vices [85] and also seamlessly deploy new variants of these services.\n4.3 Workload Generator\nThe workload generator in AIOPSLAB plays a crucial role by cre- ating simulations of both faulty and normal scenarios. It receives specifications from the Orchestrator, such as the task, desired ef- fects, scale, and duration. The generator can utilize a model trained on real production traces, to generate workloads that align with these specifications. Faulty scenarios may simulate conditions like resource exhaustion, exploit edge cases, or trigger cascading fail- ures, inspired by real incidents. Normal scenarios mimic typical production patterns, such as daily activity cycles and multi-user interactions. When various characteristics (e.g., service calls, user distribution, arrival times) can lead to the desired effect, multiple workloads can be stored in the problem cache for use by the Orches- trator. In coordination with the Fault Generator (4.4), the workload generator can also create complex fault scenarios with workloads.\n4.4 Fault Generator\nAIOPSLAB has a novel push-button fault generator designed for generic applicability across various cloud scenarios. Our approach integrates application and domain knowledge to create adaptable policies and \"oracles\" compatible with AIOps scenarios. This in- cludes fine-grained fault injection capable of simulating complex failures inspired by production incidents. Additionally, it can inject faults at various system levels, exposing root causes while maintain- ing semantic integrity and considering interdependencies between cloud microservices. The fault injector's versatility can enhance the reliability and robustness of cloud systems by enabling thorough testing and evaluation of AIOps capabilities.\n4.5 Observability\nAIOPSLAB is equipped with an extensible observability layer de- signed to provide comprehensive monitoring capabilities across various system layers for any AIOps tool. AIOPSLAB collects a wide array of telemetry data, including (1) traces from Jaeger detailing the end-to-end paths of requests through distributed systems, (2) application logs formatted and recorded by Filebeat and Logstash, and (3) system metrics monitored by Prometheus. Additionally, AIOPSLAB also captures lower-level system information such as syscall logs and cluster information. As mentioned, we handle the potential data overload through flexible APIs to tune the telemetry data relevant to the AIOps tools."}, {"title": "5 CASE STUDY AND INSIGHTS", "content": "As a proof-of-concept, we implement a prototype of AIOPSLAB and evaluate an LLM agent on an AIOps incident mitigation task. Here, we aim to demonstrate the potential of AIOPSLAB to standardize evaluation for AIOps tools and uncover novel insights.\nSetup. We deploy the SocialNetwork application from DEATHSTAR- BENCH [28] on a Kubernetes cluster. We instantiate the fault gener- ator to induce a realistic misconfiguration fault at the virtualization layer: the target port of a microservice is misconfigured, causing connectivity issues with other microservices. We generate traffic by using an exponential workload pattern from the wrk tool. We study a REACT [95] agent with a GPT-4 [3] backend. REACT is a popular LLM-agent paradigm that leverages interleaved Thought-Action traces, utilizing LLMs' reasoning and planning abilities.\nEvaluation Goals. This case study aims to showcase several of the requirements in Section 3.1. We address (R4) by employing quanti- tative metrics (Success, Time-to-Detect (TTD), Time-to-Mitigate (TTM), and Efficiency) and qualitative analysis of the agent's ac- tions. We demonstrate (R8) by covering multiple stages: detection, root-cause analysis, and mitigation. (R9) is ensured through ap- plication observability (logs, metrics, traces), and the well-defined APIs for the agent, including a shell, fulfill (R10).\nKey Insights. Figure 2 illustrates the agent's trace while attempting to first detect a service anomaly. It then takes a series of actions, as shown in Figure 3, to identify the root cause and mitigate the fault. Overall, it successfully detected the problem in 14 secs (TTD) and mitigated it in 36 secs (TTM). We measured the average resource efficiency, and found that it takes 10-12 interactions costing around $0.25 Below, we distill key insights from this case study:\n\u2460 Importance of Observability. The agent's ability to detect the misconfiguration fault hinged on the detailed telemetry data provided by AIOPSLAB. For example, the agent identified repeated connection refusals in the logs, which made it hypothesize a poten- tial misconfiguration. This highlights the critical role observability will play in AIOps in the future.\n\u2461 AIOps needs Efficient Actions. We found that the efficiency of the available actions influenced the agent's performance. For instance, too few APIs limited its ability to explore solutions, while too many arguments for each API hindered its performance. Also, flexible APIs (like exec_shell) were pivotal to the agent balancing explore and exploit actions. This reiterates the importance of a well-designed Agent-Cloud-Interface (ACI).\n\u2462Fault Injection Reflects Real-World Complexity. Interest- ingly, injecting simple faults (like a K8s misconfiguration) into a real service proves challenging problems for advanced models like GPT-4 [3], requiring 10+ interaction rounds to reach a solution. This demonstrates how automated fault generators could accelerate the creation of rigorous testbeds for reliable AIOps evaluation.\n\u2463 Incorporating Service Dependency Tools. The trace in Fig- ure 2 demonstrates the agent's implicit understanding of service dependencies just via logs. While promising, for complex appli- cations, the agent could spend significant time traversing the call graph with only partial views of the system [54]. Concurrent ef- forts in coding agents have shown the power of code dependency analysis for repository-level tasks [6, 43, 104]. We believe future research can similarly look at augmenting AIOps agents with tools for explicit service dependencies and impact analysis.\n\u2464 Error Handling goes a long way. In Figure 3, when the agent encountered a syntax error, it quickly corrected the mistake and retried the command. We even find that poor error messages hin- der the agent's performance. This insight emphasizes the need for robust error propagation for actions throughout the operations and reliability lifecycle of systems."}, {"title": "6 CONCLUSION", "content": "In this vision paper, we have framed the requirements and design principles for a framework to build, test, compare and improve Agents used for the operational management of cloud services. We also discuss our prototype implementation, AIOPSLAB, and report preliminary results. This forms the kernel for a more comprehensive framework that will address the key gap in standardized framework for building agents to help achieve autonomous clouds."}]}