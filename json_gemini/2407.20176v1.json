{"title": "EMOTION-DRIVEN MELODY HARMONIZATION VIA MELODIC VARIATION AND FUNCTIONAL REPRESENTATION", "authors": ["Jingyue Huang", "Yi-Hsuan Yang"], "abstract": "Emotion-driven melody harmonization aims to generate diverse harmonies for a single melody to convey desired emotions. Previous research found it hard to alter the perceived emotional valence of lead sheets only by harmonizing the same melody with different chords, which may be attributed to the constraints imposed by the melody itself and the limitation of existing music representation. In this paper, we propose a novel functional representation for symbolic music. This new method takes musical keys into account, recognizing their significant role in shaping music's emotional character through major-minor tonality. It also allows for melodic variation with respect to keys and addresses the problem of data scarcity for better emotion modeling. A Transformer is employed to harmonize key-adaptable melodies, allowing for keys determined in rule-based or model-based manner. Experimental results confirm the effectiveness of our new representation in generating key-aware harmonies, with objective and subjective evaluations affirming the potential of our approach to convey specific valence for versatile melody.", "sections": [{"title": "1. INTRODUCTION", "content": "Inspired by the remarkable achievements made in symbolic music generation [1, 2, 3, 4], there is a growing interest in controlling high-level musical features during the generation process, especially the perceived emotions from music. Recent years have witnessed many efforts in unconditional music generation [5, 6, 7, 8, 9] and melody harmonization [10, 11] to condition their generation on emotion.\nAccording to Russell's famous Circumplex model of affect [12], emotion could be represented in a two-dimensional space defined by valence and arousal, where valence related to the positiveness of an emotion and arousal refers to energy or activation [13]. Although many works are capable to control the arousal of music, few of them succeeded in controlling the perceived valence. For example, the piano music generation model in EMOPIA [5] fails in generating low valence (i.e., negative) music, and the melody harmonization model LHVAE [11] found it hard to change the overall emotion of music (negative, neutral and positive) by only altering chords.\nThe ignorance of musical keys when modeling music data may be responsible for the poor valence control. Valence is often found to be related to major-minor tonality [14], and keys play important roles in affecting such tonality. The histogram of keys derived from the emotion-labeled music dataset EMOPIA [5] (Fig. 1) provides further support from a data perspective, where the distribution skews to major keys for high valence clips and opposite trend for low valence ones. However, to the best of our knowledge, none of existing music generation works attempted to model keys explicitly.\nIn this work, we focus on the emotion-driven melody harmonization task, which aims to convey desired emotions through harmonizing diverse chord progressions for a single melody, resulting in the creation of lead sheets. This simplified music format free from performance variances enable us to dive into the impact of keys and chords on emotions, and we only consider the valence aspect of emotion since arousal is usually related to performance-level attributes and accompaniment patterns [15]. This task is more challenging than conventional melody harmonization from three aspects. Firstly, symbolic music datasets with both high-quality emotion and chord labels are relatively scarce. Secondly, there are even less training examples where one melody is accompanied with multiple chord progressions to convey varying emotions. Thirdly, because the key is predetermined given a melody line, there is limited room for chord progressions to contribute to the target emotion. While the first challenge was partially addressed by previous works [10, 11], the later two hinder the further improvement of valence control.\nObserving the above, we propose a novel functional representation designed as an alternative to REMI [1], a popular event representation that uses note pitch values and chord names to encode symbolic music. Our method represents both melody notes and chords with Roman numerals relative to musical keys, a functional format considering the relationships between notes, chords and scales (major or minor) [16]. In the harmonization process, driven by an emotional condition (positive or negative), a key is determined in a rule-based or model-based manner. Subsequently, a melody line encoded in functional representation is accompanied to generate key-aware functional harmonies, a process facilitated by a Transformer model.\nCompared to note pitch values and chord names, which necessitate models to infer keys implicitly, functional representation empowers us to explicitly inform musical keys while modeling music pieces. Additionally, as melodies across various scales are designed to be represented by the same set of symbols for twelve scale degrees, the likelihood of encountering two melodies with similar representations increases, and their accompanying chord progressions"}, {"title": "2. RELATED WORK", "content": "Various model architectures and chord control methods have been explored in melody harmonization [17, 18, 19, 20] and yielded remarkable results. In contrast, progress in emotion-driven melody harmonization is limited due to its inherent challenges. Some approaches relied on crowd-sourced emotion tags as weak labels to obtain music samples with both emotion and chord labels [10], while some approaches manually calculated valence simply from chord labels [11]. For better emotion annotations, we use a pop piano performance dataset with human-annotated emotions [5] and apply rule-based algorithm to extract chord labels. We also leverage a high-quality lead sheet dataset for pre-training. However, existing methods still struggle with controlling emotions effectively, as the discrepancy between the perceived and target emotions remain large. Moreover, none of these works consider the influence of musical keys in harmonization. They typically transpose all samples to C major/c minor for simplicity [17, 18], or to all 12 keys for augmentation [20]. These challenges and constraints motivate us to study functional representation for better emotion controllability."}, {"title": "3. METHOD", "content": "3.1. Functional Representation\nThe proposed functional representation is designed based on REMI [1], a widely used event (token) based representation for symbolic music, but with different note and chord events assisting to model the emotion and key information better. See Fig. 2 for illustrations.\nEmotion Events To denote distinct valences and affect overall properties, we begin the event sequence with Emotion_Positive or Emotion Negative to indicate the emotional character of music clips, following the approach introduced in CTRL [21].\nKey Events After the Emotion* event, a KEY_* event is appended to indicate the key property. A total of 24 types (12 tonic notes with two modes each) are used in this work.\nBar, Sub-Beat and EOS Events The same as REMI, a BAR event is used when a new bar begins, a SUB-BEAT-* event points to one of 16 possible discrete locations in a bar, and an EOS event will end the whole lead sheet.\nChord Events In existing melody harmonization studies, chord names are commonly employed to denote root notes and chord qualities. For example, Fmaj represents the chord F-A-C with root F and major quality. These symbols are useful when all music pieces share the same tonic, i.e., C major and c minor, but they overlook the variations in chord functions of the same chord across different keys due to different scale degrees. For example, while Fmaj serves tonic function in F major, it takes on subdominant function in C major. Moreover, the chord progressions following specific functional harmony rules establish tonality and convey musical emotion [16].\nTo introduce chord functions for better harmonization, we adopt Roman numerals adopted from Roman Numeral Analysis [22] to notate chord roots independent of keys (see Fig. 3). Given the key event, root notes belonging to the key scale (such as C, D, E, F, G, A and B in C major) are directly converted into Roman numerals based on their scale degrees relative to the tonic. For roots outside the scale, we employ a direct conversion for I, II, IV#, V# and VI # appearing in major keys, but randomly assign III # and VII # who only appear in minor keys as one of their neighboring degrees. The conversion from Roman numerals back to letters follows a similar process. This design ensures the notation remaining key-independent and allows for harmonious transposition to any desired key while minimizing pitch variations resulting from the conversion. The notations of chord qualities remain unchanged. A CHORD_* event appears every beat, even when there are no chord changes.\nNote-related Events In REMI, every single note is denoted by Pitch and Duration* events. The former indicates the onset of pitches, ranging from 21(A0) to 108(C8). To enable key-aware pitches and melodic variation, we decompose Pitch event into Octave and Degree_* events, where degrees are computed as chord roots and octaves match the tonic's octave in the key scale. For example, pitch 63(D#4) will be decomposed into Octave_4 and Degree_III in C minor. Note that this work omits performance-related events such as VELOCITY and TEMPO_*"}, {"title": "3.2. Model Architecture", "content": "We follow the conditional generation framework proposed in Compound Word Transformer [23] for our task. We firstly predict a key event k conditioned on an emotion event e, and then generate a chord sequence C by accompanying a melody sequence M with e and k, which could be formalized as \\(p(k, C|e, M) = p(k|e)p(C|e, k, M)\\). A sequence-to-sequence model [23, 24] is applied to learn \\(p(k|e)\\) and \\(p(C|e, k, M)\\) simultaneously. Moreover, with the positions of BAR events, M and C are further segmented into \\(\\{M_1,\u2026, M_b\\}\\) and \\(\\{C_1,..., C_b\\}\\), where b is the number of bars. The segmented sequences are interleaved in the form of { TRACK_MELODY, \\(M_i\\), TRACK_CHORD, \\(C_i\\)} with additional TRACK_* events, so that the target chord bar \\(C_i\\) is nearest to its corresponding conditions \\(M_i\\) and the dependency between melody and chord is easier to learn. The final model is summarized as \\(p(k, C|e, M) = p(k|e) \\prod_{i=1}^{b} p(C_i|e, k, M_{<i}, C_{<i})\\) and minimized the negative log-likelihood loss of the generated sequence.\nAt inference time, given an emotion condition e, a key event k could be determined in rule-based or model-based manner. In the former approach, we enforce the original key to its parallel major key for positive emotion or parallel minor key otherwise, inspired by Fig. 1. However, a song in major key could also convey negative emotion, so alternatively, the key can be predicted from \\(p(k|e)\\) learned above. Note that the corresponding melody will be adjusted as key changes.\nAs the size of the dataset with emotion labels is not big enough, we pretrain the model with a large lead sheet dataset without emotion annotations to establish a robust understanding of relationship between melody and chord, where EMOTION_NONE is used as the emotion event. We then finetune the model on EMOPIA [5] to learn harmonization styles specific to different emotion conditions."}, {"title": "4. EXPERIMENTS", "content": "4.1. Datasets, Preprocessing, and Model Settings\nWe adopt a large lead sheet dataset collected from HookTheory [25] released in SheetSage [26] for pre-training (refered to as \"HookTheory\" hereafter). Each piece in HookTheory contains high-quality, human-transcribed melody, chord and key annotations. After removing the pieces that are not in 4/4 time signature or major/minor keys, over 18k segments remain. We simplify 249 chord quality classes to 11 types including major, minor, augment, diminish, suspend2, suspend4, major7, minor7, dominant7, diminish7, half-diminish7, which may be sufficient to convey most emotional characters.\nWe use the piano MIDI dataset EMOPIA for model finetuning. It contains 1,071 music clips with human-annotated emotion labels. A four-class taxonomy adopted from Russell's model [12] is used for annotations, including HVHA (high valence high arousal), HVLA (high valence low arousal), LVHA (low valence high arousal) and LVLA (low valence low arousal). Since we only consider the valence dimension, clips in HVHA and HVLA are combined into Positive class and others into Negative class. To form the lead sheet samples from piano performance, we adopt a method similar to the one proposed in [27] to extract chord labels in 11 quality types, and apply a heuristic rule-based method to extract the melody line [28].\nThe statistics of two datasets after preprocessing are shown in Table 1. The clips in HookThoery are randomly divided into train and validation splits, and we follow the stratified split code provided by EMOPIA for train and validation, both with the ratio of 9:1. In our data representations, the vocabulary size of events is 217.\nA 12-layer linear Transformer with Performer attention [29] is used for generation (8 attention head, 512 hidden state dim., 38 million parameters) and trained with a batch size of 4 and a maximum sequence length of 1,024 (longer than 99.2% and 94.6% of clips in HookTheory and EMOPIA respectively) on a Tesla V100 GPU with 32G VRAM. We use Adam optimizer with 200 steps of warmup to maximum learning rate of 1e-4 and le-5 for pretrain and finetune respectively, both followed by 500k steps of cosine decay. Checkpoints with lowest validation loss are used for inference, using nucleus sampling [30] with temperatured softmax (\\(\tau\\) = 1.1, p = 0.99)."}, {"title": "4.2. Objective Evaluation", "content": "We firstly study RQ#1, i.e., can the proposed representation effectively model musical keys and yield satisfactory harmonization outcomes. We consider two baselines for comparison. One represents music clips in REMI [1] directly without any other processing, following the approach used in emotion-conditioned melody harmonization work LHVAE [11]. The other transposes ('trans') all clips to C major/c minor before REMI encoding, standing for a common method of existing melody harmonization works [17, 18]. We do not compare with other model architectures as our primary focus is to study the impact of different representations. For functional representation, besides the full format, we also compare one ablated version ('ablated'), where chords are encoded in the functional format but melody notes are represented by note pitch values as in REMI.\nFor each of the variants, we harmonize all melodies in validation set under the conditions of positive and negative emotion respectively, i.e., 88 \u00d7 2 samples each variant, to check their harmonization performance. Two groups of objective evaluation metrics are considered, including three metrics proposed in [17] to evaluate the harmonicity between chord and melody: chord tone to non-chord tone ratio (CTnCTR), pitch consonance score (PCS), melody-chord tonal distance (MCTD), and two metrics newly proposed here for evaluating the ability of modeling keys:\n\u2022 Root ratio (RR): the ratio of chord roots in key scale.\n\u2022 Note ratio (NR): the ratio of chord notes in key scale.\nTable 2 shows that, while encoding music clips into REMI representation directly have poor performances in all metrics, providing musical keys information by transposing to C major/c minor or adding key events with functional representation could greatly improve their harmonicity and almost meet the real data. This indicates that musical key is critical to melody harmonization, and that our representations enables efficient learning of their relationships despite of fewer training data for each key type. Moreover, the ablated version suffer losses on melody-related metrics, showing the necessity of encoding melodies in functional formats. In short, the answer to RQ#1 is that when applying functional representation on both melody notes and chord labels, musical keys could be effectively modeled and the melodies are well harmonized.\nNext, we study RQ#2, i.e., whether we could control the valence when generating different music variants from a melody. We select the methods perform well above, i.e., REMI (trans) and our functional representation. Besides simply setting the emotion event to the target emotion without changing keys, we further examine rule-based ('rule') and model-based ('model') methods discussed in Section 3.2 to determine keys conditioned on emotions, and build five models in total. Note that the 'rule' variant of REMI (trans) is implemented by transposing the melody to major/minor keys directly. To quantify how well the generated samples conform to the emotion conditions, we propose the following two metrics:\n\u2022 Quality Distribution (QD): compute the KL divergence between the chord quality distributions of generated samples and real data for positive and negative emotions respectively, then take average.\n\u2022 Progression Distribution (PD): calculate the KL divergence between the bi-gram chord progression distributions as above and compute the average. Progression refers to the difference between two consecutive chord root in chromatic scale. For example, the progression between Dmaj and Fmin is 3.\nTo obtain reliable distributions, for each combination, we generate harmonies for all melodies in validation set five times under both positive and negative emotion conditions, i.e., 88 \u00d7 5 \u00d7 2 samples, to match the size of training samples. As is shown in the left part of Table 3, the combination of functional representation and rule-based key determination achieves the lowest distribution distances for two metrics. Furthermore, all the versions using functional representation performs better than REMI ones, indicating the effectiveness of this representation to model emotion-related properties of music."}, {"title": "4.3. User Study", "content": "An online survey was deployed to collect user responses. Every subject needs to listen to 16 music pieces, including two original pieces in positive and two in negative as well as their four variants generated with the same melody but the opposite emotion condition by four methods introduced above. These original pieces are randomly drawn from the validation set. For each group of samples, users will assess how the variants (in random order) compare to the original piece in terms of their emotional differences on a five-point scale, specifically whether they convey a much more positive(2), more positive(1), unchanged(0), more negative(-1) or much more negative(-2) emotion, without knowing the pre-defined emotion conditions. This design is inspired by the finding that it is easier for human subjects to make relative valence comparison rather than to assign absolute rating [31]. 23 subjects participated in the survey ('Total'), 9 of them with \u22654 years of musical training or experience ('Expert').\nThe right side of Table 3 shows the average scores from the subjects, with a higher score (\\(\\in\\) [\u22122, 2]) indicating better emotion controllability, i.e., the perceived valence matches the given condition. Totally speaking, the combination of functional representation and rule-based key determination performs best, followed by REMI with rule-based keys, which indicates that musical keys play a significant role in influencing the perceived valence through melodic variation and the functional representation generates slightly better harmonies to support the perception of desire emotions, which answers the RQ#2. Moreover, users are still unable to perceive the emotion conversions by simply harmonizing the original melody without any key changes, aligning with the previous findings [11]. When determining keys in model-based method, sometimes major (minor) keys will be sampled for negative (positive) emotions and the generated harmonies lack the ability to obviously change the perceived valence by themselves, yielding worse emotion controllability. Namely, the rule-based way seems to outperform the model-based way in determining keys. Improving the latter will be a focus of future work.\nWhen diving into user responses from different musical backgrounds, those with longer years of music-related experiences seem to favor the REMI-generated samples, while people with less experiences choose the functional representation. Intuitively, with more musical training, people may analyze the conveyed emotions more from the perspective of music theory, while novices simply rely on their emotional senses, but it is hard to say which method is more accurate. Moreover, if examining the cases of 'negative to positive' (i.e., generating positive variants for negative original pieces) and 'positive to negative' separately (Fig. 5), the control of low valence is better with more samples match the negative condition, while half of samples conditioned on positive are still identified as negative ones as their ground-truth emotions. It seems that minor keys have significant influence on negative feelings, while the melody flows have strong influences on positive moods."}, {"title": "5. CONCLUSION AND FUTURE WORK", "content": "In this paper, we have proposed a novel functional representation for symbolic music, which represents melody notes and chords with Roman numerals relative to musical keys. A Transformer-based framework is then adopted to harmonize melodies conditioned on emotional valence. Objective assessments validate our approach's effectiveness in key modeling, while subjective evaluations confirm its ability to convey desired emotional valence. Future endeavors may focus on the improvement of model-based approach and the control of arousal aspect of emotion through accompaniment generation."}]}