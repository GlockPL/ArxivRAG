{"title": "CPL: CRITICAL PLANNING STEP LEARNING BOOSTS\nLLM GENERALIZATION IN REASONING TASKS", "authors": ["Tianlong Wang", "Xueting Han", "Jing Bai"], "abstract": "Post-training large language models (LLMs) to develop reasoning capa-\nbilities has proven effective across diverse domains, such as mathematical\nreasoning and code generation. However, existing methods primarily focus\non improving task-specific reasoning, but have not adequately addressed\nthe model's generalization capabilities across a broader range of reasoning\ntasks. To tackle this challenge, we introduce Critical Planning Step Learn-\ning (CPL), which leverages Monte Carlo Tree Search (MCTS) to explore\ndiverse planning steps in multi-step reasoning tasks. Based on long-term\noutcomes, CPL learns step-level planning preferences to improve the model's\nplanning capabilities and, consequently, its general reasoning capabilities.\nFurthermore, while effective in many scenarios for aligning LLMs, existing\npreference learning approaches like Direct Preference Optimization (DPO)\nstruggle with complex multi-step reasoning tasks due to their inability to\ncapture fine-grained supervision at each step. We propose Step-level Advan-\ntage Preference Optimization (Step-APO), which integrates an advantage\nestimate for step-level preference pairs obtained via MCTS into the DPO.\nThis enables the model to more effectively learn critical intermediate plan-\nning steps, thereby further improving its generalization in reasoning tasks.\nExperimental results demonstrate that our method, trained exclusively on\nGSM8K and MATH, not only significantly improves performance on GSM8K\n(+10.5%) and MATH (+6.5%), but also enhances out-of-domain reason-\ning benchmarks, such as ARC-C (+4.0%), BBH (+1.8%), MMLU-STEM\n(+2.2%), and MMLU (+0.9%).", "sections": [{"title": "1 INTRODUCTION", "content": "Recent studies focus on enhancing the reasoning capabilities of large language models (LLMs)\nthrough various approaches, including collecting high-quality and domain-specific data\ndesigning elaborate prompting\ntechniques and developing advanced optimization\nalgorithms Among these approaches, training on model-generated synthetic data is a promising\nmethod. Specifically, recent work leverages Monte Carlo Tree Search (MCTS) to iteratively collect\nreasoning paths to boost LLM's reasoning capabilities.\nMCTS strikes a balance between exploration and exploitation, utilizing its look-ahead ability\nto obtain high-quality step-level supervision. However, a primary challenge with MCTS for\nLLMs is the high inference latency and the vast search space, which limits the diversity\nof explored reasoning paths. Additionally, existing methods primarily focus on enhancing\ntask-specific or domain-specific reasoning capabilities, such as for math or code. This has led\nto significant improvements in specific tasks but has not adequately addressed the model's"}, {"title": "2 RELATED WORK", "content": "Search-Guided Reasoning in LLMs Recent advancements in enhancing LLM reasoning capabilities have focused on integrating\nMonte Carlo Tree Search (MCTS) to collect trajectories and train models, resulting in notable\nadvancements for reasoning tasks. For example, AlphaMath employs\nMCTS to automatically generate process supervision, leading to significant improvements in\nmathematical reasoning. However, these MCTS-based training methods encounter challenges\nsuch as vast search spaces, limited solution diversity for LLMs. Furthermore, there is\nlimited research on how these methods generalize to other reasoning tasks and enhance\noverall reasoning capabilities. To address these issues, we propose a method for searching\nover plan steps and learning critical plan steps for problem-solving, which aims to enhance\ngeneralization across a range of reasoning tasks.\nDirect Preference Optimization (DPO) Algorithms DPO uses\nsolution-level preference data for model optimization but has notable limitations. It struggles\nwith multi-step reasoning tasks because it cannot effectively correct specific errors within\nthe reasoning process Moreover, training on model-generated positive\ndata can amplify spurious correlations from incorrect intermediate steps, leading to poor\ngeneralization Recent work proposes step-level DPO to address these issues by providing the fine-grained error identification\nneeded for improving reasoning capabilities. For example, SELF-EXPLORE identifies the first incorrect step in a solution and constructs step-level preference\ndata to guide model improvement. Unlike these heuristic methods, we propose Step-APO to\nfully explore the step-level search space and achieve the maximum optimization potential."}, {"title": "3 \u041c\u0415\u0422\u041dODS", "content": "Our Critical Planning Step Learning (CPL) framework is illustrated in Figure 2. In this\nsection, we first introduce our planning based MCTS, which enables the LLM to learn critical\nplanning steps. Next, we present our Step-APO in detail to further explore the potential\nof step-level preference learning in multi-step reasoning task. Finally, we describe how we\niteratively optimize the policy model and value model."}, {"title": "3.1 CRITICAL PLANNING STEP LEARNING WITH MCTS", "content": "MCTS builds a reasoning tree iteratively and autonomously explores step-level reasoning\ntraces, which can be used to optimize LLMs. Existing methods that leverage MCTS to collect data for training usually focus on exploring\nsolution steps within the entire search space or on simultaneously exploring both plans\nand solutions. To improve transfer performance across a broader range of reasoning tasks,\nwe propose learning effective and diverse planning, which enables the model to acquire\nmore task-agnostic capabilities and thereby achieve better generalization. We first create a\nstep-by-step plan to solve the problem, with the final step presenting the full solution and\nfinal answer based on the plan. The prompt is provided in the Appendix A.1. Ultimately,\nwe obtain a plan tree and high-quality planning step supervision through iterative search\nsimulations with MCTS (Figure 2)."}, {"title": "3.2 STEP-APO", "content": "Unlike mainstream approaches that learn step-level\npreferences by identifying the first error step and sampling a corresponding preferred step,"}, {"title": "3.2.1 PRELIMINARIES", "content": "The Classical RL Objective RLHF approaches usually first learn a reward function from human feedback, then\noptimize it with a policy gradient-based method like PPO with an\nentropy-bonus using the following multi-step RL objective:\n$\\max \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}(-\\mid s_0)} [\\sum_{t=0}^{T} (r(s_t, a_t) + \\beta \\log \\pi_{ref}(a_t|s_t))] + \\beta H(\\pi_{\\theta}) |s_0 \\sim p(s_0)$ (5)\n,\nwhere $r(s_t, a_t)$ denotes the step-level reward function, followed by a KL penalty that aims\nto ensure the learned policy $\\pi_{\\theta}$ does not deviate significantly from the reference policy $\\pi_{ref}$.\n$\\pi_{ref}$ is typically produced via supervised fine-tuning.\nDirect Preference Optimization DPO uses the well-known closed-form optimal solution, which establishes a mapping between the reward model and the\noptimal policy under the KL divergence, obtaining the reward as:\n$r(x, y) = \\beta \\log \\pi^*(y|x) - \\beta \\log \\pi_{ref}(y|x) - Z(x),$ (6)\nwhere $x$ denotes the prompt and $y$ denotes the response, $\\pi^*$ is the optimal policy and $Z(x)$\nis the partition function that normalizes it. Substituting eq. (6) into the Bradley Terry\npreference model, and leverage the maximum likelihood objective, DPO derives the loss:\n$\\mathcal{L}_{DPO}(\\pi_{\\theta}; \\pi_{ref}) = -\\mathbb{E}_{(x, y_w, y^l) \\sim \\mathcal{D}} \\log \\sigma(\\beta \\log \\frac{\\pi_{\\theta}(y_w | x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_{\\theta}(y^l | x)}{\\pi_{ref}(y^l|x)}),$ (7)\nwhere $\\sigma$ denotes the logistic function, $y^w$ and $y^l$ denote the preferred and dis-preferred\nresponses to the prompt $x$."}, {"title": "3.2.2 DERIVING THE STEP-APO OBJECTIVE", "content": "In the general maximum entropy RL setting , the optimal policy $\\pi^*(a|s)$ of\nmulti-step RL objective in eq. (5) is:\n$\\pi^*(a_t|s_t) = e^{(Q^*(s_t,a_t)-V^*(s_t))/\\beta},$ (8)\nwhere $Q^*(s, a)$ is the optimal Q-function which models the total future reward from $(s_t, a_t)$\nunder $\\pi^*$. The optimal value function $V^*$ estimates the total future reward under state $s_t$,\nand it's a function of $Q^*$\nUnder the reward r with a KL divergence penalty, the relationship between Q-function and\nstep-level reward function can be established with the Bellman equation as follows:\n$Q^*(s_t, a_t) = r(s_t, a_t) + \\beta \\log \\pi_{ref}(a_t|s_t) + V^*(s_{t+1}).$ (9)\nBy log-linearizing the optimal policy in eq. (8) and substituting in the Bellman equation\nfrom eq. (9) , we have below equation which is\nprecisely the optimal advantage function $A^*(s, a) = Q^*(s, a) - V^*(s)$:\n$\\beta \\log \\frac{\\pi^*(a_t | s_t)}{\\pi_{ref}(a_t | s_t)} = r(s_t, a_t) + V^*(s_{t+1}) - V^*(s_t).$ (10)\nUnlike DPO utilize response-level Bradley Terry model, we introduce step-level Bradley\nTerry preference model to learn fine-grained step-level preference:\n$p^*(a^w \\succ a^l|s) = \\frac{exp (r(s, a^w))}{exp (r(s, a^w)) + exp (r(s, a^l))}$ (11)"}, {"title": "4 EXPERIMENTS", "content": "4.1 IMPLEMENTATION DETAILS\nWe iteratively generate data via MCTS and train our policy and value models in two rounds.\nIn each round, planning steps and final solution steps are generated by the policy model using"}, {"title": "5 CONCLUSION", "content": "In this work, we propose that learning planning can improve a model's reasoning and\ngeneralization capabilities. By focusing on finer-grained learning of plan step preferences\nthrough our Step-APO, the model can identify critical planning steps within the reasoning\ntrace, further enhancing its reasoning ability. Although we trained on GSM8K and MATH\ndata, our approach has demonstrated general improvements on other reasoning tasks such as\nBBH, ARC-C, and MMLU-STEM.\nFinding an effective way to improve transfer performance to more reasoning tasks and\nenhance overall model generalization in reasoning remains an open and important research\nquestion that has yet to be fully addressed. We believe that learning the critical planning\nsteps for solving a problem is crucial for enhancing the model's reasoning capabilities. At\nthe same time, the relative advantages between these planning steps are important for\noptimization. Additionally, the diversity of preference data is essential for learning various\nplanning strategies. In future work, we plan to explore the application of our method to\nother types of data, such as code. Additionally, we will continue to refine our approach,\nexploring various improvements such as enhancing the diversity of planning steps to better\ncapture a broader range of planning step preferences."}]}