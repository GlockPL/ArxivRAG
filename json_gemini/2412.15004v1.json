{"title": "Large Language Models and Code Security: A Systematic Literature Review", "authors": ["Enna Basic", "Alberto Giaretta"], "abstract": "Large Language Models (LLMs) have emerged as powerful tools for automating various programming tasks, including security-related ones, such as detecting and fixing vulnerabilities. Despite their promising capabilities, when required to produce or modify pre-existing code, LLMs could introduce vulnerabilities unbeknown to the programmer. When analyzing code, they could miss clear vulnerabilities or signal nonexistent ones. In this Systematic Literature Review (SLR), we aim to investigate both the security benefits and potential drawbacks of using LLMs for a variety of code-related tasks. In particular, first we focus on the types of vulnerabilities that could be introduced by LLMs, when used for producing code. Second, we analyze the capabilities of LLMs to detect and fix vulnerabilities, in any given code, and how the prompting strategy of choice impacts their performance in these two tasks. Last, we provide an in-depth analysis on how data poisoning attacks on LLMs can impact performance in the aforementioned tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) are Machine Learning (ML) models designed for natural language processing [1]. Representative models such as ChatGPT [2], Llama [3], GitHub Copilot [4], and BERT [5] have gained extreme popularity and recognition in the last couple of years due to their ability to perform well across different types of tasks.\nAmong other capabilities, LLMs are effective in translating natural language explanations into code, assisting in code comprehension, debugging, and answering questions related to code [6]. Many developers utilize these abilities to optimize their coding processes, reducing development time and thereby improving productivity. According to GitHub statistics, GitHub Copilot is now responsible for generating approximately 46% of code and boosts developers' coding speed by up to 55% [7]. In addition to their coding skills, LLMs can be powerful tools for vulnerability detection and fixing, particularly when provided with detailed prompts and clear background information [8], [9].\nWhile LLMs represent an advancement in code generation, they exhibit several limitations that pose security risks. For example, one concern is that, despite their proficiency in producing functional code, LLMs often lack awareness and understanding of secure coding practices [10], [11]. Another concern is that LLMs are trained on enormous datasets, often gathered from unverified online platforms (e.g., GitHub [12] and HuggingFace [13]). This reliance on potentially insecure data introduces data poisoning risks, where an attacker inserts harmful data samples into the training set [14]. These and other shortcomings can lead LLMs to generate insecure code, as well as influence their abilities to detect and fix security vulnerabilities.\nRegardless of the risks, LLMs are largely used for their code-related capabilities. Therefore, to take informed decisions, it is crucial for practitioners to have a clear understanding about the precise nature of risks and benefits. In this Systematic Literature Review (SLR), we aim to:\n\u2022 Explore the potential security vulnerabilities introduced by LLM-generated code;\n\u2022 Evaluate the effectiveness of LLMs in detecting and fixing vulnerabilities, as well as the prompting strategies used for these tasks;\n\u2022 Investigate the effects of data poisoning on LLMs ability to produce secure code and to detect and fix vulnerabilities effectively."}, {"title": "A. Outline", "content": "The remainder of this paper is organized as follows. Section II describes related works. Section III describes the methodology we have followed in this work, as well as the research questions that guided our investigation. Section IV discusses the vulnerabilities identified in LLM-generated code, by the state of the art. Section V examines LLMs ability to detect code vulnerabilities, Section VI their ability to fix pre-identified vulnerabilities, and Section VII their potential to detect and fix vulnerabilities in a unified process. Furthermore, Section VIII covers prompting techniques used for vulnerability detection and fixing, as described by the studies covered in this review. Section IX explores the impact of poisoned training data on LLMs ability to generate secure code, detect vulnerabilities, and fix them. Section X addresses the research questions, open challenges, and threats to validity. Finally, Section XI concludes the paper."}, {"title": "II. RELATED WORK", "content": "In recent years, several literature reviews have focused on different aspects of the intersection between LLMs and code security. As we discussed in Section I, in this SLR we focus on three main thematic areas. Although other surveys have covered these areas, all of them focused on only one (or two) areas at a time. To the best of our knowledge, this is the first comprehensive survey that addresses all three areas. Furthermore, this is the first paper that attempts to categorize the different vulnerabilities that could be introduced by LLMs.\nFor example, Negri et al. [15] conducted an SLR to explore the impact of AI models on software security, finding that LLMs often generate code containing specific vulnerabilities, but they did not provide a systematic categorization. Extending their work, we organize the vulnerabilities that can be introduced by LLM-generated code, into ten distinct categories. Besides, we analyze quantitatively their prevalence across studies, highlighting less explored areas.\nOther characteristics differentiate our paper from the current literature. First, while other surveys have investigated the performance of LLMs for vulnerability detection and fixing, unified processes for addressing both tasks have not been surveyed. Second, no other survey investigates how the prompting strategy of choice impacts tasks related to code security.\nYao et al. [16] reviewed various applications of LLMs in security, including vulnerability detection and fixing. However, their work did not investigate the impact of prompting techniques on such tasks, nor the merging of vulnerability detection and fixing in a unified process. Zhou et al. [17] provided an overview of prompting techniques, listing which papers applied each technique for vulnerability detection and repair. Unlike their work, we focus on how each prompting technique impacts LLMs effectiveness in these tasks.\nFurthermore, the literature does not cover the impact of data poisoning attacks on LLMs code generation, and our work fills this gap by analyzing this potential impact. Even though data poisoning attacks are explored by Yao et al. [16], the paper does not discuss the effects of these attacks on LLMs secure code generation, nor on vulnerability-related tasks. Chen et al. [18] reviewed defenses and attacks targeting LLMs used for code generation, highlighting the general impact of such attacks. Although they also discussed the consequences of poisoning attacks on secure code generation, this was not the main focus of their paper. For example, they also explored the effects on tasks unrelated to security, including code summarization and code search. In contrast, we provide an in-depth exploration of security-specific aspects, discussing how poisoning attacks influence the ability of LLMs to generate secure code, detect vulnerabilities, and propose fixes.\nA few relevant studies conducted broader literature reviews, that extend beyond the focus on code security. Xu et al. [19] examined LLM applications across multiple cybersecurity tasks, including vulnerability detection, malware analysis, network intrusion detection, and phishing detection. Hou et al. [20] conducted a comprehensive study on the use of LLMs in software engineering, including, but not limited to, code security. They pointed out that code generation and program repair are the most common tasks where LLMs are applied to, in software development and maintenance. Last, Zhang et al. [21] focused on methodologies for constructing LLMs, specifically designed for cybersecurity tasks."}, {"title": "A. Research Questions", "content": "We formulated the following Research Questions (RQs) to guide our investigation and help fulfill the aims of this study:\n\u2022 RQ1: What security vulnerabilities could be introduced by LLM-generated code?\n\u2022 RQ2: To what extent can LLMs detect and fix vulnerabilities, in both human-written and LLM-generated code?\n\u2022 RQ2.1: How do different prompting techniques impact the effectiveness of LLMs in detecting and fixing code vulnerabilities?\n\u2022 RQ3: How does the poisoning of training datasets impact the LLMs ability of producing secure code, detecting and fixing vulnerabilities?"}, {"title": "B. Search Strategy", "content": "Based on our RQs, we have identified 5 sets of keywords, which we later used to define our search strings:\n\u2022 Set 1: Large Language Models, Language Model, LLMs, CodeX, Llama, Copilot, GPT-*, ChatGPT.\n\u2022 Set 2: Code Generation, AI-generated Code, Automated Code Generation.\n\u2022 Set 3: Security Vulnerabilities, Security Risks, Security Flaws, Security Implications, Software Security, Impact On Code Security, Cybersecurity, Vulnerabilities.\n\u2022 Set 4: Training Data Poisoning, Poisoned Datasets, Data Poisoning Attacks, Adversarial Attacks, Malicious Training Data.\n\u2022 Set 5: Vulnerability Detection, Bug Detection, Security Flaw Detection, Code Analysis, Static Analysis, Vulnerability Remediation, Bug Fixing, Automated Code Repair, Security Patch, Code Patching.\nBased on these sets of keywords, we developed the following search strings to perform an automated database search.\nThe first search string is designed to address RQ1:\n(\"Large Language Models\" OR \"Language Model\" OR \"LLMS\" OR CodeX OR Llama OR Copilot OR GPT-* OR ChatGPT) AND (\"Security Vulnerabilities\" OR \"Security Risks\" OR \"Security Flaws\" OR \"Software Security\" OR \"Impact On Code Security\" OR Cybersecurity OR Vulnerabilities) AND (\"Code Generation\" OR \"AI-generated Code\" OR \"Automated Code Generation\").\nThe second search string is designed to address RQ2:\n(\"Large Language Models\" OR \"Language Model\" OR CodeX OR Llama OR Copilot OR GPT-* OR ChatGPT) AND (\"Vulnerability Detection\" OR \"Bug Detection\" OR \"Security Flaw Detection\" OR \"Code Analysis\" OR \"Static Analysis\" OR \"Vulnerability Remediation\" OR \"Bug Fixing\" OR \"Automated Code Repair\" OR \"Security Patch\" OR \"Code Patching\").\nFinally, the third search string is designed to address RQ3:"}, {"title": "III. METHODOLOGY", "content": "In this study, we conducted an SLR following the well-established guidelines proposed by Petersen et al. [22]. Our methodology, illustrated in Figure 1, includes three main phases: Planning, Conducting, and Reporting. In the remainder of this section, we detail the Planning phase."}, {"title": "C. Selection Criteria", "content": "To ensure a relevant review of the literature, we defined specific inclusion and exclusion criteria. We designed the inclusion criteria to make sure the selected papers meet certain standards and relevance, while exclusion criteria help us eliminate papers that do not fit the research objectives or quality standards. The detailed criteria for paper selection are outlined in Table I.\nGiven that the field is relatively new and has seen significant contributions in the last few years, we did not establish criteria for narrowing down the period of publications. For the same reason, we also include non-peer-reviewed papers, such as those from arXiv, to ensure that we capture the most recent and significant developments in the field."}, {"title": "IV. SECURITY VULNERABILITIES INTRODUCED BY LLMS", "content": "Although LLMs became popular for their ability to generate functional code fast and speed up development, they can introduce security vulnerabilities that developers might miss, leading to risks regarding code integrity and safety [11]. In this section, we focus on specific vulnerabilities introduced by LLM-generated code, as identified through various studies we have analyzed in this paper.\nInjection vulnerabilities are identified across most of the reviewed studies, appearing in 16 papers. This category contains various types of vulnerabilities, including SQL injection, Cross-Site Scripting (XSS), OS command injection, and regular expression (regex) injection.\nTwo studies focused extensively on injection vulnerabilities. Khoury et al. [24] conducted an experiment where they generated 21 programs in 5 different programming languages using ChatGPT 3.5. The programs were intentionally diverse, each designed to highlight the risks associated with specific vulnerabilities. Among these, 6 programs specifically targeted injection vulnerabilities. Notably, in every instance, ChatGPT 3.5 failed to do proper input sanitization, resulting in various types of injection vulnerabilities. Similarly, T\u00f3th et al. [34] focused on the security of 2500 PHP websites, generated by GPT-4. The research was conducted by analyzing the code from 2,500 PHP websites. Using both dynamic and static scanners, manual code verification, and penetration testing, they identified various vulnerabilities, particularly concerning the security of file upload functions, as well as SQL injections, XSS, and reflected XSS.\nThe following CWEs related to injections were frequently identified throughout the studies: CWE-79: Improper Neutralization of Input During Web Page Generation ('Cross-site Scripting') [23], [28], [33], [30], [31], [11], [39], [40], and CWE-89: Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection') [23], [28], [11], [33], [38], [39], [10]."}, {"title": "B. Memory Management", "content": "This category includes vulnerabilities related to memory management, which were also identified across most of the included studies. These vulnerabilities include overflow vulnerabilities, such as buffer and integer overflow, mostly caused by the lack of input sanitization. Moreover, this category includes null pointer dereference and use-after-free memory allocation vulnerabilities. These issues arise due to improper handling of memory references and pose significant risks to system security. Although [23], [24], [26], [27], [28], [31], [32], [11], [33], [35], [39] all identified memory vulnerabilities in code produced by LLMs, Liu et al. [23] placed special importance on memory management vulnerabilities. They performed a systematic empirical assessment of the quality of code generated using ChatGPT, by solving 728 algorithmic problems in 5 languages. Their study highlighted significant issues in memory management within LLM-generated code. Specifically, they found that the majority of vulnerabilities were related to missing null pointer tests, with 91.8% of the total vulnerabilities attributed to the MissingNullTest query. These code snippets failed to check for null after memory allocation, which can lead to critical security vulnerabilities such as CWE-476: Null pointer dereference, also identified in [27], [28], [32], [35].\nIn addition to null pointer dereference, other frequently identified CWEs related to memory management include: CWE-190: Integer Overflow or Wraparound [23], [24], [28], [31], [11], [35], and CWE-416: Use After Free [23], [27], [28]."}, {"title": "C. File Management", "content": "File management vulnerabilities are mostly caused by improper handling of files and directories. These vulnerabilities can lead to unauthorized access, data corruption, or system compromise. The most common file management vulnerabilities include improper path restrictions, unrestricted file uploads, and insecure file permissions. Several studies have identified file management vulnerabilities in LLM-generated code [23], [24], [26], [28], [31], [33], [34], [38], [38], [39], [40]. However, He et al. [11], and Mohsin et al.[10] dedicated more focus to these vulnerabilities, finding that LLMs often produce code that does not properly restrict file paths based on user input, potentially allowing unauthorized file access. This vulnerability is marked as CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal') [23], [28], [31], [33], [38], [39], [10], [40]."}, {"title": "D. Deserialization", "content": "Deserialization vulnerabilities happen when untrusted data is used to reconstruct objects within an application. These vulnerabilities can lead to arbitrary code execution, data manipulation, or denial-of-service attacks. These vulnerabilities are especially concerning in systems that use serialization for communication or storage, as they can be exploited if proper validation and sanitization are not implemented. The primary CWE associated with this issue is: CWE-502: Deserialization of Untrusted Data [23], [33], [37], [39], [40].\nSeveral studies have recognized deserialization vulnerabilities [23], [33], [35], [37], [39], [40], however, none of them specifically focused on this topic, leaving some open questions for further research. For example, how frequently do LLMs generate insecure code when prompted with tasks targeting deserialization vulnerabilities?"}, {"title": "E. Sensitive Data Exposure", "content": "Sensitive data Exposure occurs when critical information, such as personal data, financial details, or authentication credentials, are not properly protected and become accessible to unauthorized entities. Sensitive data exposure is often caused by ineffective data encryption, insecure storage, or flaws in data transmission protocols.\nSeveral studies have recognized the exposure of sensitive data in the code generated by LLMs [23], [24], [29], [30], [38], [39], [10], [40]. However, Pearce et al. [28] dedicated more attention to this category of vulnerabilities. They studied how frequently and why GitHub Copilot generates insecure code by testing it with 89 different scenarios related to high-risk cybersecurity vulnerabilities. Besides from finding that approximately 40% of these programs were vulnerable, they showed that Copilot consistently fails when it comes to CWE-200: Exposure of Sensitive Information to an Unauthorized Actor. The same CWE was also mentioned in [23], [39], [10], [40]. A concrete example of how Copilot-generated code"}, {"title": "F. Authentication and Authorization", "content": "Authentication and authorization vulnerabilities represent a significant category of security issues, highlighted in 12 out of 20 studies.\nAuthentication vulnerabilities occur when a system fails to properly verify the identity of users or devices. This kind of vulnerability is mostly caused by weak password requirements, insufficiently protected credentials, or the use of hard-coded credentials.\nHamer et al. [36] compared the security properties of the code generated by ChatGPT, with answers provided in Stack Overflow. They concluded that ChatGPT-generated code had 20% fewer vulnerabilities than Stack Overflow snippets, although both produced significant security flaws. One of the most frequent CWEs in both ChatGPT-generated code and Stack Overflow snippets was CWE-798: Use of Hard-coded Credentials. CWE-798 is also discussed in [23], [28], [30], [40].\nAuthorization vulnerabilities arise when a system fails to properly enforce access control policies, which allows users or processes to perform actions beyond their intended permissions. This vulnerability is often caused by improper implementation of access control. The associated CWE is CWE-284: Improper Access Control, as discussed by Asare et al. [25], who conducted a comparative empirical analysis of Copilot-generated code, showing that GitHub Copilot is less likely to produce vulnerable code, compared to human developers."}, {"title": "G. Cryptography", "content": "Cryptographic vulnerabilities arise when encryption mechanisms are poorly implemented or completely absent. Common causes of weak encryption include the use of broken or insecure cryptographic algorithms, improper validation of certificates, the use of weak or absent hashing functions, and the reuse or hard-coding of cryptographic keys. Several studies [24], [30], [33], [36], [37], [38], [39], [10], [40] identified cryptographic weaknesses in LLM-generated code.\nPerry et al. [26] conducted a user study to examine how users interact with AI code assistants to solve security-related tasks. The study found that participants with access to an AI assistant were more likely to produce incorrect and insecure solutions, compared to a control group. Specifically, in one cryptography-related task, participants with AI assistance were significantly more likely to use trivial ciphers or fail to authenticate the final output.\nA common vulnerability identified in multiple studies is CWE-327: Use of a Broken or Risky Cryptographic Algorithm [33], [36], [37], [39]."}, {"title": "H. Resource Management", "content": "Resource Management vulnerabilities refer to weaknesses in the handling, allocation, and release of system resources that are not strictly related to memory or file systems. These vulnerabilities were also identified across multiple studies as improper control of resources, resource leaks, and improper resource shutdowns.\nHamer et al. [36] found that two CWEs directly related to resource management were particularly frequent in ChatGPT. CWE-404: Improper Resource Shutdown or Release, and CWE-772: Missing Release of Resource after Effective Lifetime. Both CWEs were tied as the second most frequent vulnerabilities, each appearing in 37, out of 216 code snippets. Other CWEs within the same vulnerability category were also highlighted in the studies, including CWE-605: Multiple Binds to the Same Port [37], [40], and CWE-664: Improper Control of a Resource Through its Lifetime [25].\nIn general, resource management vulnerabilities were identified by [25], [27], [32], [37], [38], [10], [40]."}, {"title": "I. Coding Standards", "content": "This vulnerability category includes issues arising when software development practices do not adhere to established standards, leading to inconsistent, unreliable, or insecure code. Such deviations can introduce unexpected bugs, security flaws, or system crashes. Common vulnerabilities of this type include logical errors, such as division by zero and incorrect calculations, as well as issues with function calls, such as incorrect number of arguments or incorrect argument types.\nTihanyi et al. [35] conducted a comparative analysis of LLMs to examine how likely they are to generate vulnerabilities when writing simple C programs. They identified division by zero as a frequent issue in the code produced by LLMs, highlighting the difficulties in managing arrays and accurately performing arithmetic operations.\nAdditionally, CWE-758: Reliance on Undefined, Unspecified, or Implementation-Defined Behavior is another significant concern discussed in by Kim et al. [32], and Sandoval et al. [27]. Besides the mentioned studies, others have focused on this type of vulnerability [25], [29], [38], [40]."}, {"title": "J. Error Handling", "content": "Error handling vulnerabilities arise when a system or application fails to properly manage exceptional conditions. Poor error handling can lead to a variety of issues, including security vulnerabilities, system crashes, or unreliable software behavior. One common issue in error handling is the failure to check or handle return values from functions or system calls properly. A few studies focused on error handling vulnerabilities [27], [29], [37], however, none of these studies specifically addressed this category of vulnerabilities, which presents another topic for future research. Among the various CWS, CWE-703: Improper Check or Handling of Exceptional Conditions has been specifically highlighted in the literature as a notable issue [37]."}, {"title": "K. LLMs in Code Generation: Balancing Risks and Benefits", "content": "As we have described throughout this section, LLMs often produce code with security vulnerabilities that developers might overlook [26], [29], [30], [34], [35], [38]. Interestingly, when ChatGPT fixes code initially written by humans, the resulting code often contains more security vulnerabilities compared to code generated from scratch by ChatGPT [37].\nDespite these challenges, LLMs remain valuable tools in coding tasks. Research indicates that code generated by ChatGPT generally introduces fewer CWE issues compared to code found on Stack Overflow [36]. Moreover, LLMs do not significantly increase the incidence rates of severe security bugs [27], and tools like GitHub Copilot have been shown to produce code with fewer security vulnerabilities than code written by human developers [25].\nEven though LLMs may generate vulnerable code, the literature suggests that using more effective prompts can guide them to produce safer code [23], [24]. Through improved re-prompting, the security issues initially present in LLM-generated code can often be addressed, making LLMs valuable tools for creating secure code, when used appropriately. This topic will be explored more in details in Section VII."}, {"title": "V. LLMS FOR VULNERABILITY DETECTION", "content": "Security code reviews performed at the early stages of software development can prevent the introduction of security flaws into code repositories. This helps to reduce future costs, associated with fixing and maintaining the software. Security code reviews performed manually require significant time and effort, especially in large-scale open-source projects with large numbers of contributions. Hence, the automated tools able to identify security vulnerabilities during code review are highly beneficial [43]. Among these tools, LLMs emerged as a promising solution for detecting security vulnerabilities.\nThis section explores the effective use of LLMs for detecting vulnerabilities in code, in particular, we highlight their limitations, compare their performance with traditional methods, and examine the potential of fine-tuning to enhance their effectiveness in this task."}, {"title": "A. Uses of LLMs for Vulnerability Detection", "content": "Despite the growing interest in exploiting LLMs for vulnerability detection, their specific capabilities and performance in real-world scenarios require a deeper exploration.\nThe work by Mohajer et al. [44] demonstrated the ability of ChatGPT to perform static bug detection and false positive warning removal. The evaluation focused on two critical types of bugs often targeted by Static Analysis Tools (SATs): Null Dereference and Resource Leak. For detecting Null Dereference, bugsChatGPT achieved around 68% accuracy and 64% precision, and for detecting Resource Leak bugs, about 77% accuracy and 83% precision. Concerning false-positive warning removal, ChatGPT reached around 94% precision for Null Dereference bugs and 63% for Resource Leak bugs.\nThapa et al. [45] empirically evaluated the performance of several transformer-based language models for software vulnerability detection. The authors presented a framework that allows for translating C/C++ source code into vectorized inputs, a format suitable for vulnerability analysis. The authors tested various models on software vulnerability datasets, and found that, concerning Buffer Errors and Resource Management Errors, GPT-2 Large and GPT-2 XL achieved the highest F1-score of all the models, with 95.51% and 95.40%, respectively.\nA study by Zhang et al. [46] focused on LLMs capabilities for automated vulnerability detection. They tested over 10 different LLM models using various techniques like zero-shot and one-shot learning (for more details on prompting techniques, we refer the reader to Section VIII), and fine-tuning. They applied these models to various datasets of source code, exhibiting C/C++ and smart contract vulnerabilities. The study found that CodeLlama-7B achieved the highest F1-score of 82% with discriminative fine-tuning, along with a precision of approximately 89% and a recall of around 78%. In comparison, other methods were less effective. The authors have also identified issues with input length limits, and proposed a solution based on sliding windows that divide long sequences into overlapping sequences of tokens. In addition, they introduced a right-forward embedding strategy, which enhances context by making each token aware of tokens that come after. Overall, LLMs appear to be valuable tools for vulnerability detection in real-world scenarios.\nAccording to Wen et al. [47], existing pre-trained models for vulnerability detection often miss important details and struggle with complex code. As a consequence, performing accurate vulnerability detection can be difficult. To address these challenges, Wen et al. [47] proposed a Structured Natural Language Comment tree-based Vulnerability Detection framework, based on pre-trained models, named SCALE. This framework is composed of three main modules. The first module, Comment Tree Construction, helps the model understand code semantics, by incorporating LLMs for comment generation and adding the comment node to Abstract Syntax Trees (ASTs). The second module, Structured Natural Language Comment Tree Construction, refines this process by explicitly involving code execution sequences and combining code syntax templates with the comment tree. The third module, SCT-Enhanced Representation, uses these improved comment trees to capture vulnerability patterns and better understand vulnerabilities in the code.\nResults show that SCALE outperforms existing methods, including supervised, retrained model-based, and other LLM-based approaches in detecting vulnerabilities. SCALE achieved notable improvements in precision, recall, and F1 scores across multiple datasets, achieving, for instance, up to around 65% F1 score.\nOmar et al. [48] proposed a Knowledge Distillation (KD) technique, designed to improve vulnerability detection. KD is a technique where a smaller, simpler model (the \"student\") learns to replicate the behavior of a larger, more complex model (the \"teacher\"). The goal is to achieve similar or even better performance with the smaller model [49]. They assessed the performance of the KD technique when applied to various classifiers, including LLMs like GPT-2 and CodeBERT, as well as Recurrent Neural Network (RNN) models such"}, {"title": "B. Issues in Using LLMs for Vulnerability Detection", "content": "While LLMs show considerable promise in vulnerability detection, several challenges and limitations persist. One of the key challenges for LLMs is the occurrence of false positives, where the model incorrectly flags a non-existent vulnerability. For example, Steenhoek et al. [56] searched for systematic optimal prompts and proposed three prompting methods, which they used for evaluating the vulnerability detection abilities of 11 LLMs. They found that, although their proposed prompting methods enhanced LLMs performance, the LLMs still struggled with vulnerability detection. Specifically, these models achieved only 50% to 63% Balanced Accuracy, often performing close to random guessing in many scenarios. These models frequently produced false positives, mistakenly identifying vulnerabilities where none existed, a problem often referred to as hallucinations.\nAdding to that, Happe et al. [57] also discussed hallucinations phenomenon in LLMs. While not frequent, the authors claim that the presence of hallucinations implies the need for better grounding and context-awareness, especially when using LLMs for critical tasks, such as security testing. The study explored the use of LLMs in penetration testing, proposing their application in two key areas: high-level task planning and low-level vulnerability hunting. For high-level tasks, LLMs assist by creating attack strategies for various systems, suggesting realistic attack vectors. For low-level tasks, the LLMs interact with a vulnerable virtual machine via SSH, analyzing the machine state and suggesting specific attacks that are then automatically executed. This demonstrates that LLMs can be useful in penetration testing, but the potential for hallucinations reinforces the importance of ensuring that the model outputs are reliable and contextually grounded.\nYu et al. [58] explored LLMs potential in security code review. To do that, they compared the performance of 6"}, {"title": "C. Comparison with Other Approaches", "content": "LLMs have shown great promise in vulnerability detection, but it is essential to compare their performance against traditional approaches, such as SATs and other ML models, used in this field for many years. Some studies suggest that LLMS outperform state-of-the-art SATs and ML models in detecting vulnerabilities, while others disagree, highlighting a divide in current research on their comparative effectiveness. \nAccording to Yu et al. [58], traditional code analysis tools, despite their long-standing use, often struggle with issues such as high false positive rates, limited input ranges, and poor scalability. In contrast, LLMs, particularly GPT-based models provide higher detection rates, compared to contemporary SATs like CppCheck [75], Bandit [76], CodeQL [77], SonarQube [78], and Semgrep [79].\nFor example, in the experiment conducted by Mohajer et al. [44], ChatGPT outperforms Infer [80], offering greater effectiveness in vulnerability detection and code analysis. ChatGPT surpassed Infer, improving precision by around 13% for Null Dereference bugs and 43% for Resource Leak bugs. Similarly, in [72] GPT-4 outperformed Fortify [81] and Snyk [82] detecting roughly four times more vulnerabilities. In another study [54], researchers combined SAT CodeQL with LLMs, achieving a 35% increase in vulnerability detection compared to using CodeQL alone. Some SATs, such as Cppcheck, Flawfinder, RATS, and Semgrep, seem to perform poorly with respect to LLMs. As observed by Wen et al. [51], the reason seems to lie in the fact that these tools have a clear focus on specific vulnerabilities.\nWhile some researchers find SATs to perform poorly in vulnerability detection, other studies suggest that they outperform LLMs in certain aspects, such as reducing false positives. For instance, Purba et al. [60] found that LLMs do not perform as well as SATs, such as Flawfinder [83], RATS [84], and Checkmarx [85] in detecting software vulnerabilities, primarily due to high false positive rates. They therefore suggested using LLMs in combination with SATs for improved performance.\nZhou et al. [70] found that LLMs can detect up to 100% of code vulnerabilities, but they often produce false positive results. In contrast, although SATs achieve lower vulnerability detection rates of up to 44.4%, they also exhibit lower false positive rates."}, {"title": "D. Factors Influencing LLMs Performance", "content": "Despite the remarkable capabilities of LLMs in code analysis tasks, their effectiveness is influenced by various factors, including the prompts used (further explored in Section VIII), and the state of the code being analyzed.\nOne of the issues arises when resolved vulnerabilities remain in the code, often in the form of comments and annotations left by the developers. LLMs may mistakenly interpret these resolved issues as existing vulnerabilities, which can lower their ability to accurately detect existing security vulnerabilities. This implies that there is a need for developers to standardize code commenting practices, to avoid confusion and improve LLMs performance in detecting security vulnerabilities. Additionally, when handling longer pieces of code, LLMs often struggle to identify smaller, more subtle security defects [58].\nSeveral other challenges affect LLMs performance in detecting vulnerabilities. These include difficulty in understanding developers' intent, misinterpretation of code comments, and inability to execute code. Chen et al. [65] found that these challenges contributed to the high rate of false positives obtained when experimenting with ChatGPT as a vulnerability detection tool for smart contracts.\nSome authors have focused their research on identifying and addressing the factors that influence LLMs performance in code analysis tasks, aiming to enhance their accuracy and reliability. Specifically, Wang et al. [101] investigated the impact of naming on LLMs code analysis. In their experiment, they created datasets containing code with nonsensical or misleading variables, methods, and function names. Using CodeBERT for the analysis, they demonstrated that naming significantly affects LLMs ability to detect vulnerabilities, indicating that LLMs rely heavily on clear and meaningful names.\nFang et al. [102] identified a gap in research, concerning how effective LLMs are at analyzing code, particularly when dealing with confusing, obfuscated, or poorly readable code. The authors first conducted experiments on non-obfuscated code, and found that GPT-3.5 and GPT-4 were more likely than smaller models to generate accurate and detailed explanations for input code. On the other hand, obfuscated code impacted the ability of LLMs to generate explanations. GPT-3.5 and GPT-4 both had a drop in analysis accuracy, while smaller models were unable to handle obfuscated code. In conclusion, LLMs from the GPT series performed impressively in code"}, {"title": "E. LLMs Fine-tuning for Vulnerability Detection", "content": "To enhance the performance of LLMs on vulnerability detection tasks, several researchers have explored fine-tuning techniques. This process, illustrated in Figure 3, involves adapting pre-trained models to security-related tasks, by further training them on security-specific dataset.\nIn their work, Shestov et al. [103", "104": "to improve it in vulnerability detection tasks for Java code. Their first contribution involved modifying the training process, which resulted in faster training times without compromising the model performance. The fine-tuned WizardCoder outperformed models such as CodeBERT, demonstrating the potential of fine-tuning LLMs for specialized source code analysis tasks. The authors also noted that vulnerability detection faces a fundamental challenge of class imbalance, with far fewer positive cases (i.e., the presence of vulnerabilities) than negative cases. By applying a combination of focal loss and sample weighting, the authors showed that models can achieve notable performance, even when dealing with such imbalances.\nAfter evaluating LLMs for vulnerability detection, the authors of [61"}]}