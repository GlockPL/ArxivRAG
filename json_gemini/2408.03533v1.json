{"title": "Lifelong Personalized Low-Rank Adaptation of Large Language Models for Recommendation", "authors": ["Jiachen Zhu", "Jianghao Lin", "Xinyi Dai", "Bo Chen", "Rong Shan", "Jieming Zhu", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "abstract": "We primarily focus on the field of large language models (LLMs) for recommendation, which has been actively explored recently and poses a significant challenge in effectively enhancing recommender systems with logical reasoning abilities and open-world knowledge. Current mainstream efforts mainly center around injecting per-sonalized information from recommendation models into LLMs by customizing input templates or aligning representations between semantic and recommendation spaces at the prediction layer. However, they face three significant limitations: (1) LoRA is mostly used as a core component in existing works, but personalization is not well established in LoRA parameters as the LoRA matrix shared by every user may not cater to different users' characteristics, lead-ing to suboptimal performance. (2) Although lifelong personalized behavior sequences are ideal for personalization, their use raises effectiveness and efficiency issues since LLMs require escalating training and inference time to extend text lengths. (3) Existing ap-proaches aren't scalable for large datasets due to training efficiency constraints. Thus, LLMs only see a small fraction of the datasets (e.g., less than 10%) instead of the whole datasets, limiting their exposure to the full training space. To address these problems, we propose RecLoRA. This model incorporates a Personalized LoRA module that maintains independent LoRAs for different users and a Long-Short Modality Retriever that retrieves different history lengths for different modalities, significantly improving performance while adding minimal time cost. Furthermore, we design a Few2Many Learning Strategy, using a conventional recommendation model as a lens to magnify small training spaces to full spaces. Extensive experiments on public datasets demonstrate the efficacy of our RecLoRA compared to existing baseline models.", "sections": [{"title": "1 INTRODUCTION", "content": "Recommender systems play essential roles in various online services to mitigate the information overload problem and meet users' information needs [19, 43, 45, 60, 61]. Besides, large language models (LLMs) have experienced significant growth in the field of natu-ral language processing (NLP), demonstrating a remarkable ability to comprehend human language and generate text that closely resembles human writings for a broad range of tasks [3, 67, 82]. Recent studies have started to investigate the potential of LLMs for recommender systems under a variety of recommendation tasks, such as listwise ranking and pointwise scoring [42, 44]. These stud-ies usually inject personalized knowledge from recommendation domains into LLMs. [44, 80, 83]. This personalized knowledge plays a pivotal role in tailoring experiences to individual preferences and enhancing user interactions and engagements [7]. So our crucial points are what useful personalized knowledge we can provide and how to inject it into LLM to help with predictions.\nAs the proverb \"Actions speak louder than words\" suggests, a user's behavior sequence is the most direct expression of their preferences. It typically provides the most detailed personalized information about users in recommendation scenarios. As illus-trated in Figure 1, if a user's behavior can be collected and modeled from birth to the present, their interests can be accurately predicted. Several mainstream studies have designed memory mechanisms to model lifelong behavior sequences [55, 63]. Other approaches em-ploy retrieval methods to extract relevant information from entire lifelong sequences [6, 56, 59]. These methods prove the effective-ness of user behavior sequences. However, they fail to leverage general knowledge of LLMs to help users' predictions.\nRegarding the second point, there are already existing studies on how to incorporate personalized knowledge into large language models (LLMs). Some works inject personalized information from conventional recommendation models (CRMs) into LLMs by cus-tomizing input templates at the input layer [1, 38, 41, 80]. Other studies align representations between semantic and recommen-dation spaces at the prediction layer [37, 68]. Despite achieving promising results compared to conventional recommendation mod-els like SASRec [29] and DIN [84], existing works still face three significant limitations.\nFirstly, personalization is not well established when adapting LLMs to recommender systems. For the sake of memory efficiency, mainstream approaches typically adopt low-rank adaptation (LoRA) techniques [14, 26] and perform parameter-efficient finetuning (PEFT) to inject personalized knowledge from recommendation domains into LLMs [44, 80, 83]. Ideally, each user should have a unique LoRA matrix to achieve personalized recommendations. However, current methods only implement prompt personalization or representation personalization, while the LoRA module, a core component of fine-tuning LLMs, remains static for all users and tar-get items. This lack of parameter personalization results in inferior capabilities to effectively manage user dynamics and preference shifts in modern recommender systems.\nSecondly, utilizing lifelong personalized behavior sequences in the prompt or representation layer raises issues of effectiveness and efficiency. Since LLMs are sensitive to input length and have a limit on input size, they tend to perform poorly on long behavior sequences, resulting in a ceiling on effectiveness.[44] Additionally, as the length of the input behavior sequence and the volume of training data increase, the time required for training and inference escalates rapidly, leading to efficiency problems.\nThirdly, existing approaches are not scalable for large-scale in-dustrial datasets due to training inefficiency. Even with LoRA tech-niques, fine-tuning LLMs on recommendation data consumes im-mense computational resources and time, making it extremely inef-ficient for large-scale datasets, typically involving millions or even billions of records. Although some studies [44] suggest fine-tuning LLMs on a small fraction (e.g., less than 10%) of the training data to balance recommendation performance and training efficiency, this approach limits the LLMs' exposure to the full training space, resulting in suboptimal performance. Therefore, enabling LLMs to fully perceive the training space while ensuring training efficiency remains an open research question.\nTo address the aforementioned issues, we propose a novel Person-alized Low-rank Adaptation Framework of Large Language Models for Recommendation (i.e., RecLoRA). Specifically, we maintain a set of parallel, independent LoRA weights instead of a single static LoRA module as suggested in previous works [44, 80]. As shown in Figure 1, this personalized LoRA module functions to incorporate personalized user knowledge into large language models, serving as a strategy for parameter personalization. We assign a sequen-tial CRM and adapt the sequence representation to personalized meta-LoRA weights. More precisely, we use a soft routing method to aggregate meta-LoRA weights guided by the CRM. Instead of extending user behavior sequences in the text input, RecLoRA as-signs longer sequences in the sequential CRM, which significantly improves effectiveness while adding minimal time cost. Moreover, the CRM is pre-trained on the full training space, so transforming the CRM to meta-LoRA weights acts like a lens, expanding the small fraction of training data exposed to the LLM to encompass the entire training space without increasing LLM training time.\nThe main contributions are summarized as follows:\n\u2022 To the best of our knowledge, we are the first to consider LoRA parameters personalization in the recommendation task. We design a personalized LoRA module that includes a CRM on full training space and a soft routing method to aggregate meta-LoRA weights with CRM instructions.\n\u2022 We propose a novel framework, RecLoRA, which realizes a bet-ter trade-off between effectiveness and efficiency. RecLoRA has a Few2Many Magnifier where only few-shot training data is needed to efficiently tune LLM and a Long-short Modality Re-triever module, which extends the sequence length on the ID input side to improve performance while almost not increasing the time cost.\n\u2022 Our personalized LoRA module is an easy-to-plugin module, which can be used in any LLM4rec model with LoRA modules.\nWe have conducted offline experiments on three publicly avail-able datasets and achieved promising results, demonstrating the effectiveness of our proposed model."}, {"title": "2 PRELIMINARY", "content": "2.1 Problem Formulation\nThe core task of recommender systems is to estimate users' pref-erence toward target items given a certain context. We denote the historical dataset as $D = \\{(x_i, y_i)\\}_{i=1}^{N}$, where N is the number of data instances. The input $x_i$ contains features of user profile $u_i$(e.g., age, location, and historical behaviors), item attributes $v_i$ (e.g., cate-gory and brand), context information $c_i$ (e.g., time and season) and user history sequence $H_{u_i}$ (e.g., behaviors like click and ratings).\n$x_i = (u_i, v_i, c_i, H_{u_i})$\nwhere $H_{u_i} = \\{h_1, h_2, \\dots, h_{N_{u_i}}\\rangle$ represents $N_{u_i}$ sequential behaviors of user $u_i$.\nThe label $y_i \\in \\{1,0\\}$ indicating the interaction signal of the user towards the item (i.e., prefer or not).\n$y_i = \\begin{cases}1, u_i \\text{ clicks or likes } v_i; \\\\0, \\text{ otherwise}.\\end{cases}$\nAccording to different input data transformations, there gen-erally exist two different recommendation paradigms: (1) The in-put of ID modality $x_D$ obtained by one-hot encoding for conven-tional recommendation models (CRMs), and (2) The input of textual modality $x_{\\text{text}}$ generated by hard prompt template for LLMs as recommenders.\n2.2 Conventional Recommendation Models\nFor ID input $x_D$, various conventional recommendation models (CRMs) are designed to capture the collaborative patterns for pre-cise user preference estimation from various aspects (e.g., feature interactions [19, 71], user behavior modeling [56, 59]). We give the general formulation of CRMs as follows:\n$h_D = CRM(x_D)$,\n$\\hat{y}_D = \\sigma(MLP(h_D)) \\in (0, 1)$,\nwhere $\\sigma(\\cdot)$ is the sigmoid function.\n2.3 Large Language Models as Recommenders\nTypically, large language models (LLMs) refer to Transformer-based language models with at least billion-level parameters that are trained on massive text datasets, demonstrating remarkable capac-ity in various natural language tasks. When adapting directly LLMs as the recommenders, we need to convert the raw data $(x_i, y_i)$ into textual input-output pairs $(x_{\\text{text}}, y_{\\text{text}})$ with hard prompt template. We illustrate one template example in Figure 2."}, {"title": "2.4 Low-Rank Adaptation of LLMs", "content": "Low-rank adaptation (LoRA) [26] serves as a popular parameter-efficient finetuning (PEFT) method to reduce the resource consump-tion of finetuning LLMs that possess massive parameters. The basic idea of LoRA is to maintain two trainable lightweight matrices A and B that are attached to a frozen pretrained weight matrix W. Hence, the linear transformation $Y = XW$ is reformulated as:\n$Y' = XW + XAB^T = X(W + AB^T)$,\nwhere $X \\in \\mathbb{R}^{n \\times d_{in}}$, $W \\in \\mathbb{R}^{d_{in} \\times d_{out}}$, $A \\in \\mathbb{R}^{d_{in} \\times r}$, $B \\in \\mathbb{R}^{d_{out} \\times r}$, and $r < \\min\\{d_{in}, d_{out}\\}$. Initially,\n$A \\sim \\mathcal{N}(0, \\sigma^2), B = 0,$\nso as to ensure the initial output $Y' = XW+XAB^T$ equal to $Y = XW$.\nDuring finetuning, W is fixed while A and B are updated by SGD-based optimization methods like Adam [33].\nWhen applying LoRA to LLM finetuning, we can selectively attach LoRA weights to target matrices of certain types inside the LLM. e.g., the matrices of query, key and value in the self-attention modules or the linear matrices in feed-forward networks (FFNs)."}, {"title": "3 METHODOLOGY", "content": "In this section, we introduce our proposed RecLoRA (Personalized Low-rank Adaptation for Recommendation) framework and its training process in details.\n3.1 Overview of RecLORA\nWe propose the RecLoRA framework to address three major limita-tions: (1) To construct personalized low-rank adaptation for LLMs in recommendation, we employ a parallel meta-LoRA mechanism incorporating personalized knowledge. (2) To alleviate the effec-tiveness and efficiency issues associated with behavior sequence extension, we design the Long-Short Modality Retriever. (3) To expand the receptive field of LLMs to the entire training space, we develop the Few2Many Magnifier Strategy. In this section, we provide a comprehensive explanation of our RecLoRA framework, including its network architecture and training process.\nFigure 3 illustrates the overall framework of the RecLoRA model. As is shown in the figure, RecLoRA takes two input components: samples in ID modality $x_{ID}$ and samples in text modality $x_{\\text{text}}$. Each modality x has user profile, candidate item, context features and user history behaviors as formulated in Eq. (1). So we start by transforming the recommendation dataset into these two modalities and then feed $x_{ID}$ and $x_{\\text{text}}$ into CRM and LLM models respectively.\nDuring transformation, a Long-Short Modality Retriever retrieves long history for $x^{ID}$ and short history for $x^{\\text{text}}$.\n$x^{ID}_{u_i} = LSMR(u^{ID}_i, v^{ID}_i)$\n$x^{\\text{text}}_{u_i} = LSMR(u^{\\text{text}}_i, v^{\\text{text}}_i)$\n$\\N^{ID}_{u_i} > N^{\\text{text}}_{u_i}$\nwhere N is the sequence length and they would be described in detail in section 3.4.\nOnce fed with $x^{ID}$ and $x^{\\text{text}}$, RecLoRA begins its process. Our main predictor for labeling y is the LLM\n$\\hat{y} = LLM(x^{\\text{text}})$\nThus, we need to fine-tune the pre-trained LLM with the rec-ommendation dataset. To achieve memory efficiency, we choose to use LoRA for parameter-efficient tuning of the LLM, as introduced in Section 2.4. However, instead of using traditional LoRA, we de-sign a Personalized LoRA Module to incorporate user-personalized knowledge. This module can be expressed as follows:\n$h_{j+1} = Layer_j(h_j) + LoRA(h_j)$\n$h_{j+1} = Layer_j(h_j) + PLoRA(h_j, x^{ID})$\nwhere $Layer_j$ is the jth layer of LLM, and $PLORA$ is our module, which takes the hidden states from the jth layer along with $x^{ID}$ as inputs and outputs the personalized representation. The details of $PLORA$ will be provided in Section 3.2.\n3.2 Personalized Low-Rank Adaption\nIntuitively, the most straightforward way for personalized low-rank adaptation is to maintain independent LoRA weights $(A_u, B_u)$ for each user u separately, i.e.,\n$Y = XW + XA_uB^T_u, \\forall u \\in U.$\nHowever, this approach is non-scalable, as its model complexity grows linearly with the number of users, leading to a tremendous amount of memory consumption. Moreover, it overlooks the over-lap and commonality among behavior patterns of different users, which can lead to suboptimal performance.\nHence, to balance user-specific individual preference and inter-user pattern commonalities, we propose a Personalized LORA Mod-ule. Specifically, as shown in Figure 3, we maintain a set of meta-LoRA weights $\\{(A_k, B_k)\\}_{k=1}^{N_m}$, which are designed to capture the diverse user behavior patterns. Each meta-LoRA weight $(A_k, B_k)$ can be regarded as a latent subspace to boost the expressiveness and capacity.\nThen, we can obtain the personalized LoRA weights by combin-ing them with a trainable gating network.\n$A_{\\rho_{B_p}} = \\sum_{k=1}^{N_m} a_k A_kB$\nwhere $A_{\\rho_{B_p}}$ are the final personalized LoRA matrix, and $N_m$ is the number of meta-LoRA weights.\nIn this equation, the kth gating weight $a_k$ represents the impor-tance and relevance of the current user and the kth LoRA weight, acting as an indicator for personalization. Thus, To capture the complicated correlations and achieve personalized aggregation of the meta-LoRA weights, we introduce the conventional recommen-dation model (CRM) to provide personalized representations for the gating network. CRM takes discrete ID $x^{ID}$ as input and generates hidden states as personalized representations.\n$R_c = CRM(x^{ID})$\nwhere $R_c \\in \\mathbb{R}^{n \\times d_c}$ represents the CRM's output. Specifically, we use a sequential behavior model, SIM[57], as user history provides the most ideal personalization, and SIM is a state-of-the-art model for sequential behavior in recommendations."}, {"title": "3.3 Few2Many Magnifier Strategy", "content": "During LLM fine-tuning, the ideal scenario would be exposing the entire recommendation dataset to the LLM, as studies have shown that the more data an LLM is exposed to, the better its performance. However, constraints on time and efficiency make it impractical to expose large datasets on the input side. Therefore, a more efficient approach is to inject large data knowledge without significantly increasing time costs. To achieve this, we propose the Few2Many Magnifier Strategy.\nIn the first stage, we train a conventional recommendation model using the full training dataset to obtain a well-trained model capable of providing sample-level personalized vector representations with high generalization based on the input ID modal data $x^{ID}$. This is achieved through a binary cross-entropy loss\n$L^{ID} = \\sum_{x_D \\in D} [-y_D \\cdot log(\\sigma(\\hat{y}_D)) - (1 - y^D) \\cdot log(1 - \\sigma(\\hat{y}_D))]$.\nwhere $\\sigma$ is sigmoid function, $y^D$ is the true label and $\\hat{y}_D$ is the prediction label calculated in Eq. (3) Now CRM has fitted the full training space and learned most of the knowledge contained in the whole dataset.\nIn the second stage, we downsample the large-scale training set to obtain a smaller training set for the efficient parameter tun-ing of RecLoRA, which includes large language models. During this process, the fully trained traditional recommendation model from the first stage provides personalized information, ensuring full spatial perspectives. This information is combined to form a per-sonalized LoRA matrix with sufficient generalization, as detailed in Section 3.2. Thus, even though the large language model sees only a small number of training samples during fine-tuning, it extends its receptive field to the full training space through the conven-tional recommendation models and personalized LoRA matrices. This greatly enhances the sample efficiency and recommendation performance of the large language model. It should be noted that during this process, only the personalized LoRA module is updated, while the traditional recommendation models and large language models remain fixed. The training loss used is the cross-entropy loss in traditional causal language modeling.\nDuring the inference stage, large language model doesn't directly give a pointwise score $\\hat{y}_{\\text{text}} \\in \\{0, 1\\}$. Therefore, we intercept the vocabulary scores, and then conduct a bidimensional softmax over the scores of binary key answer words. Specifically, the scores for \"Yes\" and \"No\u201d are $s_y$ and $s_n$ respectively. Then the pointwise scoring of LLMs can be written as:\n$\\hat{u}_{\\text{text}} = \\frac{\\exp(s_y)}{\\exp(s_y) + \\exp(s_n)}$\nThis prediction will be used to calculate evaluation metrics."}, {"title": "3.4 Long-Short Modality Retriever", "content": "Intuitively, ideal personalized data involves a lifelong behavior se-quence. However, the time cost escalates significantly with extended behavior sequences. To address efficiency issues while improving performance, we propose a Long-Short Modality Retriever.\nIt has been widely demonstrated that retrieval is effective in extracting important information from long behavior sequences for current sample prediction. [57, 59] However, the history length remains a trade-off between effectiveness and efficiency. Therefore, we retrieve long histories for input $x^{ID}$ in the CRM and short histories for input $x^{\\text{text}}$ in the LLM. The CRM, trained on longer histories, can provide more sequential information and personalized knowledge for the LLM through LoRA parameters. This approach hardly increases the time cost, achieving a fantastic balance between effectiveness and efficiency, as will be shown in Section 4.5.\nThe retrieval method can be diverse and flexible. For example, we retrieve behaviors using semantic behavior encoding. Specifically, we input each behavior into the LLM and obtain the hidden states from the last layer of the LLM as its representation. We then apply principal component analysis (PCA) for dimension reduction and denoising. Finally, we calculate cosine similarities to identify the top-k most relevant behaviors to the current item."}, {"title": "4 EXPERIMENT", "content": "In this section, we conduct the experimental settings and results. Five research questions lead the following discussions, and our implementation code of RecLoRA is publicly available.\nRQ1 Does RecLoRA outperform existing baselines?\nRQ2 What are the influences of different components in RecLoRA?\nRQ3 What is the influence of meta-LoRA number $N_m$ in Eq. (11)?\nRQ4 How do the performance and time cost increase when ex-tending a longer behavior sequence?\nRQ5 How does RecLoRA improve the sample efficiency for LLM?"}, {"title": "4.3 Ablation Study (RQ2)", "content": "In this section, we conduct ablation experiments to analyze the effectiveness of the main components in RecLoRA: the Personalized LoRA module and Long-Short Modality Retriever.\n\u2022 RecLORA (Ours): This is the complete version of our proposed method.\n\u2022 RecLORA (w/o meta-LoRA): We remove the Personalized LoRA module and only maintain a LoRA module with a rank of 8.\n\u2022 RecLORA (w/o meta-LoRA same param): Since the person-alized meta-LoRA module has $N_m = 16$ times more training parameters, we maintain the parameter count by replacing the meta-LoRA weights with a single LoRA weight, increasing the LORA rank from 8 to 128.\n\u2022 RecLORA (w/o long retriever): To evaluate the effect of re-trieval history in $x^{ID}$, we remove the long retriever module and only use the recent history.\n\u2022 RecLORA (w/o short retriever): To evaluate the effect of re-trieval history in $x^{\\text{text}}$, we remove the short retriever module and only use the recent history.\n\u2022 RecLoRA (w/o long-short retriever): To evaluate the overall effect of retrieval history, we remove both the long and short retrievers, using only the recent history.\nThe results are shown in Table 3. We observe that the perfor-mance of RecLoRA significantly decreases when the personalized LORA module is removed. This finding confirms that the lack of user personalization knowledge in the LLM tuning process leads to suboptimal performance. Our proposed personalized LoRA module effectively addresses this issue and improves performance. Addi-tionally, we observe from the third line that the improvement is not merely due to an increase in the parameter count by $N_m$ times.\nMeanwhile, we observe that removing either the long or short retriever generally results in a performance drop. This highlights the importance of considering lifelong user sequences and demon-strates that input from different modalities both benefit from the retrieval of long sequences."}, {"title": "4.4 Hyperparameter Study (RQ3)", "content": "In this section, we conduct hyperparameter experiments to analyze the influence of the meta-LoRA number $N_m$ in personalized LoRA module. The results are shown in Figure 4.\nAs shown in Figure 4, the evaluation metric AUC generally in-creases with the hyperparameter $N_m$, reaching a maximum value at $N_m = 16$. This experiment demonstrates that increasing the parameter count improves overall performance, as the hyperpa-rameter $N_m$ determines the number of meta-LoRA weights and the associated trainable parameters.\nFurthermore, when $N_m = 1$, the personalized LoRA module collapses into a single LoRA module without CRM instruction, resulting in the worst performance. Notably, even starting from $N_m = 2$, RecLoRA performs very well, even comparable to $N_m = 16$ in GoodReads. This illustrates the effectiveness of personalized LoRA without significantly increasing the parameter count."}, {"title": "4.5 Time Efficiency (RQ4)", "content": "In this section, we conduct time efficiency experiments to analyze the balance of performance and time cost when increasing sequence lengths in the RecLoRA model. The results of GoodReads are shown in Figure 5 and Table 4. And The results of the other two datasets are similar and omitted in Table 4 due to page limits.\nFigure 5 displays the performance of RecLoRA as ID sequence lengths increase. We observe that the performance improves signif-icantly with longer ID sequences. Since SIM provides instructions for meta-LoRA weights aggregation in the personalized LoRA mod-ule, better performance of SIM theoretically enhances RecLoRA's performance, which is consistent with our experimental results.\nTable 4 shows the performance and time cost of RecLoRA with different historical lengths in text and ID inputs. It reveals that while increasing text sequence lengths improves performance by +0.73%, it also leads to a substantial and unacceptable increase in training and inference time costs of +358.16% and +205.55%, respectively. In contrast, not only the performance is improved significantly by +2.73% when extending ID history lengths, but also the corresponding training and inference time only increase by +4.04% and +8.33% respectively, which are negligible compared to the time increases associated with longer text sequences. These results highlight RecLoRA's advantage in balancing effectiveness and efficiency."}, {"title": "4.6 Sample Efficiency (RQ5)", "content": "In this few-shot setting, we want to explore how much data is necessary for LLMs to adapt to CTR distributions. So we investigate the sample efficiency by varying the number of samples used in training LLM, following previous works [44]. In Figure 6, we report the AUC performance of ReLLa (a few-shot baseline) and RecLORA with different sample sizes (ranging from 2,048 to 70,000). The text and ID behavior sequence lengths are set to 10 and 60, respectively.\nAs depicted in Figure 6, both ReLLa and RecLoRA show perfor-mance enhancement as the number of samples gradually increases. However, with the same sample number, RecLoRA consistently outperforms ReLLa by a significant margin. Moreover, even with a small sample size (10,000), RecLoRA performs better than ReLLa with a relatively large sample size (70,000). This is due to RecLoRA's Few2Many Strategy, which utilizes a fully pre-trained CRM for in-struction, helping LLMs adapt to CTR distributions better with fewer samples.\nWith a limited number of training samples, RecLoRA demon-strates remarkable sample efficiency and considerable few-shot inference ability, benefiting from the mutual assistance of the open-world knowledge of LLMs and the collaborative signals of CRMs."}, {"title": "5 RELATED WORKS", "content": "5.1 Large Language Model for Recommendation\nPrevious work [42] has suggested that the employment of language models to recommender systems can generally be categorized based on the roles they play in the recommendation pipeline. i.e., feature engineering [2, 5, 11, 34, 48, 53], feature encoder [16, 21, 23, 24, 36, 52, 52, 62, 69, 74, 75, 78], scoring/ranking function [1, 9, 18, 25, 27, 28, 31, 35, 39, 46, 50, 54, 70, 76, 79, 81].\nIn feature engineering, large language models (LLMs) process raw data (e.g., user profiles and item descriptions) as input, and output open-world knowledge or potential new attributes for data augmentation with carefully designed prompts or templates. For example, KAR [73] utilizes the potential knowledge of user pref-erences and item attributes by requesting LLMs with factoriza-tion prompting techniques. GENRE [48] employs LLMs to gener-ate news summarization, synthetic pieces, and user profiles. The obtained knowledge serves as augmented features and improves performances of recommenders in a model-agnostic manner.\nIn feature encoders, LLMs are used as auxiliary textual encoders to both enrich the user/item representations with semantic informa-tion and enable cross-domain recommendation with the open-world natural language interface. For instance, U-BERT [62] enhances user representation by encoding review texts into dense vectors via BERT. UniSRec [24] and VQ-Rec [23] apply a fixed BERT as the encoder for item descriptive texts, to achieve unified cross-domain sequential recommendation.\nIn scoring/ranking function, instead of doing assistant tasks for recommendation (e.g., feature engineering or feature encoder), LLMs are adopted to do the scoring or ranking task, which is the core component of recommendation. In this case, LLMs try to ac-complish either the item scoring task [1, 31, 35, 39, 44, 46, 50, 79, 81], or item generation task [9, 18, 25, 27, 28, 54, 70, 76]. Also, various works [12, 13, 17, 47, 65, 77] attempt to explore the multi-task ca-pacity of LLMs, and instruct LLMs with various ways to solve the multiple tasks (e.g., both scoring and generation) through a unified language interface.\n5.2 Long Sequence User Modeling\nDuring recommendation task, Kim [32] argues that long-term se-quence mean general interest, which is back to one's mind and important for personalization. Existing approaches for addressing long-term user sequence mainly focus on memory network and re-trieval methods. Hierarchical Periodic Memory Network(HPMN) [63] proposes a hierarchical and periodical updating mechanism for capturing multi-scale sequential user interests. The Memory Aug-mented Neural Network (MIMN) [55] stores behaviors in a memory matrix at the user interest center and updates the memory for new users. Sequential Interest Modeling (SIM) [56] and User Behavior Re-trieval for CTR (UBR4CTR) [59] have introduced retrieval-enhanced history and two-stage frameworks to catch user patterns in the past"}, {"title": "6 CONCLUSION", "content": "In this paper, we explore the application of large language models in recommendation systems. We begin by addressing the current challenges related to personalization and long sequences. To tackle these issues, we propose RecLoRA, a model designed for lifelong personalized low-rank adaptation. RecLoRA includes a Personal-ized LoRA module, which maintains distinct LoRAs for individual users. Additionally, a Long-Short Modality Retriever extracts vary-ing history lengths for different modalities, enhancing performance with minimal time cost. We also employ a conventional recommen-dation model to extend the scope of LLMs to the full training space. RecLoRA shows promising performance in offline experiments on three public datasets. Further, ablation, hyperparameter, and effi-ciency studies demonstrate its strong balance between effectiveness and efficiency. For future work, we plan to explore more advanced structures for personalization in LLMs and address the fairness issue, which is crucial for achieving complete personalization."}, {"title": "A DATA PREPROCESSING", "content": "Our experiments are conducted on three real-world public datasets (i.e., MovieLens-25M, MovieLens-1M, GoodReads), and the statistics of the processed datasets are show in Table 1. All these datasets are split into training, validing and testing sets with ratio of 8:1:1 according to the global timestamp [58].\n\u2022 MovieLens-25M has a scoring range from 0 to 5, with incre-ments of 0.5. We label samples with ratings above 3.0 as positive, and the rest as negative.\n\u2022 MovieLens-1M contains user-movie integer ratings ranging from 0 to 5. Samples with ratings of 4 and 5 are labeled as positive and the rest as negative.\n\u2022 GoodReads is a book recommendation dataset from GoodReads website with ratings ranging from 1 to 5. We transform the ratings into binary labels with a threshold of 4. We apply 5-core filtering to ensure each user or item has at least five interaction records.\nUnder the few-shot setting with a particular number of training data, we uniformly sample N data instances from the training set, which is then fixed during few-shot tuning."}, {"title": "B BASELINE IMPLEMENTATION", "content": "In this section", "categories": 1, "20": "AutoInt [64", "72": "as exemplars of feature interaction models, and GRU4Rec [22", "66": "SAS-Rec [30", "84": "and SIM [57", "51": "TALLRec [1", "44": "as baseline models to represent this category, and they are representatives of traditional language mod-els and large language models respectively.\nB.1 Traditional CTR Models\nWe choose the embedding size from {32, 64} on three datasets. The dropout rate is selected from {0.0, 0.1, 0.2}. The activation function is fixed to ReLU. The learning rate is selected from $1\\times10^{-3},5\\times10^{-4}, 1\\times 10^{-4}$ and AdamW [49"}, {}]}