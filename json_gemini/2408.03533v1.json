{"title": "Lifelong Personalized Low-Rank Adaptation of Large Language Models for Recommendation", "authors": ["Jiachen Zhu", "Jianghao Lin", "Xinyi Dai", "Bo Chen", "Rong Shan", "Jieming Zhu", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "abstract": "We primarily focus on the field of large language models (LLMs) for recommendation, which has been actively explored recently and poses a significant challenge in effectively enhancing recommender systems with logical reasoning abilities and open-world knowledge. Current mainstream efforts mainly center around injecting personalized information from recommendation models into LLMs by customizing input templates or aligning representations between semantic and recommendation spaces at the prediction layer. However, they face three significant limitations: (1) LoRA is mostly used as a core component in existing works, but personalization is not well established in LoRA parameters as the LoRA matrix shared by every user may not cater to different users' characteristics, leading to suboptimal performance. (2) Although lifelong personalized behavior sequences are ideal for personalization, their use raises effectiveness and efficiency issues since LLMs require escalating training and inference time to extend text lengths. (3) Existing approaches aren't scalable for large datasets due to training efficiency constraints. Thus, LLMs only see a small fraction of the datasets (e.g., less than 10%) instead of the whole datasets, limiting their exposure to the full training space. To address these problems, we propose RecLoRA. This model incorporates a Personalized LoRA module that maintains independent LoRAs for different users and a Long-Short Modality Retriever that retrieves different history lengths for different modalities, significantly improving performance while adding minimal time cost. Furthermore, we design a Few2Many Learning Strategy, using a conventional recommendation model as a lens to magnify small training spaces to full spaces. Extensive experiments on public datasets demonstrate the efficacy of our RecLoRA compared to existing baseline models.", "sections": [{"title": "1 INTRODUCTION", "content": "Recommender systems play essential roles in various online ser- vices to mitigate the information overload problem and meet users' information needs [19, 43, 45, 60, 61]. Besides, large language mod- els (LLMs) have experienced significant growth in the field of natu- ral language processing (NLP), demonstrating a remarkable ability to comprehend human language and generate text that closely resembles human writings for a broad range of tasks [3, 67, 82]. Recent studies have started to investigate the potential of LLMs for recommender systems under a variety of recommendation tasks, such as listwise ranking and pointwise scoring [42, 44]. These stud- ies usually inject personalized knowledge from recommendation domains into LLMs. [44, 80, 83]. This personalized knowledge plays a pivotal role in tailoring experiences to individual preferences and enhancing user interactions and engagements [7]. So our crucial points are what useful personalized knowledge we can provide and how to inject it into LLM to help with predictions."}, {"title": "2 PRELIMINARY", "content": null}, {"title": "2.1 Problem Formulation", "content": "The core task of recommender systems is to estimate users' pref- erence toward target items given a certain context. We denote the historical dataset as $D = \\{(x_i, y_i)\\}_{i=1}^{N}$, where N is the number of data instances. The input $x_i$ contains features of user profile $u_i$(e.g., age, location, and historical behaviors), item attributes $v_i$ (e.g., category and brand), context information $c_i$ (e.g., time and season) and user history sequence $H_{u_i}$ (e.g., behaviors like click and ratings).\n$x_i = (u_i, v_i, c_i, H_{u_i})$\nwhere $H_{u_i} = \\{h_1, h_2, \u2026 \u2026 \u2026, h_{N_{u_i}} \\}$ represents $N_{u_i}$ sequential behaviors of user $u_i$.\nThe label $y_i \u2208 \\{1, 0\\}$ indicating the interaction signal of the user towards the item (i.e., prefer or not).\n$y_i =\n\\begin{cases}\n1, u_i \\text{ clicks or likes } v_i;\\\\\n0, \\text{ otherwise}.\n\\end{cases}$\nAccording to different input data transformations, there gen- erally exist two different recommendation paradigms: (1) The in- put of ID modality $x_D$ obtained by one-hot encoding for conven- tional recommendation models (CRMs), and (2) The input of textual modality $x_{\\text{text}}$ generated by hard prompt template for LLMs as recommenders."}, {"title": "2.2 Conventional Recommendation Models", "content": "For ID input $x_D$, various conventional recommendation models (CRMs) are designed to capture the collaborative patterns for pre- cise user preference estimation from various aspects (e.g., feature interactions [19, 71], user behavior modeling [56, 59]). We give the general formulation of CRMs as follows:\n$h_D = CRM(x_D)$,\n$\\hat{y}_D = \\sigma(MLP(h_D)) \u2208 (0, 1)$,\nwhere $\u03c3(\u00b7)$ is the sigmoid function."}, {"title": "2.3 Large Language Models as Recommenders", "content": "Typically, large language models (LLMs) refer to Transformer-based language models with at least billion-level parameters that are trained on massive text datasets, demonstrating remarkable capac- ity in various natural language tasks. When adapting directly LLMs as the recommenders, we need to convert the raw data $(x_i, y_i)$ into textual input-output pairs $(x_{\\text{text}}, y_{\\text{text}})$ with hard prompt template. We illustrate one template example in Figure 2."}, {"title": "2.4 Low-Rank Adaptation of LLMs", "content": "Low-rank adaptation (LoRA) [26] serves as a popular parameter- efficient finetuning (PEFT) method to reduce the resource consump- tion of finetuning LLMs that possess massive parameters. The basic idea of LoRA is to maintain two trainable lightweight matrices A and B that are attached to a frozen pretrained weight matrix W. Hence, the linear transformation $Y = XW$ is reformulated as:\n$Y' = XW + XAB^T = X(W + AB^T)$,\nwhere $X \u2208 R^{n \u00d7 d_{\\text{in}}}$, $W\u2208 R^{d_{\\text{in}} \u00d7 d_{\\text{out}}}$, $A \u2208 R^{d_{\\text{in}}\u00d7r}$, $B \u2208 R^{d_{\\text{out}}\u00d7r}$, and $r < \\text{min}\\{d_{\\text{in}}, d_{\\text{out}}\\}$. Initially,\n$A \u223c N(0, \u03c3^2), B = 0,$\nso as to ensure the initial output $Y' = XW + XAB^T$ equal to $Y = XW$.\nDuring finetuning, W is fixed while A and B are updated by SGD- based optimization methods like Adam [33].\nWhen applying LoRA to LLM finetuning, we can selectively attach LoRA weights to target matrices of certain types inside the LLM. e.g., the matrices of query, key and value in the self-attention modules or the linear matrices in feed-forward networks (FFNs)."}, {"title": "3 METHODOLOGY", "content": "In this section, we introduce our proposed RecLoRA (Personalized Low-rank Adaptation for Recommendation) framework and its training process in details."}, {"title": "3.1 Overview of RecLORA", "content": "We propose the RecLoRA framework to address three major limita- tions: (1) To construct personalized low-rank adaptation for LLMs in recommendation, we employ a parallel meta-LoRA mechanism incorporating personalized knowledge. (2) To alleviate the effec- tiveness and efficiency issues associated with behavior sequence extension, we design the Long-Short Modality Retriever. (3) To expand the receptive field of LLMs to the entire training space, we develop the Few2Many Magnifier Strategy. In this section, we provide a comprehensive explanation of our RecLoRA framework, including its network architecture and training process.\nFigure 3 illustrates the overall framework of the RecLoRA model. As is shown in the figure, RecLoRA takes two input components: samples in ID modality $x_{ID}$ and samples in text modality $x_{\\text{text}}$. Each modality $x$ has user profile, candidate item, context features and user history behaviors as formulated in Eq. (1). So we start by transforming the recommendation dataset into these two modalities and then feed $x_{ID}$ and $x_{\\text{text}}$ into CRM and LLM models respectively."}, {"title": "3.2 Personalized Low-Rank Adaption", "content": "Intuitively, the most straightforward way for personalized low-rank adaptation is to maintain independent LoRA weights $(A_u, B_u)$ for each user u separately, i.e.,\n$Y'_u = XW + XA_uB_u^T, \u2200u \u2208 U.$\nHowever, this approach is non-scalable, as its model complexity grows linearly with the number of users, leading to a tremendous amount of memory consumption. Moreover, it overlooks the over- lap and commonality among behavior patterns of different users, which can lead to suboptimal performance.\nHence, to balance user-specific individual preference and inter- user pattern commonalities, we propose a Personalized LORA Mod- ule. Specifically, as shown in Figure 3, we maintain a set of meta- LoRA weights $\\{(A_k, B_k)\\}_{k=1}^{N_m}$, which are designed to capture the diverse user behavior patterns. Each meta-LoRA weight $(A_k, B_k)$ can be regarded as a latent subspace to boost the expressiveness and capacity.\nThen, we can obtain the personalized LoRA weights by combin- ing them with a trainable gating network.\n$A_pB_p = \\sum_{k=1}^{N_m} a_k A_kB^T$\nwhere $A_pB_p$ are the final personalized LoRA matrix, and $N_m$ is the number of meta-LoRA weights.\nIn this equation, the kth gating weight $a_k$ represents the impor- tance and relevance of the current user and the kth LoRA weight, acting as an indicator for personalization. Thus, To capture the complicated correlations and achieve personalized aggregation of the meta-LoRA weights, we introduce the conventional recommen- dation model (CRM) to provide personalized representations for the gating network. CRM takes discrete ID $x_{ID}$ as input and generates hidden states as personalized representations.\n$R_c = CRM(x_{ID})$\nwhere $R_c \u2208 R^{n \u00d7 d_c}$ represents the CRM's output. Specifically, we use a sequential behavior model, SIM[57], as user history provides the most ideal personalization, and SIM is a state-of-the-art model for sequential behavior in recommendations."}, {"title": "3.3 Few2Many Magnifier Strategy", "content": "During LLM fine-tuning, the ideal scenario would be exposing the entire recommendation dataset to the LLM, as studies have shown that the more data an LLM is exposed to, the better its performance. However, constraints on time and efficiency make it impractical to expose large datasets on the input side. Therefore, a more efficient approach is to inject large data knowledge without significantly increasing time costs. To achieve this, we propose the Few2Many Magnifier Strategy.\nIn the first stage, we train a conventional recommendation model using the full training dataset to obtain a well-trained model capable of providing sample-level personalized vector representations with high generalization based on the input ID modal data $x_{ID}$. This is achieved through a binary cross-entropy loss\n$L_{ID} = \\sum_{x_D\u2208D} [-y_D.\\text{log}(\\sigma(\\hat{y}_D)) \u2212 (1 \u2212 y^I_D) \u00b7 \\text{log}(1 \u2212 \u03c3(\\hat{y}_D))]$.\nwhere $\u03c3$ is sigmoid function, $y^D$ is the true label and $\\hat{y}_D$ is the prediction label calculated in Eq. (3) Now CRM has fitted the full training space and learned most of the knowledge contained in the whole dataset.\nIn the second stage, we downsample the large-scale training set to obtain a smaller training set for the efficient parameter tun- ing of RecLoRA, which includes large language models. During this process, the fully trained traditional recommendation model from the first stage provides personalized information, ensuring full spatial perspectives. This information is combined to form a per- sonalized LoRA matrix with sufficient generalization, as detailed in Section 3.2. Thus, even though the large language model sees only a small number of training samples during fine-tuning, it extends its receptive field to the full training space through the conven- tional recommendation models and personalized LoRA matrices. This greatly enhances the sample efficiency and recommendation performance of the large language model. It should be noted that during this process, only the personalized LoRA module is updated, while the traditional recommendation models and large language models remain fixed. The training loss used is the cross-entropy loss in traditional causal language modeling.\nDuring the inference stage, large language model doesn't directly give a pointwise score $\\hat{y}_{text} \u2208 \\{0, 1\\}$. Therefore, we intercept the vocabulary scores, and then conduct a bidimensional softmax over the scores of binary key answer words. Specifically, the scores for \"Yes\" and \"No\u201d are $s_y$ and $s_n$ respectively. Then the pointwise scoring of LLMs can be written as:\n$\\hat{u}_{text} = \\frac{\\text{exp}(s_y)}{\\text{exp}(s_y) + \\text{exp}(s_n)}$\nThis prediction will be used to calculate evaluation metrics."}, {"title": "3.4 Long-Short Modality Retriever", "content": "Intuitively, ideal personalized data involves a lifelong behavior se- quence. However, the time cost escalates significantly with extended behavior sequences. To address efficiency issues while improving performance, we propose a Long-Short Modality Retriever.\nIt has been widely demonstrated that retrieval is effective in extracting important information from long behavior sequences for current sample prediction. [57, 59] However, the history length remains a trade-off between effectiveness and efficiency. Therefore, we retrieve long histories for input $x_{ID}$ in the CRM and short histories for input $x_{\\text{text}}$ in the LLM. The CRM, trained on longer histories, can provide more sequential information and personalized knowledge for the LLM through LoRA parameters. This approach hardly increases the time cost, achieving a fantastic balance between effectiveness and efficiency, as will be shown in Section 4.5.\nThe retrieval method can be diverse and flexible. For example, we retrieve behaviors using semantic behavior encoding. Specifically, we input each behavior into the LLM and obtain the hidden states from the last layer of the LLM as its representation. We then apply principal component analysis (PCA) for dimension reduction and denoising. Finally, we calculate cosine similarities to identify the top-k most relevant behaviors to the current item."}, {"title": "4 EXPERIMENT", "content": "In this section, we conduct the experimental settings and results. Five research questions lead the following discussions, and our implementation code of RecLoRA is publicly available.\nRQ1 Does RecLoRA outperform existing baselines?\nRQ2 What are the influences of different components in RecLoRA?\nRQ3 What is the influence of meta-LoRA number $N_m$ in Eq. (11)?\nRQ4 How do the performance and time cost increase when ex- tending a longer behavior sequence?\nRQ5 How does RecLoRA improve the sample efficiency for LLM?"}, {"title": "4.1 Experiment Setup", "content": "4.1.1 Datasets. We conduct experiments on three real-world datasets (i.e., GoodReads\u00b9, MovieLens-1M\u00b2 and MovieLens-25M\u00b3).\n4.1.2 Evaluation Metrics. To assess the effectiveness of our meth- ods, we use AUC (Area Under the ROC Curve), Log Loss (binary cross-entropy loss), and Rel.Impr (Relative Improvement) as evalu- ation metrics.\n4.1.3 Baseline Models. We have divided our baselines into two main categories:(1) traditional ID-based, containing DeepFM [20], AutoInt [64], and DCNv2 [72] as exemplars of feature interaction models and GRU4Rec [22], Caser [66], SASRec [30], DIN [84], and SIM [57] as key representatives of user behavior models (2) LM- based, containing CTR-BERT[51], TALLRec [1], and ReLLa [44]."}, {"title": "4.2 Overall Performance (RQ1)", "content": "We evaluate the performance of RecLoRA in comparison to existing baseline models, with the results reported in Table 2. It is important to note that most of the other recommendation baseline models are trained in full-shot settings with the entire training set, while only TallRec, ReLLa and RecLoRA are trained on few-shot training sets with 70000(<10%) training samples in all three datasets. We set the length of text and ID behavior sequences to 10 and 60 respectively in all three datasets.\nOur observations from Table 2 are as follows.\n\u2022 SIM achieves the best performance among all the ID-based base- line models.\n\u2022 ReLLa generally achieves performance comparable to the best ID-based baseline model, SIM. ReLLa incorporates retrieval- enhanced instruction tuning (ReiT) and utilizes only 10% of the training data.\n\u2022 RecLoRA outperforms the state-of-the-art baselines significantly with a p-value < 0.01 against the best baseline, validating the effectiveness of our proposed Personalized LoRA module."}, {"title": "4.3 Ablation Study (RQ2)", "content": "In this section, we conduct ablation experiments to analyze the effectiveness of the main components in RecLoRA: the Personalized LoRA module and Long-Short Modality Retriever.\n\u2022 RecLORA (Ours): This is the complete version of our proposed method.\n\u2022 RecLORA (w/o meta-LoRA): We remove the Personalized LoRA module and only maintain a LoRA module with a rank of 8.\n\u2022 RecLORA (w/o meta-LoRA same param): Since the person- alized meta-LoRA module has $N_m$ = 16 times more training parameters, we maintain the parameter count by replacing the meta-LoRA weights with a single LoRA weight, increasing the LORA rank from 8 to 128.\n\u2022 RecLORA (w/o long retriever): To evaluate the effect of re- trieval history in $x_{ID}$, we remove the long retriever module and only use the recent history.\n\u2022 RecLORA (w/o short retriever): To evaluate the effect of re- trieval history in $x_{\\text{text}}$, we remove the short retriever module and only use the recent history.\n\u2022 RecLoRA (w/o long-short retriever): To evaluate the overall effect of retrieval history, we remove both the long and short retrievers, using only the recent history.\nThe results are shown in Table 3. We observe that the perfor- mance of RecLoRA significantly decreases when the personalized LORA module is removed. This finding confirms that the lack of user personalization knowledge in the LLM tuning process leads to suboptimal performance. Our proposed personalized LoRA module effectively addresses this issue and improves performance. Additionally, we observe from the third line that the improvement is not merely due to an increase in the parameter count by Nm times.\nMeanwhile, we observe that removing either the long or short retriever generally results in a performance drop. This highlights the importance of considering lifelong user sequences and demon- strates that input from different modalities both benefit from the retrieval of long sequences."}, {"title": "4.4 Hyperparameter Study (RQ3)", "content": "In this section, we conduct hyperparameter experiments to analyze the influence of the meta-LoRA number $N_m$ in personalized LoRA module. The results are shown in Figure 4.\nAs shown in Figure 4, the evaluation metric AUC generally in- creases with the hyperparameter $N_m$, reaching a maximum value at $N_m$ = 16. This experiment demonstrates that increasing the parameter count improves overall performance, as the hyperpa- rameter $N_m$ determines the number of meta-LoRA weights and the associated trainable parameters.\nFurthermore, when $N_m$ = 1, the personalized LoRA module collapses into a single LoRA module without CRM instruction, resulting in the worst performance. Notably, even starting from $N_m$ = 2, RecLoRA performs very well, even comparable to $N_m$ = 16 in GoodReads. This illustrates the effectiveness of personalized LoRA without significantly increasing the parameter count."}, {"title": "4.5 Time Efficiency (RQ4)", "content": "In this section, we conduct time efficiency experiments to analyze the balance of performance and time cost when increasing sequence lengths in the RecLoRA model. Figure 5 displays the performance of RecLoRA as ID sequence lengths increase. We observe that the performance improves signif- icantly with longer ID sequences."}, {"title": "4.6 Sample Efficiency (RQ5)", "content": "In this few-shot setting, we want to explore how much data is necessary for LLMs to adapt to CTR distributions. So we investigate the sample efficiency by varying the number of samples used in training LLM, RecLoRA shows performance enhancement as the number of samples gradually increases. Moreover, even with a small sample size (10,000), RecLoRA performs better than ReLLa with a relatively large sample size (70,000). This is due to RecLoRA's Few2Many Strategy, which utilizes a fully pre-trained CRM for in- struction, helping LLMs adapt to CTR distributions better with fewer samples.\nWith a limited number of training samples, RecLoRA demon- strates remarkable sample efficiency and considerable few-shot inference ability, benefiting from the mutual assistance of the open- world knowledge of LLMs and the collaborative signals of CRMs."}, {"title": "5 RELATED WORKS", "content": null}, {"title": "5.1 Large Language Model for Recommendation", "content": "Previous work [42] has suggested that the employment of language models to recommender systems can generally be categorized based on the roles they play in the recommendation pipeline. i.e., feature engineering [2, 5, 11, 34, 48, 53], feature encoder [16, 21, 23, 24, 36, 52, 52, 62, 69, 74, 75, 78], scoring/ranking function [1, 9, 18, 25, 27, 28, 31, 35, 39, 46, 50, 54, 70, 76, 79, 81].\nIn feature engineering, large language models (LLMs) process raw data (e.g., user profiles and item descriptions) as input, and output open-world knowledge or potential new attributes for data augmentation with carefully designed prompts or templates. For example, KAR [73] utilizes the potential knowledge of user pref- erences and item attributes by requesting LLMs with factoriza- tion prompting techniques. GENRE [48] employs LLMs to gener- ate news summarization, synthetic pieces, and user profiles. The obtained knowledge serves as augmented features and improves performances of recommenders in a model-agnostic manner.\nIn feature encoders, LLMs are used as auxiliary textual encoders to both enrich the user/item representations with semantic informa- tion and enable cross-domain recommendation with the open-world natural language interface. For instance, U-BERT [62] enhances user representation by encoding review texts into dense vectors via BERT. UniSRec [24] and VQ-Rec [23] apply a fixed BERT as the encoder for item descriptive texts, to achieve unified cross-domain sequential recommendation.\nIn scoring/ranking function, instead of doing assistant tasks for recommendation (e.g., feature engineering or feature encoder), LLMs are adopted to do the scoring or ranking task, which is the core component of recommendation. In this case, LLMs try to ac- complish either the item scoring task [1, 31, 35, 39, 44, 46, 50, 79, 81], or item generation task [9, 18, 25, 27, 28, 54, 70, 76]. Also, various works [12, 13, 17, 47, 65, 77] attempt to explore the multi-task ca- pacity of LLMs, and instruct LLMs with various ways to solve the multiple tasks (e.g., both scoring and generation) through a unified language interface."}, {"title": "5.2 Long Sequence User Modeling", "content": "During recommendation task, Kim [32] argues that long-term se- quence mean general interest, which is back to one's mind and important for personalization. Existing approaches for addressing long-term user sequence mainly focus on memory network and re- trieval methods. Hierarchical Periodic Memory Network(HPMN) [63] proposes a hierarchical and periodical updating mechanism for capturing multi-scale sequential user interests. The Memory Aug- mented Neural Network (MIMN) [55] stores behaviors in a memory matrix at the user interest center and updates the memory for new users. Sequential Interest Modeling (SIM) [56] and User Behavior Re- trieval for CTR (UBR4CTR) [59] have introduced retrieval-enhanced history and two-stage frameworks to catch user patterns in the past which are related to current targets."}, {"title": "6 CONCLUSION", "content": "In this paper, we explore the application of large language models in recommendation systems. We begin by addressing the current challenges related to personalization and long sequences. To tackle these issues, we propose RecLoRA, a model designed for lifelong personalized low-rank adaptation. RecLoRA includes a Personal- ized LoRA module, which maintains distinct LoRAs for individual users. Additionally, a Long-Short Modality Retriever extracts vary- ing history lengths for different modalities, enhancing performance with minimal time cost. We also employ a conventional recommen- dation model to extend the scope of LLMs to the full training space.\nRecLoRA shows promising performance in offline experiments on three public datasets. Further, ablation, hyperparameter, and effi- ciency studies demonstrate its strong balance between effectiveness and efficiency. For future work, we plan to explore more advanced structures for personalization in LLMs and address the fairness issue, which is crucial for achieving complete personalization."}, {"title": "A DATA PREPROCESSING", "content": "Our experiments are conducted on three real-world public datasets (i.e., MovieLens-25M, MovieLens-1M, GoodReads), and the statistics of the processed datasets are show in Table 1. All these datasets are split into training, validing and testing sets with ratio of 8:1:1 according to the global timestamp [58].\n\u2022 MovieLens-25M has a scoring range from 0 to 5, with incre- ments of 0.5.\n\u2022 MovieLens-1M contains user-movie integer ratings ranging from 0 to 5.\n\u2022 GoodReads is a book recommendation dataset from GoodReads website with ratings ranging from 1 to 5. We transform the ratings into binary labels with a threshold of 4. We apply 5-core filtering to ensure each user or item has at least five interaction records.\nUnder the few-shot setting with a particular number of training data, we uniformly sample N data instances from the training set, which is then fixed during few-shot tuning."}, {"title": "B BASELINE IMPLEMENTATION", "content": "In this section, we describe the baseline models. Baseline models can broadly be divided into two main categories: (1) traditional CTR models, which primarily use one-hot encoded IDs as inputs, and (2) LM-based models, which integrate pre-trained language models to approach CTR prediction as either a text classification or a sequence-to-sequence problem.\nWithin traditional CTR models, we distinguish between (1) fea- ture interaction models and (2) user behavior models. We have selected DeepFM [20], AutoInt [64], and DCNv2 [72] as exemplars of feature interaction models, and GRU4Rec [22], Caser [66], SAS- Rec [30], DIN [84], and SIM [57] as key representatives of user behavior models. For the feature interaction models, we implement average pooling across users' historical behaviors and treat the results as additional feature fields.\nSIM is a classic sequential CTR model that employs user behavior retrieval techniques to enhance recommendation performance. We include it in our evaluation to ensure a comprehensive and fair comparison, maintaining consistency with the retrieval method used in RecLORA.\nFor LM-based CTR models, we have chosen CTR-BERT [51], TALLRec [1], and ReLLa [44] as baseline models to represent this category, and they are representatives of traditional language mod- els and large language models respectively."}, {"title": "B.1 Traditional CTR Models", "content": "We choose the embedding size from $\\{32, 64\\}$ on three datasets. The dropout rate is selected from $\\{0.0, 0.1, 0.2\\}$. The activation function is fixed to ReLU. The learning rate is selected from $1 \u00d7 10^{-3}, 5 \u00d7 10^{-4}, 1 \u00d7 10^{-4}$ and AdamW [49] optimizer is used."}, {"title": "B.2 LM-based Models", "content": "The structure of the pre-trained language models is kept unchanged. And AdamW [49] optimizer is used for all the baselines. The detailed training settings are as follows:\n\u2022 CTR-BERT [51]. We maintain a two-tower model structure based on the BERT [15] model to encode the user and item information respectively.\n\u2022 TallRec [1] is a supervised fine-tuning framework with Llama as a backbone pretrained language model for recommendation task.\n\u2022 ReLLa [44] uses user behavior retrieval and supervised fine- tuning with Llama as a backbone pretrained language model for recommendation task."}, {"title": "C IMPLEMENTATION DETALS", "content": "We selected Vicuna-7B [10], provided by FastChat\u2074, as the founda- tional large language model (LLM) for our experiments. All com- putations were performed using V100 GPUs. To enhance training resource efficiency, we employed 8-bit quantization."}]}