{"title": "Secure Multiparty Generative AI", "authors": ["Manil Shrestha", "Yashodha Ravichandran", "Edward Kim"], "abstract": "The field of generative artificial intelligence has recently had an unprecedented impact and adoption. These generative machine learning models can generate data similar to the data they were trained on with remarkable fidelity. Common generative tasks include creating realistic images from text prompts via stable diffusion and generating text via large language models. However, as usage of these tools skyrockets, the amount of sensitive information being exposed to these models and centralized model providers is alarming. For example, confidential source code from Samsung suffered a data leak as the text prompt to ChatGPT encountered data leakage. An increasing number of companies are restricting the use of LLMs (Apple, Verizon, JPMorgan Chase, etc.) due to data leakage or confidentiality issues. Also, an increasing number of centralized generative model providers are restricting, filtering, aligning, or censoring what can be used. Midjourney and RunwayML, two of the major image generation platforms, restrict the prompts to their system via prompt filtering. Certain political figures are restricted from image generation, as well as words associated with women's health care, rights, and abortion.\nIn our research, we present a secure and private methodology for generative artificial intelligence that does not expose sensitive data or models to third-party AI providers. Our work modifies the key building block of modern generative AI algorithms, e.g. the transformer, and introduces confidential and verifiable multiparty computations in a decentralized network to maintain the 1) privacy of the user input and obfuscation to the output of the model, and 2) introduce privacy to the model itself. Additionally, the sharding process reduces the computational burden on any one node, enabling the distribution of resources of large generative AI processes across multiple, smaller nodes. We show that as long there exists one honest node in the decentralized computation, security is maintained. We also show that the inference process will still succeed if only a majority of the nodes in the computation are successful. Thus, our method offers both secure and verifiable computation in a decentralized network.", "sections": [{"title": "Introduction", "content": "The explosive growth of generative artificial intelligence technologies has introduced unprecedented adoption of AI-related use cases. Generative models are being used to create images, music, text, code, virtual environments, speech, and more. However, the widespread use has also raised significant privacy concerns. The core of the problem is that the data being input into these systems for inference and training may contain confidential, sensitive, or explicit material that should be private. Further, training a generative model requires enormous amounts of data, often collected from users who may or may not be fully aware of how their information will be used.\nIn our research, we investigate a Secure Multi-Party Computation (SMPC) method to protect user privacy in the generative AI inference process, see Figure 1. SMPC is a subfield of cryptography that allows multiple parties to jointly compute a function over their inputs while keeping these inputs private. These mechanisms allow a group of people, machines, or organizations to collaborate on a calculation without revealing their individual data to each other. We believe there are data-critical situations where sharing data openly would compromise privacy or security."}, {"title": "Background", "content": "In the area of AI privacy and verification, there are many alternative approaches available, including homomorphic encryption, zk-proofs, and confidential computing. Each one of these fields has a large body of literature, and will be briefly summarized in this section; while these methods have strong guarantees, they are all currently intractable solutions for a deployed privacy-preserving AI today."}, {"title": "Related Work in Privacy Preserving AI", "content": "Homomorphic Encryption - Homomorphic encryption is a class of methods that allow one to perform operations on information, while it is still encrypted. There are different levels of homomorphic encryption including Partially Homomorphic Encryption (PHE), of single operations (like addition or multiplication), and Fully Homomorphic Encryption (FHE), enabling arbitrary computations in the encrypted space. While this is ideal, fully homomorphic encryption schemes suffer from high computational complexity, low efficiencies, and inadequacy of deployment in real-world scenarios, making them impractical for real-time applications.\nZK-Proofs - A zero-knowledge proof is a method by which one party (the prover) can prove to another party (the verifier) that they know a value x, without conveying any information apart from the fact that they know the value x (Goldwasser, Micali, and Rackoff 2019). Zero-knowledge proofs must satisfy three key properties of completeness (if the statement is true, an honest verifier will be convinced of this fact by an honest prover), soundness (if the statement is false, no cheating prover can convince an honest verifier that it is true, except with some small probability), and zero-knowledge, (if the statement is true, the verifier learns nothing other than the fact that the statement is true). While there have been significant strides in reducing the proving time of zero knowledge, i.e. one scheme reduces the proving time approximately 18,000x for the canonical CNN models on VGG16 , the reality is that the zk-snark (succinct argument of knowledge) still takes 44 hours to complete, rather than 10 years. Several orders of magnitude in performance are still needed for a practical and deployable solution.\nConfidential Computing - Confidential computing focuses on protecting data while it's in use, complementing existing security measures that protect data at rest and in transit. This technology creates a secure enclave within a computer's processor, isolating sensitive data and code from the rest of the system. By doing so, it shields the data from unauthorized access, even in the event of a system compromise (Mulligan et al. 2021). Confidential computing ensures that even cloud service providers cannot access the data being processed on their systems by employing hardware-based trusted execution environments (TEEs). These TEEs, also known as secure enclaves, provide an isolated region of memory where code can be executed and data can be processed securely. The contents of this secure enclave are encrypted and can only be decrypted within the CPU itself, making it extremely difficult for malicious actors to access or tamper with the data. Unfortunately, support for these TEEs is not widespread and has little to no support for GPU processing in consumer hardware. Only recently with the H100 NVidia GPU can one perform tasks in a secure enclave.\nMultiparty Computation (MPC) in ML - There has been significant progress in making ML inferences more secure and private. Initially, much of the work focused on ML algorithms such as regression and CNN-based vision models and used arithmetic secret sharing methods. Later works highlighted the performance issues of MPCs when applied to transformer-based models. As a result, efforts have been directed toward improving the performance of existing MPC frameworks. All of the MPC frameworks mentioned here focus on distributing secret 'shares' of input to different parties holding 'shares' of a model."}, {"title": "Related Work in Distributed Computing", "content": "As for the distribution of work across multiple nodes, a common practice is the sharding of a model. Sharding a model is an industry standard when a model needs to be split across multiple devices, either due to the model's size exceeding the capacity of a single machine or to improve the system's scalability . Several open-source libraries exist for model sharding, however, the key focus of these methods is to to improve model performance and scalability for training and inference. Some of them are discussed briefly in this section. DeepSpeed is an open-source deep learning library developed by Microsoft that powers unprecedented scale and speed for training, inference, and model compression using techniques such as Zero Redundancy Optimizer (ZERO). DeepSpeed-Inference uses model parallelism, high-performance kernels, and memory optimization to enable low-latency inference. Any pre-trained model can be loaded with a user-defined parallelism parameter. DeepSpeed will automatically shard the model across multiple GPUs for inference.. Fully Sharded Data Parallel (FSDP) is a training technique in the PyTorch distributed module, designed for models that cannot fit into a single GPU's memory. Inspired by and ZeRO Stage 3, FSDP splits model parameters, gradients, and optimizer states across multiple GPUs. It uses an all-gather operation to collect parameters and gradients for processing and a reduce-scatter operation to shard gradients across GPUs after the backward pass from Nvidia's NCCL library (NVIDIA 2024). However, none of these libraries and methods tackle the core problem of privacy and verification of work in a trustless manner. Thus, we propose a Secure Multiparty Computation approach to distributed inference."}, {"title": "Methodology", "content": "In this section, we propose a Secure Multi-Party Computation (SMPC) architecture within the context of a transformer-based generative AI model. We demonstrate that given a proprietary model, both the model and user prompts remain secure in a distributed setting."}, {"title": "System Architecture", "content": "We propose a single-client, multi-server architecture. The client refers to a system environment controlled by the model owner, which could be within the organization's network or hosted in a secure environment. Servers represent distributed nodes that host small parts of the model. In the case of transformer-based models, each server hosts one or more attention layers. The model owner provides each server node with its respective model, without revealing which specific layer(s) it is computing. As long as at least one server remains honest, there is no possibility of collusion to reconstruct the entire model, ensuring the security of both the model weights and architecture."}, {"title": "SMPC in decentralized servers (Algorithm 1)", "content": "The prompt to a generative model, Pi, is kept private by processing the input within the client enclave, i.e. projecting the token ids through an embedding model, E, and transforming the vectors by the first attention layer, \u03a6\u03bf. Previous research has demonstrated that the projected textual embeddings alone are not entirely private and are vulnerable to adversarial attacks. An attacker would need access to the prompt and the corresponding embeddings to execute a black-box attack. However, in our design, only authorized users within the organization have permission to prompt the system, and the data leaving the client enclave for the external network consists of hidden states that are further transformed by the first attention layer.\nA model divided into k splits contains a variable number of attention layers. The client orchestrates the processing order, as it alone knows which servers host specific layers. The servers do not communicate directly with each other. This is a design choice essential to maintain ambiguity about the model's distribution across servers. After the servers complete the calculations for the final split, the client handles the remaining processing, including the final attention layer \u03a6\u03b9 and any post-processing \u03be, see Figure 2."}, {"title": "Verification of work (Algorithm 2)", "content": "When distributing work to a third party, there is always a risk of dishonesty. Verification entails providing a convincing argument that the system will behave correctly across different scenarios. To ensure that each split j's calculation is correct, we add redundancy of work into our design. Consensus between independent verifiers assumes that the majority of the server nodes are honest. There are n servers with the same attention layers performing identical computations in parallel. Server r with split j has a model \u03a6j. As the client receives results from n servers hosting split j, a verification process ensures that the results are valid. The outputs from each of the n servers are first hashed using a locally sensitive hashing (LSH). This results in n sets of hashes, and for each hash h, we compare the Hamming distance-based similarity to the other n - 1 hashes.\nLet h\u2081 and h\u2082 be two hash values of the same length 1. The hash similarity sim(h1, h2) is defined as:\n$sim(h_1, h_2) = 1 - \\frac{1}{l} \\sum_{i=1}^{l} \\delta(h_{1,i}, h_{2,i})$\n$\\delta(x, y) = \\begin{cases} 1 & \\text{if } x \\neq y \\\\ 0 & \\text{if } x = y \\end{cases}$\nThe decision to use an LSH was made due to structural uniformity within the outputs from attention layers. If the similarity score exceeds a predefined threshold for more than  other redundant servers, we consider the work correct and add it to a list X of valid results with similarity within the threshold.\nTo ensure consensus, we require |X| > \\frac{n}{2}; otherwise, we deem the computation untrustworthy and raise an exception. If this threshold is met, we select the mode of X as the output for split j, or arbitrarily choose any result if all values in X are unique."}, {"title": "Experiments", "content": "For our experiments, we set up a heterogeneous, decentralized GPU network. The client, running on an NVIDIA A40 GPU, is connected to the servers via the internet through designated network ports. The servers, that host the splits, consist of NVIDIA A40, GeForce RTX 3080 Ti, and GeForce RTX 4090 GPUs.\nWe conducted experiments on two modalities of generative AI: image and text. The experimental setup and a detailed discussion of the results are provided in the following subsections."}, {"title": "Experiment Setup", "content": "For image generation, we used the Stable Diffusion 3 (SD3) Medium model to demonstrate the proposed architecture. SD3 generates detailed images from text descriptions using a latent diffusion model. The model employs 24 transformer attention layers to remove successive Gaussian noise through a sequence of denoising steps. We did not include the optional T5 text encoder in the diffusion pipeline for these experiments. Throughout the experiments, we consistently set the total number of inference steps to 28 and used 20 different prompts to run the diffusion pipeline 10 times for each setting variation to get average runtimes.\nFor language generation experiments with the proposed SMPC architecture, we use the Llama 3.1 8B model, which is an auto-regressive decoder-only model. It consists of 32 transformer decoder layers that work in succession to predict the next token. We generated the text for 5 different prompts for each setting to get average runtimes.\nTable 1 shows the impact on the performance of varying the number of splits k and image sizes/token length. The first setting, k = 0, is the vanilla setup with the models running on a single client machine. In the second setting, k = 1, for SD3, the client hosts the text encoders (CLIP-G/14 and CLIP-L/14), VAE, and the first and last attention layers, while the remaining layers are handled by third-party servers. For Llama3.1, the client hosts the tokenizer, embedding layers, and the first and last decoder layers. This configuration secures user prompts but exposes most of the model to the servers. The third setting, k = 2, addresses this issue by not providing all the remaining attention layers to a single server but first splitting them into two distinct models, which are then anonymously distributed across multiple servers. Redundant work is implemented to ensure correctness. The number of independent verifiers is set to n = 3 for all experiments where k > 0. For the verification of work, hashing function \u03be is set to be perceptual hash (pHash) for image generation and difference hash (dHash) for text generation (see Alg. 2)."}, {"title": "Tolerance in Reproducibility", "content": "Even when two machines host the same model and have identical inputs, deviations can occur in CUDA-based computations due to inherent non-deterministic behavior. In our experiments, we noticed that the LSH did not always match between the honest independent verifiers. We present posterior probabilities for detecting incorrect or fraudulent behavior, assuming either a majority or super-majority of nodes in the network are honest (Figure 3).\nWe use, $P(X \\geq k) = \\sum_{i=k}^{n} {n \\choose i} p^i (1 - p)^{n-i}$, where the binomial distribution gives the probability of getting more than or equal to k successes over n independent verifiers. In the case of image generation, we observed a matching probability of p = 97.32% with a tolerance of t = 0 (hashes matched exactly between servers). Based on this, we determined that simple majority verification achieves accuracies of 99.789%, 99.815%, and 99.998% using 3, 5, and 7 independent verifiers, respectively. Supermajority verification (with >2/3 agreement) yields accuracies of 99.320%, 99.938%, and 99.857% using 5, 7, and 9 independent verifiers, respectively.\nFor text generation, the matching probability was p = 98.92% with t = 0. Simple majority verification achieves accuracies of 99.965%, 99.999%, and 99.999% using 3, 5, and 7 independent verifiers, respectively. Supermajority verification (with >2/3 agreement) yields accuracies of 96.785%, 99.938%, and 99.857% using 3, 5, and 7 independent verifiers, respectively. These results demonstrate high verification accuracy with minimal redundant work across both majority and super-majority schemes, see Figure 3."}, {"title": "Discussion", "content": "The data in table 1 shows that as the number of splits k increases, performance declines. As expected, the major bottleneck occurs in network transfer, with the number of data transfers between the client and servers increasing linearly by 2k with respect to the number of splits k. The data must be offloaded to the CPU before being sent over the internet and then reloaded into VRAM at each step, which also has a significant impact on performance. The parallelization of verifiers was possible because they all perform the same calculations. However, the servers hosting different splits must communicate sequentially, as the input for server j depends on the output of server j \u2013 1. This limitation of the algorithm prevents a fully parallel distribution of work.\nIn the case of image generation, we observe that image size directly affects the processing time. This is because, in stable diffusion, the size of latent hidden states being passed around is dependent on the image size. Similarly, in text generation, the time required to generate the next token increases as the sequence lengthens. In the current setting, each server needs a new updated KV cache to be transferred over the network after the previous iteration. This is something we are currently working to optimize.\nAlthough verification with hashes takes much less time with n = 3, it exhibits a quadratic growth pattern in relation to the number of verifiers (n). In the previous section, we demonstrated that when the majority of nodes in the network are honest, a relatively small set of verifiers is sufficient to ensure reliability."}, {"title": "Path to Deployment", "content": "We are developing secure multiparty computation methods to protect the privacy of user information in our cloud platform hosted at blockentropy.ai. On this platform, we offer API access to generative language, image, and video services. This work is an important first step towards privacy-preserving deployment. AI will be entering an \u201cHTTPS\u201d-like revolution in the next decade where data and processing will eventually need to be protected. Currently, all data must be processed in \u201cclear text\u201d, where the computing environment can see all the inputs, models, and outputs of the system. SMPC with secure enclaves for the client can provide privacy to the user prompts, and allow the model performing inference to be hosted in a trustless, decentralized fashion while ensuring the protection of intellectual property. While there are trade-offs in speed and computation off-load, we are continually improving the latency of these approaches."}, {"title": "Limitations", "content": "Firstly, the current approach faces scalability challenges. As discussed earlier, significant latency arises from multiple data transfers over the network, which impacts performance. Additionally, in the context of image generation, the hidden states computed by the servers are not entirely concealed. As illustrated in Figure 4, visualizing the hidden states during later inference steps reveals an obfuscated yet discernible output. This indicates that while the input prompt remains secure, the generated output is not fully private. Conversely, we observed that hidden states in Llama models are not easily decipherable into meaningful information, suggesting a difference in privacy levels between modalities.\nBeyond scalability and privacy, other limitations include the computational overhead associated with redundant verification processes, particularly as the number of verifiers increases. Additionally, the current method relies heavily on the assumption that at least one node is honest, which may not always hold in highly adversarial environments. Furthermore, the sequential nature of processing across server splits limits the potential for parallelism, constraining overall system efficiency."}, {"title": "Conclusion", "content": "We propose a Secure Multi-Party Computation (SMPC) architecture for transformer-based generative AI models. This ensures user input privacy and model intellectual property protection by securely sharding the model across multiple servers in a decentralized network. Our verification algorithm mitigates the risk of dishonest computations by leveraging redundant work and hash-based verification, achieving high accuracy with a small number of verifiers. This demonstrates that secure and verifiable computation is possible even in decentralized environments without assumed trust. The approach has limitations in scalability and performance, particularly as the number of splits increases, causing more network latency. Future work will focus on optimizing these aspects, exploring efficient communication protocols, and ways to optimize data load/offload between devices. Our work represents a significant step toward privacy-preserving generative AI, offering a promising path for deploying Al services securely and in a decentralized manner. As demand for AI solutions grows, protecting sensitive data and intellectual property will be increasingly critical."}]}