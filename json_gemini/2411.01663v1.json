{"title": "Unlocking the Theory Behind Scaling 1-Bit Neural Networks", "authors": ["Majid Daliri", "Zhao Song", "Chiwun Yang"], "abstract": "Recently, 1-bit Large Language Models (LLMs) have emerged, showcasing an impressive combination of efficiency and performance that rivals traditional LLMs. Research by [WMD+23, MWM+24] indicates that the performance of these 1-bit LLMs progressively improves as the number of parameters increases, hinting at the potential existence of a Scaling Law for 1-bit Neural Networks. In this paper, we present the first theoretical result that rigorously establishes this scaling law for 1-bit models. We prove that, despite the constraint of weights restricted to {-1,+1}, the dynamics of model training inevitably align with kernel behavior as the network width grows. This theoretical breakthrough guarantees convergence of the 1-bit model to an arbitrarily small loss as width increases. Furthermore, we introduce the concept of the generalization difference, defined as the gap between the outputs of 1-bit networks and their full-precision counterparts, and demonstrate that this difference maintains a negligible level as network width scales. Building on the work of [KMH+20], we conclude by examining how the training loss scales as a power-law function of the model size, dataset size, and computational resources utilized for training. Our findings underscore the promising potential of scaling 1-bit neural networks, suggesting that int1 could become the standard in future neural network precision.", "sections": [{"title": "Introduction", "content": "Large-scale neural networks, particularly Large Language Models (LLMs) [BMR+20, ZZL+23] and Large Multimodel Models (LMMs) [YFZ+23, WGC+23], are becoming increasingly relevant to our day-to-day lives, finding a huge variety of applications in both the workplace and at home [LDX+23, YJLY23]. However, it is expensive to deploy and run these models due to their substantial computational requirements, large memory footprints, and energy consumption [VSP+17, AS23, ZNH+24]. This is especially true for resource-constrained environments, such as mobile devices, edge computing, or companies with limited infrastructure [HZC+17, LYW+22, CKH+23]. To make these models more efficient and accessible, quantization techniques are used, which reduce the precision of the model's parameters (such as weights and activations) from floating-point numbers to lower-bit representations (e.g., 8-bit or even lower) [NFA+21a, FAHA22, GKD+22, LTT+24, ADC+23]. Quantization reduces the memory and computational costs of inference, enabling faster processing with less energy, while maintaining a comparable level of performance. This optimization allows language models to be more practical, scalable, and sustainable for widespread use across various platforms [BNB21, LOH22, GTH+23].\nIn particular, quantization techniques could be primarily divided into two methods: Post-Training Quantization (PTQ) [LWH+21, XLS+23, TCS+24] and Quantization-Aware Training (QAT) [LOZ+23, WMD+23, MWM+24]. PTQ methods, including uniform and non-uniform quantization, conveniently convert pre-trained model weights and activations to lower-bit representations post-training. However, this leads to accuracy loss, especially in lower precision, as the model is not optimized for these quantized representations and significant shifts in weight distribution occur [NFA+21b]. The alternative, Quantization-Aware Training (QAT), incorporates quantization during training, allowing the model to fine-tune and adapt its parameters to the quantized representation, compensating for quantization errors. Therefore, compared to PTQ, QAT maintains higher accuracy and robustness even in lower precision.\nRecent studies [LOP+22, WMD+23, MWM+24, ZZS+24] have shown that 1-bit LLMs, most of which have matrix weights in the range of {\u22121, +1}, can be trained from scratch to deliver performance that rivals that of standard LLMs. These models exhibit remarkable efficiency, particularly in terms of scaling laws. Experimental results indicate that the performance of the 1-bit model improves as the number of parameters increases, a principle that mirrors the training approach utilized in standard LLMs [KMH+20]. Despite the demonstrated efficiency of quantization methods, our understanding of the training mechanism for quantization remains limited. Specifically, it remains unclear how and why the 1-bit QAT enhances learning capability as the number of neurons in the model is scaled up. In addition, we are also concerned about whether the quantization method damages the generalization ability compared to full precision networks.\nIn this study, we initially apply the Neural Tangent Kernel (NTK) framework to delve into the optimization and generalization issues associated with a two-layer linear network operating in 1-bit (int1) precision, as detailed in Section 4. We introduce a 1-bit quantization method to the hidden-layer weights $W \\in \\mathbb{R}^{d\\times m}$ of the conventional NTK linear network, where d represents the input dimension and m indicates the model's width. Our analysis reveals that the training dynamics of the 1-bit model approximate kernel behavior as the model width m expands. This key finding paves the way for an established relationship between the theoretically guaranteed loss and the model width, endowing the model with robust learning capabilities akin to kernel regression. Ultimately, the model achieves an insignificantly small training loss, contingent on setting a sufficiently large model width, selecting an appropriate learning rate, and allowing an adequate training duration.\nMoreover, Section 5 provides a theoretical confirmation that, within the scaling trend, the disparities in predictions of the 1-bit model from those of the original linear network on identical"}, {"title": "Related Work", "content": "Efficient Training Methods for Quantized Networks Training large-scale neural networks with quantization introduces significant computational and memory savings, but it also presents challenges in optimization, particularly when dealing with extremely low precision formats like 1-bit or 8-bit. To address these challenges, several efficient training methods have been developed that aim to maintain accuracy while leveraging the benefits of quantization. One key method is Gradient Quantization, where the gradients during backpropagation are quantized to lower precision to reduce memory overhead and bandwidth during distributed training. Techniques like stochastic rounding are used to mitigate the impact of quantization noise, ensuring the training process remains stable and converges effectively.\nAnother important approach is Low-Rank Factorization [SKS+13, HHC+22], which decomposes the large weight matrices in neural networks into smaller matrices, reducing the number of parameters that need to be updated during training. When combined with quantization, this method significantly reduces both the memory footprint and computational complexity, allowing for faster training on hardware with limited resources.\nQuantization Techniques for Accelerating Language Models Beyond traditional weight and activation quantization, several advanced methods utilize quantization to enhance the efficiency of large language models (LLMs). One key approach is KV cache quantization [HKM+24, ZYXS24, LYJ+24, ZDH24], which reduces the memory footprint of transformer models during inference by quantizing the stored attention keys and values. This method is particularly beneficial for tasks involving long sequences, significantly speeding up inference and lowering memory consumption without a substantial loss in accuracy.\nAnother effective technique is mixed-precision quantization [PNvB+23, TOW+23], where different parts of the model are quantized at varying precision levels based on their sensitivity. For example, attention layers might use higher precision (e.g., 16-bit), while feedforward layers are quantized to 8-bit or lower. This balances computational efficiency and model performance. These strategies, combined with methods like activation pruning, showcase how targeted quantization can\ndrastically accelerate LLMs while maintaining their effectiveness in real-world applications.\nNeural Tangent Kernel. The study of Neural Tangent Kernel (NTK) [JGH18] focuses on the gradient flow of neural networks during the training process, revealing that neural networks are equivalent to Gaussian processes at initialization in the infinite-width limit. This equivalence has been explored in numerous studies [LL18, DZPS18, SY19, AZLS19, WLLM19, BM19, LSP+20, CB20, SWL21, ZGJ21, SK22, GMS23, LLSS24, SWL24, LSSY24] that account for the robust performance and learning capabilities of over-parameterized neural networks. The kernel-based analysis framework provided by NTK is gaining popularity for its utility in elucidating the emerging abilities of large-scale neural networks. In a remarkable stride, [ADH+19] introduced the first exact algorithm for computing the Convolutional NTK (CNTK). This was followed by [AWBB20] who proposed the Recurrent NTK, and [HBSDN20] who presented the concept of infinite attention via NNGP and NTK for attention networks. These innovative works have showcased the enhanced performance achievable with the application of NTK to various neural network architectures. In a specific study, [MWY+23] examined the training dynamics of fine-tuning Large Language Models (LLMs) using NTK, affirming the efficiency of such approaches."}, {"title": "Preliminary", "content": "In this section, we give the basic setups of this paper, which includes the introduction of the quantization method in this paper (Section 3.1), our NTK-style problem setup that we aim to solve in this paper (Section 3.2) and recalling the classical NTK setup for a two-layer linear network with ReLU activation function (Section 3.3)."}, {"title": "Quantization", "content": "We first show how we reduce the computation of the inner product of two vectors from multiplication and addition operations to addition operations only, which is achieved by binarizing one of the vectors. This method could be extended to matrix multiplication easily since the basic matrix multiplication is to implement the inner product computation of two vectors in parallels. For a vector $w \\in \\mathbb{R}^d$, we define our quantization function as [WMD+23, MWM+24]:\n$\\textrm{Quant}(w) := \\textrm{Sign} (\\textrm{Ln}(w)) \\in \\{-1,+1\\}^d,$\nwhere $\\textrm{Ln}(w)$ is the normalization method that is given by:\n$\\textrm{Ln}(w) := \\frac{w - \\textrm{\u0395}(w) \\cdot 1_d}{\\sqrt{V(w)}} \\in \\mathbb{R}^d.$\nSpecially, we use $\\textrm{\u0395}(w) := \\frac{1}{d} \\Sigma_{k=1}^d w_k \\in \\mathbb{R}$ to denote the computational expectation of vector w and use $V(w) := ||w - \\textrm{E}(w) \\cdot 1_d||_2 \\in \\mathbb{R}$ to denote the corresponding variance.\nBesides, the kth entry of signal function $\\textrm{Sign}(z) \\in \\mathbb{R}^d$ for $z \\in \\mathbb{R}^d, k \\in [d]$ is define by:\n$\\textrm{Sign}_k(z) := \\begin{cases} +1, & z_k \\geq 0 \\\\ -1, & z_k < 0 \\end{cases}$\nHence, we have a binary vector $\\textrm{Quant}(w)$ where each entry of it is limited in the range $\\{-1,+1\\}$, and we denote that $\\tilde{w} := \\textrm{Quant}(w)$ to simplify the notation. For any other vector $x \\in \\mathbb{R}^d$, addition operation $\\Sigma_{i=1}^d \\pm x_k$ is sufficient to compute $\\langle w,x \\rangle$. After that, we introduce the dequantization function to recover the original computation result by showing:\n$\\textrm{Dequant}(\\langle \\tilde{w}, x \\rangle) := \\sqrt{V(w)} \\cdot \\langle \\tilde{w}, x \\rangle + \\textrm{E}(w) \\cdot \\langle 1,x \\rangle$"}, {"title": "NTK Problem Setup", "content": "Data Points. We consider a supervised learning task with a training dataset $D = \\{(x_i, y_i)\\}_{i=1}^n \\subset \\mathbb{R}^d \\times \\mathbb{R}$, where each data point is under a mild assumption that $||x_i||_2 = 1$ and $|y_i| \\leq 1, \\forall i \\in [n]$ [DZPS18]. Moreover, we are also concerned about the problem of the generalization of 1-bit models, we define the test dataset to compare 1-bit networks with standard networks, that is $D_{\\textrm{test}} := \\{(x_{\\textrm{test},i}, y_{\\textrm{test},i})\\}_{i=1}^n \\subset \\mathbb{R}^d \\times \\mathbb{R}$, where $||x_{\\textrm{test},i}||_2 = 1$ and $|y_{\\textrm{test},i}| \\leq 1, \\forall i \\in [n]$.\nModel. Here, we use hidden-layer weights $W = [w_1,w_2,..., w_m] \\in \\mathbb{R}^{d\\times m}$ and output-layer weights $a = [a_1,a_2,...,a_m]^\\top \\in \\mathbb{R}^m$. We consider a two-layer attention model f, which is defined as follows:\n$f(x, W, a) := \\kappa \\frac{1}{\\sqrt{m}} \\Sigma_{r=1}^m a_r \\cdot \\textrm{ReLU}(dq(\\tilde{w}_r, x)),$\nwhere $\\textrm{ReLU}(z) := \\begin{cases} z, & z\\geq 0 \\\\ 0, & z < 0 \\end{cases}$ for all $z \\in \\mathbb{R}$, dq : $\\mathbb{R} \\rightarrow \\mathbb{R}$ is a omitted version of dequantization function Dequant : $\\mathbb{R} \\rightarrow \\mathbb{R}$, and $\\tilde{w}_r := \\textrm{Quant}(w_r)$ as we denoted in previous section, $\\kappa \\in (0,1]$ is a scale coefficient. Especially, we initialize each weight vector $w_r, \\forall r \\in [m]$ by sampling $w_r(0) \\sim \\mathcal{N}(0, \\sigma\\cdot I_d)$ with $\\sigma = 1$. For output-layer a, we randomly sample $a_r \\sim \\textrm{Uniform}\\{-1,+1\\}$ independently for $r \\in [m]$. Additionally, output-layer weight a is fixed during the training.\nTraining and Straight-Through Estimator (STE). The training loss is measured by quadratic $l_2$ norm of the difference between model prediction $f(x_i, W, a)$ and ideal output vector $y_i$. Formally, we consider to train $W(t) = [w_1(t), w_2(t), ..., w_m(t)] \\in \\mathbb{R}^{d\\times m}$ for $t > 0$ utilizing the following loss:\n$L(t) := \\frac{1}{2} \\Sigma_{i=1}^n |f(x_i, W(t), a) - y_i||^2.$\nMoreover, since the signal function Sign is not differentiable, we use Straight-Through Estimator (STE) to skip the signal function in back-propagation [BLC13, YLZ+19, WMD+23, MWM+24], thus updating the trainable weights W(t). For $t > 0$ and denote $\\eta$ as the learning rate, we omit $f_i(t) := f(x_i, W(t), a) \\in \\mathbb{R}, \\forall i \\in [n]$, the formulation to update rth column of W(t) for all $r \\in [m]$ is given by:\n$w_r(t + 1) := w_r(t) - \\eta \\Sigma_{i=1}^n (f_i(t) - Y_i) \\cdot \\kappa \\textit{1}_{\\{dq(\\tilde{w}_r,x_i)) \\geq 0\\}}x_i.$\nRecalling Classic NTK Setup\nWe now recall the classic NTK setup for the two-layer ReLU linear regression [KWLS21, AZL20, AZL22, ZZC+24]. The function is given by:\n$f'(x, W, a) := \\kappa \\frac{1}{\\sqrt{m}} \\Sigma_{r=1}^m a_r \\cdot \\textrm{ReLU}(<w_r,x>).$\nWe define that $W'(0) := W(0) \\in \\mathbb{R}^{d\\times m}$ to denote the trainable parameter for classic NTK setup, these two matrices are equal at initialization. For $t \\geq 0$, we define the loss of training f' as follows:\n$L'(t) := \\frac{1}{2} \\Sigma_{i=1}^n ||f'(x_i, W'(t), a) - Y_i ||^2.$"}, {"title": "Kernel Behavior and Training Convergence", "content": "We give our convergence analysis for training 1-bit model within the framework of Neural Tangent Kernel (NTK) in this section. First, we state our theoretical results that define the kernel function in training and show how it converges to NTK and maintains the PD (Positive Definite) property in Section 4.1. Then we demonstrate the arbitrary small loss convergence guarantee of training 1-bit model (Eq. (1)) in Section 4.2."}, {"title": "Neural Tangent Kernel", "content": "Here, we utilize the NTK to describe the training dynamic of the 1-bit model. Following preconditions in the previous section, we define a kernel function, that denotes $H(t) \\in \\mathbb{R}^{n\\times n}$ (Gram matrix). Especially, the (i, j)-th entry of H(t) is given by:\n$H_{ij} (t):= \\kappa^2 \\frac{1}{m} \\Sigma_{r=1}^m x_i^T x_j \\textit{1}_{\\{dq(\\tilde{w}_r(t),x_i)) \\geq 0\\}}\\textit{1}_{\\{dq(\\tilde{w}_r(t),x_j)) \\geq 0\\}}.$\nWe define the formal NTK as $H^* := H(0) \\in \\mathbb{R}^{n\\times n}$. Additionally, there's a commonly introduced assumption in NTK analysis: we denote the minimum value of eigenvalues of A with $\\lambda_{\\min}(A)$ for any $A \\in \\mathbb{R}^{n\\times n}$. In our work's context, we presuppose that H is a Positive-definite (PD) matrix, meaning that $\\lambda_{\\min}(H^*) > 0$ [DZPS18].\n1-Bit ReLU Pattern. The pattern of the Rectified Linear Unit (ReLU) function is determined by the indicator of function activation. As illustrated by [DZPS18], in the settings of Section 3.3, the event $\\textit{1}_{\\langle w_r(0),x \\rangle > 0} \\neq \\textit{1}_{\\langle w,x \\rangle > 0}$ happens infrequently for any $w, x \\in \\mathbb{R}^d$ that satisfies $||w-w_r(0)||_2 < R$. Notably, $R := \\max_{r\\in[m]} ||w_r(t) - w_r(0)||_2 = \\eta || \\Sigma_{r=1} \\Delta w_r(\\tau)||_2$. In our analysis, for Eq. (2), the event $\\textit{1}_{dq(\\tilde{w}_r(0),x)) \\geq 0} \\neq \\textit{1}_{dq(\\tilde{w}_r(t),x)) \\geq 0}$ is also unlikely to occur during training.\nThe convergence of H(t) towards $H^*$, as well as the property of H(t) being a PD matrix for any $t \\geq 0$, can be validated by the following lemma:\nLemma 4.1 (NTK convergence and PD property during the training, informal version of Lemma G.5). Assume $\\lambda_{\\min}(H^*) > 0$. $\\delta \\in (0,1)$, define $D := \\max\\{\\sqrt{\\log(md/d)},1\\}$. Let $R \\leq O(\\frac{\\lambda \\delta}{\\kappa^2 n^2 d D})$, then for any $t \\geq 0$, with probability at least $1 - \\delta$, we have:\n$\\bullet$ Part 1. $||H(t) - H^*||_F < O(\\kappa^2 n^2 d R D/\\delta)$.\n$\\bullet$ Part 2. $\\lambda_{\\min}(H(t)) \\geq \\lambda/2$"}, {"title": "Training Convergence", "content": "Having confirmed the convergence of the kernel function of the 1-bit linear network during training in Lemma 4.1, we can transform the dynamics of the loss function L(t) into the following kernel behavior:\n$L(t + 1) - L(t) = - (F(t) - y)^T H(t)(F(t) - y) + C_2 + C_3 + C_4 \\\\\n\\approx - (F(t) - y)^T H(t)(F(t) - y),$\nIn this equation, $F(t) = [f(x_1, W(t), a), ..., f(x_n, W(t), a)] \\in \\mathbb{R}^n$ and $y = [y_1,..., y_n]^T \\in \\mathbb{R}^n$, while $C_2, C_3, C_4$ are negligible terms (please refer to Appendix I for a rigorous proof).\nFurther, by $\\lambda_{\\min}(H(t)) > 0$ (as per Part 2 of Lemma 4.1), for each optimization step $t \\geq 0$, we find that $L(t + 1) \\leq (1 - \\eta \\lambda/2)L(t)$, thus ensuring a non-increase in loss. Given sufficient training iterations and an appropriately chosen learning rate, we can achieve training convergence, the confirmation of which is provided in the following section.\nTheorem 4.2 (Training convergence guarantee, informal version of Theorem I.1). Given an expected error $\\epsilon > 0$. Assume $\\lambda_{\\min}(H^*) > 0$. $\\delta \\in (0,0.1)$, define $D := \\sqrt{\\log(md/d)}$. Choose $m > \\Omega(\\lambda^{-8}n^{12}d^8/(\\delta\\epsilon)^4)$, $\\eta \\leq O(\\frac{\\lambda \\delta}{\\kappa^2 n^2 d D})$. Then let $T > \\Omega((\\eta \\lambda)^{-1}\\log(ndD^2/\\epsilon))$, with probability at least $1 - \\delta$, we have: $L(T) \\leq \\epsilon$.\nScaling Law for 1-Bit Neural Networks. Theorem 4.2 primarily illustrates a fact for any dataset with n data points. After initializing the hidden-layer weights $W \\in \\mathbb{R}^{d\\times m}$ from a normal distribution, and assuming the minimum eigenvalue of NTK $\\lambda > 0$, we set m to be a large enough value to ensure the network is sufficiently over-parameterized. With an appropriate learning rate, the loss can be minimized in finite training time to an arbitrarily small error $\\epsilon$. This offers a crucial insight that confirms the existence of a scaling law for 1-bit neural networks, which is strictly bounded by the model width m and training steps T. Consequently, we present the following Proposition that elucidates the principle of training 1-bit linear networks from scratch. This proposition is built upon Theorem 4.2 and the principle of training loss that scales as a power-law with model size, dataset size, and the amount of compute used for training [KMH+20].\nProposition 4.3 (Scaling Law for 1-Bit Neural Networks). $\\delta \\in (0,0.1)$. Define $N := O(md)$ as the number of parameters, $D := O(n)$ as the size of training dataset, $C:= O(NDT)$ as the total compute cost. Especially, we denote the scale coefficients as $\\alpha := Dd\\log(md/d)$, and we then choose $\\eta \\leq O(\\frac{\\lambda \\delta}{m\\kappa^2 n^2 d D})$ and $T > \\Omega((\\eta \\lambda m)^{-1} \\log(nd\\log(md/d)/\\epsilon))$. Thus, the training loss, denoted as $L_{\\textrm{scale}}$, satisfies:\n$L_{\\textrm{scale}} \\approx \\max\\{\\frac{D^3 \\cdot d^{2.25} \\alpha}{12 N^{0.25}}, \\exp(\\frac{\\alpha}{AC})\\}$\nProposition 4.3 demonstrates that the training loss of the prefix learning converges exponentially as we increase the computational cost C, which primarily depends on the number of parameters and\nthe training time in prefix learning. This further suggests a potential relationship for formulating a scaling law for 1-bit neural networks.\nExtensibility. Our analysis is conducted within a two-layer linear network defined in Section 3, which might raise concerns about its effectiveness in real-world multiple-layer 1-bit networks. However, due to the theory of Hierarchical Learning [BLPL06, ZF14, AAM22], the optimization of a deep neural network is equivalent to training each layer of the network greedily. Therefore, our theoretical conclusion could be easily extended to the situation of training multiple layers 1-bit model."}, {"title": "Generalization Similarity", "content": "In this section, we present our theoretical analysis that proves that training large-scale 1-bit neural networks is equivalent to training standard large-scale neural networks. In Section 5.1, we explain how the difference between the outputs of our 1-bit model and outputs of the standard NTK-style linear network for the same input at initialization, which is defined as function difference at initialization, will be kept in a small error while the model width (denoted as m) increase. Next, in Section 5.2, we confirm that in the trend of scaling up the model width, during the training, the predictions of 1-bit model and full precision model are also similar to a very slight error on both the training dataset and the test dataset."}, {"title": "Function Difference at Initialization", "content": "To begin with, at initialization, the boundary on | f(x, W (0), a) \u2013 f'(x, W'(0), a)| is stated as follows:\nLemma 5.1 (Function difference at initialization, informal version of Lemma K.4). $\\delta \\in (0,0.1)$. Denote $D := \\sqrt{\\log(md/d)}$. $\\forall x \\in \\mathbb{R}^d$ that satisfies $||x||_2 = 1$, for any initial quantization error"}, {"title": "Experiments", "content": "In this section, we aim to verify our theory by evaluating how well our quantization works for learning rigorous functions and comparing it to the standard model. We designed our experiment to 1) validate the scaling law (Section 6.1), 2) visually demonstrate that the performance difference is minimal compared to the standard model, which uses full-bit precision, through visualizations of single-variable input functions (Section 6.2), and 3) show how the test and train losses decrease as the model's parameter size increases and as the epochs progress (Section 6.3)."}, {"title": "Verification on Scaling Law", "content": "Experiment Setup In this experiment, we aimed to learn rigorous functions using a Multi-Layer Perceptron (MLP) with varying depths of 3 and 5 layers. The MLP models had different sizes for the hidden layers, and we measured the minimum loss achieved throughout the training process. Each model was trained for 100,000 steps. We experimented with various parameter sizes and plotted the corresponding loss functions. Additionally, we compared our method with the standard training approach using 32-bit floating-point precision.\nWe experimented with a variety of target functions, and for each function, the inputs $x_i$ were randomly chosen within the range [-1,1]. Specifically, each $x_i$ was sampled from a uniform distribution over this interval to ensure that the network could handle input values across the entire domain of interest. We sampled 100 data points and trained our model over the this set.\nThe functions we aimed to learn during the experiment are listed below:\n1. $f_1(x_1,x_2,x_3, x_4, x_5) = \\exp{(\\frac{1}{2}\\Sigma_i^5 \\sin^2(x_i))}$, This function takes five inputs and applies a sinusoidal transformation followed by an exponential operation.\n2. $f_2(x_1,x_2,x_3, x_4) = \\ln(1+ |x_1|) + (x_2 - x_2) + \\sin(x_3) - e^{x_4}$, the function combines logarithmic, polynomial, trigonometric, and exponential components over four input variables.\n3. $f_3(x_1,x_2,x_3) = x_1 \\times x_2 - \\frac{x_3}{13}$, This is a simple linear function over three inputs, involving multiplication and subtraction.\n4. $f_4(x_1,x_2,x_3, x_4) = x_0 \\cdot \\sin(x_1) + \\cos(x_2) - 0.5\\cdot x_3$, A four-input function mixing trigonometric and linear terms, with coefficients applied to the terms.\n5. $f_5(x_1,x_2,x_3, x_4) = \\frac{e^{x_2} + \\tanh(x_3) + \\sqrt{|x_0\\cdot x_2|}}{1+x_1}$, This function incorporates nonlinear operations like exponentials, hyperbolic tangents, and square roots.\n6. $f_6(x_1,x_2,x_3, x_4) = \\textrm{LambertW}(x_0\\cdot x_1) + \\frac{x_2}{\\log(1+e^{x_3})} + \\frac{\\Gamma(x_1)}{1+x_0}$, The most complex function we tested, which includes special functions like the Lambert W function and the Gamma function, alongside logarithmic and exponential components.\nWe compare our quantized model (using INT1, 32\u00d7 smaller) to a standard non-quantized model (using 32-bit precision). For all functions (f\u2081 to f6), we observe (in ) that as the number of"}, {"title": "Comparison on 1-D Functions", "content": "In this experiment, we aimed to visually demonstrate the performance on highly complex functions with sharp spikes between [-\u03c0,\u03c0]. We sampled 100 uniformly spaced points and trained a 2-layer MLP with 20M parameters to learn the function. Additionally, we sampled 100 random points uniformly from this interval as the test dataset.\nThe first observation from the plot is that both the standard and 1-bit methods learn all the functions almost perfectly, with minimal difference between them. Secondly, both methods perform similarly on these functions, which can be easily observed by comparing the scatter plots of the 1-bit and standard models. The 1-bit model requires 32\u00d7 less energy and computation."}, {"title": "Evaluation on Training and Generalization Similarity", "content": "For the same set of functions, we show how the loss functions for both the train and test datasets decrease as the number of epochs increases. As the training progresses, the loss converges towards zero for models with a higher number of parameters. We experimented with models containing 2.4k, 204k, and 20M parameters, each consisting of only 2 layers.\nAcross all three functions, the loss decreases rapidly in the early epochs and stabilizes for both the training and test sets. Larger models with 20M parameters consistently achieve lower final losses compared to smaller models with 2.4k and 204k parameters, demonstrating the benefit of increased model size. The gap between training and test losses remains minimal, indicating strong generalization across different parameter sizes. More importantly, the key observation is that the models predict similarly on both the training and test datasets, a behavior we refer to as generalization similarity. This means that the models, regardless of size, behave similarly across both datasets, supporting the scaling law that increasing model size leads to better convergence"}, {"title": "Conclusion", "content": "In conclusion, our theoretical results confirm the scaling law for 1-bit neural networks. We demonstrated that the model achieves a small loss as the number of parameters increases. Despite the constraint of binary weights, 1-bit models show similar behavior to full-precision models as their width grows. Our experiments support this theory, showing that 1-bit networks perform nearly as well as standard models on complex functions. As the number of parameters grows, the performance gap between 1-bit and full-precision models reduces. These findings highlight that 1-bit networks are both efficient and effective, providing a strong alternative to traditional models."}]}