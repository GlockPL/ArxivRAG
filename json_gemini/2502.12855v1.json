{"title": "Integrating Arithmetic Learning Improves Mathematical Reasoning in Smaller Models", "authors": ["Neeraj Gangwar", "Suma P Bhat", "Nickvash Kani"], "abstract": "While large models pre-trained on high-quality data exhibit excellent performance across various reasoning tasks, including mathematical reasoning (e.g. GSM8k, MultiArith), specializing smaller models to excel at mathematical reasoning remains a challenging problem. Common approaches to address this challenge include knowledge distillation, where smaller student models learn from large pre-trained teacher models, and data augmentation, such as rephrasing questions. Despite these efforts, smaller models struggle with arithmetic computations, leading to errors in mathematical reasoning. In this work, we focus on leveraging a programmatically generated arithmetic dataset to enhance the reasoning capabilities of smaller models. We investigate two key approaches to incorporate this dataset - (1) intermediate fine-tuning, where a model is fine-tuned on the arithmetic dataset before being trained on a reasoning dataset, and (2) integrating the arithmetic dataset into the instruction-tuning mixture, allowing the model to learn arithmetic skills alongside general instruction-following abilities. Our experiments on multiple reasoning benchmarks demonstrate that incorporating an arithmetic dataset, whether through targeted fine-tuning or within the instruction-tuning mixture, enhances the models' arithmetic capabilities, which in turn improves their mathematical reasoning performance.", "sections": [{"title": "1 Introduction", "content": "Scaling the model and data sizes has had a tremendous effect on performance across various natural language processing (NLP) tasks (Chowdhery et al., 2023; Achiam et al., 2023; Touvron et al., 2023b; Jiang et al., 2023). These pre-trained models can learn from a few demonstrations using in-context learning and do not require task-specific fine-tuning (Brown et al., 2020). They also benefit from generating a sequence of reasoning steps before arriving at the final answer. These strategies have been particularly effective for mathematical reasoning (Wei et al., 2022c; Nye et al., 2022; Fu et al., 2022; Zhou et al., 2022). While large models exhibit excellent performance on various reasoning benchmarks (Lambert et al., 2024; Liu et al., 2024; Jaech et al., 2024), smaller models remain essential due to their efficiency and adaptability. They require significantly fewer computational resources, making them ideal for deployment with limited infrastructure. They also enable faster inference and lower latency, which is crucial for real-time applications. Additionally, they can be fine-tuned more efficiently for specific tasks without the high costs associated with training massive models.\nTraditionally, smaller pre-trained models are adapted for downstream tasks through supervised fine-tuning on a dataset or a combination of datasets formatted as instructions. While this approach is effective for simpler tasks, it falls short when applied to mathematical reasoning, as smaller models struggle to achieve strong performance (Wei et al., 2022b). This challenge arises because math reasoning datasets, like GSM8k (Cobbe et al., 2021), consist of a small number of reasoning problems, typically paired with one solution. The scarcity of training examples makes it difficult for the model to capture the complexity of mathematical reasoning. To overcome this issue, a widely explored research direction is to distill knowledge from large pre-trained teacher models into smaller student models. Some methods use questions from existing training datasets and use prompting to generate solutions for fine-tuning smaller models (Ho et al., 2023; Magister et al., 2023; Fu et al., 2023; Hsieh et al., 2023; Yue et al., 2024). Others use various techniques to rephrase the questions to create more examples (Yu et al., 2024) or multiple views of solutions (Liang et al., 2024) to achieve better reasoning performance.\nAlthough these methods boost mathematical reasoning performance in smaller models, they still struggle with arithmetic computations. In many cases, models generate the correct reasoning steps but arrive at incorrect final answers due to numerical computation errors. Some approaches address this issue by integrating external tools (Cobbe et al., 2021; Schick et al., 2023) or using programs (Gao et al., 2023; Chen et al., 2023; Ye et al., 2023). In this work, we explore whether model performance can be improved by directly mitigating these errors without relying on external tools. Previous research has investigated ways to enhance arithmetic skills in Transformer-based models (Liu and Low, 2023; McLeish et al., 2024), but the effective transfer of these skills to downstream tasks like mathematical reasoning remains largely unexplored. To bridge this gap, we focus on how improving arithmetic skills strengthens a model's mathematical reasoning abilities.\nIn this work, we utilize a programmatically generated arithmetic dataset to enhance mathematical reasoning abilities in small-frame models. We investigate two approaches for incorporating this dataset (1) intermediate fine-tuning, where a model is fine-tuned on the arithmetic dataset before being trained on a reasoning dataset, and (2) integrating the arithmetic dataset into the instruction-tuning mixture. The first approach is inspired by transfer learning, as prior research has shown that fine-tuning a model on a related dataset before training it on the target task can significantly improve its performance (Vu et al., 2020; Phang et al., 2018; Pruksachatkun et al., 2020). The second approach aligns with post-training techniques to refine pre-trained models by exposing them to diverse tasks, helping them acquire new skills and adapt better to various downstream tasks (Wei et al., 2021; Chung et al., 2024; Lambert et al., 2024).\nEmpirical observations using several mathematical datasets lead to the following key takeaways:\n\u2022 Models fine-tuned on an arithmetic dataset before a reasoning dataset perform better than the ones directly fine-tuned on the reasoning dataset. Additionally, arithmetic datasets can be generated programmatically, eliminating the need for manual resources.\n\u2022 Based on our observations with multiple datasets with varying mathematical reasoning tasks, we find that intermediate fine-tuning results in better out-of-domain generalization.\n\u2022 Including an arithmetic dataset into the instruction-tuning mixture leads to better few-shot performance on multiple mathematical reasoning benchmarks.\n\u2022 These models show better robustness to numerical variations such as numerical substitution and digit expansion than models instruction-tuned on a mixture without the arithmetic dataset.\nThis work highlights the importance of explicit arithmetic training as a key factor in improving mathematical reasoning in smaller models. Our source code and datasets are publicly available."}, {"title": "2 Related Work", "content": "Model Specialization via Distillation. Adapting a pre-trained model for a downstream task has been traditionally done through task-specific fine-tuning. However, this approach does not work for mathematical reasoning tasks because datasets, like GSM8k, do not contain enough examples to capture the complexity of mathematical reasoning. Several works have focused on distilling multi-step reasoning solutions from large language models (LLMs) to overcome this limitation. Fu et al. (2023) prompted Codex (Chen et al., 2021) to generate multiple multi-step solutions for the examples in the GSM8k training set and fine-tuned FlanT5 on the ones that led to the correct final answer. Hsieh et al. (2023) used PaLM-540B (Chowdhery et al., 2023) for generating solutions and fine-tuned T5 (Raffel et al., 2020) in a multi-task setting to generate the labels and rationale. Liu et al. (2023) used GPT-3.5-turbo to generate synthetic GSM8k-like examples. Yue et al. (2024) showed that a hybrid of chain-of-thought and program-of-thought solutions performed better than using either format individually. In addition to using LLMs to generate more solutions, Yu et al. (2024) used LLM rephrasing and backward reasoning to augment questions and created a new dataset called MetaMathQA.\nTransfer Learning. Transfer learning has played a pivotal role in NLP. Vu et al. (2020) and Pruksachatkun et al. (2020) studied the effect of intermediate fine-tuning on the model's performance on a target task. Conneau and Lample (2019) explored cross-lingual model pre-training and showed improvements in natural language inference and machine translation. Razdaibiedina et al. (2023) introduced progressive prompts which is a continual learning approach with the forward transfer without catastrophic forgetting. Training on large multi-task mixtures is also a common trend in NLP (Aribandi et al., 2022; Wei et al., 2022a; Chung et al., 2024; Lambert et al., 2024). Another research direction explores identifying relevant examples for a given downstream task from a huge collection of datasets, like P3 (Sanh et al., 2021). These methods create embeddings for all examples of interest using hidden states (Ivison et al., 2023) or gradients (Xia et al., 2024). Given a task, a small subset of relevant examples are selected based on similarity. These methods have been mainly applied to data-efficient instruction-tuning."}, {"title": "3 Our Approach", "content": "Strong arithmetic abilities are crucial for developing robust mathematical reasoning skills. These skills serve as the foundation for solving a range of mathematical problems. While relatively smaller pre-trained models often struggle with arithmetic computations, they can enhance their proficiency through targeted fine-tuning on synthetic datasets.\nIn this study, we investigate two approaches for transferring these skills to the more complex domain of mathematical reasoning \u2013 (1) intermediate fine-tuning on an arithmetic dataset and (2) incorporating the arithmetic dataset during instruction tuning.\nIntermediate Fine-Tuning. Fine-tuning a model on an intermediate task before a downstream task can improve the model's performance on the latter (Phang et al., 2018; Vu et al., 2020; Pruksachatkun et al., 2020). The downstream task is also referred to as the target task. This is called intermediate fine-tuning and can lead to successful knowledge transfer for similar intermediate and target tasks. Building on prior work on transfer learning, we use intermediate fine-tuning to transfer arithmetic abilities to mathematical reasoning.\nUnlike natural language, mathematical computations are precise and do not contain redundancies, typically requiring more training examples for effective learning compared to NLP tasks. When a model is fine-tuned on a reasoning dataset, it must simultaneously learn to generate correct reasoning steps and arithmetic computations. Moreover, datasets like GSM8k contain a limited number of examples, restricting both the quantity and diversity of arithmetic computations. This limitation often leads to arithmetic errors during inference. To address these challenges, we adopt a two-step training approach. First, we fine-tune the model on an arithmetic dataset, allowing it to learn a broad range of numerical computations across diverse values. This intermediate fine-tuning ensures that the model develops a strong foundation in arithmetic. Following this, the model is fine-tuned on a reasoning dataset, where it focuses on applying its pre-learned arithmetic skills rather than learning them from limited examples. This two-step process leads to fewer arithmetic errors during mathematical reasoning compared to models directly fine-tuned on a reasoning dataset, leading to more accurate and reliable performance.\nInstruction Tuning. We also explore the impact of incorporating an arithmetic dataset during post-training, which involves additional training on vast corpora of text. In particular, we focus on instruction tuning-a post-training technique to enhance the ability of pre-trained large language models to follow human instructions more effectively (Wei et al., 2021; Chung et al., 2024). This process involves fine-tuning a pre-trained model on a diverse set of instructions and their responses. The fine-tuning mixture typically contains examples from a wide range of tasks, including the mathematical reasoning domain. To strengthen the arithmetic capabilities of the model, we include a synthetic arithmetic dataset in this mixture. Similar to intermediate fine-tuning, this process improves the model's arithmetic proficiency and enhances mathematical reasoning performance by enabling more accurate numerical computations within reasoning tasks."}, {"title": "4 Datasets", "content": "4.1 Arithmetic Dataset\nA simple arithmetic dataset can be generated programmatically. We refer to Liu and Low (2023) to generate our dataset. Their work has shown that LLAMA (Touvron et al., 2023a) fine-tuned on a programmatically generated dataset outperforms GPT-4 (Achiam et al., 2023) on arithmetic tasks. While the dataset from Liu and Low (2023) contains basic arithmetic operations \u2013 addition, subtraction, multiplication, and division, we extend it to include problems on fractions and percentages. Datasets used in this work do not require computations over large numbers, hence we limit the number of digits in the operands to seven. Furthermore, we use log-uniform sampling to ensure that the dataset is not skewed towards numbers with greater digits. This dataset contains nearly 1.3M examples.\n4.2 GSM8k Training Dataset\nFor the intermediate fine-tuning experiments, we use GSM8k for model specialization. As it does not have a validation set, we randomly sample 512 examples from the training set to create a validation set. We use two versions of GSM8k in this work.\nOriginal. In the first version, we use the remaining examples from the training set for model specialization. This dataset contains 6961 examples. We refer to this dataset as GSM8k (Orig.).\nDistilled. We generate a distilled dataset using the questions from GSM8k (Orig.) to evaluate if intermediate fine-tuning benefits tasks with large training datasets. This dataset is generated by prompting Mistral-7B (Jiang et al., 2023) using the prompt from Wei et al. (2022c). We generate 64 solutions per question and keep the ones that lead to the correct final answer. After removing duplicate solutions, this results in a dataset with close to 175k examples. We refer to this dataset as GSM8k (Dist.).\n4.3 Instruction Tuning Dataset\nFollowing the recent work of Lambert et al. (2024), we use the T\u00dcLU 3 SFT mixture as the instruction tuning dataset for our work. This dataset contains nearly 1M examples, making it significantly smaller compared to the Flan collection (Longpre et al., 2023). However, it contains more mathematical reasoning examples compared to datasets like Flan-mini (Ghosal et al., 2023)."}, {"title": "5 Intermediate Fine-Tuning", "content": "In this section, we present experiments with intermediate fine-tuning.\n5.1 Experimental Setup\nTasks. We evaluate our approach on the GSM8k test set. We also test out-of-domain generalization using the following datasets \u2013 (1) MultiArith (Roy and Roth, 2015) with problems focused on basic arithmetic operations and relatively simpler than GSM8k, (2) ASDiv (Miao et al., 2020) with diverse math problems focused on language usage patterns, and (3) SVAMP (Patel et al., 2021) with varying structures to ensure that a model cannot solve the problems by applying simple heuristics and ignoring question text.\nBaseline. For the baseline, we consider models that are directly fine-tuned on a reasoning dataset without any intermediate fine-tuning.\nModels and Training Details. We experiment with both encoder-decoder and decoder-only architectures. We use FlanT5 (Chung et al., 2024) and GPT2 (Radford et al., 2019). The base and large versions of FlanT5, along with the base, medium, and large versions of GPT2, are used, with parameter counts ranging from 124M to 774M. We use the AdamW optimizer (Loshchilov and Hutter, 2017) with a learning rate of $10^{-4}$, a weight decay of $10^{-4}$, and an effective batch size of 128. For FlanT5-Large and GPT2-Large, a learning rate warmup of 500 steps is used.\nThe intermediate fine-tuning is performed for two epochs without validation. To adapt these models for reasoning, we continue the training from these checkpoints on GSM8k. The models are fine-tuned for 20 and 100 epochs on GSM8k (Dist.) and GSM8k (Orig.), respectively. The best checkpoint is selected based on the GSM8k validation performance."}, {"title": "5.2 Results", "content": "In-Domain Performance. We first evaluate the models on the GSM8k test set. shows the accuracy (%) achieved by various models. These results indicate that FlanT5 benefits from the intermediate fine-tuning, significantly improving the GSM8k performance. Additionally, these results demonstrate that intermediate fine-tuning helps with both GSM8k (Orig.), which has a small training set, and GSM8k (Dist.), which already contains significantly more training examples.\nFor GPT2, we observe a slight decline in the reasoning performance when the model is specialized in reasoning using GSM8k (Orig.) after fine-tuning it on the arithmetic dataset. However, this issue does not arise with GSM8k (Dist.), where we see performance gains comparable to those of FlanT5. We attribute this behavior to the fact that intermediate fine-tuning optimizes the model for arithmetic tasks, making it more challenging to adapt to other tasks compared to the original model. A larger fine-tuning dataset mitigates this issue, as demonstrated by the significant performance improvement when reasoning specialization is performed using GSM8k (Dist.).\nOut-of-Domain Performance. Next, the models fine-tuned on GSM8k are evaluated on MultiArith, ASDiv, and SVAMP, and the results are shown in . These results indicate that intermediate fine-tuning does not hurt out-of-domain generalization. Conversely, the models fine-tuned on the arithmetic dataset first generalize better than those directly fine-tuned on GSM8k.\nArithmetic in Reasoning Context. While the models fine-tuned on the arithmetic dataset excel at basic arithmetic tasks compared to their original versions, do these skills transfer to reasoning datasets? Moreover, are they the reason behind the better reasoning performance, as shown in Table 1? Accuracy on the test sets from various reasoning datasets, such as GSM8k and MultiArith, does not directly capture this, as an incorrect final answer can stem from multiple factors beyond arithmetic errors. To better understand the impact of arithmetic computations, we specifically look at them within the reasoning process, ensuring that all other reasoning steps remain correct.\nWe use the GSM8k test set and identify the arithmetic computations using the calculation annotations (enclosed within \u00ab\u00bb). Given a question and its solution up to an annotation, the models are asked to generate the next tokens, which are then compared to the ground truth. We define the accuracy of these tokens as GSM8k arithmetic accuracy. See Appendix A for details. Figure 3 shows how well the models from handle arithmetic within a reasoning context. These results suggest that intermediate fine-tuning reduces arithmetic errors compared to the models fine-tuned directly on GSM8k, leading to an average improvement of 11.7% in arithmetic computations. Even GPT2, when fine-tuned on GSM8k (Orig.), makes fewer arithmetic errors with intermediate fine-tuning. These results corroborate the hypothesis that the slight decline in the reasoning performance of GPT2 fine-tuned on GSM8k (Orig.) after the intermediate fine-tuning results from the model being optimized for arithmetic tasks, potentially making it challenging to adapt the model for reasoning tasks with a smaller training set. The issue is mitigated by using a larger training dataset, GSM8k (Dist.) in this case.\nProlonged Intermediate Fine-Tuning. We next investigate the impact of extending the intermediate fine-tuning beyond two epochs. We find that fine-tuning models on the arithmetic dataset for additional epochs provides no benefit or adversely affects the reasoning performance when models are later fine-tuned on a reasoning dataset. We attribute this outcome to two factors. First, prolonged fine-tuning on the arithmetic dataset further optimizes the model for arithmetic tasks, limiting its adaptability for other tasks. Second, the model learns the computations for solving GSM8k problems within the first few epochs, and additional fine-tuning does not improve these computations. Figure 2 illustrates GSM8k arithmetic accuracy and overall performance as a function of intermediate fine-tuning epochs. The results show that extended intermediate fine-tuning does not improve GSM8k arithmetic accuracy while making models optimized for arithmetic tasks. This leads to no improvement or decline in GSM8k performance."}, {"title": "6 Instruction Tuning", "content": "In this section, we present experiments with instruction tuning.\n6.1 Experimental Setup\nTasks. We use nine math reasoning datasets to evaluate the impact of including an arithmetic dataset in the mixture of instruction-tuning datasets. Similar to intermediate fine-tuning experiments, we evaluate the models on GSM8k, ASDiv, and SVAMP. In addition to MultiArith, we consider the following datasets from MAWPS (Koncel-Kedziorski et al., 2016) \u2013 (1) AddSub (Hosseini et al., 2014) which is a collection of addition and subtraction problems, (2) SingleEq (Roy et al., 2015) which contains single equation problems, (3) SingleOp (Roy et al., 2015) with single operation arithmetic problems, and (4) SimulEq (Kushman et al., 2014) with multiple equation math word problems. AQUA (Ling et al., 2017), which contains algebraic problems in multiple-choice format, is also used for evaluation.\nWe also evaluate the robustness of the models against various perturbations. For this purpose, we use two datasets \u2013 GSM-Plus (Li et al., 2024) and GSM-Symbolic (Mirzadeh et al., 2024). GSM-Plus is an adversarial grade school math dataset that introduces five variations for each problem in the GSM8k test set \u2013 numerical variation, arithmetic variation, rephrasing, distractor insertion, and omissions of necessary statements. GSM-Symbolic contains 100 test problems from GSM8k for which variations can be systematically generated by altering numerical values or proper names. As this work focuses on arithmetic computations, we generate 50 variations by modifying the numerical values in the original problems for our experiments.\nBaselines. We use the following two baselines \u2013 (1) the pre-trained model and (2) the model fine-tuned only on the T\u00dcLU 3 SFT mixture.\nModel and Training Details. As large language models predominantly use the decoder-only architecture, we use the large version of GPT2 with 774M parameters for this experiment. The models are fine-tuned for five epochs. We use the AdamW optimizer with 2 \u00d7 $10^{-4}$ learning rate and a weight decay of $10^{-4}$. A learning rate warmup of 500 steps is used. As examples in the T\u00dcLU 3 SFT mixture have varied sequence lengths and differ significantly from the arithmetic examples in this regard, we use a variable batch size with approximately 0.5M tokens in each batch. The input to the model is truncated from the left to have at most 1024 tokens.\nEvaluation and Decoding. We use few-shot prompting to evaluate the models. Four exemplars are used in the prompts due to the maximum sequence length limit in GPT2. We use exemplars from the prompts used in Wei et al. (2022c). See Appendix B for more information. The models are asked to generate 256 tokens for each prompt, and the accuracy is computed based on the final answer generated by the model. We do not validate the reasoning steps. Similar to intermediate fine-tuning experiments, we use greedy and self-consistency decoding. We repeat the evaluation three times with the latter and report the mean accuracy.\n6.2 Results\n shows the results of instruction tuning GPT2-Large with and without including the synthetic arithmetic dataset in the fine-tuning mixture. The model fine-tuned on the T\u00dcLU 3 SFT mixture and synthetic arithmetic examples achieves better performance across math reasoning datasets, except AQuA, than the model only fine-tuned on the T\u00dcLU 3 SFT mixture, with both greedy and self-consistency decoding. For self-consistency decoding, the former outperforms the latter in all three evaluation attempts across datasets. We also compute the GSM8k arithmetic accuracy for these models and find a 3% increase in accuracy by including the arithmetic dataset in the fine-tuning mixture. See Appendix C for the results on individual datasets in MAWPS.\nNeither model performs well on AQuA, and the performance of all models is close to random choice. Randomly choosing an option leads to an average accuracy of 19.9% \u00b1 2.7% after 100 trials.\n6.3 Robustness to Perturbations\nWe next evaluate the robustness of our models to perturbations. We use GSM-Plus and GSM-Symbolic for this evaluation.\nGSM-Plus. The overall accuracy is shown in. The breakup of the overall performance by perturbation types is illustrated in . The model fine-tuned on the T\u00dcLU 3 SFT and arithmetic mixture performs better than the model only fine-tuned on the T\u00dcLU 3 SFT mixture across different perturbations. We further investigate the performance drop for the two models relative to the original GSM8k dataset. In particular, we are interested in numerical variations which include numerical substitutions, digit expansions, and integer-decimal-fraction conversions. For these perturbations, we find that the model fine-tuned on the T\u00dcLU 3 SFT mixture and the arithmetic dataset sees a lower performance drop relative to the original GSM8k dataset than the model fine-tuned only on the T\u00dcLU 3 SFT mixture.\nGSM-Symbolic. shows the performance of the instruction-tuned models on the problems from GSM-Symbolic. The performance of these models on the original problems is presented in"}, {"title": "7 Conclusion", "content": "In this work, we explored the impact of incorporating an arithmetic dataset through two approaches \u2013 intermediate fine-tuning and integration within the instruction-tuning mixture. Our experiments demonstrated that both approaches boost mathematical reasoning performance in smaller models. While intermediate fine-tuning can make subsequent fine-tuning on other tasks more challenging, the issue can be mitigated by using a larger dataset. Additionally, we found that the model instruction tuned on a mixture including the arithmetic dataset outperformed the one trained without it. These findings highlight the crucial role of explicit arithmetic training in strengthening mathematical reasoning in smaller models."}, {"title": "8 Limitations", "content": "While incorporating an arithmetic dataset improves a model's mathematical reasoning performance, smaller models still have considerable room for improvement. In this work, we include the arithmetic dataset in the training pipeline but do not investigate other factors, such as custom embedding schemes for arithmetic computation. A promising direction for future research is to incorporate insights from recent work, such as McLeish et al. (2024), into the model architecture. Another limitation pertains to the instruction-tuning mixture. While including the arithmetic dataset improves the few-shot performance on the mathematical reasoning benchmarks, data mixture ablations may be necessary to optimize the instruction-tuning mixture for better overall performance. Finally, while this study focuses on smaller models, its findings apply to larger models. The arithmetic capabilities of pre-trained models could be further enhanced by leveraging synthetic arithmetic datasets. We leave these explorations for future work."}]}