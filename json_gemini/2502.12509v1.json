{"title": "LegalCore: A Dataset for Event Coreference Resolution in Legal Documents", "authors": ["Kangda Wei", "Xi Shi", "Jonathan Tong", "Sai Ramana Reddy", "Anandhavelu Natarajan", "Rajiv Jain", "Aparna Garimella", "Ruihong Huang"], "abstract": "Recognizing events and their coreferential mentions in a document is essential for understanding semantic meanings of text. The existing research on event coreference resolution is mostly limited to news articles. In this paper, we present the first dataset for the legal domain, LegalCore, which has been annotated with comprehensive event and event coreference information. The legal contract documents we annotated in this dataset are several times longer than news articles, with an average length of around 25k tokens per document. The annotations show that legal documents have dense event mentions and feature both short-distance and super long-distance coreference links between event mentions. We further benchmark mainstream Large Language Models (LLMs) on this dataset for both event identification and event coreference resolution tasks, and find that this dataset poses significant challenges for both open-source and proprietary LLMs, which all perform significantly worse than a supervised baseline. We will publish the dataset as well as the code.", "sections": [{"title": "Introduction", "content": "Identifying event mentions and grouping event mentions based on their coreference relations is a key step for text semantic understanding and necessary for further event structure analysis. However, research on event coreference resolution is mostly limited to news articles as the annotated datasets for event coreference resolution are mostly for this domain (Walker and Consortium, 2005; Cybulska and Vossen, 2014; Ellis et al., 2015, 2016; Getman et al., 2017; Wang et al., 2022). There are a few datasets for other more specific domains, such as Twitter (Ritter et al., 2012), literature(Sims et al., 2019) and biomedical texts(Thompson et al., 2009), but these datasets often annotate individual event mentions only and lack event coreference information.\nIn this paper, we present the first dataset for the legal domain, LegalCore, which has been annotated with comprehensive event and event coreference information. The legal contract documents in this dataset are several times longer than news articles, with an average length of around 2.5k tokens per document. LegalCore contains 100 legal contract documents and around 250k tokens, comparable in size to the main event-annotated datasets, such as ACE 2005 (Walker and Consortium, 2005) and ECB+ (Cybulska and Vossen, 2014).\nAs illustrated in Figure 2, we first annotate individual event words as event mentions in each document, without constraints on event type. As shown by the document excerpt example, legal documents have dense event mentions, and there are 23,183 event mentions annotated in total in our dataset, roughly one event word in every ten tokens of a legal document.\nNext, we annotate event coreference relations. But as each legal document is so long and usually contains a brief introductory section followed by a series of numbered sections, it is hard to identify all the coreferential mentions of an event by going through the document once. As shown in the example document of Figure 2, the beginning section introduces the main involved parties and the theme of the contract, and the following numbered sections\nfurther specify rules or elaborate on the main terminology used in the contract. Each section often cites other sections for references. Therefore, we design a two-pass procedure for annotating event coreference relations and recover complete event clusters in a hierarchical fashion, where we first annotate local event coreference relations within each section and then annotate non-local event coreference relations spanning across sections.\nAfter completing both passes of event coreference annotations, there are 853 event clusters identified in our datasets, where 653 event clusters are local clusters and only contain event mentions within one section, and the remaining 200 event clusters contain event mentions from two or more sections. Among the 200 non-local event clusters, a little over half of them span across two sections and the remaining span across three or more sections, in particular, about 30 (15%) non-local event clusters span across six or more sections. Accordingly, the legal documents feature both short-distance and super long-distance coreference links between event mentions. If we measure the distance of coreference links between an event mention and its nearest antecedent mention as the number of tokens between the two event mentions, we observe over half of such coreference links have a distance less than 50 tokens, meanwhile, we observe a significant portion of super long coreference links covering more than 600 or even more than 1000 tokens.\nIn addition, with the increasing popularity of LLM, we benchmark the performance of the main LLMs in LegalCore, including open-source and proprietary ones, such as Llama-3.1 (Grattafiori et al., 2024) and GPT-4 (OpenAI et al., 2024). Despite strong performance of LLMs in many tasks (Chang et al., 2023; Du et al., 2024; Wei et al., 2023a,b, 2022), our experiments show that LegalCore poses significant challenges for both open-source and proprietary LLMs, and the performance of LLMs on both event identification and event coreference resolution are still worse than a supervised baseline.\nTo summarize, our contributions are mainly two:\n\u2022 We introduce LegalCore, the first dataset for the legal domain that has been annotated with comprehensive event and event coreference information.\n\u2022 We benchmark mainstream LLMs performance on LegalCore, finding that both event detection and event coreference remain challenging to LLMs."}, {"title": "Related Works", "content": "Event Identification There are many existing datasets annotated with event information, some require the models to identify event mentions and classify them into specific event types (Ellis et al., 2015, 2016; Getman et al., 2017; Wang et al., 2020; Walker and Consortium, 2005), while others require the models to extract event mentions of all types (Allan, 2002; Minard et al., 2016; Araki and Mitamura, 2018; Sims et al., 2019; Liu et al., 2019a), and LegalCore falls into the later category. Most of the previous works (Walker and Consortium, 2005; Ellis et al., 2015, 2016; Getman et al., 2017; Cybulska and Vossen, 2014; Pustejovsky et al., 2003) focus on news domains, but a few datasets have been developed for other specific domains, for example Twitter (Ritter et al., 2012; Guo et al., 2013; Chen et al., 2022), literature (Sims et al., 2019), finance (Chen et al., 2021), and biomedical texts (Pyysalo et al., 2007; Kim et al., 2007; Thompson et al., 2009; Buyko et al., 2010; N\u00e9dellec et al., 2013), but none of the previous datasets consider the legal domain. To the best of our knowledge, LegalCore is the first dataset for the legal domain annotated with event information.\nEvent Coreference Resolution Event Coreference Resolution is a key and challenging NLP task, and many event coreference datasets have been constructed. Previous datasets for event coreference are mostly based on news articles (Ellis et al., 2015, 2016; Getman et al., 2017; Cybulska and Vossen, 2014; Pradhan et al., 2007). The most recent large dataset, MAVEN-ERE (Wang et al., 2022), contains annotations of event coreference relations as well as other event relations such as temporal relations and causal relations, but the annotated Wikipedia articles are still mostly news documents. The event coreference relations annotated in LegalCore are expected to enable more studies on event coreference resolution for the legal domain and be highly valuable for developing real-world applications for this important domain."}, {"title": "Dataset Construction", "content": "The LegalCore dataset contains 100 legal contract documents that were selected from the CUAD dataset (Hendrycks et al., 2021), a public dataset used for identifying key clauses in legal contracts. We will publish our annotated dataset.\nThe dataset construction is done in three phases, namely annotating event mentions, identifying local coreference links, and identifying non-local coreference links, as shown in Figure 2. We follow the annotation guidelines of O'Gorman et al. (2016) for performing the first two phases of annotations, we create our own annotation guidelines for annotating non-local coreference links, and we will publish our data annotation guidelines."}, {"title": "Event Mentions Annotation", "content": "Task Description We first annotate event mentions in each legal contract document. We define an event as any occurrence, action, process, or state that belongs on a timeline and can take various syntactic forms, including verbs, nominalizations, nouns, or adjectives, as outlined by O'Gorman et al. (2016).\nAnnotation We follow the annotation guidelines of O'Gorman et al. (2016) and have two annotators annotate event mentions. Given the plain text of a legal document, the annotators are asked to identify all event mentions that occur in the document. The annotators are trained and instructed that event determination should rely solely on semantics-whether something belongs on a timeline. Syntactic form is secondary and considered later as an event can take any syntactic realization. At this stage, we only focus on the semantic aspect and determine whether the words represent changes, transitions, or states occurring in the world. To ensure the quality of the annotated data, we sample five documents and asked both annotators to identify all the event mentions. The inter-annotator's agreement is 80.2% (Cohen's kappa). In total, we have 23,183 event mentions annotated in this dataset."}, {"title": "Local Coreference Annotation", "content": "Task Description Event coreference relations link event mentions referring to the same event in space and time. The coreference relation is both symmetrical and transitive. In this stage, the annotators are only asked to annotated local coreference relations. A coreference link is considered local if both event mentions are within the same section of a legal document. As shown in the example of Figure 2, sections in a legal document usually have a section number, like 3. Payment.\nAnnotation We follow the annotation guidelines of O'Gorman et al. (2016) and have two trained annotators annotate local event coreference. Given a legal document with annotated event mentions, the annotators are asked to identify coreference links within the same section. An event mention should only be linked to its nearest antecedent mention if any. To ensure the quality of the annotated data,"}, {"title": "Non-local Coreference Annotation", "content": "Task Description For this stage, the annotators are only asked to annotated non-local coreference relations. A coreference link is considered non-local if the two event mentions are within different sections of a document. Non-local coreference links are essentially cross-section links.\nAnnotation We create our own annotation guidelines and have two annotators annotate non-local event coreference relations. Given a legal document with annotated event mentions, the annotators are asked to identify coreference links cross different sections. To ensure the quality of the annotated data, we sample five documents and asked both annotators identify cross-section coreference links. The inter-annotator agreement is 74.8% (Cohen's kappa)."}, {"title": "Basic Statistics of Non-singleton Events", "content": "As shown in table 1, we have 853 non-singleton events in the dataset. Among them, 653 event clusters are local coreference clusters with an average of 2.5 event mentions per cluster. For the remaining 200 non-local coreference clusters with an average of 4.4 mentions per cluster. Overall, the non-singleton events have 2.9 mentions per event.\nFigure 3 further shows the distribution of non-local coreference clusters based on the number of sections each cluster span across. A little over half of the non-local coreference clusters span across two sections and the remaining span across three or more sections, in particular, about 30 (15%) non-local event clusters span across six or more sections, and these long span coreference clusters can pose greater challenges to event coreference resolution."}, {"title": "Dataset Analysis", "content": "Statistics Comparing to Existing Datasets\ndocuments in LegalCore are several times longer and contain significant more event mentions per document."}, {"title": "Distance between Coreferential Mentions", "content": "Recognizing relations between distant event mention pairs is crucial for discourse-level document comprehension (Naik et al., 2019), yet capturing long-range dependencies remains a persistent challenge for NLP models. Therefore, we examine the distance distributions of annotated event relations in LegalCore and compare them with the most widely used existing datasets in Table 3. The distance of a coreference link is measured as the number of tokens between an event mention and its nearest antecedent mention.\nInterestingly, despite having much longer document, LegalCore has similar average distances and much more coreference event links with short distance (< 50 tokens) comparing to previous datasets. This is mainly because of two reasons: First, legal documents often reference the same event multiple times within a section for clarity and consistency. Lawyers typically avoid using synonyms, instead opting for exact wording to ensure precision and reduce ambiguity. As a result, event mentions referring to the same event tend to appear close together in order to reinforce the document's accuracy and legal integrity. Second, LegalCore is annotated with denser event mentions, averaging 10.8 tokens between mentions compared to 24.9 tokens in TAC KBP. This dense event mention annotation, combined with our thorough event coreference annotation schema-which annotates local event coreference before addressing non-local cases -results in most event coreference links occurring over short distances.\nOn the other hand, our dataset contains a significant number of super long-distance coreference links. We zoom in the long-distance coreference links where the event mentions are over 200 tokens apart, and show the histogram of long distance coreference links in Figure 4. We can see that a significant number of super long-distance coreference links, where the two event mentions have over 400 tokens or even over 600 tokens in between, dozens of coreference links have even over 1000 tokens in between. These coreference links with a super long distance pose great challenges to event coreference resolution models."}, {"title": "Experiments and Analysis", "content": "Experiment Setup\nBenchmark LLMS We also benchmark the performance of mainstream LLMs on LegalCore. The following models are used in the different event-related tasks: Llama-3.1 (Grattafiori et al., 2024), Mistral-Nemo (Mistral AI, 2024), Qwen-2.5 (Qwen et al., 2025), and GPT-4 (OpenAI et al., 2024). The exact model versions can be found in Appendix A.2.\nFor event identification, we ask LLMs to annotate event mentions sentence by sentence. For event coreference, we ask LLMs to find all event mentions at once due to the existence of non-local coreference chains, as one mention can appear at the beginning of a document while the other mention appears at the very end.\nWe also evaluate LLMs in an end-to-end fashion and reprot the final performance, where we perform event coreference resolution on the model identified noisy event mentions.\nSupervised Baseline We built a supervised baseline for both event identification and event coreference resolution. For event identification, we refer to Hicke and Mimno (2024) and fine-tune T-5 models (Raffel et al., 2023) to take a raw sentence as the input and output the same sentence marked with event mentions. Table 4 shows an example of the input and output pair. We experimented with T-5 models of different sizes: small, base, and large.\nFor event coreference resolution, we follow Wang et al. (2022) and adopt a pre-trained RoBERTa-base model (Liu et al., 2019b) for identifying event coreference relations. Firstly, the whole document is encoded using RoBERTa-base. Next, the contextualized representations at the positions of each event mentions are extracted from the encoded document. Finally. the extracted representations are then fed into a classification head in a pairwise fashion to determine wether if there is a coreference link exists between the two event mentions. We also report the end-to-end performance for the supervised baseline. All the experiments of the supervised baseline were conducted using 5-fold cross-validation.\nMetrics For event identification, we adopt the standard micro-averaged precision, recall, and F-1 metrics. Following previous works (Wei et al., 2024; Wang et al., 2022; Choubey and Huang, 2017), we evaluate event coreference resolution performance by adopting MUC(Vilain et al., 1995), B\u00b3(Bagga and Baldwin, 1998), CEAFe(Luo, 2005), and BLANC (Recasens and Hovy, 2011) metrics."}, {"title": "Event Identification Results", "content": "Table 5 shows that all LLMs significantly underperform the supervised baseline, and the latter performs almost perfectly on event identification. Furthermore, the results suggest that the primary challenge for LLMs using zero-shot prompting is their lack of awareness regarding the density of event mention annotations, as precision are much higher than recall. Therefore, we also evaluate LLMs under the one-shot and two-shot settings, and the results are reported in Table 6. Although LLM performance has improved, particularly for GPT-4, whose F1 score significantly increased from 50.7 to 77.7 in zero-shot and two-shot settings, LLMs still perform worse than the supervised baseline."}, {"title": "Event Coreference Resolution Results", "content": "Similar to what we did for event identification, we evaluate LLMs for event coreference resolution under both the zero-shot setting and the few-shot settings. Figure 5 show the results, where we report the averaged F-1 score across the four metrics for event coreference resolution. The detailed results can be found in Appendix A.4. Note that either one-shot or two-shot prompting do not significantly improve the performance of LLMs, suggesting that simply showing LLMs examples of event coreference relations does not effectively help LLMs better understand the task. The two-shot prompting performs slightly better, yields a small improvement on Mistral-Nemo and achieves comparable performance on other LLMs.\nTable 7 shows event coreference resolution results for LLMs and the supervised baseline in the two-shot setting. The supervised approach, leveraging ROBERTa-base fine-tuned on LegalCore,"}, {"title": "Local vs. Non-local Coreference Clusters", "content": "Next, we investigate how the models performance differs for local event coreference clusters and non-local event coreference clusters. We report the micro-averaged MUC scores for local and non-local coreference clusters in Table 8. We only consider MUC scores as it is the only metric among all four metrics that just consider non-singleton clusters during evaluation, which is a better indicator of model performance in this case as singleton clusters are not involved in this analysis.\nAs shown in Table 8, surprisingly, the performance of LLMs in capturing non-local (cross-section) coreference clusters is higher than their performance on local coreference clusters. This may be due to LegalCore having more local clusters and fewer non-local clusters. Since LLMS process the entire document to identify coreference relations, they may overlook local clusters, leading to lower recall as they prioritize a broader context over specific areas. In contrast, the supervised baseline is better at resolving local coreference clusters than non-local coreference clusters. We believe the main bottleneck of the supervised baseline is its context length limitation. RoBERTa-base has a maximum context length of 512 tokens, which is significantly shorter than the average document length in LegalCore. As a result, long documents are split into multiple chunks and encoded separately, making it challenging to identify coreference relations across chunks.\nWe also investigate mistakes made by LLMs and the supervised baseline model. We report the percentage of false positives (FP) and false negatives (FN) among the wrongly predicted coreference links in Table 9.\nNotice that for the supervised baseline, the majority of mistakes (63.0%) stem from FP, where the model incorrectly identifies two unrelated event mentions as coreferent. In local coreference links, FP accounts for an even larger proportion (73.3%) of errors. This may be due to the data distribution, as over half of the coreference links in LegalCore occur within 50 tokens, leading the supervised model to learn this pattern during training. For non-local coreference links, FP and FN contribute almost equally to the errors.\nFor LLMs, false positives (FP) also account for the majority of mistakes, except for GPT-4. In particular, FP makes up 84.1%, 66.7%, and 66.7% of the overall errors for Llama-3.1, Mistral-Nemo, and QWen-2.5, respectively. This suggests that LLMs tend to link irrelevant event mentions, and this issue is more pronounced in open-source models. Upon analyzing the outputs of Llama-3.1, we observed that after initially making some reasonable predictions, very quickly, the model starts to link all the event mentions together, forming a single cluster with numerous event mentions while meanwhile leaving many singleton clusters unattended. This behavior significantly increases the false positive (FP) rate, contributing to the high percentage of FP errors for Llama-3.1."}, {"title": "End-to-end Results", "content": "Table 10 presents the end-to-end results for both LLMs and the supervised baseline. Compared to event coreference results using gold event mentions (Table 7), the coreference resolution performance of LLMs decreased quickly in the end-to-end setting, this is reasonable as LLMs do not perform very well in event identification.\nIn contrast, the supervised baseline maintains high performance and is much more robust benefiting from explicit training on both event identification and coreference resolution."}, {"title": "Conclusion and Future Work", "content": "We have presented the first event dataset for the legal domain, LegalCore, which has been annotated with comprehensive event and event coreference information. We further benchmark mainstream Large Language Models (LLMs) on this dataset for both event identification and event coreference resolution tasks, and find that this dataset poses significant challenges for both open-source and proprietary LLMs.\nFor future work, we will extend the dataset to cover other event relations in legal documents, such as temporal relations and causal relations."}, {"title": "Limitations", "content": "LegalCore only contains one type of legal documents, legal contracts, there are many other types of legal documents as well, which can be further considered for event analysis. Another limitation of LegalCore is that it only covers coreference relation. The dataset can be more useful if it can be further annotated with other event relations, such as temporal, causal, and subevent relations."}, {"title": "Appendix", "content": "Hyper-parameters settings\nFor the baseline detection model, we train the model with the learning rate sets to le - 4 for the T-5 model, and the batch size sets to 4. We train the model for 100 epochs with 5-fold cross validation. For the baseline coreference model, we train the model following the hyperparameters in Wang et al. (2022). We train the model with the learning rate sets to le - 5 for the ROBERTa model, the learning rate sets to le - 5 for the classification head, and the batch size sets to 4. We train the model for 200 epochs with 5-fold cross validation. All training are conducted on A-100 GPUs.\nLLM Version\nWe list the detail information of the LLMs evaluated in the paper below:\n\u2022 Llama-3.1: We use the meta-llama/Llama-3.1-8B-Instruct\u00b2 from Huggingface 3\n\u2022 Mistral-Nemo: We use the mistralai/Mistral-Nemo-Instruct-2407 from Huggingface.\n\u2022 QWen-14b: We use the Qwen/Qwen2.5-14B-Instruct from Huggingface.\n\u2022 GPT-4-Turbo: We use the gpt-4-turbo accessed through OpenAI API 4. The experiments are conducted within the window between Jan 15th, 2025 and Feb 15th, 2025.\nAnnotators\nAll annotators are graduate student from research labs of universities who have great experiences in the field of Natural Language Processing. No additional payments to students are given other than graduate research assistant-ship the students already have.\nLLMs Few-shot Performance\nWe report the percentage of false positives (FP) and false negatives (FN) of the predicted links in Table 9 where two-shot prompting is used for LLMs. We report the zero-shot and one-shot performance of LLMs on event coreference in Table 12. We report the zero-shot and one-shot performance of LLMs with end-to-end setting in Table 13."}]}