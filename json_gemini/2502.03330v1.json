{"title": "Controllable GUI Exploration", "authors": ["ARYAN GARG", "YUE JIANG", "ANTTI OULASVIRTA"], "abstract": "We present a diffusion-based approach to the low-effort generation of interface sketches. It breaks new ground by allowing flexible control of the generation process via three types of inputs: A) prompts, B) wireframes, and C) visual flows. The designer can provide any combination of these as input at any level of detail, and will get a diverse gallery of low-fidelity solutions in response.", "sections": [{"title": "1 INTRODUCTION", "content": "The early stages of a GUI design project are critical for its eventual success. During the early stages, designers produce low-fidelity solutions in an attempt to explore the design space. Low-fidelity designs often specify, partially or in an underspecified way, key aspects of the layout, widgets, graphics etc. [28]. Designers produce them to envision how users are supported in their tasks and to learn about the trade-offs of different approaches [8, 48]. The decisions that are made at this stage are critical, because they may be hard to change later on.\nTo assist designers in this stage, computational methods should help them produce large numbers of diverse solutions to explore. Research suggests that producing large numbers of low-fidelity ideas is beneficial for creativity at this stage [1, 28, 38]. However, design fixation makes it hard for designers to generate entirely novel ideas [15]. However, existing design tools often fail to support exploration. They require designers to engage at a higher level of detail than necessary, for example, to specify which specific widgets are used and where they are exactly positioned on the frame. To support creative exploration, designers need tools that allow them to rapidly explore a high number of solutions, flexibly and with minimum effort.\nRecent advances in generative AI have created optimism that this gap could be closed. While previous work focused on retrieval of GUIs based on a query [11, 27, 32], this approach was limited to the samples available in the database. Generative AI is a way to circumvent this requirement by generating solutions to a given prompt. However, while there are successes for natural images, generative AI-based tools are much less adopted in GUI design [31, 55]. We believe that the root cause is that text-based prompting is not a natural means for a designer to express ideas that are inherently visuo-spatial by nature. Designers need to explore options by sketching: by loosely specifying where some key elements are located [28]. Designers also need to explore the functional and communicative aspects of a design but without being asked to specify lots of detail manually.\nDesigners should also be able to express what kinds of impact they want their design to have on users. An important impact concerns visual flow: the order in which users look at a UI. Improving visual flow can enhance user engagement and guide user behavior [9, 19, 20, 41, 45, 49]. Thus, designers also care about how GUIs guide users' attention to task-relevant items [20, 41, 45]. However, understanding how users engage with a design normally requires empirical studies. To our best knowledge, no prior work has considered user-centered visual flow during the GUI design process.\nIn this short paper, we propose a diffusion-based approach to controllable generation of early-stage GUIs. As shown in Figure 1, our model breaks new ground by allowing flexible control of the generation process via three types of early-stage inputs: A) prompts, B) wireframes, and C) visual flows (controlling where users should look). To our knowledge, no previous work has utilized scanpaths for GUI generation control, and no existing datasets are available to train models for this purpose. While previous work on generative approaches in this space has produced layouts"}, {"title": "2 RELATED WORK", "content": "This section highlights the challenges faced by GUI generation models, an overview of diffusion models, and the limitations of existing GUI datasets,."}, {"title": "2.1 GUI Generation", "content": "Exploring GUI alternatives plays an important role in GUI design. By comparing GUI alternatives explicitly, designers can offer stronger critiques and make more informed decisions [8, 16, 21-23, 48].\nGenerative models should support flexible input to enable effective experimentation. Although design literature often advocates for the use of rough sketches to explore design ideas [1, 28], sketching alone can be limiting. Designers may face challenges such as fixation, which can constrain the generation of novel solutions [15]. Previous constraint systems have facilitated the exploration of alternatives through the application of constraints [17, 24-26, 47]. However, these systems typically require manual detailed specifications early in the design process, which can be cumbersome and time-consuming."}, {"title": "2.2 Diffusion Models", "content": "In light of the limitations identified in current GUI generation approaches, recent advancements in diffusion models present a potentially promising way to support flexible inputs and diverse outputs for GUI design. Recent advancements in diffusion models have dramatically improved image generation, as demonstrated by models like GLIDE [35], DALL-E2 [37], Imagen [42], and Latent Diffusion Models [39]. These diffusion-based approaches offer stable training and handle multi-modal conditional generation without the need for modality-specific objectives. In particular, Stable Diffusion [39],"}, {"title": "3 DATASET", "content": "Prior Datasets. No existing datasets can be directly used to train a model conditioned on prompts, wireframes, and visual flow directions. Several datasets have been collected to support GUI tasks. The AMP dataset, comprising 77,000 high-quality screens from 4,068 iOS apps with human annotations [56], is not publicly available.\nOn the other hand, the largest publicly available dataset, Rico [6], includes 72,000 app screens from 9,700 Android apps and has been a primary resource for GUI understanding despite its inherent noise. To address its limitation, the Clay dataset [30] was created by denoising Rico using a pipeline of automated machine learning models and human annotators to provide more accurate element labels. Enrico [29] further cleaned and annotated Rico but ultimately"}, {"title": "4 METHOD", "content": "To enable the rapid exploration of diverse GUI options, we propose integrating a diffusion model with different modular adapters designed to control both local and global GUI properties. Specifically, we employ the ControlNet adapter to manage local properties (e.g., the position and type of GUI elements), as it encourages close alignment with the input wireframe. For global properties (e.g., visual flow), we propose utilizing a Flow Adapter, which provides a more global guiding signal for GUI generation."}, {"title": "4.1 Problem Formulation", "content": "We formulate the GUI exploration task as a controllable GUI generation problem, allowing designers to flexibly guide the process using three types of inputs: A) prompts, B) wireframes, and C) visual flows. For visual flows, we currently support two options for designers: they can either 1) provide a sample GUI to encourage the model to generate GUIs with similar visual flow, or 2) specify a flow direction, indicating where the flow should conclude (bottom left or bottom right). Designers can provide any combination of these inputs at any level of detail and receive a diverse gallery of low-fidelity solutions.\nWe apply classifier-free guidance (CFG) [13], a technique used in generative models, to balance fidelity to conditioning inputs and output diversity by mixing conditioned and unconditioned model outputs. For visual flow, we need to encourage the generated visual flow to align with the input visual flow. Thus, we introduce two objective terms: the classifier-free guidance loss (\\(L_{cfg}\\)) and the flow consistency loss (\\(L_{flow}\\)). The classifier-free guidance loss ensures the generated GUIs align with the provided inputs, while the flow consistency loss encourages the consistency between the desired visual flow and the visual flow of the generated GUIs. Thus, the objective function is\n\\[L(z, C_w, C_p, C_f) = L_{cfg}(z, C_w, C_p, C_v) + L_{flow}(z, C_f),\\]\nwhere \\(z\\) is the generated GUI, \\(c_w\\), \\(c_p\\), and \\(c_f\\) represent the input conditions for the wireframe, prompt, and visual flow, respectively."}, {"title": "4.2 Controllable GUI Generation", "content": "Our diffusion-based model generates GUIs conditioned on both local (e.g., position and type of elements) and global properties (e.g., visual flow). As illustrated in Figure 2, the backbone of our model is based on Stable Diffusion [39], which employs a U-Net architecture [40] consisting of an encoder E, a middle block M, and a skip-connected decoder D. The text prompt is encoded by a CLIP [36] text encoder and feeds into the diffusion model via cross-attention layers."}, {"title": "4.2.1 ControlNet Adapter for Local Properties", "content": "Recent advancements in controllable image generation have shown that additional networks can be integrated into existing text-to-image diffusion models to better guide the generation process [31, 34, 55]. Inspired by ControlNet [55], we create a trainable copy of Stable Diffusion's encoder and middle block, followed by a decoder with zero-convolution layers. The weights and biases of these zero-convolution layers are initialized to zero, allowing the adapter to efficiently capture local properties, ensuring the generated GUI aligns with input wireframes. In this framework, wireframe features are concatenated with text features to guide the generation process."}, {"title": "4.2.2 Flow Adapter for Global Properties", "content": "The cross-attention mechanism has shown effective results for enhancing models' global control without explicit spatial guidance [53, 57]. Therefore, we adopt a cross-attention mechanism to process visual flow features, adding an additional cross-attention layer in each layer of the diffusion model for this purpose.\nFlow Encoder. No existing encoders are specifically designed to handle visual flow. However, EyeFormer [18], a state-of-the-art model for scanpath prediction on GUIs, encodes GUI images and decodes the latent representations into scanpaths. We repurpose EyeFormer's decoder to train our flow encoder, using GUI scanpaths as input during training. During inference, we offer two options for designers: 1) Sample-based flow generation: If the designer provides"}, {"title": "4.2.3 Training Process", "content": "We train our model using classifier-free guidance [13] and DDIM [44] because these methods offer robust control over the generation process while ensuring high fidelity in the generated GUIs. Stable Diffusion operates by progressively adding noise to data and learning to reverse this process. Classifier-free guidance allows us to balance the diversity of the generated GUIs and alignment with input conditions without needing a separate classifier, reducing complexity and enhancing flexibility in our model. Similarly, DDIM provides a deterministic way to denoise and refine outputs, producing results that are closely aligned with the desired GUI specifications. Together, these techniques help maintain the quality of generated GUIs while ensuring they meet input requirements."}, {"title": "5 RESULTS", "content": "In this section, we demonstrate that our model efficiently explores a diverse range of GUI alternatives that closely match the specified input conditions, outperforming other models in alignment and efficiency. More results are shown in the Supplementary Materials."}, {"title": "5.0.1 Qualitative Results", "content": "To evaluate our approach, we qualitatively demonstrate that our model is capable of generating diverse low-fidelity GUI samples from various input combinations. Specifically, we aim to assess: 1) whether the model can produce diverse webpage and mobile GUI results solely based on the given prompt; 2) whether it can"}, {"title": "6 DISCUSSION AND CONCLUSION", "content": "Our results suggest that diffusion-based models are suitable for the rapid and flexible exploration of low-fidelity GUI generation. Currently, no available diffusion model is capable of producing GUIs with high-quality text and graphics, but this limitation does not impede their use for the exploration of early-stage design ideas. There the focus is more on generating a broad range of 'good enough' possibilities. Specifically, we demonstrated that the probabilistic nature of diffusion models allows for generating a multitude of diverse GUI ideas efficiently.\nBefore this paper, it was an open question of how to design generative models such that designers do not need to over-specify the input. By extending conditional diffusion to handle both wireframes and visual flow, we offer a low-effort approach to designers that simplifies interaction with diffusion models. Moreover, the inclusion of specialized adapters for considering local (e.g., the position and type of GUI elements) and global (e.g., visual flow) design properties enhances control over GUI characteristics, allowing designers to focus on broad exploration with minimal effort. We view this model as a step forward in creating user-adapted GUI designs with generated models, as it integrates GUI properties with user-centered interactions, such as eye movement, throughout the GUI design exploration process.\nLimitations. We acknowledge the following limitations. First, we cannot generate valid texts or labels for GUIs; future work could focus on producing meaningful text with appropriate font design. Second, we face challenges in generating realistic human faces, with some outputs appearing distorted. Increasing the dataset of human faces could improve the model's training. Third, the generated resolution is low since we trained on 256x256-resolution images. Future work could benefit from better computing power to train on higher-resolution images. Fourth, our model currently does not support highly specific requirements, such as generating a Margarita cocktail on a website. Additionally, our visual flow results lack finer control. We used the model EyeFormer [18] for scanpath prediction, however, it often biases scanpaths, starting from the center and moving toward the top left and right, limiting control over specific element emphasis. Although these patterns reflect natural eye movements, GUI designers may require more flexibility to direct user attention. Lastly, our current method does not support iterative design, which is important for early-stage prototyping, as each generation results in entirely new GUIs. Future work could improve the diffusion model by incorporating conditioning on the outputs from previous iterations. This could be done by using the current design as input to generate subsequent iterations. By conditioning on previous results, the model could produce variations that build upon existing designs, rather than starting anew each time."}, {"title": "9", "content": "During training, the Stable Diffusion components remain frozen, while our adapters are trained to optimize the objective. For a given timestep t and input conditions C = {cw (wireframe), cp (prompt), and cf (visual flow)}, the model learns to predict the noise e added to the noisy image z\u0142 using the loss function:\n\\[L_{cfg} (z, C) = E_{z,t,C,\\epsilon~N(0,1)} [||\\epsilon \u2013 \\epsilon_{\\theta}(z_t, t, C) ||^2],\\]\nwhere z = zo is the final GUI predicted by denoising zt over timestep t.\nWe train each adapter independently. Specifically, the ControlNet adapter is trained using both prompt and wireframe inputs, while the Flow Adapter is trained on prompt and visual flow inputs. During training, we apply a 50% dropout rate to each input condition, which encourages the model to handle various combinations of inputs. No fine-tuning across all three types of inputs is necessary.\nControlNet. The ControlNet adapter is trained solely with the classifier-free guidance loss. Thus, the objective function is\n\\[L_{ControlNet} = L_{cfg} (z, C_w, c_p) = E_{z_t,t,c_w,c_p,\\epsilon~N(0,1)} [||\\epsilon \u2013 \\epsilon_{\\\u0189} (z_t, t, c_w, c_p) ||^2],\\]\nFlow Adapter. The Flow Adapter is trained using both classifier-free guidance loss and a flow consistency objective. The latter ensures that the cross-attention layers guide generation toward a latent subspace aligned with the desired visual flow.\nTo encourage the consistency between the visual flow of the generated GUIs and the input visual flow specification, we apply Dynamic Time Warping (DTW) [43], a standard metric used to measure the similarity between two temporal sequences, even when they differ in length. DTW identifies the optimal match between the sequences and computes the distance between them. However, since the original DTW is not differentiable, it cannot be directly used to optimize deep learning models. To address this, we employ softDTW [5], a differentiable version of DTW, to optimize our model. Since the only output of the network is a synthesized GUI (z) with no ground-truth scanpath, we use EyeFormer [18] to compute a representative ground truth (EyeFromer(z) ~ \u0109). The loss is defined as:\n\\[L_{flow} (z, c_f) = softDTW(EyeFromer(z, c_f)))\\]\nThus, the total loss for the Flow Adapter is:\n\\[L_{FlowAdapter} = L_{cfg} (z, c_p, c_v) + L_{flow} (z, c_f)\\]\n\\[ = E_{z_t,t,c_p,c_v,\\epsilon~N(0,1)} [||\\epsilon \u2013 \\epsilon_{\\\u0189} (z_t, t, C_p, C_v) ||^3] + softDTW(EyeFromer(z, c_f))).\\]"}]}