{"title": "Realizing Unaligned Block-wise Pruning for DNN Acceleration on Mobile Devices", "authors": ["Hayun Lee", "Dongkun Shin"], "abstract": "With the recent proliferation of on-device AI, there is an increasing need to run computationally intensive DNNs directly on mobile devices. However, the limited computing and memory resources of these devices necessitate effective pruning techniques. Block-wise pruning is promising due to its low accuracy drop tradeoff for speedup gains, but it requires block positions to be aligned with block size, hindering optimal position selection to minimize model accuracy drop. Unaligned block pruning (UBP) addresses this by allowing blocks to be selected at arbitrary positions, yet its practical use is limited by a time-consuming optimal block se- lection algorithm and lack of efficient inference kernels. In this paper, we propose a pseudo-optimal yet fast block selection algorithm called Block Expansion and Division (BED), which can be integrated into an iterative model training process. Additionally, we introduce an efficient inference kernel implementation for mo- bile devices, enabling a UBP-based model to achieve similar latency to a DNN model compressed by aligned block pruning. We demonstrate the superiority of our techniques on a real mobile phone with MobileNet and ResNet models.", "sections": [{"title": "Introduction", "content": "Deep neural networks (DNNs) have emerged as powerful machine learning techniques, demonstrating exceptional performance across various applications. Mobile devices particularly benefit from DNN applications due to their ability to process user-generated content such as photos, videos, voice recordings, and data from various sensors. Modern mobile devices increasingly incorporate AI functionalities like scene detection and image noise reduction [1-3].\nHowever, DNN models often require substantial computational resources and memory, posing significant challenges for mobile devices with limited capabilities. Although cloud computing presents a potential solution, it raises concerns regarding privacy, network latency, and connectivity dependence. Consequently, there is a growing interest in deploying DNNs directly on mobile devices, known as on-device AI. To support this trend, manufacturers are developing AI accelerators like GPUs or NPUs and optimizing DNN technologies. Despite these advancements, scenarios still exist where only CPU acceleration is available on mobile devices.\nExtensive research focuses on optimizing DNN models for mobile deployment, leading to the development of mobile-friendly, compact models such as MobileNet [4-6], which deliver respectable performance with reduced computational demand and memory usage. Model compression techniques such as network pruning [7-16] and quantization [17\u201319] have been explored to further decrease these demands while maintaining accuracy.\nGiven the inherent redundancy in neural network models, pruning plays a crucial role by eliminating non-essential weight parameters, thus transforming dense networks into sparse ones. This transforma-"}, {"title": "Related Work", "content": "Pruning. Pruning techniques in neural networks are essential for reducing model size and com- putational complexity, which is crucial for deployment on resource-constrained devices. These techniques are typically categorized into channel-wise, element-wise, and block-wise pruning, each offering distinct trade-offs between efficiency and accuracy. Channel-wise pruning [7\u20139] removes entire channels from the network. This method maintains the convolutional structure, facilitating the continued use of efficient inference libraries. However, channel-wise pruning may significantly impact accuracy due to the coarse granularity of the reductions. Element-wise pruning [10, 11] targets individual weights to create fine-grained sparsity. This approach preserves accuracy even at high sparsity levels, but the resulting irregular memory access patterns often lead to only modest"}, {"title": "Unaligned Block-wise Pruning and Challenges", "content": "In this section, we establish the 1\u00d7N pruning pattern as the baseline for our study on Aligned Block-wise Pruning (ABP). This pattern takes full advantage of the NEON SIMD instruction set provided by the ARM architecture, facilitating efficient computational operations. By maintaining contiguous non-zero weights, the 1\u00d7N pattern aligns with the parallel processing capabilities of SIMD, enabling efficient vector operations that are crucial for mobile computing.\nBuilding on this, we define Unaligned Block-wise Pruning (UBP), an extension of the 1\u00d7N pattern. UBP aims to harness the computational efficiency of SIMD while potentially improving model accuracy by allowing more flexibility in the placement of non-zero weights across the network.\nFollowing this, we will introduce the challenges associated with UBP. These include the complexity of the block selection algorithm, which can significantly prolong the training process, and the practical difficulties in deploying efficient inference kernels, which can limit UBP's applicability in real-world scenarios."}, {"title": "Unaligned 1\u00d7N Block-wise Pruning", "content": "Unaligned Block-wise Pruning (UBP) adapts the 1\u00d7N pruning pattern, where one block in the convolution layer weights is defined as a sequence of N consecutive output kernels sharing the same input channel index. For each convolution layer l, with weights $W^l \\in R^{c_{l+1}\\times c_l \\times h_l \\times w_l}$, blocks of size $(N, 1, h_\u03b9, \u03c9_\u03b9)$ are selected based on the pruning rate pi, and weights outside these blocks are pruned. Here, $c_l$, $c_{l+1}$, $h_l$, and $w_l$ denote the input channels, output channels, kernel height, and kernel width of layer l, respectively.\nThe starting position of a block can be any output channel and input channel, allowing for a theoretical number of blocks per layer $b_l = C_{l+1} \\times C_l$. The number of selected blocks is calculated as $m_l = [b_l \\cdot (1 \u2013 \u03c1_l)/N]$. Each block starting at the i-th output channel and the j-th input channel is indexed as k (where k = i + $c_{l+1}$ \u00b7 j). The importance score $S_k^l$ of the k-th block is computed as follows:\n$S_k^l = \\begin{cases} F(W_{i:i+N,j}^l) & \\text{if } i + N-1 < C_{l+1} \\\\ -\\infty & \\text{otherwise} \\end{cases}$\nwhere F(\u00b7) is the function used to calculate the importance score, typically the l\u2081 norm in this study. If blocks that exceed the boundaries of the weights are not eligible for selection (e.g., starting output channel index i exceeds $C_{l+1}$ \u2013 N), the score is set to \u2013\u221e to exclude these blocks in subsequent calculations.\nThe mask $M_k^l \\in {0, 1}$ for the k-th block is set to 1 when the block is selected and 0 otherwise. This mask ensures that only up to one out of every consecutive N blocks is selected, preventing overlap"}, {"title": "Challenge 1: Time-consuming Optimal Block Selection Algorithm", "content": "Greedy block selection. The simplest method for block selection in block-wise pruning is the greedy approach, where blocks are sorted by their importance scores $S_k^l$ and the highest scoring blocks are selected first. The time complexity of this method is primarily determined by the sorting process, which is O($b_l$ log $b_l$). In UBP, when a k-th block is selected, the scores of any overlapping blocks are set to \u2013\u221e to prevent their selection in subsequent steps:\n$S_{k+n}^l = -\\infty, \\quad \\forall -N < n < N$\nAlthough slower than ABP's block selection, where selecting one block does not affect the selection of others, this process can still be executed relatively quickly. However, it inherently limits the ability to find the optimal solution because selected blocks generate non-selectable regions, as defined in Eq. 2.\nOptimal block selection. A recently proposed dynamic programming-based optimal block selection algorithm [23] considers all candidate blocks to find the optimal solution for UBP. When optimally selecting n blocks out of the first k, the importance score and block index set are denoted as $T_{k,n}^l$ and $I_{k,n}^l$, respectively. To determine $T_{k,n}^l$, one must decide whether selecting or not selecting the k-th block is beneficial. If the k-th block is selected, the optimal score is obtained by adding the score of the k-th block $S_k^l$ to the optimal score of selecting n \u2013 1 blocks out of the k \u2013 N non-overlapping blocks, $T_{k-N,n-1}^l$. Conversely, if the k-th block is not selected, the optimal score is the same as the optimal score of selecting n blocks from the first k \u2212 1 blocks, $T_{k-1,n}^l$. The index set $I_{k,n}^l$ that satisfies $S_k^l$ is updated accordingly:\n$T_{k.n}^l = max(T_{k-N,n-1}^l + S_k^l, T_{k-1,n}^l), I_{k,n}^l = \\begin{cases} I_{k-N,n-1}^l \\cup \\{k\\} & \\text{if } T_{k,n}^l > T_{k-1,n}^l \\\\ I_{k-1,n}^l & \\text{otherwise} \\end{cases}$\nThe ultimate goal is to determine the optimal set of block indices $I_{b_l,m_l}^l$ for selecting $m_l$ blocks out of $b_l$. Due to the iterative nature of this process, the time complexity is O($b_l \\cdot m_l^2$), and as the number of blocks increases, the computation time grows exponentially, making it impractical for training. This complexity contrasts with gradual pruning [26], ADMM [27, 15], and Grow-and-Prune [28] techniques, which require continuous block selection unlike one-shot pruning. For instance, pruning a MobileNetV1 model with N = 4 and a target sparsity of 90% on the ImageNet dataset takes about"}, {"title": "Challenge 2: Need for Efficient Inference Kernels", "content": "Overlapped Output Tiles Problem. In computational kernels, tiling is commonly used to effi- ciently leverage data locality, where increasing the size of tiles can improve performance due to enhanced data reuse. In block-wise pruning, aligning the tile size with the block size can further boost kernel performance. However, Unaligned Block Pruning (UBP) introduces the challenge of overlapping output tiles, an issue that does not occur with Aligned Block Pruning (ABP).\nNa\u00efve Implementation. In the basic implementation of Unaligned Block-wise Pruning (UBP), only the first row's output vector is saved in memory after each tile, with subsequent vectors shifted among registers. For example, post Tile 0 processing, only Ro is stored at memory location Oo, while results in R\u2081 and R2 are moved to Ro and R1. This shifting process, repeated for each tile, leads to increased overhead from frequent register transfers, particularly as block size and target sparsity grow. This results in performance inefficiencies compared to ABP kernels. Therefore, optimizing UBP requires developing an inference kernel that retains ABP's tile size but minimizes these overheads."}, {"title": "Methodology for Realizing Unaligned Block-wise Pruning", "content": ""}, {"title": "BED: Block Expansion and Division for Training", "content": "To overcome the limitations of greedy block selection, we propose the Block Expansion and Division (BED) method. Our BED method allows for the expansion of a selected block by including adjacent elements, ensuring that the total number of elements added equals the target block size. Consequently, the size of the expanded block is always a multiple of the target block size, facilitating its subdivision"}, {"title": "WROS: Weight Rotating and Output Stationary Dataflow for Inference", "content": "To address the overlapped output tile problem in UBP kernels, we propose the Weight Rotating and Output Stationary (WROS) dataflow. Unlike the naive approach depicted in Fig.3, where output register file values are rotated at the end of each microkernel, the WROS dataflow rotates the elements of the weight block in the opposite direction. This rotation maintains the stationary state of output register file values during computation. The rotation of weights is performed offline, as illustrated in Fig.5a, where weights for the output channel index i are shifted within the weight block to the position (i mod N). In WROS dataflow, only the weight data needs transformation, while the metadata (indices, indptr) remains unchanged. In this configuration, when the starting output channel index of a tile is i, only the output vector corresponding to the (i mod N)-th row is stored in"}, {"title": "Experiments", "content": ""}, {"title": "Experimental Setup", "content": "We evaluated our methods on the ImageNet dataset using MobileNetV1 and ResNet50 models, training them on an RTX 3090 GPU. For comparison, we utilized the open-source SUBP [22] implementation, a leading technique in the 1\u00d7N pruning pattern. We followed the SUBP training hyper-parameters, referred to as Aligned Block-wise Pruning (ABP). However, for Unaligned Block- wise Pruning (UBP), we diverged by using the l\u2081 norm as the pruning criterion and implementing block regrowing at the element level based on importance scores, rather than at the block level as in SUBP. Despite these changes, UBP achieved higher accuracy than ABP.\nThe models were converted to TensorFlow Lite and tested on a Samsung Galaxy S20 [30] with a Snapdragon 865, using ARM-v8.2-A ISA, an octa-core Qualcomm Kryo 585 CPU, and a Qualcomm Adreno 650 GPU. We measured execution times with a library of sparse kernels developed under our WROS dataflow, integrated into TensorFlow Lite using the TensorFlow Lite Model Benchmark Tool2."}, {"title": "Performance of Block Selection Algorithm", "content": "To evaluate the performance of block selection methods for Unaligned Block-wise Pruning (UBP), we defined an efficacy metric based on the l\u2081 norm. Specifically, the sum of the importance scores when performing element-wise pruning (|WEP|) is set as 1.0 (maximum), and the sum when performing 1\u00d7N aligned block-wise pruning (|WABP(1\u00d7N)|) is set as 0.0 (minimum). This metric evaluates the total importance scores for each block selection method, with the efficacy for UBP (1\u00d7N) defined as:\nef ficacy = $\\frac{|W_{UBP (1\\times N)}| - |W_{ABP (1\\times N)}|}{|W_{WEP}| - |W_{ABP (1\\times N)}|}$"}, {"title": "Accuracy", "content": "Table 1 presents the accuracy results for both Aligned Block-wise Pruning (ABP) and Unaligned Block-wise Pruning (UBP) applied to MobileNetV1 and ResNet50 across sparsity levels of 70%, 80%, and 90%. Training for ABP utilized the SUBP method [22], which employs the BPAR pruning criterion. The results demonstrate that UBP consistently exhibits higher accuracy than ABP, a benefit attributable to the superior block selection method, suggesting that further enhancements could be achieved through a pruning criterion specifically tailored for UBP."}, {"title": "UBP Kernel Performance", "content": "In this subsection, we evaluate the performance of the UBP kernel with the Weight Rotating and Output Stationary (WROS) dataflow. displays the relative performance of MobileNetV1 layers with a target sparsity of 80%, comparing element-wise pruning (EP) and block-wise pruning (ABP and UBP) with block sizes of 2 and 4. EP and ABP performances were measured using XNNPACK's SpMM kernel, while UBP performance was evaluated using kernels implemented with both the na\u00efve dataflow and our proposed WROS dataflow.\nFirst, we observe that increasing block size in block-wise pruning enhances performance compared to EP due to improved data reuse in sparse operations. Secondly, the UBP kernel using WROS dataflow"}, {"title": "End-to-end Performance", "content": "illustrates the latency of models with EP, ABP, and UBP at various sparsity levels on a Galaxy S20 CPU. The experiments utilized four threads for optimal performance. Notably, minimal performance differences between ABP and UBP were observed, attributed to the WROS dataflow minimizing overhead in the UBP kernel. For MobileNetV1, EP matches the performance of the dense model at approximately 65% sparsity, whereas ABP and UBP achieve similar performance at 20% sparsity due to enhanced kernel efficiency with larger block sizes. Similar trends were observed for ResNet50. At 70% sparsity, where both MobileNetV1 and ResNet50 exhibit accuracy comparable to the dense model, UBP results in latency improvements of 2.0x and 2.5x, respectively, indicating that UBP is effectively optimized and suggests significant performance enhancements in practical applications."}, {"title": "Conclusion", "content": "This paper addresses the limited practical application of unaligned pruning due to training and inference challenges by introducing the Block Expansion and Division (BED) algorithm and the Weight Rotating and Output Stationary (WROS) dataflow. These innovations facilitate efficient training and reduce inference overhead, proving the practical viability of unaligned block-wise pruning. Our experiments demonstrate that Unaligned Block-wise Pruning (UBP) not only matches but exceeds the accuracy of leading block-wise pruning methods with comparable latency.\nWhile UBP has shown promising results, it remains a relatively unexplored field. This paper fo- cused primarily on adapting UBP for practical use rather than proposing new pruning criteria or training methods. We have demonstrated that UBP can outperform aligned block-wise pruning"}]}