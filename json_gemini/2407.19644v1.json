{"title": "Realizing Unaligned Block-wise Pruning for DNN Acceleration on Mobile Devices", "authors": ["Hayun Lee", "Dongkun Shin"], "abstract": "With the recent proliferation of on-device AI, there is an increasing need to run computationally intensive DNNs directly on mobile devices. However, the limited computing and memory resources of these devices necessitate effective pruning techniques. Block-wise pruning is promising due to its low accuracy drop tradeoff for speedup gains, but it requires block positions to be aligned with block size, hindering optimal position selection to minimize model accuracy drop. Unaligned block pruning (UBP) addresses this by allowing blocks to be selected at arbitrary positions, yet its practical use is limited by a time-consuming optimal block selection algorithm and lack of efficient inference kernels. In this paper, we propose a pseudo-optimal yet fast block selection algorithm called Block Expansion and Division (BED), which can be integrated into an iterative model training process. Additionally, we introduce an efficient inference kernel implementation for mobile devices, enabling a UBP-based model to achieve similar latency to a DNN model compressed by aligned block pruning. We demonstrate the superiority of our techniques on a real mobile phone with MobileNet and ResNet models.", "sections": [{"title": "Introduction", "content": "Deep neural networks (DNNs) have emerged as powerful machine learning techniques, demonstrating exceptional performance across various applications. Mobile devices particularly benefit from DNN applications due to their ability to process user-generated content such as photos, videos, voice recordings, and data from various sensors. Modern mobile devices increasingly incorporate AI functionalities like scene detection and image noise reduction [1-3].\nHowever, DNN models often require substantial computational resources and memory, posing significant challenges for mobile devices with limited capabilities. Although cloud computing presents a potential solution, it raises concerns regarding privacy, network latency, and connectivity dependence. Consequently, there is a growing interest in deploying DNNs directly on mobile devices, known as on-device AI. To support this trend, manufacturers are developing AI accelerators like GPUs or NPUs and optimizing DNN technologies. Despite these advancements, scenarios still exist where only CPU acceleration is available on mobile devices.\nExtensive research focuses on optimizing DNN models for mobile deployment, leading to the development of mobile-friendly, compact models such as MobileNet [4-6], which deliver respectable performance with reduced computational demand and memory usage. Model compression techniques such as network pruning [7-16] and quantization [17\u201319] have been explored to further decrease these demands while maintaining accuracy.\nGiven the inherent redundancy in neural network models, pruning plays a crucial role by eliminating non-essential weight parameters, thus transforming dense networks into sparse ones. This transforma-"}, {"title": "Related Work", "content": "Pruning. Pruning techniques in neural networks are essential for reducing model size and com-putational complexity, which is crucial for deployment on resource-constrained devices. These techniques are typically categorized into channel-wise, element-wise, and block-wise pruning, each offering distinct trade-offs between efficiency and accuracy. Channel-wise pruning [7\u20139] removes entire channels from the network. This method maintains the convolutional structure, facilitating the continued use of efficient inference libraries. However, channel-wise pruning may significantly impact accuracy due to the coarse granularity of the reductions. Element-wise pruning [10, 11] targets individual weights to create fine-grained sparsity. This approach preserves accuracy even at high sparsity levels, but the resulting irregular memory access patterns often lead to only modest"}, {"title": "Unaligned Block-wise Pruning and Challenges", "content": "In this section, we establish the 1\u00d7N pruning pattern as the baseline for our study on Aligned Block-wise Pruning (ABP). This pattern takes full advantage of the NEON SIMD instruction set provided by the ARM architecture, facilitating efficient computational operations. By maintaining contiguous non-zero weights, the 1\u00d7N pattern aligns with the parallel processing capabilities of SIMD, enabling efficient vector operations that are crucial for mobile computing.\nBuilding on this, we define Unaligned Block-wise Pruning (UBP), an extension of the 1\u00d7N pattern. UBP aims to harness the computational efficiency of SIMD while potentially improving model accuracy by allowing more flexibility in the placement of non-zero weights across the network.\nFollowing this, we will introduce the challenges associated with UBP. These include the complexity of the block selection algorithm, which can significantly prolong the training process, and the practical difficulties in deploying efficient inference kernels, which can limit UBP's applicability in real-world scenarios."}, {"title": "Unaligned 1\u00d7N Block-wise Pruning", "content": "Unaligned Block-wise Pruning (UBP) adapts the 1\u00d7N pruning pattern, where one block in the convolution layer weights is defined as a sequence of N consecutive output kernels sharing the same input channel index. For each convolution layer l, with weights \\(W^l \\in \\mathbb{R}^{c_{l+1}\\times c_l \\times h_l \\times w_l}\\), blocks of size \\((N, 1, h_\u03b9, \u03c9_\u03b9)\\) are selected based on the pruning rate \u03c1l, and weights outside these blocks are pruned. Here, cl, cl+1, h\u03b9, and w\u03b9 denote the input channels, output channels, kernel height, and kernel width of layer l, respectively.\nThe starting position of a block can be any output channel and input channel, allowing for a theoretical number of blocks per layer \\(b_\u03b9 = C_{l+1} \u00d7 C_l\\). The number of selected blocks is calculated as \\(m_\u03b9 = \\lceil b_\u03b9 \u00b7 (1 \u2212 \u03c1_\u03b9)/N\\rceil\\). Each block starting at the i-th output channel and the j-th input channel is indexed as k (where \\(k = i + c_{l+1} \u00b7 j\\)). The importance score \\(S_k^l\\) of the k-th block is computed as follows:\n\\[ S_k^l = \\begin{cases} F(W_{i:i+N, j}^l) & \\text{if } i + N-1 < C_{l+1} \\\\ -\\infty & \\text{otherwise} \\end{cases} \\tag{1} \\]\nwhere F(\u00b7) is the function used to calculate the importance score, typically the l\u2081 norm in this study. If blocks that exceed the boundaries of the weights are not eligible for selection (e.g., starting output channel index i exceeds Cl+1 \u2212 N), the score is set to \u2013\u221e to exclude these blocks in subsequent calculations.\nThe mask \\(M_k^l \\in \\{0, 1\\}\\) for the k-th block is set to 1 when the block is selected and 0 otherwise. This mask ensures that only up to one out of every consecutive N blocks is selected, preventing overlap"}, {"title": "Challenge 1: Time-consuming Optimal Block Selection Algorithm", "content": "Greedy block selection. The simplest method for block selection in block-wise pruning is the greedy approach, where blocks are sorted by their importance scores \\(S^l\\) and the highest scoring blocks are selected first. The time complexity of this method is primarily determined by the sorting process, which is O(bl log bl). In UBP, when a k-th block is selected, the scores of any overlapping blocks are set to \u2013\u221e to prevent their selection in subsequent steps:\n\\[ S_{k+n}^l = -\\infty, \\quad \\forall -N < n < N \\tag{3} \\]\nAlthough slower than ABP's block selection, where selecting one block does not affect the selection of others, this process can still be executed relatively quickly. However, it inherently limits the ability to find the optimal solution because selected blocks generate non-selectable regions, as defined in Eq. 2.\nOptimal block selection. A recently proposed dynamic programming-based optimal block selection algorithm [23] considers all candidate blocks to find the optimal solution for UBP. When optimally selecting n blocks out of the first k, the importance score and block index set are denoted as Tk,n and Ik,n, respectively. To determine Tk,n, one must decide whether selecting or not selecting the k-th block is beneficial. If the k-th block is selected, the optimal score is obtained by adding the score of the k-th block \\(S_k^l\\) to the optimal score of selecting n \u2212 1 blocks out of the k \u2212 N non-overlapping blocks, Tk\u2212N,n\u22121. Conversely, if the k-th block is not selected, the optimal score is the same as the optimal score of selecting n blocks from the first k \u2212 1 blocks, Tk\u22121,n. The index set \\(I_{k,n}^l\\) that satisfies \\(S_k^l\\) is updated accordingly:\n\\[ T_{k,n}^l = \\max\\left(T_{k-N, n-1}^l + S_k^l, T_{k-1, n}^l\\right), \\quad I_{k,n}^l = \\begin{cases} I_{k-N, n-1}^l \\cup \\{k\\} & \\text{if } T_{k,n}^l > T_{k-1, n}^l \\\\ I_{k-1, n}^l & \\text{otherwise} \\end{cases} \\tag{4} \\]\nThe ultimate goal is to determine the optimal set of block indices \\(I_{m_l}^l\\) for selecting mi blocks out of b\u03b9. Due to the iterative nature of this process, the time complexity is O(bl \u00b7 m\u00b2), and as the number of blocks increases, the computation time grows exponentially, making it impractical for training. This complexity contrasts with gradual pruning [26], ADMM [27, 15], and Grow-and-Prune [28] techniques, which require continuous block selection unlike one-shot pruning. For instance, pruning a MobileNetV1 model with N = 4 and a target sparsity of 90% on the ImageNet dataset takes about"}, {"title": "Challenge 2: Need for Efficient Inference Kernels", "content": "Overlapped Output Tiles Problem. In computational kernels, tiling is commonly used to effi-ciently leverage data locality, where increasing the size of tiles can improve performance due to enhanced data reuse. In block-wise pruning, aligning the tile size with the block size can further boost kernel performance. However, Unaligned Block Pruning (UBP) introduces the challenge of overlapping output tiles, an issue that does not occur with Aligned Block Pruning (ABP).\nNa\u00efve Implementation. In the basic implementation of Unaligned Block-wise Pruning (UBP), only the first row's output vector is saved in memory after each tile, with subsequent vectors shifted among registers. For example, post Tile 0 processing, only Ro is stored at memory location Oo, while results in R\u2081 and R2 are moved to Ro and R1. This shifting process, repeated for each tile, leads to increased overhead from frequent register transfers, particularly as block size and target sparsity grow. This results in performance inefficiencies compared to ABP kernels. Therefore, optimizing UBP requires developing an inference kernel that retains ABP's tile size but minimizes these overheads."}, {"title": "Methodology for Realizing Unaligned Block-wise Pruning", "content": "To overcome the limitations of greedy block selection, we propose the Block Expansion and Division (BED) method. Our BED method allows for the expansion of a selected block by including adjacent elements, ensuring that the total number of elements added equals the target block size. Consequently, the size of the expanded block is always a multiple of the target block size, facilitating its subdivision"}, {"title": "BED: Block Expansion and Division for Training", "content": "Algorithm 1 details the expansion process. This involves expanding blocks based on importance scores and generating an index set \u012a from the selected indices. While this method resembles the greedy block selection algorithm, it differs in its handling of the scores of adjacent blocks. Unlike the greedy approach, which sets the scores of all overlapping blocks to -\u221e to prevent their selection, BED recalculates the block score assuming no weight kernels belong to the k-th block (line 8) and excludes the scores of blocks starting from those weight kernels from the score set (line 10). The index of the k-th block is then added to the index set \u012a\u00b9. As elements are continuously removed from the score set S\u00b9, an index set R is maintained for the remaining blocks. The algorithm terminates after selecting mi blocks, returning the final index set \u012a\u00b9.\nThe indices in \u012a are not final and require reconfiguration through the division process, as described in Algorithm 2. This algorithm starts by sorting the indices in \u012a. Then, if the next expected block index next_idx is greater than the index k from \u012a\u00b9, it indicates an expanded block, and next_idx is added to the final index set I\u00b9 and increased by N (lines 6-8). If not, it signals the appearance of a new block, and the index k for this new block is added to I', setting the next expected block index next_idx to k + N. The process continues until all indices are processed, resulting in the final index set I\u00b9.\niterates mi times to identify the block with the highest importance score among all b\u03b9 blocks, resulting in a time complexity of O(bi \u00b7 m\u2081). While slower than greedy block selection, it is considerably faster than optimal block selection, making it a practical and efficient approach."}, {"title": "WROS: Weight Rotating and Output Stationary Dataflow for Inference", "content": "To address the overlapped output tile problem in UBP kernels, we propose the Weight Rotating and Output Stationary (WROS) dataflow. Unlike the naive approach depicted in Fig.3, where output register file values are rotated at the end of each microkernel, the WROS dataflow rotates the elements of the weight block in the opposite direction. This rotation maintains the stationary state of output register file values during computation. Fig.5 demonstrates the sparse format and a microkernel execution example using WROS dataflow. The rotation of weights is performed offline, as illustrated in Fig.5a, where weights for the output channel index i are shifted within the weight block to the position (i mod N). In WROS dataflow, only the weight data needs transformation, while the metadata (indices, indptr) remains unchanged. Fig.5b shows an example of performing operations using WROS dataflow with pre-rotated weights. In this configuration, when the starting output channel index of a tile is i, only the output vector corresponding to the (i mod N)-th row is stored in\nthe UBP kernel with WROS dataflow is a transformation from the existing ABP kernel, involving two main changes. First, rather than storing all output vectors in memory after each tile's computation, as traditionally done, only one output vector per tile location is stored. Second, an epilogue code is added to store the remaining N - 1 output vectors, which are still in the register file after the last tile operation, to memory. Initially, the implementation of the UBP kernel was developed based on ABP convolution kernels with a kernel size of 1 and stride of 1, utilizing kernels provided by XNNPACK. Other ABP convolution kernels were implemented using microkernels proposed by NDIRECT [29]. Building upon these, we modified the ABP kernels to create the UBP kernel, which was integrated into XNNPACK to ensure end-to-end performance.\nMoreover, to exploit multithreading parallelism, we partitioned the workload across the height dimension of the input data for both ABP and UBP kernel executions, assigning tasks to individual threads. This strategy ensures that all threads engage uniformly across the entire sparse weight matrix, avoiding issues like overlapped output tiles between threads and minimizing workload imbalance."}, {"title": "Experiments", "content": "We evaluated our methods on the ImageNet dataset using MobileNetV1 and ResNet50 models, training them on an RTX 3090 GPU. For comparison, we utilized the open-source SUBP [22] implementation, a leading technique in the 1\u00d7N pruning pattern. We followed the SUBP training hyper-parameters, referred to as Aligned Block-wise Pruning (ABP). However, for Unaligned Block-wise Pruning (UBP), we diverged by using the l\u2081 norm as the pruning criterion and implementing block regrowing at the element level based on importance scores, rather than at the block level as in SUBP. Despite these changes, UBP achieved higher accuracy than ABP.\nThe models were converted to TensorFlow Lite and tested on a Samsung Galaxy S20 [30] with a Snapdragon 865, using ARM-v8.2-A ISA, an octa-core Qualcomm Kryo 585 CPU, and a Qualcomm Adreno 650 GPU. We measured execution times with a library of sparse kernels developed under our WROS dataflow, integrated into TensorFlow Lite using the TensorFlow Lite Model Benchmark Tool2."}, {"title": "Performance of Block Selection Algorithm", "content": "To evaluate the performance of block selection methods for Unaligned Block-wise Pruning (UBP), we defined an efficacy metric based on the l\u2081 norm. Specifically, the sum of the importance scores when performing element-wise pruning (|WEP|) is set as 1.0 (maximum), and the sum when performing 1\u00d7N aligned block-wise pruning (|WABP(1\u00d7N)|) is set as 0.0 (minimum). This metric evaluates the total importance scores for each block selection method, with the efficacy for UBP (1\u00d7N) defined as:\n\\[ efficacy = \\frac{|W_{UBP (1 \\times N)}| - |W_{ABP (1 \\times N)}|}{|W_{EP}| - |W_{ABP (1 \\times N)}|} \\tag{5} \\]"}, {"title": "Accuracy", "content": "Table 1 presents the accuracy results for both Aligned Block-wise Pruning (ABP) and Unaligned Block-wise Pruning (UBP) applied to MobileNetV1 and ResNet50 across sparsity levels of 70%, 80%, and 90%. Training for ABP utilized the SUBP method [22], which employs the BPAR pruning criterion. The results demonstrate that UBP consistently exhibits higher accuracy than ABP, a benefit attributable to the superior block selection method, suggesting that further enhancements could be achieved through a pruning criterion specifically tailored for UBP."}, {"title": "UBP Kernel Performance", "content": "In this subsection, we evaluate the performance of the UBP kernel with the Weight Rotating and Output Stationary (WROS) dataflow. Fig.7 displays the relative performance of MobileNetV1 layers with a target sparsity of 80%, comparing element-wise pruning (EP) and block-wise pruning (ABP and UBP) with block sizes of 2 and 4. EP and ABP performances were measured using XNNPACK's SpMM kernel, while UBP performance was evaluated using kernels implemented with both the na\u00efve dataflow and our proposed WROS dataflow.\nFirst, we observe that increasing block size in block-wise pruning enhances performance compared to EP due to improved data reuse in sparse operations. Secondly, the UBP kernel using WROS dataflow"}, {"title": "End-to-end Performance", "content": "Fig.8 illustrates the latency of models with EP, ABP, and UBP at various sparsity levels on a Galaxy S20 CPU. The experiments utilized four threads for optimal performance. Notably, minimal performance differences between ABP and UBP were observed, attributed to the WROS dataflow minimizing overhead in the UBP kernel. For MobileNetV1, EP matches the performance of the dense model at approximately 65% sparsity, whereas ABP and UBP achieve similar performance at 20% sparsity due to enhanced kernel efficiency with larger block sizes. Similar trends were observed for ResNet50. At 70% sparsity, where both MobileNetV1 and ResNet50 exhibit accuracy comparable to the dense model, UBP results in latency improvements of 2.0x and 2.5x, respectively, indicating that UBP is effectively optimized and suggests significant performance enhancements in practical applications."}, {"title": "Conclusion", "content": "This paper addresses the limited practical application of unaligned pruning due to training and inference challenges by introducing the Block Expansion and Division (BED) algorithm and the Weight Rotating and Output Stationary (WROS) dataflow. These innovations facilitate efficient training and reduce inference overhead, proving the practical viability of unaligned block-wise pruning. Our experiments demonstrate that Unaligned Block-wise Pruning (UBP) not only matches but exceeds the accuracy of leading block-wise pruning methods with comparable latency.\nWhile UBP has shown promising results, it remains a relatively unexplored field. This paper focused primarily on adapting UBP for practical use rather than proposing new pruning criteria or training methods. We have demonstrated that UBP can outperform aligned block-wise pruning"}]}