{"title": "Sequential choice in ordered bundles", "authors": ["Rajeev Kohli", "Kriste Krstovski", "Hengyu Kuang", "Hengxu Lin"], "abstract": "Experience goods such as sporting and artistic events, songs, videos, news stories, podcasts, and television series, are often packaged and consumed in bundles. Many such bundles are ordered in the sense that the individual items are consumed sequentially, one at a time. We examine if an individual's decision to consume the next item in an ordered bundle can be predicted based on his/her consumption pattern for the preceding items. We evaluate several predictive models, including two custom Transformers using decoder-only and encoder-decoder architectures, fine-tuned GPT-3, a custom LSTM model, a reinforcement learning model, two Markov models, and a zero-order model. Using data from Spotify, we find that the custom Transformer with a decoder-only architecture provides the most accurate predictions, both for individual choices and aggregate demand. This model captures a general form of state dependence. Analysis of Transformer attention weights suggests that the consumption of the next item in a bundle is based on approximately equal weighting of all preceding choices. Our results indicate that the Transformer can assist in queuing the next item that an individual is likely to consume from an ordered bundle, predicting the demand for individual items, and personalizing promotions to increase demand.", "sections": [{"title": "1. Introduction", "content": "Bundling involves selling two or more differentiated products as a single package. This strategy can serve various purposes: enabling price discrimination by monopolists (Adams and Yellen 1976, McAfee et al. 1989, Schmalensee 1984, Stigler 1968), reducing transaction costs (Eppen et al. 1991, Kenney and Klein 1983), enhancing economies of scope (Baumol 1982), segmenting consumers dynamically (Derdenger and Kumar 2013), and deterring market entry (Bernheim and Whinston 1990, Nalebuff 2004). In competitive markets, firms have incentives to offer bundled products (Carbajo et al. 1990). If the number of firms exceeds a certain threshold, bundling can lead to higher market prices, benefit firms, and disadvantage consumers (Zhou 2017).\nMany experience goods, such as sporting and artistic events, songs, videos, news stories, podcasts, and television series, are offered and consumed in bundles. A notable feature of these offerings is that they often consist of ordered bundles, meaning the items within a bundle appear in a specific order and are typically consumed in sequence, one at a time."}, {"title": "2. Related research", "content": "The present research is related to the consumer behavior literature concerning the bundling of experience goods. This literature suggests that bundling can cater to variety-seeking behavior (McAlister 1982), facilitate the consumption of complementary products (Lewbel 1985), and reduce risk"}, {"title": "3. Problem description", "content": "We begin by formalizing the sequential-prediction problem."}, {"title": "4. Models", "content": "We evaluate three Transformer models (GPT-3, a masked decoder, and BERT) for solving the sequential-prediction problem and compare their predictive performance to six other models:\ntwo deep learning models (MLP and LSTM), DQN, two Markov models that assume position-\nindependent (MC) and position-dependent (pMC) transition probabilities, and a zero-order model.\nBelow, we briefly discuss each model and provide implementation details. To provide context, we describe aspects of the models in terms of the problem considered later in the paper, which concerns predicting if an individual listening to the songs on a playlist will replay a song, play the next song, or skip the next song."}, {"title": "4.1. Transformer (decoder) model", "content": "The Transformer proposed by Vaswani et al. (2017) consists of both encoder and decoder modules. We use only the decoder module, which processes an input sequence of tokens to generate the next sequence of tokens. In contrast, an encoder-only module generates vector encodings for each element of the input sequence of tokens.\nIn a typical decoder-only setting, each output token (prediction) is fed back as an input to recursively generate a sequence of tokens. For example, GPT-3, which uses a decoder architecture, generates the next word in a sequence based on the preceding words, then uses the newly generated word to predict the next one. In contrast, we use the actual outcomes of a sequence of positions to predict only one (the next) outcome. After each prediction, the input sequence is updated to include the latest actual outcome rather than the latest predicted outcome. This method is known as teacher forcing (Williams and Zipser 1989)."}, {"title": "4.2. Bidirectional Encoder Representations from Transformers (BERT)", "content": "Unlike the decoder, BERT uses only the encoder module of the Transformer. An encoder generates an output for each input. We ignore all but the last encoder output (that is, the output corresponding to the position for which we wish to make a prediction). The key differences between BERT and the"}, {"title": "4.3. Generative Pre-trained Transformer (GPT)", "content": "The GPT family of models can be used in zero and few-shot settings for tasks such as text generation. However, our objective is not to generate text but to predict listening behavior, a non-standard task that requires fine-tuning the model using data from listening sessions. At the time of the study, GPT-3 was the most advanced GPT model available. We used its DaVinci variant for fine-tuning.\nTo our knowledge, this paper is the first to report the use of LLMs for predicting music listening behavior. GPT-3 had a limitation that only unique input sequences (listening sessions) could be used for fine-tuning. As discussed later, this constraint significantly reduced the number of listening sessions available for fine-tuning GPT-3."}, {"title": "4.4. Long Short-Term Memory (LSTM)", "content": "LSTM is a well-known model in the family of RNNs (Hochreiter and Schmidhuber 1997). Given a listening session, LSTM processes each element of the (input) feature vector $z_j$ at position $j$ into a hidden state vector $h_j$ and a cell state vector $c_j$ by recursively leveraging the current feature and the previous hidden state:\n$o_j, h_j, c_j = \\text{LSTM}(z_j, h_{j-1}, c_{j-1})$,"}, {"content": "LSTM is a well-known model in the family of RNNs (Hochreiter and Schmidhuber 1997). Given a listening session, LSTM processes each element of the (input) feature vector $z_j$ at position $j$ into a hidden state vector $h_j$ and a cell state vector $c_j$ by recursively leveraging the current feature and the previous hidden state:\n$o_j, h_j, c_j = \\text{LSTM}(z_j, h_{j-1}, c_{j-1})$,"}, {"title": "4.5. Multilayer Perceptron (MLP)", "content": "We employ a three-layer fully connected feedforward neural network, commonly known as an MLP, to serve as a cross-sectional benchmark. The output of the model is the probability of an outcome for a given position in a session, generated using a softmax function. To maintain comparable model complexity, we configure the MLP with three hidden layers, each containing 256 units. We used PyTorch to implement the MLP (Goodfellow et al. 2016)."}, {"title": "4.6. Deep Q-Network (DQN)", "content": "Meggetto et al. (2023b) use DQN, an RL model, to predict skip or play for the Spotify Challenge. Each state is associated with the time a person makes a skip or play decision. States are described using song features. The reward is one if the action matches the actual outcome, and zero otherwise. Predicting an incorrect action terminates an episode during training. The model uses Q-learning (Watkins 1989), a model-free approach to RL. The value function is approximated by a three-layer MLP with 128 hidden units."}, {"title": "4.7. Markov models", "content": "We consider two first-order Markov models. The first model (MC) assumes the same transition probabilities for each position on a playlist. The second model (pMC) allows position-specific transition probabilities. These transition probabilities are constrained to ensure that (1) the probability of a replay following a skip is zero (a listener cannot skip and then replay a song), and (2) the process terminates after the last song (skipping or playing the next song is infeasible after the last song on a playlist). Our data includes at most one replay per song, and there are playlists where all replays occur for the last song (and thus no transitions from replaying an earlier song to skipping or playing the next song)."}, {"title": "4.8. Zero-order model", "content": "A zero-order model cannot directly model replays, which are inherently conditional outcomes. However, we can infer replays from a zero-order model as follows. Let $p(x_i)$ the probability that song $i$ is played $x_i$ times, where $x_i \\geq 0$ is an integer. Then $p(x_i = 0)$ is the probability of skipping song $i$, $p(x_i \\geq 1) = 1 - p(x_i = 0)$ is the probability of playing song $i$, and $p(x_i \\geq 2)$ is the probability of replaying song $i$ at least once. In our dataset, no song is played more than twice and thus $p(x_i \\geq 2) = p(x_i = 2)$. This simple model can obtain a high hit rate when the probability of an outcome (skip, play, or replay) is high. For example, Table 1 shows that 77% of all songs on playlist 3 are skipped. Predicting that all listeners skip all songs on the playlist would thus achieve a 77% hit rate. Similarly, predicting that all listeners play all songs on playlist 9 would achieve a 64% hit rate. A zero-order model can achieve a higher hit rate by making these predictions separately for each song on a playlist."}, {"title": "5. Data", "content": "We used data provided by Spotify, a music streaming service. The dataset contains information on the playlist listening behavior of individuals and was made available by the company for a Sequential Skip Prediction Challenge (Brost et al. 2019). Each playlist represents an ordered bundle of a small number of (say, n) songs. An individual listening to a playlist can skip a song or play it one or more times. Suppose we have observed an individual's listening behavior for the first i songs on a playlist, where 0 \u2264 i \u2264 n \u2212 1. Our objective is to predict if he/she will replay song i, play song (i + 1), or skip song i + 1, based on the individual's listening pattern for the first i songs as well as information on the features of the (i + 1)st song (we do not have information on the characteristics of individuals, which, if available, could also have been used for making the predictions).\nThe full dataset contains the logs of more than 130 million listening sessions, metadata for each track, and data on user interactions with the tracks. A session log is a sequence of tracks; track metadata includes song features and popularity; user-interaction data specify if a track was skipped, paused, or played, or if a listener switched to another playlist. We inferred replays from a variable labeled \"hist_user_behavior_reason_start\"."}, {"title": "6. Analysis and results", "content": "We use the training data to estimate the probability that a song (item) on a playlist will be skipped, played, or replayed during a listening session (ordered bundle). We apply a max-probability rule, predicting the outcome with the highest probability of occurrence for each position. By comparing the predicted and actual outcomes, we calculate the hit rate, which is the percentage of correct predictions across holdout sessions for each song on each playlist. This measure is widely employed to evaluate model performance and was previously used in the Spotify Challenge (Brost et al. 2019).\nOur dataset does not record when a person stops listening to songs on a playlist. Therefore, we need to make an assumption about how many songs to consider in each session when making predictions. One possible assumption is that a person listens to all the songs on a playlist. Another is that a person listens only up to the last song he/she actually heard. To illustrate the difference, imagine a playlist with ten songs, and a session record showing that the fifth song was played but not any subsequent song. Under the first assumption, we consider all ten positions for model training, marking the last five positions as \u201cskipped.\" Under the second assumption, we truncate the training data after the fifth position. We estimate the models under both assumptions. The second assumption yields slightly lower hit rates for all models, but the overall pattern of results remains unchanged under either assumption. Here, we present the results assuming that each person considers listening to all songs on a playlist. Table C.1 in Appendix C shows the hit rates for each playlist and across playlists assuming data truncation after the last song played in a session.\nNext, we discuss the selection of input features, compare the hit rates across models, examine the accuracy of predictions for different outcomes, contrast the distribution of hit rates across models, and evaluate the effect of including the actual listening time (a data leak) as a model feature."}, {"title": "6.1. Feature selection", "content": "The first column of Table 3 lists the features available in the dataset for the Spotify Challenge. Additionally, it includes a feature called predicted remaining time, which was inferred as follows. Let $\\tau_j$ denote the listening time (i.e., the time songs are played and replayed) starting at position $j$ and ending at the last position in a session. The predicted remaining time for position $j$ is then calculated as the average value of $\\tau_j$ across all the sessions associated with a given playlist. We used the training data to estimate the predicted remaining time for each position in a session. To our knowledge, predicted remaining time has not been previously used as a feature for predicting music-listening behavior.\nWe use the variables shown in Table 3 to train the Transformer and a gradient boosting decision tree (GBDT), which is an ensemble of decision trees (Chen and Guestrin 2016). We employ the Shapley Value to assess the effect of including a variable on the predictions of the Transformer model (Lundberg and Lee 2017). We use two measures of feature importance in the GBDT model: (1) the number of splits involving each feature across all decision trees (Split), and (2) the increase in likelihood from all splits of a feature among all trees (Info. Gain). Table 3 shows the values of the three importance metrics. Both previous action and predicted remaining time have a large number of splits, high information gain, and a high Shapley Value. Most song features have low scores on the three importance metrics. These results are consistent with earlier findings that previous action is important, but song features are unimportant for predictions in the Spotify Challenge (Meggetto et al. 2023b).\nNext, we trained the Transformer, LSTM, BERT, MLP, and DQN models using (1) all features, and (2) one or both of predicted remaining time and previous action (skip, play, or replay). Table 4 presents the hit rate achieved by each model for individual playlists across sessions, and the overall hit rate across all playlists. Limiting the features to only the previous action and predicted remaining time yields marginal increases in hit rates for LSTM and BERT models. This finding suggests potential over-parameterization when these models are trained using the full feature set. In contrast, the Transformer model exhibits a slight decrease in hit rate when restricted to these two features. Removing the predicted remaining time feature further results in a minor reduction in hit rates across all five models, including the Transformer. Given these findings, we opted to use only the previous action and predicted remaining time as features."}, {"title": "6.2. Model predictions", "content": "Table 5 presents the playlist-specific hit rates, and the weighted average hit rate across playlists, for each model (the weights are proportional to the number of observations on the playlists). The table also shows the number of parameters per playlist for each model (the number of parameters varies by playlist for pMC and the zero-order model). We make the following observations based on these results."}, {"title": "6.3. Hit-rate distributions", "content": "Figure 5 shows the empirical cumulative distribution functions (CDFs) of the hit rates for the various models. The distribution functions are based on the average hit rates calculated for each of the 138 positions across the ten playlists. The Transformer CDF lies below, and the GTP-3 CDF above, all others. 70% of the Transformer hit rates exceed 80%, and approximately 30% surpass 90%; only 3% of the Transformer hit rates fall below 60%. In contrast, 26% of the GPT-3 hit rates are below 60%, only 25% exceed 80%, and a mere 8% surpass 90%. LSTM's performance closely matches the Transformer for hit rates below 70%, but the gap between their CDFs widens at higher hit rates. The zero-order model consistently outperforms GPT-3, with about 60% of its hit rates exceeding 70%, compared to 45% for GPT-3. MC, pMC, MLP, and BERT exhibit similar hit-rate distributions. DQN shows a marked improvement in the hit-rate distribution compared to both the zero-order model and GPT-3."}, {"title": "6.4. Hit rates by type of outcome", "content": "Table 7 shows the confusion matrix the hit rates of the different models when the actual outcome is skip, play or replay. All models achieve hit rates above 83% (and LSTM and the zero-order model"}, {"title": "6.5. Estimating demand", "content": "We define the demand rate for a song on a playlist as the expected number of times a random playlist listener will play the song. Multiplying the demand rate by the number of playlist listeners gives an estimate of a song's demand from the playlist."}, {"title": "6.6. Listening time", "content": "Meggetto et al. (2023b) observed that many predictive models in the Spotify Challenge suffered from a data leak problem because they used the actual listening time of a session which cannot be observed until the end of the session to predict skipping behavior. They reported sharply lower hit rates when this feature was excluded from their DQN model. Our model avoids the data leak by using the predicted remaining time as a feature instead. To assess how much the predictions would improve if we allowed the data leak, we replaced the predicted remaining time by the observed remaining time for a position (i.e., the total listening time minus the time already listened) as a feature. Table 8 shows the results. All five models (decoder-only Transformer, LSTM, BERT, MLP and DQN) perform substantially better when the observed remaining time is used as a feature. The hit rate for the Transformer and DQN increases by 7%, for MLP by 6%, for BERT by 5%, and for LSTM by 2%. One reason for this improvement is that it is trivial to predict skip when the observed remaining time is less than the length of any remaining song on a playlist. Similarly,"}, {"title": "6.7. Transformer attention and listening behavior", "content": "The defining feature of the Transformer is its self-attention mechanism, which is used to predict subsequent outcomes. Vaswani et al. (2017) define a query as a position for which a prediction is being made, and a key as a position that influences the prediction. We analyze the patterns in the self-attention weights of the (decoder) Transformer model. To our knowledge, the following method of analysis is new.\nLet n denote the number of positions in a listening session and $a_{ij}$ the self-attention weight for query $i$ and key $j$. As discussed in Appendix E, masked self-attention imposes two constraints:\n$a_{ij} = 0$, for all $i+1 \\leq j \\leq n$ and $i = 1,...,n$, and\n$\\sum_{j \\leq i} a_{ij} = 1$, for all $i = 1, ..., n$.\nLet\n$\\overline{a}_{i.} = \\frac{1}{i} \\sum_{j=1}^{i} a_{ij}$\nand\n$\\overline{a}_{.j} = \\frac{1}{n+1-j} \\sum_{i=j}^{n} a_{ij}$\nrepresent the average non-zero self-attention weights associated with query $i$ and key $j$, respectively. We call $\\overline{a}_{i.}$ the average query weight and $\\overline{a}_{.j}$ the average key weight. The constraint $\\sum_{j \\leq i} a_{ij} = 1$ implies that the average query weight is $\\overline{a}_{i.} = 1/i$, for all $i = 1, ..., n$; that is, later"}, {"title": "7. Discussion and conclusion", "content": "We conclude that the general pattern of key weights is consistent with a Transformer assigning equal weight to each non-succeeding position when predicting an outcome. That said, the strength of the self-attention mechanism is that it allows the attention weights to depend on the specific listening pattern in a session, and we do observe deviations from the general pattern in many sessions. As an illustration, Figure 9 shows two sessions, one in which the self-attention weights depart substantially from baseline 3 and the other in which they do not.\nExperience goods, such as sporting and artistic events, songs, videos, news stories, podcasts, and television series, are often offered and consumed in bundles. A notable feature of these bundles is that the individual items are ordered and can only be consumed sequentially, one at a time. This study examined whether an individual's decision to consume the next item in an ordered bundle can be predicted based on the consumption patterns of the preceding items.\nWe evaluated several models to make the prediction, including two custom-built Transformer models that use decoder-only and encoder-decoder architectures, fine-tuned GPT-3, a custom LSTM model, a reinforcement learning model, two Markov models, and a zero-order model. We tested the models using data obtained from Spotify, a music streaming service. We found that the custom-built Transformer with a decoder-only architecture provided the most accurate predictions of individual choices. The hit-rate distribution for this Transformer model stochastically dominated the distributions of all other models. The same Transformer also provided by far the most accurate predictions for the demand of each item in an ordered bundle.\nThe individual-level predictions of the custom Transformer could be used to automatically queue the next song, video, news story, television show, or podcast that an individual is likely to consume from an ordered bundle. For example, a music streaming service like Spotify could use the predictions to determine which track to queue next during an individual listening session. If the model predicts \u201cplay\u201d or \u201creplay\u201d, the next song in the queue should remain unchanged. However, if it predicts \"skip\", this prediction should be used as an input to predict the outcome for the following position, the process iterating until \"play\" is predicted, at which point the corresponding song should be placed next in the queue. Iteratively predicting outcomes is a standard decoder task, which in the present context should halt when \u201cplay\u201d is generated as an outcome.\nAccurately predicting an item's demand based on its position in a bundle can also be useful for a seller. In situations where supply is constrained, such as the number of seats in a sporting arena or"}]}