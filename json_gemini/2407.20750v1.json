{"title": "JACOLBERTv2.5: OPTIMISING MULTI-VECTOR RETRIEVERS\nTO CREATE STATE-OF-THE-ART JAPANESE RETRIEVERS WITH\nCONSTRAINED RESOURCES", "authors": ["Benjamin Clavi\u00e9"], "abstract": "Neural Information Retrieval has advanced rapidly in high-resource languages, but progress in lower-\nresource ones such as Japanese has been hindered by data scarcity, among other challenges. Conse-\nquently, multilingual models have dominated Japanese retrieval, despite their computational ineffi-\nciencies and inability to capture linguistic nuances. While recent multi-vector monolingual models\nlike JaColBERT have narrowed this gap, they still lag behind multilingual methods in large-scale\nevaluations. This work addresses the suboptimal training methods of multi-vector retrievers in lower-\nresource setting, focusing on Japanese. We systematically evaluate and improve key aspects of the\ninference and training settings of JaColBERT, and more broadly, multi-vector models. We further en-\nhance performance through a novel checkpoint merging step, showcasing it to be an effective way of\ncombining the benefits of fine-tuning with the generalization capabilities of the original checkpoint.\nBuilding on our analysis, we introduce a novel training recipe, resulting in the JaColBERTv2.5\nmodel. JaColBERTv2.5, with only 110 million parameters and trained in under 15 hours on 4 A100\nGPUs, significantly outperforms all existing methods across all common benchmarks, reaching an\naverage score of 0.754, significantly above the previous best of 0.720. To support future research,\nwe make our final models, intermediate checkpoints and all data used publicly available.", "sections": [{"title": "Introduction", "content": "Text-based Information Retrieval (IR) [29, 3] is a Natural Language Processing application that enables users to retrieve\nrelevant documents for a given query [40]. Historically, the field has been dominated by lexical-matching methods,\nwhere retrieval is performed by directly matching query terms with document content [5], with both undergoing various\nnormalization steps to facilitate this process [64]. Okapi BM25 [55], the most successful lexical-matching method,\nremains widely used today, either independently or in conjunction with more modern approaches [62].\nDespite its inability to capture semantic information, BM25 is still considered a robust baseline that can be challenging\nto surpass, even with significantly more computationally intensive approaches [25]. However, in recent years, there\nhave been large advances in the development Neural IR approaches for the English language, with numerous models\nnow consistently strongly outperforming BM25 [43].\nThese improvements have been attributed to two factors: the first is the development of much more powerful neural\narchitectures, such as the transformers architecture [66], and the pre-trained models based on it such as BERT [13].\nThe second factor is the release of large-scale retrieval training datasets, with Microsoft's MS-MARCO [45] being\nwidely credited as one of the most important factors to the rapid development of English Neural IR [79]. Moreover,\nthere also exists a large range of high quality, domain-specific datasets on which Neural retrievers can be further\ntrained, providing English models with even stronger generalisation capabilities [11, 62].\nOutside of the English language, and some other very high-resources languages such as Mandarin Chinese [72],\nprogress in Neural IR has been considerably slower and has yielded more modest results. In most mid and lower-"}, {"title": "Background", "content": "In this section, we will provide a brief overview of the mechanisms used by multi-vector retrieval models such as\nColBERT and JaColBERT, on which our work builds."}, {"title": "ColBERT", "content": "Multi-vector retrievers, also sometimes called late-interaction models, were introduced and popularised by the Con-\ntextualized Late Interaction over BERT, or ColBERT, model architecture [26].\nThe ColBERT architecture relies on a simple intuition, which is that the dominant way of creating document represen-\ntation, by creating a single vector for each document, causes too much information loss. While this information loss\ncan be mitigated when evaluated on in-domain tasks, it can result in consistently poorer out-of-domain generalisation.\nTo solve this problem, multi-vector retrievers do not create a single large vector to represent a document, but multiple\nsmaller ones with each individual vector representing a single token.\nQuery Augmentation Identifying the fact that retrieval is by nature a very asymmetrical task, with queries often\nbeing short, a query-augmentation mechanism is also introduced. Rather than padding inputs to the maximum length\nwith padding tokens, it leverages the masked-language-modeling objective of its base models [13]. To do so, it pads\nall queries with [MASK] tokens, which are then attended to by the model. These mask tokens have been heavily\nstudied in subsequent work, and appear to provide useful term-weighting and amplify semantic information, resulting\nin improved retrieval performance. [15, 17]\nMaxSim In order to perform scoring, the ColBERT authors introduce a new scoring mechanism, which they dub\nMaxSim, for Maximum Similarity. In this setting, the score of a [query, document] pair is obtained via the following\nformula, where E represents all the embeddings representing a given document or query:\nScorequery, document := \n\n\u03a3\ni\u2208[|Equery|] max\nj\u2208 [|Edocument]\nEquery; * Edo\ndocument;\n\n\n(1)\nEffectively, for a given query token, its cosine similarity with every document token is computed, and the highest\nsimilarity is kept. This process is repeated over all query tokens, and the final [query, document] score is represented\nas the sum of all those maximum similarities per query token, hence the name \"MaxSim\"."}, {"title": "ColBERTV2", "content": "A subsequently improved version of ColBERT, named ColBERTv2, seeks to address these two issues [59]. This\nsecond version overhauls many parts of the original process, using a more modern training recipe, albeit without\na clear evaluation of the impact of each component. Most notably on the training side, it introduces both in-batch\nnegatives [24] and knowledge distillation [19]. To help alleviate the storage issue, it introduces a novel indexing\napproach, allowing for a 6-fold index-size reduction by clustering the individual token representations and offloading\nmost of the stored information to the cluster centroids before compressing the vectors to just 2 bits. This method\nsuccessfully brings the storage and memory requirements of ColBERTv2 down to the same order of magnitude as that\nof single-vector models, though still noticeably higher, while reaching even stronger results on out-of-domain datasets.\nFurther work has shown that this approach also addresses the issue of weaker in-domain performance, with fine-tuned\nversions of the model being able to outperform all other approaches on multiple benchmarks [56].\nColBERT and ColBERTv2 have garnered a lot of attention, with various studies attempting to better understand and\nimprove its various mechanisms, such as the various effects of [MASK]-based query augmentation [17, 15], the\nimpact of introducing full-word rather than token-level representations [21], potential improvements to its scoring\napproach [32], or to the mechanisms around its optimised indexing approach [58, 39]."}, {"title": "JaColBERT", "content": "In Japanese, both of these approaches have been reproduced, with even greater success than their English equiva-\nlents [9]. JaColBERTv1, following the training recipe of the original ColBERT model, became the then-strongest\nmonolingual Japanese retriever. However, it fell short of the strongest multilingual models on multiple benchmarks,\nwith the most notable performance gap being found on large-scale retrieval tasks. Subsequently, JaColBERTv2, trained\nfollowing the ColBERTv2 recipe, helped address these issues. JaColBERTv2, at the time of this work, is the strongest\nout-of-domain retrievers on all existing Japanese benchmarks. However, on MIRACL [81], a large-scale retrieval\ndataset which was used to train most multilingual retrievers and on which they are therefore in-domain, it still notice-\nably lags in performance."}, {"title": "Experiments", "content": "In this section, we will present the various steps of our experimental process. As this study focuses on systematically\nevaluating various approaches to using and training multi-vector models, we will conduct short training runs, also\ncalled ablations, on small data scales, to identify the best possible setting. We believe this sort of small-scale training\nto identify optimal model settings is well-suited to helping us refine optimal training for constrained resources settings,\nas it has proved to be a particularly strong indicator of full-sized model performance. In recent months, it has notably\nbecome the preferred way of predicting model behaviour for large language models [1, 18, 63].\nIn each section, we will discuss the rationale for our proposed settings and, when relevant, the results and learnings of\nthe relevant ablations.\nFirstly, we will define our hardware constraints in Section 3.1, before discussing our choice of training data is Sec-\ntion 3.2. We then present an overview of our the baselines our models will be evaluated against in Section 3.3 and of\nour chosen valuation benchmarks in Section 3.4.\nIn Section 3.5, we will explore different approaches to defining the query length on ColBERT's query-augmentation\nmechanism.\nWe will then evaluate the impact of various alterations of the model's training settings in Section 3.6, through a series\nof small-scale training runs."}, {"title": "Hardware Constraints", "content": "All experiments conducted in this study are done under a compute constraint, in order to highlight that our final model's\nperformance is a consequence of our improvements, rather than due to substantially increased compute.\nJaColBERTv2 was trained for 28.5 hours on 8 NVidia A100 GPUs [9], representing a total training budget of 228\nA100 hours. All teacher scores used by JaColBERTv2 were re-used from the original ColBERTv2 teacher scores [59],\nand therefore came at no additional compute cost.\nBased on this, we constrain our total training compute budget to the same 228 hours, plus or minus 5%, resulting in\na final upper bound budget of 239 A100 hours. We include all time spent training each ablation model, generating\nteacher scores and training the final JaColBERTv2.5 model under this budget."}, {"title": "Training Data", "content": "For both the final model training and ablation runs, we follow existing practices [9, 69] and train our model using\nthe Japanese split of MMarco [6]. MMarco is a machine-translated of MS Marco [45], a large English Information\nRetrieval (IR) dataset which has widely been credited as unlocking vast advances in English IR, thanks in large part\nto its scale and wide variety of queries. For a long time, no equivalent dataset existed in non-English languages, and\nefforts to create ones were largely unsuccessful due to the cost of such an endeavour. MMarco was introduced to\nbridge the gap between English and other languages, by providing a fully machine-translated version of MS Marco in\n14 languages, and empirically showcasing that, while the resulting dataset produced poorer results than in English, it\nstill contained useful signal on a scale usually not available in these languages.\nRetrievals models are generally trained on triplets. These triplets can either be be standard triplets, as with Col-\nBERTv1 [26] and JaColBERTv1 described above, where each triplet contains a single query, a single positive doc-\nument, and a single negative document, or n-way triplets. n can be any number, and represents the total number of\ndocuments passed to the model: in the case of 16-way triplets, the model would be presented with a query, and 16\ndocuments, rather than just 2 in the standard setting. Doing so allows us to more efficiently use knowledge distil-\nlation [19], where the model learns from teacher scores, generally generated by strong cross-encoder models, and\nattempts to emulate them or their distribution [50].\nOur models are trained using 32-way triplets with knowledge distillation. This means that, for every single query, the\nmodel is given 32 documents per triplet, as well as teacher scores for every [query, document] pair. The goal of the\nmodel's training is to attempt to learn to reproduce the provided scores, through a knowledge distillation loss function\nwhich we explore further in Section 3.6.4.\nWe use a downsample of the set of triplets used to train the original English ColBERTv2 model [59]. We downsample\nthe triplets in two ways: firstly, in order to meet the compute constraints of this work, we randomly sample 3,200,000\ntriplets out of the 19,000,000 originally provided. Secondly, as ColBERTv2 was trained with 64-way triplets, we\nrandomly sample 31 negative documents from the original 63 in every individual triplet. We choose to train on\n3,200,000 triplets, which represents 40% of the 8,000,000 triplets JaColBERTv2 was trained on, in order to respect\nour compute constraints and allocate sufficient compute to generating teacher scores.\nAs MMarco is a direct translation of MS Marco, it is possible to reuse the ColBERTv2 triplets with no further modi-\nfications. We use the teacher scores provided by the ColBERTv2 authors as our baseline teacher scores for all of our\nexperiments, and will extensively cover the effect of different teachers in Section 3.7."}, {"title": "Baselines", "content": "Our final models are evaluated against a large range of representative baselines, including the current best-performing\nretrievers. To do so, we evaluate our model against BGE-M3 [8], the current best-performing multilingual embedding\nmodel. BGE-M3 is a multi-output model: it is capable of producing single-vector dense representations, but is also\nable to output sparse and computationally heavy multi-vector representations to act as a \"self-reranker\". As a result,\nwe report results from BGE-M3 in two settings: dense, using only its single-vector retriever output, and all, leveraging\nall three forms of outputs in the way recommended by its original authors. BGE-M3's model size is roughly 5.11x\nthat of JaColBERT.\nResults for the multilingual-E5 (mE5) family of models [69] are also presented in all three existing model sizes, small\n(JaColBERT-sized), base(2.5x JaColBERT) and large (5x JaColBERT). The mE5 family is one of the most widely\nused model family for Japanese retrieval, and has consistently shown strong results on benchmarks [9].\nWe also report results for the best-performing single-vector retrievers in Japanese, GLuCoSE, an embedding model\nbased on LUKE [74], as well as Nagoya University's SUP-SIMCSE family of models [65], in both base and large sizes.\nFinally, we also report results for JaColBERTv1 and JaColBERTv2, the two previous best multi-vector retriever mod-\nels for Japanese, respectively trained following the ColBERTv1 [26] and ColBERTv2 [59] training recipes."}, {"title": "Evaluation Data", "content": "We define two evaluation sets: one used for final evaluation, described in Section 3.4.1 and a smaller, quicker-to-run\none, that will be used for the various experiments we are planning to run to find the optimal training setting presented\nin Section 3.4.2. All metric calculations are performed using the ranx evaluation toolkit [4]."}, {"title": "Final Evaluation Data", "content": "For the final evaluation, five commonly used evaluation datasets will be used to cover the model's performance in a\nvariety of settings. For each dataset, we choose a main evaluation metric in line with previous work in order to provide\nclear comparisons. However, detailed evaluation results for our models will also be reported.\nJSQUAD is a QA dataset introduced in the JGLUE evaluation set [30], inspired by the English SQUAD [52]. We\nevaluate JSQuAD in the same setting as in previous studies [9], using Nouu.me's evaluation benchmark, where the\ndataset is reduced to 1600 passages, and the model's goal is to extract the relevant passage for each query in its top\nMIRACL [81] is a large-scale multilingual evaluation benchmarks. We use its Japanese subsplit, which is composed\nof over 6 million documents, extracted from Wikipedia, and contains human-created relevance judgements for 860\nqueries over this corpus. We choose to use exclusively MIRACL, rather than both MIRACL and Mr.TyDi [80], an-\nother large-scale multilingual information retrieval dataset, as MIRACL is a refinement of Mr.TyDi, with additional\njudgements added and dubious labels removed. The main metric reported for this dataset is NDCG@10. It has been\nnoted in the past that MIRACL contains \"holes\": that is, the positive judgements are not thorough, and the data con-\ntains many false negatives. However, it remains the only large-scale evaluation benchmark for most languages it\ncovers, including Japanese, and is one of the most commonly used non-English IR benchmark [8, 69, 9, 38].\nJQaRA [61] is another dataset built from a QA dataset commonly used for Japanese QA evaluation, JAQKET [83].\nThe aim is, similarly to SQuAD, to find a document containing the answer to a given query, over 1667 queries. The\ndataset was constructed via a mix of LLM usage, before going through human validation to ensure all negatives are\nnegatives and all questions have at least one real positive passage. The task is presented as a hard reranking task: for\neach query, we are provided with 100 documents, with one or more of them containing the information necessary\nto answer the query and all other documents containing very adjacent information which does not directly address\nthe query. These adjacent documents are called \"hard negatives\", as they're purposefully designed to be hard to\ndifferentiate from positive examples. The main evaluation for this task is NDCG@10. All evaluations on JQaRa are\nconduced with the official evaluation code provided by the dataset author [61].\nJaCWIR [60] is a medium-scale (500,000 documents) retrieval dataset, using a large variety of web-scrapped doc-\numents. It is an entirely auto-generated dataset, where GPT-3.5 [7] was asked to produce queries for which a given\ndocument would be relevant. We also use it as a reranking task, similarly to how it was introduced. For each of its\n5,000 queries, the model must attempt to identify the relevant document among 99 hard negatives. The main evalu-\nation metric for this task is NDCG@10. All evaluations on JaCWIR are conduced with the official evaluation code\nprovided by the dataset author [60].\nESCI [53] is an addition to the JaColBERT evaluation set, which was not used in previous work. It is a \"Shopping\nQueries Data Set\" dataset provided by Amazon Science as part of the KDD Cup 2022. The goal of this dataset\nis to evaluate a model's ability to match very short (1 to 5 tokens) queries with the textual description of relevant\nproducts. We use ESCI as a retrieval task, similarly to one of the settings it is available in in the Japanese Massive Text\nEmbeddings Benchmark (JMTEB), an ongoing effort inspired by the English MTEB [43]. For any given product\nquery, the model must attempt to retrieve the description of relevant products among 149,999 product descriptions.\nThe main evaluation metric for this task is NDCG@10."}, {"title": "Development Evaluations", "content": "A large part of our study focuses on systematically evaluating a large variety of improvements to the ColBERT training\nand inference routine. As a result, we need a representative development set that is computationally inexpensive to\nrun, while providing us with enough information to make decisions. We decide to use two evaluation sets, and report\ntwo key metrics for each of them. The first one is JQaRA, as presented above. We choose JQaRA due to its small size,\nbeing presented as a reranking task, while it has consistently shown a good ability to discriminate between models\nand a good correlation to performance on other datasets [61]. We report both NDCG@10 and MRR@10 as our\ndevelopment metrics.\nAs our second task, we follow ongoing efforts in creating lighter embeddings benchmarks and introduce a smaller\nversion of MIRACL's Japanese split, which we dub MIRACL-small-ja. This dataset is built through hard-negative\nmining. Using BM25, we retrieve the top 250 results for each of the 860 MIRACL development queries. We then\nenrich this data with all positive examples, if they were not present in the BM25 results. The resulting dataset contains\n197,610 documents and 860 queries."}, {"title": "Dynamic Query Length", "content": "ColBERT models use a query augmentation mechanism, leveraging the use of [MASK] tokens, which are appended\nto every query, replacing traditional padding [26], until the predefined maximum query length is reached. These tokens\nhave been shown to learn different types of information, occasionally acting as term-importance weights [15, 17].\nHowever, the exact impact of mask tokens and the best way to use them has been understudied. Instead of variable-\nlength augmentation, a past study has explored simply appending 8 [MASK] tokens to every query, regardless of the\nactual query length [20]. Another previous study has chosen to remove this augmentation mechanism entirely [21].\nHowever, no study has compared the effects of these various choices against the original implementation.\nWe believe all three of these approaches to be suboptimal, and propose dynamic query length as a replacement.\nDynamic query length effectively aims to improve the initial approach of padding each query with [MASK] tokens\nuntil the query maximum length by allowing it to more easily adapt to longer queries, while also borrowing from\nfixed-length augmentation for edge cases.\nEffectively, our approach is to set the maximum query length to the nearest higher multiple of 32 (ColBERT's original\nmaximum query length) before performing the [MASK]-padding. Additionally, if fewer than 8 augmentation tokens\nwould be appended with the new query length, we ensure that at least 8 tokens are appended, overriding the maximum\nlength.\nTo select the default query augmentation mechanism to use for JaColBERTv2, we evaluate all four of the discussed\napproaches."}, {"title": "Results", "content": "Results for this experiment are presented in Table 2. The results are pretty clear. On all datasets, disabling [MASK]\naugmentation is consistently largely outperformed by all query augmentation approaches. Fixed 8-token query aug-\nmentation, while performing considerably better than no augmentation, is similarly outperformed by both fixed and\ndynamic query lengths on every dataset.\nOn JQaRA, where the query length fluctuates more and some queries are considerably longer than others, dynamic\nquery length outperforms a flat, higher token limit, suggesting that appending too many [MASK] tokens can produce\na slightly detrimental effect. An empirical analysis of the results obtained on JQaRA also reveals that dynamic query\nlength has virtually no impact on queries that are noticeably shorter than the maximum query length in a fixed query\nlength setting. However, it noticeably improves NDCG@10 on queries which are nearer the maximum length, and\nwould thus lose most of the query augmentation mechanism. This explains the small increase in overall NDCG@10\non the full dataset.\nOn MIRACL, where all queries have similar token counts and are all well under 32 tokens, the results for fixed and\ndynamic query lengths are identical, as would be expected.\nBased on the results of this experiment, every result we report on subsequent ablation, as well as the final model results,\nwill be using dynamic query length."}, {"title": "Training Setting", "content": "In this section, we will evaluate the impact of certain changes to common components of the retrieval model training\npipeline. We will first explore concerns around the impact of using in-batch negatives in Section 3.6.1. We next\nexplore the optimal way of scheduling the model's training, and the relevance of the recently recently schedule-free\ntraining method [12] in Section 3.6.2. We will then study the benefits of score normalization, applied to both teacher\nand student models, in Section 3.6.3, as well as explore the use and impact of different commonly used knowledge\ndistillation loss functions in Section 3.6.4.\nFinally, we will present the results of all of our experimental small-scale runs and discuss their implications in Sec-\ntion 3.6.5."}, {"title": "In-Batch Negatives", "content": "In-batch negatives (IBNeg) are frequently used as a way to augment the training of retrieval models [24]. Effectively,\nwithin a given training batch, the IBNeg approach treats every other query's positive documents as additional negative\nexamples in a binary relevance classification exercise. The query's original positive example is treated as a positive\nlabel, and every other query's positive example becomes a negative example. The model is asked to predict relevance\nover those newly created [query, document] pairs, and a cross-entropy loss is calculated over this prediction and then\nadded to the model's main training loss.\nThis method has shown modest but consistent across studies performance improvements when used with single-vector\nretriever models [24, 68]. This method is added to the ColBERT training recipe in the paper introducing ColBERTv2,\nfollowing the promising results obtained by other models [59]. This choice was not thoroughly evaluated, and its\nimpact is therefore unknown.\nHowever, we empirically note that the resulting cross-entropy loss values are two orders of magnitude lower than\nthe ones from the main KL-Divergence loss used in typical ColBERTv2 training. Moreover, we hypothesize that in-\nbatch negatives are an unnecessary signal for training multi-vector models using 32-way triplets for multiple reasons,\nespecially in non-English, data-constrained settings. Firstly, the information obtained from distilling the ranking\ndistribution of a strong cross-encoder model over 32 documents should carry a much stronger signal than binary\nrelevance labelling between a positive document and randomly sampled negatives. Secondly, this loss relies on the\npositive examples consistently being well-annotated and true positives, which is not guaranteed to be the case with\nlossy annotation processes, or even the partially-automated positive selection used in ColBERTv2 [59].\nTo confirm the validity of our hypothesis, we will train two separate ablation models on the same data."}, {"title": "Scheduling", "content": "The learning rate scheduler used for the training of neural networks has been shown to have a potentially large impact\non the performances of the resulting model [78]. There are are a few common schedulers yielding strong results, such\nas WSD (Warmup-Stable-Decay) [22], which increases the learning rate steadily before plateauing for the majority\nof training and entering a decay phase, which should ideally be performed on higher quality data, or Linear-Decay,\nwhere the model's learning rate increases during a fixed number of warmup steps before linearly lowering until the\nfinal training step, among others. The original ColBERTv2, as well as JaColBERTv2 [9], used linear decay scheduling.\nHowever, while tuned learning-rate scheduling consistently outperforms non-scheduled approached, it is not without\nconstraints. Notably, for the best performance, it requires knowing the total number of steps in advance in all cases,\nor has constraints such having a higher quality data mix for schedulers relying on them for their annealing phase.\nMoreover, an optimal schedule for a large number of steps is not guaranteed to work as well for lower data quantities.\nThese constraints are especially noticeable for retrieval models, which are expected to be put to use on a wide variety\nof downstream uses with varying data distributions, and therefore benefit immensely from being able to easily resume\ntraining without huge performance impact.\nRecently, schedule-free learning has been proposed [12]. This new approach, while not yet thoroughly tested across all\ndomains, has empirically shown very encouraging results on a large number of benchmarks. In practice, it introduces\nadditional calculations as part of the optimizer steps, allowing it to vary the learning rate without the need for a fixed,\npre-defined schedule. This considerably simplifies both the pre-training and fine-tuning processes, as there is no need\nto optimise the scheduler used for training and similar parameters can be re-used for different data scales.\nSchedule-free learning has been noted as potentially requiring a higher learning rate than scheduled learning, with\noptimal values empirically falling in the range of 1 to 10 times the original learning rate [12]. We thus conduct an\nexperiment comparing the training setting used for JaColBERTv2 and its ablations and schedule-free learning, with"}, {"title": "Score Normalisation", "content": "In the current ColBERTV2 training recipe, scores are unnormalised: the raw logits outputted by the teacher cross-\nencoder are used as teacher scores, and the output of the maxsim scoring function is used as the student model's\nscore. These scores are on different scales, as the theoretical range for maxsim score is [0, Number of Query Tokens],\nwhile the range for cross-encoder logits include negative numbers and is on a scale dependant on the original model's\ntraining, which varies teacher by teacher.\nWhile some distillation losses can be seen as being partially robust to scale differences, which partially justified the\nuse of KL-Divergence in the ColBERTv2 paper [59], we believe the lack of normalisation to be suboptimal for two\nmain reasons. The first one is that, given the large difference in scale, the loss calculation is likely to lead to a better\napproximation of information loss if operating on a similar scale. The second is that normalised scores allow the\nmodels to focus purely on the relative ranking of results, rather than absolute scores, the latter of which may provide\nless useful information due to the automated nature of triplet generations.\nWe experiment with two used normalization approaches: one where only the teacher scores are normalized [31], and\none where both the teacher and student scores are normalized. In all cases, we use min-max normalization, defined as:\nScorenormalized = \nscore - scoreleast_relevant\nscoremost_relevant - scoreleast_relevant\n(2)\nEffectively, this gives a score of 1 to the most relevant document identified by the teacher and 0 to the least relevant\none, regardless of their absolute score. Every other score is then placed on this scale depending on their distance to\nthose two scores."}, {"title": "Loss functions", "content": "For knowledge distillation in retrieval models, two loss functions are commonly used [14, 54]: MarginMSE and\nKL-Divergence, the latter of which is the one used by ColBERTv2.\nMarginMSE [20] consists in computing the Mean-Squared Error on the difference in the margin between the pre-\ndictions of the model being trained and the teacher model. The margin defined is the difference between the score\nthe model gives to the positive document and the score it gives to negative documents. In the case of n-way training,\nthis margin is calculated over every [positive_document_score, negative_document_n_score]. MarginMSE is thus\ncomputed as follows (where N is the batch size):\nMarginMSE = 1N \n\u03a3N\ni=1 max(0, margin \u2013 (scoreteacher(xi) - scorestudent(xi)))2\n(3)\nEffectively, the training objective becomes for the student model's margin to reproduce the teacher's margin as closely\nas possible.\nKL-Divergence [27], on the other hand, seeks to directly minimise the difference between the distributions of scores\nof the model being trained and its teacher. KL-Divergence loss is computed as follows (where N is the batch size):\nKL-Div = 1N\n\u03a3\u03a3\ni=1 score\nPteacher (score|xi) log\n(\nPteacher (score|xi)\nPstudent (score|xi)\n\nIn effect, it computes an estimation of how much information appears to be lost between the teacher model's distribu-\ntion and the student model's one, and minimising this loss becomes the primary training objective.\nThe use of either MarginMSE or KL-Divergence has been reported for knowledge distillation into various re-\ntrieval models. While both loss functions have been shown to be strictly superior to more traditional MSE-based\nlosses [23, 20], there has been little head-to-head comparison of the two. Recently, the SPLADE-V3 authors anec-\ndotally reported noticing overall similar performances between the losses, with MarginMSE favouring recall and\nKL-Divergence precision, and opted for a mixed-loss approach for their final model, using a conjunction of both with\na lower weight attributed to MarginMSE [31]. However, SPLADE-V3 was trained on only 8-way triplets, considerably\nfewer than our 32-way approach."}, {"title": "Ablation Results", "content": "We present the results of all the ablation runs related to experiments detailed in the previous subsections in Table 3. As\nwe run our experiments sequentially", "hypothesis": "they do not appear to provide useful\ntraining to the model, even resulting in slightly decreased performance on MIRACL-small-ja. We believe this to be due\nto the factors we have highlighted with the main one being that the signal from distilling a teacher's score distribution\nover 32 documents appears to constitute a strong enough learning signal on its own. Additionally, some of our positive\nexamples may be false positives, and many of our negative examples are false negatives. In-batch negatives are costly\nto compute, as they require an additional scoring stage for every query against all the other queries' positive documents,\nand materialising and computing a cross-entropy matrix. Moreover, they place additional constraints on training as we\nneed to ensure higher data quality to fully leverage them. Removing the use of in-batch negatives thus represents an\nefficiency gain by lowering the compute and memory use of training at no performance cost. Following the results of\nthis experiment, we remove the use of in-batch negatives from our training recipe.\nScheduling The results of our experiments on learning-rate schedulers highlight schedule-free learning [12"}]}