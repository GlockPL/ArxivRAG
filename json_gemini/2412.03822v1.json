{"title": "Beyond the Binary: Capturing Diverse Preferences With Reward Regularization", "authors": ["Vishakh Padmakumar", "Chuanyang Jin", "Hannah Rose Kirk", "He He"], "abstract": "Large language models (LLMs) are increasingly deployed via public-facing inter-\nfaces to interact with millions of users, each with diverse preferences. Despite this,\npreference tuning of LLMs predominantly relies on reward models trained using\nbinary judgments where annotators select the preferred choice out of pairs of model\noutputs. In this work, we argue that this reliance on binary choices does not capture\nthe broader, aggregate preferences of the target user in real-world tasks. We propose\na taxonomy that identifies two dimensions of subjectivity where different users\ndisagree on the preferred output\u2014namely, the Plurality of Responses to Prompts,\nwhere prompts allow for multiple correct answers, and the Indistinguishability of\nResponses, where candidate outputs are paraphrases of each other. We show that\nreward models correlate weakly with user preferences in these cases.\nAs a first step to address this issue, we introduce a simple yet effective method that\naugments existing binary preference datasets with synthetic preference judgments\nto estimate potential user disagreement. Incorporating these via a margin term as\na form of regularization during model training yields predictions that better align\nwith the aggregate user preferences.", "sections": [{"title": "Introduction", "content": "A ubiquitous step in the training of contemporary large language models (LLMs) is aligning their\noutput with human preferences [1-5]. This process involves collecting human preference judgments\nover model outputs, which serve as the reward signal for either reinforcement learning with human\nfeedback [1, 6] or various direct alignment algorithms [7, 8]. Conventionally, a single human\nannotator provides a binary judgment for a pair of model outputs [9, 10].\nThis \"preferentist\" approach to alignment has been criticized [11-13], pitted against alternatives such\nas deliberative procedures that establish normative standards among diverse stakeholders [14, 15]\nor principle-based frameworks that seek higher-order rules or \u201cconstitutions\u201d to guide AI behavior\n[16-18]. However, preference judgments remain central to LLM post-training due to their cognitive\nand cost efficiency-it is far less demanding for annotators to choose between two outputs than to\narticulate an ideal demonstration\u2014and surprising efficacy in steering LLMs towards being more\nfriendly, helpful, and adept at interpreting user intent [1, 2, 19, 20]. So, even if preference fine-tuning\nis not the optimal solution, it is likely to remain a key component in LLM development for the\nforeseeable future, especially in industry settings."}, {"title": "Problem Formulation", "content": "The canonical method to train a reward model, $r_\\theta$, involves collecting a human preference judgment,\n$j$, that indicates which of two candidate outputs $y \\in \\{y_0, y_1\\}$ is preferred for a given input prompt $x$,\nresulting in dataset $D$. The model parameters are optimized by minimizing the loss function:\n$\\operatorname{loss}(r_\\theta) = -E_{(x,y_0,y_1,j)\\sim D}[\\log(\\sigma(r_\\theta(x, y_j) \u2013 r_\\theta(x, y_{1-j})))],$ (1)\nwhere $\\sigma$ denotes the sigmoid function and $j \\in \\{0,1\\}$.\nThe reward model parameters are trained to maximize the difference in score assigned to $y_j$ and\n$y_{1-j}$ based on the preference judgment, $j$. This is effective when $j$ is unanimously the 'better' output,\nas $j$ would likely be consistent with the preferences of any future user interacting with the model.\nHowever, we argue that in practical applications of LLMs, the models are frequently required to make\ndecisions on examples where there is legitimate disagreement among users. Consider a scenario\nwhere a set of $n$ users each provide a judgment, $j_{1...n}$, on the same example. The intended behavior\nof the model is to predict the aggregate preference of the population, specifically the fraction of users\nthat prefer option $y_0$ over $y_1$. We write this as:\n$p_{01}^* = \\frac{\\sum_{i=1}^n I(j_i = 0)}{n}$ (2)\nwhere $I(\\cdot)$ is and indicator function that equals 1 if user $i$ prefers option $y_0$, and 0 otherwise.\nHowever, this true preference $p_{01}^*$ is not accessible since we do not have prior knowledge about\nfuture user preferences. This can lead to poor reward model performance when $p_{01}^*$ deviates from the\nbinary judgment $j$ in Equation (1). In Section 3, we outline two dimensions along which subjective\nexamples can be identified (Section 3.1) and empirically demonstrate that the correlation between\nreward model predictions and the aggregate user preferences\u2014our estimate of $p_{01}^*$\u2014is weaker for\nthese subjective examples (Section 3.2).\nCan we use synthetic preference judgments to make more aligned predictions? As part of\nour initial efforts to mitigate this effect, we propose a simple modification to the reward modeling\nobjective. Here we take inspiration from Touvron et al. [3] who collect and incorporate the extent\nof user preferences between pairs of model outputs, i.e. significantly better vs slightly better, via a\nmargin term scaled from 0 to 1. We introduce a similar margin term that scales proportional to the\nestimated disagreement on the choice between outputs $y_0$ and $y_1$ given prompt $x$-the margin value\nshould be high when the choice between the two options is unanimous and vice versa. Our margin\nterm converts binary preference judgments from different annotators into a cardinal measure of the\nstrength of preference of the group as a whole. A gold standard implementation of the margin would\nbe to extend to the preference data collection process to obtain multiple judgments for each pair of\noutputs from different annotators. However, the prohibitive cost of re-annotating existing datasets\nmakes us favor a silver version that implements the margin using synthetic judgments from an LLM.\nGiven an input prompt and a two candidate outputs $(x, y_0, y_1)$, we sample $n$ synthetic judgments\n$j_{i=1...n} \\in \\{0,1\\}$ for a binary preference between the two outputs. We convert these into a margin\nterm $M_{x,y_0,y_1} \\in [0,1]$ as:\n$M_{x,y_0,y_1} = (\\sum_{i=1}^n j_i )^m / n$ (3)\nHere $m$ = 1 if all of the synthetic judgments are either 0 or 1, i.e. a unanimous preference in a\npopulation of annotators, and $m$ = 0 if half the of the synthetic judgments are 0 and 1 each, i.e. a\nhighly contested preference.\nThis margin term is incorporated into the loss objective as follows:\n$\\operatorname{loss}(r_\\theta) = -E_{(x,y_0,y_1,j)\\sim D}[\\log(\\sigma(r_\\theta(x, y_j) \u2013 r_\\theta(x, y_{1-j}) \u2013 M_{x,y_0,y_1}))]$ (4)\nWe test the effect of this intervention in Section 3.3 and demonstrate how this improves the correlation\nof reward model predictions to user preferences in subjective examples."}, {"title": "Breaking Down Reward Model Performance by Category", "content": "In this section, we present a categorization of examples according to the degree of disagreement in\naggregate preference among annotators (Section 3.1) and demonstrate that the performance of reward\nmodels drops in those categories with high levels of disagreement (Section 3.2). We then show\nhow the performance improves by incorporating margin-based regularization into the model training\npipeline (Section 3.3)."}, {"title": "Categories of Subjectivity", "content": "In current frameworks for training language models to learn human preferences, such as Reinforce-\nment Learning from Human Feedback (RLHF), the predominant method of preference annotation\nrelies on binary judgments between pairs of model-generated outputs. However, human preferences\nare inherently subjective and exhibit significant inter-individual variability. Hence, singular binary\nannotations may not sufficiently capture the range of opinions.\nTo better accommodate this variability in human judgment, we first propose to classify examples into\ntwo primary categories based on the nature of their prompts: (i) prompts that possess an objective,\nsingle correct answer, and (ii) prompts that are inherently subjective, characterized by admitting\nmultiple correct responses.\nMoreover, within existing datasets, we observe many instances where the two model-generated\nresponses are paraphrases of each other. This can occur in responses to both objective and subjective\nprompts. Therefore, we suggest a secondary dimension of categorization based on the distinguishabil-\nity of the two model-generated candidates: (i) responses that are distinguishable, and (ii) responses\nthat are indistinguishable paraphrases."}, {"title": "Evaluating Reward Model Performance", "content": "Experimental Setup We use a trained DeBERTa-V3 as the reward model for our experiments.\nOur goal is to evaluate its performance across examples in each of the aforementioned categories:\nMultiple/Single Correct Answer; Distinguishable/Indistinguishable Responses. We construct our\ntest set of 150 examples by randomly selecting 25 examples from each of six datasets. We use four\nin-domain (ID) datasets that were used to train the reward model-WEBGPT [40], HH-RLHF [2],\nOpen AI Summarize [6], and INSTRUCTGPT-J [41]. The remaining two datasets, PRISM [27] and\nULTRAFEEDBACK [42], serve as out-of-domain (OOD) sets used for testing purposes.\nWe recruit three human annotators to label the 150 example pairs manually. We first put the examples\ninto categories along both aforementioned dimensions via a majority vote. To estimate the aggregate\npreference of future users (Equation (2)), our annotators are instructed as follows: \"If you asked 10\npeople, how many would prefer answer A and how many would prefer answer B?\" This allows them\nto provide a judgment on a scale ranging from 10-0 to 0-10, and we average these responses to obtain a\nconsensus. Notably, this approach enables participants to first predict the believed subjectivity-even\nif they personally prefer answer A, they might project that only 2 out of 10 people would share their\npreference, thus providing insight into population-level preferences. Empirical evidence suggests\nthat this approach yields more reliable results than direct population votes asking each annotator\nfor their preference between answer A and answer B. We evaluate the performance of the reward\nmodel through the Pearson correlation (Table 1) and L1 loss (Table 2) between the collected human\npreference and this Baseline reward model (RM) predictions."}, {"title": "Regularization With Synthetic Preferences to Improve Performance", "content": "Having observed the decline in reward model performance on subjective examples, we test if training\nmodels with our proposed margin-based intervention results in better generalization to these examples.\nTo do so, we first obtain 10 synthetic annotations, $j_{i=1...10}$, for each example in the 4 training datasets\n(Section 3.2) using the Llama-3 70B Instruct model [43]. We generate synthetic annotations with"}, {"title": "Conclusion", "content": "In this work, we contend that reward models trained on binary preferences struggle to generalize to\nsubjective examples where different users may disagree on the preferred option. By categorizing\nexamples along two dimensions of subjectivity\u2014when the prompt allows for multiple correct answers\nand when the candidate outputs are paraphrases of each other\u2014we demonstrate that reward model\npredictions exhibit a weaker correlation with human preferences in these cases. To address this, we\npropose a margin-based regularization technique that mitigates this issue using synthetic annotations\nfrom an LLM to improve prediction quality in subjective scenarios. We re-iterate the scope of this\nsolution as a step to improve reward model performance as they are currently deployed, a technique\nto be used when recollecting large-scale preference data is not possible financially. We detail further\nsocial concerns that arise below."}, {"title": "Social Impacts Statement", "content": "Our work situates itself in a methodological issue of alignment fine-tuning: the overreliance on\nsingle-annotator binary judgments. We propose regularization with synthetic preferences a simple,\npragmatic solution that offers a quick and easily implementable patch to improve the calibration\nof preference signals, especially in subjective regions of input-output space and in the absence of\nextensive additional human data. However, our work raises important epistemological and ethical\nconsiderations. Our assumption that LLMs can better approximate preference heterogeneity than\nindividual annotators has mixed empirical validation [46\u201349]. The use of synthetic annotations,\nwhile a pragmatic solution, may introduce algorithmic biases that could affect model outputs in\nopaque ways, misrepresenting populations of users [50, 51]. Our demonstrated implementation\nadopts a utilitarian approach, simply taking a majority vote for preference aggregation. While our\nmethod is flexible to other specifications, our work does not resolve fundamental ethical questions\nabout how to aggregate conflicting preferences or fairly represent minority viewpoints without\nsuccumbing to the tyranny of the majority. The assumption that we need to aggregate preferences\nmay itself be flawed. In contrast to seeking a monolithic LLM with lofty ambitions to simultaneously\nrepresent the preferences of a vast user base, a promising path to pluralistic AI may come from a\nmore granular unit of alignment via personalization or community-specific steering and cultural\nfine-tuning [52]. We present this work not as a definitive solution, but as a contribution to the ongoing\ndiscourse on responsible, safe, and inclusive AI development, emphasizing the need for continued\ninterdisciplinary research to address the sociotechnical and normative challenges central to technical\nalignment methodologies."}, {"title": "Prompts for Obtaining Synthetic Preference Judgments", "content": "To gather synthetic preference judgments from Llama-3 in Section 3.3, we use the following prompt:\nPrompt for Obtaining Synthetic Preference Judgments\nI am going to give you a prompt and two answers.\nThe goal is to identify the better answer.\nIf the prompt is a question, we want the factually correct answer.\nIn a conversation, we want helpful replies that do not cause harm.\nThe output format should be either 'Choice: A' or 'Choice: B' based on the selected answer\nand nothing else.\nPrompt: {prompt}\nAnswer A: {answer_0}\nAnswer B: {answer_1}"}, {"title": "Annotator Guidelines", "content": "We outline the guidelines provided to annotators for conducting the manual annotations in Section 3.2:\nTask 1: Subjectivity\nConsider the following prompt: {prompt}\nIs there a single factually, correct response to this prompt?\nOptions:\nA) There is only one single correct choice to this prompt.\nB) There are multiple possible correct answers to this prompt"}, {"title": "Task 2: Distinguishability", "content": "Consider the following two candidate model outputs in response to the prompt: {prompt}\nAnswer A: {answer_0}\nAnswer B: {answer_1}\nCan you clearly tell the two responses apart? Are they significantly different from one\nanother?\nOptions:\nA) Both choices are essentially the same content, just with minor stylistic variations.\nB) The two choices are clearly different from each other and distinguishable."}, {"title": "Task 3: User Preference Estimation", "content": "If you asked 10 different people, how many would prefer answer A and how many would\nprefer answer B?\n1) 10 A - 0 B\n2) 9 A - 1 B\n3) 8 A-2 B\n4) 7 A-3 B\n5) 6 A - 4 B\n6) 5 A-5 B\n7) 4 A - 6 B\n8) 3 A-7 B\n9) 2 A - 8 B\n10) 1 A - 9 B\n11) 10 A - 0 \u0412"}]}