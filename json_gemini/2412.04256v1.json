{"title": "Transient Multi-Agent Path Finding for Lifelong Navigation in Dense Environments", "authors": ["Jonathan Morag", "Noy Gabay", "Daniel Koyfman", "Roni Stern"], "abstract": "Multi-Agent Path Finding (MAPF) deals with finding conflict-free paths for a set of agents from an initial configuration to a given target configuration. The Lifelong MAPF (LMAPF) problem is a well-studied online version of MAPF in which an agent receives a new target when it reaches its current target. The common approach for solving LMAPF is to treat it as a sequence of MAPF problems, periodically replanning from the agents' current configurations to their current targets. A significant drawback in this approach is that in MAPF the agents must reach a configuration in which all agents are at their targets simultaneously, which is needlessly restrictive for LMAPF. Techniques have been proposed to indirectly mitigate this drawback. We describe cases where these mitigation techniques fail. As an alternative, we propose to solve LMAPF problems by solving a sequence of modified MAPF problems, in which the objective is for each agent to eventually visit its target, but not necessarily for all agents to do so simultaneously. We refer to this MAPF variant as Transient MAPF (TMAPF) and propose several algorithms for solving it based on existing MAPF algorithms. A limited experimental evaluation identifies some cases where using a TMAPF algorithm instead of a MAPF algorithm with an LMAPF framework can improve the system throughput significantly.", "sections": [{"title": "Introduction", "content": "The Multi-Agent Path Finding (MAPF) problem deals with finding conflict-free paths for a set of agents from an initial configuration to a given target configuration. MAPF problems manifest in real-world applications such as automated warehouses (Ma et al. 2017; Li et al. 2021b; Morag, Stern, and Felner 2023), and a wide range of MAPF algorithms have been proposed (Felner et al. 2017). In many MAPF applications, it is often necessary to solve an online version of MAPF called Lifelong MAPF (LMAPF) (Li et al. 2021b; \u0160vancara et al. 2019), where when an agent reaches its target, it is assigned a new target. LMAPF has attracted significant interest in both academia and industry, including a recent LMAPF competition that was sponsored by Amazon Robotics.\u00b9 The common approach for solving LMAPF problem is to treat it as a sequence of classical MAPF problems, where each MAPF problem deals with finding paths from the agents' current configuration to their up-to-date targets."}, {"title": "Background", "content": "A Multi-Agent Path Finding (MAPF) problem is defined by a graph G = (V, E); a set of agents A; and a source and target vertex for each agent ai \u2208 A, denoted si and ti, respectively. A solution \u03c0 to a MAPF problem is a mapping of each agent ai \u2208 A to a path \u03c0\u2081 in G such that \u03c0\u2081 starts at si, ends at ti, and does not conflict with any path that is mapped in \u03c0 to a different agent. \u00b2 In this work, we consider as a conflict only vertex conflicts and swapping conflicts (Stern et al. 2019). A pair of paths \u03c0\u03af and \u03c0j has a vertex conflict if there exists an index x such that \u03c0\u2081[x] = \u03c0j[x], where \u03c0\u2081[x] is the xth vertex in the path \u03c0\u03af. A swapping conflict occurs if there exists an index x such that \u03c0\u2081[x] = \u03c0;[x + 1] and \u03c0\u2081[x + 1] = \u03c0; [x]. The length of a path is defined as the number of vertices (non-unique) in the path, minus one (len(\u03c0\u03b9) \u2013 1). This version of MAPF is known as Classical MAPF. Sum Of Costs (SOC) and Makespan are common MAPF solution cost function. SOC is the sum of the lengths of all paths in the solution (SOC(\u03c0) := \u2211(len(\u03c0\u2081) \u2212 1)|\u03c0\u03af \u2208 \u03c0)) and makespan is the length of the longest path in the solution (Makespan(\u03c0) := max(len(\u03c0i) \u2013 1|\u03c0\u03af \u0395 \u03c0)). A solution \u03c0is optimal for a given MAPF problem w.r.t. a given cost function if its cost is smaller than or equal to the cost of any solutions to that MAPF problem. Some MAPF algorithms, such as Conflict Based Search (CBS) (Sharon et al. 2015) guarantee the solution they return is optimal, while others, such as Prioritized Planning (PrP) (Bennewitz, Burgard, and Thrun 2001) do not.\nLifelong Multi-Agent Path Finding (LMAPF) (Li et al. 2021b; Morag, Stern, and Felner 2023) is a generalization of MAPF in which whenever an agent reaches its target, it may be assigned a new target to reach. Due to its online nature, an LMAPF problem is typically solved by repeatedly interleaving planning and execution. In every planning period a path for each agent is computed. Then, a prefix of the planned paths is executed, and another planning period starts. This process continues indefinitely until interrupted by the user. The performance of an LMAPF algorithm is commonly evaluated using throughput, which counts how many times agents reached their targets before a fixed number of time steps have passed. As noted above, a common approach for solving LMAPF is by iteratively calling a MAPF solver every planning period to find paths for the agents from their current locations to their current targets. Different MAPF solvers have been used for this purpose, including optimal and suboptimal MAPF algorithms."}, {"title": "MAPF-LMAPF Mismatch and Mitigations", "content": "The role of an LMAPF algorithm is to output plans for the agents in every planning period. We say that an LMAPF algorithm is complete if it does so, and incomplete otherwise. The example in Figure 1 shows that the common approach of solving LMAPF as a sequence of MAPF problems yields incomplete LMAPF algorithms. Several techniques have been proposed for mitigating, to some extent, this incompleteness.\nDynamic replanning This means whenever a target is reached, all agents immediately replan (Varambally, Li, and Koenig 2022). This option is not feasible in some cases since it requires significant computational cost. Moreover, this approach does not solve the problem in Fig. 1 as the agents will fail to find any MAPF solution.\nPre-assigned sequence of goals This means each agent is given a sequence of targets to reach and the LMAPF algorithm returns plans to visit the next couple of targets (Grenouilleau, Van Hoeve, and Hooker 2019). Unfortunately, it is not always possible to know such a sequence of tasks for each agent in advance. Also, the planning itself is harder when planning to visit more than one node. Moreover, this technique is also incomplete in some cases.\nDummy paths This means that after an agent reaches its target, it will follow a pre-defined path so as to avoid causing congestion near its target (Liu et al. 2019). This mitigation strategy still suffers the MAPF limitation that a specific configuration of the agents must be reached. Indeed, identifying effective dummy paths is not trivial and poorly located dummy paths may result in an incompleteness scenario similar to Fig. 1. This technique may also make planning more difficult, as it requires longer paths.\nLimited planning horizon This means ignoring conflicts that are further than w time steps in the future when planning, where w is a parameter. This technique is embodied in the Rolling Horizon Collision Resolution (RHCR) framework (Li et al. 2021b), which is the state of the art in LMAPF. Common implementations of RHCR use LNS (Li et al. 2021a) or PrP for planning, but any MAPF algorithm can be easily adapted for this purpose. While using a limited planning horizon is common, it may lead to deadlocks and to inefficiencies stemming from myopic planning. We show such an example in Fig. 2\nFail policies Morag et al. (2023) proposed an LMAPF framework designed to handle planning periods in which the underlying MAPF algorithm failed to return a plan. They proposed several fail policies for this purpose and techniques for utilizing partial solutions the MAPF algorithm may return. An LMAPF algorithm with a fail policy can be viewed as a complete LMAPF algorithm, yet the fail policies proposed so far are ad-hoc and may be inefficient.\nIn summary, while current techniques partially reduce the impact of the MAPF-LMAPF \u201cmismatch\u201d, they do so indirectly and often in an incomplete manner. In this work, we propose a more direct approach to address this mismatch by modifying the type of MAPF problem being solved in every planning period. We call this modified MAPF problem Transient MAPF (TMAPF) and define it below."}, {"title": "A Transient Version of MAPF", "content": "The input to a TMAPF problem is the same as the input for MAPF. The solution to a TMAPF problem is also a mapping of each agent to a path in G. The key difference between MAPF and TMAPF is that in a solution to a MAPF problem the path \u03c0\u2081 mapped to each agent a\u017c must end at ti, in"}, {"title": "Experimental Results", "content": "We conducted an experimental evaluation of our TMAPF algorithms when solving LMAPF problems on standard grids from the grid-based MAPF benchmark (Stern et al. 2019) and on the pathological cases shown in Fig. 1 and 2. In each experiment, we compare the performance of solving LMAPF problems using either a MAPF or a TMAPF algorithm in every planning period. Specifically, we used PIBT, PrP, CBS, and LNS, and the TMAPF versions of PrP, CBS, and LNS we developed, denoted as PrPt, CBSt, and LNSt. Our LNS implementation only used the Map-Based and Random Destroy Heuristics. All experiments were run on Linux virtual machines in a cluster of AMD EPYC 7763 CPUs, with 16GB of RAM each.\nBenchmark Grids This set of experiments were conducted on grids from the standard MAPF benchmark (Stern et al. 2019). All algorithms were run within the RHCR (Li et al. 2021b) framework. The experiments included LMAPF problems with between 25 and 1000 agents, limited by the maximal number of agents available for each problem in the benchmark. The number of time steps between planning iterations was set to 5, and the planning horizon (w) was set to 10, a configuration which was shown to be generally effective in previous works (Li et al. 2021b; Morag, Stern, and Felner 2023). The amount of (real) time for planning at each planning iteration was 5 seconds. The metric we measured was the throughput at time step 1000.\nWe generated LMAPF instances based on the grid-based MAPF benchmark, generating new targets by selecting locations in the grid randomly (uniformly). Consequently, multiple agents may have the same target at the same time. TMAPF algorithms handle this property organically, while MAPF cannot handle this without planning paths where agents wait until the horizon is exceeded. We run all our experiments within the robust LMAPF framework described by Morag, Stern, and Felner (2023), which includes selecting the agents that should plan in every planning period and determining a fail policy for cases where the planner fails to plan within a given time budget.\nIn all maps, we observed no significant advantage for using TMAPF, i.e., PrPt exhibited similar results to PrP and LNS exhibited similar results to LNSt. This may be due to the limited planning horizon, which, as mentioned above, can mitigate the effect of the LMAPF-MAPF mismatch. To challenge our algorithms further, we repeated this LMAPF experiments but instead of generating targets randomly from any location in the map, we randomly sampled from a restricted set of 10, 20, 30, or 40 targets. One may argue that these experiments are more realistic than spreading the targets uniformly, especially in real-world applications of LMAPF. For instance, automated warehouses often have fixed picking stations and charging stations that would make certain locations more or less likely to appear as an agent's target. Similarly, in traffic control, locations such as public transit hubs or office buildings would be common targets for vehicles. Table 1 shows the results of the dense target experiments on maps empty-48-48 and warehouse-20-40-10-2-1. Results on other maps and numbers of agents show similar trends and are available in the supplementary materials. For every map and number of agents, we highlighted in bold the algorithm that achieved the highest throughput between every TMAPF algorithm and its MAPF counterpart. The results show that PrPt and LNSt yield a relatively higher throughput when the number of possible targets is limited. For example, the highest throughput achieved\nNext, we evaluated all algorithms on two special LMAPF problems. The first LMAPF problem we consider is based on the example depicted in Fig. 1. Agent a1 starts at s\u2081 and is assigned targets going back and forth between s\u2081 and t1, indefinitely. The same is done for agent a2 with vertices 82 and t2. For example, a\u2081 will be assigned targets [t1, 81, t1, 81...] . As discussed above, without limiting the planning horizon using a MAPF solver results in planning failure, as the agents must swap their positions, which is impossible. Limiting the planning horizon by using RHCR helps mitigate this problem, which raises the question is this a substantial problem in LMAPF? We solved this LMAPF problem with a planning horizon of 10, running for 500 time steps, and using PrP, PrPt, and PIBT. We found that PrP and PrPt both achieved a throughput of 500, whereas PIBT achieved a throughput of 320. In comparing A* expansions, PrP required a total of 13,509 expansions, whereas PrPt required only 11,965 expansions. The total runtime was 56ms for PrP, 50ms for PrPt, and 163ms for PIBT. This experiment demonstrates how, even when using RHCR, the new TMAPF solver, PrPt, had advantages in runtime over the MAPF solver PrP, and in throughput over the existing TMAPF solver PIBT.\nThe second special LMAPF problem we consider is illustrated in Figure 2. Here there are two agents, a\u2081 and a2, tasked with moving repeatedly between s\u2081 and t1, or s2 and t2, respectively. All agents are allowed to re-plan at every time step. Consider a planning horizon of 1 is used. Both the MAPF and TMAPF solvers will behave as seen in Figure 2(a), moving towards each other, and then waiting in place indefinitely, always assuming that once the planning horizon passes they will be able to swap their positions and reach their targets. This results in a throughput of 0 for both solvers. The same would happen with a horizon of 2 for the TMAPF solver, and any horizon < 5 for the MAPF solver. Second, assume an infinite planning horizon. The behavior of the MAPF solver is illustrated in Figure 2(b), where it will have a\u2081 move counter-clockwise towards its target, while a2 attempts to reach its target by moving counter-clockwise. After two time steps, agent a\u2081 reaches its target and re-"}, {"title": "Conclusion and Future Work", "content": "In this work, we proposed solving LMAPF problems by solving a sequence of MAPF variants that we call Transient MAPF (TMAPF), in which agents do not have to stay at their targets after visiting them. We described that such a variant is needed to address a mismatch between the requirements of LMAPF and MAPF, and showed how to adapt existing MAPF algorithms to solve it. We evaluated using our TMAPF algorithms within RHCR to solve lifelong MAPF. Our results showed no significant advantage for using TMAPF algorithms when the targets are spread uniformly. However, when the targets are limited to a fixed small number of locations, using the TMAPF algorithms is"}]}