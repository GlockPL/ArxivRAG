{"title": "Text-Guided Multi-Property Molecular Optimization with a Diffusion Language Model", "authors": ["Yida Xiong", "Kun Li", "Weiwei Liu", "Jia Wu", "Bo Du", "Shirui Pan", "Wenbin Hu"], "abstract": "Molecular optimization (MO) is a crucial stage in drug discovery in which task-oriented generated molecules are optimized to meet practical industrial requirements. Existing mainstream MO approaches primarily utilize external property predictors to guide iterative property optimization. However, learning all molecular samples in the vast chemical space is unrealistic for predictors. As a result, errors and noise are inevitably introduced during property prediction due to the nature of approximation. This leads to discrepancy accumulation, generalization reduction and suboptimal molecular candidates. In this paper, we propose a text-guided multi-property molecular optimization method utilizing transformer-based diffusion language model (TransDLM). TransDLM leverages standardized chemical nomenclature as semantic representations of molecules and implicitly embeds property requirements into textual descriptions, thereby preventing error propagation during diffusion process. Guided by physically and chemically detailed textual descriptions, TransDLM samples and optimizes encoded source molecules, retaining core scaffolds of source molecules and ensuring structural similarities. Moreover, TransDLM enables simultaneous sampling of multiple molecules, making it ideal for scalable, efficient large-scale optimization through distributed computation on web platforms. Furthermore, our approach surpasses state-of-the-art methods in optimizing molecular structural similarity and enhancing chemical properties on the benchmark dataset. The code is available at: https://anonymous.4open.science/r/TransDLM-A901.", "sections": [{"title": "1 INTRODUCTION", "content": "Molecule generation has made significant strides with the rapid advancement of generative models, especially conditional generative models [13, 22]. These models have demonstrated promising results during tasks such as drug response prediction (DRP) and drug-target binding affinity (DTA) [14, 23]. Although the molecules generated for specific tasks possess relevant properties, they are still insufficient for application to industrial production. As a result, optimizing generated molecules has become a crucial task, drawing the attention of scientists seeking to improve their usability. However, improving the desired drug candidate properties while retaining the original structural scaffolds is challenging. Moreover, the complexity of enormous chemical space highlights the substantial quantity of molecular candidates. Consequently, traditional molecular optimization (MO) methods primarily depend on the experience, knowledge and intuition of chemists, thus resulting in time-consuming manual labor and reducing the likelihood of finding ideal molecules within a limited time.\nTo address these challenges, early computational approaches using deep learning generated computational strategies to accelerate the traditional MO paradigm [16, 28, 46]. These deep learning methods mainly learned from Simplified Molecular Input Line Entry System (SMILES), graphs and three-dimensional (3D) structures [15, 40, 52, 55]. To generate molecules with desired properties, conditional generative models [31, 35, 38] have been adopted as auxiliary controllers for the generative process. However, most of these models concentrate on generating molecules from scratch and optimizing them according to specific rules, thereby neglecting the priority of core scaffold retention during molecular optimization. Consequently, molecules optimized through these methods fail to meet the industrial demands, which requires slight changes to the molecular architecture and substantial increases in the desired properties.\nOther widely adopted approaches include guided search-based methods, which aim to find target molecules by exploring compounds' chemical or latent spaces derived from encoder-decoder models. First, latent space search involves encoding a source molecule into a low-dimensional representation, and exploring its adjacent area to find embeddings that meet the specified constraints [28, 30, 58]. Then, these embeddings are decoded into the chemical space, and a property predictor is used to guide the search process. In contrast, chemical space search methods operate directly within the high-dimensional and discrete chemical space to find molecules that meet given constraints. Various advanced optimization techniques, including reinforcement learning and genetic algorithms, have been applied to this search method.\nDespite their proven success, a significant limitation of these guided search-based methods lies in their reliance on external property predictors to iteratively optimize molecular properties. As shown in Fig. 1, external predictors, though effective for estimating target properties, inherently introduce errors and noise due to the nature of approximation. Typically, these methods are trained on finite, often biased datasets and may lack complete generalization in novel chemical spaces, resulting in inaccurate property predictions during the search process. This discrepancy can accumulate over iterations, leading to suboptimal molecular candidates or failure to meet the intended constraints. Moreover, the noise generated by these predictors may cause the search process to deviate from the optimal regions in the chemical or latent spaces, further reducing MO efficiency and effectiveness.\nTo address the aforementioned MO issues, we explore the utilization of diffusion models in target-oriented MO. Diffusion models [20] have reaped remarkable success in other scientific disciplines, such as computer vision [45], by generating high-quality data through a gradual denoising process that can capture complex distributions [54]. Accordingly, we propose text-guided multi-property molecular optimization with a transformer-based diffusion language model (TransDLM). This is a novel approach that leverages a diffusion language model to iteratively yield word vectors of molecular SMILES strings and is guided by language descriptions. Due to SMILES strings' deficiency in explicit semantic clarity of molecular structures and functional groups, we adopt standardized chemical nomenclature as informatively and intuitively molecular semantic representation. To differentiate generation from scratch, we sample molecular word vectors from the token embeddings of source molecules encoded by a pre-trained language model, which collectively and significantly retains the original molecules' core scaffolds. Instead of relying on an external property predictor, we encode the molecular structure and property information using a pre-trained language model, implicitly embedding property requirements into textual descriptions, guiding diffusion and preventing error propagation. This encoded representation serves as a guiding signal during the diffusion process. By embedding the desired molecular attributes through the language model, the diffusion process is trained on the molecule's structure and the specific properties we aim to optimize. Additionally, our method is suitable for deployment on web-based platforms due to its high parallel efficiency, enabling large-scale MO across distributed environments. This allows researchers to collaborate and iterate molecular design and optimization processes seamlessly via web interfaces. The TransDLM approach's advantages are as follows:\n\u2022 Molecules generated by TransDLM retain the core scaffolds of source molecules and structural similarity is ensured while satisfying multiple property requirements.\n\u2022 Error and noise propagation are reduced by directly training the model on the desired properties during diffusion, thereby improving the reliability of the optimized molecules.\n\u2022 TransDLM outperforms state-of-the-art methods on the benchmark dataset in optimizing three ADMET \u00b9 properties, LogD, Solubility, and Clearance while maintaining the structural similarity over several metrics."}, {"title": "2 RELATED WORK", "content": "This section provides an overview of the research landscape, focusing on different MO approaches and the role of diffusion models in language generation. These advancements establish the conditions for our novel diffusion language model application in the SMILES-based MO field."}, {"title": "2.1 Molecular Optimization", "content": "Methods that optimize molecules are classified into three categories: distribution matching-based, guided search-based, and molecular mapping-based approaches.\nDistribution Matching-Based Methods. Previous study [53] has indicated that the matched molecular set (MMS) is used in distribution matching-based methods. In an MMS, the molecular set with poor properties is named the source, and the set with the desired qualities is designated as the target. Molecular distribution matching focuses on learning the mapping relationship between two molecular sets in an MMS dataset. This ensures that the source and target molecular set are adjacent. For instance, Maziarka et al. [38] proposed a CycleGAN-based model that encodes each molecule as a point within the latent space by learning the MMS representation distribution. Barshatski et al. [1] designed a multi-property optimization technique called IPCA, which learns a distinct transformation for each optimized property while imposing constraints on the latent embedding space across all transformations. However, distribution matching-based methods are limited, as they fail to optimize specific molecules towards their desired counterparts. Since adjusting the distribution of the whole molecular set is their"}, {"title": "2.2 Language Generation with Diffusion Models", "content": "Diffusion models have been extremely successful in generating content in continuous domains, particularly in the images and audio fields [54]. Stable Diffusion [45] and AudioLDM [36] are categorized as diffusion models in continuous domains. They introduced random noises into latent variables based on latent diffusion models (LDMs) and reverse the process through a series of denoising steps to learn data generation. However, differences in data structure between languages and images exist, since languages are discrete while images exist in continuous domains. In order to tackle this problem, certain approaches retain the discrete nature of text and extend diffusion model to handle discrete data [18, 43]. On the contrary, other methods utilize embedding layers to map the text to a continuous representation domain, thereby preserving the continuous diffusion steps [11, 34, 48, 57]. Our research aligns with the latter strategy, focusing on word vectors and substantially expanding the functionality and feasibility of diffusion models in SMILES-based MO."}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 Overall Framework", "content": "With the solid foundation paved by conditional language generation tasks, exemplified by DiffuSeq [11] and SeqDiffuSeq [57], exploring text-guided MO has become feasible. In this paper, we aim to optimize a molecule until it matches a specified textual description. Formally, let C = [W0, W1, . . ., wm] represent the input description, where wi is the i-th word in the sequence, and m denotes the text length. Our objective is to develop a model F that takes this text and the original molecule Mo as input and produces the corresponding molecule as output, which can be mathematically represented as M = F(Mo, C). Overall, our TransDLM is composed of four pivotal processes: embedding, noising, denoising, and rounding. As shown in Fig. 2, these processes work in tandem to produce the desired optimized molecules.\nThe embedding process begins by treating the text sequence W = [W0, W1,..., wn] as an ordered list of words. Each word wi is mapped using an embedding function, yielding Emb(W) = [Emb(wo), Emb(w\u2081), ..., Emb(wn)] \u2208 Rd\u00d7n, where n indicates the sequence length and d represents the embedding dimension. Then, the noising process starts with the matrix x0, which is sampled from a Gaussian distribution centered at Emb (W): xo ~ N(Emb(W), \u03c3\u03bf\u0399). During the noising process, noise is gradually introduced to xo, eventually resulting in pure Gaussian noise x ~ N(0, I), where T is a hyperparameter of total diffusion steps. The transition from Xt-1 to xt is defined as follows:\n$q(x_t | x_{t-1}) = N(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI),$ (1)\nwhere \u03b2t is a hyperparameter representing the amount of noise added at diffusion step t \u2208 (0, \u03a4].\nNext, the denoising process begins with an encoded source molecule, and sequentially samples xt-1 from xt, gradually reconstructing the original content. Typically, a neural network is trained to predict xt-1 given xt. To improve the accuracy of denoising xt towards specific word vectors, a neural network fo is trained to predict xo directly from xt. Thus, the denoising transition from xt to xt-1 can be expressed as:"}, {"title": "3.2 Description Generator", "content": "Molecular Structure Feature. Molecular structure feature effectively enhances insights into molecules and model performance in specific tasks [4, 8, 51]. Among them, fragment-based molecular structure feature has garnered substantial attention from researchers [12, 56]. Accordingly, we disassemble molecules using Recap [33], which cleaves the structures into fragments based on retrosynthetic chemistry. Once key fragments are recognized as essential for chemical reactivity, they can easily be divided into latent blocks for optimization. After molecular disassembly, we concentrate on how the fragments connect with each other. RDKit 2 is an open-source Python software package providing chemical information. We use its built-in functions to determine the binding sites and connection modes between fragments.\nIUPAC Nomenclature in Organic Chemistry. The international union of pure and applied chemistry (IUPAC) [26] nomenclature system provides standardized naming conventions for chemical compounds, which is crucial for avoiding ambiguities arising from common or trivial names. This is particularly crucial in organic chemistry, where compound complexity can make identification challenging. When optimizing molecules guided by textual descriptions, we use the IUPAC names instead of SMILES strings to represent fragments, simultaneously complementing IUPAC information of the source molecules as molecular semantic representations. As shown in Fig. 3, on the one hand, IUPAC names provide a detailed and hierarchical description of a molecule's structure, including functional groups, stereochemistry, branching, and chain length. This level of granularity can potentially enhance the precision of the generative process, especially when targeting specific chemical properties or structures. On the other hand, IUPAC nomenclature conventions provide more semantic information compared to"}, {"title": "3.3 SMILES Tokenizer", "content": "Inspired by Gong et al. [10] who considered the fact that tokenizing every single a SMILES string character destroys the uniformity between multi-character units, we employ the same strategy to retain the semantic groups in SMILES strings. For instance, [CH3-] is a negatively charged methyl group rather than a sequence assembled by unrelated characters, such as [, C, and H. With all the semantic groups together forming our vocabulary, TransDLM is capable of encoding any SMILES string semantically and integrally, like CC([Si]=O)C[CH3-] being encoded as [[SOS],C,C,(,[Si],=,O,),C,[CH3-],[EOS],[PAD],...,[PAD]]. And we pad each sequence to the maximum length n.\nFollowing tokenization, the resulting tokenized molecule M is prepared for the embedding process, yielding xo through sampling from the distribution x0 ~ N(Emb(M), \u03c3\u03bf\u0399)."}, {"title": "3.4 Text-Guided Molecular Optimization", "content": "Unlike traditional diffusion models that sample from pure noise [34, 45], we propose a novel generation strategy for TransDLM, which samples from an encoded source molecule SMILES string, as mentioned in Chapter 3.3. Our approach effectively addresses the uncertainty and infeasibility of previous MO methods and adjusts the orientation to the desired optimized molecules.\nFor the purpose of converting textual description into machine-readable language, a pre-trained language model is utilized to map the text sequence C to its latent embeddings C \u2208 Rd\u2081\u00d7m, where d\u2081 represents the embedding dimension of the language model output. Our model is based on transformer framework, which functions as fe in Eq. (2). Considering the current diffusion state xt, time step t, and text embeddings C, the initial state xo is predicted as xo = fo (xt, t, C). Given that self-attention mechanism is disordered, the transformer architecture does not inherently have the location information. Hence, the transformer loses the input's sequential information. As a result, we adopt a position encoding strategy [50] in the first layer z(0) z to enable our model to recognize the position of each element in the input sequence, defined as:\n$Z_t^{(0)}= PosEmb + Winxt + TimeEmb(t),$ (3)\nwhere PosEmb is the positional embedding, TimeEmb is an operation to embed the time step t into a high-dimensional vector, Win \u2208 Rd2\u00d7d, z(0) \u2208 Rd2\u00d7n, and d2 is the dimension for transformer.\nIn the standard transformer self-attention mechanism, each token performs attention calculations on others in the same input sequence. Therefore, to improve the multi-modal data processing capability of our model and enhance the positive guidance of textual description to MO, TransDLM uses the cross-attention mechanism to calculate the input sequence with textual description as an additional context. Specifically, our transformer backbone is composed of L layers with a cross-attention block respectively, which incorporates the textual description into the hidden states as follows:\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V,$ (4)\n$Q = W_Q z^{(i)},$\n$K = W_K MLP(C),$\n$V = W_V MLP(C),$\nwhere $W_Q^{(i)} \\in R^{d_2\\times d_2}$ represents the learnable parameters in the i-th layer, and MLP is the multilayer perception.\nAfter completing the above procedures, we obtain the matrix xo from the initial embedded source molecule, guided by the textual description C. Then, the matrix x0 is prepared for rounding and subsequently transformed into a SMILES string."}, {"title": "3.5 TransDLM Training", "content": "During the training process, our methodology mainly maximizes the variational lower bound (VLB) of the marginal likelihood. Moreover, we refine and adapt our approach according to the insights from previous studies [20, 34]. As the training procedure shown in Algorithm 1, we primarily aim to train the neural network, denoted as fo, to progressively reconstruct the original data xo at every time step within the denoising diffusion trajectory, described as:\n$L_{M,C} = E_{q(x_{0:T} \\vert M)}[\\sum_{t=1}^T ||f_{\\theta}(x_t,t,C) - x_0||^2 - log p_{\\theta}(M|x_0)],$ (5)\nwhere pe (Mxo) represents the rounding process, featuring a product of the softmax distribution pe(\u039c|xo) = \u03a0i=0p(ai|xo[:,i]).\nBy iteratively refining the noisy intermediate states to their original forms, the TransDLM learns to effectively reverse the diffusion process, leveraging underlying data structure and guiding text."}, {"title": "4 THEORETICAL ANALYSIS", "content": "This section theoretically demonstrates that text-guided multi-property MO based on diffusion language models is more effective in reducing errors than traditional methods that use external property predictors to constrain optimization."}, {"title": "4.1 Gradient Error Calculation", "content": "The loss function of traditional methods usually consists of attribute and structure loss, which can be expressed as:\n$L_{trad}(M) = \\alpha \\cdot L_{property}(M) + \\beta \\cdot L_{structure}(M)$\n$= \\alpha \\cdot (\\hat{y}(M) - y_{target})^2 + \\beta \\cdot ||M - M_{target}||^2,$\n(6)\nwhere \u0177(M) is the predicted attribute value of the molecule M by the external property predictor, Mtarget denotes the target molecular structure, and \u03b1 and \u03b2 are hyperparameters that adjust the attribute and structure losses' weights. As a result, inferred from Eqs. (5) and (6), the respective gradient of molecule M are:\n$\\nabla_{\\theta} L_{M,C} =2 E_{q(x_{0:T} \\vert M)}[(f_{\\theta}(x_t,t,C) - x_0) \\nabla_{\\theta} f_{\\theta}(x_t,t,C)] - \\nabla_{\\theta} log p_{\\theta}(M|x_0),$\n(7)\n$\\nabla_M L_{trad} = \\alpha \\cdot \\nabla_M L_{property} + \\beta \\cdot \\nabla_M L_{structure}$\n$= 2\\alpha (\\hat{y}(M) - y_{target})\\nabla_M \\hat{y}(M) + 2\\beta (M - M_{target}),$ (8)\nErrors occur in the attribute loss's gradient due to the ones within the external property predictor \u0177(M). Thus, we define the following:\n\u2022 Attribute prediction error: ext = \u0177(M) \u2013 y(M).\n\u2022 Attribute gradient prediction error: grad = \u2207\u043c\u0177(M)-\u2207\u043c\u0443(\u041c).\n\u2022 Actual attribute values and gradients: y(M) and \u2207\u043c\u0443(M).\nTherefore, the error of attribute loss gradient is:\n$\\epsilon_{grad, property} = \\nabla_M L_{property} - \\nabla_M L_{property}$\n$= 2[(\\epsilon_{ext})\\nabla_M y(M) + (y(M) - y_{target})\\epsilon_{grad} + \\epsilon_{ext}\\epsilon_{grad}],$ (9)\nThe gradient error of our model mainly arises from the approximation error. Due to the direct fitting of xo and the use of text"}, {"title": "4.2 Comparison of Gradient Error Variance", "content": "For traditional methods, the attribute loss gradient error is:\n$\\sigma_{grad, trad}^2 = 4\\alpha^2 [Var (\\epsilon_{ext} \\nabla_M y(M))$\n$+ Var ((y(M) - y_{target}) \\epsilon_{grad}) + Var (\\epsilon_{ext}\\epsilon_{grad})]$\n$= 4\\alpha^2 [(||\\nabla_M y(M)||^2 \\sigma_{ext}^2 + (y(M) - y_{target})^2 \\sigma_{grad}^2 + \\sigma_{ext}^2\\sigma_{grad}^2],$ (11)\nwhere we assume ext and grad are independent and have an expected value of zero and y(M) \u2013 ytarget and \u2207\u043c\u0443(M) are deterministic or their variances can be ignored.\nSimilarly, our TransDLM's gradient error variance is:\n$\\sigma_{grad, our}^2 = 4 Var ((f_{\\theta}(x_t,t,C) - x_0) \\nabla_{\\theta} f_{\\theta}(x_t,t,C))$\n$= 4[(E[\\nabla_{\\theta} f_{\\theta}(x_t,t,C)])^2\\sigma_{model}^2 + \\sigma_{grad, model}^2 + \\sigma_{model}^2 \\sigma_{grad, model}],$ (12)\nFor traditional methods, the gradient error variance depends on the model's gradient calculation error $ \\sigma_{grad}^2$ and the error \u03c3ext of the external property predictors. Due to the potential for significant errors in external predictors, this can significantly increase the overall gradient error variance. On the contrary, the endogenous errors in TransDLM are mainly controlled by  and  and do not rely on external property predictors. As a consequence, the variance term caused by external errors is reduced.\nAssuming that the fitting ability of traditional models and Trans-DLM is the same, that is, (y(M) \u2013 ytarget)2 and E[\u2207efe(xt, t, C)] are similar, conventional methods have additional error terms related to external property predictors. We can infer that (\u2207\u043c\u0443(M))2\u03c32ext and \u03c32ext\u03c32grad are additional error terms excluded from diffusion models. Therefore, as long as \u03c32xt is not a very small value, will be larger than . All reasonings above taken into account, our text-guided MO model with the diffusion language model outperforms traditional methods in terms of controlling error propagation, that is"}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 Dataset", "content": "Based on our research purpose, our evaluation centers on an MMP dataset [17], which is currently the sole publicly available dataset. This dataset includes 198,558 source-target molecule pairs with their ADMET properties, and we ensured consistent dataset partitioning for all the experiments."}, {"title": "5.2 Metrics", "content": "Following the previous work outlined by Edwards et al. [6], we employed eight metrics to assess the structural construction abilities and four criteria to evaluate property optimization capabilities.\n\u2022 SMILES BLEU [41] score and Levenshtein [39] distance. These metrics evaluate the syntactic similarity and the distance between the optimized and target SMILES strings.\n\u2022 MACCS FTS [5], RDK FTS [47], and Morgan FTS [44]. These metrics calculate the average Tanimoto similarity between the fingerprints of the optimized and target molecules.\n\u2022 Exact match and Validity. We measure the proportions of optimized molecules identical to the target molecules and syntactically valid molecules that can be processed by the RDKit [32].\n\u2022 FCD [42]. Fr\u00e9chet ChemNet Distance (FCD) evaluates the latent information agreement.\n\u2022 ADMET properties accuracy. According to the dataset, the property changes are categorized into high->low, low->high, and no change. A property prediction model [27] trained on ADMET properties in the MMP dataset is utilized to predict the optimized molecules' corresponding attributes. Then, the accuracy of property change type between the optimized and source molecules is calculated to convey the model's optimization ability toward the desired orientation."}, {"title": "5.3 Baselines", "content": "Five baselines were selected, encompassing autoregressive models, deep generative models and variational auto encoder (VAE) models.\n1. MIMOSA [9]. This model uses their prediction tools and employs sub-structure operations to generate new molecules.\n2. Modof [3]. A pipeline of multiple and identical Modof models modify an input molecule at predicted disconnection sites.\n3. MolSearch [49]. This framework uses a two-stage search strategy to modify molecules based on transformation rules derived from compound libraries.\n4. Chemformer [25]. This is a transformer-based model pre-trained on unlabeled SMILES and finetuned for MO task.\n5. FRATTVAE [24]. A fragment tree-transformer based VAE model is trained for the MO task.\nIt is worth noting that MIMOSA, Modof, MolSearch, and FRATTVAE optimize molecules guided by the external property predictors."}, {"title": "5.4 Experiment Settings", "content": "The SMILES vocabulary included 265 tokens, with token embeddings being trainable at d = 32. We used a pre-trained SciBERT model [2] with an embedding dimension of d\u2081 = 768 to encode the textual descriptions. Moreover, the core transformer architecture, fo, consists of L = 12 layers with a hidden dimension of d2 = 1024. Our TransDLM has around 181 million trainable parameters.\nFor the MO task, we used a uniform step-skipping strategy during the denoising process, which reduced the sampling steps from 1,000 to 200. As a result, optimizing a molecule takes approximately 1.02 seconds on an AMD EPYC 7763 (64 cores) @ 2.450GHz CPU and a single NVIDIA A6000 GPU. During training, we set the total diffusion steps to T = 2000. The Adam optimizer's [29] learning rate was 5e-5, and we included a linear warm-up."}, {"title": "5.5 Overall Performance", "content": "As shown in Tables 1 and 2, TransDLM generally outperformed all baseline models. Although some baselines excelled in validity, they performed poorly in other metrics, which demonstrates limitations. Specifically, TransDLM achieved the best BLEU score and an 8.6% improvement in Levenshtein distance, indicating better alignment with the target molecules' structures and higher sequence accuracy. Most advantages on FTS criteria further highlighted its ability to"}, {"title": "5.6 Ablation Study", "content": "We performed two ablation studies to validate the effectiveness of the TransDLM strategies, as outlined below:\n\u2022 During description generation, we substituted IUPAC names with original SMILES strings to demonstrate that the former carry more semantic information.\n\u2022 During the denoising process, we directly sampled from pure noise instead of the encoded source molecules to validate the latter is efficacious for retaining the scaffolds of source molecules.\nAs shown in Tables 1 and 2, optimization guided by descriptions made of IUPAC names displayed a better performance than of original SMILES strings. This implies that IUPAC nomenclature conveys more physical and chemical semantics information which benefits text-guided MO. Similarly, though sampling from the encoded source molecules sacrificed few advantages on property criteria, it generated superior outcomes for most structural metrics than from pure noise, indicating that our sampling strategy optimizes molecules without destroying the original scaffolds."}, {"title": "5.7 Case Study Analysis", "content": "In the overall performance, the TransDLM model demonstrated robust physical construction and chemical optimization capabilities. To validate that out approach not only exhibited improvements in numerical performance but also genuinely satisfies the MO goal of moderate structural disparities and substantial chemical dissimilarities, we conducted a case study analysis as illustrated in Fig. 4.\nIn the first two cases, the two MMPs underwent one atom and one functional group substitution. TransDLM precisely identified the site that needed replacement and carried out structural modifications, fulfilling all ADMET property requirements under the guidance of the textual descriptions. In cases where TransDLM did not fully recognize all modification sites, such as the third case, TransDLM also maintained the core scaffold of the source molecule effectively, fulfilling the requirements of chemical properties without overly modifying the physical structure. In contrast, other methods either significantly disrupted the original molecule's structure or failed to meet the chemical properties required by MMPs, underscoring their limitations."}, {"title": "6 CONCLUSION", "content": "In this paper, we proposed TransDLM, a novel language diffusion model for text-guided SMILES-based MO, which utilizes IUPAC nomenclature for molecular semantic representation and optimizes molecules guided by physically and chemically detailed textual descriptions. Instead of relying on an external property predictor, error propagation caused by the predictor was prevented. Additionally, we theoretically demonstrated TransDLM's effectiveness and performed validation experiments using the MMP dataset. Notably, TransDLM outperformed other state-of-the-art methods on most structural metrics and all ADMET property criteria. Furthermore, TransDLM enables simultaneous sampling of multiple molecules, making it highly suitable for scalable web-based applications and allows for efficient large-scale optimization, leveraging the web's capacity for distributed computation. Furthermore, these achievements occurred without relying on additional data sources or pre-training, underscoring the effectiveness of TransDLM in text-guided multi-property MO."}]}