{"title": "Dataset Distillation-based Hybrid Federated Learning on Non-IID Data", "authors": ["Xiufang Shi", "Wei Zhang", "Mincheng Wu", "Guangyi Liu", "Zhenyu Wen", "Shibo He", "Tejal Shah", "Rajiv Ranjan"], "abstract": "In federated learning, the heterogeneity of client data has a great impact on the performance of model training. Many heterogeneity issues in this process are raised by non-independently and identically distributed (Non-IID) data. This study focuses on the issue of label distribution skew. To address it, we propose a hybrid federated learning framework called HFLDD, which integrates dataset distillation to generate approximately independent and equally distributed (IID) data, thereby improving the performance of model training. Particularly, we partition the clients into heterogeneous clusters, where the data labels among different clients within a cluster are unbalanced while the data labels among different clusters are balanced. The cluster headers collect distilled data from the corresponding cluster members, and conduct model training in collaboration with the server. This training process is like traditional federated learning on IID data, and hence effectively alleviates the impact of Non-IID data on model training. Furthermore, we compare our proposed method with typical baseline methods on public datasets. Experimental results demonstrate that when the data labels are severely imbalanced, the proposed HFLDD outperforms the baseline methods in terms of both test accuracy and communication cost.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the rapid advancement of deep neural networks has sparked a revolution across various areas, such as image classification [1], [2], object detection [3], [4] and semantic segmentation [5]. Traditional centralized machine learning methods typically aggregate all data to a central server for model training. However, these methods may involve transmission of a large amount of sensitive data, which can potentially induce a risk of privacy exposure to users. With the development of edge computing and the increasing importance of data privacy protection, federated learning (FL) has become an emerging machine learning paradigm and attracted widespread attentions [6]\u2013[8]. In the widely used FL-based method FedAvg [9], the server updates the global model by aggregating the local model parameters with simply weighted averaging. This method performs well when dealing with independently and identically distributed (IID) data. However, researchers have found that data among clients are often Non-IID in practice. In such cases, if the server simply aggregates the local model parameters with weighted averaging, local model drift may make the global model deviate from the global optimum [10], leading to a slower convergence of the global model. To mitigate the impact of Non-IID data on FL-based model training, researchers have explored various approaches. At the Algorithm level, efforts focus on the improvements of loss functions [11]\u2013[13] and aggregation methods [14]\u2013[16]. However, they cannot fully alleviate the impact of Non-IID data on model training and may have a poor performance when dealing with high statistical heterogeneity. At the system level, client clustering techniques are introduced to create a multi-center framework [17]\u2013[19]. These techniques aim to group clients with similar data distributions together to form clusters. Nevertheless, this approach primarily focuses on client similarity and fails to fully exploit the potential advantages of client heterogeneity. At the data level, data sharing is considered an effective strategy [20]\u2013[22]. It mitigates the negative impact of Non-IID data by sharing a small amount of local data. Although this method can improve the performance of the global model, the server typically lacks comprehensive knowledge of the clients' data distributions, making it difficult to obtain a uniformly distributed dataset. Moreover, this approach conflicts with the privacy protection principles of FL. Recent works on dataset distillation shows promising potential to overcome restrictions on data sharing while protecting privacy. Dataset distillation [23]\u2013[26] aims to achieve good generalization performance by training models on a synthesized dataset with much smaller size than the real dataset. Ref. [27] indicates that the privacy of transmitting distilled data should be no worse than transmitting model parameters. Additionally, distilled data not only provides visual privacy but also enhances the model\u2019s resistance to membership inference attacks [28]. Motivated by the above observations, we propose a new learning framework named HFLDD, integrating dataset distillation to hybrid federated learning (HFL). The key chanllenge of our research is how to effectively transmit distilled data among clients to compensate data heterogeneity. Specifically, The main idea of the design is to partition clients into a few clusters where data labels are heterogeneous within each cluster but balanced across different clusters. Furthermore, within each cluster, cluster members transmit their distilled"}, {"title": "II. RELATED WORK", "content": "The basic frameworks of FL include traditional FL, decentralized FL (DFL), and HFL. Addressing the performance decline of FL on Non-IID data is a critical challenge. Each framework employs different methods to tackle this issue. In traditional FL, all the clients train models locally and then transmit these local models to a central server for global aggregation. Various methods have been proposed since the introduction of FedAvg [9] to address this heterogeneity issue. One main direction attempts to improve the design of local loss function. FedProx [12] introduces an additional L2 regularization term in the loss function to limit the size of local updates. SCAFFOLD [11] applies the variance reduction technique to correct the local training. MOON [13] utilizes the similarity between model representations to correct the local training of clients. However, when the data heterogeneity is severe, the achieved accuracy remains low for regularization-based and weight adjustment methods. DFL can implement distributed learning from data distributed across multiple clients without the need for a central server. To address the data heterogeneity issues in DFL, the existing researches mainly focused on how to utilize the topological structure to improve the learning process. D-Cliques [29] addresses label shift problems in decentralized settings by constructing heterogeneous topological structures based on the underlying data distribution of each client, which will induce privacy leakage. In consideration of privacy preservation, Ref. [30] applies a proxy-based method to learn the local datasets\u2019 label knowledge based on a global dataset and construct heterogeneous topological structure for DFL. The proposed method has shown that when the neighbours of each client are with heterogeneous data, it can improve the convergence of model training in addressing label distribution skew problems. This insight inspires the main idea of our method. It should be noted that the paradigm of fully distributed topological structure construction and learning suffers from heavy communication overhead. Alternative to traditional and decentralized FL, HFL involves both client-to-server communication and client-to-client communication. In [31], a multi-stage HFL algorithm is developed to handle the issue of Non-IID data by continuously adjusting the rounds of client-to-client communication and global aggregation cycles. Particularly, Ref. [32] proposes an HFL framework, called FedSeq, based on client clustering and sequential training. It partitions clients into multiple clusters, with each cluster head communicating with the server. Within each cluster, clients form a ring topology for sequential training. The core idea of FedSeq is that the model in the cluster head is based on the training data from all the clients within this cluster, thereby improving the test accuracy. FedSeq requires multiple rounds of parameter transmission among clients, which still incurs significant communication cost."}, {"title": "B. Dataset Distillation", "content": "The current dataset distillation frameworks are mainly divided into meta-learning frameworks and data matching frameworks [33]. Ref. [23] proposes a meta-learning framework, which adopts a dual optimization approach. However, this method involves a nested loop that can be difficult to optimize. Considering the existence of closed-form solutions in the kernel regression paradigm, Ref. [34] replaces the neural network in the inner loop of the meta-learning framework with a kernel model. In the data matching framework, DC [35], [36] and DM [37], [38] aim to match the gradients and distribution of distilled data with original data during training. Trajectory matching [39] generates synthetic data by matching the training trajectories. Recently, the integration of dataset distillation into FL has attracted widespread attentions. In DynaFed [40], the server generates distilled data through trajectory matching after each aggregation, and further updates the global model based on this data. Ref. [41] improves communication efficiency and test accuracy by constructing distilled data on each client to locally match the loss landscape of the original data. However, these works do not consider leveraging D2D (device-to-device) networks to compensate for client heterogeneity. Ref. [42] introduces a method for extracting meta knowledge from clients and exchanging it among clients, but this knowledge sharing may be less effective in scenarios with severe data heterogeneity. In comparison with the aforementioned works, we design a new framework of HFL to address the issue of Non-IID data. It extends the idea of constructing heterogeneous topological structure in DFL to HFL, and fully leverage the potential of heterogeneity among clients."}, {"title": "III. PRELIMINARIES AND PROBLEM STATEMENT", "content": "The main idea of dataset distillation is to transfer a large dataset into a small dataset, while preserving the essential characteristics of the original dataset. Suppose the original dataset and the distilled dataset are respectively denoted by T = {(xt,i, yt,i)}=1 and S = {(xs,j, ys,j)}=1, where xt,i \u2208 Rd and yt,i \u2208 Rnc are respectively the input data and the corresponding label of the i-th sample in the original dataset. xs,j \u2208 Rd and ys,j \u2208 Rnc are respectively the input data and the corresponding label of the j-th sample in the distilled dataset. |T| and |S| represent the sizes of the original dataset and the distilled dataset, respectively. Generally, the size of T is considerably larger than |S|, i.e.,|S| < |T|. To realize dataset distillation, one of the most widely used methods is Kernel Inducing Points (KIP) [34], [43], which is a first-order meta-learning algorithm based on Kernel Ridge Regression (KRR). Specifically, KIP method is to find a synthetic dataset S that is approximate to the target dataset T by minimizing the following loss function\n$\\mathcal{L}(S)=\\frac{1}{2}||Y_t - K_{XX^*}(K_{X^*X^*} + \\lambda I)^{-1}Y_S||_F^2,$\nwhere \u03bb > 0 is a regularization parameter. yt \u2208 R|T|\u00d7nc and ys \u2208 R|S|\u00d7nc are the label of T and S respectively. Kx*x* \u2208 R|S|\u00d7|S| and Kx\u2081x* \u2208 R|T|\u00d7|S| are two kernel matrices defined as (Kx*x*)ij = k (xs,i, xs,j) and (kx+x*)ij =\nk (xt,i, xs,j), where k(\u00b7) denotes the kernel function. KIP is highly efficient for dataset distillation because it only involves first-order optimization."}, {"title": "B. Federated Learning", "content": "Consider a classic scenario of FL with N participants collaboratively training a global model. Each participant possesses a local dataset, represented as Di = {(Xi, Yi)},i = 1, . . . , N, where Xi \u2208 Rni\u00d7d and Yi \u2208 Rni\u00d7nc denote the tuple of input data points and their corresponding labels. n = \u03a3i=1 ni, where ni = |Di| is the number of samples in Di. The objective of collaborative learning is to solve the following distributed parallel optimization problem\n$\\min_w f(w) = \\frac{1}{N} \\sum_{i=1}^N \\min f_i(w),$\nwhere w is the set of model parameters. The term fi(w) is defined as END\u2081Fi (w, \u00a7i), where \u00a7i is a mini batch of data from Di, and Fi (w, \u00a7i) is the loss function associated with i and w. In the process of training, each participating client trains the local model based on their local dataset and then transmits the local models to a server for global model aggregation. Specifically, in round t, client i trains its local model w by minimizing the local loss function fi (w) using gradient descent method. Subsequently, the server updates the global model wt through aggregation rules, e.g., FedAvg. If the client data are IID, the above method can provide the optimal solution of (2). while if the client data are Non-IID, the obtained global model may be drifted from the optimum. The communication cost of the above process comes from the local model uploading and global model downloading. By referring to [27], the communication cost of the widely used FedAvg is\n$Cost_{fedavg} = N \\cdot (2T - 1) \\cdot |w| \\cdot B_1,$"}, {"title": "C. Problem Statement", "content": "In this paper, we consider a typical scenario of distributed learning with Non-IID data, i.e. label distribution skew [44]. Our focus lies in addressing the issue of quantity-based label imbalance, where each client owns data with a fixed number of labels. Supposing Nc denotes the total number of label classes, and each client owns data with C different labels. The data labels among different clients could overlap. If C = Nc, the data labels among all the clients are the same, which can be regarded as quantity-based label balance. if C < Nc, the data labels among all the clients are imbalanced. If C = 1, it represents the worst-case scenario of quantity-based label imbalance, where the local dataset on each client contains only a single label. It has been verified that the quantity-based label imbalance will decrease the performance of model training to a great extent [12], [45]. Instead of traditional FL, we will design a new HFL framework that integrates dataset distillation to create approximately label-balanced local datasets, thereby alleviating global model drift caused by Non-IID data. Furthermore, it should be noted that the clients can communicate with each other."}, {"title": "IV. THE PROPOSED HFLDD", "content": "In this section, we will firstly introduce the key idea of the proposed framework: HFLDD. Then, we will introduce the details of each part and finally provide an analysis of the communication cost of HFLDD."}, {"title": "A. Overview", "content": "The key idea of HFLDD is to construct heterogeneous client clusters, where the data labels among different clusters are balanced and approximate to the global distribution. The server can communicate with each cluster to obtain a global model similar to one achieved under IID data conditions. The overall pipeline of our proposed HFLDD is shown in Fig. 1, and Table I summarizes the important notations. Briefly, HFLDD can be mainly divided into four parts: label knowledge collection, heterogeneous client clustering, IID dataset generation, and model training. Each part includes one or several steps. We will briefly introduce each part and the details will be shown in the following subsection.\nLabel knowledge collection: Based on pre-trained models, each client predicts soft labels of the data-points in a global dataset, then uploads all the soft labels to the server (Step 1). The server obtains the label knowledge from all the clients.\nHeterogeneous client clustering: The server calculates the similarities among clients\u2019 labels (Step 2) and groups clients into homogeneous clusters. Subsequently, cluster sampling is performed by the server to identify members of heterogeneous clusters and randomly selects one cluster header for each cluster. The clustering results are then distributed to the clients (Step 3).\nIID dataset generation: All clients within a cluster, except the header, perform dataset distillation and send their distilled data to the cluster header (Step 4). As a result, The cluster header obtains a dataset that is approximately IID.\nModel training: Each cluster header conducts local model training using a combined dataset, and uploads the local model parameters to the server. The server updates the global model and distributes it to cluster headers (Step 5). This step repeats until model convergence."}, {"title": "B. Details", "content": "The details about the implementation of each part in HFLDD are introduced in this subsection."}, {"title": "1) Label knowledge collection", "content": "Each client first trains a local model on their own dataset Di. Since the trained local model is driven by the client dataset, it learns relevant knowledge about the local data. Subsequently, we introduce a finite and globally available dataset Dg = {(Xg,yg)}, where Xg \u2208 R|Dg|xd, Yg \u2208 R|Dg|\u00d7Nc and Dg is the number of samples in Dg. The global dataset has a distribution different from the clients\u2019 datasets and is used to generate soft labels for each client. Specifically, each client uses a globally available dataset as input for its locally pre-trained model to generate a two-dimensional tensor Si \u2208 R|Dg|\u00d7Nc, Vi = 1,\u2026,N. This tensor represents the soft labels of the global dataset based on client i\u2019s local model. Sik denotes the probability of sample k belonging to class c. It has been verified that the above soft labels can reflect the label knowledge of the local dataset [30]. Therefore, after receiving the soft labels from the clients, the server obtains the label knowledge of each client without knowing the original dataset, which prevents data privacy leakage."}, {"title": "2) Heterogeneous Client Clustering", "content": "Based on the soft labels of each client, the server calculates pairwise KL divergences among these tensors to obtain the similarity matrix M\u2208RN\u00d7N, where the ij-th element is calculated by\n$M_{ij}=KL(S_i||S_j) \\frac{1}{|D_g|} \\sum_{k=1}^{|D_g|} \\sum_{c=1}^{N_c} S_{i,k}^c \\log (\\frac{S_{i,k}^c}{S_{j,k}^c}),$\nThe element Mij indicates the difference in label knowledge between the local datasets of client i and client j. Based on the similarity matrix, the server employs the K-Means algorithm to cluster all the clients, by treating each row of the similarity matrix as a data point. This process groups clients with similar label knowledge into homogeneous clusters H\u00b0 = {Ho,1,\u2026\u2026\u2026, Ho,K}. However, our goal is to construct heterogeneous clusters He = {He,1,\u2026\u2026\u2026, He,H} with approximately IID intra-cluster data. Therefore, to construct heterogeneous clusters, we randomly select one client from each of the homogeneous clusters, and form a new cluster with them. The details are shown in Algorithm 1. Through this sampling process, we ensure that the newly formed clusters contain clients with different label knowledge. Fig. 2 illustrates the overall process of heterogeneous client clustering. Each client\u2019s local data contains only one label, which makes the data among different clients are severe Non-IID. After clustering, each homogeneous cluster contains only one label, while each heterogeneous cluster contains labels from all the clients. Then, the server randomly selects one client within each heterogeneous cluster as the header for this cluster, and pushes the cluster information to each client. In this way, the clients constructs a two-layer topology, as shown in Fig. 1"}, {"title": "3) IID dataset generation", "content": "In each heterogeneous cluster, the cluster header acts as an agent for all the cluster members, conducts local model training, and communicates with the server. Each cluster member, except the header, sends a distilled data of much smaller size to the cluster header. In this work, the clients employ the KIP method to obtain distilled data, as shown in Section III-A. For the h-th heterogenous cluster, when the cluster header aggregates the distilled data from the entire cluster, it obtains a dataset Dh, whose distribution approximates the global distribution. We transform the problem into a learning process between the server and a number of cluster headers with approximately IID data."}, {"title": "4) Model training", "content": "As we mentioned above, after transforming our problem into distributed learning with IID data, we can apply classical FL method, such as FedAvg [9], for model training. In communication round t, the server broadcasts the global model parameters wt to all the connected cluster headers. Subsequently, cluster header h updates its local parameters to wh wt, and performs E2 epochs of local updates on Dh. At the (k+1)-th epoch of local update, the local model parameters of cluster header h is updated as\n$w_h^{t,k+1} = w_h^{t,k} - \\eta \\nabla F_h(w^{t,k},\\xi_{h}^{t,k}),$\nwhere h = 1,\u2026\u2026, H, k = 0,\uff65\uff65\uff65, E2 \u2013 1, \u03betk denotes a randomly selected batch from Dh, \u03b7 denotes the learning rate and VFh() denotes the gradient of local loss function. Finally, the server aggregates the updated local models w\u2081,E2, from all the cluster headers to generate a new global model. The overall process of the proposed HFLDD is illustrated in Algorithm 2."}, {"title": "C. Communication Cost Analysis", "content": "Now, we analyze the total communication cost of HFLDD. Considering that the initialization of the server model can be sent as a random seed [27], we can ignore the communication cost of the server sending random seeds. Thus, we only consider the resources consumed for uploading soft labels to the server at the stage of label knowledge collection, transmitting the distilled data within each cluster at the stage of IID dataset generation, and communication between the server and cluster headers at the stage of model training. Let an denote the cluster header of heterogeneous cluster h, and Di denote the size of the distilled dataset on client i. Suppose transmitting each parameter incurs a communication overhead of B1 (usually 32 bits), and transmitting each distilled data incurs a communication overhead of B2 (1 \u00d7 8 bits or 3 \u00d7 8 bits for each gray-scale or RGB pixel). For N clients, the communication cost for uploading the soft label is N\u00b7 |Dg|\u00b7 Nc \u00b7 B1. The total communication cost for transmitting the distilled data within each cluster is \u03a3h=1 \u03a3ieHe,hi\u2260an Di \u00b7 B2. At the model training stage, the total cost for the model communication between the server and cluster headers at T rounds is H\u00b7 |\u03c9|\u00b7 (2T \u2212 1) \u00b7 B\u2081. Consequently, the total communication cost of HFLDD is\n$Cost_{hfldd} = N \\cdot |D_g| \\cdot N_c \\cdot B_1 + \\sum_{h=1}^H \\sum_{i\\in H_{e,h} \\atop i \\neq a_h} |D_i|\\cdot B_2 + H \\cdot |w| \\cdot (2T - 1) \\cdot B_1.$"}, {"title": "V. EXPERIMENTAL EVALUATION", "content": "In this section, we first analyze the performance of the proposed HFLDD and compare it with baseline methods to demonstrate how our method can mitigate the impact of quantity-based label distribution skewness. Subsequently, we compare the communication overhead of different methods. Finally, we evaluate the impact of K in the K-Means algorithm on the performance of HFLDD."}, {"title": "A. Experimental Setup", "content": "Our experiments are conducted on two widely used datasets MNIST [46], and CIFAR10 [47]. MNIST consists of 50,000 binary images of handwritten digits, which can be divided into 10 classes. CIFAR10 consists of 50,000 daily object images, which can also be divided into 10 classes. In our Non-IID setting, we investigate scenarios where each client possesses 1, 2, or 10 of the ten classes. In our experiments, each dataset is divided into 80% for training and 20% for testing. Apart from the training and testing local client dataset, we also need additional global datasets to generate soft labels for label knowledge collection. We randomly select 1,000 samples from other similar datasets and construct the corresponding global dataset. For MNIST experiment, the global dataset is\nModels: In our experiments, the applied models are all based on Convolutional Neural Network (CNN) [1]. The model architectures for different datasets are depicted in Fig. 4. For the MNIST dataset, we construct a shallow CNN model suitable for handwritten digit classification. The total number of model parameters are 44,426. For the CIFAR10 dataset, we increase the depth of the model to better capture the features of complex images in CIFAR10. The total number of model parameters are 1,020,160. The input for both models is 32 \u00d7 32 \u00d7 3 image and the output is a 10-dimensional one-hot class label. It should be noted that the proposed HFLDD is not limited by the model architectures, and it can be applied to any model. In the experiments, we only show the results based on the above mentioned models.\nBaselines: To evaluate the performance of our proposed HFLDD algorithm, we select three typical baseline methods for comparison:\nFedAvg [9], a classic FL algorithm, where each client conducts local training and the server aggregates all the local models for global model update;\nFedSeq [32], a classic HFL algorithm, which utilizes client clustering and sequential training to enhance model performance;\nCentralized learning (CL), which can provide the performance upper bound for model training.\nImplementation Details: In our experiments, conducted on a machine equipped with a Tesla V100 GPU and implemented using PyTorch, we fixed the number of clients N to 100. For the pre-training phase, both MNIST and CIFAR10 datasets were used with the following settings: learning rate \u03b7 = 0.01, local update rounds E1 = 10, a batch size of 64, and the SGD optimizer. These settings ensure that each client can effectively extract knowledge from its local data, thereby enhancing the discriminability for subsequent clustering tasks. For constructing the heterogeneous topology, we set K-Means clustering with 10 clusters. Due to the need to transmit distilled images between clients, we set each client\nconstituted from the FMNIST dataset [48]. For CIFAR10 experiments, the global dataset is constructed from the Tiny-imagenet dataset [49]."}, {"title": "B. Training Performance", "content": "To comprehensively show the training performance of the proposed HFLDD, we compare HFLDD with the baseline methods in both Non-IID and IID scenarios."}, {"title": "1) Non-IID Scenario", "content": "We consider two severe Non-IID settings: 1) C = 1, where each client holds only one class of data, making it the most challenging setting; 2) C = 2, where each client holds two classes of data. The data labels among different clients are still severely imbalanced but partially overlap. In Fig. 3, we compare the training performance of HFLDD with baseline methods on the datasets of MNIST and CIFAR10, respectively. When C = 1, as shown in Fig. 3a and Fig. 3d, it can be observed that our proposed HFLDD algorithm outperforms FedAvg and FedSeq on both MNIST and CIFAR10. Specifically, on the CIFAR10 dataset, HFLDD achieves an accuracy of 64.3% after 300 communication rounds, far exceeding FedAvg at 27.0% and FedSeq at 19.6%. On the MNIST dataset, although the final accuracy of HFLDD is similar to FedSeq, it is clear that HFLDD converges faster. When C = 2, on the CIFAR10 dataset, as shown in Fig. 3e, HFLDD achieves a final test accuracy of 70.1%, which also outperforms FedAvg and FedSeq. On the MNIST dataset, as shown in Fig. 3b, HFLDD and FedSeq achieve nearly identical training results, but both outperform FedAvg. Additionally, under the Non-IID settings described above, the performance gap between HFLDD and CL is the smallest, in comparison with FedAvg and FedSeq. Figs. 5a, 5b, 5d, and 5e specifically illustrate the heterogeneous topology formed by HFLDD in the Non-IID scenario in the experiments. Because of the significant differences in data distribution among the clients, the clustering algorithm can effectively classify the clients. In Fig. 5d, the number of clients in all heterogeneous clusters is the same. In Figs. 5a, 5b, and 5e, a few isolated nodes appear in the heterogeneous topology due to some clients being misclassified. However, they do not significantly affect overall performance. This is because the data distribution in most heterogeneous clusters is approximately the same as the global data distribution, effectively compensating for the overall performance."}, {"title": "2) IID Scenario", "content": "In this case, we set C = 10, where each client holds all ten classes of data and the data labels among all the clients are balanced. On the MNIST dataset, as shown in Fig. 3c, all the methods achieve almost identical test accuracy, and HFLDD\u2019s convergence speed is intermediate among the baseline algorithms. On the CIFAR10 dataset, as shown in Fig. 3f, the baseline methods can achieve higher test accuracy than the proposed HFLDD. This can be attributed to two main factors. First, in HFLDD, the global model is trained based the dataset of the cluster headers and the distilled data from the cluster members. The information loss caused by dataset distillation degrades the training performance and makes the test accuracy of HFLDD lower than the baseline methods based on the original datasets under IID conditions. Secondly, as seen from Figs. 5c and 5f, when each client holds 10 classes of data, the clustering algorithms cannot distinguish the differences among clients, resulting in many imbalanced heterogeneous clusters. In this case, if the distilled images are transmitted among heterogeneous clusters, the cluster headers will have unbalanced sample quantities, resulting in a quantity skew [44], which breaks the original IID relationship among clients. Nevertheless, compared to baseline distributed learning methods, the proposed HFLDD is more robust to Non-IID data. As the data distribution shifts from IID to Non-IID, i.e., from C = 10 to C = 1, as shown in Fig. 3, On the CIFAR10 dataset, the test accuracy of HFLDD drops from 72.2% to 64.3%. On the MINIST dataset, the test accuracy of HFLDD drops from 98.4% to 98.1%, demonstrating the robustness of HFLDD to Non-IID data. In contrast, the test accuracy of the baseline methods, FedSeq and FedAvg, rapidly declines or even fails to converge."}, {"title": "C. Communication Cost", "content": "The analytical communication cost of HFLDD is provided in Secion. V-B. In this part, we will compare communication cost of HFLDD with the baseline distributed learning methods, FedAvg and FedSeq. The communication cost of FedAvg in T rounds of training is given by Equation 3. For FedSeq, the communication cost comes from parameter sharing between clients within cluster and across clusters, it can be expressed as\n$Cost_{FedSeq} = O \\cdot |w| \\cdot B_1 \\cdot ((2T - 1) + T \\cdot J),$\nwhere O is the number of cluster and J is the number of clients in each cluster. Considering that when the training converges, the communication rounds and test accuracy of different methods are different. it would be unfair to compare the communication cost under a given communication rounds. Therefore, we compare the number of communication rounds and total communication cost required by different algorithms to achieve a certain test accuracy. In the scenario with IID data, we set the target test accuracy as 80.0% for the MNIST dataset. Table II shows the number of rounds and communication cost for each algorithm to reach the target test accuracy. The ratio denotes the multiple of the communication cost of each method relative to the HFLDD. It can be observed that the communication cost of HFLDD is slightly higher than that of FedSeq, and 14.86 times less than that of FedAvg. For the CIFAR10 dataset, we set the target accuracy as 70%. When each algorithm reaches the target test accuracy, the communication cost of HFLDD is still much smaller than that of FedAvg, while the communication cost consumed by FedSeq is 0.39 times that of HFLDD, as detailed in Table III. This is mainly because FedSeq converges faster than other algorithms when the data is IID. In the scenario with Non-IID data, when C = 1, due to the severe imbalance of data labels, the baseline methods may not converge and it is infeasible to find proper target test accuracy that sufficiently reflects the communication cost of each algorithm. Therefore, we consider the case when C = 2. Fig. 6 compares the communication cost of each algorithm to achieve different target accuracies on the MNIST and CIFAR10 datasets. Since the communication cost of HFLDD is much lower than FedAvg and FedSeq in this scenario, we use a logarithmic scale (base 10) on the y-axis. From the figure, it can be observed that as the target accuracy increases, HFLDD consistently maintains the lowest communication cost."}, {"title": "D. Impact of K in K-Means Clustering", "content": "In this part, we evaluate the impact of K in the K-Means homogeneous client clustering on the performance of the proposed HFLDD. Table IV shows the test accuracies of HFLDD on the MNIST and CIFAR10 datasets under different values of K. When C = 1, on the CIFAR10 dataset, the test accuracy is the highest when K = 10, and it shows a noticeable decline as K decreases or increases. However, on the MNIST dataset, the test accuracy remains relatively unchanged as K varies. This is because the MNIST dataset is relatively simple, making its data patterns and features easy to capture and understand. In this scenario, even if an inappropriate value of K is selected, HFLDD still can easily learn the features in the MNIST dataset. When C = 2 and C = 10, the heterogeneity is mitigated, and there is partial overlap in label classes among different clients, leading to a certain degree of overlap in the feature space. The test accuracy on the two datasets remains relatively stable as the value of K changes. From this, we observe that when data heterogeneity is not particularly severe, HFLDD exhibits robustness to the value of K."}, {"title": "VI. DISCUSSION", "content": "It is important to note that the actual scenario differs from the example provided above, the data heterogeneity among clients' data types is highly complex. After sampling and reorganizing homogeneous clusters, it is highly possible to have a limited number of clients in some heterogeneous clusters. There may even be many isolated nodes considered as a single heterogeneous cluster. This leads to uneven number of clients among clusters. In decentralized settings, if these uneven clusters are linked, as referenced in [30], to form a ring-shaped topology for collaborative training, it will lead to a significant decrease in convergence speed and test accuracy. The most intuitive approach to address this issue is to directly remove the heterogeneous clusters with isolated or a very small number of clients from training. However, this often results in worse training performance, failing to fully utilize the data in each client. For better understanding of this issue in HFL, we let each client hold two different classes of data. Due to the overlap of data classes among clients, it is likely to form homogeneous clusters as illustrated in Fig. 7a. In this case, through sampling and reorganization, the constructed heterogeneous clusters are with unbalanced client numbers, as shown in Fig. 7b. After transmitting the distilled data, heterogeneous clusters with fewer clients cannot obtain a dataset whose distribution approximates to the global distribution. However, unlike the decentralized learning in [30], in HFL, the impact of these heterogeneous clusters with very few clients or even just isolated nodes on the performance of model training is relatively small. This is mainly because the presence of a central server makes data exchange and collaboration more flexible. Even if some clusters have only a very few clients, the server can still integrate and utilize their information, thus making full use of all data resources without excluding these imbalanced clusters."}, {"title": "VII. CONCLUSION", "content": "In this paper, we propose a communication-efficient HFL framework that addresses the challenge of distributed learning with Non-IID data by leveraging dataset distillation. Our approach constructs heterogeneous clusters based on label knowledge from all clients. Within each heterogeneous cluster, members transmit distilled data to a cluster header. This process aims to generate approximately IID datasets among clusters. Extensive experiments demonstrate that our proposed HFLDD outperforms the considered baseline methods, and can significantly reduce the"}]}