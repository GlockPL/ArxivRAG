{"title": "COIN: Control-Inpainting Diffusion Prior for Human and Camera Motion Estimation", "authors": ["Jiefeng Li", "Ye Yuan", "Davis Rempe", "Haotian Zhang", "Pavlo Molchanov", "Cewu Lu", "Jan Kautz", "Umar Iqbal"], "abstract": "Estimating global human motion from moving cameras is challenging due to the entanglement of human and camera motions. To mitigate the ambiguity, existing methods leverage learned human motion priors, which however often result in oversmoothed motions with misaligned 2D projections. To tackle this problem, we propose COIN, a control-inpainting motion diffusion prior that enables fine-grained control to disentangle human and camera motions. Although pre-trained motion diffusion models encode rich motion priors, we find it non-trivial to leverage such knowledge to guide global motion estimation from RGB videos. COIN introduces a novel control-inpainting score distillation sampling method to ensure well-aligned, consistent, and high-quality motion from the diffusion prior within a joint optimization framework. Furthermore, we introduce a new human-scene relation loss to alleviate the scale ambiguity by enforcing consistency among the humans, camera, and scene. Experiments on three challenging benchmarks demonstrate the effectiveness of COIN, which outperforms the state-of-the-art methods in terms of global human motion estimation and camera motion estimation. As an illustrative example, COIN outperforms the state-of-the-art method by 33% in world joint position error (W-MPJPE) on the RICH dataset.", "sections": [{"title": "1 Introduction", "content": "Recovering global human and camera motion from dynamic RGB videos is an important problem with many applications, such as animation, human-computer interaction, mixed reality, and robotics. However, it is a very challenging problem due to the entanglement of human and camera motion.\nThere are only a few works [41,81,99,100] that try to address this problem. Earlier methods [47,100] only focus on human motion and ignore the camera motion. Their core insight is that the global body motion is highly correlated with the local motion. Thus, they can use the local body movements to estimate the global orientation and trajectory with a regression model [100] or by combining them with physics constraints [47]. However, these regression models ignore the camera movements, so they fail to maintain consistency with the input video, whereas physics-based methods fail to model complex in-the-wild environments so are limited to controlled scenarios. Recent works [41,99] try to jointly estimate the human and camera motion by exploiting learned motion priors [21,76] and SLAM [66, 89, 90]. They try to constrain the human body motion in a low-dimensional latent space of a motion prior model, which results in reconstructed motions that are overly smooth and do not align well with video observations. Moreover, the optimization of the camera motion is only based on the global human motion from the motion prior. Hence, they fail catastrophically if the initial human motion predictions are significantly incorrect (as shown in Fig. 1).\nMore recently, Denoising Diffusion Models [24,91] have emerged as a powerful family of generative models that can model high-quality data priors. Nonetheless, effectively leveraging the learned priors remains an ongoing challenge. Score Distillation Sampling (SDS) is commonly employed for this purpose [74], but we find that naive application of SDS also results in inconsistencies with the available observations (see Sec. 4.2). The root cause of this problem lies in the inconsistency of randomly sampled motions during SDS optimization. Without constraints, these motions may not align with observed evidence, leading to overly smoothed results that lack detail due to the mode-averaging effect.\nIn this work, we propose COIN, a hybrid COntrol-INpainting score distillation sampling method to address the aforementioned limitations of vanilla SDS. First, we use the partially observed human motion from the video as control signals to guide motion sampling. To tackle noisy observations which may be out of distribution for the prior, we propose a dynamic controlled sampling technique that iteratively refines the observed motions and updates the control signals to ensure effective distillation from the motion prior. Second, to further improve the consistency of the sampled motions, we also develop a novel soft inpaint-ing strategy. We automatically identify the high-confidence regions of the initial predicted global motion from the video and use them as soft constraints during optimization. Concretely, we sample less confident regions from scratch using the"}, {"title": "2 Related Work", "content": "Camera-Space Human Pose Estimation. Most existing works focus on root-relative local human pose estimation to bypass the difficulty in monocular depth estimation [1, 4, 6-8, 16, 32-34, 39, 42-46, 51, 54, 63, 64, 67, 68, 70, 76, 78, 84, 85,88,94,97, 106, 110, 116, 119]. These methods ignore the position of the person in the camera coordinates. To overcome this limitation, recent methods estimate camera-space human poses by regression [28,31,48,52,62,72,75,80,93,95,107,107, 109, 111] or optimization [59-61, 77, 108]. Physics-based constraints are widely used to ensure the plausibility of the estimated poses [10,13,27,29,80,95,105]. In addition to direct regression, heatmap-based representations have also been used to predict the absolute depths of multiple people [11,87, 118]. A few methods improve absolution depth estimation by using predicted camera parameters [40, 49, 109] instead of the predefined focal length. Despite the promising results\nfor camera-space pose estimation, how to decouple the camera movement and estimate global human motions is still an open problem.\nMonocular Global Human Pose Estimation. Recovering the global human motion from a monocular moving camera is challenging due to the entanglement of human and camera motions. To disentangle the camera movement, several methods use IMU sensors or pre-scanned environments to recover global human motions [17,20,57,69], which is impractical for large-scale adoption. Recent works use human motion priors [100] or physics-based constraints [47,55] to recover human motions from monocular videos, but do not consider background scene features, which limits performance on in-the-wild videos. Sun et al. [86] use optical flow as a motion cue to estimate the global motions. A contemporary work, WHAM [81], uses a lifting network to estimate global human motions from 2D keypoints and camera angular velocities. While these works can estimate accurate global human motions, they do not recover camera motions. To explicitly recover the camera motion, Liu et al. [52] use SLAM and convert the local pose from the camera to global coordinates. BodySLAM [22] jointly optimizes the human and camera motion using features of both humans and scenes. Along this line, SLAHMR [99] and PACE [41] use SLAM to initialize camera motions and optimize the camera using human motion priors [21,76]. However, these methods rely on the human motion priors to regularize the camera motion, which may lead to inaccurate camera motion when the human motion is not well initialized (as shown in Fig. 1). Such wrong camera trajectories will further affect the optimization of human motions. In contrast, our approach relies on the consistency among the local human motion, scene features, and the camera for optimization, which provides information that complements the human motion priors and enables accurate estimation of both human and camera motions.\nHuman Motion Priors. There are a significant amount of approaches proposed to study 3D human dynamics for motion prediction and synthesis [2, 3, 5, 12, 15, 18, 19, 23, 30, 36, 38, 50, 58, 71, 73, 92, 98, 101-103]. These learned human motion priors are used to help resolve pose ambiguity [21,39,76,115] in human pose estimation. Recently, diffusion models [82] have also been used as priors for motion synthesis and infilling [26, 35, 91, 104, 113]. RoHM [114], adopts motion diffusion model to recover human motions from noisy and occluded input data. Xie et al. [96] use spatial control signals to guide motion generation. They focus on generating realistic human motion given clean spatial constraints. M\u00fcller et al. [65] build a diffusion model to learn the joint distribution over the poses of two people. They use the SDS loss to guide the generation of static poses. In contrast, we focus on dynamic human motions. We find adopting SDS directly for temporal human motions encounters the inconsistency issue. Therefore, to distill knowledge from the motion diffusion model, we propose a novel control-inpainting SDS to generate high-quality and consistent motion that aligns with observed evidence."}, {"title": "3 Method", "content": "The overall framework of COIN is illustrated in Fig. 2. Given an in-the-wild RGB video with T frames captured by a dynamic camera, our goal is to estimate both the global human motion H = {h(1),h(2),...,h(T)} and the camera motion C = {c(1), c(2), ...,c(T)} in a global world coordinate system. We use off-the-shelf 3D human pose and shape estimation method HybrIK [48] to obtain per-frame initial SMPL parameters in the camera space and DROID-SLAM [89] to obtain the initial per-frame camera-to-world transforms. We convert the local human motion to the world coordinates with the estimated camera. However, because the camera trajectories from SLAM are up to an unknown scale, the initial global human motion will abnormally drift and float in the world space. To resolve the scale ambiguity and place the person in the correct global position, we jointly optimize the human and camera motion to minimize the discrepancy between the observed evidence and the estimated motion, while maintaining the plausibility of the human motion with a diffusion prior through the proposed control-inpainting SDS.\nMotion Representation. The camera motion is represented by the trajectory C = {(R(i), t(i))}7_1, where [R(i), t(i)] is the camera pose at the i-th frame, consisting of the rotation matrix R(i) \u2208 \\mathbb{R}^{3\u00d73} and the translation vector t(i) E \\mathbb{R}^3. The human motion is represented by the human trajectory H = {h(i)}=1,\nwhere h(i) = [(i), \u03c6(i), (i), f(i), \u03b2] is the human pose at the i-th frame, consisting of the global translation (i) \u2208 \\mathbb{R}^3, global orientation (i) \u2208 \\mathbb{R}^3, body pose parameters (i) \u2208 \\mathbb{R}^{23\u00d73}, foot contact labels f \u2208 {0,1}4, and the body shape parameters \u03b2\u2208 \\mathbb{R}^{10}. We use the SMPL model [53] to represent the human pose and shape. Before introducing our approach, we first revisit the formulation and drawbacks of SDS in Sec. 3.1. Then we introduce the proposed control-inpainting SDS in Sec. 3.2. Finally, we present the global optimization pipeline with the proposed human-scene interaction loss in Sec. 3.3."}, {"title": "3.1 Revisiting SDS", "content": "Score Distillation Sampling (SDS) was first introduced to distill 3D assets from pre-trained 2D text-to-image diffusion models [74]. It exploits the knowledge from the diffusion models by seeking modes for the conditional distribution in the DDPM latent space to optimize the 3D scene representation. Similarly, we can optimize global human motion by distilling knowledge from a pre-trained motion diffusion model.\nGiven an global human motion H, the marginal distribution of noisy latent Ht at timestep t \u2208 U(0, 1) is defined as:\nq(H_t|H) = N(H_t; \u221a\\bar{a}_tH, (1 \u2013 \\bar{a}_t)I), (1)\nwhere \u0101t \u2208 (0,1) is a hyperparameter controlled by the variance schedule of the diffusion model. SDS adopts the pre-trained diffusion model D\u00f8(Ht, t, y), which takes in Ht and is used to model the conditional density of the human motion, where are the parameters of the diffusion model and y is the condition. Then, SDS aims to distill global human motion H via seeking modes of the learned conditional density, which can be achieved by a weighted denoising score matching objective:\nmin_H L_{SDS} := E_{t,e} [w(t)||\u20ac \u2013 \u20ac_\u03b8||^2], (2)\nwhere is the predicted denoising direction from the diffusion model, Ht ~ q(Ht H) is sampled using the reparameterization trick, \u0454 is the corresponding sampled noise, and w(t) is a weighting function that depends on the timestep t. To clearly review the effect of SDS, we can reparameterize Eq. 2 as:\nmin_H L_{SDS} := E_t[ \\frac{w(t)\\sqrt{a_t}}{\\sqrt{1-\\bar{a}_t}} \\parallel H - H_\u03b8\\parallel^2]. (3)\nwhere\nH_\u03b8 = \\frac{H_t}{\\sqrt{a_t}} + \\frac{\\sqrt{1-\\bar{a}_t}\u20ac}{\\sqrt{a_t}}. (4)\nBased on this reparameterization, we can see that the SDS objective is to minimize the discrepancy between global human motion H and the denoised global human motion H from the motion diffusion model in a single step. The denoised"}, {"title": "3.2 Control-Inpainting SDS", "content": "The limitations of SDS originate from the randomness and inconsistency of the denoised motion \u0124, which serves as the pseudo ground truth in the objective function. To overcome this issue, we propose a novel Control-INpainting SDS (COIN-SDS) to generate high-quality and consistent pseudo-ground-truth motions. Our solution has three key ingredients (shown in Alg. 1). First, to achieve high-quality motions, we seek to produce the pseudo ground truth with multiple DDIM denoising steps. Second, to encourage consistent motions, we propose to use partially observed evidence from the video as a control signal to dynamically guide the diffusion model and align the generated motions with the observations. Third, to further align the motion with observed regions, we propose a soft in-painting strategy within the denoising process.\nMultiple Denoising Steps. Intuitively, to obtain high-quality pseudo ground truth for SDS, we can replace the single-step denoised motion Ho with a multi-step one H := Ho, following the multi-step DDIM denoising process [83]:\nH_{t-At} = \u221a{a_{t-At}} H + \u221a{1-a_{t-At}} \u20ac, (5)\nuntil \u0124o = Ht-\u2206t is obtained. By replacing \u0124 in Eq. 3 with \u0124o, we can obtain a new objective for SDS:\nmin_H L_{SDS} := E_t[ \\frac{w(t) \\sqrt{a_t}}{\\sqrt{1-\\bar{a}_t}} \\parallel H - \\bar{H_o} \\parallel_2]. (6)\nAlthough the multi-step denoising process can produce high-quality pseudo ground truth, it is computationally expensive to perform multiple denoising steps during"}, {"title": "3.3 Global Optimization", "content": "Here we present the overall optimization pipeline for the joint estimation of global human and camera motion with the proposed COIN-SDS loss. Note that we use SLAM to initialize the camera poses, which is scale-ambiguous. Therefore,"}, {"title": "Human-scene Relation Loss", "content": "The camera trajectories recovered from SLAM are scale-ambiguous. Previous works [41,99] optimize the camera scale by projecting the global human motion to the camera space using the camera poses and minimizing the reprojection error. However, such a method entirely relies on the global human motions, which is in turn affected by the camera scale. If the human motion is not initialized well, the camera scale will also be inaccurate. To solve this problem, instead of the global human motions, we propose a new human-scene relation loss that uses the depth relation between the human and scene in the camera space, which disentangles the effect of the camera itself. Specifically, we use the point cloud of the scene recovered by SLAM as a constraint. First of all, the scale of the scene point cloud is the same as the camera scale, so optimizing the scene scale is equivalent to optimizing the camera scale. Second, the scene points that are projected onto the visible vertices of the body mesh should be occluded by the person. Otherwise, the corresponding body parts are invisible. Therefore, we can constrain the depth of the occluded scene points to be larger than the depth of the human body vertices. While finding the corresponding body vertices for each scene point is time-consuming, we propose to use the depth of its nearest body joint as a proxy. Given the scene point cloud P and the camera scale s, the human-scene relation loss is formulated as:\nL^{CHSR} = \\frac{1}{T}  \\sum_{i=1}^T  \\sum_{p \\in P^*} min(0, T^{(i)}(p)_z - j^{(i)}(p)_z) \\cdot 1(T^{(i)}(p) \\text{ is invisible}), (13)\nwhere P* = P * s is the scaled point cloud of the scene, T(i) (p) = R(i)p+t(i) is the transformed point in the i-th frame, j(i) (p) is body joint that has the nearest\n2D projection to the scene point p in the i-th frame, and z denotes the depth of a given point. If the depth order is correct, i.e., T(i) (p)z \u2212 j (i) (p)z > 0, the loss is zero. The proposed human-scene relation loss uses the relation between the local motions and the scene to alleviate the scale ambiguity. The depth relation regularizes consistency among humans, cameras, and scenes."}, {"title": "4 Experiments", "content": "Datasets. We perform experiments on three human motion datasets. First is the real-world dataset RICH [25]. We follow previous works [41,81,100] to assess the performance of global human motion estimation using this dataset. The second one is EMDB [37]. We follow previous works [81] to evaluate on a subset of EMDB for which they provide ground truth global motion with dynamic cameras. The third dataset is HCM [41], a synthetic dataset. Compared to real-world datasets, HCM contains more challenging camera motions. We follow previous work [41] to evaluate the global human motion and the camera motion using this dataset.\nMetrics. We report various metrics for both human and camera motion. For human motion, standard metrics W-MPJPE and WA-MPJPE are used to evaluate global motion, while PA-MPJPE evaluates local motion. We also include an ACCEL metric to measure the joint acceleration error. For evaluation on EMDB, we follow previous work [81] to split sequences into smaller chunks of 100 frames and align each output segment with the ground-truth data using the first two frames W-MPJPE100 or the entire segment WA-MPJPE100. Root Orientation Error (ROE in deg) and Root Translation Error (RTE in m) evaluate the error over the entire trajectory after aligning with the initial camera pose.\nFor camera motion, we report the average translation error after scale alignment (ATE), without scale alignment (ATE-S), and the camera acceleration error (CAM ACCEL). ATE-S more accurately reflects inaccuracies in the captured scale of the camera.\nBaselines. As discussed in Sec. 2, there are different ways to use the pre-trained motion diffusion model as a motion prior. Here, we summarize the three main solutions and compare them with COIN in Sec. 4.2. (1) Guided Sampling, which embeds analytical guidance within the denoising procedure using objective functions, such as 2D projection and foot contact consistency. This does not suit our task because the camera trajectories are also unknown and guided gradients from the wrong camera will lead to unrealistic human motions. We need to optimize the human motion and camera motion simultaneously. (2) Noise Optimization, which represents the motion as latent noise and directly optimizes it. This is similar to other motion priors such as VAE [21]. At each optimization step, we need to calculate the gradients of the latent w.r.t. the generated motion, which is computationally expensive and we find the performance is not good enough. (3) Vanilla SDS, which removes our design and directly uses SDS for optimization."}, {"title": "4.1 Comparison with State-of-the-Art Methods", "content": "Human Motion Estimation. We compare COIN against state-of-the-art methods on the RICH, EMDB, and HCM datasets. Quantitative results are shown in Tabs. 1, 2, and 3. We observe that COIN significantly outperforms the state-of-the-art methods on all datasets. On the RICH dataset, COIN outperforms the state-of-the-art method, PACE [41], by 125.5 mm in terms of W-MPJPE, showing 33.0% relative improvement. On the EMDB and HCM datasets, COIN shows 29.1 mm and 109.0 mm improvement in terms of W-MPJPE, respectively. Qualitative comparisons with state-of-the-art methods, PACE [41] and WHAM [81], are shown in Figs. 1 and 3. More qualitative results are shown in the supplementary video.\nCOIN not only improves global motion, but also improves local body motion. In terms of PA-MPJPE, COIN shows 3.3 mm, 9.2 mm, and 19.8 mm improvement on the RICH, EMDB, and HCM datasets, respectively. This demonstrates that COIN is able to distill high-quality motion priors from the diffusion model and help both local and global motion estimation. Regarding the joint acceleration error, COIN is 0.8 mm/s\u00b2 higher than WHAM [81]. Note that the joint acceleration error reflects the smoothness. Human motion can be over-smoothed but not accurate, hence it is important to look at acceleration error in conjunction with other metrics."}, {"title": "4.2 Ablation Study", "content": "In this section, we conduct ablation studies on the RICH and HCM datasets to evaluate the impact of each component on human and camera motions, respectively. More comparisons on other datasets are provided in the appendix.\nBaselines with Motion Diffusion Model. We first compare the aforementioned baselines with COIN. To use guided sampling in our tasks, we jointly update the camera poses using the gradients from the objective function during denoising. Quantitative results of human and camera motion estimation are shown in Tabs. 1 and 4, respectively. We observe that COIN outperforms all the baselines in terms of all metrics. As expected, guided sampling shows a terrible"}, {"title": "5 Conclusion", "content": "In this paper, we propose COIN, a diffusion-based optimization framework for global human and camera motion estimation from dynamic cameras. We identify the inconsistency problem of distilling knowledge from the diffusion model"}, {"title": "A Controlled Denoiser", "content": ""}, {"title": "Architecture Details", "content": "The detailed architecture of the proposed controlled denoiser is illustrated in Fig. 4. We adopt a pre-trained transformer-based motion diffusion model as our backbone model. We create a trainable copy of the first 4"}, {"title": "B Ablation Study", "content": "Impact of Each Component. To comprehensively evaluate the impact of each component of COIN, we further conduct ablation studies on the EMDB [37] and HCM [41] datasets. Quantitative results are shown in Tabs. 5 and 7. COIN shows consistent improvement against other diffusion-based baselines."}, {"title": "Error Distribution", "content": "We further present the error distribution on the EMDB dataset to show more details of the COIN predictions. We also plot the error distribution of WHAM [81] for comparison. The scatter plot is shown in Fig. 5. Here we follow the evaluation protocol of EMDB and evaluate W-MPJPE and WA-MPJPE per 100 frames. It is shown that COIN is more robust and has fewer outlier predictions."}, {"title": "SLAM vs SfM", "content": "Compared to SLAM, SfM methods are stronger baselines for camera motion estimation. Because our cases always contain dynamic objects, we used ParticleSfM [117] for its ability to handle dynamic objects and compared it to SLAM. Comparisons are shown in Tab. 6. Note that SfM methods run extremely slow compared to SLAM. Given a video with 700 frames on the RICH dataset [25], ParticleSfM takes over 17 hours while DROID-SLAM only needs 4 minutes. For videos on the EMDB dataset with more than 2000 frames, ParticleSfM took a few days to finish, and could not converge for many. Hence, SLAM is a more viable choice for in-the-wild videos. If we replace DROID-SLAM with ParticleSfM, on the converged videos, the baseline results improve by 200 mm. However, it has minimal impact on COIN demonstrating the robustness of our method to SLAM errors. We would like the emphasize that ParticleSfM could not converge on many of the EMDB videos and the results below are only the subset where it converged."}, {"title": "C Global Optimization", "content": "Here we detail our optimization formulation for the reconstruction of global human and camera motion. Simultaneously optimizing both camera motion and global human motion can result in local minima. To address this challenge, we follow PACE [41] and adopt a multi-stage optimization pipeline. Before running optimization, we initialize the global human motion with the noisy observations using the controlled denoiser. We randomly sample a Gaussian noise and run DDPM to generate the global motion. In stage 1, we optimize only the first frame camera parameters (Ro, ho), camera scale s, and the body shape B. In stage 2, we optimize the first frame camera parameters (Ro, ho), camera scale s, the body shape B, and the global human motion H. In stage 3, we jointly optimize the full camera trajectory along with the global human motion. Given a long video, we split it into windows of T = 128 frames. We use 16 overlapping frames to help reduce discontinuities across windows. The mask M is defined by thresholding the confidence scores of the detected 2D keypoints. The threshold is 0.3. Each stage is run for 500 steps. The learning rates of the 3 stages are 0.01, 0.01, and 0.001, respectively. We use the Adam solver for optimization. Implementation is in PyTorch."}]}