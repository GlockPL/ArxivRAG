{"title": "In-Context Learning for Long-Context Sentiment Analysis on Infrastructure Project Opinions", "authors": ["Alireza Shamshiri", "Kyeong Rok Ryu", "June Young Park"], "abstract": "Large language models (LLMs) have achieved impressive results across various tasks. However, they still struggle with long-context documents. This study evaluates the performance of three leading LLMs: GPT-40, Claude 3.5 Sonnet, and Gemini 1.5 Pro on lengthy, complex, and opinion-varying documents concerning infrastructure projects, under both zero-shot and few-shot scenarios. Our results indicate that GPT-40 excels in zero-shot scenarios for simpler, shorter documents, while Claude 3.5 Sonnet surpasses GPT-40 in handling more complex, sentiment-fluctuating opinions. In few-shot scenarios, Claude 3.5 Sonnet outperforms overall, while GPT-40 shows greater stability as the number of demonstrations increases.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated human-level performance across various tasks, such as coding, question answering, mathematical problem-solving, classification, and sentiment analysis (SA) (Valmeekam et al., 2022; Shi et al., 2022; Navard et al., 2024; Chen et al., 2024; Bavaresco et al., 2024; Wankhade et al., 2022). Despite the emergence of LLMs and their in-context learning (ICL) capabilities, several critical challenges persist in leveraging LLMs for domain-specific SA task such as processing long input sequences, managing frequent sentiment shifts, and accommodating domain-specific terminology across diverse contexts (Medhat et al., 2014; Wankhade et al., 2022; Raghunathan and Saravanakumar, 2023; Zhu et al., 2024; Simmering and Huoviala, 2023; Zhang et al., 2023).\nFurthermore, to fully understand the true performance and capabilities of LLMs in various tasks, particularly SA, it is crucial to consider data contamination as an important factor which is generally disregarded. Data contamination refers to the inclusion of test data in the training dataset,"}, {"title": "2 Related Work", "content": "ICL has emerged as a novel approach that enhances the performance of LLMs without requiring additional training or weight updates (Brown, 2020; OpenAI et al., 2024; Dong et al., 2022). However, despite its widespread use, the underlying mechanisms of ICL and its effectiveness in improving performance remain unclear (Jiang, 2023; Dai et al., 2022). ICL is generally performed in few-shot and many-shot learning regimes (Agarwal et al., 2024; Li et al., 2023; Anil et al., 2024). This approach is"}, {"title": "3 Approach", "content": "In this study, we scraped and collected public opinions on the North Houston Highway Improvement Project from four sources due to its high controversy and data availability. This included scraping 230 instances each from Facebook (FB) posts and news articles (NS), as well as gathering field data and opinions from scoping meeting (SC) comments, and public hearing (PH) comments using comment cards, surveys, emails, and other sources.\nAs illustrated in Table 1, the FB dataset is the shortest and least complex, with a mean length of 436 words, while NS is the longest. PH documents fall in the middle, with a mean of 660 words, while SC average 980 words, making it the second longest dataset, with the most complex language and opinion-varying documents.\nTo provide a comprehensive evaluation, we evaluate three models including GPT-40, Claude 3.5 Sonnet, and Gemini 1.5 Pro under two settings: zero-shot and few-shot, using three-, six-, and nine-shot prompts.\nIn the zero-shot setting, models rely solely on their pre-existing knowledge without any provided examples, as illustrated in Figure 3 in Appendix B. For the few-shot setting, three-shot prompting includes one example per class (one positive, one negative, and one neutral), as shown in Figure 4 in Appendix B. Six-shot prompting expands the three-shot setup by adding three more examples from each class, and nine-shot prompting maximizes context by adding three additional examples for"}, {"title": "4 Results and Discussion", "content": "This section presents the results and discussion of the zero-shot and few-shot findings."}, {"title": "4.1 Zero-Shot Performance Evaluation", "content": "On average, although GPT-40 outperforms both Claude 3.5 Sonnet and Gemini 1.5 Pro on simpler and shorter datasets, Claude 3.5 Sonnet achieves higher performance on more complex, sentiment-wavering, and lengthier datasets such as NS and SC as shown in Figure 1. It can be seen that while Gemini performs satisfactorily on the longest dataset NS, it achieves the least performance on the other datasets.\nAll models achieve relatively low and similar performance on the NS dataset, which has the longest context and simple language, with Gemini 1.5 Pro outperforming the others and Claude 3.5 Sonnet performing comparatively lower. On the other hand, GPT-4o achieved the lowest performance on the NS dataset, which might be attributed to the size of the model's parameters compared to the other two models. It can be said that, despite the contamination and simplicity of the language in the NS dataset, the models still struggle to analyze SA in long-context comments.\nDespite the FB and PH datasets having almost similar mean comment lengths, the PH dataset comment lengths are much more spread out. Additionally, the PH dataset contains more complex and sentiment-wavering comments compared to FB. The differences in performance among the three models are relatively minor on the FB dataset. However, in the more complex and lengthier PH dataset, the performance differences across the models are notably higher. This variation might be attributed to the dataset's contamination, where the models lack prior knowledge, in contrast to the FB dataset results, or due to the more dynamic and ever-changing sentiments in the PH comments compared to FB. GPT-40 achieved the highest performance on both FB and PH datasets, while Claude 3.5 Sonnet performed relatively lower in zero-shot sentiment analysis classification.\nIn the SC dataset, which is free from data contamination and features the most challenging language with frequent shifts in opinions, all models achieved relatively low performance. However, Claude 3.5 Sonnet outperformed the other models, while Gemini 1.5 Pro recorded the lowest performance at 41.93%."}, {"title": "4.2 Few-Shot Performance Evaluation", "content": "The few-shot analysis highlights that GPT-40 and Claude 3.5 Sonnet exhibit similar performance trends, and their results remain competitive, as shown in Figure 2. However, despite Claude 3.5 Sonnet achieving higher overall performance, particularly in the nine-shot setting for the majority of datasets, GPT-40 does not benefit from an increased number of demonstrations for improved performance, as there are only slight changes in performance across the three-shot, six-shot, and nine-shot settings. Nevertheless, performance mostly fluctuates and declines across all four datasets for all three models in the six-shot and nine-shot settings.\nThis fluctuation in performance with an increasing number of shots could be attributed to several factors. Although existing studies generally suggest that increasing the number of demonstration examples enhances in-context learning performance (Liu et al., 2021), the results show that LLMs still struggle with SA task in long contexts containing multiple pieces of information, leading to inconsistent or degraded performance.\nFurthermore, it can be inferred that as input prompts become longer, model performance may either decline or fluctuate, which highlights the challenges of LLMs due to the maximum sequence length encountered during training. This limitation hampers their ability to handle task requiring deep comprehension of complex and lengthy instructions.\nAnother reason for this phenomenon might be the challenges LLMs face in low-resource domain. This stems from their struggle to fully grasp specialized terminologies and nuances unique to specific domains, which results in inconsistent performance, particularly in interpreting highly domain-specific information. Without sufficient exposure to domain-specific examples during prompt demonstrations, the model may fail to generalize effectively.\nGemini 1.5 Pro demonstrates notable performance improvement in few-shot settings compared to GPT-40 and Claude 3.5 Sonnet in NS, similar to the observations in zero-shot results. A general trend shows that as the complexity and length of datasets decrease, the increase in model performance with more shots declines and fluctuates in SC, PH, and FB, respectively. This might indicate that the performance of the Gemini model is more affected by dataset length rather than complexity or the number of sentiment shifts for performance improvement.\nAnother observation, when comparing the results of zero-shot with few-shot (three-shot) settings, considering the data contamination status of the datasets, shows higher or similar performance on contaminated datasets compared to uncontaminated datasets, which achieved lower performance in zero-shot compared to three-shot. The contaminated dataset results likely reflect memorization, while the contamination-free results reflect true learning and adaptation. This distinction highlights the importance of few-shot learning for improving performance on uncontaminated and unfamiliar datasets."}, {"title": "5 Conclusion", "content": "The zero-shot analysis indicates that GPT-40 excels in simple, short SA task across various datasets, while Claude 3.5 Sonnet outperforms it in more complex, sentiment-wavering task. In the few-shot analysis, both models exhibit similar trends, with Claude 3.5 Sonnet achieving superior results on most datasets. However, GPT-40 demonstrates greater stability with an increasing number of shots, and Gemini 1.5 Pro performs well on longer datasets in both zero-shot and few-shot scenarios.\nWe found that the models exhibit comparable or superior performance in zero-shot settings on contaminated datasets compared to uncontaminated ones. While contaminated datasets can cause an increase in memorization and performance in zero-shot scenarios, the models' performance often declines or remains unchanged as the number of shots increases across four datasets which highlights the difficulties that LLMs experience due to the maximum sequence length, restricting their capability to comprehend intricate and lengthy prompts."}, {"title": "6 Limitations", "content": "While this study aims to evaluate the performance of LLMs in analyzing complex, lengthy, and sentiment-wavering public opinions, several limitations remain. First, the scope of the evaluation is restricted by the limited number of shots, which may affect the robustness of the results. Expanding the number of shots and conducting many-shot analysis could provide more comprehensive insights. Second, this study does not explore a wider range of models, which could offer a broader comparison of performance across different architectures and versions. Finally, the evaluation is limited to overall sentiment classification task and does not extend to ABSA, which would provide a more granular understanding of opinion sentiment."}, {"title": "A Experimental Setup", "content": "For all experiments, we use gpt-40 endpoint for GPT-40, gemini-1.5-pro endpoint for Gemini 1.5 Pro, and claude-3-5-sonnet-20240620 endpoint for Claude 3.5 Sonnet. To promote deterministic outputs from the selected models, we set the temperature to 0 and the final results are the averages of three independent runs."}]}