{"title": "Empirical Insights on Fine-Tuning Large Language Models for Question-Answering", "authors": ["Junjie Ye", "Yuming Yang", "Qi Zhang", "Tao Gui", "Xuanjing Huang", "Peng Wang", "Zhongchao Shi", "Jianping Fan"], "abstract": "Large language models (LLMs) encode extensive world knowledge through pre-training on massive datasets, which can then be fine-tuned for the question-answering (QA) task. However, effective strategies for fine-tuning LLMs for the QA task remain largely unexplored. To address this gap, we categorize supervised fine-tuning (SFT) data based on the extent of knowledge memorized by the pretrained LLMs and conduct a series of empirical analyses. Our experiments, involving four LLMs from three different model families, focus on three key factors: the amount of data required for SFT, the impact of different SFT datasets on model performance, and how data requirements vary across LLMs. The results show that as few as 60 data points during the SFT stage can activate the knowledge encoded during pre-training, enabling LLMs to perform the QA task. Additionally, SFT with data of varying memory levels has a significant impact on LLM performance, with the optimal dataset differing based on the specific model being fine-tuned. Future research will delve deeper into the mechanisms underlying these phenomena.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs), such as the GPT (Brown et al., 2020; Chen et al., 2021; OpenAI, 2023; Ouyang et al., 2022), LLaMA (Dubey et al., 2024; Rozi\u00e8re et al., 2023; Touvron et al., 2023a,b), and Qwen (Bai et al., 2023; Yang et al., 2024) series, are pretrained on diverse corpora covering a wide range of genres and world knowledge. This knowledge is encoded within the model's parameters (He et al., 2024; Kritharoula et al., 2023; Qiao et al., 2023; Wang et al., 2024; Ye et al., 2023) and can be applied to the question-answering (QA) task through supervised fine-tuning (SFT) (Sticha et al., 2024; Wen et al., 2024; Zhang et al., 2024).\nAs research advances, there is growing interest in optimizing fine-tuning strategies for LLMs in the QA task. For instance, Ren et al. (2024) collected a dataset of multiple-choice questions, segmented the data based on the accuracy of pretrained LLM responses, and conducted a instruction fine-tuning study. Their findings suggest that effective SFT requires maintaining consistent knowledge within model parameters before and after fine-tuning. Similarly, using a Wikipedia-based corpus, Gekhman et al. (2024) segmented data based on the accuracy of pretrained models with different hyperparameters and found that training on poorly memorized knowledge from pretrained LLMs significantly increased hallucinations.\nHowever, these studies fall short in identifying effective fine-tuning strategies for LLMs in the QA task. On one hand, they segment the training data based on the accuracy of LLM responses in few-shot scenarios, which are highly sensitive to specific in-context examples and may introduce bias into the results (Min et al., 2022; Srivastava et al., 2024). On the other hand, the granularity of data segmentation is relatively coarse, typically limited to three categories (e.g., low, medium, and high), restricting detailed analysis and hindering the ability to draw more precise conclusions.\nTo explore this issue in depth, we develop a robust multi-template complementation mechanism (Section 2.1) to evaluate how well pretrained LLMs memorize different types of knowledge. We then conduct an empirical analysis of four LLMs from three families to address three key questions.\nQ1: How much data is needed in the SFT stage to enable LLMs to perform the QA task? We collect data from Wikipedia on 12 location-related topics to create the training and test sets, as well as data on 12 unrelated topics to build the out-of-domain test set (Section 2.2). By varying the amount of training data, we find that only 60 data points are needed in the SFT stage for LLMs to efficiently perform the QA task and demonstrate strong generalization ability. Increasing the training data does not yield significant gains and may even harm model performance (Section 3.1). We speculate that this is because SFT activates and refines knowledge already encoded during pre-training, requiring only minimal parameter tuning to optimize the process.\nQ2: How do different SFT datasets affect LLM performance on the QA task? We investigate this by categorizing training and test data into five memory levels using the multi-template complementation mechanism. Our findings reveal that fine-tuning with data of varying memory levels results in significant differences in knowledge activation. Specifically, while LLMs consistently provide more accurate answers to knowledge that is well-remembered from pre- training, using data that the model barely memorized to do SFT severely impairs its activation of high-memory-level knowledge (Section 3.2). This underscores the importance of careful data selection in SFT and demonstrates how different datasets can profoundly impact the ability of LLMs to perform the QA task.\nQ3: How do data requirements for the SFT stage vary across LLMs? We conduct a comparative analysis of the knowledge memory levels of different LLMs (Section 4.1) and fine-tune them using the same data, observing significant differences in their performance on the QA task (Section 4.2). This suggests that variations in the pre-training corpora of LLMs lead to corresponding differences in their data requirements during the SFT stage, offering new insights into the optimal composition of training data for different models.\nIn summary, our contributions are as follows:\n\u2022 We design a multi-template complementation mechanism that reliably assesses the extent to which pretrained LLMs memorize different types of knowledge.\n\u2022 We conduct an extensive empirical analysis of four LLMs from three different families to address three key questions regarding fine-tuning LLMs for the QA task.\n\u2022 We identify intrinsic differences in fine-tuning different LLMs for the QA task, offering new insights to guide the development of more effective fine-tuning strategies.\n\u2022 We plan to further explore the underlying mechanisms of fine-tuning LLMs for the QA task, providing deeper explanations for these findings."}, {"title": "2. Experimental Setup", "content": "We perform a comprehensive empirical analysis to offer insights into fine-tuning LLMs for the QA task. The corresponding experimental setup is detailed in this section."}, {"title": "2.1. Multi-Template Complementation Mechanism", "content": "Given a pre-trained LLM $M_{base}$ and a knowledge dataset $K$, we aim to explore how fine-tuning $M_{base}$ with a subset of $K$, denoted as $K_1 \\subseteq K$, impacts its performance on the QA task. Previous studies have categorized data based on the accuracy of $M_{base}$'s responses in few-shot scenarios, which can be biased by the specific examples used. To address this, we propose a memory discrimination scheme utilizing a more robust multi-template complementary mechanism.\nIllustrated as Figure 1, consider $k \\in K$ as an element in $K$ represented by a triple $(subject, relation, object)$, such as $(Painblanc, locatedin, France)$. Given a sentence $x = map(subject, relation)$ that maps the subject and relation (e.g., \u2018Painblanc is located in '), if $M_{base}$ can predict $y = map(object)$ by mapping the object (e.g., \u2018France'), such that $y \\subseteq M_{base}(x)$, we consider that $M_{base}$ has memorized knowledge $k$. Since $M_{base}$ is a probabilistic model whose output is influenced by different mapping templates and sampling probabilities, we design $N_{map} = 21$ different mappings for each piece of knowledge $k$. With the temperature set to 0.7, the model generates $N_{sample} = 10$ outputs for each mapping, and we calculate the degree to which the LLM memorizes $k$ as\n$R_k^M = \\frac{\\sum_{i=1}^{N_{map}} \\sum_{j=1}^{N_{sample}} I(y_i \\subseteq M_{base}^j(x_i))}{N_{map} \\times N_{sample}}$\nwhere $x_i$ and $y_i$ are the results from the $i$th mapping, $M_{base}^j$ represents the $j$th sample, and $I(\\cdot)$ is the indicator function."}, {"title": "2.2. Dataset", "content": "For our study, we use ENTITYQUESTIONS (Sciavolino et al., 2021), a QA dataset containing knowledge from Wikipedia on 24 different topics. We select the original training sets from 12 location-related topics as our training data $D_{train}$, their corresponding test sets as our test sets $D_{test}$, and the test sets from the remaining 12 topics as out-of-domain test sets $D_{test-ood}$. Details of the data are listed in Table 1. Detailed topics and corresponding mapping templates can be found in Appendix A."}, {"title": "2.3. Models", "content": "To ensure generalizable results, we analyze four LLMs from three different families, described as follows.\n\u2022 LLaMA-2 Family. The LLaMA-2 family (Touvron et al., 2023b) consists of open-source LLMs developed by Meta, pre-trained on over 2 trillion tokens, offering extensive world knowledge and strong semantic representation. In this paper, we select LLaMA-2-7B and LLaMA-2-13B for our analysis.\n\u2022 LLaMA-3 Family. The LLaMA-3 family (Dubey et al., 2024) represents the latest and most advanced open-source LLMs developed by Meta, available in both 8B and 70B parameter sizes. In this paper, we select LLaMA-3-8B for our analysis.\n\u2022 Qwen-2 Family. The Qwen-2 family (Yang et al., 2024), developed by Alibaba, consists of generalized LLMs trained on data in 29 languages, with parameter sizes ranging from 0.5B to 72B. In this paper, we select Qwen-2-7B for our analysis."}, {"title": "2.4. Metrics", "content": "Given a pre-traind LLM $M_{base}$, to provide a detailed analysis of its performance on the QA task after SFT, we apply the mechanism defined in Section 2.1 and divide the test set based on memorization levels:\n$\\begin{cases}D_{test-0} = \\{k \\in D_{test}|R_k^M = 0\\},\\\\D_{test-1} = \\{k \\in D_{test}|R_k^M \\in (0,0.25]\\},\\\\D_{test-2} = \\{k \\in D_{test}|R_k^M \\in (0.25,0.5]\\},\\\\D_{test-3} = \\{k \\in D_{test}|R_k^M \\in (0.5,0.75]\\},\\\\D_{test-4} = \\{k \\in D_{test}|R_k^M \\in (0.75,1]\\}\\end{cases}$\nWe calculate the proportion of $y \\subseteq M_{sft}(x)$ under the test mapping template to determine the accuracy for each sub-test set, $ACC_{test-i} (i = 0, 1, 2, 3, 4)$, where $M_{sft}$ represents for the LLM after fine-tuning. The overall performance on the test set is then obtained by averaging the accuracies:\n$ACC_{test} = AVG_{i=0}^4(ACC_{test-i})$"}, {"title": "2.5. Implementation Details", "content": "Our experiments are divided into three main stages: memory level differentiation, SFT, and performance evaluation. Below are the implementation details for each stage:\n\u2022 Memory Level Differentiation. To balance output stability and diversity, we design 21 mapping templates for the data of each topic. The temperature is set to 0.7 for model sampling, with 10 repetitions for each sample. The output token length is set to 32.\n\u2022 SFT. In this stage, we set the batch size to 16, and train for 1 epoch using the AdamW (Loshchilov and Hutter, 2017) optimizer with cosine scheduling. The learning rate is set to 2e-5 for the LLaMA-2 models and 1e-5 for the others. To fully utilize model capabilities, we use the officially recommended prompt templates for each model, as listed in Appendix B.\n\u2022 Performance Evaluation. For sampling, we use greedy search with a maximum output length of 16, maintaining the same prompt templates as during training. To reduce bias from the selected training data, we randomly select five different sets of training data for each memory level and repeat the experiments with these sets. We then report the mean and variance of the results from these five experiments."}, {"title": "3. Main Results", "content": "To provide a comprehensive analysis of how to effectively fine-tune LLMs for QA tasks, we examine the data volume requirements during the SFT stage (Section 3.1) and the impact of fine-tuning with data at different memory levels (Section 3.2)."}, {"title": "3.1. Data Volume Requirements during SFT", "content": "Focusing on Q1\u2014how much data is needed in the SFT stage to enable LLMs to perform the QA task\u2014we analyze each LLM $M_{base}$ using different memory levels of the training data $D_{train-i}$. We divide the training data into six volume levels, ranging from 60 samples to the full dataset, and compose the training sets by uniformly sampling from 12 topics. The results of the in-domain and out-of-domain evaluation experiments under different settings are shown in Figure 2 and Figure 3, respectively.\nFinding 1 The experimental results indicate that just 60 training samples are sufficient for LLMs to efficiently perform the QA task after SFT while demonstrating strong generalization ability. The in-domain results (Figure 2) reveal that LLMs perform better with fewer training samples compared to using 960 or all samples, regardless of the base model or memory level. Most models peak or approach peak performance at $N_{train}$ = 60, suggesting this amount is adequate for the in-domain task. Furthermore, the out-of-domain evaluation results (Figure 3) indicate that optimal performance can still be achieved with a limited number of training samples compared to the full dataset. We speculate that this occurs because SFT activates and refines knowledge already encoded during pre-training, requiring only minimal parameter tuning to optimize the process."}, {"title": "3.2. Impact of Fine-Tuning with Data at Different Memory Levels", "content": "Consider Q2, i.e., how do different SFT datasets affect LLM performance on the QA task? Following the method outlined in Section 2.4, we categorize the memory levels of the training data, test data, and out-of-domain test data for each LLM. Since we established in Section 3.1 that fine-tuning with just 60 samples enables LLMs to perform satisfactorily on the QA task, this section focuses solely on the results under this setup. We present the mean and standard deviation of performance for different LLMs on the in-domain and out-of-domain test sets in Table 2 and Table 3, respectively.\nFinding 2 Regardless of the data used for fine-tuning, LLMs consistently provide more accurate answers to knowledge that is better memorized during pre-training. Examining each row in Table 2 and Table 3, we observe that SFT models consistently achieves better performance on test sets with higher memory levels compared to those with lower memory levels, as indicated by the relationship $ACC_{test-4} > ACC_{test-3} > ACC_{test-2} > ACC_{test-1} > ACC_{test-0}$.\nFinding 3 Training with data at a specific memory level enhances the performance of LLMs on that level of knowledge. In Table 2, we notice an interesting \u2018diagonal phenomenon': for test data of a particular memory level, LLMs trained with data of the same memory level tend to perform the best. For example, for $D_{test-0}$, the performance using $D_{train\u20130}$ is typically the highest. This suggests that data from different memory levels may be encoded differently within the model. Therefore, selecting the appropriate dataset is crucial when aiming to enhance LLM performance across different knowledge levels. Table 3 also demonstrates a similar phenomenon in the out-of-domain test set.\nFinding 4 Overall, a more effective strategy is to use data with higher memory levels for SFT. Table 2 and Table 3 show that training with $D_{train-0}$ significantly impairs LLM performance on higher memory levels of the test data (e.g., $D_{test-3}, D_{test-4}$), negatively affecting overall performance. In contrast, models trained with relatively high memory level data tend to achieve the best overall performance $ACC_{test}$ because they maintain a more balanced approach across different memory levels."}, {"title": "4. Further Studies", "content": "In this section, we further explore Q3, i.e., how do data requirements for the SFT stage vary across LLMs? On one hand, we compare the distributions of knowledge memory levels across different LLMs to examine the variations in their knowledge memorization (Section 4.1). On the other hand, we train different models using the same data to highlight the specificity of fine-tuned data for each LLM (Section 4.2)."}, {"title": "4.1. Distribution of Knowledge Memory Levels across Different LLMs", "content": "To thoroughly analyze the differences in knowledge memory levels among individual LLMs, we compare the memory levels of different LLMs on the training data $D_{train}$ in a pairwise manner, with the results presented in Figure 4.\nFinding 5 The results in Figure 4 demonstrate significant differences in knowledge distribution among various LLMs. For instance, in the heat map (c), knowledge that is difficult to memorize in Qwen-2-7B (i.e., $D_{train-0}$) is still partially memorized in LLaMA-3-8B, where 33 items are deeply memorized (i.e., $D_{train-4}$). Additionally, LLaMA-3-8B exhibits a higher level of memorization compared to other models, suggesting a broader knowledge base. Considering that the ability of LLMs to encode knowledge is linked to the amount of corresponding data in the pre- training corpus (Allen-Zhu and Li, 2024; Kandpal et al., 2023), these differences likely stem from variations in the pre-training corpora of different LLMs."}, {"title": "4.2. Varying Data Requirements across LLMs", "content": "Given the significant differences in knowledge memory level distribution across various LLMs, we hypothesize that the appropriate SFT data required for each LLM differs. To test this hypothesis, we train different models using the same batch of SFT data. Specifically, we select 60 data samples, which may be classified into different memory levels by different base models, denoted as $D_{train}^{* 60}$, and use them to train LLMs from three families. The results are shown in Table 4.\nFinding 6 The results in Table 4 clearly illustrate the significant differences that arise when fine- tuning different LLMs using the same data. Specifically, while LLaMA-3-8B demonstrates superior performance when fine-tuned with $D_{train}^{* 60}$, the QA abilities of the other models, particularly Qwen-2-7B, are not well-developed. Combined with the findings in Figure 4, we observe that LLMs with similar distributions of knowledge memory levels (e.g., LLaMA- 2-7B and Qwen-2-7B) perform more consistently after fine-tuning with the same data. This suggests that selecting the most appropriate training data for different models should be based on their memory level distribution characteristics."}, {"title": "5. Related Work", "content": "Supervised Fine-Tuning SFT, also known as instruction tuning, is a crucial stage in the training of LLMs, primarily aimed at enhancing their performance across various downstream tasks through multi-task training (Chen et al., 2023, 2024; Ouyang et al., 2022; Zhang et al., 2023). Numerous studies (Ghosh et al., 2024; Sun and Dredze, 2024; Ye et al., 2024a,b) have examined ways to optimize SFT, highlighting that data selection during this phase significantly influences training effectiveness (Dong et al., 2024; Shumailov et al., 2024; Xia et al., 2024). For example, both Zhou et al. (2023) and Liu et al. (2024) demonstrated that LLMs can be better tuned for general tasks by using fewer but higher-quality instruction-tuned data. In this work, we investigate the mechanisms underlying SFT in the context of the QA task and explore strategies for selecting more effective SFT data.\nQuestion-Answering The QA task requires LLMs to effectively utilize the knowledge encoded during pre-training to answer users' questions (Sticha et al., 2024; Wen et al., 2024; Zhang et al., 2024). However, LLMs can sometimes provide incorrect answers to factual questions (Huang et al., 2023). Some researchers (Kandpal et al., 2023; Kang and Choi, 2023) attribute this to the underutilization of pre-training data, leading to knowledge shortcuts and recall failures, while others (Schulman, 2023) suggest that capability misalignment introduced during post-training are to blame. Regardless, enhancing the QA capabilities of LLMs remains a key concern for researchers. Ren et al. (2024) indicated that SFT for the QA task should maintain consistency in model knowledge before and after fine-training. Meanwhile, Gekhman et al. (2024) focused on hallucinations caused by new knowledge training and found that poorly memorized knowledge in pre-trained LLMs significantly increases these hallucinations. Our study aims to shed light on the role of SFT in the QA task and inform the development of more effective SFT strategies. We investigate the impact of data volume during the SFT stage, introduce a more precise multi-template complementation mechanism to evaluate knowledge memorization levels, and perform finer data segmentation to obtain more accurate and detailed results."}, {"title": "6. Conclusion and Future Work", "content": "In this paper, we present a comprehensive empirical analysis of fine-tuning LLMs for the QA task. We propose a memory discrimination approach based on a multi-template complementation mechanism to thoroughly explore the data requirements for SFT, the effects of using data with varying memory levels, and the differences in data needs across LLMs. We hope our findings will provide valuable insights for designing more effective SFT strategies.\nIn the future, we aim to build on these results by conducting deeper analyses of the underlying mechanisms in LLMs. Our objective is to investigate performance variations in LLMs caused by SFT under different conditions and the resulting model changes, with the goal of clarifying key characteristics of using LLMs for the QA task."}]}