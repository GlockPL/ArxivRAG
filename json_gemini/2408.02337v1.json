{"title": "Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction", "authors": ["Albert Sawczyn", "Katsiaryna Viarenich", "Konrad Wojtasik", "Aleksandra Domoga\u0142a", "Marcin Oleksy", "Maciej Piasecki", "Tomasz Kajdanowicz"], "abstract": "Advancements in AI and natural language processing have revolutionized machine-human language interactions, with question answering (QA) systems playing a pivotal role. The knowledge base question answering (KBQA) task, utilizing structured knowledge graphs (KG), allows for handling extensive knowledge-intensive questions. However, a significant gap exists in KBQA datasets, especially for low-resource languages. Many existing construction pipelines for these datasets are outdated and inefficient in human labor, and modern assisting tools like Large Language Models (LLM) are not utilized to reduce the workload. To address this, we have designed and implemented a modern, semi-automated approach for creating datasets, encompassing tasks such as KBQA, Machine Reading Comprehension (MRC), and Information Retrieval (IR), tailored explicitly for low-resource environments. We executed this pipeline and introduced the PUGG dataset, the first Polish KBQA dataset, and novel datasets for MRC and IR. Additionally, we provide a comprehensive implementation, insightful findings, detailed statistics, and evaluation of baseline models.", "sections": [{"title": "1 Introduction", "content": "Question answering (QA) systems serve as a sophisticated interface between humans and computers. To further enhance their utility, we need QA systems capable of answering questions based on extensive knowledge (Petroni et al., 2021). The knowledge base question answering (KBQA) task addresses this need by using structured knowledge graphs (KG) to provide accurate and relevant answers (Lan et al., 2021). KBQA leverages these graphs, which are rich with interconnected entities and relationships, to decode complex queries and deliver precise answers. Importantly, systems that reason over KGs are more resistant to the phenomenon of hallucinations, common in large language models (LLM) (Baek et al., 2023). Additionally, the inherent flexibility of KGs facilitates easy modification and updating, ensuring the use of only the most current and accurate facts.\nHowever, a significant gap exists in KBQA datasets. Many are schematic and not natural in their language, or they rely on discontinued knowledge graphs (Lan et al., 2021; Steinmetz and Sattler, 2021; Jiang and Usbeck, 2022). By natural we refer to naturally occurring questions (Kwiatkowski et al., 2019). While a broader range of KBQA datasets is available for English, most low-resource languages, including Polish, lack such resources (Korablinov and Braslavski, 2020). This scarcity is part of a broader issue prevalent in the field of NLP concerning low-resource languages (Augustyniak et al., 2022). Recognizing this gap, we set out to create a KBQA dataset for Polish. We faced several challenges during extensive studies of existing works to find the most efficient methods for dataset creation. Many datasets were built on simpler predecessors (Korablinov and Braslavski, 2020; Kaffee et al., 2023), and also many construction pipelines are inefficient regarding human labor, as they do not utilize modern tools that could reduce human work, such as assisting Large Language Models (LLM). LLMs have opened new opportunities for assisting human annotators, especially in low-resource languages where the range of pre-trained models is limited (Gilardi et al., 2023; Kuzman et al., 2023).\nConsequently, we decided to design, implement, and execute a modern approach to creating KBQA datasets tailored explicitly for the low-resource environment. We selected Wikidata as KG due to its extensive, multilingual coverage and dynamic, open, and free nature (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014). Notably, we did not use any translation, ensuring that the output data sounded natural. Moreover, an advantageous byproduct of this pipeline was the concurrent development of machine read-"}, {"title": "2 Related Work", "content": "KBQA Existing KBQA datasets have been comprehensively studied and compared in works done by Korablinov and Braslavski (2020) and Jiang and Usbeck (2022). A significant finding is the lack of a Polish KBQA dataset. Most KBQA datasets are primarily in English, with exceptions like the Chinese NLPCC-KBQA (Duan and Tang, 2018),\nRussian RuBQ (Korablinov and Braslavski, 2020), the multilingual QALD (Perevalov et al., 2022) and MCWQ (Cui et al., 2022) (both not including Polish). The closest dataset resembling a KBQA task in Polish is the multilingual MKQA (Longpre et al., 2021), where approximately 42% of its 10,000 questions are answerable by Wikidata entities. However, MKQA cannot be classified as a proper KBQA dataset due to the lack of annotated topic entities.\nThe study by Korablinov and Braslavski (2020) outlines the various question generation techniques used in existing KBQA datasets. For generating natural questions in our research, we adopted a question generation technique based on query suggestion, initially introduced by Berant et al. (2013). This technique is effective for acquiring natural factoid questions likely to be posed to a QA system, similar to the approaches used in datasets like NQ (Kwiatkowski et al., 2019) and WikiQA (Yang et al., 2015), which were built from questions asked to search engines. For template-based questions, our approach involved creating questions from predefined reasoning templates, a standard method in many KBQA datasets (Bordes et al., 2015; Su et al., 2016; Dubey et al., 2019). Several KBQA datasets used crowdsourced paraphrasing for question diversification (Talmor and Berant, 2018; Su et al., 2016; Dubey et al., 2019). In contrast, our approach only automates this process by incorporating humans during final verification.\nIR Many valuable resources for Information Retrieval in the Polish language were recently created. The BEIR-PL (Wojtasik et al., 2024) benchmark was proposed as an automatic machine translation of the BEIR (Thakur et al., 2021) benchmark. This popular zero-shot retrieval benchmark was originally only for the English language. The MQUPQA (Rybak, 2023) dataset is a composition of multiple already existing Polish and multilingual datasets, like CzyWiesz (Marci\u0144czuk et al., 2013), MKQA (Longpre et al., 2021). Additionally, the MQUPQA dataset incorporates other automatic methods for question and answer generation, such as utilizing the generative capabilities of the GPT-3 model (Brown et al., 2020) or employing templates inspired by the structure of Wikipedia. The PolEval (\u0141ukasz Kobyli\u0144ski et al., 2023) competition featured a passage retrieval task. It comprised three datasets from various domains: Wikipedia-based, e-commerce shop FAQ, and legal questions. Cur-"}, {"title": "3 Definitions", "content": "A common element in the tasks of KBQA, MRC, and IR is the textual question q. We denote the set of questions as Q. Despite query being common in the field of IR, we use question and query interchangeably, as our dataset's queries take the form of questions.\nKBQA We denote KG as a multi-relational heterogeneous graph $G = (E,R,T)$, composed of three elements: a set of entities E, a set of relation predicates R, and a set of triples (facts) T. Each triplet (h, r, t) \u2208 T indicates a relation predicate r between two entities, a head entity h and a tail entity t, where h, t \u2208 E and r\u2208 R (Hamilton et al., 2017). In the KBQA task, a textual question q and associated topic entities $E_q \\subset E$ are given. The objective is to retrieve answer entities $A_q \\subset E$ that satisfy the question based on facts in the G. Therefore, we denote KBQA dataset as $D_{KBQA} = \\{(q, E_q, A_q)\\}$.\nMRC MRC aims to answer a textual question q based on a given text passage $p_q$. We denote MRC dataset as $D_{MRC} = \\{(q,p_q, a_q)\\}$, where $a_q$ is the answer extracted from $p_q$.\nIR The IR task focuses on finding a passage p from a large corpus relevant to a query q. The corpus C is defined as a set of passages, i.e., $C = \\{p_1,p_2,\\dots,p_n\\}$. The IR dataset is denoted as $D_{IR} = \\{(q, p_q)\\}$, where $p_q \\in C$ denotes a passage that is relevant to the query q."}, {"title": "4 Construction Pipeline", "content": "This section introduces the construction pipeline for the PUGG dataset, specifically designed to create a dataset with natural and factoid questions in a semi-automated manner. This approach significantly reduces the workload of human annotators. We outline the pipeline's fundamental design, presented in Figure 1, emphasizing its adaptability to various environments. While this part focuses on the general framework, specific implementation details, such as the models and algorithms used, will be discussed in Section 5.\nQuestion Formulation The initial step of our pipeline involves acquiring a variety of natural factoid questions. We initiated our process using ex-"}, {"title": "5 Pipeline Execution", "content": "This section delves into the specific implementation of the construction pipeline for the PUGG dataset, as previously outlined in a general framework in Section 4. Our implementation was adapted for Polish NLP resources, which face challenges like limited task-specific pre-trained models and lower performance than English.\nQuestion Formulation In implementing our question acquisition step, we utilized two Polish datasets, CzyWiesz (Marci\u0144czuk et al., 2013) and PoQuAD (Tuora et al., 2023). Question prefixes were extracted either by taking the first {1,2,3} tokens from each question or by extracting text up to the first occurrence of a named entity. We employed three NER models: pl_core_news_sm, pl_core_news_lg from Spacy (Honnibal et al., 2020), and WikiNEuRal (Tedeschi et al., 2021). Each of these models provided a unique perspective in identifying named entities, thereby contributing to the variety of the prefixes. To formulate natural questions from these prefixes, we followed previous studies (Berant et al., 2013; Rybin et al., 2021) and used the Google Suggest API.\nPassage Construction We followed established methodologies from prior research (Kwiatkowski et al., 2019) and employed the Google Search Engine 5 to retrieve Wikipedia articles relevant to each question. We processed the top 10 search results using the API, focusing on Wikipedia entries. Questions without a Wikipedia article in the top 10 results were discarded. The text and inter-article references of these Wikipedia articles were then obtained using the Wikipedia API6. The retrieved articles were segmented into shorter passages using a sliding window approach, with a window length of 120 words and a step size of 60 words. We ranked these passages for each question according to their relevance. This was achieved by leveraging the PyGaggle (Pradeep et al., 2023) library with the multilingual reranker model unicamp-dl/mt5-3B-mmarco-en-pt (Bonifacio et al., 2021).\nTextual Answers, Answer Entities For textual answer tagging, we employed GPT-3.5-turbo 7 (Brown et al., 2020) with an originally designed prompt, detailed in Appendix A. Due to the model's generative nature and tendency to alter or paraphrase the original text, we developed a custom method to extract tagged segments accurately. This method is described in Appendix A. As previously described, candidate answer entities were directly referenced in the text, allowing for straightforward extraction.\nTopic Entities Implementing the entity linking step presented several challenges, primarily due to the lack of robust tools or models for entity linking in the Polish language. Our testing of multilingual models like mGENRE (De Cao et al., 2022) and adapted for Polish BLINK (Wu et al., 2020)"}, {"title": "6 Experimental Setup", "content": "In this section, we outline the evaluation methodology used to assess the performance of baseline models on the PUGG dataset.\nKBQA For the KBQA baseline, we evaluated the performance of KAPING (Baek et al., 2023), a zero-shot framework that leverages an LLM for retrieving answer entities. We slightly modified the knowledge retriever module by incorporating a step that retrieves a subgraph of the KG by traversing n edges, regardless of their direction, from the topic entities. Our preliminary experiments demonstrated enhanced performance of the modification, showcasing improvements in both accuracy and processing speed. Subsequently, we follow the original procedure, which involves retrieving k triples based on their textual embeddings. For embedding purposes, we utilized the mmlw-retrieval-roberta-large retrieval model (Dadas et al., 2024). We employed gpt-3-turbo as the LLM, prompted with tailored queries as detailed in Appendix F. The hyperparameters were selected empirically, setting k = 40 and choosing n to be 3 for Wikidata1H and 2 for Wikidata2H. As a metric, we employed accuracy, which measures the proportion of answers included in the LLM's response for each question. It is calculated as follows:\n$Accuracy = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\frac{\\text{num of incl. answers}_i}{A_i}$\nWhile Baek et al. (2023) also used accuracy, we refined it by calculating the correct answer proportion per example and excluding entities' aliases, providing a more realistic measure of KBQA efficacy.\nMRC For the MRC task, we selected models commonly used for the extractive question answering task. We trained and evaluated HerBERT (Mroczkowski et al., 2021) models in an extractive fashion alongside a generative approach using the plT5 (Chrabrowa et al., 2022) models. Models were trained for 10 epochs and evaluated with SQUAD metrics (Rajpurkar et al., 2016). Exact match measures the percentage of predictions that exactly match the gold answer. The F1 metric measures the average token overlap between the prediction and ground truth answer, where both the prediction and answer are treated as a bag of tokens.\nIR Recently, IR has gained significant interest within the Polish research community, and many models have been developed and are open to the research community. These models have already been pre-trained on large datasets, which is why we did not fine-tune them to our dataset. The silver retriever (Rybak and Ogrodniczuk, 2023) model was trained on the MAUPQA dataset. We also"}, {"title": "7 Results and Discussion", "content": "KBQA The summarized results are presented in Table 2. For natural and template-based questions, utilizing KG significantly improves accuracy. The overall accuracy is not high, indicating the challenging nature of the newly introduced PUGG dataset. This complexity highlights its potential as a valuable resource for advancing research and development in the field of KBQA. As expected, reasoning over 1-hop (1H) KG was easier than over 2-hop (2H) KG, reflecting the increased complexity of KG. There is a clear gap in efficacy between natural and template-based questions. That was expected, as template-based questions were designed to be easier. Interestingly, they benefit more from the use of KG than the natural ones. We think that it can be caused by their schematic construction mechanism. Moreover, our pipeline for natural questions does not ensure the existence of appropriate reasoning"}, {"title": "8 Limitations and Future Work", "content": "This section outlines the limitations of our study and potential directions for future work. (1) The natural questions are open domain, focused on location and time, and are created and answered from the Polish cultural, political, and historical perspective. (2) The pipeline for natural questions may sometimes miss certain answer entities. This is because not all answers are present or explicitly referenced in the textual answer. (3) Some of the KBQA natural questions might not have corresponding facts in the KG, as our pipeline does not guarantee the existence of an appropriate reasoning path between topic and answer entities. However, as Wikidata is continuously updated and expanded, this limitation may diminish in the future. (4) The questions might contain grammatical imperfections or mental shortcuts yet remain understandable. (5) Automated annotation with LLM led to variability in the precision of tagged answers in the MRC task due to the absence of specific tagging guidelines. (6) Our study examined a limited number of baseline models. Future evaluations could, in particular, include open-source LLMs like Llama (Touvron et al., 2023) for MRC and KBQA tasks, as well as models that reason directly over the KG structure, such as PullNet (Sun et al., 2019), for KBQA task. (7) While our focus was on standard tasks, we acknowledge the potential for exploring additional tasks using the PUGG dataset. These tasks include entity linking, subgraph retrieval, relation extraction, question type classification, and question generation."}, {"title": "9 Conclusion", "content": "To address the significant resource gap for low-resource languages, our work introduces the PUGG dataset, the first Polish KBQA dataset, which also encompasses MRC and IR tasks. It consists of natural and template-based factoid questions. The dataset is the outcome of our proposed semi-automated construction pipeline, designed for low-resource environments. Leveraging modern tools like LLMs as annotation assistants have significantly reduced the need for human labor. Additionally, we developed few utility methods, such as entity linking, which are useful in various contexts. The PUGG dataset and our pipeline's comprehensive implementation, findings, and detailed statistics from the PUGG dataset construction provide valuable insights for future research. Further-"}, {"title": "10 Ethical considerations", "content": "The process of dataset creation using LLMs and pre-existing datasets entails the potential risk of inheriting biases from both the models and the original data sources. To address this concern, a pipeline could incorporate multiple LLMs and diverse datasets as a mitigation strategy.\nWe used sources with a low risk of containing private data or offensive content. However, during the human verification process, we further ensured that the dataset did not include such data. As mentioned in Section 5, all annotators were employed in Poland and were fluent in Polish. They were familiar with the Polish culture and social context."}, {"title": "11 Acknowledgments", "content": "This work was supported by Polish Ministry of Education and Science under the programme: \"Support for the participation of Polish scientific teams in international research infrastructure projects\", agreement number 2024/WK/01 and project of the Minister of Digitization No. 1/WI/DBil/202 (PLLUM). This work was also partially funded by the European Union under the Horizon Europe grant OMINO \u2013 Overcoming Multilevel Information Overload (grant number 101086321, http://ominoproject.eu) co-financed with funds from the Polish Ministry of Education and Science under the programme entitled International Co-Financed Projects, grant no. 573977."}]}