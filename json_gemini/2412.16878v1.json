{"title": "Online Preference-based Reinforcement Learning with Self-augmented Feedback from Large Language Model", "authors": ["Songjun Tu", "Jingbo Sun", "Qichao Zhang", "Xiangyuan Lan", "Dongbin Zhao"], "abstract": "Preference-based reinforcement learning (PbRL) provides a power-\nful paradigm to avoid meticulous reward engineering by learning\nrewards based on human preferences. However, real-time human\nfeedback is hard to obtain in online tasks. Most work suppose there\nis a \"scripted teacher\" that utilizes privileged predefined reward\nto provide preference feedback. In this paper, we propose a RL\nSelf-augmented Large Language Model Feedback (RL-SaLLM-F)\ntechnique that does not rely on privileged information for online\nPbRL. RL-SaLLM-F leverages the reflective and discriminative capa-\nbilities of LLM to generate self-augmented trajectories and provide\npreference labels for reward learning. First, we identify a failure\nissue in LLM-based preference discrimination, specifically \"query\nambiguity\", in online PbRL. Then LLM is employed to provide pref-\nerence labels and generate self-augmented imagined trajectories\nthat better achieve the task goal, thereby enhancing the quality\nand efficiency of feedback. Additionally, a double-check mecha-\nnism is introduced to mitigate randomness in the preference labels,\nimproving the reliability of LLM feedback. The experiment across\nmultiple tasks in the MetaWorld benchmark demonstrates the spe-\ncific contributions of each proposed module in RL-SaLLM-F, and\nshows that self-augmented LLM feedback can effectively replace the\nimpractical \"scripted teacher\" feedback. In summary, RL-SaLLM-F\nintroduces a new direction of feedback acquisition in online PbRL\nthat does not rely on any online privileged information, offering\nan efficient and lightweight solution with LLM-driven feedback.", "sections": [{"title": "1 INTRODUCTION", "content": "Designing complex artificial reward functions is labor-intensive and\ntime-consuming for reinforcement learning (RL) [2, 11]. Preference-\nbased RL (PbRL) is considered a key paradigm to address this chal-\nlenge by learning rewards based on human preference [8, 16].\nHowever, the substantial human effort required to label a large\nnumber of preference queries significantly hinders its widespread\napplication in real-world scenarios [17, 35]. Especially for online\nPbRL [23], obtaining real-time preference feedback necessitates\nimmediate human interaction with the environment. Current online\nPbRL methods often assume a \"scripted teacher\" that provides real-\ntime preference feedback by comparing handcrafted rewards of\ntwo trajectories from replay buffer [15, 16]. Unfortunately, relying\non privileged rewards undermines the original intent of PbRL.\nLarge Pre-trained Models (LPMs), such as large language models\n(LLMs) [42] and vision-language models (VLMs) [41], equipped with\nextensive human prior knowledge, have recently gained significant\nattention. Recently, some studies have explored using LPMs instead\nof human supervision for reward design, including generating re-\nward code [20, 34, 39] or calculating dense rewards directly based\non the comparison of policy trajectories and targets [19, 25, 26].\nUnfortunately, the first type of approach requires access to the\nsimulation environment's code, and evaluating the reward code\noften involves multiple full RL training cycles, which is impractical\nfor real-world applications. The other type of approach relies on\ncomparing image-text similarities, but a single image may fail to re-\nflect the underlying dynamic information described in the text, and\nreward variance is easily introduced by visual noise in the images\n[33]. Instead of designing reward functions or codes, PbRL learns\nrewards by comparing trajectory pairs. This approach requires only\na single complete online training cycle and does not necessitate"}, {"title": "", "content": "access to any low-level information, such as the environment's\ncode [16, 33]. The learned downstream policies are guaranteed to\nhave a suboptimal performance bound [45].\nInstead of relying on \"scripted teacher\", some works [32, 33]\ntry to obtain preference labels by querying LPMs with manually\ndesigned prompts, to train rewards and policies that align with\nhuman intentions. Although LPMs have substantial capabilities in\nanalyzing trajectories, we find that they still struggle to distinguish\nthe quality of suboptimal trajectories generated by the poor policy,\nespecially during the early stages of training. The failure in pref-\nerence discrimination impacts the learning of the reward model,\nfurther hindering performance. We refer to this phenomenon as\n\"query ambiguity.\" Recently, LPMs have demonstrated reflective\n[22, 38, 44] and planning abilities in decision-making tasks, under-\nstanding the environment and predicting or planning future actions\nto achieve the task goal [6, 7, 24, 40, 43]. These works inspire us\nwith the following thought:\nIn addition to the discriminative capability, can LPMs lever-\nage reflection to generate self-augmented trajectories that pro-\nmoting efficient reward learning in online PbRL?\nIn this paper, we propose Reinforcement Learning from Self-\naugmented LLM Feedback (RL-SaLLM-F). This method seeks to\nmitigate query ambiguity in online PbRL and replaces \"scripted\nteacher\" with LLM-driven feedback. RL-SaLLM-F operates without\nrelying on any predefined privileged rewards, thereby establishing\na practical new paradigm for online PbRL. Specifically, state tra-\njectories in the replay buffer are converted into text descriptions,\nand LLMs are queried to assign preference labels based on these\ntrajectories. Furthermore, a second round of queries is performed to\nprompt the LLM to generate imagined trajectories aligned with task\ngoals, serving as as new preference pairs for reward learning. In ad-\ndition, we mitigate the randomness of LLM-based preference labels\nusing a double-check mechanism that swaps the order of two input\ntrajectories. Finally, we evaluate RL-SaLLM-F on multiple tasks in\nthe Metaworld benchmark [37], it achieves comparable or better\nsuccess rates than feedback from \"scripted teacher\" with privileged\nrewards. The core contributions in this paper are as follows:\n\u2022 We identify an issue of query ambiguity in online PbRL\nwith LLM feedback, and propose the RL-SaLLM-F method\nto mitigate the potential challenges.\n\u2022 We leverage self-augmented LLM feedback to obtain prefer-\nence labels efficiently and use a double-check mechanism to\nreduce randomness in the LLM-based labels.\n\u2022 RL-SaLLM-F achieves comparable performance to that of\n\"scripted teacher\" with privileged reward information in the\nMetaworld benchmark, requiring only the lightweight and\ncost-effective GPT-40-mini.\n\u2022 The overall framework of RL-SaLLM-F does not rely on any\npredefined rewards or real-time human interaction, estab-\nlishing a practical new paradigm for online PbRL."}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 Large Pre-trained Models as Rewards", "content": "Applying RL in reward-free environments is challenging. Some\nstudies assist in reward code design with the perceptual capabili-\nties of LPMs [20, 30, 34, 36, 39]. A notable example is Eureka [20],"}, {"title": "", "content": "which leverages GPT-4 [1] to evaluate environment and task infor-\nmation and generate reward code, followed by iterative updates\nusing evolutionary algorithms. Building on this, [39] introduces a\nreflective mechanism to achieve further self-alignment. However,\nthese methods assume that the environment code is accessible, and\neach evaluation of reward code requires a full RL training cycle,\nwhich is impractical for real-world deployment. Instead of design-\ning reward code directly, we utilizes a LLM to provide preference\nlabels for trajectory pairs from online replay buffer, relying on a\nmore accessible textual description of the environment rather than\nfull access to its code.\nAnother line of research obtains rewards from visual observa-\ntions by VLMs [18, 19, 21, 25, 26]. For example, RoboCLIP [26]\ncomputes reward signals by comparing video representations of\nexpert demonstrations with those of policy trajectories. Recently,\nFuRL [10] incorporates a learnable layer into CILP, and fine-tunes it\nto align with real tasks. Despite achieving impressive performances,\nthese methods often need to fine-tune with expert data to reduce\nvariance and noise in the reward [10, 26]. On the other hand, some\nworks compare image-text similarities to obtain dense rewards, but\na single image is insufficient to capture the required dynamic infor-\nmation [33]. For example, in robotic tasks like picking up an object,\na single image cannot indicate if the arm is approaching or moving\naway, causing confusion reward learning. Different from the afore-\nmentioned methods, we abstracts observed trajectories into text\nand learns a reward model through pairwise trajectory compar-\nisons. Our approach enhances the ability to understand trajectories,\nreduces query costs, and guarantees the convergence properties of\ndownstream RL with preference-based reward modeling [45]."}, {"title": "2.2 Preference-based Reinforcement Learning", "content": "A new paradigm for learning reward functions during interactions\nwith the environment, known as online PbRL, leverages human\nfeedback given in the form of trajectory preferences [8, 16]. Most of\nonline PbRL research aims to address challenges such as preference\nnoise [28], reward credit assignment [31], limited preference data\n[14] and efficient querying [23]. However, frequent querying of\nhuman preferences during online training is impractical. In previous\nonline PbRL methods, a \"scripted teacher\" is assumed to provide\npreference labels by comparing privileged predefined rewards of\ntwo trajectories in the specific task [15].\nThis \"scripted teacher\" relying on privileged rewards serves as a\ncompromise to investigate online PbRL algorithms as immediate\nhuman feedback is not available. In this work, we aim to replace\nthe \"scripted teacher\" with a LLM to provide preference feedback.\nRecently, existing works such as RL-VLM-F [33] and PrefCLM [32]\nhave explored using VLMs to evaluate the quality of trajectories\nfor visual input tasks. However, repeated image pair queries [33]\nand the aggregation of multiple GPT4 feedback [32] make these\nmethods costly. Additionally, we find samples with no significant\npreference difference in the replay buffer can lead to a potential risk\nof query ambiguity, which will further increase costs and reduce ef-\nficiency. In contrast, we use a self-augmented and double-checking\nfeedback to improve the efficiency and reliability based on two-\nround preference queries, mastering robotic manipulation tasks\nwith with cost-effective GPT-40-mini [1]."}, {"title": "3 PRELIMINARIES", "content": ""}, {"title": "3.1 Reinforcement Learning", "content": "RL is formulated as a Markov Decision Process (MDP) [27], which\nis characterized by the tuple $M = (S, A, P, r, \\gamma)$. S is the state space,\nA is the action space, $P: S\\times A\\times S \\rightarrow R$ is the transition probability\ndistribution, $r: S\\rightarrow R$ is the reward function, and $\\gamma \\in (0, 1)$ is\nthe discount factor. The objective of RL is to determine an optimal\npolicy $\\pi$ that maximizes the expected cumulative reward: $\\pi =$\narg max $E_{s_0, a_0,...} [\\Sigma_{t=0}^{\\infty}\\gamma^t r (s_t)]$.\nWe use Soft Actor-Critic (SAC) [12], an off-policy RL algorithm,\nas our low-level approach. Specifically, transitions from interactions\nwith the environment are stored in the replay buffer. The actor-critic\nis then trained by sampling data from the buffer and maximizing\nthe entropy of the stochastic policy."}, {"title": "3.2 Learning Rewards from Human Feedback", "content": "Following previous studies [9], we consider state-only trajecto-\nries of length H composed of states and actions, defined as $\\sigma =$\n{$s_k,..., s_{k+H}$}. The goal is to align human preference y between\npairs of trajectory segments $\\sigma^0$ and $\\sigma^1$, where y denotes a distribu-\ntion indicating human preference, captured as $y \\in {1,0,0.5}$. The\npreference label y = 1 indicates that $\\sigma^0$ is preferred to $\\sigma^1$, namely,\n$\\sigma^0 > \\sigma^1$, y = 0 indicates $\\sigma^1 > \\sigma^0$, and y = 0.5 indicates equal pref-\nerence for both. The preference data are stored as triples, denoted\nas D: {$\\sigma^0$, $\\sigma^1$, y}. Then, the Bradley-Terry model [3] is employed\nto couple preferences with rewards. The preference predictor is\ndefined as follows:\n$P_{\\psi}[\\sigma^1 > \\sigma^0] = \\frac{exp (\\Sigma_t f_{\\psi}(s^1_t))}{\\Sigma_{\\tau \\in {0,1}} exp (\\Sigma_t f_{\\psi}(s^{\\tau}_t))}$"}, {"title": "", "content": "where $f_{\\psi}$ is the reward model to be trained, and $\\psi$ is its parameters.\nSubsequently, the reward function is optimized using the cross-\nentropy loss, incorporating the human predefined label y and the\npreference predictor $P_{\\psi}$:\n$L_{CE} = -E_{(\\sigma^0,\\sigma^1,y)~D} {(1 - y) log P_{\\psi} [\\sigma^0 > \\sigma^1] +ylog P_{\\psi} [\\sigma^1 > \\sigma^0]}$"}, {"title": "", "content": "In online PbRL, we train an off-policy RL algorithm while pe-\nriodically sampling trajectory pairs from the replay buffer B for\npreference queries, as done in PEBBLE [16]. As online human pref-\nerence is often not available, the \"scripted teacher\" is used to provide\npreference labels y in most previous works [8, 16]. Specifically, it\ngenerates preferences based on predefined task reward r from the\nenvironment as follows: $y = i$, where i = arg max$_{\\i\\in {0,1}}$ $\\Sigma_t r(s, a)$.\nThese trajectory pairs, along with the preference labels, form the\npreference dataset D. We train the reward model on D, and subse-\nquently relabel the data in B using this model."}, {"title": "4 QUERY AMBIGUITY IN ONLINE PBRL", "content": "In this section, we highlight a potential risk of query ambiguity in\nonline PbRL. This risk stems from the inherent challenges due to the\ntrajectories sampled from the replay buffer tend to be suboptimal\nand of insufficient quality. Figure 1a illustrates an example from\nthe MetaWorld Button-Press task, with two suboptimal trajecto-\nries sampled from the replay buffer. Neither trajectory successfully\nachieves the target: trajectory A deviates upward with larger move-\nment amplitude, while trajectory B moves to the left-rear with\nsmaller movement amplitude. For such low-quality trajectories,\nhuman may be able to evaluate their quality from multiple per-\nspectives (e.g., distance to the button, trajectory smoothness), but\nit is challenging to determine which trajectory is better in terms\nof achieving the target. The \"scripted teacher\" obtains labels from\nprivileged task rewards by comparing the rewards of two trajec-\ntories and selecting the one with the higher cumulative rewards\nas the preferred trajectory, thereby avoiding this potential issue.\nHowever, when human or LLM provide feedback evaluations, the\nsituation becomes significantly more complex. In Figure 1b, we\ntrain PEBBLE [16] by directly replacing the \"scripted teacher\" with\nGPT-40-mini as feedback, and record the label accuracy (compared"}, {"title": "5 RL-SALLM-F", "content": ""}, {"title": "5.1 The Overall Framework", "content": "In this section, we propose Reinforcement Learning from Self-\naugmented LLM Feedback (RL-SaLLM-F).\nThe overall training process is shown in Figure 2, following\nPEBBLE[16], with an off-policy SAC agent as the policy, consisting\nof three steps:\n\u2022 Step 1 (Unsupervised Pre-training): In the early stage of train-\ning, to encourage exploration and increase trajectory diversity, an\nintrinsic reward $r_{int}(s_t) = log(||s_t \u2013 \\tilde{s}_k||)$ is used for pre-training\ndownstream policy $\\pi_{\\theta}$, here $\\tilde{s}_k$ is the k-th nearest neighbor of $s_t$.\n\u2022 Step 2 (Label Querying and Reward Learning): Sampling\ncandidate trajectory pairs and querying labels from the LLM (see"}, {"title": "5.2 Query Feedback Labels Judged by LLM", "content": "First, we sample a trajectory pair {$\\sigma^0$, $\\sigma^1$} from the replay buffer\nB, convert it into textual representations {text($\\sigma^0$), text($\\sigma^1$)}, and\nquery the LLM with task-directed Chain-of-Thought (CoT) prompt\nfor preference labels:\n$y = LLM (Task, Traj 0 = text(\\sigma^0), Traj 1 = text(\\sigma^1))$"}, {"title": "", "content": "To improve query accuracy, we use a double-check mechanism\nto mitigate the randomness in the labels. Specifically, we reverse\nthe order of the input trajectories and query the LLM again:\n$y_{inv} = LLM (Task, Traj 0 = text(\\sigma^1), Traj 1 = text(\\sigma^0))$"}, {"title": "", "content": "If the values of y and $y_{inv}$ indicate the same preference feedback\nis labeled, namely y = 0, $y_{inv}$ = 1 or y = 1, $y_{inv}$ = 0, we consider the\nfeedback label to be valid. Otherwise, we consider this trajectory\npair to be indistinguishable for the LLM, and discard it. When the\nlabel is valid, the trajectory pair triple ($\\sigma^0$, $\\sigma^1$, y) is stored into the\npreference dataset D.\nRemark 1. An alternative is to consider the LLM's evaluation\nof the trajectory pair as equivalent, retaining the pair and setting\nthe label as y = 0.5, treating it as soft label in reward learning. We\ncompare this approach and find that, during early training, the LLM\nstruggle to distinguish the quality of sampled trajectories, resulting\nin numerous hallucinated labels. Including such trajectory pairs is\ndetrimental to reward model learning, so we conclude that discarding\nthem leads to more efficient reward training."}, {"title": "5.3 Self-augmented Feedback Generated by LLM", "content": "Since the differences between trajectory pairs sampled from the re-\nplay buffer are small, particularly during early training, we leverage\nthe reflective and planning abilities of LLM to generate an imagined\ntrajectory that is more goal-directed to accelerate reward model\ntraining. Specifically, we prompt the LLM to generate a textual\ntrajectory text($\\sigma^{LLM}$) that outperform the better trajectory $\\sigma^{0/1}$ of\nthe current trajectory pair {$\\sigma^0$, $\\sigma^1$}, while ensuring that it shares\nthe same initial state as the input:\n$text(\\sigma^{LLM}) = LLM (Generate Prompt, Traj = text(\\sigma^{0/1}))$"}, {"title": "", "content": "Next, the algorithm checks whether $\\sigma^{LLM}$ has the same step\nlength and data format as the sampled trajectory segments. Once\nthe generated trajectory passes the format check, it is converted\ninto a state-based trajectory $\\sigma^{LLM}$ and the imagined pair triple\n($\\sigma^{LLM}$, $\\sigma^{0/1}$, y = 0) (i.e., self-augmented feedback) is stored into D.\nRemark 2. The imagined trajectories do not have to adhere to\nphysical constraints, as they serve solely for preference comparisons\nto train the reward model, rather than for extracting policies or con-\nstructing a world model. We assume the reward model is Markovian,\nimplying that the immediate reward relies exclusively on the current"}, {"title": "", "content": "state. We find that once the LLM understands the task goal, the gen-\nerated trajectories, even if not physically feasible, are still beneficial\nfor training the reward model efficiently. This insight enables us to\nuse a lightweight LLM with simple prompts for trajectory generation\nwithout incorporating numerous executable trajectory constraints.\nAfter several rounds of querying and generating, D contains\ntrajectory pairs sampled from the replay buffer B with LLM-judged\nlabels, and self-augmented trajectory pairs and labels generated by\nthe LLM. These labeled trajectory pairs will be used to train the\nreward model by minimizing Equation 2."}, {"title": "6 EXPERIMENTS", "content": ""}, {"title": "6.1 Setup", "content": "We evaluate RL-SaLLM-F on multiple robotic manipulation tasks\nin the MetaWorld [37] benchmark. The states include the coordi-\nnates of the Tool Center Point (TCP), the extent of TCP's opening,\nthe coordinates of the manipulated object and the target position.\nAll of these states can be easily converted into text-based string\nrepresentations. We conduct experiments on eight tasks:\nButton Press: Press a button;\nDrawer Open: Open a drawer;\nDrawer Close: Push and close a drawer;\nDoor Open: Open a door with a revolving joint;\nDoor Unlock: Rotate the lock and unlock the door;\nWindow Open: Push and open a window;\nHandle Pull: Pull a handle up;\nReach: Reach a goal position.\nTo be deemed successful, the agent must achieve the task target\nwithin a limited number of steps and maintain it until the end of\nthe episode. All the positions are randomized at each initialization.\nIn the following sections, we aim to address five key questions:\n(1) Can RL-SaLLM-F master robot control without any privileged\npredefined rewards or human feedback?\n(2) Does each component of RL-SaLLM-F contribute to perfor-\nmance improvement?\n(3) Does the learned reward function align effectively with task\nprogress?\n(4) How accurately does RL-SaLLM-F assess the quality of trajecto-\nries, and how high is the quality of the LLM-based generated\ntrajectories?\n(5) How do larger LLM impact performance?"}, {"title": "6.2 Comprehensive Performance Comparison", "content": "To evaluate whether RL-SaLLM-F can master robotic manipulation\ntasks without relying on prior reward information, we compare it\nagainst PEBBLE[16] and SAC. PEBBLE serves as a popular approach\nfor evaluation in online PbRL and relies on the \"scripted teacher\"\nfeedback based on task rewards. SAC directly leverages task rewards\nfor policy learning, representing the refer upper bound of online\nRL performance.\nRemark 3. At the outset, it should be clarified that our goal is not\nto surpass these methods, as they both rely on privileged information\nfrom the environment, whereas RL-SaLLM-F operates without such\nassumptions. Due to the lack of baseline comparisons under the same\nsettings, we consider the comparison in this section as reference only."}, {"title": "6.3 Ablation Study", "content": "To investigate the role of each component in RL-SaLLM-F, we con-\nduct ablation studies. Specifically, we remove different modules\nfrom the algorithm the training curves of 4 chosen tasks are shown\nin the Figure 4. In the legend, 'w/o double-check' indicates exe-\ncuting sampling feedback and self-augmented feedback without\nchecking the feedback labels; 'w/o LLM feedback' indicates execut-\ning only LLM self-augmented feedback without sampling feedback;"}, {"title": "6.4 Analysis of The Learned Reward Model", "content": "In addition to evaluating the quality of the learned policy by compar-\ning the performance, a remaining problem is, whether the learned\nreward model aligns with task progress? To answer this question,\nwe compare the reward models learned by PEBBLE and RL-SaLLM-\nF with different ablation versions. Specifically, we choose an expert\ntrajectory and a suboptimal trajectory in the Button Press task, then\nlabel the rewards for these two trajectories. In the expert trajectory,\nthe robotic arm moves directly to the button, presses it, and then\nstays still. In the suboptimal trajectory, the robotic arm moves for\na while, presses the button, and then moves away from the button\ncenter, causing it to spring back. The normalized step rewards of\neach method are shown in the Figure 5.\nWe observe that the rewards of PEBBLE exhibit a very similar\ntrend to the predefined task reward, which aligns with our intuition,\nas PEBBLE's reward model is trained relying on the task reward"}, {"title": "6.5 Feedback Labels Quality Evaluation", "content": "We further compare the quality of feedback labels to investigate the\nperformance of our method in labeling. We extract all sampled tra-\njectory pairs from the training process and analyze the accuracy of\nthe LLM in judging these trajectory pairs (compared to the \"scripted\nteacher\"). The average results of five training seeds are presented in\nTable 1. Apart from the header, the first four rows show the query\nlabel accuracy for RL-SaLLM-F and its ablation variants, while the\nlast row indicates the proportion of query trajectory pairs discarded\ndue to the the double-check mechanism in RL-SaLLM-F. For the\n'w/o LLM feedback' variant, we still query trajectory preferences\nduring training but does not perform self-checking of the LLM\nlabels.\nAs shown in the Table 1, RL-SaLLM-F achieve the highest label\naccuracy among all variants. The absence of the double-check mech-\nanism lead to a decrease in accuracy, indicating that self-checking\ncan reduce label randomness. Furthermore, compared to the 'w/o\nself-augmented' variant, the label accuracy of RL-SaLLM-F is still\nhigher, suggesting that self-augmentation improves reward and\npolicy performance, leading to greater diversity in sampled tra-\njectories. The diversity of trajectories helps the LLM make more"}, {"title": "6.6 Generated Trajectories Quality Evaluation", "content": "Furthermore, we study the quality of the trajectories generated by\nthe LLM. As analyzed in the previous section, we extract the better\nand worse trajectories from the pairs buffer during RL-SaLLM-F\ntraining, along with the imagined trajectories generated by the LLM,\nand calculate the average rewards and variance across different\nseeds using the predefined task reward function. Then, we evaluate\nthe same trajectories using the labels of \"scripted teacher\" and\nrecord the average rewards, as shown in Figure 7. Only tasks related\nto the reward function and state are considered, as the generated\ntrajectories do not include action information.\nComparing the rewards of the LLM-generated trajectories with\nthose it judged as better, we find that RL-SaLLM-F can generate\nhigher-quality trajectories than those it initially preferred, which in\nturn benefits reward learning. When comparing the LLM-generated\ntrajectories with those evaluated by the \"scripted teacher\", we find\nthat the LLM-generated trajectories perform comparably to, or\neven better than, those deemed preferred by the \"scripted teacher\u201c.\nMoreover, a positive correlation is observed between the quality of\ngenerated trajectories and task performance, particularly in the But-\nton Press and Drawer Open tasks, where RL-SaLLM-F outperforms\nPEBBLE. The average reward of the generated trajectories is also\nhigher than that of the trajectories deemed superior by PEBBLE.\nFinally, we provide two visualization examples of 2D projections\nof trajectories generated by the LLM in Figure 7. In the Door Open\ntask, compared to the sampled trajectories, the generated one first\nmoves directly towards the door and then pulls it open to the target\nposition. In the Handle Pull task, the generated trajectory first\nlocates the handle and then pulls it upward to the target position.\nThe examples demonstrate a thorough understanding of task goals\nand trajectory information by the LLM."}, {"title": "6.7 Impact of LLM Scale", "content": "An intuitive question is: would scaling up the LLM boost the per-\nformance of RL-SaLLM-F? The answer is certainly yes. To clearly\nshowcase this improvement, we repeat the experimental analysis\nfrom the previous sections on the Drawer Open task with GPT-40\nas the LLM, and conduct a comprehensive performance evaluation,\nas shown in Figure 8. In addition to the familiar metrics, we define\nan additional one: \"Equal Rate\", which is the proportion of queries\nwhere the LLM assigns equal preferences to two trajectories.\nIn terms of policy performance, trajectory quality, and prefer-\nence label accuracy, GPT-40 significantly outperforms its smaller\ncounterpart, GPT-40-mini. Additionally, GPT-40 tends to provide\nmore labels with y = 0.5, suggesting a more cautious evaluation,\nand avoiding making arbitrary judgments when the trajectories are\nsimilar. However, querying GPT-40 is expensive: about 20 times the\nprice of GPT-40-mini for the same number of tokens. Therefore, we\nstill believe that combining more affordable LLMs offers a highly\ncost-effective solution."}, {"title": "7 CONCLUSIONS", "content": "In this work, we introduce RL-SaLLM-F, a novel technique that\nleverages self-augmented feedback from LLMs for online PbRL. By\nabstracting state trajectories into textual descriptions and utiliz-\ning LLMs to generate self-augmented imagined trajectories and\nprovide preference labels, RL-SaLLM-F successfully addresses the\nl limitations of relying on online privileged rewards or real-time hu-\nman feedback. The experimental analysis from various perspectives\nhave demonstrated the effectiveness of the proposed method.\nAdditionally, we find that the accuracy of the LLM labels in\ntrajectory evaluation remains limited, and further improving this\naccuracy is essential for more efficient task training. Meanwhile, our\nmethod may not directly handle image inputs, future work could ex-\nplore obtaining precise object coordinates through methods such as\ncamera coordinate calibration to align with our algorithmic frame-\nwork, or applying VLMs and diffusion models for discrimination\nand generation of image trajectories."}]}