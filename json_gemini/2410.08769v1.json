{"title": "Efficient Multi-Object Tracking on Edge Devices via Reconstruction-Based Channel Pruning", "authors": ["Jan M\u00fcller", "Adrian Pigors"], "abstract": "The advancement of multi-object tracking (MOT) technologies presents the dual challenge of maintaining high performance while addressing critical security and privacy concerns. In applications such as pedestrian tracking, where sensitive personal data is involved, the potential for privacy violations and data misuse becomes a significant issue if data is transmitted to external servers. To mitigate these risks, processing data directly on an edge device-such as a smart camera-has emerged as a viable solution [1]-[3]. Edge computing ensures that sensitive information remains local, thereby aligning with stringent privacy principles and significantly reducing network latency.\nHowever, the implementation of MOT on edge devices is not without its challenges. Edge devices typically possess limited computational resources, necessitating the development of highly optimized algorithms capable of delivering real-time performance under these constraints. The disparity between the computational requirements of state-of-the-art MOT algorithms and the capa-bilities of edge devices emphasizes a significant obstacle.\nTo address these challenges, we propose a neural network pruning method specifically tailored to compress complex net-works, such as those used in modern MOT systems. This ap-proach optimizes MOT performance by ensuring high accuracy and efficiency within the constraints of limited edge devices, such as NVIDIA's Jetson Orin Nano. By applying our pruning method, we achieve model size reductions of up to 70% while maintaining a high level of accuracy and further improving performance on the Jetson Orin Nano, demonstrating the effectiveness of our approach for edge computing applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Multi-object tracking is a challenging task that involves detecting multiple objects across a sequence of images while preserving their identities over time. The difficulty stems from the need to manage variations in object appearances and diverse motion patterns. For instance, tracking multiple pedestrians in a densely populated scene necessitates distin-guishing between individuals with similar appearances, re-identifying them after occlusions, and accurately handling different motion dynamics such as varying walking speeds and directions. To address these challenges, modern MOT systems extensively utilize deep neural networks, with a particular emphasis on convolutional neural networks (CNNs) [4]. CNNS are highly effective in learning and recognizing complex visual patterns, which are essential for accurate identity embeddings [5] and object detection [6].\nNevertheless, CNN-based models frequently encounter dif-ficulties in achieving real-time performance on off-the-shelf hardware, and even more so when deployed on edge devices [7]. This represents a notable problem, as edge computing addresses many of the issues associated with contemporary MOT systems. By performing data processing locally, edge computing mitigates network latency, which is crucial for real-time applications such as autonomous driving, where delays can have critical safety implications [8]. In smart city applications, where pedestrian tracking is facilitated by smart cameras, edge computing enhances data privacy by reducing the need for extensive data transmission and keeping sensitive information processed locally [1]\u2013[3].\nTo address these efficiency challenges, researchers have employed various strategies, including developing specialized model architectures [9] and integrating more efficient object detectors into existing frameworks [10]. However, these ap-proaches often involve substantial modifications to the model architecture or integration framework.\nIn contrast, our research aims at compressing the network to enhance the efficiency of existing models without necessitating architectural overhauls. We focus on models based on the Joint Detection and Embedding (JDE) framework [9], such as FairMOT [11], known for its balance between accuracy and efficiency. To improve efficiency, we apply structured chan-nel pruning-a compressing technique that reduces memory footprint and computational complexity by removing entire channels from the model's weights. Structured channel pruning stands out among compression techniques due to its ability to deliver universal speedup without the need for specialized hardware or software frameworks [12], [13].\nHowever, implementing structured channel pruning presents significant challenges due to the interdependencies between different layers of the network [14]. For instance, pruning the output channels of a convolutional layer necessitates corresponding adjustments to the input channels of subsequent layers. This issue becomes particularly complex in modern models, such as those featured by JDE, which exhibit intricate and tightly coupled internal structures. FairMOT, as illustrated in Fig. 1, exemplifies these complexities with its intricate architecture."}, {"title": "II. RELATED WORK", "content": "State-of-the-art multi-object tracking methods typically fol-low the tracking-by-detection paradigm [11]. In this approach, objects are first detected in each frame, generating bounding boxes. To track these objects across frames, various association criteria are applied [11]. For instance, location-based criteria might use a metric to assess the spatial overlap between bounding boxes. Motion-based criteria often rely on position estimates provided by the Kalman Filter [18]. The criteria then involve calculating distances or overlaps between detec-tions and estimates. Feature-based criteria might utilize re-identification embeddings to assess similarity between objects using measures like cosine similarity, ensuring consistent object identities across frames.\nRecent research has focused not only on enhancing the accuracy of these tracking-by-detection methods, but also on improving their efficiency. Innovations in object detection have introduced models that are compact by design, such as MobileNet and Tiny-YOLO [10], [19], [20], which offer rapid inference times while maintaining robust performance. In addition to these inherently compact models, channel pruning techniques have been employed to enhance efficiency by selectively removing less important channels from pre-trained models [21], [22]. These advancements are complemented by improvements in the tracking pipeline itself. For instance, parallel Kalman Filters enable concurrent computations to accelerate tracking [23], while knowledge distillation tech-niques streamline Re-ID by creating more efficient models [5]. Furthermore, frameworks like JDE integrate detection and embedding tasks into a unified model, optimizing both speed and accuracy by eliminating the need for separate processing stages.\nIn our research, we focus on pruning JDE-based models like FairMOT, enabling the simultaneous pruning of both the re-identification network and the object detector."}, {"title": "III. PRUNING METHODOLOGY", "content": "In this section, we describe our proposed reconstruction-based pruning method. We begin with the definition of reconstruction-based pruning, followed by our concept and utilization of gated groups. Finally, we showcase details of the iterative and global aspects of the pipeline."}, {"title": "A. Reconstruction-based Pruning", "content": "Reconstruction-based pruning involves removing specific input channels from a layer, while aiming to keep the output as close as possible to the original.\nConsider the 2D output feature map produced by filter f across the set of all input channels C in a convolutional layer. We can approximate the output $O_f$ of the filter using a subset $C' \\subseteq C$, i. e.\n$O_f \\approx O_{f'}$.\n(1)"}, {"title": "B. Gated Groups", "content": "Pruning targets are often grouped due to their interde-pendencies, necessitating a reconsideration of layer-wise re-construction criteria. A natural approach is to aggregate the reconstruction error across all layers within a group. However, this approach has two significant drawbacks: first, it requires forward passes through each layer, which is computationally expensive; and second, it must address edge cases such as intradependencies in parameterized layers. For instance, when pruning a layer's input, one must also prune its output, as seen in depthwise convolutions, where a single convolutional filter is applied for each input channel [24].\nTo mitigate these issues, we propose a straightforward yet effective method. Instead of aggregating across all layers within a group, we focus on layers that act as gates, repre-senting the endpoints of information flow within the group's computational graph (Fig. 2). By aggregating the reconstruc-tion error over these gate layers, such as computing the mean reconstruction error, we effectively measure the impact of pruning the entire group. In other words, we aggregate the reconstruction error over the gate set, which comprises the subset of all layers within a group that have no descendants in the computational graph within the given group. To further streamline this process, we automated the identification of gate layers by incorporating the computational graph of the network within DepGraph.\nThis approach not only simplifies the importance calculation by reducing the number of layers to consider but also elimi-nates the need to handle edge cases, as the output channels of gate layers cannot be pruning targets within their group."}, {"title": "C. Iterative Pruning", "content": "In our approach, we employ a global iterative pruning pipeline to systematically reduce the number of parameters in the model. In this context, global pruning involves evaluating all groups within the network collectively at each pruning step, rather than assessing them independently. This method ensures a more holistic reduction in parameters by considering the entire network as a whole.\nThe process follows a pre-defined pruning ratio, with a constant number of steps where, at each step, the number of parameters is linearly reduced. Each pruning step is followed by a training phase, managing the trade-off between model accuracy and sparsity."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": null}, {"title": "A. Implementation Details", "content": "We evaluate our methods using the MOT20 [25] dataset. To ensure consistent comparisons, we train our own FairMOT baseline model on top of the original pretrained FairMOT, which was initially trained on the CrowdHuman [26] and MIX [9] datasets.\nOur training process involves an 80-20 split of the MOT20 and MOT17 [27] datasets, stratified by re-identification labels. The 20 percent split is reserved for performance monitoring and early stopping to prevent overfitting. We apply data augmentation techniques, including random rotation, scaling, cropping, flipping, and color jittering, to enhance the diversity of the training data. The baseline model is trained with an input resolution of 1088 \u00d7 608, a batch size of 32, and a learning rate of $10^{-4}$, with training concluding at epoch 22.\nDuring iterative pruning, we remove one percent of the model's parameters per step and retrain for one epoch only if the validation loss increases relative to the previous step."}, {"title": "B. Metrics", "content": "We evaluate tracking performance using the CLEAR metrics [28], including MOTA, FP, FN, and IDs, along with IDF1 [29] and HOTA [30]. MOTA emphasizes detection accuracy, driven by FP, FN, and IDs, while IDF1 focuses on identity preservation, highlighting association performance. HOTA ex-plicitly balances the effect of performing accurate detection, association and localization into one metric."}, {"title": "C. Benchmark Evaluation", "content": "Both our baseline and pruned models are evaluated on the MOT challenge server under the private detector protocol. As shown in Table I, our baseline model outperforms the orig-inal FairMOT on several metrics. Moreover, we successfully pruned 70% of the model's parameters while maintaining a high level of accuracy."}, {"title": "V. CONCLUSION", "content": "In this paper, we introduced a reconstruction-based pruning method designed for the acceleration and compression of complex MOT models. Our method demonstrates promising results in effectively compressing intricate model architectures.\nFuture work will focus on enhancing our pruning technique and applying it to other complex architectures."}]}