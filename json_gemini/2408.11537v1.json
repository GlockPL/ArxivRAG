{"title": "A Survey of Embodied Learning for Object-Centric Robotic Manipulation", "authors": ["Ying Zheng", "Lei Yao", "Yuejiao Su", "Yi Zhang", "Yi Wang", "Sicheng Zhao", "Yiyi Zhang", "Lap-Pui Chau"], "abstract": "Embodied learning for object-centric robotic manip-ulation is a rapidly developing and challenging area in embodied AI. It is crucial for advancing next-generation intelligent robots and has garnered significant interest recently. Unlike data-driven machine learning methods, embodied learning focuses on robot learning through physical interaction with the environment and perceptual feedback, making it especially suitable for robotic manipulation. In this paper, we provide a comprehensive survey of the latest advancements in this field and categorize the existing work into three main branches: 1) Embodied perceptual learning, which aims to predict object pose and affordance through various data representations; 2) Embodied policy learning, which focuses on generating optimal robotic decisions using methods such as reinforcement learning and imitation learning; 3) Embodied task-oriented learning, designed to optimize the robot's performance based on the characteristics of different tasks in object grasping and manipulation. In addition, we offer an overview and discussion of public datasets, evaluation metrics, representative applications, current challenges, and potential future research directions. A project associated with this survey has been established at https://github.com/RayYoh/OCRM_survey.", "sections": [{"title": "I. INTRODUCTION", "content": "URING the previous decade, remarkable progress has been made in machine learning research centered on deep learning, revolutionizing various fields such as computer vision [1], [2] and natural language processing [3], [4]. Traditional machine learning methods rely on training models using pre-constructed datasets for pattern recognition and prediction. However, these datasets are primarily derived from static sources like images, videos, and texts, which may limit their applicability and effectiveness.\nEmbodied learning, serving as the cornerstone of embodied AI, stands in stark contrast to traditional machine learning. It emphasizes knowledge acquisition through physical inter-actions and practical experiences [5], [6]. The data sources encompass a broad spectrum, including sensory inputs, bodily actions, and immediate environmental feedback. This learning mechanism is highly dynamic, continuously refining behaviors and manipulation strategies through real-time interactions and feedback loops. Embodied learning is essential in robotics as it equips robots with enhanced environmental adaptability, enabling them to handle changing conditions and undertake more intricate and complex tasks.\nWhile a plethora of embodied learning methods have been proposed, this survey primarily focuses on the task of object-centric robotic manipulation. The inputs for this task are data collected from sensors, and the outputs are operational strategies and control signals for the robot to perform manipulation tasks. The objective is to enable the robot to efficiently and autonomously perform various object-centric manipulation tasks while enhancing its generality and flexibility across different environments and tasks. This task is highly challenging due to the diversity of objects and manipulation tasks, the complexity and uncertainty of the environment, and challenges such as noise, occlusion, and real-time constraints in real-world applications.\nFig. 1 (a) illustrates a typical robotic manipulation system. It features a robotic arm equipped with sensors like cameras and end-effectors such as grippers, enabling it to manipulate a wide range of objects. The system's intelligence revolves around three key aspects, corresponding to the three types of embodied learning methods depicted in Fig. 1 (b). 1) Advanced perception capabilities, which involve utilizing data captured by different sensors to understand the target object and external environment; 2) Precise policy generation, which entails analyzing the perceived information to make optimal decisions; 3) Task-orientation, which ensures the system can adapt to specific tasks by optimizing the execution process for maximum effectiveness.\nIn recent years, extensive research has been conducted around those above three key aspects, particularly with the flourishing of Large Language Models (LLMs) [7], Neural Radiance Fields (NeRFs) [8], Diffusion Models [9], and 3D Gaussian Splatting [10], leading to a host of innovative solutions. However, there is a notable absence of a comprehensive survey that encapsulates the latest research in this rapidly evolving field. This motivates us to write this survey to systematically recap the cutting-edge advancements and sum-marize the encountered challenges, along with the prospective research directions."}, {"title": "A. Comparison with Recent Surveys", "content": "Over the past few years, many survey articles have emerged on embodied AI and robot learning, addressing various domains like navigation [11], planning [12], grasping [13], and manipulation [14]. The survey paper by Cong et al. (2021) [15] is the most closely related work to ours. Their focus is on 3D vision-based robotic manipulation and primarily reviews research on 3D visual perception up to 2021. In contrast, our work is not limited to 3D visual perception methods; we also systematically summarize and categorize representation methods based on images, 3D-aware techniques, and tactile sensing. Additionally, we provide a comprehensive introduction to critical aspects of robotic manipulation, such as policy and task-oriented learning. Notably, our survey covers a wide range of the latest research achievements published after 2021, offering a more cutting-edge and comprehensive perspective."}, {"title": "B. Text Organization", "content": "This paper presents a comprehensive survey of embodied learning methods for object-centric robotic manipulation, encompassing three main domains and seven sub-directions. The three domains are embodied perceptual learning (Sec. II), embodied policy learning (Sec. III), and embodied task-oriented learning (Sec. IV). The seven sub-directions include data representation (Sec. II-A), object pose estimation (Sec. II-B), affordance learning (Sec. II-C), policy representation (Sec. III-A), policy learning (Sec. III-B), object grasping (Sec. IV-A), and object manipulation (Sec. IV-B). We also extensively cover the commonly used datasets and evaluation metrics (Sec. V), along with several representative applications within this field (Sec. VI). Additionally, we delve into the primary challenges and provide insights into potential future research directions (Sec. VII)."}, {"title": "II. EMBODIED PERCEPTUAL LEARNING", "content": "To perform object-centric robotic manipulation, the robot must first learn to perceive the target object and its surrounding environment, which involves data representation, object pose estimation, and affordance learning. In this section, we will provide a comprehensive overview of these works."}, {"title": "A. Data Representation", "content": "In object-centric robotic manipulation, robots utilize various sensors to perceive their surroundings. These encompass visual sensors like RGB and depth cameras, which capture color images and depth maps; LiDARs, which create high-resolution 3D point clouds through distance measurements; and tactile sensors, which detect forces during grasping and pressure distribution on contact surfaces. The data collected by these sensors come in different forms, leading to various representations tailored to specific solutions. Next, we will introduce three primary types of data representation approaches: image-based representation, 3D-aware representation, and tactile-based representation.\n1) Image-Based Representation: This line of work primarily focuses on constructing effective representations solely from RGB images, thereby providing a robust foundation for subsequent tasks in robotic manipulation, such as object pose estimation. Depending on the number of input images and variations in network architecture, existing methods can be broadly categorized into four types: single-image single-branch (SISB) [40], single-image multi-branch (SIMB) [41], multi-image single-branch (MISB) [42], and multi-image multi-branch (MIMB) [43], as illustrated in Fig. 2.\n(a) As depicted in Fig. 2 (a), the SISB methods take a single RGB image as input, with a streamlined network architecture featuring a single main pathway. It conventionally employs deep learning models like CNNs to extract deep features from the source image, which are then fed into a pose estimator to generate the essential object pose information for robotic manipulation. SISB incorporates a typical approach to deep feature representation within an end-to-end network frame-work. Despite its speed and simplicity, the SISB's limitation in expressing objects' 3D geometric information may result in subsequently coarser object pose estimation.\n(b) To overcome the limitations of SISB, SIMB methods introduce extra network branches alongside the main pathway, as shown in Fig. 2 (b). These additional branches are designed to capture richer auxiliary information. For instance, MonoGrasp-Net [41] combines a keypoint network and a normal network to produce keypoint heatmaps and normal maps, respectively. It provides a more robust intermediate representation, improving pose estimation accuracy. However, this method relies heavily on the prediction accuracy of the additional branches. Due to the inherent limitations of making predictions based on a single image, errors are inevitably introduced in the generated intermediate representations. These errors can amplify the adverse effect on subsequent processing steps and increase uncertainty in robotic manipulation tasks.\n(c) Owing to the lack of scale information in a single image, accurately estimating the 3D geometric information of objects is quite challenging. Therefore, a lot of research has focused on exploring methods that use multiple images to address this constraint. Among these approaches, the MISB framework has received significant attention. As shown in Fig. 2 (c), this framework aims to use multiple images for 3D reconstruction to recover depth information of the scene [44], [45], which in turn facilitates the generation of efficient 3D representations. Specifically, the depth recovery can be achieved through advanced techniques such as NeRFs [8] or Gaussian Splatting [10].\n(d) Unlike MISB, MIMB aims to directly generate multi-view image representations from images captured by a robot at multiple positions, bypassing the phase of 3D reconstruction. As illustrated in Fig. 2 (d), the MIMB methods incorporate additional predictors to acquire extra information, compensating for the lack of 3D information and enhancing the robot's scene perception. For example, RGBManip [43] introduces a multi-view active learning method and utilizes the segmentation maps produced by the SAM model [46] to provide enhanced representations for the multi-view pose estimator.\n2) 3D-Aware Representation: This section explores 3D-aware representation, which usually takes RGB-D images as input. Existing methods fall into three categories based on the representations they generate: depth-based representation (DR), point cloud-based representation (PR), and transition-based representation (TR), as shown in Fig. 3.\n(a) The DR methods usually employ a network to extract 2D features from RGB-D images simultaneously, as illustrated in Fig. 3 (a). Some use these extracted features directly for subsequent tasks [47], [48], which typically necessitate posterior refinement. For example, Lenz et al. [48] introduced a two-stage cascade network architecture, where the first network efficiently filters out numerous unlikely grasps generated upon extracted features, and the second network concentrates on evaluating the detections from the first network. Another line of studies [49], [50] utilizes a two-stream network to inde-pendently extract 2D features from RGB and depth images. Subsequently, these features are combined or fused to generate the final feature Fa for downstream tasks.\n(b) Instead of directly extracting features on RGB-D images, PR methods first create point clouds through pre-processing, as depicted in Fig. 3 (b). Previous approaches for processing point clouds converted from RGB-D images [51] often involve voxelizing the point clouds and utilizing 3D convolutional neural networks to extract features. However, such approaches are inefficient in terms of memory usage. The introduction of PointNet [52], a network architecture designed explicitly for point clouds, has revolutionized the field. Many methods [53], [54] now prefer to leverage PointNet-like frameworks that enable direct feature extraction from individual points in the point cloud, followed by task-specific modules customized for different objectives.\n(c) Fig. 3 (c) presents the framework of TR works [55], [56] that focus on improving the model's understanding of 3D geometry by translating the input RGB-D data into 3D representations such as occupancy fields, NeRFs, or 3D Gaussians. For example, Ref. [57] involves converting RGB-D data into a voxel representation, using a voxel encoder to create a 3D feature volume. This volume is then employed to construct a neural radiance field to model the 3D space and predict robot actions. Refs. [58], [59] projects RGB-D data into dense point clouds or voxelized point clouds, which are the foundation for placing 3D Gaussians within the scene and enhancing support for robotic manipulation tasks.\n3) Tactile-Based Representation: Tactile sensing acquires crucial force and positional information, allowing the robot to perceive contact with objects and subtle surface changes sensitively. This information is vital for enhancing the robot's capacity to perform complicated tasks and improving its operational accuracy and adaptability.\nThe field of tactile sensing technologies is diverse, with examples such as Gelsight [60], DIGIT [61], and AllSight [62]. These sensors can capture various tactile information such as contact positions, normal forces, tangential forces, and torques. The representation methods for this data also vary. One common representation is time sequences obtained through multiple samplings of tactile feedback within a specific time window [63], [64]. These sequences can be converted into feature vectors using neural networks like LSTM [65], which simplifies the processing in subsequent models. Another form of representation is the tactile image [66], [67], which presents tactile information visually in an intuitive format similar to a standard RGB image and can be directly processed using CNN for feature extraction. Additionally, tactile data can be integrated with other modalities, such as vision and audio, to create a multimodal representation [68], [69], providing a comprehensive understanding of the environment and objects. Furthermore, creating high-quality tactile representations often requires extensive training data. However, gathering tactile data is more time-consuming than visual data. To overcome this challenge, researchers have proposed leveraging technologies like NeRF or GANs to generate tactile data [70], [71] or building simulation environments to imitate tactile experiences [72], [73]. With the continuous development of these techniques, we anticipate that tactile-based representations will play an even more significant role in robotic manipulation.\n4) Discussion: Image-based representation minimizes sen-sor requirements but is limited by relying solely on RGB image information. 3D-aware representation leverages both image and depth data to provide a more robust representation for learning tasks. Tactile-based representation serves as a sup-plementary method, further enhancing the robot's perception abilities. Future research should focus on combining these methods to fully exploit their respective strengths."}, {"title": "B. Object Pose Estimation", "content": "Grasp detection, an essential component of robotic ma-nipulation, relies on accurate object pose estimation as a crucial step [74]. The precision of pose estimation significantly affects the robot's ability to successfully grasp target objects, emphasizing the need to develop robust and efficient pose estimation algorithms. Based on the type of predicted output, there are two main categories of object pose estimation meth-ods: 2D planar pose estimation [75] and 6D pose estimation in 3D space [76], [77]. The former predicts the object's position in the 2D plane and a 1D rotation angle, primarily employed for manipulating objects within a 2D plane. An example application for this method is product sorting in industrial assembly lines, where robotic arm grippers are typically positioned above the sorting platform and utilize a vertical downward angle to grasp target objects. The latter predicts the object's 6DoF (Degrees of Freedom), including 3D rotation and 3D translation, which can fully describe the object's position and orientation in 3D space. Compared to 2D planar pose estimation, 6D pose estimation has a broader range of applications, allowing robotic arms to manipulate objects from any angle.\nMost existing work focuses on the 6D object pose estima-tion, which can be divided into three categories: instance-level, category-level, and novel object pose estimation.\n1) Instance-Level Object Pose Estimation (ILOPE): It refers to estimating the pose of a specific instance of an object, such as a particular cup. Existing methods typically require detailed prior knowledge of the object's shape and appearance, which a textured CAD model can furnish. Since these methods conduct training on specific samples of target objects, the trained models are object-specific.\nThe ILOPE problem can be formulated as Eq.1: Given a set of $N_o$ objects $O = \\{o_i | i = 1, 2, ..., N_o\\}$, along with their corresponding 3D models $M = \\{m_i | i = 1, 2, ..., N_o\\}$, the objective is to learn a model $\\Phi$ to estimate the transformation matrix T for each object instance S that is present in a given RGB or RGB-D image I. This transformation T consists of a 3D rotation $R \\in SO(3)$ and a translation component $t \\in R^3$, which can map the target S to the camera coordinate system. \n$T \\leftarrow \\Phi(I | O, M)$.\nSignificant research has been conducted to estimate the pose of objects at the instance level. Some methods utilize deep neural networks to directly regress the 6D pose of objects, such as PoseCNN [78] and CDPN [79]. However, these methods may still require post-processing optimization [80], [81] to achieve better prediction results, as they are relatively simple. Another class of methods involves learning 2D-3D or 3D-3D correspondences using keypoints [82] and then employing a RANSAC-based PnP (Perspective-n-Point) algorithm [83], [84] to generate pose estimation results. Furthermore, template matching [85] or feature point voting [86] are promising approaches for 6D object pose estimation.\nThe above methods have the advantage of yielding highly accurate pose estimation results. However, they require train-ing for each instance, which makes them unsuitable for handling large-scale and diverse sets of objects.\n2) Category-Level Object Pose Estimation (CLOPE): It involves estimating the pose of objects belonging to predefined categories, such as cups. Existing methods for this task gen-erally do not rely on training on specific instances of objects. Instead, they perform pose estimation using certain features within or across object classes. These methods do not require a 3D model for each instance, which is particularly beneficial when the exact shape and appearance of the objects are not known in advance.\nFormally, the CLOPE problem can be stated as Eq.2: Given a set of $N_c$ object categories $C = \\{c_i | i = 1, 2, ..., N_c\\}$ and a set of objects O belonging to different categories, the goal is to learn a model $\\Phi$ to estimate the transformation matrix T for each object instance S that appears in the observed RGB or RGB-D image I and belongs to category $c_k$. In this case, the 3D model of each object is not available. \n$T \\leftarrow \\Phi(I | O, C)$.\nTo estimate object pose at the category level, Wang et al. [87] introduced NOCS (Normalized Object Coordinate Space), a coordinate system based on the object category. NOCS encodes the pose and size of the object as a normal-ized coordinate vector, and then the correspondence between observed pixels and NOCS can be directly inferred with a neural network. Chen et al. [88] utilized the structured prior of the object category to guide pose adaptation and employed a transformer-based network to model the global structural similarity between the object instance and the prior. These methods are mainly suitable for the pose estimation of rigid objects [89], [90]. However, they are not effectively generalized for articulated objects due to the complexity of articulated object poses, which involve not only translation and rotation but also various joint movements. To address category-level articulation pose estimation (CAPE), Li et al. [91] expanded upon NOCS and introduced ANCSH (Articulation-aware Normalized Coordinate Space Hierarchy), a category-level representation method tailored for articulated objects. Additionally, Liu et al. [92] proposed a real-world task set-ting called CAPER (CAPE-Real), which can handle multiple instances and diverse kinematic structures.\nThe aforementioned methods all estimate an object's pose under the assumption that the object category is known. They typically train models using datasets of known object categories and then perform pose estimation on new instances of the object. These methods enable generalization within the predefined object categories, but they are not capable of handling new object categories.\n3) Novel Object Pose Estimation (NOPE): It has emerged as a highly active research area in recent years to estimate the pose of novel objects from previously unseen categories during training. In this case, instance-level 3D models and category-level prior information are unavailable, but we can take reference images of the target object as an aid. This problem can be formalized as Eq.3: Given one or multiple test images I along with several reference images $I_r$ associated with the target object, the objective is to learn a model $\\Phi$ to estimate the transformation matrix T within the test images by leveraging the visual information from the reference images. \n$T \\leftarrow \\Phi(I | I_r)$.\nIn this field, classic methods usually employ image match-ing [93], [94] or feature matching [95], [96] techniques and subsequently perform pose estimation on new object instances. For example, Liu et al. [93] developed Gen6D, a novel 6D pose estimation method that integrates an object detector, viewpoint selector and pose refiner, enabling the inference of the 6D pose of unseen objects without relying on 3D models. Goodwin et al. [96] proposed a method based on a self-supervised vision transformer and semantic correspondence to achieve zero-shot object pose estimation.\nRecently, the research community has been increasingly focused on utilizing large models to enhance the generaliza-tion capability of deep models for the NOPE task. Lin et al. [97] introduced the SAM-6D approach, which employs the powerful semantic segmentation capabilities of Segment Anything Model (SAM) [46] to generate potential object proposals. Simultaneously, Wen et al. [98] investigated meth-ods to integrate LLMs with contrastive learning, significantly improving model generalization by training on large-scale synthetic datasets. The primary advantage of these methods is that they can handle new object categories, thereby enhancing their generalizability and applicability in a broader range of real-world scenarios. However, it should be noted that large models usually require more training data and computational resources, which could be a potential limitation.\n4) Discussion: These three types of pose estimation meth-ods each have specific application scenarios and advantages and disadvantages: ILOPE offers high accuracy but is only suitable for known objects; CLOPE has a wide range of appli-"}, {"title": "C. Affordance Learning", "content": "Once the estimated object pose is obtained, the next step involves identifying potential interactive regions of the object as shown in Fig. 4, a process known as affordance learn-ing [99]. As a crucial component of robotic manipulation, affordance learning enables robots to comprehend the object's functionality and potential actions. Based on the data source, affordance learning can be categorized into two types: affor-dance learning by supervised learning and affordance learning from interaction.\n1) Affordance Learning by Supervised Learning: In or-der to make robots understand object manipulation, various methods have been proposed that utilize static data to learn affordances [101], [102]. For example, AffordanceNet [102] considered human-annotated RGB images from public datasets as input and simultaneously performed object localization and affordance prediction through two distinct branches. Specifi-cally, this method assigned a probable affordance label to each pixel within the predicted object, effectively making it a part of a semantic segmentation task. Additionally, Nagarajan et al. [103] utilized interaction hotspot maps to depict object affordances and trained their model on large-scale human-object interaction video datasets.\nWhile the aforementioned methods have shown promising results on static datasets, they have not explored applying learned affordances to robotic manipulation tasks. To bridge this gap, VRB [104] incorporated a trajectory prediction model to extract affordances from egocentric videos and integrated the resulting model into various robot learning frameworks. To improve generalization to unseen objects, Robo-ABC [105] emphasized semantic correspondence and has successfully implemented its model on real-world platforms for grasping novel objects. Additionally, RAM [106] developed a retrieval-based architecture that lifts 2D affordances to 3D, enabling embodiment-agnostic robotic manipulation. This framework introduced a hierarchical retrieval pipeline to transfer action-able knowledge from out-of-domain data to specific target domains. To overcome the constraints of closed-set affor-dance learning, OpenAD [107] measured the similarity be-tween language-based affordance labels and point-wise high-dimensional features and extended affordance learning to an open-vocabulary context.\n2) Affordance Learning From Interaction: Rather than re-lying on supervised learning from static data, affordance learning from interaction framework seeks to gather training data through simulations. This method allows the system to learn from interactions, providing it with essential prior knowledge for real-world deployment. As a pioneer in this field, Where2Act [108] employed self-supervised interaction for articulated 3D objects, which uses single-frame images or partial point clouds as observations in the SAPIEN [109] simulator. But AdaAfford [110] identified that this paradigm ignores hidden kinematic uncertainties that lead to inaccurate affordances. To address this, AdaAfford proposed a method that involves sampling multiple test-time interactions to facili-tate rapid adaptation. Building on similar concepts, DualAfford [111] expanded the interactive learning framework to dual-gripper manipulation to broaden the robot's manipulation capabilities. Nevertheless, relying on random interactions for data collection makes these methods sample-inefficient. Ac-tAIM [112] tackled this with a clustering-based strategy and a generative model to improve interaction diversity and data quality. Additionally, IDA [113] put forward an information-driven method for affordance discovery to boost interaction efficiency. Where2Explore [114] generalized affordance recog-nition to novel instances and even various object categories by leveraging local geometries for actionable parts.\nIt is important to note that all the aforementioned methods operate under the assumption of noiseless visual information, which is often unrealistic. In response, Ling et al. [115] introduced a coarse-to-fine architecture to reduce point cloud noise and improve the affordance learning performance. Beyond focusing solely on single-object affordances, Cheng et al. [116] incorporated realistic physical constraints within en-vironments and employed a data-efficient contrastive learning method to acquire environment-aware affordances, even under occlusions. RLAfford [117], in contrast to prior work limited by predefined affordance primitives, integrated reinforcement learning to facilitate end-to-end affordance learning. Specifically, they considered contact maps of interest during the RL process as visual affordances and seamlessly adapted the architecture to various manipulation tasks.\n3) Discussion: Current supervised affordance learning methods are limited by their focus on specific domain data or tasks, while interaction-based approaches are constrained by sample inefficiency. Future research should investigate how to design effective frameworks that can harness the vast potential of internet-scale data and rapidly adapt to specific tasks."}, {"title": "III. EMBODIED POLICY LEARNING", "content": "Embodied policy learning aims to empower robots with the sophisticated decision-making capabilities required to perform manipulation tasks efficiently. This section will delineate the process of embodied policy learning into two fundamental phases: policy representation and policy learning, elucidating how these techniques enable robots to accomplish predefined objectives. We summarize the key works in embodied policy learning in Table II."}, {"title": "A. Policy Representation", "content": "The role of policy is to model the robot's behavior by taking its observation as input and determining the corresponding action to execute. This process is mathematically represented as $ \\pi: O \\leftrightarrow A $, where O and A represent the observation space and action space, respectively. Policy representation is critical in embodied policy learning, as it significantly affects the robot's decision-making ability. Depending on the modeling options, policy representation is classified into explicit, im-plicit, and diffusion policies, regardless of whether the action space is discrete or continuous.\n1) Explicit Policy: Explicit policies utilize a parameterized function to map a robot's current observation $v \\in O$ directly to an action $a \\in A$. Typically, explicit policies are parameterized using feed-forward models like neural networks and can be either deterministic [118] or stochastic [119]. A deterministic policy directly predicts an action $a = \\pi_\\theta(v)$ to execute, while a stochastic policy samples actions from an estimated distribution $a \\sim \\pi_\\theta(\\cdot|v)$, where $\\theta$ indicates parameters of the policy. Stochastic policies enhance an agent's exploration ca-pabilities and provide greater robustness in complex, uncertain environments compared to deterministic policies.\nIn a discrete action space, policy representation can be transformed into an optimal action selection process from a finite set of actions. The categorical distribution is commonly used to calculate action probabilities, from which actions are sampled based on the estimated results. For instance, Zhang et al. [161] conceptualized the robot assembly manipulation policy as translation, rotation, and insertion primitives, with RL subsequently optimizing the policy. In continuous action spaces, a diagonal Gaussian distribution is often chosen to represent the action distribution, guided by regression losses such as mean squared error (MSE) or RL-based objectives. The policy outputs both the mean $\\mu_\\theta(v)$ and the standard deviation $\\sigma_\\theta(v)$, and actions are sampled from the resulting distribution as follows:\n$a = \\mu_\\theta(v) + \\sigma_\\theta(v) \\odot \\xi$.\nHere, $\\xi \\sim N(0, I)$ represents a vector of Gaussian noise, and $\\odot$ signifies the Hadamard product. It should be noted that, in practical applications, the logarithm of the standard deviation $\\log \\sigma_\\theta(v)$ is typically used to prevent the standard deviation from taking on negative values.\n2) Implicit Policy: Unlike explicit policy models, implicit policies attempt to assign value to each action by leveraging energy-based models (EBMs) [120], [121], which are also recognized as action-value functions [122]. This paradigm learns the policy by optimizing a continuous function to find the action with minimal energy:\n$\\hat{a} = arg \\min_{a \\in A} E_\\theta(v, a)$,\nwhere $\\theta$ denotes the parameters of the energy function $E_\\theta$. Consequently, the problem of action prediction is effectively reformulated as an optimization problem.\nGenerally, given a series of expert demonstrations or online-collected trajectories denoted as $\\{(v_t, a_t)\\}_{t=0}^T$, implicit poli-cies are trained by an InfoNCE-style loss [162]. Once trained, stochastic optimization will be applied to identify the optimal action for implicit inference. EBIL [123] incorporated EBMs into the inverse RL architecture, utilizing the estimated expert energy as a surrogate reward. Florence et al. [121] further proposed an implicit behavioral cloning approach grounded in this framework and assessed its performance across various robotic task domains such as simulated pushing and bi-manual sweeping.\n3) Diffusion Policy: Drawing inspiration from Denoising Diffusion Probabilities Models (DDPMs) [9], which gradually denoise random inputs to generate data samples, diffusion poli-cies model the policy as a conditional generative model [126]. This approach approximates the action distribution $\\pi(\\cdot|v)$, considering the observation v as a condition for producing the corresponding action a:\n$a^{k-1} = \\alpha(a^k - \\beta \\epsilon_\\theta(a^k, v, k)) + \\sigma N(0, I)$,\nwhere k = 1,2,..., K, representing the denosing iterations, $\\alpha, \\beta, \\sigma$ are functions rely on the noise schedule, $\\epsilon_\\theta$ denotes the denosing network with parameters $\\theta$, and N(0,I) represents the standard Gaussian noise.\nAs concurrent work, Decision Diffuser [125] and Diffusion-QL [126] have pioneered the integration of diffusion policies into offline RL. These studies revealed that this approach yields highly expressive policy representation that surpasses traditional policy formats. While Decision Diffuser [125] sug-gested extending diffusion policies to handle high-dimensional visual observations, its current focus remains on state-based benchmarks. In contrast, Chi et al. [124] proposed a novel diffusion policy tailored for vision-based robotic manipulation tasks. Their experimental findings highlighted the efficacy of diffusion policies in visuomotor policies and their superiority in managing behavioral multimodality in imitation learning. They also incorporated techniques such as receding horizon control and time-series diffusion transformers to adapt the policy for high-dimensional action spaces, resulting in more stable training. HDP [127] integrated diffusion policies into a high-level planning agent for multi-task robotic manipulation, whereas UniDexFPM [128] applied diffusion policies to pre-grasp manipulation. By leveraging the conditional generative paradigm, diffusion policies are well-suited for multimodal policy learning. For example, MDT [130] and Lan-o3dp [131] advanced multimodal policy learning by incorporating lan-guage instructions. Differently, BESO [129] facilitated rapid inference in diffusion policies by decoupling the score model learning from the sampling process.\n4) Discussion: Explicit policies are straightforward to im-plement but struggle with complex tasks, while implicit poli-cies face challenges in training stability and computational costs. Diffusion policies offer a promising alternative to pro-vide a more expressive and robust policy representation, but how to accelerate the sampling process remains to be explored."}, {"title": "B. Policy Learning", "content": "After establishing a suitable policy representation", "163": [164], "165": [166], "156": [157], "Learning": "By modeling the policy learn-ing procedure as a Markov Decision Process (MDP)", "as": "n$J(\\pi) = E_{\\tau \\sim \\pi"}, ["sum_{t=0}^T \\gamma^t r_t(v_t, a_t)"], "nwhere $\\tau = \\{(v_t, a_t)\\}_{t=0}^T$ denotes a trajectory, with $v_t$ and $a_t$ representing the observation and action at time step t, respectively. The function $r_t$ corresponds to the reward provided as feedback from the environment after each action is taken. Here, $\\gamma \\in [0,1"], "as": "n$\\pi^* = arg \\max_{\\pi} J(\\pi)$.\nAs a pivotal element for decision-making, RL has been extensively investigated in robotic manipulation. Researchers from OpenAI [167] developed a sim-to-real training pipeline to enable a physical five-fingered robot hand to perform vision-based object reorientation. This pipeline initially trained the policy in simulation using Proximal Policy Optimiza-tion (PPO) [168] and then adapted it to physical hardware through domain randomization. It should be highlighted that PPO is a widely used on-policy RL algorithm in robotic manipulation, valued for its simplicity and effectiveness. For long-horizon surgical robot tasks, ViSkill [132] introduced a novel mechanism named value-informed skill chaining to learn smooth subtask policies. To create generalizable ma-nipulation policies adaptable to various object shapes, RMA2 [119] presented a two-stage training framework with an extra adapter training phase within PPO, enhancing the policy's robustness across diverse objects. Inspired by model-based RL [169], SAM-RL [133] proposed a sensing-aware architecture that renders images from different viewpoints and refines the learned world model by aligning these generated images with actual raw observations, demonstrating significant real-world performance. Mandlekar et al. [134] explored the impact of various design choices in offline RL [135] and made their dataset publicly available for further research. To overcome exploration challenges in RL, Huang et al. [136] proposed demonstration-guided RL, which"}