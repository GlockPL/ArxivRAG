{"title": "RIEMANNIAN GEOMETRY-BASED EEG APPROACHES: A\nLITERATURE REVIEW", "authors": ["Imad Eddine Tibermacine", "Samuele Russo", "Ahmed Tibermacine", "Abdelaziz Rabehi", "Bachir Nail", "Kamel Kadri", "Christian Napoli"], "abstract": "The application of Riemannian geometry in the decoding of brain-computer interfaces (BCIs) has\nswiftly garnered attention because of its straightforwardness, precision, and resilience, along with\nits aptitude for transfer learning, which has been demonstrated through significant achievements in\nglobal BCI competitions. This paper presents a comprehensive review of recent advancements in the\nintegration of deep learning with Riemannian geometry to enhance EEG signal decoding in BCIs.\nOur review updates the findings since the last major review in 2017, comparing modern approaches\nthat utilize deep learning to improve the handling of non-Euclidean data structures inherent in\nEEG signals. We discuss how these approaches not only tackle the traditional challenges of noise\nsensitivity, non-stationarity, and lengthy calibration times but also introduce novel classification\nframeworks and signal processing techniques to reduce these limitations significantly. Furthermore,\nwe identify current shortcomings and propose future research directions in manifold learning and\nriemannian-based classification, focusing on practical implementations and theoretical expansions,\nsuch as feature tracking on manifolds, multitask learning, feature extraction, and transfer learning.\nThis review aims to bridge the gap between theoretical research and practical, real-world applications,\nmaking sophisticated mathematical approaches accessible and actionable for BCI enhancements.", "sections": [{"title": "1 Introduction", "content": "Brain-computer interfaces [9] revolutionize the field of human-machine interactions by enabling direct communication\npathways between the brain and external devices, bypassing traditional muscular outputs[1]. Predominantly, BCIs utilize\nelectroencephalography (EEG) due to its cost-effectiveness and minimal computational demands[2]. EEG-based BCIs\nconvert brain activity, measured through EEG signals, into computer commands, leveraging classification algorithms\nlike the common spatial pattern (CSP) to recognize signal patterns associated with specific brain activities such as motor\nimagery[3]. Despite their promise, EEG-based BCIs face critical challenges including high signal variability, noise,\nnon-stationarity, and the high dimensionality of data spaces[4].\nThe domain of BCI has expanded its utility beyond basic research, offering profound applications in rehabilitation,\nassistive technologies, and adaptive human-computer interfaces that respond to the user's mental states[5]. Yet, despite\nthese advancements, BCIs are predominantly still in the prototype stage, rarely deployed outside laboratory settings\ndue to their low robustness and the extensive calibration required for each user[6]. Current BCIs suffer from poor\nreliability, often misinterpreting user commands, which significantly reduces accuracy and information transfer rates\neven under controlled conditions[7]. This sensitivity to environmental noise and the inherent non-stationarity of EEG\nsignals further complicates their practical application[8].\nIn the rapidly evolving field of BCIs, deep learning has emerged as a transformative force, enabling sophisticated\nmethods for classifying EEG data[10]. These advancements hold the promise of revolutionizing the interaction between\nhumans and machines by interpreting neural activities directly through EEG signals[11]. Despite these advances, the\ndevelopment of BCIs faces significant challenges, notably the limited availability of large, annotated training datasets,\nwhich are crucial for training robust deep learning models[12]. In response, researchers have increasingly turned to\ngenerative modeling, a burgeoning area within machine learning, to augment existing datasets through the synthesis of\nsynthetic EEG data[13]. This approach not only enhances the diversity of training examples but also addresses the gap\ncaused by scarce data resources[14].\nAchieving practical usability of BCIs requires that they be robust across different contexts, users, and over time, all\nwhile maintaining minimal calibration requirements[15][24]. Addressing these multifaceted challenges necessitates a\ncomprehensive strategy: identifying new, reliable neurophysiological markers at the neuroscience level[16]; improving\nuser training to stabilize EEG pattern generation at the human interaction level[17]; and advancing signal processing\ntechniques to enhance feature extraction[18] and classifier robustness at the computational level[19][25]. Prominently,\nthe application of Riemannian geometry[20] to handle EEG covariance matrices has shown considerable promise[21].\nThis geometric approach not only accommodates the non-Euclidean characteristics of EEG data but also leverages the\nintrinsic geometric properties of these data to enhance classification accuracy[22][23].\nTraditional approaches like Fourier analysis[26] frequently fail due to the inherently nonstationary nature of EEG\nsignals. This limitation has paved the way for the adoption of time-frequency analysis techniques, such as the wavelet\ntransform, which are now being integrated with spatial filtering[27] methods like the CSP to improve the localizability\nof rhythmic EEG components. Such advancements are critical for enhancing the efficacy of motor imagery (MI) EEG\nclassifiers, which are vital for real-time BCI applications[28].\nMoreover, a significant aspect of reducing signal variability and enhancing classifier performance in BCIs[29] involves\nfocusing on covariance matrices, typically handled as elements within Euclidean spaces. However, considering these\nmatrices in their naturally occurring Riemannian manifold[30], which better reflects their symmetric and positive\ndefinite (SPD) properties[31], can substantially optimize the computational processes. Techniques such as Principal\nComponents Analysis (PCA)[32] and Canonical Correlation Analysis (CCA)[33] have traditionally exploited covariance\nestimates for spatial filtering in BCIs[34]. By refining how these matrices are treated acknowledging their Riemannian\nstructure-we can significantly enhance the spatial and temporal resolution of EEG signal analysis[35].\nThis paper presents a detailed review focused on the applications of Riemannian Geometry and deep learning-based\nRiemannian geometry in the domains of BCIs and EEG. We explore how these advanced methodologies enhance BCI\ndesign by addressing core challenges inherent in traditional systems. Our discussion extends to the integration of\ngeometric deep learning with generative modeling and sophisticated signal processing techniques. We propose new\nresearch directions and outline strategic advancements necessary for developing BCIs that are not only efficacious in\ncontrolled settings but also robust and accessible for practical, everyday applications."}, {"title": "2 Brain-Computer Interface", "content": "Brain-computer interfaces that utilize oscillatory neural activity are a significant branch of non-invasive systems for\nneurocommunication and control[36]. These interfaces typically leverage the modulation of brain rhythms captured via\nEEG, and are commonly employed in motor imagery-based BCIs[37]. Such BCIs extensively use the CSP algorithm\ncombined with Linear Discriminant Analysis for signal classification, making them highly effective for decoding user\nintentions based on neural activity patterns[38].\nThe CSP algorithm is pivotal for maximizing the variance of signals from one mental state while simultaneously\nminimizing it for another[39]. This is achieved by designing spatial filters that enhance the discriminability between\ntwo contrasting classes of brain activity. Mathematically, the optimization of CSP filters, represented as column vectors\nw, is framed as an eigenvalue problem aimed at maximizing the following objective function:\n$J_{CSP}(W) = \\frac{W^TC_1W}{W^TC_2W}$                                                                                                (1)"}, {"title": "3 Riemannian Geometry Concepts", "content": "In this section, we introduce the mathematical notations and basic definitions fundamental to our study on EEG\npreprocessing. Scalars are denoted by lowercase letters (e.g., a), vectors by boldface lowercase (e.g., v), representing\ncolumn vectors unless stated otherwise, matrices by boldface uppercase (e.g., M), and tensors of order three or higher\nby Euler script letters (e.g., T). The space Rn represents the n-dimensional real vector space, and Rn\u00d7n denotes all\nn\u00d7n real matrices. Vector en, comprising all ones, belongs to R\", and In denotes the identity matrix in Rn\u00d7n. For\nany vector x, the matrix diag(x) refers to a diagonal matrix with the elements of x on its main diagonal.\nWe denote by vec(A) the operation that converts a matrix A into a vector by stacking its columns. In the special case\nwhere A is symmetric, the vectorization vec(A) is defined such that it takes only the unique entries due to symmetry.\nThe norms || ||p and || . ||F correspond to the Lp and Frobenius norms of a vector or matrix, respectively.\nDefinition 1.1. A matrix A \u2208 Rn\u00d7n is deemed Symmetric Positive Definite (SPD) if it holds that A = AT and\nxT Ax > 0 for all non-zero vectors x \u2208 Rn. The eigenvalues of such a matrix A, denoted by (A), are guaranteed to\nbe positive.\nDefinition 1.2. A matrix A is considered orthogonal if its columns comprise an orthogonal unit vector set, implying\nATA = In.\nDefinition 1.3. The exponential and logarithmic functions of a matrix A \u2208 Rn\u00d7n, denoted by exp(A) and log(A), are\ndefined through its eigenvalue decomposition. If A is expressible as Udiag(11, . . ., \u03bb\u03b7)UT, where U is orthogonal\nand A\u2081 are the eigenvalues of A, then:\nexp(A) = Udiag(exp(x1), ..., exp(\u03bb\u03b7))UT,                                                              (5)\nlog(A) = Udiag(log(11), ..., log(n))UT.                                                                  (6)\nA manifold, in the simplest terms, refers to a space that, at a local level, resembles the Euclidean space of dimension n,\nknown as Rn. This property of local similarity to Rn qualifies it as a topological manifold [52]. When such a space is\nequipped with a differential structure, it ascends to become a differentiable manifold, thereby enabling the attachment\nof a tangent space at any point [53]. The tangent space encapsulates all potential vectors that are tangential to any\nconceivable curve passing through that point on the manifold.\nWithin the manifold paradigm, a Riemannian manifold stands out as a differentiable manifold that is further enhanced\nwith a smoothly varying inner product on its tangent spaces, constituting the Riemannian metric tensor [54]. This metric\ntensor facilitates the measurement of angles between vectors within the same tangent space, the magnitude of vectors,\nand the distance between vectors. It allows for the calculation of curve lengths on the manifold, including the geodesic\ndistance which is the shortest distance between two points on the manifold determined by the paths of curves [55].\nConsidering M as a Riemannian manifold and Tx M as its tangent space at point X, for any two vectors V, W \u2208 TxM,\nthe Riemannian metric gx (V, W) provides the inner product [56]. When discussing the manifold of SPD matrices, our\nfocus is particularly on the tangent space Tx M and the Riemannian metric at that point.\nFor a smooth curve \u03b3 : [0, 1] \u2192 M, parametrized with respect to a metric on M, there are numerous ways to describe\nits trajectory, each with a different speed of traversal. Of particular interest are those curves that are parametrized by arc\nlength, termed naturally parametrized curves [57]. These curves progress with a uniform rate and are defined in relation\nto the Riemannian metric. The curve's length from \u03b3(0) to \u03b3(1) is computed by integrating the metric-induced velocity\nalong the curve.\nFor two distinct points X1, X2 \u2208 M, and a naturally parametrized curve y that connects these points with y(0) = X1\nand y(1) = X2, the length L(y) of this curve is expressed by the integral[52]:\n$L(Y) = \\int_0^1 \\sqrt{g_{\\gamma(t)}(\\dot{\\gamma}(t), \\dot{\\gamma}(t))} dt$                                                (7)\nThe optimal path on a manifold that minimizes the distance between two points is called a geodesic. While geodesics\nrepresent the paths of least distance, they are uniquely characterized by a uniform rate of traversal and are not necessarily\nthe shortest paths for manifolds that are not simply connected. These paths, particularly on a spherical surface, might\nhave multiple representations, such as the numerous geodesics connecting the poles [55]. The distance along a\ngeodesic, known as the Riemannian distance between two points X1 and X2, is essential in determining the manifold's\ncompleteness. Here, we concentrate on manifolds that are geometrically complete [54].\nThe tangent space at point X on a manifold M, denoted TxM, acts as a local linear approximation to M within a\nspecific region around X, usually within a certain radius allowing for a bijection via the exponential map [58]. For every\npoint X' within this neighborhood, the exponential map is the bridge connecting X' to X through a unique geodesic.\nFor any smooth scalar function f defined on M, the Riemannian gradient at X is given as \u2207 f(X) in the direction that\nmaximizes the rate of increase of f. If y is a geodesic with y(0) = X and y(0) = V, then \u2207 f oy describes the change\nof f along y, and \u2207 f(X) is that vector in TxM such that the inner product (V, V f(X)) equals the derivative of f\nalong the curve at t = 0, defined as[58]:\n$(V, \\nabla f(X)) = \\frac{d}{dt} f(\\gamma(t))|_{t=0}$                                                   (8)\nThe Riemannian gradient facilitates the computation of directional derivatives on manifolds, linking classical differential\ncalculus to the geometry of the manifold. The exponential map, denoted as Expx : TxM \u2192 M, projects a tangent\nvector at X onto M, and conversely, the logarithm map, or Logx, maps points back to Tx M, providing a method to\ntraverse between a tangent space and its manifold [59].\nAssigning a set M of square matrices the structure of a Riemannian manifold incorporates a local Euclidean geometry,\nthereby enriching it with a rigorous mathematical underpinning. Suppose we have M' \u2208 M, a manifold of dimension\nK, and \u00a7\u2122' = M' \u2013 M representing a tangent vector at M. This vector is part of a higher-dimensional tangent space\nTMM associated with M.\nThe inner product defined by the Riemannian metric on TMM \u00d7 TMM \u2192 R induces a norm on the tangent space,\ngiven by ||\u00a7\u043c'||M = \u221a\u3008\u00a7\u043c',\u00a7\u043c')M, and facilitates the computation of geodesic distances d(M, M') on M [59].\nThese distances enable the formulation of mean values of points on the manifold as:\nMean({M1,..., MN}) = arg min \u2211 d(M, M\u2081)2.                                                       (9)\nMEM\ni=1\nThe exponential mapping at M in M, denoted as ExpM, and its inverse, the logarithm mapping Log\u2122, are both essential\nin preserving the manifold's structure, especially when mapping to and from the tangent space. Exp\u043c(\u043c) approximates\nM' when \u00a7\u2122 is small, and the distance between M and M' is given by the norm of \u00a7\u2122 in RK via LogM. This\nrelationship allows the introduction of vectorization for M, PM : M \u2192 RK, defined as PM(M') = $(Log\u043c(M'))\n[59].\nFor a compact subset of M where the matrices {M} lie, the mean M can be approximated, resulting in a simplified\ngeodesic distance expression:\nd(M, M') \u2248 ||P\u043c(\u041c) \u2013 \u0420\u043c(\u041c')||.                                                                     (10)\nIn the realm of regression on manifolds, the vectorization PM is pivotal for leveraging machine learning algorithms. It\nadapts matrix points in M onto RK, facilitating their use in regression techniques that typically assume a Euclidean\ndata structure. For instance, with a collection {Mi} and corresponding response variables {yi}, one first computes the\nmean of the samples M and applies vectorization to obtain {v}. Linear regression methods, such as ridge regression,\ncan then be applied, presupposing a linear relationship y\u2081 \u2248 v\u0142 \u00df, with \u1e9e representing the regression coefficients [60]."}, {"title": "3.1 The Covariance Matrix of EEG", "content": "Consider X \u2208 RM\u00d7T, representing an EEG signal that has undergone band-pass filtering, where M denotes the number\nof channels and T represents the number of temporal samples. To analyze the statistical properties of the EEG signal,\nwe construct the covariance matrix P as follows:\nP = \\frac{1}{T-1} XX^T                                                                                              (11)\nThis matrix P is not only symmetric but also empirically confirmed to be SPD, encapsulating important statistical\ninformation about the EEG signals. The symmetry of P arises because (XXT)T = XXT, and it attains its positive-\ndefiniteness under the condition that X has full row rank, which is typically satisfied if T > M and the EEG data are\nsufficiently diverse [61].\nProperties of Symmetric Positive-Definite Matrices: A matrix A \u2208 Rn\u00d7n is deemed positive-definite if for any\nnon-zero vector v \u2208 Rn, it holds that:"}, {"title": "3.2 Riemannian Metrics on SPD Manifolds", "content": "The choice of Riemannian metric on the manifold of SPD matrices significantly influences the analysis and processing\nof data represented by these matrices, such as EEG signals. The Log-Euclidean Metric (LEM) and the Affine-Invariant\nMetric (AIM) are two predominant metrics used in this context [63, 65].\nLog-Euclidean Metric (LEM): The Log-Euclidean Metric (LEM) offers an efficient and robust computational method\nfor handling the manifold of SPD matrices. This metric is particularly valued for preserving the bi-invariance property\nwithin the Lie group structure of SPD matrices, making it suitable for various applications, including image processing\nand medical imaging analysis, where maintaining the structure of data during transformations is crucial [65].\nUnder the LEM, the geodesic distance between two points P\u2081 and P2 on the manifold of SPD matrices is defined as:\n\u03b4\u03b9 (P1, P2) = || Log(P1) - Log(P2)||F,                                                             (14)\nwhere Log denotes the matrix logarithm, transforming matrices to a space where the Euclidean tools can be applied.\nThe norm || || F represents the Frobenius norm, which measures matrix entries' absolute differences, thus providing a\nnatural distance metric in the logarithmic domain [65].\nThe Log-Euclidean mean of a set of matrices, crucial for statistical analysis on manifolds, is calculated using:\nG = Exp (\u2211 Log(P\u2081))                                                                                     (15)\nThis formulation allows the mean of matrices to be computed efficiently and without the convergence issues that may\narise with other Riemannian metrics. Additionally, when considering a weighted mean where each matrix P\u2081 is assigned\na weight w\u2081, fulfilling the conditions \u2211=1 Wi = 1 and w\u2081 > 0, the weighted mean is given by:\nG = Exp (\u2211 w Log(P\u2081))                                                                              (16)\nAffine-Invariant Metric (AIM): The AIM is another highly regarded metric for SPD manifolds, especially valued for\nits invariant properties under affine transformations. The geodesic distance under this metric between P\u2081 and P2 is\ncomputed as:\n\u03b4A(P1, P2) = ||Log(P-1/2P2P-1/2) || '                                                       (17)"}, {"title": "3.3 Tangent Space at a Point on a Manifold", "content": "The concept of tangent space is central to understanding the local geometry of manifolds and plays a critical role in the\nanalysis of data that lie on these manifolds, such as covariance matrices of EEG signals. A tangent space at a point on a\nmanifold provides a linear approximation of the manifold near that point, facilitating operations like vector addition and\nscalar multiplication which are not inherently defined on the manifold itself [52].\nLet M be a smooth manifold and p a point on M. The tangent space to M at p, denoted as T\u266dM, is a vector space\nconsisting of the tangent vectors to all possible curves through p on the manifold. Formally, if \u03b3 : (\u2212\u20ac,\u20ac) \u2192 Mis\na smooth curve with y(0) = p, then the derivative \u03b3'(0), which represents the velocity vector of y at p, is a tangent\nvector at p [53].\nFor practical computation, particularly in applications involving SPD matrices, we express tangent vectors in coordinates.\nIf M is parameterized locally around p by a coordinate system & : U CR\u2033 \u2192 M, where U is an open set in R\", the\ntangent vectors can be expressed as:\nV = \u2211v\u00b2 \u2202/\u2202Xi | p                                                                                               (18)\nwhere v\u00b2 are components of v in the coordinate basis \u2202/\u2202Xi | p ${\u2202/\u2202xi}$ [52].\nIn the context of SPD matrices, which form an open subset of the space of symmetric matrices, the tangent space at any\npoint P \u2208 SPD(n) can be identified with the space of symmetric matrices. If P is an SPD matrix and S is a symmetric\nmatrix, then a curve on SPD(n) passing through P with direction S at t = 0 can be represented as:\n\u03b3(t) = P1/2 exp(tP-1/2SP-1/2)p1/2                                                     (19)\nwhere exp denotes the matrix exponential. The derivative of this curve at t = 0 gives a tangent vector at P, which is\nprecisely S [63].\nUnderstanding the tangent space of SPD matrices is crucial for developing algorithms in EEG signal processing. For\nexample, the process of projecting EEG covariance matrices onto the tangent space allows for the linearization of the\nmanifold structure, simplifying computations such as averaging or classification. This projection involves mapping\nSPD matrices to their tangent spaces at a chosen reference point, usually the mean covariance matrix, and performing\nlinear operations in this vector space before projecting back to the manifold [69].\""}, {"title": "3.4 Geodesic Distances on Riemannian Manifolds", "content": "An SPD matrix of size M \u00d7 M resides on a manifold denoted as P(M), characterized by its smooth and curved\nstructure. On such Riemannian manifolds, the concept of geodesics plays a crucial role in understanding and quantifying\nthe intrinsic geometric properties of the space [70]."}, {"title": "4 Riemannian Geometry-Based EEG Approaches", "content": "This article presents a synthesis of discoveries from a meticulously curated collection of 42 publications, manually\nchosen based on their pertinence to the fusion of Riemannian geometry and deep learning in brain-computer interfaces\n(BCIs). The selection of articles was made through a focused exploration conducted on Google Scholar and PubMed.\nThe exploration, executed with the objective of encompassing the most recent and pertinent research up to May 3rd,\n2024, utilized a search query formulated as: (\u201cRiemannian geometry\u201d OR \u201cmanifold learning\u201d Or \u201cSPD Manifolds\")\nAND (\"brain computer interface\u201d OR \u201cEEG\u201d). This curation procedure encompassed both peer-reviewed publications\nand preprints, ensuring a thorough incorporation of the newest advancements and dialogues in the domain."}, {"title": "4.1 Feature Extraction Methods", "content": "Feature extraction is a SOTA necessary step after collecting EEG data. It allows reducing the dimensionality of the data\nand making it more comprehended by machine learning and deep learning models. Various studies have explored the\nuse of Riemannian geometry for feature extraction and discriminant analysis in EEG-based BCIs. In [104], the authors\nused Covariance Matrices as EEG Signal Descriptors and exploring various dissimilarity metrics on SPD matrices\nand introducing a novel feature, HORC, which combines different relevance matrices under a tensor framework for\nimproved classification accuracy. A Riemannian Spatial Pattern (RSP) method was proposed in [109] to extract spatial\npatterns from Riemannian classification for motor imagery tasks. The RSP method uses a backward channel selection\nprocedure and compares it with the Common Spatial Pattern (CSP) approach. The RSP method provides precise\nmapping and clustering of imagined motor movements, which is especially useful in differentiating finger flexions. The\nspatial pattern extraction can be described using:\n$C = \\frac{1}{N} \u2211 x_i x_i^T$                                                                                               (23)\nwhere C is the covariance matrix, and xi represents the EEG signals.\nHuang et al. [90] introduce the Common Amplitude-Phase Measurement (CAPM) method, designed to simultaneously\nanalyze the amplitude and phase information of EEG signals on a Riemannian manifold. This dual consideration\npromises to enhance classification accuracy for BCI applications significantly. The initial step involves extracting the\namplitude and phase from EEG signals using the wavelet transform, specifically applying a complex Morlet wavelet.\nThe transformation is mathematically represented as:\n$x_c(t, f) = x(t) * \u03c8(t, f) = a_c(f, t)e^{id_c(f,t)}$                                                          (24)\nwhere (t, f) denotes the complex signal representation at channel c, time t, and frequency f. Here, ac(f, t) is the\namplitude, 0c(f, t) represents the phase, and (t, f) is the Morlet wavelet function. To dimensionally reduce while\npreserving discriminative features, the authors devise a Riemannian graph embedding technique. The adjacency matrix\nS, based on the Riemannian distance between the SPD covariance matrices of EEG trials, is defined by:\n$S_{nr} =  \\begin{cases}  exp(-\\eta r), & \\text{if } Y_n \\in C_z(Y_r) \\text{ and } Y_r \\in C_z(Y_n) \\\\  0, & \\text{otherwise}  \\end{cases}$                                                   (25)\nwhere dnr is the Riemannian distance between covariance matrices Pn and Pr:\n$\u03b4_{\u03b7\u03c2} = \u03b4_\u03c1(P_n, P_r) = ||log(P_n^{-1}P_r)||_F = (\u2211 log^2 \u03b2_\u03b9 )^{1/2}$                                                         (26)\nThe low-dimensional data Yn is used to compute a new covariance matrix Pn = WH P\u201eW. Classification is performed\nusing the Minimum Distance to Riemannian Mean (MDRM):\n$z_n = sign [\u2211 log^2 nm ] [\u2211 log^2 pm ]$                                             (27)\nwhere Nm and pm are the eigenvalues of P\u00af\u00b9Pn 1Pn and P\u22121 Pn respectively. A regularized linear regression model is then\napplied to optimize the classification parameter b:\nmin = ||z - Db||\u00b2 + (1 \u2212 a) ||6||2 + \u03bb\u03b1 ||6||1                                                            (28)\nwhere D is the matrix of input vectors dn and z is the corresponding label vector.\nThe CAPM method effectively captures the intrinsic amplitude and phase information of EEG signals, optimizing the\nspatial-spectral filters and enhancing classification performance through Riemannian geometry-based regularization.\nExperimental results demonstrate significant improvements in classification accuracy on BCI competition datasets.\nGurve et al. [117] proposed a framework for classifying motor imagery EEG data using covariance matrices as\ndescriptors and investigating various dissimilarity metrics on the manifold of SPD matrices. The study compares the\nperformance of Log-Euclidean distance, Stein divergence, Kullback\u2013Leibler divergence, and Von Neumann divergence\nfor classification. Additionally, the paper introduces a new feature, Heterogeneous Orders Relevance Composition\n(HORC), combining different relevance matrices (Covariance, Mutual Information, or Kernel Matrix) under a tensor\nframework and multiple kernel fusion. The framework is further refined using Neighborhood Component Feature\nSelection (NCFS) to optimize the feature subset.\n[117] also introduced a method to improve MI classification performance by employing Non-negative Matrix Factoriza-\ntion (NMF) for EEG channel selection and using the Riemannian geometry of covariance matrices for feature extraction.\nThe method reduces the dimensionality of the EEG data, mitigates overfitting, and enhances classification accuracy by\nselecting subject-specific channels. The NMF is used to decompose the covariance matrix as follows:\nC\u2248 WH                                                                                                 (29)\nwhere C is the covariance matrix, and W and H are non-negative matrices. The neighborhood component feature\nselection (NCFS) algorithm is then applied to select the most important features, further refined by:\nDw(fi, fj) = wz|fik \u2013 fjk|                                                                                    (30)\nwhere wk is the weighting vector for the k-th feature.\nThese studies collectively advance the application of Riemannian geometry in EEG feature extraction and discriminant\nanalysis, offering improved accuracy, robustness, and computational efficiency in BCI systems."}, {"title": "4.2 Classficiation Approaches", "content": "Recent approaches of BCI classification have notably shifted from traditional Euclidean metrics to employing Rie-\nmannian geometry, better reflecting the complex data structures. [94] and [80] demonstrated the use of Riemannian\ndistance-based kernels within SVMs to significantly enhance classification accuracy for motor imagery tasks, surpassing\ntraditional methods without extensive spatial filtering. Similarly, [79] documents substantial performance improvements\nin steady-state visually evoked potential (SSVEP) classification by aligning methods more closely with intrinsic data\ngeometry. The integration of Riemannian geometry with decision tree frameworks in [98] improves classification\nby capturing non-linear data relationships. Additionally, [85] introduces an expectation-maximization algorithm for\nrobust covariance estimation in the presence of incomplete EEG data, outperforming traditional imputation techniques.\nFurthermore, [123] develops new Riemannian geometry-based metrics to monitor and enhance user performance during\nBCI training, ensuring a more accurate and reliable progress assessment.\nThe work [122] explores the application of graph neural networks (GNNs) on SPD manifolds to classify motor imagery\nEEG signals. This approach leverages the time-frequency characteristics of EEG data, which are represented on SPD\nmanifolds. The mathematical formulation involves constructing a graph where each node represents an EEG channel,\nand edges are weighted by a function of the Riemannian distance between SPD matrices. The SPD matrices are\nderived from the covariance of time-frequency representations of the EEG signals, capturing both spatial and spectral\ninformation.\nThe key mathematical components include the computation of the Riemannian distance between two SPD matrices 21\nand 22:\n\u03b4\u03b9(\u03a31, 2) = ||log(\u03a311/222\u03a3 1/2) |                                                                    (31)\nwhere log denotes the matrix logarithm and ||\u00b7||F is the Frobenius norm. This distance metric is crucial for defining the\nweights of the edges in the graph neural network, ensuring that the intrinsic geometry of the data is preserved.\nThe GNN model processes the SPD matrices using graph convolutional layers specifically designed for manifold-valued\ndata. Let G = (V,E) be a graph with vertices V corresponding to EEG channels and edges & weighted by the\nRiemannian distance. The signal at each vertex v is represented by an SPD matrix \u03a3. The graph convolution operation\non SPD manifolds is defined as:\nH(1+1) \u2190 RBN (ReEig (W(t) (D-1A) H(1)W()))                                                                     (32)\nwhere H(\u00b9) is the feature matrix at layer l, W(1) is the trainable weight matrix, N(v) denotes the neighbors of vertex v,\nand o is a non-linear activation function. This operation ensures that the convolution respects the manifold structure of\nthe data.\n[113] describes a method that combines Independent Component Analysis (ICA) with Riemannian geometry to enhance\nemotion recognition from EEG signals; this involves projecting covariance matrices onto the Riemannian manifold and\nintegrating these features into a deep learning model, resulting in superior performance compared to traditional methods.\nSimilarly, [77] applies Riemannian geometry decoding algorithms to large-scale EEG datasets, where the baseline\nminimum distance to Riemannian mean approach yields the highest classification accuracy for motor imagery and\nexecution tasks, underscoring the scalability of these methods. Additionally, [102] explores the feasibility of imagined\nspeech classification using EEG signals, employing covariance matrix descriptors on the Riemannian manifold with\na relevance vector machine classifier to achieve high accuracy, revealing promising potential for BCI applications in\nspeech imagery.\nIn the paper [101] the authors propose a novel brain-ventilator interface (BVI) framework that detects patient-ventilator\ndisharmony from EEG signals. This work leverages the spatial covariance matrices of EEG signals and utilizes\nRiemannian geometry to classify different respiratory states. The approach is robust against the non-stationarity and\nnoise inherent in EEG signals. Mathematically, the framework involves calculating the spatial covariance matrix C for\nthe EEG signals, which is SPD. The Riemannian mean Ck of these matrices is computed using:\nCk = arg min \u2211 d(C, C\u2081)                                                                                        (33)\nCiESk\nThe features are then projected onto the tangent space using the logarithmic map:\nlogo(P) = SQ = Q1/2logm(Q-1/2PQ1/2)Q1/2                                                   (34)\nThese features are used in a classifier to detect respiratory states, thus enabling the BVI system.\nThe work on [91] introduces an innovative method combining Riemannian geometry with sparse optimization and\nDempster-Shafer theory for enhanced motor imagery classification. The method, known as RSODSF, extracts features\nby first calculating the covariance matrices from segmented EEG signals, projecting them into Riemannian tangent\nspace, and applying sparse optimization:\nw = arg min = ||y - Fw||2 + 1||w||1                                                                     (35)\nwhere F is the matrix of features, y is the label vector, w is the weight vector, and A is the regularization parameter. The\nprobabilistic outputs of a support vector machine (SVM) classifier are fused using Dempster-Shafer theory to improve\nclassification accuracy:\nm1,2(EA) =   \\begin{cases}    \\frac{\u2211 E_B \u2229 E_C = E_A m_1(E_B)\u00d7m_2(E_C) }{1 -\u2211 E_B \u2229 E_C=0} & \\text{if } EA \u2260 0 \\\\   0 & \\text{if } EA = 0 \\end{cases}                                                            (36)"}, {"title": "4.3 manifold learning", "content": "Integration of deep learning with Riemannian geometry-based methods was a tren in recent years. In [83], the proposed\nEEG-SPDNet incorporates Riemannian geometry into deep network architectures, enhancing the decoding by exploiting\nthe physiological plausibility of frequency regions. Concurrently, [82] introduces a method for robust representation\nof EEG signals through spatial covariance matrices, capturing homogeneous segments effectively. Furthermore,\n[86] develops a model that optimizes subject-specific frequency band selection for motor imagery classification by\nconstructing multiple Riemannian graphs and applying advanced graph embedding and fusion techniques, tailoring the\napproach to individual variations in EEG signal patterns.\n[87] introduces a novel method to address the limitations of existing SPD matrix-based Riemannian networks. They\npropose Riemannian Embedding Banks (REB), which partition the problem of learning common spatial patterns into\nmultiple subproblems, each modeled separately and then combined within SPD neural networks. The REB method\nutilizes the SPDNet framework, which includes layers such as BiMap and ReEig for transformation and feature\nextraction. The BiMap layer performs a bilinear mapping to generate more discriminative and compact SPD matrix\nfeatures. The transformation is given by:\nXk = WsXs_1W,                                                                                          (42)\nwhere Ws is the transformation matrix ensuring the output Xk remains in the form of an SPD matrix.\nSimilar to the ReLU layer in traditional neural networks, the ReEig layer rectifies SPD matrices with small positive\neigenvalues:\nXs = Us\u22121 max(\u20ac\u0399, \u03a3\u22121)U-1,                                                                                (43)"}, {"title": "4.4 Tangent Space Approaches", "content": "Recent advancements in tangent space approaches for EEG signal processing have introduced innovative methods to\nenhance feature extraction and classification in BCIs. These methods leverage the properties of Riemannian geometry\nto map covariance matrices into the tangent space, allowing for effective manipulation and analysis of EEG data.\n[108"}]}