{"title": "RIEMANNIAN GEOMETRY-BASED EEG APPROACHES: A LITERATURE REVIEW", "authors": ["Imad Eddine Tibermacine", "Samuele Russo", "Ahmed Tibermacine", "Abdelaziz Rabehi", "Bachir Nail", "Kamel Kadri", "Christian Napoli"], "abstract": "The application of Riemannian geometry in the decoding of brain-computer interfaces (BCIs) has swiftly garnered attention because of its straightforwardness, precision, and resilience, along with its aptitude for transfer learning, which has been demonstrated through significant achievements in global BCI competitions. This paper presents a comprehensive review of recent advancements in the integration of deep learning with Riemannian geometry to enhance EEG signal decoding in BCIs. Our review updates the findings since the last major review in 2017, comparing modern approaches that utilize deep learning to improve the handling of non-Euclidean data structures inherent in EEG signals. We discuss how these approaches not only tackle the traditional challenges of noise sensitivity, non-stationarity, and lengthy calibration times but also introduce novel classification frameworks and signal processing techniques to reduce these limitations significantly. Furthermore, we identify current shortcomings and propose future research directions in manifold learning and riemannian-based classification, focusing on practical implementations and theoretical expansions, such as feature tracking on manifolds, multitask learning, feature extraction, and transfer learning. This review aims to bridge the gap between theoretical research and practical, real-world applications, making sophisticated mathematical approaches accessible and actionable for BCI enhancements.", "sections": [{"title": "1 Introduction", "content": "Brain-computer interfaces [9] revolutionize the field of human-machine interactions by enabling direct communication pathways between the brain and external devices, bypassing traditional muscular outputs[1]. Predominantly, BCIs utilize electroencephalography (EEG) due to its cost-effectiveness and minimal computational demands[2]. EEG-based BCIs convert brain activity, measured through EEG signals, into computer commands, leveraging classification algorithms like the common spatial pattern (CSP) to recognize signal patterns associated with specific brain activities such as motor imagery[3]. Despite their promise, EEG-based BCIs face critical challenges including high signal variability, noise, non-stationarity, and the high dimensionality of data spaces[4].\nThe domain of BCI has expanded its utility beyond basic research, offering profound applications in rehabilitation, assistive technologies, and adaptive human-computer interfaces that respond to the user's mental states[5]. Yet, despite these advancements, BCIs are predominantly still in the prototype stage, rarely deployed outside laboratory settings due to their low robustness and the extensive calibration required for each user[6]. Current BCIs suffer from poor reliability, often misinterpreting user commands, which significantly reduces accuracy and information transfer rates even under controlled conditions[7]. This sensitivity to environmental noise and the inherent non-stationarity of EEG signals further complicates their practical application[8].\nIn the rapidly evolving field of BCIs, deep learning has emerged as a transformative force, enabling sophisticated methods for classifying EEG data[10]. These advancements hold the promise of revolutionizing the interaction between humans and machines by interpreting neural activities directly through EEG signals[11]. Despite these advances, the development of BCIs faces significant challenges, notably the limited availability of large, annotated training datasets, which are crucial for training robust deep learning models[12]. In response, researchers have increasingly turned to generative modeling, a burgeoning area within machine learning, to augment existing datasets through the synthesis of synthetic EEG data[13]. This approach not only enhances the diversity of training examples but also addresses the gap caused by scarce data resources[14].\nAchieving practical usability of BCIs requires that they be robust across different contexts, users, and over time, all while maintaining minimal calibration requirements[15][24]. Addressing these multifaceted challenges necessitates a comprehensive strategy: identifying new, reliable neurophysiological markers at the neuroscience level[16]; improving user training to stabilize EEG pattern generation at the human interaction level[17]; and advancing signal processing techniques to enhance feature extraction[18] and classifier robustness at the computational level[19][25]. Prominently, the application of Riemannian geometry[20] to handle EEG covariance matrices has shown considerable promise[21]. This geometric approach not only accommodates the non-Euclidean characteristics of EEG data but also leverages the intrinsic geometric properties of these data to enhance classification accuracy[22][23].\nTraditional approaches like Fourier analysis[26] frequently fail due to the inherently nonstationary nature of EEG signals. This limitation has paved the way for the adoption of time-frequency analysis techniques, such as the wavelet transform, which are now being integrated with spatial filtering[27] methods like the CSP to improve the localizability of rhythmic EEG components. Such advancements are critical for enhancing the efficacy of motor imagery (MI) EEG classifiers, which are vital for real-time BCI applications[28].\nMoreover, a significant aspect of reducing signal variability and enhancing classifier performance in BCIs[29] involves focusing on covariance matrices, typically handled as elements within Euclidean spaces. However, considering these matrices in their naturally occurring Riemannian manifold[30], which better reflects their symmetric and positive definite (SPD) properties[31], can substantially optimize the computational processes. Techniques such as Principal Components Analysis (PCA)[32] and Canonical Correlation Analysis (CCA)[33] have traditionally exploited covariance estimates for spatial filtering in BCIs[34]. By refining how these matrices are treated acknowledging their Riemannian structure-we can significantly enhance the spatial and temporal resolution of EEG signal analysis[35].\nThis paper presents a detailed review focused on the applications of Riemannian Geometry and deep learning-based Riemannian geometry in the domains of BCIs and EEG. We explore how these advanced methodologies enhance BCI design by addressing core challenges inherent in traditional systems. Our discussion extends to the integration of geometric deep learning with generative modeling and sophisticated signal processing techniques. We propose new research directions and outline strategic advancements necessary for developing BCIs that are not only efficacious in controlled settings but also robust and accessible for practical, everyday applications."}, {"title": "2 Brain-Computer Interface", "content": "Brain-computer interfaces that utilize oscillatory neural activity are a significant branch of non-invasive systems for neurocommunication and control[36]. These interfaces typically leverage the modulation of brain rhythms captured via EEG, and are commonly employed in motor imagery-based BCIs[37]. Such BCIs extensively use the CSP algorithm combined with Linear Discriminant Analysis for signal classification, making them highly effective for decoding user intentions based on neural activity patterns[38].\nThe CSP algorithm is pivotal for maximizing the variance of signals from one mental state while simultaneously minimizing it for another[39]. This is achieved by designing spatial filters that enhance the discriminability between two contrasting classes of brain activity. Mathematically, the optimization of CSP filters, represented as column vectors w, is framed as an eigenvalue problem aimed at maximizing the following objective function:\n$$JCSP(W) = \\frac{WTC1W}{WTC2W}$$"}, {"title": "3 Riemannian Geometry Concepts", "content": "In this section, we introduce the mathematical notations and basic definitions fundamental to our study on EEG preprocessing. Scalars are denoted by lowercase letters (e.g., a), vectors by boldface lowercase (e.g., v), representing column vectors unless stated otherwise, matrices by boldface uppercase (e.g., M), and tensors of order three or higher by Euler script letters (e.g., T). The space Rn represents the n-dimensional real vector space, and Rn\u00d7n denotes all n\u00d7n real matrices. Vector en, comprising all ones, belongs to R\", and In denotes the identity matrix in Rn\u00d7n. For any vector x, the matrix diag(x) refers to a diagonal matrix with the elements of x on its main diagonal.\nWe denote by vec(A) the operation that converts a matrix A into a vector by stacking its columns. In the special case where A is symmetric, the vectorization vec(A) is defined such that it takes only the unique entries due to symmetry. The norms || ||p and || . ||F correspond to the Lp and Frobenius norms of a vector or matrix, respectively.\nDefinition 1.1. A matrix A \u2208 Rn\u00d7n is deemed Symmetric Positive Definite (SPD) if it holds that A = AT and xT Ax > 0 for all non-zero vectors x \u2208 Rn. The eigenvalues of such a matrix A, denoted by (A), are guaranteed to be positive.\nDefinition 1.2. A matrix A is considered orthogonal if its columns comprise an orthogonal unit vector set, implying ATA = In.\nDefinition 1.3. The exponential and logarithmic functions of a matrix A \u2208 Rn\u00d7n, denoted by exp(A) and log(A), are defined through its eigenvalue decomposition. If A is expressible as Udiag(11, . . ., \u03bb\u03b7)UT, where U is orthogonal and A\u2081 are the eigenvalues of A, then:\n$$exp(A) = Udiag(exp(x1), ..., exp(\\lambda_n))UT,$$ \n$$log(A) = Udiag(log(11), ..., log(\\lambda_n))UT.$$\nA manifold, in the simplest terms, refers to a space that, at a local level, resembles the Euclidean space of dimension n, known as Rn. This property of local similarity to Rn qualifies it as a topological manifold [52]. When such a space is equipped with a differential structure, it ascends to become a differentiable manifold, thereby enabling the attachment of a tangent space at any point [53]. The tangent space encapsulates all potential vectors that are tangential to any conceivable curve passing through that point on the manifold.\nWithin the manifold paradigm, a Riemannian manifold stands out as a differentiable manifold that is further enhanced with a smoothly varying inner product on its tangent spaces, constituting the Riemannian metric tensor [54]. This metric tensor facilitates the measurement of angles between vectors within the same tangent space, the magnitude of vectors, and the distance between vectors. It allows for the calculation of curve lengths on the manifold, including the geodesic distance which is the shortest distance between two points on the manifold determined by the paths of curves [55].\nConsidering M as a Riemannian manifold and Tx M as its tangent space at point X, for any two vectors V, W \u2208 TxM, the Riemannian metric gx (V, W) provides the inner product [56]. When discussing the manifold of SPD matrices, our focus is particularly on the tangent space Tx M and the Riemannian metric at that point.\nFor a smooth curve \u03b3 : [0, 1] \u2192 M, parametrized with respect to a metric on M, there are numerous ways to describe its trajectory, each with a different speed of traversal. Of particular interest are those curves that are parametrized by arc length, termed naturally parametrized curves [57]. These curves progress with a uniform rate and are defined in relation to the Riemannian metric. The curve's length from \u03b3(0) to \u03b3(1) is computed by integrating the metric-induced velocity along the curve.\nFor two distinct points X1, X2 \u2208 M, and a naturally parametrized curve y that connects these points with y(0) = X1 and y(1) = X2, the length L(y) of this curve is expressed by the integral[52]:\n$$L(Y) = \\int_{0}^{1} \\sqrt{g_{\\gamma(t)} (\\dot{\\gamma}(t), \\dot{\\gamma}(t))} dt$$\nThe optimal path on a manifold that minimizes the distance between two points is called a geodesic. While geodesics represent the paths of least distance, they are uniquely characterized by a uniform rate of traversal and are not necessarily the shortest paths for manifolds that are not simply connected. These paths, particularly on a spherical surface, might have multiple representations, such as the numerous geodesics connecting the poles [55]. The distance along a geodesic, known as the Riemannian distance between two points X1 and X2, is essential in determining the manifold's completeness. Here, we concentrate on manifolds that are geometrically complete [54].\nThe tangent space at point X on a manifold M, denoted TxM, acts as a local linear approximation to M within a specific region around X, usually within a certain radius allowing for a bijection via the exponential map [58]. For every point X' within this neighborhood, the exponential map is the bridge connecting X' to X through a unique geodesic.\nFor any smooth scalar function f defined on M, the Riemannian gradient at X is given as \u2207 f(X) in the direction that maximizes the rate of increase of f. If y is a geodesic with y(0) = X and y(0) = V, then \u2207 f oy describes the change of f along y, and \u2207 f(X) is that vector in TxM such that the inner product (V, V f(X)) equals the derivative of f along the curve at t = 0, defined as[58]:\n$$(V, \\nabla f(X)) = \\frac{d}{dt} f(\\gamma(t))|_{t=0}$$\nThe Riemannian gradient facilitates the computation of directional derivatives on manifolds, linking classical differential calculus to the geometry of the manifold. The exponential map, denoted as Expx : TxM \u2192 M, projects a tangent vector at X onto M, and conversely, the logarithm map, or Logx, maps points back to Tx M, providing a method to traverse between a tangent space and its manifold [59].\nAssigning a set M of square matrices the structure of a Riemannian manifold incorporates a local Euclidean geometry, thereby enriching it with a rigorous mathematical underpinning. Suppose we have M' \u2208 M, a manifold of dimension K, and \u00a7\u2122' = M' \u2013 M representing a tangent vector at M. This vector is part of a higher-dimensional tangent space TMM associated with M.\nThe inner product defined by the Riemannian metric on TMM \u00d7 TMM \u2192 R induces a norm on the tangent space, given by ||\u00a7\u043c'||M = \u221a\u3008\u00a7\u043c',\u00a7\u043c')M, and facilitates the computation of geodesic distances d(M, M') on M [59]. These distances enable the formulation of mean values of points on the manifold as:\n$$Mean({M_1,..., M_N}) = arg min_{M \\in M} \\sum_{i=1}^{N} d(M, M_i)^2.$$\nThe exponential mapping at M in M, denoted as ExpM, and its inverse, the logarithm mapping Log\u2122, are both essential in preserving the manifold's structure, especially when mapping to and from the tangent space. Exp\u043c(\u043c) approximates M' when \u00a7\u2122 is small, and the distance between M and M' is given by the norm of \u00a7\u2122 in RK via LogM. This relationship allows the introduction of vectorization for M, PM : M \u2192 RK, defined as PM(M') = $(Log\u043c(M')) [59].\nFor a compact subset of M where the matrices {M} lie, the mean M can be approximated, resulting in a simplified geodesic distance expression:\n$$d(M, M') \\approx ||P_M(M) \u2013 P_M(M')||.$$\nIn the realm of regression on manifolds, the vectorization PM is pivotal for leveraging machine learning algorithms. It adapts matrix points in M onto RK, facilitating their use in regression techniques that typically assume a Euclidean data structure. For instance, with a collection {Mi} and corresponding response variables {yi}, one first computes the mean of the samples M and applies vectorization to obtain {v}. Linear regression methods, such as ridge regression, can then be applied, presupposing a linear relationship y\u2081 \u2248 v\u0142 \u00df, with \u1e9e representing the regression coefficients [60]."}, {"title": "3.1 The Covariance Matrix of EEG", "content": "Consider X \u2208 RM\u00d7T, representing an EEG signal that has undergone band-pass filtering, where M denotes the number of channels and T represents the number of temporal samples. To analyze the statistical properties of the EEG signal, we construct the covariance matrix P as follows:\n$$P = \\frac{1}{T-1}XX^T$$\nThis matrix P is not only symmetric but also empirically confirmed to be SPD, encapsulating important statistical information about the EEG signals. The symmetry of P arises because (XXT)T = XXT, and it attains its positive-definiteness under the condition that X has full row rank, which is typically satisfied if T > M and the EEG data are sufficiently diverse [61].\nProperties of Symmetric Positive-Definite Matrices: A matrix A \u2208 Rn\u00d7n is deemed positive-definite if for any non-zero vector v \u2208 Rn, it holds that:"}, {"title": "3.2 Riemannian Metrics on SPD Manifolds", "content": "The choice of Riemannian metric on the manifold of SPD matrices significantly influences the analysis and processing of data represented by these matrices, such as EEG signals. The Log-Euclidean Metric (LEM) and the Affine-Invariant Metric (AIM) are two predominant metrics used in this context [63, 65].\nLog-Euclidean Metric (LEM): The Log-Euclidean Metric (LEM) offers an efficient and robust computational method for handling the manifold of SPD matrices. This metric is particularly valued for preserving the bi-invariance property within the Lie group structure of SPD matrices, making it suitable for various applications, including image processing and medical imaging analysis, where maintaining the structure of data during transformations is crucial [65].\nUnder the LEM, the geodesic distance between two points P\u2081 and P2 on the manifold of SPD matrices is defined as:\n$$\\delta_l (P_1, P_2) = || Log(P_1) - Log(P_2)||_F,$$\nwhere Log denotes the matrix logarithm, transforming matrices to a space where the Euclidean tools can be applied. The norm || || F represents the Frobenius norm, which measures matrix entries' absolute differences, thus providing a natural distance metric in the logarithmic domain [65].\nThe Log-Euclidean mean of a set of matrices, crucial for statistical analysis on manifolds, is calculated using:\n$$G = Exp (\\frac{1}{i} \\sum_{i=1}^{k} Log(P_i))$$\nThis formulation allows the mean of matrices to be computed efficiently and without the convergence issues that may arise with other Riemannian metrics. Additionally, when considering a weighted mean where each matrix P\u2081 is assigned a weight w\u2081, fulfilling the conditions \u2211=1 Wi = 1 and w\u2081 > 0, the weighted mean is given by:\n$$G = Exp (\\sum_{i=1}^{k} w_i Log(P_i))$$\nAffine-Invariant Metric (AIM): The AIM is another highly regarded metric for SPD manifolds, especially valued for its invariant properties under affine transformations. The geodesic distance under this metric between P\u2081 and P2 is computed as:\n$$\\delta_A(P_1, P_2) = ||Log(P_1^{-1/2}P_2P_1^{-1/2}) ||_F,$$"}, {"title": "3.3 Tangent Space at a Point on a Manifold", "content": "The concept of tangent space is central to understanding the local geometry of manifolds and plays a critical role in the analysis of data that lie on these manifolds, such as covariance matrices of EEG signals. A tangent space at a point on a manifold provides a linear approximation of the manifold near that point, facilitating operations like vector addition and scalar multiplication which are not inherently defined on the manifold itself [52].\nLet M be a smooth manifold and p a point on M. The tangent space to M at p, denoted as T\u266dM, is a vector space consisting of the tangent vectors to all possible curves through p on the manifold. Formally, if \u03b3 : (\u2212\u20ac,\u20ac) \u2192 Mis a smooth curve with y(0) = p, then the derivative \u03b3'(0), which represents the velocity vector of y at p, is a tangent vector at p [53].\nFor practical computation, particularly in applications involving SPD matrices, we express tangent vectors in coordinates. If M is parameterized locally around p by a coordinate system & : U CR\u2033 \u2192 M, where U is an open set in R\", the tangent vectors can be expressed as:\n$$V = \\sum_{i=1}^{n} v^i \\frac{\\partial}{\\partial x^i}\\|_p$$\nwhere v\u00b2 are components of v in the coordinate basis { } [52].\nIn the context of SPD matrices, which form an open subset of the space of symmetric matrices, the tangent space at any point P \u2208 SPD(n) can be identified with the space of symmetric matrices. If P is an SPD matrix and S is a symmetric matrix, then a curve on SPD(n) passing through P with direction S at t = 0 can be represented as:\n$$\\gamma(t) = P^{1/2} exp(tP^{-1/2}SP^{-1/2})p^{1/2}$$\nwhere exp denotes the matrix exponential. The derivative of this curve at t = 0 gives a tangent vector at P, which is precisely S [63].\nUnderstanding the tangent space of SPD matrices is crucial for developing algorithms in EEG signal processing. For example, the process of projecting EEG covariance matrices onto the tangent space allows for the linearization of the manifold structure, simplifying computations such as averaging or classification. This projection involves mapping SPD matrices to their tangent spaces at a chosen reference point, usually the mean covariance matrix, and performing linear operations in this vector space before projecting back to the manifold [69].\""}, {"title": "3.4 Geodesic Distances on Riemannian Manifolds", "content": "An SPD matrix of size M \u00d7 M resides on a manifold denoted as P(M), characterized by its smooth and curved structure. On such Riemannian manifolds, the concept of geodesics plays a crucial role in understanding and quantifying the intrinsic geometric properties of the space [70]."}, {"title": "4 Riemannian Geometry-Based EEG Approaches", "content": "This article presents a synthesis of discoveries from a meticulously curated collection of 42 publications, manually chosen based on their pertinence to the fusion of Riemannian geometry and deep learning in brain-computer interfaces (BCIs). The selection of articles was made through a focused exploration conducted on Google Scholar and PubMed. The exploration, executed with the objective of encompassing the most recent and pertinent research up to May 3rd, 2024, utilized a search query formulated as: (\u201cRiemannian geometry\u201d OR \u201cmanifold learning\u201d Or \u201cSPD Manifolds\") AND (\"brain computer interface\u201d OR \u201cEEG\u201d). This curation procedure encompassed both peer-reviewed publications and preprints, ensuring a thorough incorporation of the newest advancements and dialogues in the domain."}, {"title": "4.1 Feature Extraction Methods", "content": "Feature extraction is a SOTA necessary step after collecting EEG data. It allows reducing the dimensionality of the data and making it more comprehended by machine learning and deep learning models. Various studies have explored the use of Riemannian geometry for feature extraction and discriminant analysis in EEG-based BCIs. In [104], the authors used Covariance Matrices as EEG Signal Descriptors and exploring various dissimilarity metrics on SPD matrices and introducing a novel feature, HORC, which combines different relevance matrices under a tensor framework for improved classification accuracy. A Riemannian Spatial Pattern (RSP) method was proposed in [109] to extract spatial patterns from Riemannian classification for motor imagery tasks. The RSP method uses a backward channel selection procedure and compares it with the Common Spatial Pattern (CSP) approach. The RSP method provides precise mapping and clustering of imagined motor movements, which is especially useful in differentiating finger flexions. The spatial pattern extraction can be described using:\n$$C = \\frac{1}{N} \\sum_{i=1}^{N}x_ix_i^T$$\nwhere C is the covariance matrix, and xi represents the EEG signals.\nHuang et al. [90] introduce the Common Amplitude-Phase Measurement (CAPM) method, designed to simultaneously analyze the amplitude and phase information of EEG signals on a Riemannian manifold. This dual consideration promises to enhance classification accuracy for BCI applications significantly. The initial step involves extracting the amplitude and phase from EEG signals using the wavelet transform, specifically applying a complex Morlet wavelet. The transformation is mathematically represented as:\n$$x_c(t, f) = x(t) * \\psi(t, f) = a_c(f, t)e^{i\\phi_c(f,t)}$$\nwhere (t, f) denotes the complex signal representation at channel c, time t, and frequency f. Here, ac(f, t) is the amplitude, 0c(f, t) represents the phase, and (t, f) is the Morlet wavelet function. To dimensionally reduce while preserving discriminative features, the authors devise a Riemannian graph embedding technique. The adjacency matrix S, based on the Riemannian distance between the SPD covariance matrices of EEG trials, is defined by:\n$$S_{nr} = \\begin{cases} exp(-\\eta r), & \\text{if } \\Upsilon_n \\in C_z(\\Upsilon_r) \\text{ and } \\Upsilon_r \\in C_z(\\Upsilon_n) \\\\ 0, & \\text{otherwise} \\end{cases}$$\nwhere dnr is the Riemannian distance between covariance matrices Pn and Pr:\n$$\\delta_{n r} = \\delta_{\\rho}(P_n, P_r) = ||log(P_n^{-1}P_r^{-1})|| = (\\sum_{l=1}^{L}log^2 \\beta_l)^{1/2}$$\nThe low-dimensional data Yn is used to compute a new covariance matrix Pn = WH P\u201eW. Classification is performed using the Minimum Distance to Riemannian Mean (MDRM):\n$$z_n = sign(\\frac{ \\sum_{l=1}^{M} log^2 \\eta_m}{(\\sum_{l=1}^{M} log^2 \\rho_m)^{1/2}})$$\nwhere Nm and pm are the eigenvalues of P\u00af\u00b9Pn 1Pn and P\u22121 Pn respectively. A regularized linear regression model is then applied to optimize the classification parameter b:\n$$\\frac{1}{2} min = ||z - Db||^2 + (1 - \\alpha) ||b||_2 + \\lambda \\alpha ||b||_1$$\nwhere D is the matrix of input vectors dn and z is the corresponding label vector.\nThe CAPM method effectively captures the intrinsic amplitude and phase information of EEG signals, optimizing the spatial-spectral filters and enhancing classification performance through Riemannian geometry-based regularization. Experimental results demonstrate significant improvements in classification accuracy on BCI competition datasets.\nGurve et al. [117] proposed a framework for classifying motor imagery EEG data using covariance matrices as descriptors and investigating various dissimilarity metrics on the manifold of SPD matrices. The study compares the performance of Log-Euclidean distance, Stein divergence, Kullback\u2013Leibler divergence, and Von Neumann divergence for classification. Additionally, the paper introduces a new feature, Heterogeneous Orders Relevance Composition (HORC), combining different relevance matrices (Covariance, Mutual Information, or Kernel Matrix) under a tensor framework and multiple kernel fusion. The framework is further refined using Neighborhood Component Feature Selection (NCFS) to optimize the feature subset.\n[117] also introduced a method to improve MI classification performance by employing Non-negative Matrix Factoriza- tion (NMF) for EEG channel selection and using the Riemannian geometry of covariance matrices for feature extraction. The method reduces the dimensionality of the EEG data, mitigates overfitting, and enhances classification accuracy by selecting subject-specific channels. The NMF is used to decompose the covariance matrix as follows:\n$$C \\approx WH$$\nwhere C is the covariance matrix, and W and H are non-negative matrices. The neighborhood component feature selection (NCFS) algorithm is then applied to select the most important features, further refined by:\n$$D_w(f_i, f_j) = \\sum_k w_z|f_{ik} \u2013 f_{jk}|$$\nwhere wk is the weighting vector for the k-th feature.\nThese studies collectively advance the application of Riemannian geometry in EEG feature extraction and discriminant analysis, offering improved accuracy, robustness, and computational efficiency in BCI systems."}, {"title": "4.2 Classficiation Approaches", "content": "Recent approaches of BCI classification have notably shifted from traditional Euclidean metrics to employing Riemannian geometry, better reflecting the complex data structures. [94] and [80] demonstrated the use of Riemannian distance-based kernels within SVMs to significantly enhance classification accuracy for motor imagery tasks, surpassing traditional methods without extensive spatial filtering. Similarly, [79] documents substantial performance improvements in steady-state visually evoked potential (SSVEP) classification by aligning methods more closely with intrinsic data geometry. The integration of Riemannian geometry with decision tree frameworks in [98] improves classification by capturing non-linear data relationships. Additionally, [85] introduces an expectation-maximization algorithm for robust covariance estimation in the presence of incomplete EEG data, outperforming traditional imputation techniques. Furthermore, [123] develops new Riemannian geometry-based metrics to monitor and enhance user performance during BCI training, ensuring a more accurate and reliable progress assessment.\nThe work [122] explores the application of graph neural networks (GNNs) on SPD manifolds to classify motor imagery EEG signals. This approach leverages the time-frequency characteristics of EEG data, which are represented on SPD manifolds. The mathematical formulation involves constructing a graph where each node represents an EEG channel, and edges are weighted by a function of the Riemannian distance between SPD matrices. The SPD matrices are derived from the covariance of time-frequency representations of the EEG signals, capturing both spatial and spectral information.\nThe key mathematical components include the computation of the Riemannian distance between two SPD matrices 21 and 22:\n$$\\delta_l(\\Sigma_1, \\Sigma_2) = ||log(\\Sigma_1^{1/2}\\Sigma_2\\Sigma_1^{1/2} )||_F$$\nwhere log denotes the matrix logarithm and ||\u00b7||F is the Frobenius norm. This distance metric is crucial for defining the weights of the edges in the graph neural network, ensuring that the intrinsic geometry of the data is preserved.\nThe GNN model processes the SPD matrices using graph convolutional layers specifically designed for manifold-valued data. Let G = (V,E) be a graph with vertices V corresponding to EEG channels and edges & weighted by the Riemannian distance. The signal at each vertex v is represented by an SPD matrix \u03a3. The graph convolution operation on SPD manifolds is defined as:\n$$H^{(1+1)} \\leftarrow RBN (ReEig (W^{(t)} (D^{-1}A) H^{(1)}W^{(t)} ))$$\nwhere H(\u00b9) is the feature matrix at layer l, W(1) is the trainable weight matrix, N(v) denotes the neighbors of vertex v, and o is a non-linear activation function. This operation ensures that the convolution respects the manifold structure of the data.\n[113] describes a method that combines Independent Component Analysis (ICA) with Riemannian geometry to enhance emotion recognition from EEG signals; this involves projecting covariance matrices onto the Riemannian manifold and integrating these features into a deep learning model, resulting in superior performance compared to traditional methods. Similarly, [77] applies Riemannian geometry decoding algorithms to large-scale EEG datasets, where the baseline minimum distance to Riemannian mean approach yields the highest classification accuracy for motor imagery and execution tasks, underscoring the scalability of these methods. Additionally, [102] explores the feasibility of imagined speech classification using EEG signals, employing covariance matrix descriptors on the Riemannian manifold with a relevance vector machine classifier to achieve high accuracy, revealing promising potential for BCI applications in speech imagery.\nIn the paper [101] the authors propose a novel brain-ventilator interface (BVI) framework that detects patient-ventilator disharmony from EEG signals. This work leverages the spatial covariance matrices of EEG signals and utilizes Riemannian geometry to classify different respiratory states. The approach is robust against the non-stationarity and noise inherent in EEG signals. Mathematically, the framework involves calculating the spatial covariance matrix C for the EEG signals, which is SPD. The Riemannian mean Ck of these matrices is computed using:\n$$C_k = arg min \\sum_{C_i \\in S_k} d(C, C_i)$$\nThe features are then projected onto the tangent space using the logarithmic map:\n$$logo(P) = SQ = Q^{1/2}logm(Q^{-1/2}PQ^{1/2})Q^{1/2}$$\nThese features are used in a classifier to detect respiratory states, thus enabling the BVI system.\nThe work on [91] introduces an innovative method combining Riemannian geometry with sparse optimization and Dempster-Shafer theory for enhanced motor imagery classification. The method, known as RSODSF, extracts features by first calculating the covariance matrices from segmented EEG signals, projecting them into Riemannian tangent space, and applying sparse optimization:\n$$ \\frac{1}{2} \\|y - Fw\\|^2 + \\lambda \\|w\\|_1$$\nwhere F is the matrix of features, y is the label vector, w is the weight vector, and A is the regularization parameter. The probabilistic outputs of a support vector machine (SVM) classifier are fused using Dempster-Shafer theory to improve classification accuracy:\n$$m_{1,2}(E_A) = \\begin{cases} \\frac{\\sum_{E_B \\cap E_C = E_A}m_1(E_B) \\times m_2(E_C)}{1-\\sum_{E_B \\cap E_C = \\varnothing} m_1(E_B) \\times m_2(E_C)} & \\text{if } E_A \\neq \\varnothing \\\\ 0 & \\text{if } E_A = \\varnothing \\end{cases}$$"}, {"title": "4.3 manifold learning", "content": "Integration of deep learning with Riemannian geometry-based methods was a tren in recent years. In [83", "82": "introduces a method for robust representation of EEG signals through spatial covariance matrices", "86": "develops a model that optimizes subject-specific frequency band selection for motor imagery classification by constructing multiple Riemannian graphs and applying advanced graph embedding and fusion techniques", "patterns.\n[87": "introduces a novel method to address the limitations of existing SPD matrix-based Riemannian networks. They propose Riemannian Embedding Banks (REB)", "by": "n$$X_k = W_sX_{s-1"}, "W^T_s$$\nwhere Ws is the transformation matrix ensuring the output Xk remains in the form of an SPD matrix.\nSimilar to the ReLU layer in traditional neural networks, the ReEig layer rectifies SPD matrices with small positive eigenvalues:\n$$X_s = U_{s-1} max(\\epsilon I, \\Sigma_{s-1})U_{s-1}^T,$$\nwhere Us\u22121 and 25\u20131 are obtained from the eigenvalue decomposition X5\u22121 = Us-1\u2211s-1UT_1, and e is a threshold parameter. The REB approach optimizes the embedding by assigning features to clusters and ensuring that the samples within each cluster are informative. The clustering is achieved by minimizing:\n$$L_A(X_i; f_p) = \\sum_{X_j \\in f_p(X_i)} \\delta_R(X_i, X)$$\nand\n$$L_P(X_i, Y_i; f_p) = - \\sum_{X_j \\in f_p(X_i), y_j=Y_i} \\delta_R(X_i, X_i),$$\n$$L_N(X_i; f_p) = \\sum_{X_m \\in f_p(X_i), Y_m\\neq Y_i} log(min(10^{-4}, Q(D(X_i, X_m)))),$$\nwhere Q(D) is a distribution function ensuring negative samples are uniformly distributed based on their distances. The final embedding is constructed by concatenating the sub-embeddings produced by individual learners:\n$$f = [f_a(X_i; f_1), f_a(X_i; f_2), ..., f_a(X_i; f_k)"], "by": "n$$L_k = 1 \\sum_{X_i\\in C_k} L_{C_k} (X_i; f_p) + 12 \\sum_{X_i \\in C_k} L_{f_k} (X_i; f_k).$$\nThe experimental results on public EEG datasets demonstrate the superiority of the proposed REB approach in learning common spatial patterns of EEG signals, increasing convergence speed, and maintaining generalization despite the non-stationary nature of the data.\nThe paper [96] leverages the graph structure of EEG trials, incorporating node influence properties and an ensemble of semantic graphs into the neural structured learning"}