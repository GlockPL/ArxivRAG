{"title": "Efficient Auto-Labeling of Large-Scale Poultry Datasets (ALPD) Using Semi-Supervised Models, Active Learning, and Prompt-then-Detect Approach", "authors": ["Ramesh Bahadur Bista", "Lilong Chai", "Shawna Weimer", "Hannah Atungulua", "Chantel Pennicott", "Xiao Yang", "Sachin Subedi", "Chaitanya Pallerla", "Yang Tian", "Dongyi Wang"], "abstract": "The rapid growth of artificial intelligence (AI) in poultry farming has highlighted the challenge of efficiently labeling large, diverse datasets. Manual annotation is time-consuming and costly, making it impractical for modern systems that continuously generate data. This study addresses this challenge by exploring semi-supervised auto-labeling methods, integrating active learning and the prompt-then-detect paradigm to develop an efficient, label-scarce framework for auto-labeling of large poultry datasets (ALPD) annotation aimed at advancing AI-driven behavior and health monitoring. For this study, video data were collected from broilers and laying hens housed at the University of Arkansas and the University of Georgia. The collected videos were then converted into images, filtered, pre-processed, augmented, and labeled. Various machine learning models, including zero-shot models like Grounding DINO, YOLO-World, and CLIP, and supervised models like YOLO and Faster-RCNN, were utilized for broilers, hens, and behavior detection. The results showed that YOLOv8s-World and YOLOv9s performed better when compared performance metrics for broiler and hen detection under supervised learning, while among the semi-supervised model, YOLOv8s-ALPD achieved the highest precision (96.1%) and recall (99.0%) with an RMSE of 1.9. The hybrid YOLO-World model, incorporating the optimal YOLOv8s backbone, demonstrated the highest overall performance. It achieved a precision of 99.2%, recall of 99.4%, and an F1 score of 98.7% for breed detection, alongside a precision of 88.4%, recall of 83.1%, and an F1 score of 84.5% for individual behavior detection. Additionally, semi-supervised models showed significant improvements in behavior detection, achieving up to 31% improvement in precision and 16% in the F1-score. The semi-supervised models with minimal active learning reduced annotation time by over 80% compared to full manual labeling. Moreover, integrating zero-shot models with the best models enhanced detection and behavior identification. In conclusion, integrating semi-supervised auto-labeling and zero-shot models significantly improves detection accuracy. It reduces manual annotation efforts, offering a promising solution to optimize AI-driven systems in poultry farming, advancing precision livestock management, and promoting more sustainable practices.", "sections": [{"title": "1. Introduction", "content": "In recent years, the livestock and poultry sectors have faced significant challenges due to rising labor costs and persistent labor shortages, particularly during peak seasons. In 2024, labor expenses, including both cash and noncash employee compensation, are projected to increase by $3.0 billion (6.1%), reaching $52.0 billion compared to 2023 (USDA, 2024). Additionally, livestock and poultry purchases are expected to rise by $4.4 billion (10.2%), totaling $47.4 billion. These trends are expected to persist, emphasizing the urgent need for advanced technologies like artificial intelligence (AI) and machine learning (ML) to enhance automation, decision-making, and efficiency in agricultural systems (Shaikh et al., 2022). These AI-driven systems rely heavily on high-quality labeled datasets, which are the foundation for training robust models. Available models are often limited to basic detection, segmentation, classifications, and auto-labeling like \"chicken\" or \"birds.\u201d However, it cannot distinguish between breeds, their behaviors, or physical conditions. Poultry data varies significantly in breed, behavior, appearance, and rearing conditions. This diversity makes accurate and timely annotation crucial for quality control, behavior and welfare analysis, health monitoring, and process optimization.\nDeep learning has significantly advanced automatic object detection, yet the need for large, high-quality, labeled datasets often constrains its practical application. Current approaches, particularly those based on supervised training, rely heavily on these datasets, posing a major challenge for widespread implementation. Supervised methods are inherently data-intensive and depend heavily on the availability of large, high-quality labeled datasets (Cao, 2024). Even with expert annotations, label noise or errors due to annotator variation are inevitable. Fully supervised labeling is effective but demands extensive manual annotation of large datasets, making it both time-consuming and expensive (Sapkota et al., 2024). This approach is often impractical in the poultry sector, where data is continuously generated in vast quantities, and tasks such as detecting subtle quality variations in poultry products require expert knowledge, adding to the complexity. Previous research on supervised learning models for object detection, which rely on paired images and labels, often faces limitations based on the volume and quality of annotated data (Wang et al., 2022). While large and diverse datasets enhance a model's pattern recognition and generalization"}, {"title": "2. Materials and methods", "content": "The Institutional Animal Care and Use Committee (IACUC) reviewed and approved this study involving broilers under protocol #23009 at the University of Arkansas. Additionally, a related laying hen study was approved by the IACUC under protocol AUP# A2020 08-014-A2 at the University of Georgia."}, {"title": "2.1. Housing and management", "content": null}, {"title": "2.1.1. Broilers", "content": "The study was conducted in broiler houses designed with specific configurations and management practices. Each house comprised 12 pens measuring 3.66m (12ft) x 2.44m (8ft), with individual programmable environmentally controlled research chambers utilized to maintain precise environmental conditions; average daily temperatures and relative humidity levels were monitored within these chambers and placed as per Cobb 500 broiler management guidelines. The experimental design included raising Cobb 500 male broilers, with a stocking density of 1.92ft\u00b2 per bird, and each pen housing 50 broilers aged from day 0 to 42. This study was conducted from February 1, 2023, to March 15, 2023. No treatments were applied during the study period, and the broilers received only commercial starter, grower, and finisher diets without additional enzymes or medications. For monitoring, cameras were strategically installed at a height of 2m (6.5ft), with angles perpendicular to the ground, ensuring comprehensive coverage of feeding stations and water sources within each pen. The camera setup utilized a LOREX 4K 32ch 8TB Wired NVR System (Model # N883A38B, Lorex Corporation, Markham, ON, Canada) with dual 8TB internal hard drives for recording, capturing footage at a frame rate of 30 Frame Per Second (FPS) and a resolution of 1920 x 1080."}, {"title": "2.1.2. Laying hens", "content": "The experiment was conducted in four identical research houses, each housing 200 Hy-Line W-36 hens. Birds were raised from day 1 to day 630 (90 weeks) in rooms measuring 7.3 m \u00d7 6.1 m \u00d7 3 m, equipped with perches, litter flooring, feeders, drinkers, lights, and nest boxes. Pine shavings were used as bedding, and a Chore-Tronics Model 8 controller (Chore-Time Equipment, Milford, IN) controlled environmental conditions such as temperature, humidity, light, and ventilation. Behavior was recorded using six night-vision network cameras (PRO-1080MSB, Swann Communications USA Inc., Santa Fe Springs, LA) mounted 3 meters above the litter floor and two cameras 0.5 meters above the ground, capturing footage 24 hours videos a day. Videos were stored in .avi format at a resolution of 1920 \u00d7 1080 pixels and 15 frames per second, using a digital video recorder (DVR-4580, Swann Communications USA Inc., Santa Fe Springs, LA) for data storage, covering the period from 25 to 50 weeks of age. The previously published paper explained the details of housing and management (Bist et al., 2023c; Yang et al., 2023a)."}, {"title": "2.2. Image labeling and data pre-processing", "content": "The images collected from broiler and laying hen experimental rooms were first preprocessed and filtered to ensure they met the requirements for training a machine learning model. The dataset was then divided into training (60%), validation (20%), and testing (20%) subsets. To enhance model performance and generalization, various augmentation techniques, including geometric transformations (such as rotations, flipping, and scaling), were applied following the previous paper (Bist et al., 2024). These augmentations simulate real-world variations, improve model"}, {"title": "2.3. Model description", "content": "In this study, we utilized various machine learning models for auto-labeling to efficiently generate labeled data for model training. The models employed for this purpose span different categories, including zero-shot, supervised, and semi-supervised models, each with distinct methodologies and capabilities."}, {"title": "2.3.1. Zero-shot models for auto-labeling", "content": null}, {"title": "2.3.1.1. Grounding DINO (GDINO)", "content": "Grounding DINO is a state-of-the-art zero-shot object detection model developed by IDEA-Research (Liu et al., 2025), designed to detect arbitrary objects or behaviors based on human input, such as category names or referring expressions. Built on the DINO model, a transformer-based architecture known for its success in image classification and object detection, Grounding DINO incorporates a grounding module. This addition enables the model to establish a relationship between language and visual content by learning to associate words in text descriptions with specific regions in images. As a result, Grounding DINO can detect objects in unseen images without prior knowledge of those objects, offering high versatility for a range of real-world applications, including agriculture."}, {"title": "2.3.1.2. YOLO-World", "content": "YOLO-World is an advanced extension of the YOLO framework, designed by AI Lab to address the limitations of traditional object detection models that rely on predefined categories (Cheng et al., 2024). By incorporating vision-language modeling and pre-training on large-scale datasets, YOLO-World enables open-vocabulary detection, allowing it to identify a diverse range of objects in a zero-shot manner without requiring task-specific annotations. The model features a Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN), which facilitates effective interaction between visual and linguistic information and utilizes a region-text contrastive loss to enhance alignment between image and text data. YOLO-World has demonstrated superior performance on challenging benchmarks, such as the LVIS dataset, achieving 35.4 Average Precision (AP) with 52.0 Frames Per Second (FPS) on a V100 GPU, surpassing several state-of-the-art methods in both accuracy and speed. Its ability to generalize to unseen categories is further proven through its fine-tuned performance on downstream tasks like object detection and open-vocabulary instance segmentation. The integration of transfer learning and pre-trained weights allows YOLO-World to analyze visual data for tasks without relying on annotated data, and its architecture, which supports user-defined vocabulary prompts, makes it adaptable to various detection tasks. This versatility ensures rapid and accurate results across diverse applications, including poultry behavior analysis in agriculture."}, {"title": "2.3.1.3. CLIP", "content": "CLIP (Contrastive Language-Image Pre-Training) is an advanced neural network developed by OpenAI, designed to bridge the gap between visual and textual information (OpenAI, 2024). Trained on a diverse dataset of image-text pairs, CLIP predicts a given image's most relevant textual description through natural language instructions, demonstrating zero-shot learning capabilities similar to models like GPT-2 and GPT-3. Remarkably, CLIP achieves comparable performance to ResNet50 on the ImageNet \"zero-shot\" benchmark without relying on the 1.28 million labeled examples from the original dataset (OpenAI, 2024). This achievement highlights its ability to address critical challenges in computer vision, such as dependency on large, labeled datasets and generalization to novel tasks. By aligning visual and textual modalities, CLIP has"}, {"title": "2.3.2. Supervised models for auto-labeling", "content": null}, {"title": "2.3.2.1. YOLO models", "content": "For this research, we utilized multiple versions of the YOLO (You Only Look Once) detector series by Ultralytics, including YOLOv8s, YOLOv9s, YOLOv10s, YOLO11s, YOLOv8s-Worldv2, and RT-DETR (Ultralytics, 2024). These models were selected for their efficiency and performance in object detection tasks focusing on their suitability for auto-labeling and detection in poultry-related applications. YOLOv8s and other small variants were chosen for their speed and lower resource consumption than larger models, making them ideal for real-time processing and handling large datasets with reduced computational overhead. While larger models provide higher accuracy, the smaller YOLO models balance speed and performance, enabling rapid detection and labeling without requiring extensive hardware resources. YOLOv8s-Worldv2, a more advanced version, was specifically leveraged for its open-vocabulary detection capabilities, allowing greater flexibility in identifying a wide range of objects without predefined labels. This combination of models provides a versatile, scalable solution for efficient and accurate auto-labeling in the context of poultry behavior detection. Once trained, they are deployed to auto-label additional unlabeled images, creating pseudo-labeled data that can be used for further training iterations."}, {"title": "2.3.2.2. Faster-RCNN", "content": "Faster-RCNN, a region-based convolutional neural network (CNN), was utilized for object detection in this study and analyzed using open-source GitHub repository (Ren et al., 2017; Yang et al., 2017). The model generates candidate regions within an image and subsequently classifies them into predefined categories. Initially, Faster-RCNN undergoes supervised training on a labeled dataset, allowing it to learn to detect various objects or behaviors. Once trained, the model can be applied to unlabeled images for auto-labeling tasks, particularly for identifying and classifying poultry behaviors. The ResNet backbone was employed for the feature extraction and image classification tasks due to its deep residual network architecture. ResNet's use of skip connections aids in effectively training deeper models, which is especially beneficial for accurately recognizing complex behaviors in poultry. After the model is trained on labeled datasets, it can generate pseudo-labels for various poultry behaviors, enabling efficient and scalable auto-labeling in real-world applications. In this research, Faster-RCNN with a ResNet backbone was selected for its robustness in object detection tasks and its ability to generate high-quality pseudo-labels for"}, {"title": "2.3.3. Semi-supervised models for auto-labeling", "content": null}, {"title": "2.3.3.1. YOLO-ALPD", "content": "The YOLO models, including YOLOv8s, YOLOv9s, YOLOv10s, YOLOv11s, YOLOv8s-Worldv2 and RT-DETR, were initially trained on small, labeled datasets to develop highly accurate baseline models. These trained models were then employed to ALPD that were previously unlabeled. The ALPD approach integrates supervised learning with semi-supervised techniques to enhance model performance in data-scarce scenarios. Specifically, the models were first fine-tuned on the small labeled datasets to optimize detection accuracy. Subsequently, these models were applied to unlabeled datasets to generate pseudo-labels, which were iteratively incorporated back into the training process. During each iteration, the model's predictions on the unlabeled data were refined, creating a feedback loop that progressively improved its detection and labeling capabilities. This iterative auto-labeling strategy enabled the semi-supervised models to efficiently label large datasets while enhancing their performance, making them suitable for scalable and accurate detection tasks in poultry datasets."}, {"title": "2.3.3.2. Faster-RCNN-ALPD", "content": "The Faster-RCNN with ResNet backbone models were adapted to employ the ALPD technique within a semi-supervised learning framework. Initially, these models underwent supervised training on small labeled datasets to establish a reliable baseline for detection tasks. Following this, the models were utilized to generate pseudo-labels for unlabeled poultry behavior datasets. These pseudo-labels were iteratively refined and incorporated back into the training process, creating a feedback loop that progressively enhanced model accuracy and generalization. In the case of Faster-RCNN, the model leveraged its region-based detection framework, while ResNet utilized its robust feature extraction capabilities to identify and label poultry datasets effectively. The ALPD approach facilitated large-scale data augmentation and reduced the dependency on extensive manual annotation, allowing the models to adapt to diverse datasets and improve their performance across successive iterations. This semi-supervised learning paradigm ensured continuous improvement, making these models highly effective for large-scale poultry dataset analysis."}, {"title": "2.3.4. Final model architecture", "content": "The models employed for auto-labeling played a pivotal role in generating large-scale labeled datasets essential for training and enhancing machine learning models designed for poultry breed and behavior detection. This study utilized a combination of zero-shot learning, supervised learning, and semi-supervised learning with active learning techniques to develop a robust detection system with improved accuracy, while significantly reducing the reliance on manual labeling. Active learning helps the model selectively query the most informative data points for labeling to improve performance efficiently (Adsule et al., 2024). The approach facilitated scalable solutions for monitoring poultry breeds and behaviors, addressing key challenges in poultry management. To further enhance prediction capabilities, the best-performing model (best.pt) was integrated with zero-shot learning frameworks, incorporating text-based prompts to improve prediction accuracy and adaptability. This integration allowed the system to manage novel"}, {"title": "2.3.5. Active learning framework", "content": "This study employs four active learning frameworks to enhance model performance while minimizing manual labeling efforts: confidence thresholding, uncertain sample selection and labeling, query by committee, and model retraining with performance evaluation. These frameworks work together to iteratively improve the detection model by prioritizing uncertain and high-impact samples for labeling and retraining."}, {"title": "2.3.5.1. Confidence thresholding", "content": "The Active Learning with Confidence Thresholding (ALCT) methodology aims to iteratively refine object detection performance. Initially, the model is trained on a small, labeled dataset to learn basic object representations. Once trained, the model makes predictions on the unlabeled data and assigns confidence scores to each detected object, reflecting the certainty of the predictions. A predefined confidence threshold is applied to these predictions, with any instances falling below the threshold flagged as uncertain (Wang et al., 2021). These uncertain samples are prioritized for relabeling again and send back to unlabeled images, ensuring the model focuses on the data points most likely to improve its performance. The new image samples are integrated into the training dataset, and the model is retrained. Over successive iterations, this process continually enhances the model's ability to make accurate predictions, improving its robustness and scalability for agricultural monitoring tasks."}, {"title": "2.3.5.2. Selection of uncertain samples and labeling", "content": "In this framework, the model ranks the unlabeled samples based on their confidence scores. Lower confidence scores indicate higher uncertainty, and these samples are extracted for further labeling (Wang et al., 2021). A user-defined confidence threshold, typically set below 50%, determines which samples are considered uncertain. These uncertain samples are then manually or semi-automatically labeled. In the semi-automated approach, the model generates initial labels, which"}, {"title": "2.3.5.3. Query by committee", "content": "The query-by-committee approach identifies instances where the model's predictions conflict or exhibit significant uncertainty. In these cases, multiple models are used to generate different predictions, and the discrepancies between them guide human annotators to focus on clarifying these uncertainties. Resolving these conflicts enriches the dataset with critical, high-impact information, allowing the model to learn more effectively from ambiguous or complex examples."}, {"title": "2.3.5.4. Model retraining and performance evaluation", "content": "Following the incorporation of newly labeled samples, the model undergoes retraining to integrate the expanded dataset. This phase is essential for allowing the model to learn from the newly labeled data, enhancing its detection capabilities. During this process, the model adapts to the additional information, which helps refine its performance on previously unseen examples. Retraining involves fine-tuning the model on the updated dataset, optimizing the loss function, adjusting learning rates, and applying data augmentation techniques to improve generalization. After retraining, a separate validation dataset evaluates the model's performance. Key metrics, such as mean Average Precision (mAP), precision, recall, and F1-score, are used to assess the model's accuracy and its ability to generalize to unseen data. These evaluation results guide the iterative learning process, ensuring that each cycle leads to measurable improvements in model performance and efficiency."}, {"title": "2.4.\nModel evaluation metrics", "content": "A range of commonly used evaluation metrics were employed to evaluate the models' performance for auto-labeling and pseudo-labeled data output (Cheng et al., 2024; Ultralytics, 2024). These metrics help assess the quality of the model's predictions, including its ability to detect behaviors and the accuracy of its auto-labeled outputs. The following metrics were applied:\n2.4.1. Precision\nPrecision measures the proportion of correctly predicted positive instances out of all instances predicted as positive. It is crucial when false positives have a high cost.\nPrecision =\nTP\nTP + FP\n(i)\nWhere, TP = True Positives and FP = False Positives"}, {"title": "2.4.2. Recall", "content": "Recall, also known as sensitivity, measures the proportion of actual positive instances that were correctly identified by the model. It is especially useful when false negatives are costly.\nRecall =\nTP\nTP + FN\n(ii)\nWhere, FN = False Negatives\nA high recall ensures that most of the true behaviors, such as breeds or behaviors, are identified, even if it results in some false positives."}, {"title": "2.4.3. Mean Average Precision", "content": "The mean average precision is a commonly used metric in object detection, evaluating the model's precision at a standard overlap threshold.\nmAP =\n1\nN\nN\n\u2211\ni=1\nAPi\n(iii)\nWhere, APi = Average Precision for class i and N = Total number of classes\nA higher mAP indicates better overall performance in detecting breeds or behaviors with an acceptable level of localization."}, {"title": "2.4.4. F1-Score", "content": "The F1-score is the harmonic means of precision and recall, providing a single metric that balances both the ability to correctly identify positive instances and the ability to detect as many positives as possible.\nF1 = 2 \u00d7\nPrecision \u00d7 Recall\nPrecision + Recall\n(iv)\nThe F1-score is particularly useful when there is an imbalance between classes (e.g., rare behaviors like mislaying). It provides a balanced measure of a model's performance, ensuring that both false positives and false negatives are accounted for in the evaluation."}, {"title": "2.4.5. Intersection over Union (IoU)", "content": "IoU measures the overlap between the predicted bounding box and the ground truth bounding box. It is widely used to assess the localization accuracy in object detection tasks."}, {"title": "2.4.6. Mean Absolute Error (MAE)", "content": "MAE measures the average absolute difference between the true and predicted values, providing a straightforward indication of prediction accuracy.\nMAE =\n1\nN\nN\n\u2211\ni=1\n|yi \u2212 \u0177i |\n(vi)\nwhere y\u2081 is the true value and \u0177\u2081 is the predicted value. N refers to the total number of samples or data points in the evaluated dataset.\nMAE measures the average magnitude of errors without considering their direction. Lower MAE values indicating better model performance."}, {"title": "2.4.7. Mean Squared Error (MSE)", "content": "MSE calculates the average squared difference between the true and predicted values, heavily penalizing larger errors.\nMSE =\n1\nN\nN\n\u2211\ni=1\n(Vi \u2013 \u0177\u2081)\u00b2\n(vii)\nMSE penalizes larger errors more heavily, making it sensitive to outliers. A lower MSE value signifies a model that makes fewer and smaller errors in its predictions."}, {"title": "2.4.8. Root Mean Square Error (RMSE)", "content": "RMSE is the square root of the MSE, offering a measure of prediction error in the same units as the data. Smaller RMSE values indicate better predictive accuracy.\nRMSE = \u221aMSE\n(viii)"}, {"title": "2.4.9. False Negative Rate (FNR)", "content": "FNR represents the proportion of actual positive instances that were incorrectly classified as negative, indicating the model's failure to detect positives.\nFNR =\nFN\nFN+TP\n(ix)\nwhere FN is the number of false negatives.\nFNR quantifies the proportion of actual positives incorrectly identified as negatives."}, {"title": "3. Results and discussions", "content": null}, {"title": "3.1. Data distribution", "content": "present the distribution of instance labels across different datasets used for model training. However, while the images allocated for broilers and hens maintain the same quantity, the instances within each image vary. shows the distribution of labels across the training datasets for broilers and hens, demonstrating a balanced representation of the various categories, ensuring the model encounters diverse instances during training. displays the distribution of labels in the pseudo-label training datasets, where the model generates additional labels based on its predictions. While this expands the dataset, slight imbalances in certain categories can be observed due to the challenges in automatically labeling some instances. Similarly, illustrates the label distribution in the training datasets for individual behaviors, maintaining a comprehensive representation of behaviors, although some categories may be slightly overrepresented or underrepresented. shows the distribution of pseudo-label datasets for individual behaviors, indicating the model's ability to generate plausible labels for less frequent behaviors, though minor inaccuracies and noise in the predictions are present. Overall, these distributions highlight the strengths of our approach in combining real and pseudo-labeled instances to create a robust training dataset."}, {"title": "3.2. Supervised and semi-supervised model comparison", "content": "The performance metrics of supervised and semi-supervised models for broiler and hen detection reveal notable distinctions in their accuracy, precision, recall, and mAP scores across different models. Among the supervised models, YOLOv8s demonstrated higher performance for broiler detection with a precision of 97.3%, recall of 98.7%, and mAP_0.50 of 99.3%. This was closely followed by YOLOv9s, which achieved similarly high mAP_0.50 of 99.3% for broilers but with slightly better precision and recall values of 98.1% and 98.6%, respectively. Similarly, YOLOv9s for hen detection achieved the best performance with a recall of 91.0% and mAP_0.50 of 96.3%, outperforming YOLOv8s with a recall of 88.1% and mAP_0.50 of 96.1%. The YOLOv8s-World model, with an overall mAP_0.50 of 97.5%, also performed well across both species, demonstrating the model's adaptability to various conditions. However, it was outperformed by YOLOv9s in terms of precision and recall for broilers. The RT-DETR-1 model, which represents a different architecture, displayed impressive performance for broiler detection (precision of 98.7%, recall of 98.2%) and hens' detection with recall at 93.4% and mAP_0.50 of 97.1%. In comparison, Faster-RCNN Restnet50 showed significantly lower performance across all categories, with an overall recall of just 66.8% and mAP_0.50 of 94.0%. These results highlight the superior capabilities of the YOLO family and RT-DETR-based models in real-time object detection tasks for poultry, outperforming Faster-RCNN in terms of both precision and recall."}, {"title": "3.3. Hybrid AI detection model performance evaluation using text-based embeddings", "content": "presents the performance metrics of various hybrid detection models for broilers and hens. Among the models, YOLO + CLIP and YOLOWorld + CLIP demonstrated competitive results, showcasing their ability to combine strong feature extraction with detection. The YOLOWorld + CLIP hybrid achieved an overall F1 score of 97.6%, with a precision of 99.1% and recall of 96.1%. This model exhibited superior performance for broilers, with a mean absolute error (MAE) of 0.3 and a root mean squared error (RMSE) of 0.6, highlighting its robustness in identifying broilers with minimal false negatives (FNR of 0.7). However, the model's performance declined slightly for hens, evidenced by a higher FNR of 7.2 and an RMSE of 2.6, suggesting room for improvement in hen detection. Comparatively, YOLO-World with the YOLOv8 backbone outperformed other hybrids' overall detection accuracy, achieving an F1 score of 98.7%, with a balanced precision (98.2%) and recall (99.4%). This model demonstrated excellent detection of both broilers and hens, with notably low MAE (0.3 for broilers and 0.6 for hens), underscoring its robustness and consistency."}, {"title": "3.4. Different amounts of image labeling", "content": "illustrate the performance metrics for broiler and hen detection across varying image quantities (I50, I100, I200, and I400). The results reveal a clear trend in the enhancement of model performance with the increase in the number of training images. For broiler detection, all key metrics, including precision, recall, F1-score, mAP_0.50, and mAP_0.50-0.95, consistently improved as the dataset size increased. At I50, the precision was 94.0%, and recall was 96.6%,"}, {"title": "3.5. Comparison based on different confidence levels", "content": "present the performance metrics for broiler and hen detection across various confidence score thresholds. The results highlight the impact of confidence score levels on detection performance. At a 12.5% confidence level, broiler detection achieved a precision of 98.2%, recall of 97.8%, and an F1 score of 98.0%, while hen detection had a precision of 91.9%, recall of 92.8%, and an F1 score of 92.3%. The overall performance for both classes was strong, with the model achieving a precision of 95.0% and an F1 score of 95.1%. As the confidence threshold increased to 25%, 50%, and 75%, precision and recall for broiler detection showed slight improvements, with the highest performance at the 75% confidence level. Specifically, at 75%, broiler detection reached a precision of 98.7%, recall of 95.7%, and an F1 score of 97.2%, while hen detection reached a precision of 95.8%, recall of 88.3%, and an F1 score of 91.9%. Therefore,"}, {"title": "3.6. Performance comparison of manual and conf50 methods", "content": "This study utilizes active learning to correct or label unlabeled or mislabeled images. Specifically, we compared two active learning approaches, the confidence threshold at 50% (conf50) and manual annotation with minimal human intervention, to determine which method yields better results. The confidence threshold was selected based on the comparison above, with conf50"}, {"title": "3.7. Behaviors detection and auto-labeling", "content": null}, {"title": "3.7.1. Behavior comparison with different models", "content": "comprehensively compare validation performance metrics for behavior detection across various models. The evaluation metrics encompass precision, recall, mAP_0.50, mAP_0.50-0.95, and F1-score across behaviors such as feeding, foraging, drinking, dustbathing, preening, perching, wing-flipping, mortality, pecking, mislaying, and piling. Among the YOLO models, YOLOv8s demonstrated higher precision in behaviors like pecking (92.0%) and mislaying (96.5%), although it performed poorly for wing-flipping (49.3%) and preening (56.9%). YOLOv9s slightly outperformed YOLOv8s' overall precision, particularly in pecking (93.5%) and mislaying (100.0%). In addition, it showed the highest recall in feeding, drinking, wing-flipping, mortality, pecking, and piling. Overall, precision, recall, mAP_0.05, and mAP_0.50-095 were the highest compared to other models. YOLOv10s showed notable success in dustbathing (F1-score of"}, {"title": "3.7.2. Behavior identification based on supervised and semi-supervised model", "content": "presents the validation performance metrics for behavior detection using semi-supervised conf50 active learning and supervised models, evaluated on precision, recall, mAP_0.50, mAP_0.50-0.95, and F1-score. The semi-supervised model trained with 200 labeled images, achieved a precision of 82.6% and an overall F1-score of 79.5%, excelling in detecting behaviors such as dustbathing (89.8%), wing-flipping (82.5%), mislaying (100.0%), and piling (97.7%). However, its recall and mAP_0.50-0.95 were notably low for behaviors like foraging (44.7% and 29.0%). In contrast, the supervised I-640 model outperformed in both precision and recall across most behaviors, achieving an overall F1-score of 79.5%. It exhibited a higher recall for perching (90.0%) and mortality (67.1%) and achieved superior mAP scores across most categories. While the supervised model demonstrated the advantages of a larger labeled dataset for improving performance across metrics, the semi-supervised model showed promising results, highlighting its potential for effective detection with fewer labeled images and trained on 2500 pseudo-labeled images. and presents the validation performance metrics results comparison for behavior detection across supervised and semi-supervised models."}, {"title": "3.7.3. Zero-shot hybrid fusion model", "content": "summarizes the prediction performance metrics of the optimal hybrid Al model (Y"}]}