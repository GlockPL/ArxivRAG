{"title": "Focus On What Matters: Separated Models For\nVisual-Based RL Generalization", "authors": ["Di Zhang", "Bowen Lv", "Hai Zhang", "Feifan Yang", "Junqiao Zhao", "Hang Yu", "Chang Huang", "Hongtu Zhou", "Chen Ye", "Changjun Jiang"], "abstract": "A primary challenge for visual-based Reinforcement Learning (RL) is to generalize\neffectively across unseen environments. Although previous studies have explored\ndifferent auxiliary tasks to enhance generalization, few adopt image reconstruction\ndue to concerns about exacerbating overfitting to task-irrelevant features during\ntraining. Perceiving the pre-eminence of image reconstruction in representation\nlearning, we propose SMG (Separated Models for Generalization), a novel ap-\nproach that exploits image reconstruction for generalization. SMG introduces\ntwo model branches to extract task-relevant and task-irrelevant representations\nseparately from visual observations via cooperatively reconstruction. Built upon\nthis architecture, we further emphasize the importance of task-relevant features for\ngeneralization. Specifically, SMG incorporates two additional consistency losses\nto guide the agent's focus toward task-relevant areas across different scenarios,\nthereby achieving free from overfitting. Extensive experiments in DMC demon-\nstrate the SOTA performance of SMG in generalization, particularly excelling\nin video-background settings. Evaluations on robotic manipulation tasks further\nconfirm the robustness of SMG in real-world applications. Source code is available\nat https://anonymous.4open.science/r/SMG/.", "sections": [{"title": "1 Introduction", "content": "Visual-based Reinforcement Learning (RL) has demonstrated remarkable success across various\ntasks, including Atari games [27, 11, 18], robotic manipulation [23, 9], and autonomous navigation\n[26, 46]. However, deploying visual-based RL algorithms in real-world applications requires a\nhigh generalization ability due to numerous factors that can induce distribution shifts between\ntraining and deployment scenarios, such as variations in lighting conditions, camera viewpoints, and\nbackgrounds. Many visual-based RL algorithms are prone to overfitting to the training observations\n[5, 34, 44], limiting their applicability in scenarios where fine-tuning with deployment observations\nis not allowed.\nTo address the generalization gap in visual-based RL, current studies primarily focus on utilizing data\naugmentation techniques [19, 20, 33] and exploring various auxiliary tasks [12, 3, 13]. However, few\nof the previous works successfully incorporate reconstruction loss to this field, which is commonly\nadopted in standard visual-based RL settings and has been demonstrated to improve the sample\nefficiency of RL agents [41, 10, 7]. This is because reconstructing the entire input observation can\nexacerbate the overfitting problem to task-irrelevant features and thus weaken the generalization ability.\nAlthough several works also explored extracting task-relevant features from visual observations"}, {"title": "2 Background", "content": "A Markov Decision Process (MDP) can be defined as a tuple $(S, A, p, r, \\gamma)$, where S is the state\nspace, A is the action space, $p : S \\times A \\times S \\rightarrow [0, 1]$ is the state transition probability function,\nr:S\u00d7A\u00d7S \u2192 R is the reward function, and \u03b3 \u2208 [0,1] is the discount factor. At each time\nstep t, the agent receives a state st \u2208 S, selects an action at \u2208 A, and then receives a reward\nrt \u2208 R. The agent's goal is to learn a optimal policy \u03c0(at|st) that maximizes the expected return\n$E_{(s_t,a_t)\\sim\\rho_{\\pi}} [\\Sigma_{\\tau=0}^{\\infty}\\gamma^{\\tau}r_t]$, where \u03c1\u03c0 defines the discounted state-action visitation of \u03c0.\nLearning an optimal policy from visual observations poses a substantial challenge for RL agents\ndue to the inherent partial observability of the environment, a characteristic of POMDPs (Partially\nObserved MDP). For one thing, at each timestep t, the visual observation of can only capture partial\ninformation about the true state st, as certain elements may be obscured in the image. For another,\nthe dimension of ot is much higher than that of st, which makes it difficult to utilize of directly for\npolicy learning.\nTo infer the true underlying state from visual observations, existing methods usually employ a\nparameterized encoder f to map a stacked frame sequence xt = (Ot', Ot'+1, ..., ot) to a compact\nlow-dimensional latent vector zt, which is then used as input by policy and value function. However,\ntraining the encoder solely to rely on the reward signal is demonstrated to sample inefficiency and\nmay lead to suboptimal performance [41]. To tackle this issue, various auxiliary tasks have been\nproposed to enhance encoder training, with one common choice being to extract features from pixels\nvia image reconstruction loss [7, 21, 2]. By adding another parameterized image decoder g, the\nreconstruction loss is defined by maximizing the likelihood function:\n$L_{recon} = -E_{o_t\\sim D}[E_{z_t\\sim f(o_t)} [log\\ g(o_t|z_t)]]$"}, {"title": "3 Approach", "content": "3.1 What Matters in a Reinforcement Learning Task?\nLearning to generalize is hard for RL agents,\nparticularly when utilizing an image reconstruc-\ntion loss. While images are rich in information,\nrequiring the agent to reconstruct the entire input\nobservation can lead the autoencoder network\nto overfit to features that are unrelated to the\ntask (e.g. colors, textures, and backgrounds). In\ncontrast, humans can accurately figure out what\nmatters visually when learning a new task. Even\nwhen colors or backgrounds are changed, hu-\nmans can still leverage the prior knowledge to\ncomplete the task by focusing on task-relevant\nfeatures.\nFormally, we decompose the latent representation z\u0142 into task-relevant part z\u0142 and task-irrelevant part\nz\u0142. These two representations are independent, as p(zt|ot) = p(z+|ot)p(z\u0142 |ot). The task-relevant\nrepresentation can be further subdivided into the \"control-relevant\" part, which is directly affected\nby the agent's actions (the arm); and the \"reward-relevant\" part, which is associated with the reward\nsignal (the arm and the target), both are crucial for policy learning."}, {"title": "3.2 Learning Task-Relevant Representations with Separated Models", "content": "3.2.1 Separated Models and Reconstruction\nThe representation learning objective of SMG is to maximize the mutual information I(ot; zt) be-\ntween the observation ot and the latent representation zt, and we further derive an image reconstruc-\ntion objective incorporating the combination of task-relevant representation z\u012b+ and task-irrelevant\nrepresentation z\u0142 as follows:\n$L_{recon} = \u2212I(o_t; z_t) < -E_{o_t~D} [E_{z^+_t ~f^+(o_t), z^-_t ~f^-(o_t)} [log\\ q(o_t|z^+_t, z^-_t)]]$"}, {"title": "3.2.2 Additional Loss Terms", "content": "Based on the separated models architecture, we define four additional loss terms to enhance the\nmodel's ability to distinguish between two types of representations. These include the mask ratio\nloss and background reconstruction loss, which supervise the model's pixel outputs; along with\nthe Q-value loss and empowerment loss, designed to consider the two properties of task-relevant\nrepresentation.\nMask ratio loss. To further refine the accuracy of mask prediction, we introduce a hyperparameter\np, termed the mask ratio, to constrain the proportion of the foreground part in the mask. As shown\nin Equation 3, we regard Lmask as an explicit form of an information bottleneck, as the percentage\np determines the number of pixels of of retained in the final reconstruction. This constraint forces\nf+ to prioritize the task-relevant parts of the observation during encoding. Empirical results in\nSection 4.4 demonstrate that Lmask facilitates learning a more precise mask.\n$L_{mask} = (\\frac{\\Sigma_{i, j} M_t(i, j)}{image\\_size^2} - p)^2$\nBackground reconstruction loss. Improving the precision of background prediction can conse-\nquently enhance the foreground as well. Since the foreground and background are complementary,\nproviding supervision for the background prevents the foreground from learning all parts of the\nobservation. Therefore, we add additional supervision to the task-irrelevant representation z\u0142. \u03a4\u03bf\nachieve this, we propose a new type of data augmentation called attribution augmentation tailored for\nSMG, as illustrated in Figure 4b. This augmentation involves augmenting the raw observation ot with\nits corresponding predicted mask Mt via $T_{attrib}(o_t) = o_t \\odot M_t + \\epsilon \\odot (1 \u2013 M_t)$, where e represents a\nrandomly sampled image. This simulates the video-background setting in deployment scenarios. We\ndefine the background reconstruction loss Lback as follows:\n$L_{back} = -E_{o_t~D}[E_{z^-_t~f^-(T_{attrib}(o_t))} [log\\ g^-(\\epsilon|z^-_t)]]$\nQ-value loss. Recall that the task-relevant representation z\u012bt has two key properties: reward-relevant\nand control-relevant. Satisfying the former is relatively straightforward, as the representation zt is\nused for policy learning. Through the Bellman residual update objective [35] outlined in Equation 5,\nzt will progressively enhance its correlation with the reward signal.\n$L_q = E_{r~D}[(Q(z^+_t, a_t) - (r_t + \\gamma V (z^+_{t+1})))^2]$\nEmpowerment loss. For the control-relevant property, we integrate an empowerment term\nI(at, z++12+) [28] based on conditional mutual information, which quantifies the relevance be-\ntween the action and latent representation. Maximizing the empowerment term further leads to\nmaximizing a variational lower bound q(at|z++1, 2+) as shown in Equation 6. This objective neces-\nsitates that at is predictable when two neighboring representations are known. We implement this\nobjective by incorporating an inverse dynamic model.\n$L_{action} = -I(a_t, z^+_{t+1}|z^+_t) \\leq -E_{p(a_t, z^+_{t+1}|z^+_t)} [log\\ q(a_t|z^+_{t+1}, z^+_t)]$"}, {"title": "3.3 Generalize Task-Relevant Representations with Separated Models", "content": "Utilizing the separated models architecture, SMG can successfully extract task-relevant representa-\ntions from raw observations. Nevertheless, the agent still lacks the ability to generalize effectively"}, {"title": "3.4 Overall Objective", "content": "Our proposed separated models architecture can seamlessly integrate as a plug-and-play module\ninto any existing off-policy RL algorithms. In this work, we leverage SAC [8] as the base algorithm.\nThroughout the training phase, SMG iteratively performs exploration, critic update, policy update,\nand auxiliary task update. We define the critic loss Lcritic as the sum of the Q-value loss Lq and the\nQ-value consistency loss Lq_consist:\n$L_{critic} = L_q + L_{q\\_consist}$\nAdditionally, the auxiliary loss Laux comprises five previously mentioned loss terms:\n$L_{aux} = \\lambda_{recon} L_{recon} + \\lambda_{mask} L_{mask} + \\lambda_{back} L_{back} + \\lambda_{action} L_{action} + \\lambda_{fore\\_consist} L_{fore\\_consist}$"}, {"title": "4 Experimental Results", "content": "4.1 Setup\nWe benchmark SMG against the following baselines: (1) SAC\n[8], serving as the foundational algorithm for all other base-\nlines; (2) DrQ [19], utilizing random shift augmentation; (3)\nSODA [12], incorporating a consistency loss on latent repre-\nsentations; (4) SVEA [14], focusing on stabilizing Q-values;\n(5) SRM [15], proposing a novel data augmentation technique;\n(6) SGQN [3], the previous SOTA method integrating saliency\nmaps into RL tasks. We reproduce the results using the same\nsettings reported in the original papers, with the exception of\nsetting the batch size to 64 for all methods. Additionally, all\nseeds."}, {"title": "4.2 DMControl Results", "content": "We first conduct experiments on five selected tasks from DMControl [36] and adopt the same\nevaluation setting as DMControl Generalization Benchmark [12] (DMC-GB) used, which contains\nrandom-colors and video-background modifications across four different levels: color-easy, color-\nhard, video-easy and video-hard. We train all\nmethods for 500k steps (except walker-stand for 250k, as it converges faster) on the training setting\nand evaluate the zero-shot generalization performance on the four evaluation settings.\nTo provide a clear explanation of how SMG reconstructs images, we present the image outputs of\nwalker-walk and cheetah-run after 500k training steps of training. The\nlast four columns illustrate the model outputs necessary for reconstructing the evaluation observations.\nThe predicted attribution (the fifth column) highlights the extracted task-relevant area, which shows\nSMG accurately depicts the attribution of the input observation while omitting the task-irrelevant\nelements such as the skybox, the floor, and even the random color variation. This indicates that the\ntask-relevant representation zit contains only the information required to accomplish the task, which\nis crucial for generalization. Note that we aim to maintain the similarity between Attrib(\u03c4(ot)) and\nAttrib(ot), even in random-color settings. As shown by the first row of color-hard setting, SMG\npredicts a yellow attribution despite the input evaluation observation being orange."}, {"title": "4.3 Robotic Manipulation Results", "content": "To further validate SMG's applicability to more realistic tasks, we conduct experiments on two\ngoal-reaching robotic manipulation tasks [17], including peg-in-box and reach, and following similar\ngeneralization settings used in [3]. We train all methods for 250k\nsteps and use random convolutions [22] as the data augmentation for baseline methods, as it aligns\nbetter with the testing scenarios. SMG continued to use hybrid augmentation as previously mentioned."}, {"title": "4.4 Ablation Study", "content": "In order to explore the role played by different loss terms in SMG, we conduct an ablation study\nin DMControl tasks. The results indicate that every loss term contributes\nsignificantly to the final performance. Notably, Lq_consist exhibits the most substantial impact on\nperformance, highlighting the importance of maintaining stable Q-value estimation in generalization\ntasks. Moreover, the performance drop without Lback or Lmask is around 20% to 30%, underlining\nthe importance of attribution augmentation in enhancing SMG's generalization in video-background\nsettings, as the two loss terms directly affect the quality of the attribution augmentation. Additionally,\nLaction aids in learning a better task-relevant representation. As for Lfore_consist, it also contributes to\nimproving generalization ability, particularly in relatively challenging tasks where the performance\nimprovement ranges from 15% to 25%."}, {"title": "5 Related Work", "content": "Improving generalization ability of RL agents has drawn increasing attention in recent years.\nResearchers primarily explore two aspects: using data augmentation techniques to inject useful priors\nwhen training [20, 15, 16, 22, 14, 32, 38] and employing various auxiliary tasks to guide the learning\nprocess [13, 3, 1, 42, 40, 12]. For example, Hansen and Wang [12] regularize the representations\nbetween observations with its augmented view through an auxiliary prediction task; Hansen et al. [14]\nstabilize Q-values via delicately design the data augmentation process; Bertoin et al. [3] introduce\nsaliency maps to visualize the focus of Q-functions; Wang et al. [40] extract the foreground objects\nby employing a segment anything model. Orthogonal to existing works, we argue that focusing the\nRL agent on task-relevant features across diverse deployment scenarios can substantially boost the\ngeneralization capability. We propose a novel reconstruction-based auxiliary task to achieve this goal.\nDecision-making based on task-relevant features can substantially enhance the performance\nand robustness of RL agents [4, 45, 43, 29]. Bharadhwaj et al. [4] use an empowerment term to\ndistill control-relevant features from the task; Zhu et al. [45] bolster the resilience of RL agents\nby regularizing the posterior predictability; Zhang et al. [43] learns compact representations by\nbisimulation metrics. Additionally, methods utilizing separated model architectures to extract different\ntypes of features simultaneously have been proposed [6, 39, 30, 25, 37]. For instance, Wang et al.\n[39] decompose the latent state into four parts based on their interaction with actions and rewards;\nPan et al. [30] leverage both controllable and non-controllable states in policy learning; Wan et al.\n[37] apply task-relevant features to imitation learning. Our work also employs separated models.\nHowever, we prudently design this architecture in a model-free setting and propose novel loss terms\nto enhance the accuracy of image predictions."}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we propose SMG for visual-based RL generalization and show its superiority in sample\nefficiency, stability, and generalization through extensive experiments. The success of SMG can\nbe attributed to two key factors: (i) a delicately designed reconstruction-based auxiliary task with\nseparated models architecture, which enables the RL agent to extract task-relevant and task-irrelevant\nrepresentations from visual observations simultaneously; (ii) two consistency losses to further guide\nthe RL agent's focus under deployment scenarios. We believe that the proposed method can be\napplied to a wide range of tasks.\nSMG is particularly well-suited for robotic manipulation tasks in realistic scenarios. However, when\nthe observation contains too many task-relevant objects, the complexity of accurately learning a\nmask increases. This can lead to a decline in SMG's performance. For instance, in an autonomous\nnavigation task, the presence of numerous pedestrians in the view makes it challenging to accurately\nmask all of them.\nThe future work includes exploring more advanced backbones for task-relevant feature extraction,\ntaking into account the generalization on non-static camera viewpoints and the test of SMG on\nrealistic tasks to verify its generalization ability in real applications."}, {"title": "A Derivations", "content": "We formulate the representation learning objective as a variational lower bound of the mutual\ninformation [31, 24] between the observation of and the representation zt. By considering the\nindependence between the task-relevant and task-irrelevant representations, we can decompose the\nmutual information as:\n$I(o_t; z_t) = E_{p(o_t,z_t)} [log\\ p(o_t|z_t) \u2013 log\\ p(o_t)]$\n$\\geq E_{p(o_t,z_t)} [log\\ p(o_t|z_t)]$\n$\\geq E_{p(o_t,z_t)} [log\\ p(o_t|z_t)] - E_{p(z_t)} [D_{KL}(P(o_t|z_t)||q(o_t|z_t))]$\n$= E_{p(z_t,o_t)} [log\\ q(o_t|z_t)]$\n$= E_{q(z_t|o_t)p(o_t)} [log\\ q(o_t|z_t)]$\n$= E_{q(z^+_t|o_t)q(z^-_t|o_t)p(o_t)} [log\\ q(o_t|z^+_t, z^-_t)]$\n$= E_{o_t~D}[E_{z^+_t~f^+(o_t),z^-_t~f^-(o_t)} [log\\ q(o_t|z^+_t, z^-_t)]]$\nWe use the empowerment term $I(a_t, z^+_{t+1}|z^+_t)$ introduced in [28] to quantify the information con-\ntained in the representation $z^+_{t+1}$ about the selected action at, in goal of enhance the control-relevant\nproperty of the task-relevant representation $z^+_t$. We derive the variational lower bound of the empow-\nerment term as:\n$I (a_t, z^+_{t+1}|z^+_t) = E_{p(a_t|z^+_t)} [log\\ \\frac{p(a_t|z^+_{t+1}, z^+_t)}{p(a_t|z^+_t)} ]$\n$= E_{p(a_t|z^+_t)} [log\\ \\frac{p(a_t|z^+_{t+1}, z^+_t)}{q(a_t|z^+_{t+1}, z^+_t)} \\frac{q(a_t|z^+_{t+1}, z^+_t)}{p(a_t|z^+_t)} ]$\n$= E_{p(a_t|z^+_t)} [log\\ \\frac{p(a_t|z^+_{t+1}, z^+_t)}{q(a_t|z^+_{t+1}, z^+_t)} + log\\ \\frac{q(a_t|z^+_{t+1}, z^+_t)}{p(a_t|z^+_t)} ]$\n$\\geq E_{p(a_t, z^+_{t+1}|z^+_t)} [log\\ q(a_t|z^+_{t+1}, z^+_t)] - \\int p(z^+_t)p(a_t)p(z^+_{t+1}|z^+_t, a_t)log\\ p(a_t|z^+_t)$\n$= E_{p(a_t, z^+_{t+1}|z^+_t)} [log\\ q(a_t|z^+_{t+1}, z^+_t)] + E_{p(z^+_t)p(z^+_{t+1}|z^+_t,a_t)} [H(p(a_t|z^+_t))]$\n$\\geq E_{p(a_t, z^+_{t+1}|z^+_t)} [log\\ q(a_t|z^+_{t+1}, z^+_t)]$\nIn practice, we integrate a parameterized inverse dynamic model to predict the action at based on\nthe two continuous representations zit and 21. We employ the Mean Squared Error (MSE) loss to\nguide the training of the inverse dynamic model."}, {"title": "B Pseudocode", "content": "Algorithm 1 SAC with Separated Models\nDenote network parameters \u03b8, mask ratio \u03c1, batch size N, replay buffer B\nDenote policy network \u03c0\u03b8, foreground encoder f +, background encoder f\u2212\nforeach iteration time step do\na, o', r \u223c \u03c0\u03b8(f+(o)), P(o, a), R(o, a)\nBBU (o, a, r, o')\nforeach update time step do\n{oi, ai, ri, o\u00bf}i\u2208[1,N] \u223c B\not, maski \u223cf+(oi)\noi \u223cf\u2212(oi)\noaugi maski + \u03f5 * (1 \u2013 maski) // \u03f5 is sampled from image dataset\nLrecon \u2190 L(oi, o+ * maski + o\u2212 * (1 \u2013 maski)) // Equation 2\nLfore_consist \u2190 L(ot, f+(oaug)) // Equation 7\nLback \u2190 L(\u03f5, f\u2212(oaug)) // Equation 4\nLaction \u2190 L(oi, o+', a) // Equation 6\nLmask \u2190 L(maski, \u03c1) // Equation 3\nLq_consist \u2190 L(Q\u03b8(f+(oi), a), Q\u03b8(f+(oaug), a)) // Equation 8\nLaux \u2190 Lrecon + Lfore_consist + Lback + Laction + Lmask // auxiliary loss\nLcritic \u2190 Lq + Lq_consist // critic loss\nupdate \u03b8 with Lactor, Lcritic, Laux\nend for\nend for\nLq, Lactor are defined by SAC"}, {"title": "C More Experiment Details", "content": "C.1 Computing Hardware\nWe conduct all experiments on a single machine equipped with an AMD EPYC 7B12 CPU (64 cores),\n512GB RAM, and eight NVIDIA GeForce RTX 3090 GPUs (24 GB memory). We report the training\nwall time of different methods on DMControl tasks in Table 4.\nC.2 Network Architecture\nWe reproduce all baseline methods with the official code of DMC-GB (https://github.com/\nnicklashansen/dmcontrol-generalization-benchmark) published by Nicklas Hansen, and\nwe build our model on top of the SAC implementation. We use the same encoder and decoder\narchitecture as the baseline methods to ensure a fair comparison."}, {"title": "C.3 Hyperparameters", "content": "We report the hyperparameters used in our experiments in Table 5. We use the same hyperparameters\nfor all seven tasks, except the action repeat and the mask ratio p. The Laux in SMG comprises five loss\nterms, which seems challenging to balance the weights. However, through experiments, we found\nthat setting average weights for Lrecon, Lmask, Laction, Lback is sufficient to achieve good performance\n(except the \u03bbback is set to 2 since the background model should train to fit more complex images).\nRegarding the \u03bbfore, a too-large weight would lead to the model overfitting the inaccurate attribution\npredictions in the early stage (as we use the model output under raw observation as ground truth), so\nwe set it to 0.1."}, {"title": "D More Experiment Results", "content": "D.1 Training Curves\nWe present the training curves for all seven tasks in Figure 11, including four evaluation settings of\nDMControl and Robotic Manipulation tasks. As depicted in the figure, SMG demonstrates notably\nfaster convergence and higher asymptotic performance across nearly all training and evaluation\nsettings, showcasing the effectiveness of the reconstruction-based auxiliary task in enhancing sample\nefficiency. SMG exhibits superiority, particularly in the video-hard setting of DMControl tasks, where\nthe performance of other methods drops evidently when random videos replace the background.\nAdditionally, the figure underscores the considerable challenge posed by Robotic Manipulation tasks,\nwith only SMG and SGQN successfully achieving zero-shot generalization in evaluation settings."}, {"title": "D.2 More Table Results", "content": "Moreover, SMG shows more stable performance across different evaluation settings, which is crucial\nfor real-world applications."}, {"title": "E More Ablation Study", "content": "We report the effect of removing each loss term to the average performance across five evaluation\nsettings in DMControl tasks in Table 9. Compared with Table 3, Lq_consist still exhibits the most\nsubstantial impact on performance, though the performance drop is slightly smaller. This may be\nbecause the random-color settings do not shift the observations heavily compared to the video-\nbackground settings, so the Q-value estimation is less affected. A similar phenomenon is observed\nin Lback and Lmask, indicating that attribution augmentation is more crucial in video-background\nsettings.\nThe mask ratio p is a hyperparameter that controls the expected proportion of the foreground area.\nHowever, this parameter is an empirical choice and may not precisely match the actual proportion of\na given task. To investigate the sensitivity of SMG to the mask ratio, we conduct experiments with\ndifferent p values in the walker-walk task of the video-hard setting. We select p values ranging from\n0.02 to 0.1 with an interval of 0.02 and report the average performance across five evaluation settings\nin Figure 9. The results indicate that variations do not significantly influence SMG in the mask ratio,\nas p values between 0.04 and 0.08 achieve similar performance. Moreover, when p is too small (0.02)\nor too large (0.1), the performance drops around 6% compared to the optimal p value (0.06). We also\nreport the predicted masks of different p values in the figure. As p increases, the predicted masks\nstart to include background areas, so a too high value leads to decreased performance. Conversely,\nwhen p is too small, the mask depicts an inaccurate foreground area (e.g. the legs of the walker with\np = 0.02), resulting in a performance drop as well."}, {"title": "F More Discussion", "content": "F.1 Bootstrapping Process in SMG\nThe attribution augmentation utilized in SMG requires the model to predict an accurate mask, and the\nforeground consistency loss also requires a precise attribution prediction of the model. This might\nseem contradictory, as the model struggles to make meaningful predictions in the early stages, which\nmeans it cannot satisfy the two requirements immediately. We dig into the training process of SMG"}, {"title": "F.2 Comparison with Related Work", "content": "TIA [6] also designs two model branches to capture task and distractor features, similar to our\nseparated models architecture. However, SMG differs from TIA in several essential aspects: (i) TIA\nis a model-based method focusing on eliminating task-irrelevant distractors in training observations,\nwhile SMG aims to utilize task-relevant features across diverse deployment scenarios to enhance\nthe generalization capability of RL agents; (ii) SMG operates in a model-free setting, which can\nbe more efficient to train and more flexible for applying data augmentation techniques; (iii) TIA\nuses a background-only reconstruction loss and requires the background model to reconstruct the\nfull observation, which may cause the background branch to overly fit task-relevant features. In\ncontrast, SMG addresses this issue by introducing attribution augmentation images to supervise the\nbackground model; (iv) SMG utilizes mask ratio loss to learn a more precise mask, while the masks\nin TIA are prone to containing distractors, as reported in its original paper.\nSODA [12] also improves the generalization ability of RL agents by regularizing the representations\nbetween observations and their augmented views, similar to the consistency losses in SMG. However,\nSODA implements this by simply minimizing the L2 distance between the two representations, which\nimposes a too rigid constraint and lacks interpretability. We achieve this by introducing Q-value\nconsistency loss and foreground consistency loss, which provide more explainable supervision and\nadditionally improve the stability of Q-values and predicted attributions.\nNote that the core idea underlying the Q-value loss in Equation 8 differs significantly from the\nconsistency regulation objective proposed by SGQN [3]. SGQN focuses on prioritizing pixels that\nbelong to the saliency map during encoding, primarily to enhance the accuracy of Q-value estimation\nunder raw observations. In contrast, SMG treats the Q-values under raw observations as the ground\ntruth and aims to achieve consistency between these Q-values and those obtained under augmented\nobservations. Thus, we additionally use a stop-gradient operation."}, {"title": "NeurIPS Paper Checklist", "content": "1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper's contributions and scope?\nAnswer: [Yes]\nJustification: The abstract and introduction clearly state the contributions of the paper.\nGuidelines:\n\u2022 The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n\u2022 The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n\u2022 The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: The limitations and future work are discussed in the Section 6.\nGuidelines:\n\u2022 The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.\n\u2022 The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n\u2022 The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n\u2022 The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n\u2022 If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n\u2022 While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren't acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory Assumptions and Proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [Yes]"}, {"title": "4"}]}