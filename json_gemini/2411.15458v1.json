{"title": "TANGNN: a Concise, Scalable and Effective Graph Neural Networks with Top-m Attention Mechanism for Graph Representation Learning", "authors": ["Jiawei E", "Yinglong Zhang", "Xuewen Xia", "Xing Xu"], "abstract": "In the field of deep learning, Graph Neural Networks (GNNs) and Graph Transformer models, with their outstanding performance and flexible ar-chitectural designs, have become leading technologies for processing struc-tured data, especially graph data. Traditional GNNs often face challenges in capturing information from distant vertices effectively. In contrast, Graph Transformer models are particularly adept at managing long-distance node relationships. Despite these advantages, Graph Transformer models still en-counter issues with computational and storage efficiency when scaled to large graph datasets. To address these challenges, we propose an innovative Graph Neural Network (GNN) architecture that integrates a Top-m attention mech-anism aggregation component and a neighborhood aggregation component, effectively enhancing the model's ability to aggregate relevant information from both local and extended neighborhoods at each layer. This method not only improves computational efficiency but also enriches the node features, facilitating a deeper analysis of complex graph structures. Additionally, to assess the effectiveness of our proposed model, we have applied it to cita-tion sentiment prediction\u2014a novel task previously unexplored in the GNN field. Accordingly, we constructed a dedicated citation network, ArXivNet. In this dataset, we specifically annotated the sentiment polarity of the cita-tions (positive, neutral, negative) to enable in-depth sentiment analysis. Our approach has shown superior performance across a variety of tasks including vertex classification, link prediction, sentiment prediction, graph regression, and visualization. It outperforms existing methods in terms of effectiveness, as demonstrated by experimental results on multiple datasets. The code and ArXivNet dataset are available at https://github.com/ejwww/TANGNN.", "sections": [{"title": "1. Introduction", "content": "In the field of deep learning, dealing with structured data has always been a hot topic of research. Structured data, especially graph data, plays a criti-cal role in a variety of application scenarios, such as social networks (Perozzi et al., 2014), citation network analysis (Saier et al., 2023; Saier & F\u00e4rber, 2020), recommendation systems (Yu et al., 2014), knowledge graph repre-sentation (Lin et al., 2015), text embedding Tang et al. (2015) and bioinfor-matics (Armah-Sekum et al., 2024). However, traditional neural network ar-chitectures face numerous challenges when handling graph datasets. Against this backdrop, Graph Neural Networks (GNNs) models have become frontier technologies due to their exceptional performance and flexible architectural design.\nGraph neural networks(GNN) directly utilize the topological structure of graphs by adjacency matrices or aggregating node features, they perform well in many graph data processing tasks, such as vertex classification (Tang et al., 2016) and link prediction (Gao et al., 2011). However, they have cer-tain limitations when dealing with node information with farther hops. In traditional graph neural networks, the receptive field of each node is lim-ited to its direct neighbors. In order to expand the receptive field of the model, it is necessary to increase the number of layers in the network, so that nodes can indirectly contact further neighbors. However, as network layers increase, node representations become less distinct, complicating the task of accurately labeling nodes by classifiers (Chen et al., 2020). More-over, the addition of layers leads to a decrease in training accuracy, and the emergence of oversmoothing issues (Nguyen et al., 2023; Yang et al., 2023; Min et al., 2020), which collectively degrade the performance of graph neural networks.\nConversely, Transformers Vaswani et al. (2017) introduce a self-attention mechanism that allows the model to directly compute dependencies between all elements on a global scale. Although Transformers have gained signif-icant recognition and success across a variety of domains such as machine translation (Wang et al., 2020), image segmentation (You et al., 2023b,a), applying standard Transformers directly to graph data processing still poses challenges. Transformers are primarily designed for processing sequential data. As the input sequence lengthens, the demands for computation and storage increase quadratically, leading to a dramatic increase in resource con-sumption, which may reduce the efficiency of the self-attention mechanism of Transformers in processing long sequences (Beltagy et al., 2020; Moro et al., 2023). Existing graph Transformers (Min et al., 2022) have intro-duced positional and structural encodings to enhance the model's perception of node positions and structures within a graph, enabling the model to un-derstand not only the features of nodes and edges but also their relative importance and relationships within the entire graph structure. For exam-ple, Graphormer (Ying et al., 2021) introduced centrality encoding, spatial encoding, and edge encoding, which help it better handle the graph data structure. TransGNN (Zhang et al., 2023) introduced multiple different po-sitional encodings to help the model capture the structural information of the graph. However, the introduction of these encodings also brings significant computational and storage overhead. This is primarily reflected in additional preprocessing steps, increased model complexity and memory requirements, and potentially extended training times. Especially when dealing with large-scale graph data, these overheads can significantly impact the scalability and practicality of the model.\nIn this paper, we propose a concise, scalable and effective GNN, TANGNN (Top-m Attention and Neighborhood Aggregation GNN). It not only ef-fectively integrates local and global information, maintains efficient com-putation but also significantly enhances the model's expressive power and performance, making it well-suited for large-scale graph data. The main contributions of this paper are as follows:\n\u2022 We propose a new GNN framework named TANGNN. It introduces two distinct aggregation components: a Top-m attention mechanism aggregation component and a neighborhood aggregation component. which effectively enhancing the model's ability to aggregate relevant"}, {"title": "2. Related work", "content": "Recently, a variety of new approaches have been developed in the field of graph representation learning, which are designed to increase the expressive capabilities of graph structures.\nMethods based on graph neural networks: Graph Neural Networks (GNNs) are a popular research direction in the current deep learning field, mainly used for processing graph-structured data. Globally, research in this field is rapidly developing and showing a rich array of application prospects. In recent years, significant progress has been made in the study of GNNs and their variants. Methods like GCN (Kipf & Welling, 2017) and SGCN (Wu et al., 2019) aggregate neighbor node information through multiple convolu-tion layers. While these methods are effective at capturing the local structure around nodes, they struggle to adequately handle global structural aspects. Some approaches concentrate on enhancing techniques for neighbor sampling. GraphSAGE (Hamilton et al., 2017) is a classic method that reduces compu-tational costs by aggregating information through the random sampling of neighbor nodes, facilitating node embedding learning on large-scale graphs. Other methods, such as GAT (Veli\u010dkovi\u0107 et al., 2018) and E-ResGAT (Brody et al., 2022), enhance this process by introducing importance weights or at-tention mechanisms. These techniques assign weights based on the degree of neighbor nodes or node similarity, and then aggregate node features using attention weights at each layer. GIN (Xu et al., 2019) mixes the original features of a graph node with those of its neighbors after each hop aggrega-tion operation, introducing a learnable parameter to adjust its own features, which are then added to the aggregated features of neighboring nodes, en-sures that central nodes remain distinguishable from their neighbors. JK-Net (Xu et al., 2018) and GraphSAGE++ (Jiawei et al., 2024) strengthen information aggregation in graph neural networks by using node representa-tions from different layers, allowing the network to learn the node's multiscale characteristics more effectively, which allow each node to adaptively select its optimal neighbor aggregation range, not only enhancing the capture of local and global information but also optimizing the common oversmooth-ing problem in deep networks. However, despite these strategies enhancing node information representation, these models are still limited by their re-ceptive field when dealing with distant neighbor nodes in the graph. This limitation means that capturing information from distant nodes, especially in large-scale or structurally complex graphs, remains a challenge.\nMethods based on random walk: DeepWalk (Perozzi et al., 2014) is an unsupervised learning technique that combines random walks with language models. It generates node sequences using truncated random walks, similar to sentences. These sequences are then processed through a Skip-gram model to learn vector representations of the nodes, capturing both local and global structural features of the graph. Node2Vec (Grover & Leskovec, 2016), build-ing on DeepWalk (Perozzi et al., 2014), introduces a flexible biased random walk strategy that balances breadth-first search (BFS) and depth-first search (DFS) strategies, making the exploration of graph structures more flexible.\nThus, Node2Vec (Grover & Leskovec, 2016) is able to capture homophily and structural equivalence patterns, improving the representation of community structures and structural roles. InfiniteWalk (Chanpuriya & Musco, 2020) learns to represent nodes in a graph through a special mathematical transfor-mation of the graph's deep structure. This method, based on extending one of the parameters of the DeepWalk (Perozzi et al., 2014) algorithm to infinity, enables it to better capture and utilize information in the graph, performing as well as other advanced methods in tasks such as node classification. Addi-tionally, the MIRW (Berahmand et al., 2022) algorithm considers the mutual influences between nodes to better capture network complexity, unlike tra-ditional random walk strategies that treat all nodes and links as equally important, failing to fully reflect the graph's overall structure. CSADW (Be-rahmand et al., 2021) enhances link prediction capabilities in social networks by combining a new transition matrix with structural and attribute similar-ities. This approach improves traditional techniques, ensuring that random walks are more likely to interact with structurally similar nodes, thereby capturing both structural and non-structural similarities. While these meth-ods are very suitable for capturing community structures and enhancing link prediction capabilities in graphs, they mainly focus on local structural infor-mation, have limited perception of global structures, and are less efficient on large-scale graph data.\nMethods based on graph transformer: The transformer model has gar-nered widespread attention due to its outstanding performance in the field of natural language processing, mainly relying on its core component\u2014the self-attention mechanism\u2014to handle dependencies in sequence data. The core idea of the self-attention mechanism is to compute attention scores for each element in the sequence regarding other elements, thereby dynamically adjusting each element's representation. When this mechanism is applied to graph data, it can naturally capture the complex interactions between nodes in the graph. GraphTransformer (Dwivedi & Bresson, 2021) adopts a unique graphical attention mechanism that uses node adjacency relationships to cal-culate attention scores, thus making the attention distribution more focused on the local structure of the nodes. This method not only captures direct connections between nodes but also enhances the model's overall perception of graph structures by introducing positional encodings through Laplacian eigenvectors. Graphormer (Ying et al., 2021) introduced centrality encod-ing, spatial encoding, and edge encoding, enabling it to better handle graph data structures. TransGNN (Zhang et al., 2023) combines Transformer and graph neural networks, alternating Transformer layers and GNN layers in the model, while introducing three types of positional encodings based on short-est paths, node degrees, and PageRank to help the Transformer layers cap-ture the strusctural information of the graph, enhancing the model's perfor-mance in processing graph data. DeepGraph (Zhao et al., 2023) introduced a substructure-based local attention mechanism, incorporating additional sub-structure tokens in the model and applying local attention to nodes related to these substructures, solving the performance bottleneck faced by traditional graph Transformers in processing deep graph structure data by increasing the number of layers. NodeFormer (Wu et al., 2022) optimizes the algorithmic complexity of message passing using a kernelized Gumbel-Softmax operator, reducing it from quadratic to linear complexity. NAGphormer (Chen et al., 2023) and SGFormer (Wu et al., 2024) are both optimized for processing large-scale graph data, with NAGphormer Chen et al. (2023) adopting an in-novative Hop2Token module by viewing each node as a sequence composed of its multilevel neighborhood features, and its attention-based readout mecha-nism optimizes learning of the importance of different neighborhoods, signif-icantly improving the accuracy of node classification. SGFormer (Wu et al., 2024) simplifies the structure of graph Transformers by adopting a single-layer global attention mechanism to effectively capture latent dependencies between nodes, significantly reducing computational complexity. However, these graph Transformers introduce significant computational and storage overhead to the model through encoding, especially when processing large-scale graph data, which can greatly impact the model's scalability and prac-ticality."}, {"title": "3. Problem definition", "content": "This section provides some basic definitions.\nDefinition 1. Given a graph $G = (V,E,X)$, where $V= {v_1, v_2, ..., v_N}$ represents a set of N nodes. $N = |V|$ is the total number of nodes in the graph, and $|E|$ is the number of edges. The edge set $E = {e_{i,j}}_{i,j=1}^N$ represents all edges in the graph, when $e_{i,j} = 1$ if there is an edge between $v_i$ and $v_j$, and $e_{i,j} = 0$ otherwise. The matrix $X \\in R^{N \\times D}$ denotes the feature matrix for all notdes, with $x_v \\in R^D$ representing the feature vector of a node v.\nDefinition 2. For a given graph with N nodes, the goal of graph represen-tation learning is to develop a mapping function $f : V \\rightarrow R^D$, that encodes each node in the graph into a low dimensional vector of dimension D, where $D \\ll |V|$. This mapping ensures that the similarities between nodes in the graph are preserved within the embedding space.\nDefinition 3. The feature of vertex v at the ith layer is denoted by $h_v^i$, which is derived from neighborhood aggregation component, and $h_v^{i'}$, which is derived from a Top-m mechanism aggregation component. The initial feature vectors at layer 0 are $g_v^0 = h_v^0 = h_v^{0'} = X$. The output vector $g_v^i$ for node v at layer i is formed by first concatenating $h_v^i$ and $h_v^{i'}$, and then processing the combined features through a multilayer perceptron (MLP), as expressed by $g_v^i = MLP([h_v^i||h_v^{i'}])$. $N(v) = {u \\in V : (u,v \\in E)}$ represents the neighborhood set of vertex v in the graph."}, {"title": "4. TANGNN", "content": "In this section, we first present the framework of TANGNN, then elab-orate on the details of each component of TANGNN, and introduce some variants and algorithm optimizations of this framework.\nTraditional GNNs only aggregate information from a node's neighbors, thereby limiting their receptive field and affecting their performance. The common practice is to increase the number of layers in GNNs, allowing nodes to aggregate information from farther neighbors, thus expanding the model's receptive field. However, this increase in layers leads to the problem of over-smoothing. Transformer models have a global receptive field, but as the sequence length increases, the computational complexity and storage require-ments grow quadratically, so introducing the global attention mechanism of the Transformer model into GNNs faces the same issues. Effectively ex-panding the receptive field of GNNs and quickly aggregating information is a challenging problem. The key issue is how to effectively select the appropriate receptive field.\nTo address the above problems, we have creatively constructed a new GNN framework (as shown in Figure 1). This GNN has L layers, each consisting of two components: Top-m attention mechanism aggregation and neighbor aggregation.\nFor the red node v in the graph, based on the node representation cal-culated from the previous layer, the Top-m nodes most similar to v are computed (the yellow nodes in the graph). Since the node representation ag-gregates the graph's structure and feature information through the preceding network layer, the Top-m nodes most similar to v are essentially calculated based on the similarity of graph structure and feature information. Therefore, the Top-m attention mechanism component aggregates the graph structure and feature information contained in the Top-m similar nodes to v.\nThe neighbor aggregation component fundamentally performs traditional GNN aggregation. Inspired by GraphSAGE, it uses random sampling of a fixed number of neighbors (the blue nodes in the graph) for aggregation, which can greatly increase the training speed of the model without reducing its performance.\nThe vector representation $h_v^i$ generated by the neighbor aggregation in the ith layer is concatenated with the vector representation $h_v^{i'}$ generated by the Top-m attention mechanism aggregation component to obtain the concatenated vector for this layer $[h_v^i, h_v^{i'}]$. Through an MLP, $g_v^i$ is obtained, which is then used as input for the next layer until the Lth layer to obtain the final vector representation $g_v^L$, enabling the model to better capture both local and extended neighborhoods' information.\nThe Top-m nodes and a fixed number of neighbors of v constitute the receptive field of node v. These nodes, based on graph structure and feature information, are most relevant to v and maintain a small scale, addressing the key issue of how to effectively select the appropriate the receptive field. Additionally, the fixed number of receptive field nodes is beneficial for model training and computation.\nAlgorithm 1 is the forward propagation process of TANGNN. Lines 5 to 6 respectively call the forward propagation functions of the neighborhood ag-gregation component and the Top-m attention mechanism aggregation com-ponent (Algorithms 2 and 3), to calculate the vector representations $h_v^i$ and $h_v^{i'}$ of node v at the ith layer. These are then concatenated and processed through a multilayer perceptron (MLP) to obtain the final vector represen-tation $g_v^i$ of node v at the ith layer (see line 7 of the code)."}, {"title": "4.1. Framework of TANGNN", "content": "Traditional GNNs only aggregate information from a node's neighbors, thereby limiting their receptive field and affecting their performance. The"}, {"title": "4.2. Model implementation", "content": "In this section, we provide a detailed introduction to the implementation of the proposed model, including neighbor aggregation component, Top-m attention mechanism aggregation component, and parameter updates."}, {"title": "4.2.1. Neighborhood Aggregation", "content": null}, {"title": "4.2.2. \u0422\u043e\u0440-\u0442 Attention Mechanism Aggregation", "content": "Algorithm 3 demonstrates the forward propagation process of the Top-m attention mechanism aggregation component. During this process, apply the Top-m strategy ( (Details are shown in section 4.3)) to the output of the previous layer of each node v, which is $g_v^{i-1}$, to select the m nodes with the highest similarity and obtain the representation $H_{Top-m}^i$ (see line 5 of the code). As shown in Eq. (1) to (3), the self-attention mechanism first maps it to the query (q), key (k), and value (v) vectors:\n$q_m^i = W_{qm} H_{Top-m}^i$                                                                                                                                                                                                                                                                    (1)\n$k_m^i = W_{km} H_{Top-m}^i$                                                                                                                                                                                                                                                                    (2)\n$v_m^i = W_{vm} H_{Top-m}^i$                                                                                                                                                                                                                                                                    (3)\nHere, $W_{qm}$, $W_{km}$, $W_{vm}$ are the learnable weight matrices for the ith layer. The query, key, and value matrices corresponding to each head typically have smaller dimensions, denoted as $d_q$, $d_k$, $d_v$, espectively. Next, compute the dot product of the query vector with the key vectors, scaled by a factor of $\\sqrt{d_k}$, and then apply softmax to obtain the attention weights, as shown in Eq. (4):\nA_{vu}^i = \\frac{exp (q_{mv}^i (k_{mu}^i)/\\sqrt{d_k})}{\\Sigma_{u=1}^m exp (q_{mv}^i (k_{mu}^i)/\\sqrt{d_k})}$                                                                                                                                                                            (4)\nHere, $A_{vu}^i$ represents the attention weight from node v to node u at the ith layer, where m is the number of nodes obtained from the Top-m attention mechanism component. Then we use Eq. (5) to calculate the output of the head:\nHead_i = \\Sigma_{u=1}^m A_{vu}^i v_{mu}^i$                                                                                                                                                                                                                                                         (5)\nThe output vectors linearly transformed by another weight matrix $W^O$, as shown in Eq. (6)."}, {"title": "4.3. Top-m Efficient Algorithm", "content": "Calculating the similarity between all node pairs is a computationally expensive task, especially in large graph data. This method not only has high computational complexity, with a time complexity of $O(N^2)$, but also significantly increases the demands on memory and storage. Inspired by DiglacianGCN (Zhuo et al., 2021), we introduce an auxiliary vector a into the model, which indirectly estimates the similarity between nodes by com-paring it with the node features' similarity. This approach utilizes the tran-sitive property of similarity, i.e., the higher the similarity between two nodes and a, the more similar these two nodes are. Specifically, to further optimize efficiency and enhance the model's robustness, The Top-m algorithm in this paper is specifically conducted on positive vectors, avoiding interference from potential noise and irrelevant information. The node vector gn is first nor-malized using L2 normalization, represented as $\\hat{g_n} = LN2(g_n) = \\frac{g_n}{||g_n||_2}$. Let $G = {g_n}_{n=1}^N, \\bar{g} = \\frac{1}{N}\\Sigma_{n=1}^N g_n$. In this way, we ensure that the norm of the node vector is 1. We will randomly initialize a as a d-dimensional vector, where $a \\neq \\bar{g}$ and $a \\neq 0$. The update rule for a is shown in Eq. (8) to ensure that a is not aligned with the feature vector of any specific node.\na = LN_2 (a - (a^T \\bar{g})\\bar{g})                                                                                                                                                                                                                                               (8)\nClearly, a is orthogonal to the unit vector $\\bar{g}$. Therefore, feature vectors with high similarity will be mapped close to the position of a. The proof of a being orthogonal to $\\bar{g}$ is as follows:\n(a - (a^T \\bar{g}))^T\\bar{g} = a^T \\bar{g} \u2212 (a^T\\bar{g})\\bar{g}^T\\bar{g} = a^T \\bar{g} \u2013 (a^T\\bar{g})||\\bar{g}||^2 = a^T\\bar{g} - a^T\\bar{g} = 0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               (9)\nAs shown in Figure 2, we use the auxiliary vector a to calculate node similarity. For each sampled node vector gn, we use Eq. (10) to compute its similarity score with the auxiliary vector a using cosine similarity. Here, Sn is the similarity score between gn and a, $a^T$ represents the transpose of a.\ns_n = similarity(a, g_n) = a^T \\hat{g_n}                                                                                                                                                                                                                                                                    (10)\nThe calculated similarity scores sn are used to sort the node indices, producing a sorted list S as shown in Eq. (11), where N is the total number of sampled nodes. The nodes are sorted based on their similarity scores in descending order, resulting in a list of nodes ranked from most to least similar. As illustrated in Figure 2, the sorted list S represents each node connecting to its Top-m most similar nodes, where the value of m is set to 2. Thus, the two most similar nodes to each node are its two nearest neighbors. The boundary nodes (in the graph, g4 and 95) have their two nearest nodes as their most similar nodes.\nS = sort({s_n}_{n=1}^N)                                                                                                                                                                                                                                                                          (11)\nBy introducing the auxiliary vector a, and utilizing the transitive property of similarity, the model no longer needs to directly compute the similarity be-tween all pairs of nodes. Instead, it only calculates the similarity of each node with a, reducing the time complexity from $O(N^2)$ to O(N). This method enables the model to be more effectively applied to large-scale graph data."}, {"title": "4.4. Model Optimization TANGNN-LC", "content": "In this section, we present several enhancements to our model. Building upon the TANGNN, we have introduced a layer concatenation strategy. As shown in Figure 3, in this model, the outputs from different layers are not only passed to the next layer but are also concatenated in the final stage of the model. The concatenated vector $[g^1, g^2, ..., g^L]$ is then processed through a multi-layer perceptron, resulting in the final vector representation $g_v^{final}$ of vertex v. This design allows the model to consider information from all layers in its final output, capturing deep structural features of the graph while also retaining key local information, thus enhancing the model's performance on complex graph data."}, {"title": "4.5. Model Variants", "content": "In this section, we have defined different methods for updating nodes, in-cluding TANGNN-NA (Neighborhood Aggregation As Input), TANGNN-TA (Top-m Attention As Input), and TANGNN-FLC (Final Layer Concatena-tion). Through experimental comparison, it was found that these algorithms also have certain competitiveness compared to baseline algorithms."}, {"title": "4.5.1. TANGNN-FLC(Final Layer Concatenation)", "content": "As shown in Figure 4, unlike TANGNN, TANGNN-FLC concatenates the node information processed by the two components only in the final layer. The neighborhood aggregation component produces the feature vector rep-resentation $h_v^L$ of the target node at the Lth layer, and the Top-m attention mechanism aggregation component then computes based on the output $h_v^{L-1}$ from the previous layer's, produces the feature vector representation $h_v^{L'}$ of the target node at the Lth layer. These two feature vectors are concatenated and then processed through a multilayer perceptron to form the final vector representation $g_v$ of the target node."}, {"title": "4.5.2. TANGNN-NAI(Neighborhood Aggregation As Input)", "content": "TANGNN-FLC cannot effectively retain the information of each layer, and as the number of layers increases, the problem of oversmoothing may occur. To address this, we designed TANGNN-NAI based on the TANGNN framework. $h_i$ is the vector representation of the neighborhood aggregation component at the ith layer, and $h_i'$ is the vector representation of the Top-m attention mechanism aggregation component at the ith layer. These two vector representations are concatenated to form the vector representation $[h_i, h_i']$, which serves as the input for the next layer of the neighborhood aggregation component, while $h_i'$ serves as the input for the next layer of the Top-m attention mechanism aggregation component, continuing until the final layer and then passing through a multilayer perceptron to obtain the fi-nal vector representation $g_v^L$. This method effectively captures the structural information of the graph."}, {"title": "4.5.3. TANGNN-TAI (Top-m Attention As Input)", "content": "The only difference between TANGNN-TAI and TANGNN-NAI is that after obtaining the concatenated vector representation at the ith layer, dif-ferent vector representations are input into different components. The con-catenated vector representation serves as the input for the next layer of the Top-m attention mechanism aggregation component, while the vector repre-sentation $h_i$ from the ith layer of the neighborhood aggregation component serves as the input for the next layer of the neighborhood aggregation com-ponent."}, {"title": "5. Experiments", "content": "We chose the PyTorch Geometric (PyG) framework for the implementa-tion of our models, renowned for its suitability for processing graph-structured data through deep learning techniques. PyG excels in managing large-scale graph datasets and offers a comprehensive suite of optimized layers for graph neural networks. We carried out all our experiments on a system equipped with an Apple M1 Pro (14 Core) @ 3.20Ghz processor, ensuring ample com-putational power for the demanding tasks of training and testing our models."}, {"title": "5.1. Experimental settings", "content": "Parameter settings: For all algorithms, the size of the training set is set from 0.1 to 0.9, with the remainder serving as the test set. The learning rate (lr) of the model is set to 0.001. The batch size is set at 128. The number of layers L for our model is set to 2 due to the increased computational bur-den associated with higher numbers of layers. The value of m in the Top-m attention mechanism aggregation component is set to 30. In the neighbor aggregation component, the aggregation method uses the Mean aggregation function, and the sampling number of the two layers is set to [20,10] (20 for the first layer and 10 for the second layer). To update model parameters, we employ the backpropagation algorithm in conjunction with gradient de-scent. In each training iteration, the gradients of the model parameters are calculated using the chain rule, and these gradients are used to update the parameters towards minimizing the loss function. Specifically, we utilize the Adam optimizer, which integrates adaptive learning rates with momentum to speed up convergence and improve generalization. Additionally, to boost the model's representational power and ability to generalize, we apply regular-ization techniques. Specifically, we use L2 regularization to limit the size of the model parameters, which helps prevent overfitting. By penalizing the L2 norm of the parameters, we encourage the model to select simpler solutions, thereby improving the model's generalization performance."}, {"title": "5.1.1. DATASETS", "content": "We conducted evaluations using eight well-established datasets frequently employed for vertex embeddings. The details of these datasets are shown in Table 1 and 2, including a variety of types such as social networks, pro-tein datasets and citation networks. Both Cora (Sen et al., 2008) and Pubmed (Sen et al., 2008) are recognized as a benchmark in citation net-work analysis, where each vertex is represented by a bag-of-words from the paper and labeled by its academic subject. Amazon (Sen et al., 2008) models the interactions between users and products, representing users and products as nodes and their transactions as graph edges. Citeseer (Sen et al., 2008) represents a citation network where each node holds specific characteristic information pertaining to the literature. Reddit (Hamilton et al., 2017) is extensively utilized for research in social network analysis and natural lan-guage processing fields. It contains post data from the Reddit platform, organized by community (Subreddit). In this dataset, each node represents a post, and the edges in the graph represent direct links between posts or explicit interactions between users, its labels correspond to the community to which the post belongs. The ZINC (Irwin & Shoichet, 2005) is a large chemical database containing millions of drug compounds' 3D structures and attribute information. In graph neural network research, ZINC is commonly used to test molecular property prediction models, such as solubility, toxicity, or biological activity of molecules. The QM9 (Ramakrishnan et al., 2014) includes nearly 134k stable small molecules' quantum chemical properties, with each molecule composed of no more than 9 atoms (C, H, O, N, F). It provides computed chemical properties of molecules, such as molecular or-bital energies, dipole moments, free energies, etc. The ArXivNet is derived from unarXive (Saier et al., 2023), where papers are extracted to construct its own citation network. The nodes in this dataset represent ArXiv aca-demic papers, directed edges represent citations between documents. We use the pre-trained model SPECTER (Cohan et al., 2020) to extract semantic representations as node features based on the titles and abstracts of papers. And we utilize the DictSentiBERT (Yu & Hua, 2023) model to annotate the sentiment polarity (positive, neutral, negative) of citations. The relevant processing code and data have been published on the aforementioned Github website."}, {"title": "5.1.2. BASELINE ALGORITHM", "content": "We use the following representative algorithms in comparative experi-ments. For all models, the learning rate is set to 0.001 and the batch size is set to 128. The number of layers (hops) for the GNN models GCN, GraphSAGE, SGCN, GAT, JK-Net, GIN, TransGNN, SAGEFormer, and NAGphormer is uniformly set to 2.\nGCN (Kipf & Welling, 2017) is one of the initial graph convolutional network models proposed, utilizes adjacency and node feature matrices to update and propagate node features. At each layer, the feature representation of a node is updated by aggregating features from its neighboring nodes.\nSGCN (Wu et al., 2019) represents a streamlined version of the graph convolutional neural network. It simplifies the graph convolution process to a single linear operation by eliminating nonlinear transformations and polynomial fitting. This reduction in complexity and computational demand enhances the model's interpretability and efficiency.\nGraphSAGE (Hamilton et al., 2017) is a graph convolutional neural network that uses an aggregation function to collect neighbor information around vertices. Through training, the algorithm updates this information, allowing vertices to access higher-order neighbor information as the number of iterations increases.\nGAT (Veli\u010dkovi\u0107 et al., 2018) is a neural network model based on graph attention mechanisms. Its core concept revolves around learning the relation-ships between nodes and the importance of features between them, utilizing attention mechanisms to dynamically calculate the influence weights of each node on its neighboring nodes.\nJK-Net (Xu et al., 2018) is a framework designed to enhance the per-formance of graph neural networks (GNNs) on tasks like node classification and graph classification. The fundamental concept of it involves leveraging jumping connections to amalgamate information from various levels of graph convolutional layers. This integration helps the model to more effectively learn and represent graph data.\\"}]}