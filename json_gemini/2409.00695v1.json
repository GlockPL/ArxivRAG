{"title": "Curriculum Prompting Foundation Models for Medical Image Segmentation", "authors": ["Xiuqi Zheng", "Yuhang Zhang", "Haoran Zhang", "Hongrui Liang", "Xueqi Bao", "Zhuqing Jiang", "Qicheng Lao"], "abstract": "Adapting large pre-trained foundation models, e.g., SAM, for medical image segmentation remains a significant challenge. A crucial step involves the formulation of a series of specialized prompts that incorporate specific clinical instructions. Past works have been heavily reliant on a singular type of prompt for each instance, necessitating manual input of an ideally correct prompt, which is less efficient. To tackle this issue, we propose to utilize prompts of different granularity, which are sourced from original images to provide a broader scope of clinical insights. However, combining prompts of varying types can pose a challenge due to potential conflicts. In response, we have designed a coarse-to-fine mechanism, referred to as curriculum prompting, that progressively integrates prompts of different types. Through extensive experiments on three public medical datasets across various modalities, we demonstrate the effectiveness of our proposed approach, which not only automates the prompt generation process but also yields superior performance compared to other SAM-based medical image segmentation methods.", "sections": [{"title": "1 Introduction", "content": "Medical image segmentation is a critical area of research within medical image analysis. It plays a vital role in identifying and delineating various tissues or lesions, thereby significantly enhancing the efficiency and accuracy of medical diagnosis [4]. Recently, with the advent of large-scale foundation models for segmentation such as SAM [16], the field of medical image segmentation has seen rapid development. SAM enables the generation of masks for regions of interest through interactive prompting, making it well-suited for universal medical image segmentation tasks. Several studies [11,24,12] have already explored the application of SAM in medical image segmentation. However, due to the substantial differences between natural and medical images, SAM struggles to achieve optimal segmentation performance across medical image datasets. One strategy to"}, {"title": "2 Methodology", "content": "Given an image $I \\in R^{H \\times W \\times 3}$ with spatial resolution $H \\times W$, large foundation models for segmentation, e.g., SAM, typically adopt an image encoder for extracting the image embedding $e$ from the image $I$, transform the prompt input $P$ through a prompt encoder $Enc$, and finally generate a segmentation mask $S$ through a mask decoder Dec, formulated as:\n$S = Dec(e, Enc(P)),$ (1)\nwhere $P$ can be in the form of various types, such as point prompt $P_{point} = [x, y]$, where $x, y$ denotes the coordinates of the point, box prompt $P_{box} = [x_1, y_1, x_2, y_2]$, composed of coordinates of the top-left and bottom-right corners of the bounding box, and mask prompt $P_{mask} \\in R^{H \\times W}$.\nPrompts play a crucial role during the segmenting process, where high-quality prompts enable SAM to produce accurate segmentation masks [13,3,5]. However, existing methods only utilize a single type of prompt, which contains limited information and often requires manual interventions.\nOur proposed curriculum prompting adheres to a straightforward idea, which aims to progressively combine different types of prompts in a coarse-to-fine way. We begin with the initial prompt $P_1$ to assist SAM in segmentation tasks. Subsequently, the intermediate prediction generated by $P_1$ is fed back together with an auxiliary prompt as supplementary into SAM, initiating a recursive process. This cycle continues $n$ steps until a satisfactory segmentation result is achieved, and our empirical observations indicate that a notably improved result can be"}, {"title": "2.1 Overview", "content": "Given an image $I \\in R^{H \\times W \\times 3}$ with spatial resolution $H \\times W$, large foundation models for segmentation, e.g., SAM, typically adopt an image encoder for extracting the image embedding $e$ from the image $I$, transform the prompt input $P$ through a prompt encoder $Enc$, and finally generate a segmentation mask $S$"}, {"title": "2.2 Coarse Prompting", "content": "During the coarse prompting phase, we aim to segment most of the foreground pixels which is an easier task compared to the fine-grained segmentation with a single step. We utilize prompts that are relatively coarse but contain sufficient information to obtain an initial coarse mask. Since empirical observations suggest that two different types of prompts, e.g., box prompt and point prompt, may conflict with each other [13,3], in this work, we choose to employ a single type of prompt as our coarse prompt. Compared to point prompts, box prompts encompass more significant information, indicating the precise location of the object and the potential intensity features within a specified limited area. Thus, we consider self-generated box prompts as coarse prompts for initial masks.\nTo break through the limitation of SAM requiring manual prompts, we intend to directly and automatically derive prompts from the original image. We generate box prompts with large pre-trained object detection models, e.g. Grounding DINO [20] or GLIP [17]. We fine-tune the pre-trained model with the given medical data and obtain the self-generated box prompts $P_{box}$ as follows,\n$P_{box} = F_{box}(I,T),$ (3)\nwhere $F_{box}$ denotes the chosen object detection model, $I$ denotes the input image and $T$ denotes the text prompt if required for the model.\nFollowing the acquisition of box prompts, a series of post-processing steps (e.g. NMS) are undertaken. We fine-tune SAM's prompt encoder with ground-truth bounding boxes, employing a combination of Dice Loss and BCE Loss as our loss function. Then, we acquire coarse masks $S_{coarse}$ utilizing these self-generated box prompts and the input image embedding $e$,\n$S_{coarse} = Dec(e, Enc_B(P_{box})),$ (4)\nwhere $Enc_B$ denotes the prompt encoder fine-tuned with bounding boxes."}, {"title": "2.3 Fine-grained Prompting", "content": "Having acquired the coarse masks, we further aim to employ more refined prompts to tackle a harder fine-grained segmentation task and guide SAM in generating the final mask. As indicated in [15], SAM struggles with precise edge segmentation, making the enhancement of edge delineation a more complex task compared to segmenting most of the foreground pixels.\nThus, we adopt edge points as additional prompts to unleash SAM's full ability for segmentation. Similar to the process of box prompt generation, we employ a keypoint detection network (e.g. HRNet [26] or ViTPose [27]) to generate point prompts. We obtain the self-generated point prompts $P_{point}$ as follows:\n$P_{point} = F_{point}(I),$ (5)\nwhere $F_{point}$ denotes the keypoint detection network.\nHowever, utilizing multiple types of prompts synergistically requires careful design. As numerous studies have indicated [13,23,28], the simultaneous use of point and box prompts can paradoxically lead to a decrease in performance. One speculation about the cause of this contradiction is due to the structure of SAM's prompt encoder. In SAM's prompt encoder $Enc$, point prompts $P_{point}$ and box prompts $P_{box}$ are processed through a series of steps and then concatenated into a sparse embedding, which is fed into the mask decoder $Dec$. During this process, different types of prompts may influence each other.\nThe question then arises: how can we effectively incorporate the guidance of point prompts while leveraging the information from box prompts? The answer lies in employing an additional type of prompt - the mask prompt, as a bridge to combine both box prompts and point prompts. This is where we take advantage of the coarse masks $S_{coarse}$ obtained in Section 2.2.\nWhile point embeddings and box embeddings influence each other, the mask prompts $P_{mask}$ will only be transformed into a dense embedding through convolutions and summed with the image embedding $e$ without interacting with the sparse embedding. Thus, we employ self-generated point prompts $P_{point}$ on the basis of coarse masks $S_{coarse}$ as mask prompts to achieve refined segmentation.\nSimilar to the process described in Section 2.2, the SAM model we use has undergone fine-tuning with medical images, and edge points and coarse masks served as prompts. Then final masks $S_{fine}$ are acquired as follows:\n$S_{fine} = Dec(e, Enc_P(S_{coarse}, P_{point})),$ (6)\nwhere $Enc_P$ denotes the prompt encoder that is fine-tuned with edge point prompts and mask prompts, and $P_{point}$ is obtained by Eq.( 5)."}, {"title": "3 Experiments and Results", "content": "We evaluate our proposed method on three public medical image datasets across various modalities, including thyroid nodule segmentation dataset TN3K [10],"}, {"title": "3.1 Dataset", "content": "We evaluate our proposed method on three public medical image datasets across various modalities, including thyroid nodule segmentation dataset TN3K [10],"}, {"title": "3.2 Experiment Settings and Metrics", "content": "Our method finetunes four distinct models, ensuring each model builds upon previous outputs. We fine-tune the object detection and keypoint detection network through the MMDetection [2] and MMPose [6] framework. Specifically, we select Grounding DINO [20] and HRNet [26] for box and point prompt generation, respectively. Specifically, we use 8 edge points as point prompts. In terms of fine-tuning SAM, we initialize the model with the pre-trained weight of SAM's ViT-H version [8]. We employ an AdamW optimizer with a learning rate of 0.0001 and a batch size of 4. Our model is implemented using PyTorch and trained and evaluated on an Nvidia RTX4090 24GB GPU. We adopt two commonly used metrics to quantitatively evaluate our proposed method, Dice (dice coefficient) and IoU (Intersection over Union)."}, {"title": "3.3 Results", "content": "Our Proposed Approach Outperforms the Baselines on All Three Datasets. We compare our method with SOTA task-specific methods and SAM-based foundation models. CaraNet [21], TRFE+ [10] and LViT-T [19] are three SOTA methods on the Kvasir, TN3K and QaTa-COV19 datasets, respectively."}, {"title": "4 Conclusion", "content": "In this paper, we present curriculum prompting for medical image segmentation using large foundation models, an efficient method to combine multiple prompts for better segmentation performance. We employ self-generated prompts that have progressively increasing granularity to systematically address segmenta-tion challenges of varying difficulty levels. Compared to utilizing a singular type of prompt, our method introduces more prompt information while avoiding pos-sible conflicts between different prompt types, and achieves state-of-the-art per-formance on three public medical datasets with different modalities and target lesions. We hope our study provides some inspiration about prompting vision foundation models for medical image segmentation."}]}