{"title": "Attention-Driven Hierarchical Reinforcement Learning with Particle Filtering for Source Localization in Dynamic Fields", "authors": ["Yiwei Shi", "Mengyue Yang", "Qi Zhang", "Weinan Zhang", "Cunjia Liu", "Weiru Liu"], "abstract": "In many real-world scenarios, such as gas leak detection or environmental pollutant tracking, solving the Inverse Source Localization and Characterization problem involves navigating complex, dynamic fields with sparse and noisy observations. Traditional methods face significant challenges, including partial observability, temporal and spatial dynamics, out-of-distribution generalization, and reward sparsity. To address these issues, we propose a hierarchical framework that integrates Bayesian inference and reinforcement learning. The framework leverages an attention-enhanced particle filtering mechanism for efficient and accurate belief updates, and incorporates two complementary execution strategies: Attention Particle Filtering Planning and Attention Particle Filtering Reinforcement Learning. These approaches optimize exploration and adaptation under uncertainty. Theoretical analysis proves the convergence of the attention-enhanced particle filter, while extensive experiments across diverse scenarios validate the framework's superior accuracy, adaptability, and computational efficiency. Our results highlight the framework's potential for broad applications in dynamic field estimation tasks.", "sections": [{"title": "Introduction", "content": "Many real-world problems, such as locating a gas leak in an industrial plant, detecting an electromagnetic anomaly in a power grid, or identifying a pollution source in a water body, involve navigating dynamic field distributions\u2014spatially and temporally varying fields shaped by complex environmental interactions. Solving these problems is critical for ensuring safety, minimizing environmental damage, and maintaining system integrity. A prominent and technically demanding instance of such problems is the Inverse Source Localization and Characterization (ISLC) problem Steiner & Bushe (2001). The ISLC problem involves inferring the location, magnitude, and temporal characteristics of an unknown source within a complex field, based on sparse, noisy, and localized observations. This problem is ubiquitous in high-stakes applications such as gas leak detection, nuclear accident response, and environmental pollutant tracking, where accurate estimation of source parameters is crucial for effective mitigation and intervention.\nHowever, solving the ISLC problem poses significant challenges due to the complex and uncertain nature of dynamic fields. First, partial observability is a fundamental obstacle, as measurements"}, {"title": "Peliminaries", "content": ""}, {"title": "Unified Mathematical Framework for Field Model Based on the Convection-Diffusion Equation", "content": "Many seemingly disparate natural phenomena, such as pollutant dispersion, gas diffusion, and electric field distributions, share common physical mechanisms: diffusion, convection, and external sources. These mechanisms can be described using the convection-diffusion equation (CDE) Holley (1969), which serves as a versatile mathematical framework:\n$\\alpha\\nabla^2\\varphi - \\vec{v} \\cdot \\nabla\\varphi + \\gamma\\varphi + S(x, y) = 0$  (1)\nwhere (x, y) represents the field variable (e.g., concentration or temperature), with $\\alpha\\nabla^2\\varphi$, $-\\vec{v}\\cdot \\nabla\\varphi$, and S(x, y) accounting for diffusion, convection, and external sources, respectively. By tuning parameters such as the diffusion coefficient \u03b1, velocity $\\vec{v}$, and source term S(x, y), the CDE can describe various field phenomena, from heat conduction to pollutant transport."}, {"title": "Gaussian Plume Model", "content": "The Gaussian plume model, shown in Fig. 2, derived from the Convection-Diffusion Equation, leverages steady-state solutions to balance simplicity and computational efficiency. The model is analytically described by:\n$\\varphi(x,y) = \\frac{q_s}{4\\pi\\psi\\nu}  exp(-\\lambda \\frac{r}{\\psi} (\\frac{(x - x_0)u_x + (y - y_0)u_y}{2\\psi}) )$.\nwhere qs is the source strength, r is the radial distance, $u_x, u_y$ are convection velocities, $\\lambda$ is a decay parameter, and $\\psi$ is the diffusion coefficient."}, {"title": "Partially Observable Markov Decision Process", "content": "Dynamic field modeling often involves uncertainties in observations or incomplete knowledge of the system's state. To address this, the problem can be formulated as a Partially Observable Markov Decision Process (POMDP), a probabilistic framework for decision-making under uncertainty. A POMDP is defined as a 7-tuple (S, \u03a9, A, T, Pr, R, \u03b3), where S is the state set, A is the action set, T is the state transition probabilities, R is the reward function, \u03a9 is the observation set, O is the observation probability function, and \u03b3 \u2208 (0, 1] is the discount factor. At each time step, the agent receives an observation $o_t \\in \\Omega$, depending on the current state and the action taken at the previous time step via the conditional observation probability $O(o_t|s_t, a_{t-1})$. By executing an action $a_t \\in A$, the environment transitions to a new state according to the state transition probability $T(s_{t+1}|s_t, a_t)$,"}, {"title": "Sequential Importance Sampling Convergence Theorem", "content": "To ensure robust estimation in partially observable systems, the convergence of Sequential Importance Sampling (SIS) is established. The following theorem guarantees that the SIS approximation converges to the true posterior distribution as the number of particles increases.\nTheorem 3.1 (Convergence of Standard SIS Doucet et al. (2001); Doucet & Johansen (2009)). Let $\\Theta_k$ represent the latent state at time k, and let $\\Theta_{1:k} = {\\Theta_1,..., \\Theta_k}$ denote the sequence of observations. Assume a Bayesian model:\n$p(\\Theta_k | \\Theta_{1:k}) \\propto p(\\Theta_{1:k} | \\Theta_k) p(\\Theta_k)$, (2)\nwhere p(\u0398k) is the prior and $p(\\Theta_{1:k} | \\Theta_k)$ the likelihood. Let $p_N^{(SIS)}(\\Theta_k | \\Theta_{1:k})$ be the particle-based approximation of the posterior. Then, under standard assumptions (e.g., bounded likelihood, consistent proposal distribution),\n$||p_N^{(SIS)}(\\Theta_k | \\Theta_{1:k}) \u2013 p(\\Theta_k | \\Theta_{1:k}) || \\rightarrow 0$, as N \u2192 \u221e. (3)"}, {"title": "Methodology", "content": "Inverse Source Localization and Characterization under partial observability requires incorporating uncertain or unobservable state components into decision-making. To address this, We propose a methodology that combines Attention Bayesian Inference (via particle filtering) and reinforcement learning (RL) to sequentially update a belief distribution over source parameters (e.g., leak location, emission rate). The incorporation of attention mechanisms allows the model to focus on the most relevant observational data, enhancing both the accuracy and efficiency of belief updates. This approach addresses the challenges of partial observability by enabling adaptive estimation, where new observations iteratively refine the understanding of the source, bridging theoretical principles with practical application."}, {"title": "Bayesian Approximation for Belief Distribution", "content": "The ISLC challenge can be characterized as a Partially Observable Markov Decision Process (POMDP), indicating that optimal decisions must not rely solely on immediate observations due to incomplete state information. Here, we integrate reinforcement learning and Bayesian inference to estimate the environmental dynamics-i.e., the belief distribution\u2014and thereby incorporate additional unobservable information into the decision-making process.\nParticle Filter for Iterative Distribution Approximation. We employ a particle filter to iteratively approximate the environmental dynamics distribution\u2014i.e., the belief state-over time. At each time step k, this approximation is maintained via a collection of N particles, {$\\{\\Theta_k^i, w_k^i\\}\\}_{i=1:N}$, where $\\Theta_k^i$ represents the i-th point estimate of the source parameters (e.g., leak location, release rate), and $w_k^i$ is the corresponding importance weight, satisfying $\\sum_{i=1}^N w_k^i = 1$. Hence, the belief distribution at time k is given by\n$b(\\Theta_k) = \\sum_{i=1}^N w_k^i \\delta(\\Theta_k \u2013 \\Theta_k^i)$, (4)\nwhere \u03b4(\u00b7) denotes the Dirac delta function.\nSequential Importance Sampling (SIS). The particle weights $\\{w_k^i\\}_{i=1:N}$ are updated following the framework of SIS Doucet et al. (2001). Specifically, we aim to approximate the posterior\n$p(\\Theta_{k+1} | \\Theta_{1:k+1}) \\propto p(\\Theta_{k+1}|\\Theta_{k+1}) p(\\Theta_{k+1}|\\Theta_k) p(\\Theta_k|\\Theta_{1:k})$."}, {"title": "MCMC Process with Attention Mechanism", "content": "The MCMC particle filtering framework enhanced with an attention mechanism leverages interconnected mathematical components to achieve robust and efficient state estimation. This process is grounded in probability theory, matrix analysis, and optimization techniques, allowing for effective handling of high-dimensional and uncertain environments.\nEffective Sample Size (ESS) and Resampling The MCMC process begins by evaluating the uniformity of particle weights using the Effective Sample Size (ESS), calculated by:\n$ESS = \\frac{\\sum_{i=1}^N w_i^2}{N \\sum_{i=1}^N \\omega_i^2}$\nwhere wi denotes the weight of the i-th particle. Low ESS values indicate that a few particles dominate the weight distribution, signaling the need for resampling. This step is crucial for maintaining the diversity of the particle set and ensuring that updates remain representative of the underlying state space dynamics."}, {"title": "Attention-Driven Weight Refinement with MCMC Move Step", "content": "After resampling, the MCMC move step employs an attention mechanism to refine particle weights by assessing their relevance to the current state estimation. The attention mechanism operates as follows:\nAttention(Q, K, V) = softmax($\\frac{QK^T}{\\sqrt{d_k}}$)V,\nwhere Q, K, V act as the queries, keys, and values, respectively, with $d_k$ representing the dimensionality of these elements. The weights {w} are treated as the queries Q, keys K and values V, thereby allowing each particle's weight to be recalibrated in relation to all others. This mechanism not only addresses particle degeneracy by rebalancing the weights but also captures both global and local interactions among particles, leading to a more robust tracking of the true distribution under conditions of noise and partial observability.\nCovariance Matrix and Particle Perturbation With weights refined, the procedure calculates the weighted covariance matrix, \u03a3, which quantifies the uncertainty and spread of the belief across the distribution of belief states :\n$\\Sigma = E_\\omega[\\Theta\\Theta^T] \u2013 E_\\omega[\\Theta]E_\\omega[\\Theta]^T$,\nwhere $E_\\omega[.]$ represents the expectation calculated with weights adjusting each particle's contribution. Regularization is applied to the covariance matrix by adding a small positive constant to its diagonal (\u03a3 + \u03f5I), ensuring numerical stability and positive definiteness for the subsequent Cholesky decomposition, L = Cholesky(\u03a3).This decomposition transforms the covariance matrix into a lower triangular matrix L, which facilitates the efficient sampling of new particle states from multivariate Gaussian distributions, crucial for effectively exploring the state space.\nState Updates and Log-Likelihood Ratio Particle states are updated using the formula:\n$\\Theta_{new} = \\Theta + h_{opt}L^{1/2}\\xi$,\nwhere $h_{opt}$ represents the optimal bandwidth for perturbation, calculated to balance exploration and the focusing on high-probability regions, and $\\xi \\sim N(0, I)$ denotes Gaussian noise. The viability of these states is assessed using a log-likelihood ratio, which compares the plausibility of transitioning from the current state to the proposed state, emphasizing states that minimize deviations from expected dynamics.\nAcceptance Ratio and System State Estimation The acceptance of new states into the particle set is governed by an acceptance ratio (\u03b2), formulated as:\n$\\beta = \\frac{p(\\Theta_{new})}{p(\\Theta)} exp(-\\frac{1}{2} [\\Delta_{\\Theta_{new}}^T\\Sigma^{-1}\\Delta_{\\Theta_{new}} \u2013 \\Delta_{\\Theta_{old}}^T\\Sigma^{-1}\\Delta_{\\Theta_{old}}])$,\nwhere $\\Delta_{\\Theta_{new}}$ and $\\Delta_{\\Theta_{old}}$ represent the deviations of the new and old belief states from their respective means, adjusted for the covariance matrix \u03a3. This measure, often referred to as the logratio, quantifies the transition plausibility by comparing the probability densities of the new and current states under the model's assumptions.\nThis detailed explanation emphasizes the sophisticated interplay of mathematical principles that underpin the MCMC process with an attention mechanism, ensuring thorough understanding and implementation fidelity in practical scenarios."}, {"title": "Cessation Mechanism", "content": "Particle filters estimate state distributions in unknown environments by refining state representations using observational data. A key feature of these models is their ability to terminate operations autonomously based on statistical criteria, improving computational efficiency. Specifically, the process ceases when the standard deviation (STD) of the belief states (\u0398k) falls below a predefined threshold (\u03b6), indicating convergence. The STD is calculated as:\nSTD = $\\sqrt{diag(E[(\u0398 \u2013 E[\u0398])^2])}$,"}, {"title": "Integration of RL and planning in POMDPS", "content": "In POMDPs, decision-making under uncertainty is achieved by integrating probabilistic belief states b(k) with real-time observations ok. The state sk = (b(\u0398k), ok) combines the predictive capabilities of the belief state with the immediate context provided by observations, enabling adaptive and robust strategies for complex tasks like source localization. Based on this enhanced state representation, we propose two approaches: Attention Particle Filtering Planning (Att-PFP) and Attention Particle Filtering RL (Att-PFRL).\nAtt-PFP aims to maximize the cumulative reward over a finite horizon: $max \\sum_{t=0}^T \\gamma R_t$, where T is the planning horizon. This approach leverages a planning process guided by an attention mechanism, which focuses on high-value regions in the belief distribution, improving computational efficiency and precision in uncertainty reduction. In contrast, Att-PFRL optimizes a policy $\u03c0_\u03b8(s_k)$ through reinforcement learning to maximize the expected cumulative reward over an infinite horizon: $max_\\pi \\mathbb{E} [\\sum_{t=0}^\\infty \\gamma^t R_t]$. An attention mechanism is used to refine noisy observations and enhance the learning process. The policy is improved iteratively by updating the value function V(sk) based on the temporal difference (TD) error: \u03b4k = rk+1 + \u03b3V(sk+1) \u2212 V(sk), and V(sk) is adjusted as V(sk) + \u03b1 \u03b4k, where \u03b1 is the learning rate."}, {"title": "Experiments", "content": "In this paper, we utilized the ISLC environments (ISLCenv) to investigate source localization in various scenarios. The STE environment, described in the Appendix or (Shi et al. (2024a)), includes a Gaussian model to simulate the field distribution from multiple sources and a sensor model to capture intensity readings. Instead of directly rewarding the agent, the environment provides positional and intensity observations at each step, requiring RL algorithms to adapt their strategies for effective source localization across diverse environmental conditions."}, {"title": "Scenario Parameterization and Evaluation", "content": "The training scenarios, defined within a 20 \u00d7 20 area as fundamental experiments, are generated by randomly initializing the source and environmental parameters at the start of each episode, including the gas source location, wind speed, and wind direction, sampled from the probability distributions in Table 1. The agent begins its search from a random position within the (0, 5) \u00d7 (0, 5) region, moving at a speed of 1 meter per step to ensure exposure to a diverse range of scenarios, facilitating robust learning. The testing scenarios comprise 1,000 randomly generated conditions distinct from the training data. While drawn from the same parameter ranges, these scenarios introduce new specific conditions to evaluate the model's performance on unseen data.\nFor the study on experiments in out-of-distribution scenarios, the training region is confined to (10, 15) \u00d7 (10, 15), while the testing regions are defined as (5, 10) \u00d7 (15, 20) and (15, 20) \u00d7 (15, 20). This setup ensures that the testing conditions differ spatially from the training region, providing a robust evaluation of the model's generalization ability beyond the distribution of the training data."}, {"title": "Fundamental Experiments", "content": "In the fundamental experiments, our methods (ATT-PFRL and ATT-PFP) showcase not only superior performance across various environmental conditions but also illustrate the benefit of robust algorithm design in Table 3 and 2. ATT-PFRL, for instance, consistently exhibits high OCE scores, such as 0.95\u00b10.05 in Temperature and 0.96\u00b10.05 in Gas, which demonstrates its effectiveness in completing missions under diverse conditions. Compared to other methods, such as AGDC-KLD and AGDC-EE, which show lower OCE scores (e.g., 0.90\u00b10.05 and 0.87\u00b10.04 respectively in Temperature), our methods prove to be more reliable. The ADE metric further accentuates this point, with ATT-PFRL maintaining lower values (e.g., 20\u00b11.0 in Temperature), suggesting more efficient navigation compared to DCEE and Entrotaxis, whose ADE exceeds 50\u00b12.5 in challenging fields."}, {"title": "Out of Distribution Problem", "content": "The Out-of-Distribution (OOD) experiments, summarized in the left part of the Fig 4, highlight the robustness of our method, ATT-PFRL, in handling unseen scenarios. ATT-PFRL achieves the highest OCE values across all fields, such as 0.95 in the Temperature field and 0.96 in the Gas field, demonstrating superior generalization capabilities. It also maintains low ADE values (e.g., 20 in Temperature and 17 in Gas), indicating efficient navigation, and stable LPS values (e.g., 0.15 in Temperature), showcasing precise source localization. In contrast, baseline methods exhibit significant performance gaps. For example, AGDC-KLD achieves moderate OCE scores (e.g., 0.448 in Temperature) but suffers from higher ADE (e.g., 23 in Temperature). AGDC-Entropy and AGDC-EE show even lower OCE (e.g., 0.271 and 0.288 in Temperature) and less efficient navigation (ADE of 25). The Energy field emerges as the most challenging, where ATT-PFRL achieves a reduced OCE of 0.63, yet still outperforms baseline methods such as AGDC-KLD (OCE 0.307) and AGDC-Entropy"}, {"title": "Ablation Experiment", "content": "The ablation study distinctly demonstrates the significant role of the attention mechanism in enhancing algorithm performance in the left part of the Fig 4. By comparing ATT-PFRL and ATT-PFP against their counterparts without attention (PFRL and PFP), we see notable improvements in both OCE and LPS metrics. For example, ATT-PFRL in the Temperature field records an LPS of 0.06, markedly lower than PFRL's 0.21, highlighting the precision gained through the integration of attention. Such findings validate the attention mechanism's capacity to refine decision-making and adaptability, particularly in environments requiring high levels of accuracy and operational efficacy."}, {"title": "Conclusion", "content": "In this paper, we presented a novel hierarchical framework to address the challenges of the ISLC problem in dynamic and uncertain environments. By integrating Bayesian inference with reinforcement learning, and enhancing particle filtering through an attention mechanism, our framework achieves"}]}