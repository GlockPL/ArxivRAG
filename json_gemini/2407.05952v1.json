{"title": "H-STAR: LLM-driven Hybrid SQL-Text Adaptive Reasoning on Tables", "authors": ["Nikhil Abhyankar", "Vivek Gupta", "Dan Roth", "Chandan K. Reddy"], "abstract": "Tabular reasoning involves interpreting unstruc-tured queries against structured tables, requir-ing a synthesis of textual understanding andsymbolic reasoning. Existing methods rely oneither of the approaches and are constrained bytheir respective limitations. Textual reasoningexcels in semantic interpretation unlike sym-bolic reasoning (SQL logic), but falls shortin mathematical reasoning where SQL excels.In this paper, we introduce a novel algorithmH-STAR, comprising table extraction and adap-tive reasoning, integrating both symbolic andsemantic (text-based) approaches. To enhanceevidence extraction, H-STAR employs a multi-view approach, incorporating step-by-step rowand column retrieval. It also adapts reason-ing strategies based on question types, utilizingsymbolic reasoning for quantitative and log-ical tasks, and semantic reasoning for directlookup and complex lexical queries. Our exten-sive experiments demonstrate that H-STAR sig-nificantly outperforms state-of-the-art methodsacross three tabular question-answering (QA)and fact-verification datasets, underscoring itseffectiveness and efficiency.", "sections": [{"title": "1 Introduction", "content": "Tabular data is one of the most widely used formatsfor storing structured information in real-world ap-plications. Table-based reasoning presents an inher-ently challenging problem, requiring logical, math-ematical, and textual reasoning over unstructuredqueries and structured tables (Ye et al., 2023). Thus,understanding tabular data has become a significantarea of research in natural language processing.Tabular reasoning tasks (illustrated in Figure 1),such as table-based question answering (Pasupatand Liang, 2015; Nan et al., 2022) and table-basedfact verification (Chen et al., 2019), have been ex-tensively explored in the past.\nLarge Language Model (LLM) based promptingmethods involving symbolic approaches such asProgram of Thought (Chen et al., 2023) and tex-tual methods like Chain of Thought (CoT) (Weiet al., 2022; Brown et al., 2020) have been usedfor table reasoning. As shown in Figure 2, thesemethods often fall short due to the complexities im-posed by the intricate mix of numerical, temporal,and textual data, coupled with complex table struc-tures (Shi et al., 2023; Liu et al., 2023; Sui et al.,2024). Recent techniques (Cheng et al., 2022; Yeet al., 2023; Wang et al., 2023; Nahid and Rafiei,2024) rely on either textual or symbolic reasoning.Textual reasoning excels in natural language under-standing but often misinterprets table structures andstruggles with quantitative reasoning. Conversely,SQL-based approaches are strong in quantitativeproblem-solving but perform poorly on noisy orunstructured inputs (Liu et al., 2023). Integratingboth methods can leverage their complementarystrengths, thereby addressing their respective weak-nesses. Therefore, it is crucial to explore the ques-"}, {"title": "2 H-STAR Approach", "content": "H-STAR addresses table reasoning challenges bydecomposing the task into extraction and reason-ing stages, combining LLM's textual comprehen-sion with symbolic reasoning. It converts the orig-inal table T into a table TCR based on the queryQ (refer Algorithm 1). Figure 3 illustrates ourH-STAR approach. Unlike, DATER (Ye et al.,2023) and TabQSLify (Nahid and Rafiei, 2024),which use textual reasoning and text-to-SQL re-spectively, our framework integrates both reason-ing types in a complementary manner. Our rea-soning step dynamically uses symbolic techniquesfor quantitative questions, overcoming the limits ofpure textual reasoning. H-STAR operates in twomain stages: 1) Table Extraction and 2) AdaptiveReasoning."}, {"title": "2.1 Table Extraction", "content": "Two-dimensional tables consist of columns (at-tributes) and rows (values). Table extraction in-volves a two-step reasoning chain: (1) column ex-traction and (2) row extraction (see Figure 3). Asillustrated in Algorithm 1, for a given table T, tableextraction returns a table TCR (Steps 1-8) based onthe input query Q.\nAs shown in Figure 3,H-STAR uses a 'multi-view' technique for columnextraction, working with both the original table T(Step 1) and its transposed form TT (Step 2). Thisensures that the LLM semantically understands andextracts correct attributes regardless of the tablestructure. Given an input table T and the questionQ ('How long did it take the New York Americansto win the National Cup after 1936?'), the LLMemploys colsql to generate a SQL query to ex-tract columns 'year' (C1). This is followed by thetext-based column verification step coltext onTT returning a new set of columns C2 (\u2018year', \u2018na-tional cup'). Table T is filtered for columns 'year',\u2018national cup' (C\u2032) derived from Steps 1 and 2 toobtain a column-filtered table Tc (Step 4).\nAfter obtaining the filtered ta-ble Tc with relevant columns, H-STAR proceeds tothe row extraction phase culminating in the query-specific table TCR (Steps 5-8). The filtered table"}, {"title": "2.2 Adaptive Reasoning", "content": "To leverage the strengths of textual reasoning whileovercoming its quantitative limitations, we intro-duce an adaptive reasoning framework. Assessingthe nature of the question, the LLM incorporatesymbolic reasoning only when required. For in-stance, in Figure 3, when faced with a questionlike \"How long did it take the New York Ameri-cans to win the National Cup after 1936?\", LLMrequires a count as an answer, and our pipeline pri-oritizes symbolic reasoning (fsql) through SQL-generated code. This code is executed on a SQLengine, enabling precise answers from the tableTCR. The output from this SQL-based evaluationEy serves as additional evidence, enhancing thetextual reasoning process (ftext). By explicitly in-tegrating this quantitative approach, our algorithmeffectively addresses LLMs' limitations in handlingmathematical operations, thereby improving theaccuracy and robustness when tackling questions thatdemand numerical reasoning."}, {"title": "3 Experiments", "content": "Benchmark Datasets. We evaluate our methodon three datasets covering fact verification, short-form, and long-form question-answering tasksusing in-context examples. (a) TabFact (Chenet al., 2019): A fact verification benchmark uti-lizing Wikipedia tables. We evaluate the test-small set, containing 2,024 statements and 298tables, with each statement labeled as Entailed(\"True\") or Refuted (\"False\"). (b) WikiTQ (Pasupatand Liang, 2015): The WikiTableQuestions (Wik-iTQ) dataset involves question-answering tasksover semi-structured Wikipedia tables. It includes astandard test set with 4,344 table-question pairs. (c)FeTaQA (Nan et al., 2022): FeTaQA (Free-FormTable Question Answering) comprises free-formquestions requiring synthesis from multiple table\nModels. In our research, we use state-of-the-art large language models (LLM) such asPaLM-2 (Anil et al., 2023) and GPT-3.5-Turbo(Brown et al., 2020; OpenAI, 2023) for table rea-soning tasks. Our model inputs include in-contextexamples, the table, and the question for each stepof the pipeline. Previous studies, such as Chain-of-Table and TabSQLify, influenced our modelchoices to ensure a fair comparison. All baselines with PALM-2 are from the propriety PALM-2"}, {"title": "3.1 Results and Analysis", "content": "Table 1 compares the performance of differentmethods on the table reasoning tasks, TabFact,and WikiTQ datasets, across several LLMs includ-ing GPT-3.5-Turbo and PaLM-2. This compari-son involves evaluating against generic reasoning,table manipulation techniques, and our method,H-STAR. Appendix A provides results on theFeTaQA dataset in addition to comparisons withother baseline models on the WikiTQ and TabFactdataset.\nAnalysis. (a) On the WikiTQ dataset, H-STARdemonstrates superior performance with both GPT-3.5-Turbo and PaLM-2 models, surpassing all base-line methods. Specifically, H-STAR achieves an ac-curacy of 68.85% on WikiTQ with GPT-3.5-Turbo,marking a significant improvement of 17.01% overthe vanilla GPT-3.5-Turbo model and a 4% increaseover the previous best method, TabSQLify. (b) Onthe TabFact dataset, H-STAR outperforms all meth-ods with GPT-3.5-Turbo, attaining an accuracy of83.74%, which is a 13.29% improvement over thevanilla model and a 4% margin over the previ-ous best, Chain-of-Table. However, with PaLM-2,H-STAR achieves comparable performance on Tab-Fact while outperforming Chain-of-Table by 1.2%on WikiTQ.\nStage-1 inH-STAR involves the extraction of the table mostrelevant to the question. Our table extractionmethod uses a two-step chain to select the relevantcolumns followed by the rows. Figure 4 showcasesthe effectiveness of the technique to extract only"}, {"title": "3.2 Efficiency Analysis", "content": "In Table 3, we study H-STAR by considering thetotal number of samples generated by the LLMs.BINDER and DATER utilize self-consistency tech-niques to refine their results, while the Chain-of-Table approach adopts a more iterative sam-ple creation process. Specifically, BINDERgenerates 50 Neural-SQL samples, implement-ing self-consistency, while DATER employs self-consistency at each step, resulting in 100 samples.In contrast, the Chain-of-Table method opts fora more resource-efficient strategy, producing 25samples across three steps: 'Dynamic Plan', 'Gen-erate Args', and 'Query'. TabSQLify generatesthe fewest samples, with one generation each for thetable decomposition and query steps. Using singleoutputs can result in a decrease in accuracy whenthe model fails to return an answer. Thus, H-STARincorporates self-consistency measures by gener-ating two outputs at each stage to ensure outputvalidity. Each step, except Query, involves twogenerations for both SQL and Text, i.e., a total offour steps for column and row retrieval. These addi-tional steps serve as a precaution against abnormalLLM generations. Overall, H-STAR is efficientand only uses 6-10 sample generations."}, {"title": "4 Ablation Study", "content": "We conduct an ablation study to measure the con-tribution of each step in the pipeline and assess theimpact of integrating symbolic-textual and adaptivereasoning in H-STAR."}, {"title": "4.1 Importance of Each Stage", "content": "To evaluate the significance of the two primarystages in H-STAR: a) Table extraction and b)Adaptive reasoning, we conducted an ablationstudy. This study involved systematically remov-ing one stage at a time. For the first condition,the final adaptive reasoning stage was substitutedwith a basic chain-of-thought reasoning process,while maintaining the table extraction stage. Con-versely, the second condition retained the adaptivereasoning stage but omitted the table extractionstage, specifically by removing the row extractionand column extraction steps individually and thencollectively. A detailed assessment of the contri-bution of each stage to the overall performance ofthe method is shown in Table 4. It highlights theimportance of each step in the pipeline. Removingany step can result in a performance loss except fora marginal increase in the case of row extractionfor TabFact."}, {"title": "4.2 Impact of a Hybrid SQL-Text Approach", "content": "We perform a quantitative evaluation of the impactof the hybrid approach in H-STAR. Figures 5, 6depict how using only one of the approaches resultsin insufficient outputs. Furthermore, it is evidentfrom Table 5 that not employing a hybrid approach(H-STAR) results in a significant performance loss."}, {"title": "5 Where do LLMs fail?", "content": "This section examines errors in H-STAR, contraststhe failure points with existing approaches, andsummarizes our key findings."}, {"title": "5.1 Error Analysis on H-STAR", "content": "The disjoint, step-wise nature of H-STAR enablesidentifying and analyzing failures. Table 6 shows100 randomly selected incorrect answer instancesfrom the Tabfact and WikiTQ datasets. In thisstudy, 'Missing Columns' and 'Missing Rows' re-fer to missing necessary columns and rows, respec-tively. 'Incorrect Reasoning' occurs when H-STARextracts the correct table, but LLM fails to producethe correct answer. 'Incorrect Annotations' include"}, {"title": "5.2 More Error Analysis: H-STAR vs Others", "content": "Table 7 compares errors in H-STAR, with Tab-SQLify, and BINDER. We use 100 WikiTQ sam-ples where TabSQLify fails. Many errors withTabSQLify (6/100 missing columns, 56/94 missingrows) result from incorrect row extraction. Outof 100, TabSQLify correctly extracts the table for38 samples, but incorrectly reasons on 29 (76%).BINDER does not extract tables but generates mul-tiple neural SQL queries for table reasoning tasks.However, its output shows that it fails to rea-"}, {"title": "5.3 What did we learn?", "content": "Firstly, our evaluation demonstrates that our hybridapproach achieves substantial improvements, sur-passing the performance of previous state-of-the-art methods across various table reasoning tasks.Secondly, the quantitative analysis demonstratesthat our 'multi-view' approach extracts tables withreduced irrelevant data, resulting in more concisetables. Furthermore, the qualitative analysis high-lights fewer errors in our table extraction methodcompared to prior approaches, confirming a de-crease in extraneous information.\nThirdly, our analysis indicates consistent perfor-mance even with longer tables, emphasizing ourmethod's effectiveness in accurately extracting rel-evant information by filtering out noise. The adap-tive reasoning component boosts performance, es-pecially for longer tables. Lastly, the ablation studyshows that breaking down the task into sub-taskssignificantly enhances the overall performance,with each sub-task playing a crucial role in achievingsuperior results. Moreover, it highlights theconstraints of relying solely on either text or SQLapproaches, which are effectively addressed by ourH-STAR approach. Together, these findings em-"}, {"title": "6 Related Work", "content": "Table reasoning tasks require the ability to rea-son over unstructured queries and structured orsemi-structured tables. Traditional approaches likeTAPAS (Herzig et al., 2020), TAPEX (Liu et al.,2021), TABERT (Yin et al., 2020), TURL (Denget al., 2022), PASTA (Gu et al., 2022) work on pre-training language models jointly on large-scale tab-ular and text data to reason in an end-to-end man-"}, {"title": "7 Conclusion", "content": "In this study, we present H-STAR, a novel methodthat effectively integrates semantic and symbolicapproaches, demonstrating superior performancecompared to existing methods in tasks involvingtable reasoning. Our approach involves a two-stepLLM-driven process: firstly, employing 'multi-view' table extraction to retrieve tables relevant to aquery, and then, implementing adaptive reasoningto select the optimal reasoning strategy based onthe input query. We address prior bottlenecks withefficient extraction and reasoning, leading to im-"}, {"title": "Limitations", "content": "Our current work has focused primarily on a subsetof table reasoning tasks using datasets sourced fromWikipedia. While this has laid a solid foundation, itlimits exploration into diverse reasoning tasks suchas table manipulation, text-to-table generation, andtable augmentation, which could provide valuableinsights and enhance our approach's capabilities.Additionally, our method's generalizability is con-fined to Wikipedia-based datasets, restricting itsapplication to other domains that require specificdomain knowledge, which our current approachlacks. Extending our approach to different domainsmay necessitate integrating domain-specific knowl-edge to ensure effective reasoning.\nFurthermore, our evaluation has been limitedto relatively straightforward table structures. Han-dling more complex data representations such assemi-structured tables, hierarchical tables, and re-lational databases remains unexplored territory.These structures present unique challenges that ourcurrent approach may not effectively address. Fi-"}, {"title": "Ethics Statement", "content": "We, the authors, affirm that our work adheres to thehighest ethical standards in research and publica-tion. We have carefully considered and addressedvarious ethical issues to ensure the responsible andfair use of computational linguistics methodolo-"}, {"title": "A Input Table Format", "content": "The input table format changes depending on thetype of reasoning used. Since H-STAR uses a hy-brid approach, the input table format also variesdepending on the type of reasoning used. Besides"}, {"title": "B Hyperparameters", "content": "Table 13 provides the details of PaLM-2 hyper-parameters used for the WikiTQ and TabFactdatasets whereas Table 14 showcases the hyper-parameter setting for GPT-3.5-Turbo. The hyperpa-rameters 'samples' indicate the number of outputstaken for each step of the pipeline whereas the'examples' indicate the number of few-shot demon-strations for each step."}]}