{"title": "H-STAR: LLM-driven Hybrid SQL-Text Adaptive Reasoning on Tables", "authors": ["Nikhil Abhyankar", "Vivek Gupta", "Dan Roth", "Chandan K. Reddy"], "abstract": "Tabular reasoning involves interpreting unstructured queries against structured tables, requiring a synthesis of textual understanding and symbolic reasoning. Existing methods rely on either of the approaches and are constrained by their respective limitations. Textual reasoning excels in semantic interpretation unlike symbolic reasoning (SQL logic), but falls short in mathematical reasoning where SQL excels. In this paper, we introduce a novel algorithm H-STAR, comprising table extraction and adaptive reasoning, integrating both symbolic and semantic (text-based) approaches. To enhance evidence extraction, H-STAR employs a multi-view approach, incorporating step-by-step row and column retrieval. It also adapts reasoning strategies based on question types, utilizing symbolic reasoning for quantitative and logical tasks, and semantic reasoning for direct lookup and complex lexical queries. Our extensive experiments demonstrate that H-STAR significantly outperforms state-of-the-art methods across three tabular question-answering (QA) and fact-verification datasets, underscoring its effectiveness and efficiency.", "sections": [{"title": "1 Introduction", "content": "Tabular data is one of the most widely used formats for storing structured information in real-world applications. Table-based reasoning presents an inherently challenging problem, requiring logical, mathematical, and textual reasoning over unstructured queries and structured tables (Ye et al., 2023). Thus, understanding tabular data has become a significant area of research in natural language processing. Tabular reasoning tasks (illustrated in Figure 1), such as table-based question answering (Pasupat and Liang, 2015; Nan et al., 2022) and table-based fact verification (Chen et al., 2019), have been extensively explored in the past.\nLarge Language Model (LLM) based prompting methods involving symbolic approaches such as Program of Thought (Chen et al., 2023) and textual methods like Chain of Thought (CoT) (Wei et al., 2022; Brown et al., 2020) have been used for table reasoning. As shown in Figure 2, these methods often fall short due to the complexities imposed by the intricate mix of numerical, temporal, and textual data, coupled with complex table structures (Shi et al., 2023; Liu et al., 2023; Sui et al., 2024). Recent techniques (Cheng et al., 2022; Ye et al., 2023; Wang et al., 2023; Nahid and Rafiei, 2024) rely on either textual or symbolic reasoning. Textual reasoning excels in natural language understanding but often misinterprets table structures and struggles with quantitative reasoning. Conversely, SQL-based approaches are strong in quantitative problem-solving but perform poorly on noisy or unstructured inputs (Liu et al., 2023). Integrating both methods can leverage their complementary strengths, thereby addressing their respective weaknesses. Therefore, it is crucial to explore the question: Can we integrate both symbolic and textual approaches into a hybrid method to enhance tabular reasoning?"}, {"title": "2 H-STAR Approach", "content": "H-STAR addresses table reasoning challenges by decomposing the task into extraction and reasoning stages, combining LLM's textual comprehension with symbolic reasoning. It converts the original table T into a table TCR based on the query Q (refer Algorithm 1). Figure 3 illustrates our H-STAR approach. Unlike, DATER (Ye et al., 2023) and TabQSLify (Nahid and Rafiei, 2024), which use textual reasoning and text-to-SQL respectively, our framework integrates both reasoning types in a complementary manner. Our reasoning step dynamically uses symbolic techniques for quantitative questions, overcoming the limits of pure textual reasoning. H-STAR operates in two main stages: 1) Table Extraction and 2) Adaptive Reasoning."}, {"title": "2.1 Table Extraction", "content": "Two-dimensional tables consist of columns (attributes) and rows (values). Table extraction involves a two-step reasoning chain: (1) column extraction and (2) row extraction (see Figure 3). As illustrated in Algorithm 1, for a given table T, table extraction returns a table TCR (Steps 1-8) based on the input query Q.\nColumn Extraction. As shown in Figure 3, H-STAR uses a 'multi-view' technique for column extraction, working with both the original table T (Step 1) and its transposed form TT (Step 2). This ensures that the LLM semantically understands and extracts correct attributes regardless of the table structure. Given an input table T and the question Q ('How long did it take the New York Americans to win the National Cup after 1936?'), the LLM employs colsql to generate a SQL query to extract columns 'year' (C1). This is followed by the text-based column verification step coltext on TT returning a new set of columns C2 (\u2018year', \u2018national cup'). Table T is filtered for columns 'year', \u2018national cup' (C') derived from Steps 1 and 2 to obtain a column-filtered table Tc (Step 4).\nRow Extraction. After obtaining the filtered table Tc with relevant columns, H-STAR proceeds to the row extraction phase culminating in the query-specific table TCR (Steps 5-8). The filtered table Tc has fewer tokens than T, enabling more efficient row extraction. The SQL query generated by rowsql on the column-filtered table Tc extracts the relevant rows R\u2081 ('row 18'). While it returns the appropriate row, it is not sufficient to answer the question. To overcome these limitations, we use text-based row verification rowtext to obtain the missing rows, ultimately retrieving 'row 1' and 'row 18' (R2). Combining R\u2081 and R2 gives the relevant rows R\u2032 (\u2018row 1', \u2018row 18'), resulting in the final table TCR."}, {"title": "2.2 Adaptive Reasoning", "content": "To leverage the strengths of textual reasoning while overcoming its quantitative limitations, we introduce an adaptive reasoning framework. Assessing the nature of the question, the LLM incorporates symbolic reasoning only when required. For instance, in Figure 3, when faced with a question like \"How long did it take the New York Americans to win the National Cup after 1936?\", LLM requires a count as an answer, and our pipeline prioritizes symbolic reasoning (fsql) through SQL-generated code. This code is executed on a SQL engine, enabling precise answers from the table TCR. The output from this SQL-based evaluation Ey serves as additional evidence, enhancing the textual reasoning process (ftext). By explicitly integrating this quantitative approach, our algorithm effectively addresses LLMs' limitations in handling mathematical operations, thereby improving the accuracy and robustness when tackling questions that demand numerical reasoning."}, {"title": "3 Experiments", "content": "Benchmark Datasets. We evaluate our method on three datasets covering fact verification, short-form, and long-form question-answering tasks using in-context examples. (a) TabFact (Chen et al., 2019): A fact verification benchmark utilizing Wikipedia tables. We evaluate the test-small set, containing 2,024 statements and 298 tables, with each statement labeled as Entailed (\"True\") or Refuted (\"False\"). (b) WikiTQ (Pasupat and Liang, 2015): The WikiTableQuestions (WikiTQ) dataset involves question-answering tasks over semi-structured Wikipedia tables. It includes a standard test set with 4,344 table-question pairs. (c) FeTaQA (Nan et al., 2022): FeTaQA (Free-Form Table Question Answering) comprises free-form questions requiring synthesis from multiple table sections for responses. It demands advanced reasoning and profound tabular data understanding, evaluated on a test set of 2003 samples.\nEvaluation Metrics. We tailor our evaluation metrics based on the task and dataset characteristics. For fact-verification datasets like TabFact, we evaluate performance using binary classification accuracy. For short-form question-answering datasets such as WikiTQ, we assess accuracy by measuring the exact match between predicted outputs and the gold-standard answers. For more complex tasks in FeTaQA, which involve long-form question answering, we evaluate performance using ROUGE-1, ROUGE-2, and ROUGE-L metrics (Lin, 2004), comparing predicted outputs against the long-form answers.\nLLM Models. In our research, we use state-of-the-art large language models (LLM) such as PaLM-2 (Anil et al., 2023) and GPT-3.5-Turbo (Brown et al., 2020; OpenAI, 2023) for table reasoning tasks. Our model inputs include in-context examples, the table, and the question for each step of the pipeline. Previous studies, such as Chain-of-Table and TabSQLify, influenced our model choices to ensure a fair comparison. All baselines with PALM-2 are from the propriety PALM-2 model, which is different from the public PALM-2 API (used in H-STAR and TabSQLify).\nBaseline Methods. We compare our method with (a) generic reasoning based on language models, and (b) table-manipulation reasoning based on language models.\n(a) Generic Reasoning. These methods direct the LLM to carry out the required downstream task based on the information from the table and the input question. They include End-to-End QA, which offers only the table and the question, and Few-Shot QA, which involves a few examples with the table-question-answer triplet alongside the table and the question. Chain-of-Thought prompts the LLMs to provide a supporting reasoning chain that leads to the answer.\n(b) Table Manipulation. These techniques involve several steps, with the initial stage dedicated to automatically pre-processing the table for the reasoning task. BINDER utilizes in-context examples to produce SQL or Python code containing LLM API calls to generate answers. DATER directs the language model to break down tables and questions, applying reasoning to the decomposed tables based on sub-questions. Chain-of-Table employs the table as an intermediate output in its reasoning process, iterating through tasks until the final answer is obtained. TabSQLify uses SQL to trim the table and then reasons based on the reduced table."}, {"title": "3.1 Results and Analysis", "content": "Table 1 compares the performance of different methods on the table reasoning tasks, TabFact, and WikiTQ datasets, across several LLMs including GPT-3.5-Turbo and PaLM-2. This comparison involves evaluating against generic reasoning, table manipulation techniques, and our method, H-STAR. Appendix A provides results on the FeTaQA dataset in addition to comparisons with other baseline models on the WikiTQ and TabFact dataset.\nAnalysis. (a) On the WikiTQ dataset, H-STAR demonstrates superior performance with both GPT-3.5-Turbo and PaLM-2 models, surpassing all baseline methods. Specifically, H-STAR achieves an accuracy of 68.85% on WikiTQ with GPT-3.5-Turbo, marking a significant improvement of 17.01% over the vanilla GPT-3.5-Turbo model and a 4% increase over the previous best method, TabSQLify. (b) On the TabFact dataset, H-STAR outperforms all methods with GPT-3.5-Turbo, attaining an accuracy of 83.74%, which is a 13.29% improvement over the vanilla model and a 4% margin over the previous best, Chain-of-Table. However, with PaLM-2, H-STAR achieves comparable performance on Tab-Fact while outperforming Chain-of-Table by 1.2% on WikiTQ."}, {"title": "Efficiency of Table Extraction", "content": "Stage-1 in H-STAR involves the extraction of the table most relevant to the question. Our table extraction method uses a two-step chain to select the relevant columns followed by the rows. Figure 4 showcases the effectiveness of the technique to extract only the relevant information from the table. Figure 4 compares the number of average table cells for the extracted table for H-STAR, H-STAR without row extraction, and H-STAR without column extraction with other baselines.\nAnalysis. In particular, H-STAR significantly reduces the average number of processed table cells from 159 to 18 for the WikiTQ dataset, from 88 to 13 for the TabFact dataset, and from 86 to 10 for the FeTaQA dataset. In contrast, other table manipulation methods, like TabSQLify and DATER yield higher values. Our approach showcases superior effectiveness, notably reducing cell counts across all datasets. This emphasizes how H-STAR effectively reduces irrelevant information, thus focusing LLMs on the right evidence for right reasoning."}, {"title": "Performance on Longer Tables", "content": "Table-CoT (Chen, 2023) highlights that longer table size, with tables exceeding 30 rows, is a significant cause of erroneous generations. This observation is supported by a decline in LLM performance as the number of tokens in the table increases. We classify LLM performance based on total tokens into three groups - small (< 2000 tokens), medium (2000 to 4000 tokens), and large (> 4000 tokens). We then compare H-STAR with other table manipulation methods, including BINDER, DATER, Chain-of-Table, and TabSQLify.\nAnalysis. The results shown in Table 2 emphasize the challenges LLMs face when reasoning over longer tables. Our findings align with previous research, revealing a significant decrease in LLM performance as table sizes increase. While other table manipulation methods experience a drop in performance with larger tables, H-STAR maintains consistent performance across all table sizes."}, {"title": "3.2 Efficiency Analysis", "content": "In Table 3, we study H-STAR by considering the total number of samples generated by the LLMs. BINDER and DATER utilize self-consistency techniques to refine their results, while the Chain-of-Table approach adopts a more iterative sample creation process. Specifically, BINDER generates 50 Neural-SQL samples, implementing self-consistency, while DATER employs self-consistency at each step, resulting in 100 samples. In contrast, the Chain-of-Table method opts for a more resource-efficient strategy, producing 25 samples across three steps: 'Dynamic Plan', 'Generate Args', and 'Query'. TabSQLify generates the fewest samples, with one generation each for the table decomposition and query steps. Using single outputs can result in a decrease in accuracy when the model fails to return an answer. Thus, H-STAR incorporates self-consistency measures by generating two outputs at each stage to ensure output validity. Each step, except Query, involves two generations for both SQL and Text, i.e., a total of four steps for column and row retrieval. These additional steps serve as a precaution against abnormal LLM generations. Overall, H-STAR is efficient and only uses 6-10 sample generations."}, {"title": "4 Ablation Study", "content": "We conduct an ablation study to measure the contribution of each step in the pipeline and assess the impact of integrating symbolic-textual and adaptive reasoning in H-STAR."}, {"title": "4.1 Importance of Each Stage", "content": "To evaluate the significance of the two primary stages in H-STAR: a) Table extraction and b) Adaptive reasoning, we conducted an ablation study. This study involved systematically removing one stage at a time. For the first condition, the final adaptive reasoning stage was substituted with a basic chain-of-thought reasoning process, while maintaining the table extraction stage. Conversely, the second condition retained the adaptive reasoning stage but omitted the table extraction stage, specifically by removing the row extraction and column extraction steps individually and then collectively. A detailed assessment of the contribution of each stage to the overall performance of the method is shown in Table 4. It highlights the importance of each step in the pipeline. Removing any step can result in a performance loss except for a marginal increase in the case of row extraction for TabFact."}, {"title": "4.2 Impact of a Hybrid SQL-Text Approach", "content": "We perform a quantitative evaluation of the impact of the hybrid approach in H-STAR. Figures 5, 6 depict how using only one of the approaches results in insufficient outputs. Furthermore, it is evident from Table 5 that not employing a hybrid approach (H-STAR) results in a significant performance loss."}, {"title": "5 Where do LLMs fail?", "content": "This section examines errors in H-STAR, contrasts the failure points with existing approaches, and summarizes our key findings."}, {"title": "5.1 Error Analysis on H-STAR", "content": "The disjoint, step-wise nature of H-STAR enables identifying and analyzing failures. Table 6 shows 100 randomly selected incorrect answer instances from the Tabfact and WikiTQ datasets. In this study, 'Missing Columns' and 'Missing Rows' refer to missing necessary columns and rows, respectively. 'Incorrect Reasoning' occurs when H-STAR extracts the correct table, but LLM fails to produce the correct answer. 'Incorrect Annotations' include semantically identical answers in different formats, ambiguous questions, and incorrect gold answers. Table 6 shows that for 100 TabFact failures, 2% are missing columns, 9% missing rows, 79% incorrect LLM reasoning, and 10% incorrect annotations. For WikiTQ, 6% are missing columns, 17% missing rows, 54% incorrect reasoning, and 23% incorrect annotations."}, {"title": "5.2 More Error Analysis: H-STAR vs Others", "content": "Table 7 compares errors in H-STAR, with TabSQLify, and BINDER. We use 100 WikiTQ samples where TabSQLify fails. Many errors with TabSQLify (6/100 missing columns, 56/94 missing rows) result from incorrect row extraction. Out of 100, TabSQLify correctly extracts the table for 38 samples, but incorrectly reasons on 29 (76%). BINDER does not extract tables but generates multiple neural SQL queries for table reasoning tasks. However, its output shows that it fails to reason correctly in 70 out of 100 cases. In contrast, H-STAR proves more effective in both table extraction and reasoning, achieving correct generations in 43 samples. Additionally, it demonstrates fewer errors in table extraction (4/100 columns; 16/96 rows) and reasoning (27/80, i.e., 34% of samples)."}, {"title": "5.3 What did we learn?", "content": "Firstly, our evaluation demonstrates that our hybrid approach achieves substantial improvements, surpassing the performance of previous state-of-the-art methods across various table reasoning tasks. Secondly, the quantitative analysis demonstrates that our 'multi-view' approach extracts tables with reduced irrelevant data, resulting in more concise tables. Furthermore, the qualitative analysis highlights fewer errors in our table extraction method compared to prior approaches, confirming a decrease in extraneous information.\nThirdly, our analysis indicates consistent performance even with longer tables, emphasizing our method's effectiveness in accurately extracting relevant information by filtering out noise. The adaptive reasoning component boosts performance, especially for longer tables. Lastly, the ablation study shows that breaking down the task into sub-tasks significantly enhances the overall performance, with each sub-task playing a crucial role in achieving superior results. Moreover, it highlights the constraints of relying solely on either text or SQL approaches, which are effectively addressed by our H-STAR approach. Together, these findings emphasize the substantial advantages of our hybrid and multi-view approach in addressing complex table reasoning tasks."}, {"title": "6 Related Work", "content": "Table reasoning tasks require the ability to reason over unstructured queries and structured or semi-structured tables. Traditional approaches like TAPAS (Herzig et al., 2020), TAPEX (Liu et al., 2021), TABERT (Yin et al., 2020), TURL (Deng et al., 2022), PASTA (Gu et al., 2022) work on pre-training language models jointly on large-scale tabular and text data to reason in an end-to-end manner. Advancements in LLMs allow them to learn from in-context examples, reducing inference costs. Text-to-SQL (Rajkumar et al., 2022) and Program-of-Thought (Chen et al., 2023) use symbolic methods to solve table-based tasks via Python/SQL programs. Textual-based reasoning techniques like Table-CoT (Chen, 2023) and Tab-CoT (Ziqi and Lu, 2023) extend prompting methods such as zero and few-shot CoT for tabular reasoning.\nDecomposing problems into smaller, manageable tasks has proven effective in solving complex reasoning challenges (Zhou et al., 2022; Khot et al., 2022). Recent techniques in table reasoning follow this approach, either by breaking tasks into fixed sub-tasks (Cheng et al., 2022; Ye et al., 2023; Nahid and Rafiei, 2024) or by employing iterative methods (Jiang et al., 2023; Wang et al., 2023). BINDER (Cheng et al., 2022) is an SQL-based approach that modifies SQL statements to include LLM API calls within SQL statements. DATER (Ye et al., 2023) and TabSQLify (Nahid and Rafiei, 2024) are table decomposition methods that use semantic and symbolic reasoning respectively. Chain-of-Table (Wang et al., 2023) uses a textual reasoning approach to update tables iteratively. The final table post-update is used for final reasoning. Unlike H-STAR, these techniques depend either on symbolic or semantic reasoning but not both."}, {"title": "7 Conclusion", "content": "In this study, we present H-STAR, a novel method that effectively integrates semantic and symbolic approaches, demonstrating superior performance compared to existing methods in tasks involving table reasoning. Our approach involves a two-step LLM-driven process: firstly, employing 'multi-view' table extraction to retrieve tables relevant to a query, and then, implementing adaptive reasoning to select the optimal reasoning strategy based on the input query. We address prior bottlenecks with efficient extraction and reasoning, leading to improved overall performance, particularly on longer tables. Our results highlight the need to move beyond relying solely on either technique and demonstrate the effectiveness of an integrated approach that combines the advantages of both methods. Future directions involve testing the adaptability of our methods to semi-structured, complex hierarchical, and relational tables. Enhancing the reasoning process through techniques such as self-consistency and self-verification shows promising potential."}, {"title": "Limitations", "content": "Our current work has focused primarily on a subset of table reasoning tasks using datasets sourced from Wikipedia. While this has laid a solid foundation, it limits exploration into diverse reasoning tasks such as table manipulation, text-to-table generation, and table augmentation, which could provide valuable insights and enhance our approach's capabilities. Additionally, our method's generalizability is confined to Wikipedia-based datasets, restricting its application to other domains that require specific domain knowledge, which our current approach lacks. Extending our approach to different domains may necessitate integrating domain-specific knowledge to ensure effective reasoning.\nFurthermore, our evaluation has been limited to relatively straightforward table structures. Handling more complex data representations such as semi-structured tables, hierarchical tables, and relational databases remains unexplored territory. These structures present unique challenges that our current approach may not effectively address. Finally, our study has focused solely on the English language, potentially limiting applicability to languages with different linguistic complexities that we have not accounted for."}, {"title": "Ethics Statement", "content": "We, the authors, affirm that our work adheres to the highest ethical standards in research and publication. We have carefully considered and addressed various ethical issues to ensure the responsible and fair use of computational linguistics methodologies. To facilitate reproducibility, we provide detailed information, including code, datasets (all publicly available and in compliance with their respective ethical standards), and other relevant resources. Our claims align with the experimental results, though some stochasticity is expected with black-box large language models, which we minimize by maintaining a fixed temperature. We provide comprehensive details on annotations, dataset splits, models used, and prompting methods, ensuring our work can be reliably reproduced."}, {"title": "B Input Table Format", "content": "The input table format changes depending on the type of reasoning used. Since H-STAR uses a hybrid approach, the input table format also varies depending on the type of reasoning used. Besides the question and the in-context examples, the table is accompanied by a list of columns and a table caption if available. Providing more context aids in better semantic understanding (Singha et al., 2023; Sui et al., 2024)."}, {"title": "B.1 Symbolic Reasoning", "content": "We adopt the table prompt format from previous SQL-based methods Text-to-SQL (Rajkumar et al., 2022; Cheng et al., 2022; Nahid and Rafiei, 2024). We include (1) the table schema containing CREATE TABLE followed by the schema, (2) the table header and all the rows separated by tabs, and (3) the list of columns along with the corresponding query. If the prompt exceeds the context limit, we truncate the table rows to fit within the limit. Example of the input table:\nCREATE TABLE 2012-13 Exeter City F.C. season (\nrow_id int,\nname text,\nleague int,\ntotal int)\n/\nAll rows of the table:\nSELECT * FROM w;\nrow_id name league total\n1 danny coles 3 3\n4 john o'flynn 11 12\n8 jamie cureton 20 20\n/\ncolumns: ['name', 'league', 'total']"}, {"title": "B.2 Textual Reasoning", "content": "We convert the tables into linear, sequential text. Continuing with the format of previous textual reasoning methods (Ye et al., 2023; Chen, 2023; Wang et al., 2023), we adopt the PIPE encoding i.e. plain text separated by 'l'. Furthermore, we append the table with the caption and the list of columns similar to the format in Section B.1. Example of the input table:\ntable caption: 2012-13 Exeter City F.C. season"}, {"title": "C Hyperparameters", "content": "Table 13 provides the details of PaLM-2 hyperparameters used for the WikiTQ and TabFact datasets whereas Table 14 showcases the hyperparameter setting for GPT-3.5-Turbo. The hyperparameters 'samples' indicate the number of outputs taken for each step of the pipeline whereas the 'examples' indicate the number of few-shot demonstrations for each step."}]}