{"title": "RAGent: Retrieval-based Access Control Policy Generation", "authors": ["Sakuna Harinda Jayasundara", "Nalin Asanka Gamagedara Archchilage", "Giovanni Russello"], "abstract": "Manually generating access control policies from an organization's high-level requirement specifications poses significant challenges. It requires laborious efforts to sift through multiple documents containing such specifications and translate their access requirements into access control policies. Also, the complexities and ambiguities of these specifications often result in errors by system administrators during the translation process, leading to data breaches. However, the automated policy generation frameworks designed to help administrators in this process are unreliable due to limitations, such as the lack of domain adaptation. Therefore, to improve the reliability of access control policy generation, we propose RAGent, a novel retrieval-based access control policy generation framework based on language models. RAGent identifies access requirements from high-level requirement specifications with an average state-of-the-art F1 score of 87.9%. Through retrieval augmented generation, RAGent then translates the identified access requirements into access control policies with an F1 score of 77.9%. Unlike existing frameworks, RAGent generates policies with complex components like purposes and conditions, in addition to subjects, actions, and resources. Moreover, RAGent automatically verifies the generated policies and iteratively refines them through a novel verification-refinement mechanism, further improving the reliability of the process by \u2248 3%, reaching the F1 score of 80.6%. We also introduce three annotated datasets for developing access control policy generation frameworks in the future, addressing the data scarcity of the domain.", "sections": [{"title": "1 Introduction", "content": "According to the 2023 Verizon data breach incident report, 74% of data breaches involve human elements [75]. These include accidental human errors system administrators make that can result in severe data breaches [59]. For example, in July 2023, Microsoft AI researchers released a public GitHub repository, providing users access to several Al models via an Azure storage bucket URL (Uniform Resource Locator) [59]. However, instead of allowing users only to download the models, the storage account administrator had mistakenly given \"full access\" to the entire Azure storage account [59]. Consequently, 38TB of company data, including passwords for Microsoft services, were leaked to the general public [59].\nSuch incidents based on human errors often occur when system administrators manually generate access control policies from the organization's high-level requirement specifications [8,29,34]. These requirements are written by security experts [8, 29] in documents explaining how information access should be managed within the organization [52]. However, they are often unstructured, ambiguous, and written in legal language that non-security experts like system administrators find difficult to understand [29, 52]. Consequently, manually identifying access control requirements from these documents and generating access control policies become laborious and error-prone, resulting in human errors, that may lead to data breaches [33, 34, 52].\nTherefore, to help administrators correctly generate access control policies, previous research has introduced automated policy generation frameworks [5\u20137, 27, 48, 50-52, 69-72, 80]. Those frameworks automatically process high-level requirement specification documents, identify Natural Language Access Control Policies (NLACPs), and extract useful information (e.g., policy components like subjects, actions, and resources) for building access control policies from them, using machine learning (ML) and natural language processing (NLP) techniques [50, 71, 72, 80, 81]. They remove the human factor almost entirely from the policy generation process, attempting to reduce access control failures due to human mistakes [36].\nHowever, the existing policy generation frameworks are far from accurate due to their limitations, indicating that they cannot be used without human involvement [18,33,34,50,52]. For instance, most deep learning-based frameworks were not specifically adapted for extracting access control policy components from NLACPs [5, 6, 50, 80]. Instead, they mostly rely on general-purpose NLP models like Semantic Role"}, {"title": "2 Related Work", "content": "Previous research has proposed numerous automated policy generation frameworks [27, 48-52, 69\u201372, 80, 81], aiming to identify NLACPs from high-level requirement specification documents and extract their policy components. They often follow a three-step approach: (1) pre-processing [69, 70, 72], (2) NLACP identification using text classification techniques such as Support Vector Machines (SVM) [70, 72], and (3) policy components (i.e., subjects, actions, and resources) extraction through information extraction techniques such as SRL [49, 50, 80]. System administrators can then use the extracted policy components to formulate machine-executable policies in standard access control languages such as XACML (eXtensible Access Control Markup Language) [57].\nXiao et al. developed Text2Policy [81], utilizing syntactic pattern matching to identify NLACPs and shallow parsing based on four semantic patterns to extract policy components from them. However, its reliance on a limited set of semantic patterns restricts its ability to extract components from NLACPs that do not conform to these patterns.\nTherefore, Slankas et al. [69\u201372] proposed a policy generation framework, Access Control Relation Extraction (ACRE) as a more reliable alternative to Text2Policy. ACRE utilizes the k-Nearest Neighbours (k-NN) algorithm to identify NLACPs with an average F1 score of 72%. Moreover, it employs dependency parsing with bootstrapping [17] to extract ACRs and their policy components from NLACPS based on their grammatical structures [72]. The bootstrapping technique enables ACRE to dynamically expand the dependency pattern database (i.e., grammatical relationships"}, {"title": "3 RAGent", "content": "The main objective of RAGent, is to improve the reliability of the access control policy generation process. It utilizes transformer-based LMs to generate access control policies in six steps, as shown in Figure 1.\n1. Step 1: Pre-processing\n2. Step 2: NLACP Identification\n3. Step 3: Information Retrieval\n4. Step 4: Access Control Policy Generation\n(a) Step 4.1 Post-processing\n5. Step 5: Access Control Policy Verification\n6. Step 6: Iterative Refinement\nAccording to Figure 1, the input to RAGent is the organization's high-level requirement specification document (i.e., input document) written by a security expert in NL. In this research, we assume that the input document does not contain any requirement conflicts, similar to previous research [50, 52, 70, 80]. First, RAGent pre-processes the sentences of the input document. Then, RAGent classifies the pre-processed sentences to identify NLACPs in the second step. Once an NLACP is identified, in the third step, RAGent retrieves organization-specific information relevant to translating the identified NLACP into an access control policy. This information will be combined with the NLACP and fed to the policy generation module in the fourth step to generate its access control policy containing ACRs with five policy components (i.e., subjects, actions, resources, purposes, and conditions) and their rule decisions (i.e., allow or deny) through RAG. RAGent generates the access control policy as a structured representation of the NLACP [16]. By translating the NLACP into a structured representation rather than utilizing a standard access control language, RAGent becomes universally applicable. It enables any organization, regardless of the language utilized in their authorization system, to use RAGent, and seamlessly transform the generated representation into their specific access control language. Then, RAGent post-processes the generated policy to ensure that it only contains policy components that align with the authorization system.\nAfter generating the access control policy, it is verified in the fifth step using a novel policy verification technique to decide whether or not it is correct. If the generated access control policy is verified as correct, it can be applied to the authorization system. On the other hand, if the policy is verified as incorrect, the verifier provides the reason (i.e., error type) for it being incorrect as feedback. In that case, as shown in Figure 1, the feedback is used to address the identified error(s) iteratively and generate the correct refined policy within n iterations. If the policy is still incorrect even after n rounds, it will be sent to the administrator with feedback for the manual refinement. This helps administrators easily find erroneous ACRs and refine them before adding them to the authorization system."}, {"title": "3.1 Step 1: Pre-processing", "content": "Once a high-level requirement specification document is provided as the input, RAGent first pre-processes it by segmenting its paragraphs. Then, RAGent performs coreference resolution on those paragraphs, which has rarely been performed in existing policy generation frameworks [50,72].\nCoreference resolution is the task of determining whether or not the two expressions refer to the same entity in a sentence or a paragraph [52]. For example, consider an NLACP, \"Nurses are allowed to read the prescription, but they are not allowed to change it.\". In the above NLACP, \"they\" refers to \"Nurses\" and \"it\" refers to \u201cthe prescription\". However, without the first rule (i.e., \u201cNurses are allowed to read the prescription\u201d), the second rule loses its meaning (i.e., \"They are not allowed to change it\") since \"They\" and \"it\" are unknown. Therefore, once the coreferences are resolved,"}, {"title": "3.2 Step 2: NLACP Identification", "content": "High-level requirement specification documents contain access control requirements (i.e., NLACP) as well as sentences that are not related to access control (i.e., non-NLACP) [52]. Therefore, only NLACPs should be identified to translate them into access control policies [27, 50, 52, 80, 81].\nWe formulate this NLACP identification as a binary text classification task where the goal is to classify each sentence into either NLACP or non-NLACP categories [27, 50, 52, 69-72, 80]. When classifying a sentence, it is important to in-"}, {"title": "3.3 Step 3: Information Retrieval", "content": "Even though the NLACPs are correctly identified in the previous step, they should be translated into access control policies accurately for them to be enforceable. Also, the generated access control policies should address the organization's pre-defined entities (e.g., users and resources). However, existing frameworks often fail to produce such access control policies when not trained on organization-specific access requirements, rendering them unenforceable [52]. Also, that training should repeat each time the framework is being employed in new organizations (i.e., change in contexts) and each time new users/roles or resources are introduced to the organization's authorization system, which can be expensive. Therefore, instead of training RAGent (i.e., updating its parametric memory) repetitively to adapt it to specific contexts, RAGent uses organization-specific information as a non-parametric memory [21], to generate policies based on them (i.e., RAG). To this end, we consider pre-defined subjects, actions, resources, purposes, and conditions as organization-specific information since they are the main policy components of an ACR [10, 11].\nHowever, all the information, like all subjects and resources defined in the system, may not be relevant to generating an access control policy from a specific NLACP. Therefore, to incorporate only required information to generate policies, RAGent uses dense retrieval [21]. Dense retrieval involves representing the organization-specific information as high-dimensional embedding vectors and retrieving them based on their similarity to the given NLACP [21]. Since it considers the semantic meaning (encoded as embedding vectors) rather than word matches (as in sparse retrieval [21]) when retrieving information, it is also robust to lexical variations such as synonyms. For example, when retrieving relevant subjects for translating the NLACP, \u201cGraduate teaching assistant can read grades.\", sparse retrieval techniques like BM25 [61] will search for the subjects containing words \u201cgraduate\u201d, \u201cteaching\u201d or \u201cassistant\u201d, overlooking synonymous subjects like \"GTA\". In contrast, since dense retrieval uses semantic similarity between entities and NLACP via embeddings, it can identify such synonymous subjects even though they do not match the exact terms of the NLACP.\nTherefore, to facilitate dense retrieval, first, RAGent generates embedding vectors for subjects, actions, resources, purposes, and conditions of the organization. To this end, it\""}, {"title": "3.4 Step 4: Access Control Policy Generation", "content": "After identifying the NLACPs of the high-level requirement specification documents (Step 2) with the relevant information (Step 3), RAGent translates them into access control policies through structured information extraction [16], as shown in Figure 1. In other words, RAGent extracts information from an NLACP according to a structure/hierarchy (e.g., JSON) delineating an access control policy. It represents underlying ACRs of the NLACP, access decisions, and policy components (i.e., subject, actions, resource, purpose, and condition) of each rule [49, 72]. For example, consider an NLACP, \"The doctor can write prescriptions, but the nurse cannot.\u201d. Given that NLACP, the RAGent generates the access control policy according to a structure as shown in Figure 2, maintaining the relationships between policy components within each ACR [16] (We removed the empty purpose and condition fields of each rule for clarity). This can easily be transformed into any standard access control language, such as XACML [11], when applying it to the authorization system.\nTo generate access control policies from NLACPs, previous research has often used general-purpose SRL models, leading to several limitations like extracting unwanted entities [50,52] and not being able to extract purposes and conditions (Section 2). Therefore, to avoid such limitations, in RAGent, we utilize domain-adapted Large LMs (LLMs), which have been proven effective in structured information extraction from input texts [16]. They show high efficacy in tasks that require"}, {"title": "3.4.1 Step 4.1: Post-processing", "content": "After generating the access control policy as a structured representation of the NLACP, RAGent then post-processes the policy using the stored information in Step 3. This step acts as a validation step to ensure that the generated policy aligns with information pre-defined on the system. To this end, RAGent retrieves the most similar entity for each policy component of the access control policy from the vector database and replaces the policy component with it."}, {"title": "3.5 Step 5: Access Control Policy Verification", "content": "Even if an LM (or any ML model) is fine-tuned using domain-specific datasets to generate access control policies from NLACPs, it might sometimes fail to generate the correct policies [27,54,68]. This could stem from various factors, such as hallucinations [54,66], the complexities and ambiguities of the NLACPS [27,52], or due to the significant difference between the NL access control requirement and the access control policy [66]. As a solution, once the access control policy is generated, it should be verified to (1) check whether or not it is correct [14,54,68] (2) refine the policy automatically if it is incorrect, and (3) alert administrators about errors of the generated policy (if it cannot be refined automatically), allowing them to refine it before adding it to the system [82]. Despite being crucial for a reliable access control policy generation, none of the existing frameworks conduct this verification and refinement step, leaving a significant gap in access control policy generation research [27,50,52, 70, 72, 80]. Therefore, by bridging this gap, RAGent first introduces a verifier to identify the incorrectly generated access control policy based on the joint representation of the NLACP and its structured representation [54] and provide feedback.\nWe develop our verifier as a multi-class sequence pair classifier that identifies correct and incorrect policies with reasons, improving the explainability of our system. It outputs a probability distribution over the m classes when the joint representation of the NLACP, and the access control policy generated in Step 4 is provided as the input. To this end, instead of directly providing the generated policy as part of the input, RAGent reconstructs the policy as an NL sentence using a pre-defined template. For example, consider a simple NLACP \"The doctor is allowed to read records.\". Once its access control policy is generated as [{'decision': 'allow', 'subject': 'doctor', 'resource': 'record', 'purpose': 'none', 'condition': 'none'}], it will be transformed into doctor can read records according to the template \u201c{subject} can {action} {resource}\". It helps the verifier to identify the semantic differences (i.e., errors) between the NLACP and the generated policy easily, as the generated policy is now expressed as an NL sentence (similar to the NLACP) rather than in a structured \"code-like\" format [66, 68].\nThen, the reconstructed policy and its NLACP are provided to the verifier to decide whether the policy represents its NLACP (i.e., correct access control policy) or, if not, what makes the policy incorrect according to 11 error types (i.e., incorrect decision, incorrect subject, missing subject, incorrect\""}, {"title": "3.6 Step 6: Iterative Refinement", "content": "After the verifier identifies the error category (if the generated policy is incorrect), it will be converted into an instruction (i.e., \"refinement instruction\u201d shown in Appendix A.2) that asks the policy generation module to correct the error and re-output the refined policy. RAGent repeats this automatic verification-generation process at most n times until the verification result changes into \"correct\" (i.e., the correct policy for the given NLACP is generated). If the verification result still indicates the refined policy is incorrect even after n iterations, the final refined policy and the error category are sent to the administrator as feedback. It will help the administrator to identify the error of the incorrect policy and refine it manually before adding it to the authorization system [82]."}, {"title": "4 Datasets", "content": "As we described, we fine-tune LMs utilized in RAGent using domain-related datasets to improve the reliability of the process [52]. This section presents those datasets, namely the access control policy dataset (Section 4.1) used to train and evaluate the NLACP identification and policy generation modules, access control policy verification dataset (Section 4.2) used to train and evaluate the verifier, and access control policy refinement dataset (Section 4.3) used to train the policy generation module to refine incorrectly generated policies."}, {"title": "4.1 Access Control Policy Dataset", "content": "While the advancements in ML/NLP domain [74] help develop reliable automated access control policy generation frameworks, effective adaptation of those advancements is restricted by the lack of domain-related datasets [6, 80]. The main reason for this gap is the confidential nature of organizations' high-level access requirements, except for a few projects that have made their access requirements public [6]. Therefore, to address this gap, we developed a synthetic access control policy dataset based on real-world high-level requirement specification datasets for fine-tuning LLM [72]. The real-world datasets are first introduced by Xiao et al. [81] and Slankas et al. [72], containing sentences from high-level requirement specifications from three systems, namely iTrust, IBM course registration, and the CyberChair conference management. Each sentence is first labeled NLACP (1) or non-NLACP (0). We call these annotations \u201csentence type annotations\u201d. Then, each NLACP sentence is divided into its ACRs and annotated by representing only three policy components: subject, action, and resource. We call those annotations \"entity annotations\u201d. However, not only do the provided entity annotations not contain the access decision of each ACR of the NLACP [80], but they also do not represent policy components that provide contextual information to the NLACP, such as purposes and conditions [11]. As a result, the provided entity annotations contradict the definitions provided by previous research [11,84]. For example, according to Slankas et al.'s annotations, the NLACP \"The organisation may use email addresses to answer inquiries.\" has two sets of entity annotations representing two ACRs: \u201c[Organisation]subject may [use]action [email addresses]resource\" and \u201c[Organisation]subject [answer]action [inquiries]resource\u201d. However, the second set of annotations is incorrect as \"answer inquiries\" should be a purpose, not a separate ACR [11,84]. Instead, the correct annotation should contain one set of an-"}, {"title": "4.2 Access Control Policy Verification Dataset", "content": "The access control policy verification dataset is used to train and evaluate the access control policy verifier discussed in Section 3.5. Therefore, the verification dataset should contain sequence pairs representing NLACP and its correct access control policy (i.e., negative sequence pairs) as well as pairs containing NLACP and its incorrect access control policy (i.e., positive sequence pairs). Since we have the entity annotations for each NLACP from the access control policy dataset (Table 2), we already have the correct access control policies for the NLACPS to create the negative sequence pairs. Therefore, to generate incorrect policies for each NLACP, we augment the correct policies by manipulating their properties according to each error type mentioned in Section 3.5.\nWe applied four manipulation techniques to the correct policies to obtain incorrect policies. They are (a) Decision change. Randomly change the decision of an ACR from allow to deny or vice versa. (b) Policy component replacement. Randomly replace the value of a policy component (i.e., sub-"}, {"title": "4.3 Access Control Policy Refinement Dataset", "content": "The access control refinement dataset is used to teach the access control policy generation module how to refine an incorrect policy identified by the access control policy verification step. Therefore, each training example of this dataset should contain the NLACP, its incorrectly generated access control policy, the error type of the incorrectly generated policy, and the correct access control policy of the NLACP as the label. All the mentioned components of a training example can be derived from the previously described access control policy dataset (Section 4.1) and the access control policy verification dataset (Section 4.2). For instance, the NLACP and its correct access control policy can be retrieved from the access control policy dataset. Also, the incorrect access control policy for the same NLACP and the error type can be retrieved from the access control policy verification dataset. The structure of a training example that combines all the mentioned components is shown in Appendix A.2.\nFinally, the dataset is combined with the access control policy dataset (Section 4.1) and used to fine-tune the access control policy generation module."}, {"title": "5 Evaluation and Results", "content": "After training the components of RAGent, as described in Section 3, we evaluate their reliability using the F1 score (i.e., the harmonic mean of the precision and recall [41]). Particularly, we evaluate the RAGent's NLACP identification (Section 5.2.1) and access control policy generation (with iterative refinement) (Section 5.2.2) performance on each document-fold and the test set of the \"Overall\" dataset (Table 2). Moreover, we evaluate the RAGent's reliability in access control policy verification (Section 5.2.3) using the access control policy verification dataset (Section 4.2)."}, {"title": "5.1 Evaluation", "content": "After training the components of RAGent, as described in Section 3, we evaluate their reliability using the F1 score (i.e., the harmonic mean of the precision and recall [41]).\nParticularly, we evaluate the RAGent's NLACP identification (Section 5.2.1) and access control policy generation (with iterative refinement) (Section 5.2.2) performance on each document-fold and the test set of the \"Overall\" dataset (Table 2). Moreover, we evaluate the RAGent's reliability in access control policy verification (Section 5.2.3) using the access control policy verification dataset (Section 4.2)."}, {"title": "5.2 Experiment Results", "content": "This section presents evaluation results of RAGent and compares them with the existing frameworks [49,50,52,71,72,80]."}, {"title": "5.2.1 NLACP Identification", "content": "We evaluate the performance of RAGent in identifying NLACPs using both document folds and the \"Overall\" datasets introduced in Section 4.1. The obtained evaluation results compared with the existing access control policy generation frameworks in terms of F1-score are shown in Table 3.\nAccording to Table 3, RAGent achieves an average document fold F1 score of 87.9%, outperforming the existing frameworks in almost all the document folds. Moreover, it attains the F1 score of 91.9% on the test set of the \"Overall\" dataset."}, {"title": "5.2.2 Access Control Policy Generation", "content": "We evaluate the access control policy generation performance of RAGent in terms of access control policy component extraction and access control rule generation. Access control policy component extraction focuses on extracting individual policy components for each \u201caction\u201d (i.e., subject, resource, purpose, and condition) of an NLACP [27, 50, 52, 70, 72, 80]. On the other hand, access control rule generation focuses on translating the ACRs of an NLACP into a structured representation, as mentioned in Section 3.4.\nThe obtained results for access control policy component extraction in terms of F1 score are reported in Table 4 under two settings of RAGent: SAR and DSARCP. In the SAR setting, we only consider subjects (S) and resources (R) for a given action (A) to calculate the F1 score, enabling the comparison with prior research [50, 52, 70, 72, 80]. In the DSARCP setting, we include access decisions (D), purposes (P), and conditions (C) in addition to subjects and resources when computing the F1 score for each action of NLACPs.\nHowever, we cannot directly compare the performance of"}, {"title": "5.2.3 Access Control Policy Verification", "content": "To evaluate the verification performance of the RAGent's verifier, we utilized the \"Overall\" dataset shown in Table 2, as it contains different types of NLACPs from different domains (e.g., healthcare, education, etc.). After creating the verification dataset based on the \"Overall\" dataset according to Section 4.2, we first divided it into train (80%), test (10%) and validation sets (10%) randomly [44]. Then, we train the verifier using the training set, select the best model/checkpoint based on the validation set, and finally, evaluate the verifier's performance in the test set. The obtained evaluation results are shown in Table 6 in terms of average accuracy and F1 scores.\nAccording to Table 6, RAGent's verifier is able to identify incorrect and correct access control policies with F1 scores"}, {"title": "6 Discussion", "content": "We propose a novel retrieval-based access control policy generation framework, RAGent as shown in Figure 1, innovatively adapting pre-trained open-source LMs to the access control policy generation domain. To the best of our knowledge, this is the first framework that utilizes LMs in identifying NLACPs, translating them into access control policies through RAG, and finally, verifying and refining the generated policies automatically using our novel iterative verification-refinement mechanism. Our evaluation suggests that RAGent outperforms all the existing state-of-the-art policy generation frameworks (by 3.2% in NLACP identification and by 39.1% in access control policy component extraction), significantly improving the reliability of access control policy generation [50\u201352,70,72,80]. In this section, we discuss those evaluation results to understand why RAGent performs better than existing frameworks and why it is vital in access control policy generation."}, {"title": "6.1 Limitations", "content": "In this section, we point out the limitations of RAGent and highlight future improvements.\nPolicy quality assurance. To ensure the quality of access control policies, we have to make sure that they are free from conflicts (i.e., consistency), they match the high-level requirements/goals (i.e., correctness), they are not redundant (i.e., minimality), they address actions only relevant to the user (i.e., relevance), and they cover all possible access request scenarios (i.e., completeness) [9]. While RAGent ensures the correctness of the generated policies via verification and two refinement stages (i.e., automatic and manual), it can further be extended to facilitate the other four requirements. To this end, we will use Binary Decision Diagrams (BDD) [32, 46] to improve RAGent further, so that it can evaluate the generated policies in the end to ensure their consistency, minimality, relevance, and completeness as a future work.\nDetailed and necessary contextual information. In this research, we showed that utilizing simple pre-defined entities like subjects and resources as the context would significantly improve the reliability of policy generation. However, more detailed information about the organization can be incorporated to make the context more meaningful than individual entities, improving RAG [21]. Therefore, RAGent can further be improved to process and use detailed information like organizational breakdown structure (OBS) as the context, potentially improving the policy generation reliability.\nFurthermore, RAGent retrieves k (a constant) most similar entities to the NLACP when generating policies through RAG. However, all the retrieved entities may not be required to generate a policy from a particular NLACP. For example, to translate the NLACP \"The doctor can read records.\" RAGent retrieves k = 5 subjects even though the NLACP has only one subject. This irrelevant information may sometimes mislead the LLM to generate incorrect policies [83]. Therefore, we will further improve RAGent to measure the relevancy of retrieved information using an access control-specific retrieval"}, {"title": "7 Conclusion and Future Works", "content": "In this research, we propose RAGent, a retrieval-based access control policy generation framework. It is designed to help system administrators reliably generate access control policies from high-level requirement specification documents, reducing human errors. Utilizing \"small\" open-source transformer-based LMS, RAGent enables its efficient and local deployment within the organization, ensuring the confidentiality of the access control policy generation process. In contrast to existing frameworks, RAGent identifies and translates complex access control requirements containing multiple ACRs with even intricate components like purposes and conditions, in addition to subjects, actions, and resources. We showed that incorporating organization-specific information like subjects and resources pre-defined in the organization provides essential contextual information in policy generation, improving RAGent's reliability significantly. We further demonstrate RAGent's ability to successfully extract hidden ACRs from NLACPs and generate access control policies by incorporating the access decisions of each ACR through a real-world application. More importantly, RAGent introduces a novel iterative verification-refinement mechanism to correct its incorrect generations automatically. While it bridges a significant gap in existing access control policy generation frameworks, it also improves the correctness of the generated policies and, in turn, improves their quality [9]. Finally, if the automatic refinement fails, RAGent provides feedback to administrators, helping them to refine incorrect policies manually. Additionally, we also release the annotated datasets used to train and evaluate RAGent, addressing the data scarcity in the domain [52, 80].\nAs future works, we will first improve the feedback that RAGent provides to the administrator, providing information such as instructions to refine the incorrect policy [82]. To this end, we plan to design the feedback according to explainable security (XSec) concepts [76], involving system administrators through user studies [28,64].\nSecondly, we will develop a usable policy configuration interface according to Nielsen's usability quality components [55] to present the policy generation framework to administrators. Subsequently, we will use contextual design [28] to involve system administrators to improve its usability further iteratively [64]. Then, we will empirically evaluate the usability and reliability of our access control policy generation tool through lab study, allowing the participants to perform policy generation tasks using the tool [10, 11]. Finally, we will evaluate their subjective satisfaction with our tool using standard evaluation instruments such as the System Usability Scale (SUS) [12] or Post-Study System Usability Questionnaire (PSSUQ) [23]."}, {"title": "Ethics Considerations and Compliance with the Open Science Policy", "content": "In this research, we introduce a RAGent, novel retrieval-based access control policy generation framework based on transformer-based LMs. It helps system administrators generate access control policies from high-level requirement specifications with low overhead, reducing access control failures due to human errors.\nFirst, when choosing the LMS/LLMs for our research, we focused on \"small\" open-source LMs rather than utilizing powerful proprietary LLMs like GPT-4. That is because closed-source GPT-4-like LMs are exclusively controlled by a private entity [16", "79": ".", "16": ".", "small\", open-source LMs, RAGent avoids those negative outcomes while improving the reliability of access control policy generation as we show in Section 5. It allows the efficient deployment of RAGent within the organization even in low resource environments, providing the control of RAGent to the organization. Therefore, RAGent does not send an organization's confidential access control requirements to a separate entity, preserving its confidentiality.\nSecondly, we adapt the LLMs by fine-tuning them to identify NLACPs, generate access control policies, and verify them (Section 4). To this end, we only use real-world yet publicly available datasets introduced by Slankas et al. [72": "ensuring that they do not contain any sensitive information like email addresses, phone numbers, etc. We also generate synthetic data and use them only during the fine-tuning stage of RAGent, as mentioned in Section 4.1. After the fine-tuning process, we evaluate RAGent using the public real-world dataset mentioned above to assess its performance in identifying real-world NLACPs and generating access control policies from them. We further showcase RAGent's performance in real-world policy generation scenarios by generating access control policies from high-level requirement specifications of the HotCRP.com conference management website as reported in Appendix A.3.\nFinally, since the existing ML-based automated policy generation approaches are far from 100% accurate [19, 50, 52, 80, 82", "59": "."}]}