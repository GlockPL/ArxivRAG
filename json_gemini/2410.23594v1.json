{"title": "How DO FLOW MATCHING MODELS MEMORIZE AND GENERALIZE IN SAMPLE DATA SUBSPACES?", "authors": ["Weiguo Gao", "Ming Li"], "abstract": "Real-world data is often assumed to lie within a low-dimensional structure embedded in high- dimensional space. In practical settings, we observe only a finite set of samples, forming what we refer to as the sample data subspace. It serves an essential approximation supporting tasks such as dimensionality reduction and generation. A major challenge lies in whether generative models can reliably synthesize samples that stay within this subspace rather than drifting away from the underlying structure. In this work, we provide theoretical insights into this challenge by leveraging Flow Matching models, which transform a simple prior into a complex target distribution via a learned velocity field. By treating the real data distribution as discrete, we derive analytical expressions for the optimal velocity field under a Gaussian prior, showing that generated samples memorize real data points and represent the sample data subspace exactly. To generalize to suboptimal scenarios, we introduce the Orthogonal Subspace Decomposition Network (OSDNet), which systematically decomposes the velocity field into subspace and off-subspace components. Our analysis shows that the off-subspace component decays, while the subspace component generalizes within the sample data subspace, ensuring generated samples preserve both proximity and diversity.", "sections": [{"title": "Introduction", "content": "The representation of real-world data in high-dimensional spaces is a fundamental topic in machine learning and data analysis. According to the widely accepted manifold hypothesis [Fefferman et al., 2016], data such as images are assumed to reside on a low-dimensional structure embedded within a high-dimensional ambient space (e.g., pixel space). In practical scenarios, however, we only observe a finite sample set, forming what we term the sample data subspace, an approximation to the full structure. This sample data subspace plays a critical role in data-driven tasks, as it provides a concrete basis for algorithms in dimensionality reduction [McInnes et al., 2018] and generation [Yao et al., 2024], which rely on observed samples to approximate the underlying structure. Despite the foundational importance of this subspace, it remains uncertain whether generative models, which synthesize new data samples, can reliably generate samples that adhere to this data-constrained subspace rather than drifting away from the underlying structure.\nIn response to this challenge, we explore solutions within the framework of Flow Matching models [Lipman et al., 2023, Liu et al., 2023]. These models generate samples by progressively transforming a simple prior distribution into a complex target distribution, guided by an ordinary differential equation (ODE) governed by a learned velocity field. Departing from the common assumption of a continuous data distribution, we assume the real data distribution is discrete, composed only of the available samples. This reflects the finite nature of real data, where we approximate the low-dimensional structure with a limited set of points.\nBy focusing on the dynamics of the generative process in Flow Matching models, we show that, under a standard Gaussian prior, the optimal velocity field leads to analytical expressions. These expressions can be interpreted as a softmax-weighted sum of the vectors pointing from the current point to the real data points, offering a new perspective on the geometry of the generation paths under the velocity field. To the best of our knowledge, this is hitherto"}, {"title": "Related Works", "content": "In this section, we briefly discuss related works, focusing on flow-based generative models, memorization and generalization in generative models, and low-dimensional structure learning. For a more comprehensive literature review, including an overview of generative models and a comparative analysis with two closely related works, please refer to appendix A.\nFlow-based generative models. Flow-based generative models generate samples by applying a sequence of trans- formations that progressively map a simple source distribution, such as a standard Gaussian, to more complex data distributions. Among these, diffusion models [Song and Ermon, 2019, Ho et al., 2020, Song et al., 2021, Karras et al., 2022] employ stochastic differential equations (SDEs) to guide the transformation process. They rely on the score function, which is the gradient of the log-density of the data distribution, to capture the structure of the underlying data. Recently, Flow Matching models [Lipman et al., 2023, Liu et al., 2023] have emerged as a promising alternative. These models use ODEs instead of SDEs to represent the generative process. By learning a velocity field that maps the source distribution directly to the target distribution, they offer both computational efficiency and greater theoretical clarity. Flow Matching models have proven successful in large-scale tasks requiring rapid and accurate inference, such as DALL-E 3 [Betker et al., 2023]. Given these strengths, we focus on Flow Matching models in this paper.\nMemorization and generalization in generative models. Memorization occurs when generative models produce samples that closely replicate the training data, effectively recalling specific examples rather than capturing the broader underlying patterns. In contrast, generalization refers to a model's ability to generate new, previously unseen samples that extend beyond the training data while still conforming to the learned distribution. Recently, memorization in diffusion models has been studied both empirically [Somepalli et al., 2023, Yoon et al., 2023] and theoretically [Li et al., 2024b]. Generalization has also been theoretically examined in diffusion models, particularly from the perspectives of implicit bias [Kadkhodaie et al., 2024] and statistical bounds [Li et al., 2024a]. Despite these advances, the concepts of memorization and generalization remain relatively underexplored in the context of Flow Matching models. Furthermore, existing research tends to focus on either memorization or generalization separately [Kadkhodaie et al., 2024, Li et al., 2024a,b], without integrating them into a unified framework. Additionally, current theoretical work often assumes the existence of a continuous real distribution, an assumption that may not hold in practical scenarios.\nLow-dimensional structure learning in generative models. According to the manifold hypothesis, data such as images lies on a low-dimensional structure within a high-dimensional space. Recent studies identify three key ways in"}, {"title": "Preliminaries", "content": "In this section, we provide background on Flow Matching models, including their foundational principles, key formulations, and the role of optimal transport paths in guiding the generative process."}, {"title": "Flow Matching Models", "content": "Flow Matching (FM) [Lipman et al., 2023] is a framework that transforms a simple prior distribution $p_0(x)$, typically a standard Gaussian, into a complex target distribution $p_1(x)$ by learning a velocity field. FM employs various probability paths, each defined by a time-dependent velocity field $u_t(x)$. This field controls the evolution of the flow $p_t(x)$ via the following ODE\n$\\frac{d\\phi_t(x)}{dt} = u_t(\\phi_t(x)), \\phi_0(x) = x, t\\in [0, 1].$\nThe evolution of the probability density $p_t(x)$, starting from $p_0(x)$, is governed by the continuity equation (also known as the transport equation):\n$\\frac{\\partial p_t(x)}{\\partial t} + \\nabla \\cdot (p_t(x)u_t(x)) = 0, t\\in [0, 1].$\nTo approximate $u_t(x)$, FM uses a neural network $v_t(x; \\theta)$, where $\\theta$ represents the network parameters. The network is trained by minimizing the following FM loss\n$\\mathcal{L}_{FM}(\\theta) = \\mathbb{E}_{t \\sim U[0, 1], x \\sim p_t(x)}[||v_t(x; \\theta) - u_t(x)||^2].$\nWhen the exact form of $p_1(x)$ is unknown and only samples from $p_1(x)$ are available, Conditional Flow Matching (CFM) constructs conditional paths $p_t(x | x_1)$ for each data sample $x_1 \\sim p_1(x_1)$. The CFM loss is then defined as\n$\\mathcal{L}_{CFM}(\\theta) = \\mathbb{E}_{t \\sim U[0, 1], x_1 \\sim p_1(x_1), x \\sim p_t(x | x_1)}[||v_t(x; \\theta) - u_t(x | x_1)||^2].$\nAssume that the conditional distribution is Gaussian, given by $p_t(x | x_1) = \\mathcal{N}(x; \\mu_t(x_1), \\sigma_t(x_1)^2 I)$, and that the flow takes the form\n$\\phi_t(x) = \\sigma_t(x_1)x + \\mu_t(x_1).$\nThe conditional velocity field becomes\n$u_t(x | x_1) = \\frac{\\dot{\\sigma_t(x_1)}}{\\sigma_t(x_1)} (x - \\mu_t(x_1)) + \\dot{\\mu_t}(x_1).$\nFM models use two primary types of probability paths: diffusion paths, which are essentially equivalent to probability flow ODEs [Song et al., 2021], and optimal transport (OT) paths, which aim to transform distributions in the most efficient manner. In this paper, we focus on OT paths due to their wide application, theoretical elegance and appealing mathematical properties. We maintain that certain analytical results derived for OT paths can be extended to more general probability paths, providing a broader applicability of our findings. For more details regarding FM models, please refer to appendix B."}, {"title": "Optimal Transport Paths", "content": "OT paths are a specific instance of probability paths, where the flow is defined by linear interpolation between the initial and final positions. The mean function is given by $\\mu_t(x_1) = tx_1$, and the standard deviation function is $\\sigma_t(x) = 1 - t$. The corresponding velocity field is\n$u_t(x | x_1) = \\frac{x_1 - x}{1 - t},$\nresulting in the flow\n$\\phi_t(x) = (1 - t)x + tx_1$\nwhen conditioned on $x_1$. In this setup, the conditional flow follows straight-line paths with constant speed, which minimizes the complexity of the trajectory compared to diffusion paths. This leads to a simplified regression task for the CFM model, where the loss function becomes\n$\\mathcal{L}_{CFM, OT}(\\theta) = \\mathbb{E}_{t \\sim U[0, 1], x_1 \\sim p_1(x_1), x_0 \\sim p_0(x_0)}[||v_t(\\phi_t(x_0); \\theta) - (x_1 - x_0)||^2].$\nNote that this type of flow is also referred to as Rectified Flow in [Liu et al., 2023]."}, {"title": "The Optimal Velocity Field", "content": "In this section, we provide a detailed analysis of the derivation of the optimal velocity field and explore its role in shaping the geometry of generation paths. Building on these geometric insights, we prove that the generated samples memorize real data points and represent the data-defined subspace exactly."}, {"title": "Derivation of the Optimal Velocity Field", "content": "We begin by presenting the optimal velocity field that minimizes the CFM loss. The proof of proposition 4.1, along with all the other proofs in this paper, is provided in appendix D.\nProposition 4.1 (Optimal velocity field for conditional flow matching) Assume that the function class $\\{v_t(x;\\theta)\\}$ has enough capacity. Then the optimal velocity field $v_t^*(x)$ which minimizes the Conditional Flow Matching (CFM) loss\n$\\mathcal{L}_{CFM}(\\theta) = \\mathbb{E}_{t \\sim U[0, 1], x_1 \\sim p_1(x_1), x \\sim p_t(x | x_1)}[||v_t(x; \\theta) - u_t(x | x_1)||^2],$\nis given by\n$v_t^*(x) = \\mathbb{E}_{x_1 \\sim p_1(x_1)}[u_t(x | x_1) | x, t].$\nHaving established the general form of the optimal vector field, we now investigate a specific case where the prior $p_0$ is standard Gaussian and the real distribution $p_1$ is a discrete distribution.\nTheorem 4.2 (Optimal velocity field for discrete target distribution) Let $p_0 \\sim \\mathcal{N}(0, I_d)$, and suppose $p_1$ is a dis- crete distribution over a set of points $\\{y^i: 1 \\leq i \\leq N\\} \\subset \\mathbb{R}^d$, given by\n$p_1 \\sim \\frac{1}{N} \\sum_{i=1}^N \\delta_{y^i},$\nwhere $\\delta_u$ denotes the Dirac delta function centered at $y^i$. Assume that the conditional velocity field is\n$u_t(x | x_1) = \\frac{\\dot{\\sigma_t(x_1)}}{\\sigma_t(x_1)} (x - \\mu_t(x_1)) + \\dot{\\mu_t}(x_1).$\nThen the optimal velocity field $v_t^*(x)$ that minimizes the CFM loss is given by\n$v_t^*(x) = \\sum_{i=1}^N \\frac{\\sigma_t(y^i)}{\\sum_{j=1}^N \\sigma_t(y^j)} (\\frac{\\dot{\\sigma_t(y^i)}}{\\sigma_t(y^i)} (x - \\mu_t(y^i)) + \\dot{\\mu_t}(y^i)) \\frac{\\exp ( - ||(x - \\mu_t(y^i))/\\sigma_t(y^i)||^2/2)}{\\exp ( - ||(x - \\mu_t(y^j))/\\sigma_t(y^j)||^2/2)}.$\nSpecifically, in the case of OT paths where $\\mu_t(x_1) = tx_1$ and $\\sigma_t(x) = 1 - t$, the optimal velocity field simplifies to\n$v_t^*(x) = \\sum_{i=1}^N \\frac{y^i - x}{1 - t} \\frac{\\exp(- ||(x - ty^i)/(1 - t)||^2/2)}{\\sum_{j=1}^N \\exp ( - ||(x - ty^j)/(1 - t)||^2/2)}.$"}, {"title": "Path Geometry under the Optimal Velocity Field", "content": "In this subsection, we investigate the geometry of generation paths under the optimal velocity field. We aim to understand how various properties of the real data points shape these paths and how the velocity field guides the generated points towards the real data points. By examining the intermediate behavior of $x_t$, we seek to gain intuitive insights into the memorization phenomenon. We focus on two scenarios: (i) when the real data points are sparse and well-separated, and (ii) when they exhibit a hierarchical structure. These cases result in distinct generation path behaviors, as formalized in theorems 4.4 and 4.5.\nWe first consider a simple setup where the real data points are sparse and well-separated. In this case, we expect generated samples to move directly toward the nearest real data point as the generation progresses. The key intuition is that the softmax weights controlling the velocity field quickly concentrate on a single data point, resulting in nearly linear generation paths directed toward that point. Theorem 4.4 formalizes this observation and provides a probabilistic bound on how quickly the softmax weights concentrate on individual points. Please refer to fig. 1 for the conceptual illustration. We also provide the visualization of generation paths under such circumstances in appendix E.\nTheorem 4.4 (Probability bound for softmax weight concentration) Let $\\{y^i: 1 \\leq i \\leq N\\} \\subset \\mathbb{R}^d$ be real data points such that $||y^i - y^j||_2 \\geq M$ for all $1 \\leq i \\neq j \\leq N$. Consider the softmax weight function:\n$w_t(x) = \\text{softmax}\\left(\\frac{-\\frac{||x - ty^1||^2}{1-t}}{2}, \\frac{-\\frac{||x - ty^2||^2}{1-t}}{2}, \\dots, \\frac{-\\frac{||x - ty^N||^2}{1-t}}{2}\\right).$\nAssume that\n$x \\sim p_t = \\frac{1}{N} \\sum_{i=1}^N \\mathcal{N}(ty^i, (1-t)^2 I_d).$\nThen, the probability that $x$ falls into the region $R := \\{x: \\max(w_t(x)) < \\tau < 1\\}$, i.e., where the softmax weight vector is not close to a one-hot vector in the $\\tau$-sense, is bounded by\n$\\mathbb{P}(x \\in R) \\leq \\frac{1}{\\sqrt{2\\pi}} \\frac{1-t}{t} \\sqrt{\\frac{1- \\tau}{\\tau(N - 1)}} \\frac{1}{M} \\log \\frac{\\tau(N - 1)}{1 - \\tau} \\cdot N(N - 1).$\nNext, we explore more complex data distributions where the real data points are hierarchically structured. In such cases, generation paths first distinguish broad clusters of points before refining to finer ones, which is formalized in theorem 4.5. Please refer to fig. 2 for the conceptual illustration. We also provide the visualization of generation paths in appendix E.\nTheorem 4.5 (Hierarchy emergence of generation paths) Let $C_{i,j}$ ($i \\in I$, $j \\in T_i$) be a collection of disjoint, bounded, closed convex sets where the distribution $p_1$ is supported. Define\n$C_i = \\text{conv} (\\cup_{j \\in T_i} C_{i,j}), i \\in I,$\nwhere $\\text{conv}(\\cdot)$ denotes the convex hull of a set. Assume that the sets $C_i$ are disjoint, and that the distribution $p_0$ is supported on a bounded, closed convex set $S$.\nThen, there exists $t_1 \\in (0, 1)$ such that for any $t \\in (t_1, 1]$, the $|I|$ convex sets\n$(1 - t)S + tC_i, i \\in I$\nare disjoint. Similarly, there exists $t_2 \\in (t_1, 1)$ such that for any $t \\in (t_2, 1]$, the $\\sum_{i \\in I} |T_i|$ convex sets\n$(1 - t)S + tC_{i,j}, i \\in I, j \\in T_i$\nare disjoint.\nLet $\\phi_t$ denote the optimal flow under the OT paths. Then, for any $x$ drawn from $p_0$, if $\\phi_{t_1}(x) \\in (1 - t_1)S + t_1C_i$, it follows that $\\phi_t(x) \\in (1 - t)S + tC_i$ for any $t \\in (t_1, 1]$. Specifically, $\\phi_1(x) \\in C_i$. Furthermore, if $\\phi_{t_2}(x) \\in (1 - t_2)S + t_2C_{i,j}$, then $\\phi_t(x) \\in (1 - t)S + tC_{i,j}$ for any $t \\in (t_2, 1]$. Specifically, $\\phi_1(x) \\in C_{i,j}$."}, {"title": "Memorization Phenomenon under the Optimal Velocity Field", "content": "We are now prepared to formally state the memorization phenomenon under the optimal velocity field. One might question why we need to restate and prove this result, as corollary 4.3 already demonstrates that $p_1(x)$ corresponds to the discrete distribution of real data points. We point out that while corollary 4.3 relies on the marginal distribution and tools such as the continuity equation to establish the result, it does not provide insight into the behavior of the individual generation paths. In contrast, theorem 4.6 directly focuses on the flow ODE, offering a more direct and transparent analysis. Moreover, the technique of isolating the linear term and applying the method of variation of constants will prove especially useful in the next section.\nTheorem 4.6 (Memorization phenomenon under the optimal velocity field) Consider the flow ODE\n$\\frac{d\\phi_t(x)}{dt} = v_t^*(\\phi_t(x)), \\phi_0(x) = x, t \\in [0, 1],$\nwhere\n$v_t^*(x) = \\sum_{i=1}^N \\frac{y^i - x}{1 - t} \\frac{\\exp(- ||(x - ty^i)/(1 - t)||^2/2)}{\\sum_{j=1}^N \\exp ( - ||(x - ty^j)/(1 - t)||^2/2)}.$\nThen the set $\\{\\phi_1(x): x \\in \\mathbb{R}^d\\}$ equals $\\{y^1, y^2, ..., y^N\\}$ up to a finite set.\nIn theorem 4.6, the expression \"up to a finite set\" is necessary and cannot be omitted. For example, in a simple scenario with only two real data points, if a noise sample starts at the midpoint between these two points, it will remain stationary"}, {"title": "The Suboptimal Velocity Field", "content": "In the previous section, we explore the theoretical properties of the optimal velocity field. However, in practical applications, neural networks are commonly used to approximate the velocity field, resulting in a suboptimal one. To better capture and analyze the resulting effects, we develop a network class called the Orthogonal Subspace Decomposition Network (OSDNet) to model the suboptimal velocity field. With OSDNet, we show that under the suboptimal velocity field, off-subspace components decay, while the components within the subspace demonstrate generalization behavior, ensuring generated samples remain close to the data-defined subspace while maintaining diversity."}, {"title": "Orthogonal Subspace Decomposition Network", "content": "We develop a network class called Orthogonal Subspace Decomposition Network (OSDNet) to model the suboptimal velocity field. First, we present the formal definition, followed by a detailed explanation of the rationale behind it. Please refer to fig. 3 for an illustration.\nDefinition 5.1 (Orthogonal Subspace Decomposition Network (OSDNet)) Let $\\{y^i: 1 \\leq i \\leq N\\} \\subset \\mathbb{R}^d$ be real data points, which form the columns of the matrix $Y = [y^1, y^2, ..., y^N]$. Let $V \\in \\mathbb{R}^{d \\times D}$ be a matrix with orthonormal columns spanning the same subspace as these data points, obtained by performing a reduced singular value decomposition on $Y$, i.e., $Y = VR$, where $R \\in \\mathbb{R}^{D \\times N}$ is the product of a diagonal matrix (containing the non-zero singular values of $Y$) and a matrix with orthonormal rows (containing the right singular vectors). The suboptimal velocity field, as defined by OSDNet, is given by\n$\\hat{v}_t(x; \\hat{O}_t, \\hat{s}_t(x)) = V^\\perp \\cdot \\hat{O}_t \\cdot (V^\\perp)^T \\cdot x + V \\cdot \\hat{s}_t(x),$\nwhere $V^\\perp$ denotes the orthogonal complement of $V$, $\\hat{O}_t$ is a time-dependent diagonal matrix, and $\\hat{s}_t(x)$ is a time- dependent vector function representing the component of the velocity field within the subspace spanned by $V$."}, {"title": "Teacher-Student Training of OSDNet", "content": "Proposition 5.2 (Optimal velocity field as a specific instance of OSDNet) The optimal velocity field $v_t^*(x)$ derived in theorem 4.2 is a specific instance of the OSDNet, where\n$\\hat{O}_t = \\frac{1}{1 - t} I_{d-D}$\nand\n$\\hat{s}_t(x) = \\frac{1}{1 - t} (R \\cdot w_t(x) - V^T \\cdot x).$\nWe now elaborate on the assumption that $\\hat{O}_t$ is diagonal. Using the method of variation of constants, we can expression the solution to the ODE\n$\\frac{d\\phi_t(x)}{dt} = v_t(\\phi_t(x); \\hat{O}_t, \\hat{s}_t(x)) = V^\\perp \\cdot \\hat{O}_t \\cdot (V^\\perp)^T \\cdot \\phi_t(x) + V \\cdot \\hat{s}_t(\\phi_t(x)), \\phi_0(x) = x, t \\in [0, 1],$\nas\n$\\phi_t(x) = T \\exp(\\int_0^t V^\\perp \\cdot \\hat{O}_s \\cdot (V^\\perp)^T ds) \\cdot (x + T \\exp(\\int_0^t V^\\perp \\cdot \\hat{O}_s \\cdot (V^\\perp)^T ds)^{-1} \\cdot V \\cdot \\hat{s}_t(\\phi_t(x)) ds),$\nwhere $T \\exp$ denotes the time-ordered matrix exponential. To ensure that the matrices $\\{V^\\perp \\cdot \\hat{O}_t \\cdot (V^\\perp)^T: 0 \\leq t \\leq 1\\}$ commute with each other, and thus making the time-ordered matrix exponentials tractable, we assume that $\\{\\hat{O}_t: 0 < t < 1\\}$ are diagonal. Under these conditions, we can decompose the solution $\\phi_1(x)$ (i.e., the generated sample) into two components: one lying within the span of $V$, and the other within the span of $V^\\perp$.\nWe begin by computing\n$\\exp(\\int_0^t V^\\perp \\cdot \\hat{O}_s \\cdot (V^\\perp)^T ds).$\nNote that\n$V^\\perp \\cdot \\hat{O}_s \\cdot (V^\\perp)^T = [V \\quad V^\\perp] [\\begin{smallmatrix} 0 & 0 \\\\ 0 & \\hat{O}_s \\end{smallmatrix}] [\\begin{smallmatrix} V^T \\\\ (V^\\perp)^T \\end{smallmatrix}].$\nBy the definition of the matrix exponential, we obtain\n$\\exp(\\int_0^t V^\\perp \\cdot \\hat{O}_s \\cdot (V^\\perp)^T ds) = VV^T + V^\\perp \\cdot \\exp(\\int_0^t \\hat{O}_s ds) \\cdot (V^\\perp)^T.$\nNext, we compute\n$\\exp(\\int_0^t V^\\perp \\cdot \\hat{O}_s \\cdot (V^\\perp)^T ds)^{-1}$\nin a similar way and get\n$\\exp(\\int_0^t V^\\perp \\cdot \\hat{O}_s \\cdot (V^\\perp)^T ds)^{-1} = VV^T + V^\\perp \\cdot \\exp(\\int_0^t -\\hat{O}_s ds) \\cdot (V^\\perp)^T.$\nSubstituting these expressions into $\\phi_1(x)$, and noting that $V$ and $V^\\perp$ have orthonormal columns, we arrive at the decomposition\n$\\phi_1(x) = V \\cdot (V^T x + \\int_0^1 \\hat{s}_t(\\phi_t(x)) ds) + V^\\perp \\cdot \\exp(\\int_0^1 -\\hat{O}_s ds) \\cdot (V^\\perp)^T \\cdot x.$\nThe OSDNet consists of two types of trainable parameters: $\\hat{O}_t$ and $\\hat{s}_t(x)$. When training OSDNet using the CFM loss, we show that these parameters can be decoupled and trained independently. Specifically, the OSDNet can be trained in a teacher-student framework, where $v_t^*(x)$ serves as the teacher network, and $\\hat{v}_t(x)$ acts as the student network, which we present in proposition 5.3."}, {"title": "Decay of Off-Subspace Components", "content": "Proposition 5.3 (Teacher-student training of OSDNet) Minimizing the CFM loss\n$\\mathcal{L}_{CFM}(\\hat{O}_t, \\hat{s}_t(x)) = \\mathbb{E}_{t \\sim U[0, 1], x_1 \\sim p_1(x_1), x \\sim p_t(x | x_1)}[||\\hat{v}_t(x; \\hat{O}_t, \\hat{s}_t(x)) - u_t(x | x_1)||^2],$\nis equivalent to minimizing the Teacher-Student Training (TST) loss\n$\\mathcal{L}_{TST}(\\hat{O}_t, \\hat{s}_t(x)) = \\mathbb{E}_{t \\sim U[0, 1], x \\sim p_t(x)}[||\\hat{v}_t(x; \\hat{O}_t, \\hat{s}_t(x)) - v_t^*(x)||^2],$\nin the sense that $\\mathcal{L}_{CFM}(\\hat{O}_t, \\hat{s}_t(x))$ and $\\mathcal{L}_{TST}(\\hat{O}_t, \\hat{s}_t(x))$ differ only by a constant. As a result, training OSDNet reduces to the two independent minimization problems below:\n$\\min_{\\hat{O}_t} \\mathbb{E}_{t \\sim U[0, 1], x \\sim p_t(x)}[||V^\\perp \\cdot (\\hat{O}_t + \\frac{1}{1 - t} I_{d-D}) \\cdot (V^\\perp)^T \\cdot x||^2],$\nand\n$\\min_{\\hat{s}_t(x)} \\mathbb{E}_{t \\sim U[0, 1], x \\sim p_t(x)}[||V \\cdot (\\hat{s}_t(x) - \\frac{1}{1 - t} (R \\cdot w_t(x) - V^T \\cdot x))||^2].$\nIn this subsection, we examine the time-dependent matrix $\\hat{O}_t$, which governs the off-subspace components of generated samples through\n$V^\\perp \\exp(\\int_0^1 \\hat{O}_s ds) (V^\\perp)^T \\cdot x.$\nWe start by simplifying the expectation term, transforming it into a function approximation problem in proposition 5.4\nProposition 5.4 (Teacher-student training of $\\hat{O}_t$) The solution to the minimization problem\n$\\min_{\\hat{O}_t} \\mathbb{E}_{t \\sim U[0, 1], x \\sim p_t(x)}[||V^\\perp (\\hat{O}_t + \\frac{1}{1 - t} I_{d-D}) (V^\\perp)^T x||^2]$\nreduces to solving the function approximation problem\n$\\min_{\\hat{O}_t} ||\\int_0^1 ((\\frac{1}{1 - t})^2 - \\hat{O}_t + I_{d-D})^2 dt||.$\nSince our problem only depends on the temporal variable $t$, we use the standard sinusoidal positional encoding commonly adopted in Transformers [Vaswani et al., 2017] and U-Nets to embed temporal information into the velocity network. Specifically, we define the time embedding as:\n$\\text{emb}(t) = (\\sin(\\frac{s t}{l^2})), \\cos(\\frac{s t}{l^2}), \\sin(\\frac{s t}{l}), \\cos(\\frac{s t}{l}), \\dots, \\sin(\\frac{s t}{l^{\\frac{\\text{dim}-2}{2}}}), \\cos(\\frac{s t}{l^{\\frac{\\text{dim}}{2}}}))$,\nwhere $s$ denotes the scaling factor, $l$ denotes the wavelength (typically set to $l = 10000$), and \"dim\" is the dimension of the embedding.\nSince $\\hat{O}_t$ is a diagonal matrix, the Frobenius norm reduces to the sum of the squares of its diagonal entries. We model each diagonal entry $\\hat{o}_t$ of $\\hat{O}_t$ as a linear transformation of the time embedding:\n$\\hat{o}_t = \\kappa^T \\text{emb}(t),$\nwhere $\\kappa \\in \\mathbb{R}^{\\text{dim}}$ is a parameter vector to be optimized. Our objective is to minimize the following loss function:\n$\\int_0^1 (\\frac{1}{1 - t} - \\hat{o}_t)^2 dt = \\int_0^1 (\\frac{1}{1 - t} - \\kappa^T \\text{emb}(t))^2 dt := \\kappa^T A \\kappa + 2 \\kappa^T b + 1,$\nwhere $A = \\int_0^1 (1 - t)^2 \\text{emb}(t) \\text{emb}(t)^T dt$ and $b = \\int_0^1 (1 - t) \\text{emb}(t) dt$. Applying gradient descent to solve this quadratic problem, the gradient flow equation becomes\n$\\frac{d\\kappa}{d\\tau} = -(2A \\kappa + 2b), \\tau \\in [0, +\\infty).$"}, {"title": "Generalization of Subspace Components", "content": "In this subsection, we analyze the behavior of the generated samples within $\\text{range}(V)$, i.e.,\n$V \\cdot (V^T x + \\int_0^1 \\hat{s}_t(\\phi_t(x)) ds).$\nWe begin by justifying why it is reasonable to assume that $x \\in \\text{range}(V)$, thereby avoiding the need to account for the off-subspace components of $x$. This assumption is supported by demonstrating that $\\hat{s}_t(x) = \\hat{s}_t(VV^T x)$, implying that the subspace components of the generated samples are solely determined by the projection of $x$ onto $\\text{range}(V)$. With this assumption in place, the subspace component of the generated samples simplifies to the following form\n$x + \\int_0^1 V \\hat{s}_t(\\phi_t(x)) ds.$\nThis expression is, in fact, the integral solution to the ODE\n$\\frac{d\\phi_t(x)}{dt} = V \\cdot \\hat"}]}