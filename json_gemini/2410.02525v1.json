{"title": "CONTEXTUAL DOCUMENT EMBEDDINGS", "authors": ["John X. Morris", "Alexander M. Rush"], "abstract": "Dense document embeddings are central to neural retrieval. The dominant paradigm\nis to train and construct embeddings by running encoders directly on individual\ndocuments. In this work, we argue that these embeddings, while effective, are im-\nplicitly out-of-context for targeted use cases of retrieval, and that a contextualized\ndocument embedding should take into account both the document and neighboring\ndocuments in context - analogous to contextualized word embeddings. We propose\ntwo complementary methods for contextualized document embeddings: first, an\nalternative contrastive learning objective that explicitly incorporates the document\nneighbors into the intra-batch contextual loss; second, a new contextual architecture\nthat explicitly encodes neighbor document information into the encoded represen-\ntation. Results show that both methods achieve better performance than biencoders\nin several settings, with differences especially pronounced out-of-domain. We\nachieve state-of-the-art results on the MTEB benchmark with no hard negative\nmining, score distillation, dataset-specific instructions, intra-GPU example-sharing,\nor extremely large batch sizes. Our method can be applied to improve performance\non any contrastive learning dataset and any biencoder.", "sections": [{"title": "INTRODUCTION", "content": "Machine learning approaches to text retrieval aim to learn an embedded representation for indexing\ndocuments. Classically, this area was dominated by statistical approaches using sparse lexical\nmatching methods based on n-gram frequencies such as BM25 (Robertson & Zaragoza, 2009).\nOnly recently have neural networks become competitive with state-of-the-art models on retrieval\ntasks (Karpukhin et al., 2020; Thakur et al., 2021). The primary neural method is a dual encoder\narchitecture that independently encodes both a document and query to a dense latent space for\nretrieval lookup. This document embedding space can improve upon a statistical model since it is\nlearned end-to-end for retrieval.\nHowever, there is at least one notable benefit of statistical approaches that is lost by neural models.\nStatistical models can easily incorporate prior corpus statistics such as inverse document frequency\n(IDF), into their representation. This prior term imparts context-dependence onto the model, since it\ncan be updated based on information specific to retrieval in a given domain at test time. We contrast\nthis contextual formulation with neural document encoders that are by definition a function of the\ndocument itself. For example consider the following document:\nThe National Football League Draft is an annual event in which the National\nFootball League (NFL) teams select eligible college football players...\nDepending on the retrieval domain, e.g. Wikipedia search, sports articles, or televised events, IDF\nmay weight terms such as NFL, draft or annual higher; a neural document embedding model\nwould need to select a global weighting for this document.\nIn this work, we explore contextualization of document embeddings produced by dense encoders.\nThe goal is to produce embeddings that are better able to handle retrieval tasks in specific challenging\ncontexts. We propose two complementary changes to document encoders: a contextual training\nprocedure and architecture.\nFor contextual training, we aim to build a notion of neighboring documents directly into the contrastive\nlearning process. We propose a method that uses on fast query-document clustering to produce a"}, {"title": "RELATED WORK", "content": "Text retrieval. Our work is related to the general field of text retrieval; we propose specific\nimprovements to the training of \"biencoder\" text embedding models such as DPR (Karpukhin et al.,\n2020), GTR (Ni et al., 2021), Contriever (Izacard et al., 2022), LaPraDoR (Xu et al., 2022), Instructor\n(Su et al., 2023), Nomic-Embed (Nussbaum et al., 2024), E5 (Wang et al., 2024), and GTE (Li et al.,\n2023). We focus on the problem of adapting these text retrieval models to new corpora at test time;\nsome prior work has noted this problem (Dai et al., 2022; Sciavolino, 2021) and proposed solutions\nsuch as unsupervised span-sampling and training on test corpora (Gao & Callan, 2021) and distillation\non the test corpus from a reranker (Sung et al., 2023). Late interaction methods (Khattab & Zaharia,\n2020; Santhanam et al., 2022) also offer one way to improve out-of-domain retrieval performance,\nbut increase the runtime and complexity of search. We propose a better sampling scheme that can be\nused to train any biencoder or late interaction model as well as a training-free method for test-time\nadaptation."}, {"title": "BACKGROUND", "content": "We can view text retrieval methods probabilistically as computing a distribution over potential\ndocuments based on a scalar score function f (d, q) matching documents and queries:\n$p(d|q) = \\frac{\\exp f (d, q)}{\\sum_{d'\\in D}\\exp f (d', q)}$\n(1)\nwhere D is a finite set of documents in a dataset. There is a wide variety of different definitions for f\nincluding full pairwise neural parameterizations (Nogueira & Cho, 2020). In this work, we focus on\nefficient retrieval methods using vector-based methods, also known as embedding models.\nVector retrieval methods assume that f (d, q) can be factored into two embedding terms, $(d) \u00b7 \u03c8(q)$,\nthe document and query embedding respectively. This factorization allows precomputation of the\ndocument embeddings $(d) for all d \u2208 D. This is critical for facilitating fast computation of\narg maxa p(d|q) or top-k variants (Douze et al., 2024).\nIn statistical retrieval, & and 4 are closed-form functions of the data, often representing unigram or\nbigram counts by the relative frequency of word types. Notably for this work, these methods can also\nutilize distributional properties of the test dataset as a prior, for example through inverse document\nfrequency (IDF). We represent this integration of dataset-level information by writing the vector\nproduct $(d; D) \u00b7 \u03c8(q; D)$.\nIn neural retrieval, we instead learn the representation as a dense vector. We assume access to a\ntraining corpus of document and query pairs (these may be supervised, i.e. gold-standard annotations,\nor unsupervised, i.e. noised synthetic examples), DT = {(d\u00b9, q\u00b9), ..., (d), q)}, with the aim of\nlearning the embedding function & and 4.\nTraining can be motivated as maximizing likelihood of the document corresponding to each query, i.e.\n\u2211; log p(di | qi). Unfortunately, since retrieval datasets can have |D| exceed millions of documents,\ncomputing the normalizer in Eq 1 at each training step is not an option. Instead contrastive learning is\nused where the likelihood is replaced with a biased approximation calculated from negative samples:"}, {"title": "METHODS", "content": "In our work, we are interested in integrating contextual information into our embedding functions\n4 and 4. The standard neural & is purely a function of the document $(d) and does not take into\naccount any notion of context. This contrasts with the statistical model $(\u00b7; D) and (\u00b7; D). Arguably\nthis is not an issue if retrieval is completely in domain, as $ is capable of learning statistics such as\nIDF and average document length on the training set through gradient descent.\nHowever, in many retrieval benchmarks, models are trained over a single set of documents D and\nthen tested in many other domains D that differs significantly from Dr. In this setting, training on\nDr alone may not be able to provide robust embeddings when used in contexts such as D."}, {"title": "CONTEXTUAL TRAINING WITH ADVERSARIAL CONTRASTIVE LEARNING", "content": "Returning to the example from the introduction, we assume that in a general purpose training corpus\nDT, the term NFL is a rare word appearing in relatively few documents and a useful signal. However,\nif at test time D is a corpus of sports articles, this word would be exceedingly common. Evaluation\nin this domain is, in a statistical sense, adversarial to the original dataset. To handle this issue,\nmeta-learning-style objectives have shown to be effective for training document embedders. In these\napproaches, instead of sampling documents-query pairs iid, the objective first sample a domain and\nthen sample a batch of examples. This ensures that the model mostly sees related training points in\neach domain.\nWe propose a training objective that synthesizes a large set of fine-grained domains to train the model\non. Formally, our aim is to partition the training dataset D\u012b into groups (B1, . . . BB) such that each\ngroup represents a self-similar pseudo-domain:\n$\\max_{\\Phi,\\psi} \\sum_{b} \\sum_{(d,q)\\in B_b} log p(d | q) = \\max_{\\Phi,\\psi} \\sum_{b} \\sum_{(d,q)\\in B_b} log \\frac{\\exp f(d, q)}{\\sum_{(d')\\in B_b} \\exp f (d', q)}$\nComputationally, the inner term can be implemented as a single batch and computed efficiently\nwithout the need for separate hard negatives (H). Ideally we want groups that are as challenging as\npossible. Zhang & Stratos (2021) show that increasing the partition term improves the contrastive\napproximation to the maximum likelihood the gradient. We can formalize the search for the most\ndifficult configuration of batches as an optimization problem:\n$ (\\max) \\sum_{b} \\sum_{\\frac{ (d,q)\\in B_b }}{(d',q')\\in B_b} f(d, q') + f(d',q) \\rightarrow \\max_{(B_1,...B_B)} \\sum_{b} \\sum_{\\frac{(d,q)\\in B_b }}{(d',q')\\in B_b} \\Phi(d)\u00b7\\psi(q') + \\Phi(d')\u00b7\\psi(q) (2)$\nSolving this combinatorial objective exactly is intractable, but we can treat approximate a solution\nusing clustering. We first move from a maximization to a minimization by replacing the two dot\nproducts with L2, i.e. m((d, q), (d', q')) = ||\u00a2(d) \u2013 \u03c8(q')|| + ||$('d) \u2013 \u03c8(q)|| which is equivalent\nfor normalized embeddings. We then note that treated as symmetric pairs, this term obeys the triangle\ninequality for any other pair m i.e:\nm((d,q), m) + m(m, (d', q')) \u2265 m((d, q), (d', q'))"}, {"title": "CONTEXTUAL DOCUMENT EMBEDDING (CDE)", "content": "Contextualization can also be added directly to the archiecture. Taking inspiration from sparse vector\nretrieval which uses corpus statistics to determine the form of the embedding, we modify the encoders\nto have access to the corpus itself, i.e. $(d; D) and $(d; D). This effectively augments the biencoder\nmodel to give it the ability to contextualize documents directly.\nThe main challenge is how to design a neural architecture that can take into account dataset contex-\ntualization. On one extreme, we could follow methods like BM25 and precompute a fixed set of\ncorpus statistics that could be fed to the document encoder. On the other extreme, we could allow the\nencoder full access to the entire corpus, through some form of cross attention. The latter approach\nhas been explored on a small scale in methods like neural processes (Garnelo et al., 2018); however,\nit would be difficult to scale to larger datasets.\nWe opt for a middleground that allows the model to learn corpus statistics, but is also relatively\nefficient to compute, shown in Figure 1. Specifically, we note that document embeddings retain\na surprising amount of lexical information even after embedding (Morris et al., 2023). Therefore,\nif we pre-embed a subset of the corpus, we believe we can still dynamically calculate key dataset\ninformation during encoding."}, {"title": "METHODS", "content": "We produce contextualized embeddings via a two-stage process:\nFirst stage: Gather and embed context. Given context documents d\u00b9, ..., dJ \u2208 D, we embed each\nusing a unique embedding model and concatenate embeddings into a sequence M\u2081(d\u00b9)...M\u2081(dJ).\nSecond stage: Embed document with additional context tokens. To compute & for document d' we\nintegrate contextual embedding sequence at the input of second-stage embedding model M2:\n$\\Phi(d'; D) = M_2(M_1(d^1), ..., M_1(d), E(d_1),...,E(d'))$\n(5)\nHere M\u2081 is the first-stage encoder model, M2 is a second-stage encoder model, and E is the token\nembedding matrix of M2 applied to each token in d'. In practice, we parameterize both M\u2081 and\nM2 using traditional bidirectional transformers, so our model is comprised of two biencoder-like\nbackbones called in sequence.\nThere is a similar contextualized model for the query encoder & which is also given document context\n(as we do not have query context at test time):\n$\\Phi(q; D) = M_2(M_1(d^1), ..., M_1(d), E(q_1), ..., E(q_t))$\n(6)\nWe note several implementation properties of this architecture. During training, computing contextual\nembeddings for each contextual document for each training instance would naively increase training\nby a computational factor proportional to J, the number of documents in context. This time increase\nwould not be tractable, since contrastive training can already take many days. We overcome this\ndifficulty by sharing context d\u00b9, ..., dJ within a batch of documents; this allows us to compute\nrepresentations just once per training step and reuse them between documents via computational\ngraph. 1\nWhen indexing a new corpus D, first stage representations M\u2081(d\u00b9)...M\u2081(d) can be computed once\nand cached, so M\u2081 does not add parameters or runtime to the search process. Query representations\ncan also use the cached context, which only require additional inputs to the encoder. (Our model does\nnot include contextualized queries, only documents, as we typically do not assume access to example\nqueries at test-time.)\nEmbedding without context. Individual corpora during training may not have sufficient or available\ncontext. To improve our model's generalization, we use sequence dropout, where we randomly replace\ncontext embeddings M\u2081(d*) with some null token up according to some a uniform probability p.\nAt test time, if no corpus information is available, our model can now function as a non-contextual\nbiencoder simply by replacing all sequence token inputs with \u03c5\u03c6.\nPosition-agnostic embedding. Since documents of D are unordered, we remove all positionality\nfrom the neural encodings. When parameterizing @ with a traditional transformer, this can be\nachieved by omitting positional embeddings at the positions corresponding to D. In practice, we use\ntransformers implementations dependent on FlashAttention with rotary positional embeddings at\neach self-attention layer. Full details of how we disable positionality are available in Section 10.4.\nTwo-stage gradient caching. To improve training we employ a gradient-caching technique analo-\ngous to a two-stage version of GradCache (Gao et al., 2021). This technique allows us to fit larger\nbatches, longer sequences with more contextual samples without running out of memory. Essentially,\nwe compute first-stage and second-stage representations independently without gradients. We then\nuse these frozen representations to compute the loss, and gradients with respect to the second-stage\nrepresentations. We then re-run the second stage with gradients enabled and use the output gradients\nto backpropagate through the second-stage model, and obtain gradients for the first-stage representa-\ntions. We repeat this process for the first-stage representations. This allows us to tradeoff computation\n(running each transformer forward pass twice) for memory."}, {"title": "EXPERIMENTAL SETUP", "content": "We consider a range of retrieval experiments across different scales. To run experiments across a suit-\nable number of settings, we devise a small setting: six-layer transformer, maximum sequence length\nof 64, and maximum number of 64 additional contextual tokens. In this scenario, we evaluate on a\ntruncated version of the BEIR benchmark (Thakur et al., 2021). Given the low cost of each experiment,\nwe are able to pre-train and fine-tune both biencoder and contextual models across a variety of batch\nsizes in {256, 512, 1024, 2048, 4096} and cluster sizes {64, 256, 1024, 4096, ..., 2097152, 4194304}.\nAs typical state-of-the-art text embedding models are trained in two phases, a large weakly-supervised\npre-training phase and a short supervised phase, we run all experiments for both phases.\nFor the large setting, we use the best settings found via small experiments. We train a single model\non sequences of length 512 with 512 contextual documents, evaluating on the full MTEB benchmark\n(Muennighoff et al., 2022). This includes tasks from retrieval as well as tasks like classification,\nclustering, and reranking.\nTraining Data and Metrics We train on the meta-datasets collected in Nussbaum et al. (2024) for\ntraining text embedding models. This collection of datasets includes data from 24 datasets scraped\nfrom web sources such as Wikipedia and Reddit. Our unsupervised training phase trains on 200M\nweakly-supervised datapoints scraped from large internet sources such as Reddit and Wikipedia.\nThe supervised training phase includes 1.8M human-written query-document pairs intended for text\nretrieval, and is aggregated from popular retrieval datasets such as HotpotQA and MS MARCO (Yang\net al., 2018; Bajaj et al., 2018). For our full model, we also consider supervised training on the BGE\nmeta-datasets (Xiao et al., 2024). We evaluate our models using NDCG@10, a conventional retrieval\nmetric that enables comparison across many disparate datasets.\nImplementation When partitioning our dataset into batches, we encode documents and queries\nusing GTR (Ni et al., 2021) and implement our clustering algorithm on top of FAISS (Douze et al.,\n2024). We cluster per-domain for 100 steps and take the best clustering out of 3 attempts. We\nselect NomicBERT as our pre-trained model backbone (Nussbaum et al., 2024), which has 137M\nparameters. We prepend all texts with short task-specific prefixes to identify each task; prefixes are\nlisted in Section 10.7.\nTraining We initialize both M\u2081 and M2 using the BERT-base model from Nussbaum et al. (2024)\nthat includes flash attention. Weights are shared between & and 4, but notably not between M1\nand M2. For all experiments, we train with the Adam optimizer with 1000 steps of warmup to a\nlearning rate of 2. 10-5 and linearly decay to 0 throughout training. We train for three epochs unless\notherwise specified. We set the maximum sequence length for all inputs to 512 and the number of\ncontextual inputs to 512 (so the second-stage model has an input length of 1024). When computing\ncontrastive loss, we use a fixed temperature of t = 0.02. When sequence dropout is enabled in our\ncontextual architecture, we set contextual input tokens to null vectors with a uniform probability\np = 0.005. If the batch size exceeds the number of contextual documents, we randomly sample to\nproduce contextual inputs."}, {"title": "RESULTS", "content": "The main results are highlighted in Table 1 and Section 6. In the smaller setting, we observe that both\nadversarial contrastive learning and our contextual architecture improve performance compared to\nvanilla biencoder training. We observe the largest improvement when we combine these techniques.\nContextual batching After controlling for batch size and filtering for false negatives, we observe a\nstrong correlation (visualized in Figure 2) between batch difficulty and downstream performance:\nreordering datapoints to make batches harder definitively enhances overall learning. This corrob-\norates prior findings (Xiong et al., 2020; Qu et al., 2021) and theory (Zhang & Stratos, 2021) that\nmore difficult batches in contrastive learning form a better overall gradient approximation and learn\nmore effectively.\nSection 6 showcases model performance across batch and cluster sizes after both phases of training.\nWe observe that although a large batch and cluster size are useful when filtering is not enacted, when\nincluding filtering, smaller cluster (and harder) are clearly better, and large batches do not add much.\nWhen comparing filtered to non-filtered models (Figure 4), filtering false negatives clearly improves\nperformance.\nContextual architecture In addition to adversarial batching, we compare our contextual architec-\nture to a biencoder across the datasets of BEIR in Table 1 (full results in appendix). Our architecture\ngenerally matches or improves performance on all downstream datasets, with largest improvements\nin ArguAna and SciFact, two of the smaller and more out-of-domain datasets.\nFull-scale training Figure 5 shows our models' performance when trained for multiple epochs on\nthe supervised datasets, relative to the best similar-sized embedding model (dashed line). We find\nbest performance when training for four epochs on the BGE meta-datasets. Although our best model\ndoes use a single hard negative per query, we are still able to to achieve state-of-the-art performance\nwithout using any hard negatives.\nFor our final model (cde-small-v1), we select the best of the supervised models, which comes\nfrom finetuning on the BGE dataset. On MTEB, cde-small-v1 obtains state-of-the-art results\ncompared to models of the same size. Although inspired by problems in the specific domain of text\nretrieval, we observe that our approach improves embedding performance in all domains, including\nclustering, classification, and semantic similarity. We also evaluate a \"random documents\u201d baseline,\nwhere we sample random documents from the training dataset to simulate a scenario where we lack\naccess to the test corpus. In this setting, we drop around 1.2 points on average across all tasks; the\nSTS tasks in particular appear to produce representations that are close to context-agnostic."}, {"title": "ANALYSIS", "content": "How hard are our clusters? To analysis the relationship between cluster size in our clustering\nalgorithm and the overall average difficulty of in-batch negatives, we measure the average difficulty"}, {"title": "SUPPLEMENTARY MATERIAL", "content": "We pre-train all models on 8 NVIDIA H100 GPUs. In the slowest setting, training a biencoder for\na single unsupervised epoch (235M pairs) takes approximately one day. Training our contextual\narchiecture for a single epoch takes approximately two days. Shorter sequence-length experiments\nare 10-20x faster, and can be run on a single GPU.\nWe conducted two preliminary experiments to verify (i) the need for contextual training strategy and\n(ii) the need for in-batch false negative filtering when doing adversarial contrastive learning on a real\ndataset.\nPreliminary experiment (i). We conduct a preliminary experiment to verify this issue. Starting\nfrom several trained retrieval systems we compute performance on a variety of different tasks from\nthe BEIR dataset. Additionally we compute the IDF statistics from the datasets, and compare the\ndivergence from the base IDF statistics of the training set. Figure 8 shows that datasets with high-\ndivergence have very high correlation with the accuracy degradation of models when measured in\ncomparison to BM25, which is able to measure and adapt to statistics of the test corpus.\nPreliminary experiment (ii). We select a random document from an unsupervised corpus and\nlook at its nearest neighbors, displayed in Table 3. We observe that the nearest neighbors to a given\ndocument in a large corpus are very close; in fact, many of them could be considered valid documents\nfor the given query as well.\nThis challenge motivates our embedding contextualization. In this section, we describe two com-\nplementary methods for remediation, (a) a contextual training method, (b) a contextual encoding\nmethod.\nThe authors note that it can be notoriously difficult to train models using both contrastive loss and\nthe distributed data parallel (DDP) setting. In particular, when aggregating samples between GPUs,\nif any artifact reveals which GPU a model came from (for example, if the GPU model weights are\ninitialized slightly differently) than the model can quickly deteriorate to a suboptimal solution, each"}]}