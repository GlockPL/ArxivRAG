{"title": "Re-evaluating Automatic LLM System Ranking for Alignment with Human Preference", "authors": ["Mingqi Gao", "Yixin Liu", "Xinyu Hu", "Xiaojun Wan", "Jonathan Bragg", "Arman Cohan"], "abstract": "Evaluating and ranking the capabilities of different LLMs is crucial for understanding their performance and alignment with human preferences. Due to the high cost and time-consuming nature of human evaluations, an automatic LLM bencher (i.e., an automatic evaluation framework that aims to rank LLMs based on their alignment with human preferences) is indispensable. An automatic LLM bencher consists of four components: the input set (e.g., a user instruction), the evaluation model (e.g., an LLM), the evaluation type (e.g., pairwise comparison), and the aggregation method (e.g., the ELO rating system). However, previous work has not thoroughly explored how to select these components or how their different combinations influence the results. In this work, through controlled experiments, we provide a series of recommendations on how to choose each component to better automate the evaluation of LLMs. Furthermore, we discovered that when evaluating LLMs with similar performance, the performance of the automatic LLM bencher declines sharply, underscoring the limitations of current benchers and calling for future work. Lastly, we found that the evaluation models' performance at the instance level (e.g., the accuracy of selecting the best output) does not always align with their effectiveness when used as a component of a bencher, highlighting the importance of dedicated system-level evaluation of benchers.", "sections": [{"title": "Introduction", "content": "Recently, various large language models (LLMs) (Ouyang et al., 2022) and LLM-based agents (Wang et al., 2024a) have been released, demonstrating strong capabilities across various tasks. These systems enable efficient interactions with human users and can be instructed to perform various complex activities, making their performance in such tasks an important aspect of evaluation. To benchmark the capabilities of these systems, human judgments of output quality remain indispensable as the gold standard, as many tasks do not have standard answers and are inherently open-ended (Zheng et al., 2023; Dubois et al., 2023). Chatbot Arena (Chiang et al., 2024) embodies this concept. It is a real-time evaluation platform aimed at a large user base, where users can freely provide input, select any two hosted LLMs to generate responses, and indicate which one they prefer. Chatbot Arena periodically derives a leaderboard of various LLMs by aggregating instance-level pairwise human evaluations. To date, Chatbot Arena has collected over 1.5 million human judgments, involving more than 100 systems. Due to its substantial size and the comprehensiveness of the systems included, its LLM ranking has been widely regarded as a trustworthy indicator of an LLM's general capabilities (Li et al., 2023, 2024b; Lin et al., 2024; Zhao et al., 2024). However, automatic LLM system rankers (i.e. benchers) are often needed because of the expensive and time-consuming nature of human evaluation.\u00b9 As a result, several widely used automatic benchers have been proposed, such as Alpaca Eval (Li et al., 2023; Dubois et al., 2024) and Arena Hard (Li et al., 2024b), which use a strong LLM such as GPT-4 (OpenAI, 2023) to compare various systems on a carefully designed input set. These automatic benchers can produce LLM rankings that correlate well with Chatbot Arena, achieving Spearman's p of around 0.95. Therefore, they have been widely used to develop and evaluate LLMs for tasks like alignment fine-tuning (Tunstall et al., 2024; Yang et al., 2024)."}, {"title": "Problem Formalization", "content": "In this section, we formalize some crucial concepts and our research questions."}, {"title": "Automatic LLM Bencher", "content": "Given a set of systems (S = {8\u2081, 8\u2082, ..., s\u2099}) to be evaluated/ranked, an automatic bencher JA is composed of the following four components:\nInput Set X = {X\u2081, X\u2082, ..., X\u2098}. Each system s\u1d62 generates a response o\u1d62\u2c7c for each input x\u2c7c.\nEvaluation Model E: This is an LLM (e.g., GPT-4-turbo) that acts as a proxy for human evaluators. It generates instance-level evaluations for each system. The evaluation model & outputs instance-level evaluation for each response o\u1d62\u2c7c.\nEvaluation Type T: This refers to the type of evaluation generated at the instance level. There are two main types\u2074:"}, {"title": "Pointwise Evaluation", "content": "The evaluation model assigns a score to a system's response. The pointwise score of the i-th system's response to the j-th input is denoted as Tpointwise(E, xj, o\u1d62\u2c7c) \u2208 R. The total number of instance-level evaluations is mn."}, {"title": "Pairwise Evaluation", "content": "the evaluation model compares the quality of two responses o\u1d62\u2c7c, o\u2096\u2c7c generated by two systems s\u1d62 and s\u2096 on the same input x\u2c7c. The base comparison result is binary, indicating whether s\u1d62 performed better than s\u2096 or vice versa. This can be expressed as: Tbase.pairwise(E, xj, o\u1d62\u2c7c, o\u2096\u2c7c) \u2208 {o\u1d62\u2c7c > o\u2096\u2c7c, o\u1d62\u2c7c < o\u2096\u2c7c}. In 5-point pairwise comparisons, the evaluation model provides more detailed feedback about the difference between two systems' performances, capturing both the magnitude and direction of the preference. The 5 possible comparison outcomes for systems s\u1d62 and s\u2096 on input x\u2c7c are: T5-point.pairwise(E, xj, o\u1d62\u2c7c, o\u2096\u2c7c) \u2208 {o\u1d62\u2c7c < o\u2096\u2c7c, o\u1d62\u2c7c < o\u2096\u2c7c, o\u1d62\u2c7c = o\u2096\u2c7c, o\u1d62\u2c7c > o\u2096\u2c7c, o\u1d62\u2c7c \u00bb o\u2096\u2c7c}. The total number of instance-level pairwise evaluations is mn(n - 1) rather than mn(n \u2212 1)/2 because we always need to swap the presentation order of the responses s\u1d62 and sj to address the position bias of LLM-based evaluations. In addition, due to the high computational cost of performing full-scale comparisons, some automatic LLM benchers (Li et al., 2023, 2024b) introduce a reference system in pairwise evaluation. In this approach, all systems under evaluation only need to be compared with a reference system (e.g. GPT-4), which reduces the complexity of pairwise evaluation to a level similar to that of pointwise evaluation."}, {"title": "Aggregation Method G", "content": "This will create a mapping from systems to scores (G(s\u1d62) \u2208 R), depending on the evaluation type.\nFor pointwise evaluations, aggregation can be done via the mean or median of the instance-level scores across all inputs:"}, {"title": null, "content": "Gmean(si) = 1/m \u2211j=1m Tpointwise (E, xj, o\u1d62\u2c7c)"}, {"title": null, "content": "Gmedian(si) =median ({Tpointwise (E, xj, o\u1d62\u2c7c)}j=1m)\nFor pairwise evaluations, we consider the Bradley-Terry model (BRADLEY and TERRY, 1952) and win ratio\u2075. They can only be applied to base pairwise instance-level evaluations, so other"}, {"title": "How to Evaluate a Bencher", "content": "Ground Truth. From Chatbot Arena, we obtain system ratings, denoted as r(s\u1d62), for each system s\u1d62 \u2208 S, where r(s\u1d62) \u2208 R represents the overall performance of the system as derived from human judgments. The ranking of the systems is obtained by sorting them based on their ratings: RH : S \u2192 {1,2,..., n}, where RH(s\u1d62) represents the rank of system s\u1d62 based on the ratings r(s\u1d62).\nEvaluation Measure. The performance of an automatic bencher J\u0104 is evaluated by comparing its ranking RA of systems with the ground truth ranking RH, using Spearman's rank correlation coefficient (p(RA, RH)) and Kendall's tau (\u03c4(RA, RH)). p is more widely used.\nControllable Kendall's Tau (Tu): Inspired by Deutsch et al. (2022), we propose controllable Kendall's tau that evaluates the agreement between rankings by focusing only on system pairs where the performance difference is small, i.e., where the absolute difference in the human-provided scores for two systems is less than a specified threshold u. This is useful for evaluating the bencher's ability to distinguish between closely matched systems.\nLet \u0394r(s\u1d62, s\u2096) = |r(s\u1d62) \u2013 r(s\u2096)| represent the performance difference between systems s\u1d62 and s\u2096, where r(s\u1d62) is the system ratings derived from Chatbot Arena. For a given threshold u, we define the set of system pairs that are considered for evaluation as: Pu = {(si,sk) | si, sk \u2208 S,\u0394r(si,sk) \u2264 u, i \u2260 k}. To prevent the performance gap between two systems from being"}, {"title": null, "content": "Tu (RA, RH) = (Cu - Du)/\u221a(Cu + Du + Ta,u)(Cu + Du + TH,\u0438)\nwhere Cu is the number of concordant pairs (i.e., pairs of systems (si, sk) where the rank order between si and sk is the same in both RA and RH). Du is the number of discordant pairs. TA,u and TH,u are the number of ties in the automatic bencher's ranking RA and the human-provided ranking RH. Only the system pairs within the set Pu are used."}, {"title": "Different Ways of Selecting Evaluation Models", "content": "We are comparing different evaluation models E based on their ability to rank systems or predict instance-level human preferences. There are three key settings for evaluating evaluation models:"}, {"title": "Standard Meta-Evaluation with System-Level Rankings (R)", "content": "The idea of Setting 1 is to rank evaluation models based on how well their corresponding automatic benchers align with the system rankings derived from Chatbot Arena. It is the standard way benchmarks such as Arena Hard are designed.\nAn automatic bencher J(i) is composed of an evaluation model E\u1d62 and fixed input set X, evaluation type T, and aggregation method G, producing a system ranking R\u1d62. The performance of evaluation model E\u1d62 is measured by comparing RA with RH using either Spearman's p(RA, RH), or Kendall's \u03c4(RA, RH) mentioned above. Evaluation models are ranked based on their performance, leading to a ranking for evaluation models: RE: {E\u2081, E\u2082, ..., EK} \u2192 {1, 2, ..., K}."}, {"title": "Instance-Level Human Judgments as Ground Truth (R)", "content": "We consider instance-level human judgments as the ground truth, without system-level aggregation or knowledge of which systems generated the responses.\nLet D = {(xj, o(1)j, o(2)j), hj)}j=1m represent the dataset, where each entry consists of an input"}, {"title": null, "content": "Accuracy (Ei) = 1/m \u2211j=1m I(h(i)j = hj)"}, {"title": "Instance-Level Human Judgments with System Information and Aggregation (R)", "content": "We design this new setting to help us compare Setting 1 and Setting 2. If we have access to instance-level human judgments as well as system information, we can aggregate both human judgments and evaluation model predictions into system-level rankings.\nDataset with system information: Each entry in the dataset D' = {(xj, o(1)j, o(2)j, s(1)j, s(2)j, hj)}j=1m includes: an input xj, two responses o(1)j, o(2)j generated by systems s(1)j, s(2)j, and human preference hj \u2208 {o(1)j > o(2)j, o(1)j < o(2)j}, indicating which response was preferred. The steps to producing evaluation model rankings are as follows: (1) Human preferences are aggregated to produce a system ranking RH with an aggregate method G. (2) The evaluation model Ei provides instance-level predictions with T, which are aggregated using the same method G to produce a system ranking R\u1d62. (3) Compare the two system rankings RH and R\u1d62 using Spearman's p(R\u1d62, RH), or Kendall's \u03c4(R\u1d62, RH). (4) The evaluation models are then ranked (R(3)E) based on the correlation between their aggregated rankings and the aggregated human judgments."}, {"title": "Research Questions", "content": "Our research questions can be formalized as:\n\u2022 RQ1: How to choose the appropriate components for building an effective automatic LLM bencher?\n\u21d2X,E,T,A= arg maxx,E,T,A P(RA, RH)\n\u2022 RQ2: Does the performance of the bencher decline when systems with smaller performance differences are selected for evaluation?\n\u21d2 Does Tu(RA, RH) decrease as u decrease?\n\u2022 RQ3: Can we use instance-level rankings of evaluation models as a good reference to select evaluation models for LLM benchers?\n\u21d2 p(R(1)E, R(2)E) \u2248 1 and p(R(1)E, R(3)E) \u2248 1 and p(R(2)E, R(3)E) \u2248 1?"}, {"title": "Setups", "content": "We selected two input sets, Arena Hard (Li et al., 2024b) and Alpaca Eval (Li et al., 2023), 18 LLMs as the systems to be evaluated, and 12 LLMs, including four proprietary OpenAI models and six open-source models, to serve as evaluation models. Please see Appendix C for detailed information."}, {"title": "RQ1: How to choose X, E, T, A to maximize the bencher's performance?", "content": "This section discusses component selection for the bencher and includes a cost analysis."}, {"title": "Input Set", "content": "Table 1 confirms that using Arena Hard as input set always yields higher correlations with the system rankings of Chatbot Arena compared to Alpaca Eval through different combinations of E, T, A."}, {"title": "Evaluation Type", "content": "Table 1 shows the performance of a bencher under different combinations of evaluation types and aggregation methods, with several representative LLMs as evaluation models. For many open-source models with fewer parameters, base pairwise is the optimal evaluation type, offering a significant advantage over other evaluation types. For strong proprietary models, the difference between base pairwise and pointwise evaluation types is small. Across all models used as evaluation models, 5-point pairwise generally performs worse than base pairwise, with the decline being particularly pronounced for open-source models, possibly due to their weaker ability to follow complex instructions. Therefore, using 5-point pairwise as the evaluation type, as in Arena Hard, may be problematic. In addition, by comparing the results of pairwise evaluation using the full data with those using GPT-4 as the reference system, we found that for most strong evaluation models, the performance of the bencher is similar when using the reference system, indicating that this is an efficient strategy. Further discussion on the reference system is in Appendix D."}, {"title": "Aggregation Method", "content": "As shown in Table 1, when the evaluation type is base pairwise, the system rankings produced by the Bradley-Terry model and win ratio aggregation methods are mostly the same. Interestingly, when the evaluation type is pointwise, mean, the most common aggregation method, is not always optimal. Moreover, when the evaluation model is a smaller open-source model, pointwise combined with the Bradley-Terry model significantly outperforms its combination with mean. After examination, we found that this is because the majority of instance-level scores produced by these evaluation models are very similar, and aggregating them using mean or median across all samples leads to system scores that are hardly effective. In contrast, converting these scores to base pairwise retains the slight differences in scores for different responses to the same input. On the other hand, for OpenAI models, the performance of aggregating pointwise type data using either mean or the Bradley-Terry model is comparable."}, {"title": "RQ2: Does the performance of automatic LLM benchers degrade when evaluating LLMs with similar performance?", "content": "As shown in Figure 14 in the appendix, the performance differences among the selected LLM systems are uneven, so the difficulty of distinguishing between different LLMs varies. We introduced a threshold u so that only system pairs with performance differences smaller than it are used to calculate Tu. Since fewer system pairs meet the requirement as u decreases, we can control the threshold u to select a specific proportion of system pairs. Specifically, we selected 5%, 10%, ... and 100% of system pairs and observed the changes in bencher performance. Figure 2 shows that as u decreases (i.e., the performance differences between systems become smaller), the performance of almost all benchers declines sharply. For example, when Alpaca Eval is used as the input set and gpt-40 is used as the evaluation model, we found that the Bencher's performance degrades drastically by 25 points when comparing system pairs whose ChatBot Arena ratings differ by approximately 40 points, which is similar to the difference between qwen-1.5-72B and gpt-4-0314."}, {"title": "RQ3: Can we use instance-level rankings of evaluation models as a good reference to select evaluation models for LLM benchers?", "content": "Figure 3 displays the Spearman's p between evaluation model rankings obtained from different datasets and settings. We observed the following key findings: First, the evaluation model rankings vary considerably across different settings, as evidenced by the fact that none of the Spearman's p values are found to be 1.0. Second, system-level evaluation results are more consistent with each other. The evaluation model ranking obtained from Setting 3 is very similar to that from Setting 1, despite the systems to be evaluated and input sets in Alpaca Farm being different from those in Setting 1. In contrast, although the input set and involved systems in alpaca_farm_instance (Setting 2) and alpaca_farm_system (Setting 3)"}, {"title": "Related Work", "content": "Automatic LLM benchers. This kind of research always create a automatic leaderboard for various LLMs. As mentioned above, the main contribution of Arena Hard (Li et al., 2024b) lies in designing a pipeline for automatically constructing a challenging input set, also for Wildbench (Lin et al., 2024). In contrast, Alpaca Eval 2 (Dubois et al., 2024), focus on designing better evaluation models and use existing instruction datasets as the input set. Besides, MixEval (Ni et al., 2024) build LLM benchmarks by strategically mixing off-the-shelf ground-truth-based benchmarks that match real-world user queries. Zhao et al. (2024) automate the entire LLM evaluation process using LLM-powered agents. There is another line of research that specifically focuses on how build LLM-based evaluation models (Gao et al., 2024).\nComparison between different evaluation types and aggregation methods. Liusie et al. (2024) compared the effectiveness of LLM-based evaluation models with absolute score prediction and pairwise comparison on several NLG tasks. However, this study is not oriented to system-level evaluation and aggregation methods are not involved. Besides, we focus on the capabilities of LLMs from a wider perspective, instruction following, not limited to NLG tasks. Peyrard et al. (2021) showed the advantages of the Bradley-Terry model over mean and median on aggregating instance-level scores of NLP systems but this study focuses on what aggregation method makes more sense to choose as part of an evaluation metric. Liu et al. (2024) investigated which evaluation type is better under the instance-level evaluation settings thoroughly but they did not explore the system-level evaluation settings. Daynauth et al. (2024) proposed three desirable properties of aggregation methods and evaluated the robustness of several aggregation methods. It is worth mentioning that a concurrent work also focused on automatic system-level LLM judges and analyzed their decisiveness and bias (Gera et al., 2024).\nInstance-level evaluation of evaluation models. A lot of studies have focused on collecting instance-level human judgments to evaluate LLM-based evaluation models (Zeng et al., 2024b; Zheng et al., 2023; Dubois et al., 2023; Zhang et al., 2023; Wang et al., 2024b; Li et al., 2024a; Ye et al., 2024). However, to the best of our knowledge, no work has examined whether the evaluation results of them are consistent with the system-level settings."}, {"title": "Conclusions", "content": "We have explored how to construct effective LLM benchers and evaluate them. We provide several recommendations on how to combine the four components of an LLM bencher, analyze the current evaluation settings for LLM benchers and the evaluation models that serve as their components, and highlight the shortcomings in these settings. Our code and data will be available at https://github.com/yale-nlp/RealRank."}, {"title": "Limitations", "content": "The observations and conclusions of this paper are dependent on the datasets and selected LLMs, and cannot be guaranteed to apply under all circumstances. Furthermore, the open-source models included as evaluation models are relatively small in scale, which may not reflect the evaluation capabilities of larger open-source models."}]}