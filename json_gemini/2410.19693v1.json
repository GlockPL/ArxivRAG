{"title": "MILES: Making Imitation Learning Easy with Self-Supervision", "authors": ["Georgios Papagiannis", "Edward Johns"], "abstract": "Data collection in imitation learning often requires significant, laborious human supervision, such as numerous demonstrations, and/or frequent environment resets for methods that incorporate reinforcement learning. In this work, we propose an alternative approach, MILES: a fully autonomous, self-supervised data collection paradigm, and we show that this enables efficient policy learning from just a single demonstration and a single environment reset. MILES autonomously learns a policy for returning to and then following the single demonstration, whilst being self-guided during data collection, eliminating the need for additional human interventions. We evaluated MILES across several real-world tasks, including tasks that require precise contact-rich manipulation such as locking a lock with a key. We found that, under the constraints of a single demonstration and no repeated environment resetting, MILES significantly outperforms state-of-the-art alternatives like imitation learning methods that leverage reinforcement learning. Videos of our experiments and code can be found on our webpage: www.robot-learning.uk/miles.", "sections": [{"title": "1 Introduction", "content": "Imitation learning is frequently described as a convenient way to teach robots new skills. But is this true in practice? Behavioral cloning (BC) methods leverage supervised learning to train robust policies, but doing so typically requires hundreds or thousands of demonstrations per task [1, 2] to collect a sufficiently diverse training dataset. Imitation learning methods that leverage Reinforcement learning (RL) offer a solution to this as policies can be learned autonomously through random"}, {"title": "2 Related Work", "content": "As follows, we ground our work relative to methods that can learn manipulation skills from a single demonstration, unlike most approaches that require large demonstration datasets [1, 8, 9, 10].\nImitation learning from prior knowledge. An effective way to compensate for the lack of large demonstration datasets is to leverage prior task knowledge such as access to ground truth object poses [11, 12] or by meta-learning policies by first pretraining on large demonstration datasets [13, 14]. However, precise knowledge of the objects' poses is hard to obtain in practice and meta-learning methods are often limited to tasks similar to the ones seen in the demonstrations. Instead, MILES can learn a new task from just a single demonstration without any prior object or task knowledge.\nImitation learning via Reinforcement learning (RL). RL-based imitation learning methods from a single demonstration learn to follow that demonstration by minimizing a similarity metric between the trajectories of the learned policy and the demonstration [3, 4, 15, 16]. Other RL methods that learn from demonstrations infer rewards through alternative means, like goal images [17]. Though effective, these methods are often inefficient as they rely on random exploration and repeated environment resets which require significant human effort. Instead, our self-supervised data collection makes MILES highly efficient and eliminates the need for repeated environment resetting."}, {"title": "3 MILES: Making Imitation Learning Easy with Self-Supervision", "content": "As follows we describe MILES, a framework that makes imitation learning easy by leveraging a single human demonstration as guidance to collect self-supervised data demonstrating to the robot how to return to, and then follow the demonstration. By training a policy with behavioral cloning on that data, MILES learns to perform a task from a range of initial states and object poses.\n3.1 Preliminaries\nAssumptions. Our setup assumes access to a wrist camera that is rigidly mounted to the robot's end-effector (EE) and (optionally) a sensor that measures external forces and torques. We follow prior work [20, 2, 18] and assume that each task is object-centric, such that only the task-relevant object is in camera view during data collection and the demonstration can be expressed relative to a single object, where combining several such tasks results in a multi-stage task. Additionally, as we are interested in dealing with all types of tasks, including those that require contact-rich manipulation, we control our robot using an impedance controller.\nSingle Demonstration. For each task, a human provides a single demonstration  $\\varsigma := \\{(\\omega_n, o_n, a_n)\\}_{n=1}^N$ comprising a sequence of N waypoints $\\omega_n$, observations $o_n$, and actions $a_n$, as shown in Figure 2 (1). A waypoint $\\omega_n$ corresponds to the EE's 6-DoF pose at timestep n captured via proprioception. An observation $o_n$ consists of an RGB image captured from the wrist camera and a force-torque measurement. We refer to $(\\omega_n, o_n)$ as the state of the environment at timestep n. An action $a_n$ contains the gripper's state and the 6-DoF pose tracked by the impedance controller at timestep n, expressed relative to the EE's pose at timestep n \u2013 1. After providing $\\varsigma$ the human resets the environment only once, such that if the actions in $\\varsigma$ are executed, the robot would successfully perform the task; a trivial process that requires a few seconds of human time.\n3.2 Self-Supervised Data Collection\nAugmentation Trajectories. Given a single demonstration $\\varsigma$, MILES collects a dataset of augmentation trajectories $D := \\{ \\tau_k \\}$, where $1 \\leq k \\leq N$ and each $\\tau_k := \\{(\\omega_m^k, o_m^k, a_m^k)\\}_{m=1}^M$ is a robot trajectory whose final, Mth state corresponds to a kth state in the demonstration, i.e., $(\\omega_M^k, o_M^k) = (\\omega_k, o_k)$. That is, each augmentation trajectory guides the robot to some kth state in the demonstration from any state $(\\omega_m^k, o_m^k) \\in \\tau_k$. We can fuse each augmentation trajectory with the demonstration segment following each kth state, $\\{(\\omega_n, o_n, a_n)\\}_{n=k}^N \\subseteq \\varsigma$, to create a new demonstration that demonstrates to the robot how to return to and then follow the human demonstration as shown in Figure 3. By collecting augmentation trajectories that densely cover the state space near the demonstration we can create a dataset of new demonstrations which we can leverage to train a"}, {"title": "3.3 Validity Conditions for Augmentation Trajectories", "content": "As follows, we introduce two conditions that determine whether an augmentation trajectory can be fused with the human demonstration. Consider an augmentation trajectory $\\tau_k$ aimed at returning the robot to the kth demonstration state, $(\\omega_k, o_k)$:\n(1) Condition 1, Reachability: After executing the augmentation trajectory, the EE's pose must equal the pose of the demonstration waypoint $\\omega_k$. This equality can be verified trivially using proprioception. In many scenarios, the environment's dynamics (e.g. collisions) or inevitable systematic inaccuracies in a robot's controller may prevent it from reaching its target waypoint $\\omega_k$.\nThus, if $\\omega_M^k \\neq \\omega_k$, the augmentation trajectory cannot return to demonstration state k, rendering the augmentation trajectory invalid.\n(2) Condition 2, Environment Disturbance: While collecting $\\tau_k$, the robot may disturb the environment, resulting in a final observation $o_M^k$ that no longer matches that of the demonstration (even if $\\omega_M^k$ is reached). For instance, during data collection if the robot's gripper pushes an object to a different pose than it had at timestep k of the demonstration, the final observation in the augmentation trajectory will differ from the demonstration's kth observation. Therefore, if $o_M^k \\neq o_k$, the augmentation trajectory cannot be combined with the human demonstration to create a new, valid demonstration. To detect such disturbances, we compare the cosine similarity of the DINO features [29, 30] of the RGB image $I_k$ from the demonstration's observation $o_k$ and the image $I_M^k$ after executing the augmentation trajectory. If the similarity falls below a threshold $\\theta$, we assume the environment has been disturbed and stop data collection."}, {"title": "3.4 Policy", "content": "Training. We train a separate policy \u03c0 for each task as an LSTM network with behavioral cloning that receives as input the RGB and force-torque observations in the dataset $D_{new}$ and regresses the corresponding actions. Note that $D_{new}$ does not contain proprioception data, allowing our policies to generalize to different object poses naturally due to the use of our wrist camera.\nInference. We deploy our policy \u03c0 to solve a task up to the Rth demonstrated state. If no environment disturbance occurred during data collection for that task, then the Rth state is the final state in the demonstration and solves the task completely in a closed-loop manner. Otherwise, after \u03c0 completes the task up to the Rth state, the remaining demonstrated action segment $\\varsigma_{remaining}$ is replayed. More details regarding how we deploy our policy, the network architecture, and how we detect that \u03c0 has reached the Rth state can be found in the supplementary material section A.4."}, {"title": "4 Experiments", "content": "We evaluate MILES through several real-world experiments. Through these experiments, we aim to answer the following questions: 1) Can MILES solve a range of everyday tasks and how does it perform against baselines that learn from a single demonstration? 2) How does MILES perform under"}, {"title": "4.1 Can MILES solve a range of everyday tasks and how does it perform against baselines that learn from a single demonstration?", "content": "In this experiment, we assess the performance of MILES across a diverse set of everyday tasks and compare it to various baseline methods capable of learning from a single demonstration. We select seven distinct tasks, shown in Figure 4, spanning a range of complexities, each learned from a single demonstration. The tasks are: 1) Lock with key; 2) Insert USB; 3) Plug into socket; 4) Insert power cable; 5) Twist screw; 6) Bread in toaster; 7) Open lid. Tasks 1-4 are contact-rich and and our setup follows prior work on contact-rich manipulation [17, 31, 32, 33, 34] and the NIST benchmark [35]. Similar to prior work [36, 37], we focus our evaluation on single-task performance, but also report results on generalization, robustness to distractors, and multi-stage tasks in section 4.5. We provide a detailed description of each task in the supplementary material section B.3 including data collection times, information on which tasks involve demonstration replay, which tasks stopped data collection due to an environment disturbance and information on the length of each human demonstration.\nBaseline Methods. We chose 4 baselines that can learn from a single demonstration without prior task knowledge, similar to MILES. (1) Demo Replay which involves replaying the demonstrated actions. (2) Pose Estimation + Demo Replay follows [18] and leverages MILES' data to perform pose estimation followed by demonstration replay. (3) Reset Free Residual RL replays the demonstration's actions at each timestep and learns corrective actions on top using DDPG [38]. Like MILES, no human intervenes to reset the environment during training, hence we call it \"Reset Free\". Finally, (4) Reset Free FISH uses the state-of-the-art RL-based imitation learning method FISH [3] but no human intervenes to reset the environment during training. Further, implementation details on the baselines can be found in our supplementary material section B.4.\nEvaluation. For a fair evaluation, we carefully tuned each method's hyperparameters. Additionally, each learning-based baseline collected the same number of observations as MILES during data collection for each task. We evaluated each method's success rate across 20 trials. For each trial we randomized the relative starting pose of the robot and the task-relevant object equivalently across all methods within a sphere of 20cm around the object as long as the object was visible to the camera. Finally, we emphasize that for all evaluations both MILES and the baselines predict 6-DoF actions."}, {"title": "4.2 How does MILES perform under different method ablations?", "content": "This section studies MILES' performance by ablating 4 different components of the method: (1) No Environment Disturbance: we ablate the environment disturbance condition by not checking for that condition when collecting augmentation trajectories. (2) No reachability: we ablate the reachability condition by relabeling each observation's action (of the existing MILES data), to move the robot to the nearest waypoint in the demonstration based on their Euclidean distance. If the constraint for reachability is not important, then simply moving from each pose to the nearest waypoint in the demonstration in a straight line would be sufficient to solve a task. (3) No sequence: we recollect MILES' data but instead of collecting Z augmentation trajectories for the first demonstration state, then progressing to the second state and so on, we collect data without following the demonstration's waypoint sequence and instead follow a random one. (4) No Memory: For this ablation we retrain a network on the existing MILES data that does not account for history.\nResults. shows MILES performance after ablating each component. Collecting augmentation trajectories for each demonstration state in a random order (No Sequence), with an average suc-"}, {"title": "4.3 How important are vision and force modalities to the performance of MILES?", "content": "In this section, we ablate the use of vision and force feedback as policy inputs for the four contact-rich tasks from our earlier experiments. We retrain and evaluate two policies: one using only vision and one using only force. The results, shown in Figure 5, indicate that the vision-based policy improves MILES' performance in the \"Insert USB\" and \"Plug into socket\" tasks but reduces performance in the other two tasks. This suggests that force feedback might not consistently benefit MILES, possibly due to its noisy signal which makes it hard to distinguish between different environment states. The force-based policy, however, fails almost completely. This is expected as force feedback is zero in free space and can be ambiguous due to symmetries in object surfaces. Overall, while force feedback aids performance in some tasks, it is not always necessary. Vision remains the most crucial modality to MILES' high performance."}, {"title": "4.4 How does MILES perform under different sizes of self-supervised data?", "content": "In this section, we ablate the dataset size used to learn four tasks by splitting their original datasets into chunks containing 75%, 50%, and 25% of the original data. We evaluate the best and worst performing contact-rich tasks (\"Lock with key\" and \"Insert USB\") and non-contact-rich tasks (\"Open lid\" and \"Twist screw\"). Data collection times for each task can be found in the supplementary material.  shows that for high tolerance tasks like \"Open lid,\" MILES achieves a 100% success rate even with 25% of the data, corresponding to only 8 minutes of data collection. However, for precise tasks, success rates decrease as dataset size is reduced. Notably, for \"Lock with key\" and \"Twist screw,\" reducing the dataset to 50% results in a high failure rate. To summarize, we observe that high tolerance tasks are likely to require less data, and in practice only a few minutes of data collection time. Instead, for high-precision tasks, like inserting a USB, the dataset size appears to impact MILES' performance significantly."}, {"title": "4.5 Further questions about MILES", "content": "Can MILES generalize? We conduct additional experiments on generalization for MILES in the supplementary material section D.1. Can MILES perform multi-stage tasks? We provide additional experimental results on how MILES performs on multi-stage tasks in the supplementary material section D.2. Is MILES robust to distractor objects? We conduct additional experiments studying MILES' performance in the presence of distractors in the supplementary material section D.3. What if MILES stops data collection early due to a detected environment disturbance? We provide a discussion and intuition on MILES' behavior in scenarios where data collection stops early in our supplementary material section D.4."}, {"title": "5 Discussion", "content": "Limitations. We now highlight some important limitations of our method. Firstly, MILES' reliance on a wrist camera enables MILES to obtain spatial generalization, however, simultaneously this limits its field of view and its applicability to larger task spaces. Future work could address this by incorporating an external camera to initially approach an object before switching to the wrist camera, similarly to [21]. Secondly, while MILES is robust to distractors at deployment before data collection begins it requires a human to set up the robot's workspace such that only the task-relevant object is in camera view for the policy to achieve spatial generalization. While this requires only a few seconds of human time, future work could address this by extending MILES to incorporate segmentation methods, similar to [21, 40], that segment the task-relevant object in the dataset. Similarly, to address any unwanted collisions that MILES could cause in the presence of multiple objects, future work could study incorporating an external camera during self-supervised data collection to plan and collect collision-free augmentation trajectories. Thirdly, our current implementation of MILES trains a separate policy for each task and hence it is unclear how well MILES would generalize to completely new tasks. In future work, we aim to study this by training a single monolithic policy on MILES' self-supervised data combined with replay-trajectory retrieval [41] similarly to our generalization task in section 4.5.\nConclusion. We introduced MILES, a framework that makes imitation learning easy. MILES requires only a single demonstration and collects self-supervised data that demonstrate to the robot how to return to and then follow that demonstration. Subsequently, this enabled us to obtain manipulation skills comprising either (1) a single end-to-end policy trained with behavioral cloning, or (2) a combination of an end-to-end policy and demonstration replay. Our real-world experiments showed that when only a single demonstration is available, self-supervised data enable the acquisition of skills that achieve considerably improved performance compared to several state-of-the-art baselines. MILES can learn everyday tasks, ranging from opening a lid, to using a key to lock a lock, to inserting a USB stick into a port, requiring complex and precise contact-rich manipulation."}]}