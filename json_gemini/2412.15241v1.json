{"title": "Quantifying Positional Biases in Text Embedding Models", "authors": ["Reagan J. Lee", "Samarth Goel", "Kannan Ramchandran"], "abstract": "Embedding models are crucial for tasks in Information Retrieval (IR) and semantic similarity measurement, yet their handling of longer texts and associated positional biases remains underexplored. In this study, we investigate the impact of content position and input size on text embeddings. Our experiments reveal that embedding models, irrespective of their positional encoding mechanisms, disproportionately prioritize the beginning of an input. Ablation studies demonstrate that insertion of irrelevant text or removal at the start of a document reduces cosine similarity between altered and original embeddings by up to 12.3% more than ablations at the end. Regression analysis further confirms this bias, with sentence importance declining as position moves further from the start, even with with content-agnosticity. We hypothesize that this effect arises from pre-processing strategies and chosen positional encoding techniques. These findings quantify the sensitivity of retrieval systems and suggest a new lens towards embedding model robustness.", "sections": [{"title": "Introduction", "content": "Embedding models are increasingly used to encode text in critical applications like document search systems. However, their effectiveness diminishes when dealing with long-context inputs, particularly in larger documents that cannot entirely fit into these models' context windows. To address these limitations, techniques such as document chunking are used to segment large documents into smaller pieces of text as model inputs [37]. Despite its utility, research into optimal chunking strategies is still an emerging field and improvements can often be highly domain-specific or underexplored in practical environments. [34].\nIn this study, we investigate the influence of content position and input size on the resulting text embedding vector from eight embedding models. Our findings reveal a systematic bias in which embedding models, regardless of their positional encoding mechanisms, disproportionately weigh the"}, {"title": "Background", "content": ""}, {"title": "Bidirectional encoding in embedding models", "content": "Embedding models, particularly those utilizing transformer encoder architectures [29], employ layers of bidirectional self-attention blocks to process text [5]. These models are distinct from decoders in that they generate a fixed-length vector representing the entire input text. This is achieved by producing an output matrix L \u00d7 D (where L is the sequence length and D is the dimensionality of the embeddings), and then applying either mean or max pooling across the L dimension [21]. Such pooling operations are position-invariant, theoretically suggesting an unbiased treatment of input positions in terms of attention and representation [24].\nWe use cosine similarity to compare the output embeddings from these models, especially to study the effects of textual modifications such as insertions or deletions. Cosine similarity measures the cosine of the angle between two vectors, thus providing a scale- and orientation-invariant metric to assess the similarity between two text representations [15]. Due to the invariance of the architecture and similarity measurement we employ, the last systematic source of bias stems from learned positional embeddings used in our models and the models' training methodology, which are heavily connected."}, {"title": "Positional Encoding Techniques", "content": "Absolute Positional Embedding (APE) assigns fixed position-specific vectors based off of position id to each token embedding. This was first popularized by BERT [5] and remains the most common technique to add positional information in encoder-style models today.\nRotary Positional Embedding (RoPE): ROPE encodes positions by applying a rotation to each token's embedding in the 2D subspaces of the embedding space. For each embedding vector x, it applies a rotation matrix R(\u03b8) based on the position pos:\n$x_{pos}^{(2i)} = x^{(2i)} cos(\u03b8_{pos}) \u2013 x^{(2i+1)} sin(\u03b8_{pos})$\n$x_{pos}^{(2i+1)} = x^{(2i)} sin(\u03b8_{pos}) + x^{(2i+1)} cos(\u03b8_{pos})$\nwhere $\u03b8_{pos} = pos/10000^{2i/d}$, i indexes the embedding dimensions, and d is the dimensionality."}, {"title": "Attention with Linear Biases (ALiBi)", "content": "ALiBi introduces a relative bias into the attention scores rather than modifying the embeddings. The bias is linear with respect to the distance between tokens. The attention score A(i, j) between token i and token j is modified by adding a bias term m(|i \u2013 j|), where i j is the distance between tokens:\n$A(i, j) = \\frac{q_i k_j}{\\sqrt{d_k}} + m(|i - j|)$\nwhere m(|i - j|) is a linear function of the relative distance between tokens i and j, and $d_k$ is the dimensionality of the key vectors."}, {"title": "Noise from Document Chunking for IR Tasks", "content": "In practical applications, documents often exceed the context length capabilities of embedding models, necessitating chunking strategies like naive, recursive, or semantic chunking [6, 7]. This process divides a document into smaller pieces that fit within a model's context window, then embeds each chunk separately for insertion into a vector database [12] and downstream use in Retrieval-Augmented Generation (RAG) [14] tasks. This causes an unintentional, outsized amount of noise in the beginning and end of documents as a function of selected chunking strategies."}, {"title": "Embedding Models Robustness", "content": "The performance of decoder models has been shown to vary significantly with the position of content within the model's context window, with pronounced degradation observed for inputs that exceed the context length seen during training [16]. Positional encoding methods have been studied to address these challenges from both decreasing the effect of content position within training context length[36], and generalizing to longer contexts from itself[13]. However, these works exhibit limitations: The former provides limited analysis of diverse encoding mechanisms, and the latter emphasizes generalization to longer inputs rather than robustness to positional shifts.\nMoreover, both studies focus exclusively on decoder-only architectures, whose causal attention mask provides the ability for the model to generalize without explicit positional information itself[13], and remains underexplored as a research direction. Existing work on embedding model robustness predominantly centers on improving training data quality or diversity[33], with relatively little attention paid to architectural components such as positional encoding mechanisms."}, {"title": "Effect of sentence-level positioning in embedding output", "content": "We explore how the position and size of a sentence in a text influence a document's final embedding vector. Our methodology adapts the needle-in-a-haystack test [10], traditionally used for generative models in information retrieval [26], to evaluate embedding models."}, {"title": "Experimental setup", "content": "We investigate the impact of adding irrelevant or adversarial text (\"needle\") to a document. After inserting the needle, we generate a new embedding for the altered text and compare it to the original using cosine similarity. We vary the needle's length (5%, 10%, 25%, 50%, and 100% of the original text's token count) and position (beginning, middle, end) across 15 experimental conditions. We use an extended version of Lorem Ipsum placeholder text [27] that exceeds the length of our longest datapoint and is structured in paragraph format to achieve a needle with structural similarity to our data while avoiding a confounding effect on the embedding model.\nIn a parallel experiment, we remove portions of text (10%, 25%, 50% of sentences, rounded up) from different positions (beginning, middle, end) in the document. The resulting text is then embedded, and its similarity to the original embedding is measured using cosine similarity. We test various models, segmented by their positional encodings, to demonstrate the consistency of our results across multiple popular embedding models. We used six open-source models utilizing various positional encoding methods - BGE-m3 [2] and E5-Large-V2 [31] using APE; Nomic-Embed-Text-v1.5 [18] and E5-ROPE base [37] using RoPE; and Jina-Embeddings-v2-Base [11] and Mosaic-Bert-Base"}, {"title": "Results and discussion", "content": "Our results indicate a pronounced drop in similarity when irrelevant text is inserted at the beginning of documents, with less impact observed when additions occur in the middle or end. Specifically, for APE models, introducing an insertion equal to 20% of the total content at the beginning results in an average cosine similarity of 0.885, compared to 0.963 at the end\u2014a relative decrease of approximately 8%. RoPE-based models show a stronger sensitivity to this disruption, with cosine similarity dropping to 0.819 at the beginning, a 15.4% decrease compared to the 0.968 similarity at the end. By contrast, AliBi models are the most robust, maintaining a high cosine similarity of 0.981 at the beginning and 0.999 at the end, reflecting only a 1.8% decrease. This suggests that earlier positions in the input sequence play a more critical role in model performance, and different positional encoding methods vary in their resilience to this type of input perturbation.\nThis trend persists across all insertion sizes, with larger insertions intensifying the drop in similarity. Even though the magnitude of the degradation varies by model, we find the trend robust to model differences. Across all five models tested, the average decrease in cosine similarity is approximately 7%, indicating a consistent pattern of sensitivity to input alterations at the beginning of the sequence.\nAdditionally, we observe that removal ablations yield similar results, although the overall similarity scores are higher in comparison to insertion ablations. This suggests that while the models are affected by both insertion and removal disruptions, the impact of irrelevant insertions at the beginning of sequences may introduce greater noise into the representations.\nSimilar trends are observed in the removal experiments, where the largest impacts on similarity occur when sentences are removed from the beginning. Removing half of the sentences from the beginning results in a median similarity that is 10.6% lower than when sentences are removed from the end, with no significant difference between middle and end removals-unlike the insertion experiments. Interestingly, even a 50% text removal from the middle maintains a median similarity"}, {"title": "Analysis of embedding decomposition", "content": "Recent advancements in embedding interpretability have demonstrated that certain dimensions in high-dimensional semantic spaces may correspond to specific linguistic or semantic features, such as sentiment or subject matter [4]. Further research has shown that vector operations, such as adding embeddings, can produce new vectors that represent the semantic meaning of their components [22].\nBuilding from these works, we explore the impact of sentence-level positioning on the final document embedding vector through regression analysis, which offers a more direct method to quantify the contribution of individual sentences to a document's embedding representation.\nHuman writing often emphasizes key information at the beginning and end of documents, a technique that may introduce biases in datasets and reason for embeddings to skew towards these positions. To address these, we employ additional data augmentation and ablation techniques aimed at isolating and understanding these effects, to ensure that our findings more accurately reflect model behavior rather than dataset peculiarities."}, {"title": "Reconstructing embedding vectors through linear combinations of constituents", "content": "To start, we wanted to validate the assumption that the sentence embeddings of a larger document can meaningfully be used as a proxy for the original document embedding [28]. To test this, we wanted to determine how much reconstruction loss we would incur from using an optimal linear combination of sentence embedding vectors instead of a full multi-sentence embedding vector. Optimizing for train R2, we use Ordinary Least Squares (OLS) regression to reconstruct the document embedding from its sentence embeddings, with the multi-sentence embedding vector as our response and each sentence vector as a predictive datapoint for our regression. Our model choice is notable for its direct interpretability [25], though we acknowledge and check for potential issues posed by OLS, such as multicollinearity. Our regressions use normalized embeddings (L2 norm of 1) to ensure scale invariance [23]. We separate our data points into their component sentences by use of punctuation such as periods, and new lines.\nWhen we regress the sentence embedding vectors onto the multi-sentence embedding vector, we find that our train R2 across the eight models and five datasets we used ranges from 0.75 to 0.99, with an average R2 or 0.876 when reconstructing the multi-sentence embedding vector. This result indicates that approximately 87.6% of the variance in a long-content document embedding can be accounted for by analyzing the embeddings of the individual sentences constituting the document. The Mean Squared Error (MAE) summed over all dimensions of this reconstruction across all models and datasets ranged from 0.001 and 0.01 with an average of 0.0069, suggesting minimal deviation in the reconstructed vectors."}, {"title": "Analyzing regression coefficients as importance weights", "content": "Given the high explanatory power of our regression models, the coefficients given to each sentence (datapoint) in our regression are strong indicators to determine their relative importance to the total document. To standardize our comparisons across documents, we standardized each coefficient vector by its L2 norm. One potential issue to note with this approach is the presence of negative coefficient values, but these tended to be rare and very low in magnitude, with very little influence on our final analysis.\nWe judge the importance of a sentence by its regression coefficient. For example, if a regression on a two-sentence document yielded weights 0.8 and 0.6, we conclude that the first sentence is 33.3% more important to the final semantic meaning of the text than the second sentence.\nAs shown in Figure 2, there is a downward trend in coefficient values with increasing sentence position, suggesting a positional bias where earlier sentences generally have a greater impact on the document's overall semantic representation. To quantify this observation, we plot regression coefficients against sentence positions over all the documents in our dataset."}, {"title": "Embedding positional bias is robust to human-level writing bias", "content": "To validate that this observed bias is not solely a byproduct of dataset-specific characteristics, namely human-level writing bias, we conducted additional regression experiments where all sentences from the above pre-processing steps were shuffled before their embeddings were generated. Using these new embeddings, remarkably, the results mirrored the original findings, with the randomly selected first sentence in the shuffled document consistently receiving a higher weight, thereby disambiguating our results from potential dataset biases.\nMore specifically, we expect the weight assigned to the first sentence to follow a uniform weight of $\\frac{1}{num\\_sentences}$. However, this analysis shows a distinct negative correlation between sentence position and importance score, with significant deviations from the expected uniform distribution (a < 0.001), confirming a systematic positional influence within document embeddings as shown in table 1. These findings suggest that the embedding models may inherently prioritize the initial information presented in any text sequence, irrespective of its original position in the document."}, {"title": "Isolating the role of training methodology in model biases", "content": "During training, input data is processed sequentially, starting at the beginning of the context window. Variable-length training samples are packed into this fixed window, often necessitating truncation when the input exceeds the window's length. Truncation typically discards content from the end, leading to a systematic bias where earlier positions in the sample receive disproportionate attention.\nFor a given position i \u2208 [0, N] within a context window of length N, the model observes $t_i$, the number of non-padding tokens encountered at position i. The importance of position i can then be modeled as $imp(t_i) = u(t_i)$, where u(\u00b7) represents the model's updates based on the presence of non-padding tokens at $t_i$.\nAs traditional truncation favors earlier positions, the frequency with which tokens are seen at the beginning of the context window is inherently higher than at the end. This can be modeled as a monotonically decreasing function, where the quantity of non-padding tokens at $t_i$ diminishes as i increases. As a result, the relative importance of earlier positions $imp(t_1) \u2265 imp(t_2) \u2265 ... > imp(t_n)$ is systematically higher, introducing an implicit bias that prioritizes early context over later content."}, {"title": "Conclusion", "content": "Our study uncovers a positional bias in embedding models, where sentences at the beginning of a document disproportionately influence the resulting embeddings. This bias is consistently observed across various models with different context sizes and datasets and is evident in both text insertion and removal experiments. We further quantified this effect through regression analysis, which highlights the extent of the model's preference for earlier content. Our findings suggest that this bias is intrinsic to the models' training methodologies, particularly the use of truncation strategies, rather than a consequence of dataset-specific patterns.\nThis positional bias poses challenges in critical applications like information retrieval in document search systems, highlighting the need for alternative positional encoding methods to mitigate these biases and achieve more balanced semantic representations. Additionally, growing research into extending context lengths offers a promising avenue for further exploration of this phenomenon and potential solutions."}]}