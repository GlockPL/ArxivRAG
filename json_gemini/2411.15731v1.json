{"title": "Fusion Matters: Learning Fusion in Deep Click-through Rate Prediction Models", "authors": ["Kexin Zhang", "Fuyuan Lyu", "Xing Tang", "Dugang Liu", "Chen Ma", "Kaize Ding", "Xiuqiang He", "Xue Liu"], "abstract": "The evolution of previous Click-Through Rate (CTR) models has mainly been driven by proposing complex components, whether shallow or deep, that are adept at modeling feature interactions. However, there has been less focus on improving fusion design. Instead, two naive solutions, stacked and parallel fusion, are commonly used. Both solutions rely on pre-determined fusion connections and fixed fusion operations. It has been repetitively observed that changes in fusion design may result in different performances, highlighting the critical role that fusion plays in CTR models. While there have been attempts to refine these basic fusion strategies, these efforts have often been constrained to specific settings or dependent on specific components. Neural architecture search has also been introduced to partially deal with fusion design, but it comes with limitations. The complexity of the search space can lead to inefficient and ineffective results. To bridge this gap, we introduce OptFusion, a method that automates the learning of fusion, encompassing both the connection learning and the operation selection. We have proposed a one-shot learning algorithm tackling these tasks concurrently. Our experiments are conducted over three large-scale datasets. Extensive experiments prove both the effectiveness and efficiency of OptFusion in improving CTR model performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Click-through rate (CTR) prediction is a vital task for commercial recommender systems and online advertising platforms, as it seeks to predict the likelihood that a user will click on a recommended item, such as a movie or advertisement [2, 29]. As deep learning-based CTR models have advanced, researchers have developed various model architectures [5, 14, 26, 36, 37, 43] to better capture feature interactions and enhance prediction performance. These deep CTR models employ a combination of explicit and implicit components to represent feature interactions. Shallow components, including inner products [26], cross layer [36], and Factorization Machines (FM) [5, 28], are used to model these interactions explicitly. Concurrently, deep components like multi-layer perceptrons (MLP) [43] and Self-attention layer [32] implicitly capture the complexity of feature interaction. With the improved deep and shallow components, deep CTR models can model feature interactions more effectively, leading to better prediction accuracy.\nDespite advances in CTR prediction through the enhancement of deep and shallow components, the design of fusion mechanisms has not been extensively studied. Fusion design is crucial as it involves aggregating representations from different model components. Previous works [26, 36], as illustrated in Figure 1, have predominantly relied on two naive fusion designs: stacked and parallel. In the stacked design, shallow components are typically placed before deep components and are trained sequentially [7, 15, 26]. For example, as depicted in Figure 1 (a), IPNN [26] uses the inner product as a shallow component, concatenating its output with original embeddings before feeding them into deep components. Conversely, the parallel design involves the joint training of shallow and deep components. Models like DCN [36] use concatenation for fusion, while others, such as DeepFM [5] and Wide&Deep [4], employ addition to combine outputs from both components [4, 14, 32]. In summary, these two naive fusion designs rely on pre-defined fusion connections and fixed fusion operations to fuse representations from both deep and shallow components. However, variations in fusion design can lead to substantial differences in performance across different datasets. For instance, DCNv2 [37] with a parallel design may outperform its stacked counterpart on the MovieLens dataset while underperforming on the Criteo dataset. This inconsistent performance among different fusion designs across various datasets is also observed in MaskNet [38]. These findings underscore the critical role of fusion in CTR predictions.\nAttempts have been made to refine the above-mentioned fusion designs. EDCN [3], for instance, introduces a manually crafted complex fusion design that incorporates additional fusion connections and sophisticated fusion operations. Despite using the same components as its predecessor DCN [36], EDCN achieves significant improvement in performance. FinalMLP [24] suggests the use of Multi-Head Bilinear Fusion Operations as a more effective means of integrating representations, outperforming naive fusion operations such as concatenation or addition. These examples demonstrate the potential of fusion design in significant improvements. However, these proposed solutions [3, 24] typically address the fusion learning problem under specific settings or depending on specific modules, failing to demonstrate the potential of fusion learning explicitly. A more general and adaptable approach to fusion learning remains a compelling challenge.\nOther researchers have explored using neural architecture search (NAS) within CTR models to deal with fusion design challenges by defining a broader search space [31, 42]. AutoCTR [31] employs an evolutionary approach to simultaneously search for the optimal component type, raw feature input, fusion connections, and component-specific hyperparameters. However, this approach incurs a considerable training cost since each candidate architecture must be trained separately during the search phase. NASRec [42] takes a different approach by expanding the search space even further for better utility while leveraging weight-sharing techniques to mitigate the training cost during the search process. Despite these innovations, searching for fusion design in conjunction with other model structures, such as component types, within such a vast search space can make it much harder to obtain an optimal result. In contrast, our research focuses solely on one of the critical aspects: fusion learning. By narrowing down the search space, better results and faster convergence speed can be achieved. The issues mentioned above highlight the necessity for a comprehensive yet lightweight approach to fusion learning that simultaneously selects both fusion connections and operations.\nTo address these challenges, we introduce OptFusion, an automated fusion learning framework for deep CTR prediction. OptFusion aims to explore how fusion, both in terms of connections and operations, can impact CTR predictions and automatically identify the most suitable fusion design. We propose a unified search space specifically tailored to the fusion learning process with shallow and deep components. This design allows each component to be connected to its predecessors. Such a design effectively focuses the search space on fusion learning and facilitates more efficient exploration compared to neural architecture search methods in CTR models. Drawing inspiration from previous work [30], we propose a one-shot learning algorithm that concurrently learns fusion connections and selects fusion operations. This algorithm considers"}, {"title": "2 PRELIMINARY", "content": "In this section, we first formulate the CTR prediction problem in Section 2.1. Then, we introduce the fusion learning for CTR prediction in Section 2.2."}, {"title": "2.1 CTR Prediction", "content": "CTR prediction is a classic supervised binary classification problem [29]. Given a dataset \\(D = \\{(x, y)\\} \\) consisting of \\(N = |D|\\) instances with each containing a pair of user and item, the CTR prediction aims to predict whether the user would click the item. Here x denotes the input data instance, and \\(y \\in \\{0, 1\\}\\) denotes the label indicating whether the user clicked the item.\nIn deep learning-based CTR prediction models, an embedding layer \\(\\mathcal{E}\\) is usually adopted to transform the input x with high-dimensional sparse raw features into low-dimensional dense embeddings e. The embedding e can be obtained via embedding lookup [5], formulated as \\(e = \\mathcal{E}(x)\\). The embedding e is further fed into the CTR model, formulated as follows:\n\\[\\hat{y} = F(e, \\Theta) = F(\\mathcal{E}(x), \\Theta)\\]\nwhere \\(\\hat{y}\\) is the probability that a user will click a given item, \\(F(\\cdot)\\) is the prediction model, and \\(\\Theta\\) is the corresponding trainable model parameters. Cross-entropy loss is commonly adopted to train \\(\\Theta\\), which can be formulated as follows:\n\\[\\underset{\\Theta}{\\text{arg min}} \\mathcal{L}_D(\\Theta) = \\sum_{(x, y) \\in D} y \\log \\hat{y} + (1 - y) \\log (1 - \\hat{y}).\\]"}, {"title": "2.2 Fusion Learning in CTR", "content": "In this section, we aim to take a deeper look at the CTR model \\(F(\\cdot)\\). Here, we formulate the CTR model as an instance of the fusion design, written as:\n\\[F = \\mathcal{P}(\\mathcal{G}|c, o).\\]\nHere \\(\\mathcal{P}\\) denotes the fusion learning, parameterized by fusion connection c and fusion operation o. \\(\\mathcal{G} = \\{G_s, G_d\\}\\) refer to the set of all components in the CTR model, with \\(G_s\\) and \\(G_d\\) represent the set of shallow and deep components, respectively. Various shallow components such as cross layer [36], factorization machine [5], or inner product [26] can be chosen. Similarly, deep components may also vary from MLP layer [43] to self-attention module [32]. Below, we separately discuss the two parameters of fusion learning: fusion connection c and fusion operation o."}, {"title": "2.2.1 Fusion Connection", "content": "Fusion connection c determines the connectivity between different components. Such connectivity determines what information is fed into the current component. There are only two potential states of connectivity between two components, i.e., CONNECTED or DISCONNECTED. Hence, we adopt a connectivity function \\(c(\\cdot)\\) to formulate the fusion connection from component \\(G_i\\) to component \\(G_j\\) as\n\\[c(G_i, G_j) = \\begin{cases} 1, & \\text{if CONNECTED} \\\\ 0, & \\text{if DISCONNECTED} \\end{cases}\\]\nIn the fusion learning, each component has its level, which constrains the direction of the connection. The current component only takes the outputs from lower-level components as the input for the fusion module. Such design is introduced to avoid cycles [30, 31]. Such a constraint can be formulated on each \\((G_i, G_j)\\) pairs:\n\\[c(G_i, G_j) = 1 \\rightarrow \\mathcal{L}(i) < \\mathcal{L}(j), \\forall (G_i, G_j)\\]\nwhere \\(\\mathcal{L}(i)\\) is a non-negative integer denotes the level of component \\(G_i\\). It increases monotonically as the component gets deeper."}, {"title": "2.2.2 Fusion Operation", "content": "After determining the fusion connection, a fusion operation o needs to be selected for each component. The fusion operation aggregates the output representations from the connected components. It outputs a fused representation, which serves as the input for the succeeding component \\(G_j\\), as shown in Figure 1. This process can be written as:\n\\[\\hat{e}_j = o_j(\\{c(G_i, G_j) \\cdot e_i\\}).\\]\nHere \\(\\hat{e}_j\\) refers to the input for component \\(G_j\\) and \\(e_i\\) denotes the output for preceding component \\(G_i\\). Note that the fusion operations are usually selected from a set of candidates \\(\\mathcal{O}\\). It may vary from simple operations such as ADD or CONCAT to complex operations like Multi-Head Bilinear Fusion Ops [24]. Given the one-to-one mapping relationship between fusion operation \\(o_j\\) and component \\(G_j\\), we have \\(|o| = |G|\\)."}, {"title": "2.2.3 Fusion Learning in CTR", "content": "After formulating fusion connection learning and fusion operation selection, the prediction in Eq. 1 can be reformulated as:\n\\[\\hat{y} = F(\\mathcal{E}(x), \\Theta) = \\mathcal{P}(\\mathcal{G}|c, o) (\\mathcal{E}(x), \\Theta).\\]\nConsequently, the training objective in Eq. 2 can be rewritten as:\n\\[\\underset{\\Theta, c \\in \\{0,1\\}, o \\in \\mathcal{O}}{\\text{arg min}} \\mathcal{L}_D(\\Theta, c, o)\\]"}, {"title": "3 OPTFUSION", "content": "In this section, we detail the OptFusion framework under the fusion learning setting. We first describe the search space of the framework in Section 3.1. Then, we introduce fusion connection learning in Section 3.2 and fusion operation selection in Section 3.3. Finally, we elaborate on the details of the one-shot learning algorithm, which jointly conducts fusion connection learning and fusion operation selection for OptFusion, in Section 3.4."}, {"title": "3.1 Search Space", "content": "In this section, we detail the search space of the framework in the fusion learning setting. The OptFusion framework consists of one embedding component \\(\\mathcal{E}(\\cdot)\\), n shallow components, n deep components, and one output component \\(\\mathcal{H}(\\cdot)\\). The number of all components in the search space is 2n + 2. The default configuration of the OptFusion framework with n = 3 is illustrated in Figure 1 (b). The setting of n is discussed in Section 4.4.4.\nCandidates of Fusion operation. Shallow and deep components may receive multiple inputs from lower-level components. Thus, a fusion operation is needed to fuse these inputs. The commonly-used fusion operations include ADD, PROD, CONCAT and ATT, which represent element-wise addition, Hadamard product, concatenation, and attention, respectively. Each component can select one of them to fuse information from lower-level components. Alternatively, each component can also output a weighted sum. Depending on this, we propose two variants of OptFusion, namely Hard and Soft.\nSearch space analysis. In this subsection, we intuitively illustrate the difficulty in selecting suitable fusions. Given that OptFusion aims to search for both fusion connections and operations, we need to jointly consider their possibility. The number of possible connections is determined by the number of components 2n + 2. The number of all valid connections equals to \\(2\\times(1 + 3 + \\dots + 2n - 1) + 2(n + 1) = 2n^2 + 2n + 1\\). For each valid connection, the number of choices for its connection state is 2. Thus, the size of the search space for connections is \\(2^{2n^2 + 2n + 1}\\). For each component, suppose the number of possible choices for its fusion operation is k. Thus, the search space size for the fusion operation is \\(k^{2n+1}\\). The number of possible fusions equals \\(2^{2n^2+2n+1} \\times k^{2n+1} = O(2^{2n \\times kn})\\). Directly selecting over such a large space is almost impossible. Hence, we separately discuss the connection learning and operation selection in the following two sections."}, {"title": "3.2 Fusion Connection Learning", "content": "A critical issue for fusion connection learning lies in the discrete selection space. Searching within a discrete candidate set of connections (i.e., \\(C = \\{\\text{CONNECTED}, \\text{DISCONNECTED}\\}\\)) is non-differentiable, which makes the architecture untrainable. To solve this problem, we relax the discrete search space to be continuous by learning the relative importance (i.e., probability) of each connection and introduce the architecture parameters \\(\\alpha \\in \\mathbb{R}^{(2n+2)^2}\\) to parameterize the connectivity function \\(c(\\cdot)\\) so that the fusion connection becomes learnable. With \\(a_{ij}\\) representing the connectivity \\(c(G_i, G_j)\\) from component \\(G_i\\) to \\(G_j\\), Eq. 4 can be rewritten as follows\n\\[c_{ij} = \\begin{cases} 1, & \\text{if } a_{ij} > 0, \\\\ 0, & \\text{if } a_{ij} \\leq 0 \\end{cases} \\quad 1 \\leq i, j \\leq 2n + 2.\\]\nTo satisfy the level constraint in Eq. 5, \\(\\alpha\\) is constrained as:\n\\[a_{ij} \\geq 0 \\rightarrow \\mathcal{L}(i) < \\mathcal{L}(j), \\forall 1 \\leq i, j \\leq 2n + 2.\\]\nTo enable end-to-end training and get meaningful gradients for \\(\\alpha\\), we adopt the straight-through estimator (STE) function [1]. The STE can be formulated as a customized function \\(S(\\cdot)\\), with its forward pass as a unit step function \\(S(x) = 0, x \\leq 0\\) and \\(S(x) = 1, x > 0\\). \\(S(x)'s\\) backward pass equals to \\(\\delta S(x) = 1\\), meaning that it will directly pass the gradient backward. Therefore, we can mimic a discrete selection while providing valid gradient information for connection parameters \\(\\alpha\\), making the whole process trainable. Hence, the final output in Eq. 7 can be rewritten as:\n\\[\\hat{y} = \\mathcal{P}(\\mathcal{G}|\\alpha, o)(\\mathcal{E}(x), \\Theta).\\]"}, {"title": "3.3 Fusion Operation Selection", "content": "For shallow or deep components, they may receive multiple connections from lower-level components. One operation needs to be selected from the set of fusion operations to fuse the information received from lower-level components. Specifically, we define the operation candidates as \\(\\mathcal{O} = \\{\\text{ADD}, \\text{PROD}, \\text{CONCAT}, \\text{ATT}\\}\\) and \\(|\\mathcal{O}| = k\\). Similar to connection learning, we also relax the discrete search space of fusion operations to be continuous by learning the relative importance of each operation and introduce the architecture parameters \\(\\beta \\in \\mathbb{R}^{k \\times (2n+2)}\\) to represent the operation selection. Given a component j, we assign an architecture parameter \\(\\beta_{j}\\) to an operation \\(o \\in \\mathcal{O}\\), the importance of operation o is computed as a softmax of all candidate operations \\(o' \\in \\mathcal{O}\\):\n\\[p_j^o = \\exp(\\beta_j^o) / \\sum_{o' \\in \\mathcal{O}} \\exp(\\beta_j^{o'}),\\]\nwhere \\(p_j^o\\) is the importance of operation o. During the selection stage, the input of component j equals a weighted summation over all candidate operations:\n\\[\\hat{e}_j = \\sum_{o \\in \\mathcal{O}} p_j^o o \\left( \\{a_{ij} e_i\\} \\right),\\]\nwhere \\(e_i\\) is the output of component i. \\(o(\\cdot)\\) fuses all the inputs by using operation o. Finally, by parameterizing the selection of fusion operation \\(o \\in \\mathcal{O}\\) with architecture parameters \\(\\beta\\), Eq. 11 can be rewritten as follows:\n\\[\\hat{y} = \\mathcal{P}(\\mathcal{G}|\\alpha, \\beta)(\\mathcal{E}(x), \\Theta).\\]"}, {"title": "3.4 One-shot Learning Algorithm", "content": "After obtaining Eq. 14, which parameterizes the fusion learning via architecture parameter \\(\\{\\alpha, \\beta\\}\\), we need to rewrite the original learning goal in Eq. 8 to incorporate the fusion learning process. The optimization process can be rewritten as:\n\\[\\underset{\\Theta,\\{\\alpha, \\beta\\}\\} {\\text{min}} \\mathcal{L}_D(\\Theta, \\{\\alpha, \\beta\\})\\]\nWe can observe that the parameters that need to be optimized include the following three categories:"}, {"title": "3.4.1 Selection Stage", "content": "The goal in the selection stage is to jointly learn \\(\\alpha\\) and \\(\\beta\\) given their mutual information. This allows the model to explore different connections and fusion operations during the selection process. The optimization can be formulated as below:\n\\[\\alpha^*, \\beta^* = \\underset{\\Theta,\\{\\alpha, \\beta\\}\\} {\\text{arg min}} \\mathcal{L}_D(\\Theta, \\{\\alpha, \\beta\\})\\]"}, {"title": "3.4.2 Re-train Stage", "content": "In the retraining stage, the parameters need to be optimized only to include the model parameters \\(\\Theta\\). We keep the selected architecture parameters \\(\\alpha^*\\) and \\(\\beta^*\\) fixed, re-train the model parameters \\(\\Theta\\) to obtain the final model. Following previous works [18], the selected connection tensor is determined as \\(\\alpha^* = 1_{a_{i,k}>0}\\) during the re-training stage. With the different ways to conduct fusion operations based on the score, we propose two variants of the proposed model, i.e., OptFusion-Soft and OptFusion-Hard, which refer to soft selection and hard selection of the fusion operations, respectively.\nOptFusion-Soft. In OptFusion-Soft, fusion operations are performed in a soft manner, i.e., combining the weighted summation over all candidate operations according to Equation 13. The final architecture parameter for fusion can be formulated as \\(\\beta^* = \\beta\\).\nOptFusion-Hard. The final fusion operation type is selected with the largest weight based on the learned fusion parameters. This is formalized as: \\(\\beta^* = 1_{\\text{if }o = \\arg \\underset{o \\in \\mathcal{O}}{\\text{max}} p_{j}^{o}}\\) and \\(\\beta^* = 0\\) otherwise.\nAfter obtaining the architecture parameters \\(\\alpha^*\\) and \\(\\beta^*\\) for fusion connection and operation. The model parameters are then re-trained with fixed architecture parameters.\n\\[\\Theta^* = \\underset{\\Theta}{\\text{arg min}} \\mathcal{L}_D(\\Theta, \\alpha^*, \\beta^*)\\]\nThis one-shot selection algorithm allows OptFusion to efficiently explore different architectures during the selection stage and then fine-tune the discovered architecture in the re-train stage. Finally,"}, {"title": "4 EXPERIMENTS", "content": "In this section, to comprehensively validate OptFusion, we design and conduct various experiments over three large-scale datasets, aiming to answer the following research questions:\n\u2022 RQ1: Could OptFusion achieve superior performance compared with mainstream deep CTR prediction models?\n\u2022 RQ2: How efficient is OptFusion compared with mainstream deep CTR prediction models?\n\u2022 RQ3: How does the selection of fusion operation influence the performance?\n\u2022 RQ4: How effective is the one-shot selection algorithm?\n\u2022 RQ5: How compatible is OptFusion with existing components?\n\u2022 RQ6: How does the number of components affect performance?\n\u2022 RQ7: Does OptFusion select the suitable fusion?"}, {"title": "4.1 Experimental Setting", "content": "4.1.1 Datasets. To demonstrate the effectiveness of OptFusion, we evaluate our model on three real-world datasets."}, {"title": "4.1.2 Metrics", "content": "To evaluate the performance of CTR Prediction, we adopt the most commonly-used evaluation metrics [5], i.e., AUC (Area Under ROC) and LogLoss (cross-entropy). Note that 0.1% improvement in AUC is considered significant [5, 26, 41]."}, {"title": "4.1.3 Baselines", "content": "To demonstrate the effectiveness of OptFusion, we compare the performance with four categories of deep CTR prediction models, including (i) stacked models: FNN [43] and PNN [26], DCNv2s [37]; (ii) parallel models: DeepFM [5], DCN [36], xDeepFM [14], DCNv2p [37]; (iii) models with expert design on fusion: EDCN [3]; (iv) NAS models: AutoCTR [31], NASRec [42]."}, {"title": "4.1.4 Implementation Details", "content": "We use Adam [12] as the optimizer for all models and set the embedding size as 40 for the Criteo and Avazu datasets and 16 for the KDD12 dataset. The batch size is fixed at 4096. Following [3], we employ a three-layer MLP with the number of neurons equal to dimemb \u00d7 numfield. We also incorporate an auxiliary shallow block \\(S_0\\) within the candidate set of connections to mimic the stacked structure. We search the optimal learning rate from \\(\\{3e-3, 1e-3, 3e-4, 1e-4, 3e-5, 1e-5\\}\\) and \\(L2\\) regularization from \\(\\{3e-6, 1e-6, 3e-7, 1e-9, 0\\}\\). For OptFusion, during the re-training phase, we reuse the optimal learning rate and \\(L2\\) regularization obtained in the initial training. a in connection search is initialized as 0.5 to ensure equal weight for each block at the start. Hyperparameters used in the experiments are reported."}, {"title": "4.2 Overall Performance (RQ1)", "content": "The overall performance of our OptFusion and other deep CTR prediction models on three datasets are reported in Table 3. We summarize the observations as follows:\nFirst, OptFusion, both soft and hard, outperforms all the SOTA baselines over three datasets in terms of both AUC and Logloss by a significant margin. This demonstrates that OptFusion can effectively find a suitable fusion connection and operation. It also echoes our intuition: fusion learning is an important but overlooked aspect of feature interaction modeling. Specifically, OptFusion improves AUC over the best baseline by 0.0011, 0.0021, and 0.0036 on three datasets, respectively.\nSecond, OptFusion, with a smaller search space, outperforms NASRec and AutoCTR, which contain a larger search space. Such an observation demonstrated that OptFusion could better exploit and explore the fusion search space, while NASRec and AutoCTR's huge search space could potentially lead to sub-optimal results.\nThird, EDCN, which aims to fuse explicit and implicit information densely, constantly performs as the best baseline over all three datasets. This proves that the naive fusion design is an obstacle towards accurate prediction, echoing previous observations [3].\nFinally, the performance of naive fusions varies across datasets. For example, on the Avazu datasets, DCNv2p, a parallel model, exhibits superior performance. On the Criteo and KDD12 datasets, DCNv2s, a stack model, outperforms other baselines. This interesting observation further reveals the limitation of naive fusion design, which we will discuss in Section 4.5."}, {"title": "4.3 Efficiency Analysis (RQ2)", "content": "In addition to model effectiveness, training, and inference efficiency are crucial considerations when deploying CTR prediction models in practice. In this section, we investigate the time complexity of OptFusion. Due to the expansive search space of AutoCTR and NAS-Rec, training efficiency experiments are conducted on an NVIDIA A40 GPU with 48G memory, while inference efficiency experiments are conducted on an NVIDIA RTX 4090 GPU with 24G memory.\nWe illustrate the total training time of NAS models trained on all three datasets in Figure 2 (a). Here, the total training time encompasses both the search and re-train stages. We observe that OptFusion achieves the shortest total training time compared to other NAS models. This is attributed to the narrowed search space for OptFusion and the adoption of a one-shot learning algorithm.\nAs depicted in Figure 2 (b-d), we plot the Inference Time-AUC curve of mainstream deep CTR models trained on three datasets, indicating the relationship between time complexity and model performance. Compared with models such as DCN, DeepFM, and PNN, which achieve the least inference time, both EDCN and OptFusion take fusion design into consideration, and they tend to achieve"}, {"title": "4.4 Ablation Study", "content": "4.4.1 Fusion Operation (RQ3). In this section, we aim to investigate how the fusion operation influences the performance of deep CTR models. We reuse the searched fusion connection and replace the searched fusion operation with four identical fusion operations: ADD, PROD, CONCAT, and ATT, as introduced in Table 1. These models corresponding to four fusion operations are retrained from scratch. Results of OptFusion-soft and OptFusion-hard selection, which adopt different operations for different blocks, in Table 3 are also listed for easy comparison. The results are shown in Table 4.\nWe can easily observe that both Soft and Hard methods exhibit significantly superior performance compared to models with fixed fusion operations. This verifies the effectiveness of our fusion operation search instead of using a fixed fusion operation.\nIn addition, ADD and PROD operations outperform the others. This may be attributed to the fact that element-wise addition and Hadamard products are parameter-free operations, making them easier to train steadily compared to methods incorporating parameters like concatenation and attention pooling."}, {"title": "4.4.2 Selection Algorithm (RQ4)", "content": "In this section, we investigate the search algorithm design. We aim to compare the one-shot selection algorithm, which jointly selects both the connection and operation simultaneously, with a sequential selection algorithm, which sequentially selects the connection and operation. Experiments are conducted over Criteo and Avazu datasets."}, {"title": "4.4.3 Shallow Component (RQ5)", "content": "In this section, we conduct an ablation study on the compatibility of OptFusion over various explicit components. In the default setting, we adopt CrossNet [36] as the explicit component for OptFusion. We further replace CrossNet with CrossNetV2 [37] and CIN [14], and construct its two variants: OptFusion-CrossNetV2 and OptFusion-CIN. The results are summarized in Table 6. We additionally adopt the fusion connection and operation from EDCN as a comparison for all three explicit components, namely EDCN, EDCN-CrossNetV2, and EDCN-CIN.\nFrom the table, we can easily observe that OptFusion and its two variants achieve the best performance on both datasets. These results underscore the robustness and compatibility of OptFusion to different explicit components. Besides, EDCNs constantly rank the 2nd, outperforming the original models. This further indicates the importance of dense fusion in deep CTR models."}, {"title": "4.4.4 Number of Components (RQ6)", "content": "This section evaluates the impact of varying the number of components (n) on OptFusion's performance. The default setting for n is 3. To investigate the effect of different configurations, we also conduct experiments with n = 2 and n = 4. Table 7 summarizes the results, including performance metrics and total training time (h) for each configuration across the Criteo and Avazu datasets.\nOur observations indicate that increasing the number of components (n) results in a marginal improvement in performance metrics at the cost of total training time. For instance, with n = 4, the training time is approximately 1.29 times longer than with n = 3 and about 1.76 times longer than with n = 2. Based on these results, we choose n = 3 as the default configuration for OptFusion, as it offers a balanced trade-off between performance and efficiency."}, {"title": "4.5 Case Study (RQ7)", "content": "This section uses a case study to investigate the selected connection and operation obtained from OptFusion on three datasets. The selected results, including both connection and operation, are shown in Figure 3. For better visualization, we only highlight the operation with the highest probability in OptFusion-soft, which is also the selected operation in OptFusion-hard.\nBased on the result, we can make the following observation: First, the fusion searched on the Criteo dataset exhibits a preference for parallel structure, while on the Avazu and KDD12 datasets,"}, {"title": "5 RELATED WORK", "content": "5.1 Deep CTR Prediction Models\nMost CTR models adopt two naive design fusion designs [35], parallel and stacked. Models with parallel fusion [5, 14, 32, 36, 37] leverage shallow and deep components that explicitly and implicitly model feature interactions, respectively. Fusion operations mainly are addition [5] or concatenation [36, 37]. Models with stacked fusion [6, 7, 15, 26, 37, 40] tend to stack the shallow components before the deep components with concatenation being the common fusion operation [26]. These models mainly advance CTR prediction by proposing various shallow components, such as inner product [26], factorization machine [5], outer product [6], convolutional operator [15], Hadamard product [40] and different customized layers [7, 14, 36, 37] or deep components, such as MLP [43] and Self-attention layer [32], to better model feature interactions.\nResearchers also proposed methods with expert-designed fusion that are beyond parallel and stacked design. EDCN [3] performs a dense fusion strategy and captures the layer-wise interactive signals between the deep and shallow components. FinalMLP [24] proposes a Multi-Head Bilinear Fusion Ops as the fusion operation. EulerNet [34], on the other hand, explores feature interaction learning using Euler's formula, enabling adaptive and efficient fusion of feature interactions in CTR prediction models. However, these proposed solutions [3, 24] tend to consider the fusion learning problem under specific settings.\nOptFusion differs itself by automatically learning connections and selecting operations. Many of the aforementioned methods can be considered as specific instances of the OptFusion framework."}, {"title": "5.2 Neural Architecture Search and its Applications in CTR Prediction", "content": "With the advancement of neural architecture search (NAS) [1, 8, 18, 19, 39], various methods have been proposed in CTR prediction [33], proving valuable for tasks such as determining appropriate embedding dimensions [10], conducting feature selection [17, 21], discovering beneficial feature interactions [16, 22], selecting integration function [11, 20], optimizing hyperparameters [13], designing comprehensive architectures for feature interaction modeling [25], or learning suitable embedding table [23]. Various techniques such as evolutionary approach [27], gradient approach [18], or reinforcement learning-based methods [44] are introduced to obtain suitable search results. Specifically, NAS techniques have also been adopted to search for suitable CTR model structures [31, 42]. Our work distinguishes itself from the existing research by addressing the challenge of fusion learning, a different problem in deep CTR models. The connection learning and operation selection among components are introduced as OptFusion's search space, enabling more efficient and effective model architectures for CTR prediction."}, {"title": "6 CONCLUSION", "content": "In this paper, we address the challenges of fusion learning in deep CTR prediction models and propose OptFusion, which automatically selects suitable fusion connections and fusion operations. OptFusion involves a one-shot learning algorithm designed to effectively conduct both tasks. The model is subsequently retrained with the learned architecture. Extensive experiments on three large-scale datasets demonstrate the superior performance of OptFusion in terms of efficiency and effectiveness. Several ablation studies"}]}